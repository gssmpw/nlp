
\section{Theory}
\label{app:theory}

In this section, we provide the proof of the theorems mentioned in the main text.

\begin{theorem}
     Assume samples in the calibration set $D_\textrm{cal}\sim p$ are independent, and the test set $(\u,\w)\sim \Tilde{p}$ is also independent with the calibration set. Assume $p$ is absolutely continuous with respect to $\Tilde{p}$, then
     \begin{align}
         \mathbb{P}(\score(\u)\in\textrm{CI}_\theta(1-\alpha,D_\textrm{cal})) \geq 1-\alpha.
     \end{align}
\end{theorem}

\paragraph{Proof.} According to Lemma 2 in the previous work \citep{Tibshirani2019ConformalPU}, the calibration set and test set are weighted exchangeable (defined in Definition 1 in the previous work \citep{Tibshirani2019ConformalPU}), with the weight function $\omega$.

According to Lemma 3 in the previous work \citep{Tibshirani2019ConformalPU}, let the $(\u,\w)$ pairs be the random variables, we can obtain that 
\begin{align}
 \mathbb{P}(\score(\u)\leq\score(\pu(\w))+Q(1-\alpha;\Tilde{\S})) \geq 1-\alpha.
\end{align}
Similarly, it can be derived that
\begin{align}
 \mathbb{P}(\score(\pu(\w))-Q(1-\alpha;\Tilde{\S})\leq\score(\u)) \geq 1-\alpha.
\end{align}
Thus      
\begin{align}
    \mathbb{P}(\score(\u)\in\textrm{CI}_\theta(1-\alpha,D_\textrm{cal})) \geq 1-\alpha.
\end{align} \qed

\begin{theorem}
    Trained with $\mathcal{L_\textrm{post-train}}$
    \begin{align}
    \mathcal{L_\textrm{post-train}}=\mathbb{E}_{\t\sim U(1,\T),(\u,\w)\sim p(\u,\w),\bepsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})}
    [e^{-\mathcal{W}(\u,\w)}\|\mathbf{\bepsilon} - \bepsilon_\theta(\sqrt{\bar{\alpha}_\t}[\u,\w] + \sqrt{1-\bar{\alpha}_\t} \bepsilon,\t)\|_2^2],
    \end{align}
    the diffusion model's distribution is $p^*(\u,\w)\propto p(\u,\w) e^{-\mathcal{W}(\u,\w)}.$
\end{theorem}

\paragraph{Proof.} Trained with the diffusion loss
\begin{align}
\mathcal{L_\textrm{diffusion}}=\mathbb{E}_{\t\sim U(1,\T),(\u,\w)\sim p(\u,\w),\bepsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})}
[\|\mathbf{\bepsilon} - \bepsilon_\theta(\sqrt{\bar{\alpha}_\t}[\u,\w] + \sqrt{1-\bar{\alpha}_\t} \bepsilon,\t)\|_2^2],
\end{align}
the diffusion model's distribution is $p$, thus to learn the distribution $p^*$, the loss should be 
\begin{align}
\mathcal{L}&=\mathbb{E}_{\t\sim U(1,\T),(\u,\w)\sim p^*(\u,\w),\bepsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})}
[\|\mathbf{\bepsilon} - \bepsilon_\theta(\sqrt{\bar{\alpha}_\t}[\u,\w] + \sqrt{1-\bar{\alpha}_\t} \bepsilon,\t)\|_2^2] \\
&=\mathbb{E}_{\t\sim U(1,\T),\bepsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})}
[\int\|\mathbf{\bepsilon} - \bepsilon_\theta(\sqrt{\bar{\alpha}_\t}[\u,\w] + \sqrt{1-\bar{\alpha}_\t} \bepsilon,\t)\|_2^2 \cdot p^*(\u,\w) \textrm{d}\u\textrm{d}\w] \\
&\propto\mathbb{E}_{\t\sim U(1,\T),\bepsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})}
[\int\|\mathbf{\bepsilon} - \bepsilon_\theta(\sqrt{\bar{\alpha}_\t}[\u,\w] + \sqrt{1-\bar{\alpha}_\t} \bepsilon,\t)\|_2^2 \cdot p(\u,\w)e^{-\W(\u,\w)} \textrm{d}\u\textrm{d}\w] \\
&=\mathbb{E}_{\t\sim U(1,\T),(\u,\w)\sim p(\u,\w),\bepsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})}
[e^{-\W(\u,\w)}\|\mathbf{\bepsilon} - \bepsilon_\theta(\sqrt{\bar{\alpha}_\t}[\u,\w] + \sqrt{1-\bar{\alpha}_\t} \bepsilon,\t)\|_2^2].
\end{align} \qed



\begin{theorem}
    If the KL divergence of the real distribution $p$ and the distribution $p_\theta$ learned by the model is less than $\epsilon$, then with $1-C\sqrt{\epsilon}$ probability, the quantile $\textrm{Quantile}(1-\alpha;\sum_{i=1}^N w_{\textrm{\textrm{norm}},i}\delta_{v_i})=\textrm{Quantile}(1-\alpha;\sum_{i=1}^N \tilde{w}_{\textrm{norm},i}\delta_{v_i})$, where $w_{\textrm{norm},i}$ and $\tilde{w}_{\textrm{norm},i}$ are the normalized $w_i$ and $\Tilde{w}_i$, $w_i=\frac{p_\theta(v_i) e^{-\mathcal{W}(v_i)}}{p(v_i)}$ and $\tilde{w}_i=e^{-\mathcal{W}(v_i)}$.
\end{theorem}

\paragraph{Proof.} Since $D_{KL}(p||p_\theta)=\int p(x)\log\frac{p(x)}{p_\theta(x)}dx\leq\epsilon$, we can get
\begin{align}
\mathbb{P}(\mathcal{A}=\{|p(x)-p_\theta(x)|>\delta\})
&\leq\frac{\int_\mathcal{A}|p(x)-p_\theta(x)|dx}{\delta} \\
&\leq\frac{\int_\Omega|p(x)-p_\theta(x)|dx}{\delta}\\
&=\frac{2TV(p,p_\theta)}{\delta} \\
&\leq\frac{2}{\delta}\sqrt{\frac{1}{2}D_{KL}(p||p_\theta)} \label{eq:pinsker} \\
&\leq\frac{\sqrt{2\epsilon}}{\delta},
\end{align}
where $TV(p,p_\theta)$ is the total variation distance between $p$ and $p_\theta$. Eq. \ref{eq:pinsker} is obtained using Pinsker’s inequality. This inequality means that with $1-\frac{\sqrt{2\epsilon}}{\delta}$ probability, $|\Delta p(v)|:=|p_\theta(v)-p(v)|\leq\delta$.

Let $\delta$ be a small quantity and $p_-=\min_{i=1,\cdots,N}(p(v_i))$, since
\begin{align}
    \sum_j(1+\frac{|\Delta p(v_j)|}{p_-})\geq\sum_j(1+\frac{\Delta p(v_j)}{p_j})\geq\sum_j(1-\frac{|\Delta p(v_j)|}{p_-}),
\end{align}
and 
\begin{align}
    w_{\textrm{norm},i}=\frac{w_i}{\sum_{j=1}^N w_j}=\frac{(1+\frac{\Delta p(v_i)}{p(v_i)})\Tilde{w}_i}{\sum_{j=1}^N(1+\frac{\Delta p(v_j)}{p(v_j)})\Tilde{w}_j},
\end{align}
thus
\begin{align}
    \frac{1+\frac{\Delta p(v_i)}{p(v_i)}}{1+\frac{|\Delta p(v_i)|}{p_-}}\Tilde{w}_{\textrm{norm},i}\leq w_{\textrm{norm},i}\leq\frac{1+\frac{\Delta p(v_i)}{p(v_i)}}{1-\frac{|\Delta p(v_i)|}{p_-}}\Tilde{w}_{\textrm{norm},i}.
\end{align}
By rearranging terms, we obtain
\begin{align}
    |w_{\textrm{norm},i}-\Tilde{w}_{\textrm{norm},i}|
    &\leq\max\{|\frac{\Delta p(v_i)/p(v_i)-|\Delta p(v_i)|/p_-}{1+|\Delta p(v_i)|/p_-}|, |\frac{\Delta p(v_i)/p(v_i)+|\Delta p(v_i)|/p_-}{1-|\Delta p(v_i)|/p_-}| \} \\
    &\leq|\Delta p(v_i)|\frac{|1/p(v_i)|+|1/p_-|}{1-\delta/p_-} \\
    &\leq C|\Delta p(v_i)| \\
    &\leq C|\delta|. \label{eq:w_diff}
\end{align}
Because we can merge the $v_i$ values that are the same, we can assume $v_1<v_2<\cdots<v_N$. Suppose $\textrm{Quantile}(\alpha;\sum_{i=1}^N w_{\textrm{norm},i}\delta_{v_i})=v_k$, which means
\begin{align}
    \sum_{i=i}^k w_{\textrm{norm},i}\geq1-\alpha, \sum_{i=i}^{k-1} w_{\textrm{norm},i}<1-\alpha.
\end{align}
With Eq. \ref{eq:w_diff}, we can get that
\begin{align}
    \sum_{i=i}^k \Tilde{w}_{\textrm{norm},i}\geq1-\alpha-Ck\delta, \sum_{i=i}^{k-1} \Tilde{w}_{\textrm{norm},i}<1-\alpha+Ck\delta.
\end{align}
We can let $\delta$ be small enough to make $k$ satisfy
\begin{align}
    \sum_{i=i}^k \Tilde{w}_{\textrm{norm},i}\geq1-\alpha, \sum_{i=i}^{k-1} \Tilde{w}_{\textrm{norm},i}<1-\alpha,
\end{align}
which means $\textrm{Quantile}(\alpha;\sum_{i=1}^N w_{\textrm{norm},i}\delta_{v_i})=\textrm{Quantile}(\alpha;\sum_{i=1}^N \tilde{w}_{\textrm{norm},i}\delta_{v_i})$. \qed


\section{Visualization of Experiment Results}
\subsection{1D Burgers' Equation}
In this section, we provide additional visualizations of the control results for the 1D Burgers' equation, as shown in Figure \ref{fig:1d_add}. In these figures, the top row represents the original trajectories corresponding to the control targets, while the bottom row displays the trajectories controlled by \proj. It can be observed that \proj successfully controls the trajectories, preventing boundary violations and guiding them to the desired final state.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{fig/burgers_sample13.pdf}
    \end{subfigure}
    \vspace{6pt}
    
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{fig/burgers_sample19.pdf}
    \end{subfigure}
    \vspace{6pt}
    
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{fig/burgers_sample30.pdf}
    \end{subfigure}
    \vspace{6pt}
    
    \caption{\textbf{Visualization of the 1D Burgers’ equation.}}
    \label{fig:1d_add}
\end{figure}



    
    

\subsection{2D Incompressible Fluid}
Here, we provide additional visualizations of the control problems of 2D incompressible fluid. From the figures, we can observe that \proj can successfully control the smoke to avoid the red hazardous region and reach the target bucket as well.

\begin{figure}[H]
\begin{center}
    \includegraphics[scale=0.4]{fig/2d_fluid_field_combined.pdf}
\end{center}
\caption{\textbf{Visualizations of the 2D incompressible fluid control problem.}.}
\label{fig:2d_add}
\end{figure}


\section{Additional Details for Conformal Prediction}
\label{app:cp}
Conformal prediction is a flexible framework that provides prediction intervals with guaranteed coverage probabilities for new, unseen data points, under the assumption that the data are exchangeable.

\textbf{Theoretical Foundations} The exchangeability assumption is a cornerstone of conformal prediction. It requires that the order of the data points does not affect their joint distribution, meaning that any permutation of the indices yields an identical distribution. In particular, exchangeability holds for independent and identically distributed (i.i.d.) samples, a common assumption in machine learning tasks. Ensuring exchangeability guarantees the validity of the prediction intervals constructed using conformal prediction.

\textbf{Implementation Details} To implement conformal prediction, the dataset is first split into two subsets: a proper training set ($D_\text{train}$) and a calibration set ($D_\text{cal}$). A predictive model \({\mu}_\theta\) is trained on the training set using a specified learning algorithm \(\mathcal{A}\). Once trained, the model generates predictions for the calibration set. These predictions are used to compute `conformity scores', which measure the model's accuracy for each calibration point. Specifically, for each instance \(i\) in the calibration set, the conformity score \(S_i\) is defined as:
   \[
   S_i = |{\mu}_\theta(X_i) - Y_i|, \quad i \in D_\text{cal}.
   \]
Additionally, a worst-case score of \(\infty\) is included to account for extreme scenarios. 

Then \(1 - \alpha\) quantile \(q_{1-\alpha}(S)\) of the set of conformity scores is calculated, where \(\alpha\) represents the desired significance level (\emph{e.g.}, $\alpha=0.05$ for 95\% confidence interval).

Given a new data point \(X_{n+1}\), the prediction interval for its corresponding output is calculated as:
   \[
   \hat{C}_\alpha(X_{n+1}) = \left[{\mu}_\theta(X_{n+1}) - q_{1-\alpha}(S), {\mu}_\theta(X_{n+1}) + q_{1-\alpha}(S)\right].
   \]
This interval provides an estimate for the range within which the true value \(Y_{n+1}\) is expected to lie, with a coverage probability of at least \(1 - \alpha\). Thus, conformal prediction offers a flexible and robust method for constructing prediction intervals that account for both the model’s accuracy and the variability in the data.

\textbf{Theoretical Guarantees} Conformal prediction provides theoretical guarantees for finite samples \citep{vovk2005algorithmic, Lei2016DistributionFreePI}. Specifically, for any new data point, the prediction interval satisfies the following probabilistic bound:
   \[
   P(Y_{n+1} \in \hat{C}_\alpha(X_{n+1})) \geq 1 - \alpha.
   \]
This ensures that the true label $Y_{n+1}$ will fall within the predicted interval at least $1 - \alpha$ percent of the time. This framework, based on the assumption of exchangeability, provides a robust method for generating reliable prediction intervals, even in settings with limited sample sizes.

\section{Details of the Method Implementation}

\subsection{Conditionally Sample $\pu(\w)$}
In our proposed algorithm, we need to sample from the conditional distribution \( p(\u|\w) \) with the model that learns the joint distribution \( p(\u, \w) \). To achieve this, at each denoising step of the sampling process, we replace the noisy \(\w\) in the input of the denoising network with the actual clean \(\w\) that serves as the condition \citep{chung2023diffusion}. In fact, this situation represents a special case of the data distribution that the denoising network encounters during training, where the \(\u\) part is noisy, while the \(\w\) part remains noise-free.

\subsection{Algorithm of Post-training}
\label{app:algpost}
Here we provide the specific algorithm of post-training in Algorithm \ref{algpost}.

\begin{algorithm}[b]
    \small
    \caption{Post-training of \proj}
    \label{algpost}
    \begin{algorithmic}[1]
    \STATE \textbf{Require} Training set $D_{\text{train}}$, Calibration set $D_{\text{cal}}$, coverage probability $\alpha$, number of epochs $N$, number of updating steps per epoch $M$ \\
    \FOR{$n = 1, \ldots, N$} 
        \STATE Compute the shifted score set $\Tilde{\S}$ with $D_{\text{cal}}$ \small{\color{gray}// Eq. \ref{eq:weightedscore}} \\
        \STATE Get the uncertainty quantile $Q(1-\alpha;\Tilde{\S})$ \\
        \FOR{$m = 1, \ldots, M$} 
            \STATE Take gradient descent step on $\nabla_{\theta}\mathcal{L}_{\text{post-train}}$ with $D_\textrm{train}$ \small{\color{gray}// Eq. \ref{eq:posttrain_obj}} 
        \ENDFOR
\hspace{0.8cm} \\
    \ENDFOR \\
    \STATE \textbf{return} $\theta$
    \end{algorithmic}
\end{algorithm}
    
\subsection{Algorithm of Inference-time Fine-tuning}
\label{app:alg}
In this subsection, we provide the entire algorithm of inference-time fine-tuning in Algorithm \ref{alg}.

\begin{algorithm}[ht]
    \small
    \caption{Inference of \proj}
    \label{alg}
    \begin{algorithmic}[1]
    \STATE \textbf{Require} Calibration set $D_{\text{cal}}$, coverage probability $\alpha$, number of iterations $N$ \\
    \FOR{$n = 1, \ldots, N$} 
        \STATE Compute the shifted score set $\Tilde{\S}$ with $D_{\text{cal}}$ \small{\color{gray}// Eq. \ref{eq:weightedscore}} \\
        \STATE Get the uncertainty quantile $Q(1-\alpha;\Tilde{\S})$ \\
        \STATE Sample the control sequence $\w$ with guidance $\G$ \small{\color{gray}// Eq. \ref{eq:guidance}} \\
        \STATE Take gradient descent step on $\nabla_{\theta}\mathcal{L}_{\text{fine-tune}}$ \small{\color{gray}// Eq. \ref{eq:finetune}} 
\hspace{0.8cm} \\
    \ENDFOR \\
    \STATE Sample the control sequence $\w$ with guidance $\G$ \small{\color{gray}// Eq. \ref{eq:guidance}} \\
    \STATE \textbf{return} $\w$
    \end{algorithmic}
\end{algorithm}



\section{Additional Details for 1D Burgers' Experiment}
\subsection{Experiment Setting}\label{app:1dexp}
Following the previous works \citep{holl2020learning, wei2024generative}, we generate the 1D Burgers' equation dataset. This equation is formulated as follows:
\begin{eqnarray}
\begin{cases}
\label{eq:burgers}
    \frac{\partial \u(t,x)}{\partial t} = -\u(t,x)\cdot \frac{\partial \u(t,x)}{\partial x}+\nu\frac{\partial^2 \u(t,x)}{\partial x^2} + \w(t,x)  &\text{in } [0,T] \times \Omega \\
    \u(t,x) =0  \quad\quad\quad\quad\quad\quad\quad\quad &\text{on } [0,T] \times \partial\Omega    \\
    \u(0,x) = \u_0(x) \quad\quad\quad\quad\quad\quad &\text{in } \{t=0\} \times \Omega,
\end{cases}
\end{eqnarray}
where $\nu$ denotes the viscosity parameter, while $\u_0$ signifies the initial condition. We set $\nu=0.01$, $T=1$ and $\Omega=[0,1]$. 

During inference, alongside the control sequence $\w(t,x)$, our diffusion model generates states $\u(t,x)$. Our reported evaluation metric $\J$ is always computed by feeding the control $\w(t,x)$ into the ground truth numerical solver to get $\u_{\text{g.t.}}(t,x)$ and computed following Eq. (\ref{eq:burgers_obj_J_actual}). Following Eq. (\ref{eq:burgers_safety_score}), we consider the safety constraint and define the safety score as $s$. In our experiment, the bound of safety score $s_0$ is set to 0.64, $89.7\%$ of samples are unsafe among the training set, $90\%$ of samples are unsafe among the calibration set and all of the samples in the test set are unsafe. The details of the 1D Burgers' equation dataset for safe PDE control problem are listed in Table \ref{tab:1d_data}.

\begin{table}[ht]
\centering
\caption{\textbf{Details of 1D Burgers' equation dataset.}}
\begin{tabular}{@{}cccc@{}}
\toprule
                    & Training Set & Calibration Set & Test Set \\ \midrule
Unsafe Trajectories & 34,985       & 900             & 50       \\
Safe Trajectories   & 4,015        & 100             & 0        \\ \bottomrule
\end{tabular}
\label{tab:1d_data}
\end{table}

\subsection{Model}
The model architecture in this experiment follows the Denoising Diffusion Probabilistic Model (DDPM) \citep{ho2020denoising}. For control tasks, we condition on \(u_0\), \(u_T\) and apply guidance to generate the full trajectories of \(u_{[0,T]}\), \(f_{[0,T]}\) and the safety score \(s\). The hyperparameters for the 2D-Unet architecture are recorded in Table \ref{tab:1d_hyperparameters}.

\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Hyperparameters of 2D-Unet architecture in 1D experiment.}}
     \label{tab:1d_hyperparameters}
    \begin{tabular}{l|l}
    \multicolumn{2}{l}{}\\
    \hline
      \text {Hyperparameter Name} & {Value}\\ \hline
        Initial dimension          & 128           \\
        Convolution kernel size    & 3             \\
        Dimension multiplier       & {[}1,2,4,8{]} \\
        Resnet block groups        & 1             \\
        Attention hidden dimension & 32            \\
        Attention heads            & 4             \\
        Number of training steps   & 200000        \\
        DDIM sampling iterations   & 100           \\
        $\eta$ of DDIM sampling    & 1             \\ \hline
   \end{tabular}
  \end{center}
\end{table}


\section{Additional Details for 2D Fluid Experiment}
\subsection{Experiment Setting}
\label{app:2dexp}
The 2D environment is modeled by the Navier-Stokes equation:
\begin{eqnarray}
\begin{cases}
&\frac{\partial \mathbf{v}}{\partial t} + \mathbf{v} \cdot \nabla \mathbf{v} - \nu \nabla^2 \mathbf{v} + \nabla p = f, \\
&\nabla \cdot \mathbf{v} = 0, \\
&\mathbf{v}(0, \mathbf{x}) = \mathbf{v}_0(\mathbf{x}),
\end{cases}
\end{eqnarray}
where $\mathbf{v}$ is the velocity, $p$ is the pressure, $f$ is the external force and $\nu$ is the viscosity coefficient. 

Following works from \citet{holl2020learning, wei2024generative, hu2024wavelet}, we use the package \texttt{PhiFlow} to generate the 2D incompressible fluid dataset. The control objective and data generation is the same as before \citep{wei2024generative}. The main difference between our data and previous ones is that we consider the safety constraint here. We define the safety score as the percentage of smoke passing through a specific region. This reflects the need to limit the amount of pollutants passing through certain areas in real-world scenarios, such as in a watershed.

We simulate the fluid on a 128$\times$128 grid. The selected hazardous region is \([44, 36] \times [40, 64]\). Since the optimal path for smoke, starting from a left-biased position, is likely to pass through this hazardous region, this poses a greater challenge for the algorithm: how to balance safety and achieving a more optimal objective, making this a more difficult problem.

\subsection{Model}
In this paper, the design of the three-dimensional U-net we use is based on the previous work \citep{ho2022video}. In our experiment, we utilize spatio-temporal 3D convolutions. The U-net consists of three key components: a downsampling encoder, a central module, and an upsampling decoder.

The diffusion model conditions on the initial density and uses guidance as previous mentioned to to generate the full trajectories of density, velocity, control, the objective $\J$ and the safety score. As $\J$ and $\score$ are scalers, we repeat them to match other channels. The hyperparameters for the 3D U-net architecture are listed in Table \ref{tab:3d-Unet}.

\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Hyperparameters of 3D-Unet architecture in 2D experiments}.}
     \label{tab:3d-Unet}
    \begin{tabular}{l|l} %
    \multicolumn{2}{l}{}\\
    \hline
      \text {Hyperparameter Name} & {Value}\\
      \hline
      Number of attention heads & 4 \\
      Kernel size of conv3d & (3, 3, 3)   \\
      Padding of conv3d & (1,1,1)  \\
      Stride of conv3d & (1,1,1)  \\
      Kernel size of downsampling & (1, 4, 4)   \\
      Padding of downsampling & (1, 2, 2)  \\
      Stride of downsampling &  (0, 1, 1)  \\
      Kernel size of upsampling & (1, 4, 4)   \\
      Padding of upsampling & (1, 2, 2)  \\
      Stride of upsampling &  (0, 1, 1)  \\
      Number of training steps &  200000  \\
      DDIM sampling iterations &  100  \\
      $\eta$ of DDIM Sampling & 1 \\
      Intensity of guidance in control & 100 \\
      Weight of safety term in guidance & 10000 \\
      \hline
   \end{tabular}
  \end{center}
\end{table}

\section{Additional Details for Tokamak Fusion Reactor}
\subsection{Experiment Setting}
Following the previous work \citep{seo2022development}, the environment used in this experiment is a data-driven simulator designed to replicate the plasma behavior in the KSTAR tokamak. It is constructed using long short-term memory (LSTM) neural networks to capture the time-dependent dynamics of plasma, such as transport processes and flux diffusion. The LSTM-based model is trained on five years of experimental data from KSTAR, incorporating various tokamak control variables as inputs, including plasma current, toroidal magnetic field, line-averaged density, neutral beam injection and electron cyclotron heating powers, and boundary shape parameters.

The simulator predicts the evolution of key zero-dimensional (0D) plasma parameters ($1-\alpha_p$, $l_i$, and $q_{95}$) on a 100 ms timescale in an autoregressive manner, using the current state and control inputs. This enables fast and experimentally relevant predictions of plasma responses. Using this model avoids the high costs and risks associated with real tokamak experiments while ensuring sufficient accuracy and efficiency for training.





\section{Baselines}

\subsection{CDT}
Constraints Decision Transformer (CDT) \citep{liu2023constrained} models control as a multi-task regression problem, extending the Decision Transformer (DT) \citep{chen2021decision}. It sequentially predicts returns-to-go, costs-to-go, observations, and actions, making actions dependent on previous returns and costs. The authors propose two techniques to adapt the model for safety-constrained scenarios:

\begin{enumerate}
\item \textbf{Stochastic Policy with Entropy Regularization}: This technique aims to reduce the risk of constraint violations due to out-of-distribution actions. In a deterministic policy, the model selects a single action based on its learned policy, which may result in unsafe actions when faced with states not well represented in the training data. By using a stochastic policy, the model samples actions from a distribution, encouraging the exploration of a wider action space. Entropy regularization further enforces diversity in the sampled actions, making the model more robust in uncertain or underrepresented situations. This approach reduces the likelihood of selecting unsafe actions when faced with states outside the distribution of the training set.

\item \textbf{Pareto-Frontier-Based Data Augmentation}: The technique tries to resolve the conflict between maximizing returns and adhering to safety constraints by leveraging a Pareto-frontier of the training data. The Pareto-frontier consists of trajectories that provide the highest possible return under specific safety constraints. Fitting a polynomial to the Pareto-frontier helps identify conflicting high-return and safety constraint pairs, which are then used for augmentation. The augmentation generates synthetic trajectories by relabeling safe trajectories from the Pareto-frontier with higher returns and assigning higher or equal safety constraints. This encourages the model to imitate the most rewarding, safe trajectories when the desired return given the safety constraint is infeasible.

\end{enumerate}

In the 2D incompressible fluid experiment, the model fails to extrapolate safe trajectories with higher returns due to the training data \textbf{covering a broad range of costs, while the desired cost lies in a narrow range}. The augmentation treats all safe trajectories on the Pareto-frontier equally, without emphasizing the region of interest. 
This complexity, however, could potentially be addressed by SafeConPhy, which is indicated in Table \ref{tab:2d}. We use the official CDT implementation and follow DT guidelines to sweep desired returns and cost constraints in testing time. In the 1D Burgers experiment, we modify the control objective from the original mean squared error $\J$ between the prediction and target, to an exponential form $\exp\left( -\J \right)$. This new objective is bounded within $[0,1]$, which better aligns with the reward-maximizing setup in reinforcement learning used by CDT. For the 2D incompressible fluid setup, the state, action, and cost prediction heads each consist of 3-layer MLPs with the transformer's hidden dimension as the inner size.

\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Hyperparameters of 1D CDT}.}
     \label{tab:1d_cdt}
    \begin{tabular}{l|c|c}
    \hline
    {1D Burgers'} & {All data} & {Safe filtered} \\
    \hline
    State Dimension                          & 256                & 256                \\
    Action Dimension                         & 128                & 128                \\
    Hidden Dimension                         & 1024               & 1024               \\
    Number of Transformer Blocks             & 2                  & 2                  \\
    Number of Attention Heads                & 8                  & 8                  \\
    Horizon (Sequence Length)                & 5                  & 5                  \\
    Learning Rate                            & 1e-4               & 1e-4               \\
    Batch Size                               & 64                 & 64                 \\
    Weight Decay                             & 1e-5               & 1e-5               \\
    Learning Steps                           & 1,000,000          & 1,000,000          \\
    Learning Rate Warmup Steps               & 500                & 500                \\
    Pareto-Frontier Fitted Polynomial Degree & 0                  & 4                  \\
    Augmentation Data Percentage             & 0.3                & 0.3                \\
    Max Augment Reward                       & 10.0               & 10.0               \\
    Min Augment Reward                       & 1.0                & 1.0                \\
    Target Entropy                           & -128               & -128               \\
    Testing Time Sweep Returns               & 9.0, 9.9           & 9.0, 9.9           \\
    Testing Time Sweep Costs                 & 0.0, 1.0, 2.0, 3.0 & 0.0, 1.0, 2.0, 3.0 \\ \hline
    \end{tabular}
  \end{center}
\end{table}

\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Hyperparameters of 2D CDT}.}
     \label{tab:2d_cdt}
    \begin{tabular}{l|c|c}
    \hline
    {2D incompressible fluid} & {All data} & {Safe filtered} \\
    \hline
    State Dimension                          & 3$\times$64$\times$64    & 3$\times$64$\times$64    \\
    Action Dimension                         & 2$\times$64$\times$64    & 2$\times$64$\times$64    \\
    Hidden Dimension                         & 512                      & 512                      \\
    Number of Transformer Blocks             & 3                        & 3                        \\
    Number of Attention Heads                & 8                        & 8                        \\
    Horizon (Sequence Length)                & 10                       & 10                       \\
    Learning Rate                            & 1e-4                     & 1e-4                     \\
    Batch Size                               & 8                        & 8                        \\
    Weight Decay                             & 1e-5                     & 1e-5                     \\
    Learning Steps                           & 1,000,000                & 1,000,000                \\
    Learning Rate Warmup Steps               & 500                      & 500                      \\
    Pareto-Frontier Fitted Polynomial Degree & 4                        & 4                        \\
    Augmentation Data Percentage             & 0.3                      & 0.3                      \\
    Max Augment Reward                       & 32.0                     & 32.0                     \\
    Min Augment Reward                       & 1.0                      & 1.0                      \\
    Target Entropy                           & -(2$\times$64$\times$64) & -(2$\times$64$\times$64) \\
    Testing Time Sweep Returns               & 18.0, 32.0               & 18.0, 32.0               \\
    Testing Time Sweep Costs                 & 0.0, 0.1, 0.2            & 0.0, 0.1, 0.2            \\ \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{BC-All}
The Behavior Cloning (BC) algorithm, introduced by \citep{pomerleau1988alvinn}, is a foundational technique in imitation learning. BC is designed to derive policies directly from expert demonstrations, utilizing supervised learning to associate states with corresponding actions. This method eliminates the necessity for exploratory steps commonly required in reinforcement learning by replicating the actions observed in expert demonstrations. One of the significant advantages of BC is that it does not involve interacting with the environment during the training phase, which streamlines the learning process and diminishes the demand for computational resources.

In this approach, a policy network is trained using standard supervised learning strategies aimed at reducing the discrepancy between the actions predicted by the model and those performed by the expert in the dataset. The commonly used loss function for this purpose is the mean squared error between the predicted actions and expert actions. The dataset for training comprises state-action pairs harvested from these expert demonstrations. In the work, we employ the implementation as \cite{liu2023datasets}.

\subsection{BC-Safe}
Following the baseline in \cite{liu2023datasets}, the BC-Safe is fed with only safe trajectories filtered from the training dataset, satisfies most safety requirements, although with conservative performance and lower rewards. Others are same as BC-All, except for the safe trajectories.

\subsection{SL-Lag}

\citet{hwang2022solving} introduces a supervised learning (SL) based method to control PDE systems. It first trains a neural surrogate model to capture the PDE dynamics, which includes a VAE to compress PDE states and controls into the latent space and another model to learn the PDE's time evolution in the latent space. To obtain the optimal control sequence, SL can compute the gradient $\nabla_{\mathbf{w}} \mathcal{J}$, where $\mathcal{J}$ is the control objective and $f$ is the input control sequence. Then iterative gradient optimization can be executed to improve the control sequence. 

To ensure that the optimal control is compatible with the hard constraint in our experiments, we follow \citet{chow2018risk} to apply the Lagrange optimization method to the constrained optimization. Specifically, we iteratively solve the optimization problem below:
\begin{align}
    \max_{\lambda \ge 0} \min_{\mathbf{w}} \mathcal{J}(\mathbf{w}) + \lambda (s(\mathbf{u}(\mathbf{w})) - c).
\end{align}


We denote the modified SL method SL-Lag. 


\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Hyperparameters of network architecture and training for SL-Lag in 1D Burgers' experiment}.}
     \label{tab:sl_1d}
    \begin{tabular}{l|l} %
    \multicolumn{2}{l}{}\\
    \hline
      \text {Hyperparameter name} & {Value}  \\
      \hline
      Initialization value of $\mathbf{w}$ & 0.001 \\
      Optimizer of $\mathbf{w}$ & LBFGS \\
      Learning rate of $\mathbf{w}$ & 0.1 \\
      Initialization value of the Lagrange multiplier $\lambda$ & 0 \\
      Optimizer of $\lambda$ & SGD \\
      Learning rate of $\lambda$ & 10 \\
      Iteration of $\lambda$ & 2 \\
      Loss function & MSE \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{MPC-Lag}

We adopt the MPC with stochastic gradient descent and choose the planning step $K=1$. The simulation model is the same as the one in SL-Lag. Also, the way we combine MPC with the Lagrangian method is also the same as SL-Lag. The hyperparameters are reported in Table \ref{tab:mpc_1d}.

\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Hyperparameters of for MPC-Lag in 1D Burgers' experiment}.}
     \label{tab:mpc_1d}
    \begin{tabular}{l|l} %
    \multicolumn{2}{l}{}\\
    \hline
      \text {Hyperparameter name} & {Value}  \\
      \hline
      Initialization value of $\mathbf{w}$ & 0.01 \\
      Optimizer of $\mathbf{w}$ & LBFGS \\
      Learning rate of $\mathbf{w}$ & 0.01 \\
      Initialization value of the Lagrange multiplier $\lambda$ & 0 \\
      Optimizer of $\lambda$ & SGD \\
      Learning rate of $\lambda$ & 10 \\
      Iteration of $\lambda$ & 2 \\
      Loss function & MSE \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{TREBI}

In \citet{lin2023safe}, the diffusion model is adopted for the planning task under safety budgets. It generates trajectory under safety constraints using classifier-guidance \citet{dhariwal_diffusion_2021} by adding a safety loss to the reward guidance following Diffuser \citet{janner2022planning}. 

However, the original setting is different from our experiments. 
In our 1D Burgers' equation control, our objective is that a certain state equals the target state, which in-painting diffusion condition \citet{janner2022planning} is more appropriate for. Furthermore, as in our method and in \citep{wei2024generative}, a conditional diffusion model can be learned to tackle the objective more directly. 
In addition, TREBI follows the setting in Diffuser where the interaction with the environment is allowed which in our experiments becomes an MPC method. Note that the reported results of our method do not involve interaction with the surrogate model (though our method can easily adapted to be an MPC method). Thus, the results of TREBI in Table \ref{table:1d} and \ref{tab:2d} have an unfair advantage.

Therefore, for 1D Burgers' experiment, we conducted different experiments on TREBI including (1) planning multiple times with interaction with the surrogate model or (2) planning only once, and with target state conditioning or target state guidance. The target state guidance + planning multiple times turned out the best and is reported in Table \ref{table:1d}. 
For 2D smoke control, the target is not a state constraint but a reward, and the planning multiple-step setting is too computationally expensive. To this end, we use reward guidance with planning one single time, which is identical to the ablation study of our method in Table \ref{tab:ablation}. The hyperparameters of the 1D experiment are reported in Table \ref{tab:TREBI_1d}, and those of 2D are in Table \ref{tab:TREBI_2d}.

\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Hyperparameters of network architecture and training for TREBI in 1D Burgers' experiment}.}
     \label{tab:TREBI_1d}
    \begin{tabular}{l|l} %
    \multicolumn{2}{l}{}\\
    \hline
      \text {Hyperparameter name} & {Value}  \\
      \hline
      Reward guidance intensity & 50 \\
      Safety guidance intensity & 1 \\
      Cost budget & 0.64 \\
      Number of guidance steps & 10 \\
      Denoising steps & 200 \\
      Sampling algorithm & DDPM \\
      U-Net dimension & 64 \\
      U-Net dimension mltiplications & 1, 2, 4, 8 \\
      Planning horizon & 8 steps \\
      Optimizer & Adam \\
      Learning rate & 0.0002 \\
      Batch size & 16 \\
      Loss function & MSE \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Hyperparameters of network architecture and training for TREBI in 2D Incompressible Fluid Control experiment}.}
     \label{tab:TREBI_2d}
    \begin{tabular}{l|l} %
    \multicolumn{2}{l}{}\\
    \hline
      \text {Hyperparameter name} & {Value}  \\
      \hline
      Reward guidance intensity & 5000 \\
      Safety guidance intensity & 0.01 \\
      Cost budget & 0.1 \\
      Number of guidance steps & 1 \\
      Denoising steps & 20 \\
      Sampling algorithm & DDPM \\
      3D U-Net dimension & 8 \\
      3D U-Net dimension mltiplications & 1, 2, 4, 8 \\
      Planning horizon & 8 steps \\
      Optimizer & Adam \\
      Learning rate & 0.0002 \\
      Batch size & 16 \\
      Loss function & MSE \\
      \hline
    \end{tabular}
  \end{center}
\end{table}





\subsection{PID}
Propercentageal Integral Derivative (PID) control \citep{1580152} is a versatile and effective method widely employed in numerous control scenarios. For 1D control task, we mainly implement the PID baseline adapted from \cite{wei2024generative}. More detailed configurations can be found in Table \ref{tab:ANN_PID_architecture}
\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Hyperparameters of network architecture and training for ANN PID}.}
     \label{tab:ANN_PID_architecture}
    \begin{tabular}{l|l} %
    \multicolumn{2}{l}{}\\
    \hline
      \text {Hyperparameter name} & {Value}  \\
      \hline
      Kernel size of conv1d & 3 \\
      Padding of conv1d & 1 \\
      Stride of conv1d & 1 \\
      Activation function & Softsign \\
      Batch size & 16 \\
      Optimizer & Adam \\
      Learning rate & 0.0001 \\
      Loss function & MAE \\
      \hline
    \end{tabular}
  \end{center}
\end{table}
