\section{Related Work}
Besides QMDP~\cite{DBLP:conf/icml/LittmanCK95}, FIB~\cite{DBLP:journals/jair/Hauskrecht00} and point-based methods~\cite{DBLP:journals/ior/Lovejoy91,DBLP:conf/ijcai/ZhouH01,DBLP:conf/icml/Bonet02,DBLP:conf/ijcai/PineauGT03,DBLP:conf/uai/SmithS04,DBLP:conf/uai/SmithS05} which we introduced in \cref{sec:background}, we mention a number of other methods used for computing upper bounds.
Firstly, a number of works consider simplifying the set of reachable beliefs by discretizing the belief space in a similar style as point-based solvers~\cite{DBLP:conf/tacas/BorkKQ22, DBLP:journals/corr/abs-2104-07276, DBLP:conf/iros/WrayZ17, DBLP:journals/rts/Norman0Z17,DBLP:conf/atva/BorkJKQ20}.
Next, \citet{DBLP:conf/aaai/YoonFGK08} introduce `hindsight optimization', which uses deterministic planning in a number of sampled `situations' to approximate the value of a (PO)MDP.
\citet{DBLP:journals/tac/HaughL20} use `information relaxation' in a similar way.
\citet{DBLP:conf/nips/BarenboimI23} consider only a subset of possible outcomes of the transition- and observation function to compute upper bounds in online solvers.
However, all these methods are typically less tight (though computationally cheaper) than our proposed bounds.
Lastly, some bounds are based on the properties of a particular type of POMDP.
For example, \citet{DBLP:journals/cor/Sinuany-SternDB97} consider POMDPs that model maintenance, while \citet{DBLP:conf/aips/KraleS023} consider POMDPs where agents have explicit measuring actions.

Our empirical analysis focuses on \SARSOP~\cite{DBLP:conf/rss/KurniawatiHL08}, but we mention a few related state-of-the-art POMDP solvers.
Firstly, POMCP \cite{DBLP:conf/nips/SilverV10}, and AdaOPS \cite{DBLP:conf/nips/WuYZYLLH21} are both variants of Monte Carlo tree search (MCTS) adapted for POMDPs.
DESPOT \cite{DBLP:journals/jair/YeSHL17} is also based on tree search but uses a method based on hindsight optimization to increase tractability.
Lastly, many methods make use of (deep) reinforcement learning to find approximate solutions to POMDPs \cite{DBLP:conf/aaaifs/HausknechtS15,DBLP:conf/nips/LeeNAL20,DBLP:conf/iclr/HanDT20}.
However, all these methods focus on large (continuous) state-, action- and observation spaces where our bounds are computationally intractable.

\emph{Deterministic Delay MDPs} (DDMDPs)~\cite{DBLP:conf/sigmetrics/AltmanN92,DBLP:journals/tac/KatsikopoulosE03,DBLP:journals/aamas/WalshNLL09} are MDPs where the agent can fully observe its state with some constant delay, which is conceptually similar to FIB and TIB. 
Finding exact solutions to DDMDPs is NP-hard~\cite{DBLP:journals/aamas/WalshNLL09}, but efficient approximate solvers exist~\cite{DBLP:journals/prl/AgarwalA21,DBLP:conf/iclr/DermanDM21}.
However, FIB and TIB  take into account (partial) observations occurring before the state is fully revealed, while such observations do not exist in DDMDPs.
This means that in POMDPs with no observations, FIB and TIB correspond to the solutions of DDMDPs with delays 1 and 2, respectively.
However, solutions for DDMDPs are not sound upper bounds for POMDPs in general, so we do not compare our method with DDMDP solvers.

Lastly, we mention a number of other works related to $\epsilon$-optimal POMDP solving.
\citet{DBLP:conf/aaai/WalravenS17} and \citet{DBLP:conf/uai/HansenB20} propose methods to speed up the incremental pruning of $\alpha$-vectors, which constitutes a considerable amount of the computation time of \SARSOP. 
Relatedly, \citet{DBLP:conf/aaai/DujardinDC17} propose a method that uses less $\alpha$-vectors instead.
\citet{DBLP:conf/aaai/WangPBS06} proposes to use quadratic functions instead of piecewise-linear functions to represent the upper bound.