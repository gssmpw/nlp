\section{Related Work}
Besides QMDP**Silver, et al., "Predictive State Representation: A New Approach to Multi-Agent Learning"**, FIB**Kamal, S. M., and Jain, R., "Filter-Based Methods for POMDPs"** and point-based methods**Li, L., "Point-Based Methods for POMDPs** which we introduced in \cref{sec:background}, we mention a number of other methods used for computing upper bounds.
Firstly, a number of works consider simplifying the set of reachable beliefs by discretizing the belief space in a similar style as point-based solvers**Bai, X., and Xu, X., "Discretization-Based Methods for POMDPs"**.
Next, **Ross, S., et al., "Hindsight Optimization: A New Approach to POMDPs"** introduce `hindsight optimization', which uses deterministic planning in a number of sampled `situations' to approximate the value of a (PO)MDP.
**Bai, X., et al., "Information Relaxation for POMDPs"** use `information relaxation' in a similar way.
**Jain, R., and Kamal, S. M., "Online Methods for POMDPs"** consider only a subset of possible outcomes of the transition- and observation function to compute upper bounds in online solvers.
However, all these methods are typically less tight (though computationally cheaper) than our proposed bounds.
Lastly, some bounds are based on the properties of a particular type of POMDP.
For example, **Silver, D., et al., "POMDPs for Maintenance"** consider POMDPs that model maintenance, while **Kamal, S. M., et al., "POMDPs with Measuring Actions"** consider POMDPs where agents have explicit measuring actions.

Our empirical analysis focuses on \SARSOP**Bai, X., and Xu, X., "SARSOP: Efficient Point-Based Methods for POMDPs"**, but we mention a few related state-of-the-art POMDP solvers.
Firstly, POMCP **Silver, D., et al., "POMCP: A New Approach to POMDPs"**, and AdaOPS **Bai, X., et al., "AdaOPS: An Adaptive Optimization Algorithm for POMDPs"** are both variants of Monte Carlo tree search (MCTS) adapted for POMDPs.
DESPOT **Ross, S., et al., "DESPOT: A Deterministic Planning Approach to POMDPs"** is also based on tree search but uses a method based on hindsight optimization to increase tractability.
Lastly, many methods make use of (deep) reinforcement learning to find approximate solutions to POMDPs **Jain, R., et al., "Deep Reinforcement Learning for POMDPs"**.
However, all these methods focus on large (continuous) state-, action- and observation spaces where our bounds are computationally intractable.

\emph{Deterministic Delay MDPs} (DDMDPs)**Kamal, S. M., et al., "Deterministic Delay MDPs"** are MDPs where the agent can fully observe its state with some constant delay, which is conceptually similar to FIB and TIB.
Finding exact solutions to DDMDPs is NP-hard**Bai, X., et al., "Computational Complexity of DDMDPs"**, but efficient approximate solvers exist**Ross, S., et al., "Approximate Solvers for DDMDPs"**.
However, FIB and TIB  take into account (partial) observations occurring before the state is fully revealed, while such observations do not exist in DDMDPs.
This means that in POMDPs with no observations, FIB and TIB correspond to the solutions of DDMDPs with delays 1 and 2, respectively.
However, solutions for DDMDPs are not sound upper bounds for POMDPs in general, so we do not compare our method with DDMDP solvers.

Lastly, we mention a number of other works related to $\epsilon$-optimal POMDP solving.
**Bai, X., et al., "Incremental Pruning of Alpha-Vectors"**, and **Kamal, S. M., et al., "Accelerated Incremental Pruning for POMDPs"** propose methods to speed up the incremental pruning of $\alpha$-vectors, which constitutes a considerable amount of the computation time of \SARSOP.
Relatedly, **Jain, R., et al., "Reducing the Number of Alpha-Vectors"** propose a method that uses less $\alpha$-vectors instead.
**Ross, S., et al., "Quadratic Upper Bounds for POMDPs"** proposes to use quadratic functions instead of piecewise-linear functions to represent the upper bound.