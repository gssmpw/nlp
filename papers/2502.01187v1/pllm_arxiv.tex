%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{lib/icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{lib/icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Skewed Memorization in Large Language Models: Quantification and Decomposition}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hao Li}{col,equal}
\icmlauthor{Di Huang}{wustl,equal}
\icmlauthor{Ziyu Wang}{uci,equal}
\icmlauthor{Amir M. Rahmani}{uci}
\end{icmlauthorlist}

\icmlaffiliation{col}{Columbia University, USA}
\icmlaffiliation{wustl}{Washington University in St. Louis, USA}
\icmlaffiliation{uci}{University of California, Irvine, USA}

\icmlcorrespondingauthor{Hao Li}{hl3776@columbia.edu}
\icmlcorrespondingauthor{Di Huang}{di.huang@wustl.edu}
\icmlcorrespondingauthor{Ziyu Wang}{ziyuw31@uci.edu}
\icmlcorrespondingauthor{Amir M. Rahmani}{amirr1@hs.uci.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract} 
Memorization in Large Language Models (LLMs) poses privacy and security risks, as models may unintentionally reproduce sensitive or copyrighted data. Existing analyses focus on average-case scenarios, often neglecting the highly skewed distribution of memorization. This paper examines memorization in LLM supervised fine-tuning (SFT), exploring its relationships with training duration, dataset size, and inter-sample similarity. By analyzing memorization probabilities over sequence lengths, we link this skewness to the token generation process, offering insights for estimating memorization and comparing it to established metrics. Through theoretical analysis and empirical evaluation, we provide a comprehensive understanding of memorization behaviors and propose strategies to detect and mitigate risks, contributing to more privacy-preserving LLMs.



\end{abstract}

\section{Introduction}

Large Language Models (LLMs) have revolutionized natural language processing by learning from vast amounts of data to generate coherent and contextually relevant text~\cite{dubey2024llama, wang2024healthq, zhao2024towards}. Despite their impressive capabilities, a critical question persists: to what extent do these models memorize their supervised fine-tuning (SFT) data versus generalize to new, unseen inputs? While LLMs can generate plausible text, they also risk reproducing verbatim passages from their training datasets, leading to significant privacy and security concerns~\cite{carlini2021extracting}. Memorization occurs when a model outputs exact or near-exact replicas of its training data, which can result in the unintended exposure of sensitive information or violations of intellectual property rights.

Recent research indicates that memorization in LLMs during SFT does not affect all training data equally; instead, certain data points are significantly more prone to memorization, leading to a highly \emph{skewed} pattern. For instance, Xie et al.~\cite{xie2024memorization} found that LLMs often rely on memorization when solving logical reasoning tasks like Knights and Knaves puzzles. The models achieved high accuracy on training puzzles but struggled with slightly perturbed versions, suggesting that they memorized specific instances rather than learning underlying reasoning principles. This skewness means that while most data points are generalized over, a small subset contributes disproportionately to the overall memorization risk. Consequently, average-case analyses or mean memorization rates~\cite{carlini2021extracting, carlini2022quantifying, feldman2020does} fail to capture these worst-case scenarios where sensitive information may be leaked, akin to assessing system security based solely on average performance without considering rare but critical failures.

Furthermore, methods relying on sampling to estimate maximum memorization can be biased and may overlook rare but significant instances, particularly under practical constraints like limited sample sizes or computational resources~\cite{carlini2022quantifying, schwarzschild2024rethinking}. Prior studies have also artificially increased memorization by replicating training data multiple times (e.g., thousands of repetitions)~\cite{carlini2021extracting}, which is not reflective of real-world training settings. Such approaches may not accurately identify where memorization is most pronounced within a model or elucidate the factors that contribute to it. This limitation hampers effective comparison of memorization across different models or training configurations and underscores the need for methods that can detect and analyze memorization without unrealistic experimental setups.

In this work, we analyze skewed memorization in LLMs trained with SFT without relying on unrealistic experimental setups like replicating training data multiple times. Employing non-parametric statistical tests that handle skewed data distributions, we capture nuances overlooked by average-case analyses, providing more accurate assessments of worst-case scenarios and valuable insights into model behavior. By decomposing term-wise probabilities in the LLM generation process, our theoretical analysis reveals that data characteristics—such as similarity gaps and local data density—influence memorization likelihood; data points with very few or very many similar neighbors are less likely to be extensively memorized due to underfitting or high local entropy, respectively. Our experimental results show that memorization increases with more training epochs, even as overall loss decreases, indicating that prolonged training exacerbates memorization risks. Altering dataset composition or size significantly affects memorization patterns; mixing datasets or changing sizes leads to noticeable differences in which data points are memorized, highlighting the crucial role of dataset characteristics. By comparing our skewness-aware memorization metrics with traditional metrics like ROUGE~\cite{lin2004rouge} and Levenshtein distance~\cite{yujian2007normalized}, we connect our findings with established evaluation methods, making comparison of results across fields and eras handy.






% \section{Related Work}
\section{Methods}

We propose a \textit{prefix continuation} framework to quantify memorization in LLMs, measuring how many tokens a model recalls beyond a given prefix. To capture its skewed distribution, we use non-parametric sampling to estimate worst-case risks~\cite{feldman2020does, carlini2021extracting}. We analyze how memorization intensifies with training, varies with dataset size and composition, and correlates with embedding-space diversity, linking trends to scaling laws~\cite{kaplan2020scaling, hoffmann2022training}. Finally, we compare our metric to standard text similarity measures, demonstrating its effectiveness in capturing extreme memorization cases. Theoretical proofs and additional comparisons are provided in the Appendix.

% We cut each datapoint in training set to two parts, the strings upto 100th character are fed in the model as input, the rest are compared with output word-by-word as lists separated by blank space. A common case is to compare the consecutive word match \emph{right after the cut}, cumulating from the first output block, we use $n_{pre}$ to denote the number of consecutive match starting from the first output block, and we use $N_{pre}$ to denote the \emph{random variable} of this number.

% When the user manually test the model and tries if the model completes the word after the cut, it is very hard to find much. \emph{A larger sample size, or making a scan of the entire training set population, is recommended to access the distribution, since the worst-case scenario within a sample of data usually underestimates the true worst case.}

% A simple formulation from combinatorics gives the following explanation. Suppose the true cumulative distribution function of memory is $F=P(N_{pre}\leq n)$ , then the probability of one sample failing to include the top k cases is $P(N_{pre} < max(N_{(-k)})) $, the true ratio of samples not being the worst case. $P(\max(n_1,\dots,n_z)< N_{(-k)})= {M \choose {z-r(k)} }/ {M \choose z}$, selecting from non-top-k cases within all samples.

% If the equation above is hard to estimate, there are two common ways to explain this property: 

% First, when the sample size is much smaller than training set, then uniform random sampling with replacement is close to that without replacement, since the probability of sampling on a repeated index is small. In a sample size of $z\ll M$. We have the cumulative density funciton of max of the sample with size $z$,  $P(\max(n_1,\dots,n_z)<n)\simeq \prod_i^z P(N_{pre}< n)$ if model-based, making it possible that the sample max is less than the true max at smaller $z$. Estimate for top k memorization is also biased.

% Second, when we have enough computing power to send the full population into a check, we can establish \emph{true distribution function F}. Therefore, we can sample from the results to get the distribution of the sampled max with respect to the sample size z with the true distribution. This is easily done with bootstrapping z from all M sets. Visualizing this figure explains why memorization is missed out when the ratio of the sample is small.

% The same sampling strategy can also tell us what metrics of the distribution are robust to small samples. For example, one can test the normality with the skewness test and acquire skewness/kurtosis of the distribution, and use resampling to see if these numbers on a smaller sample give the same conclusion.

% Once we have the true distribution skewed, we can think of two cases:For example, in one setup, perhaps a pronoun or location is leaked by everyone, in another setup, all the information for one user is leaked, but nothing is leaked for others. Both cases have a small ratio of memory, but they represent different risks. 

% This brings us to the second and third questions: how does the distribution change with training? and what data points within the data contribute to its long upper tail? and why?

\subsection{Estimating Memorization in LLMs}

Manual inspection of word completions often fails to capture memorization, as worst-case instances are systematically underestimated in small samples~\cite{carlini2021extracting, carlini2022quantifying}. Memorization in LLMs is highly skewed, requiring a distributional approach rather than reliance on mean-based measures. Denote cumulative distribution function $F = \mathbb{P}(N_{\text{pre}} \leq n)$, where $N_{\text{pre}}$ denotes random variable of consecutively recalled tokens from a training example beyond a given prefix, $n_{\text{pre}}[i]$ denotes the actual value at $i$-th data point. The probability that a sample of size $z$ fails to contain any of the top $k$ most memorized cases is given by: $\mathbb{P}(\max(n_1, \dots, n_z) < N_{(-k)}) = \frac{\binom{M - k}{z}}{\binom{M}{z}}$, which approximates how often extreme memorization cases are omitted under random sampling.

Since direct computation is impractical, we estimate $F$ via sampling-based methods. When $z \ll M$, uniform sampling with replacement closely approximates sampling without replacement, and the sample maximum follows: $\mathbb{P}(\max(n_1, \dots, n_z) < n) \approx \prod_{i=1}^{z} \mathbb{P}(N_{\text{pre}} < n)$, implying that smaller sample sizes systematically underestimate extreme memorization. To refine the probability of missing out worst cases, we use non-parametric resampling~\cite{efron1994introduction, feldman2020does}, which provides a non-parametric approximation of memorization distributions and has been widely applied in deep model analysis. To ensure robustness, we examine distributional properties such as normality, skewness, and kurtosis of the true population. 

% Our findings indicate two primary memorization risks: systemic leakage of common attributes such as entity names, and full memorization of a small subset of training samples, both of which raise privacy concerns~\cite{carlini2019secret}. Understanding these dynamics is crucial for mitigating excessive memorization while maintaining model utility.



% \subsection{Where are the checkpoints with Higher Memorization correspond to?}

% We compare the memorization distribution for 3 changes:

% Increment in an epoch in the same training: we hypothesize that the decrement of loss is related to an increment of upper quantiles of memorization by the heuristic that overfitting relates to memorization.

% Using datasets of smaller sizes: we hypothesize that a smaller set of training data leads to higher memorization at the same number of steps because a smaller dataset has lower dimensionality to restrict the optimization of loss and prevent memory. Loss, Memorization, dataset size should scale in some ways that conform to previous studies and heuristics of `scaling law'. We hypothesize that given the training loss scheduler constant, a more rapid decrement of loss happens on a smaller sample of data, which corresponds to higher memory--while this memory can be easily accessed by neither numeric difference in mean nor visually, it is captured by non-parametric test and by visualization of the quantiles.

% Using 2 datasets of the same size but merging a subsample of a base dataset into another and deleting the ones at the same index to keep the size constant. We hypothesize that the merged dataset will have a different memorization distribution as compared to the original, and the subsample will be memorized differently as well, since the distribution of 2 datasets are different, i.e., the heuristic is that they are of topics of different meanings and they restrict the embeddings differently--and we also visualize their embedding as a reference.

% Change in the top k memorization curve is more sensitive than the mean, but to systematically make a test, we need a non-parametric method to test whether two populations are different. A trivial method is to use a rank-sum test when we compare the case when data in two samples are not paired, and a Wilcoxon signed rank test to compare two checkpoints with the same training data point. If we already know the population, we can also visualize the quantiles.

% Note that the training loss is related to the match of output, although it is not possible to make a very tight bound. This is because the hack is not completely the same as the training structure.

\subsection{Checkpoint-Level Memorization Dynamics Across Training}

Understanding how memorization evolves across training is essential for assessing its risks and developing mitigation strategies. We investigate memorization trends by analyzing how distributions change under three key conditions: increasing training epochs, reducing dataset size, and altering dataset composition. Our goal is to determine how these factors influence memorization and whether memorization intensifies in specific scenarios.

As training progresses, memorization is expected to increase, particularly in the upper quantiles, as overfitting leads to stronger memorization of training data~\cite{feldman2020does, carlini2022quantifying, tirumala2022memorization}. While overall loss decreases, extreme memorization cases often become more pronounced~\cite{zhang2021understanding}. Similarly, smaller training datasets provide fewer constraints for generalization, leading to greater reliance on memorization. This follows from established scaling laws in deep learning, where dataset size significantly impacts optimization dynamics and generalization ability~\cite{kaplan2020scaling, hoffmann2022training}. Although mean memorization may remain stable, its effects manifest in the upper tail of the distribution.

Beyond the size of the data set, composition also plays a crucial role in the behavior of memorization. If two datasets differ semantically, their combination modifies the embedding space, potentially altering memorization patterns. We explore whether such changes influence memorization by examining shifts in memorization distributions when subsets of datasets are merged.

To systematically measure these effects, we adopt a \textit{ distribution analysis} approach rather than relying solely on mean-based memorization metrics. The top-$k$ memorization curve provides a more sensitive indicator of extreme cases. To formally compare memorization distributions across training conditions, we employ \textit{non-parametric statistical tests}, ensuring robustness to distributional shifts without assuming specific parametric forms. Since training loss alone does not directly quantify memorization, empirical evaluation remains essential to characterize memorization trends and assess their impact across different training configurations.



% \subsection{Where do the long memorization data points reside?}

% We propose a \textit{prefix continuation} approach to systematically measure memorization in LLMs. Given a training sequence $s$, we split it into a prefix $s_{\text{prefix}}$ containing the first $c$ characters and a suffix $s_{\text{suffix}}$. The model is prompted with $s_{\text{prefix}}$ and generates a continuation $\hat{s}$. We define the \textit{prefix match length} $n_{\text{pre}}$ as:


% $n_{\text{pre}} = \max \{ k \mid \hat{s}_{1:k} = s_{\text{suffix},1:k} \}$

% where $\hat{s}_{1:k}$ represents the first $k$ tokens of the generated output, and $s_{\text{suffix},1:k}$ is the corresponding ground truth segment. The random variable $N_{\text{pre}}$ captures the memorization distribution across the dataset.

% We are interested in the problem of which data points \emph{within} the training dataset correspond to memorization of greater lengths. For convenience, we use $i$ for denoting the data point index from the \emph{training sample}, and $j$ for denoting the index of blocks (in this case, word split by blank space) immediately after the cut. Therefore, $i$ is for indexing the data point within dataset, $j$ is for indexing at lengths. 

% Studying memorization within this further detail yields several benefits other than estimation and making new heuristics. First, we can relate the memorization of any auto-regressive model to its dynamics of generating the next token based on everything previously known. Second, we can bound and compare memorization at different lengths of input given. Third, the formulation of correct memorization at a certain position $j$ makes a handy comparison between the number of consecutive words memorized counting from the cut $N_{pre}$ we used with other well-known metrics. 


% \begin{description}
% \item[$N$] The tested inferencing length

% \item[$M$] Number of text samples in training data

% \item[$R_b$] The Bayesian risk

% \item[$D_{KL}$] Kullback-Leibler divergence

% \item[$\pi_{M}(j|.)$] The estimated distribution for token $j$ given previous truncated sentence

% \item[$\pi(j|.)$] The distribution for token $j$ given previous truncated sentence

% \item[$M_b$] Bayes Optimal Classifier(BOC) from truncated string to the rest of input data with some length

% \item[$M_t$] a term-wise version of Bayes Optimal Classifier(BOC) from truncated string and all terms before $j$ to predict only term $j$

% \item[$M_C$] The Large Language Model (LLM) with the checkpoint C

% \item[$MI(J_{pre},j )$] The mutual information for correctness at $J_{pre}$ terms before j and at j.
% \item[$Comb(j)$] All combinations of indices before j.
% \end{description}



% Examining the memorization in the samples and terms. We analyze this in two directions ($i$ and $j$) and then combine the results.


% % Where does memorization end in a prefix? The termwise direction shows the distribution close to geometric, letting us examine the probability of correct memorization at term [s]. In some cases, if this termwise probability follows some independence, then we can have descriptive statistics for the skewed distribution easily. 


% \subsubsection{Decomposing the tail}

% Since the model generates outputs block by block, the results on the right do not \emph{cause} the results on the left to change. We can use probability conditioned on the previous information directly.

% \begin{theorem}
%     The probability that the consecutive word match from the first term ends exactly at $n_{pre}$ is

%     \begin{equation} \label{eq1}
%         \begin{split}
%         P(N_{pre}=n_{pre}) \\
%          =(\prod_{j=1}^{n_{pre}} P(\text{term j is memorized}|\text{all terms before memorized})) \\
%            *P(\text{term } n_{pre}+1 \text{ fail to be memorized}| \\
%            \text{all  terms before memorized}) \\
%         \end{split}
%     \end{equation}
        
    
% \end{theorem}


% In LLM, the model generates j+1 th block with all information given up to the j th block. In this case, the P is less dependent on whether the previous terms are outputs of itself or the input as long as the values are the same. We denote this as 
% $$\prod_{j=1}^{n_{pre}} \mathbb{P}(j|r_{pre},1 \cdots j-1) \mathbb{P}(n_{pre}+1^c|r_{pre},1 \cdots n_{pre})$$

% Note that the assumption below may be reasonable, but \emph{not necessary} in the case of getting a probability value: When left-padding of model inferencing has little effect, we can assume whether the last term j is the model output or given input has little effect on the next block generated $M(input)[1]\simeq M(input \text{ appended with } M(input)[0])[0]$ to make estimation. 

% All the previous ones succeed, and $n_{pre}+1$ is a failure of the memorized block, given that all the previous ones are correct. For convenience, we use word split by blank spaces as a unit for blocks of generation. The tokenizers are not always word-wise for different models, but we can find cases where they are close.

% \begin{remark}
%     When all $\mathbb{P}(j|r_{pre},1 \cdots j-1)=p$ are equal regardless of $i$ and $j$, $N_{pre}  \sim \textbf{Geom}(1-p)$. 
% \end{remark}

% This condition is not unreasonable. If we evenly distribute the weight of term-wise loss across tokens, this may happen at a small loss. But still, it may be too strict, we release and generalize a little bit, and consider the condition.

% We denote the correctness of memorization at $j$ given inputs $C(j|\text{input})$. $C(j)$ is $0$ if the memorization is correct, $1$ if false. We are interested in whether the model, given all \emph{correct inputs} up to $j$ depends on if the model itself memorizes words prior to $j$ or not.

% $\forall i<l, m<i$,
% \begin{equation}
%     C(j|r_{pre}\cdots j-1) \perp Comb(C(m|r_{pre}\cdots m-1)) \label{Cond2} 
% \end{equation}

% for simplicity, we denote the former $C_{J[pre]}$, the latter $C_j$.

% When the condition above holds, it gives:


% \begin{multline*}
% P(\text{j th term correct}|\\ \text{correct terms before are memorized(by the model)})) \\
% = P(\text{j th term correct}|\\ \text{correct terms before are fed into the model(by a user)}))
% \end{multline*}


% It says the correctness of the next block of result is not influenced by whether the process is that the model memorized all words previous to $j$ given $r_{pre}$, or that I manually give it $r_{pre}$ plus all the previous word before $j$.

% Also, this condition is less strict than the mutual independence we defined previously.

% We compute the inference term-by-term using the correct input \emph{only}. We feed all correct terms before $j$ regardless of whether the model can inference it. It follows from a famous concept in information theory that Mutual Information can check this condition:

% $$MI(C_{J[pre]},C_j ) \forall J_{pre} \in Comb[j]$$


%     Whether the term is exact is binary and discrete, we have $MI(J_{pre},j )=H(j)-H(j|J_{pre})$. without calculating $J_{pre}$ in the case when $p[j]$ gets large and close to 1/2, we can compare the $$MI(J_{pre},j )$$ with $-2*1/2lg(1/2)=1$. Hence small MI indicates little dependency.


% For example,  one of the case is $p ~  \alpha *j + p_0 $ where $B_i$ is individual difference, and $\alpha >0$ is the increment of p per unit block(word). this simplified case is $\mathbb{P}(N_{pre}=n_{pre}) \simeq (\prod_1^{n_{pre}} p_j) (1-p_{n[pre]+1})$ substitute the term-wise probability to get $ (\prod_1^{n_{pre}} (\alpha(j)+p_0)) (1-p_{n[pre]+1})$

% \begin{theorem}

%     This is 
%     $$\mathbb{P}(N_{pre}=n_{pre}) = {p_0}^{(n_{pre})} \frac{\Gamma((n_{pre})+\alpha/p_0)}{\Gamma(1+\alpha/p_0)} (1-p_{n[pre]+1}) 
%     $$ 
%     Since $N_{pre}\leq N$, we let $p_{N+1}=0$.
    
% \end{theorem}

% %maybe this is less important?

% More generally, if we have a list of $p_j \perp i$, we have
% $$(\prod_{j=1}^{n_{pre}} p_j) (1-p_j)$$

% Where independence holds, we can also sample and plot $P(n_{pre}>j-1|{n_{pre} \geq j-1})$ as $p_j$. This does not need term-wise inference.


% \begin{remark}
%     Some entries are remembered more than others, but whether the term k is remembered, given all the previous terms are remembered correctly, is not related to whether any combination of term-wise success memory. 
%     This may gives concern, however, we exemplify in the appendix that this \emph{does not yield a contradiction}.
% \end{remark}

\subsection{Analyzing Skewed Memorization: Identifying Highly Memorized Data Points}

We introduce a \textit{prefix continuation} approach to systematically measure memorization in LLMs. Given a training sequence $s$, we partition it into a prefix $r_{\text{pre}}$ of length $c$ and a suffix $r_{\text{ref}}$. The model, prompted with $r_{\text{pre}}$, generates a continuation $s$. The \textit{prefix match length} $n_{\text{pre}}$ is defined as: $n_{\text{pre}} = \max \{ k \mid {s}_{1:k} = r_{\text{ref},1:k} \}$, where ${s}_{1:k}$ is the first $k$ tokens of the generated output, and $r_{\text{ref},1:k}$ is the corresponding ground truth segment. The random variable $N_{\text{pre}}$ captures the distribution of memorization lengths across the dataset.

We seek to determine which training samples exhibit longer memorization. Let $i$ index training samples and $j$ denote token positions within the suffix. This allows us to analyze memorization both across different training samples and within individual sequences. A deeper examination of this structure provides insights into how autoregressive models generate tokens based on prior context, allows bounding of memorization length distributions, and facilitates comparisons between memorization metrics.

\begin{table}[t]
\small
\centering
\caption{Notation used in the memorization analysis.}
\begin{tabular}{p{1.2cm}p{6.3cm}}
\toprule
\textbf{Symbol} & \textbf{Description} \\
\midrule
$N$ & Maximum tested inference length \\
$M$ & Number of training samples \\
$R_b$ & Bayesian risk of incorrect recall \\
$D_{KL}$ & Kullback-Leibler divergence between learned and true distributions \\
$\pi_{M}(j|.)$ & Estimated token distribution at $j$ given prior tokens \\
$\pi(j|.)$ & True token distribution given prior context \\
$M_b$ & Bayes Optimal Classifier (BOC) for suffix prediction \\
$M_t$ & Term-wise BOC predicting token $j$ given prior tokens \\
$M_C$ & LLM at training checkpoint $C$ \\
$MI(J_{\text{pre}}, j)$ & Mutual information between memorized prefix and token $j$ \\
$Comb(j)$ & Set of all possible combinations of prior tokens before $j$ \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Decomposing Memorization Patterns}

Since LLMs generate tokens sequentially, later outputs do not causally affect earlier ones~\cite{brown2020language}. This enables factorization of the probability of a consecutive match ending at $n_{\text{pre}}$:

% \begin{multline*}
% P(N_{\text{pre}} = n_{\text{pre}})\\
% = 
% \prod_{j=1}^{n_{\text{pre}}} P(j \text{-th term coincides with output} \\ 
% \mid \text{prev. correct tokens up to j-1}) \notag \\
% \quad \cdot P(n_{\text{pre}}+1 \text{-th term fails to coincide with output} \\ 
% \mid \text{prev. correct tokens up to } n_{\text{pre}}). 
% \end{multline*}

\begin{equation*}
\mathbb{P}(N_{\text{pre}}=n_{\text{pre}})= (\prod p_j^o) (1-p_{n[pre]}^o)
\label{decomposition}
\end{equation*}

where $p_k^o$ for each k is the probability that the model memorizes $k$-th block given $r_{pre}$ and given all previous memorization correct.
% this formulation is ugly but the previous is not quite precise in the meaning

Autoregressive models generate token $j+1$ based on all prior tokens, making $\mathbb{P}(j|r_{\text{pre}}, 1 \dots j-1)$ largely independent of whether these prior tokens were model outputs or ground truth inputs: $\prod_{j=1}^{n_{\text{pre}}} \mathbb{P}(j|r_{\text{pre}}, 1 \dots j-1) \cdot \mathbb{P}(n_{\text{pre}}+1^c | r_{\text{pre}}, 1 \dots n_{\text{pre}})$. .

If an approximately uniform token-wise memorization probability $p$ takes place, then the distribution follows a geometric pattern: $N_{\text{pre}} \sim \text{Geom}(1 - p)$. This assumption is reasonable in scenarios where token-wise loss is evenly distributed. However, a more general model incorporates position-dependent memorization probabilities. We denote correct memorization at token $j$ as $C(j|\text{input})$, where $C(j) = 0$ if correct and $1$ otherwise. We test whether memorization at $j$ depends on earlier memorization states using mutual information: $MI(C_{J_{\text{pre}}}, C_j) = H(j) - H(j | J_{\text{pre}})$. 

If $MI(J_{\text{pre}}, j) \approx 0$, memorization at token $j$ is independent of prior correct outputs, validating the geometric approximation. When this condition holds, we have $p_k=p_k^o$, can denote \ref{decomposition} as 
$(\prod^{n_{pre} } p_j) (1-p_{n}[pre])$. To further analyze memorization dynamics, we test cases where the per-token memorization probability follows a linear trend $p_j = \alpha j + p_0$. The expected prefix match length in this setting is:

$
\mathbb{P}(N_{\text{pre}} = n_{\text{pre}}) = p_0^{n_{\text{pre}}} \frac{\Gamma(n_{\text{pre}} +1+ \alpha/p_0)}{\Gamma(1 + \alpha/p_0)} (1 - p_{n_{\text{pre}}+1}).
$

This formulation allows modeling memorization behavior under different training conditions. Empirical verification is performed by plotting $\mathbb{P}(n_{\text{pre}} > j-1 | n_{\text{pre}} \geq j-1)$ as $p_j$, eliminating the need for term-wise inference.

\begin{remark}
Although some training samples exhibit more memorization than others, the probability of token $j$ being memorized is not necessarily dependent on the memorization of earlier tokens. This result holds under the assumption that memorization is driven primarily by local context and token-wise probability distributions.
\end{remark}


% \subsubsection{Diversification in neighborhood=small memorization?}

% We looked at failure of memorization at the length direction $j$ with its (conditional) probability and related values, now we look at the other (data) direction $i$, and quantify which $i$ relates to high length of memorization.

% We begin with a discussion of the mathematic properties of the limiting cases and bounds: BOC estimator up to length $N$, Bayes Optimal Classifier estimator for each term $j$ and compare their behaviors to a limit of LLM max memorization, and LLM greedy inference at the overfitting case to make bounds of the memorization behavior. Some easy derivations relate them to the training loss function, greedy inferencing algorithm, and interestingly, concepts like duplication of data and non-greedy generation with samples have some math properties related to them.


% Then we use the heuristic of embedding similarity to make hypothesis, design an experiment to explore and compare the properties shared by embedding similarity and distances. We also discuss the advantages and limitation of this 'soft' version of the pattern,


% A famous result from the Bayesian Estimator yields
% \begin{theorem}
%     The maximum ratio of the memorization no shorter than n is achieved by the Bayes Optimal Classifier choosing 
%     $s[1\dots n] = M_b(r_{ref}) = argmax_{S^n}\pi(S^n=r^n|r_{ref})$,
%     reaching the lowest possible Bayesian risk
%     $$R = \sum_{r[ref]} P(1- max_{S^n}\pi(S^n=s^n|r_{ref}) |r_{ref})P(r_{ref})$$
% \end{theorem}

% With the same input $r_{ref}$ taking values from possible given prefix texts in the training data, a diversified outcome $r^n$ makes $max_{S^n}\pi(S^n=s^n|r_{ref})$ low, hence making the risk high, making the ratio of memorization low.

% Algorithm to achieve this bound can be decomposed to cases when the input prefix $r_{ref}$ is in the same or similar category, if the candidate data for the sentence later (hence the meaning of entire sentence) is diverse, then memorization is hard. We make a hypothesis that memory happens only when the full sentence is more similar to other sentences when the given prefix is similar.



% This makes a plot; however, we would like to make a finer estimation(not just the worst case bound). This made us look at the problem: what if we choose the outcome with a most frequency given all the prefixes for each term? The Bayes Optimal Classifier. is then formulated by the term-wise Bayes Optimal Classifier.

% \begin{definition}
%     Define $s[j]=M_t(r_{ref}\dots j)$
%     $$M_t(r_{ref}\dots j) = argmax_{s[j]} \pi(s[j]=r[j]|r_{ref}\dots j-1)$$
%     For each term, choose the most probable next term with \emph{all the previous terms given}
%     based on the training data pair. Note that this is not Markov, because the space of input to choose is all the previous term up to j-1. We call this termwise-Bayes Optimal Classifier.
% \end{definition}

% \begin{corollary}
%     Let the classification problem target on predicting next one token among the same inputs. This maximizes $p_j$ for each $j$.
% \end{corollary}

% \begin{remark}
%        This term-wise Bayes Optimal Classifier does not necessarily reach the original Bayes Optimal Classifier bound, we exemplify this difference in the appendix.
% \end{remark}

% Is this a more realistic LLM memorization? Note that there are some assumptions easy to reach, but also some not.

% By definition and using Lagrangian to derive the optimization results,    $\pi_M=\pi$ if a model reaches minimum KL-divergence on the same input-output pair.


% \begin{remark}
%     In this case, LLM's loss is a bit different than this KL divergence here because we have training prompt and padding that goes beyond the $r_{ref}$ pair.  
% \end{remark}

% We review two conditions that make let LLM getting close to a termwise algorithm under inferencing of small temperature. The first is strict, the second less strict. Instead of bounding the probability to memorization of the entire sequence, this bounds $p_j$. 


% \begin{remark}
%      A famous result is that the cross-entropy estimation is equivalent to KL-Divergence estimation. As we optimize $D_{KL}(\pi || \pi_{M})$, $\pi_M \rightarrow \pi$.
% \end{remark}

% \begin{theorem}
%     The greedy inference by term of LLM reaches $M_t$ if and only if the estimated token(next term) distribution of the model given the previous term and the data distribution of the next term given the previous term have the same maximum.
%     $\text{argmax}_{\pi[M]} \pi_M(s[j])=\text{argmax}_{\pi} \pi(r[j])$
% \end{theorem}

% This is less strict than  $\pi_M=\pi$,by set inclusion. While two random variables share the same distribution, implying their argmax to be the same, the converse is not necessary. Having two distributions with the same term being the maximum is much easier, which is in favor of the idea that in some overfitting cases, the greedy algorithms may be close to term-wise Bayes Optimal Classifier. 

% Limitation is that optimizing each $p_j$ does not necessarily optimize the probability that a sequence of length $N$ is memorized entirely correct. Note that the difference between the two limits occurs and only occurs at duplications when the total length we care is conditioned. We have the property below.

% \begin{theorem}
%     Suppose BOC gives memorization with expectation higher than that of term-wise BOC up to length N, then there must be at least one pair of data replicated at some length.
% \end{theorem}


% Also, we claim that for term-wise generation, any term-wise random sampling does not get the rate of correctness at this specific term higher than term-wise Bayes Optimal Classifier

% \begin{theorem}
%     We define the index of possible outcome from the generation process by g. If this generation sampling g is independent of data sampling z within all the indices sharing the same input $r[z]_j=r_{pre}[j]=r_{ref}\dots j$ 
%     $$s_j[z][g] \perp r_j[z] |r_{ref}\dots j$$
%     One can think of this as generation process sampling cannot see the data index.
%     When the argmax of the term of greedy generation coincides with the true argmax, using a non-greedy generation sampling does not increase the expected rate of term-wise memorization.

%     \begin{equation}
%         \begin{split} 
%             P(s_j[g]=r[j]|r_{ref}\dots j)\\
%             \leq p(r_j[z]=argmax_{y} p(y|r_{ref}\dots j) ) \\
%             = \max p_j
%         \end{split}
%     \end{equation}


% \end{theorem}

% So the data with output residing close to others may have lower memorization. To make a tighter analysis: This makes term-wise BOC. The prefix memory length $n_pre$ is only interrupted when the conditional entropy is high, i.e., for each given input sequence of length up to s-1, there are many choices of the next term. The best memorization model is the BOC at this term, which LLM may get close to. At this term, if the term later varies, then it decreases the max chance a deterministic function can memorize. If the output is non-deterministic, in the case when generation choice is independent of sampling, the expected memorization cannot be improved. 

% The embedding of a sentence is a function of the string, which is not guaranteed to be a bijection, hence, there is no guarantee that 2 different sentences have the different embeddings. However, the embedding reflects the model's distinguishment of the meaning of the sentences, and, by cosine rule, the ordering of the embedding similarity preserves the ordering of Euclidean distance. The hypothesis is that for the data with high memorization $n_pre$, the mean similarity between the embedding of the full sentence to that of others is subtracted by the mean similarity between the embedding of the input sentence to that of other input sentences within the same dataset. The converse is not necesarily true.

\subsubsection{Memorization and Embedding Diversity}

Memorization in LLMs varies across training samples, with certain sequences exhibiting longer recall. We hypothesize that memorization correlates with the diversity of possible continuations for a given prefix. If a prefix appears in multiple training examples but leads to varied completions, memorization is less likely due to high entropy in suffix prediction~\cite{feldman2020does, jagielski2022measuring, carlini2022quantifying}. Conversely, when a prefix consistently maps to a single completion, memorization is more probable~\cite{zhang2021understanding}. Prior work supports that LLMs are more likely to memorize deterministic mappings~\cite{tirumala2022memorization, brown2020language}, reinforcing the role of training data structure in memorization risk.


To formalize this intuition, we compare memorization under different modeling assumptions: (i) the \textit{Bayes Optimal Classifier (BOC)}, which minimizes classification risk given prior knowledge~\cite{shalev2014understanding}, (ii) the \textit{term-wise Bayes Optimal Classifier} ($M_t$), which optimally predicts each token given its context, and (iii) \textit{LLM inference dynamics}, which approximate these classifiers in practice~\cite{zhang2021understanding}. This comparison provides insight into when and why memorization occurs.

The \textit{Bayes Optimal Classifier} provides an upper bound on memorization by selecting the most probable suffix $s[1:n]$ given a prefix $r_{\text{ref}}$:

\begin{theorem}
    The maximum fraction of memorization no shorter than $n$ is achieved by the Bayes Optimal Classifier:
    \begin{equation*}
        s[1:n] = M_b(r_{\text{ref}}) = \arg\max_{S^n} \pi(S^n = r^n \mid r_{\text{ref}}),
    \end{equation*}
    with expected Bayesian risk:
    \begin{equation*}
        R = \sum_{r_{\text{ref}}} \left( 1 - \max_{S^n} \pi(S^n = s^n \mid r_{\text{ref}}) \right) \mathbb{P}(r_{\text{ref}}).
    \end{equation*}
\end{theorem}

This result establishes that memorization likelihood depends on the entropy of suffix distributions conditioned on the prefix. When suffix diversity is high, memorization is less likely, aligning with prior findings that memorization is more common in low-entropy regions of training data. To refine this intuition, we analyze memorization at the token level using a term-wise classifier.

\begin{definition}
    The \textit{term-wise Bayes Optimal Classifier} predicts the most probable next token given prior tokens:
    \begin{equation*}
        M_t(r_{\text{ref}}, \dots, j) = \arg\max_{s[j]} \pi(s[j] = r[j] \mid r_{\text{ref}}, \dots, j-1).
    \end{equation*}
\end{definition}

This formulation decomposes memorization into per-token probabilities. If memorization follows a token-wise process, then maximizing $P(s_j = r_j \mid r_{\text{ref}}, \dots, j)$ should yield the most likely continuation. In  unless there are at least a pair of data replicated at some length

\begin{corollary}
    The term-wise classifier satisfies:
    \begin{equation*}
        \mathbb{P}(s_j = r_j \mid r_{\text{ref}}, \dots, j) \leq \max p_j.
    \end{equation*}
\end{corollary}

\begin{theorem} We define the index of possible outcome from the generation process by g. If this generation sampling g is independent of data sampling z within all the indices sharing the same input $r[z]_j=r_{pre}[j]=r_{ref}\dots j$ $$s_j[z][g] \perp r_j[z] |r_{ref}\dots j$$ One can think of this as generation process sampling cannot see the data index. When the argmax of the term of greedy generation coincides with the true argmax, using a non-greedy generation sampling does not increase the expected rate of term-wise memorization. \begin{equation} \begin{split} P(s_j[g]==r_j|r_{ref}\dots j)\\ \leq p(r_j[z]=argmax_{r[j]} p(r_j|r_{ref}\dots j) ) \\ = \max p_j \end{split} \end{equation} \end{theorem}


If LLMs behave as $M_t$, their memorization should align with per-token likelihood optimization rather than full-sequence memorization. Under small-temperature inference, if $M_t$ converges to the empirical token distribution, then: $D_{\text{KL}}(\pi \parallel \pi_M) \rightarrow 0 \Rightarrow M_t \approx \pi$. Thus, optimizing LLM loss aligns with $M_t$ rather than $M_b$, a property observed in large-scale training~\cite{kaplan2020scaling}.

\begin{theorem}
    The LLM greedy decoding process approximates $M_t$ if and only if: $\arg\max_{\pi[M]} \pi_M(s[j]) = \arg\max_{\pi} \pi(r[j])$.
\end{theorem}

This condition is weaker than full distributional convergence ($\pi_M = \pi$), meaning that LLMs may behave similarly to term-wise classifiers even if they do not fully match empirical distributions. This partially explains why LLMs can generalize while still memorizing certain phrases.



Since term-wise memorization is data-dependent, we hypothesize that memorization length is inversely correlated with local embedding diversity. Given an embedding function $f$, we define the \textit{embedding similarity gap}: $\Delta S = S_{\text{full}} - S_{\text{input}}$, where $S_{\text{input}}$ measures mean cosine similarity of embeddings between prefixes, and $S_{\text{full}}$ measures similarity between full sequences. If $\Delta S$ is large, suffixes vary more than prefixes, leading to lower memorization likelihood.

\begin{remark}
    Training samples with highly similar prefixes but highly diverse full sequences exhibit lower memorization probability. This follows from the term-wise classifier formulation, as suffix variability increases classification entropy, reducing sequence-level memorization.
\end{remark}

To empirically validate this hypothesis, we measure memorization length $N_{\text{pre}}$ against $\Delta S$ across different training configurations. This enables an assessment of how embedding space structure influences memorization, a key factor in privacy-aware training strategies.



% \subsection{Other Metrics for ease of comparison to other papers}

% The theorems above giving some limiting case, inspire us to compare memory length of $i$ th data and similarity between the $i$ th input and all inputs, and similarity between the $i$ th full data and all full data. And also making comparisons of loss and data size. Now, we think of comparing the result for consecutive matches from the beginning of the outputs $n_{pre}$ to other metrics.

% A by-product is that the decomposition at length index $j$ and data index $i$ give us formulations making bounding, estimation, and explanation handy. 

% If we define $d_w(s, r) = \sum_1^n \vec w_j c_j(s,r), w_j=w^{-j}$

% $
% \text{non-inplace matched word instance count}  \geq LCS \\
% \geq n_{max} \geq \text{Length of in-place matched n-gram} \geq n_{pre} \\
% \dots \geq min(N1,N2)-d_1\\
% = \text{Number of in-place matched word} 
% $

% Also, $d_1 \geq max(N)-LCS \geq d_{Levenshtein}/2 $. 
% For $w>2$, restricting to length N, comparing $d_r$ is equivalent to comparing $n_{pre} $.

% $
%     d_r(s[i], r[i]) > d_r(s[i'], r[i'])  \\
%     \iff n_{pre}(s[i'], r[i']) > n_{pre}(s[i], r[i])
% $

% In $w=3$ case $d_3 = \sum 3^{-j} c_j<\sum c_j<d_1$, and in the case when $n_{pre}+1$ block is wrong, we have the inequality between $d_1, d_3, n_{pre}$ as follows,
% $
%     d_1* 3^{-n_{pre}}\frac{1}{1-1/3} \geq 3^{-n_{pre}}\frac{1}{1-1/3}= \sum_{n_{pre}}^{\infty} 3^{-i} \geq d_3 \\
%     \geq \sum_{n_{pre}+1}^{\infty} 3^{-i}=3^{-(n_{pre}+1)}\frac{1}{1-1/3}
% $

% $\text{Rouge-L}=\frac{LCS}{N2} \geq \frac{n_{max}}{N2} \geq \frac{n_{pre}}{N2}$. For Rouge-n, denominators are $N_2-n+1$ , $N_1-n+1$ respectively.  The nominator is lower bounded by $n_{max}-n+1\geq n_{pre}-n+1$. 



\section{Experiments and Results}

Our initial trails revealed an unexpected challenge: memorization proved remarkably difficult to detect in domain-specific datasets. This observation led us to examine how dataset characteristics, specifically blending and size, influence memorization patterns. Upon collecting a larger sample, we discovered that the distribution of memorization instances closely approximated a geometric distribution, guiding our research toward decomposition analysis.

\subsection{General Setup}

% Our objective is to 1. examine whether memorization is more readily accessed when the attack data originates from a training domain that closely resembles the original dataset, as opposed to a more diverse training domain. 2. establish the effect of scaling on memorization with respect to dataset size and the induced changes on loss and embedding properties.



% Our objective is to: 1. find memorization 2. compare dataset blends and sizes on their effect on memorization with induced training loss 3. decompose and estimate memorization \emph{within the training dataset} on data index direction $i$ and on length index direction $j$.

% For the baseline case, we utilized Lavita-Medical-QA\footnote{\url{https://huggingface.co/datasets/lavita/medical-qa-datasets}}, a medical question-answering dataset obtained from Hugging Face. The dataset originally comprises multiple-choice questions, which we modified by converting them into single-answer question sets. We extracted 9,723 medical question-answer pairs from Lavita-Medical-QA, serving as a single-domain dataset. To create a blended dataset incorporating diverse topics, we utilized GPTeacher-General-Instruct\footnote{\url{https://huggingface.co/datasets/teknium/GPTeacher-General-Instruct}}, a modular dataset collection generated by GPT-4. We randomly selected 9,723 samples from GPTeacher to match the dataset size, ensuring a comparable dataset structure. Unlike Lavita-Medical-QA, which is domain-specific, GPTeacher represents a general, open-domain question-answering dataset. Details of both datasets are provided in the Appendix.


% We employed the Llama-3.1-8b-Instruct\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}} model from Hugging Face, fine-tuning it in a two-stage process using LoRA followed by inference. For inference, we leveraged vLLM~\cite{vllm}, a high-throughput, memory-efficient engine optimized for LLM inference, using a temperature of 0 to ensure deterministic outputs. Further implementation details are provided in the appendix.

% We cut each data point in training set to two parts, the strings up to 100th character are fed in the model as input, the rest are compared with output word-by-word as lists separated by blank spaces. A common case is to compare the consecutive word match \emph{right after the cut}, cumulating from the first output block, we use $n_{pre}$ to denote the number of consecutive match starting from the first output block, and we use $N_{pre}$ to denote the \emph{random variable} of this number.


For the baseline case, we utilized Lavita-Medical-QA\footnote{\url{https://huggingface.co/datasets/lavita/medical-qa-datasets}}, a medical question-answering dataset from Hugging Face. Originally formatted as multiple-choice questions, we modified it into a single-answer question set, extracting 9,723 question-answer pairs to form a domain-specific dataset. To introduce diversity, we incorporated GPTeacher-General-Instruct\footnote{\url{https://huggingface.co/datasets/teknium/GPTeacher-General-Instruct}}, a GPT-4-generated modular dataset. We randomly sampled 9,723 records from GPTeacher to match the dataset size, ensuring a comparable structure. While Lavita-Medical-QA is domain-specific, GPTeacher provides open-domain question-answering data. Further details on both datasets are provided in the Appendix.

For modeling, we employed Llama-3.1-8b-Instruct\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}}, fine-tuning it in a two-stage process with LoRA before inference. Inference was conducted using vLLM~\cite{vllm}, a high-throughput, memory-efficient engine optimized for LLM inference, with a temperature of 0 to ensure deterministic outputs. Implementation details are elaborated in the Appendix.

To quantify memorization, we segmented each training sample into two parts: the first 100 characters were provided as input to the model, while the remaining portion was used for evaluation. The generated output was compared to the reference text on a word-by-word basis, treating words as lists separated by blank spaces. A key measure is the onsecutive word match $n_{\text{pre}}$ occurring immediately after the cut, accumulating from the first output block. 
% We define $n_{\text{pre}}$ as the number of consecutive matches starting from the first output block and treat $N_{\text{pre}}$ as the \emph{random variable} representing this quantity.




% \subsection{Finding Memorization}


% From our method section, \emph{a larger sample size, or making a scan of the entire training set population, is used to access the distribution, when the shape of distribution is unknown, and the worst-case scenario within a sample of data usually underestimates the true worst case.} We use the \emph{entire population}.

% We are able to visualize the quantiles, skewness and \emph{probability of missing out some worst quantities if we are to use a smaller sample}.

% Memorization is observed across all evaluated cases, though instances of long prefix matches remain rare, and the overall mean memorization is low. The distribution of $N_{\text{pre}}$ exhibits a highly skewed structure, with substantial variation in the tail probabilities, consistent with prior findings on memorization in large language models \cite{carlini2021extracting, carlini2022quantifying}. To illustrate this, we visualize quantiles and skewness in Figure~\ref{fig:lavita_memorization} and perform a simulation to estimate the probability of missing high-memorization instances with result in Figure~\ref{fig:prob_missing_max_density}, as outlined in the methods section.

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/Probability_of_missing_the_max_density.pdf}
%     \caption{Distribution of sampled maximum w.r.t. sample size against true maximum in the base case}
%     \label{fig:prob_missing_max_density}
% \end{figure}



% \subsection{Where are the checkpoints with Higher
% Memorization corresponds to?}

% Memorization differ by data, data size and loss as conjectured. The memorization in the base case with Lavita dataset \ref{fig:lavita_memorization} shows max and 99.5-percentiles increasing as loss goes down. This corresponds to the relation between memorization and overfitting. However, in the base case, max (worst) memorization is already high at the epoch 10 checkpoint, suggesting memorization of some specific datapoint may happen well before the shrinkage of loss(at that point the training loss is above 1.0)--suggesting that bounding the worst cases of memorization with loss might be a very limited approach.





% For the baseline case, Lavita has 9, 723 observations. We aim to make a similar dataset of a different range of topic to make a comparison, and check what if we merge some lavita data into it. 200 datapoints are taken from Lavita as the 'subsample' with their indices recorded.

% We filtered gpteacher to have 'input' string character length in between 150 and 350, response string length 10 to 350, then sampled with size 9, 723 as well. We use the 200-observation subset to overwrite this dataset to make a mixed dataset (called 'mixed') at locations with \emph{exactly same indices}.




% The distribution of $N_{pre}$ has high skewness at \emph{every checkpoint tested}. At around 20 epochs, there is a peak of high skewness, and skewness decreases and then increases during training. Similar pattern is found within the mixed data \ref{fig:mixed_memorization} with same number of rows of training data also shows high skewness, changing max with respect to loss, but different memorization distribution, addressing our concern of limitation of average-based evaluation. In both cases, there is a noticable gap between the 99.875-percentile and the maximum. In the case when the distribution is very right-skewed, it is not suprising that the trend of the maximum with high leverage would influence the skewness factor, and that in the first stages, increment of the factors below 99.875-percentile, relates to decrement of skewness. From a memorization point of view, the output of the model for most data points changing at the first stage, and there seems to be a local over-fitting in the second stage, but whether this shape of 2 regions of maximum implies other properties of the data or training mechanism needs more investigation.




% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/paper_minimal_Memory~Loss Duality_lavita.pdf}
%     \caption{Memory versus Loss for Lavita}
%     \label{fig:lavita_memorization}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/paper_minimal_Memory~Loss Duality_mixed.pdf}
%     \caption{Memory versus Loss for Mixed}
%     \label{fig:mixed_memorization}
% \end{figure}



% Comparing the smaller subsample residing in the base case and the mixed case, we have the comparison \ref{fig:comparison}. We plug in the signed-rank test to make a comparison between checkpoints trained on the same data and shows significance in this case, but in a broader and theoretical distribution. The test with slightly smaller sample shows similar results.

% We also note that besides showing statistical significance, the difference should be substantial to make practical sence for interpretations. A more detailed look shows that most data points in this smaller sample are memorized equally long, but some other points have been memorized very differently--the difference in the memorization of the same data point that resides in different dataset mix also shows a distribution that have very high variance and non-normal. One should note that even a number of points are memorized equally, the equal cases are the ones with very low $n_{\text{pre}}$. This question the safety of using blended data from one topic merged into a dataset on broader topic. As not only does the worst case of the entire dataset get worse, the samples of individual observations also have different memorization as residing in different dataset blend.


% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/comparison.pdf}
%     \caption{The sampled data when trained within lavita versus trained within the mixed dataset}
%     \label{fig:comparison}
% \end{figure}

% Interested in conforming to some scaling-law, we experiment with what happens on a smaller random sample. 
% We made subsamples of lavita of size $2^8$, $2^9$ and $2^{10}$, called 'smaller lavita'. With same LR scheduler, the training on a smaller dataset yields lower loss per step, but a much more systematical view is by viewing epoch, loss, quantiles of memorizations and training dataset size together in \ref{fig:smaller_lavita}.

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/smaller_lavita_comparison.pdf}
%     \caption{Memorization for training on a smaller subsample of the same dataset}

%     \label{fig:smaller_lavita}
% \end{figure}

% Some trivial findings include that max memorization and loss are related but are not very related to the size of the data while sampled from the same population of larger data. Smaller dataset corresponds to less steps per epoch so less increment of max memorization per increment of epoch, but the dataset is of less dimension, so lower loss is easier to reach per step. In the case of small sample size, all three set of trained checkpoints reached very low loss and high word memory, suggesting regularization or pruning may still be needed for fine-tuning on small sample, which relates to private projects or businesses with data difficult to acquire.

\subsection{Checkpoint-Level Memorization Trends}

Memorization varies across training checkpoints and is influenced by dataset size, composition, and loss. Our analysis reveals that while memorization generally increases as loss decreases, worst-case memorization can emerge early in training. Figure~\ref{fig:lavita_memorization} illustrates this trend in the Lavita-Medical-QA dataset, showing increasing max memorization over 100 epochs.


% This refined visualization on the entire training dataset also shows that the maximum memorization rises without the necessity of having very low loss or high epochs. \ref{fig:lavita_memorization} shows max memorization with length greater than 10 even at epoch 10, when training loss remains above 1.0. Figure~\ref{fig:mixed_memorization} shows 
% maximum around 50 as at the first checkpoint at 10th epoch as well. This suggests that worst-case memorization may not be tightly bounded by loss, highlighting the limitations of using loss alone as a gauge of memorization.

This comprehensive analysis of the full training dataset reveals that maximum memorization can increase independently of achieving extremely low loss or high epochs. As shown in Figure~\ref{fig:lavita_memorization}, memorization surpasses a length of 10 as early as epoch 10, despite the training loss remaining above 1.0. Similarly, Figure~\ref{fig:mixed_memorization} demonstrates a maximum memorization length of around 50 at the first checkpoint (epoch 10). These findings indicate that worst-case memorization is not strictly constrained by loss, underscoring the limitations of relying solely on loss as a proxy for memorization.

% To examine dataset effects on the memorization on the entire training-set, we introduce a second dataset by sampling 9,723 examples from the GPTeacher dataset, ensuring a comparable distribution in sequence lengths. We then construct a mixed dataset by replacing 200 randomly selected samples with corresponding examples from Lavita-Medical-QA, maintaining index consistency. The result shows much higher quantiles of memorization at same number of epochs compared to that of Lavita-Medical-QA--from the first checkpoint at epoch 10, the max memorization in the mixed set reached above 40 as compared to less than 16 in the first checkpoint of Lavita-Medical-QA. On the last checkpoint of 100 epochs, it reached above 50 for max memorization as compared to 16 in Lavita-Medical-QA.

To investigate the impact of dataset composition on memorization across the entire training set, we introduce a second dataset by sampling 9,723 examples from GPTeacher, ensuring a comparable distribution in sequence lengths. A mixed dataset is then constructed by replacing 200 randomly selected samples with corresponding examples from Lavita-Medical-QA while maintaining index consistency. Results indicate significantly higher memorization quantiles in the mixed dataset compared to Lavita-Medical-QA alone. At the first checkpoint (epoch 10), maximum memorization in the mixed dataset exceeds 40, whereas Lavita-Medical-QA remains below 16. By epoch 100, maximum memorization in the mixed dataset surpasses 50, while Lavita-Medical-QA remains at 16, demonstrating that dataset composition plays a crucial role in memorization dynamics.

We are also interested in the subsample we used that resides in both data. We compare the memorization of the 200 overlapping samples in the Lavita-Medical-QA and mixed datasets (Figure~\ref{fig:comparison}). A signed-rank test confirms a statistically significant difference in memorization between the two settings, reinforcing that memorization is not solely a property of an individual data point but also of the dataset context. Interestingly, while many samples exhibit similar memorization across datasets, a subset shows significantly different memorization lengths, with high variance and non-normal distribution. This highlights a potential risk: blending data from different domains can exacerbate worst-case memorization while altering per-instance memorization behavior in unpredictable ways.


% less important but intersting:
% The memorization distribution remains highly skewed at every training checkpoint, with a peak in skewness observed around epoch 20. As shown in Figure~\ref{fig:mixed_memorization}, the mixed dataset exhibits a different memorization distribution, underscoring the limitations of mean-based memorization assessments. Notably, the gap between the 99.875th percentile and maximum remains high [value], suggesting that extreme memorization cases persist even as overall trends shift.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//llama3/paper_minimal_Memory_Loss_Duality_lavita.pdf}
    \caption{Memorization trends across training epochs in Lavita. Maximum and high-percentile memorization increase as loss decreases, but extreme cases appear early in training.}
    \label{fig:lavita_memorization}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//llama3/paper_minimal_Memory_Loss_Duality_mixed.pdf}
    \caption{Memorization trends in the mixed dataset. While memorization patterns remain skewed, the overall distribution differs from Lavita, highlighting dataset-dependent memorization effects.}
    \label{fig:mixed_memorization}
\end{figure}




% \subsection{Scaling Laws in Memorization}

% Following prior work on scaling laws in deep learning~\cite{kaplan2020scaling, hoffmann2022training}, we examine memorization trends in smaller subsets of Lavita. We extract three smaller datasets of size $2^8$, $2^9$, and $2^{10}$, training models with the same learning rate schedule. While smaller datasets yield lower loss per step, systematic trends emerge when viewed across epochs, loss, quantiles, and dataset size (Figure~\ref{fig:smaller_lavita}).

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/smaller_lavita_comparison.pdf}
%     \caption{Memorization trends for progressively smaller subsamples of Lavita. Despite reduced training steps per epoch, models reach high memorization levels, indicating potential risks for small-scale fine-tuning.}
%     \label{fig:smaller_lavita}
% \end{figure}

% A key observation is that dataset size does not strongly correlate with maximum memorization but affects the rate at which memorization increases across epochs. Smaller datasets have fewer unique samples constraining model learning, leading to faster convergence to high-memorization states. Notably, even at small dataset sizes, models reach high word memorization levels while achieving low loss, suggesting that regularization or pruning remains essential in fine-tuning settings where training data is scarce. This has direct implications for privacy-sensitive applications, particularly when adapting LLMs to domain-specific tasks with limited data.



% \subsection{Where do the long memorization data points reside?}

% Our method section enumerated some limiting case that when memorization happens, the output should not be too diverse given the same input. 

% We want to look at the case where the inputs (up to 100 characters given to the model) are close to other inputs and argue that the full sentence should be more alike to other full sentences to correspond to more intense memorization. So we use the mean embedding similarity of the model to build features for the input and full sentence and let $n_{\text{pre}}$ be the response variable. 

% Comparing $n_{pre}$ against the input similarity from other points-output distance from other points, we have \ref{fig:embedding_pattern} agreeing with our hypothesis. Some other checkpoints in the baseline case and mixed case show similar results.

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/embedding_pattern2.pdf}
%     \caption{Memory against similarity if itself to other data points}
%     \label{fig:embedding_pattern}
% \end{figure}



% \subsection{Scaling Laws in Memorization}

% Building on prior work on scaling laws in deep learning~\cite{kaplan2020scaling, hoffmann2022training}, we investigate how dataset size influences memorization dynamics. As detailed in our method section, we construct progressively smaller subsets of Lavita with sizes $2^8$, $2^9$, and $2^{10}$ and train models under the same learning rate schedule. While smaller datasets exhibit lower per-step loss, systematic trends in memorization emerge when analyzed across epochs, loss trajectories, and memorization quantiles (Figure~\ref{fig:smaller_lavita}).

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/smaller_lavita_comparison.pdf}
%     \caption{Memorization trends across smaller Lavita subsets. Despite fewer training steps per epoch, models rapidly reach high memorization levels, posing risks for small-scale fine-tuning.}
%     \label{fig:smaller_lavita}
% \end{figure}

% A key finding is that dataset size does not directly constrain maximum memorization but significantly affects how quickly memorization escalates across training epochs. Smaller datasets provide fewer unique constraints, leading to faster convergence to high-memorization states. The max memorization of version with size $2^8$ reached 70 at about 100th step, and reached training loss below 0.25. In comparison, the max memorization of the model with $2^{10}$ data is less than 20 before 100th step, with loss above 1.0. 


\subsection{Scaling Laws in Memorization}




Expanding on prior research on scaling laws in deep learning~\cite{kaplan2020scaling, hoffmann2022training}, we investigate how dataset size influences memorization patterns. As described in the method section, we construct progressively smaller subsets of Lavita with sizes $2^8$, $2^9$, and $2^{10}$, training models using a consistent learning rate schedule. While smaller datasets yield lower per-step loss, systematic trends in memorization emerge when analyzed across epochs, loss progression, and memorization quantiles (Figure~\ref{fig:smaller_lavita}).  


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//llama3/comparison.pdf}
    \caption{Memorization comparison for the same subset of samples trained in Lavita vs. the mixed dataset. Certain data points exhibit large memorization differences across contexts, emphasizing dataset-dependent effects.}
    \label{fig:comparison}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//llama3/smaller_lavita_comparison.pdf}
    \caption{Memorization trends across smaller Lavita subsets. Despite fewer training steps per epoch, models rapidly reach high memorization levels, increasing risks for small-scale fine-tuning.}
    \label{fig:smaller_lavita}
\end{figure}

Our findings indicate that dataset size does not directly constrain maximum memorization but significantly affects how quickly memorization escalates throughout training. Smaller datasets impose fewer constraints on generalization, leading to a steeper rise in memorization as models overfit to the limited training samples. In the $2^8$ subset, maximum memorization reaches 70 within the first 100 training steps, with training loss dropping below 0.25. In contrast, in the $2^{10}$ subset, maximum memorization remains below 20 within the same interval, with loss staying above 1.0. 

This suggests that even when training on small datasets, models rapidly memorize individual samples despite achieving seemingly lower loss values. The effect is particularly pronounced in the smallest dataset, where limited training diversity allows models to memorize sequences much earlier in training. These results highlight the necessity of regularization techniques such as dropout, weight decay, or adversarial training when fine-tuning LLMs on domain-specific data with constrained sample availability. Moreover, they emphasize the need for more nuanced memorization assessments beyond loss-based evaluations, particularly in privacy-sensitive applications where early memorization can pose risks.



\subsection{Sampling Challenges in Skewed Memorization}

Memorization in LLMs exhibits a highly skewed distribution, with extreme cases occurring infrequently but significantly. Our analysis reveals that while the average prefix match length remains low, the tail of the distribution extends substantially, indicating that certain samples experience much stronger memorization. Figure~\ref{fig:prob_missing_max_density} highlights this behavior. For example, if the sample size is $455$, there is a higher probability that the sampled max is $8$ as opposed to the true maximum $16$. showing that smaller sample sizes systematically underestimate worst-case memorization.

To quantify the reliability of detecting extreme cases, we evaluate the probability of missing high-memorization instances when sampling subsets of the training data. As expected, the likelihood of capturing maximum memorization scales with sample size, reinforcing the necessity of full-dataset scans for accurate estimation~\cite{sehanobishscalable}. This aligns with prior observations that worst-case memorization cannot be effectively characterized using small-scale evaluations~\cite{carlini2021extracting, carlini2022quantifying}.

These results emphasize the need for robust memorization assessment beyond mean-based measures. The observed variance in memorization across training checkpoints suggests that fine-grained distributional analysis is essential for understanding memorization dynamics and mitigating privacy risks in large-scale models.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//llama3/Probability_of_missing_the_max_density.pdf}
    \caption{Probability of missing high-memorization instances as a function of sample size. Smaller samples fail to capture the distribution’s upper tail, leading to systematic underestimation of extreme cases.}
    \label{fig:prob_missing_max_density}
\end{figure}



\subsection{Decomposition}
 
\subsubsection{Memorization and Input Similarity}

As discussed in our method section, we hypothesize that memorization likelihood is influenced by data redundancy and embedding-space similarity. Specifically, if an input appears frequently in different contexts but leads to diverse continuations, memorization is less likely due to higher uncertainty in the suffix. Conversely, if a prefix consistently maps to a specific completion, memorization is more probable. To test this, we compute the mean embedding similarity of each input to all other inputs and compare it to the corresponding full-sentence similarity.

Figure~\ref{fig:embedding_pattern} shows $n_{\text{pre}}$ plotted against input and output similarities. The observed trend supports our hypothesis: higher input similarity correlates with stronger memorization, suggesting that data points residing in denser regions of the embedding space are more likely to be memorized. This trend is consistent across multiple training checkpoints in both the baseline and mixed datasets.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//llama3/embedding_pattern2.pdf}
    \caption{Memorization length ($n_{\text{pre}}$) against embedding similarity. Higher similarity to other training points correlates with increased memorization.}
    \label{fig:embedding_pattern}
\end{figure}

These findings reinforce that memorization is not solely a function of dataset size but also depends on how training samples are distributed within the learned representation. This aligns with previous studies on memorization risk in LLMs~\cite{carlini2021extracting, jagielski2022measuring} and suggests that dataset diversity at the embedding level could be a potential strategy for mitigating memorization.


% \subsection{Decomposing the tail}



% Formulation of probability of correctness at $j$ gives us the algorithm to check term-wise memorization (conditional) probability: The experiment is to compute $c_{ij}$ for each $i,j$, i.e. given all correct input before $j$ th term and only check the model's output at $j$ to get a $i*j$ matrix of 0-1 loss, compute $MI(C_{J[pre]},C_j )\text{ } \forall J_{pre} \in Comb[j]$, and if $MI/C_j$ is small (we use sklearn's implementation of discrete mutual information to compute it and find it true in multiple checkpoints for the baseline case), we should have $\prod_j^n p_j \simeq \mathbb{P}(N_{pre} \geq n)=1-F(n-1)$, yielding this simple estimation and explanation.

% The term-wise inference show a term-wise probability matrix,
% Checking dependent on previous correctness records, we have \ref{fig:mutual_info_is_low}. The results hold for multiple checkpoints in the baseline training case. 

% Hence we can fit the distribution, and from the plots, not only are the 2 list of probabilities close to each other, but their cummulative product as well\ref{fig:cond_full_prob}. This corresponds to the low mutual information \ref{fig:mutual_info_is_low} between correctness at $j$ and correctness at the previous terms given appropriate input as described from the methods section. As low $MI$ implies low dependency, this condition gives results stricter than the mere estimation of $p_j$ and $F$, as it suggests at this training setup, no matter small preturbation of the data, as long as you have a correct first half of a sentence, the probability to hack the next word, and next few words are not related to whether the model can generate your known last token given all previous tokens. This means that if you use the last true word you have to test whether the model memorizes this piece of data, this correctness has little to say about whether you can trust the model's output to the next (\emph{unknown}) word. If another model is tested to have preserved this property, it will happen that even memorization happens, one cannot make sure of it by testing with merely the correctness on the previous word he has.

% Suggesting that although the structure of the models are complicated, the dynamics of generation may still satisfy some basic assumptions on some datasets, as their generation process and loss have guided and many artificial aspects.

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/mutual_info_is_low_case_lavita_checkpoint-760.pdf}
%     \caption{Mutual info is Low (taking the base case as an example)}
%     \label{fig:mutual_info_is_low}
% \end{figure}



% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{fig//llama3/decomposition_lavita_checkpoint-760.pdf}
%     \caption{Conditional vs full probability (close when MI is low)}
%     \label{fig:cond_full_prob}
% \end{figure}

\subsubsection{Decomposing Memorization at the Token Level}

To better understand how memorization propagates in LLMs, we analyze term-wise correctness probabilities by computing mutual information (MI) between memorization at different positions. Specifically, for each training sample $i$ and token position $j$, we compute a binary correctness matrix $c_{ij}$, where $c_{ij} = 1$ if the model correctly recalls the token at position $j$ given all previous tokens, and 0 otherwise. Using this matrix, we estimate the mutual information $MI(C_{J_{\text{pre}}}, C_j)$ for all valid prefixes $J_{\text{pre}} \subset j$.

Empirically, we find that $MI(C_{J_{\text{pre}}}, C_j)$ is consistently low across multiple training checkpoints in the baseline case (Figure~\ref{fig:mutual_info_is_low}). This suggests a weak dependency between correctness at $j$ and correctness at earlier positions, supporting the approximation: $P(N_{\text{pre}} \geq n) = 1 - F(n-1) \approx \prod_{j=1}^{n} p_j$. Since the cumulative product of per-token probabilities aligns closely with the observed memorization distribution (Figure~\ref{fig:cond_full_prob}), this validates our method section’s assumption that term-wise correctness probabilities approximate the memorization distribution. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//llama3/mutual_info_is_low_case_lavita_checkpoint-760.pdf}
    \caption{Mutual information between term-wise correctness and prefix correctness remains low across checkpoints, suggesting weak dependence.}
    \label{fig:mutual_info_is_low}
\end{figure}



The independence of token prediction accuracy has critical implications for the reliability of model extraction attacks. The finding fundamentally undermines a key assumption in sequence extraction methodology: that successful recall of initial tokens can serve as a confidence indicator for subsequent extraction attempts. If correctness at position $j$ is independent of correctness at prior tokens, then testing memorization using a known prefix token is insufficient to infer correctness at subsequent tokens. This means that even if a model is tested to recall a specific word correctly, it does not imply high certainty for predicting the next token correctly. Consequently, assessing memorization should go beyond isolated token-level evaluations and consider full-sequence dependencies.

More broadly, despite the complexity of LLM architectures, our findings suggest that their token generation follows systematic probabilistic patterns, influenced by both training dynamics and structured loss guidance. This reinforces the idea that memorization can be characterized through well-defined statistical properties rather than requiring heuristic evaluations alone.



% \section{Discussion}

% We conclude that:
% 1. Prefix memorization is rare but skewed, therefore, requires large samples to \emph{find}, and the worst cases are severe.
% 2. Memorization for the entire model may occur more frequently when the data is more diverse.
% 3. Within the model, we can still decompose the memory, which suggests that 1. Diversifying the data set within the small input neighborhood may work 2. term-wise diversity and correctness matters, and could be used for altering memorization. closely related to the generation mechanism.

% The rarity and skewness of memorization led us to a dive to the mechanisms, 

% The decomposition yields pattern on $i$ and $j$ direction analogical to the 'space' of meaning and 'time' step of generation. We discovered patterns of factors dependent or independent to each other, suggesting that interpretation of behaviors of generation have much more to be explored while relating theorems from representation and dynamics to be involved in interpretation of LLM behaviors. 

% From a practical point of view, the relation shows promising way to detect and interfere with LLM memorization. If taking more sample is effective, then evidence of plagarism or privacy infringement may be better captured just by a better sampling and estimation of the distribution. In other aspects, since the training and data affects the memorization, it also suggest mitigation methods. Yet, a more practical setup would be to find entities in the distribution.


% \subsection{Future works}

% The embedding distance is similar to similarity, but the output is a consequence of the embedding, can we use this property to explain mechanism of training of LLM and relating the similarities? Going into the model, is the embedding collapsing when the loss has step-like patterns? Does the shrinkage in layers before or after embedding explain collapsing and overfitting of DPO?

% If when the input are similar, the output are diverse, does it happen that the output is sensitive to factual relevance that are not captured by the similarity shown in the embedding? How are two entities with factual relevance but no semantic similarity represented in embedding space as compared to entity recognition and knowledge graph? Can we use these to align the LLM embedding space?




\section{Discussion}

Our findings highlight that prefix memorization in LLMs is rare but highly skewed, making worst-case instances difficult to detect without large samples. Dataset diversity influences memorization in two ways: it increases overall memorization but can reduce extreme cases within local input neighborhoods. Decomposing memorization into term-wise and sequence-level components further shows that token diversity within a sequence affects memorization likelihood, aligning with the structured nature of auto-regressive generation.

These insights have practical implications for detecting and mitigating memorization. Improved sampling strategies can enhance privacy risk assessment by capturing extreme cases more effectively. Additionally, increasing local diversity in training data may reduce unintended memorization while preserving model utility. Understanding how memorization relates to embedding-space structure and generation dynamics offers a path toward privacy-preserving model training and informed dataset curation.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//llama3/decomposition_lavita_checkpoint-760.pdf}
    \caption{Conditional probability of correct token generation closely matches full probability estimates when MI is low.}
    \label{fig:cond_full_prob}
\end{figure}

\section{Future Work}

Several open directions merit further investigation. The connection between embedding similarity and memorization remains underexplored, particularly whether embedding collapse during training contributes to memorization intensity. Further research should examine whether memorization sensitivity extends beyond lexical similarity to factual relevance, potentially revealing how LLM embeddings encode structured knowledge. Understanding these mechanisms could inform future approaches for aligning LLMs with factual consistency while mitigating privacy risks.

\section{Conclusion}

This work presents a principled analysis of memorization in LLMs, revealing its skewed distribution, dependence on dataset diversity, and decomposition into term-wise and sequence-level components. By linking memorization to training dynamics and embedding-space structure, we provide both theoretical insights and practical interventions. These findings offer a foundation for refining memorization detection and improving privacy-preserving training strategies in large-scale language models.



\nocite{langley00}

\bibliography{ref}
\bibliographystyle{lib/icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


% \section{Theoretical Comparison with Existing Metrics}
% \label{appendix:comparison}

% This section provides a theoretical comparison of our memorization metric with existing text similarity metrics, demonstrating its advantages in capturing sequential memorization patterns.

% \subsection{Bounding Memorization with Existing Similarity Metrics}

% Our proposed metric, $n_{\text{pre}}$, which measures consecutive token matches beyond a prefix, relates to standard sequence comparison metrics. Specifically, we establish its relationship with \emph{edit distance, longest common subsequence (LCS), and ROUGE scores}.

% Given a similarity function $d_w(s, r) = \sum_{j=1}^{n} w^{-j} c_j(s, r)$, where $c_j(s, r)$ is a correctness indicator and $w$ is a decay factor, we derive the following inequalities:

% \begin{align}
% \text{Non-inplace matched word count} &\geq \text{LCS} \geq n_{\max} \notag \\
% &\geq \text{Length of in-place matched n-gram} \geq n_{\text{pre}} \notag \\
% &\geq \min(N_1, N_2) - d_1 = \text{Number of in-place matched words}.
% \end{align}

% This establishes that $n_{\text{pre}}$ is a lower bound on existing similarity measures and offers a stricter definition of memorization than LCS.

% \subsection{Inequalities with Edit Distance and ROUGE}

% We further bound our memorization measure using edit distance and ROUGE scores:

% \begin{align}
% d_1 &\geq \max(N_1, N_2) - \text{LCS} \geq \frac{d_{\text{Levenshtein}}}{2}, \notag \\
% \text{ROUGE-L} &= \frac{\text{LCS}}{N_2} \geq \frac{n_{\max}}{N_2} \geq \frac{n_{\text{pre}}}{N_2}.
% \end{align}

% For ROUGE-n, where the denominator is $N_2 - n + 1$ for recall and $N_1 - n + 1$ for precision, the numerator is bounded as:

% \begin{align}
% \text{ROUGE-n numerator} &\geq n_{\max} - n + 1 \geq n_{\text{pre}} - n + 1.
% \end{align}

% \subsection{Decay-Based Weighting and Memorization Strength}

% For decay-weighted metrics, such as those where $w = 3$, we obtain:

% \begin{align}
% d_3 = \sum_{j=1}^{\infty} 3^{-j} c_j, \quad d_1 \cdot 3^{-n_{\text{pre}}} \frac{1}{1 - 1/3} \geq d_3 \geq 3^{-(n_{\text{pre}} + 1)} \frac{1}{1 - 1/3}.
% \end{align}

% These bounds illustrate that our metric provides a \emph{stricter, more informative measure of memorization} than conventional text similarity measures by focusing on the longest contiguous memorized subsequence rather than overall token overlap.

% Our theoretical analysis demonstrates that $n_{\text{pre}}$ is a well-founded memorization metric that captures worst-case memorization more effectively than LCS, ROUGE, or edit distance. These properties justify its use in evaluating LLM memorization with greater sensitivity to sequential recall behavior.

\section{Additional Experiment Setup Details and Generalization Explaination}

For the LoRA fine-tuning stage, we employed Llama-Factory \cite{Zheng_LlamaFactory_Unified_Efficient_2024}, an open-source and efficient framework designed for LLM Fine tuning. The following hyperparameters were consistently applied across all experiments during the LoRA fine-tuning process:

\begin{table}[h!]
\centering
\begin{tabular}{l c}
\toprule
\textbf{Parameter}                   & \textbf{Value}            \\ 
\midrule
\textbf{Base Model}                  & Llama3.1-8b-Instruct      \\ 
\textbf{Quantization Method}         & bitsandbytes              \\ 
\textbf{Quantization}                & int8                      \\ 
\textbf{LoRA Rank}                   & 256                       \\ 
\textbf{LoRA Alpha}                  & 512                       \\ 
\textbf{LoRA Dropout}                & 0.05                      \\ 
\textbf{Learning Rate}               & $1.00 \times 10^{-5}$     \\ 
\textbf{LR Scheduler}                & cosine                    \\ 
\textbf{Epochs}                      & 100                       \\ 
\textbf{Compute Type}                & bf16                      \\ 
\textbf{Attention Method}            & FlashAttention-2          \\ 
\textbf{Cutoff Length}               & 1024                      \\ 
\textbf{Batch Size}                  & 8                         \\ 
\textbf{Gradient Accumulation Steps} & 8                         \\ 
\textbf{Optimizer}                   & AdamW                     \\ 
\textbf{Warmup Ratio}                & 0.1                       \\ 
\bottomrule
\end{tabular}
\caption{Configuration Parameters for Llama3.1-8b-Instruct Training}
\label{tab:training_config}
\end{table}

We utilized vLLM \cite{vllm}, a high-throughput and memory-efficient engine designed for \emph{large language model (LLM) inference}. The LoRA fine-tuned Llama-3.1-8b-Instruct model was loaded in its full size, and the following generation configuration was applied within vLLM:

\begin{table}[h!]
\centering
\begin{tabular}{l c}
\toprule
\textbf{Parameter}                     & \textbf{Value}            \\ 
\midrule
\textbf{vLLM Version}                  & 0.63.0                   \\ 
\textbf{n (Number of output sequences)} & 1                        \\ 
\textbf{top\_k}                        & 1                        \\ 
\textbf{top\_p}                        & 1                        \\ 
\textbf{temperature}                   & 0                        \\ 
\textbf{max\_tokens}                   & 128                      \\ 
\textbf{repetition penalty}            & 1                        \\ 
\bottomrule
\end{tabular}
\caption{Configuration Parameters for vLLM}
\label{tab:vllm_config}
\end{table}


% \section{Handy properties for comparisons to other metrics}
% \begin{description}
% \item[$N1$] Candidate output length

% \item[$N2$] Reference text length

% \item [$D_{Levenshtein}$] Edit distance

% \item [$LCS$] Largest Common subsequence

% \item [$ROUGE-L$]

% \item [$n_{pre}|_N$] number of prefix match only comparing up to N

% \item [$ROUGE-n$]

% \item [$n_{max}$] Max length of shared ordered n-grams

% \item [$c_i$] 0-1 loss at term $i$, if N1[i]=N2[i], then $c_i=0$, else 1.

% \item [$d_w(s, r) = \sum_1^n \vec w_j c_j(s,r), w_j=w^{-j}$]

% \end{description}

% One concern is on the difference in lengths in generated text $N1$, reference text $N2$, and the length of text we care $N$. 

% Practically we do not using infinite $N$, and in the simple case that $N<N1, N<N2$, some properties are very easy to derive. If we only care about the contents up to such $N$ and do not distinguish the latter outputs, then all the $d$'s we used above are \emph{metrics}.

% It follows almost immediately from the properties of stacking 0-1 metrics that $d_w$ is a metric when $c$ is defined as the 0-1 difference termwise when both terms are \emph{defined}. What should we do when the two strings to compare are of different lengths, i.e. $N1\neq N2$? For convenience, we restrict $n_{pre}|_{max(N1,N2)} \leq max(N1,N2)$ and $c[i]=1$ for the index with no reference or no summary value, but 0 when both are missing. One can see that this generation preserves two properties:

% First, if we have 3 strings $s_1, s_2, s_3$, the correctness vector between pairs $c_{12}, c_{23}, c_{13}$, then we have for each term $j$, if $c_{13}[j]=1$ , then at least one of $c_{12}[j], c_{23}[j]$ is 1. Why? Because in the case $s_1[j], s_3[j]$ differ, either the value in this term is different, or one is with a value and the other not. To convert $s_1[j], s_3[j]$ by editing the term, at least one difference must be made either between $s_1[j], s_2[j]$ or $s_2[j], s_3[j]$. This guarantees the triangular inequality for $d_w(s_1, s_3) \leq d_w(s_1, s_2) +  d_w(s_1, s_2)$ is held by taking the sum of the term-wise correctness. Note that since this $d_w$ and the edit distance are metrics, more properties from metrizable spaces(a well-studied area in topology) can be applied, making it possible to sharpen the results with tighter and more versatile theoretical bounds in the future.

% Second, this preserves that the prefix length $n_{pre}$ ,and the max length of shared n-gram $n_{max}$ , is the number of consecutive 0's counting from the first term, number of consecutive 0's restricted to sequence length respectively.

% We have, by set inclusion, following from large to small, inclusive,

% \begin{multline*}
% \text{non-inplace matched word instance count}  \geq LCS \\
% \geq n_{max} \geq \text{Length of in-place matched n-gram} \geq n_{pre} \\
% \dots \geq min(N1,N2)-d_1\\
% = \text{Number of in-place matched word} 
% \end{multline*}

% Also,

% $$d_1 \geq max(N)-LCS \geq d_{Levenshtein}/2 $$


% For $w>2$, restricting to length N, comparing $d_r$ is equivalent to comparing $n_{pre} $.

% \begin{multline*}
%     d_r(s[i], r[i]) > d_r(s[i'], r[i'])  \\
%     \iff n_{pre}(s[i'], r[i']) > n_{pre}(s[i], r[i])
% \end{multline*}


% In $w=3$ case $d_3 = \sum 3^{-j} c_j<\sum c_j<d_1$, and in the case when $n_{pre}+1$ case is wrong, we have the inequality between $d_1, d_3, n_{pre}$ as follows,
% \begin{align*}
%     d_1* 3^{-n_{pre}}\frac{1}{1-1/3} \geq 3^{-n_{pre}}\frac{1}{1-1/3}= \sum_{n_{pre}}^{\infty} 3^{-i} \geq d_3 \\
%     \geq \sum_{n_{pre}+1}^{\infty} 3^{-i}=3^{-(n_{pre}+1)}\frac{1}{1-1/3}
% \end{align*}

% which is just the ternary expansion of $c$.

% $\text{Rouge-L}=\frac{LCS}{N2} \geq \frac{n_{max}}{N2} \geq \frac{n_{pre}}{N2}$

% By definition,Rouge-n is
%     $$\frac{\text{Count of summary n-gram presented in reference}}{\text{Total count n-gram in reference}}$$ (Recall)

%  $$\frac{\text{Count of summary n-gram presented in reference}}{\text{Total count n-gram in summary}}$$
%  (Precision)


% The denominators are $N_2-n+1$ , $N_1-n+1$ respectively.  The nominator is lower bounded by $n_{max}-n+1\geq n_{pre}-n+1$. 

% The above presents the math connection between prefix memorization, text algorithmic metrics, NLP evaluation criterion, and miscellaneous relations. These metrics are also built into our metric code.



\section{Comparative Analysis with Existing Text Metrics}

To systematically compare memorization with standard text similarity metrics, we establish formal relationships between prefix memorization, common text comparison methods, and NLP evaluation criteria. 

\subsection{Notation and Definitions}

Table~\ref{tab:notation} summarizes the key notations used throughout our analysis.

\begin{table}[h]
\centering
\caption{Notation for Text Comparison Metrics}
\label{tab:notation}
\begin{tabular}{ll}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
$N_1$ & Length of generated output \\
$N_2$ & Length of reference text \\
$D_{\text{Levenshtein}}$ & Edit distance \\
$LCS$ & Longest Common Subsequence \\
ROUGE-L & ROUGE score based on $LCS$ \\
$n_{\text{pre}}|_N$ & Prefix match length, up to $N$ \\
ROUGE-$n$ & ROUGE score for $n$-grams \\
$n_{\max}$ & Max length of shared ordered $n$-grams \\
$c_i$ & 0-1 correctness at position $i$ \\
$d_w(s, r)$ & Weighted difference: $d_w(s, r) = \sum_1^n w_j c_j(s,r), w_j = w^{-j}$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Handling Length Mismatch in Text Comparison}

A fundamental challenge in text similarity is handling cases where $N_1 \neq N_2$. We impose the constraint:
\[
n_{\text{pre}}|_{\max(N_1,N_2)} \leq \max(N_1, N_2).
\]
For missing indices, we set $c[i] = 1$ if one sequence has a token while the other does not, ensuring:
\[
d_w(s_1, s_3) \leq d_w(s_1, s_2) + d_w(s_2, s_3),
\]
which maintains the \emph{triangle inequality}, making $d_w$ a valid metric.

\subsection{Relations Between Memorization and NLP Metrics}

We establish the following hierarchical relationship:
\begin{multline*}
\text{Non-inplace matched word count} \geq LCS \geq n_{\max} \\
\geq \text{In-place matched $n$-gram length} \geq n_{\text{pre}} \geq \\
\min(N_1, N_2) - d_1 = \text{Total in-place matched words}.
\end{multline*}
Additionally:
\[
d_1 \geq \max(N_1, N_2) - LCS \geq \frac{D_{\text{Levenshtein}}}{2}.
\]

For weighted distances ($w > 2$), restricting to length $N$:
\[
d_r(s[i], r[i]) > d_r(s[i'], r[i']) \iff n_{\text{pre}}(s[i'], r[i']) > n_{\text{pre}}(s[i], r[i]).
\]

For $w=3$, we derive:
\begin{align*}
    d_3 &= \sum 3^{-j} c_j \leq \sum c_j \leq d_1, \\
    d_1 \cdot 3^{-n_{\text{pre}}} \frac{1}{1-1/3} &\geq \sum_{n_{\text{pre}}}^{\infty} 3^{-i} \geq d_3.
\end{align*}

\subsection{ROUGE Score Relations}

For ROUGE-L:
\[
\text{ROUGE-L} = \frac{LCS}{N_2} \geq \frac{n_{\max}}{N_2} \geq \frac{n_{\text{pre}}}{N_2}.
\]
For ROUGE-$n$:
\begin{align*}
    \text{Recall} &= \frac{\text{Count of summary $n$-grams in reference}}{\text{Total reference $n$-grams}}, \\
    \text{Precision} &= \frac{\text{Count of summary $n$-grams in reference}}{\text{Total summary $n$-grams}}.
\end{align*}

Denominators are $N_2 - n + 1$ and $N_1 - n + 1$, while the numerator satisfies:
\[
n_{\max} - n + 1 \geq n_{\text{pre}} - n + 1.
\]

\subsection{Implications for Memorization and Evaluation}

These results establish formal connections between memorization and standard NLP text similarity metrics, demonstrating that prefix memorization aligns with common evaluation measures while providing a more \emph{extreme-case-sensitive} assessment. This is crucial for detecting overfitting in models trained on small datasets or identifying privacy risks.

Since our memorization metrics satisfy fundamental \emph{metric properties}, they can be extended to generalization error analysis using clustering-based methods. Furthermore, standard evaluation metrics may systematically underestimate memorization risks, reinforcing the need for explicit memorization evaluation in LLMs.

All proposed metrics and relationships are implemented in our evaluation code, enabling direct application in diverse training setups.





% \section{Proof and examples for the theorem and properties}


% \begin{proof}
% Probability of memorization by 1 token ahead
% since sampling for the generation of new token given input is independent of sampling the actual data to hack from the training data,

% \begin{multline*}
%     \mathbb{P}(\text{memorization at this token}|\text{input, all previous info before this token}) \\
%     =\sum_{y \in \mathcal{Y}}\mathbb{P}(\text{generated token}=y|\text{input, all previous info before this token}, \text{actual token}=y) \\
%     \mathbb{P}(\text{actual token}=y|\text{input, all previous info before this token}) \\
%     =\sum_{y \in \mathcal{Y}}\mathbb{P}(\text{generated token}=y|\text{input, all previous info before this token}) \\
%     \mathbb{P}(\text{actual token}=y|\text{input, all previous info before this token})
% \end{multline*}

% Sampling procedure for $\mathbb{P}(\text{actual token}=y|\text{input, all previous info before this token})$ is fixed and only depends on the training dataset property, we can enumerate $\mathcal{Y}$ for index $i$ and rename it $p$ with index $i$.

% The generation procedure is to sample from possible outputs from $\mathcal{Y}$; we can rename the $\mathcal{P}(\text{actual token}=y|\text{input, all previous info before this token})$ as $q_i$.

% The problem becomes:

% $$argmax_{q} \sum p_i q_i$$
% with restriction $1 \geq p_i\geq 0$, $1 \geq q_i \geq 0$, and $\sum p=\sum q=1$

% To show that a greedy algorithm always selects for the term with max probability in the training set is to show that the q that satisfies this condition is
% $
% q_i = \begin{cases}
% 1 & \text{if } i = argmax_{i} p_i \\
% 0 & \text{otherwise}
% \end{cases}
% $

% One way to prove this is to call the one above $q^*$ construct an arbitrary $q$, if $q\neq q^*$, we can always construct a $q'$ in this process:

% Since $q\neq q^*$ there should be at some indexes $i' =\bar argmax_i p_i$ with $q_{i'}>0$, and also $q_i*<1 -q_{i'}$, we let $q'_i* = q_i*+q_{i'}$ and $q*_{i'}=0$. For the rest of the indices, the values are all the same. We can prove $\sum p_iq'_i>\sum p_iq_i$. 

% We keep constructing $q''$ from $q'$ to end with a long inequality $\sum p_iq*_i> \cdots >  p_iq'_i>\sum p_iq_i$. By induction, this is true. This takes at most $|\mathcal{Y}|$ steps.

% \end{proof}

% One intuitive way is to think geometrically and take the derivative of $pq$ with respect to q, and get $\nabla_q pq = p_i$. since every term is linear to $q$, a single-order polynomial (or using the first-order differential) is able to get the exact value. When $L1$ norm is restricted, the max occurs when spending all of the magnitude of q on highest $p$.




% \begin{proof}

% We construct 2 correctness vectors 

% $$C = (0,\dots,1,0,0,0\dots)$$ 

% with the first 1 occurs at n and the rest are 0

% and
% $$c = (0, \dots, 0,1,1,1\dots)$$

% with the first 1 occurs at n+1 and is followed by 1's.
% and show that

% $$\sum^{\infty} w^{-j}C_j > \sum^{\infty} w^{-j}c_j$$ when $w>2$ using the sum of geometric sequence.

% then we compare case when $n_{pre} = n-1$ and $n_{pre}'>n_{pre}$, for any correctness vector $c(n_{pre})$ corresponding to $n_{pre} = n-1$, the sum $\sum w^{-j}c_j(n_{pre})>\sum^{\infty} w^{-j}C_j $. For $c(n_{pre}')$, $\sum^{\infty} w^{-j}C_j \sum w^{-j}c_j(n_{pre}')> $. So the order is kept.

% \end{proof}


% \begin{proof}
%     We have, by set inclusion, following from large to small, inclusive,
% $$
% \text{number of non-inplace matched word instance count}  \geq LCS
% $$



% $$  \geq n_{max} \geq \text{Length of in-place matched n-gram} \geq n_{pre} $$

% $$\dots \geq max(N1,N2)-d_1= \text{Number of in-place matched word} $$

% $$d_1 \geq max(N)-LCS \geq d_{Levenshtein}/2 $$

% The set of paths to edit from one string to another contains the path of deleting all the words from the first string that are not in the LCS from the first string, and inserting all the strings from the second string that are not in the LCS, so a path with number of steps $N1+N2-2LCS$ exists, but it is not necessarily the shortest path, so this is greater or larger than $d_{Levenshtein}$


% \end{proof}





% \begin{remark}{Same Conditional Probability does not contradict that some data are more intensively memorized}

%     Some entries are remembered more than others, but whether the term k is remembered, given all the previous terms are remembered correctly, is not related to whether any combination of termwise success memorization. 

% Show that this is not a contradiction:

% Suppose the correctness for one ensemble of sequences are
% (i - data, j - per word termwise direction)
% 01 \\
% 10 \\
% 11 \\
% 00 \\

% Here sequence 3 is more remembered than anything else

% The restricted parts for termwise correctness are

% 0* \\
% 10 \\
% 11 \\
% 0* \\

% Here the * parts are unrestricted. We have (P(2 correct| 1 correct)) in a sample

% Either 
% 00 \\
% 10 \\
% 11 \\
% 01 \\
% Or \\
% 01 \\
% 10 \\
% 11 \\
% 00 \\
% Will give (P(2 correct| 1 correct) = P(2 correct)).

% This gives a minimal counterexample, showing the proposition in the concern is not a problem.

    
% \end{remark}





% \begin{remark}
    
% In what cases selecting the term with the highest probability term-by-term is different from selecting consecutive terms with the highest probability?

% We have sequences:

% 000000 \\
% 000000 \\
% 123456 \\
% 132456 \\
% 142356 \\
% 152346 \\
% 162345 \\

% The termwise prediction will chose
% 1***** greedy, then a total number of memorized terms is at most 9, and only 1 is memorized at full length.
% But if you chose 0*****, we have 2 replicated data memorized at full length.

% Note that this happens when the less term-wise favored term is replicated for more lengths and the more for less.


% What does it suggest?

% 0-00000\\
% 0-00000\\
% 1-23456\\
% 1-32456\\
% 1-42356\\
% 1-52346\\
% 1-62345\\

% the number of time that the sequence starts from 1 is at most 1, because the rest are different, a function can only choose 1

% the number of times that the sequence starts from 0 can be 2 because if we pick 0's, we memorize both.

% so sequence with the same inputs before j, if to be memorized, should not have very different output after j.

% This brings us to the bound of $R_b$ and $H$, and inspired experimentation.


% \end{remark}


% \begin{proof}
%     If $$\mathbb{E}[N_{pre}(BOC)]>\mathbb{E}[N_{pre}(\text{T-BOC})] $$
% They must be at least differ at an index j, conditioned on all correct memorization before j ($r_{ref} \dots j-1$), that the choice at the j-th term of BOC differ(and yield higher average memorization in this sub-category) than that of the term-wise BOC, and the memorization at this difference brings higher expected BOC than the termwise one. \\

% Suppose the length after j is at least $n_e$, then at least $n_e-1+n_j[max]$ are in the sum of RHS, but the LHS select $n_j'\leq n_j[max]$, so the sum must be greater than $n_e-1+n_j[max]$, the replicated length $n_r$ must follow $n_r* n_j' >n_e-1+n_j[max]$.\\
% Here $n_r$, $n_j'$ are both integers, if $n_r\leq n_e$, then $n_j' \geq 2$, at least a pair is replicated at $|r_{pre}|+j+n_r$ in the training data.
% \end{proof}

\section{Proofs and Examples for Theorems and Properties}

\subsection{Probability of Memorization by One Token Ahead}

\begin{proof}
Since the sampling for generating a new token given the input is independent of sampling the actual data from the training set, we can express the probability of memorization at a given token as:

\begin{multline*}
    \mathbb{P}(\text{memorization at this token}|\text{input, all previous tokens}) \\
    =\sum_{y \in \mathcal{Y}}\mathbb{P}(\text{generated token}=y|\text{input, all previous tokens, actual token}=y) \\
    \mathbb{P}(\text{actual token}=y|\text{input, all previous tokens}) \\
    =\sum_{y \in \mathcal{Y}}\mathbb{P}(\text{generated token}=y|\text{input, all previous tokens}) \\
    \mathbb{P}(\text{actual token}=y|\text{input, all previous tokens}).
\end{multline*}

Since the sampling process for $\mathbb{P}(\text{actual token}=y|\text{input, all previous tokens})$ is determined by the training data distribution, we can enumerate $\mathcal{Y}$ and denote this probability as $p_i$. Similarly, the probability of generating token $y$ can be represented as $q_i$. The problem then simplifies to:

\[
\arg\max_{q} \sum p_i q_i
\]

subject to constraints:

\[
1 \geq p_i \geq 0, \quad 1 \geq q_i \geq 0, \quad \sum p_i = \sum q_i = 1.
\]

To prove that a greedy algorithm selects the token with the highest probability in the training data, we show that the optimal $q$ follows:

\[
q_i = \begin{cases}
1 & \text{if } i = \arg\max_i p_i \\
0 & \text{otherwise}.
\end{cases}
\]

Suppose a suboptimal $q \neq q^*$ exists. Then, there must be some index $i' = \bar{\arg\max}_i p_i$ with $q_{i'}>0$ and another index $i$ where $q_i^* < 1 - q_{i'}$. Constructing a new probability distribution by setting $q'_{\bar{\arg\max}[i] p[i]}=0$ and $q'_{{\arg\max}[i] p[i]} = q_{{\arg\max}[i] p[i]} +q_{\bar{\arg\max}[i] p[i]} $

Keeping other indices unchanged, we show that:

\[
\sum p_i q'_i > \sum p_i q_i.
\]

By iteratively adjusting $q$ in at most $|\mathcal{Y}|$ steps, we eventually reach $q^*$, proving the greedy selection process.
\end{proof}

Geometrically, this follows from differentiating $p_i q_i$ with respect to $q_i$, yielding $\nabla_q pq = p_i$. Since the function is linear in $q$, maximizing under an $L_1$ norm constraint is achieved by allocating all weight to the highest probability $p_i$.

\subsection{Bounding Memorization Using Weighted Correctness Scores}

\begin{proof}
Define two correctness vectors:

\[
C = (0,\dots,1,0,0,0\dots),
\]

where the first 1 appears at index $n$, and the rest are 0.

\[
c = (0, \dots, 0,1,1,1\dots),
\]

where the first 1 appears at index $n+1$ and is followed by consecutive ones. We show:

\[
\sum^{\infty} w^{-j}C_j > \sum^{\infty} w^{-j}c_j, \quad \text{for } w>2.
\]

By applying the geometric series sum formula, we compare cases where $n_{\text{pre}} = n-1$ and $n_{\text{pre}}' > n_{\text{pre}}$. For any correctness vector $c(n_{\text{pre}})$ corresponding to $n_{\text{pre}} = n-1$,

\[
\sum w^{-j}c_j(n_{\text{pre}}) > \sum^{\infty} w^{-j}C_j.
\]

Similarly, for $c(n_{\text{pre}}')$,

\[
\sum^{\infty} w^{-j}C_j > \sum w^{-j}c_j(n_{\text{pre}}').
\]

Thus, the order is preserved.
\end{proof}

\subsection{Inequalities with Edit Distance and ROUGE}

\begin{proof}
Using set inclusion, we establish the following sequence of inequalities:

\begin{align*}
\text{Non-inplace matched word count} &\geq \text{LCS} \geq n_{\max} \\
&\geq \text{Length of in-place matched n-gram} \geq n_{\text{pre}} \\
&\geq \max(N_1,N_2) - d_1 = \text{Number of in-place matched words}.
\end{align*}

For edit distance:

\[
d_1 \geq \max(N) - \text{LCS} \geq \frac{d_{\text{Levenshtein}}}{2}.
\]

Since an edit sequence from one string to another must at least remove non-LCS words and insert missing ones, we obtain:

\[
N_1 + N_2 - 2\text{LCS} \geq d_{\text{Levenshtein}},
\]

which bounds the edit distance.
\end{proof}

\subsection{Remarks on Conditional Probability and Memorization Strength}

\begin{remark}
Some data entries may be memorized more intensely than others, but the probability of memorization at a token, conditioned on previous tokens being correct, remains independent of specific token-wise memorization patterns.

Consider correctness sequences (where $i$ represents data and $j$ represents token position):

\[
\begin{array}{c}
01 \\
10 \\
11 \\
00
\end{array}
\]

Here, sequence 3 is more strongly memorized. The termwise correctness constraints are:

\[
\begin{array}{c}
0* \\
10 \\
11 \\
0*
\end{array}
\]

where * represents unrestricted positions. Computing $\mathbb{P}(2 \text{ correct} \mid 1 \text{ correct})$ shows:

\[
\mathbb{P}(2 \text{ correct} \mid 1 \text{ correct}) = \mathbb{P}(2 \text{ correct}).
\]

This provides a minimal counterexample, illustrating that termwise memorization probabilities do not contradict varying levels of memorization across data points.
\end{remark}

\begin{remark}
Choosing the highest probability term-by-term differs from selecting the highest probability contiguous subsequence.

Consider:

\[
\begin{array}{c}
000000 \\
000000 \\
123456 \\
132456 \\
142356 \\
152346 \\
162345
\end{array}
\]

A termwise greedy selection yields:

\[
1*****,
\]

memorizing at most 9 tokens but only fully memorizing one sequence. In contrast, selecting:

\[
0*****
\]

results in two fully memorized sequences. This discrepancy occurs when a less probable term is replicated over a longer sequence.

Thus, sequences with identical prefixes should not diverge significantly in subsequent terms to ensure consistent memorization behavior. This insight connects to the bounds of $R_b$ and $H$, motivating further experiments.
\end{remark}

\subsection{Expected Memorization in BOC vs. Termwise BOC}

\begin{proof}
If:

\[
\mathbb{E}[N_{\text{pre}}(\text{BOC})] > \mathbb{E}[N_{\text{pre}}(\text{T-BOC})],
\]

then they must differ at some index $j$. Given correct memorization up to $j-1$, the selection at $j$ in BOC leads to higher expected memorization than termwise BOC.

If the sequence extends at least $n_e$ beyond $j$, then:

\[
n_r \cdot n_j' > n_e - 1 + n_j[\max].
\]

Since $n_r, n_j'$ are integers, if $n_r \leq n_e$, then $n_j' \geq 2$, ensuring at least one pair is replicated in the training data at $|r_{\text{pre}}| + j + n_r$.
\end{proof}



\end{document}
