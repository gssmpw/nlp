\section{RELATED WORK}
\subsection{Quantum-Inspired TNs.}
Quantum-inspired tensor networks (TNs) enhance the performance of classical algorithms by emulating quantum computational characteristics~\citep{huggins2019towards}. These methods map inputs into a Hilbert space with exponential dimensions, achieving linear separability through local mappings and tensor products, while employing multiple low-order cores to parameterize coefficients, significantly reducing computational and storage complexity~\citep{selvan2020tensor,stoudenmire2016supervised}. The avoidance of activation functions, alongside the theoretical underpinnings rooted in many-body physics, contributes to the interpretability of TNs~\citep{ran2023tensor}. Recently, numerous studies have successfully applied TNs to tasks such as image classification~\citep{stoudenmire2016supervised}, generation~\citep{cheng2019tree}, and segmentation~\citep{selvan2021patch}, effectively integrating established neural network techniques like residual connection~\citep{meng2023residual}, multiscale structure~\citep{liu2019machine}, and normalization~\citep{selvan2020tensor} into TN frameworks.
However, current TNs are predominantly suited for simpler tasks and face limitations in terms of computational efficiency and expressive power.


\subsection{Advanced Modern Networks.}
In the modern deep learning field, the design of network architectures has become increasingly complex and diverse, each showcasing unique advantages. Advanced models such as Convolutional Neural Networks (CNNs)~\citep{ref31,hou2024conv2former} with efficient feature extraction, Transformers with powerful context understanding~\citep{vaswani2017attention}, MLP-architectures~\citep{tolstikhin2021mlp} with elegant yet effective designs, and Mamba~\citep{gu2023mamba} with linear complexity all play crucial roles in various applications. These networks rely on various types of activation functions~\citep{apicella2021survey} to introduce nonlinear characteristics, but this reliance also limits their application in the fields of security and encryption, e.g., Leveled Fully Homomorphic Encryption supports solely addition and multiplication as operations~\citep{brakerski2014leveled,cheng2024multilinear}.

\subsection{Polynomial and Multilinear Networks.}
Polynomial and multilinear networks employ addition and multiplication operations to construct intricate network representation~\citep{chrysos2021deep,chrysos2023regularization,cheng2024multilinear}. 
Specifically, the pioneering polynomial network (PN)~\citep{chrysos2021deep} modularly constructs higher-order expanded polynomial forms of inputs and then training them end-to-end. It has shown good results in image recognition and generation tasks.
The study~\citep{chrysos2023regularization} introduces regularization terms for PN, incorporating techniques like data augmentation, instance normalization, and increased feature interaction orders to enhance model performance.
Cheng et al. drew inspiration from contemporary architectural designs to propose MONet~\citep{cheng2024multilinear}, aiming to narrow the gap between multilinear networks and state-of-the-art architectures.
It is important to note that both polynomial and multilinear networks can capture interactions of features at exponential orders, though the primary distinction lies in their structural flexibility: polynomial networks maintain an unfoldable structure, whereas multilinear networks lose this property when instance normalization is applied, as
\begin{equation}\begin{split}
\boldsymbol{A}\boldsymbol{z} * \boldsymbol{B}\boldsymbol{z} &= vec(\boldsymbol{z} \otimes \boldsymbol{z})(\boldsymbol{A}^T\odot \boldsymbol{B}^T),\\
LN(\boldsymbol{A}\boldsymbol{z} ) * (\boldsymbol{B}\boldsymbol{z}) &\neq vec(\boldsymbol{z} \otimes \boldsymbol{z})LN(\boldsymbol{A}^T\odot \boldsymbol{B}^T).
\label{eq2}
\end{split}\end{equation}
Here, $LN$ represents layer normalization, while $A$ and $B$ denote learnable matrixes.
Our objectives include re-establishing the polynomial expansion form and analyzing the distinctions between DTTN and quantum-inspired TNs. Additionally, by integrating layer normalization, we aim to develop a multilinear DTTN$^\dagger$ that surpasses the performance of current high-performing multilinear networks~\citep{cheng2024multilinear}