\section{RELATED WORK}
\subsection{Quantum-Inspired TNs.}
Quantum-inspired tensor networks (TNs) enhance the performance of classical algorithms by emulating quantum computational characteristics____. These methods map inputs into a Hilbert space with exponential dimensions, achieving linear separability through local mappings and tensor products, while employing multiple low-order cores to parameterize coefficients, significantly reducing computational and storage complexity____. The avoidance of activation functions, alongside the theoretical underpinnings rooted in many-body physics, contributes to the interpretability of TNs____. Recently, numerous studies have successfully applied TNs to tasks such as image classification____, generation____, and segmentation____, effectively integrating established neural network techniques like residual connection____, multiscale structure____, and normalization____ into TN frameworks.
However, current TNs are predominantly suited for simpler tasks and face limitations in terms of computational efficiency and expressive power.


\subsection{Advanced Modern Networks.}
In the modern deep learning field, the design of network architectures has become increasingly complex and diverse, each showcasing unique advantages. Advanced models such as Convolutional Neural Networks (CNNs)____ with efficient feature extraction, Transformers with powerful context understanding____, MLP-architectures____ with elegant yet effective designs, and Mamba____ with linear complexity all play crucial roles in various applications. These networks rely on various types of activation functions____ to introduce nonlinear characteristics, but this reliance also limits their application in the fields of security and encryption, e.g., Leveled Fully Homomorphic Encryption supports solely addition and multiplication as operations____.

\subsection{Polynomial and Multilinear Networks.}
Polynomial and multilinear networks employ addition and multiplication operations to construct intricate network representation____. 
Specifically, the pioneering polynomial network (PN)____ modularly constructs higher-order expanded polynomial forms of inputs and then training them end-to-end. It has shown good results in image recognition and generation tasks.
The study____ introduces regularization terms for PN, incorporating techniques like data augmentation, instance normalization, and increased feature interaction orders to enhance model performance.
Cheng et al. drew inspiration from contemporary architectural designs to propose MONet____, aiming to narrow the gap between multilinear networks and state-of-the-art architectures.
It is important to note that both polynomial and multilinear networks can capture interactions of features at exponential orders, though the primary distinction lies in their structural flexibility: polynomial networks maintain an unfoldable structure, whereas multilinear networks lose this property when instance normalization is applied, as
\begin{equation}\begin{split}
\boldsymbol{A}\boldsymbol{z} * \boldsymbol{B}\boldsymbol{z} &= vec(\boldsymbol{z} \otimes \boldsymbol{z})(\boldsymbol{A}^T\odot \boldsymbol{B}^T),\\
LN(\boldsymbol{A}\boldsymbol{z} ) * (\boldsymbol{B}\boldsymbol{z}) &\neq vec(\boldsymbol{z} \otimes \boldsymbol{z})LN(\boldsymbol{A}^T\odot \boldsymbol{B}^T).
\label{eq2}
\end{split}\end{equation}
Here, $LN$ represents layer normalization, while $A$ and $B$ denote learnable matrixes.
Our objectives include re-establishing the polynomial expansion form and analyzing the distinctions between DTTN and quantum-inspired TNs. Additionally, by integrating layer normalization, we aim to develop a multilinear DTTN$^\dagger$ that surpasses the performance of current high-performing multilinear networks____