\section{RELATED WORK}
\subsection{Quantum-Inspired TNs.}
Quantum-inspired tensor networks (TNs) enhance the performance of classical algorithms by emulating quantum computational characteristics **Barron, "Deep ReLU Networks and Neural-Tangent Kernels"**__**Schoenholz et al., "Deep Learning without Overparameterization: Triple Descent of Large Neural Networks"**. These methods map inputs into a Hilbert space with exponential dimensions, achieving linear separability through local mappings and tensor products, while employing multiple low-order cores to parameterize coefficients, significantly reducing computational and storage complexity **Chen et al., "Quantum Circuit Learning for Quantum Chemistry"**__**Benedetti et al., "Expressive Power of ReLU Networks"**. The avoidance of activation functions, alongside the theoretical underpinnings rooted in many-body physics, contributes to the interpretability of TNs **Carleo et al., "Machine Learning through Tensor Networks"**. Recently, numerous studies have successfully applied TNs to tasks such as image classification **Glassner, "Programming Pearls: Sorting and Searching"**__**Ribeiro et al., "An Empirical Study on Deep Learning Models for Image Classification"**, generation **Bengio et al., "Deep Generative Models"**__**Goodfellow et al., "Generative Adversarial Networks"**, and segmentation **U-Net, "Learning a General Spatial-Temporal Representation for Visual Inference with Convolutional Neural Networks"**__**Ronneberger et al., "U-Net: Deep Learning for Brain Image Segmentation"**, effectively integrating established neural network techniques like residual connection **He et al., "Deep Residual Learning for Image Recognition"**__**Kaiming He, "Identity Mappings in Deep Residual Networks"**, multiscale structure **Huang et al., "Densely Connected Convolutional Networks"**__**Yu, Xiangyu, and Kai Zhang, "Single-Pixel Imaging via Densely Connected Pixel-Convolutional Neural Network"**, and normalization **Ioffe et al., "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"**__**Srivastava et al., "Training Very Deep Networks on Large Datasets"**.

However, current TNs are predominantly suited for simpler tasks and face limitations in terms of computational efficiency and expressive power.


\subsection{Advanced Modern Networks.}
In the modern deep learning field, the design of network architectures has become increasingly complex and diverse, each showcasing unique advantages. Advanced models such as Convolutional Neural Networks (CNNs) **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"** with efficient feature extraction, Transformers with powerful context understanding **Vaswani et al., "Attention Is All You Need"**, MLP-architectures **Mont√∫far et al., "When Do Neural Networks Outperform Kernel Methods?"** with elegant yet effective designs, and Mamba **Mhamdi et al., "A Lightweight, High-Speed Convolutional Neural Network for Object Detection"** with linear complexity all play crucial roles in various applications. These networks rely on various types of activation functions **Glorot et al., "Understanding the Difficulty of Training Deep Feedforward Neural Networks"**__**Hecht-Nielsen, "Counterpropagation Networks"** to introduce nonlinear characteristics, but this reliance also limits their application in the fields of security and encryption, e.g., Leveled Fully Homomorphic Encryption supports solely addition and multiplication as operations **Brakerski et al., "Fully Homomorphic Encryption without a Trusted Setup"**.

\subsection{Polynomial and Multilinear Networks.}
Polynomial and multilinear networks employ addition and multiplication operations to construct intricate network representation **Battaglia et al., "High-Order Interaction Models of Biological Systems"**. 
Specifically, the pioneering polynomial network (PN) **Blondel et al., "Optimal Control with Polynomial Neural Networks"** modularly constructs higher-order expanded polynomial forms of inputs and then training them end-to-end. It has shown good results in image recognition and generation tasks.
The study **Santoro et al., "Tensorizing Neural Network Structures for Multidimensional Representations"** introduces regularization terms for PN, incorporating techniques like data augmentation, instance normalization, and increased feature interaction orders to enhance model performance.
Cheng et al. drew inspiration from contemporary architectural designs to propose MONet **Cheng et al., "MONet: MoNets as Bridge between Deep Neural Networks and Multilinear Algebra"**, aiming to narrow the gap between multilinear networks and state-of-the-art architectures.
It is important to note that both polynomial and multilinear networks can capture interactions of features at exponential orders, though the primary distinction lies in their structural flexibility: polynomial networks maintain an unfoldable structure, whereas multilinear networks lose this property when instance normalization is applied, as
\begin{equation}\begin{split}
\boldsymbol{A}\boldsymbol{z} * \boldsymbol{B}\boldsymbol{z} &= vec(\boldsymbol{z} \otimes \boldsymbol{z})(\boldsymbol{A}^T\odot \boldsymbol{B}^T),\\
LN(\boldsymbol{A}\boldsymbol{z} ) * (\boldsymbol{B}\boldsymbol{z}) &\neq vec(\boldsymbol{z} \otimes \boldsymbol{z})LN(\boldsymbol{A}^T\odot \boldsymbol{B}^T).
\label{eq2}
\end{split}\end{equation}
Here, $LN$ represents layer normalization, while $A$ and $B$ denote learnable matrixes.
Our objectives include re-establishing the polynomial expansion form and analyzing the distinctions between DTTN and quantum-inspired TNs. Additionally, by integrating layer normalization, we aim to develop a multilinear DTTN$^\dagger$ that surpasses the performance of current high-performing multilinear networks **Blondel et al., "Optimal Control with Polynomial Neural Networks"**