\documentclass{article}


\pdfoutput=1

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

% Use the postscript times font!
\usepackage{times}
\usepackage{multirow}
\usepackage[table]{xcolor} 
\usepackage{soul}
\usepackage{bbding}
\usepackage{url}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{subfig}
\usepackage{newfloat}
\usepackage{bbding}
\usepackage{listings}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amssymb}
\usepackage{color}
\usepackage{booktabs}

\newtheorem{example}{\textbf{example}}
\newtheorem{Lemma}{\textbf{Lemma}}
\newtheorem{Definition}{\textbf{Definition}}
\newtheorem{Theorem}{\textbf{Theorem}}
\newtheorem{Proposition}{\textbf{Proposition}}
\newtheorem*{proposition}{\textbf{Proposition}}
\newtheorem*{theorem}{\textbf{Theorem}}
\newtheorem{Remark}{\textbf{Remark}}

\usepackage{xcolor}
% Define colors for the code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
% Set up the listings package
\lstset{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=Python
}





\title{Deep Tree Tensor Networks for Image Recognition}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it
% \emph

\author{ {Chang Nie}\thanks{Corresponding author} \\
    Nanjing University of Science and Technology\\
	\texttt{changnie@njust.edu.cn} \\
    \And
    {Junfang Chen} \\
    East China Normal University\\
	\texttt{junfangchen@163.com}
    \And
{Yajie Chen} \\
Li Auto Inc.\\
	\texttt{yajiechen@njust.edu.com}
	%% examples of more authors
	% \And
	% \href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Elias D.~Striatum} \\
	% Department of Electrical Engineering\\
	% Mount-Sheikh University\\
	% Santa Narimana, Levand \\
	% \texttt{stariate@ee.mount-sheikh.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%renewcommand{\headeright}{Technical Report}
\renewcommand{\undertitle}{}
% \renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
% \hypersetup{
% pdftitle={A template for the arxiv style},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
% }

\begin{document}
\maketitle

\begin{abstract}
Originating in quantum physics, tensor networks (TNs) have been widely adopted as exponential machines and parameter decomposers for recognition tasks.
Typical TN models, such as Matrix Product States (MPS), have not yet achieved successful application in natural image processing. When employed, they primarily serve to compress parameters within off-the-shelf networks, thus losing their distinctive capability to enhance exponential-order feature interactions.
This paper introduces a novel architecture named \textit{\textbf{D}eep \textbf{T}ree \textbf{T}ensor \textbf{N}etwork} (DTTN), which captures $2^L$-order multiplicative interactions across features through multilinear operations, while essentially unfolding into a \emph{tree}-like TN topology with the parameter-sharing property.
DTTN is stacked with multiple antisymmetric interacting modules (AIMs), and this design facilitates efficient implementation.
Moreover, we theoretically reveal the equivalency among quantum-inspired TN models and polynomial and multilinear networks under certain conditions, and we believe that DTTN can inspire more interpretable studies in this field.
We evaluate the proposed model against a series of benchmarks and achieve excellent performance compared to its peers and cutting-edge architectures. Our code will soon be publicly available.
\end{abstract}


% keywords can be removed
\keywords{Tensor Networks \and Multilinear Networks \and Image Recognition}

\section{INTRODUCTION}
\textit{"Simplicity is the ultimate sophistication."——Leonardo da Vinci}

The wavefunction of a quantum many-body system typically resides in an extremely high-dimensional Hilbert space, with its complexity increasing exponentially as the particle count grows~\citep{jiang2008accurate,zhao2010renormalization}.
For example, consider a system consisting of $N$ spin-$\frac{1}{2}$ particles; the dimensionality of the corresponding Hilbert space would be $2^N$.
Tensor networks (TNs) offer powerful numerical techniques for tackling the "\textit{Curse of Dimensionality}"~\citep{cichocki2016tensor}.
By leveraging the local entanglement properties of quantum states, TNs represent complex wavefunctions into multilinear representations of multiple low-dimensional cores, thereby significantly reducing computational and storage requirements\footnote{In tensor network theory, states that satisfy the area law for entanglement entropy can be efficiently approximated using TNs with finite bond dimensions.}~\citep{ref1,ref12}.
This class of methods allows for an accurate representation of quantum states while mitigating the exponential growth in complexity, making it feasible to simulate large-scale quantum systems~\citep{jaschke2018one,ref12}.

Recently, TN-based interpretable and quantum-inspired white-box machine learning has attracted the attention of researchers. It holds the potential to generate novel schemes that can run on quantum hardware~\citep{huggins2019towards,ran2023tensor}.
Typical TN models, including Matrix Product States~\citep{cirac2021matrix} (MPS\footnote{The MPS is also referred to as Tensor Train~\citep{ref22}, or Tensor Ring~\citep{ref14} with the periodic condition in classical machine learning.}), Tree Tensor Network (TTN)~\citep{cheng2019tree}, and Multi-Scale Entanglement Renormalization Ansatz (MERA)~\citep{reyes2020multi} are skillfully applied in image classification.
Routine practice is to map each pixel or local patch of an image to a $d$-dimensional vector by a local map function $\phi(\cdot)$, and then use the \textit{tensor product} to obtain a joint feature map $\Phi(\boldsymbol{x})$ of $d^N$ dimensionality (see Fig.\ref{img1} left). 
The TN model used for image classification can be expressed as follows:
\begin{equation}\begin{split}
f(\boldsymbol{x})=\text{arg}\ \underset{m}{\text{max}}\ \ \boldsymbol{W}^m *\Phi(\boldsymbol{x}).
\label{eq1}
\end{split}\end{equation}
Here, $\boldsymbol{W}^m$ represents a $(N+1)$-th order tensor with an output index $m$; $f(\cdot):\mathbb{R}^{whc}\rightarrow \mathbb{R}^{m}$ denotes a multilinear function.
In principle, mapping samples into exponential dimensional space to achieve linear separability instead of adopting activation functions is the essence of TNs~\citep{selvan2020tensor,ran2023tensor,patra2024efficient}. However, existing methods are limited to simple tasks, e.g., MNIST and Fashion MNIST~\citep{cheng2019tree,ran2023tensor}, and we reveal that is mainly due to 1) \textit{low computational efficiency} and 2) \textit{lack of feature self-interaction capability} (see section 3.3 for more details). Hence, we aim to address the above challenges and apply TNs to complex benchmarks, thus bridging the gap between TNs and advanced architectures.

\begin{figure}[t]
\centering
\includegraphics[width=.8\textwidth]{./img/11.png} 
\caption{Schematic diagram of the quantum-inspired MPS model and DTTN towards image recognition task. The former is applied for simple inputs and setting a small local mapping dimension $d=2$ and bond dimension $D\leq 64$ in general\citep{ran2023tensor}. DTTN handles complex inputs while retaining spatial locality in linear projection, and its parameter-sharing nature allows for maintaining a high bond dimension.}
\label{img1}
\end{figure}

On the other hand, TNs are popularly employed to parameterize existing network models for acceleration. For example, networks' variational parameters can directly decompose into TN formats, including convolutional kernels~\citep{ref8}, fully-connected layers~\citep{li2023hybrid,nie2022stn}, and attention blocks~\citep{liang2024tensor}, to name a few. Retaining or merging the parameter-wise TN structure during the model inference is free. However, such techniques are devoid of probabilistic interpretation and feature multiplicative interaction. Notably, advanced deep learning architectures and TNs have unexplored striking similarities, including MLP-Mixer~\citep{tolstikhin2021mlp}, polynomial and multilinear networks~\citep{chrysos2021deep,chrysos2023regularization,cheng2024multilinear}. They have already achieved excellent results across complex visual tasks. As shown in Fig.\ref{img2}, we visualize different architectures from the TN perspective for comparison.
We observe that modern architectures differ from existing TNs in several key aspects, such as the use of nonlinear activations and instance normalization. Therefore, our goal is to transfer the advantages of advanced architectures to DTTN through equivalence analysis.
Thus break through the limitations of TNs for complex benchmarks. 

Concretely, we introduce a novel class of TN architectures, named \textit{Deep Tree Tensor Network} (DTTN), which uses multilinear operations to capture exponential-order interactions among features and predict targets without activation functions and attention components.
DTTN is sequentially stacked by multiple simple antisymmetric interacting modules (AIM) and inherently unfolds into a \textit{tree}-like TN structure to reach on-par performance compared to advanced architectures, with better interpretability and understanding.
Overall, the contributions of this paper are summarized as follows:

\begin{itemize}
\item We introduce DTTN, a simple yet effective architecture constructed by sequentially stacking AIMs.
DTTN captures feature $2^L$ multiplicative feature interactions without activation functions and achieves state-of-the-art performance compared to other polynomial and multilinear networks with faster convergence (see Fig.~\ref{img2}).

\item We provide a theoretical analysis of the equivalency between DTTN and other architectures under specific conditions. 
Without instance normalization, DTTN essentially transforms into a tree-topology TN, thereby overcoming the limitations of TN-based Born machines that excel mainly in simple tasks.

\item We comprehensively evaluate the performance and broader impact of the proposed model across multiple benchmarks, demonstrating that DTTN performs on par with well-designed advanced architectures.
\end{itemize}

\noindent\textbf{Notations}.
Throughout this paper, we use $\boldsymbol{x}\in\mathbb{R}^{I_1}$, $\boldsymbol{X}\in\mathbb{R}^{I_1\times I_2}$, $\boldsymbol{\mathcal{X}}\in\mathbb{R}^{I_1\times\dots \times I_N}$ to denote $1$-th order vectors, $2$-th order matrices, and $N$-th order tensors, respectively.
The blackboard letters are employed to represent a set of objects, e.g. $\mathbb{R}$ and $\mathbb{Z}$ denote real numbers and integers.
In addition, $*$, $\odot$, and $\otimes$ denote the \textit{Hadamard}, \textit{Khatri-Rao}, and \textit{tensor} products respectively.
For brevity, $|\mathbb{K}|$ denote the cardinality of a set $\mathbb{K}$, and $\mathbb{K}_N$ indicate the positive integers set $\{1,2,\dots,N\}$.
Moreover, for given tensors $\boldsymbol{\mathcal{A}}\in\mathbb{R}^{I_1\times I_2 \times I_3}$ and $\boldsymbol{\mathcal{A}}\in\mathbb{R}^{I_4\times I_5 \times I_6 \times I_7}$, with $I_2=I_4$ and $I_3=I_5$. The tensor contraction is executed by summing along the shared modes of $\boldsymbol{\mathcal{A}}$, $\boldsymbol{\mathcal{B}}$ to yield a new tensor $\boldsymbol{\mathcal{C}}=\boldsymbol{\mathcal{A}}\times_{2,3}^{1,2}\boldsymbol{\mathcal{B}} \in\mathbb{R}^{I_1\times I_6 \times I_7}$.
The entry-wise calculation can be expressed as $\boldsymbol{\mathcal{C}}_{(i_1,i_6,i_7)}=\sum_{i_2=1}^{I_2}\sum_{i_3=1}^{I_3}\boldsymbol{\mathcal{A}}_{(i_1,i_2,i_3)}\boldsymbol{\mathcal{B}}_{(i_2,i_3,i_6, i_7)}$. For more definitions refer to~\citep{ref21}


\begin{figure*}[t]
\centering
\includegraphics[width=.99\textwidth]{./img/22.png} 
\caption{(a-d) Illustration of Core Blocks for Different Architectures. The MLP-Mixer utilizes GELU activation and other networks via the Hadamard product '$*$' to enable the network to learn complex representations. We emphasize that instance batch normalization (IBN) and layer normalizations (LN)~\citep{xu2019understanding} preceded before Hadamard product operations disrupt the polynomial unfolding nature of $\mathcal{R}$-PolyNets~\citep{chrysos2023regularization} and MONet~\citep{cheng2024multilinear}. In contrast, the succinctly designed AIM circumvents this issue. The optional LN inside AIM not only enhances performance but also facilitates faster convergence. (e) Comparison of Different Networks on ImageNet-1k. When comparing various networks trained on ImageNet over different epochs, DTTN stands out by achieving state-of-the-art performance compared to other multilinear networks, significantly outperforming them.
}
\label{img2}
\end{figure*}

\section{RELATED WORK}
\subsection{Quantum-Inspired TNs.}
Quantum-inspired tensor networks (TNs) enhance the performance of classical algorithms by emulating quantum computational characteristics~\citep{huggins2019towards}. These methods map inputs into a Hilbert space with exponential dimensions, achieving linear separability through local mappings and tensor products, while employing multiple low-order cores to parameterize coefficients, significantly reducing computational and storage complexity~\citep{selvan2020tensor,stoudenmire2016supervised}. The avoidance of activation functions, alongside the theoretical underpinnings rooted in many-body physics, contributes to the interpretability of TNs~\citep{ran2023tensor}. Recently, numerous studies have successfully applied TNs to tasks such as image classification~\citep{stoudenmire2016supervised}, generation~\citep{cheng2019tree}, and segmentation~\citep{selvan2021patch}, effectively integrating established neural network techniques like residual connection~\citep{meng2023residual}, multiscale structure~\citep{liu2019machine}, and normalization~\citep{selvan2020tensor} into TN frameworks.
However, current TNs are predominantly suited for simpler tasks and face limitations in terms of computational efficiency and expressive power.


\subsection{Advanced Modern Networks.}
In the modern deep learning field, the design of network architectures has become increasingly complex and diverse, each showcasing unique advantages. Advanced models such as Convolutional Neural Networks (CNNs)~\citep{ref31,hou2024conv2former} with efficient feature extraction, Transformers with powerful context understanding~\citep{vaswani2017attention}, MLP-architectures~\citep{tolstikhin2021mlp} with elegant yet effective designs, and Mamba~\citep{gu2023mamba} with linear complexity all play crucial roles in various applications. These networks rely on various types of activation functions~\citep{apicella2021survey} to introduce nonlinear characteristics, but this reliance also limits their application in the fields of security and encryption, e.g., Leveled Fully Homomorphic Encryption supports solely addition and multiplication as operations~\citep{brakerski2014leveled,cheng2024multilinear}.

\subsection{Polynomial and Multilinear Networks.}
Polynomial and multilinear networks employ addition and multiplication operations to construct intricate network representation~\citep{chrysos2021deep,chrysos2023regularization,cheng2024multilinear}. 
Specifically, the pioneering polynomial network (PN)~\citep{chrysos2021deep} modularly constructs higher-order expanded polynomial forms of inputs and then training them end-to-end. It has shown good results in image recognition and generation tasks.
The study~\citep{chrysos2023regularization} introduces regularization terms for PN, incorporating techniques like data augmentation, instance normalization, and increased feature interaction orders to enhance model performance.
Cheng et al. drew inspiration from contemporary architectural designs to propose MONet~\citep{cheng2024multilinear}, aiming to narrow the gap between multilinear networks and state-of-the-art architectures.
It is important to note that both polynomial and multilinear networks can capture interactions of features at exponential orders, though the primary distinction lies in their structural flexibility: polynomial networks maintain an unfoldable structure, whereas multilinear networks lose this property when instance normalization is applied, as
\begin{equation}\begin{split}
\boldsymbol{A}\boldsymbol{z} * \boldsymbol{B}\boldsymbol{z} &= vec(\boldsymbol{z} \otimes \boldsymbol{z})(\boldsymbol{A}^T\odot \boldsymbol{B}^T),\\
LN(\boldsymbol{A}\boldsymbol{z} ) * (\boldsymbol{B}\boldsymbol{z}) &\neq vec(\boldsymbol{z} \otimes \boldsymbol{z})LN(\boldsymbol{A}^T\odot \boldsymbol{B}^T).
\label{eq2}
\end{split}\end{equation}
Here, $LN$ represents layer normalization, while $A$ and $B$ denote learnable matrixes.
Our objectives include re-establishing the polynomial expansion form and analyzing the distinctions between DTTN and quantum-inspired TNs. Additionally, by integrating layer normalization, we aim to develop a multilinear DTTN$^\dagger$ that surpasses the performance of current high-performing multilinear networks~\citep{cheng2024multilinear}

\section{METHOD}
In this section, we provide a detailed description of the Deep Tree Tensor Network (DTTN). We aim to construct a tree topology network by sequentially stacking AIM blocks, which involve solely multilinear operations.
For an input image $\boldsymbol{x}$, similar to other methodologies, we use a vanilla linear projection, also referred to as patch embedding~\citep{tolstikhin2021mlp}, to derive the feature map $\phi(\boldsymbol{\boldsymbol{x},\boldsymbol{\Lambda}_\phi})\in\mathbb{R}^{W\times H\times C}$. Here $\boldsymbol{\Lambda}_\phi\in\mathbb{R}^{S^2\times C}$ represents a learnable matrix, where $S,C\in\mathbb{N}$ denote the local patch size and the number of output channels, respectively.
This procedure corresponds to the local mapping illustrated in Fig.\ref{img1}, with its output serving as the input for the DTTN.
It should be noted that batch normalization (BN) operations following the linear layer have been omitted for brevity. This omission does not affect the conclusions drawn in this paper, as these operations can be integrated with the nearest linear layer during inference through structural re-parametrization techniques~\citep{ding2021repvgg}.

\subsection{Antisymmetric Interaction Module}
The antisymmetric interaction module (AIM) is the core of DTTN. As illustrqated in Fig.~\ref{img2}(d), for the $l$-th block input feature map $\boldsymbol{\mathcal{X}}^l\in\mathbb{R}^{W_l\times H_l\times C_l}$, we utilize an antisymmetric two-branch structure to capture the linear interactions of the input features separately.
Both branches, denoted as $f_1^l,f_2^l$, incorperate a depthwise convolution layer and a linear layer but apply them in reverse order.
These layers are designed to capture spatial locality and channel interactions, respectively, effectively leveraging the advantages of CNN and MLP architectures~\citep{ref31,tolstikhin2021mlp}.
Furthermore, the antisymmetric design aims to reduce the complexity of AIM. The ratio of parameters and FLOPs between the two branches is given by:
\begin{equation}\begin{split}
R_{Para}&=\frac{r_{exp}k^2C_l+r_{exp}C_l^2}{r_{exp}k^2C_l+r_{exp}^2C_l^2}\ \ \sim\ \ \frac{1}{r_{exp}},\\
R_{Flops}&=\frac{r_{exp}k^2W_lH_lC_l+r_{exp}W_lH_lC_l^2}{r_{exp}k^2W_lH_lC_l+r_{exp}^2W_lH_lC_l^2}\ \ \sim\ \ \frac{1}{r_{exp}},
\label{eq3}
\end{split}\end{equation}
where $r_{exp}\in\mathbb{N}_{+}$ represents the expansion ratio in the inverted bottleneck design, typically set to $3$, $k\in\mathbb{N}_{+}$ denotes the kernel size, and $C_l,W_l,H_l$ are the number of channels, width and height of the $l$-th block input feature map respectively.
We employ the hardware-friendly Hadamard product to capture the second-order interactions of the branch outputs, where $*$ symbolizes
an element-wise product.
Following this, an optional LN and linear projection layers are sequentially applied to the computation results.
Finally, a shortcut connection is used to preserve the input signal and accelerate training. (see Fig.\ref{img2}).
Overall, the AIM forward calculation can be expressed as
\begin{equation}\begin{split}
\boldsymbol{\mathcal{X}}^{l+1}&=\boldsymbol{\mathcal{X}}^{l}+
Pro\left(f_1^l(\boldsymbol{\mathcal{X}}^{l}) * f_2^l(\boldsymbol{\mathcal{X}}^{l})\right), \ \ for \ \ l \in \mathbb{K}_{L}.
\label{eq4}
\end{split}\end{equation}
In summary, AIM captures second-order multiplicative interactions among input elements through multilinear operations without employing nonlinear activations.
In contrast to blocks inside other architectures, such as the Basic Block in $\mathcal{R}$-PolyNets~\citep{chrysos2023regularization} and Mu-layer\footnote{The MONet architecture comprises a stack of two variants of Mu-Layer. The second variant differs from the one shown in Fig.\ref{img2}(c) in that it does not incorporate a spatial shift operation.} in MONet~\citep{cheng2024multilinear}, AIM employs an antisymmetric design with only one shortcut connection.

\noindent %
\begin{minipage}[t]{0.42\textwidth}
    \includegraphics[width=\linewidth]{./img/33.png}
    \captionof{figure}{Schematic diagram of the DTTN architecture.} 
    \label{img3}
\end{minipage}
\hfill
\begin{minipage}[t]{0.58\textwidth}
\vspace{-50mm}
\captionof{table}{Specifications of different DTTN variants configuration.  Specifically, 'Tiny', 'Small', and 'Large' correspond to DTTN-T, DTTN-S, and DTTN-L, respectively, each with distinct parameter sizes. The primary distinction among these variants lies in the number of blocks and the hidden-size within their multi-stage configurations. We use '$\dagger$' to indicate that an LN layer is used, e.g. DTTN$^\dagger$-S.} % 为表格添加标题
    \centering
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Specification} & \textbf{Tiny} & \textbf{Small} & \textbf{Large}  \\
\midrule
Numbers of Blocks & 34 &44 & 56  \\
Hidden-size & 64,128,160,192 & 96,128,192,192 & 128,192,256,384 \\
Stages & 6,6,16,6 & 6,6,24,8 & 8,8,32,8 \\
Expansion ratio & 3 & 3 & 3 &  \\
Parameters (M) & 7.1 & 12.3 & 35.9 \\
FLOPs (B)  & 2.3 & 4.1 & 12.3 \\
\bottomrule
\label{tab1}
\end{tabular}}
\end{minipage}


\subsection{NETWORK ARCHITECTURE}
Our proposed architecture involves stacking $L$ AIM blocks sequentially.
The final output is derived through average pooling and a fully-connected layer (refer to Fig.\ref{img3}).
The DTTN architecture is multi-stage, similar to other existing architectures~\citep{chrysos2021deep,hou2022vision,cheng2024multilinear}. By varying the hidden-sizes and the number of blocks in each stage, we have designed three variants of DTTN: \textit{DTTN-T}, \textit{DTTN-S}, and \textit{DTTN-L} (see Table~\ref{tab1}), each with distinct parameters to facilitate a comparative analysis.
At a high level, we assert that the DTTN possesses the following characteristic:
\begin{Proposition}
The DTTN has the capability to capture $2^L$ multiplicative interactions among input elements, which can be represented in the format of Equation~\ref{eq1} as $\Phi(\boldsymbol{x})=\otimes^{2^L}\phi(\boldsymbol{x},\boldsymbol{\Lambda}_\phi)$.
Consequently, the elements of $f(\boldsymbol{x})$ are homogeneous polynomials of degree $2^L$ over the feature map $\phi(\boldsymbol{x},\boldsymbol{\Lambda}_\phi)$.
\end{Proposition}
It should be noted, however, that this property does not hold when LN is applied within the network, introducing second-order statistical information, as seen in models like MONet and DTTN$^\dagger$. In such cases, the network reverts to functioning as an ordinary multilinear network.

\vspace{1mm}
\noindent\textbf{Unfolding Topology}.
As illustrated in Fig.~\ref{img3}, the AIM is equivalent to a binary tree node without LN. At this time, AIM forward computation can be regarded as a TN contraction, which can be represented as
\begin{equation}\begin{split}
\boldsymbol{x}^{l+1}=&\boldsymbol{x}^{l}+\boldsymbol{B}^{l} \left((\boldsymbol{A}_1^{l}\boldsymbol{x}^{l}) * (\boldsymbol{A}_2^{l}\boldsymbol{x}^{l})\right)\\
=&\boldsymbol{x}^{l}+\mathtt{Reshape}\left(  \boldsymbol{B}^{l} (\boldsymbol{A}_1^{l\ ^T}\odot \boldsymbol{A}_2^{l\ ^T})^T\right)\times_{2,3}^{1,2}(\boldsymbol{x}^{l}\otimes\boldsymbol{x}^{l})\\
=& \boldsymbol{\mathcal{C}}^l\times_{2,3}^{1,2}(\boldsymbol{x}^{l}\otimes\boldsymbol{x}^{l})
\label{eq5}
\end{split}\end{equation}
Here $\boldsymbol{x}^{l}=vec(\boldsymbol{\mathcal{X}}^l)\in\mathbb{R}^{W_l H_l C_l}$, $\boldsymbol{\mathcal{C}}_l$ signifies a $3$-th order tensor that is
structural re-parametrization with the learnable matrixes $\boldsymbol{A}_1^{l},\boldsymbol{A}_2^{l}$ and $\boldsymbol{B}^{l}$.
Conclusively, the AIM captures the second-order multiplicative interactions among input elements via a tensor product and structures the $3$-order tensor utilizing \emph{PyTorch} operators.
Thus, a DTTN comprised of $L$ AIMs can essentially be unfolded into a tree network with $2^L$ leaf nodes, as illustrated in Fig.\ref{img1}.

\subsection{DTTN vs. Other Architectures}

\noindent\textbf{DTTN $\textbf{\&}$ Polynomial Networks}. DTTN shares the same polynomial expansion form as $\prod$-Net~\citep{chrysos2021deep}, which can be formulated as
\begin{equation}\begin{split}
f(\boldsymbol{x})=\sum_{l=1}^{2^L}\left( \mathcal{W}^{[l]}\times_{2,\dots,l+1}^{1,\dots,l} (\otimes^{l}\phi(x)) \right) + \beta,
\label{eq6}
\end{split}\end{equation}
where $\beta\in\mathbb{R}^m$ represents a bias term, and $\mathcal{W}^{[l]}$ is a $(l+1)$-th order learnable parameter tensor.
It is worth noting that equations (\ref{eq1}) and (\ref{eq6}) become equivalent when a bias term is introduced for the elements of input $\boldsymbol{x}$.
The networks differ in the structured representation of the coefficients $\mathcal{W}^{[l]}, l\in\mathbb{K}_{2^L}$. Specifically, these two networks feature distinct network blocks and computational graphs.

\noindent\textbf{DTTN $\textbf{\&}$ Multilinear Networks}.
We remark that networks that solely involve multilinear operations, such as MONet and DTTN$^\dagger$ and $\mathcal{R}$-PolyNets, but lose the polynomial expansion formalism can be categorized as multilinear networks. Additionally, both Polynomial Networks and TNs fall under the category of multilinear networks.

\noindent\textbf{DTTN $\textbf{\&}$ Quantum-inspired TNs}. 
The primary advantages of DTTN over Quantum-inspired TNs are twofold. (1) The alternative of the local map function.
Existing TNs utilized trigonometric functions for local mapping, which resulted in each term of the network's unfolded form lacking higher-order ($\geq 2$) terms of the input elements, thus losing the capability of feature self-interaction. (2) The higher bond dimension induced by parameter-sharing properties.
Quantum-inspired TNs parallelize the contraction of shared indexes of among $N$ cores within the same layer, but limited memory resources restrict the bond dimension to small values.
We further explore the connection between DTTN and TNs in the following theorem.
\begin{Theorem}
Given the local mapping function $\phi^{i_1}(x_1)=[x_1^0,\cdots,x_1^{2^L}]^T$, a polynomial network with the expansion form of Equation~\ref{eq6}) can be transformed into a quantum-inspired TN model with finite bond dimension.
\end{Theorem}
Considering that the internal cores of a TN can be decomposed into a "core$\&$diagonal factor$\&$core" form via higher-order Singular Value Decomposition (HOSVD)~\citep{ref21} and merged with connectivity cores, we ignore the structural difference between DTTN and Quantum-inspired TNs.
We believe this theorem not only establishes equivalence between quantum-inspired TNs and modern architectures but also provides guidance for the further development of interpretable and high-performance TNs.

\begin{table}[!ht]
\centering
\caption{ImageNet-1K classification accuracy (\%) for various network architectures. Models that can be polynomially expanded are marked in red, whereas other multilinear models that do not include activation functions but incorporate layer normalization are marked in green. Our {DTTN$^\dagger$-T} outperforms the previous SOTA model MONet-T by 0.9\% with fewer parameters and FLOPs.}
\label{tab2}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{@{}l|ccccccc@{}}
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \textbf{Top-1(\%)}  & \small{\textbf{Params (M)}} & \small{\textbf{FLOPs(B)}} & \textbf{Epoch} & \small{\textbf{Activation}} & \small{\textbf{Attention}} & \textbf{Reso.} \\
\midrule
\multicolumn{8}{l}{\textbf{CNN-based}} \\
\midrule
ResNet-50~\citep{ref31} & {77.2} & 25.0 & 4.1 & - & ReLU & $\boldsymbol{\times}$ & $224^2$ \\
A$^2$Net~\citep{chen20182} & 77.0 & 33.4 & 31.3 & - & ReLU & \checkmark  & $224^2$\\
AA-ResNet-152~\citep{bello2019attention} & 79.1 & 61.6 & 23.8 & 100 & ReLU & \checkmark & $224^2$\\
RepVGG-B2g4~\citep{ding2021repvgg} & 79.4 & 55.7 & 11.3 & 200 & ReLU & $\boldsymbol{\times}$ & $224^2$\\
\midrule
\multicolumn{8}{l}{\textbf{Transformer- and Mamba-based}} \\
\midrule
ViT-B/16~\citep{dosovitskiy2020image} & 77.9 & 86.0 & 55.0 & 300 & GeLU & \checkmark & $224^2$\\
DeiT-S/16~\citep{touvron2021training} & 81.2 & 24.0 & 5.0 & 300 & GeLU & \checkmark & $224^2$\\
Swin-T/16~\citep{liu2021swin} & 81.3 & 29.0 & 4.5 & 300 & GeLU & \checkmark & $224^2$\\
Vim-S~\citep{zhu2024vision} & 80.5 & 26.0 & - & 300 & SiLU & \checkmark & $224^2$\\
\midrule
\multicolumn{8}{l}{\textbf{MLP-based}} \\
\midrule
\small{MLP-Mixer-B/16~\citep{tolstikhin2021mlp}}
 & 76.4 & 59.0 & 11.6 & 300 & GeLU & $\boldsymbol{\times}$ & $224^2$\\
\small{MLP-Mixer-L/16~\citep{tolstikhin2021mlp}} & 71.8 & 507.0 & 44.6 & 300 & GeLU & $\boldsymbol{\times}$ & $224^2$\\
CycleMLP-T~\citep{chen2021cyclemlp} & 81.3 & 28.8 & 4.4 & 300 & GeLU & $\boldsymbol{\times}$ & $224^2$\\
Hire-MLP-Tiny~\citep{guo2022hire} & 79.8 & 18.0 & 2.1 & 300 & GeLU & $\boldsymbol{\times}$ & $224^2$\\
ResMLP-24~\citep{touvron2022resmlp} & 79.4 & 6.0 & 30.0 & 300 & GeLU & $\boldsymbol{\times}$ & $224^2$\\
$S^2$MLP-Wide~\citep{yu2022s2} & 80.0 & 71.0 & 14.0 & 300 & GeLU & $\boldsymbol{\times}$ & $224^2$\\
$S^2$MLP-Deep~\citep{yu2022s2} & 80.7 & 10.5 & 51.0 & 300 & GeLU & $\boldsymbol{\times}$ & $224^2$\\
ViP-Small/14~\citep{hou2022vision} & 80.5 & 30.0 & 6.5 & 300 & GeLU & \checkmark & $224^2$\\
AFFNet~\citep{huang2023adaptive} & 79.8 & 6.0 & 1.5 & 300 & ReLU & \checkmark & $256^2$\\
\midrule
\multicolumn{8}{l}{\textbf{Polynomial- and Multilinear-based}} \\
\midrule
\rowcolor{pink!30}
$\Pi$-Nets~\citep{chrysos2021deep} & 65.2 & 12.3 & 1.9 & 90 & - & $\boldsymbol{\times}$ & $224^2$\\
\rowcolor{pink!30}
\textbf{DTTN-S(ours) }& \textbf{71.8}/\textbf{77.2} & 12.3 & 4.1 & 90/300 & - & $\boldsymbol{\times}$ & $224^2$\\

Hybrid $\Pi$-Nets~\citep{chrysos2021deep} & 70.7 & 11.9 & 1.9 & 90 & ReLU+Tanh & $\boldsymbol{\times}$ & $224^2$\\

PDC~\citep{chrysos2022augmenting} & 71.0 & 10.7 & 1.6 & 100 & ReLU+Tanh & $\boldsymbol{\times}$ & $224^2$\\
PDC-comp~\citep{chrysos2022augmenting} & 70.2 & 7.5 & 1.3 & 100 & ReLU+Tanh & $\boldsymbol{\times}$ & $224^2$\\
\rowcolor{green!20}
$\mathcal{R}$-PolyNets~\citep{chrysos2023regularization} & 70.2 & 12.3 & 1.9 & 120 & - & $\boldsymbol{\times}$ & $224^2$\\
\rowcolor{green!20}
$\mathcal{D}$-PolyNets~\citep{chrysos2023regularization} & 70.0 & 11.3 & 1.9 & 120 & - & $\boldsymbol{\times}$ & $224^2$\\
\rowcolor{green!20}
MONet-T~\citep{cheng2024multilinear} & 77.0 & 10.3 & 2.8 & 300 & - & $\boldsymbol{\times}$ & $224^2$\\
\rowcolor{green!20}
\textbf{DTTN$^\dagger$-T(ours) }& \textbf{77.9} & 7.1 & 2.3 & 300 & - & $\boldsymbol{\times}$ & $224^2$\\
\rowcolor{green!20}
\textbf{DTTN$^\dagger$-S(ours) }& \textbf{79.4} & 12.3 & 4.1 & 300 & - & $\boldsymbol{\times}$ & $224^2$\\\rowcolor{green!20}
MONet-S~\citep{cheng2024multilinear} & 81.3 & 32.9 & 6.8 & 300 & - & $\boldsymbol{\times}$ & $224^2$\\\rowcolor{green!20}
\textbf{DTTN$^\dagger$-L(ours) }& \textbf{82.4} & 35.9 & 12.3 & 300 & - & $\boldsymbol{\times}$ & $224^2$\\
\bottomrule
\end{tabular}}
\end{table}

\section{EXPERIMENTS}
In this section, we provide a comprehensive assessment of the DTTN's effectiveness. Specifically, in Section 4.1, we perform experiments on a series of image classification benchmarks to validate the model's superiority over other multilinear and TN architectures. In Section 4.2, we demonstrate the broader impact of DTTN's feature interactions across two downstream tasks. Section 4.3 presents ablation studies to further dissect the design choices. We conclude with a discussion of the model's limitations.
\subsection{Image Classification Benchmarks}
\noindent\textbf{Setup and Training Details}. A series of benchmarks with different types, scales, and resolutions is employed for the experiments, including CIFAR-10~\citep{krizhevsky2009learning}, Tiny ImageNet~\citep{le2015tiny}, ImageNet-100~\citep{yan2021dynamically}, ImageNet-1k~\citep{russakovsky2015imagenet}, MNIST, and Fashion-MNIST~\citep{xiao2017fashion}. A detailed description of the benchmarks and training configurations is provided below.
\begin{itemize}
\item CIFAR-10~\citep{krizhevsky2009learning} consists of 60K color images of $32\times 32$ resolution across 10 classes, with 50K images for training and 10K for testing. The model is trained using the SGD optimizer with a batch size of 128 for 160 epochs. The MultiStepLR strategy is applied to adjust the learning rate, and data augmentation settings are in accordance with~\citep{chrysos2023regularization}.

\item Tiny ImageNet~\citep{le2015tiny}, ImageNet-100~\citep{yan2021dynamically}, and ImageNet-1k~\citep{russakovsky2015imagenet} contain 100k, 100k, and 1.2M color-annotated training images with resolutions of $64\times 64$, $224\times 224$, $224\times 224$ pixels, respectively. These datasets are commonly used as standard benchmarks for image recognition tasks. During training, we optimized our model using a configuration consistent with ~\citep{cheng2024multilinear,tolstikhin2021mlp}\footnote{https://github.com/Allencheng97/Multilinear\_Operator\_Networks}. Specifically, we utilized the AdamW optimizer~\citep{loshchilov2017decoupled} alongside a cosine decay schedule for learning rate tuning, and applied data augmentations including label smoothing, Cut-Mix, Mix-Up, and AutoAugment. Note that we did not employ additional large-scale datasets such as JFT-300M for pre-training, nor did we use un- or semi-supervised methods to optimize our model. All experiments were conducted on 8 GeForce RTX 3090 GPUs using native PyTorch.

\item Both MNIST and Fashion-MNIST~\citep{xiao2017fashion} contain 60,000 grayscale images for training and 10,000 for validation, with each image sized at $28\times 28$. We conducted comparison experiments between DTTN and other TN models on these two benchmarks, employing training configurations consistent with those used for CIFAR-10.
\end{itemize}

\noindent\textbf{DTTN vs. Polynomial Networks}.
Table~\ref{tab2} reports the results of the unfoldable networks DTTN-S and $\Pi$-Nets~\citep{chrysos2021deep} on ImageNet-1k (light red areas), with neither using activation functions nor instance normalization. DTTN-S achieves Top-1 accuracies of 71.8\% and 77.2\% after 90 and 300 training epochs, respectively, significantly outperforming $\Pi$-Nets by 5.6\% and 12\%. Additionally, DTTN-S maintains a significant advantage over Hybrid $\Pi$-Nets that incorporate activation functions, thereby losing their unfolding property, with these models trailing behind our approach by almost 10\%.

\noindent\textbf{DTTN vs. Multilinear Networks}. Fig.~\ref{img2}(e) illustrates the performance of various multilinear networks trained on ImageNet-1k over different epochs, with the curve for DTTN$^\dagger$-S showing superior results compared to others. Tables~\ref{tab2} and \ref{tab3} present the Top-1 accuracies of multilinear architectures across a range of benchmarks of varying scales, including CIFAR-10, Tiny ImageNet, ImageNet-100, and ImageNet-1k. Specifically, DTTN-S achieves improvements of 0.2\%, 2.3\%, 0.5\%, and 2.4\% over the previous best models at similar scales.
Moreover, the unfoldable DTTN-S attains an impressive 77.2\% accuracy on ImageNet-1k without instance normalization. Additionally, DTTN$^\dagger$-T surpasses the prior state-of-the-art model MONet~\citep{cheng2024multilinear} by 0.9\%, while utilizing approximately 30\% fewer parameters and reducing FLOPs by 20\%.
Finally, Fig.~\ref{img4} displays the Top-1 accuracy and loss curves of the proposed model alongside other architectures—such as ResNet-50~\citep{ref31}, ViP-Small~\citep{hou2022vision} representing the MLP architecture, and MONet~\citep{cheng2024multilinear}—all trained on ImageNet-100 for 90 epochs. These comparisons demonstrate that DTTN achieves significantly faster convergence and superior performance.


\noindent\textbf{DTTN vs. Quantum-inspired TNs}.
We confirm the superiority of DTTN over other quantum-inspired tensor networks on the smaller benchmarks, MNIST and Fashion-MNIST. It should be noted that these quantum-inspired models have not yet been successfully applied to larger benchmarks such as ImageNet-100 and ImageNet-1k.
The current experiments include baselines such as MPS~\citep{stoudenmire2016supervised}, Bayesian TNS, PEPS~\citep{cheng2021supervised}, LoTeNet~\citep{selvan2020tensor}, and aResMPS~\citep{meng2023residual}. The test results for different TNs are reported in Table~\ref{tab4}, with DTTN-S consistently achieving the best results.
This success can be attributed to the higher bond dimension facilitated by the parameter-sharing property and the self-interaction capability of our model, as discussed in Section 3.3.

\noindent\textbf{DTTN vs. Advanced Architectures}.
We further demonstrate the competitive performance of DTTN against state-of-the-art architectures, marking the first application of a TN model to large-scale benchmarks.
Table~\ref{tab2} reports the Top-1 accuracy of the DTTN family compared to other models, including CNN-based, Transformer-based, Mamba-based, and MLP-based architectures tested on ImageNet-1k. The DTTN$^\dagger$-L achieves an accuracy of 82.4\% with 35.9M parameters, which is competitive and outperforms models such as DeiT-S/16~\citep{touvron2021training}, ViP-Small/14~\citep{hou2022vision}, and Vim-S~\citep{zhu2024vision}.
Additionally, the results in Table~\ref{tab3} and Fig.~\ref{img4} further illustrate that DTTN achieves the highest accuracy on small benchmarks. The ViP-Small/14~\citep{hou2022vision} with an MLP architecture and Multilinear MONet exhibit slower convergence on ImageNet-100 compared to ResNet-50, which is due to their inductive bias.
In contrast, DTTN demonstrates remarkable training efficiency and superior accuracy relative to its counterparts.

\begin{table}[t]
\begin{minipage}[t]{0.5\textwidth}
\captionof{table}{Experimental validation of various network architectures was conducted on smaller benchmarks with differing resolutions. The top-performing results are highlighted in bold. Among these, our DTTN$^\dagger$-S model achieves the best performance across all benchmarks, outperforming other polynomial and multilinear networks.}
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|ccc}
\toprule
& \textbf{CIFAR-10}  & \textbf{Tiny ImageNet} & \textbf{ImageNet-100} \\
\midrule
{Resolution} & $32^2$ & $64^2$ & $224^2$ \\
\midrule
Resnet18       & 94.4       & 61.5    & 85.6                  \\
MLP-Mixer      & 90.6       & 45.6    & 84.3                   \\
Res-MLP        & 92.3             & 58.9        & 84.8                   \\
$\Pi$-Nets       & 90.7                   & 50.2                     &  81.4                 \\
Hybrid $\Pi$-Nets & 94.4                      & 61.1                   & 85.9                   \\
PDC            & 90.9                      & 45.2    & 82.8 \\
$\mathcal{D}$-PolyNets & 94.7     & 61.8                & 86.2         \\
MONet-T        & {94.8}     & 61.5          & 87.2                \\
\midrule
DTTN$^\dagger$-S        & \textbf{95.0}    & \textbf{63.8}    & \textbf{87.7}  \\
\bottomrule
\label{tab3}
\end{tabular}}
\end{minipage}
\hspace{2mm}
\begin{minipage}[t]{0.45\textwidth}
\vspace{2mm}
\captionof{table}{
For MNIST and Fashion-MNIST, experimental validation was performed comparing DTTN-S with quantum-inspired tensor networks. The highest performances are marked in bold. Our DTTN-S architecture surpasses previous tensor networks, including aResMPS that utilizes residual connections and 'ReLU' activation.}
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|cc}
\toprule
\textbf{Model} & \textbf{MNIST}  & \textbf{Fashion-MNIST}\\
\midrule
MPS Machine        & 0.9880    & 0.8970  \\
Bayesian TN        & -   & 0.8692  \\
PEPS        & -   & 0.883  \\
LoTeNet       & 0.9822    & 0.8949  \\
aResMPS       & 0.9907    & 0.9142  \\
\midrule
DTTN-S        & \textbf{0.9930}    & \textbf{0.9236}  \\
\bottomrule
\label{tab4}
\end{tabular}}
\end{minipage}
\end{table}


\subsection{Broader Impact}

\noindent\textbf{Image Segmentation}.
We employ the Semantic FPN framework~\citep{kirillov2019panoptic}\footnote{https://github.com/CSAILVision/semantic-segmentation-pytorch} to perform semantic segmentation on the ADE20K dataset, which includes 20,000 training images and 2,000 validation images. DTTN$^\dagger$-S and DTTN$^\dagger$-L were initialized using pre-trained ImageNet-1k weights before being integrated as the backbone of the framework. Additionally, all newly added layer weights were initialized using Xavier initialization~\citep{glorot2010understanding}. 
Table~\ref{tab5} presents the outcomes of the DTTN model trained for 12 epochs using the AdamW optimizer, evaluated by Mean Intersection over Union (mIoU). Some experimental data are derived from~\citep{cheng2024multilinear}. Notably, DTTN$^\dagger$-L achieves the highest performance among all multilinear networks, underscoring the efficacy of our simply designed AIM.

\noindent\textbf{Feature Interactions in Recommendations}.
To further illustrate the advantage of AIM in enhancing feature interactions, we selected two recommendation-related datasets, Criteo and Avazu, given the importance of feature interactions in recommender systems~\citep{lian2018xdeepfm}. AIM was incorporated as a pluggable module to enhance existing Click-Through Rate (CTR) modeling capabilities, including those of DeepFM, FiBiNet~\citep{huang2019fibinet}, and DCN-V2~\citep{wang2021dcn}\footnote{https://github.com/shenweichen/DeepCTR-Torch}.
In this implementation, AIM replaced all linear layers in the target models; internal convolution operations were removed, leaving only linear and normalized layers. The experimental validation results, reported in Table~\ref{tab6} and evaluated by AUC (Area Under the ROC Curve), show that AIM consistently improves performance across both datasets for all CTR models. Specifically, FiBiNet achieved a 0.55\% improvement on the Criteo dataset.

\begin{table}[t]
\vspace{-5mm}
\begin{minipage}[t]{0.4\textwidth}
\includegraphics[width=\linewidth]{./img/44.png} 
\captionof{figure}{Top-1 accuracy and loss visualization for different architectures trained from scratch on ImageNet-100. DTTN$^\dagger$-S shows better performance and convergence} % 为图片添
\label{img4}
\end{minipage}
% \hfill
\begin{minipage}[t]{0.23\textwidth}
\vspace{-48mm}
\captionof{table}{Experimental validation of semantic segmentation on top of ADK20K with Semantic FPN.}
    \centering
    \resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|c}
\toprule
\textbf{BackBone} & \textbf{mIoU}(\%)  \\
\midrule
Resnet18 & 32.9\\
FCN & 29.3\\
Seg-Former & 37.4\\
\midrule
R-PDC & 20.7\\
$\mathcal{R}$-PolyNets & 19.9\\
MONet-S & 37.5\\ 
\midrule
DTTN$^\dagger$-S  & 36.9\\
DTTN$^\dagger$-L  & \textbf{38.6}\\
\bottomrule
\end{tabular}
\label{tab5}}
\end{minipage}
\begin{minipage}[t]{0.37\textwidth}
\vspace{-48mm}
\vspace{2mm}
\captionof{table}{Validation of AIM as a pluggable module to enhance the feature interaction capability of existing CTR models.}
    \centering
    \resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|cc}
\toprule
\textbf{Model} & Criteo & Avazu \\
\midrule
DeepFM & 80.12 & 75.46\\
DeepFM+AIM & 80.44$_{+0.32}$ & 75.73$_{+0.27}$\\
\midrule
FiBiNet & 80.42 & 76.01\\
FiBiNet+AIM & 80.97$_{+0.55}$ & 76.08$_{+0.07}$\\
\midrule
DCN-V2 & 80.93 & 76.14\\
DCN-V2+AIM & 81.15$_{+0.22}$ & 76.52$_{+0.38}$\\
\bottomrule
\end{tabular}}
\label{tab6}
\end{minipage}
\end{table}

\subsection{Ablation Study}
Here, we conduct ablation studies to evaluate the effectiveness of various design choices, including network depth and width, the antisymmetric design of AIM, and layer normalization. We utilize ImageNet-100 as our test benchmark, a representative subset of ImageNet-1K suitable for model validation and hyperparameter tuning within resource constraints. Throughout these experiments, all hyperparameters were kept consistent with those of the final model, except for the variable under investigation, aligning with the configuration detailed in Section 4.1.

\noindent\textbf{Depth and Width}.
To control the depth and width of DTTN, we vary the number of AIM blocks and the hidden-size, denoted by $L$ and $d$, respectively, for brevity. For ease of comparison, $d$ was set constant across different stages. As illustrated in Table~\ref{tab7}, we first set $L$ to $\{8, 16, 24, 32\}$ with $d=256$, and subsequently varied $d$ to $\{64, 128, 256, 512\}$ while fixing $L$ at 32. This approach allows us to explore the impact of network depth and width on model performance.
Our observations indicate that model performance sharply degrades with decreases in both $L$ and $d$, particularly with respect to width; Top-1 accuracy drops by 24.5\% when $d$ decreases from 512 to 64. Furthermore, model parameters exhibit linear growth with $L$ and quadratic growth with $d$. These findings indirectly validate the efficacy of the multi-stage design of DTTN.


\noindent\textbf{Antisymmetrical Design of AIM}.
We evaluate the effectiveness of the antisymmetric design of AIM on the DTTN$^\dagger$-T and DTTN$^\dagger$-S architectures. As shown in Table~\ref{tab8}, we compare the impact of symmetric versus antisymmetric designs on model performance. In this context, SIM-Conv and SIM-Linear denote the Symmetric Interaction Module (SIM) with convolutional- and linear prioritized layers, respectively, used as replacements for AIM.
It is observed that DTTN$^\dagger$-T achieves the best results among its counterparts, while DTTN$^\dagger$-S performs only 0.1\% lower than SIM-Conv but has nearly 20\% fewer parameters. Furthermore, the reduction in the number of parameters aligns with the theoretical predictions corresponding to Equation~\ref{eq3}.

\noindent\textbf{Importance of LN}.
Figures \ref{img2}(d) and \ref{img3} demonstrate the critical role of the Layer Normalization (LN) layer in facilitating the unfoldable nature and expressiveness of the network—a key technique for enhancing the performance of multilinear networks~\citep{chrysos2023regularization,cheng2024multilinear}. Table~\ref{tab9} reports the improvements in DTTN variants after incorporating LN, showing gains of 1.8\%, 0.4\%, and 0.5\%, respectively.
Moreover, by combining the data from Figure \ref{img2}(e) and Table~\ref{tab2}, it can be seen that DTTN-S exhibits a Top-1 accuracy that is 5.6\% and 2.2\% lower than DTTN$^\dagger$-S after training for 90 and 300 epochs, respectively. This further underscores the importance of LN in boosting the performance and convergence of our proposed architecture.

\begin{table}[t]
\vspace{-6mm}
\begin{minipage}[t]{0.35\textwidth}
\captionof{table}{The influence of network depth and width on model performance. All models are trained from scratch on ImageNet-100.}
    \centering
    \resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|cc}
\toprule
\textbf{} & Top-1 Acc(\%) & Params(M)\\
\midrule
$L$=8, $d$=256  & 79.2 & 5.6\\
$L$=16,$d$=256  & 85.5 &10.2\\
$L$=24,$d$=256  & 86.8  &14.8\\
$L$=32,$d$=256  & \textbf{87.2}  &19.4\\
\midrule
$L$=32,$d$=64  & 63.4 &1.3\\
$L$=32,$d$=128  & 82.5 &4.9\\
$L$=32,$d$=512  & \textbf{87.9 } &76.8\\
\bottomrule
\end{tabular}}
\label{tab7}
\end{minipage}
\begin{minipage}[t]{0.32\textwidth}
\vspace{-0mm}
\captionof{table}{The influence of different design choices for AIM on the performance of the DTTN variants. All models are trained from scratch on ImageNet-100.}
    \centering
    \resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|cc}
\toprule
\textbf{} & Top-1 Acc(\%) & Params(M)\\
\midrule
SIM-Conv  & 86.2 & 9.1\\
SIM-Linear  & 84.9  & 4.8\\
DTTN$^\dagger$-T   & \textbf{86.4} & 6.9\\
\midrule
Sim-Conv  & 87.8 &15.9\\
Sim-Linear  & 85.2 & 8.3\\
DTTN$^\dagger$-S   & {87.7 }& 12.1\\
\bottomrule
\end{tabular}}
\label{tab8}
\end{minipage}
\begin{minipage}[t]{0.32\textwidth}
\vspace{1mm}
\vspace{-4mm}
\captionof{table}{The influence of layer normalization inside AIM on the performance of the DTTN variants. All models are trained from scratch on ImageNet-100.}
    \centering
    \resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|cc}
\toprule
\textbf{Model} & Top-1 Acc(\%) & Params(M)\\
\midrule
DTTN-T & 85.6 &6.9\\
DTTN$^\dagger$-T  & \textbf{86.4}$_{+1.8}$&6.9\\
\midrule
DTTN-S & 87.3 &12.1\\
DTTN$^\dagger$-S  & \textbf{87.7}$_{+0.4}$ &12.1\\
\midrule
DTTN-L & 87.6 & 35.6\\
DTTN$^\dagger$-L  & \textbf{88.1}$_{+0.5}$ & 35.6\\
\bottomrule
\end{tabular}}
\label{tab9}
\end{minipage}
\end{table}

\noindent\textbf{Limitations}.
Although we have conducted an extensive study of the effectiveness and advantages of DTTN, limitations in computational resources and time constraints prevented us from performing additional experiments. These include pre-training on larger datasets such as JFT-300M, integrating physics-informed networks with AIM, and assessing model robustness.
Future work will involve theoretical analysis of the proposed model and the exploration of multilinear transformer architectures that achieve linear complexity. We believe this novel architecture has significant potential for societal benefit through enhanced effectiveness, interpretability, and energy efficiency.

\section{CONCLUSION}
This paper introduces a multilinear network architecture named DTTN, which bridges the gap between quantum-inspired TN models and advanced architectures, uniquely achieving this without activation functions. Specifically, DTTN employs a modular stacking design to capture exponential interactions among input features, essentially unfolding into a tree-structured TN. We conducted extensive experiments to showcase the effectiveness of DTTNs, including comparisons with polynomial and multilinear architectures, modern neural networks, and TNs across various recognition benchmarks. Notably, this is the first validation of TNs' effectiveness on large-scale benchmarks, yielding competitive results compared to advanced architectures. Additionally, we explored the broader applicability of DTTNs in segmentation tasks and as a plug-and-play module within recommendation systems. In summary, DTTNs leverage exponential feature interactions to achieve superior performance across multiple domains, entirely without the need for activation functions or attention mechanisms.

% \verb

\bibliographystyle{unsrtnat}
\bibliography{references}
\newpage
\section*{APPENDIX}
\renewcommand{\thesubsection}{\Alph{subsection}}

\subsection{Implementation of AIM}
In Algorithm~\ref{alg1:aim}, we provide a PyTorch-style implementation of AIM. The primary distinction between the training and inference stages is structural re-parameterization, specifically, the integration of the Batch Normalization (BN) layers with the adjacent convolutional or linear layers. During inference, the propagation process of AIM simplifies, aligning well with the schematic overview depicted in Fig.~\ref{img2}(d).

\begin{algorithm}
\caption{Code for AIM (PyTorch-like)}
\label{alg1:aim}
\begin{lstlisting}
# H: height, W: width, C: channel, R: expansion ratio
# x: input tensor of shape (B, C, H, W)
############## Initialization ##############
proj_l = nn.Linear(C, C * R) # Aggregate channel information
conv_l = nn.Conv2d(C * R, C * R, kernel_size=3, groups=C * R) # Aggregate spatial information
proj_r = nn.Linear(C * R, C * R) # Aggregate channel information
conv_r = nn.Conv2d(C, C * R, kernel_size=3, groups=C) # Aggregate spatial information
proj = nn.Conv2d(C * R, C, kernel_size=1) # For information fusion
ln_norm = nn.LayerNorm([C * R]) # Layer normalization
l_norm = nn.BatchNorm2d(C * R) # Batch normalization
r_norm = nn.BatchNorm2d(C * R) # Batch normalization
res_norm = nn.BatchNorm2d(C) # Batch normalization
scale = nn.Parameter(torch.ones(1))
############## Training stage##############
def AIM(x, use_ln):
    x_l = conv_l(proj_l(x.permute(0, 2, 3, 1).permute(0, 3, 1, 2)))
    x_r = proj_r(conv_r(x).permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
    if use_ln:
        out = ln_norm(x_l * x_r)
    else:
        out = l_norm(x_l) * r_norm(x_r)
    out = res_norm(proj(out))
    return x + scale * out
############## Inference stage  ##############
def AIM(x, use_ln):
    x_l = conv_l(proj_l(x.permute(0, 2, 3, 1).permute(0, 3, 1, 2)))
    x_r = proj_r(conv_r(x).permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
    out = x_l * x_r
    if use_ln:
        out = ln_norm()
    return x + proj(out)
\end{lstlisting}
\end{algorithm}


% \begin{figure}[h]
% \centering
% \includegraphics[width=.99\textwidth]{./img/wait.png}
% \end{figure}

\subsection{Complexity Analysis}
Here we analyze the complexity of DTTN including storage and computation. Without loss of generality, we consider the multi-stage DTTN with hidden-size $d$ for each stage, and the total depth is $L$.
We then consider the complexity of the main components of the network, including the local mapping, the core blocks, and the classification head separately.

In this section, we analyze the complexity of a DTTN, focusing on both storage requirements and computational cost. Without loss of generality, we consider a multi-stage DTTN where each stage has a hidden size of $d$, and the overall depth is $L$. We then examine the complexity associated with the main components of the network: the local mapping function, the core blocks, and the classification head.



\noindent\textbf{Local mapping}.
The local mapping function $\phi(\cdot)$ consists of two consecutive convolutional layers, each with a kernel size and stride of 2.
This function is applied to the input image $\boldsymbol{x}$ to produce a feature map $\phi(\boldsymbol{\boldsymbol{x},\boldsymbol{\Lambda}_\phi})\in\mathbb{R}^{W\times H\times C}$. Here, $\boldsymbol{\Lambda}_\phi\in\mathbb{R}^{S^2\times C}$ represents a learnable matrix, where $S$,$C=d\in\mathbb{N}$ denote the local patch size and the number of output channels, respectively.
The parameters and floating-point operations (FLOPs) involved in this process are given by:
\begin{equation}\begin{split}
Params^L&=\mathcal{O}(14\cdot d+4\cdot d^2),\\
FLOPs^L&=\mathcal{O}(48\cdot d\cdot WH+4\cdot d^2\cdot  WH).
\label{eq7}
\end{split}\end{equation}



\noindent\textbf{Core blocks}.
we analyze the complexity associated with the core AIM blocks within the DTTN architecture. Assuming that each of the four stages contains an equal number of blocks, the corresponding feature map sizes are $W\times H$,$\frac{W}{2}\times \frac{H}{2}$, $\frac{W}{4}\times \frac{H}{4}$, and $\frac{W}{8}\times \frac{H}{8}$, respectively.
Our analysis focuses on the parameters and FLOPs involved in the three linear layers and two convolutional layers of AIM.
The total parameters and FLOPs can be expressed as follows:
\begin{equation}\begin{split}
Params^{AIM}&=\mathcal{O}\left ((\sum_{s=0}^3 22\cdot r_{exp}\cdot d + (2\cdot r_{exp}+ r_{exp}^2)\cdot d^2 + d)\cdot \frac{L}{4} \right )\\
&=\mathcal{O}\left (( 22\cdot r_{exp} + 2\cdot(r_{exp}+ r_{exp}^2)\cdot d + 1)\cdot d\cdot L\right ),\\
FLOPs^{AIM}&=\mathcal{O}\left ((\sum_{s=0}^3 18\cdot r_{exp}\cdot d\cdot \frac{WH}{4^s} + (2\cdot r_{exp}+ r_{exp}^2)\cdot d^2 \cdot \frac{WH}{4^s} )\cdot \frac{L}{4}\right )\\
&=\mathcal{O}\left ((\frac{765}{128}\cdot r_{exp} + \frac{85}{128}\cdot(r_{exp}+ r_{exp}^2)\cdot d)\cdot d\cdot WH\cdot L \right ).
\label{eq8}
\end{split}\end{equation}


\noindent\textbf{Classification head}.
For a classification head with $m$ classes, we receive a feature map of size $\frac{W}{8}\times \frac{H}{8}\times d$, which is then processed through an average pooling layer followed by a fully-connected layer to output an $m$-dimensional vector. The parameters and FLOPs involved in this process are given by:
\begin{equation}\begin{split}
Params^{H}&=\mathcal{O}(d\cdot (m+1)),\\
FLOPs^{H}&=\mathcal{O}(\frac{WH}{64}\cdot d+m\cdot d).
\label{eq9}
\end{split}\end{equation}
In summary, our conclusions regarding the number of parameters and FLOPs for the DTTN architecture can be expressed as:
\begin{equation}\begin{split}
Params&=Params^L+Params^{AIM}+Params^{H},\\
FLOPs&=FLOPs^L+FLOPs^{AIM}+FLOPs^{H}.
\label{eq10}
\end{split}\end{equation}
It can be observed that the number of parameters in the DTTN architecture remains fixed, while the computational complexity exhibits a linear relationship with the input image scale. This characteristic represents a significant advantage of the DTTN over modern MLP and Transformer architectures, which typically exhibit quadratic complexity.


\subsection{Hyperparameters for Training on ImageNet-1k}
Table~\ref{tab10} shows the experimental hyperparameters used for training the DTTN family on the ImageNet-1k benchmark. We utilize the timm library\footnote{https://github.com/huggingface/pytorch-image-models} and ensure all settings are aligned with the comparison method MONet~\citep{cheng2024multilinear}.
\begin{table}[h]
\centering
\caption{Training settings for ImageNet-1k in Section 4.1}
\begin{tabular}{@{}cc@{}}
\toprule
& \textbf{Setting} \\ \midrule
Optimizer          & AdamW              \\
Base learning rate     & 1e-3            \\
Warmup-lr     & 1e-6            \\
Learning rate schedule & cosine \\
Weight Decay       & 0.01            \\
Batch size      &  $320\times4$ GPU           \\
Label smoothing       & 0.1         \\
Auto augmentation      & \checkmark         \\
Random erase      & 0.1         \\
Cutmix      & 0.5         \\
Mixup      & 0.5         \\
Dropout   & 0.0          \\
\bottomrule
\end{tabular}
\label{tab10}
\end{table}

\subsection{Proofs}
Here, we derive the proofs of Proposition 1 and Theorem 1 from the main paper and further elucidate their significance.

\noindent\textbf{Proof of Proposition 1}.
\begin{proposition}
The DTTN has the capability to capture $2^L$ multiplicative interactions among input elements, which can be represented in the format of Equation~\ref{eq1} as $\Phi(\boldsymbol{x})=\otimes^{2^L}\phi(\boldsymbol{x},\boldsymbol{\Lambda}_\phi)$.
Consequently, the elements of $f(\boldsymbol{x})$ are homogeneous polynomials of degree $2^L$ over the feature map $\phi(\boldsymbol{x},\boldsymbol{\Lambda}_\phi)$.
\end{proposition}
\begin{proof}
For each AIM block with input $\boldsymbol{x}^{l}=vec(\boldsymbol{\mathcal{X}}^l)\in\mathbb{R}^{W_l H_l C_l}$, let $D^l=W_l\times H_l\times C_l$ and $l \in \mathbb{K}_{L}$. Suppose that the left and right branches of AIM can be represented as $f_1^l(\boldsymbol{x}^{l})=\boldsymbol{A}_1^l\boldsymbol{x}^{l}$ and $f_2^l(\boldsymbol{x}^{l})=\boldsymbol{A}_2^l\boldsymbol{x}^{l}$, where $\boldsymbol{A}_1^l, \boldsymbol{A}_2^l\in\mathbb{R}^{D^l\times D^l}$ are obtained through structured combinations of convolutional and linear layer weights. The feedforward propagation of the AIM block can then be expressed as:
\begin{equation}\begin{split}
\boldsymbol{x}^{l+1}&=\boldsymbol{x}^{l}+\boldsymbol{B}^{l} \left((\boldsymbol{A}_1^{l}\boldsymbol{x}^{l}) * (\boldsymbol{A}_2^{l}\boldsymbol{x}^{l})\right)\\
&=\boldsymbol{x}^{l}+\mathtt{Reshape}\left(  \boldsymbol{B}^{l} (\boldsymbol{A}_1^{l\ ^T}\odot \boldsymbol{A}_2^{l\ ^T})^T\right)\times_{2,3}^{1,2}(\boldsymbol{x}^{l}\otimes\boldsymbol{x}^{l})\\
&=\boldsymbol{x}^{l}+\boldsymbol{\mathcal{Z}}^l\times_{2,3}^{1,2}(\boldsymbol{x}^{l}\otimes\boldsymbol{x}^{l}).
\label{eq11}
\end{split}\end{equation}
where $\boldsymbol{B}^{l}$ denotes the fused linear layer inside AIM, and $\boldsymbol{\mathcal{Z}}^l=\mathtt{Reshape}\left(  \boldsymbol{B}^{l} (\boldsymbol{A}_1^{l\ ^T}\odot \boldsymbol{A}_2^{l\ ^T})^T\right)\in\mathbb{R}^{D^l\times D^l\times D^l}$ is a structured learnable tensor.
Then each element of $\boldsymbol{x}^{l+1}$ can be calculated by
\begin{equation}\begin{split}
\boldsymbol{x}^{l+1}_\tau&=\boldsymbol{x}^{l}_\tau+\sum_{w}^{D^l}\sum_{\rho}^{D^l}\boldsymbol{\mathcal{Z}}^l_{(w,\rho,\tau)}\boldsymbol{x}^{l}_w\boldsymbol{x}^{l}_\rho.
\label{eq12}
\end{split}\end{equation}
Thus, each AIM module captures second-order multiplicative feature interactions of the input. By induction, the DTTN stacked with $L$ AIMs captures $2^L$ interactions of the input $\boldsymbol{x}$.
Note that the network's bias terms and shortcut connections can be eliminated by introducing an additional homogeneous dimension in the local mapping.
Hence, we have $\boldsymbol{x}^{l+1}_\tau=\sum_{w}^{D^l+1}\sum_{\rho}^{D^l+1}\boldsymbol{\mathcal{Z}}^{*l}_{(w,\rho,\tau)}\boldsymbol{x}^{l}_w\boldsymbol{x}^{l}_\rho\in\mathbb{R}^{D_l+1}.$
Therefore, the expression $f(\boldsymbol{x})$ is a homogeneous polynomial of degree $2^L$ of $\phi(\boldsymbol{x},\boldsymbol{\Lambda}_\phi)$, which concludes our proof.
\end{proof}

\noindent\textbf{Proof of Theorem 1}.
\begin{theorem}
Given the local mapping function $\phi^{i_1}(x_1)=[x_1^0,\cdots,x_1^{2^L}]^T$, a polynomial network with the expansion form of Equation~\ref{eq6}) can be transformed into a quantum-inspired TN model with finite bond dimension.
\end{theorem}
\begin{proof}
For an input image $x\in\mathbb{R}^{W\times H\times C}$, the computation in the vanilla quantum-inspired TN model can be expressed as
\begin{equation}\begin{split}
f(\boldsymbol{x})=TN\left (\{\phi^{i}(\boldsymbol{x}_{(\tau,\rho,w)}) \}_{i=1}^{N}, \{\boldsymbol{\mathcal{R}}_i\}_{i=1}^{N}\right ),
\label{eq13}
\end{split}\end{equation}
where $N=WHC$,$\tau\in\mathbb{K}_{W}$,$\rho\in\mathbb{K}_{H}$,$w\in\mathbb{K}_{C}$, and $\{\mathcal{R_i}\}_{i=1}^{N}\}$ denotes the tensor cores.
$TN(\cdot):\mathbb{R}^{\underbrace{(2^L+1)\cdots (2^L+1)}_{N}}\rightarrow \mathbb{R}^{m}$ represents the contraction operation that outputs an $m$-dimensional vector.
We can further complete the contraction of $2^L$ physical indices and transform Equation~\ref{eq13} into
\begin{equation}\begin{split}
f(\boldsymbol{x})&=TN\left ( \phi^{1}(\boldsymbol{x}_{(\tau,\rho,w)})\times_{1}^1 \boldsymbol{\mathcal{R}}_1,\cdots,  \phi^{N}(\boldsymbol{x}_{(\tau,\rho,w)})\times_{1}^1 \boldsymbol{\mathcal{R}}_N \right )\\
&=TN(\boldsymbol{\mathcal{Z}}_1,\cdots,\boldsymbol{\mathcal{Z}}_N).
\label{eq14}
\end{split}\end{equation}
Since each output element of a polynomial network can be expressed as
\begin{equation}\begin{split}
 \sum_{i_1}^N\cdots \sum_{i_{2^L}}^N w_{\xi}  x_{i_1}x_{i_2}\cdots x_{i_{2^L}}, \ \ \xi\leq N^{2^L}.
\label{eq15}
\end{split}\end{equation}
The above equation is equivalent to
\begin{equation}\begin{split}
 \sum_{\xi}w_{\xi}x_1^{k_1}\cdots x_N^{k_N} \ \ \ \ \ 
 where\ \ k_i \geq 0 \ \ and \ \ \ k_1+\cdots + k_N=2^L
\label{eq16}
\end{split}\end{equation}
Since Equation~\ref{eq14} encompasses each term of Equation~\ref{eq16}, this proves our claim.
\end{proof}

The above proof indicates that by selecting an appropriate local mapping, the DTTN can be transformed into a standard tensor network model. We re-emphasize that this is the first work demonstrating that tensor networks can achieve competitive performance on large-scale benchmarks, such as attaining 77.2\% Top-1 accuracy on ImageNet-1k. Previous TNs have been limited to much smaller tasks due to expression limitations and lower bond dimensions.
We hope that this research will inspire further explorations into tensor networks.

% importance

\end{document}
