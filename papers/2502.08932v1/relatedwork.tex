\section{Related Work}
Prior neurosymbolic surveys \cite{agiollo2023measuring} define how neurosymbolic AI fits into definitions of trustworthiness with overlapping themes of assurance, including oversight, cross-user performance, and robustness. While there have been a plethora of claims on the trustworthiness of neurosymbolic systems \cite{wagner2024neurosymbolic}, there is a lack of a full empirical exploration of their assurance across multiple axes. Prior work has examined the promises of robust solutions in the symbolic space \cite{marconato2024not} and found that neurosymbolic systems would routinely find shortcuts in reasoning. Other works \cite{sitawarinpart} have examined the adversarial robustness of systems with deep learning models that contain intermediate symbolic representations, finding that the intermediate representation followed by another neural network was more adversarially robust. Prior work \cite{rafanelli2024empirical} has also examined the robustness of Symbolic Knowledge Injection methods to common perturbations in the input and output space for tabular data. Neurosymbolic systems have been explored for safety modeling \cite{yang2022safe} by differentiating through neurosymbolic systems for verifiable safety and assurance of worst-case scenarios. While many of these papers either mention or look at one dimension of assurance, a recent survey \cite{michel2024neuro} points out a similar lack of work outside measuring interpretability compared to other measurements of assurance. Closely related to this investigation into end-to-end neurosymbolic programming, recent work \cite{paul2024formal} examines how to expand Scallop to provide explanations towards interpretability of the answers provided by neurosymbolic methods.