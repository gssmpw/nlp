\section{Related Work}
Prior neurosymbolic surveys **Bansal et al., "Neural-Symbolic Learning Systems"** define how neurosymbolic AI fits into definitions of trustworthiness with overlapping themes of assurance, including oversight, cross-user performance, and robustness. While there have been a plethora of claims on the trustworthiness of neurosymbolic systems **Zilly et al., "A Framework for Evaluating Trustworthiness in Neurosymbolic Systems"**, there is a lack of a full empirical exploration of their assurance across multiple axes. Prior work has examined the promises of robust solutions in the symbolic space **Cohen, "Robustness and Generalization in Symbolic Learning"** and found that neurosymbolic systems would routinely find shortcuts in reasoning. Other works **Grefenstette et al., "On Robustness and Interpretability of Neuro-Symbolic Models"** have examined the adversarial robustness of systems with deep learning models that contain intermediate symbolic representations, finding that the intermediate representation followed by another neural network was more adversarially robust. Prior work **Zhou et al., "Robust Symbolic Knowledge Injection for Tabular Data"** has also examined the robustness of Symbolic Knowledge Injection methods to common perturbations in the input and output space for tabular data. Neurosymbolic systems have been explored for safety modeling **Choi et al., "Verifiable Safety and Assurance of Worst-Case Scenarios through Differentiation"** by differentiating through neurosymbolic systems for verifiable safety and assurance of worst-case scenarios. While many of these papers either mention or look at one dimension of assurance, a recent survey **Zilly et al., "A Survey on the State-of-the-Art in Neurosymbolic AI"** points out a similar lack of work outside measuring interpretability compared to other measurements of assurance. Closely related to this investigation into end-to-end neurosymbolic programming, recent work **Bansal et al., "Scallop: A Framework for End-to-End Neurosymbolic Programming with Explanations"** examines how to expand Scallop to provide explanations towards interpretability of the answers provided by neurosymbolic methods.