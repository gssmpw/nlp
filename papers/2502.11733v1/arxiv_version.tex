\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[final]{acl}


\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}  %

\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO: #1}}}



\title{\textit{Plant in Cupboard, Orange on Table, Book on Shelf}\\ Benchmarking Practical Reasoning and Situation Modelling in a\\ Text-Simulated Situated Environment}


\author{
 \textbf{Jonathan Jordan\textsuperscript{1}},
 \textbf{Sherzod Hakimov\textsuperscript{1}},
 \textbf{David Schlangen\textsuperscript{1,2}}
\\
\\
 \textsuperscript{1}Computational Linguistics, Department of Linguistics\\University of Potsdam, Germany\\
 \textsuperscript{2}German Research Center for Artificial Intelligence (DFKI), Berlin, Germany
\\
\texttt{firstname.lastname@uni-potsdam.de} 
}

\begin{document}
\maketitle



\begin{abstract}
Large language models (LLMs) have risen to prominence as ``chatbots'' for users to interact via natural language. However, their abilities to capture common-sense knowledge make them seem promising as language-based planners of situated or embodied action as well.
We have implemented a simple text-based environment --- similar to others that have before been used for reinforcement-learning of agents --- that simulates, very abstractly, a household setting. We use this environment and the detailed error-tracking capabilities we implemented for targeted benchmarking of LLMs on the problem of practical reasoning: Going from goals and observations to actions. 
Our findings show that environmental complexity and game restrictions hamper performance, and concise action planning is demanding for current LLMs.
\end{abstract}

\section{Introduction}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/agent_graphic.pdf}
    \caption{Sample representation of an episode in our game. The agent \protect\includegraphics[width=0.023\textwidth]{figures/robot.png} (an LLM) is given a task and randomly assigned to some room (living room). The environment provides feedback for every action (go, take, put, etc.) of an agent. The top part is how the game is played in a textual world. The bottom part is the visual representation of initial and final state, also indicating the situation knowledge gained by the agent.}
    \label{fig:intro-ex}
\end{figure}


Theoretical reasoning, which involves deriving factual conclusions from given premises, has been extensively studied in the context of large language models (LLMs)~\cite{DBLP:conf/nips/BrownMRSKDNSSAA20,DBLP:conf/nips/Wei0SBIXCLZ22,DBLP:conf/nips/KojimaGRMI22}. However, there has been less work focusing on practical reasoning capabilities \cite{sep-practical-reason}, where models must generate actions based on environmental observations. While theoretical reasoning can be evaluated using reference propositions, practical reasoning presents unique challenges because the evaluation requires observing the consequences of actions in an environment. There is significant interest in using LLMs for embodied AI tasks in robotics and simulations.~\cite{DBLP:conf/corl/IchterBCFHHHIIJ22}. Many of the existing benchmarks (e.g., WebArena~\cite{zhou2024webarenarealisticwebenvironment}, ALFRED~\cite{alfworld}, AI2-THOR~\cite{kolve2022ai2thorinteractive3denvironment}, Balrog Arena~\cite{paglieri2024balrog}) include complex environments or require computational resources to execute vision-related tasks. 

In contrast, text-based environments such as TextWorld~\citep{textworld}, where a situation is described textually and updated in response to textual commands, make it possible to study practical reasoning abilities that go from goal and situation description to goal-directed action in greater isolation. We built an even more simplified environment (called \textit{AdventureGame}) for testing situated language environment understanding, spatial navigation, practical and common-sense reasoning, and instruction following abilities.

Figure~\ref{fig:intro-ex} (top)
shows an example of an interaction, while the bottom part is the visual representation of what has changed going from an initial state to the final one.

This interaction can be thought of as between a robot's ``brain'' and its perceptual and actuating parts. Our contributions are as follows: 
\begin{itemize}[itemsep=0pt, parsep=0pt]
    \item A light-weight and extensible IF environment that includes in-depth fine-grained tracking of the world state, player observations and interaction failures.
    \item A set of experimental instances condensing an established object delivery task in a typical home setting, with varied levels of complexity and task demands of a situated interaction.
    \item Quantitative comparison of recent large language models (LLM) of varying sizes.
    \item In-depth analysis of contributing factors for high-performing models.
    \item Qualitative analysis of model behaviour, examining good interaction and common failure cases.
\end{itemize}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/Q32b_b-e_ep8_example.pdf}
    \caption{Simplified illustration of \textit{AdventureGame} interaction. The agent \protect\includegraphics[width=0.023\textwidth]{figures/robot.png} is controlled by an LLM. The task is \textit{put the plate, book and pillow on the table} (marked by green crosshair). The agent starts in the pantry. Unexplored rooms are gray. The agent first goes to the kitchen, \textit{takes the plate from the counter} (Turn 1-2), then \textit{goes to the living room, takes the book} (Turn 3-4). Next, it \textit{goes to the hallway}, and from there \textit{to the bedroom where it picks up the pillow} (Turn 5-7), then it \textit{returns to the living room}, \textit{puts the carried objects on the table} (Turn 8-13).}
    \label{fig:simple-illu}
\end{figure*}

\section{Related Work}
Interactive Fiction environments have been used to \textit{train and compare agents}, starting in the field of Reinforcement Learning, using classic IF formats and interpreters (\citet{textworld}, \citet{hausknecht2020interactive}). These environments combine a specific set of agent challenges: Partial observability, large world state and action space, exploration and common sense reasoning, in addition to the text-only interaction. It has been shown that performance in text-only IF environments is transferable to embodied environments (\citet{alfworld}, \citet{jansen2021systematic}). However, the frameworks established with these works have convoluted code dependencies that hinder reproduction.

With the rise of transformer LLMs \cite{NIPS2017_3f5ee243}, IF environments have also been used to \textit{compare LLM performance} in regards to their world modelling, task solving and planning capabilities (\citet{wang2022scienceworld}, \citet{tan2023text}, \citet{tsai2023can}, \citet{ma2024agentboard}, \citet{gioacchini2024agentquest}). These works find that direct interaction with IF environments is challenging for LLMs, although they have a substantial advantage regarding common sense world knowledge compared to RL agents. Few works, however, %
have provided fine-grained success and failure metrics, hindering interpretation.

Recently, IF-based environments have been used to compare agent systems that incorporate LLMs to leverage their common sense world knowledge and reasoning capabilities (\citet{wang2022scienceworld}, \citet{basavatia2024starlingselfsupervisedtrainingtextbased}, i.a.). Theses agent systems involve extensive augmentation around LLMs, with Retrieval Augmented Generation, multi-stage prompting and external planners and verifiers to guide the LLM (\citet{park2023generative}, \citet{llm2023planninginvestigation}, \citet{tikhonov2024plugh}, \citet{jansen2024discoveryworldvirtualenvironmentdeveloping}, \citet{li2025embodiedagentinterfacebenchmarking}, i.a.). A number of works have LLM agents generate entire solution plans based on fully observable world states without direct interaction, which makes automated evaluation easier but foregoes examining incremental world modelling and exploration (\citet{xie2023translating}, \citet{silver2022pddl}, i.a.).

\textit{AdventureGame} is filling a gap in assessing the performance of unassisted LLMs, tapping into claimed capabilities with minimal guidance. \textit{AdventureGame} provides observations of action outcomes to the LLM directly each turn, combining planning and execution.

\section{\textit{AdventureGame}: IF Environment}

In this section, we describe the details of the game. A simplified illustration is given in Figure~\ref{fig:simple-illu}, where an LLM controls an agent to perform the given task. We implemented the game using the clembench framework~\cite{clembench2023} (prompts are provided in Appendix~\ref{sec:initial-prompts}).


\subsection{Task Definition}
Interactive Fiction games can be considered partially observable Markov Decision processes~\cite{KAELBLING199899}, with the player having only partial information about the game's world state from textual observations. The player must first discover all task-relevant locations and objects, making exploration as much a part of solution strategies as effectively executing a plan for manipulating objects to reach the game's goals. To do so successfully, the player has to model environment states (like room connections and locations of objects), as well as the large action space and state transitions resulting from actions, especially over multiple turns.

The task is defined by the tuple $\langle G,S,A,O,T\rangle$ with a set of goal states $G$, state space $S$, valid action space $A$, observation space $O$ (including IF interpreter feedback), and transition function $T : S\times A \rightarrow S$.

An agent with policy $\pi$ makes predictions at time step $t$ based on goals in $G$ and memory $m_t = \{o_j, a_j, o_{j+1}, a_{j+1}, . . . o_t\}, 0 \leq j < t$, which is a sequence of actions and observations. This agent trajectory $\tau = [s_0, a_0, s_1, a_1, . . . s_t]$ is formulated by policy and environmental state transitions as below where a time step $t$ is one turn of the game:
$$p_\pi(\tau) = p(s_0) \prod^{T}_{t=0}\pi(a_t|G, s_t, m_t)\tau (s_{t+1}|s_t, a_t)$$

All state transitions are deterministic, so only the actions generated by the LLM as text commands determine the agent's trajectory. Observations are likewise deterministic. We assume that the language model models an underlying policy, tapping into the capabilities under examination.

\subsection{Interaction}\label{subsec:interaction}
World state, representing a temporary subset of $S$, is stored as a set of fact tuples, describing both mutable states of entities and immutable states. Mutable states are \texttt{at}, \texttt{in}, \texttt{on}, \texttt{open} and \texttt{closed}. Immutable states describe entity types and categories and are used as conditions for actions (see Table~\ref{tab:actions}).

Actions are defined in the PDDL~\cite{fox2003pddl2} format, covering the state changes they enact (representing $T$). The action definition also contains a \citet{lark} grammar snippet that is used to form the parse-able input command grammar, feedback templates for success and individual failures (covering $O$) and a Clingo~\cite{GebserKKS2017clingo} encoding snippet of the state changes of the action for optimal solution solving (representing $T$ as well).

The player is not given a list of currently available actions but rather has to model the action space itself. Movement is only allowed to connected rooms (unlike in \citet{basavatia2024starlingselfsupervisedtrainingtextbased} and others, which allow ``teleportation'').


\textbf{Turn Limit}: The interaction is limited to fifty turns, and reaching the limit is recorded as aborting that episode.

\textbf{Formatting}: An output produced by LLM has to follow this format (current command followed by the next commands):
\begin{verbatim}
    > "COMMAND"
    Next actions: "COMMANDS"
\end{verbatim}

The basic task type requires only the first command line, while the planning task requires both outputs (see Section~\ref{subsec:game-instances}).


\textbf{Parsing \& State Change}: The interpreter attempts to parse the command, and if it passes, the corresponding action is performed in the resolution phase. State change conditions are checked, and any resolution failure stops the process. If state change conditions are fulfilled, the game world state is updated accordingly, removing and adding facts in the world state. All changes in the world state are recorded. Lastly, the interpreter checks if changed states are part of the goal state set $G$ and returns achieved goal states $G_a$, textual feedback $o\in O$ and any failure that might have occurred in processing the input command to be recorded.

For the planning variant, this procedure is performed subsequently for each input command in the ``Next actions:'' list (corresponding to $\tau_P$) until a failure is encountered. World state history is used to revert any state changes resulting from a planned action sequence. 

An episode is ended by the player using the 'done' action or reaching the turn limit. Once the episode ends, goal states achieved as of the last turn are recorded - meaning that goal states achieved intermittently are not considered for metrics.

\textbf{Exploration Tracking}: All commands, state changes, observed locations and objects are recorded for each turn, including errors while executing the commands. We label each action (inspired by \citet{KIRSH1994513}) for being \textit{epistemic} and \textit{pragmatic}. An action is \textit{epistemic} when it improves a player's perception of the game situation without directly progressing towards the goal\footnote{Kirsh and Maglio's example is moving a Tetris tile to the leftmost position to be sure about its position instead of moving it towards the location that is most beneficial to put it down in.} and \textit{pragmatic} when the action directly works towards reaching the goal. Then, we calculate \textit{epistemic gain} by counting how many world state facts the player newly observes as a result.

\subsection{Challenges \& Examined Capabilities}
The task includes various challenges and their accompanying capabilities that are targeted here, purely in text. Despite the environment being abstract compared to physically embodied agents, these capabilities are required for autonomous performance regardless of the modality.

\noindent\textbf{World state modelling}: The agent must correctly recall objects' states and locations over multiple turns under partial observability by consecutively integrating the observations provided by the game.

\noindent\textbf{Situated action selection}: Another essential capability is selecting appropriate actions in a specific situation from many options, especially without any guidance to limit this action space.

\noindent\textbf{Common sense reasoning \& world knowledge}: The agent should perform action selection, and locating objects in their ordinary locations requires world knowledge and reasoning.

\noindent\textbf{Spatial reasoning \& exploration}: Navigation relies on spatial modelling of the game environment, especially for going through multiple locations. Sufficient exploration of the environment is necessary to correctly fulfil the task, which entails a certain level of trial and error, with actions that are not immediately useful to further the task goal.

\noindent\textbf{Self-correction}: The capability to revise and correct an assumed world model in the face of new observations is crucial to any situated task solving. An agent should be capable of verifying the correctness of its world model through its interactions with the world and actively do so.

\section{Experimental Setup}

\subsection{Game Instances}\label{subsec:game-instances}

Different instances of the game are generated using the Clingo Answer Set Programming library\footnote{\url{https://potassco.org/clingo/python-api/current/clingo/}}, which wraps the Clingo solver \cite{GebserKKS2017clingo}. We create challenging instances by varying three factors for the experiments, as given below. 

\textbf{Task variants}: we have two variants of the task: basic \& planning. The core task in our experiments is a \textit{home delivery task} in which the agent is expected to deliver three objects to target receptacles in a common home environment. For the \textit{planning} variant, models are prompted also to list the next actions. Our hypothesis is that the \textit{planning} variant is more challenging because it requires knowing more about the environment, and being able to adapt across turns. We want to analyse whether models have any strategy or if they simply invent arbitrary plans.

\textbf{Delivery difficulty}: it has two levels: easy \& hard. The \textit{easy level} means that goal objects are not inside closed containers (easily accessible, immediately observable) and are located near the target receptacle in the initial world state. The \textit{hard level} means that goal objects are initially hidden in closed containers, each needs to be delivered to a different target, and there are longer paths between initial goal object locations and their targets. Our hypothesis is that it is challenging for models to navigate multi-step actions to reach a goal because hidden objects require epistemic actions, far away objects require keeping the objective in mind over several steps (increases the number of steps to reach goals).

\textbf{Inventory limit}:  by default, the number of objects the agent can carry is unlimited. We create another level by setting the limit to \textit{two}. We want to analyse whether models lean towards strategic use of the resource ``inventory'' and being efficient, when they have limit.

We have eight experiments (permutation of task types, object difficulties, inventory limit) (16 instances in each), corresponding to 128 instances.

\subsection{Metrics}
\label{sec:metrics}

\textbf{Framework-specific metrics}: The clembench framework includes two main metrics: \textit{Played} \& \textit{Quality Score}. The game finishes successfully only when a model produces ``> done'' as the last action and all goals have been achieved. The game is \textit{aborted} when a model does not follow formatting requirements (see Section~\ref{subsec:interaction}) or reaches the \textit{maximum turn limit}, which is 50. Played is the ratio of instances that were not aborted. The quality score measures how many episodes have all their goal states reached at the end. Producing the ``> done'' action command without achieving all goal states is considered a \textit{lost} episode. In cases where all goal states are achieved and ``> done'' is also generated, then the episode is \textit{successful}.

Finally, to rank the benchmarked models, the framework includes a metric called \textit{clemscore}, the macro-average quality score multiplied by the macro-average proportion of played games across all experiments.

\textbf{Game-specific metrics}:
We have a specific metric to keep track of achieved goals. It is the ratio between achieved goal states $G_a$ and all goal states $G$: Goal Success Rate (GSR) $ = \frac{|G_a|}{|G|}\times 100$.

For the planning variant of the task, we have the plan \textit{viability} metric. It is calculated as the fraction of successfully processed plan actions $a^+_{t}$ over the number of total plan actions $a_t$ in turn $t$: $v_t = \frac{|a^+_{t}|}{|a_t|}$. Then, we calculate the average plan viability in an episode as the average of each turn (except the first turn): $viability = \frac{\sum_{t=1}^{T} v_t}{T}$.

\subsection{Evaluated Models}
We evaluated open-weight and commercial models (with \textit{temp=0}). We included recent commercial models such as: \textit{o3-mini} (Jan~'25), \textit{GPT-4o} (Aug~'24) \textit{Claude-3-5} (Sonnet, Oct~'24), and \textit{Gemini-2.0-Flash} (Feb~'25). We also included recent open-weight models: \textit{Llama-3.1} (8B, 70B, 405B)~\citep{llama31}, \textit{Llama-3.3} (70B), \textit{Qwen2} (72B)~\citep{qwen2}, \textit{Qwen2.5} (Coder-32B, 72B, Max)~\citep{qwen25}, \textit{Sky-T1-32B}~\citep{sky_t1_2025} and \textit{Deepseek-v3}~\citep{deepseekv3}. We used the APIs of the respective commercial models. We ran open-weight models on two NVIDIA A100 GPUs. Deepseek-v3, Llama-3.1-405B, and Qwen-Max were run via the OpenRouter API.

\section{Results}

\subsection{Overall Comparison}
Table \ref{tab:clemscores} shows the main scores for the benchmarked models. Larger models achieve higher quality scores and better conform to the prompted output format. Most commercial models achieve higher scores than open-weight models (11 points between \textit{Claude-3.5} and \textit{Llama-3.1-70B}) except \textit{Gemini-2.0}, which scores lower on both \textit{Played} and \textit{Quality Score}. Another observation is that all high-ranking models can play the game (follow instructions) but lack performance in solving the task. Next, we analyse the cases deeper to uncover which factors contribute to low scores.

\begin{table}
\footnotesize
    \centering
    \input{adventuregame_overview_table}
    \caption{Overall benchmark scores for models.}
    \label{tab:clemscores}
\end{table}


\subsection{In-depth Analysis}

In this section, we analyse different factors that contributed to certain models' better performance than others and investigate others.

\subsubsection{Difficulty Levels}

We compared five high-ranking models across all experiments (see Table~\ref{tab:top_models_extended}). For smaller models, both higher complexity and limited inventory lead to worse performance, with the effects of both compounding. Specifically for the \textit{home delivery} task, \textit{Claude-3.5} and \textit{o3-mini} perform equally for the \textit{easy} and \textit{hard} levels. At the same time, other models tend to struggle more with the \textit{hard} episodes. One assumption on better performance on \textit{hard} level (goal objects are hidden inside containers) is that the more complex tasks inherently require more exploration, and multiple target receptacles are less semantically similar to others. Adding an inventory limit (max two objects) resulted in mixed outcomes. Some models got better results (\textit{o3-mini} on \textit{hard}, \textit{GPT-4o} on \textit{easy}), but the general trend is that adding a limit on the inventory was more challenging.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/episode_overview_stack.pdf}
    \caption{Successful, lost and aborted episode ratios.}
    \label{fig:episodes-stack}
\end{figure}

\subsubsection{Action Planning}

The \textit{planning} variants of experiments are found to be more challenging than the \textit{home delivery} one, especially for smaller models (see Table~\ref{tab:top_models_extended}). The models not only have to generate the immediate subsequent action but also the following actions. For instance, models generate generic plans such as ``find apple and table'' (where the goal statement is ``Put the apple on the table'') in the early turns because, at those stages, the environment has not been fully explored. Such plans are not parsable and result in lower plan viability. In the later turns, the plan viability scores go higher a bit (e.g., \textit{Claude-3.5}) but still fluctuate within a certain margin (see Figure~\ref{fig:plan-viability}). Overall, models struggled with this task because they moved to a more abstract level of planning (``find kitchen'') due to the lack of world information in the initial phases.




\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/aborts_distribution.pdf}
    \caption{Distribution of causes for aborted episodes.}
    \label{fig:aborts-distribution}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/exploration_epistemic_paired_vertical.pdf}
    \caption{Average observed goal objects ratio and average cumulative effective epistemic gain over all episodes.}
    \label{fig:exploration-epistemic}
\end{figure}

\subsubsection{Successful, Lose \& Aborted Episodes}

Figure~\ref{fig:episodes-stack} shows the distribution of episodes across Success (all goal states are reached), Lose (if any goal state is not reached), or Aborted (formatting was not followed through or turn limit reached). The best performing two models \textit{Claude-3.5} and \textit{o3-mini}, solve most of the episodes (>=60\%) and have lower numbers of lost episodes. The main contributing factor for other models is that they either lose episodes or have them aborted due to violating instruction formatting requirements. Figure~\ref{fig:aborts-distribution} shows the distribution of causes for aborted episodes. The most common issue for \textit{Claude-3.5} and \textit{o3-mini} is that they missed the tag ``>'' before an action statement. Some of the other models struggle with finishing the game by producing ``> done'' and reaching the maximum turn limit (50), or \textit{Llama3.1-405B} did not generate the ``Next actions'' for the planning experiment.
Overall, we can conclude that the main factor for the aborted cases is \textit{models not knowing when to stop the game}, i.e., failing to recognise that the current situation is a goal state.

\begin{figure*}[ht!]
    \begin{minipage}{0.50\textwidth}
    \includegraphics[width=\linewidth]{figures/claude_navigation_3.pdf}
    \caption{\textit{Claude-3.5} correcting navigation.}
    \label{fig:claude-navigation}
    \includegraphics[width=\linewidth]{figures/o3_planning.pdf}
    \caption{\textit{o3-mini} planning nine turns ahead.}
    \label{fig:o3-planning}
    \end{minipage}%
    \begin{minipage}{0.50\textwidth}
    \includegraphics[width=\linewidth]{figures/Qwen32_exploration_1.pdf}
    \caption{\textit{Qwen2.5-32}B exploring insufficiently.}
    \label{fig:qwen32-exploration}
    \includegraphics[width=\linewidth]{figures/Llama3.3_relabeling_1.pdf}
    \caption{\textit{Llama3.3-70B} hallucinating.}
    \label{fig:llama-hallucination-reasoning}
\end{minipage}
\end{figure*}

\subsubsection{Exploration}
All tested models have  issues with sufficiently exploring the game environment: Rooms containing task-relevant objects are not entered, and containers containing task-relevant objects are not opened. Figure \ref{fig:exploration-epistemic} (top) shows the average ratio of \textit{observed goal entities} per turn. We can see that high-performing models such as \textit{o3-mini} and \textit{Claude-3.5} explore more and discover more required objects compared to smaller models such as \textit{Llama-3.1-8B} or \textit{Qwen2.5C-32B}. For smaller models, we assume this is due to a lack of spatial modelling and the inability to handle long input contexts. In comparison, larger models show a general aversion to entering other rooms, especially if they have encountered objects that are semantically similar to task objects. Figure \ref{fig:exploration-epistemic} (bottom) shows average \textit{epistemic gain} per turn. Like the earlier observation, higher-performing models make more effective \textit{epistemic} actions. Thus, we can confirm that such exploration behaviour of models results in higher scores in both discovering other rooms and relevant objects in them.

\subsection{Qualitative Analysis}
\subsubsection{Navigation}
All models make navigation errors. These errors occur despite the observation feedback mentioning passages to all connected rooms every time the player enters a room. While lower-performing models repeat this type of failure many times in individual episodes, better-performing models acquire the game's rule that allows movement only to connected rooms and self-correct their navigation after receiving feedback. This type of failure most often occurs in the turn after a task object has been picked up or delivered.

Figure \ref{fig:claude-navigation} illustrates this: \textit{Claude3.5} attempts to "> go to hallway" from the unconnected bedroom (red arrow) and is told that this is not possible. It then respects the connection requirement for the rest of the episode and goes to the broom closet to take the mop and further to the pantry with the freezer without attempting to go to these known rooms directly.
\subsubsection{Planning \& Self-correction}
Abstract ``next actions'' aid better-performing models in adequately exploring the environment and navigating by keeping room layout and task requirements in recent context. Once sufficient information about the game situation is acquired, leading models produce extended, fully executable plans demonstrating good planning ability.

Figure \ref{fig:o3-planning} illustrates \textit{o3-mini} making the longest viable plan: After finding the pillow and placing it on the first observed side table in the living room, it enters the kitchen and observes the remaining task objects. Next, it puts the plate on the table. It correctly plans nine turns ahead, self-correcting placement of pillow and book: Moving between the living room and kitchen twice, taking a task object each time and placing it on the table, preventing inventory limit infractions, and finishing the episode.
\subsubsection{Exploration}
Figure \ref{fig:qwen32-exploration} illustrates insufficient exploration: \textit{Qwen2.5-Coder-32B} regularly checks connected rooms, the hallway in this case, which largely contributes to its performance. However, it does not do this thoroughly enough, missing the table in the pantry and incorrectly placing the task objects on the side table, as most models do.
\subsubsection{Hallucination}
Hallucinations are common, with all models attempting to interact with objects that are not mentioned to be present and equating semantically similar objects. These hallucinations often persist over entire episodes once established, regardless of later observations contradicting them. A more concerning type of hallucination occurs in larger models that reason in their plans: The models reason (in often somewhat incomprehensible fashion) about the identity of objects, coming to wrong conclusions which they then follow, disregarding observations.

Figure~\ref{fig:llama-hallucination-reasoning} illustrates this with \textit{Llama3.3-70B}: After sufficiently exploring, \textit{finding and examining the shelf while carrying the book}, the model goes to the kitchen and puts the book \textit{on the first observed counter}. It then produces post-hoc reasoning for this action, contradicting the task, providing a haphazard interpretation, using incoherent 'evidence' and asserting that the task is fulfilled.


\section{Discussion}
Compared to prior works and classic IF games, the presented environment and task are deliberately straightforward. While we do not provide human baseline data for our experiments, we are sure that this environment is trivial for humans to interact with, especially with IF familiarity. Despite the tested models showing this familiarity by producing classic IF actions with minimal prompting, they struggle with even the simple underlying common rules of this game, suggesting that if they emulate human capabilities at work when playing \textit{AdventureGame}, they do so only in rudimentary fashion.

Behaviour differs not only between models but also between episodes with the same model, suggesting an impact of the textual surface form of the interaction. This is likely an effect of model architecture and foundation training paradigms relying solely on next-token prediction. More sophisticated behaviour hinting at underlying capabilities emerges with larger models, with training data amount and content playing the most prominent role. Structured training data like code and reinforcement learning towards aligning with instructions are reflected in model behaviour, as can be seen with \textit{Qwen2.5-Coder-32B} strictly following the order of delivery given in the initial prompt.

Specifically, the highest-scoring models show pragmatic reasoning capabilities and can sufficiently model situations to infer the best course of action over long-turn sequences. The unexpectedly surfaced reasoning in ``Next actions'' shows that models can often communicate intermediate task demands and requirements, suggesting abstraction on multiple levels. However, we find that current LLMs' reasoning can be detrimental to embodied interaction, as the trained theoretical reasoning requires no grounding and can lead to models disregarding actual observations that should instead ground model behaviour. 
Relying solely on language without external grounding may be detrimental to physical or otherwise embodied agents. Recently popular embedded LLM agents aim to mitigate this, but they fall short in 'grounding' the interaction by feeding language tokens into transformer LLMs instead of extending inputs to actual embodied feedback data.
Certain failures might result from multi-turn conversation training and natural language feedback: Models break format to explain things to the 'user' in response (e.g., "I don't know how to interpret this 'look' action.") and attempts to move directly to a known room might be based on seemingly established conversational common ground. Thus, good emulation of conversational pragmatics can lead to worse performance.

\section{Conclusion}
\textit{AdventureGame} performance increases with model size, progressing from generally bad situation modelling in smaller models to a middle ground of good situation modelling but frequent interaction failures, to only a pair of SOTA models fulfilling the given task in more than two thirds of cases.

Our experiments reveal the practical reasoning capabilities of current LLMs, with their leverage of common-sense world knowledge supporting embodied interaction in a text-based home environment. Tracking of the game world state interaction, agent observations and fine-grained failures show that while the required capabilities emerge in current LLMs, there are individual strengths and weaknesses. Reasoning more strongly grounded in embodied feedback may improve model interaction in the future. We look forward to building upon the current \textit{AdventureGame} experiments with future examinations of in-context learning and pragmatic language capabilities.

\section*{Limitations}

The current study is restricted to only English in its current state. While we have yet to do this, translating the prompts and adapting the underlying grammar entries is possible for other languages, too.

The performance we measured here may not transfer to other modalities with more sophisticated demands, like visually or physically embodied agents or robots. \citet{alfworld} found that while training in text-only environments is faster and less resource-intensive than training in the AI2Thor framework, agents trained in text-only environments struggled to adapt to the requirements of more complex embodiment properly.

\section*{Ethics Statement}

In academic research, using paid proprietary APIs with underlying models about which little is known (training data, model architecture) is less than ideal. Currently, the models benchmarked here are the high-performing ones that are commercially used. We hope that more open models with high performance will be released soon and that proper research can be done on them.

\bibliography{custom}

\section{Result details}
Figure \ref{fig:action-phases-stack} shows percentages of failures by processing phase. Claude3.5 and Llama-3.1-405B produce more successful actions than the higher-scoring o3-mini.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/action_overview_stack.pdf}
    \caption{Successful and failed action quotas by IF interpreter processing phase.}
    \label{fig:action-phases-stack}
\end{figure}

Figure \ref{fig:entity-fail-stack} shows percentages of entity-related failures. o3-mini and GPT-4o do not have inventory limit failures, while Qwen2.5-32b and Qwen2.5-72b have low amounts.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/entity_failures_stack.pdf}
    \caption{Percentages of selected entity-related failures for all tested models.}
    \label{fig:entity-fail-stack}
\end{figure}

Figure \ref{fig:plan-viability} shows fluctuating average plan viability for a selected set of models.
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/comp_models_plan_viability_all.pdf}
    \caption{Average plan viability over turns over all episodes.}
    \label{fig:plan-viability}
\end{figure*}

Table \ref{tab:top_models_extended} shows individual experiment performance for selected well-performing models.
\begin{table*}
    \centering
    \footnotesize
    \input{top_model_experiment_stats}
    \caption{Scores by variant experiment for selected well-performing models. \% Lose is the percentage of episodes that were finished without fulfilling all three task goals. \textit{basic} refers to the \textit{home delivery task}.}
    \label{tab:top_models_extended}
\end{table*}

\section{Actions}
Table \ref{tab:actions} lists the actions defined for the experiments.
\begin{table*}
    \centering
    \begin{tabular}{lllll}
         Action & Targets & Description & Epistemic & Pragmatic\\
         \hline
         \texttt{open}& 'container' entities & Changes state of \texttt{closed} & Yes & Yes\\
         & & container entity to \texttt{open} & & \\
         \hline
         \texttt{close}& 'container' entities & Changes state of \texttt{open}& No & Yes\\
         & & container entity to \texttt{closed} & & \\
         \hline
         \texttt{take}& 'takeable' entities & Removes \texttt{in/on} state for & No & Yes\\
         &&'takeable' entity and adds\\
         &&\texttt{in(entity,inventory)} fact\\
         \hline
         \texttt{put}&'takeable'\& & Removes \texttt{in(entity,inventory)} & No & Yes\\
         &'container'/'support'& state for 'takeable' entity and\\
         & entities & adds \texttt{in/on(entity,target)} fact\\
        \hline
         \texttt{go}& 'room' & Changes \texttt{at} state of player entity & Yes & Yes\\
         &&and all entities in inventory\\
         && to target room \\
         \hline
         \texttt{done}& - & Ends the episode & No & Yes\\
         \hline
         \texttt{examine}& entities & Results in entity state feedback & Yes & No
    \end{tabular}
    \caption{Action types used in AdentureGame. Targets are those for which the world state holds a fact assigning the listed state.}
    \label{tab:actions}
\end{table*}

\section{Initial Prompts} \label{sec:initial-prompts}
Prompt template for 'basic' variant instances:

\begin{verbatim}
You are playing a text adventure game. I 
will describe what you can perceive in 
the game. You write the single action you 
want to take in the game starting with >. 
Only reply with an action.
For example:
> open cupboard

Your goal for this game is: $GOAL$

Once you have achieved your goal, write 
"> done" to end the game.

$INITIAL ROOM DESCRIPTION$
\end{verbatim}

\noindent Prompt template for 'basic' with limited inventory variant instances:

\begin{verbatim}
You are playing a text adventure game. I 
will describe what you can perceive in 
the game. You write the single action you 
want to take in the game starting with >. 
Only reply with an action.
For example:
> open cupboard

You can have up to two objects in your 
inventory at the same time.

Your goal for this game is: $GOAL$

Once you have achieved your goal, write 
"> done" to end the game.

$INITIAL ROOM DESCRIPTION$
\end{verbatim}

\noindent Prompt template for 'planning' variant instances:

\begin{verbatim}
You are playing a text adventure game. I 
will describe what you can perceive in 
the game. You write the single action you 
want to take in the game starting with >. 
Write your plan for your next actions on 
the line below your action, starting with 
"Next actions:".
For example:
> open cupboard
Next actions: take orange, eat orange

Your goal for this game is: $GOAL$

Once you have achieved your goal, write 
"> done" to end the game.

$INITIAL ROOM DESCRIPTION$
\end{verbatim}

\noindent Prompt template for 'planning' with limited inventory variant instances:

\begin{verbatim}
You are playing a text adventure game. I 
will describe what you can perceive in 
the game. You write the single action you 
want to take in the game starting with >. 
Write your plan for your next actions on 
the line below your action, starting with 
"Next actions:".
For example:
> open cupboard
Next actions: take orange, eat orange

You can have up to two objects in your 
inventory at the same time.

Your goal for this game is: $GOAL$

Once you have achieved your goal, write 
"> done" to end the game.

$INITIAL ROOM DESCRIPTION$
\end{verbatim}

\noindent The placeholder \texttt{\$GOAL\$} in the templates is replaced with the task goal.

Example 'easy' difficulty task goal: \texttt{Put the pillow on the table, the book on the table and the plate on the table.}

Example 'hard' difficulty task goal: \texttt{Put the pillow on the counter, the book on the shelf and the plate on the table.}

A description of the room the player is in initially is inserted at the end of the template, at the location of 
\texttt{\$INITIAL ROOM DESCRIPTION\$}.


\section{Environment Graphs}
To illustrate differences between 'easy'/'hard' environment and task complexity, Figures \ref{fig:easy-instance-graph} and \ref{fig:hard-instance-graph} show graph representations of initial game world states and task targets. House-shaped nodes are rooms, with arrow edges showing bidirectional connections between them. Round nodes are 'movable' entities, connected to rectangular receptacles and rooms by edges labelled with their prepositional state. Dashed edges connect the movable task objects to their target receptacles and are labelled with the target prepositional state.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/graph_easy_in0.pdf}
    \caption{Graph representation of an 'easy' instance.}
    \label{fig:easy-instance-graph}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/graph_hard_in0.pdf}
    \caption{Graph representation of a 'hard' instance.}
    \label{fig:hard-instance-graph}
\end{figure*}

\end{document}
