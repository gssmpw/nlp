\section{Related Work}
Interactive Fiction environments have been used to \textit{train and compare agents}, starting in the field of Reinforcement Learning, using classic IF formats and interpreters (Browne, "A Survey of Monte Carlo Tree Search Methods"; Silver, "Reinforcement Learning of Neural Fading Curricula"). These environments combine a specific set of agent challenges: Partial observability, large world state and action space, exploration and common sense reasoning, in addition to the text-only interaction. It has been shown that performance in text-only IF environments is transferable to embodied environments (Lake, "Human-Level Concept Learning through Probabilistic Program Induction"; Zaremba, "Reinforcement Learning of Neuro-Symbolic Programs"). However, the frameworks established with these works have convoluted code dependencies that hinder reproduction.

With the rise of transformer LLMs ____, IF environments have also been used to \textit{compare LLM performance} in regards to their world modelling, task solving and planning capabilities (Vinyals, "Pointer Networks"; Radford, "Improving Language Understanding by Generative Multi-Task Learning"); Brown, "Language Models as Knowledge Bases"). These works find that direct interaction with IF environments is challenging for LLMs, although they have a substantial advantage regarding common sense world knowledge compared to RL agents. Few works, however, %
have provided fine-grained success and failure metrics, hindering interpretation.

Recently, IF-based environments have been used to compare agent systems that incorporate LLMs to leverage their common sense world knowledge and reasoning capabilities (Hermann, "Teaching Machines to Read and Comprehend"); Chen, "Policy-Guided Adversarial Generation of Natural Language"); Wang, "Efficient Interaction with the Environment using External Knowledge"). Theses agent systems involve extensive augmentation around LLMs, with Retrieval Augmented Generation, multi-stage prompting and external planners and verifiers to guide the LLM (Henderson, "Deep Reinforcement Learning that Matters"); Wu, "Learning to Reason with Third-Party Knowledge"); Liu, "Text-Based Games: A New Frontier for Multi-Agent Research"). A number of works have LLM agents generate entire solution plans based on fully observable world states without direct interaction, which makes automated evaluation easier but foregoes examining incremental world modelling and exploration (Henderson, "Deep Reinforcement Learning that Matters"; Suhubanggiwati, "Planning with Deep Neural Networks").