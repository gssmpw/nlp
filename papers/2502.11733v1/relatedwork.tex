\section{Related Work}
Interactive Fiction environments have been used to \textit{train and compare agents}, starting in the field of Reinforcement Learning, using classic IF formats and interpreters (\citet{textworld}, \citet{hausknecht2020interactive}). These environments combine a specific set of agent challenges: Partial observability, large world state and action space, exploration and common sense reasoning, in addition to the text-only interaction. It has been shown that performance in text-only IF environments is transferable to embodied environments (\citet{alfworld}, \citet{jansen2021systematic}). However, the frameworks established with these works have convoluted code dependencies that hinder reproduction.

With the rise of transformer LLMs \cite{NIPS2017_3f5ee243}, IF environments have also been used to \textit{compare LLM performance} in regards to their world modelling, task solving and planning capabilities (\citet{wang2022scienceworld}, \citet{tan2023text}, \citet{tsai2023can}, \citet{ma2024agentboard}, \citet{gioacchini2024agentquest}). These works find that direct interaction with IF environments is challenging for LLMs, although they have a substantial advantage regarding common sense world knowledge compared to RL agents. Few works, however, %
have provided fine-grained success and failure metrics, hindering interpretation.

Recently, IF-based environments have been used to compare agent systems that incorporate LLMs to leverage their common sense world knowledge and reasoning capabilities (\citet{wang2022scienceworld}, \citet{basavatia2024starlingselfsupervisedtrainingtextbased}, i.a.). Theses agent systems involve extensive augmentation around LLMs, with Retrieval Augmented Generation, multi-stage prompting and external planners and verifiers to guide the LLM (\citet{park2023generative}, \citet{llm2023planninginvestigation}, \citet{tikhonov2024plugh}, \citet{jansen2024discoveryworldvirtualenvironmentdeveloping}, \citet{li2025embodiedagentinterfacebenchmarking}, i.a.). A number of works have LLM agents generate entire solution plans based on fully observable world states without direct interaction, which makes automated evaluation easier but foregoes examining incremental world modelling and exploration (\citet{xie2023translating}, \citet{silver2022pddl}, i.a.).

\textit{AdventureGame} is filling a gap in assessing the performance of unassisted LLMs, tapping into claimed capabilities with minimal guidance. \textit{AdventureGame} provides observations of action outcomes to the LLM directly each turn, combining planning and execution.