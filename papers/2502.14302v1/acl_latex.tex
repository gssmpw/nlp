% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[twocolumn]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{pdfpages}
\usepackage{latexsym}
\usepackage{array}
\usepackage{lipsum}  
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{enumitem}
% \usepackage[margin=1in]{geometry}
\usepackage{array}
\usepackage{longtable}
\usepackage{natbib} 
\usepackage{standalone}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{array}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{colortbl}  % For row colors
\usepackage{array}     % For better table formatting
\usepackage{amsmath}
\usepackage{booktabs}  % For better table formatting
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\SetCommentSty{textit}  % Make comments italic
\SetVlineSkip{0.5em}   % Adjust vertical line spacing
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{tabularx}   % For tables with adjustable-width columns
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{verbatim} % or use the listings package if preferred
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{fontawesome}
% \usepackage{bbding}
% \usepackage{tcolorbox}
% \usepackage{xcolor}
% \usepackage{fontawesome}

% Optional: If you want to customize the algorithm appearance
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\setlength{\parskip}{0.5em} % Or whatever value you prefer
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Shrey Pandit\textsuperscript{1}\thanks{Corresponding author: \href{mailto:shreypandit@utexas.edu}{shreypandit@utexas.edu}}},
 \textbf{Jiawei Xu\textsuperscript{1}},
 \textbf{Junyuan Hong\textsuperscript{1}},\\
 \textbf{Zhangyang Wang\textsuperscript{1}},
 \textbf{Tianlong Chen\textsuperscript{2}},
 \textbf{Kaidi Xu\textsuperscript{3}},
 \textbf{Ying Ding\textsuperscript{1}}
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
\\
\faGlobe~Dataset \& Code: \url{https://medhallu.github.io/} \\
 \textsuperscript{1}University of Texas at Austin,
 \textsuperscript{2}UNC Chapel Hill,
 \textsuperscript{3}Drexel University,
 % \textsuperscript{4}Affiliation 4,
 % \textsuperscript{5}Affiliation 5
% \\
 % \small{
 %      Data and Code: \href{https://medhallu.github.io/}{https://medhallu.github.io/}
 % }
}


\begin{document}
\maketitle
\begin{abstract}
Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce \textbf{MedHallu}, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting ``hard'' category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a ``not sure'' category as one of the answer categories improves the precision and F1 scores by up to 38\% relative to baselines. 
% The MedHallu benchmark is publicly accessible at [LINK].
\end{abstract}

\section{Introduction}
Recent advances in Large Language Models (LLMs)~\citep{achiam2023gpt} have catalyzed their widespread adoption as assistive tools across a multitude of domains, including software development~\citep{Krishna_2024_software}, healthcare ~\citep{singhal2022largelanguagemodelsencode_health}, weather prediction~\citep{li2024cllmatemultimodalllmweather}, and financial applications \citep{nie2024surveylargelanguagemodels}. However, LLMs are prone to hallucination~\citep{bang2023multitaskmultilingualmultimodalevaluation_hallucination}, where they generate plausible but factually incorrect or unverifiable information~\citep{Ji_2023_12RW, Huang_2025_survey11}. Hallucinations can arise from various factors, including biased or insufficient training data~\citep{han2024skipnsimplemethod_data, zhang2024knowledgeovershadowingcausesamalgamated_data}, and inherent architectural limitations of LLMs \citep{leng2023mitigatingobjecthallucinationslarge_architecture, kalai2024calibratedlanguagemodelshallucinate_architecture}. This issue is particularly problematic in high-stakes fields such as the medical domains, where the generation of incorrect information can exacerbate health disparities~\citep{singhal2022largelanguagemodelsencode_health}. 

 % Efforts to address hallucination involve implementing strategies such as Retrieval-Augmented Generation \citep{Semnani_2023_RAG_Hallu}, self-refinement techniques \citep{huang2022largelanguagemodelsselfimprove} \citep{wang2023selfconsistencyimproveschainthought}, and rigorous fine-tuning on factual datasets. These approaches aim to enhance the reliability of AI outputs in healthcare settings. Nonetheless, challenges remain in ensuring the scalability and generalizability of these strategies, highlighting the need for ongoing research and development in the field.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/figure_1.pdf}
\caption{An example of medical hallucination detection. The detailed prompt used for the hallucination detection task is presented in Appendix \ref{appendix:prompt}.}
\label{fig:task}
\end{figure}

Detecting hallucinations in LLM outputs (Figure~\ref{fig:task}) is therefore of critical importance. Various methods have been proposed to address this issue, including self-consistency~\citep{wang2023selfconsistencyimproveschainthought}, sampling-based approaches such as SelfCheckGPTZero \citep{manakul2023selfcheckgptzeroresourceblackboxhallucination_detect1}, and intrinsic methods that evaluate token-level uncertainty and entropy~\citep{azaria2023internalstatellmknows_detect2, xiao2021hallucinationpredictiveuncertaintyconditional_detect3}.
Existing benchmarks, such as HaluEval~\citep{Hallueval} and Haydes~\citep{liu2022tokenlevelreferencefreehallucinationdetection} primarily evaluate hallucination detection capabilities on general tasks, including summarization, question answering, and dialogue systems, with an emphasis on common-sense knowledge rather than domain specificity. 
% \kaidi{the following paragraph is very important, please use \ding{202} \ding{203} \ding{204} to oragnize them}  
This gap becomes particularly consequential in the medical domains, where specialized terminology requires precise handling, as minor lexical deviations can lead to substantially divergent interpretations~\citep{singhal2022largelanguagemodelsencode_health}. While recent efforts such as HaluBench~\citep{ravi2024lynxopensourcehallucination}, incorporate limited samples from the medical domains, their domain-agnostic generation frameworks lack medical curation. Similarly, Med-Halt~\citep{pal2023medhaltmedicaldomainhallucination} focuses on model benchmarking rather than providing a structured evaluation resource. Furthermore, the subtlety of hallucinations (e.g., whether they are hard or easy to detect) remains underexplored in the medical context. Additionally, the performance differences between pre-trained LLMs and fine-tuned medical LLMs are sparsely documented~\cite{ravi2024lynxopensourcehallucination, Hallueval, pal2023medhaltmedicaldomainhallucination}.

To address these gaps, we present the \textbf{Med}ical \textbf{Hallu}cination detection dataset (\textbf{MedHallu}), a comprehensive corpus of 10,000 medical question-answer pairs derived from the established PubMedQA dataset. Each pair is meticulously annotated to distinguish accurate responses from hallucinated content. Furthermore, MedHallu is stratified into easy, medium, and hard detection tiers based on the subtlety of hallucinations, enabling granular evaluation of model capabilities. The primary contributions of this research are threefold:
 \begin{itemize}
 \vspace{-3mm}
     \item We introduce MedHallu, one of the first datasets specifically designed for medical hallucination detection tasks. Comprising 10,000 entries derived from PubMedQA, MedHallu is systematically categorized into three levels of difficulty—easy, medium, and hard—based on the subtlety of hallucination detection.

     \item We find that hallucinated answers that are semantically closer to the ground truth are more challenging to detect. Furthermore, clustered answers using bi-directional entailment reveal uniformity, where all entries in a cluster are consistently either easy or hard to detect.

    \item Our evaluation shows that general-purpose LLMs outperform fine-tuned medical LLMs in medical hallucination detection tasks. Additionally, we find that model performance can be enhanced by providing relevant knowledge to LLMs. Moreover, introducing a ``not sure'' class alongside the existing classes of ``hallucinated'' and ``not-hallucinated'' leads to improved precision, which is critical in the medical domains.

 \end{itemize}

% \shreyp{WRITE SOMETHING ABOUT PAST WORK AND RESEARCH GAP}
% We further analyze the similarity between the hallucinated and ground truth samples by clustering the hallucinated answers using the bidirectional entailment method~\citep{Farquhar2024DetectingHI_semantic_entropy}. We find that hallucinated points that are harder to distinguish are semantically closer to the ground truth in metrics such as Euclidean distance, higher cosine similarity score, and high Rouge-2 score than those that are easy to detect, giving insights on LLMs discriminatory behavior, pointing to that it is the semantic meaning of a sentence that is resulting in difficult detection rather than syntactic behavior. 

% Previous works such as HaluBench \citep{ravi2024lynxopensourcehallucination}, HaluEval \cite{Hallueval}, and Med-HALT \citep{pal2023medhaltmedicaldomainhallucination} have primarily focused on examining general-purpose LLMs for their ability to detect hallucinations. These studies have not examined how domain-specific Large Language Models perform in these tasks. To address this gap, we assess fine-tuned medical LLMs, mainly Med42-8B \citep{med42v2}, Meditron3-8B, OpenBioLLM-8B \citep{OpenBioLLMs} and UltraMedical-8B \citep{zhang2024ultramedical} in the context of medical hallucination detection task as described in figure \ref{fig:task}, and discover that their performance is inferior to that of their instruction-tuned counterparts. Our methodology for dataset creation is detailed in Section \ref{Methodology}.


 
\begin{figure*}[ht]
\centering
\includegraphics[width=1\linewidth]{figures/figure_2.pdf}
\caption{\textbf{MedHallu} medical hallucinated answer generation pipeline. Each question-answer pair from the PubMedQA dataset undergoes the following steps to generate a hallucinated answer: (1) \textbf{Candidate Generation}: Given a question, relevant knowledge, and ground truth answer, the LLM is prompted to generate a hallucinated answer adhering to one of four hallucination types. (2) \textbf{Grading \& Filtering}: Generated answers undergo \textbf{quality} and \textbf{correctness} checks, being labeled as \textbf{hard}, \textbf{medium}, \textbf{easy}, or \textbf{failed} based on filtering results. (3) \textbf{Refining Failed Generation}: Failed answers are optimized using TextGrad~\cite{yuksekgonul2024textgradautomaticdifferentiationtext} and re-filtered. If they fail again, the LLM is re-prompted to generate new answers (\textbf{Regeneration}). (4) \textbf{Fallback}: If no qualified answers emerge after four regeneration attempts, the answer most similar to the ground truth is selected as an easy hallucinated example. The detailed prompt used for hallucination generation task is presented in the Appendix~\ref{appendix:prompt}.}
\label{fig:pipeline}
\end{figure*}

 % \input{Datapoint_example}
\vspace{-3mm}
\section{Related Work}
\vspace{-2mm}
\paragraph{Hallucination Detection Benchmarks.}
 Hallucination in LLMs has been extensively documented in a variety of tasks, including machine translation~\citep{lee2019hallucinations_13RW}, dialogue systems~\citep{balakrishnan-etal-2019-constrained_14RW}, text summarization~\citep{durmus-etal-2020-feqa_15RW}, and question answering~\citep{sellam2020bleurtlearningrobustmetrics_16RW}, as reviewed in recent surveys~\citep{Ji_2023_12RW}. Existing benchmarks for hallucination detection, such as Hades~\citep{liu2022tokenlevelreferencefreehallucinationdetection} and HaluEval~\citep{Hallueval}, offer robust methodologies for identifying hallucinated content. However, they predominantly employ generic techniques that fail to account for the nuanced complexities inherent in medical contexts. Similarly, while benchmarks such as HaluBench~\citep{ravi2024lynxopensourcehallucination} include some medical data samples in their data set, their data generation processes are not specifically tailored for the medical domain. Although Med-HALT~\citep{pal2023medhaltmedicaldomainhallucination} focuses on medical hallucinations, it mainly serves as a performance evaluation tool rather than providing a structured dataset. In contrast, our work introduces the first comprehensive dataset for medical hallucination detection, employing controlled methods to address these domain-specific challenges.
\vspace{-2mm}
\paragraph{Semantic Analysis of Hallucinated Text.}
Hallucinated sentences often sound over-confident~\citep{miao2021preventlanguagemodeloverconfident_overconfident, chen2022improvingfaithfulnessabstractivesummarization_overconfident} and frequently contain tokens that are statistically improbable within a given context, primarily due to suboptimal decoding strategies. Fine-tuned models have sought to mitigate this issue by adjusting decoding parameters to enhance factual accuracy, thereby reducing the occurrence of rare or anomalous terms in hallucinated outputs~\citep{Huang_2025_survey11}. Despite these advancements, previous research has not systematically compared hallucinated sentences with their corresponding ground truth to assess semantic similarities. Our work fills this gap by uncovering deeper semantic relationships between hallucinated texts and their ground truth counterparts.
\vspace{-2mm}

\paragraph{Improvement Methods in Hallucination Detection.}

Recent advancements in hallucination detection have focused on integrating external knowledge to enhance model performance. Retrieval-augmented methods~\citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp_241, li2023weboysterimprovinglarge_242} have mitigate hallucinations via grounding models in general knowledge. However, few studies have examined the impact of domain-specific knowledge on hallucination detection tasks. While HaluEval~\citep{Hallueval} evaluates knowledge-augmented detection, it lacks fine-grained, domain-relevant knowledge integration. LLMs often overestimate their competence \citep{zhang2023sirenssongaiocean_247}, which underscores the need for structured mechanisms to allow models to abstain from answering when uncertain. Prior works have leveraged reinforcement learning~\cite{xu2024rejectionimprovesreliabilitytraining_246}, conformal abstention~\citep{yadkori2024mitigatingllmhallucinationsconformal_245}, or likelihood score and entropy-based metrics~\citep{cole2023selectivelyansweringambiguousquestions_248} to guide refusal decisions. However, these methods rely on complex supervision or predefined thresholds. More straightforward approaches, such as refusing to answer out-of-domain questions~\citep{cao2024learnrefusemakinglarge_244}, offer greater practicality but lack adaptability to domain-specific tasks, particularly in complex fields like medicine. Our work addresses these limitations by (1) incorporating task-specific medical knowledge to enhance hallucination detection and (2) introducing a self-supervised “not sure” class, enabling models to autonomously abstain from answering when uncertain, without requiring elaborate supervision. This dual approach remains under-explored in medical NLP, where precision and reliability are paramount.

\vspace{-3mm}
\section{MedHallu Benchmark} \label{Methodology}
% \vspace{-9mm}
We create this dataset by proposing a simple yet effective pipeline with minimal human intervention, making it easy to scale the data generation. Figure~\ref{fig:pipeline} describes our complete generation and filtration pipeline, while Algorithm \ref{alg:one} provides a detailed approach for the same. We draw inspiration from the definitions of hallucinated answers provided by the KnowHalu paper~\citep{KnowHallu}, but modify them by adding and removing certain categories to better adapt to the medical domain. By defining the medical domain-specific hallucination categories, as presented in Table~\ref{tab:medical_hallucination_types}, we ensure that the generated dataset reflects potential hallucination in the medical domains. We present the distribution of samples by hallucination categories and levels of difficulty (Figure~\ref{fig:statistics}) for the MedHallu dataset, which consists of 10,000 samples in total. The difficulty distribution of hallucinated answers is relatively even, with the ``hard'' type being slightly more common than the ``easy'' and ``medium'' types. The distribution of hallucination categories by definition is more concentrated. Misinterpretation of the question is the most common hallucination category in MedHallu, accounting for 76\% of the entire dataset, while evidence fabrication represents the smallest portion (0.5\%).

\input{figures/defination_table}
\begin{figure}[t]
    \centering
    \tiny
    \resizebox{\columnwidth}{!}{
        \input{figures/statistics_tikz}
    }
    \caption{Statistics of the MedHallu dataset categorized by four categories of hallucinations (see Table \ref{tab:medical_hallucination_types} for detailed definitions) and levels of difficulty (easy, medium, hard).}
    \label{fig:statistics}
\end{figure}
% \textcolor{red}{TL: please show the specific statistics of generated datasets.}
\vspace{-2mm}

\subsection*{Dataset Generation Pipeline}
\vspace{-2mm}
The proposed methodological framework comprises a three-phase pipeline architected for robust hallucinated sample generation (Figure~\ref{fig:pipeline}). 
The pipeline follows a sequential approach: (1) stochastic sampling of potential hallucinated responses based on in-context examples and precise definitions, (2) LLM-based quality filtering mechanisms, (3) correctness checking using bidirectional entailment and LLM prompting. (4) Sequential Improvement via TextGrad. Finally, inspired by~\citep{Hallueval}, we select the most similar sample generated, using semantic similarity in cases where a high-quality sample is not identified. This approach enables comprehensive identification and evaluation of linguistic hallucinations while minimizing false positives through multi-layered verification protocols.

\input{figures/algorithm}

\paragraph{1) Diverse Hallucinated Answer Sampling.} \label{Sampling step}

Using a carefully crafted prompting strategy shown in Figure~\ref{fig:pipeline}, we generate multiple possible hallucinated answers with diverse temperature settings, we describe the prompt in Table~\ref{fig:system_prompt}. Through experiments, we find that allowing the model to choose the category of hallucination to apply to a given medical question performs better than manually forcing a specific hallucination category. For this generation $H_i = LM_i(Q_i,GT_i,C_i)$, we provide the LLM with precise definitions of each category, along with examples, question $Q_i$, and ground truth answers $GT_i$. The LLM is tasked with generating an answer that is semantically similar to ground truth yet incorrect. Additionally, we provide the ground truth context $C_i$, which contains precise knowledge required to answer the question. This includes intricate details necessary for crafting a strong hallucinated answer. 

\paragraph{2) Quality checking - LLM-based Discriminative Filtering.} \label{Quality check}

The second phase of our pipeline implements a comprehensive quality filtering protocol leveraging an ensemble of LLMs to minimize individual model biases. For each generated sample $H_i$, we employ a comparative assessment framework where multiple LLMs independently evaluate two candidate responses: the potentially hallucinated answer and the established ground truth. The quality assessment task is formulated as a binary classification problem, where models are prompted to identify which response appears more factually accurate given the question without access to the ground truth context. To mitigate potential biases from any single model, we implement a majority voting mechanism across different LLM architectures (including Gemma2, GPT-4o-mini, and Qwen2.5). A generated sample $H_i$ is preserved only when at least a majority of models in the ensemble incorrectly identify it as the more accurate response compared to the ground truth.
The difficulty categorization of generated samples is determined by the voting patterns across the LLM ensemble. Specifically, we classify $H_i$ as ``hard'' when all LLMs in the ensemble incorrectly identify it as accurate response, ``medium'' when multiple but not all LLMs are deceived, and ``easy'' when only a single LLM fails to identify the hallucination. This multi-model consensus approach helps ensure that preserved hallucinated samples are sufficiently convincing while reducing the impact of model-specific quirks or biases in the filtering process.
\vspace{-1mm}
\paragraph*{3) Correctness Checking via Entailment.} \label{Correctness check}
We implement a two-stage correctness verification protocol to ensure that the generated hallucinations are semantically distinct from the ground truth while maintaining coherence. First, we employ bidirectional entailment checking using a fine-tuned RoBERTa-large-MNLI model to quantify the semantic divergence between the hallucinated sample $H_i$ and ground truth $GT_i$. The bidirectional entailment score $\mathcal{E}$ is computed as:
$$\mathcal{E}(H_i, GT_i) = \min(\text{NLI}(H_i \rightarrow GT_i), \text{NLI}(GT_i \rightarrow H_i))$$

where $\text{NLI}(x \rightarrow y)$ represents the natural language inference score indicating whether $x$ entails $y$. We establish a stringent threshold $\tau$ and only retain samples that satisfy: $\mathcal{E}(H_i, GT_i) < \tau$. This ensures the hallucinated samples maintain sufficient semantic distance from the ground truth, minimizing false positives while requiring minimal human intervention.


\paragraph*{4) Sequential Improvement via TextGrad.} \label{improvement}
Our framework implements an iterative optimization step to enhance the quality of generated hallucinations that fail initial quality or correctness checks. When a generated sample $H_i$ fails to meet the established quality tests described in Section~\ref{Quality check}, we employ TextGrad optimization to refine subsequent generations through a feedback loop. The optimization process is formalized as: $H_{i+1} = \text{TextGrad}(H_i, F(H_i))$ where $F(H_i)$ represents feedback from the TextGrad optimizer, initialized with GPT-4o-mini. This refinement process (detailed in Section~\ref{Sampling step}) iterates up to five times, terminating either upon reaching a quality-compliant sample or exhausting the iteration limit. For each failed generation, TextGrad analyzes LLM feedback to identify hallucination indicators that make $H_i$ easily detectable. The feedback mechanism specifically focuses on two aspects: (1) linguistic patterns that signal artificial content and (2) structural elements that could be refined to enhance the naturalness. This feedback is then incorporated into subsequent prompt refinement, systematically improving both the content plausibility and stylistic cohesion. If no sample passes the quality filter after maximum iterations, we implement a fallback strategy based on semantic dissimilarity. Specifically, we select the candidate $H_*$ that maximizes the cosine similarity from the ground truth embedding: $H_* = \arg\max_{H_i} (\cos(\text{embed}(H_i), \text{embed}(GT_i)))$. This ensures that even in challenging cases, our pipeline produces outputs with maximum semantic similarity while preserving response coherence.

% \subsection*{Statistics about the MedHallu Dataset}




\input{gemma_results_basic/table_merged}
\vspace{-2.5mm}
\section{Implementation Details}
\vspace{-2.5mm}
% \paragraph{MedHallu Dataset Generation.} Generating hallucinated responses that preserve semantic alignment with the ground truth while subtly modifying it to outcompete discriminator models is a nuanced challenge.
\paragraph{MedHallu Dataset Generation Settings.} We generate hallucinated responses using \texttt{Qwen2.5B-14B}~\citep{qwen2025qwen25technicalreport}. The ground truth question-answer pairs are sourced from the \texttt{pqa\_labeled} split of PubMedQA~\citep{PubmedQA}, which contains 1,000 expert-annotated samples, supplemented with 9,000 instances randomly selected from the \texttt{pqa\_artificial} split. To achieve high-quality generation with adequate diversity, we utilize regulated sampling settings. The \texttt{temperature} is varied between 0.3 and 0.7, while the \texttt{nucleus sampling threshold (top-p)} is fixed at 0.95. These settings balance cohesion and variability. The maximum response length is capped at 512 tokens to ensure completeness while mitigating computational costs. Each hallucinated answer is limited to within ±10\% of its corresponding ground truth answer's length, ensuring uniform information density.

\noindent $\triangleright$ \textit{Quality \& correctness check.} For quality check, We employ three LLMs: \texttt{GPT-4o mini}~\citep{openai2024gpt4ocard}, \texttt{Gemma2-9B}~\citep{gemmateam2024gemma2improvingopen}, and \texttt{Qwen2.5-7B}~\citep{qwen2025qwen25technicalreport}. A response is retained only if it deceives at least one of these models (see Section~\ref{Quality check}). For \textit{correctness check,} we employ the \texttt{microsoft/deberta-large-mnli} model~\citep{he2021debertadecodingenhancedbertdisentangled},  applying bidirectional entailment with a confidence threshold of 0.75. 

\noindent $\triangleright$ \textit{TextGrad \& Fallback.} We integrate \texttt{TextGrad}~\cite{yuksekgonul2024textgradautomaticdifferentiationtext} with \texttt{GPT-4o mini} as the backend model to generate feedback for samples that fail either the quality or correctness checks. Each sample undergoes a maximum of five generation attempts. If no valid response is produced within these iterations, we adopt a \textbf{fallback strategy}, selecting the most semantically similar generated answer to the ground truth response.

\paragraph{Discriminator Model Settings.} We evaluate a diverse set of model architectures under two distinct settings: (1) \textbf{zero-shot} (without additional knowledge) and (2) \textbf{context-aware} (with ground truth context provision). The detection prompt is described in Figure~\ref{fig:system_prompt_for_detection}. This dual-setting approach allows us to assess both the baseline detection capabilities and the models' ability to leverage contextual information for improved discrimination. We examine both general-purpose and specialized medical models. The \textbf{general models} include \texttt{Gemma-2 (2B, 9B) Instruct}~\citep{gemmateam2024gemma2improvingopen}, \texttt{Llama-3.1 (3B, 8B) Instruct}~\citep{grattafiori2024llama3herdmodels}, \texttt{Qwen-2.5 (3B, 7B, 14B)}~\citep{qwen2025qwen25technicalreport}, \texttt{DeepSeek-R1-Llama 8B}~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}, \texttt{GPT-4o}, and \texttt{GPT-4o mini}~\citep{openai2024gpt4ocard}. Additionally, we evaluate four \textbf{fine-tuned medical LLMs} such as \texttt{OpenBioLLM-8B}~\citep{OpenBioLLMs}, \texttt{Llama3-Med42-8B}~\citep{christophe2024med42v2suiteclinicalllms}, \texttt{BioMistral-7B}~\citep{labrak2024biomistral}, and \texttt{UltraMedical}~\citep{zhang2024ultramedical} to compare domain-specific adaptations against general-purpose models. In this discriminative task, we maintain a temperature of approximately \texttt{0.2-0.3} for all models. For OpenAI models, we use the official API, while for open-weight models like Llama, Gemma, and Qwen, we utilize the \texttt{Hugging Face Pipeline} to ensure a consistent inference framework across all models.

% \paragraph{Hyperparameter Selection.} \jiawei{I prefer not to have a separate section for hyperparameter selection. We should incorporate it into the two paragraphs above.} To achieve high generation quality with adequate diversity, we utilize regulated sampling settings. The temperature is varied between 0.3 and 0.7, keeping a constant nucleus sampling threshold (top-p) of 0.95. These parameters are empirically optimized to balance between cohesion and variability in responses. For efficiency, the maximum generation length is limited to 512 tokens, ensuring complete responses without undue computational burden. Our experimental design incorporates several model access frameworks for a thorough evaluation. For OpenAI models, we utilize the official API, whereas for open-weight models like Llama, Gemma, and Qwen, we use the Hugging Face Pipeline to maintain uniform inference procedures across all models. To maintain quality control and consistency, we impose certain constraints on generated responses. Each hallucinated answer response must be approximately the same length as its ground truth answer, with a tolerance of ±10\%, to maintain similar information density. Moreover, our generation process is limited to a maximum of 5 attempts per question. If no appropriate response is found within this threshold that passes the quality and correctness check, we switch to our fallback strategy which relies on semantic similarity, as explained in Section~\ref{improvement}.

\section{Results and Analysis} \label{ResultsAnalysis}

% \textcolor{red}{TL: Structurize your results by bullets, and also let the detailed can directly reflect your conclusions.}

% \textcolor{red}{TL: the structure of the experiments part is poor. maybe consider to structure them into all studies related to detection, then generation, then extra research questions.}

% \kaidi{Do we need to compare our results with Med-Halt and/or HaluBench medical part to demonstrate we may have some conflict but reasonable results against theirs? }
\vspace{-2mm}
\subsection{Which language model performs the best at medical hallucination detection task?}

Our experimental results reveal significant variations in hallucination detection performance across model architectures in the zero-shot setting (without relevant knowledge provided). As presented in Table~\ref{tab:llm_performance_merged}, \ding{202}~the size of a model is not necessarily linked to its detection capabilities. For instance, \texttt{Qwen2.5-3B} achieves a high baseline overall F1 score (0.606), outperforming larger models such as \texttt{Gemma-9B} (0.515), \texttt{Llama-3.1-8B-Instruct} (0.522), and even the \texttt{Qwen2.5-7B} model (0.533). \ding{203}~All models exhibit notable performance degradation on ``hard'' samples, with even the best-performing models, such as \texttt{GPT-4o}, showing a significant F1 score drop and achieving only 0.625 in these challenging cases. \ding{204}~An intriguing observation is that, overall, general LLMs outperform medical fine-tuned LLMs in terms of precision and F1 scores in the easy and medium categories when no additional knowledge is provided.

% \section{Ablation Experiments}

% \kaidi{The findings here are more interesting and should be highlighted as xxx-level analysis or other things rather than ablation. We should emphasize only our benchmark can provide such information  }
% We perform an extensive ablation study on the generated samples by measuring performance gains by explicitly providing ground-truth context in the in-context prompt; we note a substantial performance gain in the detection capabilities across LLMs. Yet, the performances of powerful LLMs like Qwen-2.5 14B and Llama-3.1 8B stay in the low 80\%, showing significant scope for performance improvement. We also perform ablation on the generation samples by clustering them. Specifically, we generate 50 possible hallucinated answers for a given question and then cluster them using the bidirectional entailment method proposed in \citep{Farquhar2024DetectingHI_semantic_entropy}. We find that most clusters contain points that either all pass the quality check, leading to high-quality sentences, or all fail the quality check. This highly indicates that these clusters of meaning directly correlate to the quality of the hallucinated sample. Also, across all clusters, we find that the clusters that successfully pass the quality check are closer (measured using Euclidean distance) to the ground truth compared to those with samples that fail the quality check. Similar insights can be derived from other metrics like cosine similarity and rouge score, indicating that cluster meanings and their similarity with ground truth highly affect the quality of hallucination samples generated.
\vspace{-1mm}
\subsection{How does providing knowledge impact detection performance?}
Providing knowledge to the LLMs in this hallucination detection task, yields substantial and consistent improvements in hallucination detection across all evaluated LLM architectures. As shown in Table \ref{tab:llm_performance_merged}, \ding{202} every model benefits from the inclusion of knowledge. In general LLMs, the average overall F1 score increases from 0.533 (without knowledge) to 0.784 (with knowledge), corresponding to a gain of +0.251. In contrast, medically fine-tuned LLMs exhibit a much smaller improvement—from an average overall F1 of 0.522 to 0.660 (+0.138), likely because these models already incorporate specialized domain knowledge during training. Moreover, the scale of the model is pivotal for its performance. \ding{203} Larger structures, such as \texttt{Qwen2.5-14B}, reach an impressive overall F1 score of 0.852 when supplemented with domain knowledge, indicating that their increased capacity supports better text comprehension and integration of knowledge. In contrast, smaller models like \texttt{Qwen2.5-3B} experience just slight enhancement (+0.07 F1, from 0.606 to 0.676), underscoring the variability in how different model sizes effectively use additional information. Remarkably, \texttt{Gemma-2-9B} showed the most significant benefit from knowledge, with its overall F1 score rising from 0.515 to 0.838 (+0.323). Overall, these findings affirm the hypothesis that domain knowledge access improves an LLM's hallucination detection ability, while also emphasizing that both model scale and whether the model has been fine-tuned on medical data are critical to the extent of performance improvements.

% \input{gemma_results_basic/w_knowledge}

\input{gemma_results_basic/cluster_metric}

\subsection{Semantic analysis of hallucinated and ground truth sentences.} \label{semantic_analysis}
% \jiawei{I feel like this section is different from the previous or later sections. This section is more related to the hallucination generation part. We may need to move this to section 3, as the robust check of our data generation pipeline?}
\vspace{-1mm}
To analyze semantic patterns in hallucinated responses, we conduct a comprehensive clustering analysis on an expanded set of generations. Specifically, we generate 50 candidate hallucinated responses for each question from our sampling phase, as described in Section~\ref{Quality check}. We retain all 50 candidate hallucinated responses, including those that fail the quality or correctness checks, to capture the semantic distribution across both successful and unsuccessful hallucinated answers. Using bidirectional entailment with a threshold of 0.75, we cluster these 50 candidate hallucinated responses along with the ground truth response, forming distinct semantic clusters that represent different conceptual approaches to the same question. This clustering methodology, adapted from~\citep{Farquhar2024DetectingHI_semantic_entropy}, allows us to analyze the semantic structure of hallucinated responses relative to the ground truth, yielding three significant findings:

\paragraph{Cluster-level Detection Patterns.}
Our analysis uncovers a binary discrimination effect within semantic clusters. \ding{202} Specifically, hallucinated responses in the same cluster tend to exhibit near-uniform performance—either consistently passing LLM detection (being favored over the ground truth) or being uniformly flagged as hallucinations. This finding strongly indicates that semantic content, rather than merely surface-level linguistic features, plays a pivotal role in shaping the LLM's discrimination behavior.
\vspace{-1mm}
\paragraph{Cluster Proximity Analysis.}
\ding{203} We find that clusters containing samples that reliably fool detection LLMs (hallucinations that are harder to detect) are notably closer to the ground truth answer in semantic vector space. This closeness is quantified via Euclidean distance, with additional support from cosine similarity and ROUGE scores (Table~\ref{tab:metrics_cluster}). Such proximity suggests that well-crafted hallucinated responses strike a balance, they remain semantically similar enough to the ground truth while incorporating meaningful deviations.

\vspace{-1mm}
\paragraph{Ground Truth Isolation.}
A particularly significant finding is the distinct semantic isolation of ground truth responses from clusters of hallucinated outputs. Empirical evidence demonstrates that ground truth responses rarely, if ever, align within the semantic clusters formed by hallucinations. This clear separation validates the robustness of our generation pipeline, ensuring that hallucinated responses retain semantic distinctness from factual content while upholding contextual relevance.
\input{gemma_results_basic/not_sure}

\vspace{-5mm}
\subsection{Analysis of models' ability to decline to answer}
% We introduce a ``not sure'' category to the existing ``hallucinated'' and ``not hallucinated'' categories in our detection prompt \jiawei{something like 'Figure S1'} figure \ref{fig:system_prompt_for_detection}. The results (Table \ref{tab:model-comparison}) reveal clear patterns across model sizes and knowledge integration. When models are allowed to express uncertainty, they achieve higher precision but lower recall, effectively maintaining better F1 scores by avoiding low-confidence answers. Different model families show distinct characteristics in handling this trade-off, with Qwen models demonstrating the most consistent performance across both scenarios \jiawei{not true for 'Qwen2.5-14B-Instruct'} , while Llama models show greater variation. Larger models, particularly the 14B and 7B variants, excel in forced-answer scenarios, with Qwen-14B showing remarkable stability. Across both scenarios, knowledge-enabled models consistently outperform their counterparts. Interestingly, 
% \jiawei{What are 'these models?' knowledge-enabled models?} these models appear to use the ``not sure'' option more strategically. The performance gap between knowledge-enabled and knowledge-disabled versions widens when models are required to provide an answer.
% This suggests that enabling the ``not sure'' option benefits applications prioritizing precision, while those requiring higher coverage might benefit from forced answers. 

We introduce a ``not sure'' category alongside the existing ``hallucinated'' and ``not hallucinated'' categories in our detection prompt (Figure \ref{fig:system_prompt_for_detection}), allowing LLMs to decline to answer if they lack full confidence in their responses. Results shown in Table \ref{tab:model-comparison}, reveal that \ding{202} many models demonstrate an improved F1 score and precision when they can opt for ``not sure.'' However, the enhancement varies with model size: smaller models gain a moderate improvement of 3-5\%, whereas larger models see a significant boost of around 10-15\%. General LLMs outperform fine-tuned medical models, with some like GPT-4o achieving up to 79.5\% in performance, and Qwen2.5-14B performing closely at 76.2\%. \ding{203} In terms of the percentage of questions answered with definite ``yes'' or ``no'' (Response Rate), general LLMs respond to fewer questions, with Qwen2.5-14B responding to as little as 27.9\%, reflecting their tendency to skip uncertain questions. Conversely, fine-tuned medical models attempt to answer nearly all questions, rarely selecting the ``Not Sure'' option. This approach sometimes leads to a minor reduction in performance. For instance, UltraMedical's model has the lowest response rate among medical models at 69.7\% , while OpenBioLLM reaches as high as 99.7\%. Finally, \ding{204} when comparing the impact of adding the ``not sure'' choice with knowledge-sharing enhancements, shown in Table \ref{tab:model-comparison-with-knowledge} versus Table \ref{tab:model-comparison}, there is a marked increase in the percentage of questions attempted by General LLMs, suggesting improved confidence in task execution, along with an increase in F1 score and precision.

\begin{figure}
    \centering
    \tiny
    \resizebox{\columnwidth}{!}{
        \input{figures/plots}
    }
    \caption{Detection accuracy of different hallucination categories on MedHallu, evaluated using \texttt{Qwen2-7B-Instruct} as the discriminator.}
    \label{fig:hallucination_types}
\end{figure}

% \vspace{-9mm}
\subsection{Analysis: Hallucination category and MeSH}

\subsubsection*{Which hallucination category is hardest to detect? }
Our analysis reveals distinct patterns in detection difficulty across hallucination categories, as shown in Figure~\ref{fig:hallucination_types}. \textbf{Incomplete Information (II)} emerges as the most challenging category, with 41\% of total samples being ``hard'' cases (Figure \ref{fig:statistics}) and the lowest detection ratio (54\%), indicating models struggle significantly with validating partial information. \textbf{Mechanism/Pathway Misattribution (MPM)} and \textbf{Question Misinterpretation (MQ)} show notable patterns: MPM has a significant number of hard cases, with a 68\% detection accuracy, while MQ having higher number of hard cases but stronger detection performance (68.8\%). \textbf{Methodological and Evidence Fabrication (MEF)}, despite being the smallest category (37\% are hard), demonstrates the highest detection success rate (76.6\%).

\noindent These findings highlight a crucial insight: subtle manipulation of existing medical information, particularly through incomplete presentation, is harder to detect than outright fabrication. This is evident from II's high difficulty scores compared to MEF's better detection rates. The distribution across difficulty levels (easy, medium, hard) further supports this, with II showing the highest concentration in the ``hard'' category. This suggests that while models excel at identifying completely fabricated information, they struggle with partially accurate yet incomplete medical claims, highlighting critical areas of improvement in hallucination detection systems.

\subsubsection*{Which medical category (MeSH term) hallucination is the hardest to detect?}

\noindent To understand which medical domains are more susceptible to hallucination, we examine the MedHallu dataset with the MeSH categories within the PubMedQA dataset, identifying the top five principal categories shown in Figure~\ref{fig:mesh_categories}. These categories include Diseases (comprising 25.9\% of the samples), Analytical Procedures (20.1\%), Chemical/Drug Queries (15.8\%), Healthcare Management (9.7\%), and Psychiatric Conditions (6.7\%). Detection performance among these categories varies considerably: Disease-related instances exhibit a respectable detection accuracy of 57.1\%, despite the abundance of related medical literature in the corpus. Conversely, Chemical/Drug queries demonstrate the highest detection rate at 67.7\%. In contrast, Psychiatry ranks lowest among the top five categories with a detection rate of just 53.7\%, highlighting the need for further incorporation of this data in the training corpus.

\begin{figure}[t]
    \centering
    \tiny
    \scalebox{0.7}{
        \input{figures/mesh_plot}
    }
    \caption{Detection accuracy across Mesh categories proposed in PubMedQA. We use \texttt{Qwen2.5-7B-Instruct} as a discriminator for the 1k samples of MedHallu generated on pqa\_labeled split.}
    \label{fig:mesh_categories}
\end{figure}

\vspace{-2mm}
\section{Conclusion}
\vspace{-2mm}
We introduce MedHallu, a comprehensive benchmark comprising 10,000 rigorously curated medical question-answer pairs with hallucinated answers. MedHallu integrates fine-grained categorization of medical hallucination types, a hallucination generation framework that balances difficulty levels while mitigating single-LLM bias through multi-model majority voting, and systematically evaluates diverse LLM configurations' hallucination detection capabilities. Our evaluation reveals that existing LLMs exhibit significant limitations in detecting medical hallucinations, particularly struggling with "hard" hallucination answers, which are closer in distance to the ground truth. We also provide insights into enhancing LLMs' hallucination detection: when knowledge is provided, general-purpose LLMs can outperform medical fine-tuned models, and allowing models to decline to answer by providing a "not sure" option improves precision in critical applications. As the largest open medical hallucination benchmark to date, MedHallu serves as a valuable resource for evaluating LLMs' medical hallucination detection abilities and offers insights into the cautious use of LLMs in high-stakes medical domains.


\section{Limitations}
Our study faces three primary constraints. First, due to resource constraints, we could not employ the most advanced reasoning models (e.g., OpenAI o1, Gemini 2.0, DeepSeek-R1) for benchmark generation. While our pipeline incorporates multi-stage LLM quality checks and regeneration steps, using state-of-the-art models would incur prohibitive computational costs. Second, our evaluation of LLMs was restricted to input-output prompting (zero-shot, with/without knowledge provision); resource limitations precluded exploration of advanced techniques like chain-of-thought or self-consistency, which might better elicit model capabilities. Third, our hallucination generation pipeline relied on the PubMedQA corpus to ensure contextual fidelity. While this ensures biomedical relevance, future work should incorporate diverse high-quality corpora to improve scalability and domain coverage.

\section{Ethics Statement}
This research adheres to rigorous ethical standards in dataset creation and evaluation. The MedHallu benchmark utilizes publicly available PubMedQA data under MIT licenses, ensuring proper attribution and compliance with source terms of use. Patient privacy is preserved through the exclusive use of de-identified biomedical literature. While our work aims to improve AI safety in healthcare, we acknowledge potential dual-use risks and advocate for responsible deployment of medical LLMs with human oversight. The benchmark's stratification enables targeted mitigation of dangerous ``hard'' hallucinations that most closely resemble factual content. All artifacts will be released with detailed documentation to promote transparency and reproducibility in medical AI safety research.


% This work introduces MedHallu, a comprehensive benchmark for evaluating medical hallucination detection capabilities in large language models. Through our systematic evaluation of 50,000 question-answer pairs across various medical domains, we have uncovered several critical insights about the current state and limitations of hallucination detection in medical AI systems.
% Our findings demonstrate that even state-of-the-art language models struggle with medical hallucination detection, achieving only moderate performance in zero-shot settings (F1 scores of 0.65 or lower). The provision of ground truth context significantly improves detection capabilities across all models, with some achieving F1 scores up to 0.84, highlighting the crucial role of knowledge grounding in enhancing model reliability. However, the persistent challenges in detecting ``hard'' cases, particularly those involving incomplete information, underscore the complexity of this task and the need for continued advancement in model architectures and training approaches.

% The semantic clustering analysis revealed that hallucination detection is primarily influenced by semantic content rather than surface-level features, with clear patterns emerging in how models evaluate responses within similar semantic clusters. This finding suggests that future improvements in hallucination detection should focus on enhancing models' semantic understanding capabilities rather than purely syntactic features.

% These findings have significant implications for deploying AI systems in healthcare settings. The substantial improvement in detection performance, when models are provided with ground truth context (+35\% F1 score in some cases), suggests that future medical AI systems should be designed with robust knowledge integration capabilities. Additionally, the benefits of allowing models to express uncertainty (``not sure'' option) in improving precision highlights the importance of implementing appropriate confidence thresholds in clinical applications.

% As AI systems become increasingly integrated into healthcare decision-making processes, the ability to reliably detect and filter out hallucinated information becomes crucial. The MedHallu benchmark provides a foundation for evaluating and improving these capabilities, contributing to developing more reliable and trustworthy medical AI systems.

% \bibliographystyle{acl_natbib}
\bibliography{bib} 

\clearpage

\appendix
\input{latex/supp}

\end{document}
