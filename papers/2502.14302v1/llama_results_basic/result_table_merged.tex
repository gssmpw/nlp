\begin{table*}
\centering
\begin{tabular}{l|ccc|ccc|c|c}
\hline
\textbf{Model} & \textbf{Overall F1} & \textbf{Overall Pr} & \textbf{Overall Re} & \textbf{Easy F1} & \textbf{Med F1} & \textbf{Hard F1} & $\Delta$ \textbf{Know.} & \textbf{Avg} \\
\hline
Qwen2.5-14B \\
~~w/o knowledge & 0.69 & 0.80 & 0.61 & 0.81 & 0.72 & 0.55 & - & 0.70 \\
~~w/ knowledge & \textbf{0.86} & 0.85 & 0.87 & 0.89 & 0.87 & 0.82 & +0.17 & 0.86 \\
\hline
GPT-4o-mini \\
~~w/o knowledge & 0.71 & 0.86 & 0.61 & 0.82 & 0.72 & 0.60 & - & 0.72 \\
~~w/ knowledge & 0.85 & 0.85 & 0.85 & 0.91 & 0.86 & 0.77 & +0.14 & 0.85 \\
\hline
Qwen2.5-7B \\
~~w/o knowledge & 0.64 & 0.76 & 0.55 & 0.70 & 0.71 & 0.51 & - & 0.65 \\
~~w/ knowledge & 0.85 & 0.84 & 0.85 & 0.92 & 0.87 & 0.77 & +0.21 & 0.85 \\
\hline
GEMMA-9B \\
~~w/o knowledge & 0.68 & 0.79 & 0.61 & 0.81 & 0.70 & 0.53 & - & 0.69 \\
~~w/ knowledge & 0.82 & 0.78 & 0.86 & 0.86 & 0.82 & 0.78 & +0.14 & 0.82 \\
\hline
DeepSeek-8B \\
~~w/o knowledge & 0.61 & 0.68 & 0.55 & 0.68 & 0.61 & 0.54 & - & 0.61 \\
~~w/ knowledge & 0.82 & 0.86 & 0.79 & 0.84 & 0.85 & 0.77 & +0.21 & 0.82 \\
\hline
\end{tabular}
\caption{Performance comparison of different LLMs with and without knowledge. Overall F1/P/R represents metrics computed on the entire dataset. $\Delta$ Knowledge shows the average absolute improvement across all metrics when knowledge is provided. The rightmost column shows the average of all metrics per model condition. All values are rounded to 2 decimal places. Models are sorted by overall F1 score with knowledge.}
\label{tab:llm_performance}
\end{table*}