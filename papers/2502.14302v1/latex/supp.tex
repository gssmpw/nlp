\section*{Appendices}

\section{Additional Related Work}
% \subsection*{Not sure with knowledge - Write properly}
\paragraph{General LLMs vs Fine-tuned LLMs in Hallucination Detection.}
Extensive research has investigated hallucination in texts generated by pre-trained and domain-specific fine-tuned LLMs. Studies have revealed that fine-tuned LLMs exhibit reduced hallucination in text generation compared to their general-purpose counterparts~\citep{azaria2023internalstatellmknows_detect2, xiong2024llmsexpressuncertaintyempirical, arteaga2024hallucinationdetectionllmsfast}. However, despite these advancements, there remains a notable gap that no prior work has systematically evaluated the performance of domain-specific fine-tuned LLMs on hallucination detection tasks. Lynx~\citep{ravi2024lynxopensourcehallucination}, a model specifically designed for hallucination detection, has demonstrated superior performance over general-purpose LLMs across diverse datasets. Nevertheless, this study did not extend its evaluation to include LLMs fine-tuned for specialized domains, such as medicine or finance. To address this limitation, our work conducts a comparative analysis of several fine-tuned medical LLMs in the context of medical hallucination detection.

\paragraph{Evaluation of Hallucinations and Faithfulness}
The hallucination phenomenon in LLMs manifests as the production of content that lacks proper substantiation through contextual evidence or verified knowledge bases. This can be categorized into two distinct forms: factuality hallucination, which involves deviations from established real-world facts, and faithfulness hallucination, which occurs when the model's generated content diverges from the provided input context or prompt \citep{Huang_2025_survey11}. These dual manifestations represent significant challenges in ensuring the reliability and accuracy of LLM-generated outputs. There have been recent works in detecting the faithfulness of an LLM with the use of context \citep{ming2024faithevallanguagemodelstay_fact4} or even checking the faithfulness of LLMs in the absence of context \citep{roller2020recipesbuildingopendomainchatbot_fact1,min2023factscorefinegrainedatomicevaluation_fact2,chern2023factoolfactualitydetectiongenerative_fact3, wei2024longformfactualitylargelanguage_fact5}. Contrary to faithfulness, hallucinations are detected mainly focusing on the output of the LLMs rather than the context \citep{Hallueval,liu2022tokenlevelreferencefreehallucinationdetection,hu2024refcheckerreferencebasedfinegrainedhallucination}. 

\section{Incorporating Knowledge into the Analysis of Models’ Denial Capabilities}
We evaluate the setting where the model is given a choice of answering ``not sure'' when it lacks confident to answer (Table~\ref{tab:model-comparison}). We also provide the relevant knowledge in the prompt (Appendix~\ref{appendix:prompt}). The results in Table~\ref{tab:model-comparison-with-knowledge} clearly indicate the improvement in models' capability to answer the questions compared to the previous knowledge-disabled setting. Here \texttt{Qwen2.4-14B} surpasses all other models in terms of F1 and even precision. The results indicate that even though models' performance in terms of F1 increases slightly or even remains nearly similar, the precision of these models is generally improved. 

\input{gemma_results_basic/not_sure_knowledge}

\section{Additional Data Correctness Check}
In addition to the existing correctness check proposed in Section~\ref{Correctness check}, we leverage Llama-3.1 to perform a lightweight semantic comparison between $H_i$ and $GT_i$. Through carefully crafted prompts, the model assesses whether the hallucinated response differs meaningfully in semantic content from the ground truth. This additional verification layer provides a cost-effective mechanism to filter out subtly similar generations that might have passed the initial entailment check.

\section{Selecting Medical Hallucination Categories for MedHallu} \label{sec:hallu_categories}
We adapted hallucination categories from KnowHallu \cite{KnowHallu} to categorize generated outputs (Table~\ref{fig:hallucination_types}). KnowHallu includes categories such as \textit{Vague}, \textit{Parroting}, and \textit{Overgeneralization}, which are more suited for hallucination detection rather than generation. These categories pose challenges in a generative setting because crafting high-quality examples that convincingly exhibit extreme parroting or subtle overgeneralization in a way that can reliably mislead a discriminator is inherently difficult. Moreover, such cases may not be as informative for evaluating generative models, as they focus on stylistic nuances rather than substantive factual inconsistencies. To ensure a more effective classification for generation, we examined various medical research papers and carefully designed a set of hallucination categories that best capture the types of errors relevant to medical text generation. This approach allows for a more meaningful evaluation of generative models while maintaining both diversity and practical relevance in the generated outputs.

\input{gemma_results_basic/hardware}
\input{figures/models}


\section{MedHallu Creation Using Other Open-weights LLMs} \label{Gen_robust_check}

We construct the MedHallu dataset using open-weights LLMs, including \texttt{Qwen2.5-14B} and \texttt{Gemma2-9B}. Initially, we generate 1,000 samples based on the high-quality, human-annotated \texttt{pqa\_labeled\_split} from PubMedQA. To ensure quality, we employ smaller LLMs, including \texttt{GPT-4o mini}, \texttt{Gemma2-2B}, and \texttt{Llama-3.2-3B} variants, for verification. Subsequently, we evaluate various LLMs, including both general-purpose and fine-tuned medical models, on these datasets. The results for the \texttt{Gemma2-9B-IT} model are presented in Table~\ref{tab:llm_performance_gemma}, while those for the \texttt{Qwen2.5-14B} model are reported in Table~\ref{tab:llm_performance_qwen}. We conduct three independent runs for dataset generation and report the mean and standard deviation of the results. During our analysis, we observed that the Qwen model exhibited faster generation speeds and consistent generation quality with fewer cases that fail quality checks on average, thus saving up more on time and computing budget, so we decided to generate the entire dataset using \texttt{Qwen2.5-14B}. Consequently, we selected the \texttt{Qwen2.5-14B} to generate an expanded dataset comprising 10,000 samples. We see that the results in the Tables~\ref{tab:llm_performance_gemma} and \ref{tab:llm_performance_qwen} are also in alignment with the observations we presented in Section~\ref{ResultsAnalysis} of the paper, thereby bolstering our claim and contribution.


\input{gemma_results_basic/supp_gemma_results}
\input{gemma_results_basic/supp_qwen_results}


\section{Example Data from the MedHallu Dataset}
In Table \ref{tab:dataset-samples}, we present several randomly selected examples from our MedHallu Dataset to illustrate specific hallucination categories. Each example is accompanied by its corresponding hallucination category and assigned difficulty level, providing a concise overview of the dataset’s diversity.



\input{figures/Table_Example}

\section{Hardware Resources and Computational Costs}

We provide detailed information on our computational budget and infrastructure (see Table~\ref{tab:comp-details}). During the dataset generation process, we primarily used the \texttt{Qwen2.5-14B} model, running it for 24 hours on an NVIDIA RTX A6000 GPU with 48,685 MiB of RAM. Additionally, we employed models such as \texttt{Gemma2-9B}, \texttt{Qwen2.5-7B}, and \texttt{GPT-4o mini} as verifiers, generating a total of 10,000 samples for our dataset. To enhance the efficiency and speed of our code execution, we utilized software tools like \texttt{vLLM} and implemented batching strategies. These optimizations were critical for managing the computational load and ensuring timely processing of our experiments.


\section{LLMs Used in Discriminative Tasks}

\textbf{GPT-4o and GPT-4o mini.} GPT-4o and GPT-4o mini~\cite{openai2024gpt4ocard} are a series of commercial LLMs developed by OpenAI. Renowned for their state-of-the-art performance, these models have been extensively utilized in tasks such as medical hallucination detection. Our study employs the official API provided by the OpenAI platform to access these models. For all other models below, we implement them through Hugging Face package.

\noindent \textbf{Llama-3.1 and Llama-3.2.} Llama-3.1 and Llama-3.2~\citep{grattafiori2024llama3herdmodels} are part of Meta's open-source multilingual LLMs, Llama 3.1 (July 2024) includes 8B, 70B, and 405B parameter models optimized for multilingual dialogue. Llama 3.2 (September 2024) offers 1B, 3B, 11B, and 90B models with enhanced accuracy and speed.


\noindent \textbf{Qwen2.5.} Qwen2.5~\citep{qwen2025qwen25technicalreport} is an advanced LLM designed to handle complex language tasks efficiently. It has been applied in various domains, including medical hallucination detection. We use the 3B, 7B and 14B variants in our work.


\noindent \textbf{Gemma2.} Gemma2~\citep{gemmateam2024gemma2improvingopen} is a LLM known for its robust performance in discriminative tasks. There are various model sizes available, we use the 2B and the 9B variants.


\noindent \textbf{DeepSeek-R1-Distill-Llama-8B.} DeepSeek-R1-Distill-Llama-8B~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability} is a fine-tuned model based on Llama 3.1-8B, developed by DeepSeek AI. This model is trained using samples generated by DeepSeek-R1, with slight modifications to its configuration and tokenizer to enhance performance in reasoning tasks.

\noindent \textbf{OpenBioLLM-Llama3-8B.} OpenBioLLM-Llama3-8B~\citep{OpenBioLLMs} is a specialized LLM tailored for biomedical applications. It is fine-tuned from the Llama 3 architecture to understand and process biomedical texts effectively.


\noindent \textbf{BioMistral-7B.} BioMistral-7B~\citep{labrak2024biomistral} is an LLM designed specifically for biomedical tasks. With 7 billion parameters, it offers a balance between performance and computational efficiency. 


\noindent \textbf{Llama-3.1-8B-UltraMedical.} Llama-3.1-8B-UltraMedical~\citep{zhang2024ultramedical} is a variant of Meta's Llama 3.1-8B model, fine-tuned for medical applications. It is optimized to handle medical terminologies and contexts.


\noindent \textbf{Llama3-Med42-8B.} Llama3-Med42-8B~\cite{med42v2} is a specialized version of the Llama 3 series, fine-tuned on medical datasets to enhance its performance in medical-related tasks.





\section{Additional Implementation Details}
Our experiments were conducted using \texttt{PyTorch 2.4.0} with \texttt{CUDA 12.2}, ensuring state-of-the-art GPU acceleration and performance. For data and model access, we relied on Hugging Face resources, specifically using the \texttt{qiaojin/PubMedQA} dataset. In addition, we employed \texttt{vLLM 0.6.3.post1} with a \texttt{tensor\_parallel\_size} of 4 and maintained a \texttt{gpu\_memory\_utilization} of 0.80, which was instrumental in optimizing our inference process. The list of pre-trained models' huggingface names used in our experiments is provided in Table~\ref{tab:models}.


\section{PubMedQA}

\textbf{PubMedQA}~\cite{PubmedQA} is a biomedical research QA dataset under the MIT license. It contains 1,000 expert-annotated questions (\texttt{pqa\_labeled\_split}) and 211K machine-labeled questions from PubMed abstracts (the most widely used biomedical literature resource). PubMedQA also provides relevant context (relevant knowledge) for each question-answer pair. We utilize this relevant knowledge to help generate the hallucinated answers (Figure \ref{fig:system_prompt}). This relevant knowledge is also used in our hallucination detection task (Figure~\ref{fig:system_prompt_for_detection}).


\section{System Prompts for Hallucination Generation and Detection} \label{appendix:prompt}
Figure \ref{fig:system_prompt} shows the system prompt utilized to generate the MedHallu dataset, while Figure \ref{fig:system_prompt_for_detection} illustrates the system prompt designed for the hallucination detection task. These prompts were critical in guiding the model's behavior for both tasks. We incorporated the ``knowledge'' into various experiments, where it serves as the ``context'' provided in the original PubMedQA dataset.


% Figure \ref{fig:system_prompt_for_detection}. Figure \ref{fig:system_prompt}. 



\begin{figure*}[ht]
\centering
\small
\includegraphics[width=1\linewidth]{Prompts/systemprompt.pdf}
\caption{System prompt used to generate MedHallu dataset. The ``knowledge'' refers to the relevant context of a specific question pair. The PubMedQA dataset provides this ``knowledge''.}
\label{fig:system_prompt}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=1\linewidth]{Prompts/detectionprompt.pdf}
\caption{System prompt used for the hallucination detection task. The ``knowledge'' refers to the relevant context of a specific question pair. The PubMedQA dataset provides this ``knowledge''.}
\label{fig:system_prompt_for_detection}
\end{figure*}

\section{The Clusters Formed for a Question Using Bidirectional Entailment.}
Following the methodology proposed in Section \ref{semantic_analysis}, we create clusters. Table \ref{tab:example_clusters} presents an example of some clusters formed for a specific question using bidirectional entailment, which clearly shows sentences in the same cluster are identical in meaning (semantically) but different in syntax. The table also shows an example (Cluster 2) showing examples for sentences that fail to fool a discriminator. 
\input{gemma_results_basic/Cluster_example}



