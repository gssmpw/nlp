% \begin{table*}
% \centering
% \small
% \setlength{\tabcolsep}{4pt} % Reduce space between columns
% \begin{tabular}{l|ccccc|ccccc|c}
% \hline
% \textbf{Model} & \multicolumn{5}{c|}{Without Knowledge} & \multicolumn{5}{c|}{With Knowledge} & \textbf{$\Delta$ Know.} \\
% \hline
%               & \shortstack{Overall\\F1} & \shortstack{Overall\\P} & \shortstack{Easy\\F1} & \shortstack{Med\\F1} & \shortstack{Hard\\F1} 
%               & \shortstack{Overall\\F1} & \shortstack{Overall\\P} & \shortstack{Easy\\F1} & \shortstack{Med\\F1} & \shortstack{Hard\\F1} & \\
% \hline
% Qwen2.5-14B-Instruct   & 0.59 & 0.73 & 0.81 & 0.64 & 0.46 & \textbf{0.83} & 0.84 & \textbf{0.93} & 0.89 & 0.74 & +0.24 \\
% Gemma-2-9b-it        & 0.61 & 0.73 & \textbf{0.83} & \textbf{0.73} & 0.44 & 0.81 & 0.78 & 0.88 & 0.85 & \textbf{0.76} & +0.20 \\
% Llama-3.1-8B-Instruct  & 0.44 & \textbf{0.85} & 0.73 & 0.58 & 0.19 & 0.79 & \textbf{0.89} & 0.90 & 0.84 & 0.70 & \textbf{+0.35} \\
% Qwen2.5-7B-Instruct    & 0.51 & 0.73 & 0.78 & 0.58 & 0.31 & 0.81 & 0.85 & 0.92 & 0.83 & 0.74 & +0.30 \\
% Qwen2.5-3B-Instruct    & \textbf{0.62} & 0.47 & 0.66 & 0.67 & \textbf{0.57} & 0.66 & 0.50 & 0.69 & 0.65 & 0.66 & +0.04 \\
% GPT-4o-mini            & 0.55 & 0.78 & 0.81 & 0.65 & 0.35 & \textbf{0.83} & 0.84 & 0.90 & \textbf{0.91} & \textbf{0.76} & +0.28 \\
% DeepSeek-R1-Distill-Llama-8B & 0.51 & 0.61 & 0.64 & 0.62 & 0.40 & 0.77 & 0.83 & 0.87 & 0.80 & 0.69 & +0.26 \\
% \hline
% \textbf{Average (Pre-trained)} & 0.55 & 0.70 & 0.75 & 0.64 & 0.39 & 0.79 & 0.79 & 0.87 & 0.82 & 0.72 & +0.24 \\
% \hline
% \\[-1ex] % extra space before the next header
% \multicolumn{12}{l}{\textbf{Medical Fine-Tuned LLMs}} \\[1ex] % extra space after the header
% \hline
% OpenBioLLM-Llama3-8B & 0.58 & 0.55 & 0.61 & 0.58 & 0.56 & 0.56 & 0.56 & 0.55 & 0.56 & 0.56 & -0.02 \\
% BioMistral-7B       & 0.56 & 0.50 & 0.55 & 0.57 & 0.56 & 0.63 & 0.54 & 0.61 & 0.68 & 0.63 & +0.07 \\
% Llama-3.1-8B-UltraMedical & 0.58 & 0.58 & 0.76 & 0.65 & 0.45 & 0.70 & 0.73 & 0.79 & 0.71 & 0.65 & +0.12 \\
% \hline
% \textbf{Average (Fine-Tuned)} & 0.57 & 0.54 & 0.64 & 0.60 & 0.52 & 0.63 & 0.61 & 0.65 & 0.65 & 0.61 & +0.06 \\
% \hline
% \end{tabular}

% \caption{Performance comparison of different LLMs with and without knowledge. General pre-trained LLMs perform better than Medically fine-tuned LLMs in the task of Medical Hallucination, in terms of almost all metrics. P denotes, Precision. $\Delta$ Know is the performance change in Overall F1 when knowledge is provided to the LLM.}
% \label{tab:llm_performance_merged}
% \end{table*}



\begin{table*}[t]
    \centering
    % Reduce font size globally for this table
    % \scriptsize
    % Adjust row spacing if needed
    \renewcommand{\arraystretch}{1.2}
    % Scale the entire tabular environment to fit within \linewidth
    \begin{adjustbox}{width=1\linewidth, center}
    \begin{tabular}{lccccc|ccccc|c}
    \toprule
    \textbf{Model} & \multicolumn{5}{c|}{\textbf{Without Knowledge}} & \multicolumn{5}{c|}{\textbf{With Knowledge}} & \textbf{$\Delta$ Knowledge} \\
    \cmidrule(lr){1-6} \cmidrule(lr){7-11}
    \textbf{General LLMs} & \textbf{Overall F1} & \textbf{Overall P} & \textbf{Easy F1} & \textbf{Med F1} & \textbf{Hard F1} 
                   & \textbf{Overall F1} & \textbf{Overall P} & \textbf{Easy F1} & \textbf{Med F1} & \textbf{Hard F1} & ($\Delta$ F1)\\
    \cmidrule(lr){2-6} \cmidrule(lr){7-12}
    GPT-4o$^*$                       & \textbf{0.737} & 0.723 & \textbf{0.844} & \textbf{0.758} & \textbf{0.625} & \textbf{0.877} & \textbf{0.882} & \textbf{0.947} & \textbf{0.880} & \textbf{0.811} & 0.140 \\
    GPT-4o mini                  & 0.607 & 0.772 & 0.783 & 0.603 & 0.446 & 0.841 & 0.820 & 0.914 & 0.854 & 0.761 & 0.234 \\
    Qwen2.5-14B-Instruct         & 0.619 & 0.691 & 0.773 & 0.611 & 0.483 & 0.852 & 0.857 & 0.935 & 0.856 & 0.769 & 0.233 \\
    Gemma-2-9b-Instruct          & 0.515 & 0.740 & 0.693 & 0.512 & 0.347 & 0.838 & 0.809 & 0.918 & 0.848 & 0.758 & \textbf{0.323} \\
    Llama-3.1-8B-Instruct        & 0.522 & \textbf{0.791} & 0.679 & 0.515 & 0.372 & 0.797 & 0.775 & 0.880 & 0.796 & 0.722 & 0.275 \\
    DeepSeek-R1-Distill-Llama-8B & 0.514 & 0.570 & 0.589 & 0.515 & 0.444 & 0.812 & 0.864 & 0.895 & 0.794 & 0.751 & 0.298 \\
    Qwen2.5-7B-Instruct          & 0.553 & 0.745 & 0.733 & 0.528 & 0.402 & 0.839 & 0.866 & 0.923 & 0.832 & 0.770 & 0.286 \\
    Qwen2.5-3B-Instruct          & 0.606 & 0.495 & 0.667 & 0.602 & 0.556 &  0.676 & 0.514 & 0.693 & 0.677 & 0.661 & 0.070 \\
    Llama-3.2-3B-Instruct                  & 0.499 & 0.696 & 0.651 & 0.467 & 0.384 & 0.734 & 0.775 & 0.822 & 0.723 & 0.664 & 0.235 \\
    Gemma-2-2b-Instruct                   & 0.553 & 0.620 & 0.680 & 0.524 & 0.457 & 0.715 & 0.786 & 0.812 & 0.705 & 0.631 & 0.162 \\
    
    \midrule
    
    % \multicolumn{12}{l}{\textbf{Medical Fine-Tuned LLMs}} 
    \textbf{Medical Fine-Tuned LLMs} & \textbf{Overall F1} & \textbf{Overall P} & \textbf{Easy F1} & \textbf{Med F1} & \textbf{Hard F1} 
                   & \textbf{Overall F1} & \textbf{Overall P} & \textbf{Easy F1} & \textbf{Med F1} & \textbf{Hard F1} & {($\Delta$ F1)}\\
    \cmidrule(lr){2-6} \cmidrule(lr){7-12}
    OpenBioLLM-Llama3-8B         & 0.484 & 0.490 & 0.494 & 0.474 & 0.483 & 0.424 & 0.567 & 0.438 & 0.412 & 0.423 & -0.060 \\
    BioMistral-7B                & 0.570 & 0.518 & 0.627 & 0.563 & \textbf{0.525} & 0.648 & 0.516 & 0.652 & 0.660 & 0.634 & 0.078 \\
    Llama-3.1-8B-UltraMedical    & \textbf{0.619} & 0.657 & \textbf{0.747} & \textbf{0.596} & 0.524 & 0.773 & 0.679 & 0.832 & 0.777 & \textbf{0.718} & 0.153 \\
    Llama3-Med42-8B & 0.416 & \textbf{0.829} & 0.600 & 0.379 & 0.264 & \textbf{0.797} & \textbf{0.856} & \textbf{0.898} & \textbf{0.794} & 0.707 & \textbf{0.381} \\
    \midrule
    \textbf{Average (General LLMs, w/o GPT-4o)} 
                                 & \textbf{0.533} & \textbf{0.686} & \textbf{0.674} & \textbf{0.517} & 0.412 
                                 & \textbf{0.784} & \textbf{0.789} & \textbf{0.864} & \textbf{0.781} & \textbf{0.716} 
                                 & \textbf{0.251} \\
    \textbf{Average (Medical Fine-Tuned LLMs)} 
                                 & 0.522 & 0.623 & 0.617 & 0.503 & \textbf{0.449} 
                                 & 0.660 & 0.654 & 0.705 & 0.660 & 0.620 
                                 & 0.138 \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Performance comparison of different LLMs with and without knowledge on MedHallu (10,000 samples). General LLMs perform better than medically fine-tuned LLMs in the task of Medical Hallucination across most metrics. “Overall P” denotes precision, and “\(\Delta\) Knowledge” is the performance change in overall F1 when knowledge is provided. $^*$We exclude GPT-4o while calculating the average to have a fair comparison of model sizes for general vs. fine-tuned LLMs. Additional experimental details can be found in Appendix \ref{Gen_robust_check}.}
    \label{tab:llm_performance_merged}
\end{table*}