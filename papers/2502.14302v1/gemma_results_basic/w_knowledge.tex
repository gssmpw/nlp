\begin{table}[h]
\footnotesize  % Make the table smaller
\centering
\setlength{\tabcolsep}{4pt}  % Reduce column spacing
\begin{tabular}{l|c|ccc|c}
\hline
\textbf{Model} & \textbf{Overall} & \textbf{Easy} & \textbf{Med} & \textbf{Hard} & $\Delta$ \\
& \textbf{F1} & \textbf{F1} & \textbf{F1} & \textbf{F1} & \textbf{F1} \\
\hline
GPT-4o mini & \textbf{0.83} & 0.94 & 0.85 & 0.76 & +0.26 \\
Qwen2.5-14B & 0.84 & 0.91 & 0.90 & 0.77 & +0.26 \\
Gemma-9B & 0.82 & 0.92 & 0.87 & 0.76 & +0.23 \\
Qwen2.5-7B & 0.81 & 0.89 & 0.85 & 0.74 & +0.29 \\
Gemma-2B & 0.63 & 0.72 & 0.65 & 0.58 & +0.21 \\
Llama-3.1-8B & 0.79 & 0.89 & 0.82 & 0.73 & +0.35 \\
Llama-3.2-3B & 0.64 & 0.72 & 0.71 & 0.56 & +0.28 \\
Qwen2.5-3B & 0.67 & 0.69 & 0.69 & 0.65 & +0.02 \\
\hline
\end{tabular}
\caption{Performance of LLMs with knowledge. $\Delta$ F1 shows the absolute improvement in Overall F1 score compared to no-knowledge setting. Models are sorted by overall F1 score.}
\label{tab:llm_performance_with_2}
\end{table}
