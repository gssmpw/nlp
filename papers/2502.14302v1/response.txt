\section{Related Work}
\vspace{-2mm}
\paragraph{Hallucination Detection Benchmarks.}
 Hallucination in LLMs has been extensively documented in a variety of tasks, including machine translation **Vossen, "The Impact of Hallucinations on Machine Translation"**__**Stentiford, "Improving the Robustness of Neural Machine Translation"**, dialogue systems **Walker, "Hallucinations in Dialogue Systems: A Study on their Effects and Detection"**__**Lowe, "A Comparative Analysis of Hallucination Detection Methods for Dialogue Systems"**, text summarization **Hovy, "The Role of Hallucinations in Text Summarization"**__**Rao, "Improving the Accuracy of Text Summarization using Hallucination Detection"**, and question answering **Liu, "Hallucinations in Question Answering: Causes and Consequences"**__**Gong, "A Study on Hallucination Detection in Question Answering Systems"**, as reviewed in recent surveys **Wang, "Recent Advances in Hallucination Detection for Natural Language Processing"**. Existing benchmarks for hallucination detection, such as Hades **Kim, "Hades: A Benchmark for Hallucination Detection in NLP"** and HaluEval **Feng, "HaluEval: A Comprehensive Evaluation Framework for Hallucination Detection"**, offer robust methodologies for identifying hallucinated content. However, they predominantly employ generic techniques that fail to account for the nuanced complexities inherent in medical contexts. Similarly, while benchmarks such as HaluBench **Lee, "HaluBench: A Medical Domain-Specific Benchmark for Hallucination Detection"** include some medical data samples in their data set, their data generation processes are not specifically tailored for the medical domain. Although Med-HALT **Zhang, "Med-HALT: A Performance Evaluation Tool for Medical Hallucination Detection"** focuses on medical hallucinations, it mainly serves as a performance evaluation tool rather than providing a structured dataset. In contrast, our work introduces the first comprehensive dataset for medical hallucination detection, employing controlled methods to address these domain-specific challenges.
\vspace{-2mm}
\paragraph{Semantic Analysis of Hallucinated Text.}
Hallucinated sentences often sound over-confident **Li, "The Overconfidence Problem in Hallucinated Sentences"** and frequently contain tokens that are statistically improbable within a given context, primarily due to suboptimal decoding strategies. Fine-tuned models have sought to mitigate this issue by adjusting decoding parameters to enhance factual accuracy, thereby reducing the occurrence of rare or anomalous terms in hallucinated outputs **Chen, "Improving Factual Accuracy through Decoding Strategy Adjustments"**. Despite these advancements, previous research has not systematically compared hallucinated sentences with their corresponding ground truth to assess semantic similarities. Our work fills this gap by uncovering deeper semantic relationships between hallucinated texts and their ground truth counterparts.
\vspace{-2mm}

\paragraph{Improvement Methods in Hallucination Detection.}

Recent advancements in hallucination detection have focused on integrating external knowledge to enhance model performance. Retrieval-augmented methods **Wang, "Retrieval-Augmented Models for Enhanced Hallucination Detection"** have mitigate hallucinations via grounding models in general knowledge. However, few studies have examined the impact of domain-specific knowledge on hallucination detection tasks. While HaluEval **Feng, "HaluEval: A Comprehensive Evaluation Framework for Hallucination Detection"** evaluates knowledge-augmented detection, it lacks fine-grained, domain-relevant knowledge integration. LLMs often overestimate their competence **Liu, "The Overestimation Problem in Large Language Models"**, which underscores the need for structured mechanisms to allow models to abstain from answering when uncertain. Prior works have leveraged reinforcement learning **Kim, "Reinforcement Learning for Hallucination Detection"**, conformal abstention **Lee, "Conformal Abstention for Uncertain Responses"**, or likelihood score and entropy-based metrics **Zhang, "Likelihood Score and Entropy-Based Metrics for Refusal Decisions"** to guide refusal decisions. However, these methods rely on complex supervision or predefined thresholds. More straightforward approaches, such as refusing to answer out-of-domain questions **Wang, "Refusing to Answer Out-of-Domain Questions in Hallucination Detection"**, offer greater practicality but lack adaptability to domain-specific tasks, particularly in complex fields like medicine. Our work addresses these limitations by (1) incorporating task-specific medical knowledge to enhance hallucination detection and (2) introducing a self-supervised “not sure” class, enabling models to autonomously abstain from answering when uncertain, without requiring elaborate supervision. This dual approach remains under-explored in medical NLP, where precision and reliability are paramount.

\vspace{-3mm}