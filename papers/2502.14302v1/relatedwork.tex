\section{Related Work}
\vspace{-2mm}
\paragraph{Hallucination Detection Benchmarks.}
 Hallucination in LLMs has been extensively documented in a variety of tasks, including machine translation~\citep{lee2019hallucinations_13RW}, dialogue systems~\citep{balakrishnan-etal-2019-constrained_14RW}, text summarization~\citep{durmus-etal-2020-feqa_15RW}, and question answering~\citep{sellam2020bleurtlearningrobustmetrics_16RW}, as reviewed in recent surveys~\citep{Ji_2023_12RW}. Existing benchmarks for hallucination detection, such as Hades~\citep{liu2022tokenlevelreferencefreehallucinationdetection} and HaluEval~\citep{Hallueval}, offer robust methodologies for identifying hallucinated content. However, they predominantly employ generic techniques that fail to account for the nuanced complexities inherent in medical contexts. Similarly, while benchmarks such as HaluBench~\citep{ravi2024lynxopensourcehallucination} include some medical data samples in their data set, their data generation processes are not specifically tailored for the medical domain. Although Med-HALT~\citep{pal2023medhaltmedicaldomainhallucination} focuses on medical hallucinations, it mainly serves as a performance evaluation tool rather than providing a structured dataset. In contrast, our work introduces the first comprehensive dataset for medical hallucination detection, employing controlled methods to address these domain-specific challenges.
\vspace{-2mm}
\paragraph{Semantic Analysis of Hallucinated Text.}
Hallucinated sentences often sound over-confident~\citep{miao2021preventlanguagemodeloverconfident_overconfident, chen2022improvingfaithfulnessabstractivesummarization_overconfident} and frequently contain tokens that are statistically improbable within a given context, primarily due to suboptimal decoding strategies. Fine-tuned models have sought to mitigate this issue by adjusting decoding parameters to enhance factual accuracy, thereby reducing the occurrence of rare or anomalous terms in hallucinated outputs~\citep{Huang_2025_survey11}. Despite these advancements, previous research has not systematically compared hallucinated sentences with their corresponding ground truth to assess semantic similarities. Our work fills this gap by uncovering deeper semantic relationships between hallucinated texts and their ground truth counterparts.
\vspace{-2mm}

\paragraph{Improvement Methods in Hallucination Detection.}

Recent advancements in hallucination detection have focused on integrating external knowledge to enhance model performance. Retrieval-augmented methods~\citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp_241, li2023weboysterimprovinglarge_242} have mitigate hallucinations via grounding models in general knowledge. However, few studies have examined the impact of domain-specific knowledge on hallucination detection tasks. While HaluEval~\citep{Hallueval} evaluates knowledge-augmented detection, it lacks fine-grained, domain-relevant knowledge integration. LLMs often overestimate their competence \citep{zhang2023sirenssongaiocean_247}, which underscores the need for structured mechanisms to allow models to abstain from answering when uncertain. Prior works have leveraged reinforcement learning~\cite{xu2024rejectionimprovesreliabilitytraining_246}, conformal abstention~\citep{yadkori2024mitigatingllmhallucinationsconformal_245}, or likelihood score and entropy-based metrics~\citep{cole2023selectivelyansweringambiguousquestions_248} to guide refusal decisions. However, these methods rely on complex supervision or predefined thresholds. More straightforward approaches, such as refusing to answer out-of-domain questions~\citep{cao2024learnrefusemakinglarge_244}, offer greater practicality but lack adaptability to domain-specific tasks, particularly in complex fields like medicine. Our work addresses these limitations by (1) incorporating task-specific medical knowledge to enhance hallucination detection and (2) introducing a self-supervised “not sure” class, enabling models to autonomously abstain from answering when uncertain, without requiring elaborate supervision. This dual approach remains under-explored in medical NLP, where precision and reliability are paramount.

\vspace{-3mm}