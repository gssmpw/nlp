\section{Connections with Prior Observations}
\label{sec:prior_connect}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/dog_horse_hat_3.png}
    \caption{{\bf Composing Entangled Concepts.}
    The left image composes the text-conditions ``photo of a dog''
    with ``photo of a horse'', which both control the subject of the image,
    and produces unexpected results.
    In contrast, the right image composes ``photo of a dog''
    with ``photo, with red hat,'' which intuitively correspond
    to disentangled features.
    Both samples from SDXL using score-composition with
    an unconditional background; details in Appendix~\ref{app:sdxl_detail}.
    }
    \label{fig:dog-horse-hat}
\end{figure}



We have presented a mathematical theory of composition.
Although this theoretical model is a simplification of reality (we do not claim its assumptions hold exactly in practice) we believe the spirit of our results carries over to practical settings,
and can help understand empirical observations from prior work.
We now discuss some of these connections.


\textbf{Independence Assumptions and Disentangled Features.}
Our theory relies on a type of independence 
between distributions, related to orthogonality between scores, which we formalize as Factorized Conditionals.
While such conditional structure typically does not exist in pixel-space,
it is plausible that a factorized structure exists in an appropriate \emph{feature space}, as permitted by our theory (Section~\ref{sec:comp_feature}).
In particular, a feature space and distribution with perfectly ``disentangled'' features
would satisfy our assumptions.
Conversely, if distributions are not appropriately disentangled,
our theory predicts that linear score combinations will fail to compose correctly.
This effect is well-known; see \cref{fig:dog-horse-hat}
for an example;
similar failure cases are highlighted
in \citet{liu2022compositional} as well
(such as ``A bird'' failing to compose with ``A flower'').
Regarding successful cases, style and content compositions
consistently work well in practice,
and are often taken to be disentangled features
(e.g. \citet{karras2019style,gatys2016image,zhu2017unpaired}).
Finally, similar in spirit to our theory, methods for successful composition in practice
such as LoRA task arithmetic \cite{zhang2023composing, ilharco2022editing},
typically require some type of approximate ``concept-space orthogonality''.


\textbf{Text conditioning with location information. }
Conditioning on location is a straightforward way to achieve factorized conditionals (provided the objects in different locations are approximately independent) since the required disjointness already holds in pixel-space. 
Many successful text-to-image compositions in \citet{liu2022compositional} use location information in the prompts, either explicitly (e.g. ``A blue bird on a tree'' + ``A red car behind the tree'')
or implicitly
(``A horse'' + ``A yellow flower field''; since horses are typically in the foreground and fields in the background).

\textbf{Unconditional Backgrounds.}
Most prior works on diffusion composition use the Bayes composition, with substantial practical success. 
As discussed in \cref{sec:clevr-details}, Bayes composition may be approximately projective in ``cluttered'' settings,
helping to explain its practical success in text-to-image settings, where images often contain many different possible objects and concepts.



