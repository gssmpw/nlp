\section{Prior Definitions of Composition}
\label{sec:problematic-compositions}


In this section we will describe why two popular mathematical definitions of composition are insufficient
for our purposes: the ``simple product'' definition, and the Bayes composition.
Specifically, neither of these notions can describe
the outcome of the CLEVR length-generalization experiment from Figure~\ref{fig:len_gen}.
Our observations here will thus motivate us to propose a new definition of composition,
in the following section. As a running example, we will consider a subset of the CLEVR experiment from Figure~\ref{fig:len_gen}.
Suppose we are trying to compose two distributions $p_1, p_2$ of images each containing a single object in an otherwise empty scene, where the object is in the lower-left corner under $p_1$, and the upper-right corner under $p_2$.
We would like the composed distribution $\hat{p}$ to place objects in at least
the lower-left and upper-right, simultaneously.

\subsection{The Simple Product}
The simple product is perhaps the most familiar type of composition:
Given two distributions $p_1$ and $p_2$ over $\R^n$,
the simple product is defined\footnote{The geometric mean $\sqrt{p_1(x)p_2(x)}$ is also often used; our discussion applies equally to this as well.} as
$\hat{p}(x) \propto p_1(x) p_2(x)$.
The simple product can represent some interesting types of composition, but it 
has a key limitation: 
the composed distribution can never be truly ``out-of-distribution'' w.r.t. $p_1$ or $p_2$,
since $\hat{p}(x) = 0$ whenever $p_1(x) =0$ or $p_2(x) = 0$.
This presents a problem for our CLEVR experiment.
Using the simple product definition,
we must have $\hat{p}(x) = 0$ for any image $x$ with two objects,
since neither $p_1$ nor $p_2$ was supported on images with two objects.
Therefore, the simple product definition cannot represent our
desired composition. %

\subsection{The Bayes Composition}
Another candidate definition for composition, which we will call the ``Bayes composition'', was introduced and studied by \citet{du2023reduce,liu2022compositional}.
The Bayes composition is theoretically justified
when the desired composed distribution is formally a conditional distribution
of the model's training distribution.
However, it is not formally capable of generating truly
out-of-distribution samples, as our example below will illustrate.

Let us attempt to apply the Bayes composition methodology to our CLEVR example.
We interpret our two distributions $p_1, p_2$ as conditional distributions,
conditioned on an object appearing in the lower-left or upper-right, respectively.
Thus we write $p(x|c_1) \equiv p_1(x)$, where $c_1$ is the event that
an object appears in the lower-left of image $x$, and
$c_2$ the event an object appears in the upper-right.
Now, since we want both objects simultaneously, we define the composition as
$\hat{p}(x) := p(x | c_1, c_2)$.
Because the two events $c_1$ and $c_2$ are conditionally independent given $x$
(since they are deterministic functions of $x$), we can compute $\hat{p}$
in terms of the individual conditionals:
\begin{align}
    \hat{p}(x) := p(x | c_1, c_2) %
    & \propto p(x|c_1) p(x | c_2) / p(x) .\label{ln:indep}
\end{align}
Equivalently in terms of scores: $\grad_x \log\hat{p}_t(x) := 
\grad_x \log p(x|c_1) + \grad_x \log p(x|c_2) - \grad_x \log{p}(x)$.
Line~\eqref{ln:indep} thus serves as our definition of the Bayes composition $\hat{p}$,
in terms of the conditional distributions $p(x|c_1)$ and $p(x|c_2)$,
and the unconditional $p(x)$.

The definition of composition above seems natural: we want both objects to appear simultaneously,
so let us simply condition on both these events.
However, there is an obvious error in the conclusion:
$\hat{p}(x)$ must be $0$ whenever $p(x|c_1)$ or $p(x|c_2)$ is zero (by Line~\ref{ln:indep}).
Since neither conditional distribution have support on images with two objects,
the composition $\hat{p}$ cannot contain images of two objects either.

Where did this go wrong?
The issue is: 
$p(x | c_1, c_2)$ is not well-defined in our case.
We intuitively imagine some unconditional distribution $p(x)$
which allows both objects simultaneously,
but no such distribution has been defined,
or encountered by the models during training.
Thus, the definition of $\hat{p}$
in Line~\eqref{ln:indep} does not actually
correspond to our intuitive notion of
``conditioning on both objects at once.''
More generally, this example illustrates how the Bayes composition cannot produce truly out-of-distribution samples, with respect to the distributions being composed.\footnote{
Although \citet{du2023reduce} use the Bayes composition
to achieve a kind of length-generalization, our discussion
shows that the Bayes justification does not 
explain the experimental results.}
Figure~\ref{fig:len_gen_monster}b shows that the Bayes composition does
not always work experimentally either:
for diffusion models trained in a CLEVR setting similar to Figure~\ref{fig:len_gen}, the Bayes composition of $k > 1$ locations typically fails to produce $k$ objects (and sometimes produces zero).
The difficulties discussed lead us to propose a
precise definition of what we actually ``want'' composition to do in this case.

