\section{Related Work}
\label{sec:related_work}

\textbf{Single vs. Multiple Model Composition.}
First, we distinguish the kind of composition we study in this paper from 
approaches that rely a single model 
but with OOD conditioners;
for example, passing OOD text prompts to text-to-image models \citep{nichol2021glide, podell2023sdxl},
or works like \citet{okawa2024compositional, park2024emergence}.
In contrast, we study compositions which recombine the outputs of
multiple separate models at inference time, where each model only sees
in-distribution conditionings.
Among compositions involving multiple models,
many different variants have been explored. %
Some are inspired by logical operators like AND and OR,
which are typically implemented as
product $p_0(x)p_1(x)$ and sum $p_0(x) + p_1(x)$ \citep{du2023reduce,du2024compositional,liu2022compositional}.
Some composition methods are based on diffusion models, 
while others use energy-based models \citep{du2020visualenergy, du2023reduce, liu2021learning} or densities \cite{skreta2024superposition}. In this work, we focus specifically on product-style compositions %
implemented with diffusion models via a linear combinations of scores as in \citet{du2023reduce, liu2022compositional}. Our goal is not to propose
a new method of composition but to improve theoretical understanding of existing methods.

\textbf{Learning and Generalization.}
In this work we focus only on mathematical aspects
of composition,
and we do not consider any learning-theoretic aspects
such as inductive bias or sample complexity.
Our work is thus complementary to \citet{kamb2024analytic},
which studies how a type of compositional generalization
can arise from inductive bias in the learning procedure.
Additional related works are discussed in Appendix~\ref{app:related}.
