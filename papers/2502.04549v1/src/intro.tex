\begin{figure}[!ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/papaya_vangogh.png}}
\caption{Composing diffusion models via score combination.
Given two diffusion models, it is sometimes
possible to sample in a way that
composes content from one model (e.g. your dog) 
with style of another model (e.g. oil paintings). We aim to theoretically understand this empirical behavior.
Figure generated via score composition
with SDXL fine-tuned on the author's dog; details in Appendix~\ref{app:sdxl_detail}.
}
\label{fig:style-content}
\end{center}
\vskip -0.2in
\end{figure}
\begin{figure}[!htb]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/len-gen.png}}
\caption{Length-generalization, another capability of composition enabled by our framework. Diffusion models trained to generate a single object conditioned on location (left) can be composed at inference-time to generate images
of multiple objects at specified locations (right).
Notably, such images are strictly out-of-distribution
for the individual models being composed. (Additional samples in Figure \ref{fig:len_gen_extra}.)
}
\label{fig:len_gen}
\end{center}
\vskip -0.3in
\end{figure}

\section{Introduction}

The possibility of \emph{composing} different concepts represented by pretrained models has been of both theoretical and practical interest for some time \citep{jacobs1991adaptive, hinton2002training, du2024compositional}, with diverse applications including image and video synthesis \cite{du2023reduce, du2020visualenergy, liu2022compositional, liu2021learning, nie2021controllable, yang2023probabilistic, wang2024concept}, planning \cite{ajay2024compositional, janner2022planning}, constraint satisfaction \citep{yang2023compositional}, parameter-efficient training \citep{hu2021lora, ilharco2022editing}, and many others \citep{wu2024compositional, su2024decomposition, urain2023composable, anonymous2024scaling}.
One central goal in this field is to build novel compositions at inference time using only the outputs of pretrained models (either entirely separate models, or different conditionings of a single model), to create generations that are potentially more complex than any model could produce individually.
As a concrete example to keep in mind, suppose we have two diffusion models,
one trained on your personal photos of your dog and another trained on a collection of oil paintings,
and we want to somehow combine these to generate oil paintings of your dog.
Note that in order to achieve this goal,
compositions must be able to generate images that are out-of-distribution (OOD) with respect to each of the individual models, since for example,
there was no oil painting of your dog in either model's training set.
Prior empirical work has shown that this ambitious vision is at least partially achievable in practice.
However, the theoretical foundations of how and why composition works in practice, as well as its limitations, are still incomplete.



The goal of this work is to advance our theoretical understanding of composition---
we will take a specific family of methods used for composing diffusion models,
and we will analyze conditions under which this method 
provably generates the ``correct'' composition.
Specifically, are there sufficient properties of the distributions we are composing that can guarantee that composition will work ``correctly''?
And what does correctness even mean, formally?



We focus our study on
composing diffusion models by linearly combining their scores,
a method introduced by \citet{du2023reduce, liu2022compositional}
(though many other interesting constructions are possible, see Section \ref{sec:related_work}).
Concretely, suppose we have three separate diffusion models, one for the
distribution of dog images $p_{dog}$,
another for oil-paintings $p_{oil}$,
and another unconditional model for generic images $p_u$.
Then, we can use the individual score estimates $\grad_x \log p(x)$ 
given by the models to construct a composite score:
{\begin{align}
    \label{eq:intro-style-content}
    \grad_x &\log \hat p(x) := \\
    &\grad_x \log p_{dog}(x) + \grad_x \log p_{oil}(x) - \grad_x \log p_u(x).\notag
\end{align}
}%
This implicitly defines a distribution which we will call a ``product composition'':
$\hat{p}(x) \propto p_{dog}(x)p_{oil}(x)/p_u(x)$.
Finally, we can try to sample from $\hat{p}$
by using these scores with a generic score-based sampler,
or even reverse-diffusion.
This method of composition often achieves good results in practice,
yielding e.g. oil paintings of dogs,
but it is unclear why it works theoretically.


We are particularly interested in the OOD generalization capabilities of this style of composition. By this we mean the compositional method's ability to generate OOD with respect to each of the individual models being composed -- which may be possible even if none of the individual models are themselves capable of OOD generation.
A specific desiderata is \emph{length-generalization}, understood as the ability to compose arbitrarily many concepts.
For example, consider the CLEVR \citep{johnson2017clevr} setting shown in Figure \ref{fig:len_gen}.
Given conditional models trained on images each containing a single object
and conditioned on its location,
we want to generate images containing $k > 1$ objects composed in the same scene.
How could such length-generalizing composition be possible?
Here is one illustrative toy example---
consider the following construction, inspired by but slightly different from \citet{du2023reduce, liu2022compositional}.
Suppose $p_b$ is a distribution of empty background images,
and each $p_i$ a distribution of images with a single object at location $i$,
on an otherwise empty background. Assume all locations we wish to compose are non-overlapping.
Then, performing reverse-diffusion sampling using the following score-composition will work
--- meaning will produce images with $k$ objects at appropriate locations:
\begin{align}
    \label{eq:product_comp}
    \grad_x \log p^t_b(x)
    + \sum_{i=1}^k \underbrace{\left( \grad_x \log p^t_i(x) - \grad_x \log p^t_b(x) \right)}_{\textrm{score delta $\delta_i \in \R^n$}}.
\end{align}
Above, the notation $p_i^t$ denotes the
distribution $p_i$
after time $t$ in the forward diffusion
process (see Appendix \ref{app:samplers}).
Intuitively this works because during the reverse-diffusion process,
the update performed by model $i$
modifies only pixels in the vicinity of location $i$, and otherwise leaves them identical to the background.
Thus the different models do not interact, and the sampler acts as if each model individually ``pastes''
an object onto an empty background.
Formally, sampling works
because the score delta vectors $\delta_i$ are mutually orthogonal, and in fact have disjoint supports.
Notably, we can sample from this composition with 
a \emph{standard diffusion sampler}, in contrast to \citet{du2023reduce}'s observations that more sophisticated samplers are necessary.
This construction would not be guaranteed to work, however, if the ``background'' $p_b$
was chosen to be the unconditional distribution $p_u$
(as in Equation~\ref{eq:intro-style-content}),
a common choice in many prior works \citep{du2023reduce, liu2022compositional}.

The remainder of this paper is devoted to trying to generalize this example as far as possible,
and understand both its explanatory power and its limitations.
It turns out the core mechanism can be generalized surprisingly far, 
and does not depend on ``orthogonality'' as strongly as the above example may suggest.
We will encounter some subtle aspects along the way, starting from 
formal definitions
of what it means for composition to succeed ---
a definition that
can capture both composing objects (as in Figure~\ref{fig:len_gen}),
and composing other attributes (such as style + content, in Figure~\ref{fig:style-content}).







\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{
\includegraphics[width=1.0\textwidth]{figures/len_gen_monster_3.png}
}
\caption{\textbf{Attempted compositional length-generalization up to 9 objects.} We attempt to compose via linear score combination the distributions $p_1$ through $p_9$ shown on the far left, where each $p_i$ is conditioned on a specific object location
as described below.
Settings (A) and (C) approximately satisfy the conditions of our theory of projective composition, and thus are expected to length-generalize at least somewhat, while setting (B) does not even approximately satisfy our conditions and indeed fails to length-generalize. 
{\bf Experiment (A):} In this experiment, the distributions $p_i$ each contain a single object at a fixed location, and the background $p_b$ is empty. In this case any successful composition of more than one object represents length-generalization.
We find that composition succeeds up to several objects,
but then degrades as number of objects increases
(see \cref{sec:clevr-details} for details). 
{\bf Experiment (B):} Here the distributions $p_i$ are identical to (A),
but the background $p_b$ is chosen
as the unconditional distribution
(i.e. a single object at a random location)--- this the ``Bayes composition'' (\cref{sec:problematic-compositions}).
This composition entirely fails--- remarkably, trying
to compose many objects often produces no objects!
{\bf Experiment (C):}
Here each distribution $p_i$ contains
an object at a fixed location $i$, and $0-4$ other objects
(sampled uniformly) in random locations; see samples at far left.
The background distribution $p_b$ is a distribution of $1-5$ objects (sampled uniformly) in random locations.
In this case length-generalization means composition of more than 5 objects.
This composition can length-generalize,
but artifacts appear for large numbers of objects. %
See \cref{sec:clevr-details} for a full discussion.}
\label{fig:len_gen_monster}
\end{center}
\vskip -0.2in
\end{figure*}



\subsection{Contributions and Organization}
In this work we introduce a theoretical framework
to help understand 
the empirical success of certain methods of 
composing diffusion models,
with emphasis on
understanding how compositions can sometimes
length-generalize. 
We start by discussing the limitations
of several prior definitions of composition
in Section~\ref{sec:problematic-compositions}.
In Section~\ref{sec:composition} we offer a formal definition of
``what we want composition to do'',
given precise information about which aspects we want to compose, which we call \emph{Projective Composition} (Definition~\ref{def:proj_comp}).
(Note that there are many other valid notions of composition;
we are merely formalizing one particular goal.)
Then,
we study how projective composition can be achieved.
In Section~\ref{sec:comp_coord} we introduce formal criteria called
\emph{Factorized Conditionals} (Definition~\ref{def:factorized}),
which is a type of independence criteria along both distributions and coordinates. We prove that when this criteria holds, 
projective composition
can be achieved by linearly combining scores (as in Equation~\ref{eq:product_comp}),
and can be sampled via standard reverse-diffusion samplers.
In Section~\ref{sec:comp_feature} we show that parts of this result can be extended much further to apply even in
nonlinear feature spaces; but interestingly, even when projective composition is achievable, it may be difficult to sample.
We find that in many important cases existing constructions approximately satisfy our conditions, but the theory also helps characterize and explain certain limitations.
Finally in Section~\ref{sec:prior_connect} we discuss how our results can help explain existing experimental results in the literature where composition worked or failed, for reasons that were unclear at the time.










