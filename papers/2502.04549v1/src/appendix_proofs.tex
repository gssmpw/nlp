\section{Proof of \cref{lem:compose}}
\label{app:compose_pf}
\begin{proof} (\cref{lem:compose})
For any set of distributions $\vec{q} = (q_b, q_1, q_2, \ldots)$ satisfying Definition \ref{def:factorized}, we have
\begin{align}
\cC[\vec{q}](x) &:= q_b(x) \prod_i \frac{q_i(x)}{q_b(x)} = q_b(x) \prod_i \frac{q_b(x_t|_{M_i^c}) q_i(x|_{M_i})}{q_b(x|_{M_i^c})q_b(x|_{M_i})} \notag \\
&= q_b(x) \prod_i \frac{q_i(x|_{M_i})}{q_b(x|_{M_i})} 
= q_b(x|_{M_b}) \prod_i q_i(x_t|_{M_i})
\label{eq:comp_indep}
\end{align}
(where we used \eqref{eqn:cc-cond} in the second equality). 
Since $(p_b, p_1, p_2, \ldots)$ satisfy Definition \ref{def:factorized} by assumption, applying \eqref{eq:comp_indep} gives
$$\cC[\vec{p}](x) = p_b(x|_{M_b}) \prod_i p_i(x|_{M_i}) := \hat{p}(x),$$
so the composition at $t=0$ is projective, as desired.
Now to show that reverse-diffusion sampling with the compositional scores generates $\cC[\vec{p}]$, we need to show that
$$\cC[\vec{p^t}]= N_t[\cC[\vec{p}]],$$ 
where $p^t := N_t[p]$ denotes the $t$-noisy version of distribution $p$ under the forward diffusion process.
First, notice that if $\vec{p}$ satisfies Definition \ref{def:factorized}, then $\vec{p^t}$ does as well. This is because the diffusion process adds Gaussian noise independently to each coordinate, and thus preserves independence between sets of coordinates.
Therefore by \eqref{eq:comp_indep}, we have
$\cC[\vec{p^t}](x)= p^t_b(x|_{\bar{M}}) \prod_i p^t_i(x_t|_{M_i}).$
Now we apply the same argument (that diffusion preserves independent sets of coordinates) once again, to see that $\cC[\vec{p^t}]= N_t[\cC[\vec{p}]],$
as desired.
\end{proof}

\section{Parameterization-Independent Compositions and Proof of Lemma \ref{lem:transform_comp}}
\label{app:param-indep}

The proof of Lemma \ref{lem:transform_comp} relies on certain general fact about parametrization-independence of certain operators, which we develop here.

Suppose we have an operator that 
takes as input two probability distributions $(p, q)$
over the same space $\cX$,
and outputs a distribution over $\cX$.
That is, $F: \Delta(\cX)\x\Delta(\cX) \to \Delta(\cX)$.
We can think of such operators as performing some kind of ``composition''
of $p, q$.

Certain operators are \emph{independent of parameterization}, meaning
for any reparameterization of the base space $A: \cX \to \cY$, we have
\[
F(p, q) = A^{-1}\sharp( F(A \sharp p, A \sharp q) )
\]
or equivalently:
\[
F(A \sharp p, A \sharp q) = A \sharp F(p, q),
\]
where $\sharp$ is the pushforward:
\[ (\mathcal{A} \sharp p)(z) := \frac{1}{\left|\grad \mathcal{A}\right|} p(\mathcal{A}^{-1}(z)). \]

This means that reparameterization commutes with the operator:
it does not matter if we first reparameterize, then compose, or first compose, then reparamterize. A few examples:
\begin{enumerate}
    \item The pointwise-geometric median, $F(p, q)(x) := \sqrt{p(x)q(x)}$, is independent of reparameterization:
    \item Squaring a distribution, $F(p, q)(x) := p(x)^2$, is NOT independent of reparameterization:
    \item The ``CFG composition'' \citep{ho2022classifier}, $F(p, q)(x) := p(x)^\gamma q(x)^{1 - \gamma}$, is independent of reparameterization:
\end{enumerate}

We can analogously define parametrization-independence for operators on more than 2 distributions.
Notably, given a tuple of distributions $\vec{p} = (p_b, p_1, p_2, \dots, p_k)$,
our composition operator $\cC$ of Definition \ref{def:comp_oper}, $\cC[\vec{p}] \propto p_b(x) \prod_i \frac{p_i(x)}{p_b(x)}$ 
is independent of parameterization.

\begin{lemma}[Parametrization-independence of 1-homogeneous operators]
    If an operator $F$ is 1-homogeneous, i.e. $F(tp, tq, \ldots) = t F(p, q, \ldots)$ and operates pointwise, then it is independence of parametrization.
    \label{lem:1-homog-param-indep}
\end{lemma}

\begin{proof}
\begin{align*}
    F(\mathcal{A} \sharp p, \mathcal{A} \sharp q, \ldots)(z) &= F(\mathcal{A} \sharp p(z), \mathcal{A} \sharp q(z), \ldots), \quad \text{pointwise} \\
    &= F \left( \frac{1}{\left|\grad \mathcal{A}\right|} p(\mathcal{A}^{-1}(z)), \frac{1}{\left|\grad \mathcal{A}\right|} q(\mathcal{A}^{-1}(z)), \ldots \right) \\
    &= \frac{1}{\left|\grad \mathcal{A}\right|}F\left(p(\mathcal{A}^{-1}(z)), q(\mathcal{A}^{-1}(z)), \ldots \right), \quad \text{1-homogeneous} \\
    &= \mathcal{A} \sharp F(p, q, \ldots)(z)
\end{align*}
\end{proof}

\begin{corollary}[Parametrization-invariance of composition]
    The composition operator $\cC$ given by Definition \ref{def:comp_oper} is independent of parametrization.
    \label{corr:comp-param-indep}
\end{corollary}
\begin{proof}
    The composition operator given by Definition \ref{def:comp_oper} is 1-homogeneous:
    \begin{align*}
        \cC(tp_b, tp_1, tp_2, \ldots)(x) &= tp_b(x) \prod_i \frac{tp_i(x)}{tp_b(x)} 
        = tp_b(x) \prod_i \frac{p_i(x)}{p_b(x)} 
        = t\cC(p_b, p_1, p_2, \dots)(x)
    \end{align*}
    and so the result follows from Lemma \ref{lem:1-homog-param-indep}.
    Alternatively, a direct proof is:
    \begin{align*}
    \cC(p_b, p_1, p_2, \ldots)(x) &:= p_b(x) \prod_i \frac{p_i(x)}{p_b(x)} \\
    \cC(\mathcal{A} \sharp p_b, \mathcal{A} \sharp p_1, \mathcal{A} \sharp p_2, \ldots)(z) 
    &= (\mathcal{A} \sharp p_b)(z) \prod_i \frac{(\mathcal{A} \sharp p_i)(z)}{(\mathcal{A} \sharp p_b)(z)} 
    = \frac{1}{\left|\grad \mathcal{A}\right|} p_b(\mathcal{A}^{-1}(z)) \prod_i \frac{p_i(\mathcal{A}^{-1}(z))}{p_b(\mathcal{A}^{-1}(z))} 
    = \mathcal{A} \sharp \cC(p_b, p_1, p_2, \ldots)(z).
    \end{align*}
\end{proof}


\cref{lem:transform_comp} follows from \cref{corr:comp-param-indep}:
\begin{proof} (\cref{lem:transform_comp})
Let $(q_b, q_1, q_2, \dots, q_k) := (\cA \sharp p_b, \cA \sharp p_1, \dots \cA\sharp p_k)$, for which Definition~\ref{def:factorized} holds by assumption. Applying an intermediate result from the proof of Theorem~\ref{lem:compose} gives:
\begin{align*}
    \cC[\vec{q}](z) &:= q_b(z) \prod_i \frac{q_i(z)}{q_b(z)} 
    = q_b(z|_{\bar{M}}) \prod_i q_i(z|_{M_i}).
\end{align*}
By Corollary \ref{corr:comp-param-indep}, $\cC$ is independent of parametrization, hence 
$$ \cA \sharp \hat{p} := \cA \sharp (\cC[\vec{p}]) = \cC[\vec{\cA \sharp p}] := \cC(\vec{q}).$$
\end{proof}

\section{Proof of \cref{lem:orthogonal_sampling}}
\label{app:orthog_sample_pf}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{
\includegraphics[width=0.5\columnwidth]{figures/orthog_sampling.png}
}
\caption{Synthetic composition experiment illustrating the sampling guarantees of \cref{lem:orthogonal_sampling} in contrast to the lack-of-guarantees in the non-orthogonal case. We compare a coordinate-aligned case (which satisfies \cref{def:factorized} in the native space) (top), an orthogonal-transform case (middle) (which satisfies the assumptions of \cref{lem:orthogonal_sampling}), and a non-orthogonal-transform case (bottom) (which satisfies the assumptions of \cref{lem:transform_comp} but not of \cref{lem:orthogonal_sampling}). In the first two cases the correct composition can be sampled using either diffusion (DDIM) or Langevin dynamics (LD) at $t=0$, while in the final case DDIM sampling is unsuccessful although LD at $t=0$ still works.
The distributions are 4-dimensional and we show 8 samples (rows) for each. We show samples from the individual conditional distributions $p_0, p_1$ using DDIM, samples from the desired exact composition $\cC[p_b, p_0, p_1]$ at $t=0$ (obtained by sampling from $\cA \sharp \cC[\vec{p}]$ with DDIM and transforming by $\cA^{-1}$), samples from the composition $\cC[p_b, p_0, p_1]$ using DDIM with exact scores, and samples from the composition $\cC[p_b, p_0, p_1]$ using Langevin dynamics (LD) with exact scores at time $t=0$ in the diffusion schedule ($\sigma_{\min} = 0.02$). 
The noiseless distributions $p_0$ and $p_1$ are each 4-dimensional 2-cluster Gaussian mixtures with means as noted in the figure, equal weights, and standard deviation $\tau = 0.02$. For example, in the non-orthogonal-transform case, $p_0$ has means $[1, 0, 0, 0]$ and $[0, 0, 1, 0]$, and $p_1$ has means $[1, 1, 0, 0]$ and $[0, 0, 1, 1]$, (which can be transformed to satisfy \cref{def:factorized} via a non-orthogonal linear transform).}
\label{fig:orthog_sampling}
\end{center}
\vskip -0.2in
\end{figure}

Figure \ref{fig:orthog_sampling} shows a synthetic experiment illustrating the sampling guarantees of \cref{lem:orthogonal_sampling} in contrast to the lack-of-guarantees in the non-orthogonal case.

The proof of \cref{lem:orthogonal_sampling} relies on the fact that diffusion noising commutes with orthogonal transformation, i.e. $\cA \sharp N_t[q] = N_t[\cA \sharp q]$ if $\cA$ is orthogonal, since standard Gaussians are invariant under orthogonal transformation.

\begin{proof}(\cref{lem:orthogonal_sampling})
By assumption, $(\cA \sharp p_b, \cA \sharp p_1, \dots \cA\sharp p_k)$ satisfy Definition~\ref{def:factorized}, where $\cA(z) = Az$ with $A$ an orthonormal matrix. By Lemma \ref{lem:transform_comp}, $\hat{p} = \cC[\vec{p}]$ satisfies \eqref{eqn:p_hat_A}. To show that reverse-diffusion sampling with scores $s_t = \grad_x \log \cC[\vec{p}^t]$ generates the composed distribution $\cC[\vec{p}]$ we need to show that composition commutes with the forward diffusion process, i.e.
$$\cC[\vec{p^t}] = N_t[\cC[\vec{p}]].$$ 
Theorem~\ref{lem:compose} immediately gives us
$$ \cC[N_t[\cA \sharp p]] = N_t[\cC[\cA \sharp p]]. $$
Now we have to be careful with commuting operators. We know that composition is independent of parametrization, i.e. $\cA \sharp \cC[\vec{p}] = \cC[\vec{\cA \sharp p}]$. Diffusion noising $N_t$ commutes with orthogonal transformation, i.e. $\cA \sharp N_t[q] = N_t[\cA \sharp q]$ if $\cA$ is orthogonal, because a standard Gaussian multiplied by an orthonormal matrix $Q$ remains a standard Gaussian:  $\eta \sim \cN(0, I) \implies Q\eta \sim \cN(0, QQ^T) = \cN(0, I)$ (this is false for non-orthogonal transforms, however). Therefore, in the orthogonal case, we can rewrite:
$$ \cA \sharp  \cC[N_t[p]] = \cA \sharp  N_t[\cC[p]], $$
which implies the desired result since $\cA$ is invertible.
\end{proof}



\section{Proof and further discussion of Lemma \ref{lem:lipschitz}}
\label{app:lipschitz}

\subsection{Benefits of sampling at $t=0$}
\label{app:hmc}
Interestingly, \cite{du2023reduce} have observed that sophisticated samplers like Hamiltonian Monte Carlo (HMC) requiring energy-based formulations often outperform standard diffusion sampling for compositional quality. Lemmas \ref{lem:transform_comp} and \ref{lem:lipschitz} help explain why this may be the case. In particular, HMC (or any variant of Langevin dynamics) can enable sampling $p^0$ at time $t=0$, even when the path $p^t$ used for annealing does not necessarily represent a valid forward diffusion process starting from $p^0$ (as \citet{du2023reduce} note, $\cC[\vec{p}^t]]$ may not be). Lemma \ref{lem:transform_comp} should gives us hope that approximately-projective composition may often be possible at $t=0$, since it allows \emph{any} invertible transform $\cA$ to transform into a factorized feature space (which need not be explicitly constructed). However, that does not mean that we can actually \emph{sample} from this projection at time $t=0$. As Lemma \ref{lem:lipschitz} shows, $\cC[\vec{p}^t]]$ is not necessarily a valid diffusion path unless $\cA$ is orthogonal, so standard diffusion sampling may not work. This is consistent with \citet{du2023reduce}'s observation that non-diffusion samplers that allow sampling at $t=0$ may be necessary. Interestingly, Lemma \ref{lem:lipschitz} further cautions that sometimes $\cC[\vec{p}^t]]$ may not even be an effective annealing path for any kind of sampler (which is consistent with our own experiments but not reported by other works, to our knowledge.)

\subsection{Proof of Lemma \ref{lem:lipschitz}}
\label{app:lipschitz_proof}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{
\includegraphics[width=0.45\columnwidth]{figures/lipschitz_pf.png}
\includegraphics[width=0.55\columnwidth]{figures/yellow.png}
}
\caption{(Left) A visualization of the intuition behind the proof of Lemma \ref{lem:lipschitz}, under a 2D projection. (Right) An experiment where the colors red, green, and blue all compose projectively, while the colors red and yellow do not. We trained a Unet on images each containing a single square in one of 4 locations (selected randomly) and a certain color, conditioned on the color. We then generate composed distributions by running DDIM on the composed scores. The desired result of composing red and blue is an image containing a red and a blue square, both with randomly-chosen locations (so we occasionally get a purple square when the locations overlap). When we try to compose red and yellow, we only only ever obtain a single yellow square.Note that in pixel space, the colors are represented as red $(1,0,0)$, green $(0,1,0)$, blue $(0,0,1)$, yellow $(1,1,0)$, so that red, green and blue are all orthogonal and are expected to work by Lemma \ref{lem:compose}, while red and yellow are not orthogonal, and fail as allowed by Lemma \ref{lem:lipschitz}. In fact this experiment is closely related to the counterexample used to prove Lemma \ref{lem:lipschitz}.}
\label{fig:yellow}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{
\includegraphics[width=1.0\columnwidth]{figures/eg3.png}
}
\caption{Composition experiments for the setting in the proof of Lemma \ref{lem:lipschitz}. Left pane shows 8 samples (rows) of each distribution in the native 4d representation; right pane shows 1000 samples under the 2D projection used in Figure \ref{fig:yellow}. We show samples from the individual conditional distributions $p_0, p_1$ using DDIM, samples from the desired exact composition $\cC[p_b, p_0, p_1]$ at $t=0$ (obtained by sampling from $\cA \sharp \cC[\vec{p}]$ with DDIM and transforming by $\cA^{-1}$), and samples from the composition $\cC[p_b, p_0, p_1]$ using DDIM with exact scores.  We take $\tau = 0.02$, and set $\sigma_{\min} = 0.02$ in the diffusion schedule. In the top row we take $a=1$ (``very non-orthogonal'') as in the proof, and compare this to $a=0.3$ (``mildly non-orthogonal'') in the bottom row. With $a=1$, as in the proof we see that DDIM barely samples two of the clusters. With $a=0.3$, DDIM still slightly undersamples the ``hard'' clusters but the effect is much less pronounced.}
\label{fig:eg3}
\end{center}
\vskip -0.2in
\end{figure}

We will prove Lemma \ref{lem:lipschitz} using a counterexample, which is inspired by an experiment, shown in Figure \ref{fig:eg3} (left), where non-orthogonal conditions fail to compose projectively.

The basic idea for the counterexample is that given a distribution $p(x)$ with two conditions, $c=0,1$, such at $t=0$, 
\begin{align*}
    p_0(x) &\approx \half \delta_{e_0}(x) + \half \delta_{e_2}(x), \quad \quad \quad
    p_1(x) \approx \half \delta_{ae_0 + e_1}(x) + \half \delta_{ae_2 + e_3}(x),
\end{align*}
for some $0 < a \le 1$,
so the conditional distributions do not satisfy the independence assumption of Definition \ref{def:factorized},
However, there exists a (linear, but not orthogonal) $A$ such that the distribution of $z = Ax$ is axis-aligned
\begin{align*}
    (A \sharp p_0)(z) &\approx \half \delta_{e_0}(x) + \half \delta_{e_2}(x), \quad \quad \quad
    (A \sharp p_1)(z) \approx \half \delta_{e_1}(x) + \half \delta_{e_3}(x),
\end{align*}
and thus does satisfy Definition \ref{def:factorized} at $t=0$, which guarantees correct composition of $p$ at $t=0$ under Lemma \ref{lem:transform_comp}. The correct composition should sample uniformly from
$ \{ (1+a)e_0 + e_1, \quad e_0 + a e_2 + e_3, \quad ae_0 + e_2 + e_1, \quad (1+a) e_2 + e_3 \}$.
What goes wrong is that as soon as we add Gaussian noise to the distribution $p(x)$ at time $t > 0$ of the diffusion forward process, the relationship $z = Ax$ breaks and so we are no longer guaranteed correct composition of $p^t(x)$. In fact, the distribution is still a GMM but places nearly all its weight on only two of the four clusters, namely: 
$ \{ (1+a)e_0 + e_1, (1+a) e_2 + e_3 \}.$
Intuitively, let us focus on the mode $ae_0 + e_1$ of $p_1$ and consider how it interacts with the two modes $e_0, e_2$ of $p_0$, at some time $t > 0$ when we have isotropic Gaussians centered at each mode. 
Since $ae_0 + e_1$ is further away from $e_2$ (distance $\sqrt{a^2+2}$) than it is from $e_0$ (distance $\sqrt{a^2-2a+2})$, it is much less likely under $\cN(e_2, \sigma_t)$ than $\cN(e_0, \sigma_t)$, leading to a lower weight. This intuition is shown graphically in a 2D projection in Figure \ref{fig:yellow} (left).

For the detailed proof, we actually want to ensure that $p$ has full support even at $t=0$ so we add a little bit of noise to it, but choose the covariance such that $z=Ax$ still holds at $t=0$.

\begin{proof} (\cref{lem:lipschitz})
Define
\begin{align*}
    p_0^0(x) &= \half \cN(x; e_0, \tau^2 (A^TA)^{-1}) + \half \cN(x; e_2, \tau^2 (A^TA)^{-1}) \\
    p_1^0(x) &= \half \cN(x; ae_0 + e_1, \tau^2 (A^TA)^{-1}) + \half \cN(x; ae_2 + e_3, \tau^2 (A^TA)^{-1}) \\
    p_b^0(x) &= \cN(x; 0, \tau^2 (A^TA)^{-1}),
    \quad \text{where }
    A := \begin{bmatrix}
        1 & -a & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & -a \\
        0 & 0 & 0 & 1
    \end{bmatrix},
\end{align*}
so that in the transformed space:
\begin{align*}
    (A \sharp p)(z) &:= p(A^{-1} z), \quad z = Ax \\
    (A \sharp p_b^0)(z) &= \cN(z; 0, \tau^2) \\
    (A \sharp p_0^0)(z) &= \half \cN(z; e_0, \tau^2) + \half \cN(z; e_2, \tau^2) \\ 
    (A \sharp p_1^0)(z) &= \half \cN(z; e_1, \tau^2) + \half \cN(z; e_3, \tau^2).
\end{align*}
Therefore Lemma \ref{lem:transform_comp} implies that at time $t=0$,
\begin{align*}
    \hat \cC[\vec{p}] &:= \frac{p_0^0(x)p_1^0(x)}{p_b^0(x)} = p_0^0(x|_{(0,2)})p_1^0(x|_{(1,3)}).
\end{align*}
When we add noise at time $t > 0$ we get:
\begin{align*}
    p_i^t(x_t|x_0) &:= \cN(x_t; x_0, \sigma_t^2) \\
    p_0^t(x) &= \half \cN(x_t; e_0, \sigma_t^2 I + \tau^2 (A^TA)^{-1}) + \half \cN(x; e_2, \sigma_t^2 I + \tau^2 (A^TA)^{-1}) \\
    p_1^t(x) &= \half \cN(x_t; ae_0 + e_1, \sigma_t^2 I + \tau^2 (A^TA)^{-1}) + \half \cN(x; ae_2 + e_3, \sigma_t^2 I + \tau^2 (A^TA)^{-1}) \\
    &= \half A^{-1} (\cN(x_t; e_1, \sigma_t^2 A^TA + \tau^2 I) + \cN(x; e_3, \sigma_t^2 I A^TA + \tau^2)).
\end{align*}



We will start by using this counterexample to prove Part 2 of Lemma \ref{lem:lipschitz}, which is the hard part. Note that
$\hat p^t(x)$ is made up of terms of the following form:
\begin{align*}
    \frac{\cN(x; \mu_1; C) \cN(x; \mu_2, \Sigma)}{\cN(x; 0; \Sigma)} 
    &= (2\pi)^{-\frac{n}{2}} |\Sigma|^{-\half} \frac{e^{-\half (x-\mu_1)^T \Sigma^{-1} (x-\mu_1)} e^{-\half (x-\mu_2)^T \Sigma^{-1} (x-\mu_2)} }{e^{-\half x^T\Sigma^{-1}x}} \\
    &= (2\pi)^{-\frac{n}{2}} |\Sigma|^{-\half} \exp \left( -\half (x-\mu_1)^T \Sigma^{-1} (x-\mu_1) -\half (x-\mu_2)^T \Sigma^{-1} (x-\mu_2) + \half x^T\Sigma^{-1}x \right) \\
    &= (2\pi)^{-\frac{n}{2}} |\Sigma|^{-\half} \exp \left( -\half x^T\Sigma^{-1}x + x^T \Sigma^{-1} (\mu_1 + \mu_2) - \half \mu_1^T \Sigma^{-1} \mu_1 - \half \mu_2^T \Sigma^{-1} \mu_2 \right) \\
    &= C (2\pi)^{-\frac{n}{2}} |\Sigma|^{-\half} \exp \left( -\half (x - \mu_1 - \mu_2)^T \Sigma^{-1}  (x - \mu_1 - \mu_2) \right) \\
    &= C \cN(x; \mu_1 + \mu_2, \Sigma) \\
    C &= \exp \left( - \half \mu_1^T \Sigma^{-1} \mu_1 - \half \mu_2^T \Sigma^{-1} \mu_2 + \half (\mu_1 + \mu_2)^T \Sigma^{-1} (\mu_1 + \mu_2)   \right) \\
    &= \exp(\mu_1^T \Sigma^{-1} \mu_2)
\end{align*}


Noting that
\begin{align*}
    \tilde{\Sigma_t} &:= \sigma_t^2 I + \tau^2 (A^TA)^{-1} = \sigma_t^2 I + \tau^2
    \begin{bmatrix}
        1+a^2 & a & 0 & 0 \\
        a & 1 & 0 & 0 \\
        0 & 0 & 1+a^2 & a \\
        0 & 0 & a & 1
    \end{bmatrix} \\ 
    \\
    \tilde{\Sigma_t}^{-1} &= \frac{1}{(a^2 + 2) \sigma_t^2 \tau^2 + \sigma_t^4 + \tau^4} \begin{bmatrix}
        \sigma_t^2 + \tau^2 & -a \tau^2 & 0 & 0 \\
        -a \tau^2 & (a^2 + 1)\tau^2 + \sigma_t^2 & 0 & 0 \\
        0 & 0 & \sigma_t^2 + \tau^2 & -a \tau^2 \\
        0 & 0 & -a \tau^2 & (a^2 + 1) \tau^2 + \sigma_t^2
    \end{bmatrix},
\end{align*}

after some algebra we find that $\hat p^0(x)$ and $\hat p^t(x)$ are both GMMs with same means:
$$ \vec{\mu} = \{(1+a)e_0 + e_1, \quad e_0 + a e_2 + e_3, \quad ae_0 + e_2 + e_1, \quad (1+a) e_2 + e_3\}, $$
different variances ($\tilde{\Sigma_0} = \tau^2 (A^TA)^{-1}$ and $\tilde{\Sigma_t}$ for all clusters, respectively),
and different weights, as follows:
\begin{align*}
    \hat p^0(x): \quad w^0 &= \frac{1}{4} [ 1, 1, 1, 1 ]  \\
    \hat p^t(x): \quad w^t &\propto [M, 1, 1, M], \quad M := \exp \left( \frac{a\sigma_t^2}{(a^2 + 2) \sigma_t^2 \tau^2 + \sigma_t^4 + \tau^4} \right) \\
    &= [1-\epsilon, \epsilon, \epsilon, 1-\epsilon], \quad \epsilon := \half \frac{1}{M + 1}.
\end{align*}


The key idea is that if you compare the weight on the mode at $(1+a)e_0 + e_1$ (which is proportional to $M$) vs the weight on the mode at $e_1 + ae_2 + e_3$ (proportional to 1) the former is much more likely than the latter as $\sigma_t \to 0$.

The basic idea for lower-bounding the $W_2$ distance is that $w^t$ has almost no mass on the two of the clusters and so we will need to move a little less than $1/4$ probability over to those clusters. For example we need to move $1/4$ probability onto cluster $e_0 + a e_2 + e_3$ from either $(1+a)e_0 + e_1$ (L2 distance between means is $\sqrt{2a + 2}$) or $(1+a) e_2 + e_3$ (L2 distance $\sqrt{2}$). So overall we will have to move a bit less that $1/2$ probability at least $\sqrt{2}$ distance.

To complete the proof we will exploit the \emph{Mixture Wasserstein} distance as an intermediate. We need the following facts from \citet{delon2020wasserstein}:
\begin{align*}
    MW_2(q_0, q_1) := \inf_{\gamma \in \Pi(q_0, q_1) \cap \text{GMM}_{2d}(\infty)} \int \|y_0 - y_1\|^2 d\gamma(y_0,y_1), \\
    MW_2^2(q_0, q_1) = \min_{c \in \Pi(w_0, w_1)} \sum_{k,l} c_{k,l} W_2^2(q_0^k, q_1^l) \quad \text{(Delon Prop. 4)},\\
    MW_2(q_0, q_1) \le W_2(q_0, q_1) + 2 \sum_{i=0,1} \sum_{k=1}^{K_i} w_i^k \text{Tr}(\Sigma_i^k) \quad \text{(Delon Prop. 6)},
\end{align*}
where $\Pi(q_0, q_1)$ denotes the set of all joint distributions with marginals $q_0$ and $q_1$, and $\text{GMM}_{d}(\infty) := \cup_{K \ge 0} \text{GMM}_d(K)$ denotes the set of all finite GMMs.
Plus one more handy fact:
\begin{align*}
W_2^2(\cN(\mu_x, \Sigma_x), \cN(\mu_y, \Sigma_y)) 
\ge \| \mu_x - \mu_y \|_2^2.
\end{align*}

\begin{align*} 
    w^0 &= \frac{1}{4} [1,1,1,1] \\
    w^t &= [1-\epsilon, \epsilon, \epsilon, 1-\epsilon] \\
    MW_2^2(\hat{p}^0, \hat{p}^t) &= \min_{c \in \Pi(w^0, w^t)} \sum_{k, l} c_{k,l} W_2^2(\hat{p}^0[k], \hat{p}^t[l]) \\
    &\ge \min_{c \in \Pi(w^0, w^t)} \sum_{k, l} c_{k,l} \|\mu_k - \mu_l\|_2^2 \\
    &\ge 2 \left(\half - 2\epsilon \right) 
    = 1 - 4 \epsilon
\end{align*}
Above, we noted any $c \in \Pi(w^0, w^t)$ has to move at least $\frac{1}{4} - \epsilon$ probability each away from indices 1 and 2 in $w^0$ and onto indices either 0 or 3, and for any of these moves the squared L2 distance is at least 2, i.e.
$\|\mu_k - \mu_l\|_2^2 \ge 2$ for $k \in (1,2), l \in (0,3)$.
We can use the $MW_2$ distance to bound the $W_2$ distance:

\begin{align*}
    W_2(\hat{p}^0, \hat{p}^t) &\ge MW_2(\hat{p}^0, \hat{p}^t) - 2 \sum_{k=0}^{3} (w^0[k] \text{Tr}(\tilde{\Sigma}^0) + w^t[k]\text{Tr}(\tilde{\Sigma}^t))\\
    &\ge MW_2(\hat{p}^0, \hat{p}^t) - 2(\text{Tr}(\tilde{\Sigma}^0) + \text{Tr}(\tilde{\Sigma}^t))\\
    &= (1 - 4 \epsilon)^{\half} - 2( 4\sigma_t^2 + 2 \tau^2(4+2a^2)) \\
    &\ge 1 - 4 \epsilon - 2( 4\sigma_t^2 + 2 \tau^2(4+2a^2)), \quad \forall \epsilon \le \frac{1}{4}.
\end{align*}

Putting everything together, we have
\begin{align*}
    W_2(\hat{p}^0, \hat{p}^t) &\ge 1 - 4 \epsilon - 2( 4\sigma_t^2 + 2 \tau^2(4+2a^2)) \\
    \epsilon &:= \half \frac{1}{M+1}, \quad
    M := \exp \left( \frac{a\sigma_t^2}{(a^2 + 2) \sigma_t^2 \tau^2 + \sigma_t^4 + \tau^4} \right).
\end{align*}


If we set $\sigma_t = \tau$, then
\begin{align*}
    W_2(\hat{p}^0, \hat{p}^t) &\ge 1 - \frac{2}{\exp(\frac{a}{(a^2 + 4)\tau^2})+1} - (24+4a^2) \tau^2
\end{align*}
Choosing $a=1$ allows some further simplification:
\begin{align*}
    W_2(\hat{p}^0, \hat{p}^t) &\ge 1 - \frac{2}{\exp(\frac{1}{5\tau^2})+1} - 32\tau^2 \\
    &\ge 1 - 33 \tau^2, \quad \text{ if } \tau^2 < \frac{1}{32} \\
    &\ge 0.5 \quad \text{ if } \tau^2 < \frac{1}{66},
\end{align*}
(in the second-to-last line we used the fact that $\frac{2}{\exp(\frac{1}{5\tau^2})+1} \ll \tau^2$ if $\tau^2 < \frac{1}{32}$, and in the last line we made an arbitrary choice).

We wanted to show that
$$\exists t, t': \quad W_2(q^{t}, q^{t'}) \geq \half \tau^{-1}|t - t'|.$$
Let's use the simple schedule $\sigma_t := t$.

For any $\tau^2 < \frac{1}{66}$, if we pick $t'=0$ and $t=\tau$, then we have as desired that
\begin{align*}
    W_2(\hat{p}^0, \hat{p}^t) \geq 0.5 \equiv  0.5 \tau^{-1}|t|.
\end{align*}
For Part 1 of Lemma \ref{lem:lipschitz}, we need to show that the distributions $p_i$ satisfy: $p_i$ is 1-Lipschitz w.r.t Wasserstein 2-distance:
$$\forall i: \quad W_2(p_i^{t}, p_i^{t'}) \leq |t - t'|.$$
We can start by proving a Lipschitz upper 
\begin{align*}
    p(x) &:= \sum_{k=1}^K w_i \cN(\mu_k, C_k), \quad x \in \R^n \\
    p^t(x) &:= \sum_{k=1}^K w_i \cN(\mu_k, C_k + \sigma_t^2 I) \\
    W_2^2(\cN(\mu_x, \Sigma_x), \cN(\mu_y, \Sigma_y)) &:= \|\mu_x - \mu_y\|_2^2 + \text{Tr}(\Sigma_x + \Sigma_y - 2 (\Sigma_x^\half \Sigma_y \Sigma_x^\half)^\half) \\
    &:= \|\mu_x - \mu_y\|_2^2 + \| \Sigma_x^\half - \Sigma_y^\half\|_F^2 \quad \text{if $\Sigma_x, \Sigma_y$ commute} \\
    \implies W_2^2(p^{t'}[k], p^t[k]) &= \| (C_k + \sigma_{t}^2 I)^\half - (C_k + \sigma_{t'}^2 I)^\half\|_F^2 \\
    &= \| (\Lambda + \sigma_{t}^2 I)^\half - (\Lambda + \sigma_{t'}^2 I)^\half\|_F^2, \quad \text{where $C_k = U \Lambda U^T$ is eigendecomposition}  \\
    &\le \| (\sigma_t - \sigma_{t'}) I \|_F^2, \quad \text{(by concavity of square root and $\Lambda \succeq 0$)} \\
    &= n (\sigma_t - \sigma_{t'})^2 \\
    \\
    W_2^2(p^{t'}, p^t) &\le MW_2^2(p^{t'}, p^t) \\
    &:= \min_{c \in \Pi(w, w)} \sum_{k, l} c_{k,l} W_2^2(p^{t'}[k], p^t[l]) \\
    &\le \sum_{k}^K W_2^2(p^{t'}[k], p^t[k]), \quad \text{(since $c=I \in \Pi(w, w)$)} \\
    &\le nK (\sigma_t - \sigma_{t'})^2 \\
    \implies W_2(p^{t'}, p^t) &\le (nK)^\half |\sigma_t - \sigma_{t'}|,
\end{align*}
showing that $p$ is $(nK)^\half$-Lipschitz w.r.t. $W_2$ distance.
Specializing this to the $p_i$ used in our counterexample where $K=2$, the Lipschitz constant for each $p_i$ is $\sqrt{2n}$; that is,  $\cO(1)$ (where $\cO$ hides only constants depending
on ambient dimension $n$, and not on $\tau$). 
\end{proof}




