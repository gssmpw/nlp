\newpage

\section{Reverse Diffusion and other Samplers}
\label{app:samplers}

\subsection{Diffusion Samplers}
DDPM \citep{ho2020denoising} and DDIM \citep{song2021denoising} are standard reverse diffusion samplers \citep{OGdiffusion, song2019generative} that correspond to discretizations of a reverse-SDE and reverse-ODE, respectively (so we will sometimes refer to the reverse-SDE as DDPM and the reverse-ODE as DDIM for short).
The forward process, reverse-SDE, and equivalent reverse-ODE \citep{song2020score} for the \emph{variance-preserving} (VP) \citep{ho2020denoising} conditional diffusion are
\begin{align}
\textsf{Forward SDE}: dx &= -\half \beta_{t} x dt + \sqrt{\beta_{t}} dw. \label{eq:ddpm_sde} \\
\textsf{DDPM SDE}: \quad
dx &=
-\half \beta_{t} x ~dt 
- \beta_t \grad_x \log p_t(x|c) dt
+ \sqrt{\beta_{t}} d\bar{w}  \label{eq:ddpm} \\
\textsf{DDIM ODE}: \quad
dx &= 
-\half \beta_{t} x~dt 
- \half \beta_t \grad_x \log p_t(x|c) dt. \label{eq:ddim}
\end{align}

\subsection{Langevin Dynamics}
Langevin dynamics (LD) \citep{rossky1978brownian,parisi1981correlation}
an MCMC method for sampling from a desired distribution. It is given by the following SDE \citep{robert1999monte}
\begin{align}
    dx &= \frac{\epsilon}{2} \nabla \log \rho(x) dt + \sqrt{\epsilon} dw, \label{eq:ld}
\end{align}
which converges (under some assumptions)
to $\rho(x)$ \citep{roberts1996exponential}. That is, letting $\rho_s(x)$ denote the solution of LD at time $s$, we have $\lim_{s \to \infty} \rho_s(x) = \rho(x)$.

\section{Connections with the Bayes composition}
\label{app:bayes_connect}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{
\includegraphics[width=0.48\columnwidth]{figures/eg1.png}
\vline
\includegraphics[width=0.48\columnwidth]{figures/eg2.png}
}
\caption{Bayes composition vs. projective composition. All experiments use exact scores, which is possible since the diffusion-noised distributions are Gaussian mixtures. (Left) Distributions follow \eqref{eq:binary_single_index}: each conditional $p_i$ activates index $i$ only, unconditional $p_u$ averages over the $p_i$, and background $p_b$ is all-zeros. We attempt to compose the conditions $p_0, p_2, p_4, p_6$ and hope to obtain the result $[1, 0, 1, 0, 1, 0]$. This requires length-generalization, since each of the conditionals $p_i$ contains only a single 1. The composition using the empty background $p_b$ (top) achieves this goal, while the Bayes composition using the unconditional $p_u$ (bottom) does not. Note that $[p_b, p_1, p_2, \ldots]$ satisfy Definition \ref{def:factorized} while $[p_u, p_1, p_2, \ldots]$ does not. (Right) Distributions follow \eqref{eq:binary_bernoulli}, where each conditional $p_i$ activates index $i$ on an independently `cluttered' background. In this case the unconditional is similar to the cluttered background. Again we attempt to compose $p_0, p_2, p_4, p_6$, and in this case we find that the composition using $p_u$ works similarly well to $p_b$.}
\label{fig:bayes_binary}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{The Bayes composition and length-generalization}
\label{app:bayes_counterex}

We give a counterexample for which the Bayes composition fails to length-generalize, while composition using an ``empty background'' succeeds. The example corresponds to the experiment shown in Figure \ref{fig:bayes_binary} (left). Suppose we have conditional distributions $p_i$ that set a single index $i$ to one and all other indices to zero, a zero-background distribution $p_b$, and an unconditional distribution formed from the conditionals by assuming $p(c=i)$ is uniform. That is:
\begin{align}
    p_i^t(x_t) &= \cN(x_t; e_i, \sigma_t^2) \propto \exp \left( -\frac{\|x_t - e_i\|^2}{2\sigma_t^2} \right) \notag\\
    p_b^t(x_t) &= \cN(x_t; 0, \sigma_t^2) \propto \exp \left( -\frac{\|x_t\|^2}{2\sigma_t^2} \right) \notag \\
    p_u^t(x_t) &= \frac{1}{n} \sum_{i=1}^n p_i(x_t)
    \label{eq:binary_single_index}
\end{align}
Suppose we want to compose all $n$ distributions $p_i$, that is, we want to activate all indices. It is enough to consider $x_t$ of the special form $x_t = (\alpha, \ldots, \alpha)$ since there is no reason to favor any condition over any another. Making this restriction,
\begin{align*}
    x_t = (\alpha, \ldots, \alpha) \implies 
    p_i^t(x_t) &\propto \exp \left( -\frac{(n-1)\alpha^2 + (1-\alpha)^2}{2\sigma_t^2} \right)
    = \exp \left( -\frac{n\alpha^2-2\alpha+1}{2\sigma_t^2} \right), \quad \forall i \\
    p_u^t(x_t) &= \exp \left( -\frac{n\alpha^2-2\alpha+1}{2\sigma_t^2} \right) \\
    p_b^t(x_t) &\propto \exp \left( -\frac{n\alpha^2}{2\sigma_t^2} \right)
\end{align*}

Let us find the value of $\alpha$ that maximizes the probability under the Bayes composition of all condition:
\begin{align*}
    x_t = (\alpha, \ldots, \alpha) \implies
    \frac{p_i^t(x_t)}{p_u^t(x_t)} &= 1\\
    \implies p_u^t(x_t) \prod_{i=1}^n \frac{p_i^t(x_t)}{p_u^t(x_t)} &\propto p_u^t(x_t) \propto \exp \left( -\frac{n\alpha^2-2\alpha+1}{2\sigma_t^2} \right) 
    = \exp \left( -\frac{n(\alpha - \frac{1}{n})^2 + \text{const}}{2 \sigma_t^2} \right) \\
    \implies \alpha^\star &= \frac{1}{n},
\end{align*}
so the optimum is $\alpha^\star = \frac{1}{n}$. That is, under the Bayes composition the most likely configuration places value $\frac{1}{n}$ at each index we wished to activate, rather than the desired value 1.

On the other hand, if we instead use $p_b$ in the linear score combination and optimize, we find that:
\begin{align*}
    x_t = (\alpha, \ldots, \alpha) \implies
    \implies \frac{p_i^t(x_t)}{p_b^t(x_t)} &\propto \exp \left( -\frac{1-2\alpha}{2\sigma_t^2} \right) \\
    \implies p_b^t(x_t) \prod_{i=1}^n \frac{p_i^t(x_t)}{p_b^t(x_t)} &\propto \exp \left( -\frac{n\alpha^2}{2\sigma_t^2} \right) \exp \left( -\frac{n(1-2\alpha)}{2\sigma_t^2} \right) 
    \propto \exp \left( -\frac{n(\alpha^2-2\alpha+1)}{2\sigma_t^2} \right) \\
    &\propto \exp \left( -\frac{n(\alpha-1)^2}{2\sigma_t^2} \right) \\
    \implies \alpha^\star &= 1
\end{align*}
so the optimum is $\alpha^\star = 1$. That is, the most likely configuration places the desired value $1$ at each index we wished to activate, achieving projective composition, and in particular, length-generalizing correctly.

\subsection{Cluttered Distributions}
\label{app:clutter}
In certain ``cluttered'' settings, the Bayes composition may be approximately projective. We explore this in the following simplified setting, corresponding to the experiment in Figure \ref{fig:bayes_binary} (right). Suppose that $x$ is binary-valued, $M_i = \{i\}, \forall i$, the $x_i$ are independently Bernoulli with parameter $q$ under the background, and the projected conditional distribution $p_i(x|_i)$ just guarantees that $x_i = 1$: 
\begin{align}
    p_b(x |_{i^c}) &\sim \text{Bern}_q(x|_{i^c}), \text{ i.i.d. $\forall i$}, \quad \quad
    p_i(x |_{i}) = \mathbbm{1}_{x|_{i} = 1},
    \label{eq:binary_bernoulli}
\end{align}
The distributions $(p_b, p_1, p_2, \ldots)$ then clearly satisfy Definition \ref{def:factorized} and hence guarantee projective composition. In this case, the unconditional distribution used in the Bayes composition is similar to the background distribution if number of conditions is large. Intuitively, each conditional looks very similar to the Bernoulli background except for a single index that is guaranteed to be equal to 1, and the unconditional distribution is just a weighted sum of conditionals. Therefore, we expect the Bayes composition to be approximately projective.

More precisely, we will show that the unconditional distribution converges to the background in the limit as $n \to \infty$, where $n$ is both the data dimension and number of conditions, in the following sense:
$$ \E_{x \sim p_b} \left[ \left( \frac{p_u(x) - p_b(x)}{p_b(x)} \right)^2 \right] \to 0 \quad \text{as } n \to \infty.$$

We define the conditional and background distributions by:
\begin{align*}
    x \in \R^n, \quad M_i &= \{i\} \\ 
    p_b(x|_{i}) &\sim \text{Bern}_q(x|_{i}), \text{ i.i.d. for $i=1,\ldots, n$} \\
    p_i(x|_{i}) &= \mathbbm{1}_{x|_{i} = 1}, \text{ for all $i=1,\ldots, n$} \\
    \implies p_b(x) &= q^{nnz(x)} (1-q)^{n - nnz(x)}\\ 
    p_i(x) &= \mathbbm{1}_{x|_{i} = 1} p_b(x|_{i^c}) 
    = \mathbbm{1}_{x|_{i} = 1}  
    q^{nnz(x|_{i^c})} (1-q)^{n - 1 - nnz(x|_{i^c})}
\end{align*}
We construct the unconditional distribution with assuming uniform probabibility over all labels: $p_u(x) := \frac{1}{n} \sum_i p_i(x)$.
The number-of-nonzeros (nnz) in all of these distributions follow Binomial distributions:
\begin{align*}
    x \sim p_b \implies p_b(nnz(x) = k) &\sim \text{Binom}(k; n, q) \\
    x \sim p_i \implies p_i(nnz(x) = k) &= p_b(nnz(x|_{i^c}) = k-1) \\
    &\sim \text{Binom}(k-1; n-1, q) \quad \text{if } k > 0 \text{ else } 0 \\
    x \sim p_u \implies p_u(nnz(x) = k) &= \frac{1}{n} \sum p_i(nnz(x) = k) \\
    &\sim \text{Binom}(k-1; n-1, q) \quad \text{if } k > 0 \text{ else } 0
\end{align*}
The basic intuition is that for large $k$ and $n$, $p_b \sim \text{Binom}(k; n, q)$ and $p_u \sim \text{Binom}(k-1; n-1, q)$ are similar. More precisely, we can calculate:
\begin{align*}
    \E_{x \sim p_b} \left[ \left( \frac{p_u(x) - p_b(x)}{p_b(x)} \right)^2 \right] &=
    \E_{x \sim p_b} \left[ \left(\frac{nnz(x)}{qn} - 1\right)^2 \right], \quad \text{since } \frac{B(k-1; n-1, q)}{B(k; n, q)} = \frac{k}{qn} \\
    &= \E_{k \sim \text{Binom}(n, q)} \left[ \left(\frac{k}{qn} - 1\right)^2 \right] 
    = \frac{1}{(nq)^2} \E_{k \sim \text{Binom}(n, q)} \left[ (k - nq)^2 \right] \\
    &= \frac{1}{(nq)^2} \text{Var}(k), \quad k \sim \text{Binom}(n, q) \\
    &= \frac{1}{(nq)^2} n q (1-q) 
    = \frac{1-q}{nq} \to 0 \quad \text{as } n \to \infty.
\end{align*}

\section{Factorized conditionals vs. orthogonal score differences}
\label{app:score_orthog}
To see that Definition \ref{def:factorized} implies orthogonality between the score differences, we note that
\begin{align*}
    v_i^t(x) &:= \grad_x \log p_i^t(x_t) - \grad_x \log p_b^t(x_t) \\
    &= \grad_x \log \frac{p_i^t(x)}{p_b^t(x)} 
    = \grad_x \log \frac{p_i^t(x|_{M_i}) p_b^t(x|_{M_i^c} x)}{p_b^t(x|_{M_i}) p_b^t(x|_{M_i^c})} \\
    &= \grad_x \log \frac{p_i^t(x|_{M_i})}{p_b^t(x|_{M_i})} \\
    \implies v_i^t(x)[k] &= 0, \quad \forall k \notin M_i \\
    \implies v_i^t(x)^T v_j^t(x) &= 0, \quad \forall i \neq j, \quad \text{since } M_i \cap M_j = \emptyset,
\end{align*}
where in the second-to-last line we used the fact that the gradient of a function depending only on a subset of variables has zero entries in the coordinates outside that subset. 

In fact, the same argument implies that $\{v_i^t(x): x \in \R^n\} \subset M_i$; in other words, $\{v_i^t(x): x \in \R^n\}$ and $\{v_j^t(x): x \in \R^n\}$ occupy mutually-orthogonal subspaces. But even this latter condition does not imply the stronger condition of Definition \ref{def:factorized}. To find an equivalent definition in terms of scores we must also capture the independence of the subsets under $p_b$. Specifically:
\begin{align*}
&\left\{
    \begin{aligned}
    p_i^t(x) &= p_i^t(x|_{M_i} x) p_b^t(x|_{M_i^c} x) \\
    p_b^t(x) &= p_b^t(x|_{\bar M} x) \prod_i p_b^t(x|_{M_i})                   
    \end{aligned}
\right. \\
\iff 
&\left\{
    \begin{aligned}
     \grad_x \log p_i^t(x) &= \grad_x \log p_i^t(x|_{M_i} x) + \grad_x \log p_b^t(x|_{M_i^c} x) \\
    \grad_x \log p_b^t(x) &= \grad_x \log p_b^t(x|_{\bar M} x) + \sum_i \grad_x \log p_b^t(x|_{M_i})
    \end{aligned}
\right. \\
\iff 
&\left\{
    \begin{aligned}
     \grad_x \log p_i^t(x) - \grad_x \log p_b^t(x) 
     &= \grad_x \log \frac{p_i^t(x|_{M_i} x)}{p_b^t(x|_{M_i} x)} \\
    \grad_x \log p_b^t(x) &= \grad_x \log p_b^t(x|_{\bar M} x) + \sum_i \grad_x \log p_b^t(x|_{M_i})
    \end{aligned}
\right.
\end{align*}
So an equivalent definition in terms of scores could be:
\begin{definition}
    The distributions $(p_b, p_1, p_2, \ldots)$ form \emph{factored conditionals} if the score-deltas $v_i^t := \grad_x \log p_i^t(x) - \grad_x \log p_b^t(x)$ satisfy $\{v_i^t(x): x \in \R^n\} \subset M_i$, where the $M_i$ are mutually-orthogonal subsets, and furthermore the score of the background distribution decomposes over these subsets as follows:
    $\grad_x \log p_b^t(x) = \grad_x \log p_b^t(x|_{\bar M} x) + \sum_i \grad_x \log p_b^t(x|_{M_i})$.
\end{definition}
(Note: this is actually equivalent to a slightly more general version of Definition \ref{def:factorized} that allows for orthogonal transformations, which is the most general assumption under diffusion sampling generates a projective composition, per Lemmas \ref{lem:transform_comp} and \ref{lem:orthogonal_sampling}.)
