
\documentclass{article}

\usepackage{silence} 
\WarningFilter{caption}{Unknown document class (or package)}
\WarningFilter{hyperref}{Ignoring empty anchor}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\usepackage{src/macros}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Mechanisms of Projective Composition}

\begin{document}

\twocolumn[
\icmltitle{Mechanisms of Projective Composition of Diffusion Models}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Arwen Bradley}{equal,comp}
\icmlauthor{Preetum Nakkiran}{equal,comp}
\icmlauthor{David Berthelot}{comp}
\icmlauthor{James Thornton}{comp}
\icmlauthor{Joshua M. Susskind}{comp}
\end{icmlauthorlist}

\icmlaffiliation{comp}{Apple, Cupertino, CA, USA}

\icmlcorrespondingauthor{Arwen Bradley}{arwen\_bradley@apple.com}
\icmlcorrespondingauthor{Preetum Nakkiran}{p\_nakkiran@apple.com}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]




\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}

We study the theoretical foundations of composition in diffusion models, with a particular focus on out-of-distribution extrapolation and length-generalization.
Prior work has shown that composing distributions
via linear score combination can achieve promising results,
including length-generalization in some cases \citep{du2023reduce,liu2022compositional}.
However, our theoretical understanding of how and why such compositions work remains incomplete. In fact, it is not even entirely clear what it means for composition to ``work''.
This paper starts to address these fundamental gaps.
We begin by precisely defining one possible desired result of composition, which we
call \emph{projective composition}.
Then, we investigate: (1) when linear score combinations provably achieve projective composition, (2) whether reverse-diffusion sampling can generate the desired composition, and (3) the conditions under which composition fails. Finally, we connect our theoretical analysis to prior empirical observations where composition has either worked or failed, for reasons that were unclear at the time.
\end{abstract}

\input{src/intro}
\input{src/related}
\input{src/bayes}
\input{src/projective_comp}
\input{src/composition}
\input{src/connections}
\input{src/conclusion}



\section*{Acknowledgements}
Acknowledgements: We thank Miguel Angel Bautista Martin,  Etai Littwin, Jason Ramapuram, and Luca Zappella for helpful discussions and feedback throughout this work, and Preetum's dog Papaya for his contributions to Figure 1.



\bibliography{refs}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn
\input{src/appendix}

\end{document}
