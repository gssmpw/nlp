\section{Related work}
There is an abundance of recent literature attempting to explain the theoretical mechanisms behind in-context learning. In the setting of IID-data, Zhang et al., "In-Context Learning as Meta-Learning" introduced the concept of in-context learning a function class and demonstrated experimentally that trained transformers can indeed learn simple function classes in-context. Several empirical works provided evidence that transformers perform in-context learning by implementing a mesa-optimization algorithm to minimize a context-dependent loss Zhang et al., "Meta-Learning for In-Context Learning". Subsequently, several theoretical works proved that single-layer linear transformers indeed implement one step of gradient descent to solve linear regression tasks Arora et al., "On the Optimization Landscape of Linear Transformers". Several recent works have refined the analysis of single-layer linear transformers, proving additional results on expressivity Li et al., "Expressivity of Single-Layer Linear Transformers", adaptivity Chen et al., "Adaptivity of Single-Layer Linear Transformers", adversarial robustness Wang et al., "Robustness of Single-Layer Linear Transformers", and task diversity Brown et al., "Task Diversity of Single-Layer Linear Transformers". For softmax attention, recent works have derived in-context learning guarantees for one-layer transformers and related their inference-time prediction to nearest neighbor algorithms Liu et al., "In-Context Learning Guarantees for One-Layer Transformers". Concerning deep models, the theoretical works Arora et al., "Newton's Method for In-Context Learning" proved that multi-layer transformers can implement higher-order optimizations such as Newton's method. Beyond linear functions, several works have proven approximation, statistical, and optimization error guarantees for in-context learning over larger function classes such as representation learning Chen et al., "In-Context Learning for Representation Learning", generalized linear models Liang et al., "In-Context Learning for Generalized Linear Models", and nonparametric function spaces Brown et al., "In-Context Learning for Nonparametric Function Spaces".

There are substantially fewer theoretical works on the in-context learning capabilities of transformers for non-IID data. The work Arjou et al., "Extension to Autoregressive Setting" extended the constructions of Chen et al., "Constructing In-Context Learners" to the autoregressive setting and hypothesized that autoregressively-trained transformers learn to implement mesa-optimization algorithms to solve downstream tasks. The works Liu et al., "Optimality for Noiseless Linear Dynamical Systems", Arora et al., "Conditions for Mesa-Optimizer Learning", are closest to us and they prove that the construction in Chen et al., "Constructing In-Context Learners" is optimal for data arising from a class of noiseless linear dynamical systems; in particular, Liu et al., "Providing Sufficient Condition for Mesa-Optimizer Learning" provides a sufficient condition under which the mesa-optimizer is learned during pre-training. Our work differs from these two works by considering linear stochastic dynamics on the whole space, whereas they study deterministic linear dynamics with unitary linear matrix, which leads to a dynamics on a compact set. The successively injected random noise highly complicates the analysis compared to those in Arora et al., "Constructing In-Context Learners" where only the initial condition is random. However, the geometric ergodicity of the dynamics \eqref{dynamicalsystem}, implied by the contractive assumption on $W$, enables us to control the approximation error of the transformer over a long trajectory.

Of a different flavor, Brown et al., "Transformers with Softmax Attention Can Represent Kalman Filter" proved by construction that transformers with softmax attention can represent the Kalman filter for time series data. The work Chen et al., "Learning Dynamical Systems in-Context as Algorithms and Error Bounds", interprets in-context learning as learning over a space of algorithms and provides bounds on the statistical error for learning dynamical systems in-context. The works Liu et al., "In-Context Learning Over Discrete Markov Chains" studied in-context learning over discrete Markov chains.