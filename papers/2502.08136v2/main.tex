\documentclass[twoside,11pt]{article}

%\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{comment}
\usepackage[lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{minitoc}
\allowdisplaybreaks


\newcommand{\footremember}[2]{%
   \footnote{#2}
    \newcounter{#1}
    \setcounter{#1}{\value{footnote}}%
}
\newcommand{\footrecall}[1]{%
    \footnotemark[\value{#1}]%
}

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\dis}{\mathcal{D}}
\newcommand{\dlam}{\mathcal{D}_{\Lambda}}
\newcommand{\diag}{\textrm{diag}}
\newcommand{\query}{\textrm{query}}
\newcommand{\lsa}{\textrm{LSA}}
\newcommand{\attn}{\textrm{Attn}}
\newcommand{\trace}{\textrm{Tr}}
\newcommand{\tila}{\tilde{a}}
\newcommand{\bolda}{\bf{A}^{-2}}
\newcommand\iid{\mathrel{\stackrel{\makebox[0.05pt]{\mbox{\normalfont\tiny i.i.d.}}}{\sim}}}
\newcommand\inp{\mathrel{\stackrel{\makebox[0.05pt]{\mbox{\normalfont \small P}}}{\rightarrow}}}
\newcommand{\mL}{\mathcal{L}}
\newcommand{\mX}{\mathcal{X}}
\newcommand{\mG}{\mathcal{G}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\VV}{\mathbf{V}}
\newcommand{\LL}{\mathbf{L}}
\newcommand{\aaa}{\mathbf{A}}
\newcommand{\ff}{\mathbf{f}}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\blue}{\textcolor{blue}}
%\newcommand\myeq{\stackrel{\mathclap{P}{=}}

\newcommand{\yl}[1]{\textcolor{red}{[YL: #1]}}

\newcommand{\fc}[1]{\textcolor{blue}{[FC: #1]}}

\newcommand{\ro}[1]{\textcolor{green}{[RO: #1]}}

\newcommand{\tz}[1]{\textcolor{purple}{[TZ: #1]}}

\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{cor}{Corollary}
\newtheorem{ex}{Example}
\newtheorem{rmk}{Remark}
\newtheorem{defn}{Definition}


% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
\usepackage[abbrvbib, preprint]{jmlr2e}

%\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
%\jmlrheading{23}{2024}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Cole, Lu, O'Niell, and Zhang}

% Short headings should be running head and authors last names

\ShortHeadings{ICL of Dynamical Systems}{Cole, Lu, Zhang, Zhao}
\firstpageno{1}

\begin{document}

\title{In-Context Learning of Linear Dynamical Systems with Transformers: Error Bounds and Depth-Separation}

\author{\name Frank Cole\thanks{206 Church St. SE, School of Mathematics, University of Minnesota, Minneapolis, MN 55455} \email cole0932@umn.edu \\
       %\addr School of Mathematics \\
       %University of Minnesota
       %University of Minnesota
       %Minneapolis, MN,, USA
       \AND
       \name Yulong Lu\footnotemark[1] \email yulonglu@umn.edu \\
       %\addr Department of Mathematics\\
       %University of Minnesota\\
       %Minneapolis, MN, 55455, USA
       \AND
       \name Tianhao Zhang\footnotemark[1] \email zhan7594@umn.edu \\
       %\addr Department of Mathematics\\
       %University of Minnesota\\
       %Minneapolis, MN, 55455, USA
       \AND
       \name Yuxuan Zhao \footnotemark[1] \email zhao1895@umn.edu  \\
       %\addr Department of Mathematics\\
       %University of Minnesota\\
       %Minneapolis, MN, 55455, USA
       }

%\editor{My editor}

\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents % Run a fake tableofcontents command for the partocs

%\part{} % Start the document part
%\parttoc % Insert the document TOC

\maketitle

\begin{abstract}%
  This paper investigates approximation-theoretic aspects of the in-context learning capability of the transformers in representing a family of noisy linear dynamical systems. Our first theoretical result establishes an upper bound on the approximation error of multi-layer transformers with respect to an $L^2$-testing loss uniformly defined across tasks. This result  demonstrates that transformers with logarithmic depth can achieve error bounds comparable with those of the least-squares estimator. In contrast, our second result establishes a non-diminishing lower bound on the approximation error for a class of single-layer linear transformers, which suggests  a depth-separation phenomenon for transformers in the in-context learning of dynamical systems. Moreover, this second result uncovers a critical distinction in the approximation power of single-layer linear transformers when learning from IID versus non-IID data.
\end{abstract}

\begin{keywords}%
  In-context learning, linear transformer, depth-separation, linear dynamical system
\end{keywords}

\section{Introduction}
\label{submission}
Transformers \cite{vaswani2017attention} have achieved remarkable success in natural language processing, driving the success of modern large language models such as ChatGPT \cite{achiam2023gpt}. The impressive capabilities of transformers in NLP tasks has spurred their adoption across diverse domains beyond NLP, such as computer vision \cite{li2023transformers, khan2022transformers, chen2020generative, ramesh2021zero}, computational biology \cite{jumper2021highly, choi2023transformer}, physical modeling \cite{batatia2023foundation, mccabe2023multiple, subramanian2024towards, ye2024pdeformer}, among others.


At a high level, transformers are autoregressive sequence-to-sequence models: the input of a transformer is a sequence of tokens $(e_1, \dots, e_T)$, and the output is a corresponding sequence $(y_1, \dots, y_T)$ where $y_t$ is only a function of the previous tokens $(e_1, \dots, e_{t}).$ A particularly intriguing property of transformers is their ability to perform \textit{in-context learning}: pre-trained transformers are able to make accurate predictions on unseen sequences, even those beyond the support of their pre-training distribution, without any parameter updates. One promising explanation for these striking behaviors is the \textit{mesa-optimization hypothesis} \cite{von2023transformers,von2023uncovering}, which says that transformers make next-token predictions by implicitly optimizing a context-dependent loss according some optimization algorithm encoded during pre-training.

To elucidate these emergent properties, a lot of recent research has studied in-context learning in tractable settings. For instance, several concurrent works studied in-context learning of linear models by simplified transformer architectures \cite{zhang2023trained,mahankali2023one,ahn2023transformers}. In this setting, it has been shown that the transformer which minimizes a natural $L^2$ population risk indeed performs in-context learning over the class of linear models, and its prediction error decays at the parametric rate $O\left( \frac{1}{T} \right),$ where $T$ is the number of samples of the downstream task. Moreover, these trained transformers enjoy prediction error bounds that hold \textit{uniformly} over the set of admissible tasks, despite only being trained on an $L^2$ loss. The learning algorithm encoded by transformers trained on in-context examples of linear regression can be expressed as a single step of gradient descent on a context-dependent loss, confirming earlier hypotheses by empirical works \cite{von2023transformers}.

Most of the prior theoretical works described in the previous paragraph works assume IID data, where each sequence $(x_0,x_1, \dots, x_T)$ consists of uncorrelated tokens. A more realistic assumption is that the data/tokens are correlated, and linear dynamical systems provide a natural and tractable setting to study in-context learning of correlated data. In this non-IID setting, many theoretical questions on the in-context learning capability of transformers have remained elusive: 
\begin{enumerate}
    \item Given a class of dynamical systems, does there exist a transformer which in-context learns the class? If so, can we quantify the prediction error rate achieved by the transformer?
    \item  How complex must the transformer architecture be in order to perform in-context learning?
    \item What algorithm do transformers trained to perform in-context learning over dynamical systems encode?
\end{enumerate}

In this work, we focus on the first two questions above and  investigate the approximation power of linear transformers in in-context learning for a class of linear dynamical systems which are corrupted by noise. Specifically, we are interested in learning from sequences $(x_0,x_1, \dots, x_T)$ that are generated according to the linear stochastic dynamics
\begin{equation} x_t = W x_{t-1} + \xi_t, \; 1 \leq t \leq T,
\end{equation}
where $W \in \R^{d \times d}$ is a matrix whose eigenvalues lie in $(0,1)$ and $\xi_t$ is independent Gaussian noise. Our goal is to derive quantitative results  that characterize the ability (or inability) of linear transformers to learn the linear map defined by the conditional expectation $x_{t-1} \mapsto \E[x_t|x_{t-1}]$ in-context, from  dynamical systems governed by Equation \eqref{dynamicalsystem}. Our main contributions are highlighted as follows.
\begin{enumerate}
    \item We establish an approximation error bound for a class of deep linear transformers in learning the family of dynamical systems \eqref{dynamicalsystem} in-context; see Theorem \ref{approxerrordeeptf}. More precisely, we show that linear transformers whose depth scales as $O(\log(T))$ can achieve an error of $O\left( \frac{\log(T)}{T}\right)$ with respect to an $L^2$-testing loss uniformly defined across tasks. The proof is based on constructing a transformer which approximates the least-squares estimator of the system \eqref{dynamicalsystem} through iterative methods, while leveraging statistical properties of the least-squares estimator.
    \item We further investigate the role of depth of transformers in our first result. %We explore the ability of a sub-class of \textit{single-layer} transformers to learn the dynamical system \eqref{dynamicalsystem} in-context. 
   We demonstrate that a broad class of single-layer linear transformers is fundamentally limited in its ability to perform in-context learning for the system described by Equation \eqref{dynamicalsystem}. Specifically, the test loss satisfies a lower bound that remains independent of the transformer's parameterization, even as the length of the trajectory data $T$ approaches infinity; see Theorem \ref{approxlowerbd}. 
\end{enumerate}

Our second result uncovers a distinct feature of single-layer linear transformers: their inability to learn from dependent data. This was observed in a different setting in \cite{zheng2024mesa}. In contrast, it has been shown in the IID setting that (see e.g. \cite{zhang2023trained}) single-layer linear transformers \textit{provably} learns linear models in-context, in the sense that optimized transformer can achieve zero test loss in the infinite context-length limit. We will expand upon the reasons behind the differing outcomes between these two settings in more detail in Section \ref{iidvsnoniid}. 


\subsection{Related work}

There is an abundance of recent literature attempting to explain the theoretical mechanisms behind in-context learning. In the setting of IID-data, \cite{garg2022can} introduced the concept of in-context learning a function class and demonstrated experimentally that trained transformers can indeed learn simple function classes in-context. Several empirical works provided evidence that transformers perform in-context learning by implementing a mesa-optimization algorithm to minimize a context-dependent loss \cite{von2023transformers,akyurek2022learning,dai2022can,muller2021transformers}. Subsequently, several theoretical works proved that single-layer linear transformers indeed implement one step of gradient descent to solve linear regression tasks \cite{zhang2023trained,mahankali2023one,ahn2023transformers, wu2023many}.  Several recent works have refined the analysis of single-layer linear transformers, proving additional results on expressivity \cite{zhang2024context}, adaptivity \cite{vladymyrov2024linear}, adversarial robustness \cite{anwar2024adversarial}, and task diversity \cite{cole2024provable, lu2024asymptotic}. For softmax attention, recent works \cite{collins2024context, li2024one} derived in-context learning guarantees for one-layer transformers and related their inference-time prediction to nearest neighbor algorithms. Concerning deep models, the theoretical works \cite{giannou2024well,fu2023transformers} proved that multi-layer transformers can implement higher-order optimizations such as Newton's method. Beyond linear functions, several works have proven approximation, statistical, and optimization error guarantees for in-context learning over larger function classes such as representation learning \cite{yang2024context,guo2023transformers, kim2024transformers2}, generalized linear models \cite{bai2024transformers}, and nonparametric function spaces \cite{kim2024transformers, collins2024context}.

There are substantially fewer theoretical works on the in-context learning capabilities of transformers for non-IID data. The work \cite{von2023uncovering} extended the constructions of \cite{von2023transformers} to the autoregressive setting and hypothesized that autoregressively-trained transformers learn to implement mesa-optimization algorithms to solve downstream tasks. The works \cite{sander2024transformers, zheng2024mesa} are closest to us and they prove that the construction in \cite{von2023uncovering} is optimal for data arising from a class of noiseless linear dynamical systems; in particular, \cite{zheng2024mesa} provides a sufficient condition under which the mesa-optimizer is learned during pre-training. Our work differs from these two works by considering linear stochastic dynamics on the whole space, whereas they study deterministic linear dynamics with unitary linear matrix, which leads to a dynamics on a compact set. The successively injected random noise highly complicates the analysis compared to those in  \cite{zheng2024mesa} where only the initial condition is random. However, the geometric ergodicity of the dynamics  \eqref{dynamicalsystem}, implied by the contractive assumption on $W$, enables us to control the approximation error of the transformer over a long trajectory. 

Of a different flavor, \cite{goel2024can} proved by construction that transformers with softmax attention can represent the Kalman filter for time series data. The work \cite{li2023transformers} interprets in-context learning as learning over a space of algorithms and provides bounds on the statistical error for learning dynamical systems in-context. The works \cite{nichani2024transformers, edelman2024evolution} studied in-context learning over discrete Markov chains.


\subsection{Organization}
The paper is organized as follows. In Section \ref{setup}, we discuss the details of our problem setup, including the observation model, the transformer architecture, and the loss functions considered herein. In Sections \ref{approxupperbound} and \ref{approxlowerbdsec}, we state our theoretical results on the approximation of deep and single-layer transformers respectively. We also discuss the qualitative difference we uncover between in-context learning with IID and non-IID data in Section \ref{iidvsnoniid}. In Section \ref{proofsketchsection} we sketch the proofs of our main theorems, and in Section \ref{conclusion} we describe some directions for future work. All proofs are deferred to the appendix; Appendix \ref{appendixupperbound} provides the proofs for Section \ref{approxupperbound}, Appendix \ref{appendixupperbound} provides the proofs for Section \ref{approxlowerbdsec}, and Appendix \ref{aux} provides the proofs for various auxiliary lemmas.

\subsection{Notation} Throughout the remainder of the paper, $d$ denotes the dimension of the Euclidean space and $T$ denotes the length of a sequence or dynamical system. We denote by $\mathbf{I}_d$ the $d \times d$ identity matrix. We write $\| \cdot \|$ for the Euclidean norm on $\R^d$ and $\| \cdot \|_{\textrm{op}}$ for the operator norm on $\R^{d \times d}$ with respect to the norm $\| \cdot \|$ on $\R^d.$ For a symmetric matrix $A \in \R^{d \times d},$ we write $\lambda_{\min}(A)$ and $\lambda_{\max}(A)$ to denote the smallest and largest eigenvalues of $A$, respectively. For a random vector $X \in \R^{m}$, we use $\E[X]$ to denote the mean of $X$ and $\textrm{cov}(X)$ to denote the covariance matrix of $X$. For non-negative functions $f,g$ on $\R^d$, we write $f(x) = O(g(x))$ if there is a constant $C > 0$ independent of $x$ such that $f(x) \leq C g(x)$. We also sometimes write $f(x) \lesssim g(x)$ instead of $f(x) = O(g(x)).$ We write $f(x) = \Omega(g(x))$ (or $f(x) \gtrsim g(x)$) if $f(x) \geq C g(x)$ for a constant independent of $x$, and we write $f(x) = \Theta(g(x))$ if $f(x) = O(g(x))$ and $g(x) = O(f(x)).$

\section{Set-up}\label{setup}

\subsection{Observation model}
We consider in-context learning of sequences $(x_0,x_1, \dots, x_T)\subset \R^d$ defined by the noisy linear dynamical system
 \begin{equation}\label{dynamicalsystem}
    \begin{cases}
    x_0 = 0, \\
    x_t = Wx_{t-1} + \xi_t, \; 1 \leq t \leq T,
\end{cases}
\end{equation}

where $W \in \mathcal{W} \subseteq \R^{d \times d}$ and $\{\xi_1, \dots, \xi_t\}$ are independent noise terms which follow a Gaussian distribution $N(0,\sigma^2\mathbf{I}_d)$ for some $\sigma > 0$ which describes the signal-to-noise ratio of the tasks. We let $p_W$ denote a probability measure supported on $\mathcal{W}$. A sequence $\mathbf{x}$ is then generated according to the two-stage procedure:
\begin{enumerate}
    \item Sample a task $W \sim p_W$;
    \item Conditioned on $W$, sample $\mathbf{x}$ according to the observation process \eqref{dynamicalsystem}, where the noise is assumed to be jointly independent from $W$.
\end{enumerate}
Our theoretical results are agnostic to the choice of the task distribution $p_W$ and depend only on its support $\mathcal{W}$. To make the analysis more tractable, we place the following assumptions on $\mathcal{W}$.
\begin{assumption}\label{taskassum}
    There exist constants $0 < w_{\min} < w_{\max} < 1 $ such that 
    $$ w_{\min} \cdot \mathbf{I}_d \prec W \prec w_{\max} \cdot \mathbf{I}_d, \; \; \; \forall W \in \mathcal{W}.
        $$
\end{assumption}
The assumption that the eigenvalues of the matrices in $\mathcal{W}$ are strictly less than one ensures that the dynamical system defined in Equation \eqref{dynamicalsystem} is geometrically ergodic, which guarantees the loss functions to be considered have finite limits as $T$ tends to infinity. In contrast, previous works assume that the underlying dynamical system is noiseless, and therefore require that the task matrices have unit norm (e.g., rotation matrices) in order for the dynamics to be non-degenerate; see e.g. \cite{sander2024transformers, zheng2024mesa}. Note also that Assumption \ref{taskassum} does not impose structural constraints such as simultaneous diagonalizability on the task matrices.

\subsection{Linear transformer architecture}
We first recall the construction of a linear transformer block. Let $(e_1, \dots, e_t)$ be a sequence of tokens. A linear transformer block with attention weights $W_P, W_Q, W_V, W_K$ and MLP weights $W_1, W_2$ maps the sequence $(e_1, \dots, e_t)$ to the sequence $(\widehat{e}_1, \dots, \widehat{e}_T)$ given by
$$ \widehat{e}_t = W_{MLP} \left(e_t + W_P W_V E_{t} \cdot \frac{E_t^T W_K^T W_Q e_t}{\rho_t} \right),
$$
where $E_t = (e_1, \dots, e_t).$ Note that each output token $\widehat{e}_t$ is a function of only the first $t$ input tokens. For the purpose of our analysis, we simplify the structure of the attention layer by re-parameterizing some of the weight matrices. The simplified linear transformer block has attention weights $W_P, W_Q$ and MLP weights $W_{MLP}$ and is given by
$$ \widehat{e}_t = W_{MLP} \left( e_t + W_P E_t \cdot \frac{1}{\rho_t} E_t^T W_Q e_t \right).
$$
Note that from an approximation theory point of view, this re-parameterization of the weights does not lose us any expressiveness. In the case of learning the dynamical system defined by Equation \eqref{dynamicalsystem}, the observations $(x_0,x_1, \dots, x_T)$ are encoded into a sequence of tokens $(e_1, \dots, e_T)$, which the user has the freedom to specify. We consider the following positional encoding given by
$$ e_t = \begin{pmatrix}
    0_d \\ 0_d \\ x_t \\ x_{t-1}
\end{pmatrix} \in \R^{4d}.
$$
The first $d$ rows rows of the token $e_t$ serve as a placeholder for the transformer's prediction for the next token. That is, if $\widehat{e}_t$ is the $t^{\textrm{th}}$ element of the sequence defined by the transformer block, we write $\widehat{y}_t = (\widehat{e}_t)_{1:d}$ for the prediction of $x_{t+1}$. We note that a similar prompt embedding for autoregressive transformers, in which only the first $d$ rows of $e_t$ are held as zero, has been studied in several works \cite{von2023uncovering, sander2024transformers,zheng2024mesa}. The reason for choosing this specific embedding structure (with  zeros in the first $2d$ entries) will become clear in the proof of Theorem \ref{approxerrordeeptf}.  We take the normalization factor to be $\rho_t = t.$ 

A multi-layer linear transformer is then defined simply by composing linear transformer blocks. More precisely, an $L$-layer transformer is parameterized by weights $$\{W_{MLP}^{(\ell)}, W_P^{(\ell)}, W_Q^{(\ell)} \in \R^{4d \times 4d}: 1 \leq \ell \leq L\}.$$ As in the single-layer case, we use the first $d$ rows of the Transformer output of each token $\widehat{y}_t = (\widehat{e}_t)_{1:d}$ for prediction. Denote by $\textrm{TF}_L$ the class of $L$-layer linear transformers.

\subsection{Training and inference loss functions}\label{lossfunctions}
Transformers are typically trained to perform in-context learning by minimizing the following \textit{auto-regressive training loss} 
\begin{equation}\label{train} L_{\textrm{train}}(\theta) = \E_{(\xi_1, \dots, \xi_T, W)}\left[ \frac{1}{T-1} \sum_{t=1}^{T-1} \left\|\widehat{y}_t - x_{t+1} \right\|^2 \right],
\end{equation}
where the transformer learns to predict the next token $x_{t+1}$ from the previous $t$ tokens. Here, $\theta$ denotes the transformer parameters. In the setting where $(x_1, \dots, x_T)$ follow a dynamical system of the form in Equation \eqref{dynamicalsystem}, we emphasize that the transformer is learning to predict the mean of $x_{t+1}$ conditioned on $x_t$ for each $t \in \{1, \dots, T-1\}$. This follows from the fact that the loss $L(\theta)$ averages over the measurement noise. To analyze the in-context learning ability of a given transformer, we define the \textit{in-context test loss}
\begin{equation}\label{test} L_T(\theta) = \sup_{W \in \mathcal{W}} \E_{\xi_1, \dots, \xi_t} \left[ \left\|\widehat{y}_T - Wx_T \right\|^2 \right].
\end{equation}
The test loss admits three key differences from the train loss. First, it measures the difference between the transformer prediction $\widehat{y}_t$ and the conditional expectation $Wx_t = \E[x_{t+1}|x_t,W]$, rather than the difference between $\widehat{y}_t$ and the noisy label $x_{t+1}$ used in the training loss. By a simple calculation, this is equivalent to defining the loss using the difference between $\widehat{y}_t$ and $x_{t+1}$, up to an additive factor which depends only on the distribution of the noise. Thus, this change can be viewed as a centering of the loss to ensure that the minimum of the test loss tends to zero as $T \rightarrow \infty.$

Second, the test loss only measures the error of the transformer prediction at the terminal time $T.$ Since the transformer prediction $\widehat{y}_t$ is only a function of the first $t$ iterates of the dynamical system $(x_1, \dots, x_t)$, we cannot expect the transformer to make accurate predictions when $t$ is small.

Third, and perhaps most importantly, the test loss measures the error \textit{uniformly} over the task set $\mathcal{W}.$ This is quite different from the typical $L^2$-loss used to measure the prediction error in supervised learning. However, it is a natural metric to use because, for in-context learning, we seek guarantees on the model's inference-time prediction that hold for \textit{any} downstream task, not only in the average sense. We note that while the uniform loss is difficult to use for training purposes, several works have proven that transformers trained on an $L^2$-loss do in fact exhibit inference-time guarantees which hold uniformly over the task space \cite{zhang2023trained, yang2024context, li2024one}.

\subsection{Approximation error bound for deep linear transformers}\label{approxupperbound}
Our first theoretical result is an upper bound of the approximation error of deep transformers in learning the dynamical system \eqref{dynamicalsystem} in-context, measured by the test loss \eqref{test}. 
\begin{theorem}\label{approxerrordeeptf}
    There exists a transformer of depth $L = O(\log(T))$ with parameters $\theta$ such that
    $$ L_T(\theta) \lesssim \frac{\log(T)}{T}.
    $$
    when $T$ is sufficiently large. The implicit constants depend on $\sigma$, $w_{\max}$, and $d$, and the dependence is at most polynomial in $d.$
\end{theorem}
Theorem \ref{approxerrordeeptf} shows that there is a transformer that can closely track the condition mean as $T$ increases to infinity. The key idea underlying the proof of Theorem \ref{approxerrordeeptf} is to  construct a transformer such that, for any dynamical system $(x_1, \dots, x_T)$ according to \eqref{dynamicalsystem}, the transformer prediction $\widehat{y}_t$ approximates the least-squares prediction $\widehat{W}x_t$, where $\widehat{W} = \textrm{argmin}_W \frac{1}{t} \sum_{i=0}^{t-1}\|x_{t+1} - Wx_t\|^2$ is the least-squares estimator. The proof then proceeds by leveraging the asymptotic statistical properties of the least-squares estimator to achieve the final bound. Implementing the least squares estimator requires computing the matrix vector product $X_t^{-1} x_t$, where $X_t = \frac{1}{t} \sum_{i=0}^{t-1} x_i x_i^T$ is the empirical covariance matrix. An important ingredient of the proof of Theorem \ref{approxerrordeeptf} is the construction of a transformer which unrolls a \textit{modified Richardson iteration} algorithm for solving the  linear system $X_t b  = x_t$ for $X_t^{-1}x_t$. The convergent guarantees of the Richardson iteration allow us to derive quantitative bounds between the transformer prediction and the least-squares estimator. We note that the algorithmic unrolling idea has previously used to prove the  approximation power of deep neural networks \cite{chen2021representation,marwah2023neural} for solutions of partial differential equations. The recent works adopted the unrolling idea for proving the approximation power of  transformers for simple regression tasks \cite{bai2024transformers} and nonlinear functions \cite{guo2023transformers}. The recent  paper \cite{von2023uncovering} also utilized unrolling of the Richardson's iteration in their construction of a transformer for learning deterministic linear dynamical systems, but did not provide an approximation guarantee for the constructed transformer.

The error achieved by the deep transformer constructed in Theorem \ref{approxerrordeeptf} decays at the parametric rate $O\left(\frac{1}{T} \right)$, up to a logarithmic factor. This is not surprising, as our proof strategy constructs a transformer which (approximately) implements a parametric estimator. Theorem \ref{approxerrordeeptf} also shows that the depth of the transformer needed to achieve this rate is only logarithmic in the sequence length $T$, thanks to the fast convergence of the Richardson iteration. The question of whether the same approximation rate can be achieved by a transformer whose depth is independent of the sequence length $T$ is an interesting question which we leave to future work.

\subsection{A lower bound for single-layer transformers}\label{approxlowerbdsec}
While it is beyond the scope of this paper to characterize whether $O(\log(T))$ depth is necessary for in-context learning dynamical systems, we provide an analysis for the (limited) approximation power of  single-layer transformers. To be concrete, we focus on in-context learning of the one-dimensional dynamical system
\begin{equation}\label{dynamicalsystem1D}
    \begin{cases}
        x_{t} = wx_{t-1} + \xi_{t}, \; t \in \{1, \dots, T\}, \\
        x_0 = 0,
    \end{cases}
\end{equation}
where $\xi_t \sim N(0,\sigma^2)$ and $w \in [w_{\min},w_{\max}].$To make our analysis tractable, we fix some of the parameters of the single-layer transformer; in Section \ref{generaltfcase}, we discuss how our results might be extended to the general case. First, we fix the MLP weight matrix $W_{MLP}$ to be the $d \times d$ identity matrix. The resulting architecture is often referred to as a \textit{linear attention block} and has been the study of a lot of recent research \cite{zhang2023trained,mahankali2023one,zheng2024mesa,zhang2024context}. 
In the 1D setting, a linear attention block defines an estimator $\widehat{y}_T$ for $x_{T+1}$ given by
\begin{align*} \widehat{y}_t = \frac{1}{T} \begin{pmatrix}
    p_1 & p_2
\end{pmatrix} &\cdot \begin{pmatrix}
    \sum_{i=1}^{T} x_i^2 & \sum_{i=1}^{T} x_i x_{i-1} \\
    \sum_{i=1}^{T} x_i x_{i-1} & \sum_{i=1}^{T} x_{i-1}^2
\end{pmatrix} \\ & \cdot \begin{pmatrix}
    q_{11} & q_{12} \\ q_{21} & q_{22}
\end{pmatrix} \cdot \begin{pmatrix}
    x_T \\ x_{T-1}
\end{pmatrix},
\end{align*}
where $p_i \in \R$ and $q_{ij} \in \R$ are the learnable parameters. Since we know that $\E[x_{t+1}|x_0, x_1, \dots, x_t]$ is a linear function of $x_t$, it is natural to zero out the parameters that lead to $\widehat{y}_t$ being a linear combination of both $x_t$ and $x_{t-1}$. This motivates us to further set $q_{12} = q_{22} = 0$, leading to the simplified parameterization
\begin{align}\label{simpleparameterization}
    \widehat{y}_T = \frac{1}{T} \mathbf{p}^T \begin{pmatrix}
    \sum_{i=1}^{T} x_i^2 & \sum_{i=1}^{T} x_i x_{i-1} \\
    \sum_{i=1}^{T} x_i x_{i-1} & \sum_{i=1}^{T} x_{i-1}^2
\end{pmatrix} \mathbf{q} \cdot x_T,
\end{align}
where $\mathbf{p} = \begin{pmatrix}
    p_1 & p_2
\end{pmatrix}^T$ and $\mathbf{q} = \begin{pmatrix}
    q_1 & q_2
\end{pmatrix}^T.$ We note that a similar parametrization was also adopted  in \cite{zheng2024mesa} and  it was proven that a single layer of fully-parameterized linear attention trained on gradient flow of the $L^2$ loss will converge to a parameterization where only $p_{21}$ and $q_{11}$ are nonzero. 

In the one-dimensional setting, the test loss introduced in Subsection \ref{lossfunctions} becomes
$$ L_T(\mathbf{p},\mathbf{q}) = \left( \widehat{y}_T - wx_T \right)^2.
$$
Our aim is to understand how the approximation error $\inf_{\mathbf{p},\mathbf{q}} L_T(\mathbf{p},\mathbf{q})$ behaves as a function of $T$. Specifically, we shall show that  $\inf_{\mathbf{p},\mathbf{q}} L_T(\mathbf{p},\mathbf{q}) = \Omega(1)$ as $T \rightarrow \infty$, from which we conclude that the single-layer transformer induces an irreducible approximation error, indicating that it is insufficient in learning the linear dynamical system.

To this end, we first express the test loss function as $L_T(\mathbf{p},\mathbf{q}) = \E[\ell(\mathbf{p},\mathbf{q},w)]$, where the individual loss is defined as
$$ \ell_T(\mathbf{p},\mathbf{q},w) =  \left(\widehat{y}_T - wx_T \right)^2.
$$
The following lemma characterizes the limiting behavior of the individual loss as $T \rightarrow \infty.$

\begin{lemma}\label{limitinglossinformal}
    The individual loss function converges pointwise on $\R^2 \times \R^2 \times [w_{\min},w_{\max}]$ to a limiting function $\ell(\mathbf{p},\mathbf{q},w)$ defined by
    $$ \ell(\mathbf{p},\mathbf{q},w) = \frac{\sigma^2}{1-w^2} \left( \frac{\sigma^2}{1-w^2}(w\alpha_1 + \alpha_2) + w \right)^2,
    $$
    where $\alpha_1 = p_1q_2 + p_2q_1$ and $\alpha_2 =\mathbf{p}^T \mathbf{q}.$ Moreover, $\ell$ satisfies the lower bound
    $$ \inf_{\mathbf{p},\mathbf{q} \in \R^2} \sup_{w \in [w_{\min},w_{\max}]} \ell(\mathbf{p},\mathbf{q},w) \geq C(\sigma^2,w_{\min},w_{\max}),
    $$
    where $C(\sigma^2,w_{\min},w_{\max}) > 0$ is a strictly positive constant depending only on $\sigma^2,$ $w_{\min},$ and $w_{\max}.$
\end{lemma}
The proof of the formula for $\ell(\mathbf{p},\mathbf{q},w)$ follows from a careful computation of the expectations defining the test loss. The upshot of Lemma \ref{limitinglossinformal} is that it allows us to study the behavior of the minimum value of the test loss $L_T$ as the sequence length tends to infinity. In particular, we can translate the lower bound in Lemma \ref{limitinglossinformal} into a lower bound on the test loss in the limit $T \rightarrow \infty.$ This is the content of our main theoretical result in this section, which we state below.

\begin{theorem}\label{approxlowerbd}
    For any $R > 0$, the limit $\lim_{T \rightarrow \infty} \inf_{\|\mathbf{p}\|,\|\mathbf{q}\| \leq R} L_T(\mathbf{p},\mathbf{q})$ exists, and 
    $$ \lim_{T \rightarrow \infty} \inf_{\|\mathbf{p}\|,\|\mathbf{q}\| \leq R} L_T(\mathbf{p},\mathbf{q}) \geq C(\sigma^2,w_{\min},w_{\max}),
    $$
    where $C(\sigma^2,w_{\min},w_{\max}) > 0$ is the constant from Lemma \ref{limitinglossinformal}.
\end{theorem}
For technical reasons, we must constrain attention parameters $\mathbf{p},\mathbf{q}$ to lie in a ball of radius $R$ for Theorem \ref{approxlowerbd}. It is common in deep learning that model parameters are constrained either by explicit or implicit regularization. Note, however, that the constant in the lower bound of Theorem \ref{approxerrordeeptf} is independent of $R$, so $R$ can be arbitrarily large.

Theorem \ref{approxlowerbd} shows that a single layer of linear attention is unable to accurately capture the dynamics defined by the  system \eqref{dynamicalsystem1D}. To the best of our knowledge, Theorem \ref{approxlowerbd} provides the first \textit{lower bound} for in-context learning of dynamical systems, and it suggests that some care is required in choosing an appropriate architecture in this setting. While our results in this section are stated for 1D dynamical systems, we expect that the results generalize to arbitrary dimensions. For illustrative purposes, let us consider a $d$-dimensional dynamical system $(x_1, \dots, x_T)$ according to Equation \eqref{dynamicalsystem} where the task matrix $W \in \R^{d \times d}$ is diagonal. Then, since the noise is isotropic, this is equivalent to specifying a system of $d$ independent 1D dynamical systems of the form in Equation \eqref{dynamicalsystem1D}. We therefore expect that in-context learning a multi-dimensional dynamical system is at least as difficult as in-context learning a system of 1D dynamical, for which we have lower bounds. We leave it to future work to rigorously generalize Theorem \ref{approxlowerbd} to higher dimensions. 

\subsection{Extension to general single-layer linear transformers}\label{generaltfcase}
At a high level, the non-zero lower bound on the limiting loss in Lemma \ref{limitinglossinformal} follows from the fact that $\ell(\mathbf{p},\mathbf{q},w)$ is a polynomial in the parameters, but a non-polynomial function in $w$ (in particular, it depends on $(1-w^2)^{-1}$). Since this is still the case if we consider a fully-parameterized single-layer linear transformer with an MLP matrix, we expect an analogous result to hold for general single-layer linear transformers. However, the explicit computation of the limiting loss is more complicated, and we leave it as an interesting avenue for future work to justify this rigorously.

\subsection{In-context learning with IID vs non-IID data:}\label{iidvsnoniid}
Our theoretical results hold for non-independent data generated according to a dynamical system. A more common setting for theoretical analysis is the in-context learning of IID data, where a model, given a prompt of independent observations $((x_1,f(x_1),\dots,(x_T,f(x_T)))$ of some unseen task $f$ and an unlabeled query point $x_{T+1}$, is asked to predict the new label $f(x_{T+1}).$ Several recent works \cite{zhang2023trained,ahn2023transformers,mahankali2023one} have analyzed this problem when the task $f$ is given by a linear map $f(x) = \langle w, x \rangle.$ In this case, it has been shown that a single layer of linear attention can achieve 0 test error in the limit $T \rightarrow \infty$, even when the loss is measured by the $L^{\infty}$ norm on the task space. In contrast, we have shown in Theorem \ref{approxlowerbd}, for an analogous problem involving non-IID data, that a single layer of linear attention can never achieve zero test loss in the large sample limit.

The intuitive explanation for such a discrepancy can be understood as follows. In the IID setting, it has been shown that the optimal parameterization of a linear attention layer depends on the distribution $P_X$ of the covariates $\{x_1,x_2, \dots, x_T\}$. Specifically, \cite{zhang2023trained} shows that a block of the optimal attention matrix $W_Q$ learns to invert the covariance matrix $\textrm{Cov}(P_X)$, which is independent of the task vector $w \in \R^d$. For dynamical systems $(x_1, \dots, x_T)$ given by Equation \eqref{dynamicalsystem}, this construction fails to perform in-context learning because \textit{the covariate distribution depends on the task matrix $W$}. This indicates that transformers may have more difficulty performing in-context learning on non-IID data compared to IID data.

\section{Proof sketches}\label{proofsketchsection}
We provide an overview of the techniques used to prove Theorems \ref{approxerrordeeptf} and \ref{approxlowerbd}.

\subsection{Proof sketch for Theorem \ref{approxerrordeeptf}}\label{proofsketchupperbd}
As noted in Section \ref{approxupperbound}, our proof of Theorem \ref{approxerrordeeptf} leverages \textit{algorithm unrolling}, a powerful technique for proving approximation error bounds whereby a deep model encodes a learning algorithm, with each layer of the model corresponding to an iteration of the algorithm. Specifically, we use a transformer to unroll the least-squares algorithm, which approximates the matrix $W$ defining the dynamical system define in Equation \eqref{dynamicalsystem} given the history up to time $T$ by
$$ \widehat{W}_T := \textrm{argmin}_W \frac{1}{T} \sum_{i=0}^{T-1} \|x_{i+1} - Wx_i\|^2 = \left(\frac{1}{T} \sum_{i=0}^{T-1} x_{i+1} x_i^T \right) \left(\frac{1}{T} \sum_{i=0}^{T-1} x_{i} x_i^T \right)^{-1}.
$$
Given the history of the dynamical system $(x_0, \dots, x_T)$ up to time $T$, the goal of the transformer is to predict the conditional expectation $\E[x_{T+1}|x_T] = Wx_T.$ In light of the discussion above natural estimator for $x_{T+1}$ is $\widehat{W}_T X_T$. Let $X_T = \frac{1}{T} \sum_{i=0}^{T-1} x_i x_i^T$ denote the sample covariance matrix of the dynamical system, and assume for the moment that $X_T$ is invertible. Note that computing $\widehat{W}_T x_T$ requires one to find a vector $z_T$ such that $X_T^{-1} x_T = z_T$, or $X_T z_T = x_T$. A classical iterative method for solving linear systems of this form is the \textit{modified Richardson iteration}. Given a linear system $Ax = b,$ the modified Richardson iteration with step size $\alpha > 0$ approximates the solution $x$ by the update rule
$$ x_{(\ell)} = (\mathbf{I}_d - \alpha A) x_{(\ell-1)} + \alpha b.
$$
A key technical lemma in proving Theorem \ref{approxerrordeeptf} constructs a transformer which approximates the least-squares prediction $\widehat{W}_T x_T$ by unrolling the modified Richardson iteration. Let us denote by $z_{T,L}$ the $L^{\textrm{th}}$ step of the modified Richardson iteration 
$$ \begin{cases}
    z_{T,\ell} = (\mathbf{I}_d -\alpha X_T) z_{T,\ell-1} + \alpha x_T, \; \ell \geq 0 \\
    z_{T,0} = 0.
\end{cases}
$$
We state the result precisely below. 

\begin{lemma}\label{richardsonunrolling}
    For any positive integer $L$ and $\alpha > 0$, there exists an $(L+1)$-layer transformer with parameters $\theta^{\ast} = \{W_{MLP}^{(\ell)}, W_P^{(\ell)}, W_Q^{(\ell)}\}_{\ell=1}^{L+1}$ such that for any dynamical system $(x_0, x_1, \dots, x_T)$ defined by \eqref{dynamicalsystem}, $\theta^{\ast}$ maps the sequence $(e_1, \dots, e_T)$ defined by Equation \eqref{dynamicalsystem} to a sequence $(\widehat{e}_1, \dots, \widehat{e}_T)$ where, for each $1 \leq t \leq T$
    $$ \widehat{y}_t := (\widehat{e}_t)_{1:d} = \left( \frac{1}{t} \sum_{i=0}^{t-1} x_{i+1} x_i^T \right) z_{t,L}.
    $$
    In other words, there exists a transformer which estimates $x_{T+1}$ by 1) approximating the matrix-vector product $X_T^{-1} x_T$ via the Richardson iteration, and 2) using the approximation to $X_T^{-1} x_T$ to compute an approximation to the least-squares prediction $\widehat{x}_{T+1}$ for $x_{T+1}.$
\end{lemma}
To translate Lemma \ref{richardsonunrolling} into a bound on the test loss $L_T(\theta)$, there are several technical hurdles. First, in order for the least-squares estimator to be well-defined, the sample covariance matrix $x_T$ must be invertible. In this case, if we hope to obtain any quantitative bounds, we need upper bounds on the condition number of the sample covariance (which governs the rate of convergence of the modified Richardson iteration) and estimates on the statistical performance of the least-squares estimator. To this end, we apply results from \cite{foster2020learning} and \cite{matni2019tutorial}, which provide high-probability guarantees on the condition number of $X_T$ and the covariance and the discrepancy $\|\widehat{W}_T - W\|$, respectively. If we let $\mathcal{A}$ denote the event on which these hold (see a precise definition in Appendix \ref{appendixupperbound}), then, for fixed $W \in \mathcal{W},$ we can bound the $L^2$-error of the transformer by
$$ \E \left[\left\|\widehat{y}_t - Wx_T \right\|^2  \right] \leq 2 \E \left[ \left\|\widehat{y}_t - Wx_T \right\|^2 \cdot 1_{\mathcal{A}} \right] + 2\E \left[ \left\|(\widehat{y}_t - Wx_T) \right\|^2 \cdot 1_{\mathcal{A}^c} \right].
$$
On the event $\mathcal{A}$, we can bound the expectation further by 
$$ \E \left[ \left\|\widehat{y}_t - Wx_T \right\|^2 \cdot 1_{\mathcal{A}} \right] \leq 2 \E \left[ \left\|\widehat{y}_t - \widehat{W}_T x_T \right\|^2 \cdot 1_{\mathcal{A}} \right] + 2 \E \left[ \left\|\left(\widehat{W}_T-W\right)x_T \right\|^2 \cdot 1_{\mathcal{A}} \right].
$$
The first term above can be bounded by leveraging the convergence rate of the modified Richardson iteration, while the second term can be bounded by applying the bound on $\|\widehat{W}_T - W\|.$ The two terms can be balanced by choosing the step size as an appropriate function of $T$. When $L = O(\log(T))$, this yields a bound of $O\left(\frac{\log(T)}{T} \right).$ To bound the expectation on $\mathcal{A}$, we can apply the Cauchy-Schwartz inequality
$$ \E \left[ \left\|(\widehat{y}_t - Wx_T) \right\|^2 \cdot 1_{\mathcal{A}^c} \right] \leq \E \left[ \left\|(\widehat{y}_t - Wx_T) \right\|^4 \right]^{1/2} \cdot \mathbb{P} \left(\mathcal{A}^c \right)^{1/2}.
$$
There is some care required to handle this term, because, away from the event $\mathcal{A}$ the norm of $\widehat{y}_T$ can grow with the step size $L$, which could potentially offset the decay of $\mathbb{P} \left(\mathcal{A}^c \right)^{1/2}.$ In Lemma \ref{richardsoniteratebound} in Appendix \ref{aux}, we bound the moments of the Richardson iterate $z_{T,L}$, which allows us to prove that the above term is $O\left(\frac{\log(T)}{T} \right)$ when $L = O(\log(T)).$ This proves that $\E \left[\left\|\widehat{y}_t - Wx_T \right\|^2  \right] = O\left(\frac{\log(T)}{T} \right),$ for each $W \in \mathcal{W}$, and taking the supremum over $\mathcal{W}$ gives the result of Theorem \ref{approxerrordeeptf}. See Appendix \ref{appendixupperbound} for the detailed proof.

\subsection{Proof sketch for Theorem \ref{approxlowerbd}}
A great deal of the technical work in proving Theorem \ref{approxlowerbd} lies in computing the limit of the individual loss function in Lemma \ref{limitinglossinformal}:
$$\lim_{T \rightarrow \infty} \ell_T(\mathbf{p},\mathbf{q},w) :=  \ell(\mathbf{p},\mathbf{q},w) = \frac{\sigma^2}{1-w^2} \left( \frac{\sigma^2}{1-w^2}(w\alpha_1 + \alpha_2) + w \right)^2,
    $$
where $\alpha_1 = p_1q_2 + p_2q_1$ and $\alpha_2 =\mathbf{p}^T \mathbf{q}.$ Since these computations are cumbersome, we defer them to the Appendix (see Appendix \ref{appendixlowerbound} and also Appendix \ref{aux} for proofs of auxiliary moment computations). While the closed form expression for $\ell_T(\mathbf{p}<\mathbf{q},w)$ is a lengthy sum of many different terms, the ergodicity of the dynamical system ensures that only a few terms survive in the limit; this is an important theme of our computations. Once this formula is established, the lower bound on $\inf_{\mathbf{p},\mathbf{q}} \sup_{w \in [w_{\min},w_{\max}]} \ell(\mathbf{p},\mathbf{q},w)$ stated in Lemma \ref{limitinglossinformal} can be argued as follows: for any finite collection $\{w_1, \dots, w_K\} \in [w_{\min},w_{\max}],$ we have the lower bound
$$ \inf_{\mathbf{p},\mathbf{q}} \sup_{w \in [w_{\min},w_{\max}]} \ell(\mathbf{p},\mathbf{q},w) \geq \inf_{\alpha_1,\alpha_2 \in \R} \max_{1 \leq i \leq K} \frac{\sigma^2}{1-w_i^2} \left( \frac{\sigma^2}{1-w_i^2}(w_i\alpha_1 + \alpha_2) + w_i \right)^2.
$$
In other words, we replace the infimum over $\mathbf{p},\mathbf{q} \in \R^2$ with the infimum over all $\alpha_1,\alpha_2 \in \R$ and we replace the supremum over $[w_{\min},w_{\max}]$ with a finite maximum. The infimum is attained at a pair $(\alpha_1^{\ast},\alpha_2^{\ast})$ at which the graphs of a subset of the $K$ curves intersects (see Equation \eqref{kcurves} in Appendix \ref{appendixlowerbound} for details here). For appropriately chosen $w_1, \dots, w_K$, it can be shown that this infimum is equal to zero if and only if $w_1, \dots, w_K$ solve a certain linear system. The lower bound then follows from recognizing that the system is inconsistent.

Once the lower bound in Lemma \ref{limitinglossinformal} is proven, Theorem \ref{approxlowerbd} can be proven by using the regularity of the family of individual loss functions $\{(\mathbf{p},\mathbf{q},w) \mapsto \ell_T(\mathbf{p},\mathbf{q},w)\}$ to interchange limits and suprema/infima.

\section{Conclusion and discussion}\label{conclusion}
We studied the approximation power of transformers for performing in-context learning on data arising from linear dynamical systems. For multilayer  transformers, we showed that logarithmic depth is sufficient to achieve fast decay of the test loss as the context length tends to infinity. Conversely, we proved a lower bound for single-layer linear transformers which suggest their incapability of learning such dynamical systems in-context. We also provided numerical results that confirmed the benefits of increasing the depth of the transformer in improving the prediction performance.  %We discussed the technical reasons for this apparent difference between in-context learning with IID and non-IID data. 

There are several important directions for future research. First, we would like to better understand the apparent depth-separation observed in this paper. In particular, it remains to be determined whether there is a transformer with $O(1)$ depth whose test loss vanishes as the sequence $T \rightarrow \infty$. If this question can be answered affirmatively, we would also like to describe the mesa-optimization algorithm that such a transformer encodes. Second, it would be interesting to generalize the analysis of this paper to \textit{nonlinear} dynamical systems. We anticipate the the unrolling idea may still be effective, but carrying this out is highly non-trivial as the least-square estimator does not admit a closed form in the nonlinear setting. Finally, although this paper focused on the approximation only, it remains an open question to investigate whether the transformers trained on in-context examples of the linear dynamical system \eqref{dynamicalsystem} can in-context learn the dynamical system, in the sense of the uniform loss in Equation \eqref{test}.  We leave these various problems to future work.
%This is a highly nontrivial question, since transformers are only trained on an $L^2$ loss. An affirmative answer would be a major step towards understanding the emergent capabilities of transformers.
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{refs}

\appendix

\newpage 
% \crefalias{section}{appendix} % uncomment if you are using cleveref

\section{Proofs for Section \ref{approxupperbound}}\label{appendixupperbound}
We recall some notation from Section \ref{proofsketchupperbd}. We let $X_T = \frac{1}{T} \sum_{i=0}^{T-1} x_i x_i^T$ denote the sample covariance matrix of the dynamical system and $\widehat{W}_T$ denote the least-squares estimator of the dynamical system given by

\begin{equation}\label{leastsquares} \widehat{W}_t =  \textrm{argmin}_{W} \sum_{t=0}^{T-1} \|x_{i+1}-Wx_i\|^2  = \left(\frac{1}{T} \sum_{i=0}^{T-1} x_{i+1} x_i^T \right) \left(\underbrace{\frac{1}{T} \sum_{i=0}^{T-1} x_{i} x_i^T}_{=X_T} \right)^{-1}.
\end{equation}

We let $z_{T,L}$ be defined by the modified Richardson iteration
\begin{equation}\label{richardsonotation}
    \begin{cases}
        z_{T,L} = z_{T,L-1} + \alpha \left(x_T - X_T z_{T,L-1} \right), \; L > 0 \\
        z_{T,0} = 0.
    \end{cases}
\end{equation}

We make use of the following classical result on the convergence of the modified Richardson iteration.

\begin{lemma}\label{richardsonconvergence}[Richardson iteration, \cite{varga1962iterative}]
    Assume that the matrix $A \in \R^{d \times d}$ is invertible let $\alpha > 0$ be small enough that so that $\|\mathbf{I}_d-\alpha A\|_{\textrm{op}} < 1.$ Let $x_{\ast} \in \R^d$ denote the solution to the equation $Ax = b$ and let $x_{(k)}$ denote the vector obtained at the $k^{\textrm{th}}$ step of Richardson iteration with initialization $x_0 = 0$. Then $x_k$ takes the explicit form
    $$ x_{(k)} = \left(\mathbf{I}_d - \left(\mathbf{I}_d-\alpha A \right)^k \right)x_{\ast},
    $$
    and consequently, the error bound
    $$ \|x_{\ast} - x_{(k)}\| \leq \|\mathbf{I}_d-\alpha A\|_{\textrm{op}}^k \|x_{\ast}\|
    $$
    holds for all $k$.
\end{lemma}
In particular, Lemma \ref{richardsonconvergence} shows that the modified Richardson iterates converge to the true solution of the linear system at an exponential rate. For a fixed matrix $A$, the optimal step size $\alpha$ depends on the smallest and largest eigenvalues of $A$. 

We are interested in constructing a transformer which approximately implements the least-squares prediction $\widehat{W}_T x_T$ to estimate $x_{T+1}$. In order to approximate the term $X_T^{-1} x_T$ appearing in the expression for $\widehat{W}_T x_T$, we design a transformer which unrolls the modified Richardson iteration to solve a related linear system. Here, depth plays a crucial role, as each layer of the transformer represents a step of the modified Richardson iteration. Our result is stated precisely in Lemma \ref{richardsonunrolling} in Section \ref{proofsketchupperbd}. We now present the proof of this result.

\begin{proof}[Proof of Lemma \ref{richardsonunrolling}]
    \textbf{Step 1:} Let $a_1, \dots, a_T, b_1, \dots, b_T \in \R^d$ denote fixed vectors. We first show that for any sequence of tokens $(e_1, \dots, e_t)$ where $e_i = (a_t, b_t, x_t, x_{t-1})^T$, there exists a single layer transformer which maps the sequence $(e_1, \dots, e_T)$ to $(\widehat{e_1}, \dots, \widehat{e}_T),$ where 
    $$ \widehat{e_t} = \begin{pmatrix}
        a_t + \alpha(x_{t} - \alpha X_{t} a_t) \\ b_t + \alpha(x_{t} - X_{t} b_t) \\ x_t \\ x_{t-1}
    \end{pmatrix}.
    $$
    In words, there exists a single-layer transformer block which simultaneously implements one step of the Richardson iteration for $X_{t-1}^{-1} x_{t-1} = b$ initialized at $a_t$ and $b_t$ with step size $\alpha$. To prove this, let us first define the attention weights by
$$ W_P = \begin{pmatrix}
    0 & 0 & 0 & \mathbf{I}_d \\
    0 & 0 & 0 & \mathbf{I}_d \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
\end{pmatrix} \in \R^{4d \times 4d}, \; \; \; W_Q = \begin{pmatrix}
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    -\alpha \mathbf{I}_d & 0 & 0 & 0
\end{pmatrix} \in \R^{4d \times 4d}.
$$ 
Then we have
\begin{align*}
    W_P \cdot E_t E_t^T \cdot W_Q e_t &= \begin{pmatrix}
    0 & 0 & 0 & \mathbf{I}_d \\
    0 & 0 & 0 & \mathbf{I}_d \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
\end{pmatrix} \cdot \frac{1}{t} \sum_{i=1}^{t} \begin{pmatrix}
    a_i \\ b_i \\ x_i \\ x_{i-1}
\end{pmatrix} \begin{pmatrix}
    a_i^T & b_i^T & x_i^T & x_{i-1}^T
\end{pmatrix} \cdot \begin{pmatrix}
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    -\alpha \mathbf{I}_d & 0 & 0 & 0
\end{pmatrix} \begin{pmatrix}
    a_t \\ b_t \\ x_t \ x_{t-1}
\end{pmatrix} \\
&= \frac{1}{t} \sum_{i=1}^{t} \begin{pmatrix}
    x_{i-1} \\ x_{i-1} \\ 0 \\ 0
\end{pmatrix} \begin{pmatrix}
    -\alpha x_{i-1}^T & 0^T & 0^T & 0^T
\end{pmatrix} \begin{pmatrix}
    a_t \\ b_t \\ x_{t} \\ x_{t-1}
\end{pmatrix} \\
&= \frac{1}{t} \sum_{i=1}^{t} \begin{pmatrix}
    -\alpha x_{i-1} x_{i-1}^T & 0 & 0 & 0 \\
    -\alpha x_{i-1} x_{i-1}^T & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0
\end{pmatrix} \begin{pmatrix}
    a_t \\ b_t \\ x_t \\ x_{t-1}
\end{pmatrix} \\
&= \begin{pmatrix}
    -\alpha X_{t} a_t \\ -\alpha X_{t} b_t \\ 0 \\ 0
\end{pmatrix}.
\end{align*} 
It follows that
$$ e_t + W_P \cdot E_t E_t^T \cdot W_Q e_t = \begin{pmatrix}
    a_t -\alpha X_{t} a_t \\
    b_t -\alpha X_{t} b_t \\
    x_t \\
    x_{t-1}
\end{pmatrix}.
$$
Now, let $W_{MLP} \in \R^{4d \times 4d}$ be any matrix such that
$$ W_{MLP} \begin{pmatrix}
    a_t -\alpha X_{t} a_t \\
    b_t -\alpha X_{t} b_t \\
    x_t \\
    x_{t-1}
\end{pmatrix} = \begin{pmatrix}
    a_t +\alpha(x_{t} - X_{t} a_t) \\
    b_t + \alpha(x_{t} - X_{t} b_t) \\
    x_t \\
    x_{t-1}
\end{pmatrix}.
$$
Then the transformer block with weights $W_{MLP}, W_P, W_Q$ maps $e_t$ to 
\begin{align*}
    \widehat{e}_t &= W_{MLP} \left(e_t + W_P \cdot E_t E_t^T \cdot W_Q e_t \right) \\
    &= W_{MLP} \begin{pmatrix}
    a_t -\alpha X_{t} a_t \\
    b_t -\alpha X_{t} b_t \\
    x_t \\
    x_{t-1}
\end{pmatrix} \\
&= \begin{pmatrix}
    a_t +\alpha(x_t - X_{t} a_t) \\
    b_t + \alpha(x_t - X_{t} b_t) \\
    x_t \\
    x_{t-1}
\end{pmatrix}.
\end{align*}
\textbf{Step 2:} By composing the transformer blocks from Step 1, we have shown that there exists an $L$-layer transformer which maps the token $e_t = \begin{pmatrix}
    0 \\ 0 \\ x_t \\ x_{t-1}
\end{pmatrix}$ to the token
$$ \widehat{e}_t^{(L)} = \begin{pmatrix}
    z_{t,L}\\ z_{t,L} \\ x_t \\ x_{t-1},
\end{pmatrix}
$$
where $z_{t,L}$ Richardson iterate defined in Equation \eqref{richardsonotation}.
\textbf{Step 3:} We now show that for any sequence of tokens $(e_1, \dots, e_T)$ with $e_i = (a_i, a_i, x_i, x_{i-1})^T$, there exists a single layer transformer block which maps $(e_1, \dots, e_T)$ to $(\widehat{e}_1, \dots, \widehat{e}_T)$, where
$$ (\widehat{e}_t)_{1:d} = \left(\frac{1}{t} \sum_{i=0}^{t-1} x_{i+1} x_i^T \right) a_t.
$$
To prove this, we use a similar construction to that in Step 1. Let us first define attention weights by 
$$ W_P = \begin{pmatrix}
    0 & 0 & \mathbf{I}_d & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 
\end{pmatrix}, \; \; \; W_Q = \begin{pmatrix}
    0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ \mathbf{I}_d & 0 & 0 & 0
\end{pmatrix}.
$$
Then we have
\begin{align*}
    W_P \cdot E_t E_t^T \cdot E_W e_t &= \begin{pmatrix}
    0 & 0 & \mathbf{I}_d & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 
\end{pmatrix} \cdot \frac{1}{t} \sum_{i=1}^{t} \begin{pmatrix}
    a_i \\ a_i \\ x_i \\ x_{i-1}
\end{pmatrix} \begin{pmatrix}a_i \\ a_i \\ x_i \\ x_{i-1} \end{pmatrix}^T \cdot \begin{pmatrix}
    0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ \mathbf{I}_d & 0 & 0 & 0
\end{pmatrix} \begin{pmatrix}
    a_t \\ a_t \\ x_t \\ x_{t-1}
\end{pmatrix} \\
&= \frac{1}{t} \sum_{i=1}^{t} \begin{pmatrix}
    x_{i} \\ 0 \\ 0 \\ 0 
\end{pmatrix} \begin{pmatrix}
    x_{i-1}^T & 0^T & 0^T & 0^T 
\end{pmatrix} \begin{pmatrix}
    a_t \\ a_t \\ x_t \\ x_{t-1}
\end{pmatrix} \\
&= \begin{pmatrix}
    \left( \frac{1}{t} \sum_{i=0}^{t-1} x_{i+1} x_i^T \right) a_t \\ 0 \\ 0 \\ 0
\end{pmatrix}.
\end{align*} 
It follows that
$$ e_t + W_P \cdot E_t E_t^T \cdot E_W e_t = \begin{pmatrix}
   a_t +  \left( \frac{1}{t} \sum_{i=0}^{t-1} x_{i+1} x_i^T \right) a_t \\ a_t \\ x_t \ x_{t-1}
\end{pmatrix}.
$$
We then define $W_{MLP}$ to be any matrix such that
$$ \left(W_{MLP} \begin{pmatrix}
    a_t +  \left( \frac{1}{t} \sum_{i=0}^{t-1} x_{i+1} x_i^T \right) a_t \\ a_t \\ x_t \ x_{t-1}
\end{pmatrix}\right)_{1:d} = \left( \frac{1}{t} \sum_{i=0}^{t-1} x_{i+1} x_i^T \right) a_t.
$$
It then follows that, for $\widehat{e}_t = W_{MLP} \left( e_t + W_P \cdot E_t E_t^T \cdot W_Q e_t \right),$ we have
$$ (\widehat{e}_t)_{1:d} = \left( \frac{1}{t} \sum_{i=0}^{t-1} x_{i+1} x_i^T \right) a_t.
$$

\textbf{Step 4:}
To conclude the proof, we compose the $L$-layer transformer constructed in step 2 with the single-layer transformer constructed in step 3 to construct an $(L+1)$-layer transformer which realizes the mapping
$$ e_t = \begin{pmatrix}
    0 \\ 0 \\ x_t \\ x_{t-1} 
\end{pmatrix} \overbrace{\mapsto}^{\textrm{$L$ layers from Step 2}} \begin{pmatrix} 
    z_{t,L} \\z_{t,L} \\ x_t \\ x_{t-1}
\end{pmatrix} \overbrace{\mapsto}^{\textrm{Single layer from Step 3}} \begin{pmatrix}
    \left(\frac{1}{t} \sum_{i=0}^{t-1} x_{i+1} x_i^T \right) z_{t,L}  \\ * \\ * \\ *
\end{pmatrix},
$$
where $*$ denotes entries which are not used in prediction. Since $\widehat{x}_{t+1,L} = \left(\frac{1}{t} \sum_{i=0}^{t-1} x_{i+1} x_i^T \right) z_{t,L},$ this proves the desired claim.
\end{proof}

Before proving Theorem \ref{approxerrordeeptf}, we need to state some technical results about the dynamical system in Equation \eqref{dynamicalsystem} and the least-squares estimator for its state matrix. First, we need bounds on the smallest and largest eigenvalues of the covariance matrix $X_T$. Such bounds are necessary for our proof, because the least squares estimator is not even well-defined unless the covariance is invertible, and in this case the condition number of $X_T$ governs the convergence rate of the associated Richardson iteration. To this end, we quote the following result from \cite{foster2020learning}. Note that we have adapted the original result to our setting.

\begin{lemma}\label{covariancebounds}[\cite{foster2020learning}, Theorem 1]
    Let $(x_1, \dots, x_t)$ be generated according to Equation \eqref{dynamicalsystem}. Then there is a numerical constant $c > 0$ such that, for any $\delta > 0$, as long as $T \geq  cd \frac{\sigma^4}{(1-w_{\max}^2)^2}\log\left(\frac{d}{\delta(1+w_{\max}^2)}+1 \right)$, we have
    \begin{equation}\label{upperlowerisom} \frac{\sigma^2}{4} \cdot \mathbf{I}_d \prec X_t \prec \frac{4 \sigma^2 d}{(1-w_{\max}^2)} \cdot \mathbf{I}_d
    \end{equation}
    with probability at least $1-\delta.$ In particular, as long as $T$ is sufficiently large, the bound in Equation \eqref{upperlowerisom} holds with probability $\frac{1}{2T^4}.$
\end{lemma}

The second technical result, due to \cite{matni2019tutorial}, provides a high-probability bound for the error of the least-squares estimator. This is crucial in bounding the test error achieved by the transformer constructed in Lemma \ref{richardsonunrolling}.

\begin{lemma}[\cite{matni2019tutorial}, Theorem 4.2]\label{leastsquaresproperties}
    Let $(x_0, x_1, \dots, x_t)$ denote the dynamical system described by Equation \eqref{dynamicalsystem} and let $\widehat{W}_t$ denote the least-squares estimator \eqref{leastsquares} for the state matrix $W$. Then there exists a universal constant $c > 0$ such that whenever $T \geq \frac{cd \log(d/\delta)}{1-w_{\max}} \log \left(\frac{2\sigma^2}{1-w_{\max}^2} \right),$ we have
    $$ \E \left\|W-\widehat{W}_T \right\|_{\textrm{op}}^2 \lesssim \frac{\sigma^2 d \log(d/\delta)}{T(1-w_{\max}^2)}.
    $$
    with probability at least $1-\delta.$ In particular, as long as $T$ is sufficiently large, we have
   \begin{equation}\label{leastsquaresbounds} \E \left\|W-\widehat{W}_T \right\|_{\textrm{op}}^2 \lesssim \frac{\sigma^2 \left(\log(T) + \log(4d) \right)}{T(1-w_{\max}^2)}
    \end{equation}
    with probability at least $1- \frac{1}{2T^4}.$

\end{lemma}
We now give the proof of Theorem \ref{approxerrordeeptf}.

\begin{proof}[Proof of Theorem \ref{approxerrordeeptf}]
    We let $\widehat{y}_T$ denote the prediction of the $(L+1)$-layer transformer constructed in Lemma \ref{richardsonunrolling}, where the step size $\alpha \in (0,1)$ is independent of $T$ and $L$ and  will be specified to satisfy some constraints later in the proof. Let $\mathcal{A}_T$ denote the event on which the bounds in Equations \eqref{upperlowerisom} and \eqref{leastsquaresbounds} hold, and assume that $T$ is sufficiently large that
    $$ \mathbb{P} \left(\mathcal{A} \right) \geq 1-\frac{1}{T^4},
    $$
    We can now write
    \begin{align*}
        \E \left\|\widehat{y}_T - Wx_T \right\|^2 &=  \E \left\|\left( \widehat{y}_T - Wx_T \right) (1_{\mathcal{A}_T} + 1_{\mathcal{A}_T^c}) \right\|^2 \\
        &\leq 2 \E \left\| \left( \widehat{y}_T - Wx_T \right) 1_{\mathcal{A}_T} \right\|^2 + 2\E \left\| \left( \widehat{y}_T - Wx_T \right) 1_{\mathcal{A}_T^c} \right\|^2 
    \end{align*}
    On the event $\mathcal{A}_T$, the least-squares estimator $\widehat{W}_T$ is well-defined, so we can bound the first expectation above by
    \begin{align*}
        \E \left\| \left( \widehat{y}_T - Wx_T \right) 1_{\mathcal{A}_T} \right\| &\lesssim 2 \E \left\| \left( \widehat{y}_T - \widehat{W}_T x_T \right) 1_{\mathcal{A}_T} \right\|^2 + 2\E \left\| \left( \widehat{W}_T - W \right) x_T \cdot 1_{\mathcal{A}_T} \right\|^2.
    \end{align*}
    For the first term, we recall that the transformer prediction $\widehat{y}_T$ can be written as
    \begin{align*}
        \widehat{y}_T = \left(\frac{1}{T} \sum_{i=0}^{T-1} x_{i+1} x_i^T \right) z_{T,L},
    \end{align*}
    where $z_{T,L}$ is the $L^{\textrm{th}}$ iterate of the Richardson method defined in Equation \eqref{richardsonotation}. In particular, Lemma \ref{richardsonconvergence} implies that
    $$ \|z_{T,L} - X_T^{-1} x_T\| \leq \|\mathbf{I}_d - \alpha X_T\|_{\textrm{op}}^L \|X_T^{-1} x_T\|.
    $$
    Now, notice that $|\mathbf{I}_d - \alpha X_T\|_{\textrm{op}} = \max \left(\left| 1-\alpha \lambda_{\max}(X_T) \right|, \left| 1-\lambda_{\min}(X_T) \right| \right).$ Since $\lambda_{\max}(X_T)$ and $\lambda_{\min}(X_T)$ are bounded on $\mathcal{A}_T$ according to Equation \eqref{upperlowerisom}, we can choose $\alpha$ depending only on $d$, $\sigma^2$, and $w_{\max}$ to ensure that $\|\mathbf{I}_d - \alpha X_T\|_{\textrm{op}} \leq c_{\alpha}$ for some constant $c_{\alpha} \in (0,1)$ whenever $\mathcal{A}_T$ holds. In particular, if we set
    $$ \alpha = \frac{8(1-w_{\max}^2)}{\sigma^2 \left(16d + (1-w_{\max}^2) \right)},
    $$
    then Equation \eqref{upperlowerisom} implies that $$c_{\alpha} = \left(1 - \frac{4(1-w_{\max}^2)}{16d + (1-w_{\max}^2)} \right),$$ and $\|\mathbf{I}_d - \alpha X_T\| \leq c_{\alpha}$ on $\mathcal{A}_T$. In turn, this allows us to bound
    \begin{align*}
        \E \left\|\left(\widehat{y}_T - \widehat{W}_T x_T \right) 1_{\mathcal{A}_T} \right\|^2 &= \E \left\| \left(\frac{1}{T} \sum_{i=0}^{T-1} x_{i+1} x_i^T \right) \left(z_{T,L} - X_T^{-1} x_T \right) 1_{\mathcal{A}_T} \right\|^2 \\
        &\leq c_{\alpha}^{2L} \E \left\| \left(\frac{1}{T} \sum_{i=0}^{T-1} x_{i+1} x_i^T \right) \|X_T^{-1}\|_{\textrm{op}} \|x_T\| \right\|^2 \\
        &\leq \frac{16 c_{\alpha}^{2L}}{\sigma^4}  \E \left\| \left(\frac{1}{T} \sum_{i=0}^{T-1} x_{i+1} x_i^T \right)  \|x_T\| \cdot 1_{\mathcal{A}_T} \right\|^2,
    \end{align*}
    where we used that $\|X_T^{-1}\|_{\textrm{op}} \leq \frac{16}{\sigma^4}$ on $\mathcal{A}_T$. This gives the final bound
    \begin{equation}\label{term1}
         \E \left\|\left(\widehat{y}_T - \widehat{W}_T x_T \right) 1_{\mathcal{A}_T} \right\|^2 \leq \frac{16}{\sigma^4} \cdot M_{1,\sigma,w_{\max}} \cdot c_{\alpha}^{2L},
    \end{equation}
    where we have defined 
    $$ M_{1,\sigma,w_{\max}} := \sup_t \E\left[ \left\|\frac{1}{T} \sum_{i=0}^{T-1} x_{i+1} x_i^T \right\|_{\textrm{op}}^2 \|x_T\|^2 \right].
    $$
    Since the dynamical system is geometrically ergodic, the constant $M_{1,\sigma,w_{\max}}$ is finite and depends only on $\sigma^2,$ $w_{\max},$ and $d$ (the techniques of Lemma \ref{6thmoment} in Appendix \ref{aux} can be used to bound $M_{1,\sigma,w_{\max}}$ by the sixth moment of an appropriate Gaussian). To bound the difference between $\widehat{W}_T$ and $W$, we use Lemma \ref{leastsquaresproperties}, which gives the bound
    \begin{align}\label{term2}
        2\E \left\| \left( \widehat{W}_T - W \right) x_T \cdot 1_{\mathcal{A}_T} \right\|^2 &\lesssim \frac{\sigma^2 \left(\log(T) + \log(4d) \right)}{T(1-w_{\max}^2)} \E[\|x_T\|^2] \\
        &\leq \frac{d \sigma^4 \left(\log(T) + \log(4d) \right)}{T(1-w_{\max}^2)^2},
    \end{align}
    where we used Lemma \ref{propertiesofdynamics} to bound $\E[\|x_T\|^2].$ We bound the expectation on $\mathcal{A}_T^c$ by the Cauchy-Schwarz inequality:
    \begin{align*}
        \E \left\| \left( \widehat{y}_T - Wx_T \right) 1_{\mathcal{A}_T^c} \right\|^2 &\leq \E \left[ \left\|\widehat{y}_T - Wx_T \right\|^4 \right]^{1/2} \cdot \mathbb{P}(\mathcal{A}_T^c) \\
        &\leq 2 \left(\E\|\widehat{y}_T\|^4 + \E\|Wx_T\|^4 \right)^{1/2} \cdot \mathbb{P}(\mathcal{A_T}^c) \\
        &\leq 2 \left(\E \left\|\left( \frac{1}{T} \sum_{i=0}^{T-1} x_{i+1} x_i^T \right) z_{T,L} \right\|^4 + \frac{d(d+2) \sigma^4}{(1-w_{\max}^2)^2} \right)^{1/2} \cdot \mathbb{P}(\mathcal{A_T}^c) \\
        &\leq 2\left( M_{2,\sigma,w_{\max}} \E[\|z_{T,L}\|^8]^{1/4} + \frac{\sqrt{d(d+2)}\sigma^2}{1-w_{\max}^2} \right) \cdot \mathbb{P}(\mathcal{A_T}^c),
    \end{align*}
    where
    $$ M_{2,\sigma,w_{\max}} := \sup_t \E \left[\left\| \frac{1}{T} \sum_{i=0}^{T-1} x_{i+1} x_i^T \right\|_{\textrm{op}} \right]^{1/4}
    $$
    is a constant depending only on $\sigma,$ $w_{\max}$, and $d$. By Lemma \ref{richardsoniteratebound}, as long as $\alpha$ is small enough that $\alpha < \frac{2(1-w_{\max}^2)}{\sigma^2}$, we have the following bound for $z_{T.L}$
    $$ \E[z_{T,L}\|^8]^{1/4} = O \left(1+L^4 + T^{-2(L-1)} \cdot (16(L-1))!^{1/4} \right).
    $$
    Note also that $\mathbb{P}(\mathcal{A}_T^c) = O \left(T^{-2} \right).$ This gives the bound
    \begin{equation}\label{term3}
         \E \left\| \left( \widehat{y}_T - Wx_T \right) 1_{\mathcal{A}_T^c} \right\|^2 \lesssim  \left( M_{2,\sigma,w_{\max}} \left(1+L^4 + T^{-2(L-1)} \cdot (16(L-1))!^{1/4} \right) + \frac{d \sigma^2}{1-w_{\max}^2} \right) T^{-2}.
    \end{equation}
    Combining Equations \eqref{term1}, \eqref{term2}, and \eqref{term3}, we have the final bound
    \begin{align}
        \E \left\| \widehat{y}_T - Wx_T \right\|^2 &\lesssim \frac{16}{\sigma^4} \cdot M_{1,\sigma,w_{\max}} \cdot c_{\alpha}^{2L} + \frac{d \sigma^4 \left(\log(T) + \log(4d) \right)}{T(1-w_{\max}^2)^2} \\
        &+ \left( M_{2,\sigma,w_{\max}} \left(1+L^4 + T^{-2(L-1)} \cdot (16(L-1))!^{1/4} \right) + \frac{d \sigma^2}{1-w_{\max}^2} \right) T^{-2}.
    \end{align}
    To balance the error between $L$ and $T$, we set
    $$ L = \frac{\log(T) + \log \left( \frac{16}{\sigma^4} M_{1,\sigma,w_{\max}} \right)}{2 \log(1/c_{\alpha})}. 
    $$
    Then it is clear that, when $T$ is sufficiently large, the bound 
    $$ \left( M_{2,\sigma,w_{\max}} \left(1+L^4 + T^{-2(L-1)} \cdot (16(L-1))!^{1/4} \right) + \frac{d \sigma^2}{1-w_{\max}^2} \right) T^{-2} = O \left(\frac{\log(T)}{T} \right)
    $$
    holds. This proves the final bound
    \begin{equation}\label{finalupperbd} \E \left\| \widehat{y}_T - Wx_T \right\|^2 \lesssim \frac{\log(T)}{T},
    \end{equation}
    for $T$ sufficiently large. The implicit constants depend on $\sigma$, $w_{\max}$, and $d$. Moreover, by tracking the implicit constants it can be seen that they depend only polynomially on $d$. Since the test loss is defined as
    $$ L_T(\theta) = \sup_{W \in \mathcal{W}} \E  \left\| \widehat{y}_T - Wx_T \right\|^2,
    $$
    the bound on $L_T$ as stated in Theorem \ref{approxerrordeeptf} follows from taking the supremum over $\mathcal{W}$ on both sides of Equation \eqref{finalupperbd}.
\end{proof}


\section{Proofs for Section \ref{approxlowerbdsec}}\label{appendixlowerbound}
We break up the proof of Lemma \ref{limitinglossinformal} up into two separate lemmas. The first lemma, stated below, proves a formula for the pointwise limit of the individual loss function. The second lemma then proves the lower bound satisfied by the limiting loss. The proof of the first lemma involves several computations involving second, fourth, and sixth moments of the linear dynamical system. The proofs of these technical computations are deferred to Appendix \ref{aux}.

\begin{lemma}\label{limitingloss}
    Consider the individual loss function 
    $$ \ell_T(\mathbf{p},\mathbf{q};w) =  \left( \widehat{y}_T - wx_T \right)^2,
    $$
    where $\mathbf{p},\mathbf{q}$ are the parameters of the linear attention block and $w \in [w_{\min},w_{\max}]$ specifies the dynamical system defined by Equation \eqref{dynamicalsystem1D}. Then for any $\mathbf{p},\mathbf{q} \in \R^2$ and $w \in (0,1)$, we have
    $$ \lim_{T \rightarrow \infty} \ell_T(\mathbf{p},\mathbf{q};w) = \frac{\sigma^2}{1-w^2} \left( \frac{\sigma^2}{1-w^2}(w\alpha_1 + \alpha_2) + w \right)^2 := \ell(\mathbf{p},\mathbf{q},w),
    $$
    where $\alpha_1 = p_1q_2 + p_2q_1$ and $\alpha_2 = p_1q_1 + p_2 q_2.$
\end{lemma}
\begin{proof}
    We recall the formula for $\widehat{y}_t$:
    $$ \widehat{y}_T = \frac{1}{T} \sum_{i=1}^{T} \begin{pmatrix}
        p_1 & p_2
    \end{pmatrix} \begin{pmatrix}
        \sum_{i=1}^{T} x_i^2 & \sum_{i=1}^{T} x_i x_{i-1} \\
        \sum_{i=1}^{T} x_i x_{i-1} & \sum_{i=1}^{T} x_{i-1}^2
    \end{pmatrix} \begin{pmatrix}
        q_1 \\ q_2
    \end{pmatrix} x_T.
    $$
    We aim to compute the limit of the expression 
    \begin{equation} \E(\widehat{y}_T - wx_T)^2 =\E[\widehat{y}_T^2] + w^2\E[x_T^2] - 2w \E[\widehat{y}_T x_T].
    \end{equation}
   \textbf{Step 1:} We first compute $\E[x_T^2].$ By Lemma \ref{propertiesofdynamics}, $x_T \sim N\left(0, \sigma^2\frac{1-w^{2T}}{1-w^2} \right).$ We therefore have
    \begin{equation}
         \lim_{T \rightarrow \infty} \E[x_T^2] = \frac{\sigma^2}{1-w^2}.
    \end{equation}
    \textbf{Step 2:} Next, we compute $\E[\widehat{y}_T x_T].$ From the formula $\widehat{y}_T$, we have
    \begin{align*}
        \widehat{y}_T x_T &= p_1q_1 \left(\frac{1}{T} \sum_{i=1}^{T} x_i^2 x_T^2 \right) + p_2q_2  \left(\frac{1}{T} \sum_{i=1}^{T} x_{i-1}^2 x_T^2 \right) + \left(p_1q_2 + p_2q_1 \right) \left(\frac{1}{T} \sum_{i=1}^{T} x_i x_{i-1} x_T^2 \right).
    \end{align*}
    By Lemma \ref{4thmoments}, we have
    \begin{align*}
        \lim_{T \rightarrow \infty} \widehat{y}_T x_T &= \lim_{T \to \infty} p_1q_1 \left(\frac{1}{T} \sum_{i=1}^{T} x_i^2 x_T^2 \right) + p_2q_2  \left(\frac{1}{T} \sum_{i=1}^{T} x_{i-1}^2 x_T^2 \right) + \left(p_1q_2 + p_2q_1 \right) \left(\frac{1}{T} \sum_{i=1}^{T} x_i x_{i-1} x_T^2 \right) \\
        &= (p_1q_1 + p_2q_2) \frac{\sigma^4}{(1-w^2)^2} + (p_1q_2+p_2q_1) \frac{\sigma^4 w}{(1-w^2)^2} \\
        &= \frac{\sigma^4}{(1-w^2)^2} \begin{pmatrix}
            p_1 & p_2
        \end{pmatrix} \begin{pmatrix}
            1 & w \\ w & 1
        \end{pmatrix} \begin{pmatrix}
            q_1 \\ q_2
        \end{pmatrix}.
    \end{align*}
    \textbf{Step 3:} Next, we compute the limit of
    \begin{align*}
    \mathbb{E} [\widehat{y}_T^2] & = \frac{1}{T^2} \E\left[\left(\begin{pmatrix}
        p_1 & p_2
    \end{pmatrix}
    \begin{pmatrix}
        \sum_{i=1}^T x_i^2 & \sum_{i=1}^T x_i x_{i-1}\\
        \sum_{i=1}^T x_i x_{i-1} & \sum_{i=1}^T x_{i-1}^2
    \end{pmatrix}
    \begin{pmatrix}
        q_1 \\ q_2
    \end{pmatrix}\right)^2 x_t^2\right] \\
    & = \frac{1}{T^2} \E\left[\left(p_1 q_1 \sum_{i=1}^T x_i^2 + (p_1q_2 + q_1p_2) \sum_{i=1}^T x_i x_{i-1} + p_2q_2 \sum_{i=1}^T x_{i-1}^2\right)^2 x_T^2\right] \\
    & = \frac{1}{T^2} \E \left[ \left(p_1 q_1 x_T^2 + (p_1q_2 + q_1p_2) \sum_{i=1}^T x_i x_{i-1} + (p_1q_1 + p_2q_2)\sum_{i=1}^T x_{i-1}^2\right)^2 x_T^2 \right]. 
\end{align*}
There are six terms to be computed. However, as we are only interested in when $T \to \infty$, then only terms with double sum survive. That is, we only need to compute $(\sum_{i=1}^T x_i x_{i-1})^2 x_T^2$, $(\sum_{i=1}^T x_{i-1}^2)^2 x_T^2$ and $(\sum_{i=1}^T x_i x_{i-1}) (\sum_{i=1}^T x_{i-1}^2) x_T^2$. By Lemma \ref{6thmoment}, we have the computations
\begin{align*}
    \lim_{t \rightarrow \infty} \frac{1}{T^2}\E\left[\left( \sum_{i=1}^{T} x_i x_{i-1} \right)^2 x_T^2 \right] &= \frac{\sigma^6 w^2}{(1-w^2)^3} \\
    \lim_{T \rightarrow \infty} \frac{1}{T^2}\E\left[\left( \sum_{i=1}^{T} x_{i-1}^2 \right)^2 x_T^2 \right] &= \frac{\sigma^6}{(1-w^2)^3} \\
    \lim_{T \rightarrow \infty} \frac{1}{T^2} \E \left[\left( \sum_{i=1}^{T} x_i x_{i-1} \right) \left( \sum_{i=1}^{T} x_{i-1}^2 \right) x_T^2 \right] &= \frac{\sigma^6 w}{(1-w^2)^3}.
\end{align*}
Therefore,
\begin{align*}
    \lim_{T \rightarrow \infty} \E[\widehat{y}_t^2] &= \lim_{T \rightarrow \infty} \frac{1}{T^2} \E\left[\left(p_1 q_1 x_t^2 + (p_1q_2 + q_1p_2) \sum_{i=1}^T x_i x_{i-1} + (p_1q_1 + p_2q_2)\sum_{i=1}^T x_{i-1}^2\right)^2 x_T^2\right] \\
    &= \lim_{t \rightarrow \infty} \frac{1}{T^2}\E \Bigg[\Big((p_1q_2 + p_2q_1)^2 \left( \sum_{i=1}^{T} x_i x_{i-1} \right)^2 + (p_1q_1 + p_2q_2)^2 \left( \sum_{i=1}^{T} x_i x_{i-1} \right)^2 \\ &+ 2(p_1q_2 + p_2q_1) (p_1q_1 + p_2q_2) \left( \sum_{i=1}^{T} x_i x_{i-1} \right)\left( \sum_{i=1}^{T} x_i x_{i-1} \right) \Big)x_T^2\Bigg] \\
    &= \frac{\sigma^6}{(1-w^2)^3} \left( \begin{pmatrix}
        p_1 & p_2
    \end{pmatrix} \begin{pmatrix}
        1 & w \\ w & 1
    \end{pmatrix} \begin{pmatrix}
        q_1 \\ q_2
    \end{pmatrix} \right)^2.
\end{align*}
\textbf{Step 4:} Combining steps 1-3, we find that
\begin{align*}
    \lim_{T \rightarrow \infty} \E[(\widehat{y}_T-wx_T)^2] &= \frac{\sigma^6}{(1-w^2)^3} \left( \begin{pmatrix}
        p_1 & p_2
    \end{pmatrix} \begin{pmatrix}
        1 & w \\ w & 1
    \end{pmatrix} \begin{pmatrix}
        q_1 \\ q_2
    \end{pmatrix} \right)^2 \\
    &- 2w \frac{\sigma^4}{(1-w^2)^2} \begin{pmatrix}
            p_1 & p_2
        \end{pmatrix} \begin{pmatrix}
            1 & w \\ w & 1
        \end{pmatrix} \begin{pmatrix}
            q_1 \\ q_2
        \end{pmatrix} + \frac{\sigma^2}{1-w^2} \\
        &= \frac{\sigma^2}{1-w^2} \left(\frac{\sigma^2}{1-w^2} (w \alpha_1 + \alpha_2) + w\right)^2,
\end{align*}
where we have defined $\alpha_1 = p_1q_2 + p_2q_1$ and $\alpha_2 = p_1q_1 + p_2 q_2.$ This concludes the proof.
\end{proof}

We now present the second lemma required to prove Lemma \ref{limitinglossinformal}.
\begin{lemma}\label{lowerbdoflimitingloss}
    Let $\ell(\mathbf{p},\mathbf{q},w)$ denote the pointwise limit of the individual loss function, defined in Lemma \ref{limitingloss}. Then
    $$ \inf_{\mathbf{p},\mathbf{q}} \sup_{w \in [w_{\min},w_{\max}]} \ell(\mathbf{p},\mathbf{q},w) \geq C(\sigma^2,w_{\min},w_{\max}),
    $$
    where $ C(\sigma^2,w_{\min},w_{\max}) > 0$ is a strictly positive constant which depends only on $\sigma^2,$ $w_{\min},$ and $w_{\max}.$
\end{lemma}
\begin{proof}
    We recall that $\ell(\mathbf{p},\mathbf{q},w) = \frac{\sigma^2}{1-w^2} \left(\frac{\sigma^2}{1-w^2} (w \alpha_1 + \alpha_2) + w\right)^2$, where$\alpha_1$ and $\alpha_2$ are related to $\mathbf{p}$ and $\mathbf{q}$ by $\alpha_1 = p_1q_2 +p_2q_1$ and $\alpha_2 = p_1q_1 + p_2q_2.$ We therefore aim to show that
    $$ \inf_{\mathbf{p},\mathbf{q}} \sup_{w \in [w_{\min},w_{\max}]} \frac{\sigma^2}{1-w^2} \left(\frac{\sigma^2}{1-w^2} (w \alpha_1 + \alpha_2) + w\right)^2 \geq C(\sigma^2,w_{\min},w_{\max}),
    $$
    where $C(\sigma^2,w_{\min},w_{\max}) > 0$ is a strictly positive constant. Clearly, we have
    $$\begin{aligned}
        & \inf_{\mathbf{p},\mathbf{q}} \sup_{w \in [w_{\min},w_{\max}]} \frac{\sigma^2}{1-w^2} \left(\frac{\sigma^2}{1-w^2} (w \alpha_1 + \alpha_2) + w\right)^2\\ & \geq  \inf_{\alpha_1,\alpha_2} \sup_{w \in [w_{\min},w_{\max}]} \frac{\sigma^2}{1-w^2} \left(\frac{\sigma^2}{1-w^2} (w \alpha_1 + \alpha_2) + w\right)^2,
    \end{aligned}  
    $$
    where the infimum on the right is taken over all $(\alpha_1,\alpha_2) \in \R^2.$ We also have the lower bound
    \begin{align}\label{kcurves}
        & \inf_{\alpha_1,\alpha_2} \sup_{w \in [w_{\min},w_{\max}]} \ell(\alpha_1,\alpha_2;w) \\
        & \geq \inf_{\alpha_1,\alpha_2} \max \left(\ell(\alpha_1,\alpha_2; w_{\min}), \ell(\alpha_1,\alpha_2;w_{\max}), \ell\left(\alpha_1,\alpha_2, \frac{w_{\min}+w_{\max}}{2} \right) \right).
        \end{align} 
    The infimum on the right is attained at a point $(\alpha_1^{\ast}, \alpha_2^{\ast})$ on the intersection of the curves $g(\cdot,\cdot;w_{\min}),$ $g(\cdot, \cdot, w_{\max})$ and $g\left( \cdot, \cdot, \frac{w_{\min}+w_{\max}}{2} \right),$ and it is equal to zero if and only if there exist parameters $\alpha_1^{\ast},\alpha_2^{\ast}$ such that $g(\alpha_1^{\ast},\alpha_2^{\ast};w) = 0$ for all $w \in \left\{w_{\min},w_{\max},\frac{w_{\min}+w_{\max}}{2} \right\}.$ For a fixed $w$, a point $(\alpha_1,\alpha_2)$ satisfies $g(\alpha_1,\alpha_2;w) = 0$ if and only if it lies on the line
    $$ \frac{\sigma^2 w}{1-w^2} x + \frac{\sigma^2}{1-w^2}y = -w.
    $$
    It is easy to see, for the system of equations
    $$ \begin{cases}
        \frac{\sigma^2 w_{\min}}{1-w_{\min}^2} x + \frac{\sigma^2}{1-w_{\min}^2}y = -w_{\min} \\
        \frac{\sigma^2 w_{\max}}{1-w_{\max}^2} x + \frac{\sigma^2}{1-w_{\max}^2}y = -w_{\max} \\
        \frac{\sigma^2 \tilde{w}}{1-\tilde{w}^2} x + \frac{\sigma^2}{1-\tilde{w}^2}y = -\tilde{w}, \; \; \; \tilde{w} = \frac{1}{2}(w_{\min}+w_{\max}),
    \end{cases}
    $$
    that the rank of the augmented matrix is strictly greater than the rank of the coefficient matrix. By the Rouche-Capelli Theorem \cite{shafarevich2012linear}, the above system has no solution. This proves that the infimum
    $$ \inf_{\alpha_1,\alpha_2} \max \left(\ell(\alpha_1,\alpha_2; w_{\min}), \ell(\alpha_1,\alpha_2;w_{\max}), \ell\left(\alpha_1,\alpha_2, \frac{w_{\min}+w_{\max}}{2} \right) \right) = C(\sigma^2,w_{\min},w_{\max})
    $$
    is nonzero, and hence the desired claim is proven.
\end{proof}
We now leverage Lemmas \ref{limitingloss} and \ref{lowerbdoflimitingloss} to prove that the limit of the infimum $\inf_{\mathbf{p},\mathbf{q}} L_T(\mathbf{p},\mathbf{q})$ is lower bounded away from zero. We first need another lemma. Recall that a family of functions is \textit{equicontinuous} if it has a uniform modulus of continuity (see Appendix \ref{aux} for a precise definition). The following lemma establishes some continuity properties of the sequence of individual losses $\{\ell_T\}$.
\begin{lemma}\label{uniformconvergence}
    \begin{enumerate}
        \item For any $\mathbf{p},\mathbf{q} \in \R^2$, the sequence of functions on $[w_{\min},w_{\max}]$ given by $\{w \mapsto \ell_T(\mathbf{p},\mathbf{q},w)\}_{T=2}^{\infty}$ is equicontinuous.
        \item For any $R > 0$, the family of functions on $\{\|\mathbf{p}\|,\|\mathbf{q}\| \leq R\}$ given by $$\{(\mathbf{p},\mathbf{q}) \mapsto \ell_T(\mathbf{p},\mathbf{q},w)\}_{T \geq 2, \; w \in [w_{\min},w_{\max}]}$$ is equicontinuous.
    \end{enumerate}
\end{lemma}
\begin{proof}
    We first recall that $\ell_T(\mathbf{p},\mathbf{q},w) = \E[(\widehat{y}_T-wx_T)^2].$
    
    For 1), the expectations defining $\ell_T$ are computed in Lemmas \ref{4thmoments} and \ref{6thmoment}, and it is easily seen that, for fixed $\mathbf{p},\mathbf{q},$ the derivative in $w$ of each term of $\ell_T(\mathbf{p},\mathbf{q},w)$ is uniformly bounded for $w \in [w_{\min},w_{\max}].$

    For 2), we note that for each $T$ and $w \in [w_{\min},w_{\max}],$ $\ell_T(\mathbf{p},\mathbf{q},w)$ is a polynomial in the entries of $\mathbf{p}$ and $\mathbf{q}$, and the computations of Lemmas \ref{4thmoments} and \ref{6thmoment} show that the coefficients of these polynomials are uniformly bounded in $T$ and $w$. This proves that the derivatives of the functions $\{(\mathbf{p},\mathbf{q}) \mapsto \ell_T(\mathbf{p},\mathbf{q},w)\}_{T \geq 2, \; w \in [w_{\min},w_{\max}]}$ are uniformly bounded with respect to $T$ and $w$ on $\{\|\mathbf{p}\|, \|\mathbf{q}\| \leq R\}.$
\end{proof}

We are now ready to prove Theorem \ref{approxlowerbd}.
\begin{proof}[Proof of Theorem \ref{approxlowerbd}]
    By definition, we have
    $$L_T(\mathbf{p},\mathbf{q}) = \sup_{w \in [w_{\min},w_{\max}]} \ell_T(\mathbf{p},\mathbf{q},w).$$ The result will be established if we can prove that
    \begin{equation}\label{suffcondition}
        \lim_{T \rightarrow \infty} \inf_{\|\mathbf{p}\|,\|\mathbf{q}\| \leq R}  \sup_{w \in [w_{\min},w_{\max}]} \ell_T(\mathbf{p},\mathbf{q},w) = \inf_{\|\mathbf{p}\|,\|\mathbf{q}\| \leq R}  \sup_{w \in [w_{\min},w_{\max}]} \ell(\mathbf{p},\mathbf{q},w),
    \end{equation}
    where $\ell$ is the pointwise limit of $\ell_T$, defined in Lemma \ref{limitingloss}. Once Equation \eqref{suffcondition} has been established, the result follows from Theorem \ref{approxlowerbd}, which establishes that
    $$ \inf_{\|\mathbf{p}\|,\|\mathbf{q}\| \leq R}  \sup_{w \in [w_{\min},w_{\max}]} \ell(\mathbf{p},\mathbf{q},w) \geq C(\sigma^2,w_{\min},w_{\max}).
    $$
    To prove Equation \eqref{suffcondition}, we first note that for any $\mathbf{p},\mathbf{q} \in \R^2$ the sequence of functions on $[w_{\min},w_{\max}]$ given by $\{w \mapsto \ell_T(\mathbf{p},\mathbf{q},w)\}_T$ is equicontinuous by Lemma \ref{uniformconvergence} and converges pointwise to the function $w \mapsto \ell(\mathbf{p},\mathbf{q},w)$ by Lemma \ref{limitingloss}. Therefore, by Lemma \ref{arzelaascoli}, the sequence of functions $\{w \mapsto \ell_T(\mathbf{p},\mathbf{q},w)\}_T$ converges uniformly on $[w_{\min},w_{\max}]$ to the function $w \mapsto \ell(\mathbf{p},\mathbf{q},w).$ In particular, we have
    \begin{equation}\label{supconvergence} \lim_{T \rightarrow \infty} \sup_{w \in [w_{\min},w_{\max}]} \ell_T(\mathbf{p},\mathbf{q},w) = \sup_{w \in [w_{\min},w_{\max}]} \ell(\mathbf{p},\mathbf{q},w), \; \; \textrm{for all $\mathbf{p},\mathbf{q} \in \R^2.$}
    \end{equation}
    Next, Lemma \ref{uniformconvergence} guarantees that the sequence of functions on $\{\|\mathbf{p}\|,\mathbf{q}\| \leq R\}$ defined by $\{(\mathbf{p},\mathbf{q}) \mapsto \ell_T(\mathbf{p},\mathbf{q},w)\}_{T,w}$ is equicontinuous. Since the modulus of continuity of a family of functions is preserved under the supremum, the family of functions $\{(\mathbf{p},\mathbf{q}) \mapsto \sup_{w \in [w_{\min},w_{\max}]} \ell_T(\mathbf{p},\mathbf{q},w)\}$ is also equicontinuous $\{\|\mathbf{p}\|,\mathbf{q}\| \leq R\}$. Therefore, Equation \ref{supconvergence} and another application of Lemma \ref{arzelaascoli} together imply that 
    $$ \sup_{w \in [w_{\min},w_{\max}]} \ell_T(\mathbf{p},\mathbf{q},w) \rightarrow \sup_{w \in [w_{\min},w_{\max}]} \ell(\mathbf{p},\mathbf{q},w), \; \; \textrm{uniformly on $\{\|\mathbf{p}\|,\mathbf{q}\| \leq R\}.$}
    $$
    This proves Equation \eqref{suffcondition} and concludes the proof.
\end{proof}

\section{Auxiliary lemmas}\label{aux}
In this section, we state and prove various helpful lemmas used in the proofs of Theorems \ref{approxerrordeeptf} and \ref{approxlowerbd}. To begin, make frequent use of the following characterization of the distribution of the dynamical system iterates defined by Equation \eqref{dynamicalsystem}.
\begin{lemma}\label{propertiesofdynamics}
    Let $\mathbf{x} = (x_0,x_1, \dots, x_T)$ be as defined in Equation \eqref{dynamicalsystem}. Then for any $t \in \{1, \dots, T\}$, $x_t \sim N\left(0, \sigma^2 (\mathbf{I}_d - W^{2t})(\mathbf{I}_d-W^2)^{-1}\right).$ In addition, the random variable $z_{i,j} = \sum_{k=i+1}^{j} W^{j-k} \xi_k$ satisfies $z_{i,j} \perp x_i$, $\E[z_{i,j}] = 0$, and $x_j = W^{j-i} x_i + z_{i,j}$, and $z_{i,j}$ is a normal random variable with covariance $\sigma^2 \sum_{k=i+1}^{j} W^{2(j-k)}.$
\end{lemma}
\begin{proof}
    1) For any $t,$ $x_t = \sum_{i=1}^{t} W^{t-i} \xi_i$. It follows that
    $$ x_t \sim N \left(0, \sigma^2 \sum_{i=1}^{t} W^{2(t-i)} \right) = N\left(0, \sigma^2 (\mathbf{I}_d - W^{2t})(\mathbf{I}_d-W^2)^{-1}\right).
    $$
    2) From the above calculation, it is clear that $x_j - W^{j-i} x_i = z_{i,j}$. The independence of $z_{i,j}$ and $x_i$ follows from the independence of $\xi_1, \dots, \xi_T$, and the fact that $\E[z_{i,j}]=0$ follows from the fact that $\E[\xi_k] = 0$ for each $k \in \{1, \dots, T\}.$
\end{proof}

The following computations, involving fourth and sixth moments of the linear dynamical system, are heavily used in the proof of Lemma \ref{limitingloss}.

\begin{lemma}\label{4thmoments}[Fourth moments along the dynamics]
    Let $(x_1, \dots, x_t)$ be the iterates of the 1D dynamical system defined by Equation \eqref{dynamicalsystem1D} with parameter $w \in (0,1).$ Then
    \begin{align*}
        &1) \lim_{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^{t} \E[x_i x_{i-1} x_t] = \frac{\sigma^4 w}{(1-w^2)^2}. \\
        &2) \lim_{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^{t} \E[x_i^2 x_t^2] = \frac{\sigma^4}{(1-w^2)^2} = \lim_{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^{t} \E[x_{i-1}^2 x_t^2].
    \end{align*}
\end{lemma}
\begin{proof}
    For the first equation, we first compute $\E[x_ix_{i-1}x_t^2]$ for each $t > 0$ and $i \in \{1, \dots, t\}$. For indices $i < j$, we use the decomposition $x_j = w^{j-1} x_i + z_{i,j}$ as defined in Lemma \ref{propertiesofdynamics}. We have
    \begin{align*}
    \mathbb{E} [x_i x_{i-1} x_t^2] & = \mathbb{E} [x_i x_{i-1} (w^{t-i} x_i + z_{i, t})^2] \\
    & = \mathbb{E} [w^{2(t-i)}x_i^3 x_{i-1} + x_i x_{i-1} z_{i,t}^2] \\
    & = \mathbb{E} [w^{2(t-i)} (w x_{i-1}+\xi_i)^3 x_{i-1} + x_i x_{i-1} z_{i,t}^2] \\
    & = \mathbb{E} [w^{2(t-i)} (w^3 x_{i-1}^4 + 3w x_{i-1}^2 \xi_i^2) + x_i x_{i-1} z_{i,t}^2] \\
    & = 3 \sigma^4 w^{2(t-i)+3} \left(\frac{1-w^{2(i-1)}}{1-w^2}\right)^2  + 3 \sigma^4 w^{2(t-i)+1} \frac{1-w^{2(i-1)}}{1-w^2} + \sigma^4 w \frac{1-w^{2(i-1)}}{1-w^2} \frac{1-w^{2(t-i)}}{1-w^2} \\
    & = \sigma^4 \Bigg(
    3 w^{2(t-i)+3} \frac{1 - 2w^{2(i-1)} + w^{4(i-1)}}{(1-w^2)^2} + 3 w^{2(t-i)+1} \frac{1-w^{2(i-1)}}{1-w^2} \\
    &+ w \frac{1 - w^{2(i-1)} - w^{2(t-i)} + w^{2(t-1)}}{(1-w^2)^2}
    \Bigg) \\
    & = \sigma^4 \left(
     \frac{3w^{2(t-i)+3} - 6 w^{2(t-1)+3} + 3w^{2(t+i)-1}}{(1-w^2)^2} + \frac{3 w^{2(t-i)+1} - 3w^{2(t-1)+1}}{1-w^2} \right)\\ & +  \sigma^4 \left(\frac{w - w^{2(i-1)+1} - w^{2(t-i)+1} + w^{2(t-1)+1}}{(1-w^2)^2}
    \right) \\
    \end{align*}
    Notice that we are able to compute the above expectations easily because many of the terms vanish due to independence and the fact that all distinct random variables are centered. Summing over $t$, we get
    \begin{align*}
    \sum_{i=1}^t \mathbb{E} [x_i x_{i-1} x_t^2] & = 
    \sigma^4 \frac{3}{(1-w^2)^2}\left(
    w^3\frac{1-w^{2t}}{1-w^2} - 2t w^{2(t-1)+3} + w^{2(t+1)-1} \frac{1-w^{2t}}{1-w^2}
    \right) \\
    & + \sigma^4 \frac{3}{1-w^2} \left(
    w \frac{1-w^{2t}}{1-w^2} -tw^{2(t-1)+1}
    \right) \\
    & + \sigma^4 \frac{w}{(1-w^2)^2} \left(
    t - \frac{1-w^{2t}}{1-w^2}- \frac{1-w^{2t}}{1-w^2} + t w^{2(t-1)}
    \right) \\
    & = \sigma^4 \frac{1-w^{2t}}{1-w^2} \left(
    \frac{3w^3}{(1-w^2)^2} + \frac{3w^{2(t+1)-1}}{(1-w^2)^2} + \frac{3w}{1-w^2} - 2 \frac{w}{(1-w^2)^2}
    \right) \\
    & + \sigma^4 t \left(
    -\frac{6 w^{2(t-1)+3}}{(1-w^2)^2} - \frac{3w^{2(t-1)+1}}{1-w^2} + \frac{w}{(1-w^2)^2} + \frac{w^{2(t-1)+1}}{(1-w^2)^2}
    \right)\\
    & = \sigma^4 \frac{1-w^{2t}}{1-w^2} \frac{3w^3 + 3w^{2(t+1)-1} + 3w(1-w^2) -2w }{(1-w^2)^2} \\
    & + \sigma^4 t \frac{-6 w^{2(t-1)+3} - 3w^{2(t-1)+1}(1-w^2) + w + w^{2(t-1)+1}}{(1-w^2)^2}\\
    & = \sigma^4 \frac{1-w^{2t}}{1-w^2} \frac{ 3w^{2(t+1)-1} + w }{(1-w^2)^2} + \sigma^4 t \frac{-3w^{2(t-1)+3} - 2w^{2(t-1)+1} + w}{(1-w^2)^2}.
    \end{align*}
    We then divide by $t$ to get
    \begin{align*}
       \frac{1}{t} \sum_{i=1}^t \mathbb{E} [x_i x_{i-1} x_t^2] = \sigma^4 \frac{-3w^{2(t-1)+3} - 2w^{2(t-1)+1} + w}{(1-w^2)^2} + O\left( \frac{1}{t} \right).
    \end{align*}
    Since $w \in (0,1)$, all terms of the form $w^{at + b}$ vanish when we take the limit. This gives us the final result
    \begin{align}
        \lim_{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^t \mathbb{E} [x_i x_{i-1} x_t^2] = \frac{\sigma^2 w}{(1-w^2)^2}.
    \end{align}
    For the second equation, we first prove that 
    $$ \lim_{t \to \infty} \frac{1}{t} \sum_{i=1}^{t} \E[x_i^2 x_t^2] = \frac{\sigma^4}{(1-w^2)^2}.
    $$
    To prove this, we first rewrite the expression at hand as
    \begin{align*}
        \frac{1}{t} \sum_{i=1}^{t} \E[x_i^2 x_t^2] &= \frac{1}{t} \sum_{i=1}^{t} \E[x_i (wx_{i-1} + \xi_i) x_t^2] \\
        &= w \cdot \frac{1}{t} \sum_{i=1}^{t} \E[x_i x_{i-1} x_t^2] + \frac{1}{t} \sum_{i=1}^{t} \E[x_i \xi_i x_t^2] \\
    \end{align*}
    To simplify the second sum on the right, we further compute 
    \begin{align*}
         \E[x_i \xi_i x_t^2] &= \E[x_i \xi_i (w^{t-i}x_i + z_{i,t})^2] \\
         &= \E[w^{2(t-i)} x_i^3 \xi_i + x_i \xi_i z_{i,t}^2] \\
         &= \E[w^{2(t-i)+3} \xi_i^4 + \xi_i^2 z_{i,t}^2] \\
         &= \sigma^4 \left(3w^{2(t-i)+3} + \frac{1-w^{2(t-i)}}{1-w^2} \right).
    \end{align*}
    This allows us to compute the limit
    \begin{align*}
        \lim_{t \rightarrow \infty}  w \cdot \frac{1}{t} \sum_{i=1}^{t} \E[x_i x_{i-1} x_t^2] + \frac{1}{t} \sum_{i=1}^{t} \E[x_i \xi_i x_t^2] = \frac{\sigma^4 w^2}{(1-w^2)^2} + \frac{\sigma^4}{1-w^2} = \frac{\sigma^4}{(1-w^2)^2}.
    \end{align*}
    This proves that 
    $$ \lim_{t \to \infty} \frac{1}{t} \sum_{i=1}^{t} \E[x_i^2 x_t^2] = \frac{\sigma^4}{(1-w^2)^2},
    $$
    as desired. To prove that the sum $\frac{1}{t} \sum_{i=1}^{t} \E[x_{i-1}^2 x_t^2]$ has the same limit, we observe by a similar computation that
    \begin{align*}
        \frac{1}{t} \sum_{i=1}^{t} \E[x_i^2 x_t^2] &= \frac{1}{t} \sum_{i=1}^{t} \E[(wx_{i-1}+\xi_i)x_t^2] \\
        &= w^2\frac{1}{t} \sum_{i=1}^{t} \E[x_{i-1}^2 x_t^2] + \frac{1}{t} \sum_{i=1}^{t} \E[\xi_i^2 x_t^2] \\
        &= w^2\frac{1}{t} \sum_{i=1}^{t} \E[x_{i-1}^2 x_t^2] + \frac{1}{t} \sum_{i=1}^{t} \E[\xi_i^2 (w^{t-i}x_i + z_{i,t})^2] \\
        &=
        w^2\frac{1}{t} \sum_{i=1}^{t} \E[x_{i-1}^2 x_t^2] + \frac{1}{t} \sum_{i=1}^{t} \E[w^{2(t-i)} \xi_i^4 + \xi_i^2 z_{i,t}^2] \\
        &= w^2\frac{1}{t} \sum_{i=1}^{t} \E[x_{i-1}^2 x_t^2] + \frac{1}{t} \sum_{i=1}^{t} \sigma^4 \left(3 w^{2(t-i)} + \frac{1-w^{2(t-i)}}{1-w^2} \right).
    \end{align*}
    Rearranging terms and taking limits $t \rightarrow \infty$ on both sides, we get that
    \begin{align*}
    \lim_{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^{t} \E[x_{i-1}^2 x_i^2] &= w^{-2} \left( \frac{\sigma^4}{(1-w^2)^2} - \frac{1}{1-w^2} \right) \\
    &= \frac{\sigma^4}{(1-w^2)^2}.
    \end{align*}
    This completes the proof.
\end{proof}

\begin{lemma}\label{6thmoment}[Sixth moments along the dynamics]
    Let $(x_1, \dots, x_t)$ be the iterates of the 1D dynamical system defined by Equation \eqref{dynamicalsystem1D} with parameter $w \in (0,1).$ \begin{align*}
        &1) \lim_{t \rightarrow \infty} \frac{1}{t^2}\E\left[\left( \sum_{i=1}^{t} x_i x_{i-1} \right)^2 x_t^2 \right] = \frac{\sigma^6 w^2}{(1-w^2)^3}. \\
        &2) \lim_{t \rightarrow \infty} \frac{1}{t^2}\E\left[\left( \sum_{i=1}^{t} x_{i-1}^2 \right)^2 x_t^2 \right] = \frac{\sigma^6}{(1-w^2)^3}. \\
        &3) \lim_{t \rightarrow \infty} \frac{1}{t^2} \E \left[\left( \sum_{i=1}^{t} x_i x_{i-1} \right) \left( \sum_{i=1}^{t} x_{i-1}^2 \right) x_t^2 \right] = \frac{\sigma^6 w}{(1-w^2)^3}.
    \end{align*}
\end{lemma}
\begin{proof}
    1) For the first equation, write    
    $$
        \frac{1}{t^2}\E\left[\left( \sum_{i=1}^{t} x_i x_{i-1} \right)^2 x_t^2 \right] = \frac{1}{t^2}\E \left[ \sum_{i=1}^{t} x_i^2 x_{i-1}^2 x_t^2 + 2 \sum_{j=2}^{t} \sum_{i=1}^{j-1} x_i x_{i-1} x_j x_{j-1} x_t^2 \right].
   $$
    Notice, however, that $\frac{1}{t^2}\sum_{i=1}^{t} \E[x_i^2 x_{i-1}^2 x_t^2] = O(t^{-1})$, and thus it suffices to only consider the second sum to compute the limit. For each pair $(i,j)$, the expectation of $x_i x_{i-1} x_j x_{j-1} x_t^2$ decomposes into a sum of many different terms, so computing the precise value of the limit of the sum $\sum_{j=2}^{t} \sum_{i=1}^{j-1} x_i x_{i-1} x_j x_{j-1} x_t^2$ is ostensibly challenging. However, notice that any term $a_{i,j}$ such that $\sum_{j=2}^{t} \sum_{i=1}^{j-1} a_{i,j} = o\left(t^2 \right)$ will vanish when we take the limit. In particular, since $\sum_k w^k < \infty$, any term of the form $a_{i,j} = c \cdot w^{a(t-j)+b(j-i)+ci + d}$ where $c$ is a constant, \textit{will not contribute to the limit.} This allows us to ignore most terms arising from the subsequent expectations. To proceed with the proof, we compute for $i < j$ that
    \begin{align*}
    \E[x_ix_{i-1}x_jx_{j-1}x_t^2] &= \E\left[(w^{t-j}x_j + z_{j,t})^2 x_ix_{i-1}x_jx_{j-1} \right] \\
    &= \E\left[w^{2(t-j)}x_j^2x_{i}x_{i-1}x_jx_{j-1} \right] + \E \left[z_{j,t}^2x_{i}x_{i-1}x_jx_{j-1} \right] = I + II,
    \end{align*}
where we used the independence and mean-zero property of $z_{j,t}.$ For the first term, we get that 
\begin{align*}
    &\E \left[w^{2(t-j)}x_j^2 x_{i}x_{i-1}x_j x_{j-1} \right] = w^{2(t-j)}\E\left[x_j^3 x_{j-1} x_i x_{i-1} \right] \\
    &= w^{2(t-j)} \E \left[(wx_{j-1}+\xi_j)^3 x_{j-1} x_i x_{i-1} \right] \\
    &= w^{2(t-j)}\E\left[\Bigg(w^3x_{j-1}^3 + 3 w x_{j-1}\xi_j^2 \Bigg) x_{j-1} x_i x_{i-1} \right] \\
    &= w^{2(t-j)}\E \left[w^3x_{j-1}^4x_i x_{i-1} + 3 \sigma^2 w x_{j-1}^2 x_i x_{i-1} \right] \\
    &= w^{2(t-j)} \E \left[w^3 (w^{j-i-1}x_i+z_{i,j-1})^4 x_i x_{i-1} + 3 \sigma^2 (w^{j-i-1}x_i + z_{i,j-1})^2 x_i x_{i-1} \right] \\
    &= w^{2(t-j)}\E\left[w^3 \Bigg(w^{4(j-i-1)}x_i^4 + 6 w^{2(j-i-1)}x_i^2 z_{i,j-1}^2 + z_{i,j-1}^4 \Bigg)x_ix_{i-1} + 3\sigma^2 w \Bigg(w^{2(j-i-1)}x_i^2 + z_{i,j-1}^2 \Bigg)x_ix_{i-1} \right] \\
    &= w^{2(t-j)}\E\Bigg[w^3 \Bigg(w^{4(j-i-1)}\Bigg(wx_{i-1}+\xi_i \Bigg)^5 x_{i-1} + 6 w^{2(j-i-1)}z_{i,j-1}^2 \Bigg(wx_{i-1}+\xi_i \Bigg)^3 x_{i-1} \\ 
    &+ z_{i,j-1}^4 \Bigg( wx_{i-1}+\xi_i \Bigg) x_{i-1}\Bigg) \Bigg] \\
    &+ 3w^{2(t-j)}\E\left[\sigma^2 w \Bigg(w^{2(j-i-1)}\Bigg(wx_{i-1}+\xi_i \Bigg)^3x_{i-1} + z_{i,j-1}^2 \Bigg( wx_{i-1} + \xi_i \Bigg)x_{i-1} \Bigg) \right] \\
    &= w^{2(t-j)} w^3 w^{4(j-i-1)} \E \left[w^5 x_{i-1}^6 + 10w^3 \xi_i^2 x_{i-1}^4 + 5w \xi_i^4 x_{i-1}^2  \right] \\ 
    &+ 6 w^{2(t-j)} w^{3} w^{2(j-i-1)} \E[z_{i,j-1}^2]\E \left[w^3 x_{i-1}^4 + 3\xi_i^2 w^2 x_{i-1}^2 \right] \\
    &+ w^{2(t-j)} w^{3} \E[z_{i,j-1}^4] \E\left[wx_{i-1}^2 \right] + 3\sigma^2 w^{2(t-j)+1} \E \left[w^{2(j-i-1)} \Bigg(w^3 x_{i-1}^4 + 3\xi_i^2 w^2 x_{i-1}^2  \Bigg) \right] \\
    &+ 3 \sigma^2w^{2(t-j)+1} \E[z_{i-j-1}^2] \E\left[wx_{i-1}^2 \right] \\
    &= w^{2(t-j)} w^3 w^{4(j-i-1)}\sigma^6 \Bigg(15 w^5 \Bigg(\frac{1-w^{2i}}{1-w^2} \Bigg)^3 + 30w^3 \Bigg(\frac{1-w^{2i}}{1-w^2} \Bigg)^2 + 15 w\Bigg(\frac{1-w^{2i}}{1-w^2} \Bigg)\Bigg) \\
    &+ 6 w^{2(t-j)} w^{3} w^{2(j-i-1)} \sigma^6 \Bigg(\frac{1-w^{2(j-i)}}{1-w^2} \Bigg) \Bigg(3w^3 \Bigg( \frac{1-w^{2i}}{1-w^2}\Bigg)^2 + 3 w^2 \Bigg( \frac{1-w^{2i}}{1-w^2}\Bigg) \Bigg) \\
    &+ 3w^{2(t-j)+1} w^{3} \sigma^6 \Bigg(\frac{1-w^{2(j-i)}}{1-w^2} \Bigg)^2 \Bigg(\frac{1-w^{2i}}{1-w^2} \Bigg)\\
    &+ 3 w^{2(t-j)+1} w^{2(j-i-1)} \sigma^6 \Bigg(3w^3 \Bigg(\frac{1-w^{2i}}{1-w^2} \Bigg)^2 + 3w^2 \Bigg(\frac{1-w^{2i}}{1-w^2} \Bigg) \Bigg) + 3w^{2(t-j)+2} \sigma^6 \Bigg( \frac{1-w^{2(j-i)}}{1-w^2}\Bigg) \Bigg(\frac{1-w^{2i}}{1-w^2} \Bigg).
\end{align*}
As discussed previously, since each of the above terms is $o(t^2)$, all of them vanish when we take the limit; that is,
$$ \lim_{t \rightarrow \infty} \frac{1}{t^2} \sum_{j=2}^{t} \sum_{i=1}^{j-1} \E \left[w^{2(t-j)} x_j^3 x_i x_{i-1} x_{j-1} \right] = 0.
$$
For the second term, we compute
\begin{align*}
    &\E \left[z_{j,t}^2 x_{i}x_{i-1}x_j x_{j-1} \right] = \E [z_{j,t^2}] \E[wx_{j-1}^2 x_i x_{i-1}] \\
    &= \E[z_{j,t}^2] \E[w (w^{j-i-1}x_i + z_{i,j-1})^2x_i x_{i-1}] \\
    &= \E[z_{j,t}^2] \E[w^{j-i} x_i^3 x_{i-1} + wz_{i,j-i}^2 x_i x_{i-1}] \\
    &= \E[z_{j,t}^2] \E[w^{j-i+1}x_{i-1}^4 + w^2 z_{i,j-1}^2 x_{i-1}^2] \\
    &= w^{j-i+1}\E[z_{j,t}^2] \E[x_{i-1}^4] + w^2 \E[z_{j,t}^2] \E[z_{i,j-1}^2] \E[x_{i-1}^2] \\
    &= \sigma^6 w^{j-i+1} \left( \frac{1-w^{2(t-j)}}{1-w^2} \right) \left(\frac{1-w^{2i}}{1-w^2} \right)^2 + \sigma^6 w^2 \left( \frac{1-w^{2(t-j)}}{1-w^2} \right) \left(\frac{1-w^{2(j-i)}}{1-w^2} \right) \left(\frac{1-w^{2i}}{1-w^2} \right).
\end{align*}
The first term $\sigma^6 w^{j-i+1} \left( \frac{1-w^{2(t-j)}}{1-w^2} \right) \left(\frac{1-w^{2i}}{1-w^2} \right)^2$ is $o(t^2)$ and hence vanishes in the limit. The second term satisfies
$$ \lim_{t \rightarrow \infty} \frac{2}{t^2} \sum_{j=2}^{\infty} \sum_{i=1}^{t-1} \sigma^6 w^2 \left( \frac{1-w^{2(t-j)}}{1-w^2} \right) \left(\frac{1-w^{2(j-i)}}{1-w^2} \right) \left(\frac{1-w^{2i}}{1-w^2} \right) = \frac{\sigma^6 w^2}{(1-w^2)^3}.
$$
This proves that 
$$ \lim_{t \rightarrow \infty} \frac{1}{t^2}\E\left[\left( \sum_{i=1}^{t} x_i x_{i-1} \right)^2 x_t^2 \right] = \frac{\sigma^6 w^2}{(1-w^2)^3},
$$
as we aimed to show.
2) For the second equation, the proof is similar to that of the first equation. The expression whose limit we want to compute is given by
\begin{align*}
    \frac{1}{t^2} \E \left[\left(\sum_{i=1}^{t} x_i^2  \right)^2x_t^2 \right] = \frac{1}{t^2} \E \left[\sum_{i=1}^{t} x_i^4 x_t^2 + \sum_{j=2}^{t} x_i^2 x_j^2 x_t^2 \sum_{i=1}^{j-1} \right].
\end{align*}
As in the previous case, only the double sum contributes to the limit $t \rightarrow \infty.$ 
We compute, for $i < j$,
\begin{align*}
    \E[x_i^2 x_j^2 x_t^2] &= \E[x_i^2 x_j^2 (w^{t-j}x_j + z_{j,t})^2] \\
    &= w^{2(t-j)}\E[x_i^2 x_j^4] + \E[x_i^2 x_j^2 z_{j,t}^2] \\
    &= w^{2(t-j)}\E[x_i^2 x_j^4] + \E[x_i^2(w^{j-i}x_i+z_{i,j})^2 z_{j,t}^2] \\
    &= w^{2(t-j)}\E[x_i^2 x_j^4] + w^{2(j-i)}\E[x_i^4 z_{j,t}^2] + \E[x_i^2 z_{i,j}^2 z_{j,t}^2].
\end{align*}
Since $w^{2(t-j)}\E[x_i^2 x_j^4]$ and $w^{2(j-i)}\E[x_i^4 z_{j,t}^2]$ are $o(t^2)$, $\E[x_i^2 z_{i,j}^2 z_{j,t}^2]$ is the only term that contributes to the limit. We conclude that
\begin{align*}
    \lim_{t \rightarrow \infty} \frac{1}{t^2} \E \left[\left(\sum_{i=1}^{t} x_i^2  \right)^2x_t^2 \right] &= \lim_{t \rightarrow \infty} \frac{2}{t^2} \sum_{j=2}^{t} \sum_{i=1}^{j-1} \E[x_i^2 z_{i,j}^2 z_{j,t}^2] \\
    &= \lim_{t \rightarrow \infty} \sum_{j=2}^{t} \sum_{i=1}^{j-1} \frac{\sigma^6(1-w^{2i})(1-w^{2(j-i)})(1-w^{2(t-j)})}{(1-w^2)^3} \\
    &= \frac{\sigma^6}{(1-w^2)^3}.
\end{align*}
3) The proof of the third equation is similar to the proof of the first and second equations. We have
\begin{align*}
    \E\left[\left(\sum_{i=1}^{t} x_i x_{i-1} \right) \left( \sum_{i=1}^{t} x_{i-1}^2 \right) x_t^2\right] &= \E \left[\sum_{i=1}^{t} x_i x_{i-1}^3 x_t^2 + 2\sum_{j=2}^{t} \sum_{i=1}^{j-1} x_j x_{j-1} x_{i-1}^2 x_t^2 \right],
\end{align*}
and only the double sum contributes to the limit. We compute that
\begin{align*}
    \E[x_{i-1}^2 x_{j-1} x_j x_t^2] &= \E[x_{i-1}^2 x_{j-1} x_j (w^{t-j} x_j + z_{j,t})^2] \\
    &= w^{2(t-j)}\E[x_{i-1}^2 x_{j-1} x_j^3] + \E[x_{i-1}^2 x_{j-1} x_j z_{j,t}^2] \\
    &= w^{2(t-j)}\E[x_{i-1}^2 x_{j-1} x_j^3] + w\E[x_{i-1}^2 x_{j-1}^2 z_{j,t}^2] \\
    &= w^{2(t-j)}\E[x_{i-1}^2 x_{j-1} x_j^3] + w\E[x_{i-1}^2 (w^{j-i}x_{i-1}+z_{i-1,j-1})^2 z_{j,t}^2] \\
    &= w^{2(t-j)}\E[x_{i-1}^2 x_{j-1} x_j^3] + w^{2(j-i)+1}\E[x_{i-1}^4 z_{j,t}^2] + w\E[x_{i-1}^2 z_{i-1,j-1}^2 z_{j,t}^2].
\end{align*}
Since the first two terms are $o(t^2)$, only the third term contributes to the limit, and we conclude that
\begin{align*}
    \lim_{t \rightarrow \infty} \frac{1}{t^2}  \E\left[\left(\sum_{i=1}^{t} x_i x_{i-1} \right) \left( \sum_{i=1}^{t} x_{i-1}^2 \right) x_t^2\right] &= \lim_{t \rightarrow \infty} \frac{2w}{t^2} \sum_{j=2}^{t} \sum_{i=1}^{t} \E[x_{i-1}^2 z_{i-1,j-1}^2 z_{j,t}^2] \\
    &= \lim_{t \rightarrow \infty} \frac{2w}{t^2} \sum_{j=2}^{t} \sum_{i=1}^{t} \frac{\sigma^6(1-w^{2(i-1)})(1-w^2{(j-i)})(1-w^{2(t-j)})}{(1-w^2)^3} \\
    &= \frac{\sigma^6 w}{(1-w^2)^3}.
\end{align*}
\end{proof}
The following lemma controls the size $z_{T,L}$ along the Richardson iteration and is used in the proof of Theorem \ref{approxerrordeeptf}.
\begin{lemma}\label{richardsoniteratebound}
    Let $(x_0, \dots, x_t)$ follow the dynamical system \eqref{dynamicalsystem} and  let $z_{T,L}$ be defined by the Richardson iteration
    $$
    \begin{cases}
        z_{T,L} = z_{T,L-1} + \alpha \left(x_T - X_T z_{T,L-1} \right), \; L > 0 \\
        z_{T,0} = 0.
    \end{cases}
    $$
    Then, as long as $\alpha < \frac{2(1-w_{\max}^2)}{c\sigma^2}$ for a numerical constant $c > 0$, we have
    $$ \E[\|z_{T,L}\|^k] = O \left( k \left( 1 + L^{2k} \alpha^{2k} \right) + \alpha^{2k(L-1)} T^{-k(L-1)} \cdot (2k(L-1))! \right).
    $$
\end{lemma}
\begin{proof}
    By Fubini's theorem, we have
        \begin{align*}
            \E[\|z_{T,L}\|^k] = \int_{0}^{\infty} \mathbb{P} \left( \|z_{T,L}\|^k > r \right) dr.
        \end{align*}
        Changing variables, we find
        \begin{align*}
            \E[\|z_{T,L}\|^k] &= \int_{0}^{\infty} kr^{k-1} \mathbb{P} \left(\|z_{T,L}\| > r \right) dr \\
            &\leq k + \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left(\|z_{T,L}\| > r \right) dr.
        \end{align*}
    By the recurrence defining $z_{T,L}$ and the triangle inequality, we have
    $$ \mathbb{P} \left(\|z_{T,L}\| > r \right) \leq \mathbb{P} \left(\|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}} \|z_{T,L-1}\| + \alpha \|x_T\|  > r\right).
    $$
    Iterating this bound, we have
    \begin{align*}
        \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \|z_{T,L}\| > r \right)dr &\leq \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha \sum_{j=1}^{L-1} \|\mathbf{I}_d-\alpha X_T\|_{\textrm{op}}^j \|x_T\| > r \right)dr.
    \end{align*}
    If $\mathcal{E}$ denotes the event on which $\|(\mathbf{I}_d-\alpha X_T)\| \leq 1$, then we have
    \begin{align*}
        &\int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha \sum_{j=1}^{L-1} \|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}}^j \|x_T\| > r \right)dr \\
        &= \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha \sum_{j=1}^{L-1} \|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}}^j \|x_T\| > r \cap \mathcal{E} \right)dr \\
        &+ \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha \sum_{j=1}^{L-1} \|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}}^j \|x_T\| > r \cap \mathcal{E}^c \right)dr
    \end{align*}
    For the first term, notice that
    \begin{align*}
        \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha \sum_{j=1}^{L-1} \|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}}^j \|x_T\| > r \cap \mathcal{E} \right) &\leq \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha L \|x_t\| > r \right) dr \\
        &\leq (\alpha L)^k \int_{0}^{\infty} kr^{k-1} \mathbb{P} \left(\|x_T|\| > r \right) dr \\
        &= O \left( k L^{k} \alpha^k \right),
    \end{align*}
    where we note that the above integral converges due to the exponential concentration of $\|x_T\|$ proven in Lemma \ref{conc}. For the second term, since $\|\mathbf{I}_d-\alpha X_t\|_{\textrm{op}} > 1$ on $\mathcal{E}^c$, we have
    \begin{align*}
        &\int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha \sum_{j=1}^{L-1} \|I-\alpha X_T\|_{\textrm{op}}^j \|x_T\| > r \cap \mathcal{E}^c \right)dr \leq \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( L\alpha \|\mathbf{I}_d-\alpha X_T\|_{\textrm{op}}^{L-1} \|x_T\| > r \right)dr \\
        &= \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha L \|I-\alpha X_T\|_{\textrm{op}}^{L-1} \|x_T\| > r \cap \{\|x_T\| \leq \sqrt{r}\} \right)dr \\
        &+\int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha L \|I-\alpha X_T\|_{\textrm{op}}^{L-1} \|x_T\| > r \cap \{\|x_T\| > \sqrt{r}\} \right)dr \\
        &\leq \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha L \|I-\alpha X_T\|_{\textrm{op}}^{L-1}  > \sqrt{r} \right)dr + \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \|x_T\| > \sqrt{r} \right) dr.
    \end{align*}
    Notice that by the concentration bound for $\|x_T\|^2$ stated in Lemma \ref{conc}, the second term above is bounded by 
    \begin{align*}
        \int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \|x_T\| > \sqrt{r} \right) dr &\leq C_1 \int_{1}^{\infty} kr^{k-1} \exp \left(-\frac{r}{C_2} \right) dr = O(k)
    \end{align*}
    where $C_1$ and $C_2$ are defined in Lemma \ref{conc}. For the first term, we make the change of variables $r \mapsto \left(\frac{r^{1/2}}{\alpha L} \right)^{1/(L-1)}$ to rewrite the integral as
    \begin{align*}
        &\int_{1}^{\infty} kr^{k-1} \mathbb{P} \left( \alpha L \|\mathbf{I}_d-\alpha X_T\|_{\textrm{op}}^{L-1}  > \sqrt{r} \right)dr \\
        &= k \left(\alpha L \right)^{2k} \int_{(\alpha L)^{1/(L-1)}}^{\infty} r^{2k(L-1)-1} \mathbb{P} \left(\|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}} > r \right) dr.
    \end{align*}
    We further have
    \begin{align*}
        \int_{(\alpha L)^{1/(L-1)}}^{\infty} r^{2k(L-1)-1} \mathbb{P} \left(\|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}} > r \right) dr &\leq \int_{(\alpha L)^{1/(L-1)}}^{1} r^{2k(L-1)-1} \mathbb{P} \left(\|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}} > r \right) dr \\
        &+ \int_{1}^{\infty} r^{2k(L-1)-1} \mathbb{P} \left(\|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}} > r \right) dr \\
        &\leq 1 + \int_{1}^{\infty} r^{2k(L-1)-1} \mathbb{P} \left(\|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}} > r \right) dr.
    \end{align*}
    Note that the first integral in the sum above could be negative if $\alpha L > 1.$ This proves that
    \begin{align*}
        &k \left(\alpha L \right)^{2k} \int_{(\alpha L)^{1/(L-1)}}^{\infty} r^{2k(L-1)-1} \mathbb{P} \left(\|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}} > r \right) dr \\
        &\leq k(\alpha L)^k + \int_{1}^{\infty} r^{2k(L-1)-1} \mathbb{P} \left(\|(\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}} > r \right) dr.
    \end{align*}
To control the remaining integral, we note that
\begin{align*}
    \|(I-\alpha X_T)\|_{\textrm{op}} &= \max\left(|1-\alpha \lambda_{\min}(X_T)|, |1-\alpha \lambda_{\max}(X_T)| \right) \\
    &\leq \max \left(1, \alpha \lambda_{\max}(X_T) - 1 \right).
\end{align*}
Therefore, for $r > 1$, 
\begin{align*}
    \mathbb{P} \left(\|(I-\alpha X_T)\|_{\textrm{op}} > r \right) \leq \mathbb{P}(\left( \lambda_{\max}(X_T) > \frac{1+r}{\alpha} \right).
\end{align*}
This allows us to bound the remaining integral by
\begin{align*}
    &\int_{1}^{\infty} r^{2k(L-1)-1} \mathbb{P} \left(\|\mathbf{I}_d-\alpha X_T)\|_{\textrm{op}} > r \right) dr \leq \int_{1}^{\infty} r^{2k(L-1)-1} \mathbb{P} \left(\lambda_{\max}(X_T) > \frac{r+1}{\alpha} \right) dr \\
    &= \alpha \int_{2/\alpha}^{\infty} (\alpha r - 1)^{2k(L-1)-1} \mathbb{P} \left( \lambda_{\max}(X_T) > r \right) dr \\
    &\leq \alpha^{2k(L-1)} \int_{2/\alpha}^{\infty} r^{2k(L-1)-1}  \mathbb{P} \left( \lambda_{\max}(X_T) > r \right) dr \\
    &\leq \alpha^{2k(L-1)} \int_{2/\alpha}^{\infty} r^{2k(L-1)-1}  \exp \left( -\frac{c \sqrt{T} r (1-w_{\max}^2) }{\sigma^4} + \frac{c \sqrt{T}}{\sigma^2} \right) dr,
\end{align*}
where we used the concentration inequality proved in Lemma \ref{conc} in the last line. After another change of variables, we can rewrite the above integral as
\begin{align*}
    &\alpha^{2k(L-1)} \int_{2/\alpha}^{\infty} r^{2k(L-1)-1}  \exp \left( -\frac{c \sqrt{T} r (1-w_{\max}^2) }{\sigma^4} + \frac{c \sqrt{T}}{\sigma^2} \right) dr \\
    &= \alpha^{2k(L-1)}\left( \frac{\alpha \sigma^4}{c \sqrt{T} (1-w_{\max}^2)} \right)^{2k(L-1)} \int_{\frac{\sigma^4}{c \sqrt{T} r(1-w_{\max}^2)}\left(\frac{2}{\alpha}-\frac{c\sigma^2}{1-w_{\max}^2} \right)}^{\infty} r^{2k(L-1)-1} e^{-r} dr.
\end{align*}
If $\alpha < \frac{2(1-w_{\max}^2)}{c \sigma^2},$ then the lower limit of the above integral is non-negative, and hence we have the upper bound
\begin{align*}
    &\alpha^{2k(L-1)} \left( \frac{\alpha \sigma^4}{c \sqrt{T} (1-w_{\max}^2)} \right)^{2k(L-1)} \int_{\frac{\sigma^4}{c \sqrt{T} r(1-w_{\max}^2)}\left(\frac{2}{\alpha}-\frac{c\sigma^2}{1-w_{\max}^2} \right)}^{\infty} r^{2k(L-1)-1} e^{-r} dr \\
    &\leq \alpha^{2k(L-1)} \left( \frac{\alpha \sigma^4}{c \sqrt{T} (1-w_{\max}^2)} \right)^{2k(L-1)} \Gamma(2k(L-1)) \\
    &= \alpha^{2k(L-1)} \left( \frac{\alpha \sigma^4}{c \sqrt{T} (1-w_{\max}^2)} \right)^{2k(L-1)} \cdot (2k(L-1))!
\end{align*}
where $\Gamma(\cdot)$ denotes the $\Gamma$ function. Combining the estimates for each term gives us the bound as stated in the Lemma.
\end{proof}

We make use of the following elementary concentration inequalities for the dynamical system.
\begin{lemma}\label{conc}
    Let $(x_0, \dots, x_T)$ be defined by the dynamical system \eqref{dynamicalsystem}. Then 
    \begin{enumerate}
        \item $\mathbb{P} \left( \|x_T\| \geq r \right) \leq \exp \left( \frac{d(1-w_{\max}^2)^2}{8\sigma^2} \right) \cdot \exp\left( -\frac{r(1-w_{\max}^2)^2}{8\sigma^4} \right).$
        \item $\mathbb{P} \left( \lambda_{\max}\left(\frac{1}{T} \sum_{i=1}^{T} x_i x_i^T \right) > r \right) \leq \exp \left( -\frac{c \sqrt{T} r (1-w_{\max}^2) }{\sigma^4} + \frac{c \sqrt{T}}{\sigma^2} \right).$
    \end{enumerate}
\end{lemma}
\begin{proof}
    To prove 1), note that by Lemma \ref{propertiesofdynamics}, $x_T$ is a centered Gaussian random vector with covariance $$
        \textrm{cov}(x_T) = \sigma^2 (\mathbf{I}_d-W^{2T}) (\mathbf{I}_d - W^2)^{-1} \prec \frac{\sigma^2}{1-w_{\max}^2} \cdot \mathbf{I}_d.$$
        It follows that $\|x_T\|^2$ can be written as a sum of $d$ independent random variables
        $$ \|x_T\|^2 = \sum_{j=1}^{d} x_{T,j}^2,
        $$
        where $x_{T,j}$ are normal with $\E[x_{T,j}] = 0$ and $\E[x_{T,j}^2] \leq \frac{\sigma^2}{1-w_{\max}^2}.$ In particular, each $x_{T,j}$ is sub-exponential with parameters $\left(2,4\left(\frac{\sigma^2}{1-w_{\max}^2} \right)^2\right)$ (see \cite{wainwright2019high}). This implies that $\|x_T\|^2$ is sub-exponential with parameters $\left(2\sqrt{d}, 4 \left(\frac{\sigma^2}{1-w_{\max}^2} \right)^2 \right)$, which yields the concentration inequality
        \begin{align*}
            \mathbb{P} \left(\|x_T\|^2 > r \right) &= \mathbb{P} \left(\|x_T\|^2 - \E[\|x_T\|^2] > r - \E[\|x_T\|^2] \right) \\
            &\leq \exp \left(-\frac{(r-\E[\|x_T\|^2])(1-w_{\max}^2)^2}{8 \sigma^4} \right) \\
            &\leq \exp \left( \frac{d(1-w_{\max}^2)^2}{8\sigma^2} \right) \cdot \exp\left( -\frac{r(1-w_{\max}^2)^2}{8\sigma^4} \right).
        \end{align*}

    To prove 2), we proceed in two steps.

    \textbf{Step 1:} we first show that the 1D dynamical system
    $$ z_{t+1} = wz_t + \xi_{t+1}, \; \xi_{t+1} \sim N(0,\sigma^2), \; w \in [w_{\min},w_{\max}], \; z_0 = 0
    $$
    satisfies the concentration inequality 
    $$ \mathbb{P} \left( \frac{1}{T} \sum_{i=1}^{T} z_i^2 > r \right) \leq \exp \left( \frac{d(1-w_{\max}^2)^2}{8\sigma^2} \right) \cdot \exp\left( -\frac{r(1-w_{\max}^2)^2}{8\sigma^4} \right).
    $$
    To prove this, note that $z_i = \sum_{j=1}^{i} w^{i-j} \xi_j$, and therefore 
    $$ \frac{1}{T} \sum_{i=1}^{T} z_i^2 = \frac{1}{T} \sum_{i=1}^{T} \sum_{j,k=1}^{T} w^{2i-j+k)} \xi_j \xi_j = \xi^T A \xi,
    $$
    where $\xi = (\xi_1, \dots, \xi_T) \in \R^T$ and $A \in \R^{T \times T}$ is defined by
    \begin{align*}
    A_{jk} &= \frac{1}{T}\sum_{i=\max(j,k)}^{T} w^{2i-(j+k)} \\
    &= \frac{1}{T} \left(w^{2\max(j,k)-(j+k)} + \dots + w^{2T -(j+k)}  \right) \\
    &\leq \frac{1}{T} \cdot w^{2 \max(j,k)-(j+k)} \cdot \frac{1}{1-w^2}.
\end{align*}
It follows that
\begin{align*}
    \|A\|_F^2 &\leq \frac{1}{T^2(1-w^2)^2} \sum_{j,k=1}^{T} w^{4 \max(j,k)-2(j+k)} \\
    &= \frac{2}{T^2(1-w^2)^2} \sum_{k=1}^{T} \sum_{j=1}^{k} w^{2(k-j)} \\
    &= \frac{2}{T^2(1-w^2)^2} \sum_{k=1}^{T} \frac{1-w^{2k}}{1-w^2} \\
    &\leq \frac{2}{T(1-w^2)^2}.
\end{align*}
Hence
$$ \|A\|_F \leq \frac{\sqrt{2}}{(1-w^2)\sqrt{T}}.
$$
Then, with $\xi = (\xi_1, \dots, \xi_t)$, noting that each $\xi_i$ is $\sigma^2$-sub-Gaussian, the Hanson-Wright inequality (\cite{vershynin2018high}) guarantees that
\begin{align*} \mathbb{P} \left( \xi^T A \xi - \E[\xi^T A \xi] > r \right) &\leq \exp \left(-\frac{c \sqrt{T} r(1-w_{\max}^2)}{\sigma^4} \right),
\end{align*}
where $c > 0$ is a universal constant. Noting that $\E[\xi^T A \xi] \leq \frac{\sigma^2}{1-w_{\max}^2}$ by Lemma \ref{propertiesofdynamics} and rearranging the bound above, we find that
\begin{align*}
    \mathbb{P} \left( \xi^T A \xi > r -\right) &\leq \exp \left(\frac{c \sqrt{T} (r-\E[\xi^T A \xi]) (1-w_{\max}^2)}{\sigma^4} \right) \\
    &\leq \exp \left( -\frac{c \sqrt{T} r (1-w_{\max}^2) }{\sigma^4} + \frac{c \sqrt{T}}{\sigma^2} \right).
\end{align*}
\textbf{Step 2:} We show that the bound above implies the desired concentration bound for $\lambda_{\max} \left(\frac{1}{T} \sum_{i=1}^{T} x_i x_i^T \right).$ To begin, note that
$$ \mathbb{P} \left( \lambda_{\max} \left(\frac{1}{T} \sum_{i=1}^{T} x_i x_i^T \right) > r \right) \leq \mathbb{P} \left( \frac{1}{T} \sum_{i=1}^{T} \|x_i\|^2 > r \right).
$$
Next, by expanding each norm $\|x_i\|^2$ in the basis of eigenvectors of $W$, we can write
$$ \frac{1}{T} \sum_{i=1}^{T} \|x_i\|^2 =  \frac{1}{T} \sum_{i=1}^{T} \sum_{j=1}^{d} z_{i,j}^2,
$$
where each $(z_{1,j}, \dots, z_{T,j})$ is an independent copy of the 1D dynamical system defined in Step 1. By the Bernstein bound for sums of subexponential random variables, this implies that
$$  \mathbb{P} \left( \frac{1}{T} \sum_{i=1}^{T} \|x_i\|^2 > r \right) \leq \exp \left( -\frac{c \sqrt{T} r (1-w_{\max}^2) }{\sigma^4} + \frac{c \sqrt{T}}{\sigma^2} \right),
$$
and therefore that
$$ \mathbb{P} \left( \lambda_{\max} \left(\frac{1}{T} \sum_{i=1}^{T} x_i x_i^T \right) > r \right) \leq \exp \left( -\frac{c \sqrt{T} r (1-w_{\max}^2) }{\sigma^4} + \frac{c \sqrt{T}}{\sigma^2} \right).
$$

\end{proof}

The following lemma is used to extend pointwise convergence of functions to uniform convergence. Recall that a sequence of functions $\{f_n\}$ on a compact metric space $\mathcal{X}$ is \textit{equicontinuous} at $x \in \mathcal{X}$ if for every $\epsilon > 0$, there exists a $\delta > 0$ such that for all $x' \in \mathcal{X}$ with $|x-x'| < \delta,$ we have $\sup_n |f_n(x)-f_n(x')| < \epsilon.$ The sequence $\{f_n\}$ is \textit{equicontinuous} if it is equicontinuous at every $x \in \mathcal{X}.$

\begin{lemma}\label{arzelaascoli}
    Let $\{f_n\}$, $f$ be functions on a compact metric space $\mathcal{X}$ such that
    \begin{enumerate}
        \item The sequence $\{f_n\}$ is equicontinuous,
        \item $f_n \rightarrow f$ pointwise.
    \end{enumerate}
    Then $f_n \rightarrow f$ uniformly.
\end{lemma}
\begin{proof}
    Since $f_n \rightarrow f$ pointwise, the sequence $\{f_n(x)\}$ is bounded for each $x \in \mathcal{X}$. Therefore the sequence of functions $\{f_n\}$ is pointwise bounded and equicontinuous, so by the Arzela-Ascoli Theorem (see e.g. \cite{folland1999real}), it has a uniformly convergent subsequence. But the limit of any uniformly convergent subsequence of $\{f_n\}$ must equal the pointwise limit of the sequence. This proves that $f$ is the only subsequential limit of $\{f_n\}$ in the uniform topology, and hence $f_n \rightarrow f$ uniformly.
\end{proof}

\end{document}