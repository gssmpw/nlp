@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{von2023uncovering,
  title={Uncovering mesa-optimization algorithms in transformers},
  author={Von Oswald, Johannes and Schlegel, Maximilian and Meulemans, Alexander and Kobayashi, Seijin and Niklasson, Eyvind and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and others},
  journal={arXiv preprint arXiv:2309.05858},
  year={2023}
}

@article{li2024one,
  title={One-Layer Transformer Provably Learns One-Nearest Neighbor In Context},
  author={Li, Zihao and Cao, Yuan and Gao, Cheng and He, Yihan and Liu, Han and Klusowski, Jason M and Fan, Jianqing and Wang, Mengdi},
  journal={arXiv preprint arXiv:2411.10830},
  year={2024}
}

@article{zhang2023trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}

@article{wu2023many,
  title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  author={Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2310.08391},
  year={2023}
}

@article{kim2024transformers2,
  title={Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape},
  author={Kim, Juno and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2402.01258},
  year={2024}
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge university press}
}

@book{shafarevich2012linear,
  title={Linear algebra and geometry},
  author={Shafarevich, Igor R and Remizov, Alexey O},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}

@inproceedings{matni2019tutorial,
  title={A tutorial on concentration bounds for system identification},
  author={Matni, Nikolai and Tu, Stephen},
  booktitle={2019 IEEE 58th conference on decision and control (CDC)},
  pages={3741--3749},
  year={2019},
  organization={IEEE}
}

@book{folland1999real,
  title={Real analysis: modern techniques and their applications},
  author={Folland, Gerald B},
  volume={40},
  year={1999},
  publisher={John Wiley \& Sons}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@article{ye2024pdeformer,
  title={Pdeformer: Towards a foundation model for one-dimensional partial differential equations},
  author={Ye, Zhanhong and Huang, Xiang and Chen, Leheng and Liu, Hongsheng and Wang, Zidong and Dong, Bin},
  journal={arXiv preprint arXiv:2402.12652},
  year={2024}
}

@article{batatia2023foundation,
  title={A foundation model for atomistic materials chemistry},
  author={Batatia, Ilyes and Benner, Philipp and Chiang, Yuan and Elena, Alin M and Kov{\'a}cs, D{\'a}vid P and Riebesell, Janosh and Advincula, Xavier R and Asta, Mark and Avaylon, Matthew and Baldwin, William J and others},
  journal={arXiv preprint arXiv:2401.00096},
  year={2023}
}

@article{subramanian2024towards,
  title={Towards foundation models for scientific machine learning: Characterizing scaling and transfer behavior},
  author={Subramanian, Shashank and Harrington, Peter and Keutzer, Kurt and Bhimji, Wahid and Morozov, Dmitriy and Mahoney, Michael W and Gholami, Amir},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{mccabe2023multiple,
  title={Multiple physics pretraining for physical surrogate models},
  author={McCabe, Michael and Blancard, Bruno R{\'e}galdo-Saint and Parker, Liam Holden and Ohana, Ruben and Cranmer, Miles and Bietti, Alberto and Eickenberg, Michael and Golkar, Siavash and Krawezik, Geraud and Lanusse, Francois and others},
  journal={arXiv preprint arXiv:2310.02994},
  year={2023}
}

@article{choi2023transformer,
  title={Transformer architecture and attention mechanisms in genome data analysis: a comprehensive review},
  author={Choi, Sanghyuk Roy and Lee, Minhyeok},
  journal={Biology},
  volume={12},
  number={7},
  pages={1033},
  year={2023},
  publisher={MDPI}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={8821--8831},
  year={2021},
  organization={Pmlr}
}

@inproceedings{chen2020generative,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={1691--1703},
  year={2020},
  organization={PMLR}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{fu2023transformers,
  title={Transformers learn higher-order optimization methods for in-context learning: A study with linear models},
  author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  journal={arXiv preprint arXiv:2310.17086},
  year={2023}
}

@article{giannou2024well,
  title={How Well Can Transformers Emulate In-context Newton's Method?},
  author={Giannou, Angeliki and Yang, Liu and Wang, Tianhao and Papailiopoulos, Dimitris and Lee, Jason D},
  journal={arXiv preprint arXiv:2403.03183},
  year={2024}
}

@article{nichani2024transformers,
  title={How transformers learn causal structure with gradient descent},
  author={Nichani, Eshaan and Damian, Alex and Lee, Jason D},
  journal={arXiv preprint arXiv:2402.14735},
  year={2024}
}

@article{edelman2024evolution,
  title={The evolution of statistical induction heads: In-context learning markov chains},
  author={Edelman, Benjamin L and Edelman, Ezra and Goel, Surbhi and Malach, Eran and Tsilivis, Nikolaos},
  journal={arXiv preprint arXiv:2402.11004},
  year={2024}
}

@article{vladymyrov2024linear,
  title={Linear Transformers are Versatile In-Context Learners},
  author={Vladymyrov, Max and Von Oswald, Johannes and Sandler, Mark and Ge, Rong},
  journal={arXiv preprint arXiv:2402.14180},
  year={2024}
}

@article{anwar2024adversarial,
  title={Adversarial Robustness of In-Context Learning in Transformers for Linear Regression},
  author={Anwar, Usman and Von Oswald, Johannes and Kirsch, Louis and Krueger, David and Frei, Spencer},
  journal={arXiv preprint arXiv:2411.05189},
  year={2024}
}

@article{lu2024asymptotic,
  title={Asymptotic theory of in-context learning by linear attention},
  author={Lu, Yue M and Letey, Mary I and Zavatone-Veth, Jacob A and Maiti, Anindita and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2405.11751},
  year={2024}
}

@article{ahn2023transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={45614--45650},
  year={2023}
}

@article{mahankali2023one,
  title={One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention},
  author={Mahankali, Arvind and Hashimoto, Tatsunori B and Ma, Tengyu},
  journal={arXiv preprint arXiv:2307.03576},
  year={2023}
}

@article{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
@article{guo2023transformers,
  title={How do transformers learn in-context beyond simple functions? a case study on learning with representations},
  author={Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
  journal={arXiv preprint arXiv:2310.10616},
  year={2023}
}

@article{yang2024context,
  title={In-context learning with representations: Contextual generalization of trained transformers},
  author={Yang, Tong and Huang, Yu and Liang, Yingbin and Chi, Yuejie},
  journal={arXiv preprint arXiv:2408.10147},
  year={2024}
}

@article{kim2024transformers,
  title={Transformers are minimax optimal nonparametric in-context learners},
  author={Kim, Juno and Nakamaki, Tai and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2408.12186},
  year={2024}
}

@inproceedings{marwah2023neural,
  title={Neural network approximations of pdes beyond linearity: A representational perspective},
  author={Marwah, Tanya and Lipton, Zachary Chase and Lu, Jianfeng and Risteski, Andrej},
  booktitle={International Conference on Machine Learning},
  pages={24139--24172},
  year={2023},
  organization={PMLR}
}

@article{chen2021representation,
  title={On the representation of solutions to elliptic pdes in barron spaces},
  author={Chen, Ziang and Lu, Jianfeng and Lu, Yulong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={6454--6465},
  year={2021}
}

@article{cole2024provable,
  title={Provable in-context learning of linear systems and linear elliptic pdes with transformers},
  author={Cole, Frank and Lu, Yulong and O'Neill, Riley and Zhang, Tianhao},
  journal={arXiv preprint arXiv:2409.12293},
  year={2024}
}

@article{zheng2024mesa,
  title={On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability},
  author={Zheng, Chenyu and Huang, Wei and Wang, Rongzhen and Wu, Guoqiang and Zhu, Jun and Li, Chongxuan},
  journal={arXiv preprint arXiv:2405.16845},
  year={2024}
}

@article{sander2024transformers,
  title={How do Transformers perform In-Context Autoregressive Learning?},
  author={Sander, Michael E and Giryes, Raja and Suzuki, Taiji and Blondel, Mathieu and Peyr{\'e}, Gabriel},
  journal={arXiv preprint arXiv:2402.05787},
  year={2024}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}

@article{collins2024context,
  title={In-context learning with transformers: Softmax attention adapts to function lipschitzness},
  author={Collins, Liam and Parulekar, Advait and Mokhtari, Aryan and Sanghavi, Sujay and Shakkottai, Sanjay},
  journal={arXiv preprint arXiv:2402.11639},
  year={2024}
}

@article{varga1962iterative,
  title={Iterative matrix analysis},
  author={Varga, RS},
  journal={Englewood Cliffs, NJ},
  year={1962}
}

@inproceedings{foster2020learning,
  title={Learning nonlinear dynamical systems from a single trajectory},
  author={Foster, Dylan and Sarkar, Tuhin and Rakhlin, Alexander},
  booktitle={Learning for Dynamics and Control},
  pages={851--861},
  year={2020},
  organization={PMLR}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{goel2024can,
  title={Can a transformer represent a Kalman filter?},
  author={Goel, Gautam and Bartlett, Peter},
  booktitle={6th Annual Learning for Dynamics \& Control Conference},
  pages={1502--1512},
  year={2024},
  organization={PMLR}
}

@article{ahn2023linear,
  title={Linear attention is (maybe) all you need (to understand transformer optimization)},
  author={Ahn, Kwangjun and Cheng, Xiang and Song, Minhak and Yun, Chulhee and Jadbabaie, Ali and Sra, Suvrit},
  journal={arXiv preprint arXiv:2310.01082},
  year={2023}
}

@article{zhang2024context,
  title={In-context learning of a linear Transformer block: benefits of the MLP component and one-step GD initialization},
  author={Zhang, Ruiqi and Wu, Jingfeng and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2402.14951},
  year={2024}
}
@article{muller2021transformers,
  title={Transformers can do bayesian inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}

@article{dai2022can,
  title={Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}