@article{ahn2023transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={45614--45650},
  year={2023}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@article{anwar2024adversarial,
  title={Adversarial Robustness of In-Context Learning in Transformers for Linear Regression},
  author={Anwar, Usman and Von Oswald, Johannes and Kirsch, Louis and Krueger, David and Frei, Spencer},
  journal={arXiv preprint arXiv:2411.05189},
  year={2024}
}

@article{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{cole2024provable,
  title={Provable in-context learning of linear systems and linear elliptic pdes with transformers},
  author={Cole, Frank and Lu, Yulong and O'Neill, Riley and Zhang, Tianhao},
  journal={arXiv preprint arXiv:2409.12293},
  year={2024}
}

@article{collins2024context,
  title={In-context learning with transformers: Softmax attention adapts to function lipschitzness},
  author={Collins, Liam and Parulekar, Advait and Mokhtari, Aryan and Sanghavi, Sujay and Shakkottai, Sanjay},
  journal={arXiv preprint arXiv:2402.11639},
  year={2024}
}

@article{dai2022can,
  title={Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@article{edelman2024evolution,
  title={The evolution of statistical induction heads: In-context learning markov chains},
  author={Edelman, Benjamin L and Edelman, Ezra and Goel, Surbhi and Malach, Eran and Tsilivis, Nikolaos},
  journal={arXiv preprint arXiv:2402.11004},
  year={2024}
}

@article{fu2023transformers,
  title={Transformers learn higher-order optimization methods for in-context learning: A study with linear models},
  author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  journal={arXiv preprint arXiv:2310.17086},
  year={2023}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@article{giannou2024well,
  title={How Well Can Transformers Emulate In-context Newton's Method?},
  author={Giannou, Angeliki and Yang, Liu and Wang, Tianhao and Papailiopoulos, Dimitris and Lee, Jason D},
  journal={arXiv preprint arXiv:2403.03183},
  year={2024}
}

@inproceedings{goel2024can,
  title={Can a transformer represent a Kalman filter?},
  author={Goel, Gautam and Bartlett, Peter},
  booktitle={6th Annual Learning for Dynamics \& Control Conference},
  pages={1502--1512},
  year={2024},
  organization={PMLR}
}

@article{guo2023transformers,
  title={How do transformers learn in-context beyond simple functions? a case study on learning with representations},
  author={Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
  journal={arXiv preprint arXiv:2310.10616},
  year={2023}
}

@article{kim2024transformers,
  title={Transformers are minimax optimal nonparametric in-context learners},
  author={Kim, Juno and Nakamaki, Tai and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2408.12186},
  year={2024}
}

@article{kim2024transformers2,
  title={Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape},
  author={Kim, Juno and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2402.01258},
  year={2024}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}

@article{li2024one,
  title={One-Layer Transformer Provably Learns One-Nearest Neighbor In Context},
  author={Li, Zihao and Cao, Yuan and Gao, Cheng and He, Yihan and Liu, Han and Klusowski, Jason M and Fan, Jianqing and Wang, Mengdi},
  journal={arXiv preprint arXiv:2411.10830},
  year={2024}
}

@article{lu2024asymptotic,
  title={Asymptotic theory of in-context learning by linear attention},
  author={Lu, Yue M and Letey, Mary I and Zavatone-Veth, Jacob A and Maiti, Anindita and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2405.11751},
  year={2024}
}

@article{mahankali2023one,
  title={One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention},
  author={Mahankali, Arvind and Hashimoto, Tatsunori B and Ma, Tengyu},
  journal={arXiv preprint arXiv:2307.03576},
  year={2023}
}

@article{muller2021transformers,
  title={Transformers can do bayesian inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}

@article{nichani2024transformers,
  title={How transformers learn causal structure with gradient descent},
  author={Nichani, Eshaan and Damian, Alex and Lee, Jason D},
  journal={arXiv preprint arXiv:2402.14735},
  year={2024}
}

@article{sander2024transformers,
  title={How do Transformers perform In-Context Autoregressive Learning?},
  author={Sander, Michael E and Giryes, Raja and Suzuki, Taiji and Blondel, Mathieu and Peyr{\'e}, Gabriel},
  journal={arXiv preprint arXiv:2402.05787},
  year={2024}
}

@article{vladymyrov2024linear,
  title={Linear Transformers are Versatile In-Context Learners},
  author={Vladymyrov, Max and Von Oswald, Johannes and Sandler, Mark and Ge, Rong},
  journal={arXiv preprint arXiv:2402.14180},
  year={2024}
}

@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{von2023uncovering,
  title={Uncovering mesa-optimization algorithms in transformers},
  author={Von Oswald, Johannes and Schlegel, Maximilian and Meulemans, Alexander and Kobayashi, Seijin and Niklasson, Eyvind and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and others},
  journal={arXiv preprint arXiv:2309.05858},
  year={2023}
}

@article{wu2023many,
  title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  author={Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2310.08391},
  year={2023}
}

@article{yang2024context,
  title={In-context learning with representations: Contextual generalization of trained transformers},
  author={Yang, Tong and Huang, Yu and Liang, Yingbin and Chi, Yuejie},
  journal={arXiv preprint arXiv:2408.10147},
  year={2024}
}

@article{zhang2023trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}

@article{zhang2024context,
  title={In-context learning of a linear Transformer block: benefits of the MLP component and one-step GD initialization},
  author={Zhang, Ruiqi and Wu, Jingfeng and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2402.14951},
  year={2024}
}

@article{zheng2024mesa,
  title={On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability},
  author={Zheng, Chenyu and Huang, Wei and Wang, Rongzhen and Wu, Guoqiang and Zhu, Jun and Li, Chongxuan},
  journal={arXiv preprint arXiv:2405.16845},
  year={2024}
}

