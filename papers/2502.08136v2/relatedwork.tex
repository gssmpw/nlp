\section{Related work}
There is an abundance of recent literature attempting to explain the theoretical mechanisms behind in-context learning. In the setting of IID-data, \cite{garg2022can} introduced the concept of in-context learning a function class and demonstrated experimentally that trained transformers can indeed learn simple function classes in-context. Several empirical works provided evidence that transformers perform in-context learning by implementing a mesa-optimization algorithm to minimize a context-dependent loss \cite{von2023transformers,akyurek2022learning,dai2022can,muller2021transformers}. Subsequently, several theoretical works proved that single-layer linear transformers indeed implement one step of gradient descent to solve linear regression tasks \cite{zhang2023trained,mahankali2023one,ahn2023transformers, wu2023many}.  Several recent works have refined the analysis of single-layer linear transformers, proving additional results on expressivity \cite{zhang2024context}, adaptivity \cite{vladymyrov2024linear}, adversarial robustness \cite{anwar2024adversarial}, and task diversity \cite{cole2024provable, lu2024asymptotic}. For softmax attention, recent works \cite{collins2024context, li2024one} derived in-context learning guarantees for one-layer transformers and related their inference-time prediction to nearest neighbor algorithms. Concerning deep models, the theoretical works \cite{giannou2024well,fu2023transformers} proved that multi-layer transformers can implement higher-order optimizations such as Newton's method. Beyond linear functions, several works have proven approximation, statistical, and optimization error guarantees for in-context learning over larger function classes such as representation learning \cite{yang2024context,guo2023transformers, kim2024transformers2}, generalized linear models \cite{bai2024transformers}, and nonparametric function spaces \cite{kim2024transformers, collins2024context}.

There are substantially fewer theoretical works on the in-context learning capabilities of transformers for non-IID data. The work \cite{von2023uncovering} extended the constructions of \cite{von2023transformers} to the autoregressive setting and hypothesized that autoregressively-trained transformers learn to implement mesa-optimization algorithms to solve downstream tasks. The works \cite{sander2024transformers, zheng2024mesa} are closest to us and they prove that the construction in \cite{von2023uncovering} is optimal for data arising from a class of noiseless linear dynamical systems; in particular, \cite{zheng2024mesa} provides a sufficient condition under which the mesa-optimizer is learned during pre-training. Our work differs from these two works by considering linear stochastic dynamics on the whole space, whereas they study deterministic linear dynamics with unitary linear matrix, which leads to a dynamics on a compact set. The successively injected random noise highly complicates the analysis compared to those in  \cite{zheng2024mesa} where only the initial condition is random. However, the geometric ergodicity of the dynamics  \eqref{dynamicalsystem}, implied by the contractive assumption on $W$, enables us to control the approximation error of the transformer over a long trajectory. 

Of a different flavor, \cite{goel2024can} proved by construction that transformers with softmax attention can represent the Kalman filter for time series data. The work \cite{li2023transformers} interprets in-context learning as learning over a space of algorithms and provides bounds on the statistical error for learning dynamical systems in-context. The works \cite{nichani2024transformers, edelman2024evolution} studied in-context learning over discrete Markov chains.