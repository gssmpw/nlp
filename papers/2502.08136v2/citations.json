[
  {
    "index": 0,
    "papers": [
      {
        "key": "garg2022can",
        "author": "Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory",
        "title": "What can transformers learn in-context? a case study of simple function classes"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "von2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      },
      {
        "key": "akyurek2022learning",
        "author": "Aky{\\\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny",
        "title": "What learning algorithm is in-context learning? investigations with linear models"
      },
      {
        "key": "dai2022can",
        "author": "Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu",
        "title": "Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers"
      },
      {
        "key": "muller2021transformers",
        "author": "M{\\\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank",
        "title": "Transformers can do bayesian inference"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhang2023trained",
        "author": "Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L",
        "title": "Trained transformers learn linear models in-context"
      },
      {
        "key": "mahankali2023one",
        "author": "Mahankali, Arvind and Hashimoto, Tatsunori B and Ma, Tengyu",
        "title": "One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention"
      },
      {
        "key": "ahn2023transformers",
        "author": "Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit",
        "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"
      },
      {
        "key": "wu2023many",
        "author": "Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter L",
        "title": "How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhang2024context",
        "author": "Zhang, Ruiqi and Wu, Jingfeng and Bartlett, Peter L",
        "title": "In-context learning of a linear Transformer block: benefits of the MLP component and one-step GD initialization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "vladymyrov2024linear",
        "author": "Vladymyrov, Max and Von Oswald, Johannes and Sandler, Mark and Ge, Rong",
        "title": "Linear Transformers are Versatile In-Context Learners"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "anwar2024adversarial",
        "author": "Anwar, Usman and Von Oswald, Johannes and Kirsch, Louis and Krueger, David and Frei, Spencer",
        "title": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "cole2024provable",
        "author": "Cole, Frank and Lu, Yulong and O'Neill, Riley and Zhang, Tianhao",
        "title": "Provable in-context learning of linear systems and linear elliptic pdes with transformers"
      },
      {
        "key": "lu2024asymptotic",
        "author": "Lu, Yue M and Letey, Mary I and Zavatone-Veth, Jacob A and Maiti, Anindita and Pehlevan, Cengiz",
        "title": "Asymptotic theory of in-context learning by linear attention"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "collins2024context",
        "author": "Collins, Liam and Parulekar, Advait and Mokhtari, Aryan and Sanghavi, Sujay and Shakkottai, Sanjay",
        "title": "In-context learning with transformers: Softmax attention adapts to function lipschitzness"
      },
      {
        "key": "li2024one",
        "author": "Li, Zihao and Cao, Yuan and Gao, Cheng and He, Yihan and Liu, Han and Klusowski, Jason M and Fan, Jianqing and Wang, Mengdi",
        "title": "One-Layer Transformer Provably Learns One-Nearest Neighbor In Context"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "giannou2024well",
        "author": "Giannou, Angeliki and Yang, Liu and Wang, Tianhao and Papailiopoulos, Dimitris and Lee, Jason D",
        "title": "How Well Can Transformers Emulate In-context Newton's Method?"
      },
      {
        "key": "fu2023transformers",
        "author": "Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal",
        "title": "Transformers learn higher-order optimization methods for in-context learning: A study with linear models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yang2024context",
        "author": "Yang, Tong and Huang, Yu and Liang, Yingbin and Chi, Yuejie",
        "title": "In-context learning with representations: Contextual generalization of trained transformers"
      },
      {
        "key": "guo2023transformers",
        "author": "Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu",
        "title": "How do transformers learn in-context beyond simple functions? a case study on learning with representations"
      },
      {
        "key": "kim2024transformers2",
        "author": "Kim, Juno and Suzuki, Taiji",
        "title": "Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "bai2024transformers",
        "author": "Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song",
        "title": "Transformers as statisticians: Provable in-context learning with in-context algorithm selection"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "kim2024transformers",
        "author": "Kim, Juno and Nakamaki, Tai and Suzuki, Taiji",
        "title": "Transformers are minimax optimal nonparametric in-context learners"
      },
      {
        "key": "collins2024context",
        "author": "Collins, Liam and Parulekar, Advait and Mokhtari, Aryan and Sanghavi, Sujay and Shakkottai, Sanjay",
        "title": "In-context learning with transformers: Softmax attention adapts to function lipschitzness"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "von2023uncovering",
        "author": "Von Oswald, Johannes and Schlegel, Maximilian and Meulemans, Alexander and Kobayashi, Seijin and Niklasson, Eyvind and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and others",
        "title": "Uncovering mesa-optimization algorithms in transformers"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "von2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "sander2024transformers",
        "author": "Sander, Michael E and Giryes, Raja and Suzuki, Taiji and Blondel, Mathieu and Peyr{\\'e}, Gabriel",
        "title": "How do Transformers perform In-Context Autoregressive Learning?"
      },
      {
        "key": "zheng2024mesa",
        "author": "Zheng, Chenyu and Huang, Wei and Wang, Rongzhen and Wu, Guoqiang and Zhu, Jun and Li, Chongxuan",
        "title": "On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "von2023uncovering",
        "author": "Von Oswald, Johannes and Schlegel, Maximilian and Meulemans, Alexander and Kobayashi, Seijin and Niklasson, Eyvind and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and others",
        "title": "Uncovering mesa-optimization algorithms in transformers"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zheng2024mesa",
        "author": "Zheng, Chenyu and Huang, Wei and Wang, Rongzhen and Wu, Guoqiang and Zhu, Jun and Li, Chongxuan",
        "title": "On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zheng2024mesa",
        "author": "Zheng, Chenyu and Huang, Wei and Wang, Rongzhen and Wu, Guoqiang and Zhu, Jun and Li, Chongxuan",
        "title": "On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "goel2024can",
        "author": "Goel, Gautam and Bartlett, Peter",
        "title": "Can a transformer represent a Kalman filter?"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "li2023transformers",
        "author": "Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet",
        "title": "Transformers as algorithms: Generalization and stability in in-context learning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "nichani2024transformers",
        "author": "Nichani, Eshaan and Damian, Alex and Lee, Jason D",
        "title": "How transformers learn causal structure with gradient descent"
      },
      {
        "key": "edelman2024evolution",
        "author": "Edelman, Benjamin L and Edelman, Ezra and Goel, Surbhi and Malach, Eran and Tsilivis, Nikolaos",
        "title": "The evolution of statistical induction heads: In-context learning markov chains"
      }
    ]
  }
]