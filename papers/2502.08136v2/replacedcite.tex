\section{Related work}
There is an abundance of recent literature attempting to explain the theoretical mechanisms behind in-context learning. In the setting of IID-data, ____ introduced the concept of in-context learning a function class and demonstrated experimentally that trained transformers can indeed learn simple function classes in-context. Several empirical works provided evidence that transformers perform in-context learning by implementing a mesa-optimization algorithm to minimize a context-dependent loss ____. Subsequently, several theoretical works proved that single-layer linear transformers indeed implement one step of gradient descent to solve linear regression tasks ____.  Several recent works have refined the analysis of single-layer linear transformers, proving additional results on expressivity ____, adaptivity ____, adversarial robustness ____, and task diversity ____. For softmax attention, recent works ____ derived in-context learning guarantees for one-layer transformers and related their inference-time prediction to nearest neighbor algorithms. Concerning deep models, the theoretical works ____ proved that multi-layer transformers can implement higher-order optimizations such as Newton's method. Beyond linear functions, several works have proven approximation, statistical, and optimization error guarantees for in-context learning over larger function classes such as representation learning ____, generalized linear models ____, and nonparametric function spaces ____.

There are substantially fewer theoretical works on the in-context learning capabilities of transformers for non-IID data. The work ____ extended the constructions of ____ to the autoregressive setting and hypothesized that autoregressively-trained transformers learn to implement mesa-optimization algorithms to solve downstream tasks. The works ____ are closest to us and they prove that the construction in ____ is optimal for data arising from a class of noiseless linear dynamical systems; in particular, ____ provides a sufficient condition under which the mesa-optimizer is learned during pre-training. Our work differs from these two works by considering linear stochastic dynamics on the whole space, whereas they study deterministic linear dynamics with unitary linear matrix, which leads to a dynamics on a compact set. The successively injected random noise highly complicates the analysis compared to those in  ____ where only the initial condition is random. However, the geometric ergodicity of the dynamics  \eqref{dynamicalsystem}, implied by the contractive assumption on $W$, enables us to control the approximation error of the transformer over a long trajectory. 

Of a different flavor, ____ proved by construction that transformers with softmax attention can represent the Kalman filter for time series data. The work ____ interprets in-context learning as learning over a space of algorithms and provides bounds on the statistical error for learning dynamical systems in-context. The works ____ studied in-context learning over discrete Markov chains.