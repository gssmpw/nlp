\documentclass{article}

\usepackage[preprint]{cpal_2025}

%add packages
\usepackage{url}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.1pt] (char) {#1};}}
\newcommand{\note}[1]{\textcolor{blue}{[#1]}}
\usepackage{subfigure}
\usepackage{bm}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{ulem}

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{totcount}

\usepackage{lipsum}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Towards Vector Optimization on Low-Dimensional Vector Symbolic Architecture}


\author{%
  Shijin Duan\textsuperscript{1}, Yejia Liu\textsuperscript{2}, Gaowen Liu\textsuperscript{3}, Ramana Rao Kompella\textsuperscript{3}, Shaolei Ren\textsuperscript{2}, Xiaolin Xu\textsuperscript{1}\\
  \textsuperscript{1}Northeastern University, \textsuperscript{2}University of California, Riverside,
  \textsuperscript{3}Cisco Research\\
  \texttt{\{duan.s, x.xu\}@northeastern.edu, yliu807@ucr.edu, sren@ece.ucr.edu, \\\{gaoliu, rkompell\}@cisco.com}
}

\begin{document}

\maketitle

\begin{abstract}
Vector Symbolic Architecture (VSA) is emerging in machine learning due to its efficiency, but they are hindered by issues of hyperdimensionality and accuracy. As a promising mitigation, the Low-Dimensional Computing (LDC) method significantly reduces the vector dimension by $\sim$100 times while maintaining accuracy, by employing a gradient-based optimization.  Despite its potential, LDC optimization for VSA is still underexplored. Our investigation into vector updates underscores the importance of stable, adaptive dynamics in LDC training. We also reveal the overlooked yet critical roles of batch normalization (BN) and knowledge distillation (KD) in standard approaches. Besides the accuracy boost, BN does not add computational overhead during inference, and KD significantly enhances inference confidence. Through extensive experiments and ablation studies across multiple benchmarks, we provide a thorough evaluation of our approach and extend the interpretability of binary neural network optimization similar to LDC, previously unaddressed in BNN literature.
\end{abstract}



\section{Introduction}
Vector symbolic architecture (VSA) has
been emerging for resource-limited devices, because of their low latency and high efficiency characteristics \cite{kleyko2023survey}. Towards practice, VSA has been applied in various applications, such as DNA coding~\cite{kim2020geniehd}, holographic feature decomposition~\cite{poduval2024hdqmf}, brain-computer interface tasks~\cite{liu2024scheduled}, and cognitive tasks~\cite{moin2021wearable}; and on different hardware including micro-controller~\cite{narkthong2024microvsa}, FPGA \cite{imani2021LookHD}, and in-memory computing \cite{karunaratne2020memory}.
In VSAs, the input samples are encoded as high-dimensional vectors, i.e., $\bm{s}=\text{sgn}(\sum_i \textbf{F}_i\circ \textbf{V}_{x_i})$, enabling parallel computations. Here, $\textbf{F}$ and $\textbf{V}$ are feature-related vectors. Typically, a binary VSA model needs a few megabytes, highlighting its lightweight nature. VSA has been receiving significant attention in recent years, with focuses on model design and optimization \cite{yu2022understanding, lehdc_dac, neubert2021hyperdimensional, imani2019quanthd} as well as implementations in diverse fields \cite{karunaratne2020memory, kim2020geniehd}.
Nevertheless, further deploying binary VSA models on tiny devices with more strict resource constraints, like kilobyte-scale memory and limited computing circuits, is challenging. Since the previous training strategy is heuristic-driven, reducing vector dimensions significantly compromises the model performance. Even the basic loss functions such as cross-entropy were just explored in recent years \cite{yu2022understanding, lehdc_dac}.

More recently, a novel VSA training strategy, low-dimensional computing (LDC) \cite{ldc} has been proposed, whose vector dimension is reduced by orders of magnitude against that in previous binary VSA models (e.g., 128 vs. 10,000), without noticeable harm on the model inference accuracy. In a nutshell, LDC maps a binary VSA model to a neural network with mixed precisions (both binary and non-binary weights), thus LDC jointly trains all involved vectors in a classification task.
By doing so, the trained vectors with low dimensions compress the binary VSA model from megabytes to kilobytes. 
Despite the overwhelming advantage, the current LDC training is still empirical and under-explored, with only basic settings, such as straight-through estimator (STE) \cite{xnornet_2016}. On the other hand, while LDC is trained as a partial binary neural network (BNN), the interpretation and theoretical analysis of some critical strategies on BNN training are also absent.
For example, while batch normalization (BN) and knowledge distillation (KD), as two necessities to be discussed in our work, have been implicitly employed in BNN training \cite{liu2020reactnet, xnornet_2016}, detailed analysis is still lack on them. 


In this paper, we take LDC training as an example, to analyze the training of binary VSA models, specifically for the feature vectors \textbf{F} and class vectors \textbf{C}, from the gradient-based perspective. We explore the expected behavior of these vectors for optimal sample representation and classification. We also highlight how BN and KD enhance their optimization, which is neglected in standard LDC training.
Besides the interpretation, we further propose two novel views: \circled{1} For BN, the floating-point operation together with trained parameters can be absorbed as channel-wise thresholds during inference, thus eliminating costly computation. \circled{2} For KD, we indicate that the temperature hyperparameter can adjust the network prediction confidence, which could also be considered during hyperparameter optimization. This can also save the extra post-processing scheme for confidence calibration that has been proposed \cite{ji2019bin}.

We summarize our contributions as follows:
\begin{itemize}
    \item We investigate the binary vector training on VSA models under the LDC training strategy, and indicate the significant benefit of batch normalization and knowledge distillation. We further demonstrate corresponding numerical validation to support our analysis.
    \item We depict that BN will not burden the computing during VSA inference and KD design can provide a metric to adjust the VSA prediction confidence. Not like previous conducted work, we do not require extra mechanism to achieve these goals.
    \item Our evaluations show that the BN and KD-assisted LDC training can achieve the best or comparable accuracies over SOTA binary VSA works, while only consuming about 2\% memory footprint and $<30\%$ latency of binary VSA models.
    Other ablation studies (in Appendix) are also provided to evaluate our analysis in depth.
\end{itemize}

\section{Preliminary on Vector Symbolic Architecture} \label{sec:related_work}
Vector symbolic architecture (VSA) represents objects using vectors and performs computation element-wise \cite{kleyko2023survey}. 
Its binary format~\cite{hyperdimensionalcomputing_2009} is highly efficient in computation, favoring resource-limited devices. 
Assuming a sample $\bm{x}$ has $N$ features and each feature has $M$ discretized values, binary VSA generates the feature/value vector set $\textbf{F}\in\{1,-1\}^{N\times D}$ and $\textbf{V}\in\{1,-1\}^{M\times D}$ to represent $N$ feature positions and $M$ values, respectively.
Binary VSA encodes one sample with a binary vector $\bm{s}$, 
\begin{equation}
    \bm{s} = \textrm{sgn}\left(\sum_{i=1}^N \textbf{F}_i\circ \textbf{V}_{\bm{x}_i}\right)
\label{eq:encoding}
\end{equation}
where $\bm{x}_i$ is the value for the $i$-th feature, $\circ$ is Hadamard product, and $\text{sgn}()$ function binarizes the accumulation result. We set up $\text{sgn}(0)=1$ as tie-breaker. Given a classification task, binary VSA training generates a class vector set $\textbf{C}\in\{1,-1\}^{K\times D}$ to represent $K$ categories in this task. The similarity between vectors in \textbf{C} and $\bm{s}$ is calculated by the dot product, reduced from \textit{cosine} similarity,
\begin{equation}
    \text{label} =\arg\max_{k}\textbf{C}_k^T\ \bm{s}
\label{eq:similarity}
\end{equation}
where the most similar one (with the highest product) is the predicted label. 

Current binary VSA utilizes very high-dimensional vectors ($D \approx 10,000$) for acceptable accuracy, as \textbf{F} and \textbf{V} are generated randomly in advance \cite{yu2022understanding,lehdc_dac, imani2019quanthd}.
The recently proposed strategy for binary VSA training, low-dimensional computing (LDC) \cite{ldc}, addresses the high-dimension issue in binary VSA by approximating the value vector mapping as a shallow neural network, $\mathcal{V}(\bm{x}_i):\bm{x}_i\mapsto \textbf{V}_{\bm{x}_i}$. Then, the encoding (Eq.\ref{eq:encoding}) and similarity measurement (Eq.\ref{eq:similarity}) are expressed as a two-layer binary neural network (BNN),
\begin{equation}
        [\bm{x}_1,...,\bm{x}_N]\to \mathcal{V}(\cdot) \xrightarrow[]{[\mathcal{V}(\bm{x_i})\in\{1,-1\}^D]} \underbrace{\textstyle \bm{y} =  \sum_{i=1}^{N}\textbf{F}_i\circ \mathcal{V}(\bm{x}_i)}_\text{encoding layer} \xrightarrow[]{\bm{s}=\text{sgn}(\bm{y})} \underbrace{\bm{z} = \textbf{C}\bm{s}}_\text{similarity layer}
\end{equation}
where \textbf{F} and \textbf{C} are treated as binary weights of each layer, and sgn$()$ as the activation between layers. 
Therefore, the encoding and the similarity measurement are translated to a binary weighted sum layer and a binary linear layer, respectively. All the involved vectors, including \textbf{V}(concluded from $\mathcal{V}(\cdot)$), \textbf{F}, and \textbf{C} can be optimized by training this BNN rather than randomly generated. 
As a result, the VSA with vectors generated by LDC can achieve comparable accuracy as SOTA binary VSA \cite{lehdc_dac} while only with less than $1\%$ model size, leading to extremely lightweight hardware implementation.

While LDC provides a promising solution to binary VSA training, the training principles on it are still under-explored. Previous LDC only applied the basic STE \cite{xnornet_2016} and Adam \cite{liu2021adam} strategies that are used in BNN training. Besides, BNN optimization is challenging and ongoing research. Motivated by these considerations, we aim to explore current LDC training as a representative case study for BNN training, and discuss our perspective on its optimization.

Note that optimization of $\mathcal{V}(\cdot)$ is excluded from the discussion in this paper since it is a real-valued network that can be well-trained with modern strategies, and the architecture difference of $\mathcal{V}(\cdot)$ showed insignificant impact on LDC training \cite{ldc}. Still, we provide an introduction to $\mathcal{V}(\cdot)$ in Appendix~\ref{app:value_vector_mapping}. Besides, the Quantization-Aware Training (QAT) algorithm \cite{nagel2022overcoming} is applied, iteratively freezing oscillating weights for a smooth convergence during LDC training, see Appendix~\ref{app:training_detail}. 


\section{Training Optimization on Feature Vectors}
Similar to BNN training, LDC also utilizes real-valued (or \textit{latent}) weights for the gradient propagation, and the binarized counterparts for the forward pass. For the feature vectors \textbf{F}, we denote \textbf{F}$^r$ as the real-valued counterparts for \textbf{F} during the LDC training, which has the following property,
\begin{equation}
\textbf{F}_{i,d}=\alpha(\textbf{F}_d)\ \text{sgn}(\textbf{F}_{i,d}^r)=\begin{cases}
+\alpha({\textbf{F}_d})  & \text{if } \textbf{F}_{i,d}^r \geq 0\\
-\alpha({\textbf{F}_d})  & \text{otherwise } 
\end{cases},
\text{  where}\  \alpha({\textbf{F}_d})=\frac{\left \| \textbf{F}_{:,d}^r \right \|_{l1}}{N}.
\label{eq:latentF}
\end{equation}
Here \textbf{F}$_{i,d}$ means the binary element located at the $i$-th row and $d$-th column. Rather than directly applying sgn$()$ function for binarization, a scaling factor $\alpha$ is multiplied for weight updating, which is the $l_1$-norm mean of corresponding weights. 
Scaling factors can be directly removed during inference. Moreover, due to the zero gradient of sgn$()$, it is also approximated during backward propagation, 
\begin{equation}
    \text{\textbf{(forward)} sgn}(x) = \left\{\begin{matrix}
1,  & x\geq 0\\
-1,  & x<0
\end{matrix}\right.  \text{,  }
\text{\textbf{(backward)} sgn}(x) = \left\{\begin{matrix}
1,  & x\geq 1\\
x,  & -1\leq x<1\\
-1,  &x < -1
\end{matrix}\right.
\label{eq:latentSgn}
\end{equation}

\subsection{Analysis of Vanilla Training Process}
We first explore the updating step on \textbf{F} from Eq.\ref{eq:latentF} and Eq.\ref{eq:latentSgn} in the basic LDC training. The gradient on a certain \textit{latent} weight \textbf{F}$_{i,d}^r$, w.r.t. the sample vector $\bm{s}$, is
\begin{equation}
\frac{\partial \bm{s}}{\partial \textbf{F}_{i,d}^r} = \mathcal{V}(\bm{x}_i)_d \cdot \left [1(|\bm{y}_d|\leq 1) +0(|\bm{y}_d|> 1)\right ]\text{,  }
  \text{where } \bm{y}_d={\textstyle \sum_{i=1}^{N}\textbf{F}_{i,d}\circ \mathcal{V}(\bm{x}_{i})_d}
\end{equation}
The gradient of each weight element \textbf{F}$^r_{i,d}$ is just based on the corresponding value vector bit $\mathcal{V}(\bm{x}_i)_d$, i.e., the $d$-th bit of the $i$-th value vector, which is $\pm 1$. Following the chain rule, the magnitude of \textbf{F}$^r_{i,d}$ gradients can only be adjusted by the gradient from the loss term, $\partial \mathcal{L}/\partial \bm{s}$. 
Explicitly expressing, $|\partial \mathcal{L}/\partial \textbf{F}_{i,d}^r| = |\partial \mathcal{L}/\partial \bm{s}_d|\cdot|\partial \bm{s}_d/\partial \textbf{F}_{i,d}^r|=\{-|\partial \mathcal{L}/\partial \bm{s}_d|, 0, +|\partial \mathcal{L}/\partial \bm{s}_d|\}$. 
This is inflexible because: \circled{1} If $|\partial \mathcal{L}/\partial \textbf{F}_{i,d}^r| \ll |\textbf{F}^r_{i,d}|$, the $\textbf{F}_{i,d}$ might not change by learning from the $\textbf{F}^r_{i,d}$ updates; \circled{2} If $|\partial \mathcal{L}/\partial \textbf{F}_{i,d}^r| \gg |\textbf{F}^r_{i,d}|$, oscillation could happen~\cite{liu2021adam} since the sign of $\textbf{F}^r_{i,d}$ might change in every updating, hindering the training convergence. 

For numerical investigation, we run 10 epochs for the LDC training, and then feed in one misclassified image (supposed to induce noticeable weight gradients) for gradient calculation. We illustrate the distributions of $\bm{y}$, $\partial \mathcal{L}/\partial \textbf{F}^r$, and $\textbf{F}^r$ in Figure~\ref{fig:Fupdate}(a)(b)(c), respectively. Firstly, the zero gradient makes up the most part (78.13\%) of the distribution, indicating that most $\bm{y}$ fall outside the slope range $[-1,1]$. This can be observed from Figure~\ref{fig:Fupdate}(a) as well, 
leading to inactively updating on \textit{latent} weights. Then, the non-zero gradient magnitudes (as large as 0.8) are rather larger than the \textit{latent} weights $\textbf{F}^r$ (mostly in the range $[-0.3, 0.3]$ as shown in Figure~\ref{fig:Fupdate}(c)), which is prone to change the sign of \textbf{F}$^r$, causing oscillation. Consequently, the $\textbf{F}$ optimization under vanilla training is insensitive to the back-propagation and locally unstable due to large non-zero gradients. 

\begin{figure}[t]
    \centering
\subfigure[$\bm{y}$ Distribution w/o BN]{
        \centering
		\includegraphics[width=0.3\linewidth]{FIG/logits_wobn.pdf}}
\subfigure[\textbf{F}$^r$ Grad. Dist. w/o BN]{
        \centering
		\includegraphics[width=0.3\linewidth]{FIG/Fgrad_wobn.pdf}}
\subfigure[\textbf{F}$^r$ Distribution w/o BN]{
        \centering
		\includegraphics[width=0.3\linewidth]{FIG/Fdist_wobn.pdf}}
\subfigure[BN$(\bm{y})$ Distribution w/ BN]{
        \centering
		\includegraphics[width=0.3\linewidth]{FIG/logits_wbn.pdf}}
\subfigure[\textbf{F}$^r$ Grad. Dist. w/ BN]{
        \centering
		\includegraphics[width=0.3\linewidth]{FIG/Fgrad_wbn.pdf}}
\subfigure[\textbf{F}$^r$ Distribution w/ BN]{
        \centering
		\includegraphics[width=0.3\linewidth]{FIG/Fdist_wbn.pdf}}
    \caption{Feature vector training analysis for vanilla LDC (a)(b)(c) and our BN-based method (d)(e)(f). All figures are histograms of distributions. We run 10 epochs for (a)(b)(d)(e) to demonstrate the efficacy of BN on LDC training: BN shapes the accumulation $\bm{y}$ to zero mean and unit variance, providing more active and moderate gradients for weight updating. We run 50 epochs for (c) and (f) to show \textbf{F}$^r$ distributino during training. The case study is tested on the LDC model with dimension $D=64$ and on FashionMNIST.}
    \label{fig:Fupdate}
\end{figure}

\setlength{\intextsep}{-0pt}%
\begin{wrapfigure}[11]{r}{0.55\linewidth}
\captionof{table}{Preliminary result on methods to mitigate the \textbf{F} training deficiencies. $\alpha(\textbf{F})\downarrow$ is to reduce the scaling factor. $\bm{\delta}\uparrow$ is to increase the active range of sgn$()$. BN is the batch normalization method that we adopt in our work. We show the variance of $\bm{y}$ distribution and the range of $\partial \mathcal{L}/\partial \textbf{F}^r$ as metrics since they directly dominate the \textbf{F} updating.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc}
\toprule[2pt]
 & LDC & +($\alpha(\textbf{F})\downarrow$) & +($\bm{\delta}\uparrow$) & +BN \\\midrule
Var($\bm{y}$) & 10.49 & 6.83 & 10.11 & 0.98 \\
$\partial \mathcal{L}/\partial \textbf{F}^r$ & $[-0.8,0.8]$ & $[-0.6, 0.6]$ & $[-0.45, 0.45]$ & $[-0.3, 0.3]$ \\\bottomrule
\end{tabular}}
\label{tab:preliminaryF}
\end{wrapfigure}
Intuitively, we propose two simple tricks to mitigate the aforementioned issues. We directly reduce the scaling factor of \textbf{F} so that more $\bm{y}$ elements can fall into the active range $[-1,1]$, i.e., more $\textbf{F}^r$ with non-zero gradients. Alternatively, we can also enlarge the active range to $[-\delta, \delta]$, where $\delta>1$, to activate more $\textbf{F}^r$. We show the preliminary result in Table~\ref{tab:preliminaryF}, with a bold configuration that $\alpha(\textbf{F})$ is scaled down by half or enlarging the activate range to $\delta=1.2$. Both lower scaling factor and larger active range are beneficial to feature vector optimization, with lower $\bm{y}$ magnitudes (i.e., lower Var($\bm{y}$)) or smaller gradient magnitudes $|\partial \mathcal{L}/\partial \textbf{F}^r|$. Nevertheless, these intuitive tricks are still not sufficient. The scaling of $\alpha(\textbf{F})$ and $\delta$ should be carefully tuned as hyperparameters, which is time-consuming and tedious. On the other hand, while more active \textit{latent} weights participate in the training, the gradient magnitude is not directly tuned by these tricks.
Therefore, we emphasize the necessity of an adaptive method to solve the issues above, which is batch normalization (BN) in the feature vector training.


\subsection{Batch Normalization Benefits Feature Vector Training} \label{sec:BN_helps_training}

BN \cite{batch_normalization_analysis} aims to stabilize the training procedure by normalizing the activation in dimension-wise. Specifically, by applying BN to the encoding layer, we have
\begin{equation}
\bm{s} = \text{sgn}(\text{BN}(\bm{y}))\text{, where } \bm{y} = \sum_{i=1}^{N}\textbf{F}_{i}\circ \mathcal{V}(\bm{x}_i) 
\text{ and BN}(\bm{y})=\frac{\bm{y}-\text{E}(\bm{y})}{\sqrt{\text{Var}(\bm{y})+\epsilon}} \times w_{BN}+b_{BN}.
\label{eq:bn_encoding}
\end{equation}
E$(\bm{y})$ and Var$(\bm{y})$ are the statistical mean and variance of $\bm{y}$ during training, and $b_{BN}$ and $w_{BN}$ are the trainable parameters to further adjust the distribution. $\epsilon$ is a small constant for numerical stability. 


With BN, the gradient on \textit{latent} weight \textbf{F}$^r_{i,d}$, w.r.t. the sample vector $\bm{s}$, can be derived as
\begin{equation}
\frac{\partial \bm{s}}{\partial \textbf{F}^r_{i,d}} = \mathcal{V}(\bm{x}_i)_d \cdot \left(w_{BN, d}/\sqrt{\text{Var}(\bm{y}_d)+\epsilon}\right)\cdot \left [1(|\text{BN}(\bm{y}_d)|\leq 1) +0(|\text{BN}(\bm{y}_d)|> 1)\right ]
\end{equation}
Here, the BN contributes the gradient calculation of \textbf{F}$^r$ in two ways: (i) appropriate affine transformation $|\text{BN}(\bm{y}_d)|$ and (ii) trainable gradient magnitude $w_{BN}/\sqrt{\text{Var}(\bm{y}_d)+\epsilon}$.

\textbf{Appropriate affine transformation} is a straightforward benefit that BN provides. Since the input of sgn$()$ tends to approach a lower variance with BN (0.98 vs. 10.49 at epoch 10 in Figure~\ref{fig:Fupdate}(a) and (d)), more accumulations $\bm{y}$ after BN fall into the active range $[-1,1]$, allowing more \textit{latent} weights to participate in the updating. In Figure~\ref{fig:Fupdate}(e), only 40.63\% gradients of \textbf{F}$^r$ are still zero, which is much less than 78.13\% of the case without BN in Figure~\ref{fig:Fupdate}(b). Note that $b_{BN}$ in BN is supposed to shift the accumulation $\bm{y}$, but its influence is actually negligible. This is because binary VSA exhibits centrosymmetric properties with respect to zero, i.e., vectors are from $\{-1,+1\}$ and activation function is also symmetric w.r.t. zero. Therefore, zero-centered distribution is favored for the optimization; in fact, regardless of the absence or presence of BN, the $\bm{y}$ distribution is near-zero centered, i.e., $-0.4581$ vs. $-0.0038$ in Figure~\ref{fig:Fupdate}(a) and (d).

\textbf{Trainable gradient magnitude} benefits from the trainable BN weights $w_{BN}$. 
Since the derivation of an optimal $w_{BN}$ is rather complicated and the space is limited, we provide a detailed analysis in Appendix~\ref{app:BN} and just include a brief interpretation here. 
$w_{BN}$ is introduced to rescale the variance of $\bm{y}$ together with Var$(\bm{y})$, for potentially better distribution \cite{batch_normalization_definition}.
Regarding the similarity layer in LDC, the optimal input distribution can be learned by calculating
$\partial \mathcal{L}/ \partial \text{BN}(\bm{y})$. This leads to the optimization of BN parameter, 
$\partial \mathcal{L}/\partial w_{BN}$, which is under the influence of current loss and the \textit{latent} weight \textbf{C}$^r$. 
Thus, the BN weights $w_{BN}$ can adaptively scale the weight gradients on \textbf{F}$^r$ in an appropriate range. For example in Figure~\ref{fig:Fupdate}(e), the gradients are scaled from $[-0.8, 0.8]$ (in Figure~\ref{fig:Fupdate}(b)) to $[-0.3, 0.3]$. 
Proper gradient magnitudes can effectively prevent weight oscillation, since the sign of \textit{latent} weight is not prone to flip in one updating step.

As a straight comparison, we demonstrate the weight distribution of \textbf{F}$^r$ in Figure~\ref{fig:Fupdate}(c) and (f). Throughout the training, the \textit{latent} weights of \textbf{F}$^r$ quickly diverge from zero or are frozen to the steady state $\pm 1$ (by QAT algorithm) with the help of BN. As discussed in \cite{nagel2022overcoming}, less near-zero \textit{latent} weights will mitigate the oscillation during BNN training. 
On the other hand, without BN, a big part of \textbf{F}$^r$ stays near zero at the end of the training, indicating that the LDC training is not well converged and significant oscillation still exists in the encoding layer of LDC. 


\subsection{Batch Normalization as A Threshold}
We indicate that all BN parameters can be absorbed into binarization as a threshold during binary VSA implementation. According to Eq. \ref{eq:encoding}, ${\textstyle \bm{y}=\sum_{i=1}^{N}\textbf{F}_i\circ \mathcal{V}(\bm{x}_i)}$ is calculated for the binarization, i.e., 0 as the threshold of sgn() function. Similarly, the threshold in Eq. \ref{eq:bn_encoding} can be expressed as
\begin{equation}
\text{BN}(\bm{y}) \geq 0 \Leftrightarrow  \left({\textstyle \sum_{i=1}^{N}\textbf{F}_i\circ \mathcal{V}(\bm{x}_i)}\right) \geq \theta
\text{,  where }\theta=\left \lceil \left(\text{E}(\bm{y})-\frac{\sqrt{\text{Var}(\bm{y})+\epsilon}\cdot b_{BN}}{w_{BN}}\right) / \alpha_\textbf{F} \right \rceil 
\label{eq:BN2Threshold}
\end{equation}
Therefore, the binarization operation with BN during inference can be translated to a comparison with dimension-wise thresholds $\theta$. Since the accumulation $\bm{y}_d$ on binary vectors is an integer, the threshold $\theta$ is also rounded. 

\textbf{Extending to BNN.} This derivation can be generalized to the BN in BNNs as well. Hence, BN will not introduce additional computation in BNN if followed by sgn$()$ activation, while improving the performance during inference. Prior work has developed efficient alternatives to BN \cite{chen2021bnn, jiang2021training}, addressing its floating-point computations during inference; yet, our analysis suggests that BN can be seamlessly integrated as dimension-wise thresholds in binary VSA, without introducing additional computational overhead.

\subsection{Early Evaluation}\label{app:early_evaluation}
For a preliminary evaluation of batch normalization (BN) in LDC training, we compare its effectiveness against other normalization strategies across various vector dimensions, with results shown in Table~\ref{tab:BN_comparison} on the FashionMNIST dataset. All normalization methods improve training by stabilizing the process, as discussed in Section~\ref{sec:BN_helps_training}. (i) Comparing BN and vanilla training, BN not only can improve the inference accuracy of binary VSA models on various vector dimensions, but also mitigates the saturation issue existing in vanilla training when $D$ is large, e.g., $D=512$ and 1024. Besides, vanilla training exhibits large oscillation (higher variance on accuracy) under large vector dimension. In contrast, BN-assisted LDC shows a much lower variance in accuracy, so less oscillation occurs during LDC training with BN. This also proves that BN can yield smooth convergence. (ii) When comparing normalization strategies, BN outperforms layer normalization (LN) due to its approach of normalizing across the data batch. In a data batch for the VSA model, each bit has roughly equal probabilities of being 1 and -1 along the batch. This primarily rescales the variance without introducing significant bias, as evidenced by the near-zero $b_{BN}$ (see Appendix~\ref{app:discussion}). In contrast, LN normalizes across vector dimensions for each sample. However, in low-dimensional VSA models, each vector bit contributes individually, and the 1/-1 distributions of vectors $\bm{s}$ could vary significantly between samples across classes. This variability makes it more challenging to generalize a distribution along dimensions, potentially introducing non-zero and inconsistent biases across classes. This observation also aligns with the finding that LayerNorm slightly performs better than RMSNorm, as the latter excludes bias, which may limit its effectiveness in such scenarios. Furthermore, LN also leads to overfitting at large dimensions, as shown by increased variance and reduced accuracy from $D=512$ to $D=1024$, an issue BN avoids by averaging vectors in batches instead of individual dimensions. In addition, we evaluate the batch normalization performance by varying the batch size of data in Appendix~\ref{app:diff_bsz_BN}.


\begin{table}[t]
\centering
\caption{Top-1 accuracy of LDC training with and without normalization, with standard deviation. We consider the proposed batch normalization (BN) and other layer normalization strategies (LayerNorm~\cite{lei2016layer} and RMSNorm~\cite{zhang2019root}). We vary the vector dimension $D$ of binary VSA model. The results are on 5 runs, and the best is marked as \textbf{bold}.}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{l|cccccc}
\toprule[2pt]
Acc.(\%) & $D=$ 32 & 64 & 128 & 256 & 512 & 1024 \\\midrule
LDC & 81.20$^{\pm0.34}$ & 83.62$^{\pm0.21}$ & 85.49$^{\pm0.30}$ & 86.66$^{\pm0.25}$ & 86.94$^{\pm0.85}$ & 86.97$^{\pm0.82}$\\
+LayerNorm & 83.04$^{\pm0.25}$ & 84.35$^{\pm0.69}$ & 85.86$^{\pm0.26}$ & 87.01$^{\pm0.30}$ &  87.75$^{\pm0.19}$ &  87.31$^{\pm0.98}$ \\
+RMSNorm & 82.99$^{\pm1.17}$ & 84.69$^{\pm0.37}$ & 85.05$^{\pm0.58}$ & 86.14$^{\pm0.37}$ &  86.74$^{\pm0.20}$ &  85.86$^{\pm0.72}$ \\
\textbf{+BN} & \textbf{84.24}$^{\pm0.30}$ & \textbf{85.52}$^{\pm0.37}$ & \textbf{86.58}$^{\pm0.10}$ & \textbf{87.41}$^{\pm0.28}$& \textbf{88.01}$^{\pm0.16}$ & \textbf{88.53}$^{\pm0.17}$\\\bottomrule
\end{tabular}}
\label{tab:BN_comparison}
\end{table}



\section{Training Optimization on Class Vectors}
For the class vectors \textbf{C}, we denote its \textit{latent} weights $\textbf{C}^r$, i.e., the real-valued counterparts, as follows:
\begin{equation}
\textbf{C}_{k,d}=\alpha({\textbf{C}})\ \text{sgn}(\textbf{C}_{k,d}^r)=\begin{cases}
+\alpha({\textbf{C}}) & \text{if } \textbf{C}_{k,d}^r \geq 0\\
-\alpha({\textbf{C}})  & \text{otherwise } 
\end{cases},\text{  where } \alpha(\textbf{C})=\frac{\left \| \textbf{C}^r \right \|_{l1}}{K\cdot D}
\end{equation}
Here \textbf{C}$_{k,d}$ is the binary element located at the $k$-the row and $d$-th column. Given $K$ categories and $D$ vector dimension, $\alpha(\textbf{C})$ is the unique scaling factor for the entire class vectors. 

Unlike feature vectors, the optimization on class vectors is more straightforward, since the gradient of $\textbf{C}^r$ is directly reflected by the loss, ${\partial \mathcal{L}_{CE}}/{\partial \textbf{C}^r_{k,d}} = -\bm{s}_d (\bm{t}_k-\sigma(\bm{z})_k)$, where $\sigma(\bm{z})$ is soft-max and cross-entropy is utilized in the vanilla LDC training. $\bm{t}_k$ denotes the targeted probability on the $k$-th class, which is a hard target (0 or 1) in one-hot encoding. Since $\bm{s}$
and $\sigma(\bm{z})$ are determined by the data input and current model weights (i.e., forward-pass information), the training optimization lies in how the loss is calculated and how the target probability $\bm{t}$ is expressed. 

Binary VSA models have limited capacity~\cite{kocher1992capacity} in capturing relationships between input-target pairs due to their constrained parameter space compared to real-valued models, i.e., $2^{|\bm{\theta}|}$ vs. $\mathbb{R}^{|\bm{\theta}|}$. Consequently, the training procedure is expected to adaptively locate a more generalizable fit on complex tasks, by prioritizing success on simpler samples and selectively forgoing those that are excessively challenging. We highlight that knowledge distillation (KD)~\cite{KnowledgeDistillation2015} can be an ideal candidate. Specifically, KD requires an advanced network (namely teacher network) pre-trained for the current classification task, and uses its logits $\bm{z}_t$ as the criterion. The gradient of $\textbf{C}^r$ can be calculated as
\begin{equation}
    \frac{\partial \mathcal{L}_{KLDiv}}{\partial \textbf{C}^r_{k,d}} = -\bm{s}_d (\sigma(\bm{z}_t/T)_k-\sigma(\bm{z}/T)_k)\cdot T
\label{eq:KLD_gradient}
\end{equation}
where $T$ is the temperature hyperparameter. 
While previous work usually takes KD as regularization during training, i.e., formulate the final loss as $\mathcal{L}=\gamma \mathcal{L}_{CE} + (1-\gamma)\mathcal{L}_{KLDiv}$, we only consider the $\mathcal{L}_{KLDiv}$ to emphasize KD's influence in the following discussion.


\subsection{KD Provides More Adaptive Training}\label{sec:KD_analysis}

Compared with the hard labels that perfectly reflect the input's true category, teacher networks might produce wrong predictions on a small number of samples. These imperfect labels actually can provide a smoother classification boundary, by recognizing the hard-to-classified samples beforehand, and replace the true labels with wrong-but-smooth probability distributions. 
\setlength{\intextsep}{-1pt}%
\begin{wrapfigure}[13]{r}{0.3\linewidth}
\centering
\includegraphics[width=\linewidth]{FIG/Cgrad_comp.pdf}
\caption{The \textbf{C}$^r$ gradient distribution of binary VSA after training for 10 epochs with and without KD.}
\label{fig:Cgrad}
\end{wrapfigure}
This will mitigate the gradient magnitude of \textit{latent} weights when LDC encounters these samples, thus reducing unnecessary weight oscillations. By ``unnecessary'', we interpret it as there is no need to force LDC, to learn the probability distribution that even the teacher network cannot well fit. 
We validate this advantage in Figure~\ref{fig:Cgrad}, where 1000 misclassified samples are selected by binary VSA after 10-epoch training, to contain some hard-to-classified samples.
KD provides more near-zero gradients, mitigating the oscillation in updating \textbf{C}$^r$ from these samples; specifically, only 2.19\% of \textbf{C}$^r$ flipped signs after updated from KLDiv loss, while 15.16\% \textbf{C}$^r$ flipped signs from CE loss. The near-zero gradients in KD are different from the case we discussed in BN, because there is no inactive range on $\textbf{C}^r$ updating, and near-zero gradients are only caused by small losses. 

\setlength{\intextsep}{0pt}%
\begin{wrapfigure}{r}{0.5\linewidth}
\centering
\captionof{table}{Investigating the influence of label smoothing and teacher on LDC training. $f$ is the scaling factor for label smoothing, i.e., $f\bm{t}+(1-f)/K$. ``HL-T'' means hard-label from teacher.}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccccc}\toprule[2pt]
 & LDC & +$f=0.1$ & +$f=0.2$ & +$f=0.5$ & +HL-T \\\midrule
Acc. (\%) & 85.09 & 84.07 & 83.53 & 82.43 & 85.89 \\\bottomrule
\end{tabular}}
\label{tab:label_smoothing}
\end{wrapfigure}
Another common benefit of KD is that it can yield a soft probability distribution, facilitating easier convergence. However, this benefit is not obvious in LDC training. We provide a quick exploration in Table~\ref{tab:label_smoothing} that applies label smoothing \cite{muller2019does}, which can just soften probability distribution, under various scaling factors. The results indicate that label smoothing does not positively impact the LDC training, whereas the teacher network improves performance, even with hard labels (HL-T). This disparity is likely attributed to the limited capacity of binary VSA models. Unlike current deep learning models which employ label smoothing for better generalization, binary VSA models, due to their low capacity, focus more on boundary smoothness rather than addressing overfitting issues.




\subsection{KD Can Reshape the Confidence Distribution}
From Eq.\ref{eq:KLD_gradient}, the temperature hyperparameter $T$ will rescale the logits $\bm{z}_t$ and $\bm{z}$, in a softened scale when $T>1$.
This introduces a previously overlooked efficacy of KD that when $T$ is large, the VSA model will have higher confidence during inference. We employ Shannon entropy \cite{shannon1948mathematical} $H(\sigma(\bm{z}))=-\sum_k \sigma(\bm{z})_k log(\sigma(\bm{z})_k)$ to estimate the confidence for one inference. Specifically, the one-hot distribution (very confident prediction) induces the lowest entropy $H=0$, while the uniform distribution (all $1/K$) gives the highest entropy $H=log(K)$. For a correct prediction, we expect a low entropy; on the contrary, the wrong prediction should have a high entropy so that VSA can report an unreliable prediction. We calculate the average entropy of correct predictions $\overline{H}(\sigma(\bm{z}))_T$ ($T$ for True) and wrong predictions $\overline{H}(\sigma(\bm{z}))_F$ ($F$ for False) in Table~\ref{tab:KD_comp}. 
In the common range of $1\leq T\leq 10$, KD shows a positive influence to produce a high-confident VSA model for correct predictions. However, the entropy of wrong predictions also decreases along $T$, which is against our expectation for an ideal model. Nevertheless, the correct prediction can always show a lower entropy (meaning high confidence) than the wrong predictions on all $T$ selections. Note that the entropy of predictions under $T=0.5$ is larger than the case without KD, because $T<1$ temperature will sharpen the probability distribution. 
Consequently, we propose another metric to evaluate the performance of KD, i.e., the prediction confidence that a VSA model can provide. This can be jointly considered with accuracy as a trade-off when designing the temperature $T$. {To evaluate distillation in a more general context, we further explore teacher networks with different capacities in Appendix~\ref{app:KD_diffcapacity}, and different divergence losses in LDC training in Appendix~\ref{app:JSDiv}.}

\begin{table}[t]
\centering
\caption{The confidence and accuracy of the binary VSA model trained by LDC without KD or with KD under various temperatures.}
\resizebox{.75\linewidth}{!}{
\begin{tabular}{c|ccccccc}
\toprule[2pt]
 & w/o KD & T=0.5 & T=2 & T=4 & T=8 & T=10 & T=20 \\\midrule
$\overline{H}(\sigma(\bm{z}))_T$ & 0.1689 & 0.4681 & 0.0654 & 0.0372 & 0.0268 & 0.0249 & 0.0263 \\
$\overline{H}(\sigma(\bm{z}))_F$ & 0.6209 & 1.0738 & 0.3883 & 0.2506 & 0.1976 & 0.1897 & 0.1879 \\
Acc. (\%) & 85.09 & 84.43 & 86.17 & 86.30 & 86.51 & 86.39 & 85.07 \\\bottomrule
\end{tabular}}
\label{tab:KD_comp}
\end{table}


\section{Evaluation}\label{sec:evaluation}
\textbf{Datasets.} 
We select representative datasets for binary VSA models. ISOLET \cite{isolet} and HAR \cite{ucihar} are two commonly evaluated benchmarks in previous VSA work, where ISOLET is a voice recording collection and HAR is an activity gesture collection. Besides, we evaluate two other lightweight applications, i.e., seizure detection (CHB-MIT) \cite{CHBMIT} on brain-computer interface and credit card fraud detection (CreditCard) \cite{creditcardfrauddetection} for portable devices. Also, we include FashionMNIST \cite{FashionMNIST} since it is the most challenging one in previous VSA work. A detailed description is provided in Appendix~\ref{app:evaluation}.

\textbf{Training Setup.} As basic configuration, we follow the LDC training setup \cite{ldc}. We apply batch normalization and knowledge distillation with their default setup. For KD, we still set $\gamma=0$ (without considering $\mathcal{L}_{CE}$) to emphasize the KD benefit and $T=4$ as a common choice. We employ 3-layer MLP on the first four datasets (with accuracies in order 96.54\%, 97.08\%, 99.10\%, and 94.90\%, respectively), since they are all 1-D signal samples; and use ResNet-18 for the KD on FashionMNIST (with accuracy 92.51\%). 
Other teacher networks are evaluated in Appendix~\ref{app:ensemble}.


\textbf{Model Comparison.} We compare our strategy (noted as ``LDC+BNKD'') with the vanilla LDC, and other SOTA binary VSA works \cite{yu2022understanding, lehdc_dac, imani2019quanthd}. For LDC+BNKD and LDC, we evaluate them on different vector dimensions $D$, while we keep the configuration of $\mathcal{V}(\cdot)$ fixed as suggested, i.e., 4 \cite{ldc}. For other binary VSA models, we assume the dimension $D=10,000$, as suggested in their works.

\subsection{Inference Accuracy}
We demonstrate the inference accuracy of related binary VSA works in Table~\ref{tab:acc_comparison}. The original LDC training generally has slightly worse inference performance than SOTA binary VSA works. Also, the saturation issue is also apparent on LDC, i.e., little accuracy increment by doubling the vector dimension from 256 to 512. This highlights the necessity of investigation on better VSA training strategy, rather than directly increasing vector dimensions. On the other hand, the LDC training with BN and KD assistance can obviously improve inference performance. 
While BN and KD can mitigate but not entirely eliminate the saturation issue on certain tasks (e.g., FashionMNIST), we advocate for architectural enhancements to binary VSA models to fundamentally augment their capabilities. In the breadth view, ``LDC+BNKD'' demonstrates superior accuracy on two benchmarks while maintaining a performance gap of $<1\%$ compared to the highest-performing SOTA VSA models on two other benchmarks. Therefore, the BNKD-assisted LDC training can provide an LDC model with inference performance comparable to that of SOTA VSA work. Notably, this superiority is achieved with only 1/20 vector dimensions of VSA models, i.e., $D=512$ vs. $D=10,000$.

\begin{table*}[t]
\centering
\caption{Inference accuracy comparison between SOTA VSA works, LDC, and our training strategy. We assume $D=10,000$ for binary VSA models (the first four), and vary the dimension $D=(64, 256, 512)$ for LDC and our method. Best results are in \textbf{bold}, and second bests are \underline{underlined}.}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc|ccc|ccc}
\toprule[2pt]
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Model}\\ \textbf{Accuracy (\%)}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}QuantHD\\ \cite{imani2019quanthd}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}G$(2^3)$\\ -VSA\cite{yu2022understanding}\end{tabular}}& \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}G$(2^4)$\\ -VSA\cite{yu2022understanding}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}LeHDC\\\cite{lehdc_dac}\end{tabular}} & \multicolumn{3}{c|}{LDC\cite{ldc}} & \multicolumn{3}{c}{\textbf{LDC+BNKD}} \\
 &  &  &  &  & 64 & 256 & 512 & 64 & 256 & 512 \\\midrule
\textbf{ISOLET}      & 92.70 & 94.40 & \textbf{96.00} & \underline{94.89} & 88.26 & 92.97 & 93.70 & 88.72$^{\pm0.46}$ & 93.87$^{\pm0.34}$ & 94.28$^{\pm0.28}$ \\
\textbf{HAR}         & 91.25 & 95.60 & \textbf{96.60} & 95.23 & 93.08 & 94.67 & 94.90 & 93.66$^{\pm0.54}$ & 95.20$^{\pm0.30}$ & \underline{95.64}$^{\pm0.25}$  \\
\textbf{CHB-MIT}     & 86.57 & N/A   & N/A   & 91.16 & 95.30 & 96.39 & 96.53 & 97.32$^{\pm0.65}$ & \underline{98.01}$^{\pm0.56}$ & \textbf{98.04}$^{\pm0.35}$  \\
\textbf{CreditCard}  & \textbf{94.69} & N/A   & N/A   & 93.88 & 93.37 & 93.88 & 93.88 & 94.19$^{\pm0.46}$ & 94.49$^{\pm0.23}$ & \textbf{94.69}$^{\pm0.36}$  \\
\textbf{FashionMNIST} & 80.26 & 86.70 & 87.40 & 87.11 & 83.62 & 86.66 & 86.94 & 86.48$^{\pm0.22}$ & \underline{88.38}$^{\pm0.21}$ & \textbf{88.91}$^{\pm0.10}$ \\\bottomrule
\end{tabular}}
\label{tab:acc_comparison}
\end{table*}


\begin{table}[t]
\caption{The inference accuracy on datasets by varing $\gamma$, assuming the vector dimension $D=64$ for our LDC+BNKD model. Results are averaged on 5 runs. The best results for each benchmark are marked as \textbf{bold}, and the second bests are \underline{underlined}.}
\centering
\resizebox{.95\linewidth}{!}{
\begin{tabular}{c|cccccc}
\toprule[2pt]
\textbf{Acc. (\%)} & $\gamma=$0.0 & 0.2 & 0.4 & 0.6 & 0.8 & 1.0 \\\midrule
\textbf{ISOLET} & \textbf{88.92}$^{\pm0.36}$ & 88.02$^{\pm0.41}$ & 88.08$^{\pm0.91}$ & {87.66}$^{\pm0.97}$ & 88.06$^{\pm0.52}$ & \underline{88.30}$^{\pm0.81}$ \\
\textbf{HAR} & 93.64$^{\pm0.58}$ & \underline{93.71}$^{\pm0.26}$ & 93.56$^{\pm0.24}$ & \textbf{94.01}$^{\pm0.37}$ & 93.60$^{\pm0.24}$ & {93.48}$^{\pm0.46}$ \\
\textbf{CHB-MIT} & 97.14$^{\pm0.64}$ & \textbf{97.92}$^{\pm0.42}$ & \underline{97.35}$^{\pm0.48}$ & 97.33$^{\pm0.67}$ & 97.14$^{\pm0.65}$ & {96.84}$^{\pm0.78}$ \\
\textbf{CreditFraud} & 94.08$^{\pm0.58}$ & \underline{94.39}$^{\pm0.81}$ & \textbf{94.49}$^{\pm0.67}$ & 93.98$^{\pm0.67}$ & 94.08$^{\pm0.46}$ &  {92.86}$^{\pm0.72}$\\
\textbf{FashionMNIST} & 86.30$^{\pm0.25}$ & \underline{86.37}$^{\pm0.13}$ & 86.34$^{\pm0.26}$ & \textbf{86.39}$^{\pm0.15}$ & 86.13$^{\pm0.43}$ & {85.37}$^{\pm0.38}$ \\\bottomrule
\end{tabular}}
\label{tab:gamma_app}
\end{table}

\subsection{Trade-off Between $\mathcal{L}_{CE}$ and $\mathcal{L}_{KLDiv}$}
To evaluate the trade-off between the knowledge from the ground-truth label and the advanced framework, we varies $\gamma$ in the final KD loss, $\mathcal{L}=\gamma \mathcal{L}_{CE}+(1-\gamma)\mathcal{L}_{KLDiv}$. The accuracy results are given in Table~\ref{tab:gamma_app}. The differences between various $\gamma$ are relatively low for most benchmarks (but indeed significant in some such as CreditFraud and FashionMNIST). Nevertheless, the efficacy of KD is obvious on most benchmarks; without KD, i.e., $\gamma=1$, LDC is prone to perform the worst. On the other hand, all benchmarks have the best performance when $\gamma \leq 0.6$, meaning that partly inducing $\mathcal{L}_{KLDiv}$ are likely to give an inference improvement for LDC training.

\subsection{Hardware Preparation} 
\setlength{\intextsep}{-1pt}%
\begin{wrapfigure}{r}{0.5\linewidth}
\captionof{table}{The memory footprint (in KB) and hardware latency (in CDC) for different binary VSA models, evaluated on ISOLET and FashionMNIST.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule[2pt]
 & \multicolumn{2}{c|}{\textbf{ISOLET}} & \multicolumn{2}{c}{\textbf{FashionMNIST}} \\
 & Mem. (KB) & CDC & Mem. (KB) & CDC \\\midrule
QuantHD & 1124 & 295 & 1313 & 295 \\
LeHDC & 1124 & 295 & 1313 & 295 \\
G$(2^3)$-VSA & (1058)$^\star$ & 402 & (998)$^\star$ & 405 \\
G$(2^4)$-VSA & (1410)$^\star$ & 430 & (1330)$^\star$ & 434 \\\midrule
LDC-64 & 5.27 & 73 & 6.48 & 74 \\
LDC-256 & 20.70 & 118 & 25.54 & 119 \\
LDC-512 & 41.28 & 145 & 50.94 & 146 \\\midrule
\textbf{LDC+BNKD-64} & \textbf{5.35} & \textbf{73} & \textbf{6.56} & \textbf{74} \\
\textbf{LDC+BNKD-256} & \textbf{21.02} & \textbf{118} & \textbf{25.86} & \textbf{119} \\
\textbf{LDC+BNKD-512} & \textbf{41.92} & \textbf{145} & \textbf{51.58} & \textbf{146} \\\bottomrule
\multicolumn{5}{l}{$^\star$Not given in original work, but estimated by us.}
\end{tabular}}
\label{tab:hardware_comparison}
\end{wrapfigure}
Since binary VSA is well-known for its ultra-lightweight and real-time implementation. It is necessary to estimate the memory footprint and potential hardware latency. We calculate the memory requirement of involved binary vectors, including \textbf{V}, \textbf{F}, and \textbf{C}. We also evaluate the latency by calculating model's circuit-depth complexity (CDC). Its calculation on binary VSA is discussed in \cite{yu2022understanding}. We take the ISOLET and FashionMNIST as two examples to evaluate the hardware performance in Table~\ref{tab:hardware_comparison}, and the results of other datasets are available in Appendix~\ref{app:HP_extension}. \circled{1} SOTA binary VSA models, e.g., QuantHD, G-VSA, and LeHDC have heavy-loaded hardware overhead, because they have very large vector dimensions, $D=10,000$, showing the deficiency of current high-dimension VSA models. \circled{2} In contrast, low-dimension VSA models (LDC and LDC+BNKD) have much lower memory footprint and hardware latency. They can achieve 1/20$\sim$1/260 memory usage compared to SOTA VSA models; 
yet our LDC+BNKD strategy will induce slightly larger ($1.2\%\sim 1.6\%$) memory than their LDC counterparts, because we store the thresholds derived from BN. Nevertheless, this memory increment is negligible and can be easily handled by tiny devices. Jointly, LDC+BNKD and LDC have only $17\%\sim 49\%$ hardware latency (CDC) compared to those high-dimension VSA models, depicting their real-time advantage. {Correspondingly, we also demonstrate the real inference time of different binary VSA models in Appendix~\ref{app:HP_extension}.}
Further, LDC+BNKD has the same latency as its LDC counterpart, because the BN and KD training does not introduce extra computation.

We present other evaluations of KD temperature and model robustness in Appendix~\ref{app:temp} and~\ref{app:robustness}, and further evaluate the effectiveness of BN and KD on BNNs in Appendix~\ref{app:bnn}. 


\section{Conclusion}
Low-dimensional computing (LDC) has been put forth as a promising training strategy for binary VSA, due to its ability to generate VSA with ultra low-dimensional vectors. However, the potential for LDC training remains a lack of exploration. In this paper, we offer a thorough analysis of vector optimization in LDC and highlight the indispensability of batch normalization (BN) and knowledge distillation (KD) during training. We argue that BN does not add computational overhead to hardware implementations, and that KD positively impacts confidence calibration during training. Our analysis can also be extended to general BNN, thereby helping theoretical analysis of the roles of BN and KD in BNN optimization. Further discussion is provided in Appendix~\ref{app:discussion}.



\section*{Acknowledgements}
Shijin Duan and Xiaolin Xu were supported in part by the U.S. NSF under Grants CNS-2326597, CNS-2239672, and a Cisco Research Award. Yejia Liu and Shaolei Ren were supported in part by the U.S. NSF under Grants CNS-2326598.

\input{output.bbl}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\section{Value Vector Mapping}\label{app:value_vector_mapping}
In LDC, the vector mapping of an input value is expressed as a shallow neural network, namely \textit{ValueBox}, $\mathcal{V}(\bm{x}_i):\bm{x}_i\mapsto \textbf{V}_{\bm{x}_i}$. The structure of $\mathcal{V}(\bm{x}_i)$ applied in our work is
\begin{equation*}
\begin{split}
    \bm{x}_i &\to \text{Linear(1,20)} \to  \text{BN(20)} \xrightarrow[]{tanh()} \\
    &\text{Linear(1,$D_\textbf{v}$)} \xrightarrow[]{sgn()} \textbf{v} \xrightarrow[]{\text{duplicate}} \textbf{V}_{\bm{x}_i}
\end{split}
\end{equation*}
Here, $D_\textbf{v}$ is the output dimension of the second linear layer, which is the dimension of the value vector. As indicated in the original paper \cite{ldc}, the $D_\textbf{v}$ can be unequal to the vector dimension $D$ of \textbf{F} and \textbf{C}, but only requiring $D$ is multiples of $D_\textbf{v}$. Therefore, after the network output, \textbf{v}, this binary vector is duplicated by $D/D_\textbf{v}$ times as the final output of \textit{ValueBox}, \textbf{V}$_{\bm{x}_i}$, to align with the vector dimension of \textbf{F} and \textbf{C}. This trick can save the memory footprint and results from the observation that value vectors usually do not need as large dimensions as feature and class vectors. For the $N$ input features, only one unique \textit{ValueBox} is generated. The \textit{ValueBox} later is translated to a Look-up Table, recording the binary vector outputs for all possible input values.

{\textbf{Remark: depth of $\mathcal{V}(x)$ network.} The original LDC work~\cite{ldc} explores several designs for the value mapping function $\mathcal{V}(x):x\mapsto \mathbf{V}_x$ (see Fig. 6 in~\cite{ldc}), experimentally concluding that a shallow network is sufficient to express value mapping. While the original paper does not analyze this approximation, we offer our perspective that it may be related to the ``value importance'' in classification, where $\mathcal{V}(x)$ determines the optimal segmentation of values from $1$ to $M$. Each segment corresponds to a single vector representation, and values within a segment have similar importance for classification. For instance, $\mathcal{V}(x)$ could divide the range $[0,255]$ into segments such as $\{[1,50], [51,100], [101,150], [151,200], [201,255]\}$, corresponding to vectors $\{0000,0001,0011,0111,1111\}$ for values in each segment. The optimization of segmentations is done automatically during LDC training. This mapping is relatively easy to optimize, given the characteristics of data in VSA tasks, so a shallow network can satisfy the approximation of $\mathcal{V}(x)$. Note that more complicated value distributions could require a more complex $\mathcal{V}(x)$ architecture, e.g. forcing above segments to generate vector set $\{0000,0101,0011,0110,1111\}$, deeper network may be needed to fit this projection. However, observations in~\cite{ldc} indicate that a shallow network is adequate for current VSA tasks, as value-vector projections are not as complicated as aforementioned. Following the original LDC design, we adopt a \textbf{two-layer} network in our work.}

\section{Training Details}\label{app:training_detail}
Except for the basic training setup shown in the paper, we provide other training information here. We apply Adam as the optimizer with 1e-3 initial learning rate, which is linearly decayed to zero during training. We run our experiments for 50 epochs to guarantee convergence. The gradient of \textit{latent} weights are clipped within the magnitude $\pm 1$.

For the quantization-aware training (QAT) strategy \cite{nagel2022overcoming}, we briefly show the main idea here for quick reference. QAT first defined the oscillation of a certain \textit{latent} weight $w$, in our binary training scenario, by satisfying two conditions: \circled{1} the sign of $w$ flipped between two consecutive iterations $t-1$ and $t$, i.e., $\Delta^t=\text{sgn}(w^{t}) - \text{sgn}(w^{t-1})\neq 0$. Here $\Delta^t=2$ means the sign of $w$ is changed from -1 to 1, $\Delta^t=-2$ means the opposite, and $\Delta^t=0$ represents no sign flipping. \circled{2} one oscillation is the sign flipping directions between two consecutive iterations are opposite, i.e., $o^t=(\Delta^t\neq \Delta^{t-1})\cdot (\Delta^t\times \Delta^{t-1}\neq 0)$. Thus, $o^t=1$ means there is an oscillation in iteration $t$, and 0 otherwise. The frequency of oscillations over iterations is tracked using the exponential moving average (EMA):
\begin{equation*}
    f^t=m\cdot o^t + (1-m)\cdot f^{t-1}
\end{equation*}
For the iterative weight freezing on QAT, we define a threshold $f_{th}$ that once the oscillation frequency of a certain weight exceeds $f_{th}$, this weight will be frozen and not updated for the rest of training. To minimize the difference in computations between forward and backward \cite{liu2021adam}, we set the frozen state of a \textit{latent} weight $w_z$ as $w_z = \text{sgn}(w)$. Therefore, the frozen \textit{latent} weights will be equal to their signed results. Note that the scaling factor $\alpha$ computing will exclude the frozen weights, i.e., we only calculate the $l_1$-norm mean of active \textit{latency} weights. In our experiments, we empirically set $m=0.01$ and $f_{th}=0.02$. And in order to make QAT effective when LDC and LDC+BNKD tend to converge, we activate QAT at epoch 15 until the end.

{\textbf{Remark: advantage of LDC training over post-training quantization (PTQ).} LDC training on VSA~\cite{ldc} is a QAT strategy, which generally outperforms PTQ in accuracy, albeit being more compute-intensive. However, thanks to the lightweight nature of the LDC model with a kilobyte-sized architecture, QAT training is highly efficient, taking only about 5 minutes on an NVIDIA 3070 GPU, as measured in our experiments. Regarding the similar training procedure in BNN, QAT is also widely adopted~\cite{qin2020binary} for its ability to effectively handle the highly discrete nature of binarization, which is more extreme than integer quantization (e.g., 8-bit).}


\section{Trainable Gradient Magnitude} \label{app:BN}
We prove why the parameter $w_{BN}$ in batch normalization is optimized on both forward and backward information. Therefore, $w_{BN}$ has more adaptive adjustment on the encoding accumulations. Recall that the LDC model with BN can be expressed as, starting from the accumulation result $\bm{y}$,
\begin{equation*}
    \bm{z} = \textbf{C}\times\text{sgn}\left(\frac{\bm{y}-\text{E}(\bm{y})}{\sqrt{\text{Var}(\bm{y})+\epsilon}}\times w_{BN} + b_{BN}\right)
\end{equation*}
We still assume the LDC model is trained on softmax activation and cross-entropy loss, then after feeding in one query sample, the derivative of the $d$-th element $w_{BN,d}$ w.r.t. $\mathcal{L}_{CE}$ is
\begin{equation*}
\begin{split}
    &\frac{\partial\mathcal{L}_{CE}}{\partial w_{BN,d}} =\frac{\partial\mathcal{L}_{CE}}{\partial \bm{s}_d} \frac{\partial \bm{s}_d}{\partial w_{BN,d}} \\
    & \text{where } \frac{\partial\mathcal{L}_{CE}}{\partial \bm{s}_d} = -\left[\sum_k \textbf{C}^r_{k,d}(\bm{t}_k-\sigma(\bm{z})_k)\right], \\ & \frac{\partial \bm{s}_d}{\partial w_{BN,d}} =
    \frac{\bm{y}_d-\text{E}(\bm{y}_d)}{\sqrt{\text{Var}(\bm{y}_d)+\epsilon}} \cdot \left [1(|\text{BN}(\bm{y}_d)|\leq 1) +0(|\text{BN}(\bm{y}_d)|> 1)\right ].
\end{split}
\end{equation*}


\begin{figure}[t]
\centering
\includegraphics[width=0.5\linewidth]{FIG/C_dist.pdf}
\caption{{The \textbf{C}$^r$ distribution along 50 epochs.}}
\label{fig:C_dist}
\end{figure}
Here we boldly assume the discussed optimizing surface of $w_{BN,d}$ is not constant,
i.e., $|BN(\bm{y})|\leq 1$; otherwise the gradient is always zero, disabling updating. 
The term in $\partial \mathcal{L}_{CE}/\partial w_{BN,d}$ related to $w_{BN,d}$ is $\sum_k \textbf{C}^r_{k,d} \sigma(\bm{z})_k$; this term is not monotonous to $w_{BN,d}$ since \textbf{C}$^r_{:,d}$ are not constant and even does not has the same sign, as shown in Figure~\ref{fig:C_dist}. This conclusion can be also described as the weighted sum of softmax members is not monotonous. Therefore, the $w_{BN,d}$ optimization is not convex, and only the local optimum can be derived. Besides, since the non-linearity of softmax, there is no analytical solution for the equation $\partial \mathcal{L}_{CE}/\partial w_{BN,d}=0$. Nevertheless, if numerically solving this equation, it can be equivalent to $\textbf{C}^r_{l,d}=\sum_k \textbf{C}^r_{k,d} \sigma(\bm{z})_k$ assuming the true label of this query sample is $l$. From this analysis, we can derive that the local optimum of $w_{BN,d}$ highly relies on the \textit{latent} weight \textbf{C}$^r$. This gives a valuable perspective that the BN can be well adjusted from its followed layers through backward propagation, while the normal regularization E$(\bm{y})$ and Var$(\bm{y})$ can only statistically reshape the accumulation based on feedforward information. 



\section{Supplementary of Evaluation Setup}\label{app:evaluation}
\begin{table*}[h]
\centering
\caption{Configurations of the evaluated datasets.}
\resizebox{\linewidth}{!}{
\begin{tabular}{r|ccccc}
\toprule[2pt]
Dataset & ISOLET & HAR & CHB-MIT & CreditCard & FashionMNIST \\
\midrule
\# of features ($N$) & 617 & 561 & 1472 & 29 & 784 \\
\# of (train, test, class) & (6238, 1559, 26) & (7352, 2947, 6) & (13920, 664, 2) & (3940, 196, 2) & (60000, 10000, 10) \\\bottomrule
\end{tabular}}
\label{tab:dataset_sup}
\end{table*}
We provide the basic information of our selected benchmarks in Table~\ref{tab:dataset_sup}. Since LDC models (and even current high-dimension VSA models) have limited capacity, due to their straightforward and simple calculation, they are specifically proposed for those tasks that are not difficult to classify but still require rather limited hardware resourcesand real-time response. For example, the classification of segmented signals and simple images on portable devices is favorable to LDC. Therefore, we select the four representative datasets covering different lightweight scenarios. For image classification, binary VSA models so far can only handle easy image recognition with just passable accuracy, such as MNIST\cite{mnist} and FashionMNIST\cite{FashionMNIST}, thus we did not include complex tasks, such as CIFAR\cite{cifar10} or ImageNet\cite{krizhevsky2012imagenet} in our evaluation.




\section{Knowledge Distillation under Ensemble Models}\label{app:ensemble}

\begin{table}[h]
\centering
\caption{The inference accuracy of benchmarks by training LDC with ensemble-model KD, assuming the vector dimension $D=64$ for LDC+BNKD and the size of ensemble is $G=10$. Results are averaged on 5 runs.}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{c|ccccc}
\toprule[2pt]
Acc. (\%) & ISOLET & HAR & CHB-MIT & CreditFraud & FashionMNIST \\ \midrule
$D=64$ & $88.05^{\pm0.49}$ & $93.51^{\pm0.22}$ & $96.57^{\pm0.58}$ & $92.04^{\pm1.47}$ & $84.32^{\pm0.20}$ \\
$256$ & $93.14^{\pm0.44}$ & $95.27^{\pm0.20}$ & $98.77^{\pm0.29}$ & $92.76^{\pm0.67}$ & $87.75^{\pm0.12}$ \\
$512$ & $94.41^{\pm0.05}$ & $95.74^{\pm0.21}$ & $98.73^{\pm0.17}$ & $93.37^{\pm0.36}$ & $88.62^{\pm0.13}$ \\\bottomrule
\end{tabular}}
\label{tab:en_app}
\end{table}
\vspace{5pt}

There are many knowledge distillation strategies that have been discussed \cite{KnowledgeDistillationSurvey2021}. As a consensus, the teacher and the student networks have the similar architecture, so that the transferred knowledge can be well mimicked by the student. However, there is currently no developed model akin to the LDC. Thus, we also try to use the ensemble of several LDC models as the teacher network, so the teacher network can perform better than a single LDC model and has a similar classification boundary of which the LDC training is capable as well. As a general approach for ensemble knowledge distillation, the \textit{average} rule is mostly applied in development. Specifically, the ensemble categorical probability we utilized for $k$-th class is
\begin{equation}
    \sigma\left(\bm{z}_t\right)_k =  \frac{1}{G}\cdot \sum_{g=1}^G \sigma\left(\bm{z}_t^g\right)_k
\end{equation}
assuming there are $G$ teacher models. Since the ensemble strategy averages the prediction, the bias and noise on boundaries during training are mitigated, so that the ensemble boundary is more of generalization. We present the LDC+BNKD training results in Table~\ref{tab:en_app}, which includes 10 LDC models as the teachers. Compared with the result we demonstrated in Table~\ref{tab:acc_comparison} in the main paper, neither an ensemble model nor an advanced network consistently outperforms the other as the teacher model. The two strategies have similar performance on ISOLET and HAR; however, the ensemble strategy has a slight advantage on the CHB-MIT dataset while the advanced-network strategy has obvious superiority on CreditFraud and FashionMNIST datasets. Especially for FashionMNIST, low dimensional models, e.g., $D=64$, the real-valued advanced network has a significant advantage (as large as 2\% accuracy gap) over the ensemble model.


\section{Supplementary on Evaluation}

\subsection{{Batch Normalization Performance under Different Batch Size}}\label{app:diff_bsz_BN}

{Since BN normalizes across data batches, it is important to evaluate the accuracy of LDC+BN under various batch sizes, exploring whether the normalization strategy can be applied to different training configurations. Table~\ref{tab:diff_batch_BN} presents the results of applying LDC training with $D=64$ on FashionMNIST. }
\setlength{\intextsep}{2pt}%
\begin{wrapfigure}{r}{0.4\linewidth}
\captionof{table}{{The inference accuracy of LDC+BN under different batch sizes. Results are averaged on 5 runs.}}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc}
\toprule[2pt]
\textbf{}      & {32} & {64} & {128} & {256} \\ \midrule
\textbf{LDC+BN} & 85.28& 85.52& 85.04& 85.79\\ \bottomrule
\end{tabular}}
\label{tab:diff_batch_BN}
\end{wrapfigure}
{Across varying batch sizes, the accuracy remains consistent with minor fluctuations, i.e., there is no degradation on small batches. This suggests that batch size has minimal influence on normalization during LDC training. This behavior highlights the efficiency and flexibility of BN in LDC, as it maintains performance consistently across different training configurations.}

\subsection{{Distillation Performance under Different Capacity Mismatch}}\label{app:KD_diffcapacity}
\begin{table}[h!]
\captionof{table}{{The inference accuracy of LDC+KD with different teacher networks, with the accuracy of each teacher network provided in parentheses.}}
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{l|ccccc}
\toprule[2pt]
\textbf{}     & $-$ & MLP(89.21) & AlexNet(92.14) & ResNet18(92.51) & ResNet50(92.65) \\ \midrule
{LDC+KD} & 83.62 & 83.89 & 85.41 & 86.30 & 86.13\\ \bottomrule
\end{tabular}}
\label{tab:diff_teacher_KD}
\end{table}
{Since LDC distillation relies on learning knowledge from the teacher network, it is important to investigate how the capacity of teacher networks affects distillation performance. Table~\ref{tab:diff_teacher_KD} presents a comparison of results from teacher networks with varying capacities using the FashionMNIST dataset, taking vector dimension $D=64$ and distillation temperature $T=4$ as an example. The results show that the complexity and capacity of the teacher network significantly influence LDC+KD performance. As the teacher becomes more advanced (e.g., from MLP to ResNet18), it captures task knowledge more effectively, improving the accuracy of LDC+KD. However, overly large teacher networks, such as ResNet50, provide little additional benefit compared to ResNet18, and LDC struggles to extract more useful knowledge from them. Therefore, selecting a teacher network requires balancing model capacity with training effort to optimize both efficiency and effectiveness.}


\subsection{{Distillation Performance under Other Divergence Loss}}\label{app:JSDiv}
\begin{table}[h!]
\centering
\caption{{The inference accuracy of the binary VSA model trained by LDC under KLDiv or JSDiv~\cite{menendez1997jensen} loss, with various temperatures.}}
\resizebox{.55\linewidth}{!}{
\begin{tabular}{c|cccccc}
\toprule[2pt]
 & $T=0.5$ & $2$ & $4$ & $8$ & $10$ & $20$ \\\midrule
KLDiv & 84.43 & 86.17 & 86.30 & 86.51 & 86.39 & 85.07 \\
JSDiv & 85.03 & 85.40 & 86.05 & 85.23 & 85.59 & 84.26 \\\bottomrule
\end{tabular}}
\label{tab:JSDiv}
\end{table}

{Due to the significant capacity mismatch between low-dimensional VSA models and the teacher networks, KL divergence (KLDiv) may lead to "mode-seeking" during distillation~\cite{huszar2015not, agarwal2024policy}. To address this, we investigate whether a more generalized divergence metric, such as Jensen-Shannon divergence (JSDiv), could mitigate this issue. Table~\ref{tab:JSDiv} compares the performance of KLDiv and JSDiv under different temperatures. Our results show that KLDiv and JSDiv perform similarly in LDC training, but when $T > 1$, KLDiv achieves slightly better results despite JSDiv being the more generalized metric. This may be because (i) VSA tasks are less complex than modern vision or language tasks, making the mode distribution simpler and allowing KLDiv to capture dominant modes effectively, which the student network can learn. (ii) Less significant modes captured by the teacher network might be ignored by the student, enabling the student to focus on dominant modes, improving performance with KLDiv. This observation aligns with our discussion in Section~\ref{sec:KD_analysis}, where KD encourages the student to prioritize easier-to-classify samples while de-emphasizing harder ones.}


\subsection{Performance of BNKD under Different Distillation Temperatures}\label{app:temp}
\begin{table}[h]
\caption{The inference accuracy on datasets by varying temperature T in KD, assuming the vector dimension $D=64$ for LDC+BNKD. Results are averaged on 5 runs.}
\centering
\resizebox{.9\linewidth}{!}{
\begin{tabular}{c|cccccc}
\toprule[2pt]
Acc. (\%) & $T=$0.5 & 2 & 4 & 8 & 10 & 20 \\ \midrule
ISOLET & $87.44^{\pm0.45}$ & $88.34^{\pm0.42}$ & $\bm{88.65}^{\pm0.77}$ & $87.11^{\pm0.62}$ & $86.79^{\pm0.92}$ & $84.84^{\pm0.62}$ \\
HAR & $93.24^{\pm0.52}$ & $93.67^{\pm0.29}$ & $\bm{93.84}^{\pm0.27}$ & $93.46^{\pm0.18}$ & $93.25^{\pm0.25}$ & $93.23^{\pm0.26}$ \\
CHB-MIT & $96.84^{\pm0.83}$ & $97.14^{\pm0.60}$ & $97.05^{\pm0.64}$ & $\bm{97.68}^{\pm0.81}$ & $96.93^{\pm0.78}$ & $96.51^{\pm0.50}$ \\
CreditFraud & $93.47^{\pm0.91}$ & $93.78^{\pm0.67}$ & $\bm{94.39}^{\pm0.43}$ & $93.16^{\pm0.93}$ & $93.98^{\pm0.56}$ & $94.08^{\pm0.46}$ \\
FashionMNIST & $84.43^{\pm0.44}$ & $86.17^{\pm0.30}$ & $86.30^{\pm0.34}$ & $\bm{86.51}^{\pm0.18}$ & $ 86.39^{\pm0.22}$ & $85.07^{\pm0.19}$ \\\bottomrule
\end{tabular}}
\label{tab:T_app}
\end{table}

We further demonstrate the temperature influence on the LDC+BNKD model for other benchmarks, which is shown in Table~\ref{tab:T_app}. While temperature could have a significant impact on the accuracy (e.g., on ISOLET), empirical temperatures, such as $T=4$, indeed can perform best in most cases. On the other hand, if the temperature falls outside the common range $T\in[1,10]$ that is suggested in previous works, the accuracy usually is compromised a little. While current KD works commonly use $T=4$ as a default choice, there is no theoretical analysis on temperature optimization, which could be a potential topic, especially for binary neural networks.

\subsection{Hardware Preliminaries for Other Datasets} \label{app:HP_extension}
\begin{table}[h]
\centering
\caption{The memory footprint (in KB) and hardware latency (in CDC) for different binary VSA models, evaluated on HAR, CHB-MIT, and CreditCard. ``ours'' refers to the ``LDC+BNKD''.}
\resizebox{.6\linewidth}{!}{
\begin{tabular}{l|cc|cc|cc}
\toprule[2pt]
 & \multicolumn{2}{c|}{HAR} & \multicolumn{2}{c|}{CHB-MIT} & \multicolumn{2}{c}{CreditCard} \\
 & Mem. (KB) & CDC & Mem. (KB) & CDC & Mem. (KB) & CDC \\\midrule
QuantHD & 1029 & 295 & 2163 & 296 & 359 & 291\\
LeHDC & 1029 & 295 & 2163 & 296 & 359 & 291\\
G$(2^3)$-VSA & (983)$^\star$ & 401 & (968)$^\star$ & 414 &(968)$^\star$ &363\\
G$(2^4)$-VSA & (1310)$^\star$ & 428 & (1290)$^\star$ & 445  &(1290)$^\star$ &377\\
LDC-64 & 4.66 & 73 & 11.92 & 75 & 0.38 & 69 \\
LDC-256 & 18.27 & 118 & 47.30 & 120 & 1.12 & 134 \\
LDC-512 & 36.42 & 145 & 94.46 & 147 & 2.11 & 141 \\
\textbf{ours-64} & \textbf{4.74} & \textbf{73} & \textbf{12.01} & \textbf{75}  & \textbf{0.42} & \textbf{69}\\
\textbf{ours-256} & \textbf{18.59} & \textbf{118} & \textbf{47.65} & \textbf{120}  & \textbf{1.28} & \textbf{134}\\
\textbf{ours-512} & \textbf{37.06} & \textbf{145} & \textbf{95.17} & \textbf{147}  & \textbf{2.43} & \textbf{141}\\\bottomrule
\multicolumn{5}{l}{$^\star$Not given in original work, but estimated by us.}
\end{tabular}}
\label{tab:hardware_comparison_app}
\end{table}

Besides the ISOLET and FashionMNIST, we further demonstrate the hardware overhead of other datasets in our paper, in Table~\ref{tab:hardware_comparison_app}. The hardware performance results are akin to the observation we get from the main paper. Besides, there is another interesting observation that, compared with other high-dimension VSA models (QuantHD and LeHDC), G-VSA seems to occupy more memory when the input samples have a small number of features, such as CreditCard ($N=29$), while it has less memory footprint when the dataset has a large number of features, such as CHB-MIT ($N=1472$). This is because G-VSA does not generate feature vector set \textbf{F} but uses permutations to encode value vectors \textbf{V}. Nevertheless, LDC and LDC+BNKD models still have overwhelming advantages on the hardware overhead against other VSA models.

{Further, we exemplarily report the actual inference time of different binary VSA models on ISOLET and FashionMNIST dataset, corresponding to CDC results. The actual inference time is shown in Table~\ref{tab:actual_time}, where the data input has a batch size of 100, with vector dimensions of 10,000 for (QuantHD, LeHDC, G-VSA) and 64 for (LDC, LDC+BNKD). Similar to the results in Table~\ref{tab:hardware_comparison}, low-dimensional VSA models (LDC, LDC+BNKD) significantly outperform high-dimensional VSA models. G-VSA requires more time than QuantHD and LeHDC due to its multi-bit vector computations, whereas QuantHD and LeHDC rely on binary vectors. Among the G-VSA models, G($2^3$)-VSA and G($2^4$)-VSA exhibit similar runtimes, despite the latter using more bits, thanks to optimized integer kernels on hardware. Additionally, GPU execution is considerably faster than CPU execution, owing to parallel processing and superior computational capabilities.}

\begin{table}[t]
\centering
\caption{{Actual inference time comparison of different VSA models, on GPU and CPU devices. CPU tests are run on a 3.80GHz 16-core Intel i7-10700K, and GPU tests are run on an NVIDIA 3070 GPU. Time is measured in microseconds.}}
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|cccccc}
\toprule[2pt]
&\textbf{Latency} ($\mu$s)&\begin{tabular}{c}{QuantHD,}\\ {LeHDC}
\end{tabular}&{G($2^3$)-VSA}&{G($2^4$)-VSA}&\begin{tabular}{c}{LDC-64,}\\ {LDC+BNKD-64}
\end{tabular}&\begin{tabular}{c}{LDC-256,}\\ {LDC+BNKD-256}
\end{tabular}&\begin{tabular}{c}{LDC-512,}\\ {LDC+BNKD-512}
\end{tabular}\\ \midrule
\multirow{2}{*}{\rotatebox{90}{\textbf{GPU}}}&ISOLET&43.6&63.8&63.0&0.18&0.61&1.36\\
&FashionMNIST&55.0&77.7&77.8&0.22&0.78&1.74\\ \midrule
\multirow{2}{*}{\rotatebox{90}{\textbf{CPU}}}&ISOLET&102.7&830.3&825.6&4.6&18.2&33.9\\
&FashionMNIST&131.3&975.2&994.6&5.8&23.7&44.7\\ \bottomrule
\end{tabular}}
\label{tab:actual_time}
\end{table}



\subsection{Robustness Evaluation}\label{app:robustness}
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\linewidth]{FIG/robustness.pdf}
    \caption{Bit-error robustness for selected VSA models, tested on FashionMNIST. We compare the LDC and LDC+BNKD models with the LeHDC model.}
    \label{fig:robustness_app}
\end{figure}

The robustness of VSA models is of paramount importance. As vector dimension decreases, the models may become increasingly sensitive to bit errors, potentially impacting their performance. We investigate the bit error robustness in Figure~\ref{fig:robustness_app}. The results show that while high-dimensional VSA models indeed demonstrate excellent robustness against bit error, low-dimensional LDC models are less robust. However, our LDC+BNKD training method interestingly shows great robustness. Nevertheless, since hardware in practice will have a much lower bit-error rate than we tested, e.g., $<10^{-6}$\cite{mielke2008bit}, even the vanilla LDC is still robust against normal bit errors \cite{sridharan2015memory}.

\subsection{BN and KD on Binary Neural Networks}\label{app:bnn}
\begin{table}[h]
\centering
\caption{The accuracy (\%) of different models on FashionMNIST. The evaluation is conducted on their real model, binarized model, and the BN/KD assisted versions.}
\begin{tabular}{c|ccccc}
\toprule[2pt]
 & Real & Binary & Binary+BN & Binary+KD & Binary+BNKD \\\midrule
MLP & 90.29 & 87.20 & 88.45 & 88.39 & 89.12 \\
CNN & 90.47 & 83.67 & 84.59 & 85.53 & 86.14 \\
ResNet-18 & 92.15 & 91.31 & 91.96 & 91.93 & 92.25 \\\bottomrule
\end{tabular}
\label{tab:BNN}
\end{table}
\vspace{5pt}

We apply the BN and KD strategy to a 3-layer binary MLP, a shallow binary CNN (which has 3-layer convolution followed by a 2-layer classifier), and the binarized ResNet-18 on the FashionMNIST dataset, as a quick validation of interest. The results are presented in Table~\ref{tab:BNN}. We draw the same conclusion, as from the binary VSA training, that both BN and KD are beneficial for BNN optimization. However, there are also unique observations on each BNN. For example, the binarized CNN performs much worse than the real-valued counterpart, because we change the activation from ReLU to sgn$()$, which loses much information. On the other hand, we keep ReLU as activation in binary ResNet-18, and the trained BNN can almost achieve the same performance as its real-valued counterpart.

\section{Discussion}\label{app:discussion}
By generalizing our interpretation to other BNN networks, we evaluate the BN and KD assistance on BNN model in Appendix~\ref{app:bnn}, and address the following discussion: 

\textbf{Importance of $b_{BN}$.} $b_{BN}$ shows little help during LDC training since the LDC model is centrosymmetric w.r.t. zero. However, for general BNN models, other asymmetric activations are involved, such as PReLU \cite{liu2020reactnet}. $b_{BN}$ will potentially play an important role in these cases, thus BN is still necessary For BNNs, and its lightweight expression as a threshold (Eq.\ref{eq:BN2Threshold}) is still preserved. 

\textbf{Temperature in Knowledge Distillation.} For the KD setup, the choice of temperature $T$ is critical. Current works only evaluate the performance variation along different $T$ choices. However, the $T$ selection should be determined in advance based on the network architectures and classification tasks or adaptive during training. Since the large capacity difference between a well-performed teacher network and a BNN, it is also a potential effort to explore individual temperatures on each network. As an extension of our analysis, these could be the future effort on the KD for BNN training.

\textbf{Comparison and Relationship between LDC, VSA, and BNN.} Before LDC, VSA training is heuristic and involves random generation and iterative updates, resulting in high vector dimensionality. LDC proposes to train VSA in a BNN-like manner, optimizing binary vectors to significantly reduce dimensionality.  (i) LDC is the first gradient-based method for VSA training, but its optimization remains basic and underexplored. This is the main motivation of our work to investigate the stable and adaptive dynamics in LDC training.  (ii) Unlike BNNs, LDC is a partial-BNN architecture where the value mapping $\mathcal{V}(x)$ is real-valued and applied to individual values. This distinction makes LDC training different from typical BNNs, which often have a real-valued first layer accepting all features in input data and different activations. However, since the updates for $\mathbf{F}$ and $\mathbf{C}$ in LDC resemble BNN binary weight training, our analysis of LDC with BNKD can also be extended to BNN training.

\end{document}
