\section{Related Works}
\label{sec:related_works}
\paragraph{Concept bottlenecks.}
Concept bottleneck~\citep{koh20concept,yang2023language,oikarinen2023label} is an approach used to create interpretable-yet-powerful classifiers. 
The key idea is to train a deep model to predict a set of low-level concepts or bottlenecks and then learn a linear classifier. 
Such concept bottlenecks have the basis of methods in several areas such as fine-grained recognition \citep{ferrari2007learning, huang2016part,zhou2018interpretable,tang2020revisiting} and zero-shot learning ~\citep{lampert-13,akata-15,kodirov-17}.
However in order to train these models, expensive data is needed to be collected for the bottleneck concepts themselves. 
One way to reduce this annotation cost is to sequentially ask questions in an information-theoretically optimized way ~\citep{chattopadhyay2024bootstrapping, chattopadhyay2024information}.
Researchers have also automated this pipeline by using large-language models as a knowledge base to propose concept bottleneck models~\citep{menon2022visual,pratt2023does, han2023llms}. 
\cite{llmmutate-24} proposed an evolutionary algorithm with LLMs as the mutation operation to discover interpretable concept bottleneck models without prior information. While these models are interpretable, they are very simple in terms of expressive power.  
In this work, we instead evolve more expressive programs than a bag of words, while being interpretable.

\paragraph{Symbolic regression.}
Symbolic regression (SR)~\citep{cranmer2023interpretable} is a technique for learning equations through evolutionary search.
Several methods have been proposed to improve the search efficiency~\citep{makke2024interpretable}, however, most SR techniques cannot solve problems beyond simple mathematical formulas, with simple mathematical primitives.
This is partly because the search space of solutions is combinatorially too large. 
As a result, SR methods fail to work for images, which are too high-dimensional.
Our method instead focuses on problems with high-dimensional visual data, by leveraging visual foundation models as primitives.
This results in models that are better performing while being interpretable.
Like our approach, recent work on SR~\citep{grayeli2024symbolic, shojaee2024llm,merler2024context,li2024automated} has also looked at using LLMs to better guide the search.
However, these methods are only tested on lower-dimension mathematical problems for formula discovery, with a limited set of primitives. 
We instead propose an approach that is complementary to these methods.
Methods for SR cannot be applied directly in higher-dimensional open-world visual problems, on the other hand on low-dimensional problems existing tools for SR~\citep{grayeli2024symbolic} would perform better than~\disciple.
The focus of this work is on such real-world problems, where the primitive functions are more complex than mathematical operations and can even be open-world, for example, a text-to-image segmentation. 

\paragraph{Neuro-Symbolic Program Learning~\citep{mao2019neuro,dongneural}}is another avenue for learning programs for observation datasets or question answering. 
These methods typically try to learn both discrete program structures together with neural networks. 
However, since this optimization is non-differentiable these methods require reinforcement learning~\citep{johnson2017inferring} or complex non-differentiable optimization techniques ~\citep{ellis2021dreamcoder}.
The hard optimization issue makes the problem of learning programs sample inefficient in real-world settings.
We alternatively use LLMs ability to program to better guide the search for such programs.

\paragraph{Program synthesis with LLMs.}
Several works have utilized LLM coding ability in different applications such as VQA~\citep{suris-23, gupta2022visual} and robot manipulation~\citep{liang2023code}. 
While the zero-shot inferred code work very well on domains well-known to the internet, they tend to perform poorly on problems in scientific domains, as shown by our results. 

\paragraph{Scientific applications.}
Researchers in numerous scientific domains have used machine learning tools to build predictive models for their quantities of interest. 
In this work, we focus on two such scientific domain of: demography and climate science.
For both these domains, we use remote sensing vision language foundation models as powerful primitives, along with mathematical, logical and image operators.
In demography, we focus on the problems of socioeconomic indicator prediction~\citep{yong-24}, namely population density and poverty estimation~\citep{metzger-24,xie2017mapping}.
Similarly in climate science we focus on the problem of aboveground biomass prediction (AGB)~\citep{nathaniel2023above}.
