\section{Methodology}
\label{sec:method}
Our key contribution is a program search framework that leverages LLMs to perform evolutionary search. In \cref{ssec:formulation} we formalize the problem of program search.
In \cref{ssec:evolution} we present our method of incorporating LLMs in the evolutionary search framework. 
Finally, in \cref{ssec:setpred}, \ref{ssec:critic} and \ref{ssec:simplification}, we discuss the improvements to this framework to speed-up the search.

\begin{figure*}[ht]
    \centering
\includegraphics[width=\linewidth]{figures/method.pdf}
\caption{Overview of our evolutionary algorithm with \textit{critic} and \textit{simplification}. We start with an initialized bank of program trying to solve a task. From this bank we sample pairs of programs based on their fitness score and perform crossover/mutations over them to produce new programs. The generated program is further improved by passing it through a critic and then an analytical simplification step. This program is then evaluated and put in the next generation of program bank. The evaluation score of the program is used to determine the fitness for the next generation of evolution.}
\label{fig:pipeline}
\end{figure*}


\subsection{Problem Formulation}\label{ssec:formulation}
Our system receives as input a \textbf{dataset} $\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}$ consisting of inputs $x_i \in X$ and quantities of interest $y_i \in Y$.
For example, when estimating geospatial indicators like poverty, $x_i$ may be a latitude and longitude, along with other metadata about that location.
The system must produce an \emph{interpretable program} $P: X \rightarrow Y$ that maps inputs to corresponding outputs.
As is typical with standard supervised learning, we also assume a loss function or \textbf{metric} $\mathcal{M}$ that measures how good a particular prediction is, and seek a program that yields the best evaluation score:
\begin{equation}
\label{eq:score}
s(P; \mathcal{D}) = \frac{1}{n}\sum_{i=1}^{n}\mathcal{M} (P(x_i), y_i)    
\end{equation}
For example, in the case of population density a good metric used by domain experts is L2 error over log \textit{i.e.} $\mathcal{M}(y', y) = ||log(y')-log(y)||_2$~\citep{metzger-22,metzger-24}.



For our programs to be interpretable, they must put together modules or \textbf{primitives} in an interpretable way.
We conceptualize these primitives as a library of functions $\mathcal{F} = \{f_1, f_2 \dots f_k\}$, that can be used to construct a program $P$.
Given that we are analyzing visual data, a key primitive will be an open-vocabulary recognition model that is applied to any imagery associated with a data point.
For example, when estimating poverty for a location, we can define a primitive that 
queries the satellite images available at that location and uses an open-vocabulary recognition engine such as GRAFT~\cite{mall2023graft} to detect/segment various concepts.
Recent advances in recognition have produced such foundation models for a range of modalities~\cite{mall2023graft, stevens2024bioclip}. 
In addition, we will assume basic mathematical, logical and image operators such as
logarithms, elementwise maximum, or a distance transform.
We note that the set of primitives is often not specific to the task and can be shared across a range of problems in a domain.
That said, the set of primitives can be expanded upon if needed by domain experts for particular problems; for example, a climate scientist may want to include a function that can look up the average temperature at a particular place and time.


Finally, to enable us to search the space of programs effectively and leverage the conceptual understanding of LLMs, we assume that we have a natural language name or description $descr$ of the quantity of interest $Y$. For example, this may be the phrase ``population density". 
As we will see below, this information will be useful in guiding the LLM to search the space well.

Putting everything together, our proposed system, \disciple~(\textbf{Di}scovering \textbf{Sci}entific \textbf{P}rograms  using \textbf{L}LMs
and \textbf{E}volution), takes as input the dataset $\mathcal{D}$, the metric $\mathcal{M}$, the set of primitives $\mathcal{F}$ and the textual description $descr$. It produces an interpretable and accurate program $P$ that maps inputs $X$ to the output quantity of interest $Y$.

We next describe our proposed system.







\begin{algorithm}[t!]
\caption{DiSciPLE's learning loop}
\label{alg:algorithm}

\DontPrintSemicolon %

\KwIn{Observation set $\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}$, metric $\mathcal{M}$, an objective prompt $p_o$, a set of primitive function $\mathcal{F} = \{f_1, f_2 \dots f_k\}$} 
\Parameter{Mutation probability $\rho_m$, total number of generations $T$, population size $M$, crossover $p_c$ and and mutation $p_m$ prompts.}
\KwOut{A program $P^*$ in the form of a program that explains the observations best.} %


$B^0 \gets \{\}$ \tcp{Initialize a programs bank} 

\For{$i = 1,\ldots,M$}{
    $P^0_i \gets \mathcal{LLM}(p_o)$
   
    $B^0 \leftarrow B^0 \; \cup \; \{ P^0_i \}$ 
}
$P^*\gets P^0_i$ 


\tcp{Evolution loop} 

\For{$t = 0,\ldots, T$}{
    $B^{t+1} \gets \{\}$ \;
    \For{$i = 1,\ldots,M$}{
        $P^t_{k_1}, P^t_{k_2} \gets$ \textrm{sample\_parents}$(B_t)$ \tcp{Sample parents for crossover}

        $P^{t+1}_{i} \gets \mathcal{LLM}(P^t_{k_1}, P^t_{k_2}, s(P^t_{k_1}; \mathcal{D}), s(P^t_{k_2}; \mathcal{D}), p_o, p_c)$         \tcp{crossover operation}

        \If{$u \sim \mathcal{U}(0, 1) < \rho_m$}{
            $P_i^{t+1}  \gets \mathcal{LLM}(P_k^{t+1}, s(P_k^{t+1}; \mathcal{D}), p_o, p_m)$ \tcp{mutation operation}
        } 

        \tcp{critique and simplification}
        $P_i^{t+1} \gets$\textrm{critic}$(P_i^{t+1})$
        $P_i^{t+1} \gets$\textrm{simplifier}$(P_i^{t+1})$

        \If{$s(P_i^{t+1}; \mathcal{D}) > s(P^*; \mathcal{D})$}{
            $P^* \gets P_i^{t+1}$
        }
        $B^{t+1} \leftarrow B^{t+1} \; \cup \; \{ P^{t+1}_i \}$ 
    }
}

\Return $P^*$;

\end{algorithm}


\subsection{Evolutionary Search for Programs}\label{ssec:evolution}
To search through the vast, discrete space of programs, \disciple~adapts evolutionary search.
Evolutionary program search typically starts with a large population of random programs.
These programs are then sampled based on their fitness as parents.
The parent programs create new programs through crossover and mutation, resulting in a new population.
Newer generations improve over the previous as the population is getting optimized for the fitness function.
We use the metric $\mathcal{M}$ as the fitness function in our work.

We keep the overall evolutionary algorithm the same but replace key steps with an LLM.
First, at the start of the process, we provide the LLM with a prompt for the objective to generate the initial programs.
To leverage the prior knowledge of LLMs, we use a prompt $p_o$ that mentions the specified description of the quantity of interest:  ``\emph{Given a satellite image, write a function to estimate $\langle descr \rangle$}''. 
We do not expect the LLM to answer such a difficult scientific question without leveraging the observations $\mathcal{D}$; however, the prompt prevents the evolutionary algorithm from searching in completely random directions. 
As a result, our initial population is not entirely random.

Second, rather than using the symbolic methods of crossover and mutation, we use the LLM to perform these operations.
LLMs have common sense about programming and result in much better program modifications when performing crossover and mutations. 
More specifically, let $P^t_{k_1}$ and $P^t_{k_2}$ be two programs sampled from the $t^{th}$ generation selected as parents based on the fitness function. 
To perform a crossover operation we pass, the objective prompt $p_o$, the two programs $P^t_{k_1}$ and $P^t_{k_2}$, their corresponding scores (using \cref{eq:score}), along with a crossover prompt $p_c$ to obtain a new program:
\begin{equation}
P^{t+1}_{k} = \mathcal{LLM}(P^t_{k_1}, P^t_{k_2}, s(P^t_{k_1}; \mathcal{D}), s(P^t_{k_2}; \mathcal{D}), p_o, p_c)    
\end{equation}

The crossover prompt instructs the LLM to make use of the two-parent program and come up with a new program. 
The LLM is able to combine elements from the parents to produce something new as can be seen in \cref{fig:pipeline}. Please refer to the supplementary 
for more examples.

Similar to crossover, we also mutate a program with some probability using a mutation prompt $p_m$. 

\begin{equation}
^{m}P_k^{t+1} = \mathcal{LLM}(P_k^{t+1}, s(P_k^{t+1}; \mathcal{D}), p_o, p_m)
\end{equation}

We present the exact input fed to the LLM for crossover and mutation in the supplementary. Note that both crossover and mutation operations also include the objective prompt preventing the LLM from generating programs far away from the objective. %

\subsection{Feature Set Prediction}
\label{ssec:setpred}
Solutions for many problems require combination of multiple feature. 
Optimizing for the correct combination of these features is challenging for an LLM, as we are not doing gradient-based optimization.
Therefore instead of prompting the LLM to directly generate a predictor for $y$, we prompt it to create a list of predictive features. 
We then learn a linear regressor on top of this list and use the regressor with the list of features as the final program. 
Since both the generated program and regressor are interpretable, our final program is also interpretable.

\subsection{Program Critic}\label{ssec:critic}
The only form of supervision our method gets is through the metric score $s(P;\mathcal{D})$ created by evaluating the program $P$ on the observations. 
However, since we perform crossover and mutation through a language model, we can provide finer-grained information to LLM in order to aid the search.

More specifically, we propose a critic that performs a finer-grained evaluation of the program that we get after crossover/mutation.
In most visual domains, the data can categorically distributed by using the same set of foundational primitives.
Our critic performs a \emph{stratified evaluation} by partitioning the observation data into multiple categories and evaluating the program on individual strata.
\begin{equation}
\mathcal{D} = d_1 \cup d_2 \cup \cdots \cup d_c, \quad \text{where } d_i \cap d_j = \emptyset \text{ for } i \neq j    
\end{equation}

Since all the problems in our benchmark are geospatial, we use a critic that takes the satellite image corresponding to each input and uses a segmentation model to partition the observation dataset into land-use categories. 
The critic obtains per-partition score $s(P; d_i)$ and prompts the LLM to improve the program on categories the model is bad. The addition of a critic improves the programs on data overlooked by existing programs, resulting in reliable programs.

\subsection{Program Simplification}\label{ssec:simplification}
Successive steps of crossovers and mutations of programs result in large programs with many redundancies, hurting interpretability.
We propose an analytical approach to simplify the programs and remove the redundant parts of it. 
Our generated programs can be represented as a directed acyclic graphs (DAG) (we use the abstract syntax tree (AST); see supplementary).
In these DAGs, all the constants and the arguments of the function are root nodes. 
Only the return statement and the unused variables are the leaf nodes.
Any leaf node that is not a return statement, is a piece of code that is not needed and can be removed. 
We then recursively remove all the leaf nodes that are not return statements.

While removing such nodes (unreachable by the return statement) is useful, there could still be features that are returned by the program but are not contributing. 
Recall that we use linear regression on the list of features returned by the program. The weights assigned to individual features by the regression model are useful indicators of which features are redundant.
We remove the features that have a significantly smaller weight compared to the largest weight in the regression (a threshold of 5\% works well). Removing these features from the return statement results in several newly created leaf nodes. We therefore, redo the recursive leaf node removal to further simplify the program. 
Each generated program is first improved through the critic and then simplified before adding back to the population. Algorithm~\ref{alg:algorithm} shows the complete process. 




