
\section{Discussion and Conclusion}
\label{sec:conclusion}
\paragraph{Limitations:}One of our limitation is that we can only differentiably optimize learnable parameters in the last computational layer. 
This could miss out on programs with useful parameters in some intermediate computation layers. 
We attempted to make the whole pipeline differentiable, however the model performance did not improve much. 
Many of the operations in our pipeline even though differentiable have zero-gradient in large part of input space, making gradient optimization challenging.
Moreover, a completely differentiable programs is even slower to optimize resulting in much slower evolution.
In future work, we plan to use initialization tricks for non-linear optimization and second-order optimization to obtain even more expressive models.

\paragraph{Conclusion:}We present DiSciPLE -- an evolutionary algorithm that leverages the prior-knowledge and common sense abilities of LLMs to create \emph{interpretable, reliable and data-efficient} programs for real-world scientific visual data. 
This allows us to create programs that are more powerful than existing interpretable counterparts and more insightful than deeper uninterpretable models.
We shows its prowess on 3 scientific applications by proposing a benchmark for visual program discovery.
We believe that using DiSciPLE in tandem with a human expert can rapidly speed up the scientific process and result in numerous novel discoveries.


