\section{Results}
\label{sec:results}

\begin{figure}[ht]
    \centering
\includegraphics[width=\linewidth]{figures/progs.png}
\caption{The best performing programs for each of the 3 benchmark problems as Python programs (left in each card) and the corresponding DAG representation on the right.
The DAG representation allows better visualization of the importance of different components.
The thickness of the red edges determine how important that component is. 
A black edge represents computation; when removed it is either the same as one of its subsequent edges or removing it could result in a bug.
}
\label{fig:mainresults}

\end{figure}

\subsection{Implementation details}
For all our main experiments we used an open-source LLM \emph{llama-3-8b-instruct}~\citep{dubey2024llama} served using the vLLM library~\citep{kwon2023vllm}.
However, in the supplementary, we also explore other open-source language models.
All the visual data in our benchmarks comes from satellite images, so to allow inferring semantic information from it, we use a black-box open-world foundational model for satellite images, GRAFT~\citep{mall2023graft}. 
Some experiments use ground-truth annotations from OpenStreetMaps~\citep{vargas2020osm} as an alternative to disentangle the effect of segmentation from discovery. 

We run our evolutionary method for $T=15$ generations with a population size of $M=100$. 
For all the problems, the input observation data comes from different geographical locations around the world. 
We split this data into three parts. Two-thirds of the easternmost observations are used to create a training-testing split. The remaining one-third of the data is use to evaluate reliability (out-of-distribution generalization). 
We also release this benchmark for future research in this area. 

\subsection{Benchmark for Visual Program Discovery for Scientific Applications}
\label{ssec:setup}
Given the novelty of the visual program discovery task, there exists no pre-existing benchmark.
We define a new benchmark for this task, drawing on scientifically relevant geospatial problems.
Concretely, we choose two different problems in \emph{Demography}: population density and poverty indicators, 
and to a problem in \emph{Climate Science}: for above ground biomass (AGB) estimation. 


It is important to note that for these problems, \emph{true relationships between variables of interest are actually unknown}.
As such, an LLM cannot be expected to produce a good program in a zero-shot manner, because it has never seen these relationships before.
This is in contrast to problems like VQA~\cite{suris-23} where the reasoning required to answer a question is well known and we can simply rely on the LLM's world knowledge.
In the case of scientific discovery, actual data is needed to discover the right reasoning.

In the following, we present the observation datasets, metrics, and overview of primitives. 

\subsubsection{Population Density}

\textbf{Observation Dataset}: 
The problem seeks to predict the population density by observing the satellite images of a region~\citep{metzger-22,metzger-24}. 
We obtain the population density values ($y_i$) for various locations in the USA by using ACS Community Surveys 5-year estimates~\citep{acs2024}.
Input observations ($x_i$) are sentinel-2 satellite images at a resolution of 10m~\citep{drusch2012sentinel}. For this experiment, we also use OpenStreetMaps masks~\citep{vargas2020osm} for 42 different land-use concepts (see supplementary) as part of the input.

\textbf{Metric and Primitives:} 
Population density values are aggregated at the county block group level. The predicted population densities are therefore also aggregated at the county block group level. The metric is the per-block group level average L2 error after applying a log transformation.
Along with the arithmetic, and logical primitives (see supplementary)
, we use open-vocabulary segmentation as a primitive.
The segmentation function returns a binary mask for an input concept. 



\subsubsection{Poverty Indicator}
\textbf{Observation Dataset:} For poverty estimation, we use data from SustainBench~\citep{yeh2021sustainbench}. 
The dataset contains coordinate location as input and wealth asset index as output.

\textbf{Metric and Primitives:} We use L2 error for each location as the evaluation metric. To obtain semantic land use information about a location, we first define a \emph{get\_satellite\_image} function, that returns a sentinel-2 satellite image for any location. This can be used in conjunction with the open-world satellite image recognition model to obtain semantic information about the world. 
Other than this we also include as primitives functions that return average annual temperature, precipitation, nightlight intensity, and elevation at the input location. 

\subsubsection{Aboveground Biomass}

\textbf{Observation Dataset:}  
Similar to poverty estimation, the observation variables are an input location and the output AGB estimate. We use NASA's GEDI ~\citep{dubayah2020gedi} to obtain the observation value for three US states. 
We use data from Massachusetts and Maine (North-East) as the train/test set and Washington (NorthWest) as the out-of-distibution set.

\textbf{Metric and Primitives:} We use L2 error as the metric and the same primitives as poverty estimation.



\subsection{Experimental Setup}
For the same set of training data we compare our best generated program with a set of baselines. 
\begin{enumerate}
    \item \textbf{Mean:} A naive baseline that use the mean of the training observation as the prediction.
    \item \textbf{Concept Bottleneck (CB):} Similar to \cite{koh20concept,yang2023language,oikarinen2023label}, we first extract a list of relevant features and train a linear classifier on it. 
    This method is interpretable due to the bottleneck, however it is not very expressive (see supplementary).
     

    \item \textbf{Deep models:} We use deep models such as ResNets~\cite{he2016resnet} as baseline (see supplementary for details). We use a small and large variant for each.
    
    \item \textbf{Zero-shot:} This baseline tests how good would LLMs be on their own in generating programs solely relying on prior knowledge without any observation. Since the generated programs can vary drastically, we report an average of 5 different zero-shot programs.

    \item \textbf{Random Search:} Instead of evolutionary search, this baseline relies on the stochasticity of LLMs to perform a random search. If ~\disciple~ is better at searching, it should do better than random searching for the same number of calls to an LLM.
\end{enumerate}

\subsection{Results and Discussion}



\begin{table*}
\small
\centering
      \caption{Performance of our programs on in-distribution (left) and out-of-distribution (right) observations across various problems in the proposed benchmark. This shows the reliability of programs produced by \disciple~(\best{red} is best and \sota{blue} is second best).
      } \label{tab:performance}      
      \begin{tabular}{l c c c c c c | c c c c c c} 
        \specialrule{.12em}{.1em}{.1em}       
        & \multicolumn{6}{c}{In distribution}
        & \multicolumn{6}{|c}{OOD}\\
        & \multicolumn{2}{c}{Population Density} & \multicolumn{2}{c}{Poverty} & \multicolumn{2}{c}{AGB}
        & \multicolumn{2}{|c}{Population Density} & \multicolumn{2}{c}{Poverty} & \multicolumn{2}{c}{AGB}
        \\
        & L2-Log & L1-Log & L1 & RMSE & L1 & RMSE
        & L2-Log & L1-Log & L1 & RMSE & L1 & RMSE\\
        \specialrule{.12em}{.1em}{.1em}
        Mean 
        & 0.6696 & 0.6540 & 1.613 & 1.836 & 42.15 & 50.65
        & 0.6734 & 0.6561 & 1.591 & 1.844 & 74.15 & 83.02
        \\
        
        CB 
        & 0.8298 & 0.7279 & 1.229 & \sota{1.476} & 26.33 & 33.49
        & 0.7951 & 0.7112 & 1.257 & 1.504 & 44.19 & 63.52
        \\
        
        Deep - Small  
        & 0.4431 & 0.5006 & 1.238 & 1.637 & 30.72 & 37.03
        & 0.6623 & 0.5967 & \sota{1.284} & \sota{1.654} & \sota{35.27} & \sota{53.06} 
        \\
        
        Deep - Large  
        & \sota{0.3974} & \sota{0.4843} & \sota{1.170} & 1.478 & \best{21.15} & \best{27.86}
        & \sota{0.4460} & \sota{0.5115}  & 1.344 & 1.741 & 35.41 & 70.30  
        \\
        
        Zero-shot 
        & 0.4702 & 0.5371 & 1.525 & 1.754 & 38.80	& 46.41
        & 0.7020 & 0.6412 & 1.510 & 1.773 & 55.11 &	64.32  
        \\

        Random Search 
        & 0.4353 & 0.5118 & 1.277 & 1.679 & 29.40	& 36.70
        & 0.6763 & 0.6298 & 1.418 & 1.840 & 42.32 & 52.53  
        \\
        
        \textbf{Ours} 
        & \best{0.2607} & \best{0.3778} & \best{1.077} & \best{1.314} & \sota{24.79} & \sota{32.99}
        & \best{0.3807} & \best{0.4426} & \best{1.134} & \best{1.420} & \best{31.10} & \best{42.93}
        \\
        \specialrule{.12em}{.1em}{.1em}
      \end{tabular} 
\end{table*}




We first test our programs on unseen \emph{in-domain} observations close to the regions used for training (\cref{tab:performance} (left)).
We observe that ~\disciple~outperforms all interpretable baselines.
It can even outperform a deep model in many cases, specifically on population density estimation, while being significantly more interpretable.
\disciple~ also outperforms zero-shot program inference from LLMs.
As discussed before, this is in line with the fact that \disciple is uncovering new relationships that may not be known to us, and by extension, to the LLM.
The performance of random search while better than zero-shot is significantly worse than ~\disciple. 
This shows that~\disciple~ is able to perform a significantly faster search, by reducing the meaningful search space.
Our evolutionary process effectively leverages data to perform this novel discovery.

\paragraph{Are our programs reliable?} 
If a program is reliable it should be able to generalize to other regions. 
\cref{tab:performance} (right) shows DiSciPLE to these baselines on such an out-of-distribution set.
Here our approach outperforms all baselines \emph{including deep networks}, suggesting that due to its interpretable-by-design representation, our method learns a model that can generalize better and overfit less to the in-distribution training data.

\begin{figure*}[ht]
    \centering
\includegraphics[width=\linewidth]{figures/qualitative_main.png}
\caption{Qualitative comparison of ~\disciple~ with other baselines on the tasks of population density. ~\disciple~ Can map to the true population density maps much more accurately than the baselines (Refer to the supplementary for more comparisons). The maps display population density as the base-10 log of people per square mile.
}
\label{fig:qualitative}
\end{figure*}

We also show these results qualitatively in \cref{fig:qualitative}, by comparing population density predictions of ~\disciple~ and the baselines to the true population density.
It is very clearly evident that ~\disciple~ can model the fine-grained changes in population in unseen regions significantly better than the baselines (refer to supplementary for more visualizations). 


\begin{figure}[ht]
    \centering
\includegraphics[width=\linewidth]{figures/scale_vert.png}         
\caption{Performance of \disciple~compared to deep baselines as we reduce the amount of training observation (in terms of L2 error). The Oracle (blue) uses a program learned from all observations but uses only partial observation for parameter training. ~\disciple~(orange) uses partial observation during evolution as well. While the errors get worse as we reduce the observation data, the drop is significantly less severe for ~\disciple~ compared to deep models, which tend to overfit.}
    \label{fig:scale}
\end{figure}


\paragraph{Are our programs data-efficient?} 
Our methods are only trained on a maximum of 4000 observations.
\cref{fig:scale} further shows that even when the amount of training data is reduced, our approach shows minimal degradation in performance compared to deep networks.
This suggests that while deep models can learn to generalize with a lot more data, our model does not need as much data to begin with, making it data-efficient.


\paragraph{Are our programs interpretable?} Our programs are interpretable-by-design as we can visualize the factors contributing to performance. 
Fig.~\ref{fig:mainresults} shows such programs (left in each card) for all the problems in our benchmark. 
An expert who is working with our method to figure out such programs can add/edit parts of the formula and figure out which/how much do each of these components matters. 

We perform this step of understanding the influence of individual operations by removing each operation in our program and measuring its effects on the final score. 
The DAGs on the right of each program show the program structure and the red edges show the influence of each component proportional to the width. 
This visualization can allow experts to understand which operations are important for the model.
For example, in the program for population density \cref{fig:mainresults}, we can see that semantic concepts such as ``highway'' and ``residential building'' are very important.






\paragraph{Can our method perform better than expert humans?}
Our method would only be useful in real-world scenarios if it can come up with stronger or comparable programs to human experts.  
We test this on the task of AGB, by providing an expert (a PhD student actively working on AGB) with a user interface with the same information as our method. 
The experts took about 1.5 hours to use their domain knowledge and iterate over their program for AGB estimation. However, the best program they could come up with had an L1 error of \textbf{37.65} on the in-distribution set and \textbf{53.20} on the OOD set (compared to \textbf{24.79} and \textbf{31.10} for ~\disciple). We figure this is primarily because experts need to spend more time on the problem. 
In general, experts would spend numerous days to come up with a good program, 
while our method can come up with a better program faster.

\paragraph{Extension to more indicators}
We also test ~\disciple~ on a larger suite of demographic indicators. Using SocialExplorer, we build a suite of 34 demography indicators. Refer to the supplementary for a list of these indicators. This includes demography information such as age group, education status, etc. In \cref{tab:moreindicators}, we report the average performance of our method compared to baselines on this data.
Since different indicators can have different scales, we first normalize all of them to have zero mean and unit standard deviation. 
These indicators are challenging to predict directly from satellite images, as evidenced by the deep model failing to perform significantly better than CB and mean baselines. 
As a result while ~\disciple~ performs better than all the baselines the improvements are not huge.
Nonetheless, ~\disciple~ performs better than every baseline.
This large-scale experiment shows the potential of applying ~\disciple~ to a wider range of problems. More details about these demographic indicators and individual performance on these is shown in the supplementary.

\begin{table}
\small
\centering
      \caption{Performance of ~\disciple~ compared to baselines on a larger suite of challenging 34 demographic indicators. Since the dataset is very challenging, the deep baseline regresses to mean, however with  ~\disciple~ we can still see some improvements.
      } \label{tab:moreindicators}      
      \begin{tabular}{l c c c c} 
      & \multicolumn{2}{c}{Test} & \multicolumn{2}{c}{OOD} \\
     & L1  & RMSE & L1 & RMSE \\
        \specialrule{.12em}{.1em}{.1em}    
     Mean & 0.8578 & 1.1519 & 0.8939 & 1.1948\\
     CB & 0.8249 & 1.1159 & 0.8771 & 1.1767\\
     Deep & 0.8527 & 1.1556 & 0.8942 & 1.1990\\
     \textbf{Ours} & \textbf{0.8159} & \textbf{1.1065} & \textbf{0.8750} & \textbf{1.1719}\\
        \specialrule{.12em}{.1em}{.1em}       
      \end{tabular} 
\end{table}

\subsection{Ablations}

\paragraph{How important is the role of feature-set prediction, critic, and simplification?}
Table~\ref{tab:ablationparts} measures the performance of our model on the task of population density as we successively add these components to the evolutionary algorithm. 
The addition of feature set prediction instead of a single feature helps, as it allows our method to learn expressive linear regression parameters instead of letting the LLM come up with them.
Further adding critic results in further improvement as the programs start covering nicher concepts resulting in better unseen and OOD generalization. 
Finally adding in simplification also improves the program. 
We posit that simplification removes irrelevant features preventing the LLM from focusing on them when performing crossovers.

\begin{table}
\small
\centering
      \caption{Performance of our method as we successively remove the components. Both critic and simplification lead to performance improvement for our method.
      } \label{tab:ablationparts}      
      \begin{tabular}{c c c c c c c} 
      & & & \multicolumn{2}{c}{Test} & \multicolumn{2}{c}{OOD} \\
      Set & Critic & Simpli. & L2 log & L1 log & L2 log & L1 log \\
        \specialrule{.12em}{.1em}{.1em}       
        \xmark & \xmark & \xmark & 0.3159 & 0.4296 & 0.4835 & 0.5178\\    
        \cmark & \xmark & \xmark & 0.2906 & 0.4049 & 0.4258 & 0.4826\\    
        \cmark & \cmark & \xmark & 0.2873 & 0.3984 & 0.4184 & 0.4684 \\    
        \cmark & \cmark & \cmark & \textbf{0.2607} & \textbf{0.3778} & \textbf{0.3807} & \textbf{0.4426}\\    
        \specialrule{.12em}{.1em}{.1em}       
      \end{tabular} 
\end{table}


\paragraph{How important are common sense and prior knowledge of LLMs?}
The two major advantages an LLM provides over traditional tree-search are: 1) better crossover and mutation as LLMs can understand the meaning of the primitives. 2) use of prior knowledge for better-guided search. 
Therefore we remove these two sources of information and test how well can our method perform. 
To remove the understanding of functions we rename them with meaningless terms and remove the descriptions.
To remove the context of the problem we remove the objective prompt.
\cref{tab:ablationcommonsense}, show the performance of our method on density estimation after removing each of these prompts. 
Without common sense, the search cannot even progress away from the initial random programs, resulting in worse-than-mean results (L1 error of 0.84 vs 0.26 for ~\disciple).
This suggests that symbolic regression models, that have no understanding of open-world primitives, would struggle to search.
If we just remove the context of the problem, the model does slightly better and can obtain results better than the mean and zero-shot programs (L1 error of 0.45). This suggests that while the search is moving in the objective's direction, it is slow. 


\begin{table}
\small
\centering
      \caption{Perfomance of our method when removing the context of the problem (objective prompt from the evolution, and when renaming and not describing the primitive functions to the LLM.
      We see significant drops in performance in both cases, suggesting that both common sense and prior knowledge of LLM are important to perform efficient evolutionary search.
      )} \label{tab:ablationcommonsense}      
      \begin{tabular}{c c c} 
      Method & L1 log & L2 log \\
        \specialrule{.12em}{.1em}{.1em}       
        No common-sense & 0.8401 & 0.7186\\    
        No problem context & 0.4498 & 0.5140\\    
        \disciple~full & \textbf{0.2607} & \textbf{0.3778}\\    
        \specialrule{.12em}{.1em}{.1em}       
      \end{tabular} 
\end{table}



