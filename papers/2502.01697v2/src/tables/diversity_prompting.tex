\begin{table}[t]
\caption{Average pairwise embedding cosine similarity of various prompting techniques. GPT-4o is used as the prompting generator due to the need for strong instruction following capabilities; Llama models frequently derailed. Llama-3.1-70B-Base generations are almost uniformly more diverse than any prompting method, except for persona prompting on GSM8K, where it is comparable.}
\label{prompting-diversity-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|cc}
\toprule
Method & Enron & GSM8K \\
\midrule
GPT-4o Independent & 0.574 & 0.427 \\
GPT-4o Persona & 0.580 & \textbf{0.308} \\
GPT-4o Sequential & 0.511  & 0.398 \\
GPT-4o In-One & 0.363 & 0.347 \\
GPT-4o Dynamic Fewshot & 0.511 & 0.463 \\
\midrule
Llama-3.1-70B-Base & \textbf{0.350}  & \begin{sl}{0.313}\end{sl} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
