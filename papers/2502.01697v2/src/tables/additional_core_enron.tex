\begin{table}[H]
\caption{Average pairwise embedding cosine similarity, IR, and downstream F1 results on a randomly selected static $n=500$ subset of Enron. A BERT model with a classification head was trained for $2$ epochs on the generated data $(n = 500)$. Only pairwise similarities for generations within the same class (spam or legitimate) were calculated.}
\label{enron-core}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|ccc}
\toprule
Generation Method & Average Embedding Similarity & IR & Downstream F1 \\
\midrule
Llama 3.1 8B Instruct              & 0.500 & 86.0\% & 0.753 \\
Llama 3.1 70B Instruct             & 0.450 & 85.0\% & 0.848 \\
Llama 3.1 8B Base                  & 0.368 & 63.5\% & 0.790 \\
Llama 3.1 70B Base                 & 0.350 & 74.5\% & 0.819 \\
\Sys{} Llama 3.1 8B                & 0.413 & 85.0\% & \textbf{0.872} \\
\Sys{} Llama 3.1 70B               & 0.406 & 82.0\% & 0.771 \\
\Sys{} GPT-4o + Llama 3.1 8B Base  & 0.379 & 84.5\% & \textbf{0.872} \\
\Sys{} GPT-4o + Llama 3.1 70B Base & 0.356 & 88.5\% & 0.846 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
