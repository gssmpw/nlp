\begin{table}[H]
\caption{Temperature ablations with independent sampling from Llama-3.1-8B-Instruct. Average pairwise embedding cosine similarity, IR, and downstream accuracy results on a randomly selected static $n=100$ subset of HotpotQA RAFT. A Llama-3.1-8B-Instruct model was fine-tuned for 4 epochs on the generated data ($n=1000$). The baseline performance of the Llama-3.1-8B-Instruct model on the evaluation set prior to any fine-tuning is reported in the first row.}
\label{hotpot-temp}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|ccc}
\toprule
Generation Method & Average Embedding Similarity & IR & Downstream Accuracy \\
\midrule
Baseline Performance              &   --  &  --  & 33\% \\
\midrule
Llama 3.1 8B Instruct ($t=1.0$)   & 0.214 & 76\% & 49\% \\
Llama 3.1 8B Instruct ($t=0.7$)   & 0.216 & 77\% & 50\% \\
Llama 3.1 8B Instruct ($t=0.5$)   & 0.220 & 83\% & 42\% \\
Llama 3.1 8B Base ($t=1.0$)       & 0.221 & 62\% & 50\% \\
\Sys{} Llama 3.1 8B (All $t=0.7$) & 0.217 & 77\% & \textbf{58\%} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
