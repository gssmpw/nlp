\begin{table}[H]
\caption{Average pairwise embedding cosine similarity, IR, and downstream accuracy results on a $n=500$ randomly selected static subset of GSM8K. A Llama-3.2-1B-Instruct model was fine-tuned for 4 epochs on the generated data ($n=1000$). The baseline performance of the Llama-3.2-1B-Instruct model on the evaluation set prior to any fine-tuning is reported in the first row.}
\label{gsm8k-refine}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc} 
\begin{tabular}{c|ccc}
\toprule
Generation Method & Average Embedding Similarity & IR & Downstream Accuracy \\
\midrule
Baseline Performance                   &   --  &  --  & 21.8\% \\
\midrule
Llama 3.1 70B Instruct                 & 0.421 & 51\% & 22.4\% \\
Llama 3.1 70B Instruct Self-Refine     & 0.422 & 63\% & 25.4\% \\
GPT-4o Refining Llama 3.1 70B Instruct & 0.421 & 70\% & 30.8\% \\
\midrule
\Sys{} Llama 3.1 70B                   & 0.305 & 54\% & 29.8\% \\
\Sys{} GPT-4o + Llama 3.1 70B Base     & 0.302 & 64\% & \textbf{35.8\%} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
