\begin{table}[H]
\caption{Average pairwise embedding cosine similarity, IR, and downstream accuracy results on on a randomly selected static $n=500$ subset of Newsgroups. A BERT model with a classification head was trained for $9$ epochs on the generated data $(n = 500)$.}
\label{newsgroups-core}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|ccc}
\toprule
Generation Method & Average Embedding Similarity & IR & Downstream Accuracy \\
\midrule
Llama 3.1 8B Instruct              & 0.271 & 85\% & 26\% \\
Llama 3.1 70B Instruct             & 0.246 & 82\% & 30\% \\
Llama 3.1 8B Base                  & 0.155 & 58\% & 41\% \\
Llama 3.1 70B Base                 & 0.162 & 78\% & 29\% \\
\Sys{} Llama 3.1 8B                & 0.162 & 91\% & 40\% \\
\Sys{} Llama 3.1 70B               & 0.134 & 93\% & \textbf{49\%} \\
\Sys{} GPT-4o + Llama 3.1 8B Base  & 0.131 & 81\% & 44\% \\
\Sys{} GPT-4o + Llama 3.1 70B Base & 0.285 & 87\% & 47\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
