\begin{table}[H]
\caption{Average pairwise embedding cosine similarity, IR, and downstream accuracy results on the full LCB TOP set. A Llama-3.1-8B-Instruct model was fine-tuned for 4 epochs on the generated data ($n=1000$). The baseline performance of the Llama-3.1-8B-Instruct model on the evaluation set prior to any fine-tuning is reported in the first row.}
\label{lcb-core}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|ccc}
\toprule
Generation Method & Average Embedding Similarity & IR & Downstream Accuracy \\
\midrule
Baseline Performance               &   --  &  --  & 18.6\% \\
\midrule
Llama 3.1 8B Instruct              & 0.416 & 51\% & 20.6\% \\
Llama 3.1 70B Instruct             & 0.389 & 49\% & 26.0\% \\
Llama 3.1 8B Base                  & 0.468 & 36\% & 24.9\% \\
Llama 3.1 70B Base                 & 0.477 & 57\% & 24.4\% \\
\Sys{} Llama 3.1 8B                & 0.462 & 47\% & \textbf{28.1}\% \\
\Sys{} Llama 3.1 70B               & 0.481 & 64\% & 25.6\% \\
\Sys{} GPT-4o + Llama 3.1 8B Base  & 0.459 & 72\% & 26.7\% \\
\Sys{} GPT-4o + Llama 3.1 70B Base & 0.471 & 68\% & 27.4\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
