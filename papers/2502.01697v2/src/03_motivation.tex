\section{Motivation}
\label{motivation}

Synthetic data generation should result in a diverse dataset with high-quality entries. Thus we motivate our design of \Sys{} with investigations on the diversity and entry-wise quality of synthetically generated data from base and instruct-tuned models, independent of downstream utility.

\subsection{Diversity \& Quality}
\label{diversity-experiments}

\paragraph{Diversity.} Following \citet{tevet-berant-2021-evaluating}, we use the average neural similarity score to measure the diversity of a generated dataset. Specifically, we use OpenAI's \texttt{text-embedding-3-small} \cite{openai2024embedding} to generate embeddings and use cosine similarity to calculate similarity scores, as recommended by OpenAI. We calculate pairwise cosine similarity scores for items in a generated dataset and analyze the resulting distribution of similarities. A lower average similarity indicates a more diverse dataset.

\paragraph{Entry-wise quality.} We seek a metric measuring the quality of individual entries in the generated datasets. To this end, we propose the \textit{indistinguishability rate} (IR) of the entries as a quality metric. We believe that we are the first to propose this metric for the quality of an individual data entry. Specifically, we measure how often a strong LLM (e.g. GPT-4o) fails to identify the synthetically generated entry as being of lower quality when combined with $n=3$ real dataset entries. A high IR indicates that generated data closely matches properties of real-world data and the discriminator is randomly guessing, while a low IR indicates the generated data are out-of-distribution from real world data. An example IR prompt is available in \cref{appendix_prompts}.

\subsection{Experimental Setup}

\paragraph{Models.} To investigate diversity and quality differences between instruct-tuned and base models, we evaluate a synthetic dataset generated using Llama-3.1-70B-Instruct and Llama-3.1-70B-Base \cite{llama2024llama}. We additionally use GPT-4o \cite{openai2024hello} as an instruct-tuned model with strong instruction-following capabilities.

To allow fair comparisons and simulate a low-data setting we always provide three few-shot examples.

\paragraph{Domains.}
\label{sec:domains}

We examine a variety of domains covering many tasks. Our data generation domains are:
\begin{itemize}
    \setlength\itemsep{0em}
    \item Enron Emails \cite{Klimt2004} generating training data for classifying emails as spam or legitimate. We ensure class-balanced synthetic data by explicitly conditioning each generation on a uniform class distribution.
    \item 20 Newsgroups \cite{scikit-learn} generating training data for classifying Usenet messages into one of 20 newsgroup sources. The model generates classes along with content, allowing the generation method to determine the class distribution of the synthetic dataset.
    \item Retrieval-Augmented Fine-Tuning (RAFT) \cite{zhang2024raftadaptinglanguagemodel}, a domain and generator-agnostic synthetic data generation framework for fine-tuning data in RAG tasks. Here, Q/A pair data generation needs to be conditioned on contexts mimicking retrieval results. We use:
    \begin{itemize}
        \item HotpotQA \cite{yang2018hotpotqadatasetdiverseexplainable}, a general Wikipedia-based short-answer task.
        \item PubMedQA \cite{jin2019pubmedqadatasetbiomedicalresearch}, a medical abstract-based yes/no/maybe question-answering task.
    \end{itemize}
    \item GSM8K \cite{cobbe2021trainingverifierssolvemath}, generating grade-school math word problems and solutions for fine-tuning.
    \item LiveCodeBench's Test Output Prediction (LCB TOP) \cite{jain2024livecodebenchholisticcontaminationfree}, generating coding questions and answers on predicting test case outputs given a natural language description of an algorithm's expected behavior and test input for fine-tuning.
\end{itemize}

For the classification tasks of Enron and Newsgroups, we generate a dataset of size $n=500$. For the generative model fine-tuning tasks of HotpotQA, PubMedQA, GSM8K, and LCB TOP, we generate a larger dataset of size $n=1000$.

We sample at a default temperature of $0.7$ for base models and the highest temperature at which we experimentally found data generation is still coherent for instruct models, which is $1.0$ for Llama models and $1.2$ for GPT-4o. However, for Enron, we sample from GPT-4o at a temperature of $1.0$ to maintain generation coherence.

Note we do not measure diversity in the RAFT domains as we expect noise due to conditioning on different documents/contexts. For Enron, due to the relatively large differences between spam and legitimate emails, only similarities between emails of the same type are calculated.

\subsection{Investigation Results}
\label{qd-investigation-results}

\input{src/figures/diversity_base_v_instruct_enron}
\input{src/figures/diversity_base_v_instruct_newsgroups}

\paragraph{Diversity.} Looking at the pairwise cosine similarity distributions of the embeddings of the generated dataset in \cref{results-spam-diversity} (and recalling \cref{abstract-diversity}), the base distribution is consistently shifted to the left, indicating that the base model generations are more diverse and correspond with a decrease in average similarity (\cref{diversity-table}). The higher diversity of base models is more strikingly shown in the coverage of generated classes for Newsgroups (\cref{results-newsgroups-coverage}), where over $500$ generations the Instruct model covers only 14 of the 20 classes. In contrast, the Base model covers every class except 1 (\texttt{talk.religion.misc}).

\input{src/tables/diversity_base_v_instruct}

This trend is reflected in \cref{diversity-table}, with all domains except one showing results for base models with higher diversity. Upon inspection, we attribute the reversal in trend in LCB TOP to the base model repeating phrases from the examples in the prompt. This is related to potential issues with the quality of base model generations, which we discuss below.

\input{src/tables/diversity_prompting}

\textit{Sampling Methods}

We further explore data generation using several different sampling methods. As base models struggle to follow complex instructions, only instruct models are used for methods beyond independent sampling.

\begin{itemize}
    \setlength\itemsep{0em}
    \item \textbf{Independent (temperature) Sampling}: The model generates $n$ samples to form a dataset.
    \item \textbf{Persona Prompting}: The model is instructed to respond as a predefined persona \cite{frohling2024personas}.
    \item \textbf{Sequential Prompting}: The model iteratively generates outputs distinct from previous ones.
    \item \textbf{In-One Prompting}: The model is prompted to generate $k$ different entries in a single response \cite{zhang-etal-2024-improving-diversity}.
    \item \textbf{Dynamic Few-Shot Examples}: Few-shot examples are randomly selected for each call (breaking the assumption of low-data conditioned generation) \cite{li2022making}.
\end{itemize}

\cref{prompting-diversity-table} shows that base models generally yield higher diversity than almost all prompting methods on the average embedding distance metric in two domains. One exception is persona prompting on GSM8K --- though this diversity arises more from flavor text differences due to personas rather than actual content.

We thus find that base models are generally more diverse than instruct-tuned models and that temperature increases and prompting methods are insufficient to bridge the gap, motivating our usage of base models in the first stage of \Sys{}.

\input{src/figures/quality_base_v_instruct}

\paragraph{Entry-wise quality.} \cref{ir-bar-plot} presents our results. In general, the instruct-tuned model has a higher IR, indicating that it is better at producing generations that resemble high-quality data. Note that some IRs are well above 75\%. This outcome is not unexpected: if a model consistently generates data that aligns with the most common patterns in the real-world distribution, it becomes difficult to distinguish from actual data. Since the real-world entries often also include non-modal (less frequent) samples, a discriminator tasked with identifying lower quality data may instead misclassify these less common real-world samples as synthetic.

As mentioned above, on LCB TOP the base model repeats phrases from examples in the prompt. This leads to a higher IR as the generations copy real-world examples. Thus, while individual base generations are technically more realistic, their shortcomings are captured in the diversity metric.

We therefore find that the superior instruction follow-up capabilities of instruct-tuned models can help generate more realistic data, motivating our usage of instruct-tuned models in the second stage of \Sys{}.
