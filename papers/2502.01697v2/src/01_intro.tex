\section{Introduction}
\label{introduction}

As Large Language Models (LLMs) grow in size and capability, the demand for high-quality, diverse data in model training is outpacing human-generated data, necessitating the use of synthetically generated data \cite{villalobos2024rundatalimitsllm}.
Consequently, we expect the compute spent on curating and generating data for model training to increase significantly in the coming years, especially in low-data domains where high-quality data is scarce. For example, it is common to use LLMs to generate synthetic data for a variety of tasks such as math, code, function calling, general reasoning, etc. \cite{yu2024metamathbootstrapmathematicalquestions, guo2024deepseek, patil2023gorillalargelanguagemodel, liu2024bestpracticeslessonslearned}. 

The ease of data generation has led many dataset creators \cite{samvelyan2024rainbowteamingopenendedgeneration, zhang2024raftadaptinglanguagemodel, sky_t1_2025} and model creators \cite{llama2024llama, qwen2025qwen25technicalreport, nvidia2024nemotron4340btechnicalreport, guan2025deliberativealignmentreasoningenables, sky_t1_2025} to turn to synthetic data sampled from instruct-tuned language models as a replacement for tasks where human-generated data is lacking.

\paragraph{Challenges with Instruct-Tuned Models.}
Though instruct-tuned models have become more and more capable at complex tasks resulting in higher quality generations, there have been discussions of how accepted methods for post-training can lead to mode collapse \cite{Shumailov2024, wong2024simplestratdiversifyinglanguagemodel, lambert2024self}. Mode collapse refers to an inability for models to generate diverse outputs to queries that don't have a single response, yet synthetic data is primarily useful when it is both high quality and diverse \cite{chen2024diversitysyntheticdataimpact, ravent√≥s2023pretrainingtaskdiversityemergence}.

Intuitively, one might expect a variety of prompting techniques that explicitly encourage diversity to address this gap. For example, prompting methods we baseline against include conditioning a generation on all previous ones and prompting for it to be distinct, assuming different personas for different generations, or asking for multiple distinct entries in a single call \cite{zhang-etal-2024-improving-diversity, naik2023diversity, frohling2024personas}. However, we find that the amount of meaningful diversity from such methods is limited and cannot fully address the gap introduced by post-training. On an open-ended generation task like email generation, such prompting methods do not meaningfully improve dataset diversity (\cref{prompting-diversity-table}) or trained model accuracy (\cref{prompt-main}).

\paragraph{Base vs. Instruct-Tuned Models.} An alternative approach is to leverage base models, which are not constrained by post-training biases and thus produce outputs that better represent diversity present in real-world data \cite{openai2024gpt4technicalreport}.
More quantitatively, as shown in \cref{abstract-diversity}, base models generate outputs with noticeably lower pairwise cosine similarity (mean similarity: $0.313$) compared to instruct-tuned models (mean similarity: $0.421$). Here we use cosine similarity as a quantitative surrogate for diversity --- a dataset with greater cosine similarity has more similar items and therefore less diversity. The improved diversity in base model generations can improve downstream task performance, with \cref{abstract-accuracy} showing that the accuracy of Llama-3.2-1B-Instruct on GSM8K increases to 22.5\% from 17.8\% when fine-tuning on the Llama-3.1-70B-Base generated dataset of word problems rather than the Llama-3.1-70B-Instruct generated dataset.

\paragraph{\Sys{}.} In this work, we observe that diversity is critical for synthetic data generation when used in downstream training and fine-tuning tasks. We find that while the data generated from a base model may be more diverse, it also tends to be of lower quality (\cref{ir-bar-plot}). This can counteract the gains from diversity and hinder downstream training. Our key insight is that by combining base and instruct-tuned models, one can improve the diversity of a dataset while controlling the quality of individual data entries.

To this end, we introduce \textbf{Base-Refine} (\Sys{}) --- a novel approach that leverages the diverse data generated by the base models and refines it with instruct-tuned models. In a variety of settings, we find this two-stage process enhances diversity without compromising quality, enabling the generation of datasets that improve downstream performance.

We evaluate fine-tuning with \Sys{}-generated data on contemporary tasks, such as the recently introduced Retrieval-Augmented Fine-Tuning (RAFT) method \cite{zhang2024raftadaptinglanguagemodel}. RAFT generates synthetic question/answer pairs starting from just a corpus to fine-tune a RAG model, but we find that the diversity of data it generates is low. By replacing the data generator in RAFT with \Sys{}, we show we can achieve up to a 18.4\% fine-tuned accuracy lift over the existing work that solely uses instruct-tuned models.

As another example, \Sys{} improves the generation of synthetic math training data. We generated grade school math problems similar to GSM8K problems with several methods, shown in \cref{abstract-accuracy}. We find that refining Llama-3.1-70B-Base data generations with Llama-3.1-70B-Instruct improves a fine-tuned model's accuracy on GSM8K from 22.4\% (base) to 29.8\% (refined). Likewise, using GPT-4o as a generator out of the box (22.4\%) does not perform as well as using GPT-4o as a refiner of generations from Llama-3.1-70B-Base (35.8\%).

To summarize, our contributions are:
\begin{enumerate}
    \item We quantitatively investigate the quality and diversity of base and instruct-tuned models for synthetic data generation across various sampling methods to motivate better system design. In \cref{motivation}, we find that base models tend to produce more diverse responses whereas instruct-tuned models offer higher quality.
    \item Using these insights, in \cref{bare}, we propose \textbf{Base-Refine} (\Sys{}), a practical new method for generating synthetic data. We show in \cref{results} that \Sys{} consistently improves fine-tuned model performance over various baselines, including SOTA generation methods, and with only 1000 samples can lead to fine-tuned models comparable to SOTA models of similar sizes.
\end{enumerate}
