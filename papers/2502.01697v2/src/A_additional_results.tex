\section{Additional Results}
\label{appendix_experiments}

\subsection{Downstream Evaluation - Additional Details}

\subsubsection{Fine-Tuning Task Hyperparameters}
\label{appendix_hyperparameters}

We list below the fine-tuning hyperparameters that were used in common for HotpotQA, PubMedQA, GSM8K, and LCB TOP. Learning rate was determined independently for each domain via learning rate sweeps (across orders of magnitude); each sweep gave the same optimal learning rate.

\begin{itemize}
    \setlength\itemsep{-0.5em}
    \item Learning Rate: 0.001
    \item LoRA $\alpha$: 16
    \item LoRA Rank: 8
    \item LoRA Dropout: 0.0
\end{itemize}

\subsubsection{Classification Task Setup}
\label{appendix_class_train}

The generated data is used to train a BERT-based classifier \cite{bert} for 2 epochs on Enron and 9 epochs on Newsgroups. The trained models are evaluated on a static test set with $n=500$ examples for each domain.

\subsection{Core Experiment Results - All Domains}
\label{appendix_core}

This appendix contains diversity, IR, and downstream performance results for all core experiments: generation with Llama 3.1 8B and 70B Base and Instruct models, \Sys{} with Llama 3.1 models of both families, and \Sys{} with the use of GPT-4o.

Note that HotpotQA RAFT and PubMedQA RAFT diversity results present here were not presented in \cref{diversity-table} as we believe the numbers are noisy and not fit for drawing conclusions, due to the use of $100$ different simulated retrieval contexts that generation was conditioned on (as required by RAFT). Not only does this introduce noise to the similarity calculation, but the strong instruction following capability of instruct models allow them to better leverage the inherent diversity in different prompts. However, for completeness, we report the values in the tables in this appendix.

\input{src/tables/additional_core_enron}
\input{src/tables/additional_core_newsgroups}
\input{src/tables/additional_core_hotpot}
\input{src/tables/additional_core_pubmed}
\input{src/tables/additional_core_gsm8k}
\input{src/tables/additional_core_lcb}

\subsection{Independent Sampling Temperature Ablations - HotpotQA, PubMedQA, and LCB TOP}
\label{appendix_temp}

This appendix contains diversity, IR, and downstream performance results for our temperature ablation experiments. We perform a temperature sweep for Llama-3.1-8B-Instruct generation with $t=0.5, 0.7, 1.0$. We find that while adjusting the temperature can improve downstream performance, in general the gains are small relative to gains by using \Sys{}.

\input{src/tables/additional_temp_hotpot}
\input{src/tables/additional_temp_pubmed}
\input{src/tables/additional_temp_lcb}

\subsection{\Sys{} First Stage Ablations - GSM8K}
\label{appendix_refine}

This appendix contains diversity, IR, and downstream performance results for our ablation replacing the first stage of \Sys{} with an instruct-tuned model, specifically Llama-3.1-70B-Instruct. We refine using Llama-3.1-70B-Instruct and GPT-4o, and investigate the change in downstream performance compared to standard \Sys{} (using Llama-3.1-70B-Base in the first stage). Note that dataset diversity is unchanged compared to direct generation from Llama-3.1-70B-Instruct, that IR improves after refinement, and that downstream performance is consistently worse than standard \Sys{}.

\input{src/tables/additional_instruct_instruct_gsm8k}
