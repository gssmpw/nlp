\section{Related Work}
LLM-based agents in recommender systems can be broadly divided into two categories.
The first category focuses on \textbf{recommendation agents} that leverage LLMs to generate or improve recommendations**Vasile, "Improving Recommender Systems with Large Language Models"**.
% InteRecAgent**Li, et al., "InteRecAgent: An Interactive Recreational Agent for Enhancing User Experience"**** uses LLMs to transform traditional recommender systems into interactive systems with natural language interfaces.
% RecMind**Wang, et al., "RecMind: A Novel Recommendation Approach Combining Planning and External Knowledge"**** improves recommendations by combining planning, external knowledge, user data, and an innovative self-inspiring algorithm.
% MACRec**Kumar, et al., "MACRec: Multi-Agent Collaboration for Recommender Systems"**** enhances recommender systems through multi-agent collaboration.
% ToolRec**Zhang, et al., "ToolRec: Leveraging Large Language Models as Surrogate Users for Improved Recommendations"**** leverages LLMs as surrogate users to guide recommendations and integrate external tools, ensuring better alignment with users' preferences.
% Rec4Agentverse**Chen, et al., "Rec4Agentverse: A New Recommendation Paradigm Treating the Agent Itself as an Item"**** pioneers a new recommendation paradigm by treating the recommendation agent itself as the item to be recommended.
%
The second category explores \textbf{user agents} that leverage LLMs to simulate user behavior.
While some studies focus on simulating user dialogues in conversational recommendation**Sun, et al., "Conversational Recommendation: A Survey"****, our emphasis is on simulating user interaction behavior.
RecAgent**Huang, et al., "RecAgent: An LLM-Based Agent for Simulating User Interactions"**** and Agent4Rec**Lee, et al., "Agent4Rec: An Interactive Recommender System Using Large Language Models"**** employ LLM-based agents, incorporating user profiles, memory, and action modules, to simulate interactions with recommender systems.
RAH**Jiang, et al., "RAH: A Novel Multi-Agent Framework for Recommendation Systems"**** places LLM-based multi-agents between users and recommender systems, serving both as recommendation agents and as proxies for user interactions.
FLOW**Khan, et al., "FLOW: Facilitating Collaboration between Recommendation Agents and User Agents"**** facilitates collaboration between recommendation agents and user agents by establishing a feedback loop.
**Gupta, et al., "Simulating User Interactions using Large Language Models and Human Engagement Models"**** integrate explicit user preferences, LLM-driven sentiment analysis, a human engagement model, and a statistical framework to robustly simulate user interactions.
AgentCF**Liu, et al., "AgentCF: A Novel Collaborative Learning Strategy for Recommender Systems"**** proposes a novel approach, conceptualizing users and items as agents and employing a collaborative learning strategy to optimize them simultaneously.

Several studies have highlighted LLMs' generalization capabilities in \textbf{cross-domain recommendation}**Li, et al., "Cross-Domain Recommendation using Large Language Models"**** and explored \textbf{popularity bias} in LLM-based recommenders**Wang, et al., "Understanding Popularity Bias in LLM-Based Recommenders"****.
These works mainly use LLMs as recommenders, not for simulating user behavior.
Importantly, we emphasize the need to explicitly model popularity factors when simulating user behavior, rather than merely reducing their influence.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{case.pdf}
  \caption{Case Studies.}
  \label{fig:93mi}
\end{figure}