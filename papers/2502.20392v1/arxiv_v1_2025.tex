\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage[margin=1in]{geometry}
\usepackage{authblk}

\usepackage[numbers, compress]{natbib} % Pass options for numbered citations
\bibliographystyle{plainnat} % Numbered bibliography style

% Title, authors, and date
\title{Scalable Signature Kernel Computations for Long Time Series\newline via Local Neumann Series Expansions}
\author[1,a]{Matthew Tamayo-Rios} 
\author[1,b]{Alexander Schell}
\author[1,c]{Rima Alaifari}

\affil[]{Seminar for Applied Mathematics, ETH Zurich}
\affil[a,b,c]{{\small\href{matthew.tamayo@sam.math.ethz.ch}{\{matthew.tamayo, alexander.schell, rima.alaifari\}@sam.math.ethz.ch}}}
\date{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\theoremstyle{example}
\newtheorem{example}[theorem]{Example}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{enumitem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Shorthands and Symbols
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{soul}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathrsfs}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}

\usepackage{accents}
\newlength{\dhatheight}
\newcommand{\doublehat}[1]{%
    \settoheight{\dhatheight}{\ensuremath{\hat{#1}}}%
    \addtolength{\dhatheight}{-0.35ex}%
    \hat{\vphantom{\rule{1pt}{\dhatheight}}%
    \smash{\hat{#1}}}}

\usepackage{comment}

\begin{document}

\maketitle

\begin{abstract}
The signature kernel \cite{kiraly2019kernels} is a recent state-of-the-art tool for analyzing high-dimensional sequential data, valued for its theoretical guarantees and strong empirical performance. In this paper, we present a novel method for efficiently computing the signature kernel of long, high-dimensional time series via dynamically truncated recursive local power series expansions. Building on the characterization of the signature kernel as the solution of a Goursat PDE \cite{salvi2021signature}, our approach employs tilewise Neumann‐series expansions to derive rapidly converging power series approximations of the signature kernel that are locally defined on subdomains and propagated iteratively across the entire domain of the Goursat solution by exploiting the geometry of the time series. Algorithmically, this involves solving a system of interdependent local Goursat PDEs by recursively propagating boundary conditions along a directed graph via topological ordering, with dynamic truncation adaptively terminating each local power series expansion when coefficients fall below machine precision, striking an effective balance between computational cost and accuracy. This method achieves substantial performance improvements over state-of-the-art approaches for computing the signature kernel, providing (a) adjustable and superior accuracy, even for time series with very high roughness; (b) drastically reduced memory requirements; and (c) scalability to efficiently handle very long time series (e.g., with up to half a million points or more) on a single GPU. These advantages make our method particularly well-suited for rough-path-assisted machine learning, financial modeling, and signal processing applications that involve very long and highly volatile data.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Time series data is ubiquitous in contemporary data science and machine learning, appearing in diverse applications such as satellite communication, radio astronomy, healthcare monitoring, climate analysis, and language or video processing, among many others \cite{wang2024deep}. The sequential nature of this data presents unique challenges, as it is characterised by temporal dependencies and resulting structural patterns that must be captured efficiently to model and predict time-dependent systems and phenomena with accuracy. Robust and scalable tools for handling such data in their full temporal complexity are thus essential for advancing machine learning applications across these domains. One particularly powerful approach in this direction is the \emph{signature kernel} \cite{kiraly2019kernels, salvi2021signature}, rooted in rough path theory and stochastic analysis \cite{lyons1998differential, friz2010multidimensional}, which has recently gained relevance as an increasingly popular tool in the modern analysis of sequential data \cite{lee2023signature, lyons2022signature}.\\[-0.5em]     

\noindent
Conceptually, the signature kernel (of a family of multidimensional time series) is the Gram matrix of a highly informative and faithful feature representation of its constituent time series. This representation is realized through the \emph{signature transform}, a feature map that embeds time series into a Hilbert space, uniquely capturing their essential time-global characteristics in a hierarchical manner \cite{hambly2010uniqueness}. This structure situates the signature kernel naturally within the framework of reproducing kernel Hilbert spaces (RKHS) and enables its practical utility through kernel learning techniques. Furthermore, the rich intrinsic properties of its underlying signature feature map confer several strong theoretical properties to the signature kernel, including invariance under irregular sampling and reparametrization, universality and characteristicness, as well as robustness to noise \cite{chevyrev2022signature}. Its wide-ranging theoretical interpretability combined with its strong real-world efficiency have elevated the signature kernel to a state-of-the-art tool for the analysis of time-dependent data. \cite{lyons2022signature}\\[-0.5em]

\noindent
However, existing methods for computing the signature kernel face significant challenges with scalability, particularly when dealing with long time series or those exhibiting high variability\footnote{\ldots quantified by the sum of squared differences between consecutive points of the time series and referred to as its `roughness'.} and rapid fluctuations. The reason is that these methods typically either compute truncated approximations of the signature transforms via dynamic programming \cite{kiraly2019kernels} or solve a global Goursat PDE for the signature kernel via a two‐dimensional discretization \cite{salvi2021signature}, which can require storing a grid of function values that is quadratic in the length $\ell$ of the time series, resulting in prohibitive memory usage for long or high‐roughness time series. These limitations, along with the quadratic complexity in the number of constituent time series (which remains a bottleneck for the Gram matrices of most kernel methods and is not specific to the signature kernel), still obstruct the application of signature kernels to large-scale, real-world datasets; cf., e.g., \citep{toth2025user, lee2023signature} and the references therein.\\[-0.5em]

\noindent
To address this, we introduce a novel approach based on dynamically truncated local (tilewise) Neumann-series expansions of the signature kernel. By partitioning the $[0,1]^2$ domain of the time series into smaller tiles, we derive quickly convergent power-series representations on each tile whose bounady data come from adjacent tiles. This enables an efficient computation of the signature kernel in terms of memory and runtime while ensuring both superior accuracy and significantly improved scalability with respect to the length of the kernel's constituent time series. Because we only store local expansions (rather than a global $\ell\times\ell$ grid), memory usage grows much more slowly with $\ell$, making the method capable of handling very long time series -- up to even half a million points each on a single GPU -- where PDE or dynamic-programming-based solvers would typically run out of memory. By explicitly exploiting the time-domain geometry of piecewise-linear time series in this way, our method enables localized computations of the signature kernel, even for very long time series, that are both highly parallelizable and memory-efficient.\\[-0.5em]

\noindent
Specifically, our main contributions are:
\begin{enumerate}
    \item \textbf{Local Neumann Series Expansions of the Signature Kernel:} We use a \emph{tilewise integral-equation approach} to construct power series expansions of the signature kernel over local segments of the kernel's constituent time series, coupled with a dynamic truncation strategy that adaptively determines the number of terms retained in these expansions. This approach exploits the time-domain geometry of the signature kernel, enabling its computation with significantly reduced memory usage and computational cost while maintaining high accuracy.
    \item \textbf{Localized and Parallelizable Computations:} By exploiting the piecewise-linear structure of the input time series, the entire domain of the kernel tiled in a way that allows for its local Neumann expansions to be computed to adjustable accuracy and in parallel across subdomains, with minimal global communication.
    \item \textbf{Scalability to Very Long and Rough Time Series:} Through dynamic truncation and strictly local processing, our method -- dubbed \texttt{PowerSig} -- attains scalability to very long (up to half a million points or more on a single GPU) and rough time series, overcoming key limitations of existing methods.
    \item \textbf{Empirical Validation Across Benchmarks:} We demonstrate the advantages of our method in accuracy, runtime, and memory efficiency through comprehensive benchmarks against the state-of-the-art PDE-based solver \texttt{KSig} \cite{toth2025user}.
\end{enumerate}

\noindent
The remainder of this paper elaborates on these contributions, starting with a brief discussion of related work (Section \ref{sec:related-work}), followed by a detailed description of our methodology (Section \ref{sec:methodology}) and a report of our experimental results (Section \ref{sec:experiments}). We show, in particular, that our tilewise expansions can handle time series of length up to $5\times 10^5$ on a single GPU, with orders-of-magnitude less memory usage than finite-difference PDE or dynamic programming approaches. All proofs are provided in Appendix \ref{sect:proofs}.\\[-0.5em]

The codebase for our method, including all implementation details, is available in the public GitHub repository: \href{https://github.com/geekbeast/powersig}{\texttt{https://github.com/geekbeast/powersig}}.

\section{Related Work}\label{sec:related-work}
The computation of Gram matrices for signature-transformed time series was first recognized and systematically studied by Király and Oberhauser \cite{kiraly2019kernels} as a foundational step in the principled application of rough path theory to data science and machine learning, as broadly envisioned by Lyons \cite{lyons2014rough}. Building on the conceptual framework of RKHS, Chevyrev and Oberhauser \cite{chevyrev2022signature} then extended the theoretical foundations of the signature kernel and introduced a statistically robust variant achieved through appropriate scaling of the constituent time series. Computationally, a significant advancement beyond the dynamic programming approach proposed by \cite{kiraly2019kernels} for calculating Gram matrices of \emph{truncated} signature transforms was achieved by Salvi et al.\ \cite{salvi2021signature}. In this work, the authors characterize the signature kernel as the solution to a linear second-order hyperbolic PDE on the unit square, thus enabling its numerical computation through specialized PDE solvers. This characterization introduced a new computational framework for the signature kernel, exemplified by the widely used libraries \texttt{sigkernel} and \texttt{ksig} \cite{toth2025user}, with the latter offering GPU-accelerated implementations of the Goursat-based finite difference approximation of \cite{salvi2021signature}.\\[-0.5em] 

\noindent
While PDE solvers typically provide highly parallelizable and fast routines for moderate-length time series, they suffer from excessive memory usage that scales poorly to long or rough time series. In practice, users have reported that the PDE-based approaches can become infeasible beyond a few thousands or ten thousands of time-steps \cite{toth2025user}, particularly when GPU memory is limited. (In our testing on 4090 RTX GPUs, the length of time series that the existing algorithms can operate on is limited to $\sim 10^3$ for SigKernel \cite{salvi2021signature} and $~16 \times 10 ^4$ for KSig \cite{toth2025user}.) Dynamic programming remains an option for computing kernels of truncated signatures \cite{kiraly2019kernels}, although it likewise suffers from poor scalability. Recent efforts to reduce this cost through random Fourier features or other low-rank approximations \cite{toth2023random} often degrade in accuracy for larger time-series length or higher dimensionality.\\[-0.5em]

\noindent
By contrast, our work focuses on tilewise local expansions of the signature kernel, avoiding the need for a global $\mathcal{O}(\ell^2)$ storage footprint. Specifically, each tile of the full temporal domain of the signature kernel is handled with a Neumann-type integral expansion that converges rapidly, allowing for a truncated power-series representation of the kernel localized to that subregion. In this approach, which naturally lends itself to the integral-equation viewpoint on Volterra-type PDEs, we store only local series coefficients rather than a full two-dimensional array. This allows our \texttt{PowerSig} algorithm to compute signature kernels for time series of length of over $5 \times 10^5$ on a single GPU, while retaining superior accuracy. The principal appeal of our method thus lies in its memory efficiency and ability to scale to very long, high-dimensional time series where PDE or DP approaches risk overwhelming storage capacities. 

\section{Signature Kernels admit Recursive Neumann Series Expansions}\label{sec:methodology}
In this section, we detail how the signature kernel admits a local expansion in terms of tilewise Neumann series. After recalling the PDE characterisation \cite{salvi2021signature} of the signature kernel (Section \ref{subsect:signaturekernel}), we convert this formulation into an equivalent integral equation and show how its solution can then be recursively decomposed and computed tile by tile, with only boundary values exchanged between neighboring tiles (Section \ref{subsect:sigkernel-neumann}). This localized approach provides rapidly convergent power‐series expansions on each tile and enables an adaptive truncation scheme that balances accuracy with computational cost (Section \ref{sec:computing-neumann-coefficients}). By storing and propagating only the local series coefficients, we achieve significant memory savings compared to a global PDE or dynamic‐programming solver.

\subsection{The Signature Kernel}\label{subsect:signaturekernel}
Let $\bm{x}=(\bx_1, \ldots, \bx_{\ell_{\bx}})\subset\R^d$ and $\by= (\by_1, \ldots, \by_{\ell_{\by}})\subset\R^d$ be two given time series of length $\ell_{\bx}=\ell_{\by}\eqqcolon \ell\in\N$. (If $\ell_{\bm{x}}\neq\ell_{\bm{y}}$, one may simply pad the shorter time series by repeating its final entry.) For any such time series $\bm{z}\coloneqq(\bm{z}_1,\ldots, \bm{z}_\ell)$, define the \emph{affine interpolant} $\hat{\bm{z}} : [0,1]\rightarrow\R^d$ by
\begin{equation*}
\hat{\bm{z}}(t)\coloneqq \bm{z}_\nu + \left(\ell - 1\right)\left(t - \tfrac{\nu-1}{\ell-1}\right)\Delta_\nu^{\bm{z}}, \quad t\in\left[\tfrac{\nu-1}{\ell-1}, \tfrac{\nu}{\ell-1}\right], \quad(\nu=1, \ldots, \ell-1),
\end{equation*}
where $\Delta_\nu^{\bm{z}}\coloneqq\bm{z}_{\nu+1} - \bm{z}_{\nu}$. (This is just the unique, continuous, piecewise linear function interpolating the points $\bm{z}_1,\ldots, \bm{z}_\ell$.) The derivative of $\hat{\bm{z}}$ (defined almost everywhere) is given by
\begin{equation}\label{method:lininterpderiv}
\hat{\bm{z}}'(t)\coloneqq \left(\ell-1 \right)\Delta_\nu^{\bm{z}}, \quad t\in\left(\tfrac{\nu-1}{\ell-1}, \tfrac{\nu}{\ell-1}\right).
\end{equation}
For analytical convenience, we partition the unit square $[0,1]^2$ into \emph{tiles} as follows: define
\begin{equation}
\sigma_\mu\coloneqq\frac{\mu-1}{\ell-1}\quad\text{and}\quad\tau_\nu\coloneqq\frac{\nu-1}{\ell-1} \quad \left(\mu,\nu=1,\dots,\ell\right),
\end{equation}
then the open tiles and their closures (in the Euclidean topology on $\R^2$) are
\begin{equation}\label{eqn:tile}
\begin{gathered}
\mathcal{D}_{\mu,\nu}\!\coloneqq\!\left(\sigma_\mu, \sigma_{\mu+1}\right)\!\times\!\left(\tau_\nu, \tau_{\nu+1}\right) \quad\text{and}\quad T_{\mu,\nu}\coloneqq \overline{\mathcal{D}_{\mu,\nu}}=\left[\sigma_\mu, \sigma_{\mu+1}\right]\!\times\!\left[\tau_\nu, \tau_{\nu+1}\right],\\
\text{and the tiled domain is}\quad \mathcal{D}\coloneqq\!\!\bigcup_{\mu,\nu=1}^{\ell-1}\mathcal{D}_{\mu,\nu}\, \quad \text{with boundary} \quad \partial \mathcal{D} = [0,1]^2\setminus\mathcal{D}.
\end{gathered}
\end{equation}
Finally, $\langle\cdot, \cdot\rangle$ denotes the Euclidean inner product on $\R^d$. Building on the PDE characterization in \citep[Theorem 2.5]{salvi2021signature}, we can introduce\footnote{This avoids the need to explicitly define the signature kernel of $\bx$ and $\by$ as the inner product of their signature transforms, as originally conceived in \cite{kiraly2019kernels, salvi2021signature}.} our main object of interest as follows. 

\begin{definition}[Signature Kernel]\label{def:signaturekernel}The \emph{signature kernel} of $\bx$ and $\by$ is the unique continuous solution $K \equiv K_{\bx, \by} : [0,1]^2\rightarrow\R$ of the hyperbolic boundary value (Goursat) problem
\begin{equation}\label{eqn:GoursatPDE}
\left\{
\begin{aligned}
\frac{\partial^2 K(s,t)}{\partial s \partial t} &= \rho_{\bx, \by}(s,t)K(s,t), \quad (s,t)\in\mathcal{D},\\
K(0,\cdot) &= K(\cdot,0) = 1,
\end{aligned}
\right.
\end{equation}
where the coefficient function $\rho_{\bx, \by} : \mathcal{D}\rightarrow \R$ is defined tilewise by
\begin{equation}\label{method:rho}
\begin{aligned}
\rho_{\bx, \by}(s,t)\coloneqq \langle\hat{\bx}'(s), \hat{\by}'(t)\rangle, \quad(s,t)\in\mathcal{D}_{\mu,\nu}.   
\end{aligned}
\end{equation}
\end{definition}

An equivalent formulation defines $\rho_{\bx,\by}$ on all of $[0,1]^2$ via 
\begin{equation}\label{eqn:rho-all-tiles}
\rho_{\bx,\by} : [0,1]^2\ni(s,t)\mapsto(\ell-1)^2\!\sum_{\mu,\nu=1}^\ell\!\rho_{\mu,\nu}\mathbbm{1}_{\mathcal{D}_{\mu,\nu}}(s,t), \quad \rho_{\mu,\nu}\coloneqq \Delta_\nu^{\bm{x}}\Delta_\mu^{\bm{y}}. 
\end{equation}
In integral form, the boundary value problem \eqref{eqn:GoursatPDE} is then equivalent to the Volterra-type equation
\begin{equation}\label{eqn:Volterra}
K(s,t) = 1 + \int_{0}^{t}\!\!\int_{0}^{s}\rho_{\bm{x},\bm{y}}(u,v)\,K(u,v)\,\mathrm{d}u\,\mathrm{d}v, \quad(s,t)\in[0,1]^2.
\end{equation}
Standard fixed-point arguments, see Appendix \ref{sect:proofs}, then guarantee that \eqref{eqn:Volterra} has a unique solution in $C([0,1]^2)$, leading to the following well-known result (provided here for the reader's convenience). 

\begin{proposition}\label{prop:sigkernel-welldefined}
The Goursat problem \eqref{eqn:GoursatPDE} admits a unique solution in $C([0,1]^2)$; hence, the signature kernel of $\bx$ and $\by$ is well-defined.
\end{proposition}

The advantage of the Volterra formulation \eqref{eqn:Volterra} is that it naturally leads to a recursive series expansion for the signature kernel $K$, as we will now explain. A key step towards this observation is the following lemma, which also underlies the proof of Proposition \ref{prop:sigkernel-welldefined}:

\begin{lemma}\label{lem:spectralradius}
For any bounded, measurable function $\varrho: I\rightarrow\R$ defined on a closed rectangle $I\equiv[a_1,b_1]\times[a_2,b_2]\subseteq[0,1]^2$, the integral operator $\bm{T}_{\varrho} : (C(I),\|\cdot\|_{\infty})\rightarrow (C(I),\|\cdot\|_{\infty})$ given by
\begin{equation}\label{lem:spectralradius:eq1}
\bm{T}_{\varrho}f (s,t) = \int_{a_2}^{t}\!\int_{a_1}^{s}\!\varrho(u,v)\,f(u,v)\,\mathrm{d}u\,\mathrm{d}v, \quad (s,t)\in I,
\end{equation}
has spectral radius zero, that is, $r(\bm{T}_\varrho)\coloneqq \sup_{\lambda \in \sigma(\bm{T}_\varrho)} |\lambda| =0$.
\end{lemma}
This fact will justify the use of Neumann series when inverting operators of the form $\mathrm{id} - \bm{T}_\varrho$.

\subsection{A Recursive Local Power Series Expansion of the Signature Kernel}\label{subsect:sigkernel-neumann}
Our main computational strategy is to approximate the signature kernel $K$ by a directed family of local power series expansions that is constructed recursively over the tiles \eqref{eqn:tile}. Akin to the Adomian Decomposition Method (ADM) \cite{adomian1984new, adomian2013solving, wazwaz1995decomposition, ahmad2015exact, birem2024goursat}, the key idea is to establish a representation
\begin{equation}\label{eqn:adomian}
K(s,t) = \sum_{\alpha\in\N_0^2} K_\alpha(s,t)
\end{equation}
for some easier-to-compute functions $K_\alpha : [0,1]^2\rightarrow\R$. In our case, these $K_\alpha$ will be of the form 
\begin{equation}\label{eqn:adomian-series-tilewise}
K_\alpha(s,t) = \sum_{\mu,\nu=1}^{\ell-1}\mathbbm{1}_{\hat{T}_{\mu,\nu}\!}(s,t)\,c_{\mu,\nu}^{(\alpha)}\,(s-\sigma_\mu)^{\alpha_1}(t-\tau_\nu)^{\alpha_2},
\end{equation}
where the `half-open' tiles $\hat{T}_{\mu,\nu}\coloneqq\big[\sigma_\mu, \sigma_{\mu+1}\big)\!\times\!\big[\tau_\nu, \tau_{\nu+1}\big)$ and $\hat{T}_{\tilde{\mu},\ell-1}\coloneqq T_{\tilde{\mu},\ell-1}, \hat{T}_{\ell-1,\tilde{\nu}}\coloneqq T_{\ell-1,\tilde{\nu}}$ partition the domain $[0,1]^2$, and the tile-dependent sequences of coefficients   
\begin{equation}\label{eqn:series-coeffs}
c_{\mu,\nu}\coloneqq\big(c_{\mu,\nu}^{(\alpha)} \,\big|\, \alpha\in\N_0^2\big)\in\ell_1(\N_0^2)
\end{equation}
are summable and (for $(\mu,\nu)\neq (1,1)$) are recursively defined by
\begin{equation}\label{eqn:series-coeffs_general-recursion}
c_{\mu,\nu} = \phi_{\mu,\nu}\big(c_{\mu,\nu-1}, c_{\mu-1,\nu}\big)\, \quad\text{for maps}\quad \phi_{\mu,\nu} : \ell_1(\N_0^2)^{\times 2}\rightarrow\ell_1(\N_0^2)
\end{equation}
(here, each map $\phi_{\mu,1}$ [resp.\ $\phi_{1,\nu}$] depends only on its second [resp.\ first] argument).\\[-0.5em] 

\noindent
The recursion \eqref{eqn:series-coeffs_general-recursion} is organized over the $(\ell-1)^2$ tiles, with the idea being to compute the series coefficients \eqref{eqn:series-coeffs} on each tile from the boundary data provided by already-computed adjacent tiles. We describe this procedure in Sections  \ref{sect:ADM-singletile} and \ref{sect:ADM-multitile}, beginning with the bottom-left (first) tile.\\[-0.5em]

\noindent
The aim of the full scheme is twofold: (a) to choose the coefficients \eqref{eqn:series-coeffs} so that the series \eqref{eqn:adomian-series-tilewise} converge rapidly on each tile, providing a power series representation of each single-tile function
\begin{equation}\label{eqn:kappas}
\kappa_{\mu,\nu}\coloneqq\left.K\right|_{T_{\mu,\nu}} \,:\, [\sigma_\mu,\sigma_{\mu+1}]\times[\tau_\nu,\tau_{\nu+1}] \ni (s,t)\longmapsto K(s,t)\in\R,
\end{equation}
and (b) to truncate the respective series expansions of \eqref{eqn:kappas} on each tile in a manner that ensures a reliable and numerically efficient global approximation of the whole kernel $K$.

\subsubsection{Fast-Converging Power Series Expansion for the First Tile}\label{sect:ADM-singletile}
On the first tile $T_{1,1}= [0,\sigma_2]\times[0,\tau_2]$, adopting \eqref{eqn:adomian},  \eqref{eqn:adomian-series-tilewise} and \eqref{eqn:series-coeffs} as an ansatz, assume\footnote{This is an assumption only for the moment -- we will establish \eqref{eqn:adomian_first-tile} as a provable identity in Lemma \ref{lem:single-tile-solution}.} that
\begin{equation}\label{eqn:adomian_first-tile}
\kappa_{1,1}(s,t) = \sum_{\alpha\in\N_0^2}K_\alpha(s,t) \quad\text{where}\quad K_{(\alpha_1,\alpha_2)}(s,t)=c_{1,1}^{(\alpha)}s^{\alpha_1}t^{\alpha_2}
\end{equation} 
with $\sum_\alpha|c_{1,1}^{(\alpha)}| < \infty$ and such that only the diagonal coefficients (with $\alpha_1=\alpha_2$) of $(c_{1,1}^{(\alpha)})$ are nonzero. Then $\int_{T_{1,1}}\!\sum_{\alpha}|K_\alpha(w)|\,\mathrm{d}w <\infty$, and so \eqref{eqn:Volterra} (via Fubini) implies that, pointwise on \ $T_{1,1}$,
\begin{equation}\label{eqn:coefficients_first-tile0}
\sum_{\alpha\in\N_0^2}K_\alpha = \sum_{\alpha\in\N_0^2}\tilde{K}_\alpha \quad\text{with} \quad \tilde{K}_\alpha(s,t)\coloneqq\int_0^t\!\!\int_0^s\!\rho_{\bm{x},\bm{y}}(u,v)\,K_{\alpha-(1,1)}(u,v)\,\mathrm{d}u\,\mathrm{d}v
\end{equation}
and for $\tilde{K}_{0,0}\equiv 1$. Since $\left.\rho_{\bm{x},\bm{y}}\right|_{T_{1,1}}\!\equiv\rho_{1,1}$ (see \eqref{eqn:rho-all-tiles}), the identity \eqref{eqn:coefficients_first-tile0} implies that
\begin{equation}
\tilde{K}_\alpha(s,t) = \begin{cases}
\frac{\rho_{1,1}^{\alpha_1}}{(\alpha_1!)^2}\cdot (s,t)^\alpha & \text{if } \alpha\in\big\{(\alpha_1,\alpha_2)\in\N_0^2 \mid \alpha_1=\alpha_2\big\}, \\
\hspace{2.4em}0 & \text{if } \alpha\in\big\{(\alpha_1,\alpha_2)\in\N_0^2 \mid \alpha_1\neq\alpha_2\big\},
\end{cases}
\end{equation}
pointwise on $T_{1,1}$, as is verified easily from the definition of $\tilde{K}_{\alpha}$ by induction. Then \eqref{eqn:coefficients_first-tile0}, via the identity theorem for power series, implies that the desired coefficients $(c^{(\alpha)}_{1,1}\mid \alpha\in\N^2_0)$ must read
\begin{equation}\label{eqn:coeffs-first-tile}
c^{(i,j)}_{1,1} = \frac{\rho_{1,1}^{i}}{(i!)^2}\cdot\delta_{i,j} \qquad\big((i,j)\in\N_0^2\big).
\end{equation}
So on the first tile, the decomposition ansatz \eqref{eqn:adomian_first-tile} brings the following (well-known) result.
\begin{lemma}\label{lem:single-tile-solution}
On the first tile $T_{1,1}$, the signature kernel \eqref{eqn:Volterra} has the form:
\begin{equation}\label{lem:single-tile-solution:eq1}
K(s,t) = \sum_{i=0}^\infty \frac{\rho_{1,1}^i}{(i!)^2}s^it^i = \begin{cases} J_0\left(2\sqrt{|\rho_{1,1}|\,s\,t}\right), &\rho_{1,1} < 0,\\ 
I_0\left(2\sqrt{\rho_{1,1}\,s\,t}\right), &\rho_{1,1}\geq 0,
\end{cases}\quad\text{uniformly in } \ (s,t)\in T_{1,1},   
\end{equation}
where $J_0$ and $I_0$ are the ordinary and modified Bessel functions of the first kind of order $0$, resp. 
\end{lemma}
\noindent
Notice the very rapid convergence (of order $\mathcal{O}((n!)^{-2})$ in the truncation order $n$), making the power series representation \eqref{lem:single-tile-solution:eq1} very attractive for numerical approximations of the $T_{1,1}$-localization $\kappa_{1,1}$ of $K$, particularly when $|\rho_{1,1}|$ is of moderate size (larger $|\rho_{1,1}|$ require higher truncation orders).

\subsubsection{Recursive Neumann Series for Propagating the Signature Kernel Across Tiles}\label{sect:ADM-multitile}
The recursion \eqref{eqn:series-coeffs} for computing the local power series coefficients $c_{\mu,\nu}\equiv(c_{\mu,\nu}^{(\alpha)})\in \ell_1(\N^2_0)$ for the remaining tiles starts with the base coefficients \eqref{eqn:coeffs-first-tile} on the initial tile $T_{1,1}$ and reads as stated below.

For $\mu,\nu=1,\ldots, \ell-1$, define the operators $\bm{T}_{\mu,\nu} : C(T_{\mu,\nu})\rightarrow C(T_{\mu,\nu})$ (cf.\ Lemma \ref{lem:spectralradius}) by 
\begin{align}\label{eqn:integral-decomposition-operator}
(\bm{T}_{\mu,\nu}f)(s,t) = \int_{\tau_\nu}^{t}\!\int_{\sigma_\mu}^{s}\!\rho_{\mu,\nu}f(u,v)\,\mathrm{d}u\,\mathrm{d}v, \quad (s,t)\in T_{\mu,\nu},
\end{align}
as well as the intervals $T_{0,\nu}\coloneqq\{0\}\times[\tau_\nu,\tau_{\nu+1}]$ and $T_{\mu,0}\coloneqq[\sigma_\mu,\sigma_{\mu+1}]\times\{0\}$.

\begin{proposition}\label{prop:recursion}
For each $\mu,\nu=1,\ldots, \ell-1$, the restricted kernel $\kappa_{\mu,\nu}= \left.K\right|_{T_{\mu,\nu}}$ from \eqref{eqn:kappas} satisfies
 \begin{equation}\label{eqn:recursion-on-tiles}
\kappa_{\mu,\nu} = \sum_{k=0}^\infty\bm{T}_{\mu,\nu}^k\big(\kappa_{\mu-1,\nu}^{(\sigma_\mu,\,\cdot\,)} + \kappa_{\mu,\nu-1}^{(\,\cdot\,,\tau_\nu)} - \kappa_{\mu-1,\nu-1}^{(\sigma_\mu,\tau_\nu)}\big) \quad\text{uniformly on \ $T_{\mu,\nu}$},
\end{equation}
for $\kappa_{\mu,\nu}^{(\sigma,\tau)}\equiv\kappa_{\mu,\nu}(\sigma,\tau)$ and the `curried' functions $\kappa_{\mu,\nu}^{(\sigma,\,\cdot\,)} : T_{\mu,\nu}\ni (u,v)\mapsto \kappa_{\mu,\nu}(\sigma,v)$ and $\kappa_{\mu,\nu}^{(\,\cdot\,,\tau)} : T_{\mu,\nu}\ni (u,v)\mapsto \kappa_{\mu,\nu}(u,\tau)$, where $\kappa_{0,\nu}\coloneqq\left.K\right|_{T_{0,\nu}}$ and $\kappa_{\mu,0}\coloneqq\left.K\right|_{T_{\mu,0}}$ and $\kappa_{0,0}\equiv K(0,0)=1$.
\end{proposition}
\begin{proof}
Let us note first that any point $(s,t)\in[0,1]^2$ can be decomposed as
\begin{equation}
(s,t) = (\tilde{s}, \tilde{t}) + (\sigma_{\mu(s)}, \tau_{\nu(t)})
\end{equation}
with $(\tilde{s},\tilde{t})\in\hat{T}_{1,1}$ and position indices $(\mu(s), \nu(t))\coloneqq (\lfloor s(\ell-1)\rfloor + 1, \lfloor t(\ell-1)\rfloor + 1)\in\{1,\ldots,\ell\}^2$
determined by the location of $(s,t)$ within the tiling $(\hat{T}_{\mu,\nu})$. Hence, and by the Volterra identity \eqref{eqn:Volterra},
\begin{equation}\label{eqn:integral-decomposition}
K(s,t) = K(\sigma_{\mu(s)}, t) + K(s, \tau_{\nu(t)}) - K(\sigma_{\mu(s)}, \tau_{\nu(t)}) + \int_{\tau_{\nu(t)}}^{t}\!\int_{\sigma_{\mu(s)}}^{s}\!\!\rho_{\mu(s),\nu(t)}K(u,v)\,\mathrm{d}u\,\mathrm{d}v
\end{equation}
for all $(s,t)\in[0,1]^2$. Defining the `boundary maps' $\gamma_{\mu,\nu} : T_{\mu,\nu}\rightarrow\R$ ($\mu,\nu=1,\ldots,\ell-1$) by 
\begin{align}\label{eqn:boundary-map}
\gamma_{\mu,\nu}(s,t)= K(\sigma_{\mu}, t) + K(s, \tau_{\nu}) - K(\sigma_{\mu}, \tau_{\nu}), \quad (s,t)\in T_{\mu,\nu},
\end{align}
the identity \eqref{eqn:integral-decomposition} can be expressed as 
\begin{equation}\label{eqn:integral-decomposition-2}
K(s,t) = \gamma_{\mu(s),\nu(t)}(s,t) + (\bm{T}_{\mu(s),\nu(t)}\kappa_{\mu(s),\nu(t)})(s,t), \quad (s,t)\in[0,1]^2.
\end{equation}
Since for each $(s,t)\in\hat{T}_{\mu,\nu}$, it holds that $(\mu(s),\nu(t)) = (\mu,\nu)$, equation \eqref{eqn:integral-decomposition-2} further is equivalent to the following $(\mu,\nu)$-indexed system of identities in $C(T_{\mu,\nu})$,
\begin{equation}\label{eqn:integral-decomposition-3}
(\mathrm{id} - \bm{T}_{\mu,\nu})\kappa_{\mu,\nu} = \gamma_{\mu,\nu} \quad(\mu,\nu=1,\ldots,\ell-1).
\end{equation}
From Lemma \ref{lem:spectralradius} and basic operator theory (cf.\ \citep[Thm.\ 2.9]{sasane2017friendly}), we know the identities \eqref{eqn:integral-decomposition-3} are invertible for $\kappa_{\mu,\nu}$ and the respective inverse operators can be written as a Neumann series in $T_{\mu,\nu}$,
\begin{equation}\label{eqn:kappa-Neumann}
\kappa_{\mu,\nu} = (\mathrm{id} - \bm{T}_{\mu,\nu})^{-1}\gamma_{\mu,\nu} = \sum_{k=0}^\infty\bm{T}_{\mu,\nu}^k\gamma_{\mu,\nu},
\end{equation}
where the above series converges wrt.\ $\|\cdot\|_{\infty;T_{\mu,\nu}}$, the sup-norm on $C(T_{\mu,\nu})$.\\[-0.5em] 

\noindent
For the recursive structure of the $\kappa_{\mu,\nu}$-identities \eqref{eqn:kappa-Neumann}, note that, for any fixed $(s,t)\in T_{\mu,\nu}$, we have $(\sigma_\mu,t)\in T_{\mu-1,\nu}\cap T_{\mu,\nu}$ and $(s,\tau_\nu)\in T_{\mu,\nu}\cap T_{\mu,\nu-1}$, where by definition $T_{0,\nu}=\{0\}\times[\tau_\nu,\tau_{\nu+1}]$ and $T_{\mu,0}=[\sigma_\nu,\sigma_{\nu+1}]\times\{0\}$ and $T_{0,0}\coloneqq\{(0,0)\}$. Consequently, the boundary map \eqref{eqn:boundary-map} evaluates to
\begin{equation}\label{eqn:gamma-recursive}
\gamma_{\mu,\nu}(s,t) = \kappa_{\mu-1,\nu}(\sigma_\mu,t) + \kappa_{\mu,\nu-1}(s,\tau_\nu) - \kappa_{\mu-1,\nu-1}(\sigma_\mu,\tau_\nu), \quad(s,t)\in T_{\mu,\nu},
\end{equation}
with $\kappa_{0,\nu}\coloneqq\left.K\right|_{T_{0,\nu}}$, $\kappa_{\mu,0}\coloneqq\left.K\right|_{T_{\mu,0}}$, $\kappa_{0,0}\equiv 1$. Combining \eqref{eqn:gamma-recursive} and \eqref{eqn:kappa-Neumann} proves \eqref{eqn:recursion-on-tiles}. 
\end{proof}

\noindent
The identities \eqref{eqn:recursion-on-tiles} show that the signature kernel admits the desired local (i.e., tilewise) power series representation \eqref{eqn:adomian}\&\eqref{eqn:adomian-series-tilewise} with coefficients \eqref{eqn:series-coeffs} that, for a given tile, can be determined recursively from those on the tiles immediately to the left and below. The following example illustrates this.

\begin{example}[Computing the Recursion \eqref{eqn:recursion-on-tiles} for $T_{1,1}$ and $T_{1,2}$ and $T_{2,1}$]\label{example1} Let us compute \eqref{eqn:recursion-on-tiles} for $(\mu,\nu)=(1,1), (1,2), (2,1)$, solving for $K$ on the border tiles around the bottom-left corner tile $T_{1,1}$. %(see Figure \hl{...})% 
Recalling that $\sigma_i=\tau_i=\frac{i-1}{\ell-1}$ and $\kappa_{0,i}=\kappa_{i,0}=\kappa_{0,0}\equiv 1$, it holds by \eqref{eqn:recursion-on-tiles} that
\begin{equation}\label{example1:eq1}
\kappa_{1,1}= \sum_{k=0}^\infty\bm{T}_{1,1}^k 1 + \bm{T}_{1,1}^k1 - \bm{T}_{1,1}^k1 = \sum_{k=0}^\infty\bm{T}^k_{1,1}1\quad\text{uniformly on } \ T_{1,1},
\end{equation}
where the Volterra-type operator $\bm{T}_{1,1} : f\longmapsto \left[(s,t)\mapsto\int_{0}^{t}\!\int_{0}^{s}\!\rho_{1,1}f(u,v)\,\mathrm{d}u\,\mathrm{d}v\right]$ is applied repeatedly. Since $(\bm{T}_{1,1}^k 1)(s,t)=\frac{(\rho_{1,1}st)^k}{(k!)^2}$ for each $k\in\N_0$ (as can be readily verified by induction), the recursive formula \eqref{example1:eq1} precisely recovers the previous series expansion \eqref{lem:single-tile-solution:eq1}. On $T_{1,2}$, \eqref{eqn:recursion-on-tiles} yields
\begin{align*}
\kappa_{1,2}(s,t) &= \sum_{k=0}^\infty\left[\bm{T}_{1,2}^k\kappa_{0,2}^{(\sigma_1,t)} + \bm{T}_{1,2}^k\kappa_{1,1}^{(s,\tau_2)} - \bm{T}_{1,2}^k\kappa_{0,1}^{(\sigma_1,\tau_2)}\right]\!(s,t)\\
&= \sum_{k=0}^\infty\left[\bm{T}_{1,2}^k 1 + \sum_{l=0}^\infty\frac{\rho_{1,1}^l\tau_2^l}{(l!)^2}\bm{T}_{1,2}^k\tilde{s}^l - \bm{T}_{1,2}^k 1\right]\!\!(s,t) = \sum_{k,l=0}^\infty\frac{\rho_{1,1}^l\rho_{1,2}^k}{(k+l)!l!(\ell-1)^l}s^{k+l}(t-\tau_2)^k,
\end{align*}
where for the last equality we used the definition of $\tau_2$ and that, for each iteration index $k\in\N_0$,
\begin{equation}\label{eqn:tiled-operator-powers}
(\bm{T}_{1,2}^k\tilde{s}^l)(s,t) = \rho_{1,2}\int_{\tau_2}^t\!\int_{0}^s\!(\bm{T}^{k-1}\tilde{s}^l)(u,v)\,\mathrm{d}u\,\mathrm{d}v = \frac{\rho_{1,2}^k}{(l+1)^{\bar{k}}k!}s^{l+k}(t-\tau_2)^k
\end{equation}
for $(x)^{\bar{k}}\coloneqq\prod_{i=0}^{k-1}(x+i)$, as one verifies immediately (Lemma \ref{lem:tiled-operator-powers}). Analogous computations show
\begin{equation}
\kappa_{2,1}(s,t) = \sum_{k,l=0}^\infty\frac{\rho_{1,1}^l\rho_{2,1}^k}{(k+l)!l!(\ell-1)^l}(s - \sigma_2)^kt^{k+l}
\end{equation}
uniformly in $(s,t)\in T_{2,1}$. \hfill $\bm{\diamond}$
\end{example}

Let us encapsulate the observation \eqref{eqn:tiled-operator-powers} into a small lemma for later use. 

\begin{lemma}\label{lem:tiled-operator-powers}
Let $\varphi_\sigma^{(l)} : [0,1]^2\ni (s,t)\mapsto (s-\sigma)^l$ and $\varphi_\tau^{(l)} : [0,1]^2\ni (s,t)\mapsto (t-\tau)^l$, for any given $l\in\N_0$. Then for each $k\in\N_0$ and all $\mu,\nu\in\{1,\ldots,\ell-1\}$, we have that
\begin{equation}
\begin{aligned}
\bm{T}_{\mu,\nu}^k\big(\varphi_{\sigma_\mu}^{(l)}\big|_{T_{\mu,\nu}}\big)(s,t) &= \frac{\rho_{\mu,\nu}^k}{(l+1)^{\bar{k}}k!}(s-\sigma_\mu)^{l+k}(t-\tau_\nu)^k, \quad(s,t)\in T_{\mu,\nu}, \quad\text{and}\\ \bm{T}_{\mu,\nu}^k\big(\varphi_{\tau_\nu}^{(l)}\big|_{T_{\mu,\nu}}\big)(s,t) &= \frac{\rho_{\mu,\nu}^k}{(l+1)^{\bar{k}}k!}(s-\sigma_\mu)^{k}(t-\tau_\nu)^{l+k}, \quad(s,t)\in T_{\mu,\nu}.
\end{aligned}
\end{equation} 
\end{lemma}
\noindent
Given the recursion \eqref{eqn:recursion-on-tiles}, we require an explicit algorithm to extract the desired coefficients \eqref{eqn:series-coeffs}, which will then be used to construct the kernel approximations \eqref{eqn:adomian}. The next section provides this.

\subsection{Computing the Neumann Series Coefficients}\label{sec:computing-neumann-coefficients}
In order to algorithmically extract the coefficients \eqref{eqn:series-coeffs} of the power series expansion \eqref{eqn:adomian} from the Neumann series recursions \eqref{eqn:recursion-on-tiles}, we can capture the actions of the propagation operators \eqref{eqn:integral-decomposition-operator} on the monomial basis $\{s^i t^j\mid (i,j)\in\N_0^2\}\subset C(T_{\mu,\nu})$ with a straightforward Vandermonde scheme:\\[-0.5em] 

\noindent
Define the power map $\eta : [0,1]\rightarrow\ell_\infty(\N_0)$ by 
\begin{equation}
\eta(r) = (r^i\mid i\in\N_0) \equiv (1,r,r^2,r^3,\cdots),
\end{equation}
and, for any given $(c_{ij})_{i,j\geq 0}\in\ell_1(\N_0^2)$, the doubly-infinite matrices $C\in \mathscr{L}(\ell_\infty(\N_0),\ell_1(\N_0))$ by
\begin{equation}\label{eqn:coefficient-matrix}
C \equiv \big(c_{i,j}\big)_{\!ij\geq 0} \,:\, (a_j)_{j\geq 0} \longmapsto \left({\textstyle\sum}_{j\geq 0}c_{ij}a_j\right)_{i\geq 0}.
\end{equation}
By the Weierstrass $M$-test, each such matrix $C$ then defines a continuous function
\begin{equation}\label{eqn:matrixrepdfunction}
C_{\langle\sigma,\tau\rangle} \,:\, [0,1]^2\ni (s,t) \longmapsto \big\langle \eta(s), C\eta(t)\big\rangle= \textstyle{\sum}_{i,j\geq0}c_{ij}s^it^j\in\R, 
\end{equation}
where $\langle\cdot,\cdot\rangle$ is the dual pairing between $\ell_\infty(\N_0)$ and $\ell_1(\N_0)$. The localised kernels \eqref{eqn:kappas} can all be represented in the explicit form \eqref{eqn:matrixrepdfunction} for some recursively related coefficient matrices $C^{\mu,\nu}$.\\[-0.5em]

Here and throughout, $\delta_{i,j}\,(\equiv\mathbbm{1}_{\{j\}}(i))$ denotes the Kronecker delta.

\begin{proposition}\label{prop:coefficientrecursion}
For each $\mu,\nu\in\{1,\ldots, \ell-1\}$, there is $C^{\mu,\nu}\equiv\big(c_{\mu,\nu}^{i,j}\big)_{\!i,j\geq 0}\in\ell_1(\N_0^2)$ such that
\begin{equation}\label{prop:coefficientrecursion:eqn1}
\kappa_{\mu,\nu} = \left.C^{\mu,\nu}_{\langle\sigma,\tau\rangle}\right|_{T_{\mu,\nu}} \quad\text{and}\quad C^{\mu,\nu} = \sum_{k=0}^\infty C^{\mu,\nu}_{k}\quad\text{in } \ \ell_1(\N_0^2),
\end{equation}
with $\kappa_{\mu,\nu} = \lim_{m\rightarrow\infty}\!\big[\sum_{k=0}^mC^{\mu,\nu}_k\big]_{\!\langle\sigma,\tau\rangle}$ uniformly on $T_{\mu,\nu}$. Here, the sequence $C^{\mu,\nu}_0, C^{\mu,\nu}_1, C^{\mu,\nu}_2,\ldots\in\ell_1(\N_0^2)$ is defined recursively by 
\begin{equation}\label{prop:coefficientrecursion:eqn2}
C^{\mu,\nu}_{k+1} = \rho_{\mu,\nu}L_{\sigma_{\mu}} C^{\mu,\nu}_{k}R_{\tau_\nu} \quad(k\in\N_0)
\end{equation}
with the initial condition 
\begin{equation}\label{prop:coefficientrecursion:eqn3}
C^{\mu,\nu}_0 \coloneqq \big(\alpha_i\delta_{i0} + \beta_j\delta_{0j} - \gamma\delta_{0i}\cdot\delta_{0j}\big)_{\!i,j\geq 0}
\end{equation}
for $(\alpha_i)\coloneqq C^{\mu,\nu-1}\eta(\tau_\nu)$, $(\beta_i)\coloneqq\big(C^{\mu-1,\nu}\big)^{\!\dagger}\eta(\sigma_\mu)\in\ell_1(\N_0)$ and $\gamma\coloneqq\big\langle\eta(\sigma_\mu),C^{\mu-1,\nu}\eta(\tau_\nu)\big\rangle$ and the boundary conditions $C^{0,\iota} = C^{\iota,0} \coloneqq (\delta_{0i}\cdot\delta_{0j})_{i,j\geq 0}$ $(\forall\,\iota\in\N_0)$, and with the matrices
\begin{equation}
L_\sigma\coloneqq (I - H(\sigma))\underline{S} \quad\text{and}\quad R_\tau\coloneqq \underline{T}(I - G(\tau))
\end{equation}
defined by $I\coloneqq(\delta_{ij})_{i,j\geq 0}, H(\sigma)\coloneqq(\sigma^j\delta_{i0})_{i,j\geq 0}, G(\tau)\coloneqq(\tau^i\delta_{0j})_{i,j\geq 0}, \underline{S}\coloneqq{\!\!\!\!\!\!\footnotemark}\,\,\,\big(\tfrac{\delta_{i-1,j}}{i}\big)_{i,j\geq 0}, \underline{T}\coloneqq\big(\tfrac{\delta_{i,j-1}}{j}\big)_{i,j\geq 0}$.
\end{proposition}\footnotetext{Here and in the definition of $\underline{T}$, we adopt the convention $\tfrac{0}{0}\coloneqq 0$ for notational convenience.}
\noindent
Numerically, the primary cost driver of the Neumann iterations \eqref{prop:coefficientrecursion:eqn2} is the double integration $\int_{\tau_\nu}^\cdot\int_{\sigma_\mu}^\cdot$, which corresponds to the matrix multiplications $L_{\sigma_\mu}\!(\cdot)R_{\tau_\nu}$. The computational expense of these operations varies with the choice of center point $(s_\mu,t_\nu)\in[0,1]^2$ in the targeted power series representation
\begin{equation}
\kappa_{\mu,\nu}(s,t) = \sum_{i,j\geq 0}c_{\mu,\nu;(s_\mu,t_\nu)}^{i,j}(s-s_\mu)^i(t-t_\nu)^j \qquad \big((s,t)\in T_{\mu,\nu}\big)
\end{equation}
of the localised kernels \eqref{eqn:kappas}. Proposition \ref{prop:coefficientrecursion} established the case $(s_\mu, t_\nu)=(0,0)$ (i.e., $C^{\mu,\nu}=\big(c^{i,j}_{\mu,\nu,(0,0)}\big)_{i,j\geq 0}$), and the following complementary observation shows that this entails no loss of generality, as we can switch freely between coefficients corresponding to different center points via an inexpensive matrix-vector product. In particular, we can opt for the choices $(s_\mu, t_\nu)\coloneqq(\sigma_\mu,\tau_\nu)$ for each $\mu,\nu\in\{1,\ldots,\ell-1\}$ 
to significantly reduce the computational load of evaluating the Gram matrix $K(1,1) = \kappa_{\ell-1,\ell-1}(1,1)$ via the following routine, which updates the scheme \eqref{prop:coefficientrecursion:eqn1} and \eqref{prop:coefficientrecursion:eqn2}.

\begin{proposition}\label{prop:recursion-speedup}
For each $\mu,\nu\in\{1,\ldots, \ell-1\}$, the following holds:
\begin{enumerate}[label=\emph{(\roman*)}]
\item\label{prop:recursion-speedup:it1} Suppose that there are coefficients $(\alpha_i)_{i\geq 0}$ and $(\beta_j)_{j\geq 0}$ in $\R$ such that 
\begin{equation}\label{prop:recursion-speedup:eq1}
\kappa_{\mu,\nu}(s,\tau_\nu) = \sum_{i=0}^\infty \alpha_i(s - \sigma_{\mu})^i \quad\text{and}\quad \kappa_{\mu,\nu}(\sigma_\mu,t) = \sum_{j=0}^\infty \beta_j(t - \tau_{\nu})^j
\end{equation}
uniformly in $(s,t)\in T_{\mu,\nu}$. Then there is a coefficient matrix $\hat{C}\in\ell_1(\N^2_0)$ such that
\begin{equation}\label{prop:recursion-speedup:eq2}
\kappa_{\mu,\nu} = \left.\big\langle \eta(\,\cdot-\sigma_\mu), \hat{C}\eta(\,\cdot - \tau_\nu)\big\rangle\right|_{T_{\mu,\nu}} \quad\text{and}\quad \hat{C} = \sum_{k=0}^\infty\rho_{\mu,\nu}^k\underline{S}^k\hat{C}_0\underline{T}^k, 
\end{equation}
for the initial matrix $\hat{C}_0\coloneqq \big(\alpha_i\delta_{i0} + \beta_j\delta_{0j} - \kappa_{\mu-1,\nu-1}(\sigma_\mu,\tau_\nu)\delta_{0i}\cdot\delta_{0j}\big)_{\!i,j\geq 0}$. In particular, 
\begin{equation}\label{prop:recursion-speedup:eq3}
\kappa_{\mu,\nu}(s,\tau_{\nu+1}) = \sum_{i=0}^\infty \hat{\alpha}_i(s - \sigma_{\mu})^i \quad\text{and}\quad \kappa_{\mu,\nu}(\sigma_{\mu+1},t) = \sum_{j=0}^\infty \hat{\beta}_j(t - \tau_{\nu})^j 
\end{equation} 
uniformly on $T_{\mu,\nu}$, for the coefficient sequences $(\hat{\alpha}_i)_{i\geq 0}$ and $(\hat{\beta}_i)_{i\geq 0}$ given by
\begin{equation}\label{prop:recursion-speedup:eq4}
\begin{aligned}
\hat{\alpha}_0&\coloneqq \sum_{j= 0}^{\infty}\frac{\rho_{\mu,\nu}^j\beta_{j}(\tau_{\nu+1}-\tau_\nu)^j}{j!} &&\text{and}\quad \hat{\alpha}_i\coloneqq \sum_{j= 0}^{i}\frac{\rho_{\mu,\nu}^j\alpha_{i-j}(\tau_{\nu+1}-\tau_\nu)^j}{(i-j+1)^{\overline{j}}j!}+ \sum_{j= i}^{\infty}\frac{\rho_{\mu,\nu}^{j-i}\beta_{j-i}(\tau_{\nu+1}-\tau_\nu)^j}{(i+1)^{\overline{j-i}}(j-i)!}\\
\hat{\beta}_0&\coloneqq\sum_{i=0}^\infty\frac{\rho_{\mu,\nu}^i\alpha_i(\sigma_{\mu+1}-\sigma_\mu)^i}{i!}&&\text{and}\quad\hat{\beta}_j\coloneqq \sum_{i=0}^{j}\frac{\rho_{\mu,\nu}^i\beta_{j-i}(\sigma_{j}-\sigma_\mu)^{i}}{(j-i+1)^{\overline{i}}i!} + \sum_{i=j}^\infty\frac{\rho_{\mu,\nu}^{i-j}\alpha_{i-j}(\sigma_{\mu+1}-\sigma_\mu)^{i}}{(j+1)^{\overline{i-j}}(i-j)!}
\end{aligned}
\end{equation}
\item\label{prop:recursion-speedup:it2} We have the series representations, convergent uniformly in $(s,t)\in T_{\mu,\nu}$, 
\begin{equation}\label{prop:recursion-speedup:eq5}
\begin{aligned}
\kappa_{\mu,\nu}(s,\tau_{\nu+1}) &= \sum_{i=0}^\infty\doublehat{\alpha}_i(s-\sigma_{\mu+1})^i \quad\text{for the coefficients}\quad \doublehat{\alpha}_i\coloneqq\sum_{l=i}^\infty\hat{\alpha}_l\binom{l}{i}(\sigma_{\mu+1} - \sigma_\mu)^{l-i},\\
\kappa_{\mu,\nu}(\sigma_{\mu+1},t) &= \sum_{j=0}^\infty\doublehat{\beta}_j(t - t_{\nu+1})^j \quad\text{for the coefficients}\quad \doublehat{\beta}_j\coloneqq\sum_{l=j}^\infty\hat{\beta}_l\binom{l}{j}(\tau_{\nu+1} - \tau_\nu)^{l-j}.
\end{aligned}
\end{equation}
\end{enumerate}
\end{proposition}

\noindent
Proposition \ref{prop:recursion-speedup} suggests the following algorithmic refinement to Proposition \ref{prop:coefficientrecursion}: Provided that the boundary values $\kappa_{\mu,\nu}(\cdot,\tau_\nu)$ and $\kappa_{\mu,\nu}(\sigma_\mu,\cdot)$ of a localized kernel $\kappa_{\mu,\nu}$ can be expressed as power series in the form \eqref{prop:recursion-speedup:eq1}, the remaining kernel can be efficiently computed using \eqref{prop:recursion-speedup:eq2}. Moreover, this procedure extends naturally to the adjacent tiles $(T_{\mu,\nu+1}$ and $T_{\mu+1,\nu})$ via the coefficient transformations in \eqref{prop:recursion-speedup:eq4} and \eqref{prop:recursion-speedup:eq5}, mapping $(\alpha_i, \beta_i)$ to $(\hat{\alpha}_i, \hat{\beta}_i)$ and subsequently re-centering to $(\doublehat{\alpha}_i, \doublehat{\beta}_i)$. This ensures that the boundary contributions $\kappa_{\mu,\nu+1}(\cdot,\tau_{\nu+1}) \,(= \kappa_{\mu,\nu}(\cdot,\tau_{\nu+1}))$ and $\kappa_{\mu+1,\nu}(\sigma_{\mu+1},\cdot) \,(= \kappa_{\mu+1,\nu}(\sigma_{\mu+1},\cdot))$ remain in the required form \eqref{prop:recursion-speedup:eq1}. This way, one can compute all coefficients \eqref{prop:recursion-speedup:eq2} of the full kernel \eqref{eqn:Volterra} quickly across all tiles.  

\section{Numerical Experiments}\label{sec:experiments}
We present an empirical evaluation of our Proposed method (\texttt{PowerSig}) with respect to accuracy, memory usage, and runtime. We focus on computing the self-signature kernel of two-dimensional Brownian motion on $[0,1]$ at increasing resolutions. Specifically, time series of length $\ell = 2^k + 1$ are produced for $k \in \{0,1,2,\ldots\}$, limited only by available GPU memory. Comparisons are made to the state-of-the-art library $\texttt{KSig}$ \cite{toth2025user} that approximates the signature kernel either via truncated signature expansions or by solving the Goursat PDE with finite differences. All experiments were conducted on an NVIDIA RTX 4090 GPU (24 GB memory).\\[-0.5em]

For \texttt{KSig}'s truncated signature kernel we use a 21-level signature at order 1, and for \texttt{KSig}'s PDE-based solver we use its default dyadic order without further dyadic refinement. For \texttt{PowerSig}, we cap the local power series at truncation order 7 (though the code supports higher orders to achieve tighter tolerances). Accuracy is evaluated through the Mean Absolute Percentage Error (MAPE), memory usage is captured by recording the peak GPU memory allocated during computation, and for the runtime we measure total execution time in seconds to compute the kernel for each $\ell$.\\

The experimental results were as follows.

\paragraph{Accuracy.}
Figure~\ref{fig:mape} summarizes the mean absolute percentage error for paths of increasing length $\ell$. Even at a low polynomial truncation order (7 in our case), \texttt{PowerSig} achieves lower MAPE than \texttt{KSig}'s PDE-based solver for short-to-medium time series ($\ell \le 129$). At larger $\ell$ for which comparisons with \texttt{KSig} were still possible, the methods exhibit broadly comparable accuracy. 

It is worth noting that, even without dyadic refinement, FDM-based PDE solvers often see accuracy gains for large $\ell$ as step sizes decrease. Nevertheless, \texttt{PowerSig} maintains competitive accuracy for long paths. In practice, the choice of polynomial truncation order provides a convenient mechanism to balance accuracy and runtime requirements.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{mape_comparison.png} 
    \caption{%
       \emph{Mean Absolute Percentage Error (MAPE) comparison of \texttt{PowerSig} 
       vs.\ the PDE-based signature kernel solver in \texttt{KSig}. 
       The domain is two-dimensional Brownian motion on $[0,1]$ with increasing length $\ell$.}}
    \label{fig:mape}
\end{figure}

\paragraph{Memory Usage and Runtime.}
Beyond its strong accuracy, \texttt{PowerSig} displays substantially lower memory consumption than either PDE- or dynamic-programming-based approaches. As shown in Figure~\ref{fig:memory_and_duration}, \texttt{PowerSig} successfully computes the kernel for paths of length up to $\ell = 524\,289$ using well under 100\,MB of GPU memory, which is several orders of magnitude lower than for dynamic programming or PDE-based signature kernel libraries.

Although \texttt{PowerSig} and \texttt{KSig} share a worst-case time complexity of $O(\ell^2 d)$, the tilewise structure of \texttt{PowerSig} can lead to noticeable speed gains in practice for large $\ell$. This is partly due to the localized coefficient expansions avoiding the need to hold a large $\ell \times \ell$ grid in memory.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{memory_and_duration_comparison.png} 
    \caption{%
       \emph{Peak GPU memory usage (left axis) and runtime (right axis) for \texttt{PowerSig} 
       on 2D Brownian motion paths of increasing length $\ell$. 
       Even for $\ell = 5 \times 10^5$, memory usage of \texttt{PowerSig} remains below 100\,MB.}}
    \label{fig:memory_and_duration}
\end{figure}

\subsection{Time Complexity}
\label{sect:time_complexity}
Time complexity has already been well studied in \cite{toth2025user}, so we only briefly note that \texttt{PowerSig} has an identical runtime complexity of $\mathcal{O}(\ell^2d)$ and a lower complexity of $\mathcal{O}(\ell Pd)$ than \texttt{KSig} in space as it only maintains a single diagonal of coefficient blocks in memory. Here, $P$ denotes the polynomial truncation order, which is typically much smaller than $\ell$ in real-world settings, and $d$ is the spatial dimension of the input time series. This characteristic enables \texttt{PowerSig} to handle path lengths at scales not previously tractable under the conventional PDE or dynamic-programming paradigms.

\section{Conclusion}
\label{sec:conclusion}

We have introduced \texttt{PowerSig}, a new method for computing signature kernels of piecewise-linear time series via localized Neumann-series expansions. By partitioning the entire kernel domain $[0,1]^2$ into smaller tiles and propagating boundary data through a directed graph, \texttt{PowerSig} achieves reduced memory usage and high parallelizability. Its adaptive truncation feature ensures that each local series is extended only as far as necessary to meet precision requirements, mitigating computational overhead while preserving accuracy. Our experimental results suggest that \texttt{PowerSig} outperforms or matches existing DP- and PDE-based solvers in terms of accuracy on moderate-length paths, and crucially enables signature kernel computations on paths with hundreds of thousands of points while retaining modest memory usage. These advantages underscore the method’s suitability for large-scale time-series data in applications such as financial modeling, signal processing, and machine learning. Future research directions include refining computation of coefficients on the tile boundaries, studying and refining the interplay between truncation order and the structured ability to handle rougher paths, and the provision of rigorous error bounds on partial series expansions under regularity assumptions.\\[-0.5em]

For practitioners, \texttt{PowerSig} is implemented as an open-source, PyTorch-based, GPU-accelerated library designed for computing signature kernels on very long input time series with high accuracy, on GPUs and ARM devices with neural processors. Our source code and documentation for \texttt{PowerSig} are publicly available at:
\[
\href{https://github.com/geekbeast/powersig}{\texttt{https://github.com/geekbeast/powersig}}
\]

\begin{comment}
We evaluate the memory footprint, runtime and accuracy of the algorithms in computing the self-signature kernel of 2-dimensional Brownian Motion over [0, 1] with exponentially increasing lengths as supported by the hardware. The experiments were carried out on an NVIDIA 4090 RTX GPU with a memory capacity of 24GB. For \texttt{KSig}'s truncated signature kernel we use a 21-level order 1 signature and for \texttt{KSig}'s PDE implementation we do not use any dyadic refinement.

In our experiments, even with using an order 7 polynomial, \texttt{PowerSig}'s mean absolute percentage error was significantly lower than \texttt{KSig PDE} for time series shorter than 129 in length and thereafter remained reasonably close for lengths where a comparison was possible. It is worth noting even without dyadic refinement FDM-based PDE solvers become more accurate for longer time series with reduced step size.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{arxiv_v1_2025/mape_comparison.png} 
    \caption{\emph{MAPE comparison between \texttt{KSig} PDE-solver based implementation and \texttt{PowerSig}}}
    \label{fig:mape}
\end{figure}

We also observed that \texttt{PowerSig} was able to evaluate the signature kernel on paths of length 524289 using under 100 MB of memory, which is several orders of magnitude longer than dynamic programming and PDE-based signature kernel algorithms.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{arxiv_v1_2025/memory_and_duration_comparison.png} 
    \caption{GPU Memory Usage and Runtime for 2-dimensional Brownian motion}
    \label{fig:memory_and_duration}
\end{figure}
\vfill
\subsection{Time Complexity}
Time complexity has already been well studied in \cite{toth2025user}, so we only briefly note that PowerSig has an identical complexity of $O(L^2d)$ to KSig in time and lower complexity of $O(LPd^2)$ to KSig in space as it only maintains a single in memory. Here $L$ is the length of the time series, $P$ is the truncation order of the power series approximation, and $d$ is the dimension. 

\section{Conclusion}
\texttt{PowerSig} provides an open-source PyTorch based GPU-accelerated library for computing very long signature kernels with high accuracy on GPUs and ARM devices with neural processors. Future directions of research here include refining computation of coefficients on the boundary, studying the interplay between truncation order and the ability to handle rougher paths. Community contributions and feedback to further refine \texttt{PowerSig} are welcome at our public Github repo: \href{https://github.com/geekbeast/powersig}{https://github.com/geekbeast/powersig}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography style
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{example_paper.bib}

\appendix

\section{Mathematical Proofs}\label{sect:proofs}

\begin{proof}[Proof of Lemma \ref{lem:spectralradius}]
By definition of $r(\bm{T}_{\varrho})$, we need to show that the spectrum of $\sigma(\bm{T}_\varrho)$ is $\sigma(\bm{T}_{\varrho}) = \{0\},$ i.e., that zero is the only element in the spectrum of $\bm{T}_{\varrho}.$ This is equivalent to establishing that for all $\lambda \in \mathbb{C} \backslash \{0\},$
the operator $\lambda \, \mathrm{id} - \bm{T}_{\varrho}$ is a bijection with a bounded inverse. Here,  $\mathrm{id}$ denotes the identity on $C([0,1]^2)$. To show this, we note that it suffices to prove that 
$$
\lambda \, \mathrm{id} - \bm{T}_{\varrho}: C(I) \to C(I)
$$
is a bijection: since $C(I)$ is a Banach space, the inverse mapping theorem implies that in this case, the inverse is a bounded operator. Thus, it is left to show that 
\begin{equation}\label{eqn:FVolterra}
\left(\lambda \, \mathrm{id} - \bm{T}_{\varrho} \right) K = F
\end{equation}
is uniquely solvable in $C(I)$ for all $\lambda \in \mathbb{C} \backslash \{0\}$ and all $F \in C(I)$. 

One way to establish this result is via the Picard iteration, through proving that (see \cite{mckee2000euler}) the Picard iterates $K_n$ given iteratively by 
$$
K_{n+1}(s,t)= F(s,t) + \int_0^s\!\!\int_0^t \!\rho_{\bx,\by}(u,v) K_n(u,v) \, \mathrm{d}u \, \mathrm{d}v, \quad K_0(s,t) \equiv 0,
$$
form a Cauchy sequence in $\left(C(I), \left\| \cdot \right\|_{\infty} \right)$:
\begin{equation}\label{eqn:Cauchy}
|K_{n+1}(s,t)-K_n(s,t)| \leq \left(\frac{1}{2}\right)^n \left\|F\right\|_{\infty} e^{\beta(s+t)}, \quad (s,t) \in I,
\end{equation}
where $\beta = \left(2 \left\|\rho_{\bx,\by}\right\|_{\infty}\right)^{1/2}$.

This guarantees the existence of the limit $K_*:=\lim_{n \to \infty} K_n$ as an element in $C(I).$ Since each $K_n$ is bounded by
\begin{align*}
\left| K_n (s,t)\right| = \left| K_n (s,t)-K_0(s,t)\right| &\leq \sum_{i=0}^{n-1}\left|K_{i+1}(s,t)-K_i(s,t)\right|,\\
&\leq \sum_{i=0}^{n-1} 2^{-i} \left\|F\right\|_{\infty} e^{\beta(s+t)} \leq 2 \left\|F\right\|_{\infty} e^{\beta(s+t)},
\end{align*}
which is an integrable function on $I$, it follows by dominated convergence that
\begin{align*}
K_*(s,t) &= \lim_{n \to \infty} K_{n+1}(s,t) = F(s,t)+ \int_0^s \!\!\int_0^t \lim_{n\to \infty}\rho_{\bx,\by}(u,v) K_n(u,v) \, \mathrm{d}u \, \mathrm{d}v, \\
&=F(s,t)+ \int_0^s\!\! \int_0^t\!\rho_{\bx,\by}(u,v) K_*(u,v) \, \mathrm{d}u \, \mathrm{d}v,
\end{align*}
so that $K_*$ solves the integral equation \eqref{eqn:FVolterra}. As noted in \cite{mckee2000euler}, uniqueness can be established with another proof by induction. We give a precise argument for the convenience of the reader.

For this, suppose that there exists another solution $K \in C(I)$ to (\ref{eqn:FVolterra}) different from $K_*$. Then,
\begin{equation}\label{eqn:uniqueness}
K_{n+1}(s,t)-K(s,t) = \int_0^s\!\!\int_0^t\! \rho_{\bx,\by}(u,v) \left(K_n(u,v)-K(u,v)\right) \, \mathrm{d}u \, \mathrm{d}v. 
\end{equation}
As suggested in \cite{mckee2000euler}, showing 
\begin{equation}
\left| K_n(s,t) - K(s,t) \right| \leq 2^{-n}  \left\| K \right\|_\infty e^{\beta(s+t)},
\end{equation}
is sufficient to establish uniqueness of the solution of (\ref{eqn:FVolterra}). For the proof by induction, note that the case $n=0$ holds trivially since $K_0\equiv 0.$ The induction step $n \mapsto n+1$ is also straightforward: 
\begin{align*}
\left| K_{n+1}(s,t) - K(s,t)\right| &= \left| \int_0^s\!\!\int_0^t \!\rho_{\bx,\by}(u,v) \left(K_n(u,v)-K(u,v)\right) \, \mathrm{d}u \, \mathrm{d}v \right|,\\
& \leq  \left\| \rho_{\bx,\by}\right\|_{\infty} 2^{-n} \left\| K \right\|_\infty \int_0^s\!\! \int_0^t\! e^{\beta(u+v)} \, \mathrm{d}u \, \mathrm{d}v,\\
&\leq\left\| \rho_{\bx,\by}\right\|_{\infty} 2^{-n} \left\| K \right\|_\infty \beta^{-2} \left(e^{\beta s} -1 \right) \left(e^{\beta t}-1 \right),\\
& \leq 2^{-(n+1)} \left\| K \right\|_\infty e^{\beta(s+t)},
\end{align*}
where the first line is an application of \eqref{eqn:uniqueness}. Therefore, in $C(I)$, a solution to (\ref{eqn:FVolterra}) always exists and is unique. Together with employing the inverse mapping theorem, this concludes that the spectral radius of $\bm{T}_{\varrho}$ is zero.
\end{proof}


\begin{proof}[Proof of Proposition \ref{prop:sigkernel-welldefined}]
We can reformulate \eqref{eqn:GoursatPDE} as the equivalent integral equation \eqref{eqn:Volterra}, which in turn is equivalent to
\begin{equation}
(\mathrm{id} - \bm{T})K = u_0
\end{equation}
for $\mathrm{id}$ the identity on $C([0,1]^2)$ and $u_0$ the constant one-function $u_0\equiv 1$ and the integral operator 
\begin{equation}
\bm{T} : \big(C([0,1]^2),\|\cdot\|_{\infty}\big)\longrightarrow \big(C([0,1]^2),\|\cdot\|_{\infty}\big), \ \ f\longmapsto \left[(s,t)\mapsto\int_{0}^{t}\!\!\int_{0}^{s}\rho_{\bm{x},\bm{y}}(u,v)\,f(u,v)\,\mathrm{d}u\,\mathrm{d}v\right].
\end{equation}
The operator $\bm{T}$ is clearly linear and bounded, and has spectral radius zero, $r(\bm{T})=0$, by Lemma \ref{lem:spectralradius}. Consequently (cf.\ \cite[Theorem VI.6]{reed1981functional}), we have $\lim_{n \to \infty} \left\|\bm{T}^n\right\|^{1/n} = r(\bm{T}) = 0$, implying that
\begin{equation}\label{prop:sigkernel-welldefined:pf:aux1}
\forall\, q\in(0,1) : \,\exists\, n_q\in\N \,:\, \|\bm{T}^n\| < q^n, \ \, \forall\, n\geq n_q.
\end{equation}
Consequently, the Neumann series $\sum_{n=0}^\infty \bm{T}^n$ is $\|\cdot\|$-convergent and, hence, 
\begin{equation}
(\mathrm{id} - \bm{T}) \ \text{ is invertible} \quad\text{with}\quad (\mathrm{id} - \bm{T})^{-1} = \sum_{k=0}^\infty \bm{T}^k.
\end{equation}
This, however, implies that $K$ is indeed the only solution of \eqref{eqn:Volterra}, additionally satisfying 
\begin{equation}
K = (\mathrm{id} - \bm{T})^{-1}u_0 = \sum_{k=0}^\infty \bm{T}^k u_0.
\end{equation}
\end{proof}


\begin{remark}
Note that the $n$-th Picard iterate is related to the Neumann series via
$$
K_n = \sum_{i=0}^n \bm{T}^i K_1.
$$
%and the corresponding Neumann series converges with respect to the norm $\left\| \cdot \right\|_{C(I) \to C(I)}$. 
We remark that this convergence result is independent of any bound on $\rho_{\bx,\by}$ and extends a classical result for Volterra equations in one dimension \cite[Theorem 4.1]{engl2013integralgleichungen}. In particular, $\bm{T}$ need not be a contractive mapping. Repetition of the above argument similarly results in the converegence of $\sum_{n=0}^\infty \mu^{-n-1} \bm{T}^n$, which corresponds to the fixed point iteration for solving $(\mu \, \mathrm{id} - \bm{T}) K = f,$ for any $\mu \neq 0$ and any $f \in C(I).$ \hfill $\bm{\diamond}$
\end{remark}



\begin{proof}[Proof of Lemma \ref{lem:single-tile-solution}]
The (bivariate) power series $k_{1,1} : (s,t)\mapsto \sum_{i=0}^\infty \frac{\rho_{1,1}^i}{(i!)^2}s^it^i$ converges uniformly absolutely on $[0,1]^2\supset T_{1,1}$, since $\sum_{i=0}^\infty \big|\frac{\rho_{1,1}^i}{(i!)^2}s^it^i\big|\leq e^{|\rho_{1,1}|}$ for all $(s,t)\in [0,1]^2$. In particular, $k_{1,1}$ is partially differentiable with mixed derivatives $(\partial_s\partial_t k_{1,1})(s,t) = \rho_{1,1}\sum_{i=1}^\infty\frac{\rho_{1,1}^{i-1} i^2}{(i!)^2}(st)^{i-1} = \rho_{\bx,\by}(s,t)k_{1,1}(s,t)$, for all interior points $(s,t)$ of $T_{1,1}$. Thus, $k_{1,1}$ solves the boundary value problem \eqref{eqn:GoursatPDE} on the tile $\mathcal{D}_{1,1}$, as does $K$. By the uniqueness of solutions to \eqref{eqn:GoursatPDE}, we conclude that $\left.k_{1,1}\right|_{T_{1,1}} = \left.K\right|_{T_{1,1}}$, which establishes \eqref{lem:single-tile-solution:eq1}.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:tiled-operator-powers}]
This follows immediately by induction. Indeed, the case $k=0$ is clear and for $k\in\N$ we get
\begin{equation}
\begin{aligned}
\bm{T}_{\mu,\nu}^k\big(\varphi_\sigma^{(l)}\big|_{T_{\mu,\nu}}\big)(s,t) &= \rho_{\mu,\nu}\!\int_{\tau_\nu}^t\!\int_{\sigma_\mu}^s\!\bm{T}_{\mu,\nu}^{k-1}\!\big(\varphi_\sigma^{(l)}\big|_{T_{\mu,\nu}}\big)\!(u,v)\,\mathrm{d}u\,\mathrm{d}v\\ 
&= \rho_{\mu,\nu}\!\int_{\tau_\nu}^t\!\int_{\sigma_\mu}^s\!\frac{\rho_{\mu,\nu}^{k-1}}{(l+1)^{\overline{k-1}}(k-1)!}(u-\sigma_\mu)^{l+k-1}(v-\tau_\mu)^{k-1}\,\mathrm{d}u\,\mathrm{d}v \\
&= \frac{\rho_{\mu,\nu}^{k}}{(l+1)^{\overline{k-1}}(k-1)!}\!\int_{\tau_\nu}^t\!(v-\tau_\mu)^{k-1}\,\mathrm{d}v\!\int_{\sigma_\mu}^s\!(u-\sigma_\mu)^{l+k-1}\,\mathrm{d}u \\
&= \frac{\rho_{\mu,\nu}^{k}}{(l+1)^{\overline{k-1}}(k-1)!}\!\frac{(t-\tau_\nu)^k}{k}\frac{(s-\sigma_\mu)^{l+k}}{l+k} \quad\text{for each $(s,t)\in T_{\mu,\nu}$},
\end{aligned}
\end{equation}
as claimed. The proof for $\bm{T}_{\mu,\nu}^k\big(\varphi_\tau^{(l)}\big|_{T_{\mu,\nu}}\big)$ is entirely analogous. 
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:coefficientrecursion}]
We proceed by induction on the tile position $(\mu,\nu)$. For this, note first that, for all $\sigma, \tau\geq 0$,
\begin{align}
L_\sigma &= \big(\delta_{ij} - \sigma^j\delta_{i0}\big)_{i,j\geq 0}\cdot \Big(\tfrac{\delta_{i-1,j}}{i}\Big)_{\!i,j\geq 0} = \left({\textstyle\sum}_{k=0}^\infty(\tfrac{\delta_{ik} - \sigma^k\delta_{i0}}{k})\delta_{k-1,j}\right)_{i,j\geq 0}\\ 
&= \big(\tfrac{\delta_{i,j+1} - \sigma^{j+1}\delta_{i0}}{j+1}\big)_{i,j\geq 0}\eqqcolon(\ell_{ij}(\sigma))_{i,j\geq 0}, \quad\text{and} \label{prop:coefficientrecursion:aux1}\\
R_\tau &= \Big(\tfrac{\delta_{i,j-1}}{j}\Big)_{i,j\geq 0}\cdot (\delta_{ij} - \tau^i\delta_{0j})_{i,j\geq 0} = \big({\textstyle\sum}_{k=0}^\infty(\tfrac{\delta_{kj} - \tau^k\delta_{0j}}{k})\delta_{i,k-1}\big)_{i,j\geq 0}\\
&= \big(\tfrac{\delta_{i,j-1} - \tau^{i+1}\delta_{0j}}{i+1}\big)_{i,j\geq 0} \eqqcolon (r_{ij}(\tau))_{i,j\geq 0}.\label{prop:coefficientrecursion:aux2}
\end{align}
Then for the base case $(\mu,\nu)=(1,1)$, we have that ($C^{0,\nu}_{\langle\sigma,\tau\rangle} = C^{\mu,0}_{\langle\sigma,\tau\rangle}= C^{0,0}_{\langle\sigma,\tau\rangle}\equiv 1$ and hence) $C^{1,1}_0 = (\delta_{0i}\cdot\delta_{0j})_{i,j\geq 0}$ and further, by \eqref{prop:coefficientrecursion:eqn2}, that
\begin{equation}
\begin{aligned}
L_0C^{1,1}_0R_0 &= (\ell_{ij}(0))_{i,j\geq 0}\cdot \big({\textstyle\sum_{k=0}^\infty}(\delta_{0i}\cdot\delta_{0k})r_{kj}(0)\big)_{i,j\geq 0} = (\ell_{ij}(0))_{i,j\geq 0}\cdot \big(\delta_{0i}r_{0j}(0)\big)_{i,j\geq 0}\\
&= \big({\textstyle\sum_{k=0}^\infty}\ell_{ik}(0)r_{0j}(0)\cdot\delta_{0k}\big)_{i,j\geq 0} = \big(\ell_{i0}(0)r_{0j}(0)\big)_{i,j\geq 0} = \big(\delta_{i,1}\cdot\delta_{1,j}\big)_{i,j\geq 0}.
\end{aligned}
\end{equation}
Hence $\big[C^{1,1}_1\big]_{\langle\sigma,\tau\rangle} \stackrel{\eqref{prop:coefficientrecursion:eqn2}}{=} \big[\rho_{1,1}L_0C^{1,1}_0R_0\big]_{\langle\sigma,\tau\rangle} = \rho_{1,1}st$, whence $\left.\eqref{prop:coefficientrecursion:eqn1}\right|_{(\mu,\nu)=(1,1)}$ holds by Lemma \ref{lem:single-tile-solution} (and via induction on $k$) if, for any fixed $k\in\N_{\geq 2}$, 
\begin{equation}\label{prop:coefficientrecursion:aux4}
\Big(\tfrac{\rho_{1,1}^k}{(k!)^2}\delta_{ik}\delta_{kj}\Big)_{\!i,j\geq 0} = \rho_{1,1}L_0\Big(\tfrac{\rho_{1,1}^{k-1}}{((k-1)!)^2}\delta_{i,k-1}\delta_{k-1,j}\Big)_{\!i,j\geq 0}R_0.
\end{equation}
Denoting $\alpha_{ij}\coloneqq\tfrac{\rho_{1,1}^{k-1}}{((k-1)!)^2}\delta_{i,k-1}\delta_{k-1,j}$ for each $i,j\geq 0$, the right-hand side of \eqref{prop:coefficientrecursion:aux4} reads
\begin{equation}\label{prop:coefficientrecursion:aux5}
\rho_{1,1}L_0\cdot\big({\textstyle\sum_{m=0}^\infty}\alpha_{im}r_{mj}(0)\big)_{i,j\geq 0} = \rho_{1,1}\big({\textstyle\sum_{l,m=0}^\infty}\ell_{il}(0)\alpha_{lm}r_{mj}(0)\big)_{i,j\geq 0}.
\end{equation}
Now by \eqref{prop:coefficientrecursion:aux1} and \eqref{prop:coefficientrecursion:aux2}, we obtain for any fixed $i,j,l,m\geq 0$ that
\begin{equation}\label{prop:coefficientrecursion:aux6}
\ell_{il}(0)\alpha_{lm}r_{mj}(0) = \alpha_{lm}\cdot\tfrac{\delta_{i,l+1}}{l+1}\tfrac{\delta_{m,j-1}}{m+1} = \tfrac{\alpha_{i-1,j-1}}{ij}\cdot\delta_{i-1,l}\delta_{m,j-1}.
\end{equation}
This allows us to evaluate \eqref{prop:coefficientrecursion:aux5} to 
\begin{equation}\label{prop:coefficientrecursion:aux7}
\rho_{1,1}\big({\textstyle\sum_{l,m=0}^\infty}\ell_{il}(0)\alpha_{lm}r_{mj}(0)\big)_{i,j\geq 0} = \rho_{1,1}\big(\alpha_{i-1,j-1}\big)_{i,j\geq 0} = \rho_{1,1}\Big(\!\tfrac{\rho_{1,1}^{k-1}}{((k-1)!)^2ij}\delta_{ik}\delta_{kj}\Big)_{\!i,j\geq 0}, 
\end{equation}
proving \eqref{prop:coefficientrecursion:aux4} as desired. 

With the base case $(\mu,\nu)=(1,1)$ thus established, take now any $(\mu,\nu)\in\{1,\ldots,\ell-1\}^2$ with $\mu+\nu > 2$, and suppose that \eqref{prop:coefficientrecursion:eqn1} holds for $(\mu-1,\nu)$ and $(\mu,\nu-1)$ and $(\mu-1,\nu-1)$ (induction hypothesis). Then $\kappa_{\tilde{\mu}, \tilde{\nu}} = \left.C^{\tilde{\mu},\tilde{\nu}}_{\langle\sigma,\tau\rangle}\right|_{T_{\tilde{\mu},\tilde{\nu}}}$ for each $(\tilde{\mu},\tilde{\nu})\in\{(\mu-1,\nu), (\mu,\nu-1), (\mu-1,\nu-1)\}$, whence and by Proposition \ref{prop:recursion} we have that  
\begin{equation}\label{prop:coefficientrecursion:aux8}
\kappa_{\mu,\nu} = \sum_{k=0}^\infty\bm{T}^k_{\mu,\nu}\big(\big\langle\eta(\sigma_\mu),C^{\mu-1,\nu}\eta(\cdot)\big\rangle + \big\langle\eta(\cdot),C^{\mu,\nu-1}\eta(\tau_\nu)\big\rangle + \big\langle\eta(\sigma_\mu),C^{\mu-1,\nu-1}\eta(\tau_\nu)\big\rangle\big)
\end{equation}
uniformly on $T_{\mu,\nu}$. Since $\bm{T}^0_{\mu,\nu}=\left.\mathrm{id}\right|_{C(T_{\mu,\nu})}$, the $0^{\mathrm{th}}$ summand in \eqref{prop:coefficientrecursion:aux8} reads 
\begin{equation}\label{prop:coefficientrecursion:aux9}
\langle\eta(\sigma_\mu),C^{\mu-1,\nu}\eta(\cdot)\big\rangle + \big\langle\eta(\cdot),C^{\mu,\nu-1}\eta(\tau_\nu)\big\rangle + \big\langle\eta(\sigma_\mu),C^{\mu-1,\nu-1}\eta(\tau_\nu)\big\rangle = \left[C^{\mu,\nu}_0\right]_{\langle\sigma,\tau\rangle} 
\end{equation}
for the initial matrix $C^{\mu,\nu}_0$ from \eqref{prop:coefficientrecursion:eqn3}. Consequently, the claim \eqref{prop:coefficientrecursion:eqn1} follows if, for any fixed $k\in\N$, 
\begin{equation}\label{prop:coefficientrecursion:aux10}
\left.\left[C^{\mu,\nu}_{k+1}\right]_{\langle\sigma,\tau\rangle}\right|_{T_{\mu,\nu}} \!\!\!\!= \bm{T}_{\mu,\nu}\!\left(\left.\left[C^{\mu,\nu}_{k}\right]_{\langle\sigma,\tau\rangle}\right|_{T_{\mu,\nu}}\right).
\end{equation}
Abbreviating $u_{k+1}\coloneqq\bm{T}_{\mu,\nu}\!\left(\left.\left[C^{\mu,\nu}_{k}\right]_{\langle\sigma,\tau\rangle}\right|_{T_{\mu,\nu}}\right)$ and $C^{\mu,\nu}_{k}\eqqcolon(c_{ij})_{i,j\geq 0}$, note that by definition, 
\begin{equation}\label{prop:coefficientrecursion:aux11}
u_{k+1}(s,t) = \rho_{\mu,\nu}\!\sum_{i,j\geq 0}c_{ij}\!\int_{\tau_\nu}^t\!\int_{\sigma_\mu}^s\!\tilde{s}^i\tilde{t}^j\,\mathrm{d}\tilde{s}\,\mathrm{d}\tilde{t} = \rho_{\mu,\nu}\!\sum_{i,j\geq 0}\frac{c_{ij}}{(i+1)(j+1)}(s^{i+1} - \sigma_\mu^{i+1})(t^{j+1} - \tau_\nu^{j+1})
\end{equation}
for each $(s,t)\in T_{\mu,\nu}$ (see \eqref{eqn:integral-decomposition-operator} and \eqref{eqn:matrixrepdfunction}). Abbreviating $(\hat{c}_{ij})\coloneqq\Big(\tfrac{c_{ij}}{(i+1)(j+1)}\Big)_{\!i,j\geq 0}$ and using that
\begin{equation}\label{prop:coefficientrecursion:aux12}
(s^{i+1} - \sigma_\mu^{i+1})(t^{j+1} - \tau_\nu^{j+1}) = s^{i+1}t^{j+1} - \sigma_\mu^{i+1}t^{j+1} - \tau_\nu^{j+1}s^{i+1} + \sigma_\mu^{i+1}\tau_\nu^{j+1}
\end{equation}
for all $i,j\geq 0$, we can immediately rewrite \eqref{prop:coefficientrecursion:aux11} as $u_{k+1} = \left.\tilde{C}_{\langle\sigma,\tau\rangle}\right|_{T_{\mu,\nu}}$ for the coefficient matrix
\begin{equation}\label{prop:coefficientrecursion:aux13}
\tilde{C} = \rho_{\mu,\nu}\big(\hat{c}_{i-1,j-1} - \hat{\gamma}_{i-1|:}\cdot\delta_{0j} - \hat{\gamma}_{:|j-1}\cdot\delta_{i0} + \hat{\gamma}_{:|:}\cdot\delta_{i0}\delta_{0j}\big)_{i,j\geq 0},
\end{equation}
where $\hat{\gamma}_{i|:}\coloneqq\sum_{j=0}^\infty\hat{c}_{ij}\tau_\nu^{j+1}$ and $\hat{\gamma}_{:|j}\coloneqq\sum_{i=0}^\infty\hat{c}_{ij}\sigma_\mu^{i+1}$ $(i,j\geq 1)$ and $\hat{\gamma}_{-1|:} = \hat{\gamma}_{:|-1} = \hat{c}_{-1,j}=\hat{c}_{i,-1}\coloneqq 0$ $(i,j\geq 0)$, and $\hat{\gamma}_{:|:}\coloneqq{\textstyle\sum}_{i,j\geq 0}\sigma^{i+1}_\mu\tau^{j+1}_\nu$. Hence, the desired identity \eqref{prop:coefficientrecursion:aux10} follows if
\begin{equation}\label{prop:coefficientrecursion:aux14}
\tilde{C} = \rho_{\mu,\nu}L_{\sigma_\mu} C^{\mu,\nu}_{k}R_{\tau_\nu}. 
\end{equation}
For a proof of \eqref{prop:coefficientrecursion:aux14}, note simply that, by definition of $L_\sigma$ and $R_\tau$ (cf.\ \eqref{prop:coefficientrecursion:aux1} and \eqref{prop:coefficientrecursion:aux2}),
\begin{equation}
\begin{aligned}
L_{\sigma_\mu} C^{\mu,\nu}_{k}R_{\tau_\nu} &= \left({\textstyle\sum_{l,m\geq 0}}\ell_{il}(\sigma_\mu)c_{lm}r_{mj}(\tau_\nu)\right)_{\!i,j\geq 0}\\
&\hspace{-5em}= \left({\textstyle\sum_{l,m\geq 0}}(\delta_{i,l+1} - \sigma_\mu^{l+1}\delta_{i0})\tfrac{c_{lm}}{(l+1)(m+1)}(\delta_{m,j-1}- \tau_\nu^{m+1}\delta_{0j})\right)_{\!i,j\geq 0}\\
&\hspace{-5em}= \left(\hat{c}_{i-1,j-1} 
- {\textstyle\sum_{m=0}^\infty}\hat{c}_{i-1,m}\tau_\nu^{m+1}\delta_{0j}
- {\textstyle\sum_{l=0}^\infty}\hat{c}_{l,j-1}\sigma_\mu^{l+1}\cdot\delta_{i0}  + {\textstyle\sum_{l,m\geq 0}}\sigma_\mu^{l+1}\tau_\nu^{m+1}\cdot\delta_{i0}\delta_{0j}\right)_{\!i,j\geq 0}\\
&\hspace{-5em}=\big(\hat{c}_{i-1,j-1} - \hat{\gamma}_{i-1|:}\cdot\delta_{0j} - \hat{\gamma}_{:|j-1}\cdot\delta_{i0} + \hat{\gamma}_{:|:}\cdot\delta_{i0}\delta_{0j}\big)_{i,j\geq 0}.
\end{aligned}
\end{equation}
This implies \eqref{prop:coefficientrecursion:aux14} and hence concludes the overall proof of the proposition. 
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:recursion-speedup}]
The proof of \emph{\ref{prop:recursion-speedup:it1}} follows from Lemma \ref{lem:tiled-operator-powers} and similar reasoning as the proof of Proposition \ref{prop:coefficientrecursion}: By combination of Proposition \ref{prop:recursion} with assumption \eqref{prop:recursion-speedup:eq1}, we know that 
\begin{equation}\label{prop:recursion-speedup:aux1}
\kappa_{\mu,\nu} = \sum_{k=0}^\infty\!\left[\sum_{i=0}^\infty\tilde{\alpha}_i\bm{T}^k_{\mu,\nu}(\varphi_{\sigma_\mu}^{(i)}) + \sum_{j=0}^\infty\beta_j\bm{T}^k_{\mu,\nu}(\varphi_{\tau_\nu}^{(j)})\right]
\end{equation}
for $\tilde{\alpha}_0\coloneqq 0$ and $(\tilde{\alpha}_i)_{i\geq 0}\coloneqq(\alpha_i)_{i\geq 1}$, and with $\varphi_{\sigma_\mu}^{(i)}\,(\equiv\left.\varphi_{\sigma_\mu}^{(i)}\right|_{T_{\mu,\nu}})$ and $\varphi_{\tau_\nu}^{(j)}\,(\equiv\left.\varphi_{\tau_\nu}^{(i)}\right|_{T_{\mu,\nu}})$ as in Lemma \ref{lem:tiled-operator-powers}. Denoting $u_k\coloneqq \sum_{i=0}^\infty\tilde{\alpha}_i\bm{T}^k_{\mu,\nu}(\varphi_{\sigma_\mu}^{(i)}) + \sum_{j=0}^\infty\beta_j\bm{T}^k_{\mu,\nu}(\varphi_{\tau_\nu}^{(j)})$ $(k\in\N_0)$, we see that, pointwise on $T_{\mu,\nu}$, 
\begin{equation}
\begin{aligned}
u_{k+1}(s,t) &= (\bm{T}_{\mu,\nu}(u_k))(s,t) = \rho_{\mu,\nu}\!\sum_{i,j\geq 0}\hat{c}_{ij}\int_{\sigma_\mu}^s\!(\tilde{s}-\sigma_\mu)^i\,\mathrm{d}\tilde{s}\int_{\tau_\nu}^t\!(\tilde{t}-\tau_\nu)^j\,\mathrm{d}\tilde{t}\\
&= \sum_{i,j\geq 0}(s-\sigma_\mu)^{i+1}\frac{\rho_{\mu,\nu}\hat{c}_{ij}}{(i+1)(j+1)}(t - \tau_\nu)^{j+1} = \left\langle\eta(s-\sigma_\mu), \big(\rho_{\mu,\nu}\underline{S}\hat{C}_k\underline{T}\big)\eta(t-\tau_\nu)\right\rangle,
\end{aligned}
\end{equation}
where from the second equality onwards we assumed that $u_k = \langle\eta(\cdot-\sigma_\mu), \hat{C}_k\eta(\cdot-\tau_\nu)\rangle$ for some $\hat{C}_k\equiv(\hat{c}_{ij})\in\ell_1(\N_0^2)$ (induction hypothesis). This proves \eqref{prop:recursion-speedup:eq2} by induction on $k\in\N_0$.\\[-0.5em]

The identities \eqref{prop:recursion-speedup:eq3} follow from \eqref{prop:recursion-speedup:aux1} and Lemma \ref{lem:tiled-operator-powers}, as these imply that, uniformly on $T_{\mu,\nu}$,
\begin{equation}
\kappa_{\mu,\nu}(s,t) = \sum_{k=0}^\infty\!\!\left[\sum_{i=0}^\infty\frac{\rho_{\mu,\nu}^k\tilde{\alpha}_i}{(i+1)^{\bar{k}}k!}(s-\sigma_\mu)^{i+k}(t-\tau_\nu)^k + \sum_{j=0}^\infty\frac{\rho_{\mu,\nu}^k\beta_j}{(j+1)^{\bar{k}}k!}(s-\sigma_\mu)^{k}(t-\tau_\nu)^{j+k}\right].
\end{equation}
Consequently, and abbreviating $\gamma_{i,k}\coloneqq \frac{\rho_{\mu,\nu}^k}{(i+1)^{\overline{k}}k!}$ $($for $i,k\geq 0)$ and $\Delta\coloneqq(\tau_{\nu+1} - \tau_\nu)$, 
\begin{equation}
\begin{aligned}
\kappa_{\mu,\nu}(s,\tau_{\nu+1}) &= \sum_{i=0}^\infty\!\left[\sum_{\{m,n\geq 0\,:\, m+n = i\}}\!\!\!\!\!\!\!\!\!\gamma_{mn}\tilde{\alpha}_m\Delta^{\!n}\right]\!\!(s-\sigma_\mu)^i + \sum_{k=0}^\infty\!\left[\sum_{j=0}^\infty\gamma_{kj}\beta_j\Delta^{\!j+k}\right]\!\!(s-\sigma_\mu)^k\\
&= \sum_{i=0}^\infty\!\left[\sum_{\{m,n\geq 0\,:\, m+n = i\}}\!\!\!\!\!\!\!\!\!\gamma_{mn}\tilde{\alpha}_m\Delta^{\!n} + \sum_{j=0}^\infty\gamma_{ij}\beta_j\Delta^{\!i+j}\right]\!\!(s-\sigma_\mu)^i\\
&= \sum_{i=0}^\infty\!\left[\sum_{j=0}^i\gamma_{i-j,j}\tilde{\alpha}_{i-j}\Delta^{\!j} + \sum_{j=i}^\infty\gamma_{i,j-i}\beta_{j-i}\Delta^{\!j}\right]\!\!(s-\sigma_\mu)^i
\end{aligned}
\end{equation}
uniformly on $T_{\mu,\nu}$, which proves the first identity in \eqref{prop:recursion-speedup:eq3}. The second identity follows analogously.\\[-0.5em]

For a proof of \emph{\ref{prop:recursion-speedup:it2}}, suppose more generally that $\varphi : \R\ni z \mapsto \varphi(z)\coloneqq\sum_{j=0}^\infty\gamma_j(z - z_0)^j$ converges uniformly on some interval $I$ around $z_0$. Then, for each $z\in I$ and any $z_1\in\R$, 
\begin{equation}
\begin{aligned}
\varphi(z) = \varphi(z_1 + (z-z_1)) &= \sum_{j=0}^\infty\gamma_j((z - z_1) + (z_1-z_0))^j = \sum_{j=0}^\infty\gamma_j\sum_{i=0}^j\binom{j}{i}(z_1-z_0)^{j-i}(z-z_1)^i\\
&\hspace{-7.5em}= \sum_{j=0}^\infty\left[\sum_{i=0}^j\gamma_j\binom{j}{i}(z_1-z_0)^{j-i}\right]\!\!(z-z_1)^i = \sum_{i=0}^\infty\left[\sum_{j=i}^\infty\gamma_j\binom{j}{i}(z_1-z_0)^{j-i}\right]\!\!(z-z_1)^i.
\end{aligned}
\end{equation}
The identities \eqref{prop:recursion-speedup:eq4} thus follow from \eqref{prop:recursion-speedup:eq3} by applying the above situation to 
\begin{equation}
(\varphi, z_0, z_1)\in\{(\kappa_{\mu,\nu}(\,\cdot\,,\tau_{\nu+1}), \sigma_\mu, \sigma_{\mu+1}), (\kappa_{\mu,\nu}(\sigma_{\mu+1},\,\cdot\,), \tau_\nu, \tau_{\nu+1})\}.
\end{equation}
This concludes the proof of the proposition. 
\end{proof}

%\section{Existence and Uniqueness of Solutions to the Goursat Problem }\label{sect:existence-and-uniqueness}
% The Goursat PDE (\ref{eqn:GoursatPDE}) can be rewritten in an integral form as
% \begin{equation}\label{eqn:Integral_Goursat}
% K(s,t) = 1 + \int_0^s \int_0^t \rho_{\bx,\by}(u,v) K(u,v) \, du \, dv,
% \end{equation}
% which is in fact the original formulation of the signature kernel equation derived in \cite{salvi2021signature}. This is a two-dimensional Volterra integral equation of the second kind with integral operator $T$ defined as
% \begin{equation}\label{eqn:int-operator}
% T K (s,t) = \int_0^s \int_0^t \rho_{\bx,\by}(u,v) K(u,v) \, du \, dv, \quad (s,t) \in [0,1]\times [0,1].
% \end{equation}
% In this formulation, (\ref{eqn:Integral_Goursat}) can be abbreviated by $(\mathrm{id}-T) K = 1$. 
% Assuming that the paths $\bx$ and $\by$ are absolutely continuous, the kernel $K$ is a continuous function on $I=[0,1]^2$ (cf. \cite{salvi2021signature}) and $\rho_{\bx,\by} \in L^\infty(I)$, so that we can consider the integral operator as a map $T:C(I) \to C(I).$ It is a well-known fact that the solution $K \in C(I)$ to (\ref{eqn:Integral_Goursat}) exists and is unique. One way to establish this result is via the Picard iteration , through showing that (see \cite{mckee2000euler}) the Picard iterates $K_n$ given iteratively by 
% $$
% K_{n+1}(s,t)= 1 + \int_0^s \int_0^t \rho_{\bx,\by}(u,v) K_n(u,v) \, du \, dv, \quad K_0(s,t) \equiv 0,
% $$
% form a Cauchy sequence in $\left(C(I), \left\| \cdot \right\|_{\infty} \right)$:
% \begin{equation}\label{eqn:Cauchy}
% |K_{n+1}(s,t)-K_n(s,t)| \leq \left(\frac{1}{2}\right)^n e^{\beta(s+t)}, \quad (s,t) \in I,
% \end{equation}
% where $\beta = \left(2 \left\|\rho_{\bx,\by}\right\|_{\infty}\right)^{1/2}$. This guarantees the existence of the limit $\widetilde{K}:=\lim_{n \to \infty} K_n$ as an element in $C(I).$ Since each $K_n$ is bounded by
% \begin{align*}
% \left| K_n (s,t)\right| = \left| K_n (s,t)-K_0(s,t)\right| &\leq \sum_{i=0}^{n-1}\left|K_{i+1}(s,t)-K_i(s,t)\right|,\\
% &\leq \sum_{i=0}^{n-1} 2^{-i} e^{\beta(s+t)} \leq 2 e^{\beta(s+t)},
% \end{align*}
% which is an integrable function on $I$, it follows by dominated convergence that
% \begin{align*}
% \widetilde{K}(s,t) &= \lim_{n \to \infty} K_{n+1}(s,t) = 1+ \int_0^s \int_0^t \lim_{n\to \infty}\rho_{\bx,\by}(u,v) K_n(u,v) \, du \, dv, \\
% &=1+ \int_0^s \int_0^t \rho_{\bx,\by}(u,v) \widetilde{K}(u,v) \, du \, dv,
% \end{align*}
% so that $\widetilde{K}$ solves the integral equation (\ref{eqn:Integral_Goursat}). As noted in \cite{mckee2000euler}, uniqueness can be established with another proof by induction. We give a precise argument for convenience of the reader.

% For this, suppose that there exists another solution $K \in C(I)$ to (\ref{eqn:Integral_Goursat}) different from $\widetilde{K}$. Then,
% \begin{equation}\label{eqn:uniqueness_old}
% K_{n+1}(s,t)-K(s,t) = \int_0^s \int_0^t \rho_{\bx,\by}(u,v) \left(K_n(u,v)-K(u,v)\right) \, du \, dv. 
% \end{equation}
% As suggested in \cite{mckee2000euler}, showing 
% \begin{equation}
% \left| K_n(s,t) - K(s,t) \right| \leq 2^{-n}  \left\| K \right\|_\infty e^{\beta(s+t)},
% \end{equation}
% is sufficient to establish uniqueness of the solution of (\ref{eqn:Integral_Goursat}). For the proof by induction, note that the case $n=0$ holds trivially since $K_0\equiv 0.$ The induction step $n \mapsto n+1$ is also straightforward. 
% \begin{align*}
% \left| K_{n+1}(s,t) - K(s,t)\right| &= \left| \int_0^s \int_0^t \rho_{\bx,\by}(u,v) \left(K_n(u,v)-K(u,v)\right) \, du \, dv \right|,\\
% & \leq  \left\| \rho_{\bx,\by}\right\|_{L^\infty(I)} 2^{-n} \left\| K \right\|_\infty \int_0^s \int_0^t e^{\beta(u+v)} \, du \, dv,\\
% &\leq\left\| \rho_{\bx,\by}\right\|_{L^\infty(I)} 2^{-n} \left\| K \right\|_\infty \beta^{-2} \left(e^{\beta s} -1 \right) \left(e^{\beta t}-1 \right),\\
% & \leq 2^{-(n+1)} \left\| K \right\|_\infty e^{\beta(s+t)},
% \end{align*}
% where the first line is an application of (\ref{eqn:uniqueness_old}). Therefore, in $C(I)$, a solution to (\ref{eqn:Integral_Goursat}) always exists and is unique. Note that the same argument also yields that for any $F \in C(I)$, the modified equation
% \begin{equation*}
% K(s,t) = F(s,t) + \int_0^s \int_0^t \rho_{\bx,\by}(u,v) K(u,v) \, du \, dv,
% \end{equation*}
% is uniquely solvable in $C(I)$. Consequently, the same can be said for any equation of the form
% \begin{equation*}
% \lambda K(s,t) = F(s,t) + \int_0^s \int_0^t \rho_{\bx,\by}(u,v) K(u,v) \, du \, dv,
% \end{equation*}
% as long as $\lambda \neq 0,$ simply by rescaling the equation. Let $\mathrm{id}:C(I) \to C(I)$ denote the identity operator on $C(I)$. Then, the above implies that for any $\lambda \neq 0,$ the operator 
% $$
% \lambda \, \mathrm{id} - T: C(I) \to C(I),
% $$
% is a bijection. Since $C(I)$ is a Banach space and $\lambda \, \mathrm{id} - T$ is a bounded linear operator on $C(I)$, the inverse mapping theorem implies that $\left(\lambda \mathrm{id} - T \right)^{-1}$ is a bounded operator. In spectral theoretic terms, this implies that all $\lambda \in \mathbb{C} \backslash \{0\}$ are in the \emph{resolvent set} of the operator $T,$ and consequently its spectrum $\sigma(T)$ can only consist of the element $0.$  This establishes the interesting fact that the spectral radius $r(T)$ is equal to zero:
% $$
% r(T):= \sup_{\lambda \in \sigma(T)} |\lambda| =0.
% $$
% We further note that (cf. \cite[Theorem VI.6]{reed1981functional})
% $$
% \lim_{n \to \infty} \left\|T^n\right\|_{C(I) \to C(I)}^{1/n} = r(T) = 0.
% $$
% With these findings at hand, we can state a stronger convergence result of the above fixed point iteration in the operator norm: 
% \begin{proposition}
% Let $T$ be the operator defined in (\ref{eqn:int-operator}). Then, the Neumann series
% $$
% \sum_{n=0}^\infty T^n 
% $$
% converges in the operator norm $\| \cdot\|_{C(I) \to C(I)}$ to $(\mathrm{id}-T)^{-1}.$
% \end{proposition}
% \begin{proof}
%     This is a direct consequence of $r(T)=0:$ take any $q \in (0,1).$ Then, there exists an index $n_0 \in \mathbb{N},$ such that for all $n \geq n_0$
%     $$
%     \left\|T^n \right\|_{C(I) \to C(I)} < q^n.
%     $$
% \end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
