\section{Preliminary}

\subsection{Large Language Models}
Large Language Models (LLMs) generally refer to models that utilize the Transformer architecture and are equipped with billions of parameters trained on trillions of text tokens. These models have demonstrated substantial improvements in a myriad of tasks related to natural language understanding and generation, increasingly proving beneficial in assisting human activities. In this work, we mainly focus on autoregressive LLMs, which are based on two main architectures:  decoder-only models and encoder-decoder models. Encoder-decoder models such as Flan-T5~\citep{chung2022scaling} and ChatGLM~\citep{zeng2022glm} analyze input through the encoder for semantic representations,  making them effective in language understanding in addition to generation. 
Decoder-only LLMs focus on left-to-right generation by predicting the next token in a sequence, with numerous instances~\citep{brown2020language, chowdhery2022palm, touvron2023llama, guo2025deepseek} under this paradigm achieving breakthroughs in advanced capabilities like instruction following and reasoning.

However, these models are typically pre-trained on general-purpose data and lack an understanding of specific user information. As a result, they are unable to generate responses tailored to a user's unique tastes, preferences, and expectations, limiting their effectiveness in personalized applications where user-specific adaptation is critical.


\subsection{Problem Statement}

Personalized Large Language Models (PLLMs) generate responses that align with the user's style and expectations, offering diverse answers to the same query for different users~\citep{clarke2024peft}.
A PLLM is defined as an LLM that generates responses conditioned not only on an input query $q$, but also on a user $u$'s personalized data $\mathcal{C}_u$. It aims to predict the most probable response sequence $y$ given a query $q$ and the personalized context $\mathcal{C}_u$, such that:
$
y = \text{argmax}_y P(y \mid q, \mathcal{C}_u).
$
The personalized data $\mathcal{C}_u$ may encapsulate information about the userâ€™s preferences, history, context, and other user-specific attributes. These can include (Figure~\ref{fig:framework}):

\begin{itemize}
    \item \textbf{Profile/Relationship:} User profile, including attributes (e.g., name, gender, occupation), and relationships (e.g., friends, family members), such as $\mathcal{C}_u = \{A, 18, \text{student}, \text{friends:} \{B, C, D\} \dots\}$.
    \item \textbf{Historical Dialogues:} Historical dialogues, such as question-answer pairs that user $u$ interacts with the LLM (e.g., $\mathcal{C}_u = \{(q_0, a_0), (q_1, a_1), \dots, (q_i, a_i)\}$), where each $q_i$ is a query and $a_i$ is the corresponding answer.
    \item \textbf{Historical Content:} Includes documents, previous reviews, comments or feedback from user $u$. For example, $\mathcal{C}_u = \{\text{I like \textit{Avtar} because} \dots, \dots\}$.
    \item \textbf{Historical Interactions:} Includes historical interactions, preferences, ratings from user $u$. For example, $\mathcal{C}_u = \{\text{\textit{The Lord of the Rings}}:5, \text{\textit{Interstellar}}: 3 \dots\}$.
\end{itemize}

By incorporating personalized data, PLLMs enhance traditional LLMs, improving response generation, recommendations, and classification tasks.

\textbf{Note that} our survey differs significantly from role-play related LLM personalization~\citep{tseng2024two, chen2024large, zhang2024personalization}. While role-play focuses on mimicking characters during conversations, PLLMs in this survey focus on understanding users' contexts and preferences to meet their specific needs. 
Compared to~\citep{zhang2024personalization}, which emphasizes broad categories, our work provides a systematic analysis of techniques to enhance PLLM efficiency and performance, with a more detailed technical classification.


\subsection{Proposed Taxonomy}

We propose a taxonomy (as illustrated in Figure~\ref{fig:framework} and Figure~\ref{fig:taxonomy_of_PLLM}) from technical perspectives, categorizing the methods for Personalized Large Language Models (PLLMs) into three major levels: {\color{black}(1) \textbf{Input level: Personalized Prompting} focuses on handling user-specific data outside the LLM and injecting it into the model.
(2) \textbf{Model level: Personalized Adaptation} emphasizes designing a framework to efficiently fine-tune or adapt model parameters for personalization.
\textbf{(3) Objective Level: Personalized Alignment} aims to refine model behavior to align with user preferences effectively.
{\color{black} Due to space limitations, analysis papers, datasets, and benchmarks are summarized in the \href{https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models}{Github Repo}.}
