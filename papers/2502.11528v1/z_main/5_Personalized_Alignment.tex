\section{Personalized Alignment}
\label{sec: personalized alignment}

Alignment techniques~\citep{bai2022training,rafailov2024direct} typically optimize LLMs to match the generic preferences of humans. However, in reality, individuals may exhibit significant variations in their preferences for LLM responses across different dimensions like language style, knowledge depth, and values. Personalized alignment seeks to further align with individual users’ unique preferences beyond generic preferences. 
A significant challenge in personalized alignment is creating high-quality user-specific preference datasets, which are more complex than general alignment datasets due to data sparsity. The second challenge arises from the need to refine the canonical RLHF framework~\citep{ouyang2022training} to handle the diversification of user preferences, which is essential for integrating personalized preferences without compromising efficiency and performance.

\subsection{Personalized Alignment Data Construction}
\label{subsec:data}

High-quality data construction is critical for learning PLLMs, primarily involving self-generated data through interactions with the LLM. \cite{wu2024aligning} constructs a dataset for aligning LLMs with individual preferences by initially creating a diverse pool of 3,310 user personas, which are expanded through iterative self-generation and filtering.  This method is similar to \textbit{PLUM}~\citep{magister2024way} 
that both simulate dynamic interactions through multi-turn conversation trees, allowing LLMs to infer and adapt to user preferences. To enable LLMs to adapt to individual user preferences without re-training, \cite{lee2024aligning} utilizes diverse system messages as meta-instructions to guide the models' behavior. To support this, the MULTIFACETED COLLECTION dataset is created, comprising 197k system messages that represent a wide range of user values. To facilitate real-time, privacy-preserving personalization on edge devices while addressing data privacy, limited storage, and minimal user disruption, \cite{qin2024enabling} introduces a self-supervised method that efficiently selects and synthesizes essential user data, improving model adaptation with minimal user interaction.

Research efforts are also increasingly concentrating on developing datasets that assess models' comprehension of personalized preferences. ~\cite{kirk2024prism} introduces \textbit{PRISM Alignment Dataset} that maps the sociodemographics and preferences of 1,500 participants from 75 countries to their feedback in live interactions with 21 LLMs, focusing on subjective and multicultural perspectives on controversial topics. \textbit{PersonalLLM}~\citep{zollo2024personalllm} introduces a novel personalized testdb, which curates open-ended prompts and multiple high-quality responses to simulate diverse latent preferences among users. It generates simulated user bases with varied preferences from pre-trained reward models, addressing the challenge of data sparsity in personalization.

\subsection{Personalized Alignment Optimization}
\label{subsec:optimization}

Personalized preference alignment is usually modeled as a multi-objective reinforcement learning (MORL) problem, where personalized preference is determined as the user-specific combination of multi-preference dimensions. Based on this, a typical alignment paradigm involves using a personalized reward derived from multiple reward models to guide during the training phase of policy LLMs, aiming for personalization. \textbit{MORLHF}~\citep{wu2023fine} separately trains reward models for each dimension and retrains the policy language models using proximal policy optimization, guided by a linear combination of these multiple reward models. This approach allows for the reuse of the standard RLHF pipeline. \textbit{MODPO}~\citep{zhou2023beyond} introduces a novel RL-free algorithm extending Direct Preference Optimization (DPO) for managing multiple alignment objectives. It integrates linear scalarization directly into the reward modeling process, allowing it to train language models via a simple margin-based cross-entropy loss as implicit collective rewards functions.

Another strategy for MORL is to consider ad-hoc combinations of multiple trained policy LLMs during the decoding phase to achieve personalization. \textbit{Personalized Soups}~\citep{jang2023personalized} 
and \textbit{Reward Soups}~\citep{rame2024rewarded} address the challenge of RL from personalized human feedback by first training multiple policy models with distinct preferences independently and then merging their parameters post-hoc during inference. Both methods allow for dynamic weighting of the networks based on user preferences, enhancing model alignment and reducing reward misspecification.  Also, the personalized fusion of policy LLMs can be achieved not only through parameter merging but also through model ensembling.  \textbit{MOD}~\citep{shi2024decoding}  outputs the next token from a linear combination of all base models, allowing for precise control over different objectives by combining their predictions without the need for retraining. The method demonstrates significant effectiveness when compared to the parameter-merging baseline. \textbit{PAD}~\citep{chen2024pad} leverages a personalized reward modeling strategy to generate token-level personalized rewards, which are then used to guide the decoding process, allowing dynamic tailoring of the base model's predictions to individual preferences. 
Figure~\ref{fig:personalized_alignment} visualizes above typical approaches of MORL for personalized alignment.

There are some other emerging personalized alignment studies beyond the “multi-objective” paradigm. \textbit{PPT}~\citep{lau2024personalized} unlocks the potential of in-context learning for scalable and efficient personalization by generating two potential responses for each user prompt, asking the user to rank them, and incorporating this feedback into the model’s context to dyanmic adapt to individual preferences over time.  \textbit{VPL}~\citep{poddar2024personalizing} utilizes a variational inference framework to capture diverse human preferences through user-specific latent variables. Inferring user-specific latent distributions from a few preference annotations enables more accurate and personalized reward modeling with better data efficiency.


\subsection{Discussions}
Current mainstream personalized alignment technologies mainly model personalization as multi-objective reinforcement learning problems, where personalized user preferences are taken into account during the training phase of policy LLMs via canonical RLHF, or the decoding phase of policy LLM via parameter merging or model ensembling.  Typically, these methods are limited to a small number (e.g., three) of predefined preference dimensions, represented through textual user preference prompts. However, in real-world scenarios, there could be a large number of personalized users, and their preference vectors may not be known, with only their interaction history accessed. Consequently, developing more realistic alignment benchmarks to effectively assess these techniques is a critical area for future research.
