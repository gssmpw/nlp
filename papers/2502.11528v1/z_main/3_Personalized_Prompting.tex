\section{Personalized Prompting}
\label{sec: personalized prompting}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99999\linewidth]{Figures/Prompting2.pdf}
    \caption{The illustration of personalized prompting approaches: \textbf{a) Profile-Augmented,} \textbf{b) Retrieval-Augmented,} \textbf{c) Soft-Fused.} }
    \label{fig:prompting}
\vspace{-10pt}
\end{figure*}

Prompt engineering acts as a bridge for interaction between users and LLMs. In this survey, prompting involves guiding an LLM to generate desired outputs using various techniques, from traditional text prompts to advanced methods like soft embedding. Soft embedding can be extended not only through input but also via cross-attention or by adjusting output logits, enabling more flexible and context-sensitive responses.


The framework can be expressed as, for each user $u$:

\begin{equation}
y = f_{\text{LLM}} \left(q \oplus \phi\left(\mathcal{C}_u\right) \right),
\end{equation}
where, 
$f_{\text{LLM}}$ is the LLM model that generates the response;
$\phi$ is a function that extracts relevant context from the user's personal context $\mathcal{C}_u$;
$\oplus$ represents the combination operator that fuses the query $q$ and the relevant personalized context $\phi(\mathcal{C}_u)$, producing enriched information for the LLM.


\subsection{Profile-Augmented Prompting}
\label{subsec:PAG}

Profile-augmented prompting methods explicitly utilize summarized user preferences and profiles in natural language to augment LLMsâ€™ input at the token level ($\phi$ is the \textit{summarizer} model).  \figref{fig:prompting}(a) shows the illustration.


\paragraph{Non-tuned Summarizer} A frozen LLM can be directly used as the summarizer to summarize user profiles due to its strong language understanding capabilities, i.e., $\mathcal{\phi}\left(\mathcal{C}_u\right) = f_{\text{LLM}}\left(\mathcal{C}_u\right)$. 
For instance, \textbit{Cue-CoT}~\citep{wang2023cue} employs chain-of-thought prompting for personalized profile augmentation, using LLMs to extract and summarize user status (e.g., emotion, personality, and psychology) from historical dialogues. 
\textbit{PAG}~\citep{richardson2023integrating} leverages instruction-tuned LLMs to pre-summarize user profiles based on historical content. The summaries are stored offline, enabling efficient personalized response generation while meeting runtime constraints.
\textbit{ONCE}~\citep{DBLP:conf/wsdm/LiuCS024} prompts closed-source LLMs to summarize topics and regions of interest from users' browsing history, enhancing personalized recommendations.


\paragraph{Tuned Summarizer} 
Black-box LLMs are sensitive to input noise, like off-topic summaries, and struggle to extract relevant information. Thus, training the summarizer to adapt to user preferences and style is essential.
\textbit{Matryoshka}~\citep{li2024matryoshka} uses a white-box LLM to summarize user histories, similar to PAG, but fine-tunes the summarizer instead of the generator LLM.
\textbit{RewriterSlRl}~\citep{li2024learning}  rewrites the query $q$ instead of concatenating summaries, optimized with supervised and reinforcement learning.
 
\textbit{CoS}~\citep{he2024cos} is a special case that assumes a brief user profile $\phi\left(\mathcal{C}_u\right)$ and amplifies its influence in LLM response generation by comparing output probabilities with and without the profile, adjusting personalization without fine-tuning.


\subsection{Retrieval-Augmented Prompting}
\label{subsec:RAG}

Retrieval-augmented prompting~\citep{gao2023retrieval, fan2024survey,qiu2024entropy} excels at extracting the most relevant records from user data to enhance PLLMs (See \figref{fig:prompting}(b)).
Due to the complexity and volume of user data, many methods use an additional \textit{memory} for more effective retrieval. Common retrievers including sparse (e.g., BM25~\citep{robertson1995okapi}),  and dense retrievers (e.g., Faiss~\citep{johnson2019billion}, Contriever~\citep{izacard2021unsupervised}).
These methods effectively manage the increasing volume of user data within the LLM's context limit, improving relevance and personalization by integrating key evidence from the user's personalized data.


\subsubsection{3.2.1 Personalized Memory Construction} 

This part designs mechanisms for retaining and updating memory to enable efficient retrieval of relevant information.

\paragraph{Non-Parametric Memory} 
This category maintains a token-based database, storing and retrieving information in its original tokenized form without using parameterized vector representations. For example, 
\textbit{MemPrompt}~\citep{madaan2022memory} and \textbit{TeachMe}~\citep{dalvi2022towards} maintain a dictionary-based feedback memory (key-value pairs of mistakes and user feedback).
MemPrompt focuses on prompt-based improvements, whereas TeachMe emphasizes continual learning via dynamic memory that adapts over time.
\textbit{MaLP}~\citep{zhang2024llm} further integrates multiple memory types, leveraging working memory for immediate processing, short-term memory (STM) for quick access, and long-term memory (LTM) for storing key knowledge.


\paragraph{Parametric Memory} 
Recent studies parameterize and project personalized user data into a learnable space, with parametric memory filtering out redundant context to reduce noise.
For instance, 
\textbit{LD-Agent}~\citep{li2024hello} maintains memory with separate short-term and long-term banks, encoding long-term events as parametric vector representations refined by a tunable module and retrieved via an embedding-based mechanism.
\textbit{MemoRAG}~\citep{qian2024memorag}, in contrast, adopts a different approach by utilizing a lightweight LLM as memory to learn user-personalized data. Instead of maintaining a vector database for retrieval, it generates a series of tokens as a draft to further guide the retriever, offering a more dynamic and flexible method for retrieval augmentation.



\subsubsection{3.2.2 Personalized Memory Retrieval}

The key challenge in the personalized retriever design lies in selecting not only relevant but also representative personalized data for downstream tasks. \textbit{LaMP}~\citep{salemi2023lamp} investigates how retrieved personalized information affects the responses of large language models (LLMs) through two mechanisms: in-prompt augmentation (IPA) and fusion-in-decoder (FiD).
\textbit{PEARL}~\citep{mysore2023pearl} and \textbit{ROPG}~\citep{salemi2024optimization} similarly aim to enhance the retriever using personalized generation-calibrated metrics, improving both personalization and text quality of retrieved documents.
Meanwhile, \textbit{HYDRA}~\citep{zhuang2024hydra} trains a reranker to prioritize the most relevant information additionally from top-retrieved historical records for
enhanced personalization.


\subsection{Soft-Fused Prompting}
\label{subsec:soft}

Soft prompting differs from profile-augmented prompting by compressing personalized data into soft embeddings, rather than summarizing it into discrete tokens.
These embeddings
are generated by a user feature \textit{encoder} $\phi$. 

In this survey, we generalize the concept of soft prompting, showing that soft embeddings can be integrated (combination operator $\oplus$) not only through the input but also via cross-attention or by adjusting output logits, allowing for more flexible and context-sensitive responses (See \figref{fig:prompting}(c)).


\paragraph{Input Prefix} 
Soft prompting, used as an input prefix, focuses on the embedding level by concatenating the query embedding with the soft embedding, and is commonly applied in recommendation tasks.
\textbit{UEM}~\citep{doddapaneni2024user} is a user embedding module (transformer network) that generates a soft prompt conditioned on the user's personalized data. 
\textbit{PERSOMA}~\citep{hebert2024persoma} enhances UEM by employing resampling, selectively choosing a subset of user interactions based on relevance and importance. 
\textbit{REGEN}~\citep{sayana2024beyond} combines item embeddings from user-item interactions via collaborative filtering and item descriptions using a soft prompt adapter to generate contextually personalized responses.
\textbit{PeaPOD}~\citep{ramos2024preference} personalizes soft prompts by distilling user preferences into a limited set of learnable, dynamically weighted prompts. Unlike previously mentioned methods, which focus on directly embedding user interactions or resampling relevant data, PeaPOD adapts to user interests by weighting a shared set of prompts. 


\paragraph{Cross-Attention}
Cross-attention enables the model to process and integrate multiple input sources by allowing it to attend to personalized data and the query. 
\textbit{User-LLM}~\citep{ning2024user} uses an autoregressive user encoder to convert historical interactions into embeddings through self-supervised learning, which are then integrated via cross-attention. The system employs joint training to optimize both the retriever and generator for better performance.
\textbit{RECAP}~\citep{liu2023recap} utilizes a hierarchical transformer retriever designed for dialogue domains to fetch personalized information. This information is integrated into response generation via a context-aware prefix encoder, improving the model's ability to generate personalized, contextually relevant responses. 

\paragraph{Output Logits}

\textbit{GSMN}~\citep{wu2021personalized} retrieves relevant information from personalized data, encodes it into soft embeddings, and uses them in attention with the query vector. Afterward, the resulting embeddings are concatenated with the LLM-generated embeddings, modifying the final logits to produce more personalized and contextually relevant responses.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99999\linewidth]{Figures/PEFT2.pdf}
    \caption{The illustration of personalized adaptation approaches: \textbf{a) One PEFT for all users,} \textbf{b) One PEFT per user.}  }
    \label{fig:peft}
\vspace{-8pt}
\end{figure*}


\subsection{Discussions}

The three prompting methods have distinct pros and cons: 1) Profile-augmented prompting improves efficiency by compressing historical data but risks information loss and reduced personalization. 2) Retrieval-augmented prompting offers rich, context-aware inputs and scales well for long-term memory but can suffer from computational limits and irrelevant data retrieval.  3) Soft prompting efficiently embeds user-specific info, capturing semantic nuances without redundancy, but is limited to black-box models and lacks explicit user preference analysis.
Overall, prompting-based methods are efficient and adaptable, and enable dynamic personalization with minimal computational overhead.
However, they lack deeper personalization analysis as they rely on predefined prompt structures to inject user-specific information and are limited in accessing global knowledge due to the narrow scope of prompts.

