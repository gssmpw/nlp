\section{Personalized Adaptation}
\label{sec: personalized adatation}

PLLMs require balancing fine-tuning's deep adaptability with the efficiency of prompting.
Therefore, specialized methods need to be specifically designed for PLLMs to address these challenges utilizing parameter-efficient fine-tuning methods (PEFT),  such as LoRA~\citep{hu2021lora, yang2024low}, IA3~\citep{liu2022few}, etc.  (See~\figref{fig:peft}). 


\subsection{One PEFT All Users}
\label{subsec:one4all}

This method trains on all users' data using a \textit{shared PEFT module}, eliminating the need for separate modules per user. The shared module's architecture can be further categorized.



\paragraph{Single PEFT} 
\textbit{PLoRA}~\citep{DBLP:conf/aaai/ZhangWYXZ24} and \textbit{LM-P}~\citep{wozniak2024personalized} 
utilize LoRA for PEFT of LLM, injecting personalized information via user embeddings and user IDs, respectively. PLoRA is further extended and supports online training and prediction for cold-start scenarios. \textbit{UserIdentifier}~\citep{mireshghallah2021useridentifier} uses a static, non-trainable user identifier to condition the model on user-specific information, avoiding the need for trainable user-specific parameters and reducing training costs. 
\textbit{Review-LLM}~\citep{peng2024llm} aggregates users' historical behaviors and ratings into prompts to guide sentiment and leverages LoRA for efficient fine-tuning.
However, these methods rely on a single architecture with fixed configurations (e.g., hidden size, insertion layers), making them unable to store and activate diverse information for personalization~\citep{zhou2024autopeft}. To solve this problem,
\textbit{MiLP}~\citep{zhang2024personalized} utilizes a Bayesian optimization strategy to automatically identify the optimal configuration for applying multiple LoRA modules, enabling efficient and flexible personalization.


\paragraph{Mixture of Experts (MoE)}


Several methods use the LoRA module, but with a static configuration for all users. This lack of parameter personalization limits adaptability to user dynamics and preference shifts, potentially resulting in suboptimal performance~\citep{cai2024survey}. \textbit{RecLoRA}~\citep{zhu2024lifelong} addresses this limitation by maintaining a set of parallel, independent LoRA weights and employing a soft routing method to aggregate meta-LoRA weights, enabling more personalized and adaptive results. Similarly, 
\textbit{iLoRA}~\citep{kongcustomizing} creates a diverse set of experts (LoRA) to capture specific aspects of user preferences and generates dynamic expert participation weights to adapt to user-specific behaviors.

Shared PEFT methods rely on a centralized approach, where user-specific data is encoded into a shared adapter by centralized LLMs. This limits the model's ability to provide deeply personalized experiences tailored to individual users. Furthermore, using a centralized model often requires users to share personal data with service providers, raising concerns about the storage, usage, and protection of this data.


\subsection{One PEFT Per User}
\label{subsec:one4one}

Equipping \textit{a user-specific PEFT module} makes LLM deployment more personalized while preserving data privacy. However, the challenge lies in ensuring efficient operation in resource-limited environments, as users may lack sufficient local resources to perform fine tuning. 

\paragraph{No Collaboration} 
There is no collaboration or coordination between adapters or during the learning process for each use in this category.
\textbit{UserAdapter}~\citep{zhong2021useradapter} personalizes models through prefix-tuning, fine-tuning a unique prefix vector for each user while keeping the underlying transformer model shared and frozen. 
\textbit{PocketLLM}~\citep{peng2024pocketllm} utilizes a derivative-free optimization approach, based on MeZo~\citep{malladi2023fine}, to fine-tune LLMs on memory-constrained mobile devices. \textbit{OPPU}~\citep{DBLP:conf/emnlp/Tan000Y024} equips each user with a LoRA module.

\paragraph{Collaborative Efforts} 
The ``one-PEFT-per-user" paradigm without collaboration is computationally and storage-intensive, particularly for large user bases. Additionally, individually owned PEFTs hinder community value, as personal models cannot easily share knowledge or benefit from collaborative improvements.
\textbit{PER-PCS}~\citep{tan2024personalized} enables efficient and collaborative PLLMs by sharing a small fraction of PEFT parameters across users. It first divides PEFT parameters into reusable pieces with routing gates and stores them in a shared pool. For each target user, pieces are autoregressively selected from other users, ensuring scalability, efficiency, and personalized adaptation without additional training. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99999\linewidth]{Figures/personalized_alignment.pdf}
    \caption{The illustration of personalized alignment method under the multi-objective reinforcement learning paradigm.}
    \label{fig:personalized_alignment}
\vspace{-10pt}
\end{figure*}


Another efficient collaborative strategy is based on the federated learning (FL) framework.
For example, 
\cite{wagner2024personalized} introduces a FL framework for on-device LLM fine-tuning, using strategies to aggregate LoRA model parameters and handle data heterogeneity efficiently, outperforming purely local fine-tuning. 
\textbit{FDLoRA}~\citep{qi2024fdlora} introduces a personalized FL framework using dual LoRA modules to capture personalized and global knowledge. It shares only global LoRA parameters with a central server and combines them via adaptive fusion, enhancing performance while minimizing communication and computing costs.

There are other frameworks that can be explored, such as \textbit{HYDRA}~\citep{zhuang2024hydra}, which also employs a base model to learn shared knowledge. However, in contrast to federated learning, it assigns distinct heads to each individual user to extract personalized information.



\subsection{Discussions}

Fine-tuning methods enable deep personalization by modifying a large set of model parameters, and parameter-efficient fine-tuning methods (e.g., prefix vectors or adapters) reduce computational cost and memory requirements while maintaining high personalization levels. These methods improve task adaptation by tailoring models to specific user needs, enhancing performance in tasks like sentiment analysis and recommendations. They also offer flexibility, allowing user-specific adjustments while leveraging pre-trained knowledge. However, they still face the risk of overfitting, particularly with limited or noisy user data, which can impact generalization and performance for new or diverse users.