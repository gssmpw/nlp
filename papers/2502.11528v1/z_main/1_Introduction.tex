\section{Introduction}

In recent years, substantial progress has been made in Large Language Models (LLMs) such as GPT, PaLM, LLaMA, DeepSeek, and their variants. These models have demonstrated remarkable versatility, achieving state-of-the-art performance across various natural language processing tasks, including question answering, reasoning, and machine translation~\citep{zhao2023survey}, with minimal task-specific adaptation.

\paragraph{The Necessity of Personalized LLMs (PLLMs)} While LLMs excel in general knowledge and multi-domain reasoning, their lack of personalization creates challenges in situations where user-specific understanding is crucial. For instance, conversational agents need to adapt to a user's preferred tone and incorporate past interactions to deliver relevant, personalized responses. 
8As LLMs evolve, integrating personalization capabilities has become a promising direction for advancing human-AI interaction across diverse domains such as education, healthcare, and finance~\citep{hu-etal-2024-serts,zhang2024memsim,zhang2024survey, zhu2024lifelong,wang2023large,wang2024tpe}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.99999\linewidth]{Figures/Framework2.pdf}
    \caption{Illustration of PLLM techniques for generating personalized responses through three levels: prompting, adaptation, and alignment.}
    \label{fig:framework}
\vspace{-10pt}
\end{figure}

\paragraph{Technical Challenges} Despite its promise, personalizing LLMs presents several challenges. These include efficiently representing and integrating diverse user data, addressing privacy concerns, managing long-term user memories, 
adaptivity accommodating users' diverse needs and evolving behaviors
~\citep{salemi2023lamp}. 
Moreover, achieving personalization often requires balancing accuracy and efficiency while addressing biases and maintaining fairness in the generated outputs.

\input{Figures/Taxonomy}



\paragraph{Contributions} Despite growing interest, the field of PLLMs lacks a systematic review that consolidates recent advancements.
This survey aims to bridge the gap by systematically organizing existing research on PLLMs and offering insights into their methodologies and future directions. The contributions of this survey can be summarized as follows:
\textit{(1) A structured taxonomy:} We propose a comprehensive taxonomy, providing a technical perspective on the existing approaches to building PLLMs.
\textit{(2) A comprehensive review:} We systematically review state-of-the-art methods for PLLMs, analyzing fine-grained differences among the methods.
\textit{(3) Future directions:} We highlight current limitations, such as data privacy and bias, and outline promising avenues for future research, including multimodal personalization, edge computing, lifelong updating, trustworthiness, and other emerging challenges.


