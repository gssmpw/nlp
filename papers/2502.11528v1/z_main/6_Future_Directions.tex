\section{Future Directions}

Despite recent advances in PLLMs, challenges and opportunities remain. This section discusses key limitations and promising future research directions.

\paragraph{Complex User Data}
While current approaches effectively handle basic user preferences, processing complex, multi-source user data remains a significant challenge. For example, methods that use user relationships in graph-like structures are still limited to retrieval augmentation ~\citep{du2024perltqa}. How to effectively leverage this complex user information to fine-tune LLM parameters remains a significant challenge. 
Most methods focus on text data, while personalized foundation models for multimodal data (e.g., images, videos, audio) remain underexplored, despite their significance for real-world deployment and applications~\citep{wu2024personalized, pi2024personalized}.

\paragraph{Edge Computing} 
A key challenge in edge computing is efficiently updating models on resource-constrained devices (e.g., phones), where storage and computational resources are limited. For example, fine-tuning offers deeper personalization but is resource-intensive and hard to scale, especially in real-time applications. Balancing resources with personalization needs is important. 
A potential solution is to build personalized small models~\citep{lu2024small} for edge devices, using techniques like quantization and distillation.

\paragraph{Edge-Cloud Collaboration}
The deployment of PLLMs in real-world scenarios encounters significant challenges in edge-cloud computing environments. Current approaches utilizing collaborative efforts often lack efficient synchronization mechanisms between cloud and edge devices. This highlights the need to explore the balance between local computation and cloud processing for PLLMs ~\citep{tian2024edge}.


\paragraph{Efficient Adaptation to Model Updates}
When the base LLM parameters are updated, such as with a new version, efficiently adapting the fine-tuned PEFT parameters for each user becomes a challenge. Given the large volume of user data and limited resources, the cost of retraining can be prohibitive. Future research should focus on efficient strategies for updating user-specific parameters without requiring complete retraining, such as leveraging incremental learning, transfer learning, or more resource-efficient fine-tuning techniques.

\paragraph{Lifelong Updating}
Given the large variety of user behaviors, a key challenge is preventing catastrophic forgetting while ensuring the efficient update of long-term and short-term of the memory. Future research could explore continual learning~\citep{wu2024continual} and knowledge editing~\citep{wang2024knowledge} to facilitate dynamic updates of user-specific information.


\paragraph{Trustworthy} 
Ensuring user privacy is crucial, especially when summarized or retrieved data is used to generate personalized responses. Since LLMs cannot be deployed locally due to resource limits, there is a risk of privacy leakage. Future research could focus on privacy-preserving methods like federated learning, secure computation, and differential privacy to protect user data~\citep{yao2024federated, liu2024client}.







