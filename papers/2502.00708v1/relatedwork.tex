\section{Related Work}
\begin{figure*}[!h]
\begin{center}
\centerline{\includegraphics[scale=0.55]{overview.pdf}}
\vskip -0.2in
\caption{\textbf{Overview of PhiP-G.} Given a complex scene description, PhiP-G employs an LLM-based agent to perform text analysis and construct a scene graph. Graph-based 3D asset generation is carried out using a 2D generation agent and the 3D Gaussian model, where the 2D asset with the highest CLIP score is stored in the 2D retrieval library for future use. 
Subsequently, Blender serves as the foundational environment, where a  world model consisting of the physical pool and a visual supervision agent enables coarse layout and iterative refinement. PhiP-G ensures improved semantic consistency and physical coherence in the generated scene.}
\label{overview}  
\end{center}
\vskip -0.4in
\end{figure*}
\textbf{3DGS for text-to-scene generation.}
Traditional text-to-3D methods primarily rely on generative approaches based on adversarial networks or variational autoencoders \citep{gan1,gan2,ferreira2022gan,VAE1,VAE2,petrovich2021action}, utilizing 2D images or textual descriptions to infer and generate complex 3D shapes, which are often computationally expensive and slow to produce. The recently popular 3DGS \citep{Splatting} demonstrates a method for representing 3D spaces by optimizing 3D Gaussian spheres, enabling fast rendering and making it popular in 3D scene reconstruction. \citep{chung2023luciddreamer} generates 3D scenes through image inpainting with stable diffusion \citep{stablediffusion}, using reference images or text to expand different viewpoints. GALA3D \citep{Gala3d} utilizes object-level text-to-3D modeling, and MVDream \citep{MVDream} generates realistic objects, combining them using scene-level diffusion models. Text-to-scene generation methods based on 3DGS excel in specific tasks but struggle with consistency and stability in complex, detail-rich scenes. Moreover, their training demands substantial computational resources. Our approach integrates 3DGS with LLM-based scene layout, utilizing 3DGS for individual asset generation to enhance speed and stability while ensuring broad compatibility with most 3DGS methods without requiring additional training.

\textbf{Multimodal LLM agents for scene generation.} With the advancement of multimodal LLMs, models equipped with visual perception capabilities, such as GPT-V \citep{openai2023gpt4v}, have become increasingly sophisticated. Consequently, there is a growing interest in integrating multimodal LLMs into 3D scene generation tasks. For instance, Holodeck \citep{Holodeck} employs a multi-stage process to transform initial 3D scene layouts derived from text into realistic environments. The process uses ChatGPT-4 \citep{openai2023gpt4v} for spatial reasoning, layout generation, material selection, and object arrangement, optimizing spatial relationships to achieve realistic 3D interactions. Similarly, SceneCraft \citep{SCENECRAFT} utilizes a modular architecture with LLMs to iteratively convert textual descriptions into 3D spatial layouts, object selection, and attribute settings, enabling interactive 3D world creation and code generation from natural language instructions. However, most LLM-based scene generation methods rely on LLMs for reasoning, code generation, and asset retrieval, but are limited by their inability to independently create 3D assets and their dependence on pre-existing libraries. In contrast, our approach fully leverages the advanced multimodal capabilities of LLMs and integrates a 3DGS model, allowing for the free generation of target 3D assets from scene descriptions.