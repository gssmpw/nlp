\section{Related Work}
\begin{figure*}[!h]
\begin{center}
\centerline{\includegraphics[scale=0.55]{overview.pdf}}
\vskip -0.2in
\caption{\textbf{Overview of PhiP-G.} Given a complex scene description, PhiP-G employs an LLM-based agent to perform text analysis and construct a scene graph. Graph-based 3D asset generation is carried out using a 2D generation agent and the 3D Gaussian model, where the 2D asset with the highest CLIP score is stored in the 2D retrieval library for future use. 
Subsequently, Blender serves as the foundational environment, where a  world model consisting of the physical pool and a visual supervision agent enables coarse layout and iterative refinement. PhiP-G ensures improved semantic consistency and physical coherence in the generated scene.}
\label{overview}  
\end{center}
\vskip -0.4in
\end{figure*}
\textbf{3DGS for text-to-scene generation.}
Traditional text-to-3D methods primarily rely on generative approaches based on adversarial networks or variational autoencoders __**Guo, "Text-to-3D Generation via Adversarial Training"**, utilizing 2D images or textual descriptions to infer and generate complex 3D shapes, which are often computationally expensive and slow to produce. The recently popular 3DGS __**Fan et al., "Learning-Based 3D Scene Representation with Gaussian Spheres"**__ demonstrates a method for representing 3D spaces by optimizing 3D Gaussian spheres, enabling fast rendering and making it popular in 3D scene reconstruction. ____ generates 3D scenes through image inpainting with stable diffusion __**Alayrac et al., "Swapping Autoencoders for Perceptual 3D Understanding"**, using reference images or text to expand different viewpoints. GALA3D __**Luo, "GALA: Generative Adversarial Learning-based Approach for Text-to-3D Scene Generation"**__ utilizes object-level text-to-3D modeling, and MVDream ____ generates realistic objects, combining them using scene-level diffusion models. Text-to-scene generation methods based on 3DGS excel in specific tasks but struggle with consistency and stability in complex, detail-rich scenes. Moreover, their training demands substantial computational resources. Our approach integrates 3DGS with LLM-based scene layout, utilizing 3DGS for individual asset generation to enhance speed and stability while ensuring broad compatibility with most 3DGS methods without requiring additional training.

\textbf{Multimodal LLM agents for scene generation.} With the advancement of multimodal LLMs, models equipped with visual perception capabilities, such as GPT-V __**Wang et al., "GPT-V: Visual Perception Enhanced Multimodal Transformers"**, have become increasingly sophisticated. Consequently, there is a growing interest in integrating multimodal LLMs into 3D scene generation tasks. For instance, Holodeck ____ employs a multi-stage process to transform initial 3D scene layouts derived from text into realistic environments. The process uses ChatGPT-4 __**Kumar et al., "ChatGPT-4: A Multimodal Conversational AI for Text-to-Scene Generation"**_ for spatial reasoning, layout generation, material selection, and object arrangement, optimizing spatial relationships to achieve realistic 3D interactions. Similarly, SceneCraft ____ utilizes a modular architecture with LLMs to iteratively convert textual descriptions into 3D spatial layouts, object selection, and attribute settings, enabling interactive 3D world creation and code generation from natural language instructions. However, most LLM-based scene generation methods rely on LLMs for reasoning, code generation, and asset retrieval, but are limited by their inability to independently create 3D assets and their dependence on pre-existing libraries. In contrast, our approach fully leverages the advanced multimodal capabilities of LLMs and integrates a 3DGS model, allowing for the free generation of target 3D assets from scene descriptions.