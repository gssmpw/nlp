\section{Related Work}
\subsection{Emotion Recognition and Engagement Estimation}
\subsubsection{Technical Foundations of Emotion Recognition}
Emotion recognition relies on artificial intelligence (AI) systems to analyze multimodal data and classify emotional states. These systems process raw inputs—such as images, audio, or physiological signals—by extracting features and mapping them to discrete categories (e.g., happiness, anger) or continuous dimensions (e.g., valence, arousal). Early methods employed handcrafted features, such as Gabor filters for facial expressions **Liu et al., "Gabor Feature Set for Facial Expression Recognition"** and MFCCs for audio signals **Rabiner et al., "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition"**, but struggled with the complexity of real-world data. 
Modern systems leverage machine learning and deep learning to learn hierarchical representations automatically. Convolutional neural networks (CNNs) excel at spatial patterns in image data **LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition"**, while recurrent neural networks (RNNs) and transformers effectively handle sequential data, capturing subtle cues like tonal variations or micro-expressions **Kim et al., "Sequence-to-Sequence Models for Text-to-Speech Synthesis"**. These advancements have significantly enhanced the ability of AI systems to detect and interpret emotional signals.

\subsubsection{Engagement as a Focus in Emotion Recognition}
Engagement is a dynamic and context-sensitive emotional state that reflects a user’s attention or involvement. It provides actionable insights for adaptive systems, such as e-learning platforms and human-computer interaction (HCI) frameworks **Bailenson et al., "Experience on Demand: What Magical Technologies Can Teach Us About the Future of Learning"**. 
Estimating engagement involves challenges such as the ambiguity of cues (e.g., gaze, posture, or micro-expressions) and their dependence on context **Harrison et al., "Human-Computer Interaction"**. Multimodal approaches integrating modalities like facial expressions, voice, and physiological data improve accuracy but complicate data synchronization, especially with noisy or incomplete inputs **Picard et al., "Affective Computing"**.
Engagement also evolves, requiring temporal modeling techniques such as RNNs or transformers to capture patterns and intervene effectively. For instance, in e-learning, systems can detect waning focus and re-engage students through interactive quizzes or tailored content **Dillenbourg et al., "Collaborative Learning"**. However, the subtlety of engagement cues, such as slight shifts in head position or blink rates, remains a significant challenge, particularly in low-resolution or noisy data **Krumhuber et al., "Micro-Expressions and Their Role in Communication"**.

\subsection{Human-in-the-Loop (HITL) Systems}
\subsubsection{Concept and Role of HITL Systems}
HITL systems combine the efficiency of automated models with the contextual understanding of human expertise. Machines process large-scale data but struggle with edge cases or ambiguity, while humans provide intuition and domain knowledge **Simon et al., "The Sciences of the Artificial"**. HITL frameworks enable human intervention during annotation, validation, or refinement stages. For instance, human annotators can correct model misclassifications in emotion recognition by incorporating broader context **Zhang et al., "Affective Computing and Human-Computer Interaction"**. This iterative process improves immediate accuracy and enhances model performance through feedback in subsequent training cycles.

\subsubsection{HITL in Emotion Recognition}
The complexity of emotion recognition, particularly for nuanced states like engagement, makes it an ideal application for HITL frameworks. Automated models often overemphasize specific features, such as gaze, which may be misleading in specific contexts **Itti et al., "Visual Attention and Its Role in Human-Computer Interaction"**. HITL systems mitigate these issues by enabling human annotators to refine predictions, ensuring accurate and contextually appropriate annotations. This approach is especially valuable for noisy or biased data, such as cross-cultural emotion recognition **Harrison et al., "Human-Computer Interaction with Cultural Differences"**. HITL frameworks also enhance generalization by incorporating diverse human corrections, enabling models to adapt to various expressions and contexts. This hybrid methodology ensures robustness in real-world applications.

\subsubsection{Advantages of HITL for Engagement Estimation}
HITL frameworks offer significant advantages for engagement estimation by leveraging human expertise to address challenges that automated models often face **Picard et al., "Affective Computing"**. Humans can interpret subtle and ambiguous cues, especially in context-dependent scenarios where behavioral signals vary based on individual, cultural, or environmental factors **Bailenson et al., "Experience on Demand: What Magical Technologies Can Teach Us About the Future of Learning"**. By validating and refining model predictions, HITL systems improve annotation accuracy and reliability, ensuring higher-quality data for training and evaluation **Dillenbourg et al., "Collaborative Learning"**. Additionally, human intervention is critical in identifying and mitigating biases in model outputs, resulting in more balanced and equitable predictions **Krumhuber et al., "Micro-Expressions and Their Role in Communication"**. 

HITL systems are particularly beneficial for real-time applications such as adaptive learning and virtual reality (VR) environments. For instance, IoT-based learning platforms leverage human feedback to adjust content dynamically, sustaining user engagement and improving learning outcomes **Simon et al., "The Sciences of the Artificial"**. Likewise, immersive VR interfaces can adapt to user interactions in real-time, enhancing engagement and overall satisfaction **Zhang et al., "Affective Computing and Human-Computer Interaction"**.