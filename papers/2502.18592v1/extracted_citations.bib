@InProceedings{Kolouri_2020_CVPR,
author = {Kolouri, Soheil and Saha, Aniruddha and Pirsiavash, Hamed and Hoffmann, Heiko},
title = {Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@ARTICLE{Shi2022multimodalattackrs,
  author={Shi, Cheng and Dang, Yenan and Fang, Li and Zhao, Minghua and Lv, Zhiyong and Miao, Qiguang and Pun, Chi-Man},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Multifeature Collaborative Adversarial Attack in Multimodal Remote Sensing Image Classification}, 
  year={2022},
  volume={60},
  number={},
  pages={1-15},
  keywords={Perturbation methods;Collaboration;Task analysis;Training;Mathematical models;Generators;Deep learning;Generative adversarial networks (GANs);multimodal adversarial attack;multimodal remote sensing (RS) image classification},
  doi={10.1109/TGRS.2022.3208337}
}

@phdthesis{Vartak2022thesis,
author={Vartak, Akash A.},
year={2022},
title={Using Text to Improve Classification of Man-Made Objects},
school={University of Maryland, Baltimore County},
journal={ProQuest Dissertations and Theses},
pages={55},
note={Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2023-08-04},
abstract={People identify man-made objects by their visual appearance and the text on them e.g., does a bottle say water or shampoo? We use text as an important visual cue to help distinguish between similar looking objects. This thesis explores a novel joint model of visual appearance and textual cues for image classification.We perform this in three functions - (a) Isolating an object in an input image; (b) Extracting text from the image; (c) Training a joint vision/text model. We simplify the task by extracting text separately and presenting it to the model in machine readable format. Such a joint model has utility in many real world challenges where language is interpreted through a sensory perception like vision or sound.The aim of the research is to understand whether visual percepts, when understood in the context of extracted language, will provide a better classification of image objects than using only pure vision to perform image classification. In conclusion, we show that joint classifier models can successfully make use of text present in images to classify objects, provided that the extracted text from images is of high quality and we have the number of images proportional to the number of classification classes.},
keywords={Classification; Computer vision; Deep learning; Machine learning; Natural language processing; Text extraction; Computer science; 0984:Computer science},
isbn={9798837525858},
language={English},
url={http://proxy-bc.researchport.umd.edu/login?url=https://www.proquest.com/dissertations-theses/using-text-improve-classification-man-made/docview/2695026845/se-2},
}

@inproceedings{Zhang2022multimodalattack,
author = {Zhang, Jiaming and Yi, Qi and Sang, Jitao},
title = {Towards Adversarial Attack on Vision-Language Pre-training Models},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547801},
doi = {10.1145/3503161.3547801},
abstract = {While vision-language pre-training model (VLP) has shown revolutionary improvements on various vision-language (V+L) tasks, the studies regarding its adversarial robustness remain largely unexplored. This paper studied the adversarial attack on popular VLP models and V+L tasks. First, we analyzed the performance of adversarial attacks under different settings. By examining the influence of different perturbed objects and attack targets, we concluded some key observations as guidance on both designing strong multimodal adversarial attack and constructing robust VLP models. Second, we proposed a novel multimodal attack method on the VLP models called Collaborative Multimodal Adversarial Attack (Co-Attack), which collectively carries out the attacks on the image modality and the text modality. Experimental results demonstrated that the proposed method achieves improved attack performances on different V+L downstream tasks and VLP models. The analysis observations and novel attack method hopefully provide new understanding into the adversarial robustness of VLP models, so as to contribute their safe and reliable deployment in more real-world scenarios.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5005â€“5013},
numpages = {9},
keywords = {adversarial attack, multimodal, vision-and-language pre-training},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{chen2018detecting,
  title={Detecting backdoor attacks on deep neural networks by activation clustering},
  author={Chen, Bryant and Carvalho, Wilka and Baracaldo, Nathalie and Ludwig, Heiko and Edwards, Benjamin and Lee, Taesung and Molloy, Ian and Srivastava, Biplav},
  journal={arXiv preprint arXiv:1811.03728},
  year={2018}
}

@inproceedings{chen2021badnl,
  title={Badnl: Backdoor attacks against nlp models with semantic-preserving improvements},
  author={Chen, Xiaoyi and Salem, Ahmed and Chen, Dingfan and Backes, Michael and Ma, Shiqing and Shen, Qingni and Wu, Zhonghai and Zhang, Yang},
  booktitle={Annual Computer Security Applications Conference},
  pages={554--569},
  year={2021}
}

@inproceedings{chou2020sentinet,
  title={SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems},
  author={Chou, Edward and Tramer, Florian and Pellegrino, Giancarlo},
  booktitle={2020 IEEE Security and Privacy Workshops (SPW)},
  pages={48--54},
  year={2020},
  organization={IEEE}
}

@article{gu2017badnets,
  title={Badnets: Identifying vulnerabilities in the machine learning model supply chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:1708.06733},
  year={2017}
}

@article{guo2019tabor,
  title={Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in ai systems},
  author={Guo, Wenbo and Wang, Lun and Xing, Xinyu and Du, Min and Song, Dawn},
  journal={arXiv preprint arXiv:1908.01763},
  year={2019}
}

@inproceedings{kiourti2020trojdrl,
  title={TrojDRL: evaluation of backdoor attacks on deep reinforcement learning},
  author={Kiourti, Panagiota and Wardega, Kacper and Jha, Susmit and Li, Wenchao},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@inproceedings{liu2017trojaning,
  title={Trojaning attack on neural networks},
  author={Liu, Yingqi and Ma, Shiqing and Aafer, Yousra and Lee, Wen-Chuan and Zhai, Juan and Wang, Weihang and Zhang, Xiangyu},
  booktitle={NDSS},
  year={2017}
}

@inproceedings{liu2019abs,
  title={Abs: Scanning neural networks for back-doors by artificial brain stimulation},
  author={Liu, Yingqi and Lee, Wen-Chuan and Tao, Guanhong and Ma, Shiqing and Aafer, Yousra and Zhang, Xiangyu},
  booktitle={Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1265--1282},
  year={2019}
}

@misc{turner2018clean,
  title={Clean-label backdoor attacks},
  author={Turner, Alexander and Tsipras, Dimitris and Madry, Aleksander},
  year={2018}
}

@inproceedings{wang2019neural,
  title={Neural cleanse: Identifying and mitigating backdoor attacks in neural networks},
  author={Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
  booktitle={2019 IEEE Symposium on Security and Privacy (SP)},
  pages={707--723},
  year={2019},
  organization={IEEE}
}

@inproceedings{wang2020practical,
  title={Practical detection of trojan neural networks: Data-limited and data-free cases},
  author={Wang, Ren and Zhang, Gaoyuan and Liu, Sijia and Chen, Pin-Yu and Xiong, Jinjun and Wang, Meng},
  booktitle={European Conference on Computer Vision},
  pages={222--238},
  year={2020},
  organization={Springer}
}

