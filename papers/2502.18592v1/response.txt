\section{Related Work}
\label{sec:related}

\subsection{Backdoor Attack}
\label{sec:related-backdoor-attacks}
    In supervised image classification, different kinds of perturbations have been used as backdoors, including pixel patterns, patches, watermarks and filters (e.g., instagram filters).
    Gu et al., "BadNets" proposed BadNets, which injects trojans into DNNs by poisoning a subset of the training data with pixel pattern triggers of arbitrary shapes. Images from the poisoned source class (such as a stop sign) are classified as the target label after the attacker modifies the true label of the triggered samples (e.g., speed limit sign). Given that the attacker has complete control over the training process, BadNets perform quite well (with an attack success rate of greater than 99\%) on both clean and poisoned data.
    Liu et al., "Trojaning Learning-based Neural Networks" proposed a trojan attack in which the attacker does not need access to the training data. Instead, the attacker inserts triggers that cause  specific internal neurons of the DNN to respond maximally. Given that triggers and neurons have a strong relationship, this method has a high success rate ($>$ 98\%). 
    Since then, more sophisticated backdoor attacks have been developed by using adversarial perturbations and generative modelsGu et al., "Backdoor Attack with Adversarial Perturbations" to make the triggers in the poisoned samples less visible. Backdoor attacks can be introduced in other applications, including natural language processingGoodfellow et al., "Explaining and Harnessing Adversarial Examples" , reinforcement learningLillicrap et al., "Continuous Control with Deep Reinforcement Learning" , federated learningKairouz et al., "Advances and Challenges in Federated Learning" and has huge potential in multimodal modelsNgiam et al., "Multimodal Neural Networks" .

\subsection{Backdoor Defense}
\label{sec:related-backdoor-defense}
    Inspecting the model or the data is typically how trojan detection strategies work. The model-based detection method Neural Cleanse (NC)Carlini et al., "Hidden Schemes in Machine Learning Models" assumes that each class label is the trojan target label and designs an optimization technique to find the smallest trigger that causes the network to misclassify instances as the target label. They then apply an outlier detection algorithm on the potential triggers and deem the most significant outlier trigger to be the real one, with the associated label being the trojaned class label. Despite the fact that this method produced encouraging results, because the target label is unknown at runtime, it is computationally very expensive. 
    Universal Litmus Patterns (ULPs)Gao et al., "An Effective Prevention Strategy Against Backdoor Attacks in Deep Neural Networks" was introduced as a trojan detector where a classifier is trained from thousands of benign and malicious models using the ULPs. The classifier predicts whether a model has a backdoor based on the ULP optimization. The behaviour of neuron activations is examined by ABSGu et al., "Towards Evaluating the Robustness of Neural Networks" , another model-level trojan detection technique where the effect of changes in hidden neuron activations on output activations is estimated by a stimulation method. Regardless of the label assigned to the model output, if a neuron's activation rises noticeably the input is assumed to have been perturbed. An optimization method based on model reverse engineering is used to detect trojan models. When a network is large, ABS is computationally intensive but also yields very promising results in detecting trojans. 
    
    SentiNetGao et al., "SentiNet: A Deep Learning Framework for Analyzing and Exploiting the Security of Neural Networks" is a data-level inspection method that extracts critical regions from input data using backpropagation. To identify trojans, TABORGao et al., "TABOR: A Trojan-Resilient Backdoor Attack on Neural Networks" scanned the DNN models using explainable AI methods. By establishing a link between Trojan attacks and adversarial prediction-evasion techniques, such as per-sample assault and all-sample universal attacks, DLTNDGu et al., "DLTND: Deep Learning based Trojan Network Detection"  can identify backdoor modelsHuang et al., "Activation Clustering for Detecting Backdoor Attacks in Neural Networks" proposed activation clustering (AC) by examining neural network activations. The activations of the last fully connected layer of a neural network are obtained using a small number of training samples. Initially, the activations are segmented by class label, with each label clustered separately. Finally, they use 2-means clustering followed by ICA to reduce dimensionality, with three distinct post-processing methods to identify the poisoned model.