\section{Related Work}
\label{sec:related}

\subsection{Backdoor Attack}
\label{sec:related-backdoor-attacks}
    In supervised image classification, different kinds of perturbations have been used as backdoors, including pixel patterns, patches, watermarks and filters (e.g., instagram filters).
    ____ proposed BadNets, which injects trojans into DNNs by poisoning a subset of the training data with pixel pattern triggers of arbitrary shapes. Images from the poisoned source class (such as a stop sign) are classified as the target label after the attacker modifies the true label of the triggered samples (e.g., speed limit sign). Given that the attacker has complete control over the training process, BadNets perform quite well (with an attack success rate of greater than 99\%) on both clean and poisoned data.
    ____ proposed a trojan attack in which the attacker does not need access to the training data. Instead, the attacker inserts triggers that cause  specific internal neurons of the DNN to respond maximally. Given that triggers and neurons have a strong relationship, this method has a high success rate ($>$ 98\%). 
    Since then, more sophisticated backdoor attacks have been developed by using adversarial perturbations and generative models____ to make the triggers in the poisoned samples less visible. Backdoor attacks can be introduced in other applications, including natural language processing____, reinforcement learning____, federated learning and has huge potential in multimodal models____.

\subsection{Backdoor Defense}
\label{sec:related-backdoor-defense}
    Inspecting the model or the data is typically how trojan detection strategies work. The model-based detection method Neural Cleanse (NC)____ assumes that each class label is the trojan target label and designs an optimization technique to find the smallest trigger that causes the network to misclassify instances as the target label. They then apply an outlier detection algorithm on the potential triggers and deem the most significant outlier trigger to be the real one, with the associated label being the trojaned class label. Despite the fact that this method produced encouraging results, because the target label is unknown at runtime, it is computationally very expensive. 
    Universal Litmus Patterns (ULPs)____ was introduced as a trojan detector where a classifier is trained from thousands of benign and malicious models using the ULPs. The classifier predicts whether a model has a backdoor based on the ULP optimization. The behaviour of neuron activations is examined by ABS____, another model-level trojan detection technique where the effect of changes in hidden neuron activations on output activations is estimated by a stimulation method. Regardless of the label assigned to the model output, if a neuron's activation rises noticeably the input is assumed to have been perturbed. An optimization method based on model reverse engineering is used to detect trojan models. When a network is large, ABS is computationally intensive but also yields very promising results in detecting trojans. 
    
    SentiNet____ is a data-level inspection method that extracts critical regions from input data using backpropagation. To identify trojans, TABOR____ scanned the DNN models using explainable AI methods. By establishing a link between Trojan attacks and adversarial prediction-evasion techniques, such as per-sample assault and all-sample universal attacks, DLTND____  can identify backdoor models.____ proposed activation clustering (AC) by examining neural network activations. The activations of the last fully connected layer of a neural network are obtained using a small number of training samples. Initially, the activations are segmented by class label, with each label clustered separately. Finally, they use 2-means clustering followed by ICA to reduce dimensionality, with three distinct post-processing methods to identify the poisoned model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%