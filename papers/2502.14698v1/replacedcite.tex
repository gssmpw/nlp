\section{Related Work}
The proposed Delta Variance family bridges and extends Bayesian, frequentist and heuristic notions of variance. Furthermore it generalizes related work by considering explicit and implicit quantities of interest other than the neural network itself $\derivedQtheta \neq \ftheta$ and permits learning improved covariances $\PosteriorCovarianceMatrix$ -- see Section~\ref{sec:delta_illustations_and_extensions}. Below we give a brief historic account of related methods that mostly consider the $\derivedQtheta=\ftheta$ case.


\paragraph{Delta Method}
The Delta Method dates back to ____, ____ and ____ in the context of error propagation and received a modern treatment by ____ -- see____ for a historical account. ____ apply the Delta Method to the outputs of neural networks $\ftheta(x)$ and ____ improves computational efficiency. When applied to neural networks the Delta Method requires strong assumptions about the posterior (e.g. unique optimum) or training process, which have not been proven to hold.
Delta Variances --  named after the Delta Method -- provide multiple alternative theoretical justification through its unifying perspective. Furthermore Delta Variances generalize to the $\derivedQtheta\neq\ftheta$ case and other $\PosteriorCovarianceMatrix$.

\paragraph{Laplace Approximation}
Building on work by ____, ____ and ____ apply the Laplace approximation to neural networks. 
Approximating functions at an optimum by what should later be called a Gaussian distribution dates back to ____. 
While only applicable to a single optimum ____ heuristically argues for its applicability to posterior distributions of neural networks. Given such Gaussian posterior approximation they apply the Delta Method yielding a special instance of the Delta Variance family with $\derivedQtheta=\ftheta$ and $\Sigma=\iHessian$ -- see Section~\ref{sec:delta_bayesian_derivation}.

\paragraph{Influence Functions and Jackknife Methods}
Influence functions were proposed in ____ concurrently with the closely related Infinitesimal Jackknife by ____ which approximates cross validation ____. ____ apply the influence function analysis to neural networks to evaluate how training data influences predictions. In Sections~\ref{sec:delta_frequentist_derivation} and \ref{sec:delta_adverserial_derivation} we apply similar techniques to general quantities of interest different from $\ftheta$.


\paragraph{Uncertainty Estimation for Deep Neural Networks}
We focus our comparison on two popular methods: ____ train multiple neural networks to form an ensemble and ____ which re-interprets Monte-Carlo dropout as variational inference. In Table~\ref{tab:delta_benefits} we compare their properties with Delta Variances observing that they come at larger inference cost.
____ aims to reduce the training costs of ensemble methods. To this extent they change the neural network architecture and training procedure, however how to reduce the remaining $k$-fold inference cost and memory requirements remain open research questions. Other popular methods come with similar requirements to change the architecture or training procedure ____,
while approaches like ____ are of non-parametric flavour exhibiting inference cost that increases with the dataset size. 
%
SWAG ____ reduces the training and memory cost by considering an ensemble of parameters from a single learning trajectory with stochastic gradient descent and approximating it with a Gaussian posterior. For inference they employ expensive k-fold sampling. We note that it is natural to derive a SWAG-inspired Delta Variances that employs the $\Sigma$ from SWAG inside the computationally efficient Delta Variance formula -- we leave those considerations for future research.
%
Finally
____ propose a Delta Method inspired approach to approximate Epistemic Variance with an ensemble of two predictors and ____ learn scale parameters in Gaussian prior distributions for transfer learning.