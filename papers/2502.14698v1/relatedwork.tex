\section{Related Work}
The proposed Delta Variance family bridges and extends Bayesian, frequentist and heuristic notions of variance. Furthermore it generalizes related work by considering explicit and implicit quantities of interest other than the neural network itself $\derivedQtheta \neq \ftheta$ and permits learning improved covariances $\PosteriorCovarianceMatrix$ -- see Section~\ref{sec:delta_illustations_and_extensions}. Below we give a brief historic account of related methods that mostly consider the $\derivedQtheta=\ftheta$ case.


\paragraph{Delta Method}
The Delta Method dates back to \citet{cotes:1722Harmonia}, \citet{Lambert:1765Beytraege} and \citet{gauss:1823ErrorTheory} in the context of error propagation and received a modern treatment by \citet{Kelley:1928CrossroadsMind,Wright:1934PathCoefficients, Doob:1935LimitingDistributions,Dorfman:1938note} -- see~\citet{Gorroochurn:2020WhoInventedDelta} for a historical account. \citet{Denker:1990Transforming} apply the Delta Method to the outputs of neural networks $\ftheta(x)$ and \citet{Nilsen:2022DeltaMethod} improves computational efficiency. When applied to neural networks the Delta Method requires strong assumptions about the posterior (e.g. unique optimum) or training process, which have not been proven to hold.
Delta Variances --  named after the Delta Method -- provide multiple alternative theoretical justification through its unifying perspective. Furthermore Delta Variances generalize to the $\derivedQtheta\neq\ftheta$ case and other $\PosteriorCovarianceMatrix$.

\paragraph{Laplace Approximation}
Building on work by \citet{Gull:1989MaximumEntropy}, \citet{MacKay:92b} and \citet{ritter2018:laplace} apply the Laplace approximation to neural networks. 
Approximating functions at an optimum by what should later be called a Gaussian distribution dates back to \citet{laplace1774:integral}. 
While only applicable to a single optimum \citet{MacKay:92b} heuristically argues for its applicability to posterior distributions of neural networks. Given such Gaussian posterior approximation they apply the Delta Method yielding a special instance of the Delta Variance family with $\derivedQtheta=\ftheta$ and $\Sigma=\iHessian$ -- see Section~\ref{sec:delta_bayesian_derivation}.

\paragraph{Influence Functions and Jackknife Methods}
Influence functions were proposed in \citet{Hampel:1974Influence} concurrently with the closely related Infinitesimal Jackknife by \citet{Jaeckel:1972InfinitesimalJackknife} which approximates cross validation \cite{Quenouille:1949Correlation}. \citet{Koh:2017UnderstandingBlackBox} apply the influence function analysis to neural networks to evaluate how training data influences predictions. In Sections~\ref{sec:delta_frequentist_derivation} and \ref{sec:delta_adverserial_derivation} we apply similar techniques to general quantities of interest different from $\ftheta$.


\paragraph{Uncertainty Estimation for Deep Neural Networks}
We focus our comparison on two popular methods: \citet{Lakshminarayanan2017:ensembles} train multiple neural networks to form an ensemble and \citet{Gal2016:dropout} which re-interprets Monte-Carlo dropout as variational inference. In Table~\ref{tab:delta_benefits} we compare their properties with Delta Variances observing that they come at larger inference cost.
\citet{Osband2023:EpistemicNeuralNetworks} aims to reduce the training costs of ensemble methods. To this extent they change the neural network architecture and training procedure, however how to reduce the remaining $k$-fold inference cost and memory requirements remain open research questions. Other popular methods come with similar requirements to change the architecture or training procedure \citep{blundell2015:BBB,Amersfoort:2020UncertaintyDUQ,Immer:2021LocalLinearization},
while approaches like \citet{Sun:2022OOD} are of non-parametric flavour exhibiting inference cost that increases with the dataset size. 
%
SWAG \cite{Maddox2019:SWAG} reduces the training and memory cost by considering an ensemble of parameters from a single learning trajectory with stochastic gradient descent and approximating it with a Gaussian posterior. For inference they employ expensive k-fold sampling. We note that it is natural to derive a SWAG-inspired Delta Variances that employs the $\Sigma$ from SWAG inside the computationally efficient Delta Variance formula -- we leave those considerations for future research.
%
Finally
\citet{Kallus2022:Implicit} propose a Delta Method inspired approach to approximate Epistemic Variance with an ensemble of two predictors and \citet{Schnaus:2023LearningPriors} learn scale parameters in Gaussian prior distributions for transfer learning.