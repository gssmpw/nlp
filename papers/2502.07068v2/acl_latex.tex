\pdfoutput=1

\documentclass[11pt]{article}
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{bm}

\definecolor{customgray}{HTML}{eeefee} % 定义颜色
\usepackage{url}

\newcommand{\yong}[1]{\textcolor{red}{#1}}
\newcommand{\rnv}[1]{\textcolor{red}{#1}}

\title{Specializing Large Language Models to Simulate\\ Survey Response Distributions for Global Populations}

% \author{Yong Cao \and Haijiang Liu \and Paul Röttger \and Arnav Arora \and Isabelle Augenstein \and Daniel Hershcovich}


\newcommand{\tubingen}{$^1$}
\newcommand{\cuhksz}{$^2$}
\newcommand{\ku}{$^3$}
\newcommand{\boc}{$^4$}


\author{Yong Cao\tubingen, Haijiang Liu\cuhksz, Arnav Arora\ku, Isabelle Augenstein\ku, \\ \textbf{Paul Röttger}\boc, \textbf{Daniel Hershcovich}\ku \\
{\tubingen}University of Tübingen, Tübingen AI Center \\
{\cuhksz}Wuhan University of Science and Technology  \\
{\ku}University of Copenhagen, {\boc}Bocconi University
 \\
% \texttt{lizhou21@cuhk.edu.cn}
\texttt{ yong.cao@uni-tuebingen.de}, 
\texttt{ dh@di.ku.dk}
% \texttt{dh@di.ku.dk}
% \texttt{\small \{lizhou21, liuwanlong\}@std.uestc.edu.cn, taelin.karidi@mail.huji.ac.il}\\
% \texttt{\small\{nicolas.garneau, dh\}@di.ku.dk, yongcao2018@gmail.com}
}

\begin{document}
\maketitle
\begin{abstract}
% Large-scale social value surveys, such as the World Values Survey, are a rich source of culturally diverse values and opinions. This paper addresses the challenges inherent in using large language models (LLMs) for simulating survey responses, particularly underrepresented and non-Western demographics. We introduce a methodological framework to evaluate and improve the generalization of LLMs in simulating the distribution of multiple-choice survey responses in a given country. Our approach involves adapting calibration techniques to human label variation, to better match the real-world distributions of responses from globally diverse surveys, thereby ensuring more accurate representations across new populations, countries, or questions. We incorporate an innovative prompting strategy, as well as two fine-tuning methods based on, respectively, first token distributions and full-text responses, to address the known limitations of LLMs in capturing diverse perspectives. The findings suggest that while there are significant challenges, there is also potential for LLMs to become more reliable tools for simulating public opinion, particularly in global market research and sociopolitical contexts.
%Large Language Models (LLMs) have become valuable tools for social science research, offering capabilities like simulating demographic samples and human behaviors. However, their ability to generalize across diverse cultural contexts remains limited, restricting their effectiveness in global applications. In this paper, we introduce a novel framework to enhance the reliability of LLMs in simulating response option distributions in multiple-choice survey questions. Leveraging the World Values Survey dataset, we propose a first-token probability fine-tuning method to improve the simulated distribution's similarity to human response distribution, particularly for unseen populations and questions. Extensive experiments show that fine-tuned models outperform zero-shot prompting, achieving a notable \textit{34.3\%} improvement in $1-$JSD for Llama3-8B-Instruct, and up to \textit{27.4\%} accuracy gain in robustness tests with the Pew Global Attitudes Survey. Our open-source dataset and code will provide a valuable resource for further exploration and benchmarking in cross-cultural opinion simulation tasks.

Large-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If one could accurately simulate group-level survey results, this could be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed for this task, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method substantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen questions, our results demonstrate the benefits of specialization for simulation, which may accelerate progress towards sufficiently accurate simulation in the future.
\end{abstract}


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{img/figure_1.pdf} 
    \caption{\textbf{Overview of our proposed survey response distribution simulation framework} (right panel) versus direct answer prediction (left panel), highlighting a novel perspective on cultural simulation with LLMs.}
    \label{fig:figure_1}
\end{figure}

\section{Introduction}\label{sec:intro}
Humans are diverse, and they hold diverse opinions.
This is why surveys are essential tools for informing decision-making in policy and industry as well as social science research. 
Running large-scale surveys, however, is often costly and time-intensive.

Large language models (LLMs) have demonstrated promising potential for simulating human behaviors across groups and individuals \citep[][\textit{inter alia}]{argyle2023out,aher2023using, manning2024automated}.
LLM simulations of survey responses, if accurate towards the corresponding populations, could accelerate social science research and aid in more informed policy decisions.
Out of the box, however, LLMs are known to generate erroneous, stereotypical, or overconfident answers, especially in culturally diverse contexts \cite{yang2024calibrationmultilingualquestionanswering}, which limits their usefulness for survey simulations.
Prior work has at most tried to improve simulation accuracy through prompting strategies \citep{kwok2024evaluating,manning2024automated,sun2024random}.

In this study, our goal is instead to \textbf{\textit{specialize} LLMs for survey simulation} and gain a better understanding of how good LLMs can be at simulating survey responses when trained to do so, rather than how good they are when prompted out of the box.
As our main testbed, we use country-level response distributions from the widely-used World Values Survey \cite[WVS;][]{haerpfer2022world}.
When prompted with a survey question (e.g., ``In your opinion, should the use of nuclear power in Japan be reduced, maintained at its current level, or increased?''), corresponding answering options (e.g.\ ``Reduced'', ``Maintained at current level'', ``Increased'') and a target country (e.g.\ ``Japan''), we want our model to predict the distribution over the answering options for that target country.
To specialize LLMs for this task, we devise a fine-tuning method based on first-token probabilities, where the goal is to minimize divergence between predicted and actual country-level response distributions for a given survey question.

As shown in Figure~\ref{fig:figure_1}, we train models on one set of questions and countries from the WVS, then evaluate on both seen and unseen countries and questions as well as another completely unseen survey.
Across seven LLMs from three model families, our fine-tuning method substantially boosts prediction accuracy on seen and unseen WVS countries and questions.
These results also hold for a completely unseen survey, i.e. the Pew Global Attitudes Survey.
Simultaneously, we find the performance of even the best-fine-tuned models to be far from perfect, especially on unseen questions.
We also find that all LLMs we tested, whether fine-tuned or not, are less diverse in their predictions across countries than the actual human survey data.

In summary, we make following \textbf{three main contributions}:
\begin{itemize}[noitemsep]
    \item  We introduce group-level survey response distribution prediction as a simulation task, and share three datasets adapted for training and testing models on this task: two in English and one in Chinese. 
\item We propose a fine-tuning method based on first-token probabilities of multi-choice question answering, and show that this method performs best among the methods tested for our simulation task, which demonstrates the benefits of specialization for simulation. 
\item We contextualize these positive results with evidence of systematic inaccuracies in even the best-performing simulations, thus cautioning against the use of LLMs, specialized or not, for simulating survey response distributions today.%
\footnote{We make all code and used dataset available at \href{https://github.com/yongcaoplus/SimLLMCultureDist}{github.com/yongcaoplus/SimLLMCultureDist}.}
\end{itemize}

%Large-scale surveys are essential in social science research but can be time-intensive, costly, and sometimes impractical \cite{hewitt2024predicting}. Large Language Models (LLMs) have demonstrated potential for simulating demographic samples \cite{argyle2023out, aher2023using} and user behavior \cite{wang2023user, wang-etal-2023-humanoid,manning2024automated}, offering faster and scalable alternatives. LLM-based simulations may accelerate intervention design by evaluating many treatment messages rapidly, reducing harm to human participants by simulating risky experiments, and helping researchers pilot test study materials before actual trials. However, LLMs often generate incorrect or overconfident answers, especially in culturally diverse contexts \cite{yang2024calibrationmultilingualquestionanswering}, limiting their accuracy and reliability in global applications \cite{durmus2023globalopinionqa,alkhamissi-etal-2024-investigating}.




% Comparison of direct answer alignment (left panel) versus our proposed response distribution simulation (right panel) in the culturally-relevant multi-choice question answering task, where cultural relevant questions (left) are posed to LLMs.

%To address this challenge, we propose an approach to evaluate and enhance the capabilities of LLMs in the task of simulating survey responses. As shown in Figure~\ref{fig:figure_1}, unlike existing research \cite{cao-etal-2023-assessing, alkhamissi-etal-2024-investigating} that primarily focuses on obtaining single answers from LLMs, we emphasize the predicted distribution of response options. Specifically, when prompted with a survey question (e.g., ``In your opinion, should the use of nuclear power in Japan be reduced, maintained at its current level, or increased?''), corresponding answering options (e.g.\ ``Reduced'', ``Maintained at current level'', ``Increased'') and a target population (e.g.\ ``people living in Japan''), we want our model to predict the distribution over the answering options for a given target population.

% Recent studies explored models' tendency to reflect the biases inherent in their training data, most of which is disproportionately representative of Western perspectives \cite{naous-etal-2024-beer, alkhamissi-etal-2024-investigating}. This bias leads to a limited ability of models to generalize across culturally and geographically diverse populations in various cultural tasks, such as survey question answering. The core challenge lies in the accurate simulation of human opinion distributions, an area that has received little attention in NLP research.

%In this paper, \textbf{our goal is to fine-tune an LLM to respond to survey questions with the distribution of answers from specified human populations}, focusing on multiple-choice question surveys. To improve performance of LLMs on unseen questions and countries, we use the World Values Survey \cite[WVS;][]{haerpfer2022world} as the primary data source for investigation due to its rich repository of culturally diverse values and opinions. By employing a fine-tuning method, our approach aims to align model outputs more closely with the real-world distributions observed in global surveys. We train the model on known demographic groups and evaluate its ability to generalize to new, unseen populations or survey scenarios, such as those represented in the Pew Global Attitudes Survey \cite[PEW;][]{durmus2023globalopinionqa}. We validate the effectiveness of our proposed framework, and conduct a comprehensive analysis of the implications related to the diverse attributes of the trained models.

%Experimental results demonstrate the effectiveness of our proposed approach. Fine-tuned models show a \textit{34.3\%} improvement in the ($1-$JSD) metric for Llama3-8B-Instruct model, compared to zero-shot prompting. We also observe that fine-tuning enhances generalization, with models handling unseen countries and questions more effectively, and larger models like Vicuna1.5-13B outperforming smaller ones. Robustness tests using PEW show further gains, with Llama3 achieving up to a \textit{27.4\%} accuracy improvement, confirming the model’s adaptability to unseen data.

%In summary, our contributions are as follows: (1) We introduce a novel task of simulating the distribution of human responses in multiple-choice survey questions. (2) Leveraging the WVS, we are the first to propose a first-token probability-based fine-tuning method to simulate response distributions. (3) Extensive experiments demonstrate the effectiveness of the proposed approach and provide in-depth analyses, showing that fine-tuning improves survey simulations, but sensitivity to the specific cultural group remains challenging.\footnote{We will release our dataset and code on GitHub.}




% Large Language Models (LLMs) have emerged as powerful tools in various applications, ranging from natural language understanding to generating textual content that mirrors human-like reasoning. However, their application in simulating human responses to surveys—particularly in contexts that involve diverse global opinions—presents significant challenges. LLMs often struggle with maintaining accuracy and fairness when applied to non-Western or non-US demographics, a limitation that undermines their utility in global applications \cite{durmus2023globalopinionqa,alkhamissi-etal-2024-investigating}.

% Much recent research has highlighted the limitations of using survey questions to evaluate out-of-the-box LLMs due to biases and instabilities in model responses \citep{wang2024my, lyu2024beyond, zheng2024large, rottger2024political}.
% Our focus, however, is on model training rather than evaluation. 
% We train models to give stable and unbiased responses.
% We also do not focus on out-of-the box LLMs optimised for general chat applications, although previous work finds detrimental effects of chat fine-tuning on global representation \cite{ryan2024unintended}.
% The models we train are highly specialized and can only be used for the specific task of predicting the distribution of answers on a given survey question from a specified human population.

% Our research contributes to ... and focus...

% In summary, our contribution includes ...

% The core challenge lies in the models' tendency to reflect the biases inherent in their training data, most of which is disproportionately representative of Western perspectives. This bias results in a skewed ability of the models to generalize across culturally and geographically diverse populations. 



% Therefore, improving the calibration and generalization of LLMs for survey simulation is not merely a technical challenge but also a necessity for ethical AI practice.

% As shown in Figure~\ref{fig:figure_1}, this paper presents a systematic approach to evaluating and enhancing the generalization abilities of LLMs in the task of simulating calibrated survey responses. By leveraging modified calibration techniques, our methodology aims to align model outputs more closely with the real-world distributions observed in global surveys. We propose a dual-focus approach: first, to ensure the calibrated accuracy of responses within known demographic groups and, second, to enhance the model's ability to generalize this accuracy to new, unseen populations or survey scenarios.

% Our research contributes to the broader discourse on responsible AI by providing a framework that not only addresses technical aspects of model training and evaluation, but also considers the socio-cultural implications of deploying LLMs in diverse global settings. By improving how these models handle tasks involving multiple-choice question-answering formats, we help pave the way for more reliable and fair use of AI in capturing the spectrum of human opinions across the globe.

% \section{PR Notes}

% Framing:
% - LLMs are increasingly used for social science applications \citep{ziems}.
% - Much recent work explores the use of LLMs for simulating human samples \citep{aher, horton, argyle}.
% - Large-scale surveys are a particularly interesting application: expensive and time-consuming to run.
% - However, there are many problems with using out-of-the-box LLMs to answer survey questions: biases, instabilities and inconsistencies in model responses \citep{wang2024my, lyu2024beyond, zheng2024large, rottger2024political}.
% - Therefore, we explore whether we can train specialized LLMs to mitigate these issues and predict survey responses from specified human populations to a useful/high degree of accuracy.



% The models we train are \textit{distributionally pluralistic} \citep{sorensen2024roadmap} in the sense that they reflect in their answers the pluralistic opinions of (different groups of humans) on a prompted topic.

% Much recent research has highlighted the limitations of using survey questions to evaluate out-of-the-box LLMs due to biases and instabilities in model responses \citep{wang2024my, lyu2024beyond, zheng2024large, rottger2024political}.
% Our focus, however, is on model training rather than evaluation. 
% We train models to give stable and unbiased responses.
% We also do not focus on out-of-the box LLMs optimised for general chat applications, although previous work finds detrimental effects of chat fine-tuning on global representation \cite{ryan2024unintended}.
% The models we train are highly specialized and can only be used for the specific task of predicting the distribution of answers on a given survey question from a specified human population.

% \paragraph{Contributions.}
% \begin{itemize}
%     \item New task and benchmark with split
%     \item Method to calibrate first-token distribution in LLMs
%     \item Using this method, we train the first-ever models for predicting country-level response distributions on seen and unseen survey questions, outperforming zero-shot classifier LLMs on a new survey (?)
%     \item Experiments show that these models outperform zero-shot classifiers on both seen and unseen questions.
%     \item Ablations to demonstrate the robustness
%     \item ... showing that diversity decreases for base models but increases for instruct models.
% \end{itemize}

\section{Related Work} % Paul

\paragraph{LLM Simulations}
Collecting human response data is one of the most challenging and costly aspects of social science research \cite{argyle2023out,hewitt2024predicting}.
Consequently, much prior work has investigated the extent to which LLMs can accurately simulate human responses in surveys and experimental settings.
Most prominently, \citet{argyle2023out},  \citet{horton2023large} and \citet{aher2023using} all found evidence of LLMs providing reasonably accurate group-level simulations in behavioral science and economics experiments as well as for US political surveys.
Some follow-up work has highlighted biases and conceptual challenges in such simulations \citep{bisbee2023synthetic,bail2024can,park2024diminished,kozlowski2024simulating}.
Other work has explored prompting strategies and frameworks for improving simulation accuracy \citep{kwok2024evaluating,manning2024automated,sun2024random}.
Relatedly, several works have used survey questionnaires, including the WVS used in this paper, with the goal of \textit{evaluating} values and opinions reflected in LLMs rather than simulating human survey responses \citep[][inter alia]{benkler2023assessing,arora-etal-2023-probing,cao-etal-2023-assessing,alkhamissi-etal-2024-investigating,zhao2024worldvaluesbench,wright2024revealingfinegrainedvaluesopinions}.
In contrast to these efforts, our focus is on \textit{specializing} LLMs to simulate group-level survey response distribution, which could aid in survey data collection.
While we do measure the performance of LLMs out of the box (\textit{zero-shot}), our main goal is to investigate the extent to which we can \textit{improve} LLM performance on simulating group-level survey response distributions through fine-tuning, and explore their potential as specialized tools for social science research.

%\cite{doi:10.1073/pnas.2314021121}
% https://arxiv.org/abs/2408.16482

\paragraph{Distribution Simulation as Calibration.}
% Multiple Choice Questions (MCQs) are often used to evaluate LLMs \cite{hendrycks2020measuring,srivastava2023imitation,zhong2023agieval}. Unlike in surveys, a single correct answer is often assumed in MCQs. %\citet{zheng2024large} show that LLMs suffer from behavior bias when option position changes with first-token evaluation. \citet{wang2024look} suggest that first-token evaluation is sensitive to prompt format and often does not align with full-text model output.
\textit{Calibration} is aligning classifier predictive probabilities with the classification uncertainty. While most work focuses on majority class accuracy \cite{li-etal-2024-multiple,he2024investigating}, this is problematic when human label variation is substantial \cite{baan-etal-2022-stop,baan-etal-2024-interpreting}, and \textit{human calibration} should consider the full human judgment distribution.
% % metrics for measuring calibration in this case, particularly Human Entropy Calibration Error, which captures the alignment between disagreement among humans and a model’s indecisiveness on the instance level; Human Ranking Calibration Score, a global measure that can be viewed as a stricter alternative to majority vote accuracy; and Human Distribution Calibration Error, a statistical distance metric based on total variation distance (TVD). We opt for Jensen-Shannon Distance (JSD) and Earth-Moved Distance (EMD) instead as adopted by \citet{durmus2023globalopinionqa} and \citet{zhao-etal-2024-worldvaluesbench-large} respectively, for comparing model predictions to WVS samples. \citet{ulmer2024calibrating} propose a novel calibration technique for LLMs called APRICOT, which, instead of directly tuning the LLM's weights, trains an auxiliary classifier (based on a small pre-trained LM) to predict confidence scores based on the LLM's full text decoded output.
Our task can therefore be viewed as \textit{human calibration} for multiple-choice surveys, as opposed to many previous studies, which focused on accuracy measured against the majority answer \cite{arora-etal-2023-probing, cao-etal-2023-assessing, alkhamissi-etal-2024-investigating}.

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.7\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/wvs_geo.png}
%         \caption{Country distribution across training, validation, and testing sets.}  % 子图标题
%         \label{fig:country_split}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.29\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/wvs_sunburst.png}
%         \caption{Cultural dimension distribution.}  % 子图标题
%         \label{fig:question_split}
%     \end{subfigure}
%     \caption{Visualization of country and cultural dimension divisions of Wvs.\ Countries are categorized into three groups, and questions are divided based on selected cultural dimensions. Further details are shown in the Appendix.}
%     \label{fig:wvs_distritbuion}
% \end{figure*}

\begin{table}[t]
\centering
\resizebox{0.45\textwidth}{!}{%
\small
\begin{tabular}{@{}p{0.11\textwidth} p{0.3\textwidth}@{}}
\toprule
\textbf{Instruction} & How would someone from Andorra answer the following question: \\
\midrule
\textbf{Input} & How interested would you say you are in politics? Here are the options: \\ 
\midrule
\textbf{Options} & \text{(A) Very interested} \\
                 & \text{(B) Somewhat interested} \\
                 & \text{(C) Not very interested} \\
                 & \text{(D) Not at all interested} \\
\midrule
\textbf{Format} & If had to select one of the options, my answer would be ([A/B/...]) \\
\midrule
\textbf{Options} & \text{(A) 15.16\%} \quad \text{(B) 29.02\%}  \\ 
\textbf{Distribution}& \text{(C) 28.31\%} \quad  \text{(D) 27.51\%} \\           \bottomrule
\end{tabular}}
\caption{\textbf{Example entry} from our formatted WVS dataset for the country of Andorra.}
\label{tb:questionnaire_example}
\end{table}

\section{Cultural Survey Simulation Dataset}\label{sec:dataset}

\subsection{Data Source}

We use the 2017-2022 wave of the World Values Survey (WVS)\footnote{\url{https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp}} to construct our main simulation dataset.
% because it contains rich information about country-level social values and norms. 
WVS was conducted across 66 countries with over 80,000 respondents. This extensive survey captures societal attitudes on various cultural dimensions, including \textit{family, regional values, education, moral principles, corruption, accountability, etc}.
Our analysis includes all countries with more than 1,000 respondents to ensure robust cross-cultural representation, resulting in a set of 65 countries (Northern Ireland did not qualify).

For our analysis, we use the original questions and answers, excluding validity-check options such as ``not applicable'' and ``refuse to answer'', given their infrequent occurrence in human-collected responses. We conduct experiments using the English and Chinese versions of the datasets obtained from the official source, enabling the analysis of cross-linguistic differences in this task\footnote{We use GLM-4 to translate the missing questions in the Chinese questionnaire.}.

\subsection{Prompt Settings}   

We preserve the original questions and response options, adhering to the GlobalOpinionQA template for consistency \cite{durmus2023globalopinionqa}. As shown in Table~\ref{tb:questionnaire_example}, the model input consists of fields for \textit{instruction}, \textit{input}, \textit{options}, and \textit{format}, while the target for alignment is the \textit{distribution} of the options.  Note that the \textit{format} field is used to restrict the vocabulary of the first token to valid options.



\subsection{Dataset Split} 
We use the first 259 questions of the WVS to construct our dataset, excluding demographics and notes for interviewers. We divide them into three parts based on topics: $Q_1$ (questions 1-163), $Q_2$ (questions 164-198), and $Q_3$ (questions 199-259). Additionally, we divide the countries into three groups to ensure they are challenging to generalize between: $C_1$ (all countries that are not included in the following two sets), $C_2$ (the 8 surveyed countries that are in Africa\footnote{For subset $C_2$, we select countries from one random continent (i.e. Africa). The selected African countries $C_2$ are Egypt, Ethiopia, Kenya, Libya, Morocco, Nigeria, Tunisia, and Zimbabwe. }), and $C_3$ (medium-GDP countries sampled from each continent\footnote{Our selected medium-GDP countries $C_3$ are Malaysia, Thailand, Czechia, Greece, Nigeria, Morocco, Peru, Colombia, Mexico, Puerto Rico, and New Zealand.}).

We split training, validation, and test sets for the aforementioned questions and country subsets. The split and statistical information of the dataset are presented in Table~\ref{tb:dataset_split} and Table~\ref{tb:dataset_statistics} respectively. The test set comprises five subsets, designed to evaluate the performance of models in answering unseen value questions, unseen regional countries, and representative medium-GDP countries.
% Examples of our processed data are shown in Figure 1. 



\subsection{Unseen Survey Dataset}\label{sec:unseen_data} To evaluate generalization to a completely unseen survey, we use an additional subset of GlobalOpinionQA \cite{durmus2023globalopinionqa}, the Pew Global Attitudes Survey (Pew), which maintains a similar format to the WVS but includes different cultural questions. 
% We also use another homogeneous dataset from , which is derived from the Pew Global Attitudes Survey. 
We compile two sets of countries for this test set: $C_1^\prime$ and $C_3$. For $C_1^\prime$, we sample ten countries from $C_1$ to maintain geographical and GDP-level diversity and then use the GlobalOpinionQA data specifically for these countries for evaluation (see Appendix~\ref{ax:pew_data_distribution}). For $C_3$, we include the same medium-GDP countries as in $C_3$.


% \begin{table}[]
% \centering
% \resizebox{1.00\columnwidth}{!}{
% \begin{tabular}{l|llccc}
% \toprule
% \multicolumn{2}{l}{Split} & \multicolumn{1}{l}{Type} & Country & Question & Samples \\ \midrule
% \multicolumn{2}{l}{Train} & $C_1$, $Q_1$               & 46      & 150      & 6841    \\
% \multicolumn{2}{l}{Valid} & $C_1$, $Q_2$               & 46      & 35       & 1586    \\ \midrule
% \multirow{7}{}{Test}  &  Unseen & $C_1$, $Q_3$               & 46      & 59       &  2673   \\ \midrule
%  & \multirow{3}{}{African}  & $C_2$, $Q_1$          & 8       & 150     & 1179    \\
% &  & $C_2$, $Q_2$ & 8 & 35 & 271  \\
%  &     & $C_2$, $Q_3$               & 8       & 59       &  463    \\ \midrule
%  & \multirow{3}{}{Medium-GDP}                  & $C_3$, $Q_1$         & 11      & 150      &  1644   \\
% &  & $C_3$, $Q_2$ & 11 & 35 & 385 \\
%  & & $C_3$, $Q_3$ & 11      & 59       &   649   \\ \bottomrule
% \end{tabular}}
% \caption{\label{tb:dataset_statitics} The statistics of used dataset, derived from the World Values Survey (WVS). Group $C_2$ includes eight countries selected from the African continent, and Group $C_3$ consists of countries with medium GDP selected from each continent.}
% \end{table}


% \begin{table}[]
% \centering
% \resizebox{0.6\columnwidth}{!}{
% \begin{tabular}{l|ccc}
% \toprule
% \textbf{Split} & \textbf{Name}  & \textbf{Subset} & \textbf{\#Samples} \\ \midrule
% \textbf{Train} &  & $C_1, Q_1$  & 6841 \\
% \textbf{Valid} &  & $C_1, Q_2$  & 1586 \\ \midrule
% \multirow{7}{*}{\textbf{Test}} & UnseenWVS & $C_1, Q_3$  & 2673 \\ \cmidrule{2-4}
%  & \multirow{3}{*}{African} & $C_2, Q_1$  & 1179 \\
%  &  & $C_2, Q_2$ & 271 \\
%  &  & $C_2, Q_3$ & 463 \\ \cmidrule{2-4}
%  & \multirow{3}{*}{Mid-GDP} & $C_3, Q_1$ & 1644 \\
%  &  & $C_3, Q_2$  & 385 \\
%  &  & $C_3, Q_3$  & 649 \\ \bottomrule
% \end{tabular}}
% \caption{\label{tb:dataset_statistics} Dataset statistics derived from the World Values Survey. Group $C_2$ comprises eight countries from the African continent, while Group $C_3$ includes eleven medium-GDP countries selected from various continents. Group $C_1$ excludes countries in $C_2$ and $C_3$.
% Test \textit{UnseenWVS} contains held-out WVS questions, whereas \textit{African} and \textit{Mid-GDP} contain held-out countries \textit{and} questions.
% \#Samples is not necessarily \#Countries $\times$ \#Ques. because some entries are missing from survey results.}
% \end{table}

% \begin{table}[]
% \centering
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|c|c|c|cc|cc}
% \toprule
% \textbf{\multirow{2}{*}{Split}} & \textbf{\multirow{2}{*}{Train}} & \textbf{\multirow{2}{*}{Valid}} & \multicolumn{5}{c}{\textbf{Test}} \\ \cmidrule{4-8}
% &  &  & \textbf{UnseenWVS} & \multicolumn{2}{c}{\textbf{African}} & \multicolumn{2}{|c}{\textbf{Mid-GDP}} \\ \midrule
% \textbf{Set} & $C_1, Q_1$ & $C_1, Q_2$ & $C_1, Q_3$ & $C_2, Q_1$ &  $C_2, Q_3$ & $C_3, Q_1$ & $C_3, Q_3$ \\ 
% \textbf{Case} & 6,841 & 1,586 & 2,673 & 271 & 463  & 385 & 649 \\ 
% \bottomrule
% \end{tabular}}
% \caption{\label{tb:dataset_statistics} \textbf{WVS dataset statistics} across the country ($C$) and question ($Q$) splits we use in our experiments.
% $C_2$ contains eight countries from the African continent, while $C_3$ contains eleven medium-GDP countries across continents.
% $C_1$ contains all other countries.
% Questions are split as described in Figure~\ref{}.
% \#Samples is not necessarily \#Countries $\times$ \#Ques. because some entries are missing from survey results.}
% \end{table}

\begin{table}[t]
\small
    \begin{tabular}{p{1.2cm}p{4.8cm}p{0.4cm}}
        \toprule
        \textbf{Countries} & \textbf{Description} & \textbf{N}\\
        \midrule
        \bm{$C_1$} & All WVS countries not in $C_2$ or $C_3$ & 46 \\
        $C_2$ & African countries & 8 \\
        $C_3$ & Medium-GDP countries & 11 \\
        \bottomrule
    \end{tabular}

    \vspace{0.3cm}

    \begin{tabular}{p{1.2cm}p{4.8cm}p{0.4cm}}
        \toprule
        \textbf{Questions} & \textbf{Description} & \textbf{N}\\
        \midrule
        \bm{$Q_1$} & All WVS questions not in $Q_2$ or $Q_3$ & 150 \\
        $Q_2$ & Q's about religious and ethical values & 35 \\
        $Q_3$ & Q's about political interest and culture & 59 \\
        \bottomrule
    \end{tabular}

\caption{\label{tb:dataset_split} 
\textbf{Country and question splits} that we use in our experiments with WVS data. Splits seen during training are highlighted in \textbf{bold}. For additional descriptive statistics on the dataset, see Appendix~\ref{ax:wvs_data_distibution}.
}
\end{table}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
    \begin{tabular}{l|c|c|c|cc|cc}
        \toprule
        \textbf{Split} & \textbf{Train} & \textbf{Valid} & \multicolumn{5}{c}{\textbf{Test}} \\  \midrule \midrule
        \textbf{Countries} & \bm{$C_1$} & \bm{$C_1$} & \bm{$C_1$} & $C_2$ &  $C_2$ & $C_3$ & $C_3$ \\ 
        \textbf{Questions} &  \bm{$Q_1$} & $Q_2$ & $Q_3$ & \bm{$Q_1$} &  $Q_3$ & \bm{$Q_1$} & $Q_3$ \\  \midrule
        \textbf{Entries} & 6,841 & 1,586 & 2,719 & 1,179 & 471  & 1,644 & 660 \\ 
        \bottomrule
    \end{tabular}}

\caption{
    \label{tb:dataset_statistics} \textbf{WVS dataset statistics} across the country ($C$) and question ($Q$) splits we use in our experiments.
    Splits seen during training are highlighted in \textbf{bold}. Number of entries is not necessarily $C$ $\times$ $Q$ because some entries are missing from survey results.
    }
\end{table}

% $C_2$ contains eight countries from the African continent, while $C_3$ contains eleven medium-GDP countries across continents.
% $C_1$ contains all other countries.
% Questions are split as shown in Figure~\ref{fig:question_split}.





% Statistics of the dataset derived from the World Values Survey. Group $C_2$ includes eight countries selected from the African continent, and Group $C_3$ consists of eleven medium-GDP countries selected from each continent. $C_1$ excludes $C_2$ and $C_3$.

\section{Methodology}
To address the challenges of simulating culturally diverse survey responses, we introduce a framework for first-token alignment fine-tuning for distribution prediction, designed to improve generalization across populations and survey questions.

\subsection{Probability Distribution Simulation}

Unlike most existing studies that directly prompt LLMs with multiple-choice questions to assess their cultural knowledge or behavior \cite{arora-etal-2023-probing, cao-etal-2023-assessing, alkhamissi-etal-2024-investigating}, we propose a novel task that focuses on simulating the distribution of response options for given questions rather than predicting single answers.

Specifically, let ${Q}$ denote a multiple-choice question and ${O}=\{o_1, o_2, ..., o_n\}$ be the corresponding set of response options, where $n$ is the total number of options, which can vary between questions. The objective is to simulate the option distribution $P(O|{Q})$ to match human response distribution from a particular group (e.g., country).
Consequently, models are evaluated by comparing the alignment of observed vs.\ predicted distributions rather than focusing on majority response options.

\subsection{First-Token Probability Alignment}
Using the dataset introduced in \S\ref{sec:dataset}, we present first-token alignment fine-tuning, to align model outputs with the observed response distributions of specific population groups (e.g., countries).

The processed question $Q$ is used as input into LLMs. The model outputs logits $\{ z_1, z_2, \ldots, z_n \}$ for the first token of each question's corresponding options $O$. The probability distribution for the first token is obtained by applying the softmax function to normalize the indexed option logits:
\[ P_{\text{LLM}}(o_i|{Q}) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \]

For the training optimization objective, we employ Kullback-Leibler Divergence loss ($\text{Loss}_{\text{KL}}$) to align the LLM's first-token probability distribution with the human response distribution:
\[ \text{Loss}_{\text{KL}} = \sum_{i=1}^{n} P_{\text{human}}(o_i|{Q}) \log \left( \frac{P_{\text{human}}(o_i|{Q})}{P_{\text{LLM}}(o_i|{Q})} \right) \]
where \( P_{\text{human}}(o_i|{Q})\) is the probability of option \( o_i \) based on human survey data, and \( P_{\text{LLM}}(o_i|{Q})\) is the probability output by the LLM.

To improve the efficiency of the fine-tuning process, we implement Low-Rank Adaptation \cite[LoRA;][]{hu2022lora}, a parameter-efficient method specifically designed for optimizing LLMs.

% We combine the user instructions, survey questions, and each of its options as input for auxiliary models to train the model learning the human preference distribution. We build a linear classifier on top of an auxiliary model and take the softmax of its logits as the model prediction of the preference probabilities. The optimization objective is the cross-entropy loss over model predictions and labels.

% During inference time, we ask LLMs to generate responses on their observation of humans in specific cultural backgrounds and combine it the same way as the training input and ask the auxiliary model to predict the accurate preference probability over the model choice.

\section{Experimental Setup}





\subsection{Models}
\label{exp:experiment_setup}
We fine-tune seven models across three model families using our processed dataset: Vicuna1.5 \cite{vicuna2023} in its 7B and 13B parameter versions, Llama3 \cite{llama3modelcard} in its 8B Base and Instruct versions, and Deepseek-Distilled-Qwen \cite{guo2025deepseek} in 7B, 14B, and 32B.
Vicuna1.5 is a version of Llama2 \cite{touvron2023llama} fine-tuned on user conversations with ChatGPT, whereas Llama3 is a stronger model. As Vicuna1.5 is less powerful than recent LLMs, it is chosen to evaluate the effect of fine-tuning as an equalizer despite zero-shot performance differences. DeepSeek is a state-of-the-art model series known for its strong performance across diverse benchmarks. We use the DeepSeek-Distilled-Qwen models, which are distilled from the DeepSeek-R1, the current state-of-the-art open weights model.
% excelling in efficiency and capability through advanced distillation techniques.} 
For further details on all models and our inference setup, see Appendix~\ref{app:hyperparams}.



\input{table/main_table}


\subsection{Baselines}
In the main body of this paper, we compare our fine-tuning method (FT) to a zero-shot prompting (ZS) baseline, which is the default method explored in prior work.
ZS involves directly querying the models with the country context and questions.
As an additional control setting, for both ZS and FT, we replace countries in the queries with other countries randomly selected from among the full set of countries in the WVS. This approach is designed to investigate the sensitivity of the LLMs to the specific country given in the context vs.\ the prior distributions of response options.
We denote this control setting as ``[ctrl]''.
In Appendix \ref{ax:more_baseline}, we show additional baselines such as K-Nearest Neighbors, which generally perform worse than FT.

\subsection{Metrics}
To measure the alignment of predicted response distributions with country-level reference distributions, we adopt two metrics for evaluation: i) \textbf{1-JSD}, where JSD is Jensen-Shannon divergence, also employed by \citet{durmus2023globalopinionqa}, is a symmetric measure of the similarity between two probability distributions, with higher values indicating greater similarity; ii) \textbf{Earth Mover Distance} \cite[EMD;][]{rubner1998metric}, also known as the Wasserstein distance, quantifies the minimum amount of work required to transform one distribution into another, with lower values indicating greater similarity in distribution. Both the 1-JSD and EMD metrics range from 0 to 1.



\section{Results}
We investigate two primary research questions: 
\begin{description} 
\item[RQ1] How effectively does the proposed alignment method improve the distribution simulation of the model on unseen countries and questions?
\item[RQ2] What is necessary to perform well on the task---how much is dependent on modeling the prior distribution, and how much on context sensitivity?
\end{description}
We present comprehensive experimental results to address the two research questions. 


\subsection{RQ1: Generalization Performance}
To address RQ1 (how effectively FT improves distribution simulation on unseen countries and questions), we train the selected models using our proposed simulation methods and evaluate their performance on unseen countries and questions. Table~\ref{tb:main_results} presents the evaluation scores across models of varying sizes and types.

\paragraph{Zero-shot [ZS] vs.\ Fine-tuning [FT].} Across all model sizes and types, we observe that zero-shot prompting (ZS) consistently yields worse scores compared to fine-tuned models (FT), indicating that while ZS is capable of addressing unseen countries and questions, it lacks the adaptability needed for effective distribution simulation. In contrast, fine-tuned models show improved performance, with higher $1-$JSD and lower EMD scores (e.g., a \textit{34.3\% $1-$JSD} increase and \textit{0.069 EMD} decrease for Llama3-8B-Instruct \textit{Avg.}), demonstrating their enhanced ability to align with real-world distributions. This suggests that fine-tuning enables models to internalize more detailed patterns and relationships, making them more effective for simulation.





\paragraph{Unseen Countries vs.\ Unseen Questions.} The generalization capabilities are revealed by evaluation on unseen attributes. Across all models and settings, unseen questions ($Q_3$) tend to present a greater challenge than unseen countries ($C_2$ or $C_3$), as indicated by slightly worse scores for unseen questions (e.g., \textit{0.781} vs.\ \textit{0.886 $1-$JSD} for Llama3-Instruct). This suggests that while the models are reasonably robust in handling new country distributions, they struggle more with generating accurate distributions for questions that were not encountered during training. 
% Notably, FT yields comparable improvements on both unseen questions and unseen countries, as evidenced by a \textit{35.5\%} vs \textit{33.3\% $1-$JSD} increase for Llama3-Instruct.

\paragraph{Comparison Across Models.} 
Our method demonstrates consistent performance improvements across three representative model families and various model sizes, highlighting the effectiveness of our proposed first-token alignment approach. Specifically, while there are notable performance differences in the zero-shot setting—for example, Llama3-Base achieves a higher $1-$JSD value (0.765) compared to Llama3-Instruct (0.613), suggesting that the base model better predicts the option token distribution—our fine-tuning procedure significantly bridges this gap. After fine-tuning, Llama3-Instruct not only closes the initial performance disparity but even surpasses Llama3-Base (\textit{34.3\% $1-$JSD increase} vs. \textit{6.1\%} increase). Similarly, experiments with Distilled Qwen models of different sizes reveal no clear scaling trends. Moreover, although Vicuna1.5 is generally considered weaker compared to Llama and Qwen, it surprisingly delivers similarly competitive results on this task. Overall, analysis across all models further revealing the finding that, \textbf{regardless of the starting model, our fine-tuning approach consistently produces models with similar strong performance on our task}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{img/accuracy_prediction_bar.png}
    \caption{Option prediction accuracy for cultural questions using the Llama3-8B-Instruct. The final option is simulated by selecting the option with the highest probabilities,  compared against human majority choice.}
    \label{fig:prediction_acc}
\end{figure}
% \vspace{-6mm}



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{img/diversity_analysis.pdf} 
    \caption{Model Diversity and Country Accuracy Analysis. (a)-(d) denotes the comparison of $1-$JSD scores across countries for specific questions (\textit{$C_2$-$Q_3$} and \textit{$C_3$-$Q_3$}). The blue-shaded area represents diversity changes, with the lower boundary indicating the mean of survey response scores and the upper boundary representing the mean of model outputs. (e)-(f) denotes the accuracy of options on African countries in both \textit{$C_2$-$Q_1$} and \textit{$C_2$-$Q_3$}.}
    \label{fig:diversity_distribution}
\end{figure*}


\paragraph{Correlation Between First-Token Distribution and Response Accuracy.} Beyond the alignment of first-token distributions, we further evaluate of the mode accuracy of models in responding to questionnaire items following fine-tuning. We consider the options with the highest probabilities as (single, argmax) predictions and calculate accuracy against the survey majority choice per country \cite{arora-etal-2023-probing, cao-etal-2023-assessing, alkhamissi-etal-2024-investigating}. As depicted in Figure~\ref{fig:prediction_acc}, our findings reveal a significant increase in accuracy across all models and test subsets, highlighting the effectiveness of the fine-tuning process. Notably, performance improvements are particularly significant in unseen countries, as demonstrated by $C_2$-$Q_1$ and $C_3$-$Q_1$ subsets. These results suggest that our proposed method not only improves the simulation of option distributions but also strengthens the models' alignment with the correct responses, underscoring the interdependence of distribution alignment and answer accuracy.



\begin{table*}[!]
\centering
\resizebox{0.999\textwidth}{!}{
\begin{tabular}{l|c|ccccc|c|ccccc|c}
\toprule
\multirow{2}{*}{Base Model} & \multirow{2}{*}{Methods} &  \multicolumn{6}{c|}{($1-$JSD) $\uparrow$} & \multicolumn{6}{c}{EMD $\downarrow$} \\  \cmidrule{3-14}
                            & & $C_1$-$Q_3$ & $C_2$-$Q_1$ & $C_2$-$Q_3$ & $C_3$-$Q_1$ & $C_3$-$Q_3$ & \textit{Avg.} & $C_1$-$Q_3$ & $C_2$-$Q_1$ & $C_2$-$Q_3$ & $C_3$-$Q_1$ & $C_3$-$Q_3$ & \textit{Avg.}  \\ \midrule \midrule 
\multirow{2}{*}{\textit{Llama3-8B-Instruct}} & ZS & 0.603 & 0.654 & 0.601 & 0.659 & 0.613 & 0.626 & 0.125 & 0.140 & 0.131 & 0.145 & 0.132 & 0.135 \\
& \cellcolor{customgray}\textbf{FT} & 
\cellcolor{customgray}\textbf{0.777} & 
\cellcolor{customgray}\textbf{0.852} & 
\cellcolor{customgray}\textbf{0.789} & 
\cellcolor{customgray}\textbf{0.870} & 
\cellcolor{customgray}\textbf{0.791} & 
\cellcolor{customgray}\textbf{0.816} & 
\cellcolor{customgray}\textbf{0.081} & 
\cellcolor{customgray}\textbf{0.078} & 
\cellcolor{customgray}\textbf{0.087} & 
\cellcolor{customgray}\textbf{0.065} & 
\cellcolor{customgray}\textbf{0.073} & 
\cellcolor{customgray}\textbf{0.077} \\ \midrule
\multirow{2}{*}{\textit{Distil-Qwen-7B}} &
ZS & 0.605 & 0.706 & 0.712 & 0.696 & 0.693 & 0.682 & 0.084 & 0.091 & 0.086 & 0.087 & 0.190 & 0.108 \\
& \cellcolor{customgray}\textbf{FT} & 
\cellcolor{customgray}\textbf{0.764} & 
\cellcolor{customgray}\textbf{0.779} & 
\cellcolor{customgray}\textbf{0.771} & 
\cellcolor{customgray}\textbf{0.791} & 
\cellcolor{customgray}\textbf{0.851} & 
\cellcolor{customgray}\textbf{0.791} & 
\cellcolor{customgray}\textbf{0.067} & 
\cellcolor{customgray}\textbf{0.085} & 
\cellcolor{customgray}\textbf{0.081} & 
\cellcolor{customgray}\textbf{0.082} & 
\cellcolor{customgray}\textbf{0.130} & 
\cellcolor{customgray}\textbf{0.089} \\
\bottomrule
\end{tabular}}
\caption{\label{tb:zh_results}\textbf{Model Performance on Chinese World Values Survey}. 
Performance comparison between zero-shot prompting (ZS) and supervised fine-tuning (SFT) on two model families.}
\end{table*}




\subsection{RQ2: Variation Sensitivity}
To address RQ2 (contribution of modeling the prior distribution vs.\ context sensitivity), we compare the control setting (with countries randomly replaced), analyze diversity changes of models, and explore shifts in response accuracy for unseen countries.

\paragraph{ZS[ctrl] vs. FT[ctrl].} In the control setting, ZS[ctrl] shows a smaller performance drop than ZS, while FT[ctrl] sees a larger decline, with a \textit{16.7\% avg. ($1-$JSD)} drop between FT[ctrl] and FT across seven models, compared to \textit{3.7\%} for ZS[ctrl] and ZS. This indicates that fine-tuned models (FT) are more sensitive to the country context and not just the prior distribution of responses (which FT[ctrl] is trained to simulate), suggesting they have become more specialized in capturing cultural nuances during training. In contrast, the smaller gap between ZS and ZS[ctrl] implies that zero-shot models maintain a more generalized understanding of cultural contexts, making them less affected by random permutations of country data. This difference highlights the fine-tuned models’ improved capability in simulating response distributions for specific countries.
% The score distribution indicates that the improvement achieved by the Instruct model is significantly greater than that of the Base model. Additionally, the performance of both the fine-tuned Base model and the Instruct model is observed to be very similar across different countries, demonstrating that our methods can effectively align the Base and Instruct models.

% Figure~\ref{fig:global_distrition} presents the $1-$JSD scores of the models in both zero-shot and fine-tuned modes of $C_1$-$Q_3$ (Other sets are provided in Appendix). The score distribution indicates that the improvement achieved by the Instruct model is significantly greater than that of the Base model. Additionally, the performance of both the fine-tuned Base model and the Instruct model is observed to be very similar across different countries, demonstrating that our methods can effectively align the Base and Instruct models. However, the distribution predictions of the Instruct model in all countries under zero-shot conditions are notably poor. This implies that, after the implementation of safety strategies and reinforcement learning, the model becomes less sensitive to the cultural values of the respective countries.


\paragraph{Country Diversity in Model Outputs.}
We define diversity as the divergence across countries of response distributions given the same question. To assess whether FT can enhance model output diversity, we calculate $1-$JSD for Llama3-Base and Llama3-Instruct (ZS and FT) output across countries for each question.  A lower $1-$JSD score suggests greater diversity in responses between countries, whereas a higher score indicates greater similarity in distributions. Results are shown in Figure~\ref{fig:diversity_distribution}, where the scores of survey responses are compared with those of the model predictions.

This visualization provides several interesting insights. Firstly, the outputs of Base models exhibit a high degree of uniformity across countries, indicating a \textbf{limited sensitivity to national variations} when addressing cultural values questions. After fine-tuning, there is a slight reduction in $1-$JSD, suggesting an enhancement in the responsiveness to diverse cultural contexts. Secondly, although the Instruct model, having undergone alignment fine-tuning, initially produces a more varied distribution of answers, \textbf{this diversity diminishes following distribution simulation fine-tuning}. Thirdly,
we observe \textbf{no consistent correlation between the diversity of generated responses and the accuracy of simulated distributions}. Lastly, the post-fine-tuning diversity of responses from both models converges, indicating that our fine-tuning approach \textbf{improves the sensitivity to national differences}.


\begin{table}[t]
\centering
\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{Methods}  & \multicolumn{2}{c|}{$C_1^\prime$} & \multicolumn{2}{c}{$C_3$}
 \\ \cmidrule{2-5}
& ($1-$JSD) $\uparrow$ & ACC $\uparrow$ & ($1-$JSD) $\uparrow$ & ACC $\uparrow$   \\ \midrule \midrule
\textit{Vicuna1.5} (ZS)         & 0.690 & 0.360  & 0.668 & 0.346 \\
\textit{Vicuna1.5} (FT)        & \cellcolor{customgray}\textbf{0.725} & 
\cellcolor{customgray}\textbf{0.442} & 
\cellcolor{customgray}\textbf{0.709} &  
\cellcolor{customgray}\textbf{0.456} \\ \midrule
% Llama-base-zs      &  0.718     &  0.426     \\
% Llama-base-sft     &  \textbf{0.750}      &   \textbf{0.546}    \\ \midrule
\textit{Llama3} (ZS)  & 0.617 & 0.472 &   0.613    &   0.446    \\
\textit{Llama3} (FT) & \cellcolor{customgray}\textbf{0.767} & 
\cellcolor{customgray}\textbf{0.562} &  
\cellcolor{customgray}\textbf{0.755} &  
\cellcolor{customgray}\textbf{0.568} \\ \bottomrule  
\end{tabular}}
\caption{\label{tb:pew_evaluation} Evaluation results on GlobalOpinionQA Pew dataset for \textit{Llama3-Instruct} and \textit{Vicuna1.5-7B}. ($1-$JSD) and option accuracy scores are reported. }
\end{table}


\paragraph{Unseen Country Shifts.} 
We visualize the option accuracy of African countries as observation objects in Figure~\ref{fig:diversity_distribution}e-\ref{fig:diversity_distribution}f. For seen questions, we find that all models show a relatively high performance across most countries, with Ethiopia and Nigeria displaying the highest accuracies close to 80\%. For unseen questions, the performance decreases greatly compared to seen questions, particularly in countries like Egypt and Tunisia. Besides, the Instruct models maintain a higher level of accuracy relative to the Base models in most cases, showing the relative robustness of Instruct models in both seen and unseen scenarios across the African countries, which is consistent with results in Figure~\ref{fig:prediction_acc}.

\subsection{Robustness Analysis}
In this section, we explore the robustness of the models on survey language and unseen survey. 

\paragraph{Impact of Survey Language on Results.}
We fine-tuned the Llama3-8B-Instruct and Distil-Qwen-7B models on the official Chinese translation dataset, and the results are presented in Table \ref{tb:zh_results}. While the performance of both models in Chinese is marginally lower than in English both on (1-JSD) and EMD metrics, the difference is not significant, suggesting that current LLMs exhibit limited sensitivity to language differences in this task. Besides, while the Distilled-Qwen model demonstrates a better performance than Llama3 on most benchmarks, it does not outperform Llama3 in this task. 
% We also calculate the Pearson correlation between ZS and FT in English, and 
%  exhibiting slightly lower performance.

\paragraph{Generalization to a New Survey.}
% To validate the robustness of our trained model, we employ a homogeneous dataset from GlobalOpinionQA \cite{durmus2023globalopinionqa}, i.e. the Pew Global Attitudes Survey (Pew), which maintains a similar format but includes different cultural questions.
We use Pew introduced in \S\ref{sec:unseen_data} to test the generalization of our fine-tuned models.
Table~\ref{tb:pew_evaluation} presents the results in both $1-$JSD scores and accuracy. Notably, all metric scores show significant improvements for both models after fine-tuning. Additionally, across both $C_1^\prime$ and $C_3$, Llama3 outperforms Vicuna1.5 in terms of accuracy, particularly in the fine-tuned setting, where Llama3 (FT) achieves \textit{19.1\%} and \textit{27.4\%} improvement for two datasets, respectively. The consistent improvements prove its capability to generalize well to unseen surveys.


\begin{table}[t]
\centering
\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
Methods         & $C_1$-$Q_3$ & $C_2$-$Q_1$ & $C_2$-$Q_3$ & $C_3$-$Q_1$ & $C_3$-$Q_3$ \\ \midrule \midrule
\cellcolor{customgray}\textbf{KL (Orig)}   & \cellcolor{customgray}\textbf{0.777} & 
\cellcolor{customgray}\textbf{0.881} & 
\cellcolor{customgray}\textbf{0.783} & 
\cellcolor{customgray}\textbf{0.890} & 
\cellcolor{customgray}\textbf{0.784}  \\
\midrule
WA Loss &   0.733 & 0.774 & 0.744 & 0.782 & 0.749 \\
JS Loss   & 0.745 & 0.790 & 0.756 & 0.799 & 0.763   \\
CE Loss & 0.746 & 0.809 & 0.772 & 0.807 & 0.756 \\
 \midrule
Shuffled  &  0.753 & 0.820 & 0.761 & 0.815 & 0.761    \\
\bottomrule
\end{tabular}}
\caption{\label{tb:ablation_study} \textbf{Results of our ablation studies}.
We compare different loss functions (WA, JS and CE) to our KL loss setup.
We also evaluate on a test set with shuffled option orders.
All results are for Llama3-8B-Instruct.}
% \vspace{-3mm}
\end{table}

\subsection{Ablation Studies}
We conduct ablation studies to analyze the impact of the training loss function and option ordering.

\paragraph{Loss Function.} As shown in Table~\ref{tb:ablation_study}, KL Loss is the most effective loss function for our task. However, Wasserstein (WA) Loss Jensen-Shannon (JS), and Cross-Entropy (CE) Loss also improve over zero-shot prompting.

\paragraph{Option Ordering.} \citet{dominguez2023questioning} observed an A-bias effect, where models tend to disproportionately select the answer choice labeled ``A''. To assess this bias, we re-evaluate our fine-tuned model on the same dataset where the answer options are shuffled. As shown in Table~\ref{tb:ablation_study} (``Shuffled''), there is a reduction in performance, but it is small compared to the effect of fine-tuning or model choice. This indicates that the option ordering is not a major concern in our experiments.



% \begin{table}[tbp]
% \centering
% \begin{tabular}{l|ccc}
% \hline
% \textbf{Model} & Mode & \textbf{Spearman $\uparrow$} & \textbf{MSE $\downarrow$} \\
% \hline
% \multirow{2}{*}{vicuna-7b} & zs & nan & 0.277 \\
%  & sft & 0.525 & 0.659 \\
% \hline
% \multirow{2}{*}{vicuna-13b} & zs & 0.090 & 2.069 \\
%  & sft & 0.298 & 0.463 \\
% \hline
% \multirow{2}{*}{Llama-base} & zs & 0.545 & 1.075 \\
%  & sft & 0.759 & 0.570 \\
% \hline
% \multirow{2}{*}{Llama-instruct} & zs & 0.757 & 0.813 \\
%  & sft & 0.838 & 0.528 \\
% \hline
% \end{tabular}
% \caption{Performance evaluation of various models on the MFQ dataset. The table presents the Spearman correlation coefficient ($\uparrow$) and Mean Squared Error (MSE) ($\downarrow$) for both zero-shot (zs) and supervised fine-tuning (sft) modes across different model architectures.}
% \end{table}

\section{Conclusion}
In this paper, we explored the task of specialising LLMs to simulate survey response distributions across diverse countries and questions.
For this task, we devised a fine-tuning method based on first-token probabilities.
Our experiments demonstrate that fine-tuning models substantially improves response simulation prediction compared to zero-shot models, for both seen and unseen countries and questions. Further, fine-tuned models also show improved generalization to an entirely new survey dataset. Despite these improvements, our results also highlight systematic limitations of the models, particularly when simulating responses to unseen questions. We also observed that the models, whether fine-tuned or not, were less diverse in their predictions compared to the human survey response data, raising questions about their utility.

While our results provide clear evidence for the benefits of specializing LLMs for survey simulation tasks, they also underscore the need for caution when using LLMs for this task, as even the best-performing models exhibited systematic inaccuracies, especially in culturally diverse contexts.

% \clearpage


\section*{Limitations} 
While we proved the effectiveness of our proposed method, several limitations remain in our work.

\paragraph{Scope.} Our trained models are highly specialized and can only be used for the specific task of predicting the distribution of answers to a given survey question from a specified human population. Future work will investigate whether the fine-tuning approach also results in less biased or more aligned models in general-purpose applications, but this cannot be claimed only based on our study.

\paragraph{Language and Countries Coverage.} Our study only uses English prompts for experiments and uses countries to represent specific cultures, consistent with existing studies \cite{cao-etal-2023-assessing, alkhamissi-etal-2024-investigating}. While this approach offers some valuable insights, it may limit the applicability of our findings to non-English LLMs and diverse fine-grained cultural contexts. We hope that future research could benefit from exploring broader languages and countries to enhance the robustness of the proposed framework.

\paragraph{Model Choice.} Due to computational resource consideration, we did not fine-tune LLMs with more than 32B parameters and instead selected a limited number of models for validation. Despite this limitation, in future work, we aim to cover a range of powerful models of varying sizes, which will allow us to uncover interesting observations regarding their performance. We believe that the insights we draw will still contribute to future research, encouraging further exploration of larger models to better understand their capabilities in simulating cultural diversity.

% While our refined models can reduce the differences between model predictions and human preferences, we have observed limitations to specific countries and questions in Figure~\ref{fig:diversity_distribution}. This restricts the practical application of the model, as it can only offer general trends in value preferences rather than substitute for human participants in real surveys. We anticipate that implementing a smaller level of alignment will further enhance the model's performance.

\section*{Ethics Statement}
This research adheres to strict ethical standards, ensuring that all datasets, large language models, and prompt settings used are sourced from open-access repositories and are properly licensed to their original creators. 

While our proposed framework does not involve any inherently risky operations, we acknowledge that the deployment of LLMs carries inevitable potential ethical implications. Therefore, users interacting with our models are strongly encouraged to consider safety and ethical factors, remaining aware of the potential risks and harms that may arise from misuse or misinterpretation of the generated content. Through this work, we aim to contribute positively to a better understanding of cultural diversities and promote responsible practices in the simulation of cultural contexts.

\section*{Acknowledgments}
% Ruixiang for preliminary discussions
% CopeNLU people for comments
% The reviewers
We thank anonymous reviewers for their valuable
comments. This research was co-funded by a DFF Sapere
Aude research leader grant under grant agreement
No 0171-00034B, and supported by the Pioneer Centre for AI, DNRF grant number P1.  
Yong Cao was supported by a VolkswagenStiftung Momentum grant.
\bibliography{anthology, custom, pr_refs}

\input{appendix}

\end{document}

