% \clearpage
\appendix

\section{Pew Data Distribution}
\label{ax:pew_data_distribution}
The detailed statistics for the C2 and C3 split of the cultural survey simulation and the C3 split of GlobalOpinionQA data mentioned are available in Table~\ref{tb:c2_c3}.  The sampled countries, along with the sampling reason of Pew-$C_1^\prime$, are in Table~\ref{tb:pew-c1}.

\begin{table}[ht]
\small
\begin{tabular}{ll|lll}
\toprule
\multicolumn{2}{c|}{C2} & \multicolumn{3}{c}{C3}  \\ \midrule \midrule
Country       & CSS-N      & Country     & CSS-N & PWE-N \\
Egypt         &  157      & Malaysia    & 150  & 516      \\
Ethiopia      &  185      & Thailand    & 150  & 319      \\
Kenya         &  185      & Czechia     & 150  & 212      \\
Libya         &   185     & Greece      & 150  & 648      \\
Morocco       &   184     & Nigeria     & 210  & 1044      \\
Nigeria       &   185     & Morocco     & 209  & 450      \\
Tunisia       &   184     & Peru        & 146  & 545      \\
Zimbabwe      &   185     & Colombia    & 150  & 369      \\
              &        & Mexico      &  150 & 890      \\
              &        & Puerto Rico &  150 & 221      \\
              &        & New Zealand &  149 & 274      \\ \bottomrule
\end{tabular}
\caption{Number of Entries Used in C2 and C3 Split of Cultural Survey Simulation and C3 Split of GlobalOpinionQA Data.}
\label{tb:c2_c3}
\end{table}

\begin{table}[ht]
\small
\begin{tabular}{llll}
\toprule
\textbf{Country} & \textbf{Continent} & \textbf{GDP-level} & \textbf{N} \\ \midrule \midrule
Germany         & Europe            & High               & 1130             \\
Japan            & Asia              & High               & 891              \\
Brazil          & South America     & Upper-middle       & 922              \\
Australia        & Oceania           & High               & 627              \\
India            & Asia              & Lower-middle       & 932              \\
Nigeria          & Africa            & Lower-middle       & 1044             \\
United States    & North America     & High               & 1104             \\
Vietnam          & Asia              & Lower-middle       & 471              \\
Chile            & South America     & Upper-middle       & 542              \\
Ukraine          & Europe            & Lower-middle       & 661              \\ \bottomrule
\end{tabular}
\caption{Sampled countries for Pew-$C_1^\prime$ regarding geographical and GDP-level diversity. The dataset consists of 8,324 entries in total.\label{tb:pew-c1}}
\end{table}

\section{WVS data Distribution}
\label{ax:wvs_data_distibution}
Figure \ref{fig:wvs_sample_distribution} illustrates the sample distribution across various countries in World Values Survey. For the purpose of balanced representation, in our formatted dataset, we only include countries with more than 1,000 samples, ensuring consistency in data distribution across countries and minimizing sample size discrepancies in the analysis.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{img/sample_distribution.png} 
    \caption{Sample distribution of the World Values Survey across countries, including only countries with more than 1,000 samples for balanced representation.}
    \label{fig:wvs_sample_distribution}
\end{figure*}



\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/wvs_geo.png}
        \caption{Country distribution across training, validation, and testing sets.}  % 子图标题
        \label{fig:country_split}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/wvs_sunburst.png}
        \caption{Cultural dimension distribution.}  % 子图标题
        \label{fig:question_split}
    \end{subfigure}
    \caption{Visualization of country and cultural dimension divisions of WVS. Countries are categorized into three groups, and questions are divided based on selected cultural dimensions.}
    \label{fig:wvs_distritbuion}
\end{figure*}

Regarding dataset split, we split WVS by cultural dimensions and countries vis multiple sets, which is visualized in Figure~\ref{fig:wvs_distritbuion} and the corresponding cultural dimensions are detailed in Table~\ref{tb:survey_dimensions}, offering a comprehensive view of the overall distribution in the cultural survey simulation.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.46\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/figure4_radar_C3Q1.png}
        \caption{Country Accuracy on C3-Q1.}  % 子图标题
        \label{fig:country_accuracy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.46\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/figure4_radar_C3Q3.png}
        \caption{Country Accuracy on C3-Q3.}  % 子图标题
        \label{fig:question_accuracy}
    \end{subfigure}
    \caption{Llama3 Accuracy of options on African countries in both seen C3-Q1 and unseen C3-Q3 questions.}
    \label{fig:acc_wvs_c3}
\end{figure}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|p{8.5cm}c}
\toprule
\textbf{Set} & \textbf{Cultural Dimension} & \textbf{Number} \\ 
\midrule
\multirow{9}{*}{$Q_1$} & Social values, attitudes and stereotypes    & 45  \\
 & Societal well-being       & 12   \\
 & Social capital, trust and organizational membership &  49  \\
 & Economic values                                    &   6   \\
 & Corruption                                          &   9   \\
 & Migration                                           &  10    \\
 & Post-materialist index                              &  21   \\
 & Science  and technology                                & 6   \\
 & Religious values                                    &    6   \\ \midrule
 \multirow{2}{*}{$Q_2$ } & Security                                            &   12  \\
 & Ethical values and norms                             &   23     \\ \midrule
 \multirow{2}{*}{$Q_3$}  & Political interest and political participation      &   36  \\
 & Political culture and political regimes            &   25   \\ \midrule
 % \midrule
\multirow{1}{*}{$C_1$}  & Andorra, Argentina, Australia, Bangladesh, Armenia, Bolivia, Brazil, Myanmar, Canada, Chile, China, Taiwan ROC, Cyprus, Ecuador, Germany, Guatemala, Hong Kong SAR, Indonesia, Iran, Iraq, Japan, Kazakhstan, Jordan, South Korea, Kyrgyzstan, Lebanon, Macao SAR, Maldives, Mongolia, Netherlands, Nicaragua, Pakistan, Philippines, Romania, Russia, Serbia, Singapore, Slovakia, Vietnam, Tajikistan, Turkey, Ukraine, Great Britain, United States, Uruguay, Venezuela & {46} \\ \midrule
\multirow{1}{*}{$C_2$} & Egypt, Ethiopia, Kenya, Libya, Morocco, Nigeria, Tunisia, Zimbabwe & {8} \\ \midrule
\multirow{1}{*}{$C_3$} & Malaysia, Thailand, Czechia, Greece, Nigeria, Morocco, Peru, Colombia, Mexico, Puerto Rico, New Zealand & {11} \\
\bottomrule
\end{tabular}}
\caption{\label{tb:survey_dimensions} Cultural dimensions and question ids of WVS. Question 82-223 and 94-106 are excluded in $Q_1$ as they are demo graphical questions. Demo graphical questions are excluded as they are related to individual attribute regarding participants and does not have relevance to group culture.}
\end{table}

\section{Hyperparameter Settings}\label{app:hyperparams}
Training is performed on a single A100 GPU with a batch size of 16 for Llama3 and Vicuna1.5-7B, and 4 for Vicuna1.5-13B. Both models use the AdamW optimizer with a learning rate of 1e-4 and implement Fully Sharded Data Parallel along with a mixed precision strategy to enhance computational efficiency. In our experiments, we employ LoRA with a rank of 8, a scaling factor lora\_alpha set to 32, and a dropout rate of 0.05. 


% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/training_compare.png}
%         \caption{Training Loss Comparison}
%         \label{fig:training_loss_comparison}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.31\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/figure4_radar_C3Q1.png}
%         \caption{Country Accuracy on C3-Q1}
%         \label{fig:country_accuracy_C3_Q1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.31\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/figure4_radar_C3Q3.png}
%         \caption{Country Accuracy on C3-Q3}
%         \label{fig:country_accuracy_C3_Q3}
%     \end{subfigure}
%     \caption{Comparison of training loss among three different models and Llama3 accuracy on African countries in both seen C3-Q1 and unseen C3-Q3 questions.}
%     \label{fig:combined_figure}
% \end{figure*}



\paragraph{Model Download} We list all used models here: 

\begin{itemize}
    \item \textbf{Vicuna1.5-7B}: \url{https://huggingface.co/lmsys/vicuna-7b-v1.5}
    \item \textbf{Vicuna1.5-13B}: \url{https://huggingface.co/lmsys/vicuna-13b-v1.5}
    \item \textbf{Distil-Qwen-7B}: \url{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B}
    \item \textbf{Distil-Qwen-14B}: \url{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B}
    \item \textbf{Distil-Qwen-32B}: \url{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B}
    \item \textbf{Llama3-8B-Base}: \url{https://huggingface.co/meta-llama/Meta-Llama-3-8B}
    \item \textbf{Llama3-8B-Instruct}: \url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}
    \item \textbf{Qwen-7B}: \url{https://huggingface.co/Qwen/Qwen-7B}
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{img/training_compare.png} 
  \caption{Training loss comparsion of three models: Qwen-7B, Llama-8B-Instruct, Distill-Qwen-32B(Deepseek)}
  \label{fig:training_loss_comparison}
\end{figure}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{img/global_dist.pdf} 
    \caption{Distribution of $1-$JSD Global Scores for the Model on Unseen Cultural Questions ($C_1$-$Q_3$). The Instruct model exhibits a more distinct improvement compared to the Base model.}
    \label{fig:global_distrition}
\end{figure*}

\section{More Option Prediction Analysis}

\paragraph{Option Prediction Accuracy.}
In line with Figure~\ref{fig:diversity_distribution}e-\ref{fig:diversity_distribution}f, we present the remaining two sets of Llama3 option prediction accuracy for C3-Q1 and C3-Q3 here (see Figure \ref{fig:acc_wvs_c3}). 

The results indicate that the model performs significantly better on seen questions (C3-Q1) compared to unseen ones (C3-Q3), with Llama3 Instruct models consistently outperforming Base models, and fine-tuned approaches demonstrating superior accuracy over zero-shot methods across most countries. Specifically, the model achieves its highest accuracy in Greece, Nigeria, and Peru, while showing weaker performance in Thailand and Mexico. Despite the challenges in predicting unseen questions, fine-tuning through simulation leads to noticeable improvements, highlighting the potential of fine-tuned models to enhance generalization in novel scenarios. 


\paragraph{Country Distributions.} 
Figure~\ref{fig:global_distrition} visualizes the distribution of $1-$JSD scores for the Llama3-Base and Llama3-Instruct models across unseen cultural questions ($C_1$-$Q_3$) in both zero-shot (ZS) and fine-tuned (FT) modes. In ZS mode, both models perform poorly across many regions, with lower scores indicating limited cultural sensitivity. This suggests that without additional tuning, both models struggle to effectively generalize to simulate culturally diverse distributions. 
Furthermore, the Instruct model performs slightly worse, likely due to the reinforcement learning and safety strategies, which may inadvertently reduce its sensitivity to cultures in this scenario.

However, after FT, both models show improvement across all countries, particularly significant in regions previously displaying poor performance in the ZS setting. Additionally, the performance of both the fine-tuned Base model and the Instruct model is observed to be very similar across different countries, demonstrating that our methods can effectively align the Base and Instruct models.




% \begin{table*}[!]
% \centering
% \resizebox{0.99\textwidth}{!}{
% \begin{tabular}{l|l|ccccc|c|ccccc|c}
% \toprule
% \multirow{2}{*}{Base Model} & \multirow{2}{*}{Methods} &  \multicolumn{6}{c|}{($1-$JSD) $\uparrow$} & \multicolumn{6}{c}{EMD $\downarrow$} \\  \cmidrule{3-14}
%                             & & $C_1$-$Q_3$ & $C_2$-$Q_1$ & $C_2$-$Q_3$ & $C_3$-$Q_1$ & $C_3$-$Q_3$ & \textit{Avg.} & $C_1$-$Q_3$ & $C_2$-$Q_1$ & $C_2$-$Q_3$ & $C_3$-$Q_1$ & $C_3$-$Q_3$ & \textit{Avg.}  \\ \midrule \midrule
% % \multicolumn{13}{c}
% % {\textit{Vicuna1.5\_7B}} \\ \midrule
% \multirow{4}{*}{\textit{Vicuna1.5-7B}} & ZS [ctrl] & 0.732 & 0.754 & 0.748 & 0.761 & 0.747 & 0.748 & 0.095 & 0.108 & 0.095 & 0.106 & 0.086 & 0.098 \\
% & ZS & 0.732 & 0.754 & 0.749 & 0.761 & 0.749 & 0.749 & 0.095 & 0.107 & 0.096 & 0.106 & 0.085  &  0.098 \\
% & FT [ctrl] & 0.754 & 0.829 & 0.754 & 0.842 & 0.765 & 0.789 & 0.089 & 0.078 & 0.092 & 0.072 & 0.080  & 0.082 \\ 
% & \cellcolor{customgray}\textbf{FT} & 
% \cellcolor{customgray}\textbf{0.766} & 
% \cellcolor{customgray}\textbf{0.859} & 
% \cellcolor{customgray}\textbf{0.767} & 
% \cellcolor{customgray}\textbf{0.875} & 
% \cellcolor{customgray}\textbf{0.775} & 
% \cellcolor{customgray}\textbf{0.808} & 
% \cellcolor{customgray}\textbf{0.087} & 
% \cellcolor{customgray}\textbf{0.071} & 
% \cellcolor{customgray}\textbf{0.091} & 
% \cellcolor{customgray}\textbf{0.062} & 
% \cellcolor{customgray}\textbf{0.078} & 
% \cellcolor{customgray}\textbf{0.078}
%  \\ \midrule
% %  \multicolumn{13}{c}
% % {\textit{Vicuna1.5\_13B}} \\ \midrule
% \multirow{4}{*}{\textit{Vicuna1.5-13B}} & ZS [ctrl]  & 0.735 & 0.755 & 0.743 & 0.775 & 0.747 & 0.751 & 0.095 & 0.102 & 0.096 & 0.096 & 0.086 & 0.095 \\
% & ZS & 0.738 & 0.756 & 0.744 & 0.779 & 0.752 & 0.754 & 0.094 & 0.103 & 0.097 & 0.097 & 0.085 & 0.095 \\
% & FT [ctrl]  & 0.760 & 0.820 & 0.761 & 0.830 & 0.762 & 0.787 & 0.083 & 0.080 & 0.085 & 0.074 & 0.077 & 0.080 \\ 
% & \cellcolor{customgray}\textbf{FT} & 
% \cellcolor{customgray}\textbf{0.781} & 
% \cellcolor{customgray}\textbf{0.869} & 
% \cellcolor{customgray}\textbf{0.781} & 
% \cellcolor{customgray}\textbf{0.882} & 
% \cellcolor{customgray}\textbf{0.781} & 
% \cellcolor{customgray}\textbf{0.819} & 
% \cellcolor{customgray}\textbf{0.079} & 
% \cellcolor{customgray}\textbf{0.063} & 
% \cellcolor{customgray}\textbf{0.084} & 
% \cellcolor{customgray}\textbf{0.057} & 
% \cellcolor{customgray}\textbf{0.073} & 
% \cellcolor{customgray}\textbf{0.071} \\
% \bottomrule
% \end{tabular}}
% \caption{\label{tb:ax_more_results}\textbf{ Results on Vicuna1.5 models for predicting country-level survey response distributions on the WVS data}.
% We test all models with zero-shot prompting (ZS) and our proposed fine-tuning approach (FT). [ctrl] indicates a control setup, where we randomly replace countries in test prompts with other countries, to evaluate country context sensitivity.
% We report Jensen-Shannon Divergence ($1-$JSD , $\uparrow$) and Earth Mover Distance (EMD, $\downarrow$).}
% \end{table*}


\section{More Baseline Comparsion}
\label{ax:more_baseline}

To further assess the effectiveness of our proposed approach, we compare it against several other baseline methods as follows:

\begin{itemize}
    \item \textbf{KNN}: This method identifies the most similar training question and country (top-1, $k=1$) using BERT embeddings and returns the corresponding option distributions as predictions.
    \item \textbf{Avg\_Culture}: We computes the mean option distribution across all training countries for each known question and adopts a uniform random distribution for unknown questions.
    \item \textbf{JSON-ZS}: This approach prompts the models to directly generate option distributions in JSON format without additional fine-tuning.
\end{itemize}

Table \ref{tab:ex_baseline_results} presents the 1-JSD scores for various baseline methods and our proposed approach. The results indicate that all baseline methods perform substantially worse than our fine-tuned (FT) models. Among the baselines, the JSON-ZS method demonstrates superior performance compared to KNN and Avg\_Culture; however, it remains less effective than both zero-shot (ZS) and fine-tuned (FT) approaches. Notably, fine-tuning consistently yields the highest scores across all evaluated settings, underscoring its effectiveness in improving model performance. 

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Methods} & \textbf{C1-Q3} & \textbf{C2-Q1} & \textbf{C2-Q3} & \textbf{C3-Q1} & \textbf{C3-Q3} & \textbf{Avg.} \\
        \midrule \midrule
        \textbf{KNN} & 0.381 & 0.518 & 0.371 & 0.541 & 0.384 & 0.439 \\
        \textbf{Avg\_Culture} & 0.360 & 0.509 & 0.348 & 0.518 & 0.368 & 0.421 \\
        \midrule
        \multicolumn{7}{c}{\textit{Llama3-8B-Base}} \\
        \textbf{JSON-ZS} & 0.754 & 0.728 & 0.581 & 0.729 & 0.751 & 0.709 \\
        \textbf{ZS} & 0.749 & 0.768 & 0.759 & 0.781 & 0.770 & 0.765 \\
        \rowcolor{customgray}
        \textbf{FT} & \textbf{0.770} & \textbf{0.858} & \textbf{0.773} & \textbf{0.877} & \textbf{0.781} & \textbf{0.812} \\
        \midrule
        \multicolumn{7}{c}{\textit{Llama3-8B-Instruct}} \\
        \textbf{JSON-ZS} & 0.735 & 0.728 & 0.747 & 0.729 & 0.751 & 0.738 \\
        \textbf{ZS} & 0.585 & 0.650 & 0.589 & 0.657 & 0.584 & 0.613 \\
        \rowcolor{customgray}
        \textbf{FT} & \textbf{0.777} & \textbf{0.881} & \textbf{0.783} & \textbf{0.890} & \textbf{0.784} & \textbf{0.823} \\
        \bottomrule
    \end{tabular}}
    \caption{Comparison of 1-JSD scores across different baseline methods and our approach. Higher values indicate better alignment with the ground-truth distribution.}
    \label{tab:ex_baseline_results}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{img/jsd_score.png} 
  \caption{Comparsion of different languages in 1-JSD score on random selected 100 samples.}
  \label{fig:language_effect}
\end{figure}

\section{More Model Evaluation}
\label{ax:vicuna_results}
In this section, we present additional experiment results on original Qwen model. We observe that the original Qwen models present challenges in training for our task. As shown in Figure \ref{fig:training_loss_comparison}, we compare the training progress of three models. The loss of the Qwen-7B model exhibits difficulty in converging compared to Llama-8B-Instruct and Distill-Qwen-32B. This difficulty may stem from the extensive safety alignment or policy alignment incorporated into Qwen, which could introduce additional constraints or optimization challenges during training.

Secondly, we visualized the 1-JSD score distribution of the model for English ZS, FT, and Chinese FT, as shown in Figure \ref{fig:language_effect}, with the red dashed line indicating the average value. The results show that language differences have a smaller impact on the model than training levels. Additionally, we calculate the Pearson correlation coefficient between English ZS and FT which (0.349) is lower than that between English FT and Chinese FT (0.579). While both correlations are positive, the stronger correlation observed between English FT and Chinese FT suggests that training level exerts a greater influence than language differences.

% As shown in Table \ref{tb:ax_more_results}, for Vicuna1.5-13B, the FT model achieves an average (1-JSD) score of 0.819, surpassing the best ZS result (0.751) by a significant margin. A similar trend is observed for Vicuna1.5-7B, where FT yields the highest (1-JSD) score of 0.808. Moreover, FT achieves the lowest EMD values across all settings, indicating that fine-tuned models generate distributions that more closely align with ground truth data. The control experiments further reveal a degradation in model performance when countries are randomly replaced in prompts, suggesting that both ZS and FT models exhibit some level of context sensitivity. These findings are consistently aligned with other models.