@article{elazar-etal-2021-measuring,
    title = "Measuring and Improving Consistency in Pretrained Language Models",
    author = {Elazar, Yanai  and
      Kassner, Nora  and
      Ravfogel, Shauli  and
      Ravichander, Abhilasha  and
      Hovy, Eduard  and
      Sch{\"u}tze, Hinrich  and
      Goldberg, Yoav},
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.60",
    doi = "10.1162/tacl_a_00410",
    pages = "1012--1031",
    abstract = "Consistency of a model{---}that is, the invariance of its behavior under meaning-preserving alternations in its input{---}is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel��, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel��, we show that the consistency of all PLMs we experiment with is poor{---} though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1",
}


@misc{liang2023holistic,
      title={Holistic Evaluation of Language Models}, 
      author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
      year={2023},
      eprint={2211.09110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{petroni-etal-2019-language,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at \url{https://github.com/facebookresearch/LAMA}.",
}


@inproceedings{mandl2021hasoc,
author = {Modha, Sandip and Mandl, Thomas and Shahi, Gautam Kishore and Madhu, Hiren and Satapara, Shrey and Ranasinghe, Tharindu and Zampieri, Marcos},
title = {Overview of the HASOC Subtrack at FIRE 2021: Hate Speech and Offensive Content Identification in English and Indo-Aryan Languages and Conversational Hate Speech},
year = {2021},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503176},
doi = {10.1145/3503162.3503176},
abstract = {The HASOC track is dedicated to the evaluation of technology for finding Offensive Language and Hate Speech. HASOC is creating a multilingual data corpus mainly for English and under-resourced languages(Hindi and Marathi). This paper presents one HASOC subtrack with two tasks. In 2021, we organized the classification task for English, Hindi, and Marathi. The first task consists of two classification tasks; Subtask 1A consists of a binary and fine-grained classification into offensive and non-offensive tweets. Subtask 1B asks to classify the tweets into Hate, Profane and offensive. Task 2 consists of identifying tweets given additional context in the form of the preceding conversion. During the shared task, 65 teams have submitted 652 runs. This overview paper briefly presents the task descriptions, the data and the results obtained from the participant’s submission.},
booktitle = {Forum for Information Retrieval Evaluation},
pages = {1–3},
numpages = {3},
keywords = {Under-resourced language, Multilingual Datasets, social media, hate speech},
location = {Virtual Event, India},
series = {FIRE 2021}
}

@article{durmus2023globalopinionqa,
  title={Towards measuring the representation of subjective global opinions in language models},
  author={Durmus, Esin and Nyugen, Karina and Liao, Thomas I and Schiefer, Nicholas and Askell, Amanda and Bakhtin, Anton and Chen, Carol and Hatfield-Dodds, Zac and Hernandez, Danny and Joseph, Nicholas and others},
  journal={arXiv preprint arXiv:2306.16388},
  year={2023}
}

@article{motoki2023more,
  title={More human than human: Measuring ChatGPT political bias},
  author={Motoki, Fabio and Neto, Valdemar Pinho and Rodrigues, Victor},
  journal={Public Choice},
  pages={1--21},
  year={2023},
  publisher={Springer}
}

@article{kwok2024evaluating,
  title={Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas},
  author={Kwok, Louis and Bravansky, Michal and Griffin, Lewis D},
  journal={arXiv preprint arXiv:2408.06929},
  year={2024}
}

@inproceedings{zhao2024worldvaluesbench,
  title={WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural Value Awareness of Language Models},
  author={Zhao, Wenlong and Mondal, Debanjan and Tandon, Niket and Dillion, Danica and Gray, Kurt and Gu, Yuling},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={17696--17706},
  year={2024}
}

@article{zheng2023lmsys,
  title={Lmsys-chat-1m: A large-scale real-world llm conversation dataset},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric and others},
  journal={arXiv preprint arXiv:2309.11998},
  year={2023}
}

@article{ouyang2023shifted,
  title={The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions},
  author={Ouyang, Siru and Wang, Shuohang and Liu, Yang and Zhong, Ming and Jiao, Yizhu and Iter, Dan and Pryzant, Reid and Zhu, Chenguang and Ji, Heng and Han, Jiawei},
  journal={arXiv preprint arXiv:2310.12418},
  year={2023}
}

@inproceedings{scherrer2023evaluating,
  title={Evaluating the Moral Beliefs Encoded in LLMs},
  author={Scherrer, Nino and Shi, Claudia and Feder, Amir and Blei, David},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@article{poole1998recovering,
  title={Recovering a basic space from a set of issue scales},
  author={Poole, Keith T},
  journal={American Journal of Political Science},
  pages={954--993},
  year={1998},
  publisher={JSTOR}
}

@article{chong2007framing,
  title={Framing theory},
  author={Chong, Dennis and Druckman, James N},
  journal={Annu. Rev. Polit. Sci.},
  volume={10},
  pages={103--126},
  year={2007},
  publisher={Annual Reviews}
}

@article{busby2018studying,
  title={Studying framing effects on political preferences},
  author={Busby, Ethan and Flynn, D and Druckman, James N and D’Angelo, P},
  journal={Doing news framing analysis II: Empirical and theoretical perspectives},
  pages={27--50},
  year={2018},
  publisher={Routledge New York}
}

@misc{milanlp-2023-simple-generation,
  author = {Giuseppe Attanasio},
  title = {{S}imple {G}eneration},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/MilaNLProc/simple-generation}}
}

@article{wang2024my,
  title={" My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models},
  author={Wang, Xinpeng and Ma, Bolei and Hu, Chengzhi and Weber-Genzel, Leon and R{\"o}ttger, Paul and Kreuter, Frauke and Hovy, Dirk and Plank, Barbara},
  journal={arXiv preprint arXiv:2402.14499},
  year={2024}
}

@article{poole1991patterns,
  title={Patterns of congressional voting},
  author={Poole, Keith T and Rosenthal, Howard},
  journal={American journal of political science},
  pages={228--278},
  year={1991},
  publisher={JSTOR}
}



@misc{janus2022simulators,
  author={Janus},
  title={Simulators. {LessWrong online forum, 2nd September}},
  year={2022},
  note={\url{https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/}},
}

@inproceedings{feng2023pretraining,
    title = "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair {NLP} Models",
    author = "Feng, Shangbin  and
      Park, Chan Young  and
      Liu, Yuhan  and
      Tsvetkov, Yulia",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.656",
    doi = "10.18653/v1/2023.acl-long.656",
    pages = "11737--11762",
    abstract = "Language models (LMs) are pretrained on diverse data sources{---}news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.",
}

@article{binz2023using,
  title={Using cognitive psychology to understand GPT-3},
  author={Binz, Marcel and Schulz, Eric},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={6},
  pages={e2218523120},
  year={2023},
  publisher={National Acad Sciences}
}

@inproceedings{kirk2023past,
    title = "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
    author = "Kirk, Hannah  and
      Bean, Andrew  and
      Vidgen, Bertie  and
      Rottger, Paul  and
      Hale, Scott",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.148",
    doi = "10.18653/v1/2023.emnlp-main.148",
    pages = "2409--2430",
    abstract = "Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.",
}

@misc{narayanan2023liberalbias,
  author={Narayanan, Arvind and Kapoor, Sayash},
  title={Does ChatGPT have a liberal bias?},
  year={2023},
  note={\url{https://www.aisnakeoil.com/p/does-chatgpt-have-a-liberal-bias}},
}


@article{shanahan2023role,
  title={Role play with large language models},
  author={Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
  journal={Nature},
  volume={623},
  number={7987},
  pages={493--498},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{andrade2018internal,
  title={Internal, external, and ecological validity in research design, conduct, and evaluation},
  author={Andrade, Chittaranjan},
  journal={Indian journal of psychological medicine},
  volume={40},
  number={5},
  pages={498--499},
  year={2018},
  publisher={SAGE Publications Sage India: New Delhi, India}
}

@article{rottger2023xstest,
  title={Xstest: A test suite for identifying exaggerated safety behaviours in large language models},
  author={R{\"o}ttger, Paul and Kirk, Hannah Rose and Vidgen, Bertie and Attanasio, Giuseppe and Bianchi, Federico and Hovy, Dirk},
  journal={arXiv preprint arXiv:2308.01263},
  year={2023}
}

@article{xu2023cvalues,
  title={Cvalues: Measuring the values of chinese large language models from safety to responsibility},
  author={Xu, Guohai and Liu, Jiayi and Yan, Ming and Xu, Haotian and Si, Jinghui and Zhou, Zhuoran and Yi, Peng and Gao, Xing and Sang, Jitao and Zhang, Rong and others},
  journal={arXiv preprint arXiv:2307.09705},
  year={2023}
}

@inproceedings{
  zhao2024inthewildchat,
  title={(InThe)WildChat: 570K Chat{GPT} Interaction Logs In The Wild},
  author={Zhao, Wenting and Ren, Xiang and Hessel, Jack and Cardie, Claire and Choi, Yejin and Deng, Yuntian},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=Bl8u7ZRlbM}
}

@inproceedings{hendrycks2020aligning,
  title={Aligning AI With Shared Human Values},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{slapin2008scaling,
  title={A scaling model for estimating time-series party positions from texts},
  author={Slapin, Jonathan B and Proksch, Sven-Oliver},
  journal={American Journal of Political Science},
  volume={52},
  number={3},
  pages={705--722},
  year={2008},
  publisher={Wiley Online Library}
}

@article{king2004enhancing,
  title={Enhancing the validity and cross-cultural comparability of measurement in survey research},
  author={King, Gary and Murray, Christopher JL and Salomon, Joshua A and Tandon, Ajay},
  journal={American political science review},
  volume={98},
  number={1},
  pages={191--207},
  year={2004},
  publisher={Cambridge University Press}
}

@article{lauderdale2016measuring,
  title={Measuring political positions from legislative speech},
  author={Lauderdale, Benjamin E and Herzog, Alexander},
  journal={Political Analysis},
  volume={24},
  number={3},
  pages={374--394},
  year={2016},
  publisher={Cambridge University Press}
}


@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944},
  year={2023}
}

@inproceedings{wang2021adversarial,
  title={Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models},
  author={Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}


@inproceedings{wang2023robustness,
  title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective},
  author={Wang, Jindong and Xixu, HU and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Ye, Wei and Huang, Haojun and Geng, Xiubo and others},
  booktitle={ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models},
  year={2023}
}

@inproceedings{thapa2023assessing,
    title = "Assessing Political Inclination of {B}angla Language Models",
    author = "Thapa, Surendrabikram  and
      Maratha, Ashwarya  and
      Hasib, Khan Md  and
      Nasim, Mehwish  and
      Naseem, Usman",
    editor = "Alam, Firoj  and
      Kar, Sudipta  and
      Chowdhury, Shammur Absar  and
      Sadeque, Farig  and
      Amin, Ruhul",
    booktitle = "Proceedings of the First Workshop on Bangla Language Processing (BLP-2023)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.banglalp-1.8",
    doi = "10.18653/v1/2023.banglalp-1.8",
    pages = "62--71",
    abstract = "Natural language processing has advanced with AI-driven language models (LMs), that are applied widely from text generation to question answering. These models are pre-trained on a wide spectrum of data sources, enhancing accuracy and responsiveness. However, this process inadvertently entails the absorption of a diverse spectrum of viewpoints inherent within the training data. Exploring political leaning within LMs due to such viewpoints remains a less-explored domain. In the context of a low-resource language like Bangla, this area of research is nearly non-existent. To bridge this gap, we comprehensively analyze biases present in Bangla language models, specifically focusing on social and economic dimensions. Our findings reveal the inclinations of various LMs, which will provide insights into ethical considerations and limitations associated with deploying Bangla LMs.",
}

@article{rutinowski2023self,
  title={The Self-Perception and Political Biases of ChatGPT},
  author={Rutinowski, J{\'e}r{\^o}me and Franke, Sven and Endendyk, Jan and Dormuth, Ina and Pauly, Markus},
  journal={arXiv preprint arXiv:2304.07333},
  year={2023}
}

@article{van2023chatgpt,
  title={ChatGPT’s left-leaning liberal bias},
  author={van den Broek, Merel},
  journal={University of Leiden},
  year={2023}
}

@article{fujimoto2023revisiting,
  title={Revisiting the political biases of ChatGPT},
  author={Fujimoto, Sasuke and Kazuhiro, Takemoto},
  journal={Frontiers in Artificial Intelligence},
  volume={6},
  year={2023},
  publisher={Frontiers}
}

@article{rozado2023political,
  title={The political biases of chatgpt},
  author={Rozado, David},
  journal={Social Sciences},
  volume={12},
  number={3},
  pages={148},
  year={2023},
  publisher={MDPI}
}

@article{rozado2023danger,
  title={Danger in the Machine: The Perils of Political and Demographic Biases Embedded in AI Systems},
  author={Rozado, David},
  journal={Manhattan Institute},
  year={2023}
}

@inproceedings{ghafouri2023ai,
  title={AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics},
  author={Ghafouri, Vahid and Agarwal, Vibhor and Zhang, Yong and Sastry, Nishanth and Such, Jose and Suarez-Tangil, Guillermo},
  booktitle={Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
  pages={556--565},
  year={2023}
}

@article{rozado2024political,
  title={The Political Preferences of LLMs},
  author={Rozado, David},
  journal={arXiv preprint arXiv:2402.01789},
  year={2024}
}

@inproceedings{espana2023multilingual,
    title = "Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a {C}hat{GPT} and Bard Newspaper",
    author = "Espa{\~n}a-Bonet, Cristina",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.787",
    doi = "10.18653/v1/2023.findings-emnlp.787",
    pages = "11757--11777",
    abstract = "Neutrality is difficult to achieve and, in politics, subjective. Traditional media typically adopt an editorial line that can be used by their potential readers as an indicator of the media bias. Several platforms currently rate news outlets according to their political bias. The editorial line and the ratings help readers in gathering a balanced view of news. But in the advent of instruction-following language models, tasks such as writing a newspaper article can be delegated to computers. Without imposing a biased persona, where would an AI-based news outlet lie within the bias ratings? In this work, we use the ratings of authentic news outlets to create a multilingual corpus of news with coarse stance annotations (Left and Right) along with automatically extracted topic annotations. We show that classifiers trained on this data are able to identify the editorial line of most unseen newspapers in English, German, Spanish and Catalan. We then apply the classifiers to 101 newspaper-like articles written by ChatGPT and Bard in the 4 languages at different time periods. We observe that, similarly to traditional newspapers, ChatGPT editorial line evolves with time and, being a data-driven system, the stance of the generated articles differs among languages.",
}


@inproceedings{miotto2022whoisgpt,
    title = "Who is {GPT}-3? An exploration of personality, values and demographics",
    author = "Miotto, Maril{\`u}  and
      Rossberg, Nicola  and
      Kleinberg, Bennett",
    editor = "Bamman, David  and
      Hovy, Dirk  and
      Jurgens, David  and
      Keith, Katherine  and
      O'Connor, Brendan  and
      Volkova, Svitlana",
    booktitle = "Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS)",
    month = nov,
    year = "2022",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlpcss-1.24",
    doi = "10.18653/v1/2022.nlpcss-1.24",
    pages = "218--227",
    abstract = "Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its self-reported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa.",
}

@article{hartmann2023political,
  title={The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation},
  author={Hartmann, Jochen and Schwenzow, Jasper and Witte, Maximilian},
  journal={arXiv preprint arXiv:2301.01768},
  year={2023}
}

@inproceedings{mandl2019overview,
  title={Overview of the hasoc track at fire 2019: Hate speech and offensive content identification in indo-european languages},
  author={Mandl, Thomas and Modha, Sandip and Majumder, Prasenjit and Patel, Daksh and Dave, Mohana and Mandlia, Chintak and Patel, Aditya},
  booktitle={Proceedings of the 11th Forum for Information Retrieval Evaluation},
  pages={14--17},
  year={2019}
}

@article{banerjee2021exploring,
  title={Exploring Transformer Based Models to Identify Hate Speech and Offensive Content in {English} and {Indo-Aryan} Languages},
  author={Banerjee, Somnath and Sarkar, Maulindu and Agrawal, Nancy and Saha, Punyajoy and Das, Mithun},
  journal={arXiv preprint arXiv:2111.13974},
  year={2021},
  url = {https://arxiv.org/abs/2111.13974}
}

@inproceedings{ousidhoum2019multilingual,
    title = "Multilingual and Multi-Aspect Hate Speech Analysis",
    author = "Ousidhoum, Nedjma  and
      Lin, Zizheng  and
      Zhang, Hongming  and
      Song, Yangqiu  and
      Yeung, Dit-Yan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1474",
    doi = "10.18653/v1/D19-1474",
    pages = "4675--4684",
    abstract = "Current research on hate speech analysis is typically oriented towards monolingual and single classification tasks. In this paper, we present a new multilingual multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art multilingual multitask learning approaches. We evaluate our dataset in various classification settings, then we discuss how to leverage our annotations in order to improve hate speech detection and classification in general.",
}

@inproceedings{conneau2020xlmr,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}


%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Paul at 2020-02-28 15:39:58 +0000 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{davidson2017automated,
  title={Automated hate speech detection and the problem of offensive language},
  author={Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},
  booktitle={Proceedings of the 11th International AAAI Conference on Web and Social Media},
  pages = {512--515},
  year={2017},
  publisher = {Association for the Advancement of Artificial Intelligence}
}

@inproceedings{bohra2018dataset,
  title = {A {{Dataset}} of {{Hindi}}-{{English Code}}-{{Mixed Social Media Text}} for {{Hate Speech Detection}}},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Computational Modeling}} of {{People}}'s {{Opinions}}, {{Personality}}, and {{Emotions}} in {{Social Media}}},
  author = {Bohra, Aditya and Vijay, Deepanshu and Singh, Vinay and Akhtar, Syed Sarfaraz and Shrivastava, Manish},
  date = {2018-06},
  pages = {36--41},
  publisher = {{Association for Computational Linguistics}},
  location = {{New Orleans, Louisiana, USA}},
  doi = {10.18653/v1/W18-1105},
  url = {https://www.aclweb.org/anthology/W18-1105},
  urldate = {2020-11-23},
  abstract = {Hate speech detection in social media texts is an important Natural language Processing task, which has several crucial applications like sentiment analysis, investigating cyberbullying and examining socio-political controversies. While relevant research has been done independently on code-mixed social media texts and hate speech detection, our work is the first attempt in detecting hate speech in Hindi-English code-mixed social media text. In this paper, we analyze the problem of hate speech detection in code-mixed texts and present a Hindi-English code-mixed dataset consisting of tweets posted online on Twitter. The tweets are annotated with the language at word level and the class they belong to (Hate Speech or Normal Speech). We also propose a supervised classification system for detecting hate speech in the text using various character level, word level, and lexicon based features.},
  file = {/Users/Paul/Zotero/storage/F6KB5XL6/Bohra et al. - 2018 - A Dataset of Hindi-English Code-Mixed Social Media.pdf}
}



    
    
@inproceedings{kaushik2020learning,
  title = {Learning the Difference That Makes a Difference with Counterfactually-Augmented Data},
  author = {Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C.},
  date = {2020-02-14},
  year = {2020},
  url = {http://arxiv.org/abs/1909.12434},
  urldate = {2020-11-24},
  eventtitle = {{{ICLR}} 2020},
  booktitle= {Proceedings of the 8th International Conference on Learning Representations},
  abstract = {Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.},
  archivePrefix = {arXiv},
  eprint = {1909.12434},
  eprinttype = {arxiv},
  file = {/Users/Paul/Zotero/storage/8G2ALLKK/Kaushik et al. - 2020 - Learning the Difference that Makes a Difference wi.pdf;/Users/Paul/Zotero/storage/9VNEPSR9/1909.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}



@inproceedings{nie2020adversarial,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and
      Williams, Adina  and
      Dinan, Emily  and
      Bansal, Mohit  and
      Weston, Jason  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.441",
    doi = "10.18653/v1/2020.acl-main.441",
    pages = "4885--4901",
    abstract = "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
}



    
    
    
@inproceedings{unsvaag2018effects,
  ids = {fehnunsvaag2018effects},
  title = {The {{Effects}} of {{User Features}} on {{Twitter Hate Speech Detection}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Abusive Language Online}} ({{ALW2}})},
  author = {Unsvaag, Elise Fehn and Gambäck, Björn},
  date = {2018-10},
  pages = {75--85},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/W18-5110},
  url = {https://www.aclweb.org/anthology/W18-5110},
  urldate = {2020-11-21},
  abstract = {The paper investigates the potential effects user features have on hate speech classification. A quantitative analysis of Twitter data was conducted to better understand user characteristics, but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classifier based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of classification performance. While the incorporation of user features resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements.},
  file = {/Users/Paul/Zotero/storage/GZBQJGAX/Fehn Unsvaag and Gambäck - 2018 - The Effects of User Features on Twitter Hate Speec.pdf;/Users/Paul/Zotero/storage/J92VDYPA/Fehn Unsvaag and Gambäck - 2018 - The Effects of User Features on Twitter Hate Speec.pdf}
}



    
@inproceedings{ousidhoum2020comparative,
  title = {Comparative {{Evaluation}} of {{Label}}-{{Agnostic Selection Bias}} in {{Multilingual Hate Speech Datasets}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Ousidhoum, Nedjma and Song, Yangqiu and Yeung, Dit-Yan},
  date = {2020-11},
  year = {2020},
  pages = {2532--2542},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.199},
  urldate = {2020-11-24},
  abstract = {Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.},
  eventtitle = {{{EMNLP}} 2020},
  file = {/Users/Paul/Zotero/storage/AX2VGU3X/Ousidhoum et al. - 2020 - Comparative Evaluation of Label-Agnostic Selection.pdf}
}



    
@inproceedings{levesque2012winograd,
  title={The {W}inograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={13th International Conference on the Principles of Knowledge Representation and Reasoning},
  year={2012}
}


@inproceedings{dinan2019build,
    title = "Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack",
    author = "Dinan, Emily  and
      Humeau, Samuel  and
      Chintagunta, Bharath  and
      Weston, Jason",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1461",
    doi = "10.18653/v1/D19-1461",
    pages = "4537--4546",
    abstract = "The detection of offensive language in the context of a dialogue has become an increasingly important application of natural language processing. The detection of trolls in public forums (Gal{\'a}n-Garc{\'\i}a et al., 2016), and the deployment of chatbots in the public domain (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, fix it scheme with humans and models in the loop. In detailed experiments we show this approach is considerably more robust than previous systems. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and cannot be viewed as a single sentence offensive detection task as in most previous work. Our newly collected tasks and methods are all made open source and publicly available.",
}

@inproceedings{park2018reducing,
    title = "Reducing Gender Bias in Abusive Language Detection",
    author = "Park, Ji Ho  and
      Shin, Jamin  and
      Fung, Pascale",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1302",
    doi = "10.18653/v1/D18-1302",
    pages = "2799--2804",
    abstract = "Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, {``}You are a good woman{''} was considered {``}sexist{''} when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98{\%} and can be extended to correct model bias in other scenarios.",
}


@inproceedings{nobata2016abusive,
author = {Nobata, Chikashi and Tetreault, Joel and Thomas, Achint and Mehdad, Yashar and Chang, Yi},
title = {Abusive Language Detection in Online User Content},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883062},
doi = {10.1145/2872427.2883062},
abstract = {Detection of abusive language in user generated online content has become an issue of increasing importance in recent years. Most current commercial methods make use of blacklists and regular expressions, however these measures fall short when contending with more subtle, less ham-fisted examples of hate speech. In this work, we develop a machine learning based method to detect hate speech on online user comments from two domains which outperforms a state-of-the-art deep learning approach. We also develop a corpus of user comments annotated for abusive language, the first of its kind. Finally, we use our detection tool to analyze abusive language over time and in different settings to further enhance our knowledge of this behavior.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {145–153},
numpages = {9},
keywords = {hate speech, natural language processing, nlp, abusive language, discourse classification, stylistic classification},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{calabrese2021aaa,
  title={AAA: Fair Evaluation for Abuse Detection Systems Wanted},
  author={Calabrese, Agostina and Bevilacqua, Michele and Ross, Bj{\"o}rn and Tripodi, Rocco and Navigli, Roberto},
  booktitle={13th ACM Web Science Conference 2021},
  pages={243--252},
  year={2021}
}


@inproceedings{wu2019errudite,
    title = "{E}rrudite: Scalable, Reproducible, and Testable Error Analysis",
    author = "Wu, Tongshuang  and
      Ribeiro, Marco Tulio  and
      Heer, Jeffrey  and
      Weld, Daniel",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1073",
    doi = "10.18653/v1/P19-1073",
    pages = "747--763",
    abstract = "Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",
}

@inproceedings{levy2022safetext,
    title = "{S}afe{T}ext: A Benchmark for Exploring Physical Safety in Language Models",
    author = "Levy, Sharon  and
      Allaway, Emily  and
      Subbiah, Melanie  and
      Chilton, Lydia  and
      Patton, Desmond  and
      McKeown, Kathleen  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.154",
    pages = "2407--2421",
    abstract = "Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.",
}

@article{liang2022helm,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@inproceedings{perez2022red,
    title = "Red Teaming Language Models with Language Models",
    author = "Perez, Ethan  and
      Huang, Saffron  and
      Song, Francis  and
      Cai, Trevor  and
      Ring, Roman  and
      Aslanides, John  and
      Glaese, Amelia  and
      McAleese, Nat  and
      Irving, Geoffrey",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.225",
    pages = "3419--3448",
    abstract = "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ({``}red teaming{''}) using another LM. We evaluate the target LM{'}s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot{'}s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",
}

@inproceedings{korbak2023pretraining,
  title={Pretraining language models with human preferences},
  author={Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika Vinayak and Buckley, Christopher and Phang, Jason and Bowman, Samuel R and Perez, Ethan},
  booktitle={International Conference on Machine Learning},
  pages={17506--17533},
  year={2023},
  organization={PMLR}
}

@inproceedings{hartvigsen2022toxigen,
    title = "{T}oxi{G}en: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    author = "Hartvigsen, Thomas  and
      Gabriel, Saadia  and
      Palangi, Hamid  and
      Sap, Maarten  and
      Ray, Dipankar  and
      Kamar, Ece",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.234",
    doi = "10.18653/v1/2022.acl-long.234",
    pages = "3309--3326",
    abstract = "Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language.To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5{\%} of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.",
}

@article{touvron2023llama2,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{wei2023jailbroken,
  title={Jailbroken: How Does LLM Safety Training Fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2307.02483},
  year={2023}
}

@inproceedings{gehman2020realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}

@article{perez2022discovering,
  title={Discovering language model behaviors with model-written evaluations},
  author={Perez, Ethan and Ringer, Sam and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and others},
  journal={arXiv preprint arXiv:2212.09251},
  year={2022}
}

@article{lanham2023measuring,
  title={Measuring faithfulness in chain-of-thought reasoning},
  author={Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and others},
  journal={arXiv preprint arXiv:2307.13702},
  year={2023}
}

@article{pacchiardi2023catch,
  title={How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions},
  author={Pacchiardi, Lorenzo and Chan, Alex J and Mindermann, S{\"o}ren and Moscovitz, Ilan and Pan, Alexa Y and Gal, Yarin and Evans, Owain and Brauner, Jan},
  journal={arXiv preprint arXiv:2309.15840},
  year={2023}
}

@inproceedings{welbl2021challenges,
    title = "Challenges in Detoxifying Language Models",
    author = "Welbl, Johannes  and
      Glaese, Amelia  and
      Uesato, Jonathan  and
      Dathathri, Sumanth  and
      Mellor, John  and
      Hendricks, Lisa Anne  and
      Anderson, Kirsty  and
      Kohli, Pushmeet  and
      Coppin, Ben  and
      Huang, Po-Sen",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.210",
    doi = "10.18653/v1/2021.findings-emnlp.210",
    pages = "2447--2469",
    abstract = "Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions{---}highlighting further the nuances involved in careful evaluation of LM toxicity.",
}

@article{zhou2023controlled,
  title={Controlled text generation with natural language instructions},
  author={Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Wilcox, Ethan and Cotterell, Ryan and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2304.14293},
  year={2023}
}

@misc{pozzobon2023goodtriever,
      title={Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models}, 
      author={Luiza Pozzobon and Beyza Ermis and Patrick Lewis and Sara Hooker},
      year={2023},
      eprint={2310.07589},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{lamalfa2023arrt,
  title={The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges},
  author={La Malfa, Emanuele and Petrov, Aleksandar and Frieder, Simon and Weinhuber, Christoph and Burnell, Ryan and Cohn, Anthony G and Shadbolt, Nigel and Wooldridge, Michael},
  journal={arXiv preprint arXiv:2309.16573},
  year={2023}
}


@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@article{hosseini2017deceiving,
  title = {Deceiving {Google}'s {Perspective API} Built for Detecting Toxic Comments},
  author = {Hosseini, Hossein and Kannan, Sreeram and Zhang, Baosen and Poovendran, Radha},
  date = {2017-02-26},
  year = {2017},
  url = {http://arxiv.org/abs/1702.08138},
  urldate = {2020-11-23},
  abstract = {Social media platforms provide an environment where people can freely engage in discussions. Unfortunately, they also enable several problems, such as online harassment. Recently, Google and Jigsaw started a project called Perspective, which uses machine learning to automatically detect toxic language. A demonstration website has been also launched, which allows anyone to type a phrase in the interface and instantaneously see the toxicity score [1]. In this paper, we propose an attack on the Perspective toxic detection system based on the adversarial examples. We show that an adversary can subtly modify a highly toxic phrase in a way that the system assigns significantly lower toxicity score to it. We apply the attack on the sample phrases provided in the Perspective website and show that we can consistently reduce the toxicity scores to the level of the non-toxic phrases. The existence of such adversarial examples is very harmful for toxic detection systems and seriously undermines their usability.},
  archivePrefix = {arXiv},
  eprint = {1702.08138},
  eprinttype = {arxiv},
  journal={arXiv preprint arXiv:1702.08138},
  file = {/Users/Paul/Zotero/storage/V4E65NV2/Hosseini et al. - 2017 - Deceiving Google's Perspective API Built for Detec.pdf;/Users/Paul/Zotero/storage/GG9URQCE/1702.html},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  primaryClass = {cs}
}


@article{salminen2018anatomy,
  title = {Anatomy of Online Hate: Developing a Taxonomy and Machine Learning Models for Identifying and Classifying Hate in Online News Media},
  shorttitle = {Anatomy of {{Online Hate}}},
  author = {Salminen, Joni and Almerekhi, Hind and Milenković, Milica and Jung, Soon-gyo and An, Jisun and Kwak, Haewoon and Jansen, Bernard},
  date = {2018-06-15},
  year = {2018},
  journal = {Proceedings of the 12th International AAAI Conference on Web and Social Media},
  journaltitle = {Proceedings of the 12th International AAAI Conference on Web and Social Media},
  shortjournal = {ICWSM},
  issn = {2334-0770},
  url = {https://ojs.aaai.org/index.php/ICWSM/article/view/15028},
  urldate = {2021-01-22},
  file = {/Users/Paul/Zotero/storage/2AVIGUNT/15028.html;/Users/Paul/Zotero/storage/5WHTN4CB/15028.html},
  issue = {1},
  volume = {1},
  keywords = {machine learning},
  langid = {english},
  number = {1},
  pages = {330--339},
  publisher = {Association for the Advancement of Artificial Intelligence}
}


@inproceedings{waseem2017understanding,
    title = "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
    author = "Talat, Zeerak  and
      Davidson, Thomas  and
      Warmsley, Dana  and
      Weber, Ingmar",
    booktitle = "Proceedings of the First Workshop on Abusive Language Online",
    month = aug,
    year = "2017",
    address = "Vancouver, BC, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-3012",
    doi = "10.18653/v1/W17-3012",
    pages = "78--84",
    abstract = "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.",
}

@incollection{waseem2018bridging,  
  author    = {Talat, Zeerak and Thorne, James and Bingel, Joachim},
  title     = {Bridging the Gaps: Multi-Task Learning for Domain Transfer of Hate Speech Detection},
  year      = {2018},
  editor    = {Golbeck, Jennifer},
  booktitle = {Online Harassment},
  publisher = {Springer}
}

@inproceedings{sohn2019mc,
  title={MC-BERT4HATE: Hate Speech Detection using Multi-channel BERT for Different Languages and Translations},
  author={Sohn, Hajung and Lee, Hyunju},
  booktitle={2019 International Conference on Data Mining Workshops (ICDMW)},
  pages={551--559},
  year={2019},
  organization={IEEE}
}

@article{mendelsohn2020framework,
  title = {A Framework for the Computational Linguistic Analysis of Dehumanization},
  author = {Mendelsohn, Julia and Tsvetkov, Yulia and Jurafsky, Dan},
  date = {2020-08-07},
  year = {2020},
  journaltitle = {Frontiers in Artificial Intelligence},
  shortjournal = {Front. Artif. Intell.},
  journal = {Frontiers in Artificial Intelligence},
  volume = {3},
  pages = {55},
  issn = {2624-8212},
  doi = {10.3389/frai.2020.00055},
  url = {http://arxiv.org/abs/2003.03014},
  urldate = {2020-11-22},
  abstract = {Dehumanization is a pernicious psychological process that often leads to extreme intergroup bias, hate speech, and violence aimed at targeted social groups. Despite these serious consequences and the wealth of available data, dehumanization has not yet been computationally studied on a large scale. Drawing upon social psychology research, we create a computational linguistic framework for analyzing dehumanizing language by identifying linguistic correlates of salient components of dehumanization. We then apply this framework to analyze discussions of LGBTQ people in the New York Times from 1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ people over time. However, we find that the label homosexual has emerged to be much more strongly associated with dehumanizing attitudes than other labels, such as gay. Our proposed techniques highlight processes of linguistic variation and change in discourses surrounding marginalized groups. Furthermore, the ability to analyze dehumanizing language at a large scale has implications for automatically detecting and understanding media bias as well as abusive language online.},
  archivePrefix = {arXiv},
  eprint = {2003.03014},
  eprinttype = {arxiv},
  file = {/Users/Paul/Zotero/storage/KN5QJBFY/Mendelsohn et al. - 2020 - A Framework for the Computational Linguistic Analy.pdf;/Users/Paul/Zotero/storage/FZGN3GDF/2003.html},
  keywords = {Computer Science - Computation and Language,I.2.7,J.4}
}


@inproceedings{zampieri2019predicting,
    title = "Predicting the Type and Target of Offensive Posts in Social Media",
    author = "Zampieri, Marcos  and
      Malmasi, Shervin  and
      Nakov, Preslav  and
      Rosenthal, Sara  and
      Farra, Noura  and
      Kumar, Ritesh",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1144",
    doi = "10.18653/v1/N19-1144",
    pages = "1415--1420",
    abstract = "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.",
}


@inproceedings{warner2012detecting,
    title = "Detecting Hate Speech on the {World Wide Web}",
    author = "Warner, William  and
      Hirschberg, Julia",
    booktitle = "Proceedings of the Second Workshop on Language in Social Media",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W12-2103",
    pages = "19--26",
}

@inproceedings{wolf2020transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@article{sanh2019distilbert,
  title={{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}

@inproceedings{loshchilov2019decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={Proceedings of the 7th International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@inproceedings{tran2020habertor,
    title = "{HABERTOR}: An Efficient and Effective Deep Hatespeech Detector",
    author = "Tran, Thanh  and
      Hu, Yifan  and
      Hu, Changwei  and
      Yen, Kevin  and
      Tan, Fei  and
      Lee, Kyumin  and
      Park, Se Rim",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.606",
    doi = "10.18653/v1/2020.emnlp-main.606",
    pages = "7486--7502",
    abstract = "We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. HABERTOR inherits BERT{'}s architecture, but is different in four aspects: (i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; (ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; (iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our HABERTOR is 4 5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1{\%} of the number of words. Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.",
}


@inproceedings{qian2018leveraging,
    title = "Leveraging Intra-User and Inter-User Representation Learning for Automated Hate Speech Detection",
    author = "Qian, Jing  and
      ElSherief, Mai  and
      Belding, Elizabeth  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-2019",
    doi = "10.18653/v1/N18-2019",
    pages = "118--123",
    abstract = "Hate speech detection is a critical, yet challenging problem in Natural Language Processing (NLP). Despite the existence of numerous studies dedicated to the development of NLP hate speech detection approaches, the accuracy is still poor. The central problem is that social media posts are short and noisy, and most existing hate speech detection solutions take each post as an isolated input instance, which is likely to yield high false positive and negative rates. In this paper, we radically improve automated hate speech detection by presenting a novel model that leverages intra-user and inter-user representation learning for robust hate speech detection on Twitter. In addition to the target Tweet, we collect and analyze the user{'}s historical posts to model intra-user Tweet representations. To suppress the noise in a single Tweet, we also model the similar Tweets posted by all other users with reinforced inter-user representation learning techniques. Experimentally, we show that leveraging these two representations can significantly improve the f-score of a strong bidirectional LSTM baseline model by 10.1{\%}.",
}

@inproceedings{suryawanshi2021findings,
  title={Findings of the shared task on Troll Meme Classification in Tamil},
  author={Suryawanshi, Shardul and Chakravarthi, Bharathi Raja},
  booktitle={Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages},
  pages={126--132},
  year={2021}
}


@article{kiela2020hateful,
  title={The hateful memes challenge: Detecting hate speech in multimodal memes},
  author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2611--2624},
  year={2020}
}


@article{feng2019misleading,
  title={Misleading failures of partial-input baselines},
  author={Feng, Shi and Wallace, Eric and Boyd-Graber, Jordan},
  journal={arXiv preprint arXiv:1905.05778},
  year={2019}
}

@inproceedings{magu2018determining,
  title={Determining code words in euphemistic hate speech using word embedding networks},
  author={Magu, Rijul and Luo, Jiebo},
  booktitle={Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)},
  pages={93--100},
  year={2018}
}

@article{magu2017detecting,
  title={Detecting the hate code on social media},
  author={Magu, Rijul and Joshi, Kshitij and Luo, Jiebo},
  journal={arXiv preprint arXiv:1703.05443},
  year={2017}
}


@online{benesch2016counterspeech,
  title = {Counterspeech on {{Twitter}}: {{A Field Study}}},
  shorttitle = {Counterspeech on {{Twitter}}},
  author = {Benesch, Susan and Ruths, Derek and Dillon, Kelly P and Saleem, Haji Mohammad and Wright, Lucas},
  date = {2016-10-14T20:00:00+00:00},
  year = {2016},
  journaltitle = {Dangerous Speech Project},
  url = {https://dangerousspeech.org/counterspeech-on-twitter-a-field-study/},
  urldate = {2020-11-19},
  abstract = {This report from our two year study of hateful speech and counterspeech on Twitter reviews existing literature on counterspeech, examines cases of counterspeech through the vector in which it was delivered, and develops a taxonomy of counterspeech strategies.},
  file = {/Users/Paul/Zotero/storage/2S6LRNWC/counterspeech-on-twitter-a-field-study.html},
  langid = {english}
}


@inproceedings{gardner2020evaluating,
    title = "Evaluating Models{'} Local Decision Boundaries via Contrast Sets",
    author = "Gardner, Matt  and
      Artzi, Yoav  and
      Basmov, Victoria  and
      Berant, Jonathan  and
      Bogin, Ben  and
      Chen, Sihao  and
      Dasigi, Pradeep  and
      Dua, Dheeru  and
      Elazar, Yanai  and
      Gottumukkala, Ananth  and
      Gupta, Nitish  and
      Hajishirzi, Hannaneh  and
      Ilharco, Gabriel  and
      Khashabi, Daniel  and
      Lin, Kevin  and
      Liu, Jiangming  and
      Liu, Nelson F.  and
      Mulcaire, Phoebe  and
      Ning, Qiang  and
      Singh, Sameer  and
      Smith, Noah A.  and
      Subramanian, Sanjay  and
      Tsarfaty, Reut  and
      Wallace, Eric  and
      Zhang, Ally  and
      Zhou, Ben",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.117",
    doi = "10.18653/v1/2020.findings-emnlp.117",
    pages = "1307--1323",
    abstract = "Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model{'}s decision boundary, which can be used to more accurately evaluate a model{'}s true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets{---}up to 25{\%} in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.",
}

@inproceedings{sap2020social,
    title = "Social Bias Frames: Reasoning about Social and Power Implications of Language",
    author = "Sap, Maarten  and
      Gabriel, Saadia  and
      Qin, Lianhui  and
      Jurafsky, Dan  and
      Smith, Noah A.  and
      Choi, Yejin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.486",
    doi = "10.18653/v1/2020.acl-main.486",
    pages = "5477--5490",
    abstract = "Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people{'}s judgments about others. For example, given a statement that {``}we shouldn{'}t lower our standards to hire more women,{''} most listeners will infer the implicature intended by the speaker - that {``}women (candidates) are less qualified.{''} Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80{\%} F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.",
}



@inproceedings{hu2020xtreme,
  title={Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation},
  author={Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle={International Conference on Machine Learning},
  pages={4411--4421},
  year={2020},
  organization={PMLR}
}


@inproceedings{zhao2021closer,
    title = "A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters",
    author = {Zhao, Mengjie  and
      Zhu, Yi  and
      Shareghi, Ehsan  and
      Vuli{\'c}, Ivan  and
      Reichart, Roi  and
      Korhonen, Anna  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.447",
    doi = "10.18653/v1/2021.acl-long.447",
    pages = "5751--5767",
    abstract = "Few-shot crosslingual transfer has been shown to outperform its zero-shot counterpart with pretrained encoders like multilingual BERT. Despite its growing popularity, little to no attention has been paid to standardizing and analyzing the design of few-shot experiments. In this work, we highlight a fundamental risk posed by this shortcoming, illustrating that the model exhibits a high degree of sensitivity to the selection of few shots. We conduct a large-scale experimental study on 40 sets of sampled few shots for six diverse NLP tasks across up to 40 languages. We provide an analysis of success and failure cases of few-shot transfer, which highlights the role of lexical features. Additionally, we show that a straightforward full model finetuning approach is quite effective for few-shot transfer, outperforming several state-of-the-art few-shot approaches. As a step towards standardizing few-shot crosslingual experimental designs, we make our sampled few shots publicly available.",
}

@inproceedings{lauscher2020zero,
    title = "From Zero to Hero: {O}n the Limitations of Zero-Shot Language Transfer with Multilingual {T}ransformers",
    author = "Lauscher, Anne  and
      Ravishankar, Vinit  and
      Vuli{\'c}, Ivan  and
      Glava{\v{s}}, Goran",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.363",
    doi = "10.18653/v1/2020.emnlp-main.363",
    pages = "4483--4499",
    abstract = "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
}

@book{matsuda1993words,
  title={Words that wound: Critical race theory, assaultive speech, and the first amendment},
  author={Matsuda, Mari J},
  year={1993},
  publisher={Routledge}
}

@article{uae2022hate,
 author  = {UAE},
 date    = {2022-05-18},
 title   = {Anti-Discrimination / Anti-Hatred Law},
 journal = {United Arab Emirates},
 year = {2022},
 url     = {https://u.ae/en/about-the-uae/culture/tolerance/anti-discriminationanti-hatred-law},
 urldate = {2022-06-22}
}

@article{bbc2020ptsd,
 author  = {BBC},
 date    = {2020-03-13},
 title   = {Facebook to pay \$52m to content moderators over PTSD},
 journal = {BBC News},
 year = {2020},
 url     = {https://www.bbc.co.uk/news/technology-52642633},
 urldate = {2022-03-03}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={preprint},
  url= {https://cdn.openai.com/papers/gpt-4.pdf},
  year={2023}
}

@article{he2021debertav3,
  title={Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing},
  author={He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2111.09543},
  year={2021}
}

@article{elliott2020moderators,
 author  = {Elliott, Vittoria and Parmar, Tekendra},
 date    = {2020-03-13},
 title   = {Report: ''The despair and darkness of people will get to you''},
 journal = {Rest of World},
 year = {2020},
 url     = {https://restofworld.org/2020/facebook-international-content-moderators/},
 urldate = {2023-03-16}
}

@article{time2023openaikenya,
 author  = {Billy Perrigo},
 date    = {2023-01-18},
 title   = {Exclusive: OpenAI Used Kenyan Workers on Less Than \$2 Per Hour to Make ChatGPT Less Toxic},
 journal = {Time},
 year = {2023},
 url     = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
 urldate = {2023-03-16}
}

@article{foxglove2021modletter,
 author  = {Foxglove},
 date    = {2021-07-25},
 title   = {Open letter from 60 content moderators to Facebook calling for proper mental health support and an end to outsourcing},
 journal = {Foxglove},
 year = {2021},
 url     = {https://www.foxglove.org.uk/2021/07/25/open-letter-from-60-content-moderators-to-facebook/},
 urldate = {2023-03-16}
}


@article{langran2021football,
 author  = {Langran, Craig},
 date    = {2021-07-18},
 title   = {Why was my tweet about football labelled abusive?},
 journal = {BBC News},
 year = {2021},
 url     = {https://www.bbc.co.uk/news/technology-57836409},
 urldate = {2022-03-08}
}

@article{guynn2019facebook,
 author  = {Guynn, Jessica},
 date    = {2019-04-24},
 title   = {Facebook while black: Users call it getting Zucked, say talking about racism is censored as hate speech},
 journal = {USA Today},
 year = {2019},
 url     = {https://eu.usatoday.com/story/news/2019/04/24/facebook-while-black-zucked-users-say-they-get-blocked-racism-discussion/2859593002/},
 urldate = {2022-03-08}
}

@article{yin2021towards,
  title={Towards generalisable hate speech detection: a review on obstacles and solutions},
  author={Yin, Wenjie and Zubiaga, Arkaitz},
  journal={PeerJ Computer Science},
  volume={7},
  pages={e598},
  year={2021},
  publisher={PeerJ Inc.}
}

@inproceedings{chuang2021mitigating,
    title = "Mitigating Biases in Toxic Language Detection through Invariant Rationalization",
    author = "Chuang, Yung-Sung  and
      Gao, Mingye  and
      Luo, Hongyin  and
      Glass, James  and
      Lee, Hung-yi  and
      Chen, Yun-Nung  and
      Li, Shang-Wen",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.12",
    doi = "10.18653/v1/2021.woah-1.12",
    pages = "114--120",
    abstract = "Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse. However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection. The biases make the learned models unfair and can even exacerbate the marginalization of people. Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (InvRat), a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels. We empirically show that our method yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods.",
}

@inproceedings{kwok2013locate,
  title={Locate the Hate: Detecting Tweets against Blacks},
  author={Irene Kwok and Yuzhou Wang},
  booktitle={AAAI},
  year={2013}
}

@article{scott2021facebook,
 author  = {Scott, Mark},
 date    = {2021-10-25},
 title   = {Facebook did little to moderate posts in the world’s most violent countries},
 journal = {Politico},
 year = {2021},
 url     = {https://www.politico.eu/article/facebook-content-moderation-posts-wars-afghanistan-middle-east-arabic/},
 urldate = {2022-03-08}
}

@inproceedings{kirk2022data,
    title = "Is More Data Better? Re-thinking the Importance of Efficiency in Abusive Language Detection with Transformers-Based Active Learning",
    author = "Kirk, Hannah  and
      Vidgen, Bertie  and
      Hale, Scott",
    booktitle = "Proceedings of the Third Workshop on Threat, Aggression and Cyberbullying (TRAC 2022)",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.trac-1.7",
    pages = "52--61",
    abstract = "Annotating abusive language is expensive, logistically complex and creates a risk of psychological harm. However, most machine learning research has prioritized maximizing effectiveness (i.e., F1 or accuracy score) rather than data efficiency (i.e., minimizing the amount of data that is annotated). In this paper, we use simulated experiments over two datasets at varying percentages of abuse to demonstrate that transformers-based active learning is a promising approach to substantially raise efficiency whilst still maintaining high effectiveness, especially when abusive content is a smaller percentage of the dataset. This approach requires a fraction of labeled data to reach performance equivalent to training over the full dataset.",
}

@inproceedings{kirk2022handling,
    title = "Handling and Presenting Harmful Text in {NLP} Research",
    author = "Kirk, Hannah  and
      Birhane, Abeba  and
      Vidgen, Bertie  and
      Derczynski, Leon",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.35",
    pages = "497--510",
    abstract = "Text data can pose a risk of harm. However, the risks are not fully understood, and how to handle, present, and discuss harmful text in a safe way remains an unresolved issue in the NLP community. We provide an analytical framework categorising harms on three axes: (1) the harm type (e.g., misinformation, hate speech or racial stereotypes); (2) whether a harm is sought as a feature of the research design if explicitly studying harmful content (e.g., training a hate speech classifier), versus unsought if harmful content is encountered when working on unrelated problems (e.g., language generation or part-of-speech tagging); and (3) who it affects, from people (mis)represented in the data to those handling the data and those publishing on the data. We provide advice for practitioners, with concrete steps for mitigating harm in research and in publication. To assist implementation we introduce HarmCheck {--} a documentation standard for handling and presenting harmful text in research.",
}


@book{facebook2022report,
  title={Community Standards Enforcement Report Q2 2022},
  author={Meta},
  year={2022},
  publisher={Meta},
  url={https://transparency.fb.com/data/community-standards-enforcement/hate-speech/facebook/#prevalence}
}

@article{muller2021fanning,
  title={Fanning the flames of hate: Social media and hate crime},
  author={M{\"u}ller, Karsten and Schwarz, Carlo},
  journal={Journal of the European Economic Association},
  volume={19},
  number={4},
  pages={2131--2167},
  year={2021},
  publisher={Oxford Academic}
}

@book{waldron2012harm,
  title={The harm in hate speech},
  author={Waldron, Jeremy},
  year={2012},
  publisher={Harvard University Press}
}

@article{williams2020hate,
  title={Hate in the machine: Anti-Black and anti-Muslim social media posts as predictors of offline racially and religiously aggravated crime},
  author={Williams, Matthew L and Burnap, Pete and Javed, Amir and Liu, Han and Ozalp, Sefa},
  journal={The British Journal of Criminology},
  volume={60},
  number={1},
  pages={93--117},
  year={2020},
  publisher={Oxford University Press UK}
}

@article{vidgen2021understanding,
  title={Understanding online hate: VSP Regulation and the broader context},
  author={Vidgen, Bertie and Burden, Emily and Margetts, Helen},
  journal={Turing Institute},
  year={2021}
}

@inproceedings{garg2019counterfactual,
author = {Garg, Sahaj and Perot, Vincent and Limtiaco, Nicole and Taly, Ankur and Chi, Ed H. and Beutel, Alex},
title = {Counterfactual Fairness in Text Classification through Robustness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3317950},
doi = {10.1145/3306618.3317950},
abstract = {In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that "Some people are gay" is toxic while "Some people are straight" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {219–226},
numpages = {8},
keywords = {fairness, robustness, text classification, counterfactual fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{banko2020unified,
  title = {A {{Unified Taxonomy}} of {{Harmful Content}}},
  booktitle = {Proceedings of the {{Fourth Workshop}} on {{Online Abuse}} and {{Harms}}},
  author = {Banko, Michele and MacKeen, Brendon and Ray, Laurie},
  date = {2020-11},
  year = {2020},
  pages = {125--137},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://www.aclweb.org/anthology/2020.alw-1.16},
  urldate = {2020-11-17},
  abstract = {The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent years, the community has not standardized around how various harmful behaviors are defined, creating challenges for reliable moderation, modeling and evaluation. As a step towards attaining shared understanding of how online abuse may be modeled, we synthesize the most common types of abuse described by industry, policy, community and health experts into a unified typology of harmful content, with detailed criteria and exceptions for each type of abuse.},
  eventtitle = {{{ALW}}-{{EMNLP}} 2020}
}

@inproceedings{kurrek2020comprehensive,
    title = "Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage",
    author = "Kurrek, Jana  and
      Saleem, Haji Mohammad  and
      Ruths, Derek",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.alw-1.17",
    doi = "10.18653/v1/2020.alw-1.17",
    pages = "138--149",
    abstract = "Abusive language classifiers have been shown to exhibit bias against women and racial minorities. Since these models are trained on data that is collected using keywords, they tend to exhibit a high sensitivity towards pejoratives. As a result, comments written by victims of abuse are frequently labelled as hateful, even if they discuss or reclaim slurs. Any attempt to address bias in keyword-based corpora requires a better understanding of pejorative language, as well as an equitable representation of targeted users in data collection. We make two main contributions to this end. First, we provide an annotation guide that outlines 4 main categories of online slur usage, which we further divide into a total of 12 sub-categories. Second, we present a publicly available corpus based on our taxonomy, with 39.8k human annotated comments extracted from Reddit. This corpus was annotated by a diverse cohort of coders, with Shannon equitability indices of 0.90, 0.92, and 0.87 across sexuality, ethnicity, and gender. Taken together, our taxonomy and corpus allow researchers to evaluate classifiers on a wider range of speech containing slurs.",
}

@article{markov2022holistic,
  title={A Holistic Approach to Undesired Content Detection in the Real World},
  author={Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Eloundou, Tyna and Lee, Teddy and Adler, Steven and Jiang, Angela and Weng, Lilian},
  journal={arXiv preprint arXiv:2208.03274},
  year={2022}
}

@inproceedings{zhao2020gender,
    title = "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer",
    author = "Zhao, Jieyu  and
      Mukherjee, Subhabrata  and
      Hosseini, Saghar  and
      Chang, Kai-Wei  and
      Hassan Awadallah, Ahmed",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.260",
    doi = "10.18653/v1/2020.acl-main.260",
    pages = "2896--2907",
    abstract = "Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.",
}

@inproceedings{joshi2020state,
    title = "The State and Fate of Linguistic Diversity and Inclusion in the {NLP} World",
    author = "Joshi, Pratik  and
      Santy, Sebastin  and
      Budhiraja, Amar  and
      Bali, Kalika  and
      Choudhury, Monojit",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.560",
    doi = "10.18653/v1/2020.acl-main.560",
    pages = "6282--6293",
    abstract = "Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the {``}language agnostic{''} status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.",
}

@inproceedings{pires2019multilingual,
    title = "How Multilingual is Multilingual {BERT}?",
    author = "Pires, Telmo  and
      Schlinger, Eva  and
      Garrette, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1493",
    doi = "10.18653/v1/P19-1493",
    pages = "4996--5001",
    abstract = "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
}

@inproceedings{wu2019beto,
    title = "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}",
    author = "Wu, Shijie  and
      Dredze, Mark",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1077",
    doi = "10.18653/v1/D19-1077",
    pages = "833--844",
    abstract = "Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.",
}

@inproceedings{zia2022improving,
  title={Improving Zero-Shot Cross-Lingual Hate Speech Detection with Pseudo-Label Fine-Tuning of Transformer Language Models},
  author={Zia, Haris Bin and Castro, Ignacio and Zubiaga, Arkaitz and Tyson, Gareth},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={16},
  pages={1435--1439},
  year={2022}
}

@inproceedings{bigoulaeva2021cross,
    title = "Cross-Lingual Transfer Learning for Hate Speech Detection",
    author = "Bigoulaeva, Irina  and
      Hangya, Viktor  and
      Fraser, Alexander",
    booktitle = "Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion",
    month = apr,
    year = "2021",
    address = "Kyiv",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.ltedi-1.3",
    pages = "15--25",
    abstract = "We address the task of automatic hate speech detection for low-resource languages. Rather than collecting and annotating new hate speech data, we show how to use cross-lingual transfer learning to leverage already existing data from higher-resource languages. Using bilingual word embeddings based classifiers we achieve good performance on the target language by training only on the source dataset. Using our transferred system we bootstrap on unlabeled target language data, improving the performance of standard cross-lingual transfer approaches. We use English as a high resource language and German as the target language for which only a small amount of annotated corpora are available. Our results indicate that cross-lingual transfer learning together with our approach to leverage additional unlabeled data is an effective way of achieving good performance on low-resource target languages without the need for any target-language annotations.",
}

@inproceedings{ahn2020nlpdove,
    title = "{NLPD}ove at {S}em{E}val-2020 Task 12: Improving Offensive Language Detection with Cross-lingual Transfer",
    author = "Ahn, Hwijeen  and
      Sun, Jimin  and
      Park, Chan Young  and
      Seo, Jungyun",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.206",
    doi = "10.18653/v1/2020.semeval-1.206",
    pages = "1576--1586",
    abstract = "This paper describes our approach to the task of identifying offensive languages in a multilingual setting. We investigate two data augmentation strategies: using additional semi-supervised labels with different thresholds and cross-lingual transfer with data selection. Leveraging the semi-supervised dataset resulted in performance improvements compared to the baseline trained solely with the manually-annotated dataset. We propose a new metric, Translation Embedding Distance, to measure the transferability of instances for cross-lingual data selection. We also introduce various preprocessing steps tailored for social media text along with methods to fine-tune the pre-trained multilingual BERT (mBERT) for offensive language identification. Our multilingual systems achieved competitive results in Greek, Danish, and Turkish at OffensEval 2020.",
}

@inproceedings{hung2022multi2woz,
    title = "{M}ulti2{WOZ}: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog",
    author = "Hung, Chia-Chien  and
      Lauscher, Anne  and
      Vuli{\'c}, Ivan  and
      Ponzetto, Simone  and
      Glava{\v{s}}, Goran",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.270",
    doi = "10.18653/v1/2022.naacl-main.270",
    pages = "3687--3703",
    abstract = "Research on (multi-domain) task-oriented dialog (TOD) has predominantly focused on the English language, primarily due to the shortage of robust TOD datasets in other languages, preventing the systematic investigation of cross-lingual transfer for this crucial NLP application area. In this work, we introduce Multi2WOZ, a new multilingual multi-domain TOD dataset, derived from the well-established English dataset MultiWOZ, that spans four typologically diverse languages: Chinese, German, Arabic, and Russian. In contrast to concurrent efforts, Multi2WOZ contains gold-standard dialogs in target languages that are directly comparable with development and test portions of the English dataset, enabling reliable and comparative estimates of cross-lingual transfer performance for TOD. We then introduce a new framework for multilingual conversational specialization of pretrained language models (PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream TOD tasks. Using such conversational PrLMs specialized for concrete target languages, we systematically benchmark a number of zero-shot and few-shot cross-lingual transfer approaches on two standard TOD tasks: Dialog State Tracking and Response Retrieval. Our experiments show that, in most setups, the best performance entails the combination of (i) conversational specialization in the target language and (ii) few-shot transfer for the concrete TOD task. Most importantly, we show that our conversational specialization in the target language allows for an exceptionally sample-efficient few-shot transfer for downstream TOD tasks.",
}

@inproceedings{martin2020camembert,
    title = "{C}amem{BERT}: a Tasty {F}rench Language Model",
    author = "Martin, Louis  and
      Muller, Benjamin  and
      Ortiz Su{\'a}rez, Pedro Javier  and
      Dupont, Yoann  and
      Romary, Laurent  and
      de la Clergerie, {\'E}ric  and
      Seddah, Djam{\'e}  and
      Sagot, Beno{\^\i}t",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.645",
    doi = "10.18653/v1/2020.acl-main.645",
    pages = "7203--7219",
    abstract = "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models {--}in all languages except English{--} very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",
}

@inproceedings{armengol2021multilingual,
    title = "Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? {A} Comprehensive Assessment for {C}atalan",
    author = "Armengol-Estap{\'e}, Jordi  and
      Carrino, Casimiro Pio  and
      Rodriguez-Penagos, Carlos  and
      de Gibert Bonet, Ona  and
      Armentano-Oller, Carme  and
      Gonzalez-Agirre, Aitor  and
      Melero, Maite  and
      Villegas, Marta",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.437",
    doi = "10.18653/v1/2021.findings-acl.437",
    pages = "4933--4946",
}

@inproceedings{phang2020english,
    title = "{E}nglish Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too",
    author = "Phang, Jason  and
      Calixto, Iacer  and
      Htut, Phu Mon  and
      Pruksachatkun, Yada  and
      Liu, Haokun  and
      Vania, Clara  and
      Kann, Katharina  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-main.56",
    pages = "557--575",
    abstract = "Intermediate-task training{---}fine-tuning a pretrained model on an intermediate task before fine-tuning again on the target task{---}often improves model performance substantially on language understanding tasks in monolingual English settings. We investigate whether English intermediate-task training is still helpful on non-English target tasks. Using nine intermediate language-understanding tasks, we evaluate intermediate-task transfer in a zero-shot cross-lingual setting on the XTREME benchmark. We see large improvements from intermediate training on the BUCC and Tatoeba sentence retrieval tasks and moderate improvements on question-answering target tasks. MNLI, SQuAD and HellaSwag achieve the best overall results as intermediate tasks, while multi-task intermediate offers small additional improvements. Using our best intermediate-task models for each target task, we obtain a 5.4 point improvement over XLM-R Large on the XTREME benchmark, setting the state of the art as of June 2020. We also investigate continuing multilingual MLM during intermediate-task training and using machine-translated intermediate-task data, but neither consistently outperforms simply performing English intermediate-task training.",
}

@article{nozza2020what,
  title={What the {[MASK]?} Making Sense of Language-Specific {BERT} Models},
  author={Nozza, Debora and Bianchi, Federico and Hovy, Dirk},
  journal={arXiv preprint arXiv:2003.02912},
  year={2020}
}

@inproceedings{lin2019choosing,
    title = "Choosing Transfer Languages for Cross-Lingual Learning",
    author = "Lin, Yu-Hsiang  and
      Chen, Chian-Yu  and
      Lee, Jean  and
      Li, Zirui  and
      Zhang, Yuyan  and
      Xia, Mengzhou  and
      Rijhwani, Shruti  and
      He, Junxian  and
      Zhang, Zhisong  and
      Ma, Xuezhe  and
      Anastasopoulos, Antonios  and
      Littell, Patrick  and
      Neubig, Graham",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1301",
    doi = "10.18653/v1/P19-1301",
    pages = "3125--3135",
    abstract = "Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method.",
}

@article{artetxe2019massively,
    title = "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond",
    author = "Artetxe, Mikel  and
      Schwenk, Holger",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1038",
    doi = "10.1162/tacl_a_00288",
    pages = "597--610",
    abstract = "We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/LASER.",
}

@inproceedings{ruder2021xtremer,
    title = "{XTREME}-{R}: Towards More Challenging and Nuanced Multilingual Evaluation",
    author = "Ruder, Sebastian  and
      Constant, Noah  and
      Botha, Jan  and
      Siddhant, Aditya  and
      Firat, Orhan  and
      Fu, Jinlan  and
      Liu, Pengfei  and
      Hu, Junjie  and
      Garrette, Dan  and
      Neubig, Graham  and
      Johnson, Melvin",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.802",
    doi = "10.18653/v1/2021.emnlp-main.802",
    pages = "10215--10245",
    abstract = "Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.",
}

@inproceedings{liang2020xglue,
    title = "{XGLUE}: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation",
    author = "Liang, Yaobo  and
      Duan, Nan  and
      Gong, Yeyun  and
      Wu, Ning  and
      Guo, Fenfei  and
      Qi, Weizhen  and
      Gong, Ming  and
      Shou, Linjun  and
      Jiang, Daxin  and
      Cao, Guihong  and
      Fan, Xiaodong  and
      Zhang, Ruofei  and
      Agrawal, Rahul  and
      Cui, Edward  and
      Wei, Sining  and
      Bharti, Taroon  and
      Qiao, Ying  and
      Chen, Jiun-Hung  and
      Wu, Winnie  and
      Liu, Shuguang  and
      Yang, Fan  and
      Campos, Daniel  and
      Majumder, Rangan  and
      Zhou, Ming",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.484",
    doi = "10.18653/v1/2020.emnlp-main.484",
    pages = "6008--6018",
    abstract = "In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.",
}

@inproceedings{hedderich2020transfer,
    title = "Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on {A}frican Languages",
    author = "Hedderich, Michael A.  and
      Adelani, David  and
      Zhu, Dawei  and
      Alabi, Jesujoba  and
      Markus, Udia  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.204",
    doi = "10.18653/v1/2020.emnlp-main.204",
    pages = "2580--2591",
    abstract = "Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.",
}


@article{chung2019conan,
  title={CONAN--COunter NArratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech},
  author={Chung, Yi-Ling and Kuzmenko, Elizaveta and Tekiroglu, Serra Sinem and Guerini, Marco},
  journal={arXiv preprint arXiv:1910.03270},
  year={2019}
}

@inproceedings{rottger2022mhc,
    title = "Multilingual {H}ate{C}heck: Functional Tests for Multilingual Hate Speech Detection Models",
    author = {R{\"o}ttger, Paul  and
      Seelawi, Haitham  and
      Nozza, Debora  and
      Talat, Zeerak  and
      Vidgen, Bertie},
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.15",
    doi = "10.18653/v1/2022.woah-1.15",
    pages = "154--169",
    abstract = "Hate speech detection models are typically evaluated on held-out test sets. However, this risks painting an incomplete and potentially misleading picture of model performance because of increasingly well-documented systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, recent research has thus introduced functional tests for hate speech detection models. However, these tests currently only exist for English-language content, which means that they cannot support the development of more effective models in other languages spoken by billions across the world. To help address this issue, we introduce Multilingual HateCheck (MHC), a suite of functional tests for multilingual hate speech detection models. MHC covers 34 functionalities across ten languages, which is more languages than any other hate speech dataset. To illustrate MHC{'}s utility, we train and test a high-performing multilingual hate speech detection model, and reveal critical model weaknesses for monolingual and cross-lingual applications.",
}


@article{corbin1990grounded,
  title={Grounded theory research: Procedures, canons, and evaluative criteria},
  author={Corbin, Juliet M and Strauss, Anselm},
  journal={Qualitative Sociology},
  volume={13},
  number={1},
  pages={3--21},
  year={1990},
  publisher={Springer}
}


@online{card2020little,
  title = {With {{Little Power Comes Great Responsibility}}},
  author = {Card, Dallas and Henderson, Peter and Khandelwal, Urvashi and Jia, Robin and Mahowald, Kyle and Jurafsky, Dan},
  date = {2020-10-13},
  url = {http://arxiv.org/abs/2010.06595},
  urldate = {2020-11-13},
  abstract = {Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75\% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.},
  archivePrefix = {arXiv},
  eprint = {2010.06595},
  eprinttype = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}


@inproceedings{waseem2016hateful,
    title = "Hateful Symbols or Hateful People? {P}redictive Features for Hate Speech Detection on {T}witter",
    author = "Talat, Zeerak  and
      Hovy, Dirk",
    booktitle = "Proceedings of the {NAACL} Student Research Workshop",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N16-2013",
    doi = "10.18653/v1/N16-2013",
    pages = "88--93",
}

@inproceedings{djuric2015hate,
	Author = {Djuric, Nemanja and Zhou, Jing and Morris, Robin and Grbovic, Mihajlo and Radosavljevic, Vladan and Bhamidipati, Narayan},
	Booktitle = {24th international WWW conference},
	Date-Added = {2020-02-01 11:42:46 +0000},
	Date-Modified = {2020-02-01 12:39:50 +0000},
	Pages = {29--30},
	Title = {Hate speech detection with comment embeddings},
	Year = {2015}}
    

@article{blank2017digital,
  title={The digital divide among Twitter users and its implications for social research},
  author={Blank, Grant},
  journal={Social Science Computer Review},
  volume={35},
  number={6},
  pages={679--697},
  year={2017},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{arango2019hate,
  title={Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation},
  author={Arango, Aym{\'e} and P{\'e}rez, Jorge and Poblete, Barbara},
  booktitle={Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={45--54},
  year={2019},
  publisher = {Association for Computing Machinery}
}

@inproceedings{hamilton2016diachronic,
    title = "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change",
    author = "Hamilton, William L.  and
      Leskovec, Jure  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1141",
    doi = "10.18653/v1/P16-1141",
    pages = "1489--1501",
}

@inproceedings{hamilton2016cultural,
    title = "Cultural Shift or Linguistic Drift? {C}omparing Two Computational Measures of Semantic Change",
    author = "Hamilton, William L.  and
      Leskovec, Jure  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1229",
    doi = "10.18653/v1/D16-1229",
    pages = "2116--2121",
}

@inproceedings{golbeck2017large,
  title={A large labeled corpus for online harassment research},
  author={Golbeck, Jennifer and Ashktorab, Zahra and Banjo, Rashad O and Berlinger, Alexandra and Bhagwan, Siddharth and Buntain, Cody and Cheakalos, Paul and Geller, Alicia A and Gnanasekaran, Rajesh Kumar and Gunasekaran, Raja Rajan and others},
  booktitle={Proceedings of the 9th ACM Conference on Web Science},
  pages={229--233},
  year={2017},
  publisher = {Association for Computing Machinery}
}

@article{ross2017measuring,
  title={Measuring the reliability of hate speech annotations: The case of the european refugee crisis},
  author={Ross, Bj{\"o}rn and Rist, Michael and Carbonell, Guillermo and Cabrera, Benjamin and Kurowsky, Nils and Wojatzki, Michael},
  journal={arXiv preprint arXiv:1701.08118},
  year={2017}
}

@article{ptaszynski2019results,
  title={Results of the PolEval 2019 Shared Task 6: First Dataset and Open Shared Task for Automatic Cyberbullying Detection in Polish Twitter},
  author={Ptaszynski, Michal and Pieciukiewicz, Agata and Dyba{\l}a, Pawe{\l}},
  journal={Proceedings of the PolEval 2019 Workshop},
  pages={89},
  year={2019}
}

@inproceedings{schmidt2017survey,
    title = "A Survey on Hate Speech Detection using Natural Language Processing",
    author = "Schmidt, Anna  and
      Wiegand, Michael",
    booktitle = "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-1101",
    doi = "10.18653/v1/W17-1101",
    pages = "1--10",
    abstract = "This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches.",
}

@article{fortuna2018survey,
  title={A survey on automatic detection of hate speech in text},
  author={Fortuna, Paula and Nunes, S{\'e}rgio},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={4},
  pages={1--30},
  year={2018},
  publisher = {Association for Computing Machinery}
}

@inproceedings{sanguinetti2018italian,
  title={An {I}talian {T}witter corpus of hate speech against immigrants},
  author={Sanguinetti, Manuela and Poletto, Fabio and Bosco, Cristina and Patti, Viviana and Stranisci, Marco},
  booktitle={Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

@inproceedings{liu2018forecasting,
  title={Forecasting the presence and intensity of hostility on Instagram using linguistic and social features},
  author={Liu, Ping and Guberman, Joshua and Hemphill, Libby and Culotta, Aron},
  journal = {Proceedings of the 12th International AAAI Conference on Web and Social Media},
  year={2018},
  publisher = {Association for the Advancement of Artificial Intelligence}
}

@article{mellon2017twitter,
  title={Twitter and Facebook are not representative of the general population: Political attitudes and demographics of British social media users},
  author={Mellon, Jonathan and Prosser, Christopher},
  journal={Research \& Politics},
  volume={4},
  number={3},
  pages={2053168017720008},
  year={2017},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{he2009learning,
  title={Learning from imbalanced data},
  author={He, Haibo and Garcia, Edwardo A},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={21},
  number={9},
  pages={1263--1284},
  year={2009},
  publisher={IEEE}
}

@inproceedings{founta2019unified,
  title={A unified deep learning architecture for abuse detection},
  author={Founta, Antigoni Maria and Chatzakou, Despoina and Kourtellis, Nicolas and Blackburn, Jeremy and Vakali, Athena and Leontiadis, Ilias},
  booktitle={Proceedings of the 10th ACM Conference on Web Science},
  pages={105--114},
  year={2019}
}

@article{founta2018large, 
title={Large Scale Crowdsourcing and Characterization of {T}witter Abusive Behavior}, 
volume={12}, 
url={https://ojs.aaai.org/index.php/ICWSM/article/view/14991}, 
number={1}, 
journal={Proceedings of the International AAAI Conference on Web and Social Media}, 
author={Founta, Antigoni and Djouvas, Constantinos and Chatzakou, Despoina and Leontiadis, Ilias and Blackburn, Jeremy and Stringhini, Gianluca and Vakali, Athena and Sirivianos, Michael and Kourtellis, Nicolas}, 
year={2018}, 
month={Jun.} 
}

@inproceedings{cao2020deephate,
  title={{DeepHate}: Hate speech detection via multi-faceted text representations},
  author={Cao, Rui and Lee, Roy Ka-Wei and Hoang, Tuan-Anh},
  booktitle={Proceedings of the 12th ACM Conference on Web Science},
  pages={11--20},
  year={2020}
}

@article{burnap2015cyber,
  title={Cyber hate speech on {T}witter: An application of machine classification and statistical modeling for policy and decision making},
  author={Burnap, Pete and Williams, Matthew L},
  journal={Policy \& Internet},
  volume={7},
  number={2},
  pages={223--242},
  year={2015},
  publisher={Wiley Online Library}
}

@article{barbera2015understanding,
  title={Understanding the political representativeness of Twitter users},
  author={Barber{\'a}, Pablo and Rivero, Gonzalo},
  journal={Social Science Computer Review},
  volume={33},
  number={6},
  pages={712--729},
  year={2015},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{hargittai2018potential,
  title={Potential biases in big data: Omitted voices on social media},
  author={Hargittai, Eszter},
  journal={Social Science Computer Review},
  pages={0894439318788322},
  year={2018},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{blank2017representativeness,
  title={Representativeness of social media in Great Britain: investigating Facebook, Linkedin, Twitter, Pinterest, Google+, and Instagram},
  author={Blank, Grant and Lutz, Christoph},
  journal={American Behavioral Scientist},
  volume={61},
  number={7},
  pages={741--756},
  year={2017},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@book{newman2019reuters,
  title={Reuters Institute Digital News Report 2019},
  author={Newman, Nic and Fletcher, Richard and Kalogeropoulos, Antonis and Nielsen, Rasmus},
  year={2019},
  publisher={Reuters Institute for the Study of Journalism}
}


@article{hargittai2015bigger,
  title={Is bigger always better? Potential biases of big data derived from social network sites},
  author={Hargittai, Eszter},
  journal={The Annals of the American Academy of Political and Social Science},
  volume={659},
  number={1},
  pages={63--76},
  year={2015},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@misc{mcintosh1988white,
  title={White privilege: Unpacking the invisible knapsack},
  author={McIntosh, Peggy},
  year={1988},
  publisher={ERIC}
}

@inproceedings{cho2011friendship,
  title={Friendship and mobility: user movement in location-based social networks},
  author={Cho, Eunjoon and Myers, Seth A and Leskovec, Jure},
  booktitle={Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1082--1090},
  year={2011},
  publisher = {Association for Computing Machinery}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{barbera2015birds,
  title={Birds of the same feather tweet together: Bayesian ideal point estimation using Twitter data},
  author={Barber{\'a}, Pablo},
  journal={Political analysis},
  volume={23},
  number={1},
  pages={76--91},
  year={2015},
  publisher={Cambridge University Press}
}

@article{stewart1964jacobellis,
  title={Jacobellis v ohio},
  author={Stewart, Potter},
  journal={US Rep},
  volume={378},
  pages={184},
  year={1964}
}

@inproceedings{davidson2019racialbias,
    title = "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    author = "Davidson, Thomas  and
      Bhattacharya, Debasmita  and
      Weber, Ingmar",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3504",
    doi = "10.18653/v1/W19-3504",
    pages = "25--35",
    abstract = "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.",
}

@inproceedings{waseem2016you,
    title = "Are You a Racist or Am {I} Seeing Things? {A}nnotator Influence on Hate Speech Detection on {T}witter",
    author = "Talat, Zeerak",
    booktitle = "Proceedings of the First Workshop on {NLP} and Computational Social Science",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W16-5618",
    doi = "10.18653/v1/W16-5618",
    pages = "138--142",
}


@article{vidgen2020directions,
  title = {Directions in Abusive Language Training Data, a Systematic Review: {{Garbage}} in, Garbage Out},
  shorttitle = {Directions in Abusive Language Training Data, a Systematic Review},
  author = {Vidgen, Bertie and Derczynski, Leon},
  date = {2020-12-28},
  year = {2020},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  journal = {PLOS ONE},
  volume = {15},
  pages = {e0243300},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0243300},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0243300},
  urldate = {2020-12-30},
  abstract = {Data-driven and machine learning based approaches for detecting, categorising and measuring abusive content such as hate speech and harassment have gained traction due to their scalability, robustness and increasingly high performance. Making effective detection systems for abusive content relies on having the right training datasets, reflecting a widely accepted mantra in computer science: Garbage In, Garbage Out. However, creating training datasets which are large, varied, theoretically-informed and that minimize biases is difficult, laborious and requires deep expertise. This paper systematically reviews 63 publicly available training datasets which have been created to train abusive language classifiers. It also reports on creation of a dedicated website for cataloguing abusive language data hatespeechdata.com. We discuss the challenges and opportunities of open science in this field, and argue that although more dataset sharing would bring many benefits it also poses social and ethical risks which need careful consideration. Finally, we provide evidence-based recommendations for practitioners creating new abusive content training datasets.},
  archivePrefix = {arXiv},
  eprint = {2004.01670},
  eprinttype = {arxiv},
  file = {/Users/Paul/Zotero/storage/GVN3BZMT/Vidgen and Derczynski - 2020 - Directions in abusive language training data, a sy.pdf;/Users/Paul/Zotero/storage/RVWVQB5J/Vidgen and Derczynski - 2020 - Directions in abusive language training data, a sy.pdf;/Users/Paul/Zotero/storage/W2CKDA3K/Vidgen and Derczynski - 2020 - Directions in Abusive Language Training Data Garb.pdf;/Users/Paul/Zotero/storage/XN782NJE/Vidgen and Derczynski - 2020 - Directions in Abusive Language Training Data Garb.pdf;/Users/Paul/Zotero/storage/DPTK8ZHC/2004.html;/Users/Paul/Zotero/storage/MBL76BF9/2004.html;/Users/Paul/Zotero/storage/W7VYLTZL/article.html;/Users/Paul/Zotero/storage/Z92T5RVI/article.html},
  keywords = {Computer Science - Computation and Language,Internet,Language,Machine learning,Metadata,Social discrimination,Speech,Taxonomy,Twitter},
  langid = {english},
  number = {12}
}




@inproceedings{van2018challenges,
    title = "Challenges for Toxic Comment Classification: An In-Depth Error Analysis",
    author = {van Aken, Betty  and
      Risch, Julian  and
      Krestel, Ralf  and
      L{\"o}ser, Alexander},
    booktitle = "Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2)",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5105",
    doi = "10.18653/v1/W18-5105",
    pages = "33--42",
    abstract = "Toxic comment classification has become an active research field with many recently proposed approaches. However, while these approaches address some of the task{'}s challenges others still remain unsolved and directions for further research are needed. To this end, we compare different deep learning and shallow approaches on a new, large comment dataset and propose an ensemble that outperforms all individual models. Further, we validate our findings on a second dataset. The results of the ensemble enable us to perform an extensive error analysis, which reveals open challenges for state-of-the-art methods and directions towards pending future research. These challenges include missing paradigmatic context and inconsistent dataset labels.",
}

@article{mchugh2012interrater,
  title={Interrater reliability: the kappa statistic},
  author={McHugh, Mary L},
  journal={Biochemia medica: Biochemia medica},
  volume={22},
  number={3},
  pages={276--282},
  year={2012},
  publisher={Medicinska naklada}
}

@inproceedings{d2020bert,
  title={{BERT} and {fastText} Embeddings for Automatic Detection of Toxic Speech},
  author={d'Sa, Ashwin Geet and Illina, Irina and Fohr, Dominique},
  booktitle={SIIE 2020-Information Systems and Economic Intelligence},
  year={2020}
}

@article{deangelis2009unmasking,
  title={Unmasking racial micro aggressions},
  author={DeAngelis, Tori},
  journal={Monitor on Psychology},
  volume={40},
  number={2},
  pages={42},
  year={2009}
}

@article{malmasi2018challenges,
  title={Challenges in discriminating profanity from hate speech},
  author={Malmasi, Shervin and Zampieri, Marcos},
  journal={Journal of Experimental \& Theoretical Artificial Intelligence},
  volume={30},
  number={2},
  pages={187--202},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{wiegand2019detection,
    title = "{D}etection of Abusive Language: The Problem of Biased Datasets",
    author = "Wiegand, Michael  and
      Ruppenhofer, Josef  and
      Kleinbauer, Thomas",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1060",
    doi = "10.18653/v1/N19-1060",
    pages = "602--608",
    abstract = "We discuss the impact of data bias on abusive language detection. We show that classification scores on popular datasets reported in previous work are much lower under realistic settings in which this bias is reduced. Such biases are most notably observed on datasets that are created by focused sampling instead of random sampling. Datasets with a higher proportion of implicit abuse are more affected than datasets with a lower proportion.",
}

@inproceedings{mozafari2019bert,
  title={A {BERT}-based transfer learning approach for hate speech detection in online social media},
  author={Mozafari, Marzieh and Farahbakhsh, Reza and Crespi, Noel},
  booktitle={International Conference on Complex Networks and Their Applications},
  pages={928--940},
  year={2019},
  organization={Springer}}
  
@article{mozafari2020hate,
  title={Hate speech detection and racial bias mitigation in social media based on {BERT} model},
  author={Mozafari, Marzieh and Farahbakhsh, Reza and Crespi, No{\"e}l},
  journal={PloS one},
  volume={15},
  number={8},
  pages={e0237861},
  year={2020},
  publisher={Public Library of Science}
}
  
  @article{amaya2020total,
  title={Total Error in a Big Data World: Adapting the TSE Framework to Big Data},
  author={Amaya, Ashley and Biemer, Paul P and Kinyon, David},
  journal={Journal of Survey Statistics and Methodology},
  volume={8},
  number={1},
  pages={89--119},
  year={2020},
  publisher={Oxford University Press}}

@inproceedings{zampieri2019semeval,
    title = "{S}em{E}val-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media ({O}ffens{E}val)",
    author = "Zampieri, Marcos  and
      Malmasi, Shervin  and
      Nakov, Preslav  and
      Rosenthal, Sara  and
      Farra, Noura  and
      Kumar, Ritesh",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S19-2010",
    doi = "10.18653/v1/S19-2010",
    pages = "75--86",
    abstract = "We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.",
}

@inproceedings{
su2023selective,
title={Selective Annotation Makes Language Models Better Few-Shot Learners},
author={Hongjin Su and Jungo Kasai and Chen Henry Wu and Weijia Shi and Tianlu Wang and Jiayi Xin and Rui Zhang and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Tao Yu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=qY1hlv7gwg}
}

@inproceedings{zhang2018detecting,
  title={Detecting hate speech on {T}witter using a convolution-{GRU} based deep neural network},
  author={Zhang, Ziqi and Robinson, David and Tepper, Jonathan},
  booktitle={European Semantic Web conference},
  pages={745--760},
  year={2018},
  organization={Springer}
}

@article{pitsilis2018detecting,
  title={Detecting offensive language in tweets using deep learning},
  author={Pitsilis, Georgios K and Ramampiaro, Heri and Langseth, Helge},
  journal={arXiv preprint arXiv:1801.04433},
  year={2018}
}

@article{pitsilis2018effective,
  title={Effective hate-speech detection in Twitter data using recurrent neural networks},
  author={Pitsilis, Georgios K and Ramampiaro, Heri and Langseth, Helge},
  journal={Applied Intelligence},
  volume={48},
  number={12},
  pages={4730--4742},
  year={2018},
  publisher={Springer}
}

@inproceedings{vidgen2019challenges,
    title = "Challenges and frontiers in abusive content detection",
    author = "Vidgen, Bertie  and
      Harris, Alex  and
      Nguyen, Dong  and
      Tromble, Rebekah  and
      Hale, Scott  and
      Margetts, Helen",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-3509",
    doi = "10.18653/v1/W19-3509",
    pages = "80--93",
    abstract = "Online abusive content detection is an inherently difficult task. It has received considerable attention from academia, particularly within the computational linguistics community, and performance appears to have improved as the field has matured. However, considerable challenges and unaddressed frontiers remain, spanning technical, social and ethical dimensions. These issues constrain the performance, efficiency and generalizability of abusive content detection systems. In this article we delineate and clarify the main challenges and frontiers in the field, critically evaluate their implications and discuss potential solutions. We also highlight ways in which social scientific insights can advance research. We discuss the lack of support given to researchers working with abusive content and provide guidelines for ethical research.",
}
	
@article{mishra2020tackling,
  title={Tackling online abuse: A survey of automated abuse detection methods},
  author={Mishra, Pushkar and Yannakoudakis, Helen and Shutova, Ekaterina},
  journal={arXiv preprint arXiv:1908.06024},
  year={2020}
}

@inproceedings{park2017one,
    title = "One-step and Two-step Classification for Abusive Language Detection on {T}witter",
    author = "Park, Ji Ho  and
      Fung, Pascale",
    booktitle = "Proceedings of the First Workshop on Abusive Language Online",
    month = aug,
    year = "2017",
    address = "Vancouver, BC, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3006",
    doi = "10.18653/v1/W17-3006",
    pages = "41--45",
    abstract = "Automatic abusive language detection is a difficult but important task for online social media. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 F-measure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps.",
}
    
 @article{malmasi2017detecting,
  title={Detecting hate speech in social media},
  author={Malmasi, Shervin and Zampieri, Marcos},
  journal={arXiv preprint arXiv:1712.06427},
  year={2017}
}


@article{hallgren2012computing,
  title = {Computing Inter-Rater Reliability for Observational Data: An Overview and Tutorial},
  shorttitle = {Computing {{Inter}}-{{Rater Reliability}} for {{Observational Data}}},
  author = {Hallgren, Kevin A.},
  date = {2012},
  year = {2012},
  journaltitle = {Tutorials in Quantitative Methods for Psychology},
  shortjournal = {Tutor Quant Methods Psychol},
  journal = {Tutorials in Quantitative Methods for Psychology},
  volume = {8},
  pages = {23--34},
  issn = {1913-4126},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402032/},
  urldate = {2020-12-16},
  abstract = {Many research designs require the assessment of inter-rater reliability (IRR) to demonstrate consistency among observational ratings provided by multiple coders. However, many studies use incorrect statistical procedures, fail to fully report the information necessary to interpret their results, or do not address how IRR affects the power of their subsequent analyses for hypothesis testing. This paper provides an overview of methodological issues related to the assessment of IRR with a focus on study design, selection of appropriate statistics, and the computation, interpretation, and reporting of some commonly-used IRR statistics. Computational examples include SPSS and R syntax for computing Cohen’s kappa and intra-class correlations to assess IRR.},
  eprint = {22833776},
  eprinttype = {pmid},
  file = {/Users/Paul/Zotero/storage/XVFB4IND/Hallgren - 2012 - Computing Inter-Rater Reliability for Observationa.pdf},
  number = {1},
  pmcid = {PMC3402032}
}




@article{mitchell2019model,
  title = {Model {{Cards}} for {{Model Reporting}}},
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  date = {2019},
  year = {2019},
  journaltitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* '19},
  pages = {220--229},
  doi = {10.1145/3287560.3287596},
  url = {http://arxiv.org/abs/1810.03993},
  urldate = {2020-12-01},
  abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
  archivePrefix = {arXiv},
  eprint = {1810.03993},
  eprinttype = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}


@article{landis1977measurement,
  title = {The Measurement of Observer Agreement for Categorical Data},
  author = {Landis, J. Richard and Koch, Gary G.},
  date = {1977-03},
  year = {1977},
  journaltitle = {Biometrics},
  shortjournal = {Biometrics},
  journal = {Biometrics},
  volume = {33},
  pages = {159},
  issn = {0006341X},
  doi = {10.2307/2529310},
  eprint = {2529310},
  eprinttype = {jstor},
  file = {/Users/Paul/Zotero/storage/SFMJL9JT/Landis and Koch - 1977 - The Measurement of Observer Agreement for Categori.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{xiao2020timme,
  title={TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding},
  author={Xiao, Zhiping and Song, Weiping and Xu, Haoyan and Ren, Zhicheng and Sun, Yizhou},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2258--2268},
  year={2020}
}

@inproceedings{iyyer2014political,
    title = "Political Ideology Detection Using Recursive Neural Networks",
    author = "Iyyer, Mohit  and
      Enns, Peter  and
      Boyd-Graber, Jordan  and
      Resnik, Philip",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P14-1105",
    doi = "10.3115/v1/P14-1105",
    pages = "1113--1122",
}

@inproceedings{del2017semantic,
    title = "Semantic Variation in Online Communities of Practice",
    author = "Del Tredici, Marco  and
      Fern{\'a}ndez, Raquel",
    booktitle = "{IWCS} 2017 - 12th International Conference on Computational Semantics - Long papers",
    year = "2017",
    url = "https://www.aclweb.org/anthology/W17-6804",
}

@inproceedings{kannangara2018mining,
  title={Mining {Twitter} for fine-grained political opinion polarity classification, ideology detection and sarcasm detection},
  author={Kannangara, Sandeepa},
  booktitle={Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
  pages={751--752},
  year={2018}
}

@inproceedings{conover2011predicting,
  author={M. D. {Conover} and B. {Goncalves} and J. {Ratkiewicz} and A. {Flammini} and F. {Menczer}},
  booktitle={2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social Computing}, 
  title={Predicting the Political Alignment of {T}witter Users}, 
  year={2011},
  volume={},
  number={},
  pages={192-199},
  doi={10.1109/PASSAT/SocialCom.2011.34}}





@report{cameron2016attitudes,
  title = {Attitudes to Potentially Offensive Language and Gestures on {{TV}} and Radio: {{Quick}} Reference Guide},
  author = {Cameron, Daniel and Stevenson, Neil},
  year = {2016},
  pages = {15},
  institution = {{Ofcom}},
  file = {/Users/Paul/Zotero/storage/QPVH6PI3/Stevenson - Attitudes to potentially offensive language and ge.pdf},
  langid = {english}
}






@inproceedings{del2017hate,
  title={Hate me, hate me not: Hate speech detection on {F}acebook},
  author={Del Vigna, Fabio and Cimino, Andrea and Dell'Orletta, Felice and Petrocchi, Marinella and Tesconi, Maurizio},
  booktitle={Proceedings of the First Italian Conference on Cybersecurity (ITASEC17)},
  pages={86--95},
  year={2017}
}


@article{degibert2018hate,
  title = {Hate {{Speech Dataset}} from a {{White Supremacy Forum}}},
  author = {de Gibert, Ona and Perez, Naiara and Garcia-Pablos, Aitor and Cuadros, Montse},
  date = {2018-09-12},
  year = {2018},
  url = {http://arxiv.org/abs/1809.04444},
  urldate = {2020-11-25},
  eprint = {1809.04444},
  eprinttype = {arxiv},
  journal = {arXiv},
  file = {/Users/Paul/Zotero/storage/QNPAVKXE/de Gibert et al. - 2018 - Hate Speech Dataset from a White Supremacy Forum.pdf;/Users/Paul/Zotero/storage/2LVJC5ZB/1809.html},
  keywords = {Computer Science - Computation and Language},
  options = {useprefix=true},
  primaryClass = {cs}
}



@inproceedings{badjatiya2019stereotypical,
  title = {Stereotypical {{Bias Removal}} for {{Hate Speech Detection Task}} Using {{Knowledge}}-Based {{Generalizations}}},
  booktitle = {The {{World Wide Web Conference}} on   - {{WWW}} '19},
  author = {Badjatiya, Pinkesh and Gupta, Manish and Varma, Vasudeva},
  date = {2019},
  pages = {49--59},
  publisher = {{ACM Press}},
  location = {{San Francisco, CA, USA}},
  doi = {10.1145/3308558.3313504},
  url = {http://dl.acm.org/citation.cfm?doid=3308558.3313504},
  urldate = {2020-11-25},
  abstract = {With the ever-increasing cases of hate spread on social media platforms, it is critical to design abuse detection mechanisms to proactively avoid and control such incidents. While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. Bias removal has been traditionally studied for structured datasets, but we aim at bias mitigation from unstructured text data.},
  eventtitle = {The {{World Wide Web Conference}}},
  file = {/Users/Paul/Zotero/storage/XLH7RREE/Badjatiya et al. - 2019 - Stereotypical Bias Removal for Hate Speech Detecti.pdf},
  isbn = {978-1-4503-6674-8},
  langid = {english}
}

@inproceedings{badjatiya2017deep,
  title={Deep learning for hate speech detection in tweets},
  author={Badjatiya, Pinkesh and Gupta, Shashank and Gupta, Manish and Varma, Vasudeva},
  booktitle={Proceedings of the 26th International Conference on World Wide Web Companion},
  pages={759--760},
  year={2017}
}

@inproceedings{wulczyn2017ex,
  title={Ex machina: Personal attacks seen at scale},
  author={Wulczyn, Ellery and Thain, Nithum and Dixon, Lucas},
  booktitle={Proceedings of the 26th International Conference on World Wide Web},
  pages={1391--1399},
  year={2017}
}

@article{garten2019measuring,
  title={Measuring the importance of context when modeling language comprehension},
  author={Garten, Justin and Kennedy, Brendan and Sagae, Kenji and Dehghani, Morteza},
  journal={Behavior research methods},
  volume={51},
  number={2},
  pages={480--492},
  year={2019},
  publisher={Springer}
}


@article{kennedy2018gab,
  title = {The {{Gab Hate Corpus}}: {{A}} Collection of 27k Posts Annotated for Hate Speech},
  shorttitle = {The {{Gab Hate Corpus}}},
  author = {Kennedy, Brendan and Atari, Mohammad and Davani, Aida Mostafazadeh and Yeh, Leigh and Omrani, Ali and Kim, Yehsong and Coombs, Kris and Havaldar, Shreya and Portillo-Wightman, Gwenyth and Gonzalez, Elaine and Hoover, Joseph and Azatian, Aida and Hussain, Alyzeh and Lara, Austin and Olmos, Gabriel and Omary, Adam and Park, Christina and Wijaya, Clarisa and Wang, Xin and Zhang, Yong and Dehghani, Morteza},
  date = {2018-07-18T18:29:22},
  year={2018},
  publisher = {{PsyArXiv}},
  journal = {PsyArXiv},
  doi = {10.31234/osf.io/hqjxn},
  url = {https://psyarxiv.com/hqjxn/},
  urldate = {2021-04-08},
  abstract = {The growing prominence of online hate speech is a threat to a safe and just society. This endangering phenomenon requires collaboration across the sciences in order to generate evidence-based knowledge of, and policies for, the dissemination of hatred in online spaces. To foster such collaborations, here we present the Gab Hate Corpus (GHC), consisting of 27,665 posts from the social network service gab.ai, each annotated by a minimum of three trained annotators. Annotators were trained to label posts according to a coding typology derived from a synthesis of hate speech definitions across legal, computational, psychological, and sociological research. We detail the development of the corpus, describe the resulting distributions of hate-based rhetoric, target group, and rhetorical framing labels, and establish baseline classification performance for each using standard natural language processing methods. The GHC, which is the largest theoretically-justified, annotated corpus of hate speech to date, provides opportunities for training and evaluating hate speech classifiers and for scientific inquiries into the linguistic and network components of hate speech.},
  file = {/Users/Paul/Zotero/storage/K6TBHE28/Kennedy et al. - 2018 - The Gab Hate Corpus A collection of 27k posts ann.pdf},
  keywords = {Annotation Guide,Computational Linguistics,Hate Crime,Hate Speech,Linguistics,Natural Language Processing,Politics,Prejudice and Discrimination,Social and Behavioral Sciences,Social and Personality Psychology,Text Analysis}
}


@inproceedings{qian2019benchmark,
  ids = {qian2019benchmarka},
  title = {A {{Benchmark Dataset}} for {{Learning}} to {{Intervene}} in {{Online Hate Speech}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Qian, Jing and Bethke, Anna and Liu, Yinyin and Belding, Elizabeth and Wang, William Yang},
  date = {2019-11},
  year={2019},
  pages = {4755--4764},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1482},
  url = {https://www.aclweb.org/anthology/D19-1482},
  urldate = {2021-02-22},
  abstract = {Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets collected from Gab and Reddit. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.},
  eventtitle = {{{EMNLP}}-{{IJCNLP}} 2019},
  file = {/Users/Paul/Zotero/storage/SP6A4FHJ/Qian et al. - 2019 - A Benchmark Dataset for Learning to Intervene in O.pdf;/Users/Paul/Zotero/storage/X6527T7K/Qian et al. - 2019 - A Benchmark Dataset for Learning to Intervene in O.pdf}
}




@inproceedings{gamback2017using,
  title={Using convolutional neural networks to classify hate-speech},
  author={Gamb{\"a}ck, Bj{\"o}rn and Sikdar, Utpal Kumar},
  booktitle={Proceedings of the first workshop on abusive language online},
  pages={85--90},
  year={2017}
}

@inproceedings{kennedy2020contextualizing,
    title = "Contextualizing Hate Speech Classifiers with Post-hoc Explanation",
    author = "Kennedy, Brendan  and
      Jin, Xisen  and
      Mostafazadeh Davani, Aida  and
      Dehghani, Morteza  and
      Ren, Xiang",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.483",
    doi = "10.18653/v1/2020.acl-main.483",
    pages = "5435--5442",
    abstract = "Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like {``}gay{''} or {``}black{''} are used in offensive or prejudiced ways. Such biases manifest in false positives when these identifiers are present, due to models{'} inability to learn the contexts which constitute a hateful usage of identifiers. We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms. Then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves. Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance.",
}
    
 @inproceedings{olteanu2017limits,
  title={The limits of abstract evaluation metrics: The case of hate speech detection},
  author={Olteanu, Alexandra and Talamadupula, Kartik and Varshney, Kush R},
  booktitle={Proceedings of the 9th ACM Conference on Web Science},
  pages={405--406},
  year={2017}
}

@inproceedings{zhou2020curse,
    title = "The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions",
    author = "Zhou, Xiang  and
      Nie, Yixin  and
      Tan, Hao  and
      Bansal, Mohit",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.659",
    doi = "10.18653/v1/2020.emnlp-main.659",
    pages = "8215--8228",
    abstract = "We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability of the conclusions drawn based on these analysis sets? (2) Where does this instability come from? (3) How should we handle this instability and what are some potential solutions? For the first question, we conduct a thorough empirical study over analysis sets and find that in addition to the unstable final performance, the instability exists all along the training curve. We also observe lower-than-expected correlations between the analysis validation set and standard validation set, questioning the effectiveness of the current model-selection routine. Next, to answer the second question, we give both theoretical explanations and empirical evidence regarding the source of the instability, demonstrating that the instability mainly comes from high inter-example correlations within analysis sets. Finally, for the third question, we discuss an initial attempt to mitigate the instability and suggest guidelines for future work such as reporting the decomposed variance for more interpretable results and fair comparison across models.",
}

@inproceedings{vidgen2020detecting,
    title = "Detecting {E}ast {A}sian Prejudice on Social Media",
    author = "Vidgen, Bertie  and
      Hale, Scott  and
      Guest, Ella  and
      Margetts, Helen  and
      Broniatowski, David  and
      Talat, Zeerak  and
      Botelho, Austin  and
      Hall, Matthew  and
      Tromble, Rebekah",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.alw-1.19",
    doi = "10.18653/v1/2020.alw-1.19",
    pages = "162--172",
    abstract = "During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.",
}


@inproceedings{dixon2018measuring,
  title = {Measuring and Mitigating Unintended Bias in Text Classification},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  date = {2018-12-27},
  year = {2018},
  pages = {67--73},
  publisher = {Association for Computing Machinery},
  location = {{New Orleans, USA}},
  doi = {10.1145/3278721.3278729},
  url = {https://dl.acm.org/doi/10.1145/3278721.3278729},
  urldate = {2020-11-21},
  abstract = {We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.},
  eventtitle = {{{AIES}} '18: {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  file = {/Users/Paul/Zotero/storage/PNRJH27D/Dixon et al. - 2018 - Measuring and Mitigating Unintended Bias in Text C.pdf},
  isbn = {978-1-4503-6012-8},
  langid = {english}
}

@inproceedings{kiela2021dynabench,
    title = "Dynabench: Rethinking Benchmarking in {NLP}",
    author = "Kiela, Douwe  and
      Bartolo, Max  and
      Nie, Yixin  and
      Kaushik, Divyansh  and
      Geiger, Atticus  and
      Wu, Zhengxuan  and
      Vidgen, Bertie  and
      Prasad, Grusha  and
      Singh, Amanpreet  and
      Ringshia, Pratik  and
      Ma, Zhiyi  and
      Thrush, Tristan  and
      Riedel, Sebastian  and
      Waseem, Zeerak  and
      Stenetorp, Pontus  and
      Jia, Robin  and
      Bansal, Mohit  and
      Potts, Christopher  and
      Williams, Adina",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.324",
    doi = "10.18653/v1/2021.naacl-main.324",
    pages = "4110--4124",
    abstract = "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
}

@inproceedings{vidgen2021learning,
    title = "Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection",
    author = "Vidgen, Bertie  and
      Thrush, Tristan  and
      Talat, Zeerak  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.132",
    doi = "10.18653/v1/2021.acl-long.132",
    pages = "1667--1682",
    abstract = "We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54{\%} of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also have better performance on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use.",
}

@inproceedings{cercas2021convabuse,
    title = "{C}onv{A}buse: Data, Analysis, and Benchmarks for Nuanced Detection in Conversational {AI}",
    author = "Cercas Curry, Amanda  and
      Abercrombie, Gavin  and
      Rieser, Verena",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.587",
    pages = "7388--7403",
    abstract = "We present the first English corpus study on abusive language towards three conversational AI systems gathered {`}in the wild{'}: an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more {`}nuanced{'} approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, we report results from bench-marking existing models against this data. Unsurprisingly, we find that there is substantial room for improvement with F1 scores below 90{\%}.",
}

@inproceedings{raji2020accountability,
author = {Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N. and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
title = {Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372873},
doi = {10.1145/3351095.3372873},
abstract = {Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {33–44},
numpages = {12},
keywords = {algorithmic audits, responsible innovation, accountability, machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{caselli2020feeloffended,
    title = "{I} Feel Offended, Don{'}t Be Abusive! Implicit/Explicit Messages in Offensive and Abusive Language",
    author = "Caselli, Tommaso  and
      Basile, Valerio  and
      Mitrovi{\'c}, Jelena  and
      Kartoziya, Inga  and
      Granitzer, Michael",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.760",
    pages = "6193--6202",
    abstract = "Abusive language detection is an unsolved and challenging problem for the NLP community. Recent literature suggests various approaches to distinguish between different language phenomena (e.g., hate speech vs. cyberbullying vs. offensive language) and factors (degree of explicitness and target) that may help to classify different abusive language phenomena. There are data sets that annotate the target of abusive messages (i.e.OLID/OffensEval (Zampieri et al., 2019a)). However, there is a lack of data sets that take into account the degree of explicitness. In this paper, we propose annotation guidelines to distinguish between explicit and implicit abuse in English and apply them to OLID/OffensEval. The outcome is a newly created resource, AbuseEval v1.0, which aims to address some of the existing issues in the annotation of offensive and abusive language (e.g., explicitness of the message, presence of a target, need of context, and interaction across different phenomena).",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{uma2020case,
  title={A case for soft loss functions},
  author={Uma, Alexandra and Fornaciari, Tommaso and Hovy, Dirk and Paun, Silviu and Plank, Barbara and Poesio, Massimo},
  journal={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={8},
  number={1},
  pages={173--177},
  year={2020}
}

@article{aroyo2015truth,
  title={Truth is a lie: Crowd truth and the seven myths of human annotation},
  author={Aroyo, Lora and Welty, Chris},
  journal={AI Magazine},
  volume={36},
  number={1},
  pages={15--24},
  year={2015}
}

@inproceedings{plank2014disagreement,
    title = "Linguistically debatable or just plain wrong?",
    author = "Plank, Barbara  and
      Hovy, Dirk  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-2083",
    doi = "10.3115/v1/P14-2083",
    pages = "507--511",
}

@inproceedings{zampieri2020semeval,
    title = {{SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (OffensEval 2020)}},
    author = {Zampieri, Marcos and Nakov, Preslav and Rosenthal, Sara and Atanasova, Pepa and Karadzhov, Georgi and Mubarak, Hamdy and Derczynski, Leon and Pitenis, Zeses and \c{C}\"{o}ltekin, \c{C}a\u{g}r{\i}},
    booktitle = {Proceedings of SemEval},
    year = {2020}
}

@inproceedings{uma2021semevaldisagree,
    title = "{S}em{E}val-2021 Task 12: Learning with Disagreements",
    author = "Uma, Alexandra  and
      Fornaciari, Tommaso  and
      Dumitrache, Anca  and
      Miller, Tristan  and
      Chamberlain, Jon  and
      Plank, Barbara  and
      Simpson, Edwin  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.semeval-1.41",
    doi = "10.18653/v1/2021.semeval-1.41",
    pages = "338--347",
    abstract = "Disagreement between coders is ubiquitous in virtually all datasets annotated with human judgements in both natural language processing and computer vision. However, most supervised machine learning methods assume that a single preferred interpretation exists for each item, which is at best an idealization. The aim of the SemEval-2021 shared task on learning with disagreements (Le-Wi-Di) was to provide a unified testing framework for methods for learning from data containing multiple and possibly contradictory annotations covering the best-known datasets containing information about disagreements for interpreting language and classifying images. In this paper we describe the shared task and its results.",
}

@article{zhang2019hate,
  title={Hate speech detection: A solved problem? {T}he challenging case of long tail on {T}witter},
  author={Zhang, Ziqi and Luo, Lei},
  journal={Semantic Web},
  volume={10},
  number={5},
  pages={925--945},
  year={2019},
  publisher={IOS Press}
}

@article{agic2016multilingual,
	Author = {Agi{\'c}, {\v{Z}}eljko and Johannsen, Anders and Plank, Barbara and Alonso, H{\'e}ctor Mart{\'\i}nez and Schluter, Natalie and S{\o}gaard, Anders},
	Date-Added = {2020-02-28 15:30:45 +0000},
	Date-Modified = {2020-02-28 15:30:45 +0000},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {301--312},
	Publisher = {MIT Press},
	Title = {Multilingual projection for parsing truly low-resource languages},
	Volume = {4},
	Year = {2016}}

@inproceedings{vaswani2017attention,
	Author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2020-02-28 13:15:47 +0000},
	Date-Modified = {2020-02-28 13:15:47 +0000},
	Pages = {5998--6008},
	Title = {Attention is all you need},
	Year = {2017}}

@article{bender2018data,
    title = "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    author = "Bender, Emily M.  and
      Friedman, Batya",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    url = "https://www.aclweb.org/anthology/Q18-1041",
    doi = "10.1162/tacl_a_00041",
    pages = "587--604",
    abstract = "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
}

@inproceedings{liu2019inoculation,
    title = "Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets",
    author = "Liu, Nelson F.  and
      Schwartz, Roy  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1225",
    doi = "10.18653/v1/N19-1225",
    pages = "2171--2179",
    abstract = "Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks. While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model{'}s specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI {``}stress tests{''} (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.",
}

@inproceedings{vidgen2020recalibrating,
    title = "Recalibrating classifiers for interpretable abusive content detection",
    author = "Vidgen, Bertie  and
      Hale, Scott  and
      Staton, Sam  and
      Melham, Tom  and
      Margetts, Helen  and
      Kammar, Ohad  and
      Szymczak, Marcin",
    booktitle = "Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.nlpcss-1.14",
    doi = "10.18653/v1/2020.nlpcss-1.14",
    pages = "132--138",
    abstract = "We investigate the use of machine learning classifiers for detecting online abuse in empirical research. We show that uncalibrated classifiers (i.e. where the {`}raw{'} scores are used) align poorly with human evaluations. This limits their use for understanding the dynamics, patterns and prevalence of online abuse. We examine two widely used classifiers (created by Perspective and Davidson et al.) on a dataset of tweets directed against candidates in the UK{'}s 2017 general election. A Bayesian approach is presented to recalibrate the raw scores from the classifiers, using probabilistic programming and newly annotated data. We argue that interpretability evaluation and recalibration is integral to the application of abusive content classifiers.",
}

@inproceedings{qian2018hierarchical,
    title = "Hierarchical {CVAE} for Fine-Grained Hate Speech Classification",
    author = "Qian, Jing  and
      ElSherief, Mai  and
      Belding, Elizabeth  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1391",
    doi = "10.18653/v1/D18-1391",
    pages = "3550--3559",
    abstract = "Existing work on automated hate speech detection typically focuses on binary classification or on differentiating among a small set of categories. In this paper, we propose a novel method on a fine-grained hate speech classification task, which focuses on differentiating among 40 hate groups of 13 different hate group categories. We first explore the Conditional Variational Autoencoder (CVAE) as a discriminative model and then extend it to a hierarchical architecture to utilize the additional hate category information for more accurate prediction. Experimentally, we show that incorporating the hate category information for training can significantly improve the classification performance and our proposed model outperforms commonly-used discriminative models.",
}


@inproceedings{mikolov2013distributed,
	Author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2020-02-28 13:13:09 +0000},
	Date-Modified = {2020-02-28 13:13:09 +0000},
	Pages = {3111--3119},
	Title = {Distributed representations of words and phrases and their compositionality},
	Year = {2013}}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@book{asher2011lexical,
	Author = {Asher, Nicholas},
	Date-Added = {2020-02-28 12:58:13 +0000},
	Date-Modified = {2020-02-28 12:58:13 +0000},
	Publisher = {Cambridge University Press},
	Title = {Lexical meaning in context: A web of words},
	Year = {2011}}
  
@inproceedings{naik2018stress,
    title = "Stress Test Evaluation for Natural Language Inference",
    author = "Naik, Aakanksha  and
      Ravichander, Abhilasha  and
      Sadeh, Norman  and
      Rose, Carolyn  and
      Neubig, Graham",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1198",
    pages = "2340--2353",
    abstract = "Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed {``}stress tests{''} that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area.",
}

@online{wang2019glue,
  title = {{{GLUE}}: {{A Multi}}-{{Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  date = {2019-02-22},
  url = {http://arxiv.org/abs/1804.07461},
  urldate = {2020-11-01},
  abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
  archivePrefix = {arXiv},
  eprint = {1804.07461},
  eprinttype = {arxiv},
  file = {/Users/Paul/Zotero/storage/B6TDPRS5/Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf;/Users/Paul/Zotero/storage/WHU6U5SB/1804.html},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}


@online{iyyer2018adversarial,
  title = {Adversarial {{Example Generation}} with {{Syntactically Controlled Paraphrase Networks}}},
  author = {Iyyer, Mohit and Wieting, John and Gimpel, Kevin and Zettlemoyer, Luke},
  date = {2018-04-17},
  url = {http://arxiv.org/abs/1804.06059},
  urldate = {2020-11-01},
  abstract = {We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) "fool" pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.},
  archivePrefix = {arXiv},
  eprint = {1804.06059},
  eprinttype = {arxiv},
  file = {/Users/Paul/Zotero/storage/9MPWCV6Y/Iyyer et al. - 2018 - Adversarial Example Generation with Syntactically .pdf;/Users/Paul/Zotero/storage/AZ52DU5D/1804.html},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{ethayarajh2020utility,
  title={Utility is in the Eye of the User: A Critique of NLP Leaderboards},
  author={Ethayarajh, Kawin and Jurafsky, Dan},
  journal={arXiv preprint arXiv:2009.13888},
  year={2020}
}

@inproceedings{mahler2017breaking,
  title = {Breaking {{NLP}}: {{Using Morphosyntax}}, {{Semantics}}, {{Pragmatics}} and {{World Knowledge}} to {{Fool Sentiment Analysis Systems}}},
  shorttitle = {Breaking {{NLP}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Building Linguistically}}           {{Generalizable NLP Systems}}},
  author = {Mahler, Taylor and Cheung, Willy and Elsner, Micha and King, David and de Marneffe, Marie-Catherine and Shain, Cory and Stevens-Guille, Symon and White, Michael},
  date = {2017},
  pages = {33--39},
  publisher = {{Association for Computational Linguistics}},
  location = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/W17-5405},
  url = {http://aclweb.org/anthology/W17-5405},
  urldate = {2020-10-22},
  abstract = {This paper describes our “breaker” submission to the 2017 EMNLP “Build It Break It” shared task on sentiment analysis. In order to cause the “builder” systems to make incorrect predictions, we edited items in the blind test data according to linguistically interpretable strategies that allow us to assess the ease with which the builder systems learn various components of linguistic structure. On the whole, our submitted pairs break all systems at a high rate (72.6\%), indicating that sentiment analysis as an NLP task may still have a lot of ground to cover. Of the breaker strategies that we consider, we find our semantic and pragmatic manipulations to pose the most substantial difficulties for the builder systems.},
  eventtitle = {Proceedings of the {{First Workshop}} on {{Building Linguistically}}           {{Generalizable NLP Systems}}},
  file = {/Users/Paul/Zotero/storage/WZ6CRNVH/Mahler et al. - 2017 - Breaking NLP Using Morphosyntax, Semantics, Pragm.pdf},
  langid = {english},
  options = {useprefix=true}
}

@incollection{wang2019superglue,
  title = {{{SuperGLUE}}: {{A Stickier Benchmark}} for {{General}}-{{Purpose Language Understanding Systems}}},
  shorttitle = {{{SuperGLUE}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d\textbackslash textquotesingle Alché-Buc, F. and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {3266--3280},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf},
  urldate = {2020-11-01},
  file = {/Users/Paul/Zotero/storage/68577S5R/Wang et al. - 2019 - SuperGLUE A Stickier Benchmark for General-Purpos.pdf;/Users/Paul/Zotero/storage/U2H79TIM/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.html}
}

@article{ahmed2013managerial,
	Author = {Ahmed, Anwer S and Duellman, Scott},
	Date-Added = {2020-02-28 12:32:55 +0000},
	Date-Modified = {2020-02-28 12:32:55 +0000},
	Journal = {Journal of Accounting Research},
	Number = {1},
	Pages = {1--30},
	Publisher = {Wiley Online Library},
	Title = {Managerial overconfidence and accounting conservatism},
	Volume = {51},
	Year = {2013}}

@article{phillips1966conservatism,
	Author = {Phillips, Lawrence D and Edwards, Ward},
	Date-Added = {2020-02-28 12:20:17 +0000},
	Date-Modified = {2020-02-28 12:20:17 +0000},
	Journal = {Journal of experimental psychology},
	Number = {3},
	Pages = {346},
	Publisher = {American Psychological Association},
	Title = {Conservatism in a simple probability inference task.},
	Volume = {72},
	Year = {1966}}
    
@inproceedings{sap2019risk,
    title = "The Risk of Racial Bias in Hate Speech Detection",
    author = "Sap, Maarten  and
      Card, Dallas  and
      Gabriel, Saadia  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1163",
    doi = "10.18653/v1/P19-1163",
    pages = "1668--1678",
    abstract = "We investigate how annotators{'} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet{'}s dialect they are significantly less likely to label the tweet as offensive.",
}

@article{huff2015these,
  title={“Who are these people?” Evaluating the demographic characteristics and political preferences of MTurk survey respondents},
  author={Huff, Connor and Tingley, Dustin},
  journal={Research \& Politics},
  volume={2},
  number={3},
  pages={2053168015604648},
  year={2015},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{wang2021dcn,
  title={DCN V2: Improved deep \& cross network and practical lessons for web-scale learning to rank systems},
  author={Wang, Ruoxi and Shivanna, Rakesh and Cheng, Derek and Jain, Sagar and Lin, Dong and Hong, Lichan and Chi, Ed},
  booktitle={Proceedings of the Web Conference 2021},
  pages={1785--1797},
  year={2021}
}

@article{loepp2020distinction,
  title={Distinction without a difference? An assessment of MTurk Worker types},
  author={Loepp, Eric and Kelly, Jarrod T},
  journal={Research \& politics},
  volume={7},
  number={1},
  pages={2053168019901185},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{burnham2018mturk,
  title={Who is Mturk? Personal characteristics and sample consistency of these online workers},
  author={Burnham, Martin J and Le, Yen K and Piedmont, Ralph L},
  journal={Mental Health, Religion \& Culture},
  volume={21},
  number={9-10},
  pages={934--944},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{gordon2022jury,
  title={Jury Learning: Integrating Dissenting Voices into Machine Learning Models},
  author={Gordon, Mitchell L and Lam, Michelle S and Park, Joon Sung and Patel, Kayur and Hancock, Jeffrey T and Hashimoto, Tatsunori and Bernstein, Michael S},
  journal={arXiv preprint arXiv:2202.02950},
  year={2022}
}

@inproceedings{kumar2021designing,
  title={Designing Toxic Content Classification for a Diversity of Perspectives},
  author={Kumar, Deepak and Kelley, Patrick Gage and Consolvo, Sunny and Mason, Joshua and Bursztein, Elie and Durumeric, Zakir and Thomas, Kurt and Bailey, Michael},
  booktitle={Seventeenth Symposium on Usable Privacy and Security (SOUPS 2021)},
  pages={299--318},
  year={2021}
}

@article{kivlichan2021measuring,
  title={Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation},
  author={Kivlichan, Ian D and Lin, Zi and Liu, Jeremiah and Vasserman, Lucy},
  journal={arXiv preprint arXiv:2107.04212},
  year={2021}
}

@article{ettinger2020bert,
    title = "What {BERT} Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
    author = "Ettinger, Allyson",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.3",
    doi = "10.1162/tacl_a_00298",
    pages = "34--48",
    abstract = "Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction{---} and, in particular, it shows clear insensitivity to the contextual impacts of negation.",
    publisher = {Association for Computational Linguistics}
}

@article{ettinger2017towards,
  title={Towards linguistically generalizable NLP systems: A workshop and shared task},
  author={Ettinger, Allyson and Rao, Sudha and Daum{\'e} III, Hal and Bender, Emily M},
  journal={arXiv preprint arXiv:1711.01505},
  year={2017}
}



@article{palmer2020cold,
  title={{COLD}: Annotation scheme and evaluation data set for complex offensive language in {English}},
  author={Palmer, Alexis and Carr, Christine and Robinson, Melissa and Sanders, Jordan},
  journal={Journal for Language Technology and Computational Linguistics},
  year={2020},
  pages = {1--28},
  issue = {1}
}
	
@inproceedings{ribeiro2018semantically,
  title={Semantically equivalent adversarial rules for debugging nlp models},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={856--865},
  year={2018}
}

@inproceedings{ribeiro2020beyond,
    title = "Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist",
    author = "Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.442",
    doi = "10.18653/v1/2020.acl-main.442",
    pages = "4902--4912",
    abstract = "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",
}

@inproceedings{sennrich2017grammatical,
    title = "How Grammatical is Character-level Neural Machine Translation? {A}ssessing {MT} Quality with Contrastive Translation Pairs",
    author = "Sennrich, Rico",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2060",
    pages = "376--382",
    abstract = "Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present LingEval97, a large-scale data set of 97000 contrastive translation pairs based on the WMT English-{\textgreater}German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.",
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1151",
    doi = "10.18653/v1/D18-1151",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@article{akhtar2021opinions,
  title={Whose opinions matter? perspective-aware models to identify opinions of hate speech victims in abusive language detection},
  author={Akhtar, Sohail and Basile, Valerio and Patti, Viviana},
  journal={arXiv preprint arXiv:2106.15896},
  year={2021}
}

@inproceedings{geva2019modeling,
    title = "Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets",
    author = "Geva, Mor  and
      Goldberg, Yoav  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1107",
    doi = "10.18653/v1/D19-1107",
    pages = "1161--1166",
    abstract = "Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.",
}

@inproceedings{akhtar2020modeling,
  title={Modeling annotator perspective and polarized opinions to improve hate speech detection},
  author={Akhtar, Sohail and Basile, Valerio and Patti, Viviana},
  booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={8},
  pages={151--154},
  year={2020}
}

@inproceedings{akhtar2019polarisation,
  title={A New Measure of Polarization in the Annotation of Hate Speech},
  author={Sohail Akhtar and Valerio Basile and Viviana Patti},
  booktitle={AI*IA},
  year={2019}
}

@article{northcutt2021confident,
  title={Confident learning: Estimating uncertainty in dataset labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}

@article{zhang2017improving,
  title={Improving crowdsourced label quality using noise correction},
  author={Zhang, Jing and Sheng, Victor S and Li, Tao and Wu, Xindong},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={5},
  pages={1675--1688},
  year={2017},
  publisher={IEEE}
}

@inproceedings{hovy2013learning,
    title = "Learning Whom to Trust with {MACE}",
    author = "Hovy, Dirk  and
      Berg-Kirkpatrick, Taylor  and
      Vaswani, Ashish  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N13-1132",
    pages = "1120--1130",
}

@inproceedings{rottger2021temporal,
    title = "Temporal Adaptation of {BERT} and Performance on Downstream Document Classification: Insights from Social Media",
    author = {R{\"o}ttger, Paul  and
      Pierrehumbert, Janet},
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.206",
    pages = "2400--2412",
    abstract = "Language use differs between domains and even within a domain, language use changes over time. For pre-trained language models like BERT, domain adaptation through continued pre-training has been shown to improve performance on in-domain downstream tasks. In this article, we investigate whether temporal adaptation can bring additional benefits. For this purpose, we introduce a corpus of social media comments sampled over three years. It contains unlabelled data for adaptation and evaluation on an upstream masked language modelling task as well as labelled data for fine-tuning and evaluation on a downstream document classification task. We find that temporality matters for both tasks: temporal adaptation improves upstream and temporal fine-tuning downstream task performance. Time-specific models generally perform better on past than on future test sets, which matches evidence on the bursty usage of topical words. However, adapting BERT to time and domain does not improve performance on the downstream task over only adapting to domain. Token-level analysis shows that temporal adaptation captures event-driven changes in language use in the downstream task, but not those changes that are actually relevant to task performance. Based on our findings, we discuss when temporal adaptation may be more effective.",
}

@inproceedings{rottger2021hatecheck,
    title = "{H}ate{C}heck: Functional Tests for Hate Speech Detection Models",
    author = {R{\"o}ttger, Paul  and
      Vidgen, Bertie  and
      Nguyen, Dong  and
      Talat, Zeerak  and
      Margetts, Helen  and
      Pierrehumbert, Janet},
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.4",
    doi = "10.18653/v1/2021.acl-long.4",
    pages = "41--58",
    abstract = "Detecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HateCheck, a suite of functional tests for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HateCheck{'}s utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.",
}

@inproceedings{arora2020harassment,
    title = "A Novel Methodology for Developing Automatic Harassment Classifiers for {T}witter",
    author = "Arora, Ishaan  and
      Guo, Julia  and
      Levitan, Sarah Ita  and
      McGregor, Susan  and
      Hirschberg, Julia",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.alw-1.2",
    doi = "10.18653/v1/2020.alw-1.2",
    pages = "7--15",
    abstract = "Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annotators {--} not the targets of the abuse themselves. While this method of data collection supports fast development of machine learning classifiers, the models built on them often fail in the context of real-world harassment and abuse, which contain nuances less easily identified by non-targets. Here, we present a mixed-methods approach to create classifiers for abuse and harassment which leverages direct engagement with the target group in order to achieve high quality and ecological validity of data sets and labels, and to generate deeper insights into the key tactics of bad actors. We use women journalists{'} experience on Twitter as an initial community of focus. We identify several structural mechanisms of abuse that we believe will generalize to other target communities.",
}

@misc{waseem2021disembodied,
      title={Disembodied Machine Learning: On the Illusion of Objectivity in NLP}, 
      author={Zeerak Talat and Smarika Lulz and Joachim Bingel and Isabelle Augenstein},
      year={2021},
      eprint={2101.11974},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@book{alm2008affect,
  title={Affect in text and speech},
  author={Alm, Ebba Cecilia Ovesdotter},
  year={2008},
  publisher={University of Illinois at Urbana-Champaign}
}

@inbook{diaz2018addressing,
author = {Diaz, Mark and Johnson, Isaac and Lazar, Amanda and Piper, Anne Marie and Gergle, Darren},
title = {Addressing Age-Related Bias in Sentiment Analysis},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173986},
abstract = {Computational approaches to text analysis are useful in understanding aspects of online interaction, such as opinions and subjectivity in text. Yet, recent studies have identified various forms of bias in language-based models, raising concerns about the risk of propagating social biases against certain groups based on sociodemographic factors (e.g., gender, race, geography). In this study, we contribute a systematic examination of the application of language models to study discourse on aging. We analyze the treatment of age-related terms across 15 sentiment analysis models and 10 widely-used GloVe word embeddings and attempt to alleviate bias through a method of processing model training data. Our results demonstrate that significant age bias is encoded in the outputs of many sentiment analysis algorithms and word embeddings. We discuss the models' characteristics in relation to output bias and how these models might be best incorporated into research.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14}
}

@inproceedings{luo2020detecting,
    title = "Detecting Stance in Media On Global Warming",
    author = "Luo, Yiwei  and
      Card, Dallas  and
      Jurafsky, Dan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.296",
    doi = "10.18653/v1/2020.findings-emnlp.296",
    pages = "3296--3315",
    abstract = "Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, {``}Leading scientists agree that global warming is a serious concern,{''} framing a clause which affirms their own stance ({``}that global warming is serious{''}) as an opinion endorsed (''[scientists] agree{''}) by a reputable source ({``}leading{''}). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: {``}Mistaken scientists claim [...].'' Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce DeSMOG, a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other{'}s opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author{'}s own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.",
}

@article{pavlick2019inherent,
    title = "Inherent Disagreements in Human Textual Inferences",
    author = "Pavlick, Ellie  and
      Kwiatkowski, Tom",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    month = mar,
    year = "2019",
    url = "https://aclanthology.org/Q19-1043",
    doi = "10.1162/tacl_a_00293",
    pages = "677--694",
    abstract = "We analyze human{'}s disagreements about the validity of natural language inferences. We show that, very often, disagreements are not dismissible as annotation {``}noise{''}, but rather persist as we collect more ratings and as we vary the amount of context provided to raters. We further show that the type of uncertainty captured by current state-of-the-art models for natural language inference is not reflective of the type of uncertainty present in human disagreements. We discuss implications of our results in relation to the recognizing textual entailment (RTE)/natural language inference (NLI) task. We argue for a refined evaluation objective that requires models to explicitly capture the full distribution of plausible human judgments.",
}

@inproceedings{mccoy2019right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, Tom  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
    abstract = "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",
}

@article{warstadt2020blimp,
    title = "{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish",
    author = "Warstadt, Alex  and
      Parrish, Alicia  and
      Liu, Haokun  and
      Mohananey, Anhad  and
      Peng, Wei  and
      Wang, Sheng-Fu  and
      Bowman, Samuel R.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.25",
    doi = "10.1162/tacl_a_00321",
    pages = "377--392",
    abstract = "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
}

@inproceedings{belinkov2018synthetic,
  title={Synthetic and Natural Noise Both Break Neural Machine Translation},
  author={Belinkov, Yonatan and Bisk, Yonatan},
  booktitle={Proceedings of the 6th International Conference on Learning Representations},
  year={2018}
}

@inproceedings{isabelle2017challenge,
    title = "A Challenge Set Approach to Evaluating Machine Translation",
    author = "Isabelle, Pierre  and
      Cherry, Colin  and
      Foster, George",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1263",
    doi = "10.18653/v1/D17-1263",
    pages = "2486--2496",
    abstract = "Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system{'}s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.",
}

@inproceedings{glockner2018breaking,
    title = "Breaking {NLI} Systems with Sentences that Require Simple Lexical Inferences",
    author = "Glockner, Max  and
      Shwartz, Vered  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2103",
    doi = "10.18653/v1/P18-2103",
    pages = "650--655",
    abstract = "We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences.",
}

@article{belinkov2019analysis,
    title = "Analysis Methods in Neural Language Processing: A Survey",
    author = "Belinkov, Yonatan  and
      Glass, James",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    month = mar,
    year = "2019",
    url = "https://www.aclweb.org/anthology/Q19-1004",
    doi = "10.1162/tacl_a_00254",
    pages = "49--72",
    abstract = "The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.",
}

@inproceedings{sharma2020data,
author = {Sharma, Shubham and Zhang, Yunfeng and R\'{\i}os Aliaga, Jes\'{u}s M. and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R.},
title = {Data Augmentation for Discrimination Prevention and Bias Disambiguation},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375865},
doi = {10.1145/3375627.3375865},
abstract = {Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an "ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {358–364},
numpages = {7},
keywords = {discrimination prevention, fairness in machine learning, responsible artificial intelligence},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{devassimonmanela2021stereotype,
    title = "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models",
    author = "de Vassimon Manela, Daniel  and
      Errington, David  and
      Fisher, Thomas  and
      van Breugel, Boris  and
      Minervini, Pasquale",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.190",
    doi = "10.18653/v1/2021.eacl-main.190",
    pages = "2232--2242",
    abstract = "This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice.",
}

@inproceedings{bartl2020unmasking,
    title = "Unmasking Contextual Stereotypes: Measuring and Mitigating {BERT}{'}s Gender Bias",
    author = "Bartl, Marion  and
      Nissim, Malvina  and
      Gatt, Albert",
    booktitle = "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.gebnlp-1.1",
    pages = "1--16",
    abstract = "Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models.",
}

@inproceedings{tiwari2022robust,
    title = "Robust Hate Speech Detection via Mitigating Spurious Correlations",
    author = "Tiwari, Kshitiz  and
      Yuan, Shuhan  and
      Zhang, Lu",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-short.7",
    pages = "51--56",
    abstract = "We develop a novel robust hate speech detection model that can defend against both word- and character-level adversarial attacks. We identify the essential factor that vanilla detection models are vulnerable to adversarial attacks is the spurious correlation between certain target words in the text and the prediction label. To mitigate such spurious correlation, we describe the process of hate speech detection by a causal graph. Then, we employ the causal strength to quantify the spurious correlation and formulate a regularized entropy loss function. We show that our method generalizes the backdoor adjustment technique in causal inference. Finally, the empirical evaluation shows the efficacy of our method.",
}

@inproceedings{attanasio2022ear,
    title = "Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists",
    author = "Attanasio, Giuseppe  and
      Nozza, Debora  and
      Hovy, Dirk  and
      Baralis, Elena",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.88",
    doi = "10.18653/v1/2022.findings-acl.88",
    pages = "1105--1119",
    abstract = "Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance.Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected.Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low self-attention entropy.We fine-tune BERT via EAR: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian.EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.",
}

@inproceedings{nejadgholi2020cross,
    title = "On Cross-Dataset Generalization in Automatic Detection of Online Abuse",
    author = "Nejadgholi, Isar  and
      Kiritchenko, Svetlana",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.alw-1.20",
    doi = "10.18653/v1/2020.alw-1.20",
    pages = "173--183",
    abstract = "NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inherited in this task aggravates the discrepancies between source and target datasets. We explore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform-specific topics. We identify these examples using unsupervised topic modeling and manual inspection of topics{'} keywords. Removing these topics increases cross-dataset generalization, without reducing in-domain classification performance. For a robust dataset design, we suggest applying inexpensive unsupervised methods to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.",
}

@inproceedings{tahmasbi2021go,
  title={“Go eat a bat, Chang!”: On the Emergence of Sinophobic Behavior on Web Communities in the Face of COVID-19},
  author={Tahmasbi, Fatemeh and Schild, Leonard and Ling, Chen and Blackburn, Jeremy and Stringhini, Gianluca and Zhang, Yang and Zannettou, Savvas},
  booktitle={Proceedings of the web conference 2021},
  pages={1122--1133},
  year={2021}
}

@inproceedings{niven2019probing,
    title = "Probing Neural Network Comprehension of Natural Language Arguments",
    author = "Niven, Timothy  and
      Kao, Hung-Yu",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1459",
    doi = "10.18653/v1/P19-1459",
    pages = "4658--4664",
    abstract = "We are surprised to find that BERT{'}s peak performance of 77{\%} on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",
}

@techreport{ISO24765,
type = {Standard},
key = {ISO/IEC/IEEE 24765:2017(E)},
author = {ISO},
year = {2017},
title = {Systems and {S}oftware {E}ngineering -- {V}ocabulary},
address = {Geneva, Switzerland},
institution = {International Organization for Standardisation}
}

@article{samory2021sexist,
  title={''Call me sexist, but...'' : Revisiting Sexism Detection Using Psychological Scales and Adversarial Samples},
  author={Samory, Mattia and Sen, Indira and Kohne, Julian and Floeck, Fabian and Wagner, Claudia},
  journal={Proceedings of the International AAAI Conference on Web and Social Media,},
  year={2021}
}

@inproceedings{shah2020predictive,
    title = "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview",
    author = "Shah, Deven Santosh  and
      Schwartz, H. Andrew  and
      Hovy, Dirk",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.468",
    doi = "10.18653/v1/2020.acl-main.468",
    pages = "5248--5264",
    abstract = "An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.",
}

@inproceedings{plank2022problem,
    title = "The {``}Problem{''} of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation",
    author = "Plank, Barbara",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.731",
    pages = "10671--10682",
    abstract = "Human variation in labeling is often considered noise. Annotation projects for machine learning (ML) aim at minimizing human label variation, with the assumption to maximize data quality and in turn optimize and maximize machine learning metrics. However, thisconventional practice assumes that there exists a *ground truth*, and neglects that there exists genuine human variation in labeling due to disagreement, subjectivity in annotation or multiple plausible answers.In this position paper, we argue that this big open problem of \textit{human label variation} persists and critically needs more attention to move our field forward. This is because human label variation impacts all stages of the ML pipeline: *data, modeling and evaluation*. However, few works consider all of these dimensions jointly; and existing research is fragmented. We reconcile different previously proposed notions of human label variation, provide a repository of publicly-available datasets with un-aggregated labels, depict approaches proposed so far, identify gaps and suggest ways forward. As datasets are becoming increasingly available, we hope that this synthesized view on the {``}problem{''} will lead to an open discussion on possible strategies to devise fundamentally new directions.",
}

@inproceedings{mathew2021hatexplain,
  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},
  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={14867--14875},
  year={2021}
}

@inproceedings{ribeiro2016should,
  title={" Why should I trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016},
  publisher = {Association for Computing Machinery}
}

@book{beizer1995black,
  title={Black-box testing: techniques for functional testing of software and systems},
  author={Beizer, Boris},
  year={1995},
  publisher={John Wiley \& Sons, Inc.}
}

@inproceedings{rychalska2019models,
  title={Models in the wild: On corruption robustness of neural nlp systems},
  author={Rychalska, Barbara and Basaj, Dominika and Gosiewska, Alicja and Biecek, Przemys{\l}aw},
  booktitle={International Conference on Neural Information Processing},
  pages={235--247},
  year={2019},
  organization={Springer}
}

@inproceedings{grondahl2018all,
  title={All You Need is "Love": Evading Hate Speech Detection},
  author={Gr{\"o}ndahl, Tommi and Pajola, Luca and Juuti, Mika and Conti, Mauro and Asokan, N},
  booktitle={Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security},
  pages={2--12},
  year={2018},
  publisher = {Association for Computing Machinery}
}

@article{tversky1974judgment,
	Author = {Tversky, Amos and Kahneman, Daniel},
	Date-Added = {2020-02-28 11:48:02 +0000},
	Date-Modified = {2020-02-28 11:48:02 +0000},
	Journal = {science},
	Number = {4157},
	Pages = {1124--1131},
	Publisher = {American association for the advancement of science},
	Title = {Judgment under uncertainty: Heuristics and biases},
	Volume = {185},
	Year = {1974}}

@article{tappin2017heart,
	Author = {Tappin, Ben M and van der Leer, Leslie and McKay, Ryan T},
	Date-Added = {2020-02-28 11:13:37 +0000},
	Date-Modified = {2020-02-28 11:13:37 +0000},
	Journal = {Journal of Experimental Psychology: General},
	Number = {8},
	Pages = {1143},
	Publisher = {American Psychological Association},
	Title = {The heart trumps the head: Desirability bias in political belief revision.},
	Volume = {146},
	Year = {2017}}

@book{kahneman2011thinking,
	Author = {Kahneman, Daniel},
	Date-Added = {2020-02-28 11:06:29 +0000},
	Date-Modified = {2020-02-28 11:06:29 +0000},
	Publisher = {Macmillan},
	Title = {Thinking, fast and slow},
	Year = {2011}}
    

@article{florio2020time,
  title = {Time of Your Hate: The Challenge of Time in Hate Speech Detection on Social Media},
  shorttitle = {Time of {{Your Hate}}},
  author = {Florio, Komal and Basile, Valerio and Polignano, Marco and Basile, Pierpaolo and Patti, Viviana},
  date = {2020-01},
  year={2020},
  journaltitle = {Applied Sciences},
  journal = {Applied Sciences},
  volume = {10},
  pages = {4180},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/app10124180},
  url = {https://www.mdpi.com/2076-3417/10/12/4180},
  urldate = {2021-02-16},
  abstract = {The availability of large annotated corpora from social media and the development of powerful classification approaches have contributed in an unprecedented way to tackle the challenge of monitoring users\&rsquo; opinions and sentiments in online social platforms across time. Such linguistic data are strongly affected by events and topic discourse, and this aspect is crucial when detecting phenomena such as hate speech, especially from a diachronic perspective. We address this challenge by focusing on a real case study: the \&ldquo;Contro l\&rsquo;odio\&rdquo; platform for monitoring hate speech against immigrants in the Italian Twittersphere. We explored the temporal robustness of a BERT model for Italian (AlBERTo), the current benchmark on non-diachronic detection settings. We tested different training strategies to evaluate how the classification performance is affected by adding more data temporally distant from the test set and hence potentially different in terms of topic and language use. Our analysis points out the limits that a supervised classification model encounters on data that are heavily influenced by events. Our results show how AlBERTo is highly sensitive to the temporal distance of the fine-tuning set. However, with an adequate time window, the performance increases, while requiring less annotated data than a traditional classifier.},
  file = {/Users/Paul/Zotero/storage/A43U8XGC/Florio et al. - 2020 - Time of Your Hate The Challenge of Time in Hate S.pdf;/Users/Paul/Zotero/storage/WKIPD6UJ/htm.html},
  issue = {12},
  keywords = {diachronic analysis,hate speech monitoring,microblogging data,supervised machine learning},
  langid = {english},
  number = {12}
}


    
@inproceedings{beltagy2019scibert,
    title = "{S}ci{BERT}: A Pretrained Language Model for Scientific Text",
    author = "Beltagy, Iz  and
      Lo, Kyle  and
      Cohan, Arman",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1371",
    doi = "10.18653/v1/D19-1371",
    pages = "3615--3620",
    abstract = "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
}

@inproceedings{hofmann2021dynamic,
    title = "Dynamic Contextualized Word Embeddings",
    author = {Hofmann, Valentin  and
      Pierrehumbert, Janet  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.542",
    doi = "10.18653/v1/2021.acl-long.542",
    pages = "6970--6984",
    abstract = "Static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts. Building on prior work on contextualized and dynamic word embeddings, we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. Based on a pretrained language model (PLM), dynamic contextualized word embeddings model time and social space jointly, which makes them attractive for a range of NLP tasks involving semantic variability. We highlight potential application scenarios by means of qualitative and quantitative analyses on four English datasets.",
}

@inproceedings{han2019unsupervised,
    title = "Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling",
    author = "Han, Xiaochuang  and
      Eisenstein, Jacob",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1433",
    doi = "10.18653/v1/D19-1433",
    pages = "4238--4248",
    abstract = "Contextualized word embeddings such as ELMo and BERT provide a foundation for strong performance across a wide range of natural language processing tasks by pretraining on large corpora of unlabeled text. However, the applicability of this approach is unknown when the target domain varies substantially from the pretraining corpus. We are specifically interested in the scenario in which labeled data is available in only a canonical source domain such as newstext, and the target domain is distinct from both the labeled and pretraining texts. To address this scenario, we propose domain-adaptive fine-tuning, in which the contextualized embeddings are adapted by masked language modeling on text from the target domain. We test this approach on sequence labeling in two challenging domains: Early Modern English and Twitter. Both domains differ substantially from existing pretraining corpora, and domain-adaptive fine-tuning yields substantial improvements over strong BERT baselines, with particularly impressive results on out-of-vocabulary words. We conclude that domain-adaptive fine-tuning offers a simple and effective approach for the unsupervised adaptation of sequence labeling to difficult new domains.",
}

@inproceedings{card2015media,
    title = "The Media Frames Corpus: Annotations of Frames Across Issues",
    author = "Card, Dallas  and
      Boydstun, Amber E.  and
      Gross, Justin H.  and
      Resnik, Philip  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-2072",
    doi = "10.3115/v1/P15-2072",
    pages = "438--444",
}

@article{huang2019clinicalbert,
  author    = {Kexin Huang and
               Jaan Altosaar and
               Rajesh Ranganath},
  title     = {Clinical{BERT}: Modeling Clinical Notes and Predicting Hospital Readmission},
  journal   = {CoRR},
  volume    = {abs/1904.05342},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.05342},
  archivePrefix = {arXiv},
  eprint    = {1904.05342},
  timestamp = {Thu, 25 Apr 2019 13:55:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-05342.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{henderson2020towards,
  author  = {Peter Henderson and Jieru Hu and Joshua Romoff and Emma Brunskill and Dan Jurafsky and Joelle Pineau},
  title   = {Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {248},
  pages   = {1-43},
  url     = {http://jmlr.org/papers/v21/20-312.html}
}

@inproceedings{bender2021stochastic,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}



@inproceedings{strubell2019energy,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1355",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
}

@inproceedings{chakrabarty2019imho,
    title = "{IMHO} Fine-Tuning Improves Claim Detection",
    author = "Chakrabarty, Tuhin  and
      Hidey, Christopher  and
      McKeown, Kathy",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1054",
    doi = "10.18653/v1/N19-1054",
    pages = "558--563",
    abstract = "Claims are the central component of an argument. Detecting claims across different domains or data sets can often be challenging due to their varying conceptualization. We propose to alleviate this problem by fine-tuning a language model using a Reddit corpus of 5.5 million opinionated claims. These claims are self-labeled by their authors using the internet acronyms IMO/IMHO (in my (humble) opinion). Empirical results show that using this approach improves the state of art performance across four benchmark argumentation data sets by an average of 4 absolute F1 points in claim detection. As these data sets include diverse domains such as social media and student essays this improvement demonstrates the robustness of fine-tuning on this novel corpus.",
}

@inproceedings{alsentzer2019publicly,
    title = "Publicly Available Clinical {BERT} Embeddings",
    author = "Alsentzer, Emily  and
      Murphy, John  and
      Boag, William  and
      Weng, Wei-Hung  and
      Jindi, Di  and
      Naumann, Tristan  and
      McDermott, Matthew",
    booktitle = "Proceedings of the 2nd Clinical Natural Language Processing Workshop",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1909",
    doi = "10.18653/v1/W19-1909",
    pages = "72--78",
    abstract = "Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.",
}

@inproceedings{clark2020pretraining,
    title = "Pre-Training Transformers as Energy-Based Cloze Models",
    author = "Clark, Kevin  and
      Luong, Minh-Thang  and
      Le, Quoc  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.20",
    doi = "10.18653/v1/2020.emnlp-main.20",
    pages = "285--294",
    abstract = "We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.",
}
}

@inproceedings{clark2020electra,
title={{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1xMH1BtvB}
}

@article{hofmann2021modeling,
  author    = {Valentin Hofmann and
               Janet B. Pierrehumbert and
               Hinrich Sch{\"{u}}tze},
  title     = {Modeling Ideological Agenda Setting and Framing in Polarized Online
               Groups with Graph Neural Networks and Structured Sparsity},
  journal   = {CoRR},
  volume    = {abs/2104.08829},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08829},
  archivePrefix = {arXiv},
  eprint    = {2104.08829},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08829.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{demszky2019analyzing,
    title = "Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings",
    author = "Demszky, Dorottya  and
      Garg, Nikhil  and
      Voigt, Rob  and
      Zou, James  and
      Shapiro, Jesse  and
      Gentzkow, Matthew  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1304",
    doi = "10.18653/v1/N19-1304",
    pages = "2970--3005",
    abstract = "We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our methods to study 4.4M tweets on 21 mass shootings. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. We identify framing devices, such as grounding and the contrasting use of the terms {``}terrorist{''} and {``}crazy{''}, that contribute to polarization. Results pertaining to topic choice, affect and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.",
}

@inproceedings{lukes2018sentiment,
    title = "Sentiment analysis under temporal shift",
    author = "Lukes, Jan  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6210",
    doi = "10.18653/v1/W18-6210",
    pages = "65--71",
    abstract = "Sentiment analysis models often rely on training data that is several years old. In this paper, we show that lexical features change polarity over time, leading to degrading performance. This effect is particularly strong in sparse models relying only on highly predictive features. Using predictive feature selection, we are able to significantly improve the accuracy of such models over time.",
}

@article{altmann2009beyond,
  title={Beyond word frequency: Bursts, lulls, and scaling in the temporal distributions of words},
  author={Altmann, Eduardo G and Pierrehumbert, Janet B and Motter, Adilson E},
  journal={PLOS one},
  volume={4},
  number={11},
  pages={e7678},
  year={2009},
  publisher={Public Library of Science}
}

@incollection{pierrehumbert2012burstiness,
  title={Burstiness of verbs and derived nouns},
  author={Pierrehumbert, Janet B},
  booktitle={Shall We Play the Festschrift Game?},
  editor = {Santos, Diana and Linden, Krister and Ng'ang'a, Wanjiku},
  pages={99--115},
  year={2012},
  publisher={Springer}
}

@inproceedings{church2000empirical,
    title = "Empirical Estimates of Adaptation: The chance of Two {N}oriegas is closer to $p/2$ than $p2$",
    author = "Church, Kenneth W.",
    booktitle = "{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics",
    year = "2000",
    url = "https://www.aclweb.org/anthology/C00-1027",
}

@article{blei2003latent,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent Dirichlet Allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
journal = {Journal of Machine Learning Research},
month = mar,
pages = {993–1022},
numpages = {30}
}

@article{church1995poisson,
  title={Poisson mixtures.},
  author={Church, Kenneth Ward and Gale, William A},
  journal={Nat. Lang. Eng.},
  volume={1},
  number={2},
  pages={163--190},
  year={1995},
  publisher={Citeseer}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}


@inproceedings{rudolph2018dynamic,
  title = {Dynamic Embeddings for Language Evolution},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  author = {Rudolph, Maja and Blei, David},
  date = {2018-04-10},
  year = {2018},
  pages = {1003--1011},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  location = {{Republic and Canton of Geneva, CHE}},
  doi = {10.1145/3178876.3185999},
  url = {https://doi.org/10.1145/3178876.3185999},
  urldate = {2021-04-10},
  abstract = {Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the ArXiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.},
  isbn = {978-1-4503-5639-8},
  keywords = {dynamic modeling,exponential family embeddings,probabilistic modeling,semantic change,word embeddings},
  series = {{{WWW}} '18}
}


@article{statista2019reddit,
  title = {Reddit: Traffic by country},
  author = {Statista},
  journal = {Statista},
  year = {2019},
  url = {https://www.statista.com/statistics/325144/reddit-global-active-user-distribution/},
  urldate = {2021-04-13},
  abstract = {Users from the United States account for half of all desktop traffic to Reddit. Around eight percent of Reddit users are from the UK.},
  langid = {english},
  organization = {{Statista}}
}


@inproceedings{dai2019transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}

@inproceedings{tsakalidis2020sequential,
    title = "Sequential Modelling of the Evolution of Word Representations for Semantic Change Detection",
    author = "Tsakalidis, Adam  and
      Liakata, Maria",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.682",
    doi = "10.18653/v1/2020.emnlp-main.682",
    pages = "8485--8497",
    abstract = "Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time. In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time. Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection. Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance.",
}

@inproceedings{gong2020enriching,
    title = "Enriching Word Embeddings with Temporal and Spatial Information",
    author = "Gong, Hongyu  and
      Bhat, Suma  and
      Viswanath, Pramod",
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.conll-1.1",
    doi = "10.18653/v1/2020.conll-1.1",
    pages = "1--11",
    abstract = "The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may require us to capture more refined semantics for use in time-specific or location-aware situations, such as the study of cultural trends or language use. However, popular vector representations for words do not adequately include temporal or spatial information. In this work, we present a model for learning word representation conditioned on time and location. In addition to capturing meaning changes over time and location, we require that the resulting word embeddings retain salient semantic and geometric properties. We train our model on time- and location-stamped corpora, and show using both quantitative and qualitative evaluations that it can capture semantics across time and locations. We note that our model compares favorably with the state-of-the-art for time-specific embedding, and serves as a new benchmark for location-specific embeddings.",
}




@inproceedings{huang2019neural,
    title = "Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models",
    author = "Huang, Xiaolei  and
      Paul, Michael J.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1403",
    doi = "10.18653/v1/P19-1403",
    pages = "4113--4123",
    abstract = "Language usage can change across periods of time, but document classifiers models are usually trained and tested on corpora spanning multiple years without considering temporal variations. This paper describes two complementary ways to adapt classifiers to shifts across time. First, we show that diachronic word embeddings, which were originally developed to study language change, can also improve document classification, and we show a simple method for constructing this type of embedding. Second, we propose a time-driven neural classification model inspired by methods for domain adaptation. Experiments on six corpora show how these methods can make classifiers more robust over time.",
}


@article{krause2019dynamic,
  author    = {Ben Krause and
               Emmanuel Kahembwe and
               Iain Murray and
               Steve Renals},
  title     = {Dynamic Evaluation of Transformer Language Models},
  journal   = {CoRR},
  volume    = {abs/1904.08378},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.08378},
  archivePrefix = {arXiv},
  eprint    = {1904.08378},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-08378.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{huang2018examining,
  title = {Examining Temporality in Document Classification},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Huang, Xiaolei and Paul, Michael J.},
  date = {2018-07},
  year = {2018},
  pages = {694--699},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-2110},
  url = {https://www.aclweb.org/anthology/P18-2110},
  urldate = {2021-02-16},
  abstract = {Many corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years). We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.},
  eventtitle = {{{ACL}} 2018},
  file = {/Users/Paul/Zotero/storage/2ZD3TFMF/Huang and Paul - 2018 - Examining Temporality in Document Classification.pdf}
}


@inproceedings{baumgartner2020pushshift,
  title={The {P}ushshift {R}eddit Dataset},
  author={Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={14},
  pages={830--839},
  year={2020}
}

@article{liu2019survey,
  title={A survey of sentiment analysis based on transfer learning},
  author={Liu, Ruijun and Shi, Yuqian and Ji, Changjiang and Jia, Ming},
  journal={IEEE Access},
  volume={7},
  pages={85401--85412},
  year={2019},
  publisher={IEEE}
}

@inproceedings{jimenez2020document,
    title = "Document Classification for {COVID-19} Literature",
    author = "Jim{\'e}nez Guti{\'e}rrez, Bernal  and
      Zeng, Juncheng  and
      Zhang, Dongdong  and
      Zhang, Ping  and
      Su, Yu",
    booktitle = "Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.nlpcovid19-acl.3",
    abstract = "The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset. We find that pre-trained language models outperform other models in both low and high data regimes, achieving a maximum F1 score of around 86{\%}. We note that even the highest performing models still struggle with label correlation, distraction from introductory text and CORD-19 generalization. Both data and code are available on GitHub.",
}

@inproceedings{medina2020nlp,
    title = "{NLP}-based Feature Extraction for the Detection of {COVID}-19 Misinformation Videos on {Y}ou{T}ube",
    author = "Medina Serrano, Juan Carlos  and
      Papakyriakopoulos, Orestis  and
      Hegelich, Simon",
    booktitle = "Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.nlpcovid19-acl.17",
    abstract = "We present a simple NLP methodology for detecting COVID-19 misinformation videos on YouTube by leveraging user comments. We use transfer learning pre-trained models to generate a multi-label classifier that can categorize conspiratorial content. We use the percentage of misinformation comments on each video as a new feature for video classification.",
}

@article{jiang2021understanding,
  title={Understanding international perceptions of the severity of harmful content online},
  author={Jiang, Jialun Aaron and Scheuerman, Morgan Klaus and Fiesler, Casey and Brubaker, Jed R},
  journal={PloS one},
  volume={16},
  number={8},
  pages={e0256762},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}

@book{pfeffermann2009sample,
  title={Sample surveys: design, methods and applications},
  author={Pfeffermann, Danny and Rao, Calyampudi Radhakrishna},
  year={2009},
  publisher={Elsevier}
}

@article{basile2021toward,
  title={Toward a Perspectivist Turn in Ground Truthing for Predictive Computing},
  author={Basile, Valerio and Cabitza, Federico and Campagner, Andrea and Fell, Michael},
  journal={Conference of the Italian Chapter of the Association for Intelligent Systems (ItAIS 2021)},
  year={2021}
}

@article{guttman1945basis,
  title={A basis for analyzing test-retest reliability},
  author={Guttman, Louis},
  journal={Psychometrika},
  volume={10},
  number={4},
  pages={255--282},
  year={1945},
  publisher={Springer}
}

@article{heise1969separating,
  title={Separating reliability and stability in test-retest correlation},
  author={Heise, David R},
  journal={American sociological review},
  pages={93--101},
  year={1969},
  publisher={JSTOR}
}

@inproceedings{leonardelli2021agreeing,
    title = "Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators{'} Disagreement",
    author = "Leonardelli, Elisa  and
      Menini, Stefano  and
      Palmero Aprosio, Alessio  and
      Guerini, Marco  and
      Tonelli, Sara",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.822",
    pages = "10528--10539",
    abstract = "Since state-of-the-art approaches to offensive language detection rely on supervised learning, it is crucial to quickly adapt them to the continuously evolving scenario of social media. While several approaches have been proposed to tackle the problem from an algorithmic perspective, so to reduce the need for annotated data, less attention has been paid to the quality of these data. Following a trend that has emerged recently, we focus on the level of agreement among annotators while selecting data to create offensive language datasets, a task involving a high level of subjectivity. Our study comprises the creation of three novel datasets of English tweets covering different topics and having five crowd-sourced judgments each. We also present an extensive set of experiments showing that selecting training and test data according to different levels of annotators{'} agreement has a strong effect on classifiers performance and robustness. Our findings are further validated in cross-domain experiments and studied using a popular benchmark dataset. We show that such hard cases, where low agreement is present, are not necessarily due to poor-quality annotation and we advocate for a higher presence of ambiguous cases in future datasets, in order to train more robust systems and better account for the different points of view expressed online.",
}

@inproceedings{gordon2021disagreement,
  title={The disagreement deconvolution: Bringing machine learning performance metrics in line with reality},
  author={Gordon, Mitchell L and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2021}
}

@phdthesis{cambo2021model,
  title={Model Positionality: A Novel Framework for Data Science with Subjective Target Concepts},
  author={Cambo, Scott Allen},
  year={2021},
  school={Northwestern University}
}

@misc{davani2021hate,
      title={Hate Speech Classifiers Learn Human-Like Social Stereotypes}, 
      author={Aida Mostafazadeh Davani and Mohammad Atari and Brendan Kennedy and Morteza Dehghani},
      year={2021},
      eprint={2110.14839},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{alm2011subjective,
    title = "Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications",
    author = "Alm, Cecilia Ovesdotter",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-2019",
    pages = "107--112",
}

@article{talat2021word,
  title={A Word on Machine Ethics: A Response to Jiang et al.(2021)},
  author={Talat, Zeerak and Blix, Hagen and Valvoda, Josef and Ganesh, Maya Indira and Cotterell, Ryan and Williams, Adina},
  journal={arXiv preprint arXiv:2111.04158},
  year={2021}
}

@article{zaenen2006markup,
author = {Zaenen, Annie},
title = {Mark-up Barking Up the Wrong Tree},
year = {2006},
issue_date = {December 2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {32},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli.2006.32.4.577},
doi = {10.1162/coli.2006.32.4.577},
journal = {Comput. Linguist.},
month = {dec},
pages = {577–580},
numpages = {4}
}

@inproceedings{basile2021considerdisagreement,
    title = "We Need to Consider Disagreement in Evaluation",
    author = "Basile, Valerio  and
      Fell, Michael  and
      Fornaciari, Tommaso  and
      Hovy, Dirk  and
      Paun, Silviu  and
      Plank, Barbara  and
      Poesio, Massimo  and
      Uma, Alexandra",
    booktitle = "Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.bppf-1.3",
    doi = "10.18653/v1/2021.bppf-1.3",
    pages = "15--21",
    abstract = "Evaluation is of paramount importance in data-driven research fields such as Natural Language Processing (NLP) and Computer Vision (CV). Current evaluation practice largely hinges on the existence of a single {``}ground truth{''} against which we can meaningfully compare the prediction of a model. However, this comparison is flawed for two reasons. 1) In many cases, more than one answer is correct. 2) Even where there is a single answer, disagreement among annotators is ubiquitous, making it difficult to decide on a gold standard. We argue that the current methods of adjudication, agreement, and evaluation need serious reconsideration. Some researchers now propose to minimize disagreement and to fix datasets. We argue that this is a gross oversimplification, and likely to conceal the underlying complexity. Instead, we suggest that we need to better capture the sources of disagreement to improve today{'}s evaluation practice. We discuss three sources of disagreement: from the annotator, the data, and the context, and show how this affects even seemingly objective tasks. Datasets with multiple annotations are becoming more common, as are methods to integrate disagreement into modeling. The logical next step is to extend this to evaluation.",
}

@inproceedings{salminen2019hateratings,
author = {Salminen, Joni and Almerekhi, Hind and Kamel, Ahmed Mohamed and Jung, Soon-gyo and Jansen, Bernard J.},
title = {Online Hate Ratings Vary by Extremes: A Statistical Analysis},
year = {2019},
isbn = {9781450360258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295750.3298954},
doi = {10.1145/3295750.3298954},
abstract = {Analyzing 5,665 crowd ratings on 1,133 social media comments, we find that individuals tend to agree on the extremes of a hate rating scale more than in the middle when evaluating the hatefulness of online comments. The agreement is higher for less hateful comments and lowest on moderately hateful comments. The results have implications for researchers developing machine learning models for online hate processing, as the extreme classes are likely to require fewer annotations for reaching statistical stability. Our findings suggest that the models developed in this domain should consider the distributions of hate ratings rather than average hate scores.},
booktitle = {Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
pages = {213–217},
numpages = {5},
keywords = {toxicity, crowdsourcing, interpretation, online hate, ratings},
location = {Glasgow, Scotland UK},
series = {CHIIR '19}
}

@inproceedings{prabhakaran2021releasing,
    title = "On Releasing Annotator-Level Labels and Information in Datasets",
    author = "Prabhakaran, Vinodkumar  and
      Mostafazadeh Davani, Aida  and
      Diaz, Mark",
    booktitle = "Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop",
    month = nov # " 11",
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.law-1.14",
    pages = "133--138",
    abstract = "A common practice in building NLP datasets, especially using crowd-sourced annotations, involves obtaining multiple annotator judgements on the same data instances, which are then flattened to produce a single {``}ground truth{''} label or score, through majority voting, averaging, or adjudication. While these approaches may be appropriate in certain annotation tasks, such aggregations overlook the socially constructed nature of human perceptions that annotations for relatively more subjective tasks are meant to capture. In particular, systematic disagreements between annotators owing to their socio-cultural backgrounds and/or lived experiences are often obfuscated through such aggregations. In this paper, we empirically demonstrate that label aggregation may introduce representational biases of individual and group perspectives. Based on this finding, we propose a set of recommendations for increased utility and transparency of datasets for downstream use cases.",
}

@article{davani2021disagreement,
    title = "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
    author = "Davani, Aida Mostafazadeh  and
      D{\'\i}az, Mark  and
      Prabhakaran, Vinodkumar",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.6",
    doi = "10.1162/tacl_a_00449",
    pages = "92--110",
    abstract = "Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators{'} judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important.",
}

@inproceedings{kenyon2018sentiment,
    title = "Sentiment Analysis: It{'}s Complicated!",
    author = "Kenyon-Dean, Kian  and
      Ahmed, Eisha  and
      Fujimoto, Scott  and
      Georges-Filteau, Jeremy  and
      Glasz, Christopher  and
      Kaur, Barleen  and
      Lalande, Auguste  and
      Bhanderi, Shruti  and
      Belfer, Robert  and
      Kanagasabai, Nirmal  and
      Sarrazingendron, Roman  and
      Verma, Rohit  and
      Ruths, Derek",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1171",
    doi = "10.18653/v1/N18-1171",
    pages = "1886--1895",
    abstract = "Sentiment analysis is used as a proxy to measure human emotion, where the objective is to categorize text according to some predefined notion of sentiment. Sentiment analysis datasets are typically constructed with gold-standard sentiment labels, assigned based on the results of manual annotations. When working with such annotations, it is common for dataset constructors to discard {``}noisy{''} or {``}controversial{''} data where there is significant disagreement on the proper label. In datasets constructed for the purpose of Twitter sentiment analysis (TSA), these controversial examples can compose over 30{\%} of the originally annotated data. We argue that the removal of such data is a problematic trend because, when performing real-time sentiment classification of short-text, an automated system cannot know a priori which samples would fall into this category of disputed sentiment. We therefore propose the notion of a {``}complicated{''} class of sentiment to categorize such text, and argue that its inclusion in the short-text sentiment analysis framework will improve the quality of automated sentiment analysis systems as they are implemented in real-world settings. We motivate this argument by building and analyzing a new publicly available TSA dataset of over 7,000 tweets annotated with 5x coverage, named MTSA. Our analysis of classifier performance over our dataset offers insights into sentiment analysis dataset and model design, how current techniques would perform in the real world, and how researchers should handle difficult data.",
}

@inproceedings{jamison2015softlabel,
    title = "Noise or additional information? Leveraging crowdsource annotation item agreement for natural language tasks.",
    author = "Jamison, Emily  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1035",
    doi = "10.18653/v1/D15-1035",
    pages = "291--297",
}

@inproceedings{fornaciari2021softlabel,
    title = "Beyond Black {\&} White: Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning",
    author = "Fornaciari, Tommaso  and
      Uma, Alexandra  and
      Paun, Silviu  and
      Plank, Barbara  and
      Hovy, Dirk  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.204",
    doi = "10.18653/v1/2021.naacl-main.204",
    pages = "2591--2597",
    abstract = "Supervised learning assumes that a ground truth label exists. However, the reliability of this ground truth depends on human annotators, who often disagree. Prior work has shown that this disagreement can be helpful in training models. We propose a novel method to incorporate this disagreement as information: in addition to the standard error computation, we use soft-labels (i.e., probability distributions over the annotator labels) as an auxiliary task in a multi-task neural network. We measure the divergence between the predictions and the target soft-labels with several loss-functions and evaluate the models on various NLP tasks. We find that the soft-label prediction auxiliary task reduces the penalty for errors on ambiguous entities, and thereby mitigates overfitting. It significantly improves performance across tasks, beyond the standard approach and prior work.",
}

@article{hallinan2022detoxifying,
  title={Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts},
  author={Hallinan, Skyler and Liu, Alisa and Choi, Yejin and Sap, Maarten},
  journal={arXiv preprint arXiv:2212.10543},
  year={2022}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@inproceedings{
ouyang2022training,
title={Training language models to follow instructions with human feedback},
author={Long Ouyang and Jeffrey Wu and Xu Jiang and Diogo Almeida and Carroll Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Gray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=TG8KACxEON}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{demus2022comprehensive,
    title = "DeTox: A Comprehensive Dataset for {G}erman Offensive Language and Conversation Analysis",
    author = {Demus, Christoph  and
      Pitz, Jonas  and
      Sch{\"u}tz, Mina  and
      Probol, Nadine  and
      Siegel, Melanie  and
      Labudde, Dirk},
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.14",
    doi = "10.18653/v1/2022.woah-1.14",
    pages = "143--153",
    abstract = "In this work, we present a new publicly available offensive language dataset of 10.278 German social media comments collected in the first half of 2021 that were annotated by in total six annotators. With twelve different annotation categories, it is far more comprehensive than other datasets, and goes beyond just hate speech detection. The labels aim in particular also at toxicity, criminal relevance and discrimination types of comments.Furthermore, about half of the comments are from coherent parts of conversations, which opens the possibility to consider the comments{'} contexts and do conversation analyses in order to research the contagion of offensive language in conversations.",
}

@inproceedings{naito2022typic,
    title = "{TYPIC}: A Corpus of Template-Based Diagnostic Comments on Argumentation",
    author = "Naito, Shoichi  and
      Sawada, Shintaro  and
      Nakagawa, Chihiro  and
      Inoue, Naoya  and
      Yamaguchi, Kenshi  and
      Shimizu, Iori  and
      Mim, Farjana Sultana  and
      Singh, Keshav  and
      Inui, Kentaro",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.636",
    pages = "5916--5928",
    abstract = "Providing feedback on the argumentation of the learner is essential for developing critical thinking skills, however, it requires a lot of time and effort. To mitigate the overload on teachers, we aim to automate a process of providing feedback, especially giving diagnostic comments which point out the weaknesses inherent in the argumentation. It is recommended to give specific diagnostic comments so that learners can recognize the diagnosis without misinterpretation. However, it is not obvious how the task of providing specific diagnostic comments should be formulated. We present a formulation of the task as template selection and slot filling to make an automatic evaluation easier and the behavior of the model more tractable. The key to the formulation is the possibility of creating a template set that is sufficient for practical use. In this paper, we define three criteria that a template set should satisfy: expressiveness, informativeness, and uniqueness, and verify the feasibility of creating a template set that satisfies these criteria as a first trial. We will show that it is feasible through an annotation study that converts diagnostic comments given in a text to a template format. The corpus used in the annotation study is publicly available.",
}

@article{dhingra2022time,
  title={Time-aware language models as temporal knowledge bases},
  author={Dhingra, Bhuwan and Cole, Jeremy R and Eisenschlos, Julian Martin and Gillick, Daniel and Eisenstein, Jacob and Cohen, William W},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={257--273},
  year={2022},
  publisher={MIT Press}
}

@inproceedings{jang2022temporalwiki,
    title = "{T}emporal{W}iki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models",
    author = "Jang, Joel  and
      Ye, Seonghyeon  and
      Lee, Changho  and
      Yang, Sohee  and
      Shin, Joongbo  and
      Han, Janghoon  and
      Kim, Gyeonghun  and
      Seo, Minjoon",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.418",
    pages = "6237--6250",
    abstract = "Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM{'}s ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.",
}


@article{gillespie2020content,
  title={Content moderation, AI, and the question of scale},
  author={Gillespie, Tarleton},
  journal={Big Data \& Society},
  volume={7},
  number={2},
  pages={2053951720943234},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{lees2022new,
  title={A new generation of perspective api: Efficient multilingual character-level transformers},
  author={Lees, Alyssa and Tran, Vinh Q and Tay, Yi and Sorensen, Jeffrey and Gupta, Jai and Metzler, Donald and Vasserman, Lucy},
  journal={arXiv preprint arXiv:2202.11176},
  year={2022}
}

@article{aldayel2021stance,
  title={Stance detection on social media: State of the art and trends},
  author={AlDayel, Abeer and Magdy, Walid},
  journal={Information Processing \& Management},
  volume={58},
  number={4},
  pages={102597},
  year={2021},
  publisher={Elsevier}
}

@article{gorwa2020algorithmic,
  title={Algorithmic content moderation: Technical and political challenges in the automation of platform governance},
  author={Gorwa, Robert and Binns, Reuben and Katzenbach, Christian},
  journal={Big Data \& Society},
  volume={7},
  number={1},
  pages={2053951719897945},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{poria2020beneath,
  title={Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research},
  author={Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Mihalcea, Rada},
  journal={IEEE Transactions on Affective Computing},
  year={2020},
  publisher={IEEE}
}

@article{gillespie2022not,
  title={Do Not Recommend? Reduction as a Form of Content Moderation},
  author={Gillespie, Tarleton},
  journal={Social Media+ Society},
  volume={8},
  number={3},
  pages={20563051221117552},
  year={2022},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{hern2021facebook,
  title={Decoding emojis and defining 'support': Facebook's rules for content revealed},
  author={Hern, Alex},
  journal={The Guardian},
  year={2021}
}

@article{marinescu2021facebook,
  title={Facebook's Content Moderation Language Barrier},
  author={Marinescu, Delia},
  journal={New America},
  year={2021}
}

@inproceedings{rottger2022two,
    title = "Two Contrasting Data Annotation Paradigms for Subjective {NLP} Tasks",
    author = "R{\"o}ttger, Paul  and
      Vidgen, Bertie  and
      Hovy, Dirk  and
      Pierrehumbert, Janet",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.13",
    doi = "10.18653/v1/2022.naacl-main.13",
    pages = "175--190",
    abstract = "Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss benefits and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.",
}

@article{simonite2021facebook,
  title={Facebook Is Everywhere; Its Moderation Is Nowhere Close},
  author={Simonite, Tom},
  journal={Wired},
  year={2021}
}

@inproceedings{sap2022annotators,
    title = "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    author = "Sap, Maarten  and
      Swayamdipta, Swabha  and
      Vianna, Laura  and
      Zhou, Xuhui  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.431",
    doi = "10.18653/v1/2022.naacl-main.431",
    pages = "5884--5906",
    abstract = "The perceived toxicity of language can vary based on someone{'}s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system{'}s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.",
}

@book{gillespie2018custodians,
  title={Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape social media},
  author={Gillespie, Tarleton},
  year={2018},
  publisher={Yale University Press}
}

@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}

@article{gitari2015lexicon,
  title={A lexicon-based approach for hate speech detection},
  author={Gitari, Njagi Dennis and Zuping, Zhang and Damien, Hanyurwimfura and Long, Jun},
  journal={International Journal of Multimedia and Ubiquitous Engineering},
  volume={10},
  number={4},
  pages={215--230},
  year={2015}
}

@article{tulkens2016dictionary,
  title={A dictionary-based approach to racism detection in dutch social media},
  author={Tulkens, St{\'e}phan and Hilte, Lisa and Lodewyckx, Elise and Verhoeven, Ben and Daelemans, Walter},
  journal={arXiv preprint arXiv:1608.08738},
  year={2016}
}

@article{burnap2014hate,
  title={Hate speech, machine classification and statistical modelling of information flows on Twitter: Interpretation and communication for policy decision making},
  author={Burnap, Peter and Williams, Matthew Leighton},
  journal={Proceedings of the 2014 Internet, Policy and Politics Conference},
  year={2014}
}

@inproceedings{nozza2022hateita,
    title = "{HATE}-{ITA}: Hate Speech Detection in {I}talian Social Media Text",
    author = "Nozza, Debora  and
      Bianchi, Federico  and
      Attanasio, Giuseppe",
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.24",
    doi = "10.18653/v1/2022.woah-1.24",
    pages = "252--260",
    abstract = "Online hate speech is a dangerous phenomenon that can (and should) be promptly counteracted properly. While Natural Language Processing supplies appropriate algorithms for trying to reach this objective, all research efforts are directed toward the English language. This strongly limits the classification power on non-English languages. In this paper, we test several learning frameworks for identifying hate speech in Italian text. We release HATE-ITA, a multi-language model trained on a large set of English data and available Italian datasets. HATE-ITA performs better than mono-lingual models and seems to adapt well also on language-specific slurs. We hope our findings will encourage the research in other mid-to-low resource communities and provide a valuable benchmarking tool for the Italian community.",
}

@inproceedings{pavlopoulos2021semevaltoxicspans,
    title = "{S}em{E}val-2021 Task 5: Toxic Spans Detection",
    author = "Pavlopoulos, John  and
      Sorensen, Jeffrey  and
      Laugier, L{\'e}o  and
      Androutsopoulos, Ion",
    booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.semeval-1.6",
    doi = "10.18653/v1/2021.semeval-1.6",
    pages = "59--69",
    abstract = "The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions.",
}

@inproceedings{caselli2021hatebert,
    title = "{H}ate{BERT}: Retraining {BERT} for Abusive Language Detection in {E}nglish",
    author = "Caselli, Tommaso  and
      Basile, Valerio  and
      Mitrovi{\'c}, Jelena  and
      Granitzer, Michael",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.3",
    doi = "10.18653/v1/2021.woah-1.3",
    pages = "17--25",
    abstract = "We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.",
}

@inproceedings{
hoffmann2022chinchilla,
title={An empirical analysis of compute-optimal large language model training},
author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katherine Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack William Rae and Laurent Sifre},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=iBBcRUlOAPR}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{alkuwatly2020identifying,
    title = "Identifying and Measuring Annotator Bias Based on Annotators{'} Demographic Characteristics",
    author = "Al Kuwatly, Hala  and
      Wich, Maximilian  and
      Groh, Georg",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.alw-1.21",
    doi = "10.18653/v1/2020.alw-1.21",
    pages = "184--190",
    abstract = "Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. One type is annotator bias caused by the subjective perception of the annotators. In this work, we investigate annotator bias using classification models trained on data from demographically distinct annotator groups. To do so, we sample balanced subsets of data that are labeled by demographically distinct annotators. We then train classifiers on these subsets, analyze their performances on similarly grouped test sets, and compare them statistically. Our findings show that the proposed approach successfully identifies bias and that demographic features, such as first language, age, and education, correlate with significant performance differences.",
}

@book{de2011course,
  title={Course in general linguistics},
  author={De Saussure, Ferdinand},
  year={2011},
  publisher={Columbia University Press}
}




@inproceedings{zeinert2021misogyny,
    title = "Annotating Online Misogyny",
    author = "Zeinert, Philine  and
      Inie, Nanna  and
      Derczynski, Leon",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.247",
    doi = "10.18653/v1/2021.acl-long.247",
    pages = "3181--3197",
    abstract = "Online misogyny, a category of online abusive language, has serious and harmful social consequences. Automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse. This paper makes three contributions in this area: Firstly, we describe the detailed design of our iterative annotation process and codebook. Secondly, we present a comprehensive taxonomy of labels for annotating misogyny in natural written language, and finally, we introduce a high-quality dataset of annotated posts sampled from social media posts.",
}

@inproceedings{vidgen2021contextual,
    title = "Introducing {CAD}: the Contextual Abuse Dataset",
    author = "Vidgen, Bertie  and
      Nguyen, Dong  and
      Margetts, Helen  and
      Rossini, Patricia  and
      Tromble, Rebekah",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.182",
    doi = "10.18653/v1/2021.naacl-main.182",
    pages = "2289--2303",
    abstract = "Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets.We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.",
}

@article{jiang2021delphi,
  title={Delphi: Towards Machine Ethics and Norms},
  author={Jiang, Liwei and Hwang, Jena D and Bhagavatula, Chandra and Bras, Ronan Le and Forbes, Maxwell and Borchardt, Jon and Liang, Jenny and Etzioni, Oren and Sap, Maarten and Choi, Yejin},
  journal={arXiv preprint arXiv:2110.07574},
  year={2021}
}


@article{marchal2020polarizing,
  title = {The Polarizing Potential of Intergroup Affect in Online Political Discussions: Evidence From {Reddit} r/{Politics}},
  author = {Marchal, Nahema},
  date = {2020},
  year = {2020},
  journaltitle = {SSRN},
  journal = {SSRN},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3671497&download=yes},
  urldate = {2021-04-09},
  file = {/Users/Paul/Zotero/storage/LRRPD5QJ/papers.html}
}


@article{lee2020biobert,
  title = {{{BioBERT}}: A Pre-Trained Biomedical Language Representation Model for Biomedical Text Mining},
  shorttitle = {{{BioBERT}}},
  author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  date = {2020-02-15},
  year={2020},
  journaltitle = {Bioinformatics},
  journal = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {36},
  pages = {1234--1240},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btz682},
  url = {https://doi.org/10.1093/bioinformatics/btz682},
  urldate = {2021-02-16},
  abstract = {Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.},
  file = {/Users/Paul/Zotero/storage/6QEL945W/Lee et al. - 2020 - BioBERT a pre-trained biomedical language represe.pdf;/Users/Paul/Zotero/storage/VXBGSMVM/5566506.html},
  number = {4}
}

@article{davies2012expanding,
  title={Expanding horizons in historical linguistics with the 400-million word Corpus of Historical {A}merican {E}nglish},
  author={Davies, Mark},
  journal={Corpora},
  volume={7},
  number={2},
  pages={121--157},
  year={2012},
  publisher={Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK}
}

@inproceedings{zheng2021when,
author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R. and Henderson, Peter and Ho, Daniel E.},
title = {When Does Pretraining Help? {A}ssessing Self-Supervised Learning for Law and the {CaseHOLD} Dataset of 53,000+ Legal Holdings},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466088},
doi = {10.1145/3462757.3466088},
abstract = {While self-supervised learning has made rapid advances in natural language processing,
it remains unclear when researchers should engage in resource-intensive domain-specific
pretraining (domain pretraining). The law, puzzlingly, has yielded few documented
instances of substantial gains to domain pretraining in spite of the fact that legal
language is widely seen to be unique. We hypothesize that these existing results stem
from the fact that existing legal NLP tasks are too easy and fail to meet conditions
for when domain pretraining can help. To address this, we first present CaseHOLD (Case
Holdings On Legal Decisions), a new dataset comprised
of over 53,000+ multiple choice questions to identify the relevant holding of a cited
case. This dataset presents a fundamental task to lawyers and is both legally meaningful
and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second,
we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer
architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves
performance, domain pretraining (on a corpus of ≈3.5M decisions across all courts
in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the
most substantial performance gains with CaseHOLD (gain of 7.2% on F1, representing
a 12% improvement on BERT) and consistent performance gains across two other legal
tasks. Third, we show that domain pretraining may be warranted when the task exhibits
sufficient similarity to the pretraining corpus: the level of performance increase
in three legal tasks was directly tied to the domain specificity of the task. Our
findings inform when researchers should engage in resource-intensive pretraining and
show that Transformer-based architectures, too, learn embeddings suggestive of distinct
legal language.},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {159–168},
numpages = {10},
keywords = {benchmark dataset, law, natural language processing, pretraining},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}
    
    
@article{lazaridou2021pitfalls,
  author    = {Angeliki Lazaridou and
               Adhiguna Kuncoro and
               Elena Gribovskaya and
               Devang Agrawal and
               Adam Liska and
               Tayfun Terzi and
               Mai Gimenez and
               Cyprien de Masson d'Autume and
               Sebastian Ruder and
               Dani Yogatama and
               Kris Cao and
               Tom{\'{a}}s Kocisk{\'{y}} and
               Susannah Young and
               Phil Blunsom},
  title     = {Pitfalls of Static Language Modelling},
  journal   = {CoRR},
  volume    = {abs/2102.01951},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.01951},
  archivePrefix = {arXiv},
  eprint    = {2102.01951},
  timestamp = {Tue, 09 Feb 2021 13:35:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-01951.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }


    
    
@inproceedings{jaidka2018diachronic,
  title = {Diachronic Degradation of Language Models: {{Insights}} from Social Media},
  shorttitle = {Diachronic Degradation of Language Models},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Jaidka, Kokil and Chhaya, Niyati and Ungar, Lyle},
  date = {2018-07},
  year = {2018},
  pages = {195--200},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-2032},
  url = {https://www.aclweb.org/anthology/P18-2032},
  urldate = {2021-02-16},
  abstract = {Natural languages change over time because they evolve to the needs of their users and the socio-technological environment. This study investigates the diachronic accuracy of pre-trained language models for downstream tasks in machine learning and user profiling. It asks the question: given that the social media platform and its users remain the same, how is language changing over time? How can these differences be used to track the changes in the affect around a particular topic? To our knowledge, this is the first study to show that it is possible to measure diachronic semantic drifts within social media and within the span of a few years.},
  eventtitle = {{{ACL}} 2018},
  file = {/Users/Paul/Zotero/storage/SND87LP6/Jaidka et al. - 2018 - Diachronic degradation of language models Insight.pdf}
}



    
    
@inproceedings{gururangan2020don,
  title = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  shorttitle = {Don't {{Stop Pretraining}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  date = {2020-07},
  year = {2020},
  pages = {8342--8360},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.740},
  url = {https://www.aclweb.org/anthology/2020.acl-main.740},
  urldate = {2021-02-16},
  abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
  eventtitle = {{{ACL}} 2020},
  file = {/Users/Paul/Zotero/storage/BYE9XGAJ/Gururangan et al. - 2020 - Don't Stop Pretraining Adapt Language Models to D.pdf}
}

@inproceedings{plank2014learning,
    title = "Learning part-of-speech taggers with inter-annotator agreement loss",
    author = "Plank, Barbara  and
      Hovy, Dirk  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E14-1078",
    doi = "10.3115/v1/E14-1078",
    pages = "742--751",
}

@article{paun2018comparingbayesian,
    title = "Comparing {B}ayesian Models of Annotation",
    author = "Paun, Silviu  and
      Carpenter, Bob  and
      Chamberlain, Jon  and
      Hovy, Dirk  and
      Kruschwitz, Udo  and
      Poesio, Massimo",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    url = "https://aclanthology.org/Q18-1040",
    doi = "10.1162/tacl_a_00040",
    pages = "571--585",
    abstract = "The analysis of crowdsourced annotations in natural language processing is concerned with identifying (1) gold standard labels, (2) annotator accuracies and biases, and (3) item difficulties and error patterns. Traditionally, majority voting was used for 1, and coefficients of agreement for 2 and 3. Lately, model-based analysis of corpus annotations have proven better at all three tasks. But there has been relatively little work comparing them on the same datasets. This paper aims to fill this gap by analyzing six models of annotation, covering different approaches to annotator ability, item difficulty, and parameter pooling (tying) across annotators and items. We evaluate these models along four aspects: comparison to gold labels, predictive accuracy for new annotations, annotator characterization, and item difficulty, using four datasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation.",
}

@book{justice2006relevant,
  title={Relevant Linguistics},
  edition = {2nd},
  author={Justice, Paul},
  publisher = {Center for the Study of Language and Information, Stanford University},
  year = {2006}
}

@book{thiroux2015ethics,
  title={Ethics: Theory and Practice},
  edition = {11th},
  author={Thiroux, Jacques P and Krasemann, Keith W},
  year = {2015},
  publisher = {Pearson}
}

@inproceedings{fortuna2019hierarchically,
    title = "A Hierarchically-Labeled {P}ortuguese Hate Speech Dataset",
    author = "Fortuna, Paula  and
      Rocha da Silva, Jo{\~a}o  and
      Soler-Company, Juan  and
      Wanner, Leo  and
      Nunes, S{\'e}rgio",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3510",
    doi = "10.18653/v1/W19-3510",
    pages = "94--104",
    abstract = "Over the past years, the amount of online offensive speech has been growing steadily. To successfully cope with it, machine learning are applied. However, ML-based techniques require sufficiently large annotated datasets. In the last years, different datasets were published, mainly for English. In this paper, we present a new dataset for Portuguese, which has not been in focus so far. The dataset is composed of 5,668 tweets. For its annotation, we defined two different schemes used by annotators with different levels of expertise. Firstly, non-experts annotated the tweets with binary labels ({`}hate{'} vs. {`}no-hate{'}). Secondly, expert annotators classified the tweets following a fine-grained hierarchical multiple label scheme with 81 hate speech categories in total. The inter-annotator agreement varied from category to category, which reflects the insight that some types of hate speech are more subtle than others and that their detection depends on personal perception. This hierarchical annotation scheme is the main contribution of the presented work, as it facilitates the identification of different types of hate speech and their intersections. To demonstrate the usefulness of our dataset, we carried a baseline classification experiment with pre-trained word embeddings and LSTM on the binary classified data, with a state-of-the-art outcome.",
}

@comment{BibDesk Smart Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>0</integer>
				<key>key</key>
				<string>Date-Added</string>
				<key>value</key>
				<string></string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Smart Group</string>
	</dict>
</array>
</plist>
}}


@inproceedings{rodriguez_detecting_2021,
	address = {Cham},
	title = {Detecting {Hate} {Speech} in {Cross}-{Lingual} and {Multi}-lingual {Settings} {Using} {Language} {Agnostic} {Representations}},
	isbn = {978-3-030-93420-0},
	doi = {10.1007/978-3-030-93420-0_8},
	abstract = {The automatic detection of hate speech is a blooming field in the natural language processing community. In recent years there have been efforts in detecting hate speech in multiple languages, using models trained on multiple languages at the same time. Furthermore, there is special interest in the capabilities of language agnostic features to represent text in hate speech detection. This is because models can be trained in multiple languages, and then the capabilities of the model and representation can be tested on a unseen language.},
	language = {en},
	booktitle = {Progress in {Pattern} {Recognition}, {Image} {Analysis}, {Computer} {Vision}, and {Applications}},
	publisher = {Springer International Publishing},
	author = {Rodríguez, Sebastián E. and Allende-Cid, Héctor and Allende, Héctor},
	editor = {Tavares, João Manuel R. S. and Papa, João Paulo and González Hidalgo, Manuel},
	year = {2021},
	pages = {77--87},
	file = {Springer Full Text PDF:/Users/nozza/Zotero/storage/KZ9CL6TQ/Rodríguez et al. - 2021 - Detecting Hate Speech in Cross-Lingual and Multi-l.pdf:application/pdf},
}

@article{pamungkas2021towards,
	title = {Towards multidomain and multilingual abusive language detection: a survey},
	issn = {1617-4917},
	shorttitle = {Towards multidomain and multilingual abusive language detection},
	url = {https://doi.org/10.1007/s00779-021-01609-1},
	doi = {10.1007/s00779-021-01609-1},
	abstract = {Abusive language is an important issue in online communication across different platforms and languages. Having a robust model to detect abusive instances automatically is a prominent challenge. Several studies have been proposed to deal with this vital issue by modeling this task in the cross-domain and cross-lingual setting. This paper outlines and describes the current state of this research direction, providing an overview of previous studies, including the available datasets and approaches employed in both cross-domain and cross-lingual settings. This study also outlines several challenges and open problems of this area, providing insights and a useful roadmap for future work.},
	language = {en},
	urldate = {2022-04-11},
	journal = {Personal and Ubiquitous Computing},
	author = {Pamungkas, Endang Wahyu and Basile, Valerio and Patti, Viviana},
	month = aug,
	year = {2021},
	file = {Springer Full Text PDF:/Users/nozza/Zotero/storage/5RL4E4ST/Pamungkas et al. - 2021 - Towards multidomain and multilingual abusive langu.pdf:application/pdf},
}

@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}

@inproceedings{dinan2022safetykit,
    title = "{S}afety{K}it: First Aid for Measuring Safety in Open-domain Conversational Systems",
    author = "Dinan, Emily  and
      Abercrombie, Gavin  and
      Bergman, A.  and
      Spruit, Shannon  and
      Hovy, Dirk  and
      Boureau, Y-Lan  and
      Rieser, Verena",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.284",
    doi = "10.18653/v1/2022.acl-long.284",
    pages = "4113--4133",
    abstract = "The social impact of natural language processing and its applications has received increasing attention. In this position paper, we focus on the problem of safety for end-to-end conversational AI. We survey the problem landscape therein, introducing a taxonomy of three observed phenomena: the Instigator, Yea-Sayer, and Impostor effects. We then empirically assess the extent to which current tools can measure these effects and current systems display them. We release these tools as part of a {``}first aid kit{''} (SafetyKit) to quickly assess apparent safety concerns. Our results show that, while current tools are able to provide an estimate of the relative safety of systems in various settings, they still have several shortcomings. We suggest several future directions and discuss ethical considerations.",
}

@article{zou2023universal,
  title={Universal and Transferable Adversarial Attacks on Aligned Language Models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@inproceedings{wallace2019universal,
    title = "Universal Adversarial Triggers for Attacking and Analyzing {NLP}",
    author = "Wallace, Eric  and
      Feng, Shi  and
      Kandpal, Nikhil  and
      Gardner, Matt  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1221",
    doi = "10.18653/v1/D19-1221",
    pages = "2153--2162",
    abstract = "Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94{\%} to 0.55{\%}, 72{\%} of {``}why{''} questions in SQuAD to be answered {``}to kill american people{''}, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",
}

@article{arango_cross-lingual_2021,
	title = {Cross-lingual hate speech detection based on multilingual domain-specific word embeddings},
	url = {http://arxiv.org/abs/2104.14728},
	abstract = {Automatic hate speech detection in online social networks is an important open problem in Natural Language Processing (NLP). Hate speech is a multidimensional issue, strongly dependant on language and cultural factors. Despite its relevance, research on this topic has been almost exclusively devoted to English. Most supervised learning resources, such as labeled datasets and NLP tools, have been created for this same language. Considering that a large portion of users worldwide speak in languages other than English, there is an important need for creating efficient approaches for multilingual hate speech detection. In this work we propose to address the problem of multilingual hate speech detection from the perspective of transfer learning. Our goal is to determine if knowledge from one particular language can be used to classify other language, and to determine effective ways to achieve this. We propose a hate specific data representation and evaluate its effectiveness against general-purpose universal representations most of which, unlike our proposed model, have been trained on massive amounts of data. We focus on a cross-lingual setting, in which one needs to classify hate speech in one language without having access to any labeled data for that language. We show that the use of our simple yet specific multilingual hate representations improves classification results. We explain this with a qualitative analysis showing that our specific representation is able to capture some common patterns in how hate speech presents itself in different languages. Our proposal constitutes, to the best of our knowledge, the first attempt for constructing multilingual specific-task representations. Despite its simplicity, our model outperformed the previous approaches for most of the experimental setups. Our findings can orient future solutions toward the use of domain-specific representations.},
	urldate = {2022-04-11},
	journal = {arXiv:2104.14728 [cs]},
	author = {Arango, Aymé and Pérez, Jorge and Poblete, Barbara},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.14728},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/nozza/Zotero/storage/7L6I9DWB/Arango et al. - 2021 - Cross-lingual hate speech detection based on multi.pdf:application/pdf;arXiv.org Snapshot:/Users/nozza/Zotero/storage/4JRQVS3N/2104.html:text/html},
}

@article{mcinnes2017hdbscan,
  title={hdbscan: Hierarchical density based clustering.},
  author={McInnes, Leland and Healy, John and Astels, Steve},
  journal={J. Open Source Softw.},
  volume={2},
  number={11},
  pages={205},
  year={2017}
}

@inproceedings{
bianchi2024safetytuned,
title={Safety-Tuned {LL}a{MA}s: Lessons From Improving the Safety of Large Language Models that Follow Instructions},
author={Federico Bianchi and Mirac Suzgun and Giuseppe Attanasio and Paul Rottger and Dan Jurafsky and Tatsunori Hashimoto and James Zou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gT5hALch9z}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@inproceedings{reimers2019sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

@inproceedings{wang2022expanding,
    title = "Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation",
    author = "Wang, Xinyi  and
      Ruder, Sebastian  and
      Neubig, Graham",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.61",
    doi = "10.18653/v1/2022.acl-long.61",
    pages = "863--877",
    abstract = "The performance of multilingual pretrained models is highly dependent on the availability of monolingual or parallel text present in a target language. Thus, the majority of the world{'}s languages cannot benefit from recent progress in NLP as they have no or limited textual data. To expand possibilities of using NLP technology in these under-represented languages, we systematically study strategies that relax the reliance on conventional language resources through the use of bilingual lexicons, an alternative resource with much better language coverage. We analyze different strategies to synthesize textual or labeled data using lexicons, and how this data can be combined with monolingual or parallel text when available. For 19 under-represented languages across 3 tasks, our methods lead to consistent improvements of up to 5 and 15 points with and without extra monolingual text respectively. Overall, our study highlights how NLP methods can be adapted to thousands more languages that are under-served by current technology.",
}

@inproceedings{wang2021practical,
	address = {Online},
	title = {Practical {Transformer}-based {Multilingual} {Text} {Classification}},
	url = {https://aclanthology.org/2021.naacl-industry.16},
	doi = {10.18653/v1/2021.naacl-industry.16},
	abstract = {Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.},
	urldate = {2022-04-11},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Industry} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Cindy and Banko, Michele},
	month = jun,
	year = {2021},
	pages = {121--129},
	file = {Full Text PDF:/Users/nozza/Zotero/storage/JXB9HA8Z/Wang e Banko - 2021 - Practical Transformer-based Multilingual Text Clas.pdf:application/pdf},
}

@inproceedings{liska2022streamingqa,
  title={Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models},
  author={Liska, Adam and Kocisky, Tomas and Gribovskaya, Elena and Terzi, Tayfun and Sezener, Eren and Agrawal, Devang and Cyprien De Masson, D’Autume and Scholtes, Tim and Zaheer, Manzil and Young, Susannah and others},
  booktitle={International Conference on Machine Learning},
  pages={13604--13622},
  year={2022},
  organization={PMLR}
}

@article{agarwal2022temporal,
  title={Temporal effects on pre-trained models for language processing tasks},
  author={Agarwal, Oshin and Nenkova, Ani},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={904--921},
  year={2022},
  publisher={MIT Press}
}

@inproceedings{jin2022lifelong,
    title = "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
    author = "Jin, Xisen  and
      Zhang, Dejiao  and
      Zhu, Henghui  and
      Xiao, Wei  and
      Li, Shang-Wen  and
      Wei, Xiaokai  and
      Arnold, Andrew  and
      Ren, Xiang",
    booktitle = "Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models",
    month = may,
    year = "2022",
    address = "virtual+Dublin",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bigscience-1.1",
    doi = "10.18653/v1/2022.bigscience-1.1",
    pages = "1--16",
    abstract = "Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a chronologically-ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning). We evaluate PTLM{'}s ability to adapt to new corpora while retaining learned knowledge in earlier corpora. Our experiments show distillation-based approaches to be most effective in retaining downstream performance in earlier domains. The algorithms also improve knowledge transfer, allowing models to achieve better downstream performance over latest data, and improve temporal generalization when distribution gaps exist between training and evaluation because of time. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.",
}

@article{pelicon2021investigating,
	title = {Investigating cross-lingual training for offensive language detection},
	volume = {7},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-559},
	doi = {10.7717/peerj-cs.559},
	abstract = {Platforms that feature user-generated content (social media, online forums, newspaper comment sections etc.) have to detect and filter offensive speech within large, fast-changing datasets. While many automatic methods have been proposed and achieve good accuracies, most of these focus on the English language, and are hard to apply directly to languages in which few labeled datasets exist. Recent work has therefore investigated the use of cross-lingual transfer learning to solve this problem, training a model in a well-resourced language and transferring to a less-resourced target language; but performance has so far been significantly less impressive. In this paper, we investigate the reasons for this performance drop, via a systematic comparison of pre-trained models and intermediate training regimes on five different languages. We show that using a better pre-trained language model results in a large gain in overall performance and in zero-shot transfer, and that intermediate training on other languages is effective when little target-language data is available. We then use multiple analyses of classifier confidence and language model vocabulary to shed light on exactly where these gains come from and gain insight into the sources of the most typical mistakes.},
	language = {en},
	urldate = {2022-04-11},
	journal = {PeerJ Computer Science},
	author = {Pelicon, Andraz and Shekhar, Ravi and Skrlj, Blaz and Purver, Matthew and Pollak, Senja},
	month = jun,
	year = {2021},
	note = {Publisher: PeerJ Inc.},
	pages = {e559},
	file = {Full Text PDF:/Users/nozza/Zotero/storage/GTYA2A5C/Pelicon et al. - 2021 - Investigating cross-lingual training for offensive.pdf:application/pdf;Snapshot:/Users/nozza/Zotero/storage/QCZNW8ZJ/cs-559.html:text/html},
}

@inproceedings{corazza_hybrid_2020,
	address = {Online},
	title = {Hybrid {Emoji}-{Based} {Masked} {Language} {Models} for {Zero}-{Shot} {Abusive} {Language} {Detection}},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.84},
	doi = {10.18653/v1/2020.findings-emnlp.84},
	abstract = {Recent studies have demonstrated the effectiveness of cross-lingual language model pretraining on different NLP tasks, such as natural language inference and machine translation. In our work, we test this approach on social media data, which are particularly challenging to process within this framework, since the limited length of the textual messages and the irregularity of the language make it harder to learn meaningful encodings. More speciﬁcally, we propose a hybrid emoji-based Masked Language Model (MLM) to leverage the common information conveyed by emojis across different languages and improve the learned cross-lingual representation of short text messages, with the goal to perform zeroshot abusive language detection. We compare the results obtained with the original MLM to the ones obtained by our method, showing improved performance on German, Italian and Spanish.},
	language = {en},
	urldate = {2022-04-11},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Corazza, Michele and Menini, Stefano and Cabrio, Elena and Tonelli, Sara and Villata, Serena},
	year = {2020},
	pages = {943--949},
	file = {Corazza et al. - 2020 - Hybrid Emoji-Based Masked Language Models for Zero.pdf:/Users/nozza/Zotero/storage/LKHRXK8J/Corazza et al. - 2020 - Hybrid Emoji-Based Masked Language Models for Zero.pdf:application/pdf},
}

@article{mollas2022ethos,
  title={ETHOS: a multi-label hate speech detection dataset},
  author={Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios},
  journal={Complex \& Intelligent Systems},
  pages={1--16},
  year={2022},
  publisher={Springer}
}

@article{krishnan_multilingual_2021,
	title = {Multilingual {Code}-{Switching} for {Zero}-{Shot} {Cross}-{Lingual} {Intent} {Prediction} and {Slot} {Filling}},
	url = {http://arxiv.org/abs/2103.07792},
	abstract = {Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding (NLU). In the context of zero-shot learning, this task is typically approached by either using representations from pre-trained multilingual transformers such as mBERT, or by machine translating the source data into the known target language and then fine-tuning. Our work focuses on a particular scenario where the target language is unknown during training. To this goal, we propose a novel method to augment the monolingual source data using multilingual code-switching via random translations to enhance a transformer's language neutrality when fine-tuning it for a downstream task. This method also helps discover novel insights on how code-switching with different language families around the world impact the performance on the target language. Experiments on the benchmark dataset of MultiATIS++ yielded an average improvement of +4.2\% in accuracy for intent task and +1.8\% in F1 for slot task using our method over the state-of-the-art across 8 different languages. Furthermore, we present an application of our method for crisis informatics using a new human-annotated tweet dataset of slot filling in English and Haitian Creole, collected during Haiti earthquake disaster.},
	urldate = {2022-04-11},
	journal = {arXiv:2103.07792 [cs]},
	author = {Krishnan, Jitin and Anastasopoulos, Antonios and Purohit, Hemant and Rangwala, Huzefa},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.07792},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nozza/Zotero/storage/IFATANPK/Krishnan et al. - 2021 - Multilingual Code-Switching for Zero-Shot Cross-Li.pdf:application/pdf;arXiv.org Snapshot:/Users/nozza/Zotero/storage/TIPQHUV2/2103.html:text/html},
}

@incollection{waltman2017understanding,
  title={Understanding hate speech},
  author={Waltman, Michael S and Mattheis, Ashely A},
  booktitle={Oxford research encyclopedia of communication},
  year={2017},
  publisher = {Oxford University Press}
}

@inproceedings{manerba2021abusechecklist,
    title = "Fine-Grained Fairness Analysis of Abusive Language Detection Systems with {C}heck{L}ist",
    author = "Manerba, Marta Marchiori  and
      Tonelli, Sara",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.9",
    doi = "10.18653/v1/2021.woah-1.9",
    pages = "81--91",
    abstract = "Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender. This is a crucial issue, which may harm minorities and underrepresented groups if such systems were integrated in real-world applications. In this paper, we create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English. We compare the behaviour of two BERT-based models, one trained on a generic hate speech dataset and the other on a dataset for misogyny detection. Our evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation. We release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList.",
}

@article{pamungkas2021joint,
	title = {A joint learning approach with knowledge injection for zero-shot cross-lingual hate speech detection},
	volume = {58},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457321000510},
	doi = {10.1016/j.ipm.2021.102544},
	abstract = {Hate speech is an increasingly important societal issue in the era of digital communication. Hateful expressions often make use of figurative language and, although they represent, in some sense, the dark side of language, they are also often prime examples of creative use of language. While hate speech is a global phenomenon, current studies on automatic hate speech detection are typically framed in a monolingual setting. In this work, we explore hate speech detection in low-resource languages by transferring knowledge from a resource-rich language, English, in a zero-shot learning fashion. We experiment with traditional and recent neural architectures, and propose two joint-learning models, using different multilingual language representations to transfer knowledge between pairs of languages. We also evaluate the impact of additional knowledge in our experiment, by incorporating information from a multilingual lexicon of abusive words. The results show that our joint-learning models achieve the best performance on most languages. However, a simple approach that uses machine translation and a pre-trained English language model achieves a robust performance. In contrast, Multilingual BERT fails to obtain a good performance in cross-lingual hate speech detection. We also experimentally found that the external knowledge from a multilingual abusive lexicon is able to improve the models’ performance, specifically in detecting the positive class. The results of our experimental evaluation highlight a number of challenges and issues in this particular task. One of the main challenges is related to the issue of current benchmarks for hate speech detection, in particular how bias related to the topical focus in the datasets influences the classification performance. The insufficient ability of current multilingual language models to transfer knowledge between languages in the specific hate speech detection task also remain an open problem. However, our experimental evaluation and our qualitative analysis show how the explicit integration of linguistic knowledge from a structured abusive language lexicon helps to alleviate this issue.},
	language = {en},
	number = {4},
	urldate = {2022-04-11},
	journal = {Information Processing \& Management},
	author = {Pamungkas, Endang Wahyu and Basile, Valerio and Patti, Viviana},
	month = jul,
	year = {2021},
	keywords = {Cross-lingual classification, Hate speech detection, Social media, Transfer learning, Zero-shot learning},
	pages = {102544},
	file = {ScienceDirect Snapshot:/Users/nozza/Zotero/storage/CAREDVYT/S0306457321000510.html:text/html},
}

@inproceedings{bigoulaeva_cross-lingual_2021,
	address = {Kyiv},
	title = {Cross-{Lingual} {Transfer} {Learning} for {Hate} {Speech} {Detection}},
	url = {https://aclanthology.org/2021.ltedi-1.3},
	abstract = {We address the task of automatic hate speech detection for low-resource languages. Rather than collecting and annotating new hate speech data, we show how to use cross-lingual transfer learning to leverage already existing data from higher-resource languages. Using bilingual word embeddings based classifiers we achieve good performance on the target language by training only on the source dataset. Using our transferred system we bootstrap on unlabeled target language data, improving the performance of standard cross-lingual transfer approaches. We use English as a high resource language and German as the target language for which only a small amount of annotated corpora are available. Our results indicate that cross-lingual transfer learning together with our approach to leverage additional unlabeled data is an effective way of achieving good performance on low-resource target languages without the need for any target-language annotations.},
	urldate = {2022-04-11},
	booktitle = {Proceedings of the {First} {Workshop} on {Language} {Technology} for {Equality}, {Diversity} and {Inclusion}},
	publisher = {Association for Computational Linguistics},
	author = {Bigoulaeva, Irina and Hangya, Viktor and Fraser, Alexander},
	month = apr,
	year = {2021},
	pages = {15--25},
	file = {Full Text PDF:/Users/nozza/Zotero/storage/XJ9RDKIN/Bigoulaeva et al. - 2021 - Cross-Lingual Transfer Learning for Hate Speech De.pdf:application/pdf},
}

@inproceedings{glavas_xhate-999_2020,
	address = {Barcelona, Spain (Online)},
	title = {{XHate}-999: {Analyzing} and {Detecting} {Abusive} {Language} {Across} {Domains} and {Languages}},
	shorttitle = {{XHate}-999},
	url = {https://www.aclweb.org/anthology/2020.coling-main.559},
	doi = {10.18653/v1/2020.coling-main.559},
	abstract = {We present XHATE-999, a multi-domain and multilingual evaluation data set for abusive language detection. By aligning test instances across six typologically diverse languages, XHATE-999 for the ﬁrst time allows for disentanglement of the domain transfer and language transfer effects in abusive language detection. We conduct a series of domain- and language-transfer experiments with state-of-the-art monolingual and multilingual transformer models, setting strong baseline results and proﬁling XHATE-999 as a comprehensive evaluation resource for abusive language detection. Finally, we show that domain- and language-adaptation, via intermediate masked language modeling on abusive corpora in the target language, can lead to substantially improved abusive language detection in the target language in the zero-shot transfer setups.},
	language = {en},
	urldate = {2022-04-11},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Glavaš, Goran and Karan, Mladen and Vulić, Ivan},
	year = {2020},
	pages = {6350--6365},
	file = {Glavaš et al. - 2020 - XHate-999 Analyzing and Detecting Abusive Languag.pdf:/Users/nozza/Zotero/storage/RY45P78A/Glavaš et al. - 2020 - XHate-999 Analyzing and Detecting Abusive Languag.pdf:application/pdf},
}

@article{poletto2021resources,
	title = {Resources and benchmark corpora for hate speech detection: a systematic review},
	volume = {55},
	issn = {1574-0218},
	shorttitle = {Resources and benchmark corpora for hate speech detection},
	url = {https://doi.org/10.1007/s10579-020-09502-8},
	doi = {10.1007/s10579-020-09502-8},
	abstract = {Hate Speech in social media is a complex phenomenon, whose detection has recently gained significant traction in the Natural Language Processing community, as attested by several recent review works. Annotated corpora and benchmarks are key resources, considering the vast number of supervised approaches that have been proposed. Lexica play an important role as well for the development of hate speech detection systems. In this review, we systematically analyze the resources made available by the community at large, including their development methodology, topical focus, language coverage, and other factors. The results of our analysis highlight a heterogeneous, growing landscape, marked by several issues and venues for improvement.},
	language = {en},
	number = {2},
	urldate = {2022-04-11},
	journal = {Language Resources and Evaluation},
	author = {Poletto, Fabio and Basile, Valerio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana},
	month = jun,
	year = {2021},
	pages = {477--523},
	file = {Springer Full Text PDF:/Users/nozza/Zotero/storage/Z3Z8E9KY/Poletto et al. - 2021 - Resources and benchmark corpora for hate speech de.pdf:application/pdf},
}

@misc{noauthor_directions_nodate,
	title = {Directions in abusive language training data, a systematic review: {Garbage} in, garbage out},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0243300},
	urldate = {2022-04-11},
	file = {Directions in abusive language training data, a systematic review\: Garbage in, garbage out:/Users/nozza/Zotero/storage/F9BVRQNB/article.html:text/html},
}

@inproceedings{akhtar_new_2019,
	address = {Cham},
	title = {A {New} {Measure} of {Polarization} in the {Annotation} of {Hate} {Speech}},
	isbn = {978-3-030-35166-3},
	doi = {10.1007/978-3-030-35166-3_41},
	abstract = {The number of social media users is ever-increasing. Unfortunately, this has also resulted in the massive rise of uncensored online hate against vulnerable communities such as immigrants, LGBT and women. Current work on the automatic detection of various forms of hate speech (HS) typically employs supervised learning, requiring manually annotated data. The highly polarizing nature of the topics involved raises concerns about the quality of annotations these systems rely on, because not all the annotators are equally sensitive to different kinds of hate speech. We propose an approach to leverage the fine-grained knowledge expressed by individual annotators, before their subjectivity is averaged out by the gold standard creation process. This helps us to refine the quality of training sets for hate speech detection. We introduce a measure of polarization at the level of single instances in the data to manipulate the training set and reduce the impact of most polarizing text on the learning process.},
	language = {en},
	booktitle = {{AI}*{IA} 2019 – {Advances} in {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Akhtar, Sohail and Basile, Valerio and Patti, Viviana},
	editor = {Alviano, Mario and Greco, Gianluigi and Scarcello, Francesco},
	year = {2019},
	pages = {588--603},
	file = {Springer Full Text PDF:/Users/nozza/Zotero/storage/Z3QWUUKF/Akhtar et al. - 2019 - A New Measure of Polarization in the Annotation of.pdf:application/pdf},
}

@InProceedings{germeval18,
  author = 	"Wiegand, Michael
		and Siegel, Melanie
		and Ruppenhofer, Josef",
  title = 	"{Overview of the GermEval 2018 Shared Task on the Identification of Offensive Language}",
  booktitle = 	"Proceedings of GermEval 2018, 14th Conference on Natural Language Processing (KONVENS 2018)",
  year = 	"2018",
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{wang2023self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2023}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{gudibande2023false,
  title={The false promise of imitating proprietary llms},
  author={Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
  journal={arXiv preprint arXiv:2305.15717},
  year={2023}
}

@inproceedings{Fersini2018OverviewOT,
  title={Overview of the {EVALITA} 2018 task on Automatic Misogyny Identification ({AMI})},
  author={Fersini, Elisabetta and Nozza, Debora and Rosso, Paolo},
  journal={Proceedings of the 6th evaluation campaign of Natural Language Processing and Speech tools for Italian (EVALITA 2018)},
  volume={12},
  pages={59},
  year={2018},
  url={http://ceur-ws.org/Vol-2263/paper009.pdf},
publisher = {CEUR.org}, 
address = {Turin, Italy} 
}

@InProceedings{fersini2020ami, 
author = {Fersini, Elisabetta and Nozza, Debora and Rosso, Paolo}, 
title = {{AMI @ EVALITA2020}: Automatic Misogyny Identification}, 
booktitle = {{Proceedings of the 7th evaluation campaign of Natural Language Processing and Speech tools for Italian (EVALITA 2020)}}, 
url = {http://ceur-ws.org/Vol-2765/paper161.pdf},
year = {2020}, 
publisher = {CEUR.org}, 
address = {Online} 
}

@inproceedings{hala2021armi,
author = {Mulki, Hala and Ghanem, Bilal},
title = {Working Notes of the Workshop Arabic Misogyny Identification (ArMI-2021)},
year = {2021},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503178},
doi = {10.1145/3503162.3503178},
booktitle = {Forum for Information Retrieval Evaluation},
pages = {7–8},
numpages = {2},
keywords = {Social media, Arabic language, Misogyny identification},
location = {Virtual Event, India},
series = {FIRE 2021}
}

@inproceedings{poletto2017hate,
  title={Hate speech annotation: Analysis of an italian twitter corpus},
  author={Poletto, Fabio and Stranisci, Marco and Sanguinetti, Manuela and Patti, Viviana and Bosco, Cristina},
  booktitle={4th Italian Conference on Computational Linguistics, CLiC-it 2017},
  volume={2006},
  pages={1--6},
  year={2017},
  organization={CEUR-WS}
}

@inproceedings{sanguinetti2020haspeede,
  author    = {Manuela Sanguinetti and
               Gloria Comandini and
               Elisa Di Nuovo and
               Simona Frenda and
               Marco Stranisci and
               Cristina Bosco and
               Tommaso Caselli and
               Viviana Patti and
               Irene Russo},
  title     = {HaSpeeDe 2 @ {EVALITA2020:} Overview of the {EVALITA} 2020 Hate Speech
               Detection Task},
  booktitle = {Proceedings of the Seventh Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop {(EVALITA}
               2020), Online event, December 17th, 2020},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {2765},
  publisher = {CEUR-WS.org},
  year      = {2020},
  url       = {http://ceur-ws.org/Vol-2765/paper162.pdf},
  timestamp = {Wed, 16 Dec 2020 16:53:24 +0100},
  biburl    = {https://dblp.org/rec/conf/evalita/SanguinettiCNFS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{hayes2007using,
  title={Using heteroskedasticity-consistent standard error estimators in {OLS} regression: An introduction and software implementation},
  author={Hayes, Andrew F and Cai, Li},
  journal={Behavior research methods},
  volume={39},
  number={4},
  pages={709--722},
  year={2007},
  publisher={Springer},
  url={https://doi.org/10.3758/BF03192961}
}

@InProceedings{souza2020bertimbau,
    author="Souza, F{\'a}bio and Nogueira, Rodrigo and Lotufo, Roberto",
    editor="Cerri, Ricardo and Prati, Ronaldo C.",
    title="{BERTimbau}: Pretrained {BERT} Models for {Brazilian Portuguese}",
    booktitle="Intelligent Systems",
    year="2020",
    publisher="Springer International Publishing",
    address="Cham",
    pages="403--417",
    isbn="978-3-030-61377-8",
    url={https://doi.org/10.1007/978-3-030-61377-8\_28},
}

@inproceedings{barbieri2022xlmt,
    title = "{XLM}-{T}: Multilingual Language Models in {T}witter for Sentiment Analysis and Beyond",
    author = "Barbieri, Francesco  and
      Espinosa Anke, Luis  and
      Camacho-Collados, Jose",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.27",
    pages = "258--266",
    abstract = "Language models are ubiquitous in current NLP, and their multilingual capacity has recently attracted considerable attention. However, current analyses have almost exclusively focused on (multilingual variants of) standard benchmarks, and have relied on clean pre-training and task-specific corpora as multilingual signals. In this paper, we introduce XLM-T, a model to train and evaluate multilingual language models in Twitter. In this paper we provide: (1) a new strong multilingual baseline consisting of an XLM-R (Conneau et al. 2020) model pre-trained on millions of tweets in over thirty languages, alongside starter code to subsequently fine-tune on a target task; and (2) a set of unified sentiment analysis Twitter datasets in eight different languages and a XLM-T model trained on this dataset.",
}

@inproceedings{liu2014combining,
  title={Combining N-gram based Similarity Analysis with Sentiment Analysis in Web Content Classification.},
  author={Liu, Shuhua and Forss, Thomas},
  booktitle={KDIR},
  pages={530--537},
  year={2014}
}


@inproceedings{gomez2020exploring,
  title={Exploring Hate Speech Detection in Multimodal Publications},
  author={Gomez, Raul and Gibert, Jaume and Gomez, Lluis and Karatzas, Dimosthenis},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  pages={1470--1478},
  year={2020}
}

@inproceedings{yang2019exploring,
    title = "Exploring Deep Multimodal Fusion of Text and Photo for Hate Speech Classification",
    author = "Yang, Fan  and
      Peng, Xiaochang  and
      Ghosh, Gargi  and
      Shilon, Reshef  and
      Ma, Hao  and
      Moore, Eider  and
      Predovic, Goran",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3502",
    doi = "10.18653/v1/W19-3502",
    pages = "11--18",
    abstract = "Interactions among users on social network platforms are usually positive, constructive and insightful. However, sometimes people also get exposed to objectionable content such as hate speech, bullying, and verbal abuse etc. Most social platforms have explicit policy against hate speech because it creates an environment of intimidation and exclusion, and in some cases may promote real-world violence. As users{'} interactions on today{'}s social networks involve multiple modalities, such as texts, images and videos, in this paper we explore the challenge of automatically identifying hate speech with deep multimodal technologies, extending previous research which mostly focuses on the text signal alone. We present a number of fusion approaches to integrate text and photo signals. We show that augmenting text with image embedding information immediately leads to a boost in performance, while applying additional attention fusion methods brings further improvement.",
}

@incollection{verschueren2005pragmatics,
  title={Pragmatics},
  author={Verschueren, Jef},
  booktitle={The Routledge Companion to Semiotics and Linguistics},
  pages={99--110},
  year={2005},
  publisher={Routledge}
}

@book{aitchison2001language,
  title={Language change: Progress or decay?},
  author={Aitchison, Jean},
  year={2001},
  publisher={Cambridge university press}
}

@inproceedings{eisenstein2013bad,
  title={What to do about bad language on the internet},
  author={Eisenstein, Jacob},
  booktitle={Proceedings of the 2013 conference of the North American Chapter of the association for computational linguistics: Human language technologies},
  pages={359--369},
  year={2013}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{perez2021robertuito,
    title = "{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish",
    author = "P{\'e}rez, Juan Manuel  and
      Furman, Dami{\'a}n Ariel  and
      Alonso Alemany, Laura  and
      Luque, Franco M.",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.785",
    pages = "7235--7243",
    abstract = "Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.",
}

@inproceedings{antoun2020arabert,
    title = "{A}ra{BERT}: Transformer-based Model for {A}rabic Language Understanding",
    author = "Antoun, Wissam  and
      Baly, Fady  and
      Hajj, Hazem",
    booktitle = "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resource Association",
    url = "https://aclanthology.org/2020.osact-1.2",
    pages = "9--15",
    abstract = "The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.",
    language = "English",
    ISBN = "979-10-95546-51-1",
}

@article{stappen2020crosslingual,
  author    = {Lukas Stappen and
               Fabian Brunn and
               Bj{\"{o}}rn W. Schuller},
  title     = {Cross-lingual Zero- and Few-shot Hate Speech Detection Utilising Frozen
               Transformer Language Models and {AXEL}},
  journal   = {CoRR},
  volume    = {abs/2004.13850},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.13850},
  archivePrefix = {arXiv},
  eprint    = {2004.13850},
  timestamp = {Sat, 02 May 2020 19:17:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-13850.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{aluru2020multi,
author = {Aluru, Sai Saketh and Mathew, Binny and Saha, Punyajoy and Mukherjee, Animesh},
title = {A Deep Dive into Multilingual Hate Speech Classification},
year = {2020},
isbn = {978-3-030-67669-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67670-4_26},
doi = {10.1007/978-3-030-67670-4_26},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science and Demo Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part V},
pages = {423–439},
numpages = {17},
keywords = {Hate speech, Embeddings, BERT, Multilingual, Classification},
location = {Ghent, Belgium}
}

@inproceedings{bosco2018overview,
  title={Overview of the evalita 2018 hate speech detection task},
  author={Bosco, Cristina and Felice, Dell'Orletta and Poletto, Fabio and Sanguinetti, Manuela and Maurizio, Tesconi},
  booktitle={EVALITA 2018-Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian},
  volume={2263},
  pages={1--9},
  year={2018},
  organization={CEUR}
}

@misc{feng2020agnostic,
  doi = {10.48550/ARXIV.2007.01852},
  url = {https://arxiv.org/abs/2007.01852},
  author = {Feng, Fangxiaoyu and Yang, Yinfei and Cer, Daniel and Arivazhagan, Naveen and Wang, Wei},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language-agnostic BERT Sentence Embedding},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{capozzi2019computational,
  title={Computational linguistics against hate: Hate speech detection and visualization on social media in the" Contro L’Odio" project},
  author={Capozzi, Arthur TE and Lai, Mirko and Basile, Valerio and Poletto, Fabio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana and Ruffo, Giancarlo and Musto, Cataldo and Polignano, Marco and others},
  booktitle={6th Italian Conference on Computational Linguistics, CLiC-it 2019},
  volume={2481},
  pages={1--6},
  year={2019},
  organization={CEUR-WS}
}

@inproceedings{leite2020toxic,
    title = "Toxic Language Detection in Social Media for {B}razilian {P}ortuguese: New Dataset and Multilingual Analysis",
    author = "Leite, Jo{\~a}o Augusto  and
      Silva, Diego  and
      Bontcheva, Kalina  and
      Scarton, Carolina",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-main.91",
    pages = "914--924",
}

@article{lorenz2022systematic,
  title={A systematic review of worldwide causal and correlational evidence on digital media and democracy},
  author={Lorenz-Spreen, Philipp and Oswald, Lisa and Lewandowsky, Stephan and Hertwig, Ralph},
  journal={Nature human behaviour},
  pages={1--28},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{kirk2022hatemoji,
    title = "{H}atemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-Based Hate",
    author = "Kirk, Hannah  and
      Vidgen, Bertie  and
      Rottger, Paul  and
      Thrush, Tristan  and
      Hale, Scott",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.97",
    doi = "10.18653/v1/2022.naacl-main.97",
    pages = "1352--1368",
    abstract = "Detecting online hate is a complex task, and low-performing models have harmful consequences when used for sensitive applications such as content moderation. Emoji-based hate is an emerging challenge for automated detection. We present HatemojiCheck, a test suite of 3,930 short-form statements that allows us to evaluate performance on hateful language expressed with emoji. Using the test suite, we expose weaknesses in existing hate detection models. To address these weaknesses, we create the HatemojiBuild dataset using a human-and-model-in-the-loop approach. Models built with these 5,912 adversarial examples perform substantially better at detecting emoji-based hate, while retaining strong performance on text-only hate. Both HatemojiCheck and HatemojiBuild are made publicly available.",
}

@inproceedings{nozza2021exposing,
    title = "Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection",
    author = "Nozza, Debora",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.114",
    doi = "10.18653/v1/2021.acl-short.114",
    pages = "907--914"
}

@article{mehrabi2023flirt,
  title={FLIRT: Feedback Loop In-context Red Teaming},
  author={Mehrabi, Ninareh and Goyal, Palash and Dupuy, Christophe and Hu, Qian and Ghosh, Shalini and Zemel, Richard and Chang, Kai-Wei and Galstyan, Aram and Gupta, Rahul},
  journal={arXiv preprint arXiv:2308.04265},
  year={2023}
}

@inproceedings{ousidhoum2021probing,
    title = "Probing Toxic Content in Large Pre-Trained Language Models",
    author = "Ousidhoum, Nedjma  and
      Zhao, Xinran  and
      Fang, Tianqing  and
      Song, Yangqiu  and
      Yeung, Dit-Yan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.329",
    doi = "10.18653/v1/2021.acl-long.329",
    pages = "4262--4274",
}

@inproceedings{nozza2021honest,
    title = "{HONEST}: Measuring Hurtful Sentence Completion in Language Models",
    author = "Nozza, Debora  and
      Bianchi, Federico  and
      Hovy, Dirk",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.191",
    doi = "10.18653/v1/2021.naacl-main.191",
    pages = "2398--2406",
}

@inproceedings{rottger2022data,
    title = "Data-Efficient Strategies for Expanding Hate Speech Detection into Under-Resourced Languages",
    author = {R{\"o}ttger, Paul  and
      Nozza, Debora  and
      Bianchi, Federico  and
      Hovy, Dirk},
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.383",
    pages = "5674--5691",
    abstract = "Hate speech is a global phenomenon, but most hate speech datasets so far focus on English-language content. This hinders the development of more effective hate speech detection models in hundreds of languages spoken by billions across the world. More data is needed, but annotating hateful content is expensive, time-consuming and potentially harmful to annotators. To mitigate these issues, we explore data-efficient strategies for expanding hate speech detection into under-resourced languages. In a series of experiments with mono- and multilingual models across five non-English languages, we find that 1) a small amount of target-language fine-tuning data is needed to achieve strong performance, 2) the benefits of using more such data decrease exponentially, and 3) initial fine-tuning on readily-available English data can partially substitute target-language data and improve model generalisability. Based on these findings, we formulate actionable recommendations for hate speech detection in low-resource language settings.",
}

@article{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  journal={arXiv preprint arXiv:2304.03442},
  year={2023}
}

@article{lyu2024beyond,
  title={Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models},
  author={Lyu, Chenyang and Wu, Minghao and Aji, Alham Fikri},
  journal={arXiv preprint arXiv:2402.13887},
  year={2024}
}

@inproceedings{
zheng2024large,
title={Large Language Models Are Not Robust Multiple Choice Selectors},
author={Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=shr9PXz7T0}
}

@article{sorensen2024roadmap,
  title={A Roadmap to Pluralistic Alignment},
  author={Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others},
  journal={arXiv preprint arXiv:2402.05070},
  year={2024}
}

@article{rottger2024political,
  title={Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models},
  author={R{\"o}ttger, Paul and Hofmann, Valentin and Pyatkin, Valentina and Hinck, Musashi and Kirk, Hannah Rose and Sch{\"u}tze, Hinrich and Hovy, Dirk},
  journal={arXiv preprint arXiv:2402.16786},
  year={2024}
}

@inproceedings{santurkar2023opinionqa,
  title={Whose opinions do language models reflect?},
  author={Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  booktitle={International Conference on Machine Learning},
  pages={29971--30004},
  year={2023},
  organization={PMLR}
}

@techreport{horton2023large,
  title={Large language models as simulated economic agents: What can we learn from homo silicus?},
  author={Horton, John J},
  year={2023},
  institution={National Bureau of Economic Research}
}

@inproceedings{basile2019semeval,
    title = "{S}em{E}val-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in {T}witter",
    author = "Basile, Valerio  and
      Bosco, Cristina  and
      Fersini, Elisabetta  and
      Nozza, Debora  and
      Patti, Viviana  and
      Rangel Pardo, Francisco Manuel  and
      Rosso, Paolo  and
      Sanguinetti, Manuela",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S19-2007",
    doi = "10.18653/v1/S19-2007",
    pages = "54--63",
    abstract = "The paper describes the organization of the SemEval 2019 Task 5 about the detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter. The task is organized in two related classification subtasks: a main binary subtask for detecting the presence of hate speech, and a finer-grained one devoted to identifying further features in hateful contents such as the aggressive attitude and the target harassed, to distinguish if the incitement is against an individual rather than a group. HatEval has been one of the most popular tasks in SemEval-2019 with a total of 108 submitted runs for Subtask A and 70 runs for Subtask B, from a total of 74 different teams. Data provided for the task are described by showing how they have been collected and annotated. Moreover, the paper provides an analysis and discussion about the participant systems and the results they achieved in both subtasks.",
}

@inproceedings{
aghajanyan2021better,
title={Better Fine-Tuning by Reducing Representational Collapse},
author={Armen Aghajanyan and Akshat Shrivastava and Anchit Gupta and Naman Goyal and Luke Zettlemoyer and Sonal Gupta},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=OQ08SN70M1V}
}
