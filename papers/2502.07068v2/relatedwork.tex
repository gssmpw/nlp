\section{Related Work}
% Paul

\paragraph{LLM Simulations}
Collecting human response data is one of the most challenging and costly aspects of social science research \cite{argyle2023out,hewitt2024predicting}.
Consequently, much prior work has investigated the extent to which LLMs can accurately simulate human responses in surveys and experimental settings.
Most prominently, \citet{argyle2023out},  \citet{horton2023large} and \citet{aher2023using} all found evidence of LLMs providing reasonably accurate group-level simulations in behavioral science and economics experiments as well as for US political surveys.
Some follow-up work has highlighted biases and conceptual challenges in such simulations \citep{bisbee2023synthetic,bail2024can,park2024diminished,kozlowski2024simulating}.
Other work has explored prompting strategies and frameworks for improving simulation accuracy \citep{kwok2024evaluating,manning2024automated,sun2024random}.
Relatedly, several works have used survey questionnaires, including the WVS used in this paper, with the goal of \textit{evaluating} values and opinions reflected in LLMs rather than simulating human survey responses \citep[][inter alia]{benkler2023assessing,arora-etal-2023-probing,cao-etal-2023-assessing,alkhamissi-etal-2024-investigating,zhao2024worldvaluesbench,wright2024revealingfinegrainedvaluesopinions}.
In contrast to these efforts, our focus is on \textit{specializing} LLMs to simulate group-level survey response distribution, which could aid in survey data collection.
While we do measure the performance of LLMs out of the box (\textit{zero-shot}), our main goal is to investigate the extent to which we can \textit{improve} LLM performance on simulating group-level survey response distributions through fine-tuning, and explore their potential as specialized tools for social science research.

%\cite{doi:10.1073/pnas.2314021121}
% https://arxiv.org/abs/2408.16482

\paragraph{Distribution Simulation as Calibration.}
% Multiple Choice Questions (MCQs) are often used to evaluate LLMs \cite{hendrycks2020measuring,srivastava2023imitation,zhong2023agieval}. Unlike in surveys, a single correct answer is often assumed in MCQs. %\citet{zheng2024large} show that LLMs suffer from behavior bias when option position changes with first-token evaluation. \citet{wang2024look} suggest that first-token evaluation is sensitive to prompt format and often does not align with full-text model output.
\textit{Calibration} is aligning classifier predictive probabilities with the classification uncertainty. While most work focuses on majority class accuracy \cite{li-etal-2024-multiple,he2024investigating}, this is problematic when human label variation is substantial \cite{baan-etal-2022-stop,baan-etal-2024-interpreting}, and \textit{human calibration} should consider the full human judgment distribution.
% % metrics for measuring calibration in this case, particularly Human Entropy Calibration Error, which captures the alignment between disagreement among humans and a model’s indecisiveness on the instance level; Human Ranking Calibration Score, a global measure that can be viewed as a stricter alternative to majority vote accuracy; and Human Distribution Calibration Error, a statistical distance metric based on total variation distance (TVD). We opt for Jensen-Shannon Distance (JSD) and Earth-Moved Distance (EMD) instead as adopted by \citet{durmus2023globalopinionqa} and \citet{zhao-etal-2024-worldvaluesbench-large} respectively, for comparing model predictions to WVS samples. \citet{ulmer2024calibrating} propose a novel calibration technique for LLMs called APRICOT, which, instead of directly tuning the LLM's weights, trains an auxiliary classifier (based on a small pre-trained LM) to predict confidence scores based on the LLM's full text decoded output.
Our task can therefore be viewed as \textit{human calibration} for multiple-choice surveys, as opposed to many previous studies, which focused on accuracy measured against the majority answer \cite{arora-etal-2023-probing, cao-etal-2023-assessing, alkhamissi-etal-2024-investigating}.

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.7\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/wvs_geo.png}
%         \caption{Country distribution across training, validation, and testing sets.}  % 子图标题
%         \label{fig:country_split}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.29\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/wvs_sunburst.png}
%         \caption{Cultural dimension distribution.}  % 子图标题
%         \label{fig:question_split}
%     \end{subfigure}
%     \caption{Visualization of country and cultural dimension divisions of Wvs.\ Countries are categorized into three groups, and questions are divided based on selected cultural dimensions. Further details are shown in the Appendix.}
%     \label{fig:wvs_distritbuion}
% \end{figure*}

\begin{table}[t]
\centering
\resizebox{0.45\textwidth}{!}{%
\small
\begin{tabular}{@{}p{0.11\textwidth} p{0.3\textwidth}@{}}
\toprule
\textbf{Instruction} & How would someone from Andorra answer the following question: \\
\midrule
\textbf{Input} & How interested would you say you are in politics? Here are the options: \\ 
\midrule
\textbf{Options} & \text{(A) Very interested} \\
                 & \text{(B) Somewhat interested} \\
                 & \text{(C) Not very interested} \\
                 & \text{(D) Not at all interested} \\
\midrule
\textbf{Format} & If had to select one of the options, my answer would be ([A/B/...]) \\
\midrule
\textbf{Options} & \text{(A) 15.16\%} \quad \text{(B) 29.02\%}  \\ 
\textbf{Distribution}& \text{(C) 28.31\%} \quad  \text{(D) 27.51\%} \\           \bottomrule
\end{tabular}}
\caption{\textbf{Example entry} from our formatted WVS dataset for the country of Andorra.}
\label{tb:questionnaire_example}
\end{table}