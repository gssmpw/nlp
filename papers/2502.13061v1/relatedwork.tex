\section{Related Work}
%\subsection{Hateful Memes Detection}
%The Hateful Memes Challenge competition \cite{KielaFBHMC2020} released a benchmark dataset for hateful meme detection. 
%The best baseline model from the challenge is the Visual BERT, achieving an AUROC of 75.4. The prize-winning solution \cite{RonHMC1st2020, VilioHMC2nd2020, RizaHMC3rd2020} of the challenge managed to push the AUROC to 84.5 with additional extracted features and ensembling of models.
\subsection{Hateful Meme Detection}
Most existing approaches to hateful meme detection use supervised learning. Early systems~\cite{RonHMC1st2020, VilioHMC2nd2020} fine-tuned object detection–based vision-language models such as OSCAR~\cite{li2020oscar}, and UNITER~\cite{Uniter2019}, which utilize Faster R-CNN~\cite{fater_RCNN_2015}-based object detectors~\cite{Anderson2017up-down, vinVL2021}. 

More recently, research has shifted toward CLIP \cite{clip2021} for its end-to-end simplicity and stronger multimodal alignment. Numerous studies have fine-tuned models based on CLIP using different modality fusion mechanisms~\cite{PramanickMomenta2021, KumarHateClip2022, Shah2024memeclip_pridemm}. Other works incorporate caption models into the CLIP-based feature fusion network to further enhance performance~\cite{Burbi_2023_Issues, Cao_2023_ProCap, Ji2024CapAlign}. Additionally, contrastive learning techniques have been explored to address confounding factors in meme classification~\cite{LippeHMFramework2020, RGCL2024Mei}. %MOMENTA \cite{PramanickMomenta2021} and PromptHate \cite{caoPromptHate2022} augment CLIP representations with additional features such as text attributes and image captions.
% While these augmentations improve performance, they also introduce additional latency to the inference process.
%HateCLIPper \cite{KumarHateClip2022} explored different types of modality interaction for CLIP vision and language representations to address challenging hateful memes. 

While LMMs such as Flamingo~\cite{Flamingo22} have shown promise in hateful meme detection via SFT, fine-tuning strategies for LMMs remain underexplored relative to CLIP-based approaches. In fact, \citet{RGCL2024Mei} demonstrated that fine-tuned CLIP models can outperform much larger LMMs, highlighting the need for specialized fine-tuning methods. In this work, we propose a novel fine-tuning approach for LMMs to improve their effectiveness in hateful meme detection.  

\subsection{Low resource hateful meme detection} 
Low-resource hateful meme detection has received relatively little attention, despite its growing importance in real-world deployments that require out-of-domain generalization. In this setting, an initially trained model is deployed to a new domain without gradient updates, relying only on demonstration examples for inference \cite{Huang_LowResourceLMMAgentHatefulMeme_2024}.  \citet{Hee2024BridgeModality} utilized text similarity–based few-shot examples to help LMMs generalize to unseen memes. Similarly, \citet{Hu_2024_VPD} and \citet{Huang_LowResourceLMMAgentHatefulMeme_2024} explored agent-based LMM systems with few-shot learning for out-of-domain settings. However, \citet{Huang_LowResourceLMMAgentHatefulMeme_2024} observed that in-context learning is less effective for meme classification compared to other tasks, highlighting the need for more effective strategies to use demonstration examples.
\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{Figs/system_diagram_new.png}
    \caption{Architecture of LMM-RGCL.
    We decompose the LMM into two components: the LMM Backbone and the LM Head (LMH). For each training example $i$, the last hidden state $\mathbf{h}_i$ is fed to the LMH to obtain the LM loss $\mathcal{L}_i^{LM}$. $\mathbf{h}_i$ is also fed to a trainable multilayer perceptron (MLP) to generate an embedding $\mathbf{g}_i$ for use as a retrieval query and as a feature for the Logistic Regression Classifier (LRC) to compute the cross entropy loss $\mathcal{L}_i^{LR}$. During training, pseudo-gold and hard negative examples are retrieved from the encoded meme database $\mathbf{G}$ for computing the contrastive loss $\mathcal{L}_i^{RGCLL}$. At inference, the same process retrieves the $K$ nearest neighbors for Retrieval-based KNN Classification (RKC),  which predicts the label $\hat{y}_t^{RKC}$ for an inference example $t$.}
    \label{fig:system}
\end{figure*}
In contrast, we use a retrieval-based majority voting scheme for classifying unseen memes and find that it makes more effective use of demonstration examples than conventional in-context learning.