% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


% Custom package
%\usepackage{fontspec}
\usepackage{comment}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[export, demo]{adjustbox}  
\usepackage{multicol, makecell}
\usepackage{bbm}
\usepackage{tablefootnote}
\usepackage{romannum}
\usepackage{subcaption}
%\NewDocumentCommand\boldvalue{m}{\text{\num{\textbf{ #1}}}}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.1pt] (char) {#1};}}
 
\renewcommand\tabularxcolumn[1]{m{#1}}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\DeclareMathOperator*{\argmax}{argmax}
\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

\newcommand{\howard}[1]{\textcolor{red}{#1}}
\newcommand{\eric}[1]{\textcolor{purple}{#1}}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\definecolor{olive}{RGB}{0,153,51}

\title{Improved Fine-Tuning of Large Multimodal Models for Hateful Meme Detection}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{ \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

\author{Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, Bill Byrne \\
%         Address line \\ ... \\ Address line}
Department of Engineering\\
University of Cambridge\\
Cambridge, United Kingdom, CB2 1PZ \\
  \texttt{\{jm2245, jc2124, gy266, wl356, wjb31\}@cam.ac.uk} \\}
  
\begin{document}
\pagenumbering{arabic}
\maketitle

\begin{abstract}
Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While large multimodal models have shown strong generalization across various tasks, they exhibit poor generalization to hateful meme detection due to the dynamic nature of memes tied to emerging social trends and breaking news. Recent work further highlights the limitations of conventional supervised fine-tuning for large multimodal models in this context. To address these challenges, we propose Large Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a novel two-stage fine-tuning framework designed to improve both in-domain accuracy and cross-domain generalization. Experimental results on six widely used meme classification datasets demonstrate that LMM-RGCL achieves state-of-the-art performance, outperforming agent-based systems such as VPD-PALI-X-55B. Furthermore, our method effectively generalizes to out-of-domain memes under low-resource settings, surpassing models like GPT-4o.
\end{abstract}

 \textcolor{red}{This paper contains content for demonstration purposes that may be disturbing for some readers.}



\section{Introduction}

The rise of social media has led to a surge in hateful content, notably in the form of memes. Manual detection is infeasible due to the vast amount of content and psychological risks for human moderators. Consequently, hateful meme detection systems have attracted considerable research interest \cite{KielaFBHMC2020, LiuFigMemes2022, PrakashTotalDefMeme2023, Shah2024memeclip_pridemm}. 

Previous studies focus on supervised settings, fine-tuning neural networks with in-domain training data \cite{PramanickMomenta2021, KumarHateClip2022, Burbi_2023_Issues, Lin_2024_ExplainHM}.  
However, real-world challenges arise as memes continuously evolve with social trends and breaking news. This creates an out-of-domain generalization gap where these systems often fail to recognize new hate patterns \cite{Cao_2024_ModHate}. Frequent retraining becomes impractical given daily content generation rates and associated annotation requirements. These challenges highlight the need for generalizable detection methods that perform well on in-domain examples while maintaining robustness in low-resource, out-of-domain scenarios \cite{Huang_LowResourceLMMAgentHatefulMeme_2024}.

While large language models (LLMs) have demonstrated strong generalization across various tasks \cite{OpenAI_GPT4_2023}, recent studies indicate that vanilla Large Multimodal Models (LMMs) struggle with hateful meme detection, both in-domain and out-of-domain \cite{RGCL2024Mei, Huang_LowResourceLMMAgentHatefulMeme_2024, Hee2024BridgeModality, Cao_2024_ModHate}. Notably, \citet{RGCL2024Mei} found that fine-tuned CLIP \cite{clip2021} outperforms supervised fine-tuned (SFT) LMMs such as LLaVA-13B \cite{LiuLLAVA2023} and Flamingo-80B \cite{Flamingo22}, thus exposing shortcomings in standard SFT approaches for LMMs. Additionally, they showed that LMMs exhibit poor cross-domain generalization: for instance, a LLaVA model fine-tuned on the HatefulMemes dataset \cite{KielaFBHMC2020} (focusing on racist, sexist, and religion-based hate speech) fails to generalize to HarMeme \cite{pramanickCovidMeme2021}, which targets COVID-related political memes.
In addition, \citet{Huang_LowResourceLMMAgentHatefulMeme_2024} observed that hateful meme detection remains challenging for in-context learning framework in LMMs, suggesting that innovative approaches are needed to make better use of few-shot meme examples.

To address these challenges, we propose\textbf{ Large Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL)}, a two-stage fine-tuning framework designed to improve both in-domain and out-of-domain hateful meme detection. %The first stage, \textbf{Joint Multimodal Fine-tuning}, optimizes the LMM by integrating cross-entropy loss with language modeling loss, where the classifier receives the last token embedding from the LMM. 
The first stage, \textbf{Joint Multimodal Fine-tuning}, rapidly adapts the LMM to the hateful meme detection task by integrating language modeling loss with cross-entropy loss for the classifier.
%The second stage, \textbf{RGCL Fine-tuning}, freezes the LMM and fine-tunes the classifier using cross-entropy loss and a contrastive loss objective. 
The second stage, \textbf{RGCL Fine-tuning}, learns to discriminate between retrieved positive and negative examples, improving both classification and retrieval capabilities by aligning representations of semantically similar meme pairs. After LMM-RGCL fine-tuning, the learned embedding space enables retrieval-based KNN majority voting, improving classification performance on previously unseen examples. We show that this approach makes better use of few-shot meme examples than in-context learning.
LMM-RGCL enables Qwen2VL-7B \cite{Wang_2024_Qwen2vl} to achieve state-of-the-art performance on six popular meme classification datasets, surpassing larger agent-based systems such as VPD-PALI-X 55B \cite{Hu_2024_VPD}. Additionally, we show that LMM-RGCL fine-tuned models exhibit significantly stronger generalization to unseen meme datasets compared to standard SFT. Under low-resource settings, Qwen2VL-7B, using the retrieval-based KNN classifier, surpasses models such as GPT-4o \cite{OpenAI2024gpt4o} and LOREHM with LLaVA-34B \cite{Liu_2023_LLAVA1.5}.
Our contributions are:

\begin{enumerate}
    \item We propose a novel two-stage fine-tuning framework, LMM-RGCL, which integrates joint multimodal fine-tuning with an RGCL fine-tuning phase. This approach optimizes both classification and retrieval capabilities for hateful meme detection.
    
    \item We demonstrate that LMM-RGCL achieves state-of-the-art performance on six widely used meme classification datasets, outperforming larger agent-based systems that rely on standard supervised fine-tuning. 
    \item We show that LMM-RGCL significantly improves generalization to unseen meme datasets, offering a comprehensive solution for both in-domain and out-of-domain hateful meme detection. To our knowledge, this is the first system to attain state-of-the-art results in both scenarios.
\end{enumerate}


\section{Related Work}
%\subsection{Hateful Memes Detection}
%The Hateful Memes Challenge competition \cite{KielaFBHMC2020} released a benchmark dataset for hateful meme detection. 
%The best baseline model from the challenge is the Visual BERT, achieving an AUROC of 75.4. The prize-winning solution \cite{RonHMC1st2020, VilioHMC2nd2020, RizaHMC3rd2020} of the challenge managed to push the AUROC to 84.5 with additional extracted features and ensembling of models.
\subsection{Hateful Meme Detection}
Most existing approaches to hateful meme detection use supervised learning. Early systems~\cite{RonHMC1st2020, VilioHMC2nd2020} fine-tuned object detection–based vision-language models such as OSCAR~\cite{li2020oscar}, and UNITER~\cite{Uniter2019}, which utilize Faster R-CNN~\cite{fater_RCNN_2015}-based object detectors~\cite{Anderson2017up-down, vinVL2021}. 

More recently, research has shifted toward CLIP \cite{clip2021} for its end-to-end simplicity and stronger multimodal alignment. Numerous studies have fine-tuned models based on CLIP using different modality fusion mechanisms~\cite{PramanickMomenta2021, KumarHateClip2022, Shah2024memeclip_pridemm}. Other works incorporate caption models into the CLIP-based feature fusion network to further enhance performance~\cite{Burbi_2023_Issues, Cao_2023_ProCap, Ji2024CapAlign}. Additionally, contrastive learning techniques have been explored to address confounding factors in meme classification~\cite{LippeHMFramework2020, RGCL2024Mei}. %MOMENTA \cite{PramanickMomenta2021} and PromptHate \cite{caoPromptHate2022} augment CLIP representations with additional features such as text attributes and image captions.
% While these augmentations improve performance, they also introduce additional latency to the inference process.
%HateCLIPper \cite{KumarHateClip2022} explored different types of modality interaction for CLIP vision and language representations to address challenging hateful memes. 

While LMMs such as Flamingo~\cite{Flamingo22} have shown promise in hateful meme detection via SFT, fine-tuning strategies for LMMs remain underexplored relative to CLIP-based approaches. In fact, \citet{RGCL2024Mei} demonstrated that fine-tuned CLIP models can outperform much larger LMMs, highlighting the need for specialized fine-tuning methods. In this work, we propose a novel fine-tuning approach for LMMs to improve their effectiveness in hateful meme detection.  

\subsection{Low resource hateful meme detection} 
Low-resource hateful meme detection has received relatively little attention, despite its growing importance in real-world deployments that require out-of-domain generalization. In this setting, an initially trained model is deployed to a new domain without gradient updates, relying only on demonstration examples for inference \cite{Huang_LowResourceLMMAgentHatefulMeme_2024}.  \citet{Hee2024BridgeModality} utilized text similarity–based few-shot examples to help LMMs generalize to unseen memes. Similarly, \citet{Hu_2024_VPD} and \citet{Huang_LowResourceLMMAgentHatefulMeme_2024} explored agent-based LMM systems with few-shot learning for out-of-domain settings. However, \citet{Huang_LowResourceLMMAgentHatefulMeme_2024} observed that in-context learning is less effective for meme classification compared to other tasks, highlighting the need for more effective strategies to use demonstration examples.
\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{Figs/system_diagram_new.png}
    \caption{Architecture of LMM-RGCL.
    We decompose the LMM into two components: the LMM Backbone and the LM Head (LMH). For each training example $i$, the last hidden state $\mathbf{h}_i$ is fed to the LMH to obtain the LM loss $\mathcal{L}_i^{LM}$. $\mathbf{h}_i$ is also fed to a trainable multilayer perceptron (MLP) to generate an embedding $\mathbf{g}_i$ for use as a retrieval query and as a feature for the Logistic Regression Classifier (LRC) to compute the cross entropy loss $\mathcal{L}_i^{LR}$. During training, pseudo-gold and hard negative examples are retrieved from the encoded meme database $\mathbf{G}$ for computing the contrastive loss $\mathcal{L}_i^{RGCLL}$. At inference, the same process retrieves the $K$ nearest neighbors for Retrieval-based KNN Classification (RKC),  which predicts the label $\hat{y}_t^{RKC}$ for an inference example $t$.}
    \label{fig:system}
\end{figure*}
In contrast, we use a retrieval-based majority voting scheme for classifying unseen memes and find that it makes more effective use of demonstration examples than conventional in-context learning.


\section{LMM-RGCL Methodology}
\subsection{Preliminaries}
\label{sec:preliminaries}
\textbf{Problem Statement } Hateful memes datasets are defined as
$\{ (I_i,T_i,y_i)\}_{i=1}^N$, %$\{ (I_i,T_i)\}_{i=1}^N$
where $I_i \in \mathbb{R}^{C\times H \times W }$ is the image portion of the meme in pixels; $T_i$ is the caption overlaid on the meme; $y_i\in\{0,1\}$ is the label, where 0 stands for benign, 1 for hateful.
\\
\textbf{Large Multimodal Models}
Some prior work in using LMMs for hateful meme detection has approached the problem via text generation, where the LMM takes a meme $(I_i, T_i)$ as an input to predict a single token label $\hat{y}_i^{LMH} \in \{\textrm{``benign'', ``hateful''}\}$~\cite{Lin_2024_ExplainHM, Huang_LowResourceLMMAgentHatefulMeme_2024}.
We refer to the final linear layer of the LMM as the LM Head (\textbf{LMH}), which maps hidden representations to a probability distribution over the vocabulary via a softmax function. For meme classification, the LMH decodes the hidden state of the last token and generates the output label.
This contrasts with approaches based on CLIP, which train Logistic Regression Classifiers \textbf{(LRC)} on encoder CLS tokens \cite{KumarHateClip2022, RGCL2024Mei}.

%\textbf{Supervised Fine-Tuning (SFT).} SFT adapts LMMs to downstream tasks by optimizing the language modeling objective $\mathcal{L}_i^{LM}$, the negative log-likelihood of the next token prediction.  Our framework, LMM-RGCL, extends this paradigm with a two-stage fine-tuning strategy to enhance both classification and retrieval capabilities.

%\textbf{Low resource Hateful Memes Detection}
%Following previous works \cite{Huang_LowResourceLMMAgentHatefulMeme_2024}, we define a 
\subsection{LMM-RGCL Framework}
\textbf{Architecture} 
As illustrated in Figure~\ref{fig:system}, LMM-RGCL integrates an  LMM with two additional trainable components: a Multilayer Perceptron (MLP) that projects the LMM final hidden state $\mathbf{h}_i$ into an embedding $\mathbf{g}_i$ for use in classification and retrieval; and an LRC
operating on $\mathbf{g}_i$ . Figure~\ref{fig:system} shows how the architecture supports multiple fine-tuning and inference modes.%In Figure~\ref{fig:system} We decompose the LMM into the LMM Backbone and the LMH to contrast LMH with other inference modes in our framework. 
%\\
%\textbf{Fine-tuning Stage 1 - Joint Multimodal Fine-tuning :} %Initially, the LMM’s language model and LM head are optimized alongside the MLP and LR classifier, while the vision encoder remains frozen. 
%\howard{Initially, the LMM is fine-tuned via LoRA~\cite{Hu_LORA_2021} alongside the MLP and LRC.}
%\\
%\textbf{Fine-tuning Stage 2 - RGCL Fine-tuning:} %In this stage the LMM-LM is frozen; only the MLP and LR are fine-tuned to refine retrieval-aligned representations. 
%In this stage the LMM is frozen; only the MLP and LR are fine-tuned to refine retrieval-aligned representations.

\noindent\textbf{Retrieval} During training, FAISS-based~\cite{johnson_Faiss2019billion} nearest neighbor search retrieves pseudo-gold positive \cite{RGCL2024Mei} and hard negative examples \cite{Schroff_FaceNet_2015} for contrastive learning from the encoded meme database $\mathbf{G}$. At inference, FAISS is used to retrieve neighbors for the Retrieval-based KNN Classifier (\textbf{RKC}).

\noindent\textbf{Inference modes} 
Figure~\ref{fig:system} shows three different classifiers: LMH, LRC, and RKC.
%1. The LM head provides baseline inference results for pre-trained and SFT LMMs, following prior work (Section~\ref{sec:preliminaries}). 
%2. The \howard{LRC} is our default classifier in supervised settings. 3. RKC is our default classifier in out-of-domain settings. 
For pre-trained and SFT LMMs, we report LMH results following prior work (Section~\ref{sec:preliminaries}). For LMM-RGCL models, we report the LRC results, unless otherwise specified. 
Section~\ref{sec:abl_infer_mode} presents a detailed comparison of the three inference modes.
\subsection{Stage 1: Joint Multimodal Fine-tuning}
In stage 1, the LMM is fine-tuned via Low-Rank Adaptation~\cite{Hu_LORA_2021},  which applies trainable low-rank matrices to the model while freezing its original weights. The MLP and LRC are updated simultaneously.
We optimize the joint loss for each training example $i$:
\begin{equation}
\mathcal{L}_{i}^{\textrm{Stage1}}=
\mathcal{L}_i^{LM} +\mathcal{L}_i^{LR},
\label{eq:stage1}
\end{equation}
%$\howard{where \mathcal{L}_i^{LM}$ is the language modeling objective used in SFT, and the $\mathcal{L}_i^{LR}$ is the binary cross entropy loss applied to the LRC prediction $\hat{y}_i^{LRC}$:}
where $\mathcal{L}_i^{LM}$ is the language modeling objective used in SFT. In the context of meme classification, the model is trained to predict a single target token $s(y_i)$:
\begin{equation}
    s(y_i)     =      \begin{cases}
        \textrm{``benign''} &\text{if } y_i=0\\
        \textrm{``hateful''}  &\text{if } y_i= 1

    \end{cases}.
\end{equation}
$\mathcal{L}_i^{LM}$ is computed as the negative log-likelihood of generating the correct target token, conditioned on the input image and text:
\begin{equation}
    \mathcal{L}_i^{LM} = -\log p(\hat{y}_i^{LMH} =s(y_i)  \mid I_i, T_i)   
\end{equation}
The $\mathcal{L}_i^{LR}$ is the binary cross-entropy loss applied to the LRC prediction $\hat{y}_i^{LRC}$:
\begin{equation}
\mathcal{L}_i^{LR} = - y_i\log \hat{y}_i^{LRC} - (1-y_i)\log(1-\hat{y}_i^{LRC})
\label{eq:CE}
\end{equation}
%Jointly optimizing the \textit{LM} loss with the \textit{CE} loss allows the LMM to rapidly adapt to the hateful meme detection task while preserving its generative capabilities.
Jointly optimizing the language modeling loss $\mathcal{L}_i^{LM}$ with the cross-entropy loss $\mathcal{L}_i^{LR}$ allows the LMM to rapidly adapt to the hateful meme detection task.


\subsection{Stage 2: RGCL Fine-tuning}
In stage 2, the LMM is frozen; only the MLP and LRC are fine-tuned to refine retrieval-aligned representations.
%We further improve the system’s retrieval and classification performance by fine-tuning the MLP and the \howard{\textbf{LRC}}  while keeping the LMM fixed. 
Stage 2 jointly optimizes:
\begin{equation}
    \mathcal{L}_i^{\textrm{Stage2}} = \mathcal{L}_i^{RGCLL} + \mathcal{L}_i^{LR},
\label{eq:stage2}
\end{equation}
where $\mathcal{L}_i^{LR}$ is defined in Eq.~\ref{eq:CE}, and \(\mathcal{L}_i^{RGCLL}\) is the Retrieval-Guided Contrastive Learning Loss.

To compute \(\mathcal{L}_i^{RGCLL}\), we retrieve pseudo-gold positive and hard negative examples from the training set. Specifically, for a given sample \(i\) with embedding \(\mathbf{g}_i\), we use FAISS \cite{johnson_Faiss2019billion} to perform the nearest neighbor search between \(\mathbf{g}_i\) and every other target embedding \(\mathbf{g}_j \in \mathbf{G}\) from the training set. The encoded meme database $\mathbf{G}$ is updated every 100 steps during fine-tuning. 

Pseudo-gold positive examples are same-label examples that have high similarity scores with \(\mathbf{g}_i\), while hard negative examples are opposite-label examples that have high similarity scores. We denote the embedding of the pseudo-gold positive example and hard negative example as \(\mathbf{g}_i^{+}\) and \(\mathbf{g}_i^{-}\), respectively.
\(\mathcal{L}_i^{RGCLL}\) is then computed as:
\begin{align}
\nonumber \mathcal{L}_i^{RGCLL}&= L(\mathbf{g}_i,  \mathbf{g}_i^{+}, \mathbf{g}_{i}^{-})\\
        &= - \log \frac{ e^{\textrm{sim}(\mathbf{g}_i,\mathbf{g}_{i}^{+})}}{ e^{\textrm{sim}(\mathbf{g}_i,\mathbf{g}_{i}^{+})} + e^{\textrm{sim}(\mathbf{g}_i,\mathbf{g}_{i}^{-})}},
\end{align}
where \(\textrm{sim}(\cdot,\cdot)\) denotes the cosine similarity function. Stage 2 fine-tuning explicitly aligns the representations of semantically similar meme pairs, thereby improving the generalization of LMMs to distribution shifts in unseen datasets.


\subsection{Retrieval Based KNN Classification}
In addition to the LMH and LRC, RKC is used specifically for out-of-domain meme classification. 
For a test meme $t$, we retrieve $K$ similar memes within the embedding space from the meme database $\mathbf{G}$.
We perform similarity-weighted majority voting to obtain the prediction:
\begin{equation}
    \hat{y}_t^{RKC} = \sigma(\sum_{k=1}^K\overline{y}_k \cdot \text{sim}(g_k, g_t)),
\end{equation}
where $\sigma(\cdot)$ is the sigmoid function and 
\begin{equation}
    \overline{y}_k:=
    \begin{cases}
        1  &\text{if } y_k= 1\\
        -1 &\text{if } y_k=0
    \end{cases}.
    \label{eq:indicator_function}
\end{equation}
Additionally, to enable RKC on pretrained or SFT LMMs that do not incorporate an MLP, we use the last hidden state \(\mathbf{h}_i\) for the nearest neighbor search. The results are provided in Appendix~\ref{appendix:rkc_icl_ft}.
\section{Experiments}
We evaluate the performance of our systems on six popular meme classification datasets: \textbf{HatefulMemes}~\cite{KielaFBHMC2020}, \textbf{HarMeme}~\cite{pramanickCovidMeme2021}, \textbf{MAMI}~\cite{Fersini_MAMI_2022}, \textbf{Harm-P}~\cite{PramanickMomenta2021}, \textbf{MultiOFF}~\cite{suryawanshi-etal-2020-MultiOFF} and \textbf{PrideMM}~\cite{Shah2024memeclip_pridemm}. These datasets encompass varying definitions of harmful content (hateful, offensive, or targeted harassment) across different sociopolitical contexts.
A detailed description and statistics are in Appendix~\ref{appendix:data_stats}. 


\begin{table*}[h]
\sisetup{table-format=2.1, table-auto-round=true, table-number-alignment=center, detect-weight=true, detect-family=true,mode=text}
\small
\centering
\setlength{\tabcolsep}{4pt}{
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{ll|SS|SS|SS|SS|SS|SS}
\toprule 
\multicolumn{2}{c|}{} &
 \multicolumn{2}{c|}{\textbf{HatefulMemes}} & \multicolumn{2}{c|}{\textbf{HarMeme}} & \multicolumn{2}{c|}{\textbf{MAMI}} & \multicolumn{2}{c|}{\textbf{Harm-P}} & \multicolumn{2}{c|}{\textbf{MultiOFF}} & \multicolumn{2}{c}{\textbf{PrideMM}}  \\
 & Model  & \textbf{AUC} &  \textbf{Acc.} & \textbf{AUC} & \textbf{Acc.} & \textbf{AUC} &  \textbf{Acc.} & \textbf{Acc.} & \textbf{F1} & \textbf{Acc.} & \textbf{F1} & \textbf{Acc.} & \textbf{F1} \\ 
\midrule
\multicolumn{2}{c|}{\multirow{2}{*}{Best prior results}}& \multicolumn{2}{c|}{VPD-55B} & \multicolumn{2}{c|}{ISSUES} & \multicolumn{2}{c|}{Pro-Cap} & \multicolumn{2}{c|}{ExplainHM} & \multicolumn{2}{c|}{RGCL} & \multicolumn{2}{c}{MemeCLIP} \\
\multicolumn{2}{c|}{}& 89.20 & 80.80 & 92.83 & 81.64 & 83.77 & 73.63 & \underline{90.7} & \underline{90.7} & 67.13 & 58.11 & 76.06 & 75.09 \\
 \midrule
 \multicolumn{8}{l}{\textit{~~~~~Supervised fine-tuned CLIP-based Classifiers}} \\
%Underline the second best
 \midrule
1& CLIP  & 79.81 & 72.04 & 82.63 & 76.78 & 77.66 & 68.44 & 80.55 & 80.25 & 62.41 & 48.14 & 72.39 & 72.33   \\
%PromptHate & 81.5 & 73.0 & 90.9 & 84.5\\ 
%ISSUES &  85.51 & 77.70  &  92.83 & 81.64 \\
%DisMultiHate & 82.8 & 75.8 & 86.39 & 81.24 & & & & \\
2& MOMENTA & 69.17 & 61.34 & 86.32 & 80.48 & 81.68 & 72.10 & 89.84 & 88.26 & \text{-} & \text{-} & 72.23 & 71.78  \\ 
3& HateCLIPper & 85.48 & 76.10 & 89.72 & 84.80 & 87.15 & 74.82 & 87.60 & 86.90 & 62.40 & 54.80 & 75.53 & 74.08  \\
4& RGCL & 87.04 & 78.82 & 91.81 & 87.03 & 89.39 & 78.35 & 89.92 & 89.51 & 67.13 & 58.11  & 76.34 & 76.50 \\ 
\midrule
\multicolumn{8}{l}{\textit{~~~~~Large Multimodal Models}} \\\midrule
\multicolumn{2}{c|}{LLaVA-1.5-7B} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{} \\
5 & ~\textit{w/ zero-shot} & 63.69 & 57.60 & 71.38 & 48.59 & 67.60 & 58.30  &  61.57  & 46.39 & 59.55 & 51.68 & 63.43 & 65.64 \\
6 & ~\textit{w/ few-shot} & 63.38 & 57.20 & 73.42 & 59.60 & 68.09 & 62.70 & 53.52 & 52.17 & 38.93 & 56.04 & 62.13 & 64.04
  \\
7 & ~\textit{w/ SFT} & 85.19 & 78.71 & 91.38 & 79.10 & 85.97 & 73.92 & 82.76 & 82.76 & 67.79 & 57.81 & 73.18 & 75.97 \\
8 & ~\textit{w/ LMM-RGCL} & \underline{89.7} & \underline{80.9} & \textbf{93.5} & \textbf{88.2} & \textbf{91.2} & \underline{79.7} & 89.64 & 89.34 & \underline{70.9} & \underline{63.6} & \textbf{78.1} & \textbf{78.7} \\
& ~\textit{p}-value & \multicolumn{1}{c}{9.8$e^{-3}$} &  \multicolumn{1}{c|}{3.5$e^{-3}$} & \multicolumn{1}{c}{1.2$e^{-2}$} & \multicolumn{1}{c|}{8.5$e^{-3}$} & \multicolumn{1}{c}{4.4$e^{-3}$} & \multicolumn{1}{c|}{6.2$e^{-3}$} & \multicolumn{1}{c}{2.5$e^{-3}$} & \multicolumn{1}{c|}{1.6$e^{-3}$} & \multicolumn{1}{c}{6.1$e^{-3}$} & \multicolumn{1}{c|}{4.6$e^{-3}$} & \multicolumn{1}{c}{5.6$e^{-3}$} & \multicolumn{1}{c}{8.9$e^{-3}$} \\ 
 \multicolumn{2}{c|}{Qwen2VL-2B} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{} \\
9 & ~\textit{w/ zero-shot} & 64.84 & 54.20 & 61.05 & 56.67 & 67.19 & 51.01 & 53.92 & 21.78 & 63.11 & 36.33 & 57.79 & 53.27 \\
10 & ~\textit{w/ few-shot} & 61.72 & 59.10 & 62.11 & 65.82 & 64.84 & 58.80 & 52.96 & 51.62 & 67.11 & 44.92 & 55.42 & 54.30  \\
11 & ~\textit{w/ SFT} & 83.98 & 76.20 & 90.23 & 82.49 & 77.73 & 68.64 & 80.25 & 79.69 & 66.44 & 54.53 & 73.74 & 74.17 \\
12 & ~\textit{w/ LMM-RGCL} & 88.41 & 79.13 & 92.93 & 87.66 & 89.26 & 79.40 & 88.91 & 88.70 & 68.46 & 61.79 & 76.03 & 76.72 \\
& ~\textit{p}-value & \multicolumn{1}{c}{4.2$e^{-3}$} &  \multicolumn{1}{c|}{4.8$e^{-3}$} & \multicolumn{1}{c}{9.1$e^{-3}$} & \multicolumn{1}{c|}{7.6$e^{-3}$} & \multicolumn{1}{c}{6.5$e^{-4}$} & \multicolumn{1}{c|}{1.9$e^{-4}$} & \multicolumn{1}{c}{9.3$e^{-4}$} & \multicolumn{1}{c|}{1.1$e^{-3}$} & \multicolumn{1}{c}{2.0$e^{-2}$} & \multicolumn{1}{c|}{7.7$e^{-3}$} & \multicolumn{1}{c}{7.1$e^{-3}$} & \multicolumn{1}{c}{8.3$e^{-3}$} \\ 
 \multicolumn{2}{c|}{Qwen2VL-7B} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{} \\
13 & ~\textit{w/ zero-shot} & 71.88 & 63.20 & 64.84 & 64.12 & 76.17 & 58.50 & 55.49 & 22.85 & 63.44 & 35.94 & 65.29 & 62.89   \\
14 & ~\textit{w/ few-shot} & 71.48 & 63.80 & 71.48 & 67.23 & 73.44 & 66.10 & 55.55 & 65.23 & 64.43 & 54.69 & 69.14 & 56.61 \\
15 & ~\textit{w/ SFT} & 86.33 & 78.60 & 91.80 & 85.88 & 82.64 & 72.39 & 85.89 & 86.33 & 67.79 & 55.51 & 75.10 & 74.88    \\
16 & ~\textit{w/ LMM-RGCL} &
\textbf{91.1}
 & \textbf{82.1} & \underline{93.2} & \underline{88.1} & \underline{90.4} & \textbf{79.9} & \textbf{91.6} & \textbf{91.1} & \textbf{71.1} & \textbf{64.8} & \textbf{78.1} & \underline{78.4} \\ 

%& Average gain $\uparrow$& +4.56  & +2.87 & +2.07 & +5.49 & +8.17 & +8.04  &  +5.74 & +5.46 & +2.81 & +6.29 & +2.05 & +1.60\\  
& ~\textit{p}-value & \multicolumn{1}{c}{8.7$e^{-4}$} & \multicolumn{1}{c|}{2.4$e^{-3}$} & \multicolumn{1}{c}{3.5$e^{-2}$} & \multicolumn{1}{c|}{1.6$e^{-2}$} & \multicolumn{1}{c}{2.5$e^{-3}$} & \multicolumn{1}{c|}{2.6$e^{-3}$} & \multicolumn{1}{c}{6.3$e^{-3}$} & \multicolumn{1}{c|}{8.6$e^{-3}$} & \multicolumn{1}{c}{1.3$e^{-2}$} & \multicolumn{1}{c|}{5.9$e^{-3}$} &\multicolumn{1}{c}{9.2$e^{-3}$} & \multicolumn{1}{c}{7.2$e^{-3}$}\\

\bottomrule
\end{tabular}
}}
\caption{Comparison with baseline systems under supervised settings. For large multimodal models, we report the pre-trained models zero-shot and few-shot performance (using 4-shot evaluation), along with a comparison between SFT and LMM-RGCL. Additionally, for each LMM, we provide the ~\textit{p}-value from significance testing between SFT and LMM-RGCL. All ~\textit{p}-values are below 0.05. %\howard{For pre-trained LMM, we use the LMH for inference. For LMM-RGCL, we use LRC.} 
Best performance is highlighted in \textbf{bold}; second-best is \underline{underlined}.}
\label{tab:results_supervised}
\end{table*}
For HatefulMemes, HarMeme, and MAMI, we report the Area Under the Receiver Operating Characteristic Curve (AUC) and Accuracy (Acc) in line with previous studies \cite{KumarHateClip2022, Cao_2023_ProCap, RGCL2024Mei, Cao_2024_ModHate}. For Harm-P, MultiOFF, and PrideMM, we report Accuracy and F1 score consistent with the literature \cite{PramanickMomenta2021, RGCL2024Mei, Shah2024memeclip_pridemm, Lin_2024_ExplainHM}.
Implementation details, including hyperparameters, and statistical significance test procedures are detailed in Appendix~\ref{appendix:exp_setup}.

\subsection{Comparing LMM-RGCL to Baseline Systems under Supervised Settings}
\label{sec:results_supervise}
Table~\ref{tab:results_supervised} presents the performance of baseline systems under supervised fine-tuning settings. We compare LMM-RGCL against a range of strong baselines: the best prior models for each dataset\footnote{From a recent paper \cite{Nguyen_2024_computationalMemeUnderstanding}; some datasets have been updated with the new best results.}; supervised fine-tuned CLIP-based classifiers; and Large Multimodal Models (LMMs). 
All models are fine-tuned and evaluated for each dataset separately. 

\textbf{CLIP-based Classifiers}
We compare the performance of fine-tuned CLIP \cite{clip2021} model with three other fine-tuning methods for CLIP-based systems: MOMENTA \cite{PramanickMomenta2021}, HateCLIPper \cite{KumarHateClip2022} and RGCL \cite{RGCL2024Mei}.

\textbf{Large Multimodal Models}
We experiment with three LMMs from two model families: LLaVA-1.5-7B \cite{Liu_2023_LLAVA1.5}, Qwen2VL-2B and Qwen2VL-7B \cite{Wang_2024_Qwen2vl}. We report the performance of these LMMs in the following settings: pre-trained models with zero-shot and few-shot prompts using the LMH; SFT LMMs using the LMH; and classification using LRC under the LMM-RGCL fine-tuning framework.

\textbf{Best Prior Models}
 Visual Program Distillation (VPD) \cite{Hu_2024_VPD} and ExplainHM \cite{Lin_2024_ExplainHM} are LLM agent-based systems. 
 %VPD fine-tunes PaLI-X 55B \cite{Chen2023PALIX} to utilize tools and execute programs, while ExplainHM fine-tunes three LLMs as two debaters and one judge to explain and classify hateful memes. 
 The remaining state-of-the-art models, including ISSUES \cite{Burbi_2023_Issues}, Pro-Cap \cite{Cao_2023_ProCap}, RGCL \cite{RGCL2024Mei} and MemeCLIP \cite{Shah2024memeclip_pridemm}, are based on fine-tuning CLIP-based vision and language models. Detailed descriptions of these methods are provided in Appendix~\ref{appendix:baseline_models}.



\begin{table*}[h]
\small
\sisetup{table-format=2.1, table-auto-round=true, table-number-alignment=center, detect-weight=true, detect-family=true,mode=text}
\centering
\setlength{\tabcolsep}{5pt}{
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{ll|SS| SS| SS| SS| SS| SS}
\toprule 
&\multicolumn{1}{l|}{\textbf{Evaluated on}}&
 \multicolumn{2}{c|}{\textbf{HatefulMemes}} & \multicolumn{2}{c|}{\textbf{HarMeme}} & \multicolumn{2}{c|}{\textbf{MAMI}} & \multicolumn{2}{c|}{\textbf{Harm-P}} & \multicolumn{2}{c|}{\textbf{MultiOFF}} & \multicolumn{2}{c}{\textbf{PrideMM}}  \\
 &Model  & \textbf{AUC} &  \textbf{Acc.} & \textbf{AUC} & \textbf{Acc.} & \textbf{AUC} &  \textbf{Acc.} & \textbf{Acc.} & \textbf{F1} & \textbf{Acc.} & \textbf{F1} & \textbf{Acc.} & \textbf{F1} \\ 
\midrule
\multicolumn{8}{l}{\textit{~~~~~Low resourced systems}}\\\midrule
1 & GPT-4o & \text{-} & 65.00 & \text{-}  & 67.23 & \text{-} & 62.90 & 52.39 & 5.59 & 63.07 & 15.38 & 57.99 & 25.78   \\ % add to both tables 
2 & Mod-Hate & 64.5 & 58.0 & 73.4 & 69.5&  67.4 & 61.0 & \text{-} &  \text{-} & \text{-} & \text{-} & \text{-} & \text{-}\\ 
3 & LOREHM &  \text{-} & \underline{65.6} & \text{-}  & 73.73 & \text{-} & \underline{75.4} & \text{-} &  \text{-} & \text{-} & \text{-} & \text{-} & \text{-} \\

\midrule 
%&        63.69 & 57.60 & 71.38 &   48.59 & 46.39 & 61.57 & 51.68 & 59.55 & 63.43 &  65.64 & 58.30 & 67.60 \\
\multicolumn{8}{l}{\textit{~~~~~Systems fine-tuned under cross-dataset settings}}\\\midrule
& \multicolumn{1}{l|}{Fine-tuning set }&
 \multicolumn{2}{c|}{ HarMeme} & \multicolumn{10}{c}{HatefulMemes}  \\
 \midrule
4 & RGCL & 69.9 & 66.9 & 64.32 & 61.11 & 67.8 & 62.4& 56.42 & 57.10 & 53.69 & 45.1 & 59.76 & 61.48   \\ 
 & LLaVA-1.5-7B & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{} \\
5 & ~~~\textit{SFT + zero-shot}
& 63.79 & 59.40 & 61.92 & 48.47 & 69.14 & 61.10 & 55.21 & 28.70 & 62.76 & 32.50 & 58.11 & 53.25 \\ 
6 & ~~~\textit{SFT + few-shot} & 63.07 & 56.40 & 69.88 & 52.82 & 65.54 & 50.10 & 55.64 & 49.58 &  56.04 & 38.93 & 48.52 & 55.34 \\ 
7 & ~~~\textit{LMM-RGCL + RKC} & \underline{74.2} & 65.17 & \textbf{89.5} & \textbf{81.9} & \underline{80.0} & 74.45 & \textbf{ 67.3} & \textbf{67.8} & 62.42 & 51.73 & 68.84 & 67.72 \\ 
 & Qwen2VL-2B & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{} \\
8 & ~~~\textit{SFT + zero-shot}
& 64.06 & 59.70 & 61.30 & 52.16 & 66.41 & 57.30 & 53.52 & 20.31 & 62.33 & 29.30 & 56.41 & 58.98 \\ 
9 & ~~~\textit{SFT + few-shot} & 61.33 & 53.80 & 57.42 & 64.97 & 73.83 & 66.00 & 56.89 & 55.49 & 53.69 & 42.42 & 55.42 & 60.94 \\ 
10 & ~~~\textit{LMM-RGCL + RKC} & 70.86 & 62.77 & 85.98 & 78.37 & 74.79 & 72.27 & 63.38 & 65.97 & \underline{63.4} & 53.37 & \underline{69.0} & \underline{69.1} \\ 
 & Qwen2VL-7B & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}\\
11 & ~~~\textit{SFT + zero-shot} & 71.09 & 64.10 & 62.99 & 55.16 & 71.09 & 61.90 & 54.65 & 21.48 & 63.13 & 29.69 & 64.47 & 63.58 \\ 
12 & ~~~\textit{SFT + few-shot} & 72.27 & 60.60 & 67.19 & 62.43 & 73.44 & 66.00 & 56.36 & 64.90 & 62.02 & \underline{53.7} & 55.42 & 60.94 \\ 
13 & ~~~\textit{LMM-RGCL + RKC} & \textbf{77.1} & \textbf{69.3} & \underline{88.8} & \underline{81.7} & \textbf{81.4} & \textbf{75.6} & \underline{64.5} & \underline{66.4} & \textbf{63.8} & \textbf{55.6} & \textbf{69.3} & \textbf{69.3} \\ 
\bottomrule
\end{tabular}
}}
\caption{Comparing out-of-domain meme classification performance under low-resource settings. For systems fine-tuned under cross-dataset settings, models are fine-tuned on HarMeme and evaluated on the HatefulMemes dataset. For the remaining evaluation datasets, models are fine-tuned on the HatefulMemes dataset. For LMMs, we compare the SFT models using zero-shot and in-context learning with the LMM-RGCL fine-tuned models using RKC. Few-shot examples (4-shot) and RKC examples are drawn from the training split of each evaluation dataset. \#2 and \#3 are taken from the original paper. 
Best performance is highlighted in \textbf{bold}; second-best is \underline{underlined}. }
\label{tab:results_low_resource}
\end{table*}
\textbf{Observation 1: Fine-tuned CLIP-based classifiers outperform baseline LMMs.}\\
As shown in Table~\ref{tab:results_supervised}, RGCL (\#4) achieves the highest performance among CLIP-based classifiers, surpassing standard fine-tuned CLIP (\#1) by approximately 10\% across multiple datasets. On 5 out of 6 datasets, RGCL performs better than, or on par with, all three SFT LMMs (\#7, \#11, \#15).
%These results align with \citealp{RGCL2024Mei}'s finding and underpin that we need better fine-tuning approaches for LMMs. The two exceptions are MultiOFF and PrideMM. On MultiOFF, Qwen2VL-7B SFT performs slightly better on accuracy with 0.7\% gains over RGCL, however, its F1 score is almost 3\% lower than RGCL. On PrideMM, RGCL performs slightly worse than Qwen2VL-7B SFT.

\textbf{Observation 2: In-context learning exhibits limited efficacy for meme classification.}\\
We compare the zero-shot (\#5, \#9, \#13) and few-shot (\#6, \#10, \#14) performance of the pre-trained LMMs. Our findings indicate that, unlike in some other tasks, in-context learning does not benefit meme classification, which is consistent with previous results \cite{Hee2024BridgeModality, Huang_LowResourceLMMAgentHatefulMeme_2024}. HarMeme is the only dataset where few-shot systems consistently outperform zero-shot systems. On Harm-P and MultiOFF, although the accuracies of zero-shot and few-shot remain comparable, the few-shot experiments yield a significant gain in F1 score. This improvement is due to a more balanced precision and recall after providing demonstration examples to the system.
% Precision and Recall balance explanation in appendix
% Try find explanation for harmeme's gain 

\textbf{Observation 3: LMM-RGCL outperforms all strong baseline systems across six datasets}\\
For the six datasets and the three LMMs, fine-tuning with LMM-RGCL significantly improves performance over SFT (Table~\ref{tab:results_supervised}: \#7, \#8; \#11, \#12; \#15, \#16). Statistical significance tests between LMM-RGCL and SFT further confirm this, with all \textit{p}-values below 0.05. Notably, as indicated in \#16, Qwen2VL-7B fine-tuned with LMM-RGCL outperforms VPD-PaLI-X-55B on HatefulMemes. Moreover, LMM-RGCL improves upon RGCL with gains of over 4\% in AUC and 3\% in accuracy on HatefulMemes. These gains show LMM-RGCL’s effectiveness in improving LMMs for meme classification over SFT.




\subsection{Comparing LMM-RGCL with Baseline Systems under Low-Resource Settings}
\label{sec:results_lowres}
Online hate speech is constantly evolving, posing a challenge to systems as the distribution of memes encountered in the wild departs from that of the training data. To simulate real-world deployment constraints, we evaluate systems on out-of-domain examples under low-resource settings where gradient updates are prohibited and only demonstration examples are available \cite{Huang_LowResourceLMMAgentHatefulMeme_2024, Hee2024BridgeModality, Cao_2024_ModHate}.

We adopt a cross-dataset evaluation protocol similar to \citet{RGCL2024Mei}: models fine-tuned on \textbf{HarMeme} are evaluated on \textbf{HatefulMemes}, while models trained on \textbf{HatefulMemes} are evaluated on all other datasets. This protocol simulates a scenario in which a trained meme classification system is deployed to evaluate trending memes. Few-shot and RKC examples are drawn from the training split of each of the target evaluation datasets to avoid test set contamination.

%We compare our Retrieval-based KNN Classifier (RKC) fine-tuned with LMM-RGCL against the following systems: 
We compare LMM-RGCL fine-tuned LMM with the RKC against the following systems: SFT LMMs with zero-shot and few-shot prompting using LMH; GPT-4o \cite{OpenAI2024gpt4o}; specialized low-resource systems (LOREHM \cite{Huang_LowResourceLMMAgentHatefulMeme_2024}, Mod-hate \cite{Cao_2024_ModHate}). For GPT-4o, the token likelihood is not accessible to compute the AUC score. For Mod-Hate and LOREHM, we report the results from the original papers.

\textbf{Observation 1: Fine-tuning on one memes classification dataset does not help LMMs to improve generalization on other meme classification datasets}\\
Cross-domain fine-tuned LMMs show no consistent improvements over pre-trained LMMs for either zero-shot or few-shot prompting.  Qwen2VL-7B zero-shot (\#11 in Table~\ref{tab:results_low_resource}) matches its SFT model performance (\#13 in Table~\ref{tab:results_supervised}) on HatefulMemes and PrideMM but has performance degradation on the remaining four datasets.

\textbf{Observation 2: In-context learning remains ineffective for SFT LMMs}\\
%Consistent with Observation 2 of Section~\ref{sec:results_supervise}, after LMMs are fine-tuned on different domains of hateful meme datasets, the few-shot approach remains similarly ineffective as shown in Table~\ref{tab:results_generalize} \#6, \#9 \#12.
As shown in Table~\ref{tab:results_low_resource} \#6, \#9 and \#12, the few-shot approach remains similarly ineffective after LMMs are fine-tuned on different domains of hateful meme datasets, offering no significant gains over the SFT zero-shot models (Table~\ref{tab:results_low_resource} \#5, \#8,  \#11).
%The same conclusions apply, highlighting the limited benefit of in-context learning in these scenarios. 
In Section~\ref{sec:ablate_icl_rkc}, we further compare the number of shots for in-context learning and find that adding more shots does not improve performance.

\textbf{Observation 3: LMM-RGCL fine-tuned LMMs with RKC inference mode outperforms baseline methods}\\
LMM-RGCL fine-tuned LMMs using RKC outperform the baseline SFT LMMs in both zero-shot and few-shot settings under the same cross-dataset fine-tuning settings. Notably, LMM-RGCL trained Qwen2VL-7B with RKC improves over the baseline SFT few-shot model by 21.6\% in AUC and 19.3\% in accuracy on HarMeme (Table~\ref{tab:results_low_resource} \#11-13). The ablation study in Section~\ref{sec:ablate_icl_rkc}, which varies the number of top k for RKC, further demonstrates that RKC uses demonstration examples more effectively than the few-shot in-context learning framework. Additionally, we compare RKC results for pre-trained and SFT LMMs in Appendix~\ref{appendix:rkc_icl_ft}.

\textbf{Observation 4: LMM-RGCL trained LMMs with RKC inference outperform other low resource methods}\\
Compared to GPT-4o, our LMM-RGCL fine-tuned Qwen2VL-7B with RKC achieves 14.5\% higher accuracy on HarMeme and 11.3\% higher accuracy on PrideMM. When compared to other low-resource methods using similar open-source LMMs, our LMM-RGCL fine-tuned LLaVA-1.5-7B with RKC matches the performance of LOREHM on the HatefulMemes dataset. Notably, LOREHM uses a much larger LLaVA-1.6-34B within an agent-based framework. Furthermore, our method outperforms LOREHM by 8.2\% in accuracy on HarMeme, highlighting our methods' effectiveness under low-resource settings.


% two observations baseline/gpt4o
%\subsection{Ablation Study}
\subsection{Effects of Two-Stage Fine-tuning}
We assess the impact of each stage in the two-stage LMM-RGCL fine-tuning process. As shown in Table~\ref{tab:ablation_stages_supervised} and Table~\ref{tab:ablation_stages_cross}, omitting either stage results in performance losses under both supervised and cross-dataset settings. Excluding Stage 1 results in the largest performance drop, particularly because the Large Multimodal Model (LMM) backbone remains frozen during Stage 2.

\begin{table}[htb]
\centering

\begin{subtable}[t]{0.46\textwidth}
\small
\centering
\begin{tabular}{l|ll|ll}
\toprule
 &\multicolumn{2}{c|}{\textbf{HatefulMemes}} & \multicolumn{2}{c}{\textbf{HarMeme}}\\
 Mode & \textbf{AUC} & \textbf{Acc.} & \textbf{AUC} & \textbf{Acc.} \\ \midrule
LMM-RGCL & \textbf{91.1} & \textbf{82.1} & \textbf{93.2} & \textbf{88.1} \\
~~~\textit{w/o Stage 1} & 84.4 & 74.2 & 90.1 & 85.6 \\
~~~\textit{w/o Stage 2} & 90.2 & 81.4 & 92.0 & 86.2 \\
~~~\textit{Combined stages} & 88.9 & 77.8 & 90.2 & 83.4 \\
\bottomrule
\end{tabular}
\caption{Supervised settings, see Table~\ref{tab:results_supervised} for detailed settings}
\label{tab:ablation_stages_supervised}
\vspace{3pt}
\end{subtable}
\hfill
\begin{subtable}[t]{0.46\textwidth}
\small
\centering
\begin{tabular}{l|ll|ll}
\toprule
 &\multicolumn{2}{c|}{\textbf{HatefulMemes}} & \multicolumn{2}{c}{\textbf{HarMeme}}\\
 Mode & \textbf{AUC} & \textbf{Acc.} & \textbf{AUC} & \textbf{Acc.} \\ \midrule
LMM-RGCL & \textbf{77.1} & \textbf{69.3} & \textbf{88.8} & \textbf{81.7} \\ 
~~~\textit{w/o Stage 1} & 72.0 & 62.1 & 84.9 & 78.1 \\

~~~\textit{w/o Stage 2} & 74.4 & 66.7 & 86.3 & 78.7 \\
~~~\textit{Combined stages} & 72.2 & 65.3 & 87.5 & 80.2 \\
\bottomrule
\end{tabular}
\caption{Cross-dataset settings, see Table~\ref{tab:results_low_resource} for detailed settings}
\label{tab:ablation_stages_cross}
\end{subtable}
\caption{Ablation study of LMM-RGCL two-stage fine-tuning framework on Qwen2VL-7B, evaluating the impact of Stage 1 Joint Multimodal Fine-tuning and Stage 2 RGCL Fine-tuning. For `combined stages', we jointly optimize the three loss objectives from both stages in a single training process.}
\label{tab:ablation_stages}
\end{table} 
When stage 2 is omitted, the performance loss in supervised settings is less severe than in cross-dataset settings. We attribute this to the use of contrastive loss in Stage 2, which explicitly optimizes retrieval capabilities by aligning representations of semantically similar meme pairs. This alignment enhances robustness to distribution shifts in unseen datasets.

For `combined stages', we jointly optimize the objectives for both stages: 
\begin{equation}
\mathcal{L}_i^{\textrm{Combined}} = \mathcal{L}_i^{RGCLL} + \mathcal{L}_i^{LR} + \mathcal{L}_i^{LM}.
\label{eq:combined}
\end{equation}
It yields suboptimal results, with performance drops of 2.2\% AUC on HatefulMemes and 3.0\% AUC on HarMeme under supervised settings. These results show that the two-stage fine-tuning approach effectively mitigates the optimization conflict between task adaptation in stage 1 and representation alignment in stage 2. Furthermore, since the LMM remains trainable under this combined loss objective, updating the encoded meme database incurs significantly higher computational costs compared to the two-stage fine-tuning approach, where the LMM is frozen in stage 2. This separation of fine-tuning stages enables more efficient training while preserving strong performance. Additional details on training time are provided in Appendix~\ref{appendix:run_time}.

For each stage, we perform an ablation study by removing individual loss objectives during fine-tuning. Our results indicate that each term is essential for optimal performance. Detailed results are provided in Appendix~\ref{appendix:abl_loss}.

\subsection{Numbers of Shots and Neighbors}
\label{sec:ablate_icl_rkc}
We ablate the effects of varying the number of shots for few-shot in-context learning and varying the number of top K nearest neighbors for RKC.

Figure~\ref{fig:abl_shots_topk} demonstrates that increasing the number of in-context examples for LMMs does not consistently yield performance improvements over the zero-shot setting, and in some cases even causes loss.
These findings suggest that merely adding more shots does not necessarily improve performance, which is consistent with findings from \citet{Huang_LowResourceLMMAgentHatefulMeme_2024}. 

Figure~\ref{fig:abl_shots_topk} shows that as the number of nearest neighbors $K$ for RKC increases, the performance continues to increase for both AUC and accuracy, plateauing at around $K=20$. The consistent improvement in performance indicates that RKC trained with LMM-RGCL utilizes demonstration examples more effectively than the standard in-context learning framework.

\begin{figure}[h]
    \centering    \includegraphics[width=\linewidth]{Figs/combined_performance_new.png}
    \caption{Effects of increasing number of shots for in-context learning with pre-trained LMM and effects of increasing top K nearest neighbors for RKC trained with LMM-RGCL }
    \label{fig:abl_shots_topk}
\end{figure}
\subsection{Comparing Different Inference Modes}
\label{sec:abl_infer_mode}
%In our experiments the default inference mode varies by setting: for the pre-trained and SFT models, we use the \howard{LM Head} (LMH), whereas for LMM-RGCL under supervised settings, we use logistic regression (LR). The Retrieval-based KNN Classifier (RKC) is employed as the default for LMM-RGCL in cross-dataset settings. 
Table~\ref{tab:ablation_modes} compares Qwen2VL-7B fine-tuned with LMM-RGCL using the three classifiers. Our results indicate that under supervised settings, the differences among the three inference modes are minimal. However, under cross-dataset settings, there is a significant disparity in generalization performance. Notably, RKC outperforms both LMH and LRC, underscoring its superior effectiveness in handling out-of-domain examples.
\begin{table}[htb]
\centering
\begin{subtable}[t]{0.45\textwidth}
\small
\centering
\begin{tabularx}{\textwidth}{l|ll|ll}
\toprule
 &\multicolumn{2}{c|}{\textbf{HatefulMemes}} & \multicolumn{2}{c}{\textbf{HarMeme}}\\
 Inference Mode & \textbf{AUC} & \textbf{Acc.} & \textbf{AUC} & \textbf{Acc.} \\ \midrule
LMH & 90.2 & 81.9 & 92.8 & 88.0 \\
LRC & 91.1 & 82.1 & 93.2 & 88.1 \\
RKC & 90.8 & 81.8 & 93.2 & 88.0 \\
\bottomrule
\end{tabularx}
\caption{Supervised settings, see Table~\ref{tab:results_supervised} for detailed settings}
\label{tab:ablation_mode_supervised}
\vspace{3pt}
\end{subtable}
\hfill
\begin{subtable}[t]{0.45\textwidth}
\small
\centering
\begin{tabularx}{\textwidth}{l|ll|ll}
\toprule
 &\multicolumn{2}{c|}{\textbf{HatefulMemes}} & \multicolumn{2}{c}{\textbf{HarMeme}}\\
 Inference Mode & \textbf{AUC} & \textbf{Acc.} & \textbf{AUC} & \textbf{Acc.} \\ \midrule
LMH & 70.2 & 64.3 & 64.5 & 60.3 \\
LRC & 59.5 & 55.4 & 57.9 & 52.2 \\
RKC & 77.1 & 69.3 & 88.8 & 81.7\\ 
\bottomrule
\end{tabularx}
\caption{Cross-dataset settings, see Table~\ref{tab:results_low_resource} for detailed settings}
\label{tab:ablation_mode_cross}
\end{subtable}
\caption{Comparing different inference modes using LMM-RGCL fine-tuned Qwen2VL-7B. RKC shows much better out-of-domain generalization compared to other inference modes.}
\label{tab:ablation_modes}
\end{table}


 

\section{Conclusion}
We propose LMM-RGCL, a two-stage fine-tuning framework designed to improve LMM performance on hateful meme classification, addressing the ineffectiveness of standard SFT. Our approach effectively improves both in-domain accuracy and cross-domain generalization. State-of-the-art performance across six meme classification datasets demonstrates the effectiveness of LMM-RGCL.
%We introduce Retrieval-Guided Contrastive Learning to enhance any VL encoder in addressing challenges in distinguishing confounding memes. Our approach uses novel auxiliary loss with dynamically retrieved examples and significantly improves contextual understanding. 
%Achieving an AUC score of $87.0\%$ on the HatefulMemes dataset, our system outperforms prior state-of-the-art models. Our approach also transfers to different tasks, emphasizing its usefulness across diverse meme domains.

\section*{Limitations}

%Hate speech can be defined by different terminologies, such as online harassment, online aggression, cyberbullying, or harmful speech. The United Nations Strategy and Plan of Action on Hate Speech notes that definitions of hateful speech could be controversial and disputed \cite{united_nations_2020}. 
%According to the UK Online Harms White Paper, harms may also be insufficiently defined \cite{UKparliament2022}. 

Hate speech is described using various terminologies, including online harassment, online aggression, cyberbullying, and harmful speech. The United Nations Strategy and Plan of Action on Hate Speech acknowledges that definitions of hate speech can be controversial and subject to debate \cite{united_nations_2020}. Similarly, the UK Online Harms White Paper highlights that certain harms may be insufficiently defined \cite{UKparliament2022}.

We acknowledge that the definition of hate speech can be subjective and varies across different cultural and legal contexts. 
To this end, we evaluate our methods on six widely used meme classification datasets, allowing for generalization across different definitions of hate speech. As the discourse on defining hate speech evolves, we align our research with this ongoing process and plan to incorporate new datasets as they become available.

In our error analysis, we find that the system is unable to recognize subtle visual details in memes. Enhancing image understanding through a more powerful vision encoder could further improve performance, which we leave for future work.
%This limitation might arise from the models' inability to comprehend facial expressions, which remains a constraint of our approach. Such challenges could potentially be addressed with a more advanced vision encoder.

\section*{Ethical Statement}
\paragraph{Reproducibility.} Detailed experimental setups, implementation specifics, and hyperparameter settings are provided in Appendix~\ref{appendix:exp_setup} to ensure reproducibility. The source code will be released upon publication.

\paragraph{Usage of Datasets.}
The datasets used in this study—HatefulMemes, HarMeme, MAMI, Harm-P, MultiOFF, and PrideMM—were curated for research purposes to combat online hate speech. We strictly adhere to the terms of use established by the dataset authors.

\paragraph{Societal benefits.}
Hateful meme detection systems, like LMM-RGCL, can be used to automatically detect hateful content online, contributing significantly to reducing online hate speech. By reducing hate speech, fostering safer digital environments, and supporting human content moderators, these systems can make a significant impact on online communication and safety. We believe these benefits are both substantial and essential in the broader effort to create a more secure and respectful digital space.

\paragraph{Intended use.}
We intend to enforce strict access controls for model release. The model will be available only to researchers who agree to our terms of use, which explicitly state that the system is designed solely for the detection and prevention of hateful speech. Its use for any purposes that promote, condone, or encourage hate speech or harmful content is strictly prohibited.

\paragraph{Misuse Potential.} 
Although our system is not inherently designed to induce bias, training on existing datasets such as HatefulMemes may inadvertently propagate existing biases towards certain individuals, groups, or entities~\cite{PramanickMomenta2021}. To mitigate the risk of unfair moderation resulting from these dataset-induced biases, it is essential to incorporate human oversight into the moderation process if deployed.

\paragraph{Deployment consideration.}


Cultural differences and subjective topics introduce biases in moderating online hate speech. Expressions that may seem benign to some can be deeply offensive to others.
Our RKC inference mode relies on retrieving examples that generalize well across various domains, allowing the creation of multiple retrieval sets tailored to diverse cultural sensitivities without requiring retraining. However, before deploying such systems, it is crucial to carefully evaluate dataset annotations, particularly when addressing cultural differences and subjective interpretations. Key factors include data curation guidelines, potential annotator biases, and the inherently context-dependent definitions of hate speech. These considerations are essential to ensuring the system is deployed responsibly and effectively across varied cultural contexts.


\paragraph{Environmental Impact}
Training large-scale models is computationally intensive and contribute to global warming due to heavy GPU/TPU usage. However, our approach mitigates this issue by fine-tuning LMMs using quantized LoRA, a parameter-efficient method. As a result, our system can be trained in under four hours on a single GPU, significantly reducing both training time and computational cost compared to full-scale LMM fine-tuning. Furthermore, since our method generalizes across different domains without requiring retraining, it further minimizes computational overhead.

\begin{comment}
    

\section*{Acknowledgments}
Jingbiao Mei is supported by Cambridge Commonwealth, European and International Trust for the undertaking of the PhD in Engineering at the University of Cambridge.


Jinghong Chen is supported by the Warwick Postgraduate Studentship from Christ’s College and the Huawei Hisilicon Studentship for the undertaking of the PhD in Engineering at the University of Cambridge.

Weizhe Lin is supported by a Research Studentship funded by Toyota Motor Europe (RG92562(24020)) for the undertaking of the PhD in Engineering at the University of Cambridge.

Prof. Bill Byrne holds concurrent appointments as a Professor of Information Engineering at Cambridge University and as an Amazon Scholar.  This publication describes work performed at Cambridge University and is not associated with Amazon.

We would also like to thank all the reviewers for their knowledgeable reviews.

\end{comment}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}


\appendix
\label{sec:appendix}
\section{Dataset details and statistics}
\label{appendix:data_stats}


Table~\ref{tab:dataset_stats} shows the data split for our evaluation datasets. 

\paragraph{HatefulMemes} \cite{KielaFBHMC2020}
Released by Meta in 2020, HatefulMemes contains 12,000 memes annotated as hateful or benign by trained experts. This benchmark dataset synthesizes memes targeting religion, race, disability, and gender. It includes confounder examples where the benign memes are generated by altering either the image or text to challenge models’ ability in multimodal reasoning. 

\paragraph{HarMeme and Harm-P} 

HarMeme is a dataset containing approximately 3,000 memes centered on COVID-19 related political memes. A companion dataset, Harm-P \cite{PramanickMomenta2021}, contains around 3,000 memes related to US politics. Although the original HarMeme was later renamed Harm-C in subsequent work, we adhere to its original name following previous studies \cite{caoPromptHate2022}. In HarMeme, memes are annotated into three classes: very harmful, partially harmful, and harmless. Consistent with prior work \cite{caoPromptHate2022, PramanickMomenta2021}, we merge the very harmful and partially harmful categories into a single hateful class, while treating harmless memes as benign.

\paragraph{MAMI} \cite{Fersini_MAMI_2022}
The MAMI dataset focuses on detecting misogynistic memes sourced from various social media platforms, including Twitter and Reddit, as well as meme creation and sharing websites, and even anti-women websites and forums. It contains annotation for two tasks: (1) binary classification of misogyny and (2) categorization of misogyny types. In this work, we address the binary task of identifying whether a meme is misogynistic.

\paragraph{MultiOFF} \cite{suryawanshi-etal-2020-MultiOFF}
MultiOFF consists of memes gathered from Reddit, Facebook, Twitter, and Instagram, curated specifically for the detection of offensive content. Notably, the training set is extremely small, containing fewer than 500 meme examples. We use this dataset to evaluate the applicability of our methods under ultra low-resource conditions.

\paragraph{PrideMM} \cite{Shah2024memeclip_pridemm}
PrideMM contains LGBTQ+-themed memes annotated for four tasks: hate speech detection, hate target identification, topical stance classification, and humor detection. In this work, we use the hate speech classification annotations for hateful meme detection.

\begin{table}[h]
\small
\centering

\begin{tabular}{l|cc|cc}
\toprule
Datasets  & \multicolumn{2}{c}{Train}  & \multicolumn{2}{c}{Test}  \\
& \#Benign & \#Hate & \#Benign & \#Hate  \\
\midrule
HatefulMemes &   5450 & 3050 & 500 & 500      \\
HarMeme  & 1949 & 1064 & 230 & 124          \\ 
MAMI &  4500 & 4500 & 500 & 500 \\ 
Harm-P & 1534 & 1486 & 173 & 182 \\ 
MultiOFF & 258 & 187 & 58 & 91 \\ 
PrideMM & 2581 & 2482 & 260 & 247  \\ 

\bottomrule
\end{tabular}%
\caption{Statistical summary of HatefulMemes and HarMeme datasets}
\label{tab:dataset_stats}
\end{table}


To access the Facebook HatefulMemes dataset, one must follow the license from Facebook\footnote{\href{https://hatefulmemeschallenge.com/\#download}{https://hatefulmemeschallenge.com/\#download}}. HarMeme and Harm-P are distributed for research purposes only, without a license for commercial use. 
MultiOFF is licensed under CC-BY-NC. MAMI is under Apache License 2.0. There is no specified license for PrideMM.

\section{Experiment Setup and Implementation Details}
\label{appendix:exp_setup}
\paragraph{Environment.} 
\texttt{PyTorch 2.5.1}, \texttt{CUDA 12.4}, Huggingface \texttt{Transformer 4.45.0 } and \texttt{Python 3.10.12} were used for implementing the experiments. FAISS \cite{johnson_Faiss2019billion} vector similarity search library with version \texttt{faiss-gpu 1.7.2} was used to perform dense retrieval.  All the reported metrics were computed by \texttt{TorchMetrics 1.0.1}. 
\paragraph{Implementation Details.} We use QLoRA \cite{Dettmers2023qlora} to fine-tune all LMMs, as our experiments show that LoRA and QLoRA perform similarly on this task while significantly outperforming full-parameter fine-tuning. The details for fine-tuning are covered in Appendix~\ref{appendix:LMM_experiments}. All reported metrics were based on the mean of five runs with different seeds. For statistical significance testing, each model is run five times with different random seeds. For baseline models, we strictly follow the settings specified in their original papers.
\paragraph{Implementation environment.}
We conducted our experiments on a workstation equipped with NVIDIA RTX 3090 was used for the experiments. 
The full parameter fine-tuning experiments were carried out on 4 A100-80GB-SXM GPUs. 

\paragraph{Run time}
\label{appendix:run_time}
The run time for LMM-RGCL two-stage fine-tuning on the HatefulMemes dataset is approximately 4 hours on a single RTX 3090, which is comparable to SFT.

To optimize efficiency in stage 2, we pre-extract the final hidden states from the frozen LMM and store them on disk before training, avoiding redundant LMM computations. This reduces the stage 2 training time to approximately 10 minutes.

In our ablation study, we examine the performance impact of merging the two-stage loss into a single fine-tuning stage. Since the LMM remains trainable in this setting, we cannot precompute and store the frozen LMM features, leading to significantly higher computational costs. This approach requires approximately 12 hours to complete fine-tuning on a single RTX 3090.

For full-parameter fine-tuning, training takes 6 hours on 4 A100-80G.

\subsection{LLaVA and Qwen2VL experiments}
\label{appendix:LMM_experiments}
We freeze the vision module throughout fine-tuning, following the standard LMM fine-tuning protocol. For prompt formatting, we adhere to InstructBLIP~\cite{DaiInstructBLIP2023}. For LLaVA few-shot experiments, since LLaVA is not explicitly trained to support in-context learning, we follow the procedure outlined by \citet{zong2024vlicl} to enable few-shot learning on LLaVA.
For fine-tuning LLaVA~\cite{LiuLLAVA2023, Liu_2023_LLAVA1.5}, we follow the original hyperparameters setting\footnote{\href{https://github.com/haotian-liu/LLaVA}{https://github.com/haotian-liu/LLaVA}} for fine-tuning on downstream tasks for both the SFT and LMM-RGCL stage 1 fine-tuning. 

For Qwen2VL fine-tuning, we employ the officially recommended fine-tuning library \texttt{LLaMA-Factory 0.9.1}\footnote{\href{https://github.com/hiyouga/LLaMA-Factory}{https://github.com/hiyouga/LLaMA-Factory}} with official hyperparameter settings for downstream tasks in both the SFT and LMM-RGCL stage 1 fine-tuning. For few-shot learning with Qwen2VL, we follow the official multi-round conversation prompt format to ensure consistency with the model’s intended usage.

\subsection{Hyperparameters for MLP and Stage 2 Fine-tuning}
\label{appendix:hyperparam}
The default hyperparameters for the MLP and the stage 2 RGCL fine-tuning are shown in Table~\ref{tab:hyperparameters}. The modeling hyperparameters are based on RGCL's setting \cite{RGCL2024Mei}. With this configuration of hyperparameters, the number of trainable parameters is about 5 million. 
\begin{table}[h]
\small
\centering
\begin{tabular}{ll}
\toprule
Modelling hyperparameter & Value \\
\midrule
Projection dimension of MLP & 1024 \\
Number of layers in the MLP & 2 \\
Optimizer & AdamW \\
Maximum epochs & 30 \\
Batch size & 64 \\
Learning rate & 0.0001 \\
Weight decay & 0.0001 \\
Gradient clip value & 0.1 \\
\midrule 
RGCL hyperparameter  & Value \\
\midrule
\# hard negative examples & 1 \\
\# pseudo-gold positive examples & 1 \\
Similarity metric & Cosine similarity \\
Loss function & NLL \\
Top-K for RKC & 20\\ 

\bottomrule
\end{tabular}
\caption{Default hyperparameter values}
\label{tab:hyperparameters}
\end{table}




\section{Baseline Methods}
\label{appendix:baseline_models}
\begin{itemize}
    \item \textbf{Visual Programming Distillation (VPD)} \cite{Hu_2024_VPD} builds an agentic LMM framework by fine-tuning the model's ability to use external tools (e.g., writing and executing programs). VPD fine-tunes PaLI-X 55B, achieving state-of-the-art performance on the HatefulMemes dataset.
    \item \textbf{ISSUES} \cite{Burbi_2023_Issues} employs text inversion along with several projection layers and a feature combiner to enhance the pre-trained CLIP encoder, yielding state-of-the-art results on the HarMeme dataset. 
    \item \textbf{RGCL} \cite{RGCL2024Mei} learns hate-aware vision and language representations through a contrastive learning objective applied to a pre-trained CLIP encoder, achieving state-of-the-art performance on the MultiOFF dataset.
    \item \textbf{ExplainHM} \cite{Lin_2024_ExplainHM} fine-tunes three LLMs arranged as two debaters (arguing whether a meme is hateful) and one judge (summarizing the debaters’ points) to both explain and classify hateful memes.
    \item \textbf{Pro-Cap} \cite{Cao_2023_ProCap} employs prompting techniques to guide pre-trained vision-language models in generating image captions that reflect hateful content. These generated captions are then combined with textual information to improve hateful meme detection.
    \item \textbf{MemeCLIP} \cite{Shah2024memeclip_pridemm} utilizes CLIP features along with feature adapters to mitigate overfitting and employs a cosine classifier to address class imbalance.
    \item \textbf{MOMENTA} \cite{PramanickMomenta2021} leverages trainable fusion layers—such as Cross-Modal Attention Fusion—to integrate multimodal features extracted by CLIP for improved hateful meme detection.
    \item \textbf{HateCLIPper} \cite{KumarHateClip2022} explores various strategies to align and fuse the visual and textual modalities in CLIP-based encoders, enhancing their performance on challenging hateful meme cases.
    \item \textbf{LOREHM} \cite{Huang_LowResourceLMMAgentHatefulMeme_2024} adopts an agent-based LMM framework that leverages few-shot in-context learning and self-improvement capabilities for low-resource hateful meme detection.
    \item \textbf{Mod-Hate} \cite{Cao_2024_ModHate} trains a suite of LoRA modules and utilizes few-shot demonstration examples to train a module composer, which assigns weights to the LoRA modules for effective low-resource hateful meme detection.
    
\end{itemize}



\section{Comparing RKC and In-Context Learning under different Fine-tuning Schemes} 
\label{appendix:rkc_icl_ft}
In this section, we compare the performance of the RKC inference mode against few-shot in-context learning for pre-trained LMMs, SFT LMMs, and LMMs fine-tuned using our proposed LMM-RGCL framework under the cross-dataset setting. As shown in Table~\ref{tab:ablation_rkc_lmms}, RKC consistently outperforms the few-shot in-context learning approach across all LMM variants, indicating that RKC makes better use of demonstration examples. Furthermore, LMM-RGCL fine-tuned LMMs with RKC outperform SFT LMMs with RKC, highlighting the advantages of our fine-tuning strategy.  
\begin{table}[h]
\centering
\small
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{ll|ll|ll}
\toprule
 & &\multicolumn{2}{c|}{\textbf{HatefulMemes}} & \multicolumn{2}{c}{\textbf{HarMeme}}\\
 Model & Mode & \textbf{AUC} & \textbf{Acc.} & \textbf{AUC} & \textbf{Acc.} \\ \midrule
Pre-trained & Few-shot & 71.5 & 63.8 & 71.5 & 67.2 \\
Pre-trained & RKC & 74.5 & 64.5 & 80.1 & 72.4 \\
SFT & Few-shot & 72.3 & 60.6 & 67.2 & 62.4 \\
SFT & RKC & 75.8 & 67.1 & 84.5 & 75.4 \\
LMM-RGCL&Few-shot & 70.8 & 63.5 & 73.2 & 68.1 \\
LMM-RGCL&RKC & 77.1 & 69.3 & 88.8 & 81.7\\ 

\bottomrule
\end{tabular}
}
\caption{Comparing Pre-trained, SFT and LMM-RGCL systems with few-shot learning and RKC with Qwen2VL-7B under cross-dataset settings. See Table~\ref{tab:results_low_resource}}
\label{tab:ablation_rkc_lmms}
\end{table}



\section{Ablation study on the loss function}
\label{appendix:abl_loss}
Table~\ref{tab:ablation_loss} shows the results when each loss objective is removed from different stages of fine-tuning. Notably, when the cross-entropy loss is removed in stage 1 for the logistic regression component, the LRC fails to train properly via backpropagation, resulting in performance that is equivalent to random guessing. Consequently, we exclude this case from our comparison. Overall, we observe that removing any loss function from the fine-tuning objective leads to a significant drop in performance, highlighting the importance of each loss term in optimizing the model.
\begin{table}[h]
\centering
\begin{subtable}[t]{0.5\textwidth}
\small
\centering
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{l|ll|ll}
\toprule
 &\multicolumn{2}{c|}{\textbf{HatefulMemes}} & \multicolumn{2}{c}{\textbf{HarMeme}}\\
 Mode & \textbf{AUC} & \textbf{Acc.} & \textbf{AUC} & \textbf{Acc.} \\ \midrule
LMM-RGCL & \textbf{91.1} & \textbf{82.1} & \textbf{93.2} & \textbf{88.1} \\
~\textit{w/o  $\mathcal{L}^{LM}$ in stage 1} & 88.4 & 79.6 & 90.9 & 85.1 \\
~\textit{w/o  $\mathcal{L}^{RGCLL}$ in stage 2} & 90.2 & 81.2 & 91.9 & 86.4 \\
~\textit{w/o  $\mathcal{L}^{LR}$ in stage 2} & 89.2 & 80.6 & 91.6 & 87.2 \\
\bottomrule
\end{tabular}
}
\caption{Supervised settings, see Table~\ref{tab:results_supervised}}
\label{tab:ablation_loss_supervised}
\vspace{3pt}
\end{subtable}
\hfill
\begin{subtable}[t]{0.5\textwidth}
\small
\centering
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{l|ll|ll}
\toprule
 &\multicolumn{2}{c|}{\textbf{HatefulMemes}} & \multicolumn{2}{c}{\textbf{HarMeme}}\\
 Mode & \textbf{AUC} & \textbf{Acc.} & \textbf{AUC} & \textbf{Acc.} \\ \midrule
LMM-RGCL & \textbf{77.1} & \textbf{69.3} & \textbf{88.8} & \textbf{81.7} \\ 
~\textit{w/o  $\mathcal{L}^{LM}$ in stage 1} & 75.4 & 66.6 & 87.3 & 81.1 \\
~\textit{w/o  $\mathcal{L}^{RGCLL}$ in stage 2} & 73.8 & 64.3 & 82.9 & 76.5 \\
~\textit{w/o  $\mathcal{L}^{LR}$ in stage 2} & 76.4 & 67.9 & 86.9 & 80.6 \\
\bottomrule
\end{tabular}
}
\caption{Cross-dataset settings, see Table~\ref{tab:results_low_resource}}
\label{tab:ablation_loss_cross}
\end{subtable}
\caption{Ablation study of LMM-RGCL two-stage fine-tuning framework on Qwen2VL-7B, evaluating the impact of removing any of the loss objectives.}
\label{tab:ablation_loss}
\end{table} 


\section{Case Analysis}
\label{appendix:case_analysis}



\begin{table*}[hb]
    \small
    \centering
    %\renewcommand{\arraystretch}{1.5}

    \begin{tabularx}{\textwidth}{Xccc}
    \toprule
        & Case 1 & Case 2 & Case 3 \\
        \midrule
        Meme &\includegraphics[valign=c, width=0.265\textwidth]{Figs/demo/3.png} & 
        \includegraphics[valign=c, width=0.26\textwidth]{Figs/demo/2.png} &  
        \includegraphics[valign=c, width=0.22\textwidth]{Figs/demo/1.png}   \\
        \midrule
        Ground Truth &  \#Hateful & \#Hateful & \#Hateful  \\
        SFT  &  \#Benign & \#Benign & \#Benign \\
        LMM-RGCL  &  \#Hateful & \#Hateful & \#Hateful\\
        \toprule
        &Case 4 & Case 5 & Case 6 \\
        \midrule
        Meme &\includegraphics[valign=c, width=0.26\textwidth]{Figs/demo/4.png} &  
        \includegraphics[valign=c, width=0.26\textwidth]{Figs/demo/b1.jpg} &  
        \includegraphics[valign=c, width=0.22\textwidth]{Figs/demo/b2.jpg}   \\
        \midrule
        Ground Truth &  \#Hateful &  \#Benign & \#Benign  \\
        SFT  &  \#Benign & \#Hateful & \#Hateful \\
        LMM-RGCL  &  \#Hateful & \#Benign & \#Benign \\
        \bottomrule
    \end{tabularx}
    \caption{Visualization of cases from SFT Qwen2VL-7B and LMM-RGCL Qwen2VL-7B Models on the HatefulMemes Dataset. Case 5 contains an insect in the meme; we applied a blurring filter to obscure it. }
    \label{tab:case}
\end{table*}

\begin{table*}[h]
    \small
    \centering
    %\renewcommand{\arraystretch}{1.5}

    \begin{tabularx}{\textwidth}{Xccc}
    \toprule
        & Case 1 & Case 2 & Case 3 \\
        \midrule
        Meme &\includegraphics[valign=c, width=0.26\textwidth]{Figs/demo/wrong1.png} & 
        \includegraphics[valign=c, width=0.26\textwidth]{Figs/demo/wrong2.png} &  
        \includegraphics[valign=c, width=0.26\textwidth]{Figs/demo/wrong3.png}   \\
        \midrule
        Ground Truth &  \#Hateful & \#Hateful & \#Hateful  \\
        SFT  &  \#Benign & \#Benign & \#Benign \\
        LMM-RGCL  &  \#Benign & \#Benign & \#Benign\\
        \bottomrule
    \end{tabularx}
    \caption{The error cases of SFT Qwen2VL-7B and LMM-RGCL Qwen2VL-7B models on HatefulMemes dataset}
    \label{tab:error_case}
\end{table*}
\subsection{Comparing SFT and LMM-RGCL Predictions}
Table~\ref{tab:case} presents examples where our LMM-RGCL method successfully corrects prediction errors made by the SFT model on Qwen2VL-7B. Cases 1-4 involve hateful memes, while Cases 5-6 are benign memes that the SFT model misclassified, primarily due to poor multimodal alignment. These examples require a deep, joint understanding of both the image and text, a challenge that our LMM-RGCL effectively addresses. For example, in Case 2, the model needs to use its understanding of Japanese culture and associate this knowledge with the visual cues in the image.

\subsection{Error Analysis}
\label{appendix:error_analysis}
In Table~\ref{tab:error_case}, we present examples where LMM-RGCL was unable to correct errors made by the baseline SFT model. In the first case, the model struggles with the nuanced visual understanding required to interpret the disabled body of the swimmer. Additionally, these examples demand complex reasoning to assess the hatefulness of the memes. Interpreting such nuanced meanings remains a challenge for current models. However, we anticipate that the advanced reasoning capabilities of emerging systems like OpenAI-o1 \cite{OpenAI2024o1} and DeepSeek-R1 \cite{DeepSeekAI2025_r1} will help address these limitations.

\section{AI Assistance}
Our coding work was assisted by Github Copilot.
OpenAI ChatGPT was only used in proofreading and spell-checking. We claim that the content
presented in this paper was fully original.
\end{document}
