% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}



%%% Social issues
 @misc{socialmediadata_2022, title={Digital 2022: Global Overview Report - DataReportal – Global Digital Insights}, url={https://datareportal.com/reports/digital-2022-global-overview-report}, journal={DataReportal}, publisher={DataReportal – Global Digital Insights}, author={Kemp, Simon}, year={2022}, month={May}} 

 @misc{onlinehatebbc_2021, title={Online hate speech rose 20\% during pandemic: 'we've Normalised it'}, url={https://www.bbc.co.uk/news/newsbeat-59292509}, journal={BBC News}, publisher={BBC}, author={Baggs, Michael}, year={2021}, month={Nov}} 


 @misc{FBtrauma_2019, title={The trauma floor}, url={https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona}, journal={The Verge}, publisher={The Verge}, author={Newton, Casey}, year={2019}, month={Feb}} 

 @misc{FBTimes_hate_algorithm, title={Facebook's hate speech algorithms leave out some languages}, url={https://time.com/5739688/facebook-hate-speech-languages/}, journal={Time}, publisher={Time}, author={Perrigo, Billy}, year={2019}, month={Nov}} 

 @misc{anti-hate_ADL2020, title={An open letter to the companies that advertise on Facebook}, url={https://www.adl.org/news/letters/an-open-letter-to-the-companies-that-advertise-on-facebook}, journal={Anti-Defamation League}, year={2020}, month={Jun}} 
 
 @misc{Fbdoc_2021, title={Facebook claims it uses AI to identify and remove posts containing hate speech and violence, but the technology doesn't really work, report says}, url={https://www.businessinsider.com/facebook-ai-doesnt-work-to-remove-hate-speech-and-violence-2021-10?r=US&amp;IR=T}, journal={Business Insider}, publisher={Business Insider}, author={Walsh, Emily}, year={2021}, month={Oct}} 


 @misc{united_nations_2020, title={United Nations Strategy and Plan of Action on Hate Speech}, url={https://www.un.org/en/genocideprevention/hate-speech-strategy.shtml}, abstractNote={In response to current alarming trends of growing xenophobia, racism and intolerance, violent misogyny, anti-Semitism and anti-Muslim hatred around the world, on 18 June, Secretary-General Antonio Guterres launched the United Nations Strategy and Plan of Action on Hate Speech. The strategy recognizes that over the past 75 years, hate speech has been a precursor to atrocity crimes, including genocide, from Rwanda to Bosnia to Cambodia. As noted by the Secretary-General at the launch}, journal={United Nations Strategy and Plan of Action on Hate Speech}, author={Wairimu Nderitu}, year={2020}, month=may }


@article{UKparliament2022, title={Regulating online harms}, url={https://commonslibrary.parliament.uk/research-briefings/cbp-8743/}, abstractNote={Harmful online content and activity includes cyberbullying, racism, misogynistic abuse, pornography, and material promoting violence and self-harm. The Covid-19 pandemic has seen social media platforms used to spread anti-vaccine disinformation.Critics, including parliamentary committees, academics, and children’s charities, have argued that self-regulation by internet companies is not enough to keep users safe and that statutory regulation should be introduced.}, journal={UK Parliament}, publisher={UK Parliament}, author={Woodhouse, John}, year={2022}, month=mar }

 
%%%% Related work 
%%%%%
@inproceedings{agrawal2019nocaps,
  title={nocaps: novel object captioning at scale},
  author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={8948--8957},
  year={2019}
}

@misc{efficient_trans_2020,
  doi = {10.48550/ARXIV.2009.06732},
  
  url = {https://arxiv.org/abs/2009.06732},
  
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Efficient Transformers: A Survey},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



%%%% FBMM Challenge started

@article{RonHMC1st2020, title={Enhance Multimodal Transformer With External Label And In-Domain Pretrain: Hateful Meme Challenge Winning Solution}, url={http://arxiv.org/abs/2012.08290}, DOI={10.48550/arXiv.2012.08290}, note={arXiv:2012.08290 [cs]}, number={arXiv:2012.08290}, publisher={arXiv}, author={Zhu, Ron}, year={2020}, month={Dec} }



 @article{VilioHMC2nd2020, title={Vilio: State-of-the-art Visio-Linguistic Models applied to Hateful Memes}, url={http://arxiv.org/abs/2012.07788}, DOI={10.48550/arXiv.2012.07788}, note={arXiv:2012.07788 [cs]}, number={arXiv:2012.07788}, publisher={arXiv}, author={Muennighoff, Niklas}, year={2020}, month={Dec} }


 @article{RizaHMC3rd2020, title={Detecting Hate Speech in Memes Using Multimodal Deep Learning Approaches: Prize-winning solution to Hateful Memes Challenge}, url={http://arxiv.org/abs/2012.12975}, DOI={10.48550/arXiv.2012.12975}, note={arXiv:2012.12975 [cs]}, number={arXiv:2012.12975}, publisher={arXiv}, author={Velioglu, Riza and Rose, Jewgeni}, year={2020}, month={Dec} }


 @article{LippeHMFramework2020, title={A Multimodal Framework for the Detection of Hateful Memes}, url={http://arxiv.org/abs/2012.12871}, note={arXiv:2012.12871 [cs]}, number={arXiv:2012.12871}, publisher={arXiv}, author={Lippe, Phillip and Holla, Nithin and Chandra, Shantanu and Rajamanickam, Santhosh and Antoniou, Georgios and Shutova, Ekaterina and Yannakoudakis, Helen}, year={2020}, month={Dec} }


 @article{KielaFBHMC2020, title={The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes}, url={http://arxiv.org/abs/2005.04790}, note={arXiv:2005.04790 [cs]}, number={arXiv:2005.04790}, publisher={arXiv}, author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide}, year={2021}, month={Apr} }


@article{Mathias_Fine_GrainedFBHM_2021, title={Findings of the WOAH 5 Shared Task on Fine Grained Hateful Memes Detection}, DOI={10.18653/v1/2021.woah-1.21}, journal={Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)}, author={Mathias, Lambert and Nie, Shaoliang and Davani, Aida Mostafazadeh and Kiela, Douwe and Prabhakaran, Vinodkumar and Vidgen, Bertie and Waseem, Zeerak}, year={2021}, pages={201–206} }

 @article{Hee_DecodeMeaningFBHM_Hatred_2023, title={Decoding the Underlying Meaning of Multimodal Hateful Memes}, url={http://arxiv.org/abs/2305.17678}, note={arXiv:2305.17678 [cs]}, number={arXiv:2305.17678}, publisher={arXiv}, author={Hee, Ming Shan and Chong, Wen-Haw and Lee, Roy Ka-Wei}, year={2023}, month={Jun} }

 @inbook{li2020oscar, address={Cham}, series={Lecture Notes in Computer Science}, title={Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks}, volume={12375}, ISBN={978-3-030-58576-1}, url={https://link.springer.com/10.1007/978-3-030-58577-8_8}, DOI={10.1007/978-3-030-58577-8_8}, booktitle={Computer Vision – ECCV 2020}, publisher={Springer International Publishing}, author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and Choi, Yejin and Gao, Jianfeng}, editor={Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael}, year={2020}, pages={121–137}, collection={Lecture Notes in Computer Science}, language={en} }


 @inbook{Uniter2019, address={Cham}, series={Lecture Notes in Computer Science}, title={UNITER: UNiversal Image-TExt Representation Learning}, volume={12375}, ISBN={978-3-030-58576-1}, url={https://link.springer.com/10.1007/978-3-030-58577-8_7}, DOI={10.1007/978-3-030-58577-8_7}, booktitle={Computer Vision – ECCV 2020}, publisher={Springer International Publishing}, author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing}, editor={Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael}, year={2020}, pages={104–120}, collection={Lecture Notes in Computer Science}, language={en} }


 @article{VisualBert2019, title={VisualBERT: A Simple and Performant Baseline for Vision and Language}, url={http://arxiv.org/abs/1908.03557}, DOI={10.48550/arXiv.1908.03557}, note={arXiv:1908.03557 [cs]}, number={arXiv:1908.03557}, publisher={arXiv}, author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei}, year={2019}, month={Aug} }


 @article{ErnieViL2020, title={ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph}, url={http://arxiv.org/abs/2006.16934}, DOI={10.48550/arXiv.2006.16934}, note={arXiv:2006.16934 [cs]}, number={arXiv:2006.16934}, publisher={arXiv}, author={Yu, Fei and Tang, Jiji and Yin, Weichong and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng}, year={2021}, month={Mar} }


@article{Paddle,
author = {Yanjun Ma and Dianhai Yu and Tian Wu and Haifeng Wang},
title = {PaddlePaddle: An Open-Source Deep Learning Platform from Industrial Practice},
publisher = {Frontiers of Data and Domputing},
year = {2019},
journal = {Frontiers of Data and Domputing},
volume = {1},
number = {1},
eid = {105},
numpages = {10},
pages = {105},
keywords = {;PaddlePaddle;artificial intelligence;deep learning;deep learning framework},
url = {http://www.jfdc.cnic.cn/EN/abstract/article_2.shtml},
doi = {10.11871/jfdc.issn.2096.742X.2019.01.011}
}    

@article{BERT2018,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


%%%% FBMM Challenge completed




%%%% New models started


 @inproceedings{vinVL2021, title={VinVL: Revisiting Visual Representations in Vision-Language Models}, url={https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.html}, author={Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng}, year={2021}, pages={5579–5588}, language={en} }


@misc{ViLBERT2019,
  doi = {10.48550/ARXIV.1908.02265},
  
  url = {https://arxiv.org/abs/1908.02265},
  
  author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{KumarHateClip2022,
    title = "Hate-{CLIP}per: Multimodal Hateful Meme Classification based on Cross-modal Interaction of {CLIP} Features",
    author = "Kumar, Gokul Karthik  and
      Nandakumar, Karthik",
    booktitle = "Proceedings of the Second Workshop on NLP for Positive Impact (NLP4PI)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlp4pi-1.20",
    doi = "10.18653/v1/2022.nlp4pi-1.20",
    pages = "171--183",
}

@inproceedings{caoPromptHate2022,
    title = "Prompting for Multimodal Hateful Meme Classification",
    author = "Cao, Rui  and
      Lee, Roy Ka-Wei  and
      Chong, Wen-Haw  and
      Jiang, Jing",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.22",
    doi = "10.18653/v1/2022.emnlp-main.22",
    pages = "321--332",
}

 @article{Zia_RacistSexist2021, title={Racist or Sexist Meme? Classifying Memes beyond Hateful}, DOI={10.18653/v1/2021.woah-1.23}, journal={Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)}, author={Zia, Haris Bin and Castro, Ignacio and Tyson, Gareth}, year={2021}, pages={215–219} }



 @article{Flamingo22, title={Flamingo: a Visual Language Model for Few-Shot Learning}, volume={35}, journal={Advances in Neural Information Processing Systems}, author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L. and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bińkowski, Mikołaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karén}, year={2022}, month={Dec}, pages={23716–23736}, language={en}, url={https://openreview.net/forum?id=EbMuimAbPbs}}

 @article{DaiInstructBLIP2023, title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, url={http://arxiv.org/abs/2305.06500}, DOI={10.48550/arXiv.2305.06500}, note={arXiv:2305.06500 [cs]}, number={arXiv:2305.06500}, publisher={arXiv}, author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale and Hoi, Steven}, year={2023}, month={May} }
 @article{LiuLLAVA2023, title={Visual Instruction Tuning}, url={http://arxiv.org/abs/2304.08485}, DOI={10.48550/arXiv.2304.08485}, note={arXiv:2304.08485 [cs]}, number={arXiv:2304.08485}, publisher={arXiv}, author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae}, year={2023}, month={Apr} }

 @article{BerriosLens2023, title={Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language}, url={http://arxiv.org/abs/2306.16410}, DOI={10.48550/arXiv.2306.16410}, note={arXiv:2306.16410 [cs]}, number={arXiv:2306.16410}, publisher={arXiv}, author={Berrios, William and Mittal, Gautam and Thrush, Tristan and Kiela, Douwe and Singh, Amanpreet}, year={2023}, month={Jun} }


%%%% New models finshed



%%%%% New dataset started


@article{MMHS150K, title={Exploring Hate Speech Detection in Multimodal Publications}, url={http://arxiv.org/abs/1910.03814}, DOI={10.48550/arXiv.1910.03814}, note={arXiv:1910.03814 [cs]}, number={arXiv:1910.03814}, publisher={arXiv}, author={Gomez, Raul and Gibert, Jaume and Gomez, Lluis and Karatzas, Dimosthenis}, year={2019}, month={Oct} }




@article{IAAKappa,
author = {McHugh, Mary},
year = {2012},
month = {10},
pages = {276-82},
title = {Interrater reliability: The kappa statistic},
volume = {22},
journal = {Biochemia medica : časopis Hrvatskoga društva medicinskih biokemičara / HDMB},
doi = {10.11613/BM.2012.031}
}

 @inproceedings{TamilTroll2020, address={Marseille, France}, title={A Dataset for Troll Classification of TamilMemes}, ISBN={979-10-95546-67-2}, url={https://aclanthology.org/2020.wildre-1.2}, booktitle={Proceedings of the WILDRE5– 5th Workshop on Indian Language Data: Resources and Evaluation}, publisher={European Language Resources Association (ELRA)}, author={Suryawanshi, Shardul and Chakravarthi, Bharathi Raja and Verma, Pranav and Arcan, Mihael and McCrae, John Philip and Buitelaar, Paul}, year={2020}, month={May}, pages={7–13}, language={English} }

 @article{SahinARCHateSpeechEvent2023, title={ARC-NLP at Multimodal Hate Speech Event Detection 2023: Multimodal Methods Boosted by Ensemble Learning, Syntactical and Entity Features}, url={http://arxiv.org/abs/2307.13829}, DOI={10.48550/arXiv.2307.13829}, note={arXiv:2307.13829 [cs]}, number={arXiv:2307.13829}, publisher={arXiv}, author={Sahin, Umitcan and Kucukkaya, Izzet Emre and Ozcelik, Oguzhan and Toraman, Cagri}, year={2023}, month={Jul} }

@inproceedings{pramanickCovidMeme2021,
    title = "Detecting Harmful Memes and Their Targets",
    author = "Pramanick, Shraman  and
      Dimitrov, Dimitar  and
      Mukherjee, Rituparna  and
      Sharma, Shivam  and
      Akhtar, Md. Shad  and
      Nakov, Preslav  and
      Chakraborty, Tanmoy",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.246",
    doi = "10.18653/v1/2021.findings-acl.246",
    pages = "2783--2796",
}

 @inproceedings{PrakashTotalDefMeme2023, address={New York, NY, USA}, series={MMSys ’23}, title={TotalDefMeme: A Multi-Attribute Meme dataset on Total Defence in Singapore}, ISBN={9798400701481}, url={https://dl.acm.org/doi/10.1145/3587819.3592545}, DOI={10.1145/3587819.3592545}, booktitle={Proceedings of the 14th Conference on ACM Multimedia Systems}, publisher={Association for Computing Machinery}, author={Prakash, Nirmalendu and Hee, Ming Shan and Lee, Roy Ka-Wei}, year={2023}, month={Jun}, pages={369–375}, collection={MMSys ’23} }


 @article{SuryawanshiTrollWithOpinion2023, title={TrollsWithOpinion: A taxonomy and dataset for predicting domain-specific opinion manipulation in troll memes}, volume={82}, ISSN={1573-7721}, DOI={10.1007/s11042-022-13796-x}, number={6}, journal={Multimedia Tools and Applications}, author={Suryawanshi, Shardul and Chakravarthi, Bharathi Raja and Arcan, Mihael and Buitelaar, Paul}, year={2023}, month={Mar}, pages={9137–9171}, language={en} }


 @inproceedings{DimitrovPropagandaMeme2021, address={Online}, title={Detecting Propaganda Techniques in Memes}, url={https://aclanthology.org/2021.acl-long.516}, DOI={10.18653/v1/2021.acl-long.516}, booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, publisher={Association for Computational Linguistics}, author={Dimitrov, Dimitar and Bin Ali, Bishr and Shaar, Shaden and Alam, Firoj and Silvestri, Fabrizio and Firooz, Hamed and Nakov, Preslav and Da San Martino, Giovanni}, year={2021}, month={Aug}, pages={6603–6617} }

 @inproceedings{HossainMUTEMeme2022, address={Online}, title={MUTE: A Multimodal Dataset for Detecting Hateful Memes}, url={https://aclanthology.org/2022.aacl-srw.5}, booktitle={Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Student Research Workshop}, publisher={Association for Computational Linguistics}, author={Hossain, Eftekhar and Sharif, Omar and Hoque, Mohammed Moshiul}, year={2022}, month={Nov}, pages={32–39} }

 @inproceedings{LiuFigMemes2022, address={Abu Dhabi, United Arab Emirates}, title={FigMemes: A Dataset for Figurative Language Identification in Politically-Opinionated Memes}, url={https://aclanthology.org/2022.emnlp-main.476}, booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, publisher={Association for Computational Linguistics}, author={Liu, Chen and Geigle, Gregor and Krebs, Robin and Gurevych, Iryna}, year={2022}, month={Dec}, pages={7069–7086} }


 @inproceedings{PramanickMomenta2021, address={Punta Cana, Dominican Republic}, title={MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets}, url={https://aclanthology.org/2021.findings-emnlp.379}, DOI={10.18653/v1/2021.findings-emnlp.379}, booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021}, publisher={Association for Computational Linguistics}, author={Pramanick, Shraman and Sharma, Shivam and Dimitrov, Dimitar and Akhtar, Md. Shad and Nakov, Preslav and Chakraborty, Tanmoy}, year={2021}, month={Nov}, pages={4439–4455} }





 @inproceedings{suryawanshi-etal-2020-MultiOFF, address={Marseille, France}, title={Multimodal Meme Dataset (MultiOFF) for Identifying Offensive Content in Image and Text}, ISBN={979-10-95546-56-6}, url={https://aclanthology.org/2020.trac-1.6}, booktitle={Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying}, publisher={European Language Resources Association (ELRA)}, author={Suryawanshi, Shardul and Chakravarthi, Bharathi Raja and Arcan, Mihael and Buitelaar, Paul}, year={2020}, month={May}, pages={32–41}, language={English} }


 @article{Alex_DetectArabicTweets2015, title={Detection of Abusive Accounts with Arabic Tweets}, volume={1}, ISSN={23826185}, DOI={10.7763/IJKE.2015.V1.19}, number={2}, journal={International Journal of Knowledge Engineering-IACSIT}, author={ Abozinadah, Ehab A. and Mbaziira, Alex V. and Jones, James H. Jr.}, year={2015}, pages={113–119}, language={en} }

 @article{ozelTukrishCyberBully2017, address={Antalya}, title={Detection of cyberbullying on social media messages in Turkish}, ISBN={9781538609309}, DOI={10.1109/UBMK.2017.8093411}, journal={2017 International Conference on Computer Science and Engineering (UBMK)}, publisher={IEEE}, author={Ozel, Selma Ayse and Sarac, Esra and Akdemir, Seyran and Aksu, Hulya}, year={2017}, month={Oct}, pages={366–370} }


 @article{Jaki_RightWingHateSpeechGerman2019, title={Right-wing German Hate Speech on Twitter: Analysis and Automatic Detection}, url={http://arxiv.org/abs/1910.07518}, DOI={10.48550/arXiv.1910.07518}, note={arXiv:1910.07518 [cs]}, number={arXiv:1910.07518}, publisher={arXiv}, author={Jaki, Sylvia and De Smedt, Tom}, year={2019}, month={Oct} }

 @inproceedings{Liu_SemEvalWinner2019, address={Minneapolis, Minnesota, USA}, title={NULI at SemEval-2019 Task 6: Transfer Learning for Offensive Language Detection using Bidirectional Transformers}, url={https://aclanthology.org/S19-2011}, DOI={10.18653/v1/S19-2011}, booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation}, publisher={Association for Computational Linguistics}, author={Liu, Ping and Li, Wen and Zou, Liang}, year={2019}, month={Jun}, pages={87–91} }


 @article{Chiu_GPT3HateSpeech_2021, title={Detecting Hate Speech with GPT-3}, url={http://arxiv.org/abs/2103.12407}, DOI={10.48550/arXiv.2103.12407}, note={arXiv:2103.12407 [cs]}, number={arXiv:2103.12407}, publisher={arXiv}, author={Chiu, Ke-Li and Collins, Annie and Alexander, Rohan}, year={2022}, month={Mar} }


 @article{Brown_GPT3_2020, title={Language Models are Few-Shot Learners}, url={http://arxiv.org/abs/2005.14165}, DOI={10.48550/arXiv.2005.14165}, note={arXiv:2005.14165 [cs]}, number={arXiv:2005.14165}, publisher={arXiv}, author={Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario}, year={2020}, month={Jul} }


 @inproceedings{Hartvigsen_ToxiGen_2022, address={Dublin, Ireland}, title={ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection}, url={https://aclanthology.org/2022.acl-long.234}, DOI={10.18653/v1/2022.acl-long.234}, booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, publisher={Association for Computational Linguistics}, author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece}, year={2022}, month={May}, pages={3309–3326} }


 @inproceedings{Sharma_Memotion_2020, address={Barcelona (online)}, title={SemEval-2020 Task 8: Memotion Analysis- the Visuo-Lingual Metaphor!}, url={https://aclanthology.org/2020.semeval-1.99}, DOI={10.18653/v1/2020.semeval-1.99}, booktitle={Proceedings of the Fourteenth Workshop on Semantic Evaluation}, publisher={International Committee for Computational Linguistics}, author={Sharma, Chhavi and Bhageria, Deepesh and Scott, William and PYKL, Srinivas and Das, Amitava and Chakraborty, Tanmoy and Pulabaigari, Viswanath and Gambäck, Björn}, year={2020}, month={Dec}, pages={759–773} }



 @inproceedings{Ramamoorthy_Memotion2_2022, title={Memotion 2: Dataset on sentiment and emotion analysis of memes}, booktitle={Proceedings of De-Factify: Workshop on Multimodal Fact Checking and Hate Speech Detection, CEUR}, author={Ramamoorthy, Sathyanarayanan and Gunti, Nethra and Mishra, Shreyash and Suryavardan, S. and Reganti, Aishwarya and Patwa, Parth and DaS, Amitava and Chakraborty, Tanmoy and Sheth, Amit and Ekbal, Asif}, year={2022} }


 @article{Mishra_Memotion3_2023, title={Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed Hindi-English Memes}, url={http://arxiv.org/abs/2303.09892}, DOI={10.48550/arXiv.2303.09892}, note={arXiv:2303.09892 [cs]}, number={arXiv:2303.09892}, publisher={arXiv}, author={Mishra, Shreyash and Suryavardan, S. and Patwa, Parth and Chakraborty, Megha and Rani, Anku and Reganti, Aishwarya and Chadha, Aman and Das, Amitava and Sheth, Amit and Chinnakotla, Manoj and Ekbal, Asif and Kumar, Srijan}, year={2023}, month={Mar} }

 @article{Chandra_Jewtocracy_2021, title={“Subverting the Jewtocracy”: Online Antisemitism Detection Using Multimodal Deep Learning}, url={http://arxiv.org/abs/2104.05947}, note={arXiv:2104.05947 [cs]}, number={arXiv:2104.05947}, publisher={arXiv}, author={Chandra, Mohit and Pailla, Dheeraj and Bhatia, Himanshu and Sanchawala, Aadilmehdi and Gupta, Manish and Shrivastava, Manish and Kumaraguru, Ponnurangam}, year={2021}, month={Jun} }

 @inproceedings{Fersini_MAMI_2022, address={Seattle, United States}, title={SemEval-2022 Task 5: Multimedia Automatic Misogyny Identification}, url={https://aclanthology.org/2022.semeval-1.74}, DOI={10.18653/v1/2022.semeval-1.74}, booktitle={Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)}, publisher={Association for Computational Linguistics}, author={Fersini, Elisabetta and Gasparini, Francesca and Rizzi, Giulia and Saibene, Aurora and Chulvi, Berta and Rosso, Paolo and Lees, Alyssa and Sorensen, Jeffrey}, year={2022}, month={Jul}, pages={533–549} }
 @inproceedings{Xu_MetMeme_2022, address={New York, NY, USA}, series={SIGIR ’22}, title={MET-Meme: A Multimodal Meme Dataset Rich in Metaphors}, ISBN={978-1-4503-8732-3}, url={https://dl.acm.org/doi/10.1145/3477495.3532019}, DOI={10.1145/3477495.3532019}, booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, publisher={Association for Computing Machinery}, author={Xu, Bo and Li, Tingting and Zheng, Junzhe and Naseriparsa, Mehdi and Zhao, Zhehuan and Lin, Hongfei and Xia, Feng}, year={2022}, month={Jul}, pages={2887–2899}, collection={SIGIR ’22} }


@inproceedings{fater_RCNN_2015,
 author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
 url = {https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf},
 volume = {28},
 year = {2015}
}



 @article{ResNexT, title={Aggregated Residual Transformations for Deep Neural Networks}, url={http://arxiv.org/abs/1611.05431}, DOI={10.48550/arXiv.1611.05431}, note={arXiv:1611.05431 [cs]}, number={arXiv:1611.05431}, publisher={arXiv}, author={Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming}, year={2017}, month={Apr} }


 @article{Supervised_Multimodal_BiTrans_2019, title={Supervised Multimodal Bitransformers for Classifying Images and Text}, url={http://arxiv.org/abs/1909.02950}, DOI={10.48550/arXiv.1909.02950}, note={arXiv:1909.02950 [cs, stat]}, number={arXiv:1909.02950}, publisher={arXiv}, author={Kiela, Douwe and Bhooshan, Suvrat and Firooz, Hamed and Perez, Ethan and Testuggine, Davide}, year={2020}, month={Nov} }


@article{coco2014,
  author    = {Tsung{-}Yi Lin and
               Michael Maire and
               Serge J. Belongie and
               Lubomir D. Bourdev and
               Ross B. Girshick and
               James Hays and
               Pietro Perona and
               Deva Ramanan and
               Piotr Doll{\'{a}}r and
               C. Lawrence Zitnick},
  title     = {Microsoft {COCO:} Common Objects in Context},
  journal   = {CoRR},
  volume    = {abs/1405.0312},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.0312},
  eprinttype = {arXiv},
  eprint    = {1405.0312},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LinMBHPRDZ14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{VisualGenome2016,
  author    = {Ranjay Krishna and
               Yuke Zhu and
               Oliver Groth and
               Justin Johnson and
               Kenji Hata and
               Joshua Kravitz and
               Stephanie Chen and
               Yannis Kalantidis and
               Li{-}Jia Li and
               David A. Shamma and
               Michael S. Bernstein and
               Li Fei{-}Fei},
  title     = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense
               Image Annotations},
  journal   = {CoRR},
  volume    = {abs/1602.07332},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.07332},
  eprinttype = {arXiv},
  eprint    = {1602.07332},
  timestamp = {Wed, 15 Sep 2021 14:13:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KrishnaZGJHKCKL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{VQA2019,
  author    = {Difei Gao and
               Ruiping Wang and
               Shiguang Shan and
               Xilin Chen},
  title     = {From Two Graphs to {N} Questions: {A} {VQA} Dataset for Compositional
               Reasoning on Vision and Commonsense},
  journal   = {CoRR},
  volume    = {abs/1908.02962},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.02962},
  eprinttype = {arXiv},
  eprint    = {1908.02962},
  timestamp = {Thu, 02 Dec 2021 17:27:17 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-02962.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{GQA2019,
  author    = {Drew A. Hudson and
               Christopher D. Manning},
  title     = {{GQA:} a new dataset for compositional question answering over real-world
               images},
  journal   = {CoRR},
  volume    = {abs/1902.09506},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.09506},
  eprinttype = {arXiv},
  eprint    = {1902.09506},
  timestamp = {Tue, 21 May 2019 18:03:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-09506.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



 @inproceedings{sharma2018conceptual, address={Melbourne, Australia}, title={Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning}, url={https://aclanthology.org/P18-1238}, DOI={10.18653/v1/P18-1238}, booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, publisher={Association for Computational Linguistics}, author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu}, year={2018}, month={Jul}, pages={2556–2565} }


@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}



@article{Resnet_2015_He, title={Deep Residual Learning for Image Recognition}, url={http://arxiv.org/abs/1512.03385}, DOI={10.48550/arXiv.1512.03385}, note={arXiv:1512.03385 [cs]}, number={arXiv:1512.03385}, publisher={arXiv}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, year={2015}, month={Dec} }


@article{OpenImages_Google_2020,
	doi = {10.1007/s11263-020-01316-z},
  
	url = {https://doi.org/10.1007%2Fs11263-020-01316-z},
  
	year = 2020,
	month = {mar},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {128},
  
	number = {7},
  
	pages = {1956--1981},
  
	author = {Alina Kuznetsova and Hassan Rom and Neil Alldrin and Jasper Uijlings and Ivan Krasin and Jordi Pont-Tuset and Shahab Kamali and Stefan Popov and Matteo Malloci and Alexander Kolesnikov and Tom Duerig and Vittorio Ferrari},
  
	title = {The Open Images Dataset V4},
  
	journal = {International Journal of Computer Vision}
}

 @inproceedings{Objects365, title={Objects365: A Large-Scale, High-Quality Dataset for Object Detection}, ISSN={2380-7504}, DOI={10.1109/ICCV.2019.00852}, booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, author={Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian}, year={2019}, month={Oct}, pages={8429–8438} }

 @inproceedings{SBU2011, title={Im2Text: Describing Images Using 1 Million Captioned Photographs}, volume={24}, url={https://papers.nips.cc/paper_files/paper/2011/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara}, year={2011} }


@article{Flikr30k2015,
  author    = {Bryan A. Plummer and
               Liwei Wang and
               Chris M. Cervantes and
               Juan C. Caicedo and
               Julia Hockenmaier and
               Svetlana Lazebnik},
  title     = {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
               Richer Image-to-Sentence Models},
  journal   = {CoRR},
  volume    = {abs/1505.04870},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.04870},
  eprinttype = {arXiv},
  eprint    = {1505.04870},
  timestamp = {Mon, 13 Aug 2018 16:48:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PlummerWCCHL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

 @article{Diaz_passageRetrieval2021, title={Passage Retrieval for Outside-Knowledge Visual Question Answering}, DOI={10.1145/3404835.3462987}, journal={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval}, author={Diaz, Fernando and Shah, Chirag and Suel, Torsten and Castells, Pablo and Jones, Rosie and Sakai, Tetsuya and Qu, Chen and Zamani, Hamed and Yang, Liu and Croft, W Bruce and Learned-Miller, Erik}, year={2021}, pages={1753–1757} }

 @article{Sanh_distillBERT2020, title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, url={http://arxiv.org/abs/1910.01108}, DOI={10.48550/arXiv.1910.01108}, note={arXiv:1910.01108 [cs]}, number={arXiv:1910.01108}, publisher={arXiv}, author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas}, year={2020}, month={Feb} }
 @article{Robertson_BM25_2009, title={The Probabilistic Relevance Framework: BM25 and Beyond}, volume={3}, DOI={10.1561/1500000019}, journal={Foundations and Trends in Information Retrieval}, author={Robertson, Stephen and Zaragoza, Hugo}, year={2009}, month={Jan}, pages={333–389} }


 @article{johnson_Faiss2019billion, title={Billion-Scale Similarity Search with GPUs}, volume={7}, ISSN={2332-7790}, DOI={10.1109/TBDATA.2019.2921572}, number={3}, journal={IEEE Transactions on Big Data}, author={Johnson, Jeff and Douze, Matthijs and Jégou, Hervé}, year={2021}, month=jul, pages={535–547} }




 @article{Chung_FLAN_2022, title={Scaling Instruction-Finetuned Language Models}, url={http://arxiv.org/abs/2210.11416}, DOI={10.48550/arXiv.2210.11416}, note={arXiv:2210.11416 [cs]}, number={arXiv:2210.11416}, publisher={arXiv}, author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}, year={2022}, month={Dec} }


 @inproceedings{Kiela_FBHMC_Report_2021, title={The Hateful Memes Challenge: Competition Report}, ISSN={2640-3498}, url={https://proceedings.mlr.press/v133/kiela21a.html}, booktitle={Proceedings of the NeurIPS 2020 Competition and Demonstration Track}, publisher={PMLR}, author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Fitzpatrick, Casey A. and Bull, Peter and Lipstein, Greg and Nelli, Tony and Zhu, Ron and Muennighoff, Niklas and Velioglu, Riza and Rose, Jewgeni and Lippe, Phillip and Holla, Nithin and Chandra, Shantanu and Rajamanickam, Santhosh and Antoniou, Georgios and Shutova, Ekaterina and Yannakoudakis, Helen and Sandulescu, Vlad and Ozertem, Umut and Pantel, Patrick and Specia, Lucia and Parikh, Devi}, year={2021}, month={Aug}, pages={344–360}, language={en} }

@misc{ditch_the_label_2021, title={Uncovered: Online hate speech in the covid era}, url={https://www.ditchthelabel.org/research-papers/hate-speech-report-2021/}, journal={Ditch the Label}, author={{Ditch the Label}}, year={2021}, month={Nov}} 

%%%%% New dataset finished

@article{simonyan2015deep_vgg, title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, url={http://arxiv.org/abs/1409.1556}, DOI={10.48550/arXiv.1409.1556}, note={arXiv:1409.1556 [cs]}, number={arXiv:1409.1556}, publisher={arXiv}, author={Simonyan, Karen and Zisserman, Andrew}, year={2015}, month={Apr} }



@article{Transformer2017, title={Attention Is All You Need}, url={http://arxiv.org/abs/1706.03762}, DOI={10.48550/arXiv.1706.03762}, note={arXiv:1706.03762 [cs]}, number={arXiv:1706.03762}, publisher={arXiv}, author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia}, year={2023}, month={Aug} }


 @article{Das_HateVideo_hatemm_2023, title={HateMM: A Multi-Modal Dataset for Hate Video Classification}, volume={17}, rights={Copyright (c) 2023 Association for the Advancement of Artificial Intelligence}, ISSN={2334-0770}, DOI={10.1609/icwsm.v17i1.22209}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Das, Mithun and Raj, Rohit and Saha, Punyajoy and Mathew, Binny and Gupta, Manish and Mukherjee, Animesh}, year={2023}, month={Jun}, pages={1014–1023}, language={en} }

 @article{Wu_HateSpeechVideo_2020, title={Detection of Hate Speech in Videos Using Machine Learning}, volume={00}, DOI={10.1109/csci51800.2020.00104}, journal={2020 International Conference on Computational Science and Computational Intelligence (CSCI)}, author={Wu, Ching Seh and Bhandary, Unnathi}, year={2020}, pages={585–590} }




 @article{Kiela_MMBT_2020, title={Supervised Multimodal Bitransformers for Classifying Images and Text}, url={http://arxiv.org/abs/1909.02950}, DOI={10.48550/arXiv.1909.02950}, note={arXiv:1909.02950 [cs, stat]}, number={arXiv:1909.02950}, publisher={arXiv}, author={Kiela, Douwe and Bhooshan, Suvrat and Firooz, Hamed and Perez, Ethan and Testuggine, Davide}, year={2020}, month={Nov} }

 @article{Su_VLBERT_2020, title={VL-BERT: Pre-training of Generic Visual-Linguistic Representations}, url={http://arxiv.org/abs/1908.08530}, DOI={10.48550/arXiv.1908.08530}, note={arXiv:1908.08530 [cs]}, number={arXiv:1908.08530}, publisher={arXiv}, author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng}, year={2020}, month={Feb} }
 @article{Gan_VILLA_2020, title={Large-Scale Adversarial Training for Vision-and-Language Representation Learning}, url={http://arxiv.org/abs/2006.06195}, note={arXiv:2006.06195 [cs]}, number={arXiv:2006.06195}, publisher={arXiv}, author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing}, year={2020}, month={Oct} }


 @inproceedings{Kim_ViLT2021, title={ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision}, ISSN={2640-3498}, url={https://proceedings.mlr.press/v139/kim21k.html}, booktitle={Proceedings of the 38th International Conference on Machine Learning}, publisher={PMLR}, author={Kim, Wonjae and Son, Bokyung and Kim, Ildoo}, year={2021}, month={Jul}, pages={5583–5594}, language={en} }


 @article{Jia_ALIGN_2021, title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}, DOI={10.48550/arxiv.2102.05918}, journal={arXiv}, author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V and Sung, Yunhsuan and Li, Zhen and Duerig, Tom}, year={2021} }


 @article{Yuan_florence_21, title={Florence: A New Foundation Model for Computer Vision}, url={http://arxiv.org/abs/2111.11432}, note={arXiv:2111.11432 [cs]}, number={arXiv:2111.11432}, publisher={arXiv}, author={Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Liu, Mengchen and Liu, Zicheng and Lu, Yumao and Shi, Yu and Wang, Lijuan and Wang, Jianfeng and Xiao, Bin and Xiao, Zhen and Yang, Jianwei and Zeng, Michael and Zhou, Luowei and Zhang, Pengchuan}, year={2021}, month={Nov} }
@misc{alpaca2023,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}
 @article{Touvron_llama_2023, title={LLaMA: Open and Efficient Foundation Language Models}, url={http://arxiv.org/abs/2302.13971}, note={arXiv:2302.13971 [cs]}, number={arXiv:2302.13971}, author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume}, year={2023}, month={Feb} }


 @article{OpenAI_GPT4_2023, title={GPT-4 Technical Report}, url={http://arxiv.org/abs/2303.08774}, DOI={10.48550/arXiv.2303.08774}, note={arXiv:2303.08774 [cs]}, number={arXiv:2303.08774}, publisher={arXiv}, author={OpenAI}, year={2023}, month={Mar} }

@misc{Ouyang_InstructGPT_2022, title={Training language models to follow instructions with human feedback}, url={https://arxiv.org/abs/2203.02155v1}, journal={arXiv.org}, author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan}, year={2022}, month={Mar}, language={en} }


@inproceedings{schuhmann2022laionb,
  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
  author={Christoph Schuhmann and
          Romain Beaumont and
          Richard Vencu and
          Cade W Gordon and
          Ross Wightman and
          Mehdi Cherti and
          Theo Coombes and
          Aarush Katta and
          Clayton Mullis and
          Mitchell Wortsman and
          Patrick Schramowski and
          Srivatsa R Kundurthy and
          Katherine Crowson and
          Ludwig Schmidt and
          Robert Kaczmarczyk and
          Jenia Jitsev},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022},
  url={https://openreview.net/forum?id=M3Y74vmsMcY}
}

@software{ilharco_gabriel_OPENCLIP2021,author={Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}

@inproceedings{schuhmann2022laionbopenclip,
  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
  author={Christoph Schuhmann and
          Romain Beaumont and
          Richard Vencu and
          Cade W Gordon and
          Ross Wightman and
          Mehdi Cherti and
          Theo Coombes and
          Aarush Katta and
          Clayton Mullis and
          Mitchell Wortsman and
          Patrick Schramowski and
          Srivatsa R Kundurthy and
          Katherine Crowson and
          Ludwig Schmidt and
          Robert Kaczmarczyk and
          Jenia Jitsev},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022},
  url={https://openreview.net/forum?id=M3Y74vmsMcY}
}
@inproceedings{cherti2023reproducibleopenclip,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2818--2829},
  year={2023}
}

@article{chen2022altclip, title={AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities}, url={http://arxiv.org/abs/2211.06679}, DOI={10.48550/arXiv.2211.06679}, abstractNote={In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at https://github.com/FlagAI-Open/FlagAI.}, note={arXiv:2211.06679 [cs]}, number={arXiv:2211.06679}, publisher={arXiv}, author={Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell}, year={2022}, month=nov }

 @article{Jia2021ALIGN, title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}, url={http://arxiv.org/abs/2102.05918}, DOI={10.48550/arXiv.2102.05918}, abstractNote={Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.}, note={arXiv:2102.05918 [cs]}, number={arXiv:2102.05918}, publisher={arXiv}, author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yunhsuan and Li, Zhen and Duerig, Tom}, year={2021}, month=jun }


%%% Object detection/ Feature Extraction Started


 @inproceedings{Anderson2017up-down, title={Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering}, ISSN={2575-7075}, DOI={10.1109/CVPR.2018.00636}, booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei}, year={2018}, month={Jun}, pages={6077–6086} }


@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}

@inproceedings{tan2019lxmert,
  title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  author={Tan, Hao and Bansal, Mohit},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}

@misc{VG-QA,
  doi = {10.48550/ARXIV.1511.03416},
  
  url = {https://arxiv.org/abs/1511.03416},
  
  author = {Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Visual7W: Grounded Question Answering in Images},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{VCR2018,
  doi = {10.48550/ARXIV.1811.10830},
  
  url = {https://arxiv.org/abs/1811.10830},
  
  author = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {From Recognition to Cognition: Visual Commonsense Reasoning},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



%%% Object detection/ Feature Extraction Finished





@article{AdamW2017,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  eprinttype = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%%% optimization finishes



%%% Hate speech related work started

 @article{CharDectHateTwitter2018, title={Characterizing and Detecting Hateful Users on Twitter}, url={http://arxiv.org/abs/1803.08977}, DOI={10.48550/arXiv.1803.08977}, note={arXiv:1803.08977 [cs]}, number={arXiv:1803.08977}, publisher={arXiv}, author={Ribeiro, Manoel Horta and Calais, Pedro H. and Santos, Yuri A. and Almeida, Virgílio A. F. and Meira Jr, Wagner}, year={2018}, month={Mar} }


@inproceedings{waseem-2016-racist-Twitter,
    title = "Are You a Racist or Am {I} Seeing Things? Annotator Influence on Hate Speech Detection on {T}witter",
    author = "Waseem, Zeerak",
    booktitle = "Proceedings of the First Workshop on {NLP} and Computational Social Science",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-5618",
    doi = "10.18653/v1/W16-5618",
    pages = "138--142",
}

@inproceedings{waseem-hovy-2016-hateful-pred-feature-Twitter,
    title = "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on {T}witter",
    author = "Waseem, Zeerak  and
      Hovy, Dirk",
    booktitle = "Proceedings of the {NAACL} Student Research Workshop",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-2013",
    doi = "10.18653/v1/N16-2013",
    pages = "88--93",
}

@inproceedings{waseem-etal-2017-understanding-abuse,
    title = "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
    author = "Waseem, Zeerak  and
      Davidson, Thomas  and
      Warmsley, Dana  and
      Weber, Ingmar",
    booktitle = "Proceedings of the First Workshop on Abusive Language Online",
    month = aug,
    year = "2017",
    address = "Vancouver, BC, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3012",
    doi = "10.18653/v1/W17-3012",
    pages = "78--84",
    abstract = "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.",
}





 @inproceedings{SchmidtSurveyHSDNLP2017, address={Valencia, Spain}, title={A Survey on Hate Speech Detection using Natural Language Processing}, url={https://aclanthology.org/W17-1101}, DOI={10.18653/v1/W17-1101}, booktitle={Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media}, publisher={Association for Computational Linguistics}, author={Schmidt, Anna and Wiegand, Michael}, year={2017}, month={Apr}, pages={1–10} }


@article{Fortuna_Numes_Survey_2018,
author = {Fortuna, Paula and Nunes, S\'{e}rgio},
title = {A Survey on Automatic Detection of Hate Speech in Text},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3232676},
doi = {10.1145/3232676},
abstract = {The scientific study of hate speech, from a computer science point of view, is recent. This survey organizes and describes the current state of the field, providing a structured overview of previous approaches, including core algorithms, methods, and main features used. This work also discusses the complexity of the concept of hate speech, defined in many platforms and contexts, and provides a unifying definition. This area has an unquestionable potential for societal impact, particularly in online communities and digital media platforms. The development and systematization of shared resources, such as guidelines, annotated datasets in multiple languages, and algorithms, is a crucial step in advancing the automatic detection of hate speech.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {85},
numpages = {30},
keywords = {opinion mining, natural language processing, Hate speech, text mining, literature review}
}


 @article{Davidson2017AutomatedHS, title={Automated Hate Speech Detection and the Problem of Offensive Language}, url={http://arxiv.org/abs/1703.04009}, DOI={10.48550/arXiv.1703.04009}, note={arXiv:1703.04009 [cs]}, number={arXiv:1703.04009}, publisher={arXiv}, author={Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar}, year={2017}, month={Mar} }

@incollection{golbeck2017largeScaleTwitterDataset,
 author = "Jennifer Golbeck and Zahra Ashktorab and Rashad O. Banjo and Alexandra Berlinger and Siddharth Bhagwan and Cody Buntain and Paul Cheakalos and Alicia A. Geller and Quint Gergory and Rajesh Ku?mar Gnanasekaran and Raja Rajan Gunasekaran and Kelly M. Hoffman and Jenny Hottle and Vi?chita Jienjitlert and Shivika Khare and Ryan Lau and Marianna J. Martindale and Shalmali Naik and Heather L. Nixon and Piyush Ramachandran and Kristine M. Rogers and Lisa Rogers and Meghna Sar?dana Sarin and Gaurav Shahane and Jayanee Thanki and Priyanka Vengataraman and Zijian Wan and Derek Michael Wu",
 title = "A large labeled corpus for online harassment research",
 booktitle = "Proceedings of the 2017 {ACM} on Web Science Conference",
 pages = "229-233",
 year = 2017
}





@article{Founta2018CrowdsourcingTwitter,
  author    = {Antigoni{-}Maria Founta and
               Constantinos Djouvas and
               Despoina Chatzakou and
               Ilias Leontiadis and
               Jeremy Blackburn and
               Gianluca Stringhini and
               Athena Vakali and
               Michael Sirivianos and
               Nicolas Kourtellis},
  title     = {Large Scale Crowdsourcing and Characterization of Twitter Abusive
               Behavior},
  journal   = {CoRR},
  volume    = {abs/1802.00393},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.00393},
  eprinttype = {arXiv},
  eprint    = {1802.00393},
  timestamp = {Mon, 13 Aug 2018 16:48:19 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-00393.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


%%% Structure
@inproceedings{kumar-etal-2018-benchmarking,
    title = "Benchmarking Aggression Identification in Social Media",
    author = "Kumar, Ritesh  and
      Ojha, Atul Kr.  and
      Malmasi, Shervin  and
      Zampieri, Marcos",
    booktitle = "Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying ({TRAC}-2018)",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-4401",
    pages = "1--11",
    abstract = "In this paper, we present the report and findings of the Shared Task on Aggression Identification organised as part of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018. The task was to develop a classifier that could discriminate between Overtly Aggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants were provided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each in Hindi (in both Roman and Devanagari script) and English for training and validation. For testing, two different sets - one from Facebook and another from a different social media - were provided. A total of 130 teams registered to participate in the task, 30 teams submitted their test runs, and finally 20 teams also sent their system description paper which are included in the TRAC workshop proceedings. The best system obtained a weighted F-score of 0.64 for both Hindi and English on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50 for English and Hindi respectively. The results presented in this report depict how challenging the task is. The positive response from the community and the great levels of participation in the first edition of this shared task also highlights the interest in this topic.",
}

@article{M_Shervin_Challenge_2018,
  doi = {10.48550/ARXIV.1803.05495},
  
  url = {https://arxiv.org/abs/1803.05495},
  
  author = {Malmasi, Shervin and Zampieri, Marcos},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Challenges in Discriminating Profanity from Hate Speech},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


%%% Unwanted Bias
@inproceedings{sap-etal-2019-risk,
    title = "The Risk of Racial Bias in Hate Speech Detection",
    author = "Sap, Maarten  and
      Card, Dallas  and
      Gabriel, Saadia  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1163",
    doi = "10.18653/v1/P19-1163",
    pages = "1668--1678",
    abstract = "We investigate how annotators{'} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet{'}s dialect they are significantly less likely to label the tweet as offensive.",
}


@inproceedings{davidson-etal-2019-racial,
    title = "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    author = "Davidson, Thomas  and
      Bhattacharya, Debasmita  and
      Weber, Ingmar",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3504",
    doi = "10.18653/v1/W19-3504",
    pages = "25--35",
    abstract = "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.",
}

@inproceedings{Measurebiastext2018, address={New York, NY, USA}, series={AIES ’18}, title={Measuring and Mitigating Unintended Bias in Text Classification}, ISBN={978-1-4503-6012-8}, url={https://dl.acm.org/doi/10.1145/3278721.3278729}, DOI={10.1145/3278721.3278729}, booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society}, publisher={Association for Computing Machinery}, author={Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy}, year={2018}, month={Dec}, pages={67–73}, collection={AIES ’18} }




%%% Hate speech related work Finished



%%% Multimodal Hate speech related work started
@inproceedings{exploring-deep-fusion-vl2019,
    title = "Exploring Deep Multimodal Fusion of Text and Photo for Hate Speech Classification",
    author = "Yang, Fan  and
      Peng, Xiaochang  and
      Ghosh, Gargi  and
      Shilon, Reshef  and
      Ma, Hao  and
      Moore, Eider  and
      Predovic, Goran",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3502",
    doi = "10.18653/v1/W19-3502",
    pages = "11--18",
    abstract = "Interactions among users on social network platforms are usually positive, constructive and insightful. However, sometimes people also get exposed to objectionable content such as hate speech, bullying, and verbal abuse etc. Most social platforms have explicit policy against hate speech because it creates an environment of intimidation and exclusion, and in some cases may promote real-world violence. As users{'} interactions on today{'}s social networks involve multiple modalities, such as texts, images and videos, in this paper we explore the challenge of automatically identifying hate speech with deep multimodal technologies, extending previous research which mostly focuses on the text signal alone. We present a number of fusion approaches to integrate text and photo signals. We show that augmenting text with image embedding information immediately leads to a boost in performance, while applying additional attention fusion methods brings further improvement.",
}

@article{DetectInsCyberbull,
  author    = {Homa Hosseinmardi and
               Sabrina Arredondo Mattson and
               Rahat Ibn Rafiq and
               Richard Han and
               Qin Lv and
               Shivakant Mishra},
  title     = {Detection of Cyberbullying Incidents on the Instagram Social Network},
  journal   = {CoRR},
  volume    = {abs/1503.03909},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.03909},
  eprinttype = {arXiv},
  eprint    = {1503.03909},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HosseinmardiMRH15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Singh_2017_cyberbullying,
author = {Singh, Vivek K. and Ghosh, Souvick and Jose, Christin},
title = {Toward Multimodal Cyberbullying Detection},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3053169},
doi = {10.1145/3027063.3053169},
abstract = {As human beings utilize computing technologies to mediate multiple aspects of their lives, cyberbullying has grown as an important societal challenge. Cyberbullying may lead to deep psychiatric and emotional disorders for those affected. Hence, there is an urgent need to devise automated methods for cyberbullying detection and prevention. While recent cyberbullying detection efforts have defined sophisticated text processing methods for cyberbullying detection, there are as yet few efforts that leverage visual data processing to automatically detect cyberbullying. Based on early analysis of a public, labeled cyberbullying dataset, we report that visual features complement textual features in cyberbullying detection and can help improve predictive results.},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2090–2099},
numpages = {10},
keywords = {social media analysis, cyberbullying detection},
location = {Denver, Colorado, USA},
series = {CHI EA '17}
}


 @inproceedings{Zhong_2016_instagram_hate, address={New York, New York, USA}, series={IJCAI’16}, title={Content-driven detection of cyberbullying on the instagram social network}, ISBN={978-1-57735-770-4}, booktitle={Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence}, publisher={AAAI Press}, author={Zhong, Haoti and Li, Hao and Squicciarini, Anna and Rajtmajer, Sarah and Griffin, Christopher and Miller, David and Caragea, Cornelia}, year={2016}, month={Jul}, pages={3952–3958}, collection={IJCAI’16} }



@article{Soleymani2017multimodalsentiment_analysis,
  title={A survey of multimodal sentiment analysis},
  author={M. Soleymani and David Garc{\'i}a and Brendan Jou and Bj{\"o}rn Schuller and Shih-Fu Chang and Maja Pantic},
  journal={Image Vis. Comput.},
  year={2017},
  volume={65},
  pages={3-14}
}


@inproceedings{social_media_target_sentiment_2019,
  title     = {Adapting BERT for Target-Oriented Multimodal Sentiment Classification},
  author    = {Yu, Jianfei and Jiang, Jing},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {5408--5414},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/751},
  url       = {https://doi.org/10.24963/ijcai.2019/751},
}

@inproceedings{news_article_analysis_2017,
  author    = {Arnau Ramisa},
  title     = {Multimodal News Article Analysis},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {5136--5140},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/737},
  url       = {https://doi.org/10.24963/ijcai.2017/737},
}


@misc{fake_news_detection_2017,
  doi = {10.48550/ARXIV.1708.01967},
  
  url = {https://arxiv.org/abs/1708.01967},
  
  author = {Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
  
  keywords = {Social and Information Networks (cs.SI), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences, H.2.8},
  
  title = {Fake News Detection on Social Media: A Data Mining Perspective},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{GMU_MM_IMDB_2017,
  doi = {10.48550/ARXIV.1702.01992},
  
  url = {https://arxiv.org/abs/1702.01992},
  
  author = {Arevalo, John and Solorio, Thamar and Montes-y-Gómez, Manuel and González, Fabio A.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Gated Multimodal Units for Information Fusion},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


%%% Multimodal Hate speech related work Finished

%%% Vision Language Tasks
@article{Mogadala_2021_Survey_VL,
	doi = {10.1613/jair.1.11688},
  
	url = {https://doi.org/10.1613%2Fjair.1.11688},
  
	year = 2021,
	month = {aug},
  
	publisher = {{AI} Access Foundation},
  
	volume = {71},
  
	pages = {1183--1317},
  
	author = {Aditya Mogadala and Marimuthu Kalimuthu and Dietrich Klakow},
  
	title = {Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods},
  
	journal = {Journal of Artificial Intelligence Research}
}


@misc{Agrawal_2015_vqa,
  doi = {10.48550/ARXIV.1505.00468},
  
  url = {https://arxiv.org/abs/1505.00468},
  
  author = {Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C. Lawrence and Batra, Dhruv and Parikh, Devi},
  
  keywords = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {VQA: Visual Question Answering},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@misc{MSCOCO_2015,
  doi = {10.48550/ARXIV.1504.00325},
  
  url = {https://arxiv.org/abs/1504.00325},
  
  author = {Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Dollar, Piotr and Zitnick, C. Lawrence},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Microsoft COCO Captions: Data Collection and Evaluation Server},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{young-etal-2014-image_caption,
    title = "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    author = "Young, Peter  and
      Lai, Alice  and
      Hodosh, Micah  and
      Hockenmaier, Julia",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q14-1006",
    doi = "10.1162/tacl_a_00166",
    pages = "67--78",
    abstract = "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",
}



@misc{CLEVR_2016_Visual_Reasoning,
  doi = {10.48550/ARXIV.1612.06890},
  
  url = {https://arxiv.org/abs/1612.06890},
  
  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{Park_Kim_2015_Story,
 author = {Park, Cesc C and Kim, Gunhee},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Expressing an Image Stream with a Sequence of Natural Sentences},
 url = {https://proceedings.neurips.cc/paper/2015/file/17e62166fc8586dfa4d1bc0e1742c08b-Paper.pdf},
 volume = {28},
 year = {2015}
}

@misc{Hao_2016_Story,
  doi = {10.48550/ARXIV.1604.03968},
  
  url = {https://arxiv.org/abs/1604.03968},
  
  author = {Ting-Hao  and {Huang} and Ferraro, Francis and Mostafazadeh, Nasrin and Misra, Ishan and Agrawal, Aishwarya and Devlin, Jacob and Girshick, Ross and He, Xiaodong and Kohli, Pushmeet and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi and Vanderwende, Lucy and Galley, Michel and Mitchell, Margaret},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Visual Storytelling},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{VisualDialog_2017_Das,
  doi = {10.48550/ARXIV.1611.08669},
  
  url = {https://arxiv.org/abs/1611.08669},
  
  author = {Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, José M. F. and Parikh, Devi and Batra, Dhruv},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Visual Dialog},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{VisualObjectDiscovery_2017_Vries,
  doi = {10.48550/ARXIV.1611.08481},
  
  url = {https://arxiv.org/abs/1611.08481},
  
  author = {de Vries, Harm and Strub, Florian and Chandar, Sarath and Pietquin, Olivier and Larochelle, Hugo and Courville, Aaron},
  
  keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GuessWhat?! Visual object discovery through multi-modal dialogue},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@misc{Multi30kTranslation_2016,
  doi = {10.48550/ARXIV.1605.00459},
  
  url = {https://arxiv.org/abs/1605.00459},
  
  author = {Elliott, Desmond and Frank, Stella and Sima'an, Khalil and Specia, Lucia},
  
  keywords = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multi30K: Multilingual English-German Image Descriptions},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{specia-etal-2016-multimodalMachineTranslation,
    title = "A Shared Task on Multimodal Machine Translation and Crosslingual Image Description",
    author = "Specia, Lucia  and
      Frank, Stella  and
      Sima{'}an, Khalil  and
      Elliott, Desmond",
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2346",
    doi = "10.18653/v1/W16-2346",
    pages = "543--553",
}


%%% CV transformers starts

@article{SwingTrans2021,
  author    = {Ze Liu and
               Yutong Lin and
               Yue Cao and
               Han Hu and
               Yixuan Wei and
               Zheng Zhang and
               Stephen Lin and
               Baining Guo},
  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  journal   = {CoRR},
  volume    = {abs/2103.14030},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.14030},
  eprinttype = {arXiv},
  eprint    = {2103.14030},
  timestamp = {Thu, 08 Apr 2021 07:53:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-14030.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ViT,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  journal   = {CoRR},
  volume    = {abs/2010.11929},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.11929},
  eprinttype = {arXiv},
  eprint    = {2010.11929},
  timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ERNIE_Transformer,
  doi = {10.48550/ARXIV.1905.07129},
  
  url = {https://arxiv.org/abs/1905.07129},
  
  author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ERNIE: Enhanced Language Representation with Informative Entities},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{RoBERTa,
  doi = {10.48550/ARXIV.1907.11692},
  
  url = {https://arxiv.org/abs/1907.11692},
  
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ALBERT,
  doi = {10.48550/ARXIV.1909.11942},
  
  url = {https://arxiv.org/abs/1909.11942},
  
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



%%% CV transformers starts


@article{BRADLEY1997_AUCROC,
title = {The use of the area under the ROC curve in the evaluation of machine learning algorithms},
journal = {Pattern Recognition},
volume = {30},
number = {7},
pages = {1145-1159},
year = {1997},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(96)00142-2},
url = {https://www.sciencedirect.com/science/article/pii/S0031320396001422},
author = {Andrew P. Bradley},
keywords = {The ROC curve, The area under the ROC curve (AUC), Accuracy measures, Cross-validation, Wilcoxon statistic, Standard error},
abstract = {In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.}
}


@misc{GELU_2016,
  doi = {10.48550/ARXIV.1606.08415},
  
  url = {https://arxiv.org/abs/1606.08415},
  
  author = {Hendrycks, Dan and Gimpel, Kevin},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Gaussian Error Linear Units (GELUs)},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{HuggingFace_trans_2019,
  doi = {10.48550/ARXIV.1910.03771},
  
  url = {https://arxiv.org/abs/1910.03771},
  
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{han2021image_SGG,
      title={Image Scene Graph Generation (SGG) Benchmark}, 
      author={Xiaotian Han and Jianwei Yang and Houdong Hu and Lei Zhang and Jianfeng Gao and Pengchuan Zhang},
      year={2021},
      eprint={2107.12604},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{massa2018mrcnn,
author = {Massa, Francisco and Girshick, Ross},
title = {{maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},
year = {2018},
howpublished = {\url{https://github.com/facebookresearch/maskrcnn-benchmark}},
note = {Accessed: [Insert date here]}
}
@misc{DETR,
  doi = {10.48550/ARXIV.2005.12872},
  
  url = {https://arxiv.org/abs/2005.12872},
  
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {End-to-End Object Detection with Transformers},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}


@misc{SCST_IR_2016,
  doi = {10.48550/ARXIV.1612.00563},
  
  url = {https://arxiv.org/abs/1612.00563},
  
  author = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jarret and Goel, Vaibhava},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Self-critical Sequence Training for Image Captioning},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



 @article{CLIPCAP, title={ClipCap: CLIP Prefix for Image Captioning}, url={http://arxiv.org/abs/2111.09734}, DOI={10.48550/arXiv.2111.09734}, note={arXiv:2111.09734 [cs]}, number={arXiv:2111.09734}, publisher={arXiv}, author={Mokady, Ron and Hertz, Amir and Bermano, Amit H.}, year={2021}, month={Nov} }


@article{passageRetrievalOKVQA2021, title={Passage Retrieval for Outside-Knowledge Visual Question Answering}, DOI={10.1145/3404835.3462987}, journal={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval}, author={Diaz, Fernando and Shah, Chirag and Suel, Torsten and Castells, Pablo and Jones, Rosie and Sakai, Tetsuya and Qu, Chen and Zamani, Hamed and Yang, Liu and Croft, W Bruce and Learned-Miller, Erik}, year={2021}, pages={1753–1757} }


 @inproceedings{dpr2020, address={Online}, title={Dense Passage Retrieval for Open-Domain Question Answering}, url={https://aclanthology.org/2020.emnlp-main.550}, DOI={10.18653/v1/2020.emnlp-main.550}, booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, publisher={Association for Computational Linguistics}, author={Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau}, year={2020}, month={Nov}, pages={6769–6781} }


 @inproceedings{clip2021, title={Learning Transferable Visual Models From Natural Language Supervision}, ISSN={2640-3498}, url={https://proceedings.mlr.press/v139/radford21a.html}, booktitle={Proceedings of the 38th International Conference on Machine Learning}, publisher={PMLR}, author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya}, year={2021}, month={Jul}, pages={8748–8763}, language={en} }


@article{SBERT, title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, DOI={10.48550/arxiv.1908.10084}, journal={arXiv}, author={Reimers, Nils and Gurevych, Iryna}, year={2019} }


 @article{reverseEngSSL2023, title={Reverse Engineering Self-Supervised Learning}, url={http://arxiv.org/abs/2305.15614}, DOI={10.48550/arXiv.2305.15614}, note={arXiv:2305.15614 [cs]}, number={arXiv:2305.15614}, publisher={arXiv}, author={Ben-Shaul, Ido and Shwartz-Ziv, Ravid and Galanti, Tomer and Dekel, Shai and LeCun, Yann}, year={2023}, month={May} }


%Contrastive
 @inbook{Xuan_hardNegHardButUseful_2020, address={Cham}, series={Lecture Notes in Computer Science}, title={Hard Negative Examples are Hard, but Useful}, volume={12359}, ISBN={978-3-030-58567-9}, url={https://link.springer.com/10.1007/978-3-030-58568-6_8}, DOI={10.1007/978-3-030-58568-6_8}, booktitle={Computer Vision – ECCV 2020}, publisher={Springer International Publishing}, author={Xuan, Hong and Stylianou, Abby and Liu, Xiaotong and Pless, Robert}, editor={Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael}, year={2020}, pages={126–142}, collection={Lecture Notes in Computer Science}, language={en} }


 @article{XiongApproxNearestNeighbourCLDenseRetrieval2020, title={Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval}, url={http://arxiv.org/abs/2007.00808}, DOI={10.48550/arXiv.2007.00808}, note={arXiv:2007.00808 [cs]}, number={arXiv:2007.00808}, publisher={arXiv}, author={Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung and Liu, Jialin and Bennett, Paul and Ahmed, Junaid and Overwijk, Arnold}, year={2020}, month={Oct} }





 @inproceedings{KumarHateSpeechSurvey2022, title={Hate Speech Detection:A Survey}, DOI={10.1109/ICAC3N56670.2022.10074044}, booktitle={2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)}, author={Kumar, Sanjay and Nagar, Abhishek and Kumar, Akash and Singh, Amar}, year={2022}, month={Dec}, pages={171–176} }

 @article{Jahan_SystematicReviewHateSpeech2023, title={A systematic review of hate speech automatic detection using natural language processing}, volume={546}, ISSN={0925-2312}, DOI={10.1016/j.neucom.2023.126232}, journal={Neurocomputing}, author={Jahan, Md Saroar and Oussalah, Mourad}, year={2023}, month={Aug}, pages={126232} }




 @inproceedings{Santhanam_ColBERTV2_2021, address={Seattle, United States}, title={ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction}, url={https://aclanthology.org/2022.naacl-main.272}, DOI={10.18653/v1/2022.naacl-main.272}, booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, publisher={Association for Computational Linguistics}, author={Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei}, year={2022}, month={Jul}, pages={3715–3734} }
 @inproceedings{Khattab_Zaharia_COLBERT_2020, address={New York, NY, USA}, series={SIGIR ’20}, title={ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT}, ISBN={978-1-4503-8016-4}, url={https://dl.acm.org/doi/10.1145/3397271.3401075}, DOI={10.1145/3397271.3401075}, booktitle={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval}, publisher={Association for Computing Machinery}, author={Khattab, Omar and Zaharia, Matei}, year={2020}, month={Jul}, pages={39–48}, collection={SIGIR ’20} }
 @inproceedings{Harbecke_WhyonlyMicroF12022, address={Dublin, Ireland}, title={Why only Micro-F1? Class Weighting of Measures for Relation Classification}, url={https://aclanthology.org/2022.nlppower-1.4}, DOI={10.18653/v1/2022.nlppower-1.4}, booktitle={Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP}, publisher={Association for Computational Linguistics}, author={Harbecke, David and Chen, Yuxuan and Hennig, Leonhard and Alt, Christoph}, year={2022}, month={May}, pages={32–41} }

 @inproceedings{Lewis_RAG_2020, address={Red Hook, NY, USA}, series={NIPS’20}, title={Retrieval-augmented generation for knowledge-intensive NLP tasks}, ISBN={978-1-71382-954-6}, booktitle={Proceedings of the 34th International Conference on Neural Information Processing Systems}, publisher={Curran Associates Inc.}, author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe}, year={2020}, month={Dec}, pages={9459–9474}, collection={NIPS’20} }


 @article{Parnami_Lee_Fewshot_summary_2022, title={Learning from Few Examples: A Summary of Approaches to Few-Shot Learning}, url={http://arxiv.org/abs/2203.04291}, DOI={10.48550/arXiv.2203.04291}, note={arXiv:2203.04291 [cs]}, number={arXiv:2203.04291}, publisher={arXiv}, author={Parnami, Archit and Lee, Minwoo}, year={2022}, month={Mar} }

 @article{Lin_Cross_Modal_Few-Shot2023, title={Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models}, url={http://arxiv.org/abs/2301.06267}, DOI={10.48550/arXiv.2301.06267}, note={arXiv:2301.06267 [cs, eess]}, number={arXiv:2301.06267}, publisher={arXiv}, author={Lin, Zhiqiu and Yu, Samuel and Kuang, Zhiyi and Pathak, Deepak and Ramanan, Deva}, year={2023}, month={Aug} }

 @inproceedings{Schroff_FaceNet_2015, title={FaceNet: A Unified Embedding for Face Recognition and Clustering}, url={http://arxiv.org/abs/1503.03832}, DOI={10.1109/CVPR.2015.7298682}, note={arXiv:1503.03832 [cs]}, booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James}, year={2015}, month={Jun}, pages={815–823} }

 @inbook{Chechik_LargeScale_ImageSim_Rank2009, address={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={Large Scale Online Learning of Image Similarity through Ranking}, volume={5524}, ISBN={978-3-642-02171-8}, url={http://link.springer.com/10.1007/978-3-642-02172-5_2}, DOI={10.1007/978-3-642-02172-5_2}, booktitle={Pattern Recognition and Image Analysis}, publisher={Springer Berlin Heidelberg}, author={Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy}, editor={Araujo, Helder and Mendonça, Ana Maria and Pinho, Armando J. and Torres, María Inés}, year={2009}, pages={11–14}, collection={Lecture Notes in Computer Science}, language={en} }

 @article{Zhu_MiniGPT4_2023, title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models}, url={http://arxiv.org/abs/2304.10592}, DOI={10.48550/arXiv.2304.10592}, note={arXiv:2304.10592 [cs]}, number={arXiv:2304.10592}, publisher={arXiv}, author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed}, year={2023}, month={Apr} }
 @article{Lu_SQA_2022, title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering}, url={http://arxiv.org/abs/2209.09513}, DOI={10.48550/arXiv.2209.09513}, note={arXiv:2209.09513 [cs]}, number={arXiv:2209.09513}, publisher={arXiv}, author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin}, year={2022}, month={Oct} }
%CL

 @article{Logeswaran_Contrastive_Sentence_representation_2018, title={An efficient framework for learning sentence representations}, url={http://arxiv.org/abs/1803.02893}, DOI={10.48550/arXiv.1803.02893}, note={arXiv:1803.02893 [cs]}, number={arXiv:1803.02893}, publisher={arXiv}, author={Logeswaran, Lajanugen and Lee, Honglak}, year={2018}, month={Mar} }

 @article{Oord_Li_contrastive_predictive_coding_2019, title={Representation Learning with Contrastive Predictive Coding}, url={http://arxiv.org/abs/1807.03748}, DOI={10.48550/arXiv.1807.03748}, note={arXiv:1807.03748 [cs, stat]}, number={arXiv:1807.03748}, publisher={arXiv}, author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol}, year={2019}, month={Jan} }

 @article{Purushwalkam_demystifyingContrastiveSSL2020, title={Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases}, url={http://arxiv.org/abs/2007.13916}, DOI={10.48550/arXiv.2007.13916}, note={arXiv:2007.13916 [cs]}, number={arXiv:2007.13916}, publisher={arXiv}, author={Purushwalkam, Senthil and Gupta, Abhinav}, year={2020}, month={Jul} }

 @article{Robinson_ContrastiveLearningwithHardNegative_2021, title={Contrastive Learning with Hard Negative Samples}, url={http://arxiv.org/abs/2010.04592}, DOI={10.48550/arXiv.2010.04592}, note={arXiv:2010.04592 [cs, stat]}, number={arXiv:2010.04592}, publisher={arXiv}, author={Robinson, Joshua and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie}, year={2021}, month={Jan} }

 @inproceedings{Song_metriclearningLifetedFeatureEmbedding_2016, address={Las Vegas, NV, USA}, title={Deep Metric Learning via Lifted Structured Feature Embedding}, ISBN={978-1-4673-8851-1}, url={http://ieeexplore.ieee.org/document/7780803/}, DOI={10.1109/CVPR.2016.434}, booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Song, Hyun Oh and Xiang, Yu and Jegelka, Stefanie and Savarese, Silvio}, year={2016}, month={Jun}, pages={4004–4012}, language={en} }

 @inbook{Xuan_HardNegativesareHard2020, address={Cham}, series={Lecture Notes in Computer Science}, title={Hard Negative Examples are Hard, but Useful}, volume={12359}, ISBN={978-3-030-58567-9}, url={https://link.springer.com/10.1007/978-3-030-58568-6_8}, DOI={10.1007/978-3-030-58568-6_8}, booktitle={Computer Vision – ECCV 2020}, publisher={Springer International Publishing}, author={Xuan, Hong and Stylianou, Abby and Liu, Xiaotong and Pless, Robert}, editor={Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael}, year={2020}, pages={126–142}, collection={Lecture Notes in Computer Science}, language={en} }

 @article{Harwood_SmartMiningDeepMetricLearning_2017, title={Smart Mining for Deep Metric Learning}, url={http://arxiv.org/abs/1704.01285}, DOI={10.48550/arXiv.1704.01285}, note={arXiv:1704.01285 [cs]}, number={arXiv:1704.01285}, publisher={arXiv}, author={Harwood, Ben and G, Vijay Kumar B. and Carneiro, Gustavo and Reid, Ian and Drummond, Tom}, year={2017}, month={Jul} }

 @inproceedings{Suh_StochasticHardExampleMiningForDeepMetricLearning2019, title={Stochastic Class-Based Hard Example Mining for Deep Metric Learning}, url={https://openaccess.thecvf.com/content_CVPR_2019/html/Suh_Stochastic_Class-Based_Hard_Example_Mining_for_Deep_Metric_Learning_CVPR_2019_paper.html}, author={Suh, Yumin and Han, Bohyung and Kim, Wonsik and Lee, Kyoung Mu}, year={2019}, pages={7251–7259} }

 @inproceedings{Kalantidis_HardNegMixing2020, title={Hard Negative Mixing for Contrastive Learning}, volume={33}, url={https://proceedings.neurips.cc/paper_files/paper/2020/hash/f7cade80b7cc92b991cf4d2806d6bd78-Abstract.html}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Kalantidis, Yannis and Sariyildiz, Mert Bulent and Pion, Noe and Weinzaepfel, Philippe and Larlus, Diane}, year={2020}, pages={21798–21809} }

 @inproceedings{Chopra_Sim_Metrics_FaceVeri_LeCun_2005, title={Learning a similarity metric discriminatively, with application to face verification}, volume={1}, ISSN={1063-6919}, DOI={10.1109/CVPR.2005.202}, booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)}, author={Chopra, S. and Hadsell, R. and LeCun, Y.}, year={2005}, month={Jun}, pages={539–546 vol. 1} }


 @inproceedings{Hadsell_DimReduction_InvariantMapping_LeCun_2006, title={Dimensionality Reduction by Learning an Invariant Mapping}, volume={2}, ISSN={1063-6919}, DOI={10.1109/CVPR.2006.100}, booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06)}, author={Hadsell, R. and Chopra, S. and LeCun, Y.}, year={2006}, month={Jun}, pages={1735–1742} }

 @inproceedings{Chen_SimCLR_Hinton_2020, title={A Simple Framework for Contrastive Learning of Visual Representations}, ISSN={2640-3498}, url={https://proceedings.mlr.press/v119/chen20j.html}, booktitle={Proceedings of the 37th International Conference on Machine Learning}, publisher={PMLR}, author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey}, year={2020}, month={Nov}, pages={1597–1607}, language={en} }


 @article{Tian_goodviews_CL_2020, title={What Makes for Good Views for Contrastive Learning?}, url={http://arxiv.org/abs/2005.10243}, DOI={10.48550/arXiv.2005.10243}, note={arXiv:2005.10243 [cs]}, number={arXiv:2005.10243}, publisher={arXiv}, author={Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip}, year={2020}, month={Dec} }



 @article{Wei_COT_2023, title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, url={http://arxiv.org/abs/2201.11903}, DOI={10.48550/arXiv.2201.11903}, note={arXiv:2201.11903 [cs]}, number={arXiv:2201.11903}, publisher={arXiv}, author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny}, year={2023}, month={Jan} }

 @article{Lu_LearntoExplain_MultimodalReasoningThoughChain_2022, title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering}, url={http://arxiv.org/abs/2209.09513}, DOI={10.48550/arXiv.2209.09513}, note={arXiv:2209.09513 [cs]}, number={arXiv:2209.09513}, publisher={arXiv}, author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin}, year={2022}, month={Oct} }


@inproceedings{
Hu_LORA_2021,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}


%Cite for ib batch negative
 @inproceedings{YihToutanovaPlattMeek2011inbatchnegative, address={Portland, Oregon, USA}, title={Learning Discriminative Projections for Text Similarity Measures}, url={https://aclanthology.org/W11-0329}, booktitle={Proceedings of the Fifteenth Conference on Computational Natural Language Learning}, publisher={Association for Computational Linguistics}, author={Yih, Wen-tau and Toutanova, Kristina and Platt, John C. and Meek, Christopher}, editor={Goldwater, Sharon and Manning, Christopher}, year={2011}, month=jun, pages={247–256} }

 @article{Henderson2017effiNLPinbatchnegative, title={Efficient Natural Language Response Suggestion for Smart Reply}, url={http://arxiv.org/abs/1705.00652}, DOI={10.48550/arXiv.1705.00652}, abstractNote={This paper presents a computationally efficient machine-learned method for natural language response suggestion. Feed-forward neural networks using n-gram embedding features encode messages into vectors which are optimized to give message-response pairs a high dot-product value. An optimized search finds response suggestions. The method is evaluated in a large-scale commercial e-mail application, Inbox by Gmail. Compared to a sequence-to-sequence approach, the new system achieves the same quality at a small fraction of the computational requirements and latency.}, note={arXiv:1705.00652 [cs]}, number={arXiv:1705.00652}, publisher={arXiv}, author={Henderson, Matthew and Al-Rfou, Rami and Strope, Brian and Sung, Yun-hsuan and Lukacs, Laszlo and Guo, Ruiqi and Kumar, Sanjiv and Miklos, Balint and Kurzweil, Ray}, year={2017}, month=may }
 @article{Herzig2021QATable, title={Open Domain Question Answering over Tables via Dense Retrieval}, DOI={10.48550/arxiv.2103.12011}, abstractNote={Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of Natural Questions (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a BERT based retriever.}, journal={arXiv}, author={Herzig, Jonathan and Müller, Thomas and Krichene, Syrine and Eisenschlos, Julian Martin}, year={2021} }

 @article{Diaz2021PR, title={Passage Retrieval for Outside-Knowledge Visual Question Answering}, DOI={10.1145/3404835.3462987}, abstractNote={In this work, we address multi-modal information needs that contain text questions and images by focusing on passage retrieval for outside-knowledge visual question answering. This task requires access to outside knowledge, which in our case we define to be a large unstructured passage collection. We first conduct sparse retrieval with BM25 and study expanding the question with object names and image captions. We verify that visual clues play an important role and captions tend to be more informative than object names in sparse retrieval. We then construct a dual-encoder dense retriever, with the query encoder being LXMERT, a multi-modal pre-trained transformer. We further show that dense retrieval significantly outperforms sparse retrieval that uses object expansion. Moreover, dense retrieval matches the performance of sparse retrieval that leverages human-generated captions.}, journal={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval}, author={Diaz, Fernando and Shah, Chirag and Suel, Torsten and Castells, Pablo and Jones, Rosie and Sakai, Tetsuya and Qu, Chen and Zamani, Hamed and Yang, Liu and Croft, W Bruce and Learned-Miller, Erik}, year={2021}, pages={1753–1757} }

 @inproceedings{Lin2023FLMR, title={Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering}, volume={36}, rights={All rights reserved}, url={https://proceedings.neurips.cc/paper_files/paper/2023/file/47393e8594c82ce8fd83adc672cf9872-Paper-Conference.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Lin, Weizhe and Chen, Jinghong and Mei, Jingbiao and Coca, Alexandru and Byrne, Bill}, editor={Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.}, year={2023}, pages={22820–22840} }

 @article{Lin2024Preflmr, title={PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers}, rights={All rights reserved}, url={http://arxiv.org/abs/2402.08327}, DOI={10.48550/arXiv.2402.08327}, abstractNote={Large Multimodal Models (LMMs) excel in natural language and visual understanding but are challenged by exacting tasks such as Knowledge-based Visual Question Answering (KB-VQA) which involve the retrieval of relevant information from document collections to use in shaping answers to questions. We present an extensive training and evaluation framework, M2KR, for KB-VQA. M2KR contains a collection of vision and language tasks which we have incorporated into a single suite of benchmark tasks for training and evaluating general-purpose multi-modal retrievers. We use M2KR to develop PreFLMR, a pre-trained version of the recently developed Fine-grained Late-interaction Multi-modal Retriever (FLMR) approach to KB-VQA, and we report new state-of-the-art results across a range of tasks. We also present investigations into the scaling behaviors of PreFLMR intended to be useful in future developments in general-purpose multi-modal retrievers.}, note={arXiv:2402.08327 [cs]}, number={arXiv:2402.08327}, publisher={arXiv}, author={Lin, Weizhe and Mei, Jingbiao and Chen, Jinghong and Byrne, Bill}, year={2024}, month=feb }

 @inproceedings{Nguyen2023PassageBM25, address={Hong Kong, China}, title={Passage-based BM25 Hard Negatives: A Simple and Effective Negative Sampling Strategy For Dense Retrieval}, url={https://aclanthology.org/2023.paclic-1.59}, booktitle={Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation}, publisher={Association for Computational Linguistics}, author={Nguyen, Thanh-Do and Bui, Chi Minh and Vuong, Thi-Hai-Yen and Phan, Xuan-Hieu}, editor={Huang, Chu-Ren and Harada, Yasunari and Kim, Jong-Bok and Chen, Si and Hsu, Yu-Yin and Chersoni, Emmanuele and A, Pranav and Zeng, Winnie Huiheng and Peng, Bo and Li, Yuxi and Li, Junlin}, year={2023}, month=dec, pages={591–599} }

@inproceedings{RGCL2024Mei,
  title = "Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning",
  author = "Mei, Jingbiao  and
    Chen, Jinghong  and
    Lin, Weizhe  and
    Byrne, Bill  and
    Tomalin, Marcus",
  editor = "Ku, Lun-Wei  and
    Martins, Andre  and
    Srikumar, Vivek",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = aug,
  year = "2024",
  address = "Bangkok, Thailand",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.acl-long.291",
  doi = "10.18653/v1/2024.acl-long.291",
  pages = "5333--5347"
}

 @inproceedings{Cao_Fan_Lee_Chong_Jiang_2021, title={Disentangling Hate in Online Memes}, url={http://arxiv.org/abs/2108.06207}, DOI={10.1145/3474085.3475625}, note={arXiv:2108.06207 [cs]}, booktitle={Proceedings of the 29th ACM International Conference on Multimedia}, author={Cao, Rui and Fan, Ziqing and Lee, Roy Ka-Wei and Chong, Wen-Haw and Jiang, Jing}, year={2021}, month=oct, pages={5138–5147} }

 @inproceedings{Shah2024memeclip_pridemm, address={Miami, Florida, USA}, title={MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification}, url={https://aclanthology.org/2024.emnlp-main.959/}, DOI={10.18653/v1/2024.emnlp-main.959}, booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, publisher={Association for Computational Linguistics}, author={Shah, Siddhant Bikram and Shiwakoti, Shuvam and Chaudhary, Maheep and Wang, Haohan}, editor={Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung}, year={2024}, month=nov, pages={17320–17332} }


 @inproceedings{Wang_MultiHateClip_2024, title={MultiHateClip: A Multilingual Benchmark Dataset for Hateful Video Detection on YouTube and Bilibili}, url={http://arxiv.org/abs/2408.03468}, DOI={10.1145/3664647.3681521}, note={arXiv:2408.03468 [cs]}, booktitle={Proceedings of the 32nd ACM International Conference on Multimedia}, author={Wang, Han and Yang, Tan Rui and Naseem, Usman and Lee, Roy Ka-Wei}, year={2024}, month=oct, pages={7493–7502} }

 @article{Wang_LVBench_longvideo2024, title={LVBench: An Extreme Long Video Understanding Benchmark}, url={http://arxiv.org/abs/2406.08035}, DOI={10.48550/arXiv.2406.08035}, note={arXiv:2406.08035 [cs]}, number={arXiv:2406.08035}, publisher={arXiv}, author={Wang, Weihan and He, Zehai and Hong, Wenyi and Cheng, Yean and Zhang, Xiaohan and Qi, Ji and Gu, Xiaotao and Huang, Shiyu and Xu, Bin and Dong, Yuxiao and Ding, Ming and Tang, Jie}, year={2024}, month=oct }
 @inproceedings{Huang_LowResourceLMMAgentHatefulMeme_2024, address={Miami, Florida, USA}, title={Towards Low-Resource Harmful Meme Detection with LMM Agents}, url={https://aclanthology.org/2024.emnlp-main.136}, booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, publisher={Association for Computational Linguistics}, author={Huang, Jianzhao and Lin, Hongzhan and Ziyan, Liu and Luo, Ziyang and Chen, Guang and Ma, Jing}, editor={Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung}, year={2024}, month=nov, pages={2269–2293} }




@inproceedings{Cao_2024_ModHate,
author = {Cao, Rui and Lee, Roy Ka-Wei and Jiang, Jing},
title = {Modularized Networks for Few-shot Hateful Meme Detection},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3589334.3648145},
doi = {10.1145/3589334.3648145},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {4575–4584},
numpages = {10},
keywords = {few-shot learning, hateful content, multimodal memes, parameter-efficient tuning},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{Cao_2023_ProCap,
author = {Cao, Rui and Hee, Ming Shan and Kuek, Adriel and Chong, Wen-Haw and Lee, Roy Ka-Wei and Jiang, Jing},
title = {Pro-Cap: Leveraging a Frozen Vision-Language Model for Hateful Meme Detection},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3581783.3612498},
doi = {10.1145/3581783.3612498},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {5244–5252},
numpages = {9},
keywords = {memes, multimodal, semantic extraction},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{Lin_2024_ExplainHM,
author = {Lin, Hongzhan and Luo, Ziyang and Gao, Wei and Ma, Jing and Wang, Bo and Yang, Ruichao},
title = {Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645381},
doi = {10.1145/3589334.3645381},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2359–2370},
numpages = {12},
keywords = {LLMs, explainability, harmful meme detection, multimodal debate},
location = {Singapore, Singapore},
series = {WWW '24}
}



 @article{Liu_2023_LLAVA1.5, title={Improved Baselines with Visual Instruction Tuning}, url={http://arxiv.org/abs/2310.03744}, note={arXiv:2310.03744 [cs]}, number={arXiv:2310.03744}, publisher={arXiv}, author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae}, year={2023}, month=oct }

 @inproceedings{Nguyen_2024_computationalMemeUnderstanding, address={Miami, Florida, USA}, title={Computational Meme Understanding: A Survey}, url={https://aclanthology.org/2024.emnlp-main.1184}, booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, publisher={Association for Computational Linguistics}, author={Nguyen, Khoi P. N. and Ng, Vincent}, editor={Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung}, year={2024}, month=nov, pages={21251–21267} }

 @inproceedings{Hu_2024_VPD, address={Seattle, WA, USA}, title={Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models}, rights={https://doi.org/10.15223/policy-029}, ISBN={979-8-3503-5300-6}, url={https://ieeexplore.ieee.org/document/10655837/}, DOI={10.1109/CVPR52733.2024.00916},  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Hu, Yushi and Stretcu, Otilia and Lu, Chun-Ta and Viswanathan, Krishnamurthy and Hata, Kenji and Luo, Enming and Krishna, Ranjay and Fuxman, Ariel}, year={2024}, month=jun, pages={9590–9601}, language={en} }


 @article{Wang_2024_Qwen2vl, title={Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution}, url={http://arxiv.org/abs/2409.12191}, DOI={10.48550/arXiv.2409.12191}, note={arXiv:2409.12191 [cs]}, number={arXiv:2409.12191}, publisher={arXiv}, author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang}, year={2024}, month=oct }


@INPROCEEDINGS{Burbi_2023_Issues,
  author={Burbi, Giovanni and Baldrati, Alberto and Agnolucci, Lorenzo and Bertini, Marco and Del Bimbo, Alberto},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
  title={Mapping Memes to Words for Multimodal Hateful Meme Classification}, 
  year={2023},
  volume={},
  number={},
  pages={2824-2828},
  keywords={Training;Adaptation models;Visualization;Computer vision;Codes;Conferences;Computational modeling;Textual inversion;CLIP;Hateful Memes Challenge;HMC;Vision and Language;Hate speech detection;HarMeme},
  doi={10.1109/ICCVW60793.2023.00303}}


 @article{OpenAI2024gpt4o, title={GPT-4o System Card}, url={http://arxiv.org/abs/2410.21276}, DOI={10.48550/arXiv.2410.21276}, note={arXiv:2410.21276 [cs]}, number={arXiv:2410.21276}, publisher={arXiv}, author={OpenAI}, year={2024}, month=oct }

 @inproceedings{Hee2024BridgeModality, address={Miami, Florida, USA}, title={Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning}, url={https://aclanthology.org/2024.emnlp-main.445/}, DOI={10.18653/v1/2024.emnlp-main.445},booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, publisher={Association for Computational Linguistics}, author={Hee, Ming Shan and Kumaresan, Aditi and Lee, Roy Ka-Wei}, editor={Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung}, year={2024}, month=nov, pages={7785–7799} }

 @article{Chen2023PALIX, title={PaLI-X: On Scaling up a Multilingual Vision and Language Model}, url={http://arxiv.org/abs/2305.18565}, DOI={10.48550/arXiv.2305.18565}, note={arXiv:2305.18565 [cs]}, number={arXiv:2305.18565}, publisher={arXiv}, author={Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and Shakeri, Siamak and Dehghani, Mostafa and Salz, Daniel and Lucic, Mario and Tschannen, Michael and Nagrani, Arsha and Hu, Hexiang and Joshi, Mandar and Pang, Bo and Montgomery, Ceslee and Pietrzyk, Paulina and Ritter, Marvin and Piergiovanni, A. J. and Minderer, Matthias and Pavetic, Filip and Waters, Austin and Li, Gang and Alabdulmohsin, Ibrahim and Beyer, Lucas and Amelot, Julien and Lee, Kenton and Steiner, Andreas Peter and Li, Yang and Keysers, Daniel and Arnab, Anurag and Xu, Yuanzhong and Rong, Keran and Kolesnikov, Alexander and Seyedhosseini, Mojtaba and Angelova, Anelia and Zhai, Xiaohua and Houlsby, Neil and Soricut, Radu}, year={2023}, month=may }

@misc{bai_qwen-vl_2023,
    title = {Qwen-{VL}: {A} {Versatile} {Vision}-{Language} {Model} for {Understanding}, {Localization}, {Text} {Reading}, and {Beyond}},
    shorttitle = {Qwen-{VL}},
    url = {http://arxiv.org/abs/2308.12966},
    abstract = {We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.},
    language = {en},
    urldate = {2023-10-08},
    publisher = {arXiv},
    author = {Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
    month = sep,
    year = {2023},
    note = {arXiv:2308.12966 [cs]},
    keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{
lang2025bitingoff,
title={Biting Off More Than You Can Detect: Retrieval-Augmented Multimodal Experts for Short Video Hate Detection},
author={Jian Lang and Rongpei Hong and Jin Xu and YILI LI and Xovee Xu and Fan Zhou},
booktitle={THE WEB CONFERENCE 2025},
year={2025},
url={https://openreview.net/forum?id=GrJYzmDzfW}
}

@inproceedings{Ji2024CapAlign, address={Singapore Singapore}, title={CapAlign: Improving Cross Modal Alignment via Informative Captioning for Harmful Meme Detection}, ISBN={979-8-4007-0171-9}, url={https://dl.acm.org/doi/10.1145/3589334.3648146}, DOI={10.1145/3589334.3648146}, booktitle={Proceedings of the ACM Web Conference 2024}, publisher={ACM}, author={Ji, Junhui and Lin, Xuanrui and Naseem, Usman}, year={2024}, month=may, pages={4585–4594}, language={en} }


 @article{Dettmers2023qlora, title={QLoRA: Efficient Finetuning of Quantized LLMs}, url={http://arxiv.org/abs/2305.14314}, DOI={10.48550/arXiv.2305.14314}, note={arXiv:2305.14314 [cs]}, number={arXiv:2305.14314}, publisher={arXiv}, author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke}, year={2023}, month=may }


 @article{DeepSeekAI2025_r1, title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, url={http://arxiv.org/abs/2501.12948}, DOI={10.48550/arXiv.2501.12948}, note={arXiv:2501.12948 [cs]}, number={arXiv:2501.12948}, publisher={arXiv}, author={DeepSeek-AI}, year={2025}, month=jan }


 @article{OpenAI2024o1, title={OpenAI o1 System Card}, url={http://arxiv.org/abs/2412.16720}, DOI={10.48550/arXiv.2412.16720}, note={arXiv:2412.16720 [cs]}, number={arXiv:2412.16720}, publisher={arXiv}, author={OpenAI}, year={2024}, month=dec }

@article{zong2024vlicl,
  title={VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning},
  author={Zong, Yongshuo and Bohdal, Ondrej and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2403.13164},
  year={2024}
}