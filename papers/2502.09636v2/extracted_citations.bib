@inproceedings{NEURIPS2024_8eb88844,
 author = {Myung, Junho and Lee, Nayeon and Zhou, Yi and Jin, Jiho and Putri, Rifki and Antypas, Dimosthenis and Borkakoty, Hsuvas and Kim, Eunsu and Perez-Almendros, Carla and Ayele, Abinew Ali and Gutierrez Basulto, Victor and Ibanez-Garcia, Yazmin and Lee, Hwaran and Muhammad, Shamsuddeen H and Park, Kiwoong and Rzayev, Anar and White, Nina and Yimam, Seid Muhie and Pilehvar, Mohammad Taher and Ousidhoum, Nedjma and Camacho-Collados, Jose and Oh, Alice},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {78104--78146},
 publisher = {Curran Associates, Inc.},
 title = {BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/8eb88844dafefa92a26aaec9f3acad93-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}

@article{aixela1996culture,
  title={Culture-specific items in translation},
  author={Aixel{\'a}, Javier Franco},
  journal={Translation, power, subversion},
  volume={8},
  pages={52--78},
  year={1996}
}

@article{bassnett2007culture,
  title={Culture and translation},
  author={Bassnett, Susan},
  journal={A companion to translation studies},
  pages={13--23},
  year={2007},
  publisher={Clevedon, UK, Buffalo, USA, Toronto, Canada: Multilingual Matters Ltd}
}

@book{book1,
author = {Hofstede, Geert},
year = {2001},
month = {01},
pages = {},
title = {Culture's Consequences: Comparing Values, Behaviors, Institutions and Organizations Across Nations},
volume = {41},
journal = {Behaviour Research and Therapy - BEHAV RES THER},
doi = {10.1016/S0005-7967(02)00184-5}
}

@book{book2,
author = {Geert, Hofstede and Hofstede, Gert Jan},
year = {2004},
month = {01},
pages = {},
title = {Cultures and Organizations. Software of the Mind},
volume = {2},
isbn = {978-0071439596}
}

@article{bourges1998meaning,
  title={Meaning, the central issue in cross-cultural HCI design},
  author={Bourges-Waldegg, Paula and Scrivener, Stephen AR},
  journal={Interacting with computers},
  volume={9},
  number={3},
  pages={287--309},
  year={1998},
  publisher={Elsevier}
}

@inproceedings{cao-etal-2023-assessing,
    title = "Assessing Cross-Cultural Alignment between {C}hat{GPT} and Human Societies: An Empirical Study",
    author = "Cao, Yong  and
      Zhou, Li  and
      Lee, Seolhwa  and
      Cabello, Laura  and
      Chen, Min  and
      Hershcovich, Daniel",
    editor = "Dev, Sunipa  and
      Prabhakaran, Vinodkumar  and
      Adelani, David  and
      Hovy, Dirk  and
      Benotti, Luciana",
    booktitle = "Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP)",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.c3nlp-1.7",
    doi = "10.18653/v1/2023.c3nlp-1.7",
    pages = "53--67",
}

@misc{chiu2024culturalbenchrobustdiversechallenging,
      title={CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring the (Lack of) Cultural Knowledge of LLMs}, 
      author={Yu Ying Chiu and Liwei Jiang and Bill Yuchen Lin and Chan Young Park and Shuyue Stella Li and Sahithya Ravi and Mehar Bhatia and Maria Antoniak and Yulia Tsvetkov and Vered Shwartz and Yejin Choi},
      year={2024},
      eprint={2410.02677},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02677}, 
}

@misc{dawson2024evaluatingculturalawarenessllms,
      title={Evaluating Cultural Awareness of LLMs for Yoruba, Malayalam, and English}, 
      author={Fiifi Dawson and Zainab Mosunmola and Sahil Pocker and Raj Abhijit Dandekar and Rajat Dandekar and Sreedath Panat},
      year={2024},
      eprint={2410.01811},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2410.01811}, 
}

@article{duan2023ranking,
  title={Ranking of large language model (llm) regional bias},
  author={Duan, Yucong and Tang, Fuliang and Wu, Kunguang and Guo, Zhendong and Huang, Shuaishuai and Mei, Yingtian and Wang, Yuxing and Yang, Zeyu and Gong, Shiming},
  year={2023}
}

@article{gardner1962cross,
  title={Cross cultural communication},
  author={Gardner, George H},
  journal={The Journal of social psychology},
  volume={58},
  number={2},
  pages={241--256},
  year={1962},
  publisher={Taylor \& Francis}
}

@book{gudykunst2003cross,
  title={Cross-cultural and intercultural communication},
  author={Gudykunst, William B},
  year={2003},
  publisher={Sage}
}

@article{heimgartner2018culturally,
  title={Culturally-aware HCI systems},
  author={Heimg{\"a}rtner, R{\"u}diger},
  journal={Advances in culturally-aware intelligent systems and in cross-cultural psychological studies},
  pages={11--37},
  year={2018},
  publisher={Springer}
}

@book{hurn2013cross,
  title={What is cross-cultural communication?},
  author={Hurn, Brian J and Tomalin, Barry and Hurn, Brian J and Tomalin, Barry},
  year={2013},
  publisher={Springer}
}

@inproceedings{jha-etal-2023-seegull,
    title = "{S}ee{GULL}: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models",
    author = "Jha, Akshita  and
      Mostafazadeh Davani, Aida  and
      Reddy, Chandan K  and
      Dave, Shachi  and
      Prabhakaran, Vinodkumar  and
      Dev, Sunipa",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.548",
    doi = "10.18653/v1/2023.acl-long.548",
    pages = "9851--9870",
}

@misc{kharchenko2024llmsrepresentvaluescultures,
      title={How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions}, 
      author={Julia Kharchenko and Tanya Roosta and Aman Chadha and Chirag Shah},
      year={2024},
      eprint={2406.14805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.14805}, 
}

@article{koto-etal-2024-indoculture,
    title = "{I}ndo{C}ulture: Exploring Geographically Influenced Cultural Commonsense Reasoning Across Eleven {I}ndonesian Provinces",
    author = "Koto, Fajri  and
      Mahendra, Rahmad  and
      Aisyah, Nurul  and
      Baldwin, Timothy",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.92/",
    doi = "10.1162/tacl_a_00726",
    pages = "1703--1719",
    abstract = "Although commonsense reasoning is greatly shaped by cultural and geographical factors, previous studies have predominantly centered on cultures grounded in the English language, potentially resulting in an Anglocentric bias. In this paper, we introduce IndoCulture, aimed at understanding the influence of geographical factors on language model reasoning ability, with a specific emphasis on the diverse cultures found within eleven Indonesian provinces. In contrast to prior work that has relied on templates (Yin et al., 2022) and online scrapping (Fung et al., 2024), we create IndoCulture by asking local people to manually develop a cultural context and plausible options, across a set of predefined topics. Evaluation of 27 language models reveals several insights: (1) the open-weight Llama{--}3 is competitive with GPT{--}4, while other open-weight models struggle, with accuracies below 50{\%}; (2) there is a general pattern of models generally performing better for some provinces, such as Bali and West Java, and less well for others; and (3) the inclusion of location context enhances performance, especially for larger models like GPT{--}4, emphasizing the significance of geographical context in commonsense reasoning.1"
}

@misc{kovač2023largelanguagemodelssuperpositions,
      title={Large Language Models as Superpositions of Cultural Perspectives}, 
      author={Grgur Kovač and Masataka Sawayama and Rémy Portelas and Cédric Colas and Peter Ford Dominey and Pierre-Yves Oudeyer},
      year={2023},
      eprint={2307.07870},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.07870}, 
}

@article{kyriakoullis2016culture,
  title={Culture and HCI: a review of recent cultural studies in HCI and social networks},
  author={Kyriakoullis, Leantros and Zaphiris, Panayiotis},
  journal={Universal Access in the Information Society},
  volume={15},
  pages={629--642},
  year={2016},
  publisher={Springer}
}

@misc{li2024culturegenrevealingglobalcultural,
      title={CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting}, 
      author={Huihan Li and Liwei Jiang and Jena D. Hwang and Hyunwoo Kim and Sebastin Santy and Taylor Sorensen and Bill Yuchen Lin and Nouha Dziri and Xiang Ren and Yejin Choi},
      year={2024},
      eprint={2404.10199},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.10199}, 
}

@inproceedings{li2024culturellm,
  title={Culturellm: Incorporating cultural differences into large language models},
  author={Li, Cheng and Chen, Mengzhou and Wang, Jindong and Sitaram, Sunayana and Xie, Xing},
  booktitle={Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@inproceedings{lin-etal-2025-investigating,
    title = "Investigating Bias in {LLM}-Based Bias Detection: Disparities between {LLM}s and Human Perception",
    author = "Lin, Luyang  and
      Wang, Lingzhi  and
      Guo, Jinsong  and
      Wong, Kam-Fai",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.709/",
    pages = "10634--10649",
    abstract = "The pervasive spread of misinformation and disinformation in social media underscores the critical importance of detecting media bias. While robust Large Language Models (LLMs) have emerged as foundational tools for bias prediction, concerns about inherent biases within these models persist. In this work, we investigate the presence and nature of bias within LLMs and its consequential impact on media bias detection. Departing from conventional approaches that focus solely on bias detection in media content, we delve into biases within the LLM systems themselves. Through meticulous examination, we probe whether LLMs exhibit biases, particularly in political bias prediction and text continuation tasks. Additionally, we explore bias across diverse topics, aiming to uncover nuanced variations in bias expression within the LLM framework. Importantly, we propose debiasing strategies, including prompt engineering and model fine-tuning. Extensive analysis of bias tendencies across different LLMs sheds light on the broader landscape of bias propagation in language models. This study advances our understanding of LLM bias, offering critical insights into its implications for bias detection tasks and paving the way for more robust and equitable AI systems"
}

@inproceedings{liu-etal-2024-multilingual,
    title = "Are Multilingual {LLM}s Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings",
    author = "Liu, Chen  and
      Koto, Fajri  and
      Baldwin, Timothy  and
      Gurevych, Iryna",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.112/",
    doi = "10.18653/v1/2024.naacl-long.112",
    pages = "2016--2039",
    abstract = "Large language models (LLMs) are highly adept at question answering and reasoning tasks, but when reasoning in a situational context, human expectations vary depending on the relevant cultural common ground. As languages are associated with diverse cultures, LLMs should also be culturally-diverse reasoners. In this paper, we study the ability of a wide range of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings in a conversational context. Our experiments reveal that: (1) mLLMs {\textquotedblleft}know{\textquotedblright} limited proverbs and memorizing proverbs does not mean understanding them within a conversational context; (2) mLLMs struggle to reason with figurative proverbs and sayings, and when asked to select the wrong answer (instead of asking it to select the correct answer); and (3) there is a {\textquotedblleft}culture gap{\textquotedblright} in mLLMs when reasoning about proverbs and sayings translated from other languages. We construct and release our evaluation dataset MAPS (MulticulturAl Proverbs and Sayings) for proverb understanding with conversational context for six different languages."
}

@inproceedings{mostafazadeh-davani-etal-2024-d3code,
    title = "{D}3{CODE}: Disentangling Disagreements in Data across Cultures on Offensiveness Detection and Evaluation",
    author = "Mostafazadeh Davani, Aida  and
      Diaz, Mark  and
      Baker, Dylan K  and
      Prabhakaran, Vinodkumar",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1029/",
    doi = "10.18653/v1/2024.emnlp-main.1029",
    pages = "18511--18526",
    abstract = "While human annotations play a crucial role in language technologies, annotator subjectivity has long been overlooked in data collection. Recent studies that critically examine this issue are often focused on Western contexts, and solely document differences across age, gender, or racial groups. Consequently, NLP research on subjectivity have failed to consider that individuals within demographic groups may hold diverse values, which influence their perceptions beyond group norms. To effectively incorporate these considerations into NLP pipelines, we need datasets with extensive parallel annotations from a variety of social and cultural groups.In this paper we introduce the D3CODE dataset: a large-scale cross-cultural dataset of parallel annotations for offensive language in over 4.5K English sentences annotated by a pool of more than 4k annotators, balanced across gender and age, from across 21 countries, representing eight geo-cultural regions. The dataset captures annotators' moral values along six moral foundations: care, equality, proportionality, authority, loyalty, and purity. Our analyses reveal substantial regional variations in annotators' perceptions that are shaped by individual moral values, providing crucial insights for developing pluralistic, culturally sensitive NLP models."
}

@inproceedings{nadeem-etal-2021-stereoset,
    title = "{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",
    author = "Nadeem, Moin  and
      Bethke, Anna  and
      Reddy, Siva",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.416",
    doi = "10.18653/v1/2021.acl-long.416",
    pages = "5356--5371"
}

@inproceedings{nangia-etal-2020-crows,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967"
}

@article{newmark2003textbook,
  title={A textbook of translation},
  author={Newmark, Peter},
  year={2003},
  publisher={Shaghai Foreign language education press}
}

@misc{owen2024komodolinguisticexpeditionindonesias,
      title={Komodo: A Linguistic Expedition into Indonesia's Regional Languages}, 
      author={Louis Owen and Vishesh Tripathi and Abhay Kumar and Biddwan Ahmed},
      year={2024},
      eprint={2403.09362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.09362}, 
}

@inproceedings{putri-etal-2024-llm,
    title = "Can {LLM} Generate Culturally Relevant Commonsense {QA} Data? Case Study in {I}ndonesian and {S}undanese",
    author = "Putri, Rifki Afina  and
      Haznitrama, Faiz Ghifari  and
      Adhista, Dea  and
      Oh, Alice",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1145/",
    doi = "10.18653/v1/2024.emnlp-main.1145",
    pages = "20571--20590",
    abstract = "Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in 4.5K questions per language (9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally {\textquoteleft}deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages."
}

@inproceedings{rao-etal-2023-ethical,
    title = "Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in {LLM}s",
    author = "Rao, Abhinav Sukumar  and
      Khandelwal, Aditi  and
      Tanmay, Kumar  and
      Agarwal, Utkarsh  and
      Choudhury, Monojit",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.892",
    doi = "10.18653/v1/2023.findings-emnlp.892",
    pages = "13370--13388"
}

@misc{rao2024normadbenchmarkmeasuringcultural,
      title={NormAd: A Benchmark for Measuring the Cultural Adaptability of Large Language Models}, 
      author={Abhinav Rao and Akhila Yerukola and Vishwa Shah and Katharina Reinecke and Maarten Sap},
      year={2024},
      eprint={2404.12464},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.12464}, 
}

@inproceedings{singh-etal-2024-translating,
    title = "Translating Across Cultures: {LLM}s for Intralingual Cultural Adaptation",
    author = "Singh, Pushpdeep  and
      Patidar, Mayur  and
      Vig, Lovekesh",
    editor = "Barak, Libby  and
      Alikhani, Malihe",
    booktitle = "Proceedings of the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-1.30/",
    doi = "10.18653/v1/2024.conll-1.30",
    pages = "400--418",
    abstract = "LLMs are increasingly being deployed for multilingual applications and have demonstrated impressive translation capabilities between several low and high-resource languages. An aspect of translation that often gets overlooked is that of cultural adaptation, or modifying source culture references to suit the target culture. While specialized translation models still outperform LLMs on the machine translation task when viewed from the lens of correctness, they are not sensitive to cultural differences often requiring manual correction. LLMs on the other hand have a rich reservoir of cultural knowledge embedded within its parameters that can be potentially exploited for such applications. In this paper, we define the task of cultural adaptation and create an evaluation framework to evaluate the performance of modern LLMs for cultural adaptation and analyze their cross-cultural knowledge while connecting related concepts across different cultures. We also analyze possible issues with automatic adaptation. We hope that this task will offer more insight into the cultural understanding of LLMs and their creativity in cross-cultural scenarios."
}

@article{sperber1994cross,
  title={Cross-cultural translation: methodology and validation},
  author={Sperber, Ami D and Devellis, Robert F and Boehlecke, Brian},
  journal={Journal of cross-cultural psychology},
  volume={25},
  number={4},
  pages={501--524},
  year={1994},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@misc{tanmay2023probingmoraldevelopmentlarge,
      title={Probing the Moral Development of Large Language Models through Defining Issues Test}, 
      author={Kumar Tanmay and Aditi Khandelwal and Utkarsh Agarwal and Monojit Choudhury},
      year={2023},
      eprint={2309.13356},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.13356}, 
}

@article{tannen1983cross,
  title={Cross-Cultural Communication.},
  author={Tannen, Deborah},
  year={1983},
  publisher={ERIC}
}

@article{tao2023auditing,
  title={Auditing and mitigating cultural bias in llms},
  author={Tao, Yan and Viberg, Olga and Baker, Ryan S and Kizilcec, Rene F},
  journal={arXiv preprint arXiv:2311.14096},
  year={2023}
}

@article{trivedi2007translating,
  title={Translating culture vs. cultural translation},
  author={Trivedi, Harish},
  journal={Benjamins translation library},
  volume={71},
  pages={277},
  year={2007},
  publisher={JOHN BENJAMINS BV}
}

@inproceedings{wan-etal-2023-personalized,
    title = "Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems",
    author = "Wan, Yixin  and
      Zhao, Jieyu  and
      Chadha, Aman  and
      Peng, Nanyun  and
      Chang, Kai-Wei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.648",
    doi = "10.18653/v1/2023.findings-emnlp.648",
    pages = "9677--9705",
}

@inproceedings{wang-etal-2024-cdeval,
    title = "{CDE}val: A Benchmark for Measuring the Cultural Dimensions of Large Language Models",
    author = "Wang, Yuhang  and
      Zhu, Yanxu  and
      Kong, Chao  and
      Wei, Shuyu  and
      Yi, Xiaoyuan  and
      Xie, Xing  and
      Sang, Jitao",
    editor = "Prabhakaran, Vinodkumar  and
      Dev, Sunipa  and
      Benotti, Luciana  and
      Hershcovich, Daniel  and
      Cabello, Laura  and
      Cao, Yong  and
      Adebara, Ife  and
      Zhou, Li",
    booktitle = "Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.c3nlp-1.1/",
    doi = "10.18653/v1/2024.c3nlp-1.1",
    pages = "1--16",
    abstract = "As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4`s automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. This benchmark serves as a valuable resource for cultural studies in LLMs, paving the way for more culturally aware and sensitive models."
}

@inproceedings{wibowo-etal-2024-copal,
    title = "{COPAL}-{ID}: {I}ndonesian Language Reasoning with Local Culture and Nuances",
    author = "Wibowo, Haryo  and
      Fuadi, Erland  and
      Nityasya, Made  and
      Prasojo, Radityo Eko  and
      Aji, Alham",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.77/",
    doi = "10.18653/v1/2024.naacl-long.77",
    pages = "1404--1422",
    abstract = "We present COPAL-ID, a novel, public Indonesian language common sense reasoning dataset. Unlike the previous Indonesian COPA dataset (XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and therefore, provides a more natural portrayal of day-to-day causal reasoning within the Indonesian cultural sphere. Professionally written by natives from scratch, COPAL-ID is more fluent and free from awkward phrases, unlike the translated XCOPA-ID. In addition, we present COPALID in both standard Indonesian and in Jakartan Indonesian{--}a dialect commonly used in daily conversation. COPAL-ID poses a greater challenge for existing open-sourced and closedstate-of-the-art multilingual language models, yet is trivially easy for humans. Our findings suggest that general multilingual models struggle to perform well, achieving 66.91{\%} accuracy on COPAL-ID. South-East Asian-specific models achieve slightly better performance of 73.88{\%} accuracy. Yet, this number still falls short of near-perfect human performance. This shows that these language models are still way behind in comprehending the local nuances of Indonesian."
}

@inproceedings{zhang-etal-2024-cultural,
    title = "Cultural Adaptation of Menus: A Fine-Grained Approach",
    author = "Zhang, Zhonghe  and
      He, Xiaoyu  and
      Iyer, Vivek  and
      Birch, Alexandra",
    editor = "Haddow, Barry  and
      Kocmi, Tom  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Ninth Conference on Machine Translation",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.wmt-1.120/",
    doi = "10.18653/v1/2024.wmt-1.120",
    pages = "1258--1271",
    abstract = "Machine Translation of Culture-Specific Items (CSIs) poses significant challenges. Recent work on CSI translation has shown some success using Large Language Models (LLMs) to adapt to different languages and cultures; however, a deeper analysis is needed to examine the benefits and pitfalls of each method. In this paper, we introduce the ChineseMenuCSI dataset, the largest for Chinese-English menu corpora, annotated with CSI vs Non-CSI labels and a fine-grained test set. We define three levels of CSI figurativeness for a more nuanced analysis and develop a novel methodology for automatic CSI identification, which outperforms GPT-based prompts in most categories. Importantly, we are the first to integrate human translation theories into LLM-driven translation processes, significantly improving translation accuracy, with COMET scores increasing by up to 7 points. The code and dataset are available at https://github.com/Henry8772/ChineseMenuCSI."
}

@misc{zhou2024doesmapotofucontain,
      title={Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge}, 
      author={Li Zhou and Taelin Karidi and Nicolas Garneau and Yong Cao and Wanlong Liu and Wenyu Chen and Daniel Hershcovich},
      year={2024},
      eprint={2404.06833},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06833}, 
}

