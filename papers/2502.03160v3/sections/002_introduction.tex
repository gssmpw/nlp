
\section{introduction}
\label{sec:intro}
As software grows in size and complexity, logging has become increasingly essential to ensuring software reliability~\cite{he2021survey, Chen2021ASO}. 
Logging means writing log statements into the source code, which generate runtime logs that record valuable information for a range of downstream tasks such as anomaly detection \cite{Zhang2019RobustLA, Lou2010MiningIF, He2016ExperienceRS, Du2017DeepLogAD, Zhang2019RobustLA}, fault diagnosis \cite{Zou2016UiLogIL,xu2023hue, xu2024divlog}, root cause analysis \cite{Amar2019MiningHT, He2018IdentifyingIS, Lin2016LogCB,xu2025openrca}, and program analysis \cite{Ding2015Log2AC, Shang2013AssistingDO, xu2024aligning}. 
The effectiveness of these downstream tasks heavily relies on the quality of the software logs~\cite{He2018CharacterizingTN}. Therefore, appropriate logging is essential to capture critical behaviors during software operation~\cite{YuanUsenixA1}.
\input{insert/logging_example}

% Numerous logging tools have been developed to assist software developers by automatically suggesting log statements based on provided code snippets~\cite{Mastropaolo2022UsingDL, Liu2021WhichVS, Xu2024UniLogAL, xie2024fastlog}.
To help software developers implement effective and efficient logging practices, researchers have been focusing on exploring methodologies for automatic logging in recent years.
As illustrated in Figure \ref{fig:logging_example}, automatic logging typically includes three steps: (1) determining the position, (2) deciding the verbosity level, and (3) specifying the message to be recorded.
% Leveraging the advanced text generation capabilities of large language models (LLMs) \cite{Floridi2020GPT3IN, raffel2020exploring}, \textit{LANCE}~\cite{Mastropaolo2022UsingDL} is the first end-to-end tool to integrate positioning, level selection, and message generation.
To address the automatic logging tasks, numerous logging methods have been proposed thanks to the advanced development of the Large Language Model (LLM).
For example, \textit{LANCE}~\cite{Mastropaolo2022UsingDL} was introduced as the first end-to-end logging method, seamlessly integrating the recommendation of log statement position, verbosity level, and message content.
Building on this foundation,
\textit{UniLog} \cite{Xu2024UniLogAL} employed a warm-up and in-context learning (ICL) strategy to enhance performance. 
\textit{FastLog} \cite{xie2024fastlog} improved the generation efficiency while maintaining precision. 
\textit{LEONID} \cite{Mastropaolo2023LogSG}, based on \textit{LANCE}, combined with deep learning and information retrieval technologies to enhance performance. \textit{SCLogger}~\cite{Li2024GoSC} adapted static analysis to extend the context for the code snippet.
% These studies typically evaluate performance using ad-hoc data splitting from the entire dataset, focusing on metrics such as the accuracy of log statement components, including position, level, and message.
% Machine translation metrics, such as BLEU~\cite{Papineni2002BleuAM} and ROUGE~\cite{Lin2004ROUGEAP}, are also employed to evaluate the quality of generated log messages. Although these evaluations offer valuable insights into logging tool performance, the use of low-quality data and incomplete assessments undermines the reliability of the results.
%%% TODO：陈述事实。（下面这句我随便写的。后面你看情况再作修改。
% However, we find that they all face certain limitations in the method evaluation phases:

% 然而，当前的logging研究面临了一个最大挑战：缺乏一套通用的评估数据和评估方法，使得不同的logging方法之间可以公平对比。这具体体现在两个方面：1. 每个logging method的test set实际上是从其自身training set之间抽取出来的。其ad-hoc的性质导致不同方法在实验中的表现都是来自于不同的数据集，而method之间的有效性无法公平对比 2. 每个logging method都不会使用完全一致的同一套metric做评估。这就导致每个method可能会在不同的维度上体现出更明显的优势性。出于对这一现状的认知，我们认为提出一套comprehensive的benchmark是有必要的。然而，由于当前可用的数据和评估策略都有flaw，因此我们无法trival的从现有数据和方法中构建benchmark，而这我们将在下方讨论。

However, current logging research faces a significant challenge: \textbf{The absence of a standardized evaluation dataset and methodology hinders fair comparisons between different logging methods}. This issue manifests in two key aspects:
(1) The test sets of different logging methods are often derived from their own training data, leading to inconsistent experimental performance and hindering fair comparisons.
(2) The evaluation metrics used for different logging methods are not fully consistent, resulting in varying advantages across different dimensions.
Given these challenges, we argue that \textit{establishing a comprehensive logging benchmark is essential}. However, it is infeasible to naively construct a benchmark via \textit{ensembling} existing evaluation data and metrics due to two major limitations of existing evaluation datasets and methods:


%% 修改策略：多搞几个bad pattern。说明人家有垃圾数据，我们没有垃圾数据（我们自己筛掉了）。此外，人家数据限制512，我们没有这种妥协于小LM的下策。
% \textbf{First, loose standards collection and inappropriate clean strategy compromise the quality of evaluation data and effectiveness in assessing the performance.}
\textbf{First, existing evaluation datasets lack the quality required for reliable assessment.}
For example, 18.45\% cases (2218/12020) in LANCE's test set and 30.36\% cases (2197/7237) in FastLog's test set involve numerous bad patterns of log messages. 
As illustrated in Figure~\ref{fig:low_quality} these anomalies predominantly fall into the following types: (a) duplicated content, (b) empty string, (c) duplicated empty and special characters, (d) contents that mismatch the log levels, and (e) explicit type casting.
These issues distort evaluation outcomes, as tools producing such flawed log statements tend to appear better on metrics despite having poor logging quality.
Moreover, these datasets are restricted to cases with very short contents ($\leq$ 512 tokens), a limitation imposed by their use of foundation models (e.g., T5, PLBART), which support a maximum input of 512 tokens. This constraint not only reflects a compromise to accommodate their models but also inhibits the evaluation of realistic, function-level logging capabilities, as many industrial functions exceed this token limit. Consequently, the datasets fail to meet the requirements for assessing logging quality in real-world scenarios.
% Previous evaluation datasets were typically created by splitting the entire dataset.
% The data selection rules of the entire dataset are commonly loose standards to ensure sufficient data for training. These criteria fail to ensure the quality and consistency of the data.
% Moreover, to accommodate the limitations of the tools, they filtered out all instances exceeding 512 tokens, ignoring the long code snippets that are commonly encountered in real-world development environments.
% This approach undermines the overall effectiveness and real-world applicability of the evaluation results, as it does not accurately reflect the true complexity of software projects. 

%% 修改策略：合并到第三点。
% \textbf{Second, the current evaluation method does not verify whether the generated log statements are compilable.} Generating the compilable log statements is the first requirement when applying automatic logging tools in practice. To relieve developers from the heavy effort required to design and maintain log statements~\cite{Chen2019ExtractingAS, Chen2017CharacterizingAD}, the basic requirement is to ensure that our tool can be seamlessly integrated into the DevOps process without introducing additional errors that require extra debugging effort from developers. Current evaluation methods merely focus on whether each component of log statements (\ie position, verbosity level, message) matches the ground truth but cannot assess whether the generated log statements might introduce compilation errors such as wrong code format or use undefined variables. Evaluating the compilability of predicted log statements reflects the effectiveness and reliability of logging tools, which are essential for practical use.

%% 修改策略：1. imply一下“similarity-based” quality是唯一解。 2. 不要强调2k个dynamic来自hadoop。
% \textbf{Third, the evaluation method cannot evaluate the quality of runtime logs generated by the predicted log statements}. Current evaluation methods assess the performance of tools based on the correctness of individual components of log statements.
% However, current metrics struggle to accurately reflect the quality of runtime logs in the real execution environment. Even a slight shift of log statement can lead to the miss of essential runtime details, such as a several-line shift from the ground truth or a mismatch in verbosity level. For instance, a minor difference in verbosity levels (e.g., debug vs. info) can cause critical information missed due to log level threshold settings in the source code.
% Therefore, we need to evaluate the quality of log statements in a real execution environment, with the goal of obtaining appropriate logs in specific scenarios, rather than merely focusing on the correctness of individual components or relying on statistical metrics to reflect the performance of logging tools.
\textbf{Second, current evaluation methods lack a fine-grained view for comprehensive assessment.}
Existing evaluations primarily focus on static code similarity between candidate log statements generated by logging methods and oracle log statements written by developers.
However, the code compilability is entirely neglected and can not be measured by static evaluation.
Our preliminary analysis revealed that 20\%-80\% of log statements generated by current methods fail to compile, a figure that has not been previously reported.
Furthermore, such static similarity fails to capture logging performance during program execution.
For instance, placing a log statement inside or outside a \textit{loop} may differ by only one line in code but result in vastly different numbers of log entries at runtime. Conversely, moving a log statement above or below a \textit{comment} may have no effect on the runtime logs despite a one-line difference in the code. Such runtime differences are overlooked in static evaluations.
These limitations highlight fundamental flaws in the current evaluation approach.
\input{insert/low_quality_data_example}
% 具体来说，当前的评估指标仅仅从candidate log statement generated by logging methods和oracle log statement written by developers的静态代码相似度来评估logging quality。然而静态代码相似度并不能体现log statement在程序执行时的performance。例如把一个log statement生成在循环体内部或外部可能只相差一行，但执行时产生的log数量可能会有数量级上的差距。但是把log statement生成在一个注释语句的上方或下方，差距也只有一行，但是却可能产生的日志不会有任何差异。这种运行时产生日志的差异是无法从静态评估中得到的。在我们的初步评估中，我们发现现有方法产生的日志中，有20\%甚至都无法通过编译并生成日志文件——而这种可编译性的评估在当前的logging research中是完全被忽视的。这一切都说明当前的评估是flaw的。

%%% 由于评估数据来自大量GitHub仓库导致我们无法直接从真实的部署场景中使用日志做一些well-designed下游任务来直接评估日志语句的好坏，researcher只能暂时妥协于使用similarity-based strategy来作为proxy，来评估日志语句的好坏。我们的信念应当是：oracle log被长久维护且广泛认可，那么接近于它的生成的log应该是好的（即similarity-based）。但是，当前只从静态代码的角度评估similarity是非常不全面的。

%%% TODO: 我们的insight？对similarity-based的考虑？都没写。

% 为了在解决这两个问题的前提下构建一个全新的comprehensive logging benchmark，我们的insight是：1. 更热门且迭代更频繁的仓库的日志语句的质量相比大多数不知名代码仓库的质量会更高，减少各类bad-pattern存在的可能性。2. 通过代码执行的方式直接评估生成的日志文件的质量可以提供一种run-time的视角，enabling fine-grained logging quality evaluation。基于这种insight，我们提出了\methodname.

  % \textbf{Our Work.} 
To address these two challenges, our key insights are: (1) \textit{\textbf{Log statements from the most widely used and frequently updated repositories tend to be of higher quality with fewer bad logging patterns.}} (2) \textit{\textbf{Log files printed in runtime can serve as a proxy for evaluating log statements quality with a more comprehensive view of program logging performance.}}
Based on these insights, we introduce \methodname, the first unified logging benchmark involving a large-scale evaluation dataset and a comprehensive evaluation method for assessing both the static log statements and runtime log files printed via code execution.
Our dataset comprises 21,804 instances mined from the 10 most popular and actively maintained GitHub projects~\cite{GitHub}, each with at least 10,000 stars, and 500 log-related issues spanning diverse domains and application scenarios.
Furthermore, \methodname introduces \textit{dynamic evaluation} in addition to traditional static evaluation for log statements.
Specifically, it not only applies a suite of static metrics to quantify the quality of generated log statements but also reintegrates them into real project code, compiles, and executes them.
This approach enables a realistic assessment beyond the traditional static approach, highlighting major limitations in existing logging methods:  even the best-performing logging method fails to compile in 20.1\% of cases, and the logs printed in runtime show only 21.32\% cosine similarity to the oracle logs.
% Through its rigorous and standardized evaluation approach, \methodname bridges the gap between real-world logging requirements and prior assessments, highlighting substantial opportunities to further advance the development of automatic logging tools.
By providing a unified, standardized, and comprehensive evaluation dataset and methodology, we believe \methodname establishes a foundational step toward advancing research in automatic logging.


To sum up, our contributions are shown as follows:
\begin{enumerate}
    \item We collected a high-quality, diverse, and large-scale dataset comprising 21,804 instances from 10 popular, high-quality GitHub~\cite{GitHub} projects, spanning various domains with differing logging requirements.
    \item We propose a dynamic evaluation approach that reintegrates the generated log statements into projects to evaluate their printed runtime logs as a more fine-grained proxy for illustrating logging performance.
    \item We conducted a comprehensive evaluation of popular automatic logging tools and revealed the key limitations based on the analysis of the evaluation results. 
    \item All the data and code for \methodname are publicly available\footnote{\url{https://github.com/shuaijiumei/logging-benchmark-scripts}}, providing valuable resources for both developers and researchers to advance the field of automatic logging.
\end{enumerate}

  % \textit{Paper Structure:} The rest of the paper is organized as follows.
  % The section \ref{sec:moti} introduces existing related work and motivates our work by analyzing the existing dataset, and evaluation methodology. Section \ref{sec:method} introduces \methodname. Section \ref{sec:eval} we conduct a comprehensive evaluation for recent automatic logging tools via \methodname. Section~\ref{sec: vadility} discusses the potential threats to validity. Finally, Section~\ref{sec: conclusion} gives a conclusion for this paper.
  