\subsubsection{Evaluation Data}
\label{sec:moti:dataset}
Current evaluation datasets are ad-hoc, derived from splitting the entire collected data according to loose standards. More specifically, to ensure a sufficient amount of training data, previous studies had to adopt lenient rules for data collection. Additionally, some tools, to accommodate the limitations of their backbone models, further filter out data exceeding 512 tokens in length.
These strategies compromise data quality, reducing the reliability and effectiveness of evaluation results.
For example, \textit{LANCE} adopted the strictest data collection rules among all tools, requiring a minimum of 500 commits, 10 contributors, and 10 stars, while also excluding forked repositories on GitHub~\cite{GitHub}. However, even in the \textit{LANCE} dataset, low-quality data is pervasive. As shown in Figure~\ref{fig:low_quality}, several patterns of low-quality logging practices persist. These patterns include the duplication of variables within a single log statement, creating unnecessary redundancy; the inclusion of empty strings or meaningless content, resulting in uninformative messages; excessive use of special characters or punctuation, making printed logs difficult to parse; and mismatched logging levels, where critical messages are assigned inappropriate severities, leading to misclassification~\cite{gojko2006logging, Chen2017CharacterizingAD}.
Using low-quality data as an evaluation dataset fails to produce reliable results. Such results do not align with the expectation that logging tools should generate high-quality log statements within the code.
Furthermore, to accommodate the input length limitations of these tools,  previous studies~\cite{Mastropaolo2022UsingDL, Mastropaolo2023LogSG, Xu2024UniLogAL, xie2024fastlog} have filtered out instances longer than 512 tokens. While this data-cleaning strategy reduces the complexity of evaluation data, it also introduces significant biases by excluding longer code snippets, which are common in real-world development environments. This approach simplifies the evaluation process but fails to capture the full scope of challenges that logging tools would face when dealing with complex and extended code bases. Consequently, the evaluation results may not accurately reflect the tools’ ability to handle the demands of real-world software development, where longer snippets and more intricate code structures are prevalent. Therefore, there is a pressing need for a public, large-scale, high-quality, and diverse benchmark dataset that can better represent real-world codebases and provide a standardized platform for evaluating automatic logging tools. 

\input{insert/static_evaluation_example}

\subsubsection{Evaluation Method}
\label{sec: Motivation: Evaluation Method}
The current evaluation method focuses on assessing model performance by comparing the accuracy of each log statement component (\ie position, verbosity level, message). We refer to this as \textit{\textbf{static evaluation}} in this paper. Figure \ref{fig:static_evaluation} provides an example of static evaluation to demonstrate how it works. In this example, only the verbosity level matches, so the matched level count will increase by one, and BLEU scores will be used to calculate the average score. Finally, the matched portion of each component and the average BLEU score will be combined to reflect the performance of the tools. These statistical metrics provide insights into evaluating the quality of predicted log statements; however, this approach still does not align with the goal of ensuring that the predicted log statements generate high-quality logs.

% \input{insert/log_message_BLEU}
\input{insert/pic_unreachable_statements}
\textbf{First, it cannot assess the compilability of predicted log statements.} Compilability is a fundamental requirement for generating appropriate logs. This capability ensures that log statements can be applied to the source code without introducing compilation errors, avoiding wasting developers' efforts to debug for the predicted log statements. 
However, the current evaluation methodology, which focuses solely on comparing the individual components of log statements, cannot identify predicted log statements that use undefined variables or contain incorrect code syntax which could introduce compilation errors. As shown in Figure~\ref{fig:unreachable_statements}, the predicted log statement is injected into an unreachable position. Applying static evaluation would fail to recognize that this prediction could cause compilation errors. Instead of classifying it as a completely incorrect prediction, the evaluation would count it as a level match instance and assign it a BLEU score of 55.03, contributing to the statistical value. This could introduce significant bias when evaluating the performance, appearing favorable in metrics but performing poorly in actual usage.

\textbf{Second, it cannot assess the runtime logs generated by predicted log statements. }
The goal of writing log statements is to obtain the appropriate logs during software execution so that the quality of log statement is ultimately decided by the quality of runtime logs. 
Although comparing the correctness of log statement components provides insights into assessing the quality of logs, it can introduce bias when using statistical metrics that focus solely on log statements to evaluate the performance of logging tools.
A simple example is a mismatched verbosity level between the predicted log statement and the ground truth. In a case where only the verbosity level is mismatched while the other components are exactly matched, this instance would receive a high score in static evaluation. However, in a real-world scenario, for important error logs, if the predicted level is below the threshold, the logs will not be generated, regardless of how critical the message is. Conversely, for debug logs, if the predicted level exceeds the threshold, these logs could be printed in the production environment, leading to issues such as sensitive information leakage and increased storage costs. These two situations demonstrate that even a shift in verbosity level can turn a log statement into a completely bad case.
A more complicated case is presented in Figure~\ref{fig:static_evaluation_bad_2}. Under \textit{\textbf{static evaluation}}, this prediction with only a shift in two lines in position, also could be rewarded highly in statistic metrics. 
However, it is completely incorrect: placing the log statement at line 2 risks a \textit{NullPointerException} if the \textit{timer} object is \textit{null} when \textit{timer.get()} is called. This would cause the program to crash during execution, meaning that not only would the logs fail to be generated, but the entire program could terminate unexpectedly.
In real-world scenarios, this kind of error is critical and goes unnoticed in static evaluations. While static evaluation rewards this prediction for structural correctness, it fails to account for the actual behavior of the code when executed. Therefore, this example reinforces the need for more comprehensive evaluation methods that go beyond code-level metrics and consider the real-world implications of log statements. 

In conclusion, a log statement’s quality should be determined by its ability to avoid introducing extra errors and generate meaningful, contextually appropriate logs.

    \input{insert/static_evaluation_example_2}

\begin{tcolorbox}
\textbf{Insights:} The motivating study underscores major limitations in current evaluation efforts, including the lack of a diverse, high-quality dataset and methods that do not meet real-world needs. To improve log statement evaluation, we should collect a high-quality, diverse dataset and use an execution-based method to directly evaluate the runtime logs generated by log statements.
\end{tcolorbox}