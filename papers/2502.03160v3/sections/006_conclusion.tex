\section{Threats to Vadility}
\label{sec: vadility}
\begin{itemize}
    % \item \textbf{Construct Validity}: We use BLEU \modify{and ROUGE?} to assess the quality of the generated log messages. Although text similarity metrics may not fully capture the quality of the generated text~\cite{Gros2020CodeTC, Roy2021ReassessingAE}, we follow previous works~\cite{xie2024fastlog, Xu2024UniLogAL, He2018CharacterizingTN, Ding2023LoGenTextPlusIN} to use BLEU as widely-accepted quality measures for the generated log messages. We also adapt Cosine Similarity based on TF-IDF to evaluate the quality of log content. Since log content, excluding the log header, is primarily composed of natural language, cosine similarity effectively captures the semantic meaning~\cite{Salton1975AVS}.
    \item \textbf{Construct Validity}: The first threat comes from the use of two widely-accepted syntactic metrics in previous logging research (BLEU and ROUGE)~\cite{xie2024fastlog, Xu2024UniLogAL, He2018CharacterizingTN, Ding2023LoGenTextPlusIN}, as they may not fully capture the semantics of the log messages~\cite{Gros2020CodeTC, Roy2021ReassessingAE}. To mitigate this threat, we also adapt Cosine Similarity to evaluate the semantic quality of log messages.  Since all log headers are excluded, leaving log messages primarily in natural language, Cosine Similarity can effectively capture the log's semantic meaning~\cite{Salton1975AVS}.
    \item \textbf{Internal Validity}: The second threat to validity concerns reproducing the baseline. To minimize inconsistencies, we adapted the released models from previous work~\cite{xie2024fastlog, Mastropaolo2022UsingDL, Mastropaolo2023LogSG} and sought guidance from the authors of the closed-source model UniLog~\cite{Xu2024UniLogAL} to reproduce the tool under supervision.
    \item \textbf{External Validity}: 
    % In this work, we evaluate the tools with 10 widely recognized Java projects in different domains. The results of each tool are examined in different requirements of logging practices. The novel evaluation method we propose is not limited to Java projects, we will explore its effects in other programming languages in the future. 
    {The third threat comes from the potential data contamination risks in our dataset. 
    % we adopt the methodology proposed by UniLog~\cite{Xu2024UniLogAL}.
    To mitigate it, we follow the methodology used by GPT-3~\cite{Brown2020LanguageMA} to assess data contamination severity, i.e., considering test samples with more than a 13-gram overlap with the original code samples as contaminated.
    % Following the established GPT-3 contamination assessment protocol 
    Specifically, we perform a 13-gram overlap analysis between test and training samples of all evaluation methods. Results indicate a peak contamination rate of 1.24\% across all training datasets, empirically demonstrating no evidence of data leakage in our framework.} 
\end{itemize}


\section{Conclusion}
\label{sec: conclusion}
% This paper introduces \methodname, designed specifically for automatic logging tools. It includes a high-quality dataset and a novel dynamic evaluation method focused on runtime logs, addressing key limitations of prior studies and bridging the gap between real-world requirements and existing evaluation frameworks. This dynamic evaluation method assesses both the compilability of predicted log statements and their effectiveness in generating runtime logs. Using \methodname, we evaluate popular end-to-end automatic logging tools and find that generated log statements fail to compile in 20.1\% to 83.6\% of cases. Even the best predictions achieve only 0.213 cosine similarity between generated and ground-truth runtime logs. These results show that automatic logging still has a long way to go.

In this paper, we introduced \methodname, a comprehensive benchmark for automatic logging tools evaluation, featuring a diverse dataset from 10 established projects and a novel dynamic evaluation methodology. Our benchmark revealed significant limitations in existing tools through both static and dynamic perspectives: substantial accuracy drops in log position, level, and message prediction compared to reported results, high compilation failure rates (20.1\%-83.6\%), and low runtime log similarity (maximum 21.32\%). These findings highlight critical gaps between current tool capabilities and practical requirements. We believe \methodname establishes a solid foundation for advancing automatic logging research by providing standardized evaluation metrics and exposing areas requiring improvement.

\section{DATA AVAILABILITY}
All the code and data used in our study are publicly available on \url{https://github.com/shuaijiumei/logging-benchmark-scripts}.