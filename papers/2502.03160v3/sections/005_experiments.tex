\section{EXPERIMENTS}
\label{sec:eval}
In this section, we use \methodname to evaluate existing end-to-end automatic logging tools and analyze the evaluation results to find insights to guide the following work. From the perspective of empirical software engineering, we set three research questions:
\begin{itemize}
    % \item \textbf{RQ1:} How accurately can automatic logging tools predict logging statements in static evaluation?
    % \item \textbf{RQ2:} What percentage of source code remains compilable after automated logging statement insertion?
    % \item \textbf{RQ3:} How similar do the generated log statements print logs in dynamic evaluation?
    \item \textbf{RQ1:} How similar are automatically generated log statements to human-written logging practices?
    \item \textbf{RQ2:} How compilable are the log statements generated by automatic logging tools?
    \item \textbf{RQ3:} How consistent are runtime logs from auto-logged programs with oracle-generated logs?
    %%% 当前logging methods在\methodname上的
\end{itemize}

% examines how well logging tools predict log statements using the \methodname static evaluation dataset and corresponding static evaluation method. 
    Specifically, RQ1 aims to evaluate the performance of the tools in generating log statements that closely resemble the ground truth. RQ2 focuses on assessing the compilability of the predicted log statements, determining whether these predictions can be seamlessly integrated into the code without causing compilation errors. Finally, RQ3 evaluates the runtime logs produced by the generated log statements. We examine the similarity between the predicted logs and the expected logs. We begin by introducing the automatic logging tools selected for evaluation. Then we provide a detailed analysis of these tools by answering the three research questions. This comprehensive evaluation allows us to draw insights into the strengths and limitations of current approaches for automatic logging.

\subsection{Automatic Logging Tools}
Automatic logging is a hot topic, leading to the development of many tools for determining specific parts of log statements and end-to-end automatic logging tools in recent years. In this paper, we focus on end-to-end logging tools. We reached out to the authors of popular end-to-end automatic logging tools for assistance in rebuilding these tools. Because SCLogger~\cite{Li2024GoSC} is still under construction,
we ultimately selected four methods for evaluation: \textit{LANCE~\cite{Mastropaolo2022UsingDL}, LEONID~\cite{Mastropaolo2023LogSG}, FastLog~\cite{xie2024fastlog}, UniLog~\cite{Xu2024UniLogAL}}. We detailed the method in the following.

\textit{\textbf{LANCE:}} \(LANCE\)~\cite{Mastropaolo2022UsingDL} is the first model designed to generate and insert complete log statements in code. It takes a method requiring a log statement and outputs a meaningful log message with an appropriate logging level in the correct position. Built on the Text-To-Text Transfer Transformer (T5) model~\cite{Raffel2019ExploringTL}, \(LANCE\) is trained specifically for injecting proper logging statements.

\textit{\textbf{LEONID}:} \(LEONID\)~\cite{Mastropaolo2023LogSG} is the updated version of \(LANCE\). With a combination of DL and Information Retrieval (IR), \(LEONID\) achieved a better performance. \(LEONID\) provided two versions, \(LEONID_S\) is for single log statement generation, and \(LEONID_M\) is for multiple log statements generation. Since \(LEONID_M\) can generate more than one log statement at a time, it introduces ambiguity in determining the correct correspondence between the generated and expected log statements when more than one log statements are generated by static evaluation~\cite{Mastropaolo2023LogSG}. Therefore, we only applied \(LEONID_S\) for static evaluation.

\textit{\textbf{UniLog:}} \(UniLog\)~\cite{Xu2024UniLogAL} is the first attempt to adapt Warm-up and In-context-learning strategy to enhance the model's ability to generate log statements.
Due to limitations in assessing the original \textit{UniLog}, we reproduced it using two backbone models: CodeLlama-7B~\cite{Rozire2023CodeLO} and DeepSeek-V3~\cite{DeepSeekAI2024DeepSeekV3TR}. We applied the warmup process exclusively to the CodeLlama-7B backbone model while employing the ICL strategy to construct prompts for both models.
The data are sourced from \textit{LANCE}~\cite{Mastropaolo2022UsingDL} to warm up and generate ICL content. The effectiveness of In-Context Learning often depends on whether the examples are in-distribution or out-of-distribution relative to the evaluation data. Since \textit{LANCE’s} data distribution differs from our evaluation data, this may affect \textit{UniLog’s} performance. We will use \(\bm{UniLog_{{cl}}}\) to represent the version based on CodeLlama-7B, \(\bm{UniLog_{{ds}}}\) to represent the version based on DeepSeek-V3.

\textit{\textbf{FastLog:}} \textit{FastLog}~\cite{xie2024fastlog} defines the logging task in two steps: finding the position and generating and inserting a complete log statement into the source code. This approach avoids rewriting the source code, a key limitation of \(LANCE\). They utilized PLBART~\cite{Ahmad2021UnifiedPF} as the base model to fine-tune two separate models: one for predicting insertion position, the other for generating log statements. With the heuristic rule, log statements only appear after certain special characters, \textit{FastLog} enhances efficiency while maintaining accuracy in generating log statements.


 \input{insert/table_static_evaluation_results}
\subsection{RQ1: How similar are automatically generated log statements to human-written logging practices?}
\label{rq1}
\input{insert/table_static_length_compare}
% Following previous studies~\cite{Mastropaolo2022UsingDL, Xu2024UniLogAL,xie2024fastlog,Mastropaolo2023LogSG}, we first processed our dataset to create evaluation pairs \(<M_s, M_t>\) with \(M_s\) representing the input provided to the model (\ie \(M_s\) with one removed log statement) and \(M_t\) being the expected output (\ie \(M_t\) is the removed log statement). After processing, we got 39,600 instances. 
We evaluated four automatic logging tools using \methodname, leveraging 39,600 static evaluation pairs.
The evaluation results are presented in Table~\ref{table:static_evaluation_results}. 
% The PA, LA, and MA metrics align with prior evaluation frameworks from the selected logging tools, with values directly sourced from their original studies. Our analysis reveals substantial disparities in effectiveness across the four evaluated tools, underscoring variability in their logging capabilities.
All logging tools show average accuracy drops of 37.49\%, 23.43\%, and 15.80\% in predicting log position, level, and message compared to their reported results. Most strikingly, \(LEONID_S\) collapses from 31.55 → 1.96 in MA (\(\downarrow93.8\%\)) and \(UniLog_{cl}\) plummets from 76.90 → 23.49 in PA (\(\downarrow69.5\%\)). Those drops expose severe overfitting to narrow, synthetic datasets in prior studies.
When evaluated on \methodname, state-of-the-art tools achieved limited scores of 18.00 (DEA), 20.14 (BLEU-4), and 29.88 (ROUGE-L) in STS. These results highlight persistent limitations in generating semantically coherent natural language descriptions and reliably identifying contextually relevant dynamic content for logging.

% Despite these shortcomings, \textit{FastLog} shows only minor deviations from previous studies, maintaining stable performance across all metrics. In contrast, the other three tools, particularly \textit{LEONID}, experience a significant decline in accuracy across all evaluated metrics.

% These factors collectively contribute to the observed instability in logging tool performance across different projects.  As demonstrated in our results, the variability in tool performance across different projects underscores the necessity of diverse evaluation data to capture the full spectrum of a tool’s capabilities and limitations.

To investigate the performance discrepancy relative to prior studies, we partitioned the dataset into two subsets: instances with sub-512 token lengths and those exceeding 512 tokens. This division enables us to determine whether instance length contributes to the performance discrepancies observed among the logging tools, providing further insight into how different data characteristics influence tool effectiveness.
After dividing the data, we obtained two groups: long data containing 8,925 instances and short data with 30,675 instances. The evaluation results in Table~\ref{table:static_len_compare} show a clear difference in tool performance based on the length of the instances.
Notably, \textit{LANCE} and \textit{LEONID} struggled with instances longer than 512 tokens, failing to generate syntax-correct code and, in some cases, producing incomplete code. This explains why their scores for these cases are reported as zero, highlighting the input length limitations of both tools and their inability to handle longer, more complex instances effectively.
\(UniLog_{cl}\) and \textit{FastLog} show a considerable drop in PA and DEA when handling longer data, indicating that they struggle to predict log positions and select dynamic variables in more complex instances. \(UniLog_{ds}\) exhibits a significant decline in positional accuracy but only marginal degradation in dynamic variable identification. Notably, it demonstrates stronger performance on metrics such as MA and STS when processing longer input sequences. This observation implies that enhancing a model’s architectural foundation and providing richer contextual data can improve its capacity to generate syntactically and semantically coherent logging statements through greater contextual awareness.

Although \(UniLog_{ds}\) is equipped with a more powerful backbone, it still struggles to choose the appropriate position for the log statement. As widely demonstrated that the LLMs are not good at counting numbers~\cite{Ahn2024LargeLM}, making LLM generate the exact line number might not be a wise choice. Furthermore, by simply analyzing the control flow graph of the code, we might be able to exclude positions where logging is not feasible to leave fewer choices.
\input{insert/pic_projects_diversity}


To evaluate tool stability, we analyzed performance consistency across individual projects. As shown in Figure~\ref{fig:project_diveristy}, tool performance exhibits significant variability across projects. For instance, in Flink, the state-of-the-art tool achieves an MA score of 13.68, contrasting sharply with the overall project average of 6.93. Similarly, in Keycloak, the state-of-the-art DEA reaches 43.84 compared to the aggregate average of 16.60. These stark discrepancies underscore inconsistencies in the reliability and generalizability of logging tools when applied to projects with distinct requirements and contexts.


\begin{tcolorbox}
    \textbf{Answer to RQ1.} 
    All logging tools show average accuracy drops of 37.49\%, 23.43\%, and 15.80\% in
predicting log position, level, and message compared to their reported results.
    All logging tools exhibit persistent limitations in generating semantically coherent log descriptions and reliably identifying contextually relevant dynamic variables. Moreover, they fail to achieve consistent performance reliability across diverse logging requirements and project-specific contexts.
    % Since LLMs struggle with counting, it might be better to avoid outputting exact line numbers and instead add tags in the source code. Additionally, analyzing the control graph to exclude impossible positions might be useful to enhance log position determination.
\end{tcolorbox}

\input{insert/table_compiled_failed}

\subsection{RQ2: How compilable are the log statements generated by automatic logging tools?}
\label{rq2}
To evaluate the tools’ ability to generate compilable log statements, we replace existing log statements with predicted ones and recompile the project to check for successful compilation. As shown in Table~\ref{table:compile_failed_rate},
the best-performing tool, \textit{FastLog}, achieves a 79.9\% Compilation Success Rate, followed by \(UniLog_cl\) at 70.3\%, \(UniLog_ds\) at 60.2\%, \textit{LANCE} at 49.4\%, \(LEONID_M\) and \(LEONID_S\) at 25.0\% and 16.4\%, respectively.
The results reveal a key limitation of \textit{LANCE} and \textit{LEONID} that they regenerate the entire code snippet, which increases the risk of unintended code changes and can potentially lead to compilation errors. In contrast, \textit{FastLog} and \textit{UniLog} focus solely on generating the new log statement, minimizing the risk of errors by limiting codebase modifications. Although \textit{FastLog} is one of the best tools available, it still leads to significant instances (20.1\%) of compile failure. 

% When log statements fail to compile, they can prevent the system from functioning as intended and complicate debugging or error tracking. To ensure the reliability of automatic logging tools in real-world environments, it's crucial that the log statements they generate integrate smoothly into the existing codebase.

% Given the importance of reliable compilation,

To understand the reason for the compilation failures reason, we conducted a manual review of the compilation failures to identify the specific causes. 
% This analysis will provide further insight into the key weaknesses of these tools and potential areas for improvement. 
We randomly selected 100 failed instances from the best tool, \textit{FastLog}, and the first two authors cross-checked the causes. The analysis results are presented in Table~\ref{table:compile_failed_reason}. The most common failure, occurring in 56 instances, is due to using the wrong logging name~(\ie \textit{Logger}, \textit{LOG}), indicating that incorrect or non-existent log functions are being invoked. Using undefined methods accounts for 21 failures, followed by using undefined variables with 15 failures. Less frequent issues include incompatible types (3), and unreachable statements (1). Others mean generating the wrong syntax code, which we will not analyze.

The majority of failures (totaling 92 instances) involve undefined references to methods (21 instances), variables (15 instances), or logging names (56 instances). These undefined reference failures are primarily due to the limited context provided by existing function-level logging tools, lacking critical details on valid variables, methods, libraries, and packages relevant to the target function. Current tools are designed for function-level input, highlighting the need for logging tools to integrate better context awareness and validation checks to ensure compatibility with the existing codebase. The less frequent errors are also noticeable. For instance, the issue of unreachable statements points to a weakness in that tools lack an understanding of the code’s control flow, resulting in log statements placed in non-executable paths. Collectively, these errors highlight the need for automated logging tools to adopt enhanced context-aware strategies, improving the accuracy and contextual relevance of generated logs.


% Figure~\ref{fig:unreachable_statements} shows an example of unreachable statements, the code contains a \textit{while (true)} loop with a conditional check for \textit{shutdown}. If the \textit{shutdown} condition is true, the program will execute the return statement, which will immediately exit the method, preventing any further code from being executed. As a result, the log statement at the bottom \textit{LOG.info("Started CacheReplicationMonitor.")} becomes unreachable because the return statement ensures that the method terminates before reaching this log statement. This example shows that not all locations are ideal for log statements. By carefully analyzing the control graph, we can narrow down suitable positions, helping us make better choices for log placement.


% \textbf{ \textit{Findings:}} The low compilation success rates suggest that automatic logging tools should incorporate more advanced context-awareness mechanisms. Given that current tools are designed for function-level input, we could either extend them to class-level or use static analysis to enable variable scope tracking, control flow analysis, and accurate type-checking. Especially, by analyzing the control graph, we can exclude unsuitable positions, helping us decide more effectively where to log.
\begin{tcolorbox}
    \textbf{Answer to RQ2.} \textsc{FastLog} achieved the best performance in generating compilable log statements, yet over 20\% of the generated log statements still failed to be compiled. According to our analysis of compilation failure reasons, these failures primarily stem from a lack of critical contexts corresponding to the target function, e.g., valid variables, methods, libraries, packages, execution paths, and type information. To improve the reliability of automatic logging tools, it is crucial that they incorporate mechanisms to gather and utilize this additional context during the log statement generation process.
\end{tcolorbox}
 
\subsection{RQ3: How consistent are runtime logs from auto-logged programs with oracle-generated logs?}
\label{rq3}
To answer RQ3, we compare the semantic similarity of logs generated by predicted log statements and by the oracle.
It is important to highlight that, aside from \(LEONID_M\), all other tools operate under the strong assumption that the given code snippet requires exactly one log statement. 
% To highlight this inappropriate assumption, we only allow the tools one chance to predict the log statement, even in cases where multiple log statements might be needed. 
To highlight the inappropriateness of this assumption, we only allow the tools one chance to predict the log statement, even in cases where multiple log statements are needed. 
Our experimental design emulates real-world logging scenarios by abandoning the assumption that single log statements universally suffice, thereby exposing a critical limitation in existing tools' capacity to determine contextually appropriate logging density for a given code context.

\input{insert/table_log_similarity}

Table~\ref{table:log_similarity} compares the semantic similarity between logs from source and predicted log statements.
The results indicate that logs from the predicted statements significantly deviate from the ground truth, with consistently low scores across metrics like Cosine Similarity, BLEU, and ROUGE.
 Limiting each code snippet to a single log statement often sharply compromises log similarity, especially when multiple statements are needed. This limitation is particularly clear when comparing \(LEONID_S\) and \(LEONID_M\). Although \(LEONID\) overall performs poorly, \(LEONID_M\) stands out as the only tool capable of generating multiple log statements, which enables it to outperform \(LEONID_S\) in similarity scores. This difference underscores the importance of tools being able to determine the appropriate number of log statements for accurate and effective logging, rather than assuming a single statement suffices.
\input{insert/table_FPLG_FNLG}

To understand the low semantic similarity scores, we examined the log generation process and found many instances where predicted logs were either redundant or missed key information present in the original logs. We quantify this issue using two metrics: FPLR and FNLR, as reported in Table~\ref{tab:table_FPLG_FNLG}. For example, \textit{FastLog} reports a 9.28\% FPLR, meaning logs record redundant information in 9.28\% of cases, and an 18.28\% FNLR, indicating expected information in logs was missing in 18.28\% of cases. We sampled 100 examples from FastLog, the state-of-the-art (SOTA) model, and manually analyzed the reasons for mismatches with the original logs. The results are presented in Table~\ref{tab:table_FPLG_FNLG_Reason}. We found that the primary reasons for failures in FNLR and FPLR differ significantly. For FNLR, the most common issue was the predicted log statements being beyond the execution path (35 cases), followed by lower verbosity levels (24 cases), and a smaller number caused by wrong code format (4 cases). In contrast, for FPLR, the main problem was higher verbosity levels (30 cases), with a few instances of log statements being beyond the execution path (3 cases). Overall, verbosity mismatches and execution path discrepancies were the dominant contributors, highlighting challenges in aligning predicted logs with actual logging requirements. The factors leading to FPLR and FNLR underscore a critical issue: while static metrics offer valuable insights into the quality of generated log statements, the actual logs are shaped by numerous contextual factors. Without a thorough understanding of the execution context, it is not possible to comprehensively evaluate the quality of log statements. Even minor discrepancies can cause significant deviations between generated logs and source logs. For instance, while the predicted log statement may capture the key information required to reflect system behavior, its effectiveness can be compromised if it is not positioned along a critical execution path or if its verbosity level is mismatched. In such cases, the log statement may fail to record essential information when key events occur. This issue majorly arises from the tools’ lack of awareness of verbosity thresholds and the control graph of the code, which limits their ability to adjust verbosity and determine appropriate log positions based on the context or execution requirements. These two limitations highlight that current logging tools lack the adaptability and context-awareness needed for effective real-world applications.

\begin{tcolorbox}
    \textbf{Answer to RQ3.} The best-predicted log statements by \textit{FastLog} achieve only 21.32\% cosine similarity with the original logs. Many predictions record redundant information, while others miss key details. The missing key information is primarily due to the prediction being placed beyond the execution path during important events, while setting a higher verbosity level in the log statements leads to redundancy. This result highlights that automatic logging tools still have significant room for improvement.
\end{tcolorbox}



