\section{\methodname}
\label{sec:method}
\methodname is a novel benchmark designed to evaluate automatic logging tools in codebases systematically. The benchmark comprises 21,804 code snippets and 39,600 log statements extracted from 10 most popular open-source repositories, selected for their active maintenance, diverse application domains, and representation of modern software engineering practices. \methodname employs a dual evaluation framework: \textbf{Static Evaluation} evaluates the \textit{similarity of log statements} (\eg \textit{position}, \textit{level}, \textit{variable}, \textit{message}) between predicted and reference log statements, while \textbf{Dynamic Evaluation} assesses code \textit{compileability} and the \textit{similarity of logs} printed by those log statements during runtime execution. Both evaluation methods will be described later in this section.







% 思考一下，我们的数据集应该有哪些 feature 才能说明数据集质量高？
% 1. 数据本身质量高
% 2. 数据集足够 diversity

% 这段内容依然缺少了两个关键arguement：
% 1. repository是按rank筛选的the best ones。
% 2. log statement是tested by time。长期不动的。默认是开发者觉得高质量不用管的。
% 最开始的思路是， log statement 的数量足够多 -> 开发者对 日志代码质量的重视程度高 -> 所以单个的质量放心


\subsection{Dataset Construction}
\label{sec:method:static_dataset_building} 
As discussed in Sec.~\ref{sec:intro}, the quality of the evaluation dataset is crucial for assessing the performance of tools. 
% The strictest existing rules for dataset collection—500 commits, 10 contributors, and 10 stars—are inadequate to ensure the quality of the evaluation dataset. 
% Drawing inspiration from the use of GitHub repository stars as an effective metric for identifying high-quality code datasets ~\cite{Jiang2024ASO}, and considering the unique characteristics of log statements, 
% Drawing on evidence that GitHub repository stars effectively signal code quality ~\cite{Jiang2024ASO}, we propose dataset inclusion criteria tailored to log statement analysis: repositories must have at least 10,000 stars and 500 log-related issues to qualify for inclusion in our dataset. 
% Besides those requirements, we consider the amount of log statements are bigger, the developpers more care about the quality of log statements. So we rank the repos by the log statement number, in the same time, we consider the diverse log requirements are also important. So in the rank list, we choose 10 diversity projects.
Building on empirical evidence that GitHub repository stars correlate with code quality~\cite{Jiang2024ASO}, we determine dataset inclusion criteria for log statement analysis by requiring repositories to have $\geq$ 10,000 stars and $\geq$ 500 log-related issues to ensure community validation and logging relevance. Candidate repositories are then ranked by total log statement count to prioritize those with extensive logging practices.
% From the top of this ranked list, we manually curate a final dataset of 10 repositories, applying two qualitative criteria: (i) diverse logging needs: including projects with conflicting priorities, such as performance-critical systems (minimal logs), trace-intensive applications (detailed logs), and privacy-sensitive services (anonymized logs). and (ii) active maintenance: measured by frequent commits (>50/month), high issue resolution rate (>70\%), and recent updates (within 6 months). This dual-phase approach balances the quantitative scale with domain representativeness.
From the top of this ranked list, we manually curate a final dataset of 10 repositories to ensure that the set covers different priorities in the logging effectiveness-efficiency trade-off, including performance-critical systems (requiring minimal logs) and trace-intensive applications (requiring detailed logs). After that, we ensure that all of these repositories are actively maintained, with frequent commits (>50/month), a high issue resolution rate (>70\%), and recent updates (within the past 6 months). This dual-phase approach balances the quantitative scale with domain representativeness.


As shown in Table~\ref{table:dataset}, our final dataset includes projects with a total of 21,804 code snippets and 39,600 log statements, covering a wide range of logging needs and practices.
The dataset spans multiple domains, including database management, task scheduling, distributed storage, messaging systems, and IoT platforms.
% Each domain presents \textit{unique requirements} for log statements.
%% 这些仓库都因为其特定场景的需求，导致其有完全不同的logging的目标（即具体用log来做哪种下游任务），同时也有不同的effectiveness-efficiency trade-off，就如同Sec.3.3所示。
These repositories have \textit{\textbf{distinct logging objectives (targeted downstream tasks)}} based on the specific needs of their respective scenarios and exhibit \textit{\textbf{unique effectiveness-efficiency trade-offs (more logging vs less logging)}}, as shown in Sec.~\ref{sec:phy}.
% For example, database management systems such as DBeaver and Doris prioritize minimizing the impact of logging on high performance. Task scheduling systems, including DolphinScheduler, rely on logging to trace task dependencies and monitor runtime statistics. Similarly, distributed systems like Hadoop and Zookeeper require robust logging practices to address challenges in distributed coordination, fault tolerance, and scalability, which differ from the requirements of other command logs. Messaging systems such as Kafka and Pulsar have adapted logging practices to trace message flows, ensure reliable message delivery, and debug asynchronous communication. Meanwhile, IoT platforms like ThingsBoard utilize logging to manage device connectivity, monitor data streams, and enable real-time system oversight. Identity and access management systems such as Keycloak prioritize protecting sensitive information in logs to prevent privacy breaches.
% \textbf{\textit{Logging requirements diverge sharply across domains}}: 
For example, database systems like DBeaver minimize logging overhead to sustain performance, while task schedulers such as DolphinScheduler emphasize dependency-tracing logs for runtime monitoring. Distributed systems (e.g., Hadoop) enforce rigorous logging for fault tolerance and distributed coordination, contrasting with messaging platforms like Kafka, which track message flows to ensure delivery reliability. IoT solutions (e.g., ThingsBoard) leverage logs for real-time device connectivity oversight, whereas security-focused systems like Keycloak prioritize log anonymization to safeguard sensitive data.
% 这些不同的日志需求充分展现了日志代码的多样性和 xxx， 充分证明了我们无法设定一个通用的标准来评估日志代码的质量。 选择这些充满多样性，并且受到社区认可，经受住时间考验的项目来作为评估日志质量的 testbed，无疑 xxx


These diverse logging requirements not only demonstrate the heterogeneity and inherent contradictions in logging practices (e.g., performance-sensitive scenarios demand minimal logging overhead, while debugging scenarios necessitate comprehensive event records) but also substantiate the impossibility of establishing universal standards for assessing log code quality.
By documenting real-world logging practices from community-vetted, widely-used projects across domains, we establish a foundation for evaluating logging strategies in their native environments. This domain-rooted dataset supports comprehensive analysis of automated logging techniques across diverse operational scenarios.
    
% In addition to emphasizing data quality, we also addressed the potential risk of data contamination. Since all our data were extracted from public GitHub repositories, which may have been used for training pre-trained models, we implemented precautions to minimize this risk. Specifically, we collected the latest version of each project to reduce the likelihood of it being included in any model’s pre-training data. Furthermore, we wrapped the code snippets in a common class and standardized the formatting using Google-Java-Format~\cite{Google-java-format}. This approach altered the format of the code to prevent pre-trained models from recognizing the same information and structure. These strategies have been demonstrated as effective in recent studies~\cite{Xu2024UniLogAL, Tian2024DebugBenchED}.
% Although we adopted effective strategies, data contamination cannot be entirely avoided~\cite{Cao2024ConcernedWD}. However, our methods minimize this risk and have been proven effective in previous work, providing a solid foundation for analysis and evaluation. In the future, we plan to regularly update our dataset to ensure that the evaluation data remains current. After completing the necessary appeal actions, we finalized our static evaluation dataset.

 \input{insert/table_dataset_projects}


\subsection{Static Evaluation}
\label{sec:statci_evaluation}
% \subsubsection{Static Evaluation Method.}
% The static evaluation focuses on log statement components—position, verbosity, and message, with details provided in Section~\ref{sec: Motivation: Evaluation Method}.
\subsubsection{Task Formulation}
We structure each entry in our dataset as a tuple \(<Code_{w/o~ LogStmt}, LogPos, LogStmt>\). As illustrated in Figure~\ref{fig:static_evaluation_task_formulation},  \(Code_{w/o~ LogStmt}\) represents the input code context with one log statement deliberately removed, \(LogPos\) indicates the ground truth position for the missing log statement, and \(LogStmt\) represents the exact log statement to be predicted.
 This process yielded 39,600 high-quality instances, providing a robust foundation for evaluating log generation models.

 \subsubsection{Metrics}
 \label{sec:method:metrics}
% We adopted five metrics for static evaluation. In addition to the previously established metrics, 
% To advance log-quality evaluation beyond conventional metrics, we introduce \textbf{Dynamic Expression Accuracy (DEA)} and \textbf{Static Text Similarity (STS)}. DEA assesses whether generated logs preserve the structural integrity of composite runtime expressions (\eg \(retries\_left * timeout\)), ensuring they encapsulate the same operational logic as ground-truth rather than merely replicating isolated variables. 


% STS evaluates the semantic alignment of static template messages (e.g., "Connection refused by host") using complementary metrics: BLEU and ROUGE. These metrics collectively enhance the fidelity of log information assessment under code-only evaluation constraints by jointly validating runtime semantic consistency and static descriptive intent. 

We introduce six static evaluation metrics to assess the similarity between the generated and reference log statements.

\textit{\textbf{Metric 1: Position Accuracy (PA):}} PA focuses on the precise line of log statements within the source code. The correct placement of log statements helps accurately trace the execution flow and diagnose issues. For Position Accuracy, we rigorously compare the predicted positions of log statements with their actual positions in the source code. This metric is calculated by taking the number of correctly positioned log statements \(P_c\) and dividing it by the total number of log statements \(N_a\) to obtain the accuracy value: \(PA = \frac{P_c}{N_a}\).

\textit{\textbf{Metric 2: Level Accuracy (LA):}}  
LA evaluates the exact match between predicted and reference log levels, which are essential for prioritizing operational events in DevOps pipelines. Common log levels carry distinct semantic implications, like "\textit{info}," referring to the normal information of runtime behavior.  "\textit{Warning}" indicates potential problems that might not immediately cause disruption but could lead to future issues if not resolved. "\textit{Error}" refers to runtime anomalies or issues that need to be addressed.
LA is calculated as the ratio of correctly predicted levels $L_c$ to the total log statements \(LA = \frac{L_c}{N_a}\).


\textbf{\textit{Metric 3: Average Level Distance (ALD):}} ALD further quantifies the \textit{severity deviation} of mispredicted levels from the reference level. We assign ordinal values to log levels: 
\textit{trace: 0, debug: 1, info: 2, warn: 3, error:4, fatal: 5}, and compute the absolute difference between predicted level $L_p^{(i)}$ and oracle level $L_a^{(i)}$ for each log. ALD is the mean deviation across all instances: $ ALD = \frac{1}{N_a} \sum_{i=1}^{N_a} \left| L_p^{(i)} - L_a^{(i)} \right| $.


\textit{\textbf{Metric 4: Message Accuracy (MA):}}
MA evaluates how accurately the predicted log messages match the oracle, which is essential for providing meaningful and relevant information during runtime. The content of log messages helps developers understand the system’s behavior, and inaccuracies in message generation can lead to confusion or missed insights during debugging. For Message Accuracy, we compare the predicted log messages to the actual messages in the source code. This metric is calculated by determining the number of log messages that are fully identical to the ground truth \(M_c\) and dividing it by the total number of log messages \(N_a\), yielding the accuracy value: \(MA = \frac{M_c}{N_a}\).

\textit{\textbf{Metric 5: Dynamic Expression Accuracy (DEA)}:} DEA evaluates whether generated logs preserve the structural integrity of runtime expressions, including both individual variables and composite logic (\eg ternary operators, arithmetic). For example, in the log template:\textit{("The server run on the ports, \{\}", args.status ? localPort : remotePort)}, the conditional expression \textit{(args.status ? localPort : remotePort)} is treated as a single semantic unit.
We aim to use this metric to ensure that the dynamic information recorded in logs remains consistent. This metric is calculated by taking the number of the exactly matched \(DP_c\) and dividing it by the total number of log statements \(N_a\) to obtain the accuracy value: \(DEA = \frac{DP_C}{N_a}\).

 \textit{\textbf{Metric 6: Static Text Similarity (STS) (with BLEU or ROUGE):}} 
 STS focuses on the static part of the log message. Unlike the dynamic variable, the static part always records the same information in log files, which will not vary due to the runtime behavior of software. For example, in the log message \textit{("The server is running, \{\}", status)}, "The server is running, \{\}" is regarded as the static part.
 Since this part primarily consists of natural language content, we use BLEU~\cite{Papineni2002BleuAM} and ROUGE~\cite{Lin2004ROUGEAP} metrics to evaluate the quality of the static text.
 In our implementation, we use the DM variant of BLEU~\cite{Chen2014ASC, Shi2021OnTE}, \ie the sentence-level BLEU without any smoothing method — coupled with ROUGE-L to holistically assess both lexical precision and long-sequence coherence. Specifically, ROUGE-L focuses on the longest common subsequence that effectively captures key operational patterns (e.g., error codes or API call chains) in multi-line logs, while BLEU's n-gram overlap measurement complements it by evaluating template fidelity at the token level.


 \input{insert/static_evaluation_task_formulation}
%  The calculation methods for BLEU-DM are detailed below as shown in Eq.~\ref{eq:BLEU}.
%     \begin{equation}
%     \label{eq:BLEU}
% \text{BLEU} = \frac{1}{N} \sum_{i \in N} (\text{BP} \cdot \exp(\sum_{n=1}^{4} w_n \log p_n))
% \end{equation}
% where \(w_n\) denotes the weight of each n-gram and \(p_n\) represents the precision of each n-gram, which can be calculated as Eq.~\ref{eq:pn}
%     \begin{equation}
%     \label{eq:pn}
% \text{\(p_n\)} = \frac{\#\text{n-grams in the reference}}{\#\text{n-grams in the candidate}} 
% \end{equation}
% \(BP\) is the penalty value parameter for short candidate texts, and its calculation method is shown in Eq.~\ref{eq:bp}
%     \begin{equation}
%     \label{eq:bp}
% BP = \begin{cases} 
% 1 & \text{if } c \geq r \\ 
% e^{(1-r/c)} & \text{if } c \leq r 
% \end{cases}
% \end{equation}
%  where \(r\) is the length of reference and \(c\) is the length of the candidate text.

\subsection{Dynamic Evaluation}
\input{insert/dynamic_evaluation_workflow}
\label{sec:method:dynamic_evaluation}
\subsubsection{Task Formulation}
In dynamic evaluation, we provide the runtime perspective of evaluating the automatic logging tools. 
Two key evaluation metrics are employed: (1) compilability—ensuring the code with predicted log statements compiles without errors, and (2) log file similarity—measuring the alignment between log files generated by the predicted log statements and the oracle through textual similarity analysis. These criteria jointly validate both the functional correctness of log integration and the relevance of logged content.
% The compilability of the code and the similarity of log files generated by the log statements with oracle are the two primary criteria.
% 这种方法在可控并且不需要消耗大量计算资源的情况下，模拟了软件运行时的状态，提供给我们宝贵的运行时日志。

As illustrated in Figure~\ref{fig:dynamic_evaluation_workflow}, the core innovation of our methodology lies in utilizing unit tests to emulate runtime log generation under controlled conditions. By executing these tests, we synthetically replicate software runtime states with minimal computational overhead, thereby enabling systematic collection of runtime logs that mirror real-world execution patterns for downstream analysis.
% As illustrated in Figure~\ref{fig:dynamic_evaluation_workflow}, the novelty of our approach is to leverage unit tests to simulate runtime log generation in a controlled environment. This approach validates both the compilability of code with inserted logging statements and generates runtime logs, enabling direct comparison against expected log outputs.
% We define evaluation quadruple as \(<Code_{input}, Code_{output}, Log_{output}>\), where \(Code_{input}\) is the code snippets without log statements covered by unit test, \(Code_{output}\) is the code snippets with predicted log statement, and \(Log_{output}\) is the logs produced by those log statements in \(Code_{output}\) via unit tests. If the \(Code_{output}\) is not compiliable, \(Log_{output}\) is null.
We define task formulation as a tuple \(<Code_{w/o\ \ LogStmt}, Code_{w/\ \ LogStmt}, Logs>\), where \(Code_{w/o\ \ LogStmt}\) is the code snippets without log statements covered by unit test, \(Code_{w/\ \ LogStmt}\) is the code snippets with predicted log statement, and \(Logs\) is the logs produced by those log statements in \(Code_{w/\ \ LogStmt}\) via unit tests. If the \(Code_{w/\ \ LogStmt}\) is not compiliable, \(Logs\) is null.

To build the dynamic evaluation pair based on our dataset, we begin by compiling the project to ensure all dependencies are resolved and the project is ready for execution. Next, we systematically identify all available unit tests within the project. For each unit test, we execute it individually while employing the Jacoco Plugin~\cite{Jacoco} to trace code coverage, specifically identifying whether the unit test covers any log statements in the codebase. Simultaneously, we use the SureFire Plugin~\cite{surefire} to capture the logs generated during the execution of the unit tests.
By correlating Jacoco coverage data with SureFire logs, we can match specific code snippets containing log statements to the corresponding unit tests that cover them, along with the runtime logs they generate.
We finally built 2,238 instances for dynamic evaluation.

% To construct a dynamic evaluation dataset, it is essential to select high-quality projects with comprehensive unit tests, as these tests provide a realistic simulation of diverse production environments. Each instance of dynamic evaluation requires recompiling the project and executing the test to collect logs, making the process highly time-consuming. To balance this intensive time requirement with the need for sufficient dataset diversity and quantity, we employed Hadoop as our dynamic evaluation platform. 




% \subsubsection{Dynamic Evaluation Method}
% Different from static evaluation, dynamic evaluation focuses on compiling the code and runtime-generated logs, addressing static evaluation’s inability to verify code compilability and runtime logs. 
% To directly assess runtime logs, we generate them using unit tests, which are widely used in software development to verify code functionality in isolated scenarios. Unit tests are readily available in most projects and they are designed to test the functionality and behaviors of code when facing different situations. They offer a natural method for simulating realistic situations, allowing generating logs without the need for complex runtime environments. 
 % Figure~\ref{fig:dynamic_evaluation_workflow} demonstrates the general workflow of dynamic evaluation. The process begins with compiling the source code and executing the unit tests to obtain logs from the original log statements. Using tools like Jacoco~\cite{Jacoco} and SureFire~\cite{surefire}, we collect the logs and remove the log statements covered by unit tests in the source code.
% Next, we input the modified source code into an automatic logging tool to generate predicted log statements. Then we inserted the predicted log statements back into the source code, replacing the original log statements. After recompiling the modified code, we rerun the unit tests to capture the logs produced by the inserted predicted log statements. The whole process provides us with two sets of logs—those generated by the original log statements and those generated by the predicted log statements.
% Finally, we evaluate the effectiveness of the predicted log statements using two key metrics: Compilation Success Rate and Log File Similarity. Compilation Success Rate ensures that the predicted log statements do not introduce compilation errors, while Log File Similarity measures the similarity between the logs generated by the predicted log statements and those generated by the original log statements.
% In the following sections, we will detail how we built the dataset for dynamic evaluation and introduce the specific metrics used to measure the performance of the automatic logging tools.


% \subsubsection{Dataset Construction}
% To build the dynamic evaluation dataset, we begin by compiling the entire project to ensure all dependencies are resolved and the project is ready for execution. Next, we systematically identify all available unit tests within the project. For each unit test, we execute it individually while employing the Jacoco Plugin~\cite{Jacoco} to trace code coverage, specifically identifying whether the unit test interacts with or covers any log statements in the codebase. Simultaneously, we use the SureFire Plugin~\cite{surefire} to capture the logs generated during the execution of the unit tests.

% By correlating Jacoco coverage data with SureFire logs, we can match specific code snippets containing log statements to the corresponding unit tests that cover them, along with the runtime logs they generate. This process enables us to construct a comprehensive dataset consisting of triples: the code snippet, the unit test that triggers it, and the recorded logs. These triples are critical for dynamic evaluation, as they provide the ground truth for assessing the quality and effectiveness of predicted log statements in actual runtime scenarios.

% To construct a dynamic evaluation dataset, it is essential to select high-quality projects with comprehensive unit tests, as these tests provide a realistic simulation of diverse production environments. Each instance of dynamic evaluation requires recompiling the project and executing the test to collect logs, making the process highly time-consuming. To balance this intensive time requirement with the need for sufficient dataset diversity and quantity, we employed Hadoop as our dynamic evaluation platform. This approach allowed us to build a dataset of 2,238 instances that balances diversity with sufficient size. Additionally, we open-source the entire suite of tools used in this evaluation process, enabling researchers and organizations to easily deploy and customize their own dynamic evaluation datasets.

\subsubsection{Metrics}
\label{sect:method_dynamic_metrics}
% In dynamic evaluation, we proposed four metrics to assess the performance of logging tools: \textbf{\textit{Compilation Success Rate}}, \textbf{\textit{Log Similarity}}, \textbf{\textit{False Positive Log Generation Rate}}, and \textbf{\textit{False Negative Log Generation Rate}}, which we will introduce below.
We propose four dynamic evaluation metrics to assess the similarity of logs printed by the generated and reference log statements in runtime execution.

\textit{\textbf{Metric 7: Compilation Success Rate (CSR):}} CSR measures the compilability of the predicted log statements. Due to issues such as undefined variables in the predictions or missing/outdated dependencies in the project environment, not all code snippets with predicted log statements can successfully compile.  We recorded the successfully compiled code snippet number as \(C_s\) and the evaluation pair number as \(C_a\). The metric is then calculated as: \(CSR = \frac{C_s}{C_a}\).
 
 \textit{\textbf{Metric 8: Log File Similarity (LFS) (with COS, BLEU, ROUGE):}}
 LFS evaluates how closely logs generated by predicted statements match those produced by ground truth statements.
 % To eliminate unnecessary differences, we remove log headers (e.g., timestamps), retaining only log content.
 We remove log headers (e.g., timestamps) and only assess the main message body of each log entry to eliminate unnecessary differences.
 For a comprehensive assessment, we apply multiple similarity measures, including Cosine Similarity (COS)~\cite{Salton1975AVS}, BLEU~\cite{Papineni2002BleuAM}, and ROUGE~\cite{Lin2004ROUGEAP}. Cosine Similarity, commonly used in text analysis, calculates the cosine of the angle between two TF-IDF~\cite{SprckJones2021ASI} vectors, yielding 1 for identical vectors and 0 for orthogonal ones. Using TF-IDF, we down-weight frequent terms, emphasizing distinctive content in logs. This method effectively captures the similarity between meaningful log content, filtering out redundant information for a more accurate relevance measure.
% The formula for Cosine Similarity is shown in Eq.~\ref{eq:cosine_similarity}.
% \begin{equation}
%     \label{eq:cosine_similarity}
%     \text{Cosine Similarity} = \frac{A \cdot B}{||A|| \, ||B||} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \, \sqrt{\sum_{i=1}^{n} B_i^2}}
% \end{equation}
ROUGE, on the other hand, focuses on recall by comparing n-grams between the predicted and reference logs. It evaluates how much of the reference content is preserved in the prediction. The most commonly used variant is ROUGE-N, which calculates the overlap of n-grams between two texts. 

% The formula for ROUGE-N is shown in Eq.~\ref{eq:rouge}
% \begin{equation}
%     \label{eq:rouge}
%     \text{ROUGE-N} = \frac{\sum_{\text{match} \in \text{n-gram}} \text{Count}(\text{match})}{\sum_{\text{n-gram} \in \text{reference}} \text{Count}(\text{n-gram})}
% \end{equation}

 \textit{\textbf{Metric 9: False Positive Log Generation Rate (FPLR):}}
FPLR measures the proportion of predicted log statements that generate logs during unit test execution when the ground truth log statements would not have produced any logs. It helps assess whether the predicted log statements introduce unnecessary or redundant logs in scenarios where no log should be generated. The number of false positive instances is recorded as \(FP\), and the total number of predictions is \(P\). the metric is calculated as: \(FRLR=\frac{FP}{P}\).

 \textit{\textbf{Metric 10: False Negative Log Generation Rate (FNLR):}}
FNLR evaluates the proportion of predicted log statements that fail to generate logs during unit test execution when the ground truth log statements should have produced logs. It highlights instances where the predicted logs miss important events or information. The number of false negative instances is recorded as \(FN\), and the total number of predictions is \(P\). The metric is calculated as: \(FNLR = \frac{FN}{P}\).

% In summary, the evaluation of log statements in this paper leverages a comprehensive set of metrics designed to assess various aspects of both static and dynamic evaluations. These include syntactic correctness through Compilation Success Rate (CSR), practical effectiveness via runtime analysis such as False Positive Log Generation Rate (FPLR) and False Negative Log Generation Rate (FNLR), and content similarity through Cosine Similarity, BLEU, and ROUGE. Together, these metrics provide a thorough assessment of log statement quality, capturing both surface-level correctness and deeper runtime effectiveness. This multi-faceted evaluation framework ensures that automatic logging tools are tested not only for their technical accuracy but also for their real-world utility in generating meaningful and actionable logs.