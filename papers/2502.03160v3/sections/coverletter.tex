\section{cover letter}

\noindent Dear Editors,

We are pleased to submit our manuscript, “AL-Bench: A Benchmark for Automatic Logging”,
to \textbf{ACM Transactions on Software Engineering and Methodology}. 

Logging plays a vital role in enhancing software reliability by enabling runtime monitoring. While recent advances in large language models demonstrate promising capabilities for automating log statement generation, existing evaluation methodologies suffer from two fundamental flaws: the reliance on ad-hoc datasets compromises the fairness of cross-study comparisons, and code similarity metrics fail to account for runtime logging behavior—particularly how code modifications induce compilation failures or semantic distortions in log outputs. These limitations undermine the validity of current performance claims and hinder systematic progress in automated logging research.

To address these challenges, we propose AL-Bench, a comprehensive benchmarking framework that establishes rigorous evaluation standards. 
AL-Bench includes a large-scale, high-quality, diverse dataset collected from 10
widely recognized projects with varying logging requirements. Moreover, it introduces a novel dynamic evaluation methodology
to provide a run-time perspective of logging quality in addition to the traditional static evaluation at source code level. 
AL-Bench reveals significant limitations in existing static evaluation, as all logging tools show average accuracy drops of 37.49\%, 23.43\%, and 15.80\% in predicting log position, level,
and message compared to their reported results. Furthermore, with dynamic evaluation, AL-Bench reveals that 20.1\%-83.6\% of these
generated log statements are unable to compile. Moreover, the best-performing tool achieves only 21.32\% cosine similarity between
the log files of the oracle and generated log statements. To summarize, this work makes the following novel contributions to quantify the journal first:

\begin{itemize}
  \item We collected a high-quality, diverse, and large-scale dataset comprising 21,804 instances from 10 popular, high-quality GitHub projects, spanning various domains with differing logging requirements.

  \item We propose a dynamic evaluation approach that reintegrates the generated log statements into projects to evaluate their printed runtime logs as a more fine-grained proxy for illustrating logging performance.

  \item We conducted a comprehensive evaluation of popular automatic logging tools and revealed the key limitations based on the analysis of the evaluation results.

  \item All the data and code for AL-Bench are publicly available, providing valuable resources for both developers and researchers to advance the field of automatic logging.
\end{itemize}

We believe this work establishes a foundational step in furthering this research direction. We appreciate your time and look forward to your insightful comments.

\noindent Yours sincerely,

\noindent Boyin Tan, Junjielong Xu, Zhouruixing Zhu, and Pinjia He