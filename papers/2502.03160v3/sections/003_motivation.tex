% \section{background and motivation}
\section{Background}
\label{sec:moti}
    % This section introduces the overview of the current research on automatic logging tools, followed by the shortcomings in the evaluation work, and explains the motivation for \methodname.

% \subsection{Log Statement Generation}
\subsection{Related Work of Automatic Logging}
Logging, the process of generating informative log messages with appropriate verbosity levels at strategically placed locations within code, has long been recognized as a critical challenge in software engineering~\cite{he2021survey, Chen2019ExtractingAS, Chen2021ASO, Chen2021ExperienceRD}. Over the years, substantial research efforts have aimed to support developers in crafting more effective logging statements, which in turn enhance software maintenance and testing~\cite{Jia2018SMARTLOGPE, Zhao2017Log20FA, Zhu2015LearningTL}. Early studies in this domain often addressed isolated subproblems, typically operating under stringent assumptions that limit the applicability of their findings in real-world scenarios. For example, Li \etal~\cite{Li2021DeepLVSL} proposed \textit{DeepLV} to predict the appropriate logging level by taking surrounding code features into a neural network. Liu \etal~\cite{Liu2022TeLLLL} proposed \textit{Tell} to further adapted flow graphs to help the suggestions of verbosity levels. Zhu \etal~\cite{Zhu2015LearningTL} proposed \textit{LogAdvisor} and Yao \etal~\cite{Yao2019Log4PerfSA} proposed \textit{Log4Perf} to assist developers to add new log statements in a specific position. Ding \etal proposed \textit{LoGenText}~\cite{Ding2022LoGenTextAG} and \textit{LoGenText-Plus}~\cite{Ding2023LoGenTextPlusIN} to advise developers what should be logged, and Liu \etal \cite{Liu2021WhichVS} proposed tools for deciding which variables should be logged. However, none of them can generate a complete log statement. 

%%% 从intro中搬运替换过来的内容。
Recently, with the advanced capabilities of large language models (LLMs)~\cite{Floridi2020GPT3IN, raffel2020exploring}, numerous LLM-based logging methods have been proposed. 
Specifically, \textit{LANCE}~\cite{Mastropaolo2022UsingDL} was introduced as the first end-to-end logging method, seamlessly integrating the recommendation of log statement position, verbosity level, and message content.
Building on this foundation, \textit{UniLog} \cite{Xu2024UniLogAL} employed a warm-up and in-context learning (ICL) strategy to enhance performance. 
\textit{FastLog} \cite{xie2024fastlog} improved the generation efficiency while maintaining precision. 
\textit{LEONID} \cite{Mastropaolo2023LogSG}, based on \textit{LANCE}, combined with deep learning and information retrieval technologies to enhance performance. \textit{SCLogger}~\cite{Li2024GoSC} adapted static analysis to extend the context for the code snippet.

% Recently, Mastropaolo \etal \cite{Mastropaolo2022UsingDL} proposed the first end-to-end tool \textit{LANCE} to generate complete log statements based on T5~\cite{Raffel2019ExploringTL}. Following \textit{LANCE}, Xu \etal~\cite{Xu2024UniLogAL} proposed \textit{UniLog} to adapt ICL and warm-up strategy to enhance the LLM ability for generating log statements. Xie \etal~\cite{xie2024fastlog} proposed \textit{FastLog} increase the generation time while keeping the accuracy, and Mastropaolo \etal~\cite{Mastropaolo2023LogSG} further proposed \textit{LEONID} with a combination of Deep Learning (DL) and Information Retrieval (IR) achieving a better performance. 
    
% While end-to-end automatic logging tools have demonstrated promising results in their respective evaluations, our analysis reveals notable issues in both the datasets used and the evaluation methods employed.


\subsection{Philosophy of Logging Evaluation}\label{sec:phy}
%%% 这个section需要讨论一下我们对logging的评估的认知和信念。不要重复提到之前方法的问题了。intro已经详细讨论过了。
% \subsection{Limitations of Evaluation Methodology}
%%% TODO: 介绍logging评估的哲学。从downstream task说开去。

%%% 如何评价logging质量，一直是automatic logging领域一个关键但是又难以解决的问题。在理想情况下，高质量的logging可以意味着其产生的日志能够充分记录软件系统的关键运行状态信息，同时又不至于过度logging以导致占用过量计算资源和存储资源的开销。因此从直觉上来说，最直接的logging evaluation方法是依靠日志在这些下游任务上的表现来评估logging质量。然而，当我们想要量化这种logging质量的时候，就会出现许多的挑战。首先，日志的下游任务非常多。除了常见的log-based debugging、anomaly detection、root cause analysis等等任务以外，也可以单纯用于记录软件运行时产生的事件序列以监控系统运行状态，或者其他开发者认为可以用日志完成的任务等等。由于我们没法保证我们能够覆盖日志的所有可能的应用场景，因此我们构建由特定的下游任务作为代理，也只能评估在这些task-specific的logging表现，而无法声称这种方法可以评估general-purpose logging能力。此外，即使像anomaly detection等传统log analysis任务理论上可以通过故障注入的方式来评估logging有效性，但是对单纯的log-based monitoring而言，我们没法自动化地量化评估log是否提供了开发者所需的信息记录。另外，实际上要对多样化的软件构建传统log analysis任务如anomaly detection的故障数据集也是non-trival的，因为对于不同的软件系统而言，存在的故障类型是各不一样的，因此没法scalable的实现注入。最后，考虑到打印log本身也是会占据计算和存储资源开销的。为了避免产生大量日志并加重后续log compression或log reduction等任务的负担，在实际logging practice中，开发者也会对logging的effectiveness和efficiency做一个trade-off。因此，哪怕我们能够非常全面地评估logging在任何任务上的有效性并构建一套benchmark，我们也不能说明这样的logging是好的——很可能为了解决一些平时不常见的问题，日常打印log的开销会成倍增加，而这是不划算的。至于到底对于什么样的trade-off是合适的，一般不同的软件系统上开发者都有一套ad-hoc的认知，而无法通过指标量化的形式确定“什么样的logging是合适的”。综上所述，要通过采用下游任务作为proxy来体现general-purpose logging quality的好坏是无论从理论还是实践上来说都challenging的。

Logging quality evaluation has always been a critical yet challenging problem in automatic logging~\cite{Bogatinovski2022QuLogDA, Li2019DLFinderCA, Chen2017CharacterizingAD, Chen2021ASO, he2021survey}. In an ideal scenario, high-quality logs should accurately capture key runtime state information of a software system while avoiding excessive logging that may lead to unnecessary consumption of computational and storage resources. Intuitively, the most direct approach to assessing logging quality is to evaluate how well the logs support specific \textbf{downstream tasks}~\cite{YuanUsenixA1}. However, using downstream tasks as proxies to measure general-purpose logging quality in a scalable way is both theoretically and practically problematic for several reasons:

\begin{enumerate}
    \item First, it is very challenging to cover all potential downstream tasks for logs exhaustively. There are countless log-based downstream tasks: beyond the tasks with clear task formulations (e.g., debugging, anomaly detection, root cause analysis), logs are also used for a wide range of tasks that lack a universal formulation (e.g., recording sequences of runtime events to monitor system status), often driven by the unique needs of each system’s developer~\cite{Zhang2019RobustLA, xu2024divlog, Amar2019MiningHT}. Even if we only choose a fixed set of tasks with clear formulations to evaluate logging quality, these proxies can only assess \textit{task-specific aspects} rather than provide a comprehensive measure of \textit{overall} logging quality. This limitation makes them unreliable indicators of \textit{general-purpose} logging quality.
    \item  Second, it is not practical to build testbeds for these downstream tasks across all software systems. While it might be theoretically possible to create a testbed for evaluating logging quality in common tasks like anomaly detection, assessing tasks like whether logs capture key information for event monitoring needs is challenging, as it depends heavily on subjective developer requirements. Moreover, even creating fault datasets to support log-based anomaly detection is also challenging since different software systems have unique fault characteristics, making scalable fault injection across diverse environments infeasible~\cite{zhao_identifying_2021}.
    % \item Another key consideration is the trade-off between logging effectiveness and efficiency. Since logging incurs computational and storage costs, excessive logging can introduce significant overhead and complicate related processes such as log compression and reduction. Consequently, practitioners must carefully balance these trade-offs to optimize real-world logging practices based on their ad-hoc empirical experience on certain software application scenarios. For instance, overemphasizing solutions for some rare or edge-case scenarios could substantially increase routine logging costs without proportional benefits. Thus, such a process is still challenging for us to design an appropriate trade-off mechanism to evaluate whether a logging strategy is appropriate for each specific software.
    \item Third, the trade-off between logging effectiveness and efficiency is another unmeasurable aspect of logging quality. Massive logging offers detailed runtime data for fault diagnosis but increases storage costs and resource usage, while sparse logging reduces expenses but risks missing critical information. System designers balance these tradeoffs primarily through operational experience from maintaining and debugging specific systems. However, such experiences are inherently context-dependent, varying across systems due to divergent functional, architectural, and operational priorities~\cite{rong2023developers}. Consequently, it remains challenging to establish a universal trade-off mechanism for assessing the appropriateness of logging strategies for each specific software.
\end{enumerate}

% However, it is both theoretically and practically problematic to use downstream tasks as proxies to measure general-purpose logging quality for several reasons.


% 1. 下游任务太多，选其中几个无法为 general purpose logging benchmark
% 2. 其中一些下游任务构建 testbed 过于麻烦
% 3. 我们要评估 trade off , 但每一个软件系统的 trade off strategy 是不一样的，甚至在同一软件的不同部分的 trade off strategy 也是不一样的。 所有我们没办法去设定一定 standard 的 标准来评估这个 trade off，必须 case by case。

% 我们要评估 trade off , 但每一个软件系统的 trade off strategy 是不一样的，甚至在同一软件的不同部分的 trade off strategy 也是不一样的。 所有我们没办法去设定一定 standard 的 标准来评估这个 trade off，必须 case by case。 我理解到的是这个意思。 但感觉有点没说清楚。 首先我们这三点都是为了说明为什么用下游任务作为 evaluation proxy 是不可行的， 第一点说 下游任务多，不好选 第二点说有些下游任务难，不好构建。 这两点感觉都很清晰，并且 make sense。 但第三点感觉读第一遍有点没有强调好重点。 
% 我感觉逻辑应该是 effectiveness and efficiency trade off 是现代软件设计中很重要的一环。过多的 log 会提供更详尽的运行时状态，便于开发者们定位问题，但随之而来的是高额的存储成本和处理资源消耗； 过少的 log 虽然能节约成本，但是或许会漏掉许多关键信息。 对于每个系统的设计开发者来说，如何平衡 effectiveness and efficiency 主要依赖的是他们长期以来对运维和修复系统漏洞积累的专属于这个系统的宝贵经验。 对我们来说设计一套通用的 trade off strategy 来 suit 每一个系统是不可能的。


%%% 因此，作为一种妥协和替代方案，当前logging research通常采用了基于相似度比较大logging评估方式。其核心思路是，既然我们难以直接根据下游任务评估日志语句的质量，我们可以把那些本来具有较好质量的日志语句作为参考，通过构建benchmark来评估logging method能否生成和这些高质量的参考相似的日志语句。这种方式使得logging evaluation可以以scalable的方式完成，因为大量开源软件库中已经有相当大数量log statement可以作为reference了。然而，这种similarity-based evaluation strategy也有其固有的弱点，即，无法正确评价那些比reference log statement质量更好的log statement。那些quality比reference更好的predicted log statement很可能因为和reference不够相似从而获得更低的similarity score。要尽量缓解这一局限带来的影响，就必须确保作为reference的log statement质量足够高。这也就对数据的收集提出了挑战。要确保reference的质量，我们认为那些最热门的project中长期保持不变、没有被修改过的log statement是高质量的、经过时间检验的，因此也是最适合作为reference的。此外，当前的similarity-based evaluation仅仅停留在静态代码层面的评估。然而，静态代码上非常小的差异可能导致其产生的日志文件内容和规模上都有十分大的改变。因此，我们认为在评估静态代码相似度的前提下，也要额外评估生成的日志文件的相似度，这样才能提供一个全面的视野来评估质量。这两个insight drives us to propose our first logging benchmark: \methodname.

As an alternative, logging research often adopts a \textbf{similarity-based} approach to evaluate logging quality.
The core idea is that, since assessing log quality via downstream tasks is challenging, we can use high-quality log statements as references to evaluate new ones.
Thus, by constructing benchmarks, we can check if a logging method generates log statements similar to these reference ones, making evaluation scalable with log statements from open-source software.

However, this similarity-based approach does have limitations.
It may not accurately assess log statements of \textit{higher quality} than the reference, as they could receive lower similarity scores simply for not matching closely enough. To mitigate this, reference log statements must be of sufficiently high quality, which poses a challenge in data collection. We argue that log statements from the most \textit{widely-used}, \textit{time-tested} projects are the best reference candidates. Additionally, current evaluations focus solely on static code, but small changes in code can lead to significant differences in log content. Therefore, evaluating both static code and the generated log files provides a more comprehensive measure of logging quality. These insights inform the development of our first logging benchmark: \methodname.