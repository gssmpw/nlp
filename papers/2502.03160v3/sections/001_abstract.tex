\begin{abstract}
Logging, the practice of inserting log statements into source code, is critical for improving software reliability. 
Recently, language model-based techniques have been developed to automate log statement generation based on input code. 
% These tools show promising results in their own evaluation. 
% However, current evaluation practices in log statement generation face significant challenges. The lack of a unified, large-scale dataset forces studies to rely on ad-hoc data, hindering consistency and reproducibility.
While these tools show promising results in prior studies, the fairness of their {results} comparisons is not guaranteed due to the use of ad hoc datasets.
% {Additionally, existing assessments solely based on metrics like code similarity fail to reflect real-world effectiveness.}
In addition, {existing evaluation approaches exclusively dependent on code similarity metrics fail to capture the impact of code diff on runtime logging behavior, as minor code modifications can induce program uncompilable and substantial discrepancies in log output semantics.}
% These limitations underscore the need for a comprehensive public benchmark to standardize evaluation.
To enhance the consistency and reproducibility of logging evaluation, we introduce \methodname, a comprehensive benchmark designed specifically for automatic logging tools. \methodname includes a large-scale, high-quality, diverse dataset collected from 10 widely recognized projects with varying logging requirements. Moreover, it introduces a novel dynamic evaluation methodology to provide a run-time perspective of logging quality in addition to the traditional static evaluation at source code level.
Specifically, \methodname not only evaluates the similarity between the oracle and predicted log statements in source code, but also evaluates the difference between the log files printed by both log statements {during runtime}.
% Different from the existing evaluations that focus only on components of log statements like code similarity, \methodname assesses both the compilability of the code with inserted log statements and the effectiveness of the logs generated by them during runtime, which we believe can better reflect the effectiveness of logging techniques in practice.
%%% 在静态评估上有啥点数的区别吗。是不是也得提一句？
%%% “所有现有方法在AL-Bench上的PA,LA,MA这三个传统静态评估指标上平均下降了X\%, Y\%, Z\%.
\methodname reveals significant limitations in existing static evaluation, as all logging tools show average accuracy drops of 37.49\%, 23.43\%, and 15.80\% in predicting log position, level, and message compared to their reported results.
Furthermore, with dynamic evaluation, \methodname reveals that 20.1\%-83.6\% of these generated log statements are unable to compile. 
% In addition, even the best-performing tool only achieves 0.213 cosine similarity between the runtime logs produced by the generated log statements and the ground-truth log statements.
Moreover, the best-performing tool achieves only 21.32\% cosine similarity between the log files of the oracle and generated log statements.
% The results reveal substantial opportunities to further enhance the development of automatic logging tools.
These results underscore substantial opportunities to advance the development of automatic logging tools.
We believe this work establishes a foundational step in furthering this research direction.

\end{abstract}

% attains a 57.54\% accuracy in logging position, 62.72\% in level accuracy,
% 17.66\% in dynamic variable accuracy
% a BLEU score of 20.20 for text accuracy

% are limited by the use of low-quality data and incomplete assessments.
% More specifically,  (1) low-quality evaluation data compromise effectiveness in assessing the performance, (2)  the evaluation does not verify if the generated code is compilable, and (3) there is no assessment of runtime logs generated by predicted log statements. 