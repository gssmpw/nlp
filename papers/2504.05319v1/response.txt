\section{Background and Related work}
\subsection{Command prediction based on BIM logs}

BIM logs are a chronological record of events automatically generated during the use of BIM authoring software. The techniques and methods used to analyze these logs can be collectively referred to as BIM log mining **Lee, "Mining BIM Log Data for Design Process Understanding"**. This includes but is not limited to examining designers' social networks to understand collaboration patterns **Pan et al., "Collaboration Patterns in BIM-Based Architectural Design"**, identifying the productivity of modelers **Kim, "Productivity Analysis of BIM Authors"**, analyzing the creativity or quality of designs **Zhou, "Design Creativity Evaluation Using BIM Logs"**, and enhancing the reproducibility of the modeling process **Guo et al., "Reproducibility Enhancement in BIM-Based Design Process"**, etc. One important use case is leveraging the sequential user operations recorded in the logs to predict the next command.

Pan et al. extracted command sequences from Revit log files and grouped them into 14 classes that attempted to summarize the generic intent of the different commands at a high level **Pan et al., "Command Sequence Analysis for BIM-Based Design"**. Eventually, a long-short-term memory network (LSTM) **Hochreiter & Schmidhuber, "Long Short-Term Memory"** was employed to predict the class labels of upcoming commands.  **Zhou et al., "LSTM-Based Command Prediction in BIM Logs"** used a similar concept to predict command classes based on Revit log files. They built models at different scales using different numbers of LSTM layers and compared the prediction results. Guo et al. developed a custom log in Rhino to combine modeling commands with their resulting 3D models, proposing the command-object graph to represent the modeling design behavior **Guo et al., "Command-Object Graph for BIM-Based Design"**. Their subsequent research **Zhou et al., "Path Extraction and Sequence Composition from Command-Object Graphs"** extracted paths from the graphs to compose extensive command sequences, and used the native Transformer model **Vaswani et al., "Attention Is All You Need"** to achieve the instance-level command prediction. Although the custom log can extract command sequences more effectively by integrating specific information of model elements compared to native logs, its limitations lie in limited public access and the necessity for manual updates **Kim et al., "Custom Log Design for BIM-Based Design Process"**. This results in a constrained dataset that may not accurately reflect real-world software usage. Furthermore, the basic Transformer model used in the study does not fully leverage the additional meta-information in the log files.

The CommunityCommands **Kang et al., "CommunityCommands: A Collaborative Filtering-based Command Recommender for AutoCAD"** for AutoCAD provides personalized command recommendations using an item-based collaborative filtering algorithm. It evaluates the importance of commands through a command frequency-inverse user frequency (cf-iuf) rating **Wu et al., "Command Frequency-Inverse User Frequency Rating for Recommendation"**, combining personal usage frequency with community-wide rarity. Recommendations are based on the cosine similarity between unused commands and those already used by the active user. Despite its innovative statistical approach, an evaluation of data from 4,033 users revealed limited predictive accuracy, with a 21\% of hit rate among the top 10 recommendations.

\subsection{Transformers for sequential recommendation}

Recent advancements in large language models (LLMs) have highlighted their remarkable ability to comprehend language sequences. The underlying Transformer architecture, originally developed for Natural Language Processing (NLP) tasks, has gained widespread adoption across various fields due to its scalability and proficiency in modeling complex patterns in sequential data. The sequential nature of user interactions aligns closely with language modeling tasks, leading to the adoption of NLP-inspired architectures in recommendation systems **Kang et al., "Transformers for Sequential Recommendation"**. The success of the Transformer architecture lies in its attention mechanism, which effectively captures dependencies between representations regardless of their position in the sequence **Vaswani et al., "Attention Is All You Need"**. This capability makes it particularly well-suited for modeling the dynamic and evolving behavior of users -- akin to the dynamic design process where designers' actions and interests vary across workflows and projects **Kim et al., "Dynamic Design Process Modeling using Transformers"**.

Transformer-based recommendation systems often enhance user interaction sequences by incorporating meta-information as additional features. In the e-commerce scenario, such features may include user comments, product descriptions, prices, or images **Wu et al., "Meta-Information Enhanced Recommendation with Transformers"**. This differs from the Transformers in NLP, where additional meta-information is absent. Language models typically consist of three components: (1) a tokenizer that converts raw text into a sequence of index-encoded tokens, (2) a Transformer architecture that learns latent representations of the sequence, and (3) a task-specific prediction head designed for applications such as sentiment analysis or next-token prediction **Kang et al., "Language Models for Sentiment Analysis and Next-Turn Prediction"**.

In our study, we focus exclusively on the (2) Transformer architecture to model the sequential dependencies among commands. We replace the tokenizer and NLP-specific prediction heads with a feature fusion module capable of integrating additional information and a prediction head tailored for the command recommendation task. Our implementation builds on Transformer4Rec **Kang et al., "Transformer4Rec: An Open-Source Framework for Transformers in Sequential Recommendation"**, a flexible open-source framework that bridges NLP and sequential recommendation systems. By leveraging the Transformer backbone from language models like XLNet **Yang et al., "XLNet: Generalized Autoregressive Pretraining for Language Understanding"**, this framework has demonstrated state-of-the-art performance in generating recommendations for news and e-commerce domains. However, our research specifically explores Transformers for BIM command recommendation, which poses unique challenges such as the lack of rich meta-information, varying session lengths, and the long-tail distribution of commands.


\subsection{Summary and research gaps}
\label{gaps}

In summary, we identify the existing research gaps in the literature as follows:
\begin{itemize}

    \item From the algorithm perspective,
    existing research primarily employs statistical methods or basic sequence prediction models, overlooking advanced deep sequential recommendation systems that have shown success in other domains. Moreover, current studies focus on predicting single-step command instances or classes. However, design tasks typically involve multi-command workflows, making it more practical to enhance prediction granularity and recommend optimal workflows to users.
    \item From the system integration perspective, current studies have not proposed an end-to-end pipeline that seamlessly integrates the prediction model into BIM authoring software for real-time command recommendation.
    \item From the data perspective, existing studies often rely on small-scale datasets generated through bespoke logging mechanisms, neglecting the engineering challenges of processing real-world log data at the billions-scale in its raw format. This limitation also constrains the scalability of existing data enhancement methods based on customized loggers.
    
\end{itemize}