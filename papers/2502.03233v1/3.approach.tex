\section{Problem Formulation}
\subsection{Threat model}
\label{subsec:threat_model}
We characterize the threat model of this study with respect to the attacker's goals, capabilities, and attack scenarios.
\subsubsection{Attacker's goals}
Assume that a set of user queries (\ie questions for RACG) is denoted as $\mathcal{Q}$. For each query $q \in \mathcal{Q}$, the attacker selects a set of $m$ vulnerable examples (with or without access to $q$) aimed at compromising the security of generated code. The vulnerable examples are denoted as $\mathcal{V} = \{v_1, v_2, \dots, v_m\}$. For instance, the query $q$ from the user could be ``implement OAuth authentication'', and the vulnerable examples might include vulnerable OAuth implementations with subtle security flaws. When developers use these queries, the retriever is likely to fetch such vulnerable examples due to their high relevance, raising the risk of generating vulnerable code.

\subsubsection{Attacker's Capabilities and Attack Scenarios}
\label{subsec:scenarios}
In real-world RACG application scenarios, attackers cannot determine which retrievers the systems will employ.
Despite that, attackers can still poison the knowledge base, which is typically collected from public sources like GitHub~\cite{carlini2024poisoning}.
Therefore, to mimic the real-world scenarios, in this study, we identify two distinct attack scenarios based on whether the attacker can anticipate developers' programming intents (\ie the queries they use during the RACG process). The two scenarios are as follows:
\begin{itemize}[leftmargin=*]
    \item {\bf Scenario I: Exposed Programming Intent}. 
    In this scenario, the attacker can observe or predict the developers' programming intentions prior to their interaction with the RACG system. Such exposure typically occurs through various organizational artifacts and development processes. For example, attackers can gain insights into future queries by monitoring public project requirements, following code review discussions, or intercepting development team communications. With advanced knowledge of potential queries, {\bf attackers can strategically inject vulnerable code examples into the knowledge base that are semantically related to the developers' needs}. 
    When developers use these queries, the retriever is likely to fetch such vulnerable examples due to their high relevance, raising the risk of generating vulnerable code.

    \item {\bf Scenario II: Hidden Programming Intent}.
    In this scenario, attackers cannot know specific developer queries in advance, forcing them to poison the knowledge base blindly. 
    In this study, we assume that instead of targeting specific queries, {\bf attackers focus on injecting vulnerable code examples across common programming patterns and functionalities}. The behind intuition of this strategy is that common functionalities are prone to be more frequently retrieved as examples, and thus would cast more significant risks to the RACG system. 
    \end{itemize}
    
In these scenarios, we assume an attacker could construct a set of vulnerabilities $\mathcal{V}$ containing $m$ vulnerabilities to be injected into the knowledge base $\mathcal{K}$. This assumption is realistic and widely adopted by existing poisoning studies~\cite{biggio2012poisoning,liu2018trojaning,biggio2018wild,zou2024poisonedrag}. For example, attackers can maliciously edit Wikipedia pages to inject their desired content, as demonstrated by a recent study~\cite{carlini2024poisoning}. 
Regarding the construction of $\mathcal{V}$, in Scenario I, where the target query $q$ is accessible, the set $\mathcal{V}$ can be directly constructed based on $q$. However, in Scenario II, where users' queries are invisible, we construct $\mathcal{V}$ by selecting representative vulnerable examples through clustering. The detailed construction process of $\mathcal{V}$ is illustrated in \S\ref{subsec:kn_construct}.

For each scenario, we evaluate four LLMs with two typical retrievers, resulting in {\em 2 × 4 × 2  = 16} sub-scenarios, covering a wide range of realistic conditions. 
The detailed construction process for both scenarios is described in~\S\ref{subsec:kn_construct}.

\subsection{Knowledge Poisoning Attack to RACG}
\label{subsec:task_formulation}
Under our threat model, we formulate the knowledge poisoning attack on RACG as an optimization problem from the attacker's perspective.
Specifically, the goal is to construct a set of vulnerable examples $\mathcal{V}$, 
% query $q$,
so that the LLM in the RACG system is likely to generate more vulnerable code when utilizing the $r$ examples retrieved from the poisoned knowledge base $\mathcal{K} \cup \mathcal{V}$ as the context.
Note that in Scenario I, each $\mathcal{V}$ is constructed based on the query $q$, meaning that for each query, the set of vulnerable examples ($\mathcal{V}$) is different. In contrast, in Scenario II, the set $\mathcal{Q}$ is constructed by inserting common programming patterns and functionalities. Therefore, in this scenario, all queries $q$ share the same set of vulnerable examples $\mathcal{Q}$, provided that they fall under the same poisoning configuration (e.g., poisoning proportion). Formally, we have the following definition:
\begin{equation*}
\begin{aligned}
    \max _{\mathcal{V}} \frac{1}{|\mathcal{Q}|} \sum_{q\in\mathcal{Q}} \mathbb{I}\left(LLM\left(q ; \operatorname{RETRIEVE}\left(q, \mathcal{K} \cup \mathcal{V}\right)\right)\right)
\end{aligned}
\end{equation*}
where the $\mathbb{I}(\cdot)$ is the function that evaluates the security of LLM-generated code. If the generated code is vulnerable, the output of $\mathbb{I}(\cdot)$ is 1, otherwise, it outputs 0. $|\mathcal{Q}|$ represents the number of elements in the set $\mathcal{Q}$. The $\operatorname{RETRIEVE}(\cdot)$ function retrieves a set of $r$ texts from the poisoned knowledge base $\mathcal{K} \cup \mathcal{V}$ for target query $q$. The objective function increases as the LLM generates more vulnerabilities. The LLM-generated code security evaluation details are described in \S\ref{subsec:validation}.


\section{Experiment Settings} 
\label{sec:exp_setting}
This section describes our experimental setup for answering the research questions. 

\subsection{Dataset Construction}
\label{subsec:dataset_cons}
\input{tables/dataset_selection}
Existing code generation datasets, such as CodeSearchNet~\cite{husain2019codesearchnet}, mainly consist of code collected from GitHub, without explicitly including any vulnerable code. In this study, we investigate the security of code generated by RACG techniques with the poisoned knowledge base. For example, in Scenario I, where the programmer's intent (i.e., the query) is exposed, the attacker would inject semantically matching vulnerable code into the knowledge base. This implies that for each query, two corresponding versions of the code exist: one secure and one vulnerable. The secure version acts as an oracle for validating the functionality of the generated code, while the vulnerable version serves as the source for poisoning. 
Therefore, our study requires a dataset that includes both secure and vulnerable code for each query.


The dataset was selected based on four criteria: (1) coverage of multiple common programming languages, (2) vulnerabilities from realistic development scenarios (not synthetic, \eg Juliet~\cite{boland2012juliet}), (3) inclusion of both vulnerable and secure code versions, and (4) cleaning to avoid biases from unprocessed sources like tangled commits and outdated patches.
We identified the 10 most widely used vulnerability datasets from 2011 to 2023, as reported in a recent systematic survey~\cite{shiri2024systematic}, and supplemented this list with datasets published after the survey (i.e., post-July 2023), resulting in 12 preliminary candidates. Table~\ref{tab:dataset_criteria} provides an overview of these datasets, indicating the extent to which each criterion is satisfied. We selected ReposVul as our base dataset because it satisfies all requirements.
\input{tables/dataset}
After filtering functions with implementations shorter than three lines and names containing ``test'', the statistics of the dataset are shown in Table~\ref{tab:dataset}. Specifically, the dataset spans four widely used programming languages: C, C++, Java, and Python. Among these, C contributes the highest number of vulnerable functions (6,956) and CWEs (139), reflecting its extensive use and susceptibility to a diverse range of vulnerabilities. Java and Python exhibit comparable CWE diversity, with 115 and 110 types, respectively. Overall, the dataset encompasses 12,052 instances and 236 distinct CWEs, providing a comprehensive basis for analyzing the security of code generated by RACG. Each instance in the dataset is represented as a tuple $(q, v, s)$, where $q$ is the query for generating the desired code, $v$ is the vulnerable version code, and $s$ is the secure version corresponding to $v$. However, not all instances contain corresponding queries since some functions lack code comments that explicitly describe their functionalities, which typically serve as queries in code generation~\cite{husain2019codesearchnet}. 
To address this issue, we generated the missing queries by feeding the secure version of each function into the LLM (\ie DeepSeek-V2.5, as detailed in \S\ref{subsec:imple_details}). We used the secure version to avoid incorporating vulnerable descriptions in the queries.
Prompt~\ref{prompt:1} (in Appendix) illustrates the prompt we used to generate queries, adapted from~\cite{geng2024large}, to describe the functionality of each given function.


\subsection{Knowledge Base Construction and Poisoning}
\label{subsec:kn_construct}
As formulated in \S\ref{subsec:task_formulation}, for code generation, the LLM processes query $q$ along with
% $m$ 
texts retrieved from the poisoned knowledge base $\mathcal{K} \cup \mathcal{V}$. This section explains how we construct and poison the knowledge base $\mathcal{K}$.

\subsubsection{Scenario I}
\label{subsubsec:scenario_1}
In Scenario I, we assume that the programming intents (i.e., queries) are exposed to the attacker. This exposure allows attackers to inject vulnerabilities into $\mathcal{K}$ based on the semantics of $q$. In this scenario, $\mathcal{K}$ represents the collection of all secure codes from the dataset. The vulnerable examples $\mathcal{V}$ to be injected into $\mathcal{K}$ are identified by the poisoning retriever, which selects the $m$ examples most similar to $q$ from the vulnerability knowledge base (i.e., the collection of all vulnerable code from the dataset). Note that the poisoning retriever is only accessible to the attackers to determine the semantic similarity between the query and the vulnerable code. Consequently, the poisoned knowledge is defined as $\mathcal{K} \cup \mathcal{V}$, which combines the original knowledge base with the vulnerable examples.

\subsubsection{Scenario II}
\label{subsubsec:s2_construct}
In this scenario, the attacker does not know the user's query directly. Without this information, the attacker cannot leverage the query to retrieve vulnerable code samples from the knowledge base for direct injection, as in Scenario I.
Instead, we assume that attackers aim to poison the knowledge base with representative functionalities, which have a higher likelihood of being retrieved in RACG and affect a broader range of queries.
For this purpose, we propose a clustering-based approach to select the vulnerable examples $\mathcal{V}$ for poisoning. The process consists of the following steps:

\textbf{Step 1: Clustering of Knowledge Base Elements.}  
Let $\mathcal{K} = \{k_1, k_2, \dots, k_n\}$ represent the set of code elements in the knowledge base. Each code element $k_i$ is represented as a feature vector $\mathbf{f}_{k_i}$. The clustering process groups semantically and functionally similar code elements. We define the clustering process as a function $C$, which takes the set of feature vectors as input:
\begin{equation*}
    C(\mathcal{F}) = \{\mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_{t}\},
\end{equation*}
where $\mathcal{F} = \{\mathbf{f}_{k_1}, \mathbf{f}_{k_2}, \dots, \mathbf{f}_{k_n}\}$ is the set of feature vectors, and $\{\mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_t\}$ represents the clusters of code elements. Each cluster contains a subset of elements whose feature vectors are similar according to a defined distance metric. In this paper, we focus on the security of LLM-generated code rather than clustering algorithms. Therefore, we implement the widely-adopted K-means algorithm~\cite{macqueen1967some} for code clustering, with the number of clusters $t$ determined by the elbow method~\cite{bholowalia2014ebk} following previous studies~\cite{liu2020determine,cui2020introduction,syakur2018integration}.


\textbf{Step 2: Selecting Representative Elements.}  
For cluster $\mathcal{C}_i$, we select a subset of representative code elements. The selection criterion is based on a poisoning proportion $p$ (discussed in \S\ref{subsec:pos_level}), which determines the fraction of elements chosen from each cluster. Let $n_i$ denote the number of elements in cluster $C_i$, and $n'_i$ represent the number of selected elements. The number of elements to be selected from $\mathcal{C}_i$ is given by:
\[
n'_i = \lfloor p \cdot n_i \rfloor.
\]
The set of selected elements from $\mathcal{C}_i$, denoted as $\mathcal{C}'_i$ ($ \mathcal{C}'_i = \{c^{i}_{1}, c^{i}_{2}, \dots, c^{i}_{n'_i}\}$), are chosen based on their centrality in the cluster, typically determined by their proximity to the cluster's centroid in the feature space.

\textbf{Step 3: Vulnerability Injection.}  
Let $\mathcal{K}'$ represent the set of vulnerable code examples in the vulnerability knowledge base. Each vulnerable code $k' \in \mathcal{K}'$ is characterized by a feature vector $\mathbf{f}_{k'}$. For each selected representative code element $c^{i}_j \in \mathcal{C}'_i$, where $j=1,2,\dots,n'_{i}$, characterized by $\mathbf{f}_{c^{i}_{j}}$, we retrieve the most similar vulnerable code example $v^i_{j}$ from $\mathcal{K}'$ as follows:
\[
v^i_{j} = \operatorname{arg} \underset{k' \in \mathcal{K}'}{\operatorname{max}} \operatorname{similarity}(\mathbf{f}_{k'}, \mathbf{f}_{c^{i}_{j}}),
\]
where $\operatorname{similarity}(\cdot)$ measures the cosine similarity between feature vectors. 
The final set of vulnerable code examples injected into the knowledge base is constructed as:
\[
\mathcal{V} = \bigcup_{i=1}^t \bigcup_{j=1}^{n'_i} v^i_{j},
\]
where the $t$ is the number of clusters. Thus, the poisoned knowledge base $\mathcal{K} \cup \mathcal{V}$ is updated to include both the original knowledge base $\mathcal{K}$ and the vulnerable code examples $\mathcal{V}$.


\subsection{Result Validation}
\label{subsec:validation}
After obtaining the generated code from different settings, we evaluate its security through utilizing LLMs as security judges for three reasons: (1) The generated code varies greatly in semantics and format, making manual analysis time-consuming and error-prone. (2) Validation through static analysis tools is difficult since the LLMs generate standalone functions that lack context and are hard to compile. (3) Recent research has demonstrated that ``LLM-as-a-Judge'' systems achieve performance comparable to human judgment across a wide range of tasks~\cite{zheng2023judging,huang2024empirical,chang2024survey,wang2024reposvul,chen2024rmcbench}.

To this end, we perform a two-step security evaluation process: vulnerability knowledge extraction and vulnerability detection. The first step involves extracting patterns for introducing vulnerabilities, along with corresponding fixing patterns. The second step uses this knowledge to assess whether generated code contains vulnerabilities. This extraction-detection pipeline is also adopted by a recent vulnerability detection study~\cite{du2024vul}, which demonstrates its effectiveness. Our inspection (in \S\ref{subsec:judge_effectivenss}) confirms the effectiveness of the LLM judge, with accuracy rates ranging from 77\% to 81\% across different programming languages by manual inspection. A detailed implementation of the judge is provided in Appendix~\ref{sec_append:llm_juedge}.
