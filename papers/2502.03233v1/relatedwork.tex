\section{Background and Related Work}
\label{sec:bg}
\subsection{Large Language Models}
Recent advancements in natural language processing have greatly enhanced LLM performance and adoption. These developments enable the creation of LLMs with billions of parameters, trained on extensive datasets. Designed for versatility, LLMs excel at integrating cross-disciplinary knowledge, attracting significant research interest and achieving remarkable performance in specialized applications.

General LLMs are trained on diverse textual data from sources like Wikipedia and GitHub. GPT~\cite{brown2020language} and Llama~\cite{touvron2023llama} are widely used, excelling in mathematics, writing, and reasoning~\cite{liu2024empirical,zhao2023survey,chang2024survey}. LLMs can be refined through instruction fine-tuning. For instance, ChatGPT is the fine-tuned version of GPT with reinforcement learning~\cite{brown2020language}. 



Code LLMs are domain-specific LLMs that are optimized for code-related tasks, including code generation and comment generation. These models are typically fine-tuned using external code-related datasets. For instance, CodeLlama~\cite{roziere2023code} is a fine-tuned variant of the base model, trained on 500 billion tokens spanning 80 programming languages. This domain-specific knowledge significantly enhances the performance of code LLMs, enabling them to excel in related tasks. 

\subsection{Retrieval Augmented Generation}
Retrieval-augmented generation (RAG) enhances the performance of LLMs by integrating relevant knowledge retrieved from external knowledge bases, thereby significantly improving the capabilities of LLMs in knowledge-intensive domains. The standard paradigm of RAG consists of three key stages: information retrieval, knowledge augmentation, and final generation~\cite{gao2023retrieval}. This framework is commonly referred to as the ``Retrieve-Read'' model~\cite{ma2023query}. 
Building upon this paradigm, researchers have focused on refining the RAG pipeline from two main perspectives: \textbf{what} to retrieve and \textbf{how} to retrieve it. \textbf{What} pertains to the sources leveraged to enhance generation, including an LLM's internal memory~\cite{cheng2024lift}, internet search engines~\cite{parvez2021retrieval,zhuang2023open}, and knowledge graphs~\cite{matsumoto2024kragen,wen2023mindmap}. \textbf{How} involves two key aspects: how to perform the information retrieval stage and how to leverage the retrieved information. For information retrieval, researchers employ query expansion~\cite{wang2023query2doc}, query rewriting~\cite{ma2023query,tan2024small}, and query routing~\cite{li2023classification} to improve query quality and information richness. For leveraging retrieved information, efforts have centered on selecting critical and essential contents through reranking~\cite{glass2022re2g}, summarization~\cite{gao2023retrieval}, and fusion techniques~\cite{rackauckas2024rag}, aiming to minimize the noise interference in the generation stage.

\subsection{Retrieval Augmented Code Generation}
Inspired by RAG, the field of code generation has witnessed the emergence of RACG as a promising paradigm in recent years. RACG enhances code generation efficiency and quality by retrieving relevant external documents or code snippets~\cite{gao2024preference,wang2023rap,parvez2021retrieval}. Beyond directly applying RAG to code generation, many studies optimize RACG for domain-specific requirements. 
For example, KNN-TRANX~\cite{zhang2023syntax} introduces a syntax-aware model to improve syntactic correctness. 


Existing research in RACG has made notable progress. However, it has overlooked the security implications of RAG-generated contents, particularly when the knowledge base has been poisoned, potentially leading LLMs to produce vulnerable code. To address this gap, our work conducts an extensive empirical study on the security risks associated with RACG systems. This is the first study to evaluate the security of code generated by LLMs in RACG poisoning scenarios. Our research highlights that while LLMs show great promise for code generation, deploying their output in production environments requires thorough security assessment and validation.

\subsection{Existing Attacks on LLMs}

Various LLM attacks have been proposed, including prompt injection~\cite{liu2024formalizing,pedro2023prompt,perez2022ignore,liu2023prompt}, jailbreaking~\cite{li2023multi,qi2024visual,deng2024masterkey,zou2023universal,chen2024rmcbench}, and data poisoning~\cite{shafahi2018poison,biggio2012poisoning,carlini2024poisoning}. Prompt injection attacks aim to craft malicious inputs that manipulate the model into producing unintended outputs. In contrast, jailbreaking attacks focus on bypassing safety mechanisms, enabling the model to generate harmful, unethical, or otherwise prohibited content. Data poisoning attacks involve injecting malicious data during the training phase to corrupt its learning process.

Additionally, recent studies have begun to explore knowledge base poisoning in RAG systems. For instance, PoisonedRAG~\cite{zou2024poisonedrag} proposes an approach to poison the knowledge base, ultimately affecting the functionality of the LLM, such as causing it to output incorrect facts. Unlike this previous study, which focuses on the functionality of LLMs in general tasks, our study targets the security of LLMs in code generation.
The distinctions between this study and previous ones are detailed in \S\ref{subsec:diff_rag_poisoning}, emphasizing how our study is specifically tailored to code generation scenarios.