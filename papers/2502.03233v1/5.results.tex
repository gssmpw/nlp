\section{Results}
\label{sec:exp_results}
We conducted experiments for the following research questions (RQs):
\begin{itemize}[leftmargin=*]
    \item {\bf RQ1:} Do vulnerable code examples compromise the security of code generated by LLMs?
    \item {\bf RQ2:} To what extent do different factors in RACG affect the security of the generated code?
\end{itemize}

\subsection{Vulnerable Code Impact on LLM Security}
This research question investigates whether vulnerable code examples compromise LLM-generated code security in typical RACG configurations by analyzing two key aspects: the different poisoning quantities in the knowledge base and the different number of shots in RACG.

\subsubsection{Security Under Different Poisoning Quantities}
\label{subsec:pos_level}
Intuitively, the number of vulnerable code snippets injected into the knowledge base critically impacts the security of generated code. Increasing the amount of vulnerable code injected raises the probability of retrieving vulnerable code, potentially boosting the VR. However, this also increases the poisoning rate, which may make the attack more detectable or less practical. By systematically exploring different numbers of poisoned code snippets, we aim to not only assess the impact of poisoning on the security of the generated code and its practical feasibility, but also to provide valuable insights for designing more robust defenses against such attacks. 

\input{tables/RQ1_S1_pois_num}
\textbf{Scenario I.} In Scenario I, we analyze the impact of knowledge base poisoning on the security and performance of LLMs in a one-shot setting by varying the number of poisoned examples (i.e., the $m$ value defined in \S\ref{subsubsec:scenario_1}) from zero to nine, with poisoning rates ranging from 0.008\% (1/12,054) to 0.075\% (9/12,062), as shown in Table~\ref{tab:pos_num}. The VRRC metric, which measures the average ratio of vulnerable code retrieved, depends solely on the number of poisoned examples and is independent of the model.

The results demonstrate that poisoning compromises LLM security under both retrievers. With the JINA retriever, we observe a consistent increase in VR as the number of poisoned examples increases, although this effect plateaus at higher numbers. Specifically, increasing the number of poisoned samples from zero to one results in a substantial VR spike across all models. For instance, with the JINA retriever, CodeLlama's VR increases from 0.29 to 0.48, and GPT-4o's VR increases from 0.26 to 0.42. This indicates that even a single poisoned sample can significantly impact model security. Further increases in poisoned samples (from three to nine) yield more gradual VR increases of approximately 0.01--0.03 with the JINA retriever. With the JINA retriever, CodeLlama exhibits the highest susceptibility, increasing from a baseline of 0.29 to 0.53 with nine poisoned examples, while Llama-3 demonstrates the lowest susceptibility, reaching only 0.37.

With the BM25 retriever, the effects of poisoning are less pronounced than with JINA, exhibiting smaller VR increases (e.g., GPT-4o increases from 0.27 to 0.33). We attribute this difference to the distinct retrieval preferences of BM25 and the poisoning retriever (\ie TE3). BM25 tends to retrieve code sharing more terms with the query, whereas TE3 retrieves code based on cosine similarity between query and code embeddings. This distinction is supported by comparing the VRRC between the two retrievers. For instance, the VRRC of JINA and BM25 when poisoning with 5 vulnerabilities are 0.41 and 0.06, respectively. Additionally, we found that all models would generate vulnerabilities even with an unpoisoned knowledge base (poisoning number of 0). This suggests inherent limitations in LLMs' ability to generate secure code, as observed in previous studies~\cite{tihanyi2025secure,khoury2023secure}.

From the LLM perspective, code LLMs are more prone to generate vulnerable code compared to general-purpose LLMs. Specifically, CodeLlama exhibits the highest overall VR, followed by DS-Coder across all configurations (\ie different poisoning numbers and retrievers), while GPT-4o and Llama-3 show noticeably lower VR with the JINA retriever. For instance, CodeLlama achieves VRs of 0.53 and 0.37 on the JINA and BM25 retrievers, respectively, when the poisoning number is 9, whereas Llama-3 demonstrates comparable rates of 0.37 and 0.33. As a code-specialized version of Llama, CodeLlama has significantly higher VR compared to Llama-3, highlighting that code LLMs are more prone to generate vulnerable code than general-purpose LLMs.
We attribute this to the fact that code LLMs are explicitly optimized for code-related tasks and trained on larger, code-focused datasets. Consequently, these models have learned more code patterns, including vulnerable ones, making them more susceptible to the influence of vulnerable code in the input.


Unlike the security of the generated code (measured by VR), which decreases with increasing poisoned examples, the number of poisoned examples has minimal impact on performance, as measured by the similarity metric. For example, the similarity between code generated by DS-Coder with the JINA retriever and the ground truth increases only slightly from 0.76 to 0.78 as poisoned examples rise from zero to nine. This indicates that while knowledge base poisoning compromises the security of code generated by LLMs in the RACG system, its impact on performance is negligible.

\input{tables/RQ1_S1_pois_rate}
\textbf{Scenario II.} In Scenario II, attackers lack access to users' queries and instead select poisoning examples solely by observing the knowledge base, making it significantly more challenging compared to Scenario I. Under these constraints, we vary the poisoning proportion from 0\% to 100\%, where a 100\% poisoning proportion indicates that attackers inject vulnerable code for each instance in the knowledge base as defined in \S\ref{subsubsec:s2_construct}. Table~\ref{tab:pos_rate} shows the results for Scenario II across different poisoning proportions in the one-shot setting.

Overall, the primary trend in Scenario II is consistent with Scenario I: the VR increases with the poisoning proportion for both retrievers, while the similarity metric is affected only slightly. The key distinction between the two scenarios lies in the effectiveness of poisoning as reflected by the VRRC. Specifically, in Scenario I, attackers achieve a VRRC of 0.42 by injecting 7 vulnerability instances into the knowledge base (poisoning proportion of approximately 0.075\%). However, in Scenario II, achieving a similar VRRC requires a poisoning proportion of 100\%. This underscores the significant difficulty of poisoning in Scenario II without access to users' queries. 
Despite the low efficiency of poisoning, attackers can still achieve a VRRC of 0.09 at a poisoning proportion of 0.2. At this rate, approximately 37\% and 35\% of the code generated by CodeLlama is vulnerable when using the JINA and BM25 retrievers, respectively. This suggests that, even though Scenario II is more challenging for attackers due to the lack of access to the user's query, the security of code generated by LLMs in the RACG system remains vulnerable to attacks. 

\notez{
{\bf Finding 1}: Knowledge base poisoning presents a significant security threat to LLM-generated code in RACG systems, especially for code LLMs. For example, even a single poisoned sample using JINA retriever with CodeLlama can render approximately 48\% of the generated code vulnerable.
}

\subsubsection{Security Under Different Numbers of Shots}
Few-shot learning has been shown to improve the performance of LLMs and has been widely adopted in recent studies~\cite{song2023comprehensive,wang2020generalizing}. While we have shown that vulnerable code in the knowledge base compromises LLM-generated code security, the impact of introducing more examples to prompts remains unclear. This analysis examines how the inclusion of one-shot versus few-shot examples in RACG affects the security of the generated code. Due to LLM context window limitations, we compare one-shot and three-shot settings. Besides, based on our investigation in \S\ref{subsec:pos_level}, LLM-generated code exhibits similar patterns across all metrics, regardless of poisoning quantity. For clarity, we present results using moderate poisoning quantities: five poisoned samples in Scenario I and a 60\% poisoning proportion in Scenario II as shown in Table~\ref{tab:rq1_shots}.
\input{tables/RQ1_shots}

Overall, increasing the number of demonstration examples (from one-shot to three-shot) generally raises VR and slightly improves similarity. We attribute this to the increased number of demonstration examples in inputs, which not only results in more vulnerable code examples being retrieved for each query but also increases the proportion of vulnerable code in the retrieved examples. For instance, VRRC rises from 0.41 to 0.44 in Scenario I, indicating 44\% of retrieved code is vulnerable in the three-shot setting compared to 41\% in one-shot. This finding underscores a potential trade-off between providing contextual information to improve performance and ensuring security when the knowledge base contains vulnerable code.

From the perspective of retrievers, we observed that the JINA retriever introduces significantly more vulnerabilities than BM25 in the three-shot. For instance, in Scenario I with the JINA retriever, the aggregated VR (\ie in "All" column) increased by 6.5\% (0.46 $\rightarrow$ 0.49) from one-shot to three-shot. In contrast, the BM25 introduced only a 3.0\% increase (0.33 $\rightarrow$ 0.34) under the same conditions. This discrepancy can be attributed to the higher relevance of examples retrieved by JINA, making LLMs more prone to replicating vulnerabilities, as discussed in \S\ref{subsec:similarity}.
This trend is further evidenced by the results in Scenario II with BM25, where the impact of three-shot settings is minor for LLMs. We conclude that less relevant examples lead LLMs to rely more on their own domain knowledge than the retrieved examples.

From the perspective of LLMs, Llama-3 exhibited notable sensitivity, with VRs increasing significantly in the three-shot setting. Other models, such as GPT-4o and CodeLlama, also showed smaller but consistent increases in VRs when transitioning from one-shot to three-shot. This trend is similarly reflected in the similarity metric, particularly for Llama-3. For example, in Scenario I with the JINA retriever, Llama-3's similarity metric was improved from 0.58 to 0.62, whereas DS-Coder's similarity metric remained unchanged. 


\notez{
{\bf Finding 2}: Providing more examples increases the likelihood of LLMs generating vulnerabilities, although model performance remains stable and shows slight improvement with additional examples.
}



\subsection{Influencing Factors Analysis}
\label{subsec:cause_analysis}
In this RQ, we investigate the factors influencing the security of LLM-generated code through three aspects: programming language, similarity between retrieved code and query, and vulnerability types. Based on RQ1 findings, LLM-generated code shows similar patterns across all metrics regardless of the poisoning quantities and number of shots. To enhance clarity, the analysis in this RQ is conducted under a one-shot setting and moderate poisoning quantities: five poisoned samples in Scenario I and a 0.6 poisoning proportion in Scenario II.



\subsubsection{The Impact of Programming Language}
\input{tables/RQ2_inherent}
This sub-question explores how the characteristics of different programming languages influence the security of LLM-generated code in RACG systems. The analysis is based on metrics derived from four LLMs evaluated under two retrieval approaches (JINA and BM25) across two scenarios each (I and II). Note that we only present the VR metric here, as our focus is on the impact of programming languages on the security of LLM-generated code.

Table~\ref{tab:diff_lang} reveals distinct VR trends among languages. C++ consistently exhibits the highest VR across most sub-scenarios, with values reaching as high as 0.47 in JINA-I and 0.44 in BM25-II on average (\ie ``All'' column). This elevated VR is likely attributed to C++'s inherent complexity, extensive feature set, and lower safety abstractions, which make it more prone to vulnerabilities when retrieved samples contain flaws. Notably, C exhibits a VR nearly comparable to C++ in JINA-based scenarios, with 0.45 in JINA-I and 0.41 in JINA-II on average, indicating higher sensitivity to retrieval strategy when generating C program. JINA's strong retrieval capability amplifies the vulnerability risk for C, as retrieving highly similar examples likely introduces more vulnerability-prone code, highlighting how Câ€™s simplicity and lack of safety abstractions make it more susceptible to vulnerabilities in RACG.

In contrast, Python and Java exhibit distinct trends, showing similar but lower VR compared to C and C++. For instance, the average VRs of Java and Python in JINA-II are 0.38 and 0.39, respectively, while those of C and C++ are 0.41 and 0.46. This stability can be attributed to the built-in safety mechanisms of Java and Python, which reduce vulnerability risks even when the retrieved code contains vulnerabilities.

\notez{
{\bf Finding 3}: LLMs generate more vulnerable code in C++ language but show greater resistance to creating vulnerable code in Java in typical RACG scenarios.
}


\subsubsection{The Impact of Example-Query Similarity}
\label{subsec:similarity}
In this sub-RQ, we investigate whether code examples with higher semantic similarity to the query introduce more vulnerabilities in RACG. To this end, we analyze the distribution of VR and VRCC across different ranges of the similarity between retrieved code examples and queries. For this end, assume that the feature vector of query $q$ is $\mathbf{f}_{q}$, $\mathbf{f}_{c}$ is the feature vector of retrieved code, we measure the similarity between retrieved code and query by calculating the cosine similarity between $\mathbf{f}_{q}$ and $\mathbf{f}_{c}$. This feature vector is generated by the {\tt text-embedding-3-large} model, which is not used by the retriever. This can avoid the bias caused by using the same embedding model as the retriever. 

\input{tables/RQ2_similarity}
Table~\ref{tab:rq_2_similarity} presents the VR and VRRC across different example-query similarity range, with the corresponding VR across various LLMs in two typical RACG scenarios (results from both retrievers are combined). The VRs are shown for each LLM and similarity range, while the VRRC (displayed in the last column) is reported only for similarity ranges, as it pertains to the retriever rather than the specific LLM.

Overall, the similarity between the retrieved code and queries is positively correlated with both VR and VRRC. This indicates that when retrieved code examples are more semantically aligned with the query, they are more likely to be vulnerable, thereby increasing the likelihood of generating code that contains vulnerabilities.
Specifically, in the lower similarity ranges \([0, 60)\), the VRs remain relatively stable, with only modest variations observed. For example, in Scenario I, the aggregated VR (``All'') increases marginally from 0.26 in the \([0, 20)\) range to 0.35 in the \([40, 60)\) range, reflecting a limited impact of similarity on vulnerability likelihood within this interval. Similarly, in Scenario II, the rates increase only slightly from 0.25 to 0.35 across the same ranges. In contrast, a significant increase is observed in the higher similarity ranges \([60, 100]\), where VRs rise sharply as similarity increases. In Scenario I, the aggregated rate escalates from 0.35 in the \([60, 80)\) range to 0.55 in the \([80, 100]\) range. A comparable trend is evident in Scenario II, with rates increasing from 0.35 to 0.47. Among individual models, CodeLlama exhibits the most pronounced increase, with its VR reaching 0.57 in Scenario I for the \([80, 100]\) range. We attribute this to two main reasons. First, when the retrieved code example is highly semantically aligned with the query, the LLM is more likely to generate code that closely resembles the provided example. If the provided code example is vulnerable, the generated code is also highly likely to contain vulnerabilities. Second, the poisoning process is designed to inject vulnerable code that aligns with the query's semantics, meaning that most injected vulnerable code examples exhibit a high degree of similarity to the query. This relationship is supported by the VRRC, which increases as the similarity range increases.

This trend indicates that lower similarity levels have a relatively minor impact on vulnerability likelihood, whereas higher similarity levels, particularly above 60\%, significantly heighten the risk of generating vulnerable code. These findings highlight the critical importance of rigorous validation when using high-similarity code retrieval in RACG, as such code, while more contextually relevant, is also more prone to introducing security vulnerabilities if it is vulnerable.

\notez{
{\bf Finding 4}: The likelihood of generating vulnerable code increases with the example-query similarity. The similarity above 60\% shows a significant increase in vulnerability risk, while lower similarity levels (0-60\%) have a relatively minor impact on VRs.
}


\subsubsection{The Impact of Vulnerability Types}
\label{subsec:vul_type}
\input{tables/RQ2_vul_types}

In this sub-RQ, we investigate whether different types of vulnerability have varying impacts on the security of code generated by LLMs. Specifically, we examine whether the presence of certain types of vulnerabilities in the retrieved examples is more inclined to lead to the generation of exploitable vulnerabilities in the code, thereby increasing the probability of successful attacks.
For this analysis, we focus exclusively on queries that retrieved vulnerable code examples, excluding cases where all retrieved examples were secure.

Table~\ref{tab:rq_2_vul_type} shows the VRs across the Top-10 most dangerous software weaknesses from MITRE~\cite{CWE_Top25_2024}. Full results for Top-25 weaknesses can be found in Table~\ref{tab:rq_2_vul_type_full} in the appendix. As this sub-RQ focuses on the impact of different vulnerability types, the results from both retrievers are combined in the table. Furthermore, since code generated using secure code examples is excluded, the VR values reported are intuitively higher than those in RQ1. 
Among the vulnerability types analyzed, CWE-352 (Cross-Site Request Forgery) exhibits consistently high VRs across all LLMs in both scenarios, with an average of 0.79 in each scenario. This suggests that even without exposed developer intents (Scenario II), LLMs remain prone to generating code susceptible to cross-site request attacks, potentially due to inherent biases or insufficient understanding of secure practices for handling such vulnerabilities.
Conversely, CWE-434 (Unrestricted Upload of File with Dangerous Type) demonstrates the lowest VRs (averaging 0.36 in Scenario I and 0.26 in Scenario II), indicating that LLMs are relatively more successful in avoiding this type of vulnerability. This might be attributed to the simpler nature of validation checks required to prevent such vulnerabilities. Comparing the two scenarios reveals generally higher VRs in Scenario I, particularly for CWE-79 (Cross-site Scripting), CWE-787 (Out-of-bounds Write), and CWE-89 (SQL Injection). This indicates that the presence of vulnerable code snippets in the retrieval set can inadvertently lead to their reproduction in the generated code, highlighting the risk of retrieval augmentation inadvertently amplifying existing vulnerabilities.

\notez{
{\bf Finding 5}: VRs vary across CWE types. While most CWEs exhibit moderate VRs, CWE-352 consistently demonstrates the highest VRs (around 0.8) among the MITRE Top-10 in typical RACG scenarios.
}

