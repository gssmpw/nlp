\section{Introduction}
In recent years, Large Language Models (LLMs) have revolutionized software development through their remarkable ability~\cite{wang2024benchmark,jain2022jigsaw}. As these models become increasingly integrated into development workflows, their capability has been further enhanced through Retrieval-Augmented Code Generation (RACG), a technique that augments LLM responses with relevant information from the external knowledge base to improve the quality of code generation~\cite{gao2024preference,wang2024coderag,parvez2021retrieval}. 
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/Scenario.png}
    \caption{A typical workflow of the RACG system.}
    \vspace{-2pt}
    \label{fig:sceanario}
\end{figure}
However, the adoption of RACG presents a double-edged sword: while it can improve code quality by providing relevant examples, it also introduces potential security vulnerabilities when the knowledge base contains vulnerable examples. This risk is particularly concerning given that public code repositories, which often serve as the sources for knowledge base collection in RACG systems, are usually accessible to anyone in the community. This means that malicious attackers can inject vulnerable code into the knowledge base in a carefully designed way, making the knowledge base susceptible to poisoning, as demonstrated by a recent study~\cite{carlini2024poisoning}.
Conceptually, Figure~\ref{fig:sceanario} illustrates an example workflow of the RACG system. Initially, a query representing the coding task is provided to the system. This query is processed by the retriever component, which searches a pre-indexed knowledge base of code snippets to retrieve the most relevant contextual information. The system then combines the retrieved data with the input query and feeds it into an LLM for code generation.
However, if the knowledge base contains vulnerable code and such code is retrieved, it compromises the security of the generated code, potentially affecting the overall security of the project. For instance, {\tt gets}\footnote{\url{https://www.man7.org/linux/man-pages/man3/gets.3.html}} is an insecure function in the C language that can lead to undefined behavior or exploitation from the attacker. If the retrieved code contains {\tt gets}, there is a possibility that the generated code will also utilize {\tt gets} to read a string from standard input, resulting in insecure code.
Unfortunately, little research has investigated the security threats posed by knowledge base poisoning in the RACG systems.
Previous research has mainly focused on the security of code directly generated by LLMs~\cite{tihanyi2025secure,pearce2022asleep,klemmer2024using}, leaving a critical gap in understanding the security of code generated by RACG systems, particularly when the knowledge base is poisoned by attackers. As RACG rapidly becomes a mainstream paradigm in modern LLM-based systems~\cite{microsoft2024,openai2024,su2024evor}, this knowledge gap becomes even more pressing.

In this paper, we present the {\bf first comprehensive study} on the security risks associated with RACG systems, specifically focusing on the security of code generated by LLMs under varying degrees of exposed programming intent. To that end, we tackle two real-world scenarios: one where the programming intent is exposed, and the other where the intent is hidden, across different settings (\eg different retrievers).
Through extensive experimentation involving four prominent LLMs, two retrievers, and two poisoning scenarios, we explore 16 sub-scenarios, encompassing a wide range of realistic development contexts. This study analyzes vulnerability propagation patterns and identifies key factors that influence the security of generated code, thereby providing critical insights into the risks posed by RACG systems. Our study is guided by the following research questions:
\begin{itemize}[leftmargin=*]
    \item RQ1: Do vulnerable code examples compromise the security of code generated by LLMs?
    \item RQ2: To what extent do different factors in RACG affect the security of the generated code?
\end{itemize}

Based on our study, we present the following main findings. Firstly, knowledge base poisoning in RACG systems poses a significant security threat to LLM-generated code. For instance, when the user's programming intent (i.e., query) is exposed to the attacker, even a single poisoned sample can render approximately 48\% of the generated code vulnerable, as observed in the case of CodeLlama. Even without access to the user's intent, attackers can still poison the knowledge base by inserting vulnerable code examples across common programming patterns and functionalities. Our results demonstrate that injecting vulnerability code equivalent to 20\% of the total knowledge base can lead to approximately 36\% of the generated code being vulnerable when using CodeLlama.
Secondly, we found that in few-shot learning, although more examples improve the LLM's performance in code generation, they also increase the likelihood of generating vulnerabilities. For instance, when the programming intent is exposed, the investigated LLMs generated 6.5\% more vulnerabilities from one-shot to three-shot settings with JINA retriever. 
Finally, we discovered that the security of LLM-generated code is influenced by various factors, including the programming language, example-query similarity, and the type of vulnerability in the retrieved code. Our paper provides a detailed analysis of these factors, which helps to devise security protection mechanism for RACG in the future.
In summary, our study makes the following key contributions:

\begin{itemize}[leftmargin=*]
    \item {\bf First Comprehensive Study}: This paper conducts the first in-depth investigation of RACG system security risks, focusing on how vulnerable code examples in the knowledge base compromise the security of generated code.
    \item {\bf Large-Scale Experimentation}: We conduct a thorough experimental analysis involving four LLMs, two retrievers, and two poisoning scenarios, resulting a total of 16 sub-scenarios. Our findings show that knowledge base poisoning has significant impacts (e.g., 48\% of generated code are vulnerable with a single poisoned sample) and more demonstration examples in the few-shot learning setting leads to increased vulnerability risks (e.g., a 6.5\% rise in vulnerabilities from one-shot to three-shot).
    \item {\bf Practical Insights}: The research identifies key factors affecting generated code security: programming language, example-query similarity, and vulnerability type, offering practical recommendations to enhance the security of RACG systems.
\end{itemize}

