
\subsection{Study Subjects}
\subsubsection{Studied LLMs}
\label{subsec:llms}
We select the studied LLMs based on the following criteria: (1) All models are evaluated via the official Huggingface platform and are demonstrated on the LLM Safety Leaderboard (as of October 2024)~\cite{SecureLearningLab2024}. These models have been assessed on multiple dimensions, demonstrating their ability to refuse harmful content. (2) All selected models are either open-sourced or accessible via public APIs. (3) Open-source LLMs without accessible weight files or those exceeding hardware requirements for local deployment (typically models with over 20 billion parameters) are excluded. (4) To ensure the diversity of models under study, we include both general-purpose LLMs and code LLMs. (5) All selected LLMs have undergone instruction-based fine-tuning, as our experiments require models capable of understanding instructions and correctly leveraging the provided information.

\input{tables/LLMs}
Table~\ref{tab:llms} shows all the LLMs examined in our experiments. We selected four representative LLMs as our research subjects. These models include both open-source and closed-source LLMs, ranging from small parameter scales (\eg 8B) to standard scales (e.g., GPT-4o), and encompass both general-purpose and code-oriented models.
For the closed-source LLM (i.e., GPT-4o), we accessed them through the official OpenAI API~\cite{openai2024apin}. For open-source LLMs, we obtained the model weights from their official Hugging Face repositories. For brevity, we refer to Llama-3-8B, CodeLLAMA-13B, and DeepSeek-Coder-V2-16B as LLAMA-3, CodeLLAMA, and DS-Coder respectively in the following sections.

\subsubsection{Retriever}  
The retriever is the key component of RACG systems, responsible for retrieving relevant code snippets as references to enhance the code generation process. RACG systems primarily use two types of retrievers~\cite{gao2024preference,wang2024coderag}: sparse and dense retrievers.  
Sparse retrievers (\eg TF-IDF~\cite{sparck1972statistical} and BM25~\cite{robertson2009probabilistic}) rely on sparse vector representations to retrieve documents or passages. Dense retrievers, in contrast, use dense vector representations (e.g., learned embeddings from neural networks) to capture semantic relationships between queries and documents~\cite{wang2024coderag,gao2024preference}. While dense retrievers excel at understanding context, they are computationally more expensive.  
With advancements in language models, dense retrievers have become predominant and are widely adopted in recent studies~\cite{parvez2021retrieval,gao2024preference,wang2023rap,wang2024coderag}.  
In this study, we implement BM25 and JINA retrievers as representatives of sparse and dense retrievers, respectively. The number of retrieved instances is determined by the specific settings in the RACG system.
\begin{itemize}[leftmargin=*]  
    \item {\bf BM25}: BM25 is an enhanced version of TF-IDF that typically demonstrates better performance. It ranks code snippets based on the frequency of query tokens appearing in the tokens of the code examples stored in the knowledge base. The top $n$ snippets with the highest scores are selected as examples for code generation.  
    \item {\bf JINA}: For this retriever, we use the state-of-the-art embedding model {\tt jina-embeddings-v3}~\cite{sturua2024jina} to generate feature vectors for both queries and code snippets in the knowledge base. For each query, the top $n$ most similar instances, as measured by cosine similarity, are retrieved as examples for subsequent code generation.  
\end{itemize}  

Additionally, as specified in our threat model (\S\ref{subsec:threat_model}), attackers are assumed to lack access to the retriever's parameters and cannot directly query the retrievers. Consequently, an external retriever is required for two distinct purposes: (1) retrieving vulnerable code from the vulnerability knowledge base in Scenario I, and (2) generating embeddings for code in Scenario II.  
To this end, we employ a {\bf TE3} retriever as the poisoning retriever, which is based on the \texttt{text-embedding-3-large} embedding model~\cite{OpenAI_Embeddings}. This retriever is exclusively used for embedding and retrieving vulnerable code for knowledge base poisoning and is not integrated into the RACG system itself.



\subsection{Metrics} 
To quantitatively evaluate the impact of vulnerable code within the poisoned knowledge base on the security and functionality of generated code, we employ the following metrics:

{\bf Vulnerability Rate (VR)}:
This metric quantifies the likelihood of an LLM generating vulnerable code. It is defined as the percentage of generated code snippets that exhibit security vulnerabilities. Formally, the VR is given by $VR = \frac{N_{v}}{N_{t}}$, where $N_{v}$ denotes the number of vulnerable code snippets generated by LLMs (evaluated by the LLM judge described in~\S\ref{subsec:validation}), and $N_{t}$ represents the total number of code snippets generated by LLMs. The VR provides a clear measure of RACG security risk when vulnerable code exists in the knowledge base. Higher VR values indicate a higher likelihood of generating insecure code, highlighting the need for improved security measures within the RACG system.

{\bf Similarity}: To evaluate how poisoned code affects LLM-generated code functionality in RACG systems, we measure similarity between the generated code and ground truth using CrystalBLEU~\cite{eghbali2022crystalbleu}, a BLEU variant designed for code similarity~\cite{phan2023evaluating,storhaug2023efficient}. CrystalBLEU is an optimized version of the BLEU~\cite{papineni2002bleu} that distinguishes between similar and dissimilar code examples 1.9â€“4.5 times more precisely.

{\bf Vulnerability Rate in Retrieved Code (VRRC)}: To investigte to what extent the retrieved examples are poisoned, we define VRRC as the average proportion of vulnerable code retrieved as examples in the input among all retrieved codes. Formally, it is given by:
\begin{equation*}
    VRRC = \frac{1}{|\mathcal{Q}|} \sum_{q \in \mathcal{Q}} \frac{|\mathcal{V}_q|}{r},
\end{equation*}
\noindent
where {\small $|\mathcal{V}_q|$} denotes the number of retrieved vulnerable codes for query $q$, $r$ is the number of retrieved codes, and $|\mathcal{Q}|$ is the number of queries. A higher VRRC indicates that a greater proportion of the poisoned examples are retrieved and used as context during code generation.


\subsection{Implementation Details}
\label{subsec:imple_details}
All experiments were conducted on a single A100-40G GPU server using the Ollama framework~\cite{ollama_website}. For the LLMs, we configured them with a temperature of 0 to reduce non-determinism~\cite{ouyang2024empirical}, a top-p value of 0.95, a \texttt{max\_new\_tokens} setting of 4096, and a context window of 8192, keeping other parameters at default values. We adhered to each model's recommended prompt format, using predefined chat templates or formats from model cards, GitHub repositories, or original papers. For query generation (\S\ref{subsec:dataset_cons}) and result validation (\S\ref{subsec:validation}), we used DeepSeek-V2.5 as the LLM backend, which excels in code-related tasks~\cite{DeepSeek2024}.
For retrievers, we reused the BM25 implementation from an open-access GitHub repository\footnote{\url{https://github.com/dorianbrown/rank_bm25}}, loaded the JINA retriever from Huggingface\footnote{\url{https://huggingface.co/jinaai/jina-embeddings-v3}}, and used the OpenAI API for the TE3 retriever~\cite{OpenAI_Embeddings}.
