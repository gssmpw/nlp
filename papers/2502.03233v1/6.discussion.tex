\section{Discussion}


\subsection{Implications}
Our investigation highlights the critical need to mitigate knowledge base poisoning. The findings from our study have several implications for enhancing the security of code generated by RACG systems.
Firstly, existing retrieval strategies naturally favor the most relevant examples from the knowledge base, which gives attackers the opportunity to successfully mislead the generation process with a small number of vulnerable examples. We argue that this risk can be mitigated by adjusting the retrieval strategy. For instance, an alternative strategy would be to select the second most similar example or randomly choose from a candidate pool containing several of the most similar examples, and we plan to explore such a way in future work.


Secondly, based on the results from RQ1, we found that hiding the programming intent (i.e., in Scenario II) increases the difficulty of successful poisoning. For instance, attackers can achieve a VRRC of 0.38 in Scenario I with only one poisoned vulnerable example. However, in Scenario II, attackers achieve a VRRC of 0.35 with a poisoning proportion of 0.8, meaning that they need to inject 9,642 vulnerable examples into the knowledge base (calculated as $\lfloor12,053 \times 0.8\rfloor$).
% which corresponds to a poisoning rate of 40\% (9,642/21,695). 
This indicates that concealing the programming intent makes poisoning more intricate and easier to detect.

Thirdly, according to the finding from \S\ref{subsec:vul_type}, the security of LLM-generated code vary across CWE types. This suggests that RACG systems could devise special strategies to check for the existence of several specific CWE types in the knowledge base, such as CWE-352, with the aim of improving the security of the generated code, as these vulnerabilities are more likely to induce the generation of vulnerable code.
% key types of vulnerabilities in the knowledge base, thereby improving the security of the generated code.

\subsection{Effectiveness of Judge}
\label{subsec:judge_effectivenss}
To assess the performance of using an LLM as a judge to label responses, we evaluate the effectiveness through both manual sampling inspection and automated inspection. For {\bf manual} inspection, we determine the sample size based on a 95\% confidence level and a 10 confidence interval, using a population size of 12,053 responses from GPT-4o. The final sample sizes for C, C++, Java, and Python are 95, 81, 93, and 91, respectively, as calculated using an off-the-shelf tool.\footnote{\url{https://www.surveysystem.com/sscalc.htm}} The evaluated code was sampled from GPT-4o in a moderated, one-shot setting using the JINA retriever in Scenario I with five poisoning vulnerabilities.
Two authors independently evaluated the samples through manual review, followed by a double-check to ensure consistency. For {\bf automated} inspection, we using a dataset containing both vulnerable code and its fixed version. Specifically, we evaluate pairs of items (i.e., the vulnerable version and its fixed counterpart) using our LLM-based judge to see if it can distinguish between them. This evaluation is performed on the full dataset.
We classify a code sample as positive when the judge correctly identifies it as vulnerable and the results are presented in Table~\ref{tab:dis_judge_combined}. 
\input{tables/judge_eval}
Overall, the LLM-based judge demonstrates commendable performance, consistently achieving high accuracy, precision, recall, and F1 scores across both inspection approaches. This indicates that the judge is capable of effectively detecting vulnerabilities in generated code, making it a reliable evaluation method. Among the four programming languages evaluated, the judge's performance remains generally consistent.

In the results under manual inspection, Python yields the highest performance, with accuracy and F1 scores reaching 0.84 and 0.81, respectively. The performance is slightly lower for C and Java, with F1 scores of 0.79 and 0.78, respectively, but still remains at a high level. In the results under automated inspection, the judge's F1 scores across all four programming languages hover around 0.8, aligning closely with the results from manual inspection. These evaluations demonstrate that the judge effectively detects vulnerabilities in generated code.


\subsection{Effectiveness of Retrievers}
\input{tables/Dis_retriever}
Our influencing factors analysis of LLM-introduced vulnerabilities (\S\ref{subsec:cause_analysis}) reveals that different retrievers impact the security of generated code. Specifically, LLMs using the JINA retriever are more prone to generating vulnerable code than those using BM25 across various LLMs and scenarios. We attribute this to JINA's superior retrieval of relevant code. To validate this, we evaluate retriever effectiveness using MRR and SuccessRate@k (Table~\ref{tab:dis_retriever}), following prior work~\cite{liu2021opportunities,wang2024fusing}. MRR is the average reciprocal rank of results for a set of queries $q$, and SuccessRate@k is the percentage of queries where the relevant code snippet appears within the top-k results. As shown, JINA significantly outperforms BM25 across all metrics: MRR (0.85 vs. 0.20), SR@1 (0.79 vs. 0.14), SR@5 (0.91 vs. 0.19), and SR@10 (0.93 vs. 0.24). This confirms JINA's superior retrieval capability, which, while beneficial for general code generation, exposes LLMs to more potentially vulnerable code, thus increasing the likelihood of generating vulnerable code.

\subsection{The Difference with RAG Poisoning}
\label{subsec:diff_rag_poisoning}
RAG and RACG systems share the use of external knowledge to enhance content generation. However, they differ significantly in the nature of poisoning attacks and their consequences. Specifically, RAG poisoning targets the functional accuracy of the system~\cite{zou2024poisonedrag,zhang2024hijackrag}. In a RAG setup, attackers inject false or misleading examples into the knowledge base, causing the system to retrieve incorrect information. This disrupts the model’s ability to generate factually accurate outputs, undermining its usefulness in tasks requiring reliable information. The primary goal here is to compromise the system’s ability to produce correct content. 

In contrast, RACG poisoning aims to compromise the security of generated code without impacting functionality. Otherwise, the developer would discard the generated code and there would not be targeted vulnerability in the software.
By introducing vulnerable code examples into the knowledge base, attackers aim to influence the code generation process and lead to the creation of code with exploitable vulnerabilities, such as buffer overflows or SQL injection risks. This poisoning could propagate security vulnerabilities, creating potential real-world risks. RACG poisoning aims to infect the generated code with vulnerabilities that could be exploited.

This paper is the first comprehensive study examining how vulnerable code examples in the knowledge base impact the security of code generated by RACG systems. We focus on how these poisoned examples can lead to the generation of insecure code, introducing potential vulnerabilities. Our work highlights the need for securing RACG knowledge sources to prevent the propagation of security risks in generated code.

\subsection{Threats to Validity}

{\bf Query Generation through LLM.} In Section~\ref{subsec:dataset_cons}, we use DeepSeek-V2.5 to generate queries for code. However, there is a possibility that DeepSeek-V2.5 may produce inaccurate content. To mitigate this threat, we manually review the generated queries. Specifically, we randomly select 100 queries for each programming language and have them reviewed by the two authors. Any inconsistencies in the evaluation results were resolved through discussion between the authors. The manual review indicates that 86\% of the generated queries accurately reflect the functionality of the code on average. Therefore, the impact of this threat is minimal.

\noindent

{\bf Programming Languages Investigated.} In this study, we conduct experiments using four widely-used programming languages: C, C++, Java, and Python. One potential threat is that the selected languages may not fully represent real-world development scenarios. However, according to GitHub usage statistics (measured by the number of pull requests) for the first quarter of 2024~\cite{githut2024}, these four languages account for 42.7\% of the total activity. Among them, Python and Java are the most popular. Additionally, as other languages like Go gain popularity, we plan to extend our study to include more programming languages in future work.