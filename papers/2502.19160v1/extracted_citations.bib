@article{Salih_2024,
   title={A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME},
   volume={7},
   ISSN={2640-4567},
   url={http://dx.doi.org/10.1002/aisy.202400304},
   DOI={10.1002/aisy.202400304},
   number={1},
   journal={Advanced Intelligent Systems},
   publisher={Wiley},
   author={Salih, Ahmed M. and Raisi‐Estabragh, Zahra and Galazzo, Ilaria Boscolo and Radeva, Petia and Petersen, Steffen E. and Lekadir, Karim and Menegaz, Gloria},
   year={2024},
   month=jun }

@misc{bai_measuring_2024,
	title = {Measuring {Implicit} {Bias} in {Explicitly} {Unbiased} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.04105},
	abstract = {Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both challenges by introducing two new measures of bias: LLM Implicit Bias, a promptbased method for revealing implicit bias; and LLM Decision Bias, a strategy to detect subtle discrimination in decision-making tasks. Both measures are based on psychological research: LLM Implicit Bias adapts the Implicit Association Test, widely used to study the automatic associations between concepts held in human minds; and LLM Decision Bias operationalizes psychological results indicating that relative evaluations between two candidates, not absolute evaluations assessing each independently, are more diagnostic of implicit biases. Using these measures, we found pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes (such as race and criminality, race and weapons, gender and science, age and negativity). Our prompt-based LLM Implicit Bias measure correlates with existing language model embedding-based bias methods, but better predicts downstream behaviors measured by LLM Decision Bias. These new prompt-based measures draw from psychology’s long history of research into measuring stereotype biases based on purely observable behavior; they expose nuanced biases in proprietary value-aligned LLMs that appear unbiased according to standard benchmarks.},
	language = {en},
	urldate = {2024-09-12},
	publisher = {arXiv},
	author = {Bai, Xuechunzi and Wang, Angelina and Sucholutsky, Ilia and Griffiths, Thomas L.},
	month = may,
	year = {2024},
	note = {arXiv:2402.04105 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Computation and Language, ToRead, BiasModelEvaluation, ImplicitBias, Evaluation of Downstreambias, R3},
	file = {Bai et al. - 2024 - Measuring Implicit Bias in Explicitly Unbiased Lar.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\ZFW32DWJ\\Bai et al. - 2024 - Measuring Implicit Bias in Explicitly Unbiased Lar.pdf:application/pdf},
}

@inproceedings{blodgett_language_2020,
    title = "Language (Technology) is Power: A Critical Survey of {\textquotedblleft}Bias{\textquotedblright} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.485/",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing {\textquotedblleft}bias{\textquotedblright} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {\textquotedblleft}bias{\textquotedblright} is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating {\textquotedblleft}bias{\textquotedblright} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {\textquotedblleft}bias{\textquotedblright} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {\textquotedblleft}bias{\textquotedblright}{---}i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities."
}

@inproceedings{blodgett_stereotyping_2021,
	address = {Online},
	title = {Stereotyping {Norwegian} {Salmon}: {An} {Inventory} of {Pitfalls} in {Fairness} {Benchmark} {Datasets}},
	shorttitle = {Stereotyping {Norwegian} {Salmon}},
	url = {https://aclanthology.org/2021.acl-long.81},
	doi = {10.18653/v1/2021.acl-long.81},
	language = {en},
	urldate = {2023-07-07},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
	year = {2021},
	keywords = {Fairness, ToRead, NLU, Dataset},
	pages = {1004--1015},
	file = {Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\5BFWHDLV\\Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf:application/pdf},
}

@article{bolukbasi_man_nodate,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},

	language = {en},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	file = {Bolukbasi et al. - Man is to Computer Programmer as Woman is to Homem.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\6S524DW5\\Bolukbasi et al. - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf},
}

@article{caliskan_semantics_2017,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
  journal={Science},
  volume={356},
  number={6334},
  pages={183--186},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@misc{dige_can_2023,
	title = {Can {Instruction} {Fine}-{Tuned} {Language} {Models} {Identify} {Social} {Bias} through {Prompting}?},
	url = {http://arxiv.org/abs/2307.10472},
	abstract = {As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7\%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Dige, Omkar and Tian, Jacob-Junqi and Emerson, David and Khattak, Faiza Khan},
	month = jul,
	year = {2023},
	note = {arXiv:2307.10472 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, CoT, RelatedWork},
	file = {Dige et al. - 2023 - Can Instruction Fine-Tuned Language Models Identif.pdf:/Users/rgoerge/Zotero/storage/ISJF9DEY/Dige et al. - 2023 - Can Instruction Fine-Tuned Language Models Identif.pdf:application/pdf},
}

@article{fiske2007universal,
  title={Universal dimensions of social cognition: Warmth and competence},
  author={Fiske, Susan T and Cuddy, Amy JC and Glick, Peter},
  journal={Trends in cognitive sciences},
  volume={11},
  number={2},
  pages={77--83},
  year={2007},
  publisher={Elsevier}
}

@inproceedings{fleisig_fairprism_2023,
	address = {Toronto, Canada},
	title = {{FairPrism}: {Evaluating} {Fairness}-{Related} {Harms} in {Text} {Generation}},
	shorttitle = {{FairPrism}},
	url = {https://aclanthology.org/2023.acl-long.343},
	doi = {10.18653/v1/2023.acl-long.343},
	language = {en},
	urldate = {2024-03-04},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fleisig, Eve and Amstutz, Aubrie and Atalla, Chad and Blodgett, Su Lin and Daumé Iii, Hal and Olteanu, Alexandra and Sheng, Emily and Vann, Dan and Wallach, Hanna},
	year = {2023},
	keywords = {NLU, Dataset},
	pages = {6231--6251},
	file = {Fleisig et al. - 2023 - FairPrism Evaluating Fairness-Related Harms in Te.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\6H6MXCWB\\Fleisig et al. - 2023 - FairPrism Evaluating Fairness-Related Harms in Te.pdf:application/pdf},
}

@article{fraser_computational_2022,
	title = {Computational {Modeling} of {Stereotype} {Content} in {Text}},
	volume = {5},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2022.826207/full},
	doi = {10.3389/frai.2022.826207},
	language = {en},
	urldate = {2024-07-03},
	journal = {Frontiers in Artificial Intelligence},
	author = {Fraser, Kathleen C. and Kiritchenko, Svetlana and Nejadgholi, Isar},
	month = apr,
	year = {2022},
	keywords = {ToRead, stereotypes},
	pages = {826207},
	file = {Fraser et al. - 2022 - Computational Modeling of Stereotype Content in Te.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\EP6ZWXBQ\\Fraser et al. - 2022 - Computational Modeling of Stereotype Content in Te.pdf:application/pdf},
}

@article{gallegos_bias_2023,
    title = "Bias and Fairness in Large Language Models: A Survey",
    author = "Gallegos, Isabel O.  and
      Rossi, Ryan A.  and
      Barrow, Joe  and
      Tanjim, Md Mehrab  and
      Kim, Sungchul  and
      Dernoncourt, Franck  and
      Yu, Tong  and
      Zhang, Ruiyi  and
      Ahmed, Nesreen K.",
    journal = "Computational Linguistics",
    volume = "50",
    number = "3",
    month = sep,
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.cl-3.8/",
    doi = "10.1162/coli_a_00524",
    pages = "1097--1179",
    abstract = "Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs."
}

@article{huang2024trustllm,
  title={Trustllm: Trustworthiness in large language models},
  author={Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and others},
  journal={arXiv preprint arXiv:2401.05561},
  year={2024}
}

@inproceedings{liu-2024-quantifying,
    title = "Quantifying Stereotypes in Language",
    author = "Liu, Yang",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.74",
    pages = "1223--1240",
    abstract = "A stereotype is a generalized perception of a specific group of humans. It is often potentially encoded in human language, which is more common in texts on social issues. Previous works simply define a sentence as stereotypical and anti-stereotypical. However, the stereotype of a sentence may require fine-grained quantification. In this paper, to fill this gap, we quantify stereotypes in language by annotating a dataset. We use the pre-trained language models (PLMs) to learn this dataset to predict stereotypes of sentences. Then, we discuss stereotypes about common social issues such as hate speech, sexism, sentiments, and disadvantaged and advantaged groups. We demonstrate the connections and differences between stereotypes and common social issues, and all four studies validate the general findings of the current studies. In addition, our work suggests that fine-grained stereotype scores are a highly relevant and competitive dimension for research on social issues. The models and datasets used in this paper are available at https://anonymous.4open.science/r/quantifying{\_}stereotypes{\_}in{\_}language.",
}

@inproceedings{nadeem2021stereoset,
  title={StereoSet: Measuring stereotypical bias in pretrained language models},
  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5356--5371},
  year={2021}
}

@inproceedings{nangia_crows-pairs_2020,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154/",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress."
}

@inproceedings{neveol-etal-2022-french,
    title = "{F}rench {C}row{S}-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than {E}nglish",
    author = {N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Dupont, Yoann  and
      Bezan{\c{c}}on, Julien  and
      Fort, Kar{\"e}n},
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.583/",
    doi = "10.18653/v1/2022.acl-long.583",
    pages = "8521--8531",
}

@inproceedings{pujari_reinforcement_2022,
	address = {Dublin, Ireland},
	title = {Reinforcement {Guided} {Multi}-{Task} {Learning} {Framework} for {Low}-{Resource} {Stereotype} {Detection}},
	url = {https://aclanthology.org/2022.acl-long.462},
	doi = {10.18653/v1/2022.acl-long.462},
	language = {en},
	urldate = {2025-01-03},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Pujari, Rajkumar and Oveson, Erik and Kulkarni, Priyanka and Nouri, Elnaz},
	year = {2022},
	keywords = {RelatedWork, stereotypes},
	pages = {6703--6712},
	file = {PDF:/Users/rgoerge/Zotero/storage/JYNTL7WI/Pujari et al. - 2022 - Reinforcement Guided Multi-Task Learning Framework for Low-Resource Stereotype Detection.pdf:application/pdf},
}

@inproceedings{sap_social_2020,
	address = {Online},
	title = {Social {Bias} {Frames}: {Reasoning} about {Social} and {Power} {Implications} of {Language}},
	shorttitle = {Social {Bias} {Frames}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.486},
	doi = {10.18653/v1/2020.acl-main.486},
	abstract = {Warning: this paper contains content that may be offensive or upsetting.},
	language = {en},
	urldate = {2024-07-12},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Gabriel, Saadia and Qin, Lianhui and Jurafsky, Dan and Smith, Noah A. and Choi, Yejin},
	year = {2020},
	keywords = {stereotypes},
	pages = {5477--5490},
	file = {2020.acl-main.486.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\AAX97ZLH\\2020.acl-main.486.pdf:application/pdf},
}

@inproceedings{soundararajan-delany-2024-investigating,
    title = "Investigating Gender Bias in Large Language Models Through Text Generation",
    author = "Soundararajan, Shweta  and
      Delany, Sarah Jane",
    editor = "Abbas, Mourad  and
      Freihat, Abed Alhakim",
    booktitle = "Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024)",
    month = oct,
    year = "2024",
    address = "Trento",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.icnlsp-1.42/",
    pages = "410--424"
}

@inproceedings{wang_decodingtrust_2024,
  title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  booktitle={NeurIPS},
  year={2023}
}

@misc{wu_auditing_2024,
	title = {Auditing {Large} {Language} {Models} for {Enhanced} {Text}-{Based} {Stereotype} {Detection} and {Probing}-{Based} {Bias} {Evaluation}},
	url = {http://arxiv.org/abs/2404.01768},
	language = {en},
	urldate = {2024-09-12},
	publisher = {arXiv},
	author = {Wu, Zekun and Bulathwela, Sahan and Perez-Ortiz, Maria and Koshiyama, Adriano Soares},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01768 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, DataBias, stereotypes},
	file = {Wu et al. - 2024 - Auditing Large Language Models for Enhanced Text-B.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\DA76NMLQ\\Wu et al. - 2024 - Auditing Large Language Models for Enhanced Text-B.pdf:application/pdf},
}

