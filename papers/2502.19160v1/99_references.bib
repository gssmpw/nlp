% A %

@misc{dige_can_2023,
	title = {Can {Instruction} {Fine}-{Tuned} {Language} {Models} {Identify} {Social} {Bias} through {Prompting}?},
	url = {http://arxiv.org/abs/2307.10472},
	abstract = {As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7\%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Dige, Omkar and Tian, Jacob-Junqi and Emerson, David and Khattak, Faiza Khan},
	month = jul,
	year = {2023},
	note = {arXiv:2307.10472 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, CoT, RelatedWork},
	file = {Dige et al. - 2023 - Can Instruction Fine-Tuned Language Models Identif.pdf:/Users/rgoerge/Zotero/storage/ISJF9DEY/Dige et al. - 2023 - Can Instruction Fine-Tuned Language Models Identif.pdf:application/pdf},
}

@misc{tian_interpretable_2024,
	title = {Interpretable {Stereotype} {Identification} through {Reasoning}},
	url = {http://arxiv.org/abs/2308.00071},
	abstract = {Given that large language models (LLMs) are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure that these models are equitable and free of bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B and -33B and LLaMA-2-Chat-13B and -70B. Although we observe improved accuracy by scaling from 13B to larger models, it is shown that the performance gain from reasoning can significantly exceeds the gain from scaling up. Our findings suggest that reasoning is a key factor that enables LLMs to transcend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning improves not just accuracy, but also the interpretability of the decision.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Tian, Jacob-Junqi and Dige, Omkar and Emerson, David and Khattak, Faiza Khan},
	month = mar,
	year = {2024},
	note = {arXiv:2308.00071 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, CoT, RelatedWork, ToRead},
	file = {Tian et al. - 2024 - Interpretable Stereotype Identification through Re.pdf:/Users/rgoerge/Zotero/storage/9LJYZDND/Tian et al. - 2024 - Interpretable Stereotype Identification through Re.pdf:application/pdf},
}

@misc{sun_trustllm_2024,
	title = {{TrustLLM}: {Trustworthiness} in {Large} {Language} {Models}},
	shorttitle = {{TrustLLM}},
	url = {http://arxiv.org/abs/2401.05561},
	abstract = {Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.},
	language = {en},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Sun, Lichao and Huang, Yue and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and Liu, Zhengliang and Liu, Yixin and Wang, Yijue and Zhang, Zhikun and Vidgen, Bertie and Kailkhura, Bhavya and Xiong, Caiming and Xiao, Chaowei and Li, Chunyuan and Xing, Eric and Huang, Furong and Liu, Hao and Ji, Heng and Wang, Hongyi and Zhang, Huan and Yao, Huaxiu and Kellis, Manolis and Zitnik, Marinka and Jiang, Meng and Bansal, Mohit and Zou, James and Pei, Jian and Liu, Jian and Gao, Jianfeng and Han, Jiawei and Zhao, Jieyu and Tang, Jiliang and Wang, Jindong and Vanschoren, Joaquin and Mitchell, John and Shu, Kai and Xu, Kaidi and Chang, Kai-Wei and He, Lifang and Huang, Lifu and Backes, Michael and Gong, Neil Zhenqiang and Yu, Philip S. and Chen, Pin-Yu and Gu, Quanquan and Xu, Ran and Ying, Rex and Ji, Shuiwang and Jana, Suman and Chen, Tianlong and Liu, Tianming and Zhou, Tianyi and Wang, William and Li, Xiang and Zhang, Xiangliang and Wang, Xiao and Xie, Xing and Chen, Xun and Wang, Xuyu and Liu, Yan and Ye, Yanfang and Cao, Yinzhi and Chen, Yong and Zhao, Yue},
	month = mar,
	year = {2024},
	note = {arXiv:2401.05561 [cs]},
	keywords = {Computer Science - Computation and Language, RelatedWork, ToRead},
	file = {2401.pdf:/Users/rgoerge/Zotero/storage/W77LYKQD/2401.pdf:application/pdf},
}



@inproceedings{soundararajan-delany-2024-investigating,
    title = "Investigating Gender Bias in Large Language Models Through Text Generation",
    author = "Soundararajan, Shweta  and
      Delany, Sarah Jane",
    editor = "Abbas, Mourad  and
      Freihat, Abed Alhakim",
    booktitle = "Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024)",
    month = oct,
    year = "2024",
    address = "Trento",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.icnlsp-1.42/",
    pages = "410--424"
}


@inproceedings{
tian2023using,
title={Using Chain-of-Thought Prompting for Interpretable Recognition of Social Bias},
author={Jacob-Junqi Tian and Omkar Dige and D. Emerson and Faiza Khattak},
booktitle={Socially Responsible Language Modelling Research},
year={2023},
url={https://openreview.net/forum?id=QyRganPqPz}
}

@inproceedings{
king2024hearts,
title={{HEARTS}: A Holistic Framework for Explainable, Sustainable and Robust Text Stereotype Detection},
author={Theo King and Zekun Wu and Adriano Koshiyama and Emre Kazim and Philip Colin Treleaven},
booktitle={Neurips Safe Generative AI Workshop 2024},
year={2024},
url={https://openreview.net/forum?id=arh91riKiQ}
}

@misc{dong2024,
      title={A Survey on In-context Learning}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Jingyuan Ma and Rui Li and Heming Xia and Jingjing Xu and Zhiyong Wu and Tianyu Liu and Baobao Chang and Xu Sun and Lei Li and Zhifang Sui},
      year={2024},
      eprint={2301.00234},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.00234}, 
}

@misc{gu2025,
      title={A Survey on LLM-as-a-Judge}, 
      author={Jiawei Gu and Xuhui Jiang and Zhichao Shi and Hexiang Tan and Xuehao Zhai and Chengjin Xu and Wei Li and Yinghan Shen and Shengjie Ma and Honghao Liu and Yuanzhuo Wang and Jian Guo},
      year={2025},
      eprint={2411.15594},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.15594}, 
}

@article{Hovy2021,
author = {Hovy, Dirk and Prabhumoye, Shrimai},
title = {Five sources of bias in natural language processing},
journal = {Language and Linguistics Compass},
volume = {15},
number = {8},
pages = {e12432},
doi = {https://doi.org/10.1111/lnc3.12432},
url = {https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12432},
eprint = {https://compass.onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12432},
abstract = {Abstract Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter-measures.},
year = {2021}
}


% B %

@article{bai2024measuring,
  title={Measuring implicit bias in explicitly unbiased large language models},
  author={Bai, Xuechunzi and Wang, Angelina and Sucholutsky, Ilia and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2402.04105},
  year={2024}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@inproceedings{blodgett_stereotyping_2021,
	address = {Online},
	title = {Stereotyping {Norwegian} {Salmon}: {An} {Inventory} of {Pitfalls} in {Fairness} {Benchmark} {Datasets}},
	shorttitle = {Stereotyping {Norwegian} {Salmon}},
	url = {https://aclanthology.org/2021.acl-long.81},
	doi = {10.18653/v1/2021.acl-long.81},
	language = {en},
	urldate = {2023-07-07},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
	year = {2021},
	keywords = {Fairness, ToRead, NLU, Dataset},
	pages = {1004--1015},
	file = {Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\5BFWHDLV\\Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf:application/pdf},
}
@article{Jussim2024,
  author    = {Lee Jussim and Nathan Honeycutt},
  title     = {Bias in Psychology: A Critical, Historical and Empirical Review},
  journal   = {Swiss Psychology Open},
  volume    = {4},
  number    = {1},
  pages     = {5},
  year      = {2024},
  doi       = {10.5334/spo.77},
  url       = {https://doi.org/10.5334/spo.77}
}
@inproceedings{barikeri-etal-2021-redditbias,
    title = "{R}eddit{B}ias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models",
    author = "Barikeri, Soumya  and
      Lauscher, Anne  and
      Vuli{\'c}, Ivan  and
      Glava{\v{s}}, Goran",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.151/",
    doi = "10.18653/v1/2021.acl-long.151",
    pages = "1941--1955",
    abstract = "Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance."
}
@inproceedings{blodgett_language_2020,
    title = "Language (Technology) is Power: A Critical Survey of {\textquotedblleft}Bias{\textquotedblright} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.485/",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing {\textquotedblleft}bias{\textquotedblright} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {\textquotedblleft}bias{\textquotedblright} is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating {\textquotedblleft}bias{\textquotedblright} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {\textquotedblleft}bias{\textquotedblright} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {\textquotedblleft}bias{\textquotedblright}{---}i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities."
}

@article{bolukbasi2016man,
  title={Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
  author={Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016},
}
@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	language = {en},
	urldate = {2023-02-24},
	publisher = {arXiv},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\MRZ7SXH9\\Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf},
}

@misc{boggust_embedding_2022,
	title = {Embedding {Comparator}: {Visualizing} {Differences} in {Global} {Structure} and {Local} {Neighborhoods} via {Small} {Multiples}},
	shorttitle = {Embedding {Comparator}},
	url = {http://arxiv.org/abs/1912.04853},
	language = {en},
	urldate = {2023-04-30},
	publisher = {arXiv},
	author = {Boggust, Angie and Carter, Brandon and Satyanarayan, Arvind},
	month = mar,
	year = {2022},
	note = {arXiv:1912.04853 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {Boggust et al. - 2022 - Embedding Comparator Visualizing Differences in G.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\3VJEXV3L\\1912.04853.pdf:application/pdf},
}


% C %


@article{caliskan_semantics_2017,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
  journal={Science},
  volume={356},
  number={6334},
  pages={183--186},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

% D %
@book{dovidio2010sage,
  title={The SAGE handbook of prejudice, stereotyping and discrimination},
  author={Dovidio, John F},
  year={2010},
  publisher={Sage Publications}
}


% E %
@misc{euaiact2024,
  title        = {Regulation (EU) 2024/1689 of the European Parliament and of the Council on Artificial Intelligence (EU AI Act)},
  author       = {European Parliament and Council of the European Union},
  year         = {2024},
  note         = {Official Journal of the European Union},
  url          = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32024R1689},
}
% F %
@article{fraser_computational_2022,
	title = {Computational {Modeling} of {Stereotype} {Content} in {Text}},
	volume = {5},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2022.826207/full},
	doi = {10.3389/frai.2022.826207},
	language = {en},
	urldate = {2024-07-03},
	journal = {Frontiers in Artificial Intelligence},
	author = {Fraser, Kathleen C. and Kiritchenko, Svetlana and Nejadgholi, Isar},
	month = apr,
	year = {2022},
	keywords = {ToRead, stereotypes},
	pages = {826207},
	file = {Fraser et al. - 2022 - Computational Modeling of Stereotype Content in Te.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\EP6ZWXBQ\\Fraser et al. - 2022 - Computational Modeling of Stereotype Content in Te.pdf:application/pdf},
}

% G %

% H %

% I %
@misc{isoiec24027:2021,
  title        = {ISO/IEC TR 24027:2021- Information technology — Artificial intelligence (AI) — Bias in AI systems and AI aided decision making},
  author  = {ISO/IEC},
  number       = {ISO/IEC TR 24027:2021},
  year         = {2021},
}

% J %

% K %

% L %
@inproceedings{liu-2024-quantifying,
    title = "Quantifying Stereotypes in Language",
    author = "Liu, Yang",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.74",
    pages = "1223--1240",
    abstract = "A stereotype is a generalized perception of a specific group of humans. It is often potentially encoded in human language, which is more common in texts on social issues. Previous works simply define a sentence as stereotypical and anti-stereotypical. However, the stereotype of a sentence may require fine-grained quantification. In this paper, to fill this gap, we quantify stereotypes in language by annotating a dataset. We use the pre-trained language models (PLMs) to learn this dataset to predict stereotypes of sentences. Then, we discuss stereotypes about common social issues such as hate speech, sexism, sentiments, and disadvantaged and advantaged groups. We demonstrate the connections and differences between stereotypes and common social issues, and all four studies validate the general findings of the current studies. In addition, our work suggests that fine-grained stereotype scores are a highly relevant and competitive dimension for research on social issues. The models and datasets used in this paper are available at https://anonymous.4open.science/r/quantifying{\_}stereotypes{\_}in{\_}language.",
}

% M %
@techreport{maslej2024ai,
  author       = {Nestor Maslej and Loredana Fattorini and Raymond Perrault and Vanessa Parli and Anka Reuel and Erik Brynjolfsson and John Etchemendy and Katrina Ligett and Terah Lyons and James Manyika and Juan Carlos Niebles and Yoav Shoham and Russell Wald and Jack Clark},
  title        = {The AI Index 2024 Annual Report},
  institution  = {AI Index Steering Committee, Institute for Human-Centered AI, Stanford University},
  address      = {Stanford, CA},
  month        = {April},
  year         = {2024},
}

% N %

@inproceedings{nangia_crows-pairs_2020,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154/",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress."
}

@inproceedings{nadeem2021stereoset,
  title={StereoSet: Measuring stereotypical bias in pretrained language models},
  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5356--5371},
  year={2021}
}
@inproceedings{neveol-etal-2022-french,
    title = "{F}rench {C}row{S}-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than {E}nglish",
    author = {N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Dupont, Yoann  and
      Bezan{\c{c}}on, Julien  and
      Fort, Kar{\"e}n},
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.583/",
    doi = "10.18653/v1/2022.acl-long.583",
    pages = "8521--8531",
}



% O %

% P %

@inproceedings{pujari_reinforcement_2022,
	address = {Dublin, Ireland},
	title = {Reinforcement {Guided} {Multi}-{Task} {Learning} {Framework} for {Low}-{Resource} {Stereotype} {Detection}},
	url = {https://aclanthology.org/2022.acl-long.462},
	doi = {10.18653/v1/2022.acl-long.462},
	language = {en},
	urldate = {2025-01-03},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Pujari, Rajkumar and Oveson, Erik and Kulkarni, Priyanka and Nouri, Elnaz},
	year = {2022},
	keywords = {RelatedWork, stereotypes},
	pages = {6703--6712},
	file = {PDF:/Users/rgoerge/Zotero/storage/JYNTL7WI/Pujari et al. - 2022 - Reinforcement Guided Multi-Task Learning Framework for Low-Resource Stereotype Detection.pdf:application/pdf},
}

% Q %

% R %

% S %
@inproceedings{sap_social_2020,
	address = {Online},
	title = {Social {Bias} {Frames}: {Reasoning} about {Social} and {Power} {Implications} of {Language}},
	shorttitle = {Social {Bias} {Frames}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.486},
	doi = {10.18653/v1/2020.acl-main.486},
	abstract = {Warning: this paper contains content that may be offensive or upsetting.},
	language = {en},
	urldate = {2024-07-12},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Gabriel, Saadia and Qin, Lianhui and Jurafsky, Dan and Smith, Noah A. and Choi, Yejin},
	year = {2020},
	keywords = {stereotypes},
	pages = {5477--5490},
	file = {2020.acl-main.486.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\AAX97ZLH\\2020.acl-main.486.pdf:application/pdf},
}

@inproceedings{see-etal-2017-get,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}


% T %

% U %

% V %

% W %
@misc{wu_auditing_2024,
	title = {Auditing {Large} {Language} {Models} for {Enhanced} {Text}-{Based} {Stereotype} {Detection} and {Probing}-{Based} {Bias} {Evaluation}},
	url = {http://arxiv.org/abs/2404.01768},
	language = {en},
	urldate = {2024-09-12},
	publisher = {arXiv},
	author = {Wu, Zekun and Bulathwela, Sahan and Perez-Ortiz, Maria and Koshiyama, Adriano Soares},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01768 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, DataBias, stereotypes},
	file = {Wu et al. - 2024 - Auditing Large Language Models for Enhanced Text-B.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\DA76NMLQ\\Wu et al. - 2024 - Auditing Large Language Models for Enhanced Text-B.pdf:application/pdf},
}

% X %

% Y %

% Z %



@misc{wang_towards_2020,
	title = {Towards {Fairness} in {Visual} {Recognition}: {Effective} {Strategies} for {Bias} {Mitigation}},
	shorttitle = {Towards {Fairness} in {Visual} {Recognition}},
	url = {http://arxiv.org/abs/1911.11834},
	language = {en},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Wang, Zeyu and Qinami, Klint and Karakozis, Ioannis Christos and Genova, Kyle and Nair, Prem and Hata, Kenji and Russakovsky, Olga},
	month = apr,
	year = {2020},
	note = {arXiv:1911.11834 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Fairness, CelebA},
	file = {Wang et al. - 2020 - Towards Fairness in Visual Recognition Effective .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\A9TALKUY\\Wang et al. - 2020 - Towards Fairness in Visual Recognition Effective .pdf:application/pdf},
}


@misc{lee_deduplicating_2022,
	title = {Deduplicating {Training} {Data} {Makes} {Language} {Models} {Better}},
	url = {http://arxiv.org/abs/2107.06499},
	language = {en},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
	month = mar,
	year = {2022},
	note = {arXiv:2107.06499 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, ToRead, NLU, Dataset, Motivation},
	file = {Lee et al. - 2022 - Deduplicating Training Data Makes Language Models .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\JJNNN8CK\\Lee et al. - 2022 - Deduplicating Training Data Makes Language Models .pdf:application/pdf},
}

@misc{kumar_nurse_2020,
	title = {Nurse is {Closer} to {Woman} than {Surgeon}? {Mitigating} {Gender}-{Biased} {Proximities} in {Word} {Embeddings}},
	shorttitle = {Nurse is {Closer} to {Woman} than {Surgeon}?},
	url = {http://arxiv.org/abs/2006.01938},
	language = {en},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Kumar, Vaibhav and Bhotia, Tenzin Singhay and Kumar, Vaibhav and Chakraborty, Tanmoy},
	month = jun,
	year = {2020},
	note = {arXiv:2006.01938 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Fairness, ToRead},
	file = {Kumar et al. - 2020 - Nurse is Closer to Woman than Surgeon Mitigating .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\6Q389LPF\\Kumar et al. - 2020 - Nurse is Closer to Woman than Surgeon Mitigating .pdf:application/pdf},
}

@misc{parraga_debiasing_2022,
	title = {Debiasing {Methods} for {Fairer} {Neural} {Models} in {Vision} and {Language} {Research}: {A} {Survey}},
	shorttitle = {Debiasing {Methods} for {Fairer} {Neural} {Models} in {Vision} and {Language} {Research}},
	url = {http://arxiv.org/abs/2211.05617},
	language = {en},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Parraga, Otávio and More, Martin D. and Oliveira, Christian M. and Gavenski, Nathan S. and Kupssinskü, Lucas S. and Medronha, Adilson and Moura, Luis V. and Simões, Gabriel S. and Barros, Rodrigo C.},
	month = nov,
	year = {2022},
	note = {arXiv:2211.05617 [cs]},
	keywords = {Fairness, ToRead, Motivation},
	file = {Parraga et al. - 2022 - Debiasing Methods for Fairer Neural Models in Visi.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\EF9KN9YP\\Parraga et al. - 2022 - Debiasing Methods for Fairer Neural Models in Visi.pdf:application/pdf},
}


@inproceedings{steed_image_2021,
	title = {Image {Representations} {Learned} {With} {Unsupervised} {Pre}-{Training} {Contain} {Human}-like {Biases}},
	url = {http://arxiv.org/abs/2010.15052},
	doi = {10.1145/3442188.3445932},
	language = {en},
	urldate = {2023-07-19},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Steed, Ryan and Caliskan, Aylin},
	month = mar,
	year = {2021},
	note = {arXiv:2010.15052 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Fairness, Embeddings},
	pages = {701--713},
	file = {Steed und Caliskan - 2021 - Image Representations Learned With Unsupervised Pr.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\Y7J7TA3Y\\Steed und Caliskan - 2021 - Image Representations Learned With Unsupervised Pr.pdf:application/pdf},
}

@article{fabris_algorithmic_2022,
	title = {Algorithmic fairness datasets: the story so far},
	volume = {36},
	issn = {1384-5810, 1573-756X},
	shorttitle = {Algorithmic fairness datasets},
	url = {https://link.springer.com/10.1007/s10618-022-00854-z},
	doi = {10.1007/s10618-022-00854-z},
	language = {en},
	number = {6},
	urldate = {2023-07-20},
	journal = {Data Mining and Knowledge Discovery},
	author = {Fabris, Alessandro and Messina, Stefano and Silvello, Gianmaria and Susto, Gian Antonio},
	month = nov,
	year = {2022},
	keywords = {Fairness, Dataset},
	pages = {2074--2152},
	file = {Fabris et al. - 2022 - Algorithmic fairness datasets the story so far.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\JV3CNHMQ\\Fabris et al. - 2022 - Algorithmic fairness datasets the story so far.pdf:application/pdf},
}

@misc{cohen_lm_2023,
	title = {{LM} vs {LM}: {Detecting} {Factual} {Errors} via {Cross} {Examination}},
	shorttitle = {{LM} vs {LM}},
	url = {http://arxiv.org/abs/2305.13281},
	language = {en},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {Cohen, Roi and Hamri, May and Geva, Mor and Globerson, Amir},
	month = may,
	year = {2023},
	note = {arXiv:2305.13281 [cs]},
	keywords = {Computer Science - Computation and Language, FMasATool, Haluzination},
	file = {30142-Article Text-34196-1-2-20240324.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\JGCA27PF\\30142-Article Text-34196-1-2-20240324.pdf:application/pdf;Cohen et al. - 2023 - LM vs LM Detecting Factual Errors via Cross Exami.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\4GKUR6JE\\Cohen et al. - 2023 - LM vs LM Detecting Factual Errors via Cross Exami.pdf:application/pdf},
}

@article{navigli_biases_2023,
	title = {Biases in {Large} {Language} {Models}: {Origins}, {Inventory}, and {Discussion}},
	volume = {15},
	issn = {1936-1955, 1936-1963},
	shorttitle = {Biases in {Large} {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3597307},
	doi = {10.1145/3597307},
	language = {en},
	number = {2},
	urldate = {2023-07-31},
	journal = {Journal of Data and Information Quality},
	author = {Navigli, Roberto and Conia, Simone and Ross, Björn},
	month = jun,
	year = {2023},
	keywords = {Fairness, Motivation, FoundationModels, LLM-Bias},
	pages = {1--21},
	file = {Navigli et al. - 2023 - Biases in Large Language Models Origins, Inventor.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\YTTKX5CF\\Navigli et al. - 2023 - Biases in Large Language Models Origins, Inventor.pdf:application/pdf},
}

@inproceedings{steed_upstream_2022,
	address = {Dublin, Ireland},
	title = {Upstream {Mitigation} {Is} {Not} {All} {You} {Need}: {Testing} the {Bias} {Transfer} {Hypothesis} in {Pre}-{Trained} {Language} {Models}},
	shorttitle = {Upstream {Mitigation} {Is} {Not} {All} {You} {Need}},
	url = {https://aclanthology.org/2022.acl-long.247},
	doi = {10.18653/v1/2022.acl-long.247},
	language = {en},
	urldate = {2023-08-08},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Steed, Ryan and Panda, Swetasudha and Kobren, Ari and Wick, Michael},
	year = {2022},
	keywords = {Fairness, ToRead, FineTuning},
	pages = {3524--3542},
	file = {Steed et al. - 2022 - Upstream Mitigation Is Not All You Need Testing t.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\U3FMZU4W\\Steed et al. - 2022 - Upstream Mitigation Is Not All You Need Testing t.pdf:application/pdf},
}

@inproceedings{chen_general_2020,
	address = {Honolulu HI USA},
	title = {A {General} {Methodology} to {Quantify} {Biases} in {Natural} {Language} {Data}},
	isbn = {978-1-4503-6819-3},
	url = {https://dl.acm.org/doi/10.1145/3334480.3382949},
	doi = {10.1145/3334480.3382949},
	language = {en},
	urldate = {2023-10-09},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Chen, Jiawei and Xu, Anbang and Liu, Zhe and Guo, Yufan and Liu, Xiaotong and Tong, Yingbei and Akkiraju, Rama and Carroll, John M.},
	month = apr,
	year = {2020},
	keywords = {Fairness, Text, ReferenceDataset},
	pages = {1--9},
	file = {Chen et al. - 2020 - A General Methodology to Quantify Biases in Natura.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\ZFWBJXUZ\\Chen et al. - 2020 - A General Methodology to Quantify Biases in Natura.pdf:application/pdf},
}


@article{gallegos_bias_2023,
    title = "Bias and Fairness in Large Language Models: A Survey",
    author = "Gallegos, Isabel O.  and
      Rossi, Ryan A.  and
      Barrow, Joe  and
      Tanjim, Md Mehrab  and
      Kim, Sungchul  and
      Dernoncourt, Franck  and
      Yu, Tong  and
      Zhang, Ruiyi  and
      Ahmed, Nesreen K.",
    journal = "Computational Linguistics",
    volume = "50",
    number = "3",
    month = sep,
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.cl-3.8/",
    doi = "10.1162/coli_a_00524",
    pages = "1097--1179",
    abstract = "Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs."
}

@article{tian_image_2022,
	title = {Image fairness in deep learning: problems, models, and challenges},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	shorttitle = {Image fairness in deep learning},
	url = {https://link.springer.com/10.1007/s00521-022-07136-1},
	doi = {10.1007/s00521-022-07136-1},
	language = {en},
	number = {15},
	urldate = {2023-10-27},
	journal = {Neural Computing and Applications},
	author = {Tian, Huan and Zhu, Tianqing and Liu, Wei and Zhou, Wanlei},
	month = aug,
	year = {2022},
	keywords = {Fairness, Motivation, Survey, Images},
	pages = {12875--12893},
	file = {Tian et al. - 2022 - Image fairness in deep learning problems, models,.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\5FDUR3QN\\Tian et al. - 2022 - Image fairness in deep learning problems, models,.pdf:application/pdf},
}

@misc{lu_gender_2019,
	title = {Gender {Bias} in {Neural} {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/1807.11714},
	language = {en},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
	month = may,
	year = {2019},
	note = {arXiv:1807.11714 [cs]},
	keywords = {Computer Science - Computation and Language, Huawei},
	file = {Lu et al. - 2019 - Gender Bias in Neural Natural Language Processing.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\YTFKZA5L\\Lu et al. - 2019 - Gender Bias in Neural Natural Language Processing.pdf:application/pdf},
}

@misc{stanczak_survey_2021,
	title = {A {Survey} on {Gender} {Bias} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2112.14168},
	language = {en},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Stanczak, Karolina and Augenstein, Isabelle},
	month = dec,
	year = {2021},
	note = {arXiv:2112.14168 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Computation and Language},
	file = {2112.14168.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\52IB35JY\\2112.14168.pdf:application/pdf},
}

@misc{meade_empirical_2022,
	title = {An {Empirical} {Survey} of the {Effectiveness} of {Debiasing} {Techniques} for {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2110.08527},
	language = {en},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Meade, Nicholas and Poole-Dayan, Elinor and Reddy, Siva},
	month = apr,
	year = {2022},
	note = {arXiv:2110.08527 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, ToRead, Huawei},
	file = {Meade et al. - 2022 - An Empirical Survey of the Effectiveness of Debias.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\WMS29UCJ\\Meade et al. - 2022 - An Empirical Survey of the Effectiveness of Debias.pdf:application/pdf},
}

@inproceedings{may_measuring_2019,
	address = {Minneapolis, Minnesota},
	title = {On {Measuring} {Social} {Biases} in {Sentence} {Encoders}},
	url = {http://aclweb.org/anthology/N19-1063},
	doi = {10.18653/v1/N19-1063},
	language = {en},
	urldate = {2023-12-05},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R. and Rudinger, Rachel},
	year = {2019},
	pages = {622--628},
	file = {May et al. - 2019 - On Measuring Social Biases in Sentence Encoders.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\VB5QNCRB\\May et al. - 2019 - On Measuring Social Biases in Sentence Encoders.pdf:application/pdf},
}

@article{garrido-munoz_survey_2021,
	title = {A {Survey} on {Bias} in {Deep} {NLP}},
	volume = {11},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/7/3184},
	doi = {10.3390/app11073184},
	language = {en},
	number = {7},
	urldate = {2023-12-05},
	journal = {Applied Sciences},
	author = {Garrido-Muñoz, Ismael and Montejo-Ráez, Arturo and Martínez-Santiago, Fernando and Ureña-López, L. Alfonso},
	month = apr,
	year = {2021},
	keywords = {Huawei},
	pages = {3184},
	file = {Garrido-Muñoz et al. - 2021 - A Survey on Bias in Deep NLP.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\VQYNCC8W\\Garrido-Muñoz et al. - 2021 - A Survey on Bias in Deep NLP.pdf:application/pdf},
}

@misc{webster_measuring_2021,
	title = {Measuring and {Reducing} {Gendered} {Correlations} in {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2010.06032},
	language = {en},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Webster, Kellie and Wang, Xuezhi and Tenney, Ian and Beutel, Alex and Pitler, Emily and Pavlick, Ellie and Chen, Jilin and Chi, Ed and Petrov, Slav},
	month = mar,
	year = {2021},
	note = {arXiv:2010.06032 [cs]},
	keywords = {Computer Science - Computation and Language, Huawei, MitigationMethod},
	file = {Webster et al. - 2021 - Measuring and Reducing Gendered Correlations in Pr.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\3J9QL2HP\\Webster et al. - 2021 - Measuring and Reducing Gendered Correlations in Pr.pdf:application/pdf},
}

@inproceedings{zhao_gender_2018,
	address = {New Orleans, Louisiana},
	title = {Gender {Bias} in {Coreference} {Resolution}: {Evaluation} and {Debiasing} {Methods}},
	shorttitle = {Gender {Bias} in {Coreference} {Resolution}},
	url = {http://aclweb.org/anthology/N18-2003},
	doi = {10.18653/v1/N18-2003},
	language = {en},
	urldate = {2023-12-07},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	year = {2018},
	keywords = {Dataset, Huawei},
	pages = {15--20},
	file = {Zhao et al. - 2018 - Gender Bias in Coreference Resolution Evaluation .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\SV28P4HB\\Zhao et al. - 2018 - Gender Bias in Coreference Resolution Evaluation .pdf:application/pdf},
}

@article{bartl_unmasking_nodate,
	title = {Unmasking {Contextual} {Stereotypes}: {Measuring} and {Mitigating} {BERT}’s {Gender} {Bias}},

	language = {en},
	author = {Bartl, Marion and Nissim, Malvina and Gatt, Albert},
	keywords = {ToRead, Huawei},
	file = {Bartl et al. - Unmasking Contextual Stereotypes Measuring and Mi.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\E3RLL6NI\\Bartl et al. - Unmasking Contextual Stereotypes Measuring and Mi.pdf:application/pdf},
}

@inproceedings{lucy_gender_2021,
	address = {Virtual},
	title = {Gender and {Representation} {Bias} in {GPT}-3 {Generated} {Stories}},
	url = {https://www.aclweb.org/anthology/2021.nuse-1.5},
	doi = {10.18653/v1/2021.nuse-1.5},
	language = {en},
	urldate = {2023-12-12},
	booktitle = {Proceedings of the {Third} {Workshop} on {Narrative} {Understanding}},
	publisher = {Association for Computational Linguistics},
	author = {Lucy, Li and Bamman, David},
	year = {2021},
	keywords = {ToRead, Huawei},
	pages = {48--55},
	file = {Lucy und Bamman - 2021 - Gender and Representation Bias in GPT-3 Generated .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\LQDJQSS4\\Lucy und Bamman - 2021 - Gender and Representation Bias in GPT-3 Generated .pdf:application/pdf},
}

@article{dev_measuring_2020,
	title = {On {Measuring} and {Mitigating} {Biased} {Inferences} of {Word} {Embeddings}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6267},
	doi = {10.1609/aaai.v34i05.6267},
	language = {en},
	number = {05},
	urldate = {2023-12-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Dev, Sunipa and Li, Tao and Phillips, Jeff M. and Srikumar, Vivek},
	month = apr,
	year = {2020},
	keywords = {ToRead, Huawei},
	pages = {7659--7666},
	file = {Dev et al. - 2020 - On Measuring and Mitigating Biased Inferences of W.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\H2IJSQMZ\\Dev et al. - 2020 - On Measuring and Mitigating Biased Inferences of W.pdf:application/pdf},
}

@inproceedings{field_survey_2021,
	address = {Online},
	title = {A {Survey} of {Race}, {Racism}, and {Anti}-{Racism} in {NLP}},
	url = {https://aclanthology.org/2021.acl-long.149},
	doi = {10.18653/v1/2021.acl-long.149},

	language = {en},
	urldate = {2023-12-12},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Field, Anjalie and Blodgett, Su Lin and Waseem, Zeerak and Tsvetkov, Yulia},
	year = {2021},
	pages = {1905--1925},
	file = {Field et al. - 2021 - A Survey of Race, Racism, and Anti-Racism in NLP.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\C5T23GRY\\Field et al. - 2021 - A Survey of Race, Racism, and Anti-Racism in NLP.pdf:application/pdf},
}

@inproceedings{abid_persistent_2021,
	address = {Virtual Event USA},
	title = {Persistent {Anti}-{Muslim} {Bias} in {Large} {Language} {Models}},
	isbn = {978-1-4503-8473-5},
	url = {https://dl.acm.org/doi/10.1145/3461702.3462624},
	doi = {10.1145/3461702.3462624},

	language = {en},
	urldate = {2023-12-12},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Abid, Abubakar and Farooqi, Maheen and Zou, James},
	month = jul,
	year = {2021},
	keywords = {ToRead, Motivation, Huawei},
	pages = {298--306},
	file = {Abid et al. - 2021 - Persistent Anti-Muslim Bias in Large Language Mode.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\YHU9P8NH\\Abid et al. - 2021 - Persistent Anti-Muslim Bias in Large Language Mode.pdf:application/pdf},
}

@article{czarnowska_quantifying_2021,
	title = {Quantifying {Social} {Biases} in {NLP}: {A} {Generalization} and {Empirical} {Comparison} of {Extrinsic} {Fairness} {Metrics}},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {Quantifying {Social} {Biases} in {NLP}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00425/108201/Quantifying-Social-Biases-in-NLP-A-Generalization},
	doi = {10.1162/tacl_a_00425},
	language = {en},
	urldate = {2023-12-12},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Czarnowska, Paula and Vyas, Yogarshi and Shah, Kashif},
	month = nov,
	year = {2021},
	keywords = {Huawei, FairnessDefinition, FairnessMetrics},
	pages = {1249--1267},
	file = {Czarnowska et al. - 2021 - Quantifying Social Biases in NLP A Generalization.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\LQGJA6JF\\tacl_a_00425.pdf:application/pdf},
}

@article{tomalin_practical_2021,
	title = {The practical ethics of bias reduction in machine translation: why domain adaptation is better than data debiasing},
	volume = {23},
	issn = {1388-1957, 1572-8439},
	shorttitle = {The practical ethics of bias reduction in machine translation},
	url = {https://link.springer.com/10.1007/s10676-021-09583-1},
	doi = {10.1007/s10676-021-09583-1},

	language = {en},
	number = {3},
	urldate = {2023-12-12},
	journal = {Ethics and Information Technology},
	author = {Tomalin, Marcus and Byrne, Bill and Concannon, Shauna and Saunders, Danielle and Ullmann, Stefanie},
	month = sep,
	year = {2021},
	keywords = {ToRead, Huawei},
	pages = {419--433},
	file = {Tomalin et al. - 2021 - The practical ethics of bias reduction in machine .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\KT6GLFUY\\Tomalin et al. - 2021 - The practical ethics of bias reduction in machine .pdf:application/pdf},
}

@misc{bansal_survey_2022,
	title = {A {Survey} on {Bias} and {Fairness} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2204.09591},
	language = {en},
	urldate = {2023-12-15},
	publisher = {arXiv},
	author = {Bansal, Rajas},
	month = mar,
	year = {2022},
	note = {arXiv:2204.09591 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Bansal - 2022 - A Survey on Bias and Fairness in Natural Language .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\LP4428M6\\Bansal - 2022 - A Survey on Bias and Fairness in Natural Language .pdf:application/pdf},
}



@misc{delobelle_measuring_2021,
	title = {Measuring {Fairness} with {Biased} {Rulers}: {A} {Survey} on {Quantifying} {Biases} in {Pretrained} {Language} {Models}},
	shorttitle = {Measuring {Fairness} with {Biased} {Rulers}},
	url = {http://arxiv.org/abs/2112.07447},

	language = {en},
	urldate = {2023-12-15},
	publisher = {arXiv},
	author = {Delobelle, Pieter and Tokpo, Ewoenam Kwaku and Calders, Toon and Berendt, Bettina},
	month = dec,
	year = {2021},
	note = {arXiv:2112.07447 [cs]},
	keywords = {ToRead},
	file = {Delobelle et al. - 2021 - Measuring Fairness with Biased Rulers A Survey on.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\3R4YFTBV\\Delobelle et al. - 2021 - Measuring Fairness with Biased Rulers A Survey on.pdf:application/pdf},
}



@inproceedings{parrish_bbq_2022,
	address = {Dublin, Ireland},
	title = {{BBQ}: {A} hand-built bias benchmark for question answering},
	shorttitle = {{BBQ}},
	url = {https://aclanthology.org/2022.findings-acl.165},
	doi = {10.18653/v1/2022.findings-acl.165},
	language = {en},
	urldate = {2024-01-08},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel},
	year = {2022},
	pages = {2086--2105},
	file = {Parrish et al. - 2022 - BBQ A hand-built bias benchmark for question answ.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\GE5S6S7R\\Parrish et al. - 2022 - BBQ A hand-built bias benchmark for question answ.pdf:application/pdf},
}

@article{bolukbasi_man_nodate,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},

	language = {en},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	file = {Bolukbasi et al. - Man is to Computer Programmer as Woman is to Homem.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\6S524DW5\\Bolukbasi et al. - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf},
}

@inproceedings{wang_assessing_2022,
	address = {Dublin, Ireland},
	title = {Assessing {Multilingual} {Fairness} in {Pre}-trained {Multimodal} {Representations}},
	url = {https://aclanthology.org/2022.findings-acl.211},
	doi = {10.18653/v1/2022.findings-acl.211},

	language = {en},
	urldate = {2024-01-12},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Jialu and Liu, Yang and Wang, Xin},
	year = {2022},
	pages = {2681--2695},
	file = {Wang et al. - 2022 - Assessing Multilingual Fairness in Pre-trained Mul.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\RJ33CQ3E\\Wang et al. - 2022 - Assessing Multilingual Fairness in Pre-trained Mul.pdf:application/pdf},
}

@inproceedings{fleisig_fairprism_2023,
	address = {Toronto, Canada},
	title = {{FairPrism}: {Evaluating} {Fairness}-{Related} {Harms} in {Text} {Generation}},
	shorttitle = {{FairPrism}},
	url = {https://aclanthology.org/2023.acl-long.343},
	doi = {10.18653/v1/2023.acl-long.343},
	language = {en},
	urldate = {2024-03-04},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fleisig, Eve and Amstutz, Aubrie and Atalla, Chad and Blodgett, Su Lin and Daumé Iii, Hal and Olteanu, Alexandra and Sheng, Emily and Vann, Dan and Wallach, Hanna},
	year = {2023},
	keywords = {NLU, Dataset},
	pages = {6231--6251},
	file = {Fleisig et al. - 2023 - FairPrism Evaluating Fairness-Related Harms in Te.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\6H6MXCWB\\Fleisig et al. - 2023 - FairPrism Evaluating Fairness-Related Harms in Te.pdf:application/pdf},
}

@article{liang_towards_nodate,
	title = {Towards {Understanding} and {Mitigating} {Social} {Biases} in {Language} {Models}},

	language = {en},
	author = {Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
	keywords = {ToRead, NLU, MitigationMethod},
	file = {Liang et al. - Towards Understanding and Mitigating Social Biases.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\XHNM6GN5\\Liang et al. - Towards Understanding and Mitigating Social Biases.pdf:application/pdf},
}

@inproceedings{goldfarb-tarrant_intrinsic_2021,
	address = {Online},
	title = {Intrinsic {Bias} {Metrics} {Do} {Not} {Correlate} with {Application} {Bias}},
	url = {https://aclanthology.org/2021.acl-long.150},
	doi = {10.18653/v1/2021.acl-long.150},
	language = {en},
	urldate = {2024-03-19},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Goldfarb-Tarrant, Seraphina and Marchant, Rebecca and Muñoz Sánchez, Ricardo and Pandya, Mugdha and Lopez, Adam},
	year = {2021},
	keywords = {ToRead, DownstreamBias},
	pages = {1926--1940},
	file = {Goldfarb-Tarrant et al. - 2021 - Intrinsic Bias Metrics Do Not Correlate with Appli.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\URHBIM3P\\Goldfarb-Tarrant et al. - 2021 - Intrinsic Bias Metrics Do Not Correlate with Appli.pdf:application/pdf},
}

@inproceedings{sesari_empirical_2022,
	address = {Seattle, Washington},
	title = {An {Empirical} {Study} on the {Fairness} of {Pre}-trained {Word} {Embeddings}},
	url = {https://aclanthology.org/2022.gebnlp-1.15},
	doi = {10.18653/v1/2022.gebnlp-1.15},
	language = {en},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the 4th {Workshop} on {Gender} {Bias} in {Natural} {Language} {Processing} ({GeBNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Sesari, Emeralda and Hort, Max and Sarro, Federica},
	year = {2022},
	keywords = {WordEmbeddings},
	pages = {129--144},
	file = {Sesari et al. - 2022 - An Empirical Study on the Fairness of Pre-trained .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\YRSMAYCZ\\Sesari et al. - 2022 - An Empirical Study on the Fairness of Pre-trained .pdf:application/pdf},
}

@misc{zhao_gptbias_2023,
	title = {{GPTBIAS}: {A} {Comprehensive} {Framework} for {Evaluating} {Bias} in {Large} {Language} {Models}},
	shorttitle = {{GPTBIAS}},
	url = {http://arxiv.org/abs/2312.06315},

	language = {en},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Zhao, Jiaxu and Fang, Meng and Pan, Shirui and Yin, Wenpeng and Pechenizkiy, Mykola},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06315 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Computation and Language},
	file = {Zhao et al. - 2023 - GPTBIAS A Comprehensive Framework for Evaluating .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\Y9DXERNR\\Zhao et al. - 2023 - GPTBIAS A Comprehensive Framework for Evaluating .pdf:application/pdf},
}

@article{mehrabi_survey_2022,
	title = {A {Survey} on {Bias} and {Fairness} in {Machine} {Learning}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3457607},
	doi = {10.1145/3457607},
	language = {en},
	number = {6},
	urldate = {2024-03-26},
	journal = {ACM Computing Surveys},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	month = jul,
	year = {2022},
	keywords = {ToRead, FairnessDefinition, BiasDefinition},
	pages = {1--35},
	file = {Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\WED3HQNL\\Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf:application/pdf},
}
@article{fiske2007universal,
  title={Universal dimensions of social cognition: Warmth and competence},
  author={Fiske, Susan T and Cuddy, Amy JC and Glick, Peter},
  journal={Trends in cognitive sciences},
  volume={11},
  number={2},
  pages={77--83},
  year={2007},
  publisher={Elsevier}
}

@article{cohen1960coefficient,
  title={A coefficient of agreement for nominal scales},
  author={Cohen, Jacob},
  journal={Educational and psychological measurement},
  volume={20},
  number={1},
  pages={37--46},
  year={1960},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{mock2024developing,
  title={Developing trustworthy AI applications with foundation models},
  author={Mock, Michael and Schmidt, Sebastian and M{\"u}ller, Felix and G{\"o}rge, Rebekka and Schmitz, Anna and Haedecke, Elena and Voss, Angelika and Hecker, Dirk and Poretschkin, Maximillian},
  journal={arXiv preprint arXiv:2405.04937},
  year={2024}
}


@article{Salih_2024,
   title={A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME},
   volume={7},
   ISSN={2640-4567},
   url={http://dx.doi.org/10.1002/aisy.202400304},
   DOI={10.1002/aisy.202400304},
   number={1},
   journal={Advanced Intelligent Systems},
   publisher={Wiley},
   author={Salih, Ahmed M. and Raisi‐Estabragh, Zahra and Galazzo, Ilaria Boscolo and Radeva, Petia and Petersen, Steffen E. and Lekadir, Karim and Menegaz, Gloria},
   year={2024},
   month=jun }

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@incollection{stephan1993cognition,
  title={Cognition and affect in stereotyping: Parallel interactive networks},
  author={Stephan, Walter G and Stephan, Cookie White},
  booktitle={Affect, cognition and stereotyping},
  pages={111--136},
  year={1993},
  publisher={Elsevier}
}


@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@book{louviere2015best,
  title={Best-worst scaling: Theory, methods and applications},
  author={Louviere, Jordan J and Flynn, Terry N and Marley, Anthony Alfred John},
  year={2015},
  publisher={Cambridge University Press}
}

@article{maystre2015fast,
  title={Fast and accurate inference of Plackett--Luce models},
  author={Maystre, Lucas and Grossglauser, Matthias},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}



@inproceedings{suresh_framework_2021,
	address = {-- NY USA},
	title = {A {Framework} for {Understanding} {Sources} of {Harm} throughout the {Machine} {Learning} {Life} {Cycle}},
	isbn = {978-1-4503-8553-4},
	url = {https://dl.acm.org/doi/10.1145/3465416.3483305},
	doi = {10.1145/3465416.3483305},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Suresh, Harini and Guttag, John},
	month = oct,
	year = {2021},
	keywords = {ToRead, FairnessDefinition, BiasDefinition},
	pages = {1--9},
	file = {Suresh und Guttag - 2021 - A Framework for Understanding Sources of Harm thro.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\LYJLGRRP\\Suresh und Guttag - 2021 - A Framework for Understanding Sources of Harm thro.pdf:application/pdf},
}

@inproceedings{hutchinson_50_2019,
	address = {Atlanta GA USA},
	title = {50 {Years} of {Test} ({Un})fairness: {Lessons} for {Machine} {Learning}},
	isbn = {978-1-4503-6125-5},
	shorttitle = {50 {Years} of {Test} ({Un})fairness},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287600},
	doi = {10.1145/3287560.3287600},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Hutchinson, Ben and Mitchell, Margaret},
	month = jan,
	year = {2019},
	pages = {49--58},
	file = {Hutchinson und Mitchell - 2019 - 50 Years of Test (Un)fairness Lessons for Machine.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\47HEY3SL\\Hutchinson und Mitchell - 2019 - 50 Years of Test (Un)fairness Lessons for Machine.pdf:application/pdf},
}

@inproceedings{rogers_changing_2021,
	address = {Online},
	title = {Changing the {World} by {Changing} the {Data}},
	url = {https://aclanthology.org/2021.acl-long.170},
	doi = {10.18653/v1/2021.acl-long.170},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rogers, Anna},
	year = {2021},
	pages = {2182--2194},
	file = {Rogers - 2021 - Changing the World by Changing the Data.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\SEQC8USB\\Rogers - 2021 - Changing the World by Changing the Data.pdf:application/pdf},
}

@article{shahbazi_representation_2023,
	title = {Representation {Bias} in {Data}: {A} {Survey} on {Identification} and {Resolution} {Techniques}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Representation {Bias} in {Data}},
	url = {https://dl.acm.org/doi/10.1145/3588433},
	doi = {10.1145/3588433},
	
	language = {en},
	number = {13s},
	urldate = {2024-04-19},
	journal = {ACM Computing Surveys},
	author = {Shahbazi, Nima and Lin, Yin and Asudeh, Abolfazl and Jagadish, H. V.},
	month = dec,
	year = {2023},
	pages = {1--39},
	file = {Shahbazi et al. - 2023 - Representation Bias in Data A Survey on Identific.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\5AU4DW69\\Shahbazi et al. - 2023 - Representation Bias in Data A Survey on Identific.pdf:application/pdf},
}

@inproceedings{schmahl_is_2020,
	address = {Online},
	title = {Is {Wikipedia} succeeding in reducing gender bias? {Assessing} changes in gender bias in {Wikipedia} using word embeddings},
	shorttitle = {Is {Wikipedia} succeeding in reducing gender bias?},
	url = {https://www.aclweb.org/anthology/2020.nlpcss-1.11},
	doi = {10.18653/v1/2020.nlpcss-1.11},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Natural} {Language} {Processing} and {Computational} {Social} {Science}},
	publisher = {Association for Computational Linguistics},
	author = {Schmahl, Katja Geertruida and Viering, Tom Julian and Makrodimitris, Stavros and Naseri Jahfari, Arman and Tax, David and Loog, Marco},
	year = {2020},
	keywords = {DataBias},
	pages = {94--103},
	file = {Schmahl et al. - 2020 - Is Wikipedia succeeding in reducing gender bias A.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\TIUGJIIQ\\Schmahl et al. - 2020 - Is Wikipedia succeeding in reducing gender bias A.pdf:application/pdf},
}

@article{jones_stereotypical_2020,
	title = {Stereotypical {Gender} {Associations} in {Language} {Have} {Decreased} {Over} {Time}},
	volume = {7},
	issn = {23306696},
	url = {https://www.sociologicalscience.com/articles-v7-1-1/},
	doi = {10.15195/v7.a1},
	language = {en},
	urldate = {2024-04-19},
	journal = {Sociological Science},
	author = {Jones, Jason and Amin, Mohammad and Kim, Jessica and Skiena, Steven},
	year = {2020},
	pages = {1--35},
	file = {Jones et al. - 2020 - Stereotypical Gender Associations in Language Have.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\U8KYN7ZJ\\Jones et al. - 2020 - Stereotypical Gender Associations in Language Have.pdf:application/pdf},
}

@article{noauthor_overview_nodate,
	title = {An {Overview} of {Fairness} in {Data} – {Illuminating} the {Bias} in {Data} {Pipeline}},
	language = {en},
	keywords = {DataBias},
	file = {An Overview of Fairness in Data – Illuminating the.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\BFH4MNPL\\An Overview of Fairness in Data – Illuminating the.pdf:application/pdf},
}

@inproceedings{dixon_measuring_2018,
	address = {New Orleans LA USA},
	title = {Measuring and {Mitigating} {Unintended} {Bias} in {Text} {Classification}},
	isbn = {978-1-4503-6012-8},
	url = {https://dl.acm.org/doi/10.1145/3278721.3278729},
	doi = {10.1145/3278721.3278729},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
	month = dec,
	year = {2018},
	pages = {67--73},
	file = {2020.aacl-main.76.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\BX7XRBMV\\2020.aacl-main.76.pdf:application/pdf;Dixon et al. - 2018 - Measuring and Mitigating Unintended Bias in Text C.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\GIHYLSAV\\Dixon et al. - 2018 - Measuring and Mitigating Unintended Bias in Text C.pdf:application/pdf},
}

@article{wagner_its_2015,
	title = {It's a {Man}'s {Wikipedia}? {Assessing} {Gender} {Inequality} in an {Online} {Encyclopedia}},
	volume = {9},
	copyright = {Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media},
	issn = {2334-0770},
	shorttitle = {It's a {Man}'s {Wikipedia}?},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14628},
	doi = {10.1609/icwsm.v9i1.14628},
	language = {en},
	number = {1},
	urldate = {2024-04-24},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Wagner, Claudia and Garcia, David and Jadidi, Mohsen and Strohmaier, Markus},
	year = {2015},
	note = {Number: 1},
	keywords = {encyclopedia},
	pages = {454--463},
	file = {Full Text PDF:C\:\\Users\\rgoerge\\Zotero\\storage\\7XU37WCG\\Wagner et al. - 2015 - It's a Man's Wikipedia Assessing Gender Inequalit.pdf:application/pdf},
}

@inproceedings{valentini_interpretability_2023,
	address = {Toronto, Canada},
	title = {On the {Interpretability} and {Significance} of {Bias} {Metrics} in {Texts}: a {PMI}-based {Approach}},
	shorttitle = {On the {Interpretability} and {Significance} of {Bias} {Metrics} in {Texts}},
	url = {https://aclanthology.org/2023.acl-short.44},
	doi = {10.18653/v1/2023.acl-short.44},
	language = {en},
	urldate = {2024-04-24},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Valentini, Francisco and Rosati, Germán and Blasi, Damián and Fernandez Slezak, Diego and Altszyler, Edgar},
	year = {2023},
	pages = {509--520},
	file = {Valentini et al. - 2023 - On the Interpretability and Significance of Bias M.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\NE2I9YK4\\Valentini et al. - 2023 - On the Interpretability and Significance of Bias M.pdf:application/pdf},
}

@inproceedings{guo_detecting_2021,
	address = {Virtual Event USA},
	title = {Detecting {Emergent} {Intersectional} {Biases}: {Contextualized} {Word} {Embeddings} {Contain} a {Distribution} of {Human}-like {Biases}},
	isbn = {978-1-4503-8473-5},
	shorttitle = {Detecting {Emergent} {Intersectional} {Biases}},
	url = {https://dl.acm.org/doi/10.1145/3461702.3462536},
	doi = {10.1145/3461702.3462536},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Guo, Wei and Caliskan, Aylin},
	month = jul,
	year = {2021},
	pages = {122--133},
	file = {Guo und Caliskan - 2021 - Detecting Emergent Intersectional Biases Contextu.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\CT3RGCVU\\Guo und Caliskan - 2021 - Detecting Emergent Intersectional Biases Contextu.pdf:application/pdf},
}

@article{raza_nbias_2024,
	title = {Nbias: {A} natural language processing framework for {BIAS} identification in text},
	volume = {237},
	issn = {09574174},
	shorttitle = {Nbias},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417423020444},
	doi = {10.1016/j.eswa.2023.121542},
	language = {en},
	urldate = {2024-05-16},
	journal = {Expert Systems with Applications},
	author = {Raza, Shaina and Garg, Muskan and Reji, Deepak John and Bashir, Syed Raza and Ding, Chen},
	month = mar,
	year = {2024},
	pages = {121542},
	file = {Raza et al. - 2024 - Nbias A natural language processing framework for.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\M9EN9CK9\\Raza et al. - 2024 - Nbias A natural language processing framework for.pdf:application/pdf},
}

@inproceedings{spliethover_bias_2021,
	address = {Montreal, Canada},
	title = {Bias {Silhouette} {Analysis}: {Towards} {Assessing} the {Quality} of {Bias} {Metrics} for {Word} {Embedding} {Models}},
	isbn = {978-0-9992411-9-6},
	shorttitle = {Bias {Silhouette} {Analysis}},
	url = {https://www.ijcai.org/proceedings/2021/77},
	doi = {10.24963/ijcai.2021/77},
	language = {en},
	urldate = {2024-05-23},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Spliethöver, Maximilian and Wachsmuth, Henning},
	month = aug,
	year = {2021},
	pages = {552--559},
	file = {Spliethöver und Wachsmuth - 2021 - Bias Silhouette Analysis Towards Assessing the Qu.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\YCTLHJ8X\\Spliethöver und Wachsmuth - 2021 - Bias Silhouette Analysis Towards Assessing the Qu.pdf:application/pdf},
}

@inproceedings{du_assessing_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Assessing the {Reliability} of {Word} {Embedding} {Gender} {Bias} {Measures}},
	url = {https://aclanthology.org/2021.emnlp-main.785},
	doi = {10.18653/v1/2021.emnlp-main.785},
	language = {en},
	urldate = {2024-06-04},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Du, Yupei and Fang, Qixiang and Nguyen, Dong},
	year = {2021},
	pages = {10012--10034},
	file = {Du et al. - 2021 - Assessing the Reliability of Word Embedding Gender.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\TMHRJFS3\\Du et al. - 2021 - Assessing the Reliability of Word Embedding Gender.pdf:application/pdf},
}

@article{zhang_robustness_nodate,
	title = {Robustness and {Reliability} of {Gender} {Bias} {Assessment} in {Word} {Embeddings}: {The} {Role} of {Base} {Pairs}},

	language = {en},
	author = {Zhang, Haiyang and Sneyd, Alison and Stevenson, Mark},
	keywords = {Stability},
	file = {Zhang et al. - Robustness and Reliability of Gender Bias Assessme.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\GHFC7KYC\\Zhang et al. - Robustness and Reliability of Gender Bias Assessme.pdf:application/pdf},
}

@article{ding_is_nodate,
	title = {Is {GPT}-3 a {Good} {Data} {Annotator}?},
	language = {en},
	author = {Ding, Bosheng and Qin, Chengwei and Liu, Linlin and Chia, Yew Ken and Li, Boyang and Joty, Shafiq and Bing, Lidong},
	keywords = {GPTLabelling},
	file = {Ding et al. - Is GPT-3 a Good Data Annotator.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\QN3A2FGU\\Ding et al. - Is GPT-3 a Good Data Annotator.pdf:application/pdf},
}

@inproceedings{luccioni_whats_2021,
	address = {Online},
	title = {What’s in the {Box}? {An} {Analysis} of {Undesirable} {Content} in the {Common} {Crawl} {Corpus}},
	shorttitle = {What’s in the {Box}?},
	url = {https://aclanthology.org/2021.acl-short.24},
	doi = {10.18653/v1/2021.acl-short.24},
	language = {en},
	urldate = {2024-06-06},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Luccioni, Alexandra and Viviano, Joseph},
	year = {2021},
	keywords = {DataBias, CommonCrawl},
	pages = {182--189},
	file = {Luccioni und Viviano - 2021 - What’s in the Box An Analysis of Undesirable Cont.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\9H8W7H66\\Luccioni und Viviano - 2021 - What’s in the Box An Analysis of Undesirable Cont.pdf:application/pdf},
}

@inproceedings{baack_critical_2024,
	address = {Rio de Janeiro Brazil},
	title = {A {Critical} {Analysis} of the {Largest} {Source} for {Generative} {AI} {Training} {Data}: {Common} {Crawl}},
	isbn = {9798400704505},
	shorttitle = {A {Critical} {Analysis} of the {Largest} {Source} for {Generative} {AI} {Training} {Data}},
	url = {https://dl.acm.org/doi/10.1145/3630106.3659033},
	doi = {10.1145/3630106.3659033},
	language = {en},
	urldate = {2024-06-06},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Baack, Stefan},
	month = jun,
	year = {2024},
	keywords = {Motivation, CommonCrawl},
	pages = {2199--2208},
	file = {Baack - 2024 - A Critical Analysis of the Largest Source for Gene.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\G6NDSPN2\\Baack - 2024 - A Critical Analysis of the Largest Source for Gene.pdf:application/pdf},
}

@inproceedings{hube_detecting_2018,
	address = {Lyon, France},
	title = {Detecting {Biased} {Statements} in {Wikipedia}},
	copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
	isbn = {978-1-4503-5640-4},
	url = {http://dl.acm.org/citation.cfm?doid=3184558.3191640},
	doi = {10.1145/3184558.3191640},
	language = {en},
	urldate = {2024-06-06},
	booktitle = {Companion of the {The} {Web} {Conference} 2018 on {The} {Web} {Conference} 2018 - {WWW} '18},
	publisher = {ACM Press},
	author = {Hube, Christoph and Fetahu, Besnik},
	year = {2018},
	keywords = {Dataset},
	pages = {1779--1786},
}



@misc{gallegos_self-debiasing_2024,
	title = {Self-{Debiasing} {Large} {Language} {Models}: {Zero}-{Shot} {Recognition} and {Reduction} of {Stereotypes}},
	shorttitle = {Self-{Debiasing} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.01981},
	language = {en},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Yu, Tong and Deilamsalehy, Hanieh and Zhang, Ruiyi and Kim, Sungchul and Dernoncourt, Franck},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01981 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Computation and Language, ToRead, stereotypes},
	file = {2402.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\R8SKKGHI\\2402.pdf:application/pdf},
}



@article{schmeisser-nieto_criteria_nodate,
	title = {Criteria for the {Annotation} of {Implicit} {Stereotypes}},
	language = {en},
	author = {Schmeisser-Nieto, Wolfgang and Nofre, Montserrat and Taulé, Mariona},
	keywords = {stereotypes},
	file = {Schmeisser-Nieto et al. - Criteria for the Annotation of Implicit Stereotype.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\ZQXBWZS9\\Schmeisser-Nieto et al. - Criteria for the Annotation of Implicit Stereotype.pdf:application/pdf},
}

@article{burgers_how_2020,
	title = {How {Language} {Contributes} to {Stereotype} {Formation}: {Combined} {Effects} of {Label} {Types} and {Negation} {Use} in {Behavior} {Descriptions}},
	volume = {39},
	issn = {0261-927X, 1552-6526},
	shorttitle = {How {Language} {Contributes} to {Stereotype} {Formation}},
	url = {http://journals.sagepub.com/doi/10.1177/0261927X20933320},
	doi = {10.1177/0261927X20933320},
	language = {en},
	number = {4},
	urldate = {2024-07-03},
	journal = {Journal of Language and Social Psychology},
	author = {Burgers, Christian and Beukeboom, Camiel J.},
	month = sep,
	year = {2020},
	keywords = {stereotypes, linguistics},
	pages = {438--456},
	file = {Burgers und Beukeboom - 2020 - How Language Contributes to Stereotype Formation .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\IB6P5MMY\\Burgers und Beukeboom - 2020 - How Language Contributes to Stereotype Formation .pdf:application/pdf},
}

@article{beukeboom2019stereotypes,
  title={How stereotypes are shared through language: a review and introduction of the aocial categories and stereotypes communication (SCSC) framework},
  author={Beukeboom, Camiel J and Burgers, Christian},
  journal={Review of Communication Research},
  volume={7},
  pages={1--37},
  year={2019},
  publisher={ESP}
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{son2024multitaskinferencelargelanguage,
      title={Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?}, 
      author={Guijin Son and Sangwon Baek and Sangdae Nam and Ilgyun Jeong and Seungone Kim},
      year={2024},
      eprint={2402.11597},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11597}, 
}

@inproceedings{sahoo_detecting_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Detecting {Unintended} {Social} {Bias} in {Toxic} {Language} {Datasets}},
	url = {https://aclanthology.org/2022.conll-1.10},
	doi = {10.18653/v1/2022.conll-1.10},
	abstract = {Warning: This paper has contents which may be offensive, or upsetting however this cannot be avoided owing to the nature of the work.},
	language = {en},
	urldate = {2024-07-08},
	booktitle = {Proceedings of the 26th {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {Sahoo, Nihar and Gupta, Himanshu and Bhattacharyya, Pushpak},
	year = {2022},
	pages = {132--143},
	file = {Sahoo et al. - 2022 - Detecting Unintended Social Bias in Toxic Language.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\F662V5UE\\Sahoo et al. - 2022 - Detecting Unintended Social Bias in Toxic Language.pdf:application/pdf},
}

@article{wiegand_detection_nodate,
	title = {Detection of {Abusive} {Language}: the {Problem} of {Biased} {Datasets}},
	language = {en},
	author = {Wiegand, Michael and Ruppenhofer, Josef and Kleinbauer, Thomas},
	file = {Wiegand et al. - Detection of Abusive Language the Problem of Bias.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\SJYAZGWN\\Wiegand et al. - Detection of Abusive Language the Problem of Bias.pdf:application/pdf},
}

@inproceedings{wang_decodingtrust_2024,
  title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  booktitle={NeurIPS},
  year={2023}
}


@inproceedings{
wu2023towards,
title={Towards Auditing Large Language Models: Improving Text-based Stereotype Detection},
author={Zekun Wu and Sahan Bulathwela and Adriano Koshiyama},
booktitle={Socially Responsible Language Modelling Research},
year={2023},
url={https://openreview.net/forum?id=Wu975eZkO3}
}

@article{huang2024trustllm,
  title={Trustllm: Trustworthiness in large language models},
  author={Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and others},
  journal={arXiv preprint arXiv:2401.05561},
  year={2024}
}



@article{nagireddy_socialstigmaqa_2024,
	title = {{SocialStigmaQA}: {A} {Benchmark} to {Uncover} {Stigma} {Amplification} in {Generative} {Language} {Models}},
	volume = {38},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{SocialStigmaQA}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/30142},
	doi = {10.1609/aaai.v38i19.30142},
	language = {en},
	number = {19},
	urldate = {2024-07-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Nagireddy, Manish and Chiazor, Lamogha and Singh, Moninder and Baldini, Ioana},
	month = mar,
	year = {2024},
	keywords = {Dataset},
	pages = {21454--21462},
	file = {Nagireddy et al. - 2024 - SocialStigmaQA A Benchmark to Uncover Stigma Ampl.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\W9R746RM\\Nagireddy et al. - 2024 - SocialStigmaQA A Benchmark to Uncover Stigma Ampl.pdf:application/pdf},
}



@misc{thakur_judging_2024,
	title = {Judging the {Judges}: {Evaluating} {Alignment} and {Vulnerabilities} in {LLMs}-as-{Judges}},
	shorttitle = {Judging the {Judges}},
	url = {http://arxiv.org/abs/2406.12624},
	language = {en},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Thakur, Aman Singh and Choudhary, Kartik and Ramayapally, Venkat Srinik and Vaidyanathan, Sankaran and Hupkes, Dieuwke},
	month = jul,
	year = {2024},
	note = {arXiv:2406.12624 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, InContextLearning},
	file = {Thakur et al. - 2024 - Judging the Judges Evaluating Alignment and Vulne.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\9QW8FUGE\\Thakur et al. - 2024 - Judging the Judges Evaluating Alignment and Vulne.pdf:application/pdf},
}

@inproceedings{mun_beyond_2023,
	address = {Singapore},
	title = {Beyond {Denouncing} {Hate}: {Strategies} for {Countering} {Implied} {Biases} and {Stereotypes} in {Language}},
	shorttitle = {Beyond {Denouncing} {Hate}},
	url = {https://aclanthology.org/2023.findings-emnlp.653},
	doi = {10.18653/v1/2023.findings-emnlp.653},
	language = {en},
	urldate = {2024-07-19},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Mun, Jimin and Allaway, Emily and Yerukola, Akhila and Vianna, Laura and Leslie, Sarah-Jane and Sap, Maarten},
	year = {2023},
	pages = {9759--9777},
	file = {Mun et al. - 2023 - Beyond Denouncing Hate Strategies for Countering .pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\I9FQ9T9I\\Mun et al. - 2023 - Beyond Denouncing Hate Strategies for Countering .pdf:application/pdf},
}

@article{shrawgi_uncovering_nodate,
	title = {Uncovering {Stereotypes} in {Large} {Language} {Models}: {A} {Task} {Complexity}-based {Approach}},
	language = {en},
	author = {Shrawgi, Hari and Rath, Prasanjit and Singhal, Tushar and Dandapat, Sandipan},
	keywords = {Dataset, Motivation},
	file = {Shrawgi et al. - Uncovering Stereotypes in Large Language Models A.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\LNHHI65U\\Shrawgi et al. - Uncovering Stereotypes in Large Language Models A.pdf:application/pdf},
}

@article{donald_bias_2023,
	title = {Bias {Detection} for {Customer} {Interaction} {Data}: {A} {Survey} on {Datasets}, {Methods}, and {Tools}},
	volume = {11},
	issn = {2169-3536},
	shorttitle = {Bias {Detection} for {Customer} {Interaction} {Data}},
	url = {https://ieeexplore.ieee.org/document/10126086/?arnumber=10126086&tag=1},
	doi = {10.1109/ACCESS.2023.3276757},
	
	urldate = {2024-09-12},
	journal = {IEEE Access},
	author = {Donald, Andy and Galanopoulos, Apostolos and Curry, Edward and Muñoz, Emir and Ullah, Ihsan and Waskow, M. A. and Dabrowski, Maciej and Kalra, Manan},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Survey},
	pages = {53703--53715},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\rgoerge\\Zotero\\storage\\SH5KLLVA\\10126086.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\rgoerge\\Zotero\\storage\\E94Y9V59\\Donald et al. - 2023 - Bias Detection for Customer Interaction Data A Su.pdf:application/pdf},
}

@inproceedings{shen_words_2023,
	address = {Cham},
	title = {Words {Can} {Be} {Confusing}: {Stereotype} {Bias} {Removal} in {Text} {Classification} at the {Word} {Level}},
	isbn = {978-3-031-33383-5},
	shorttitle = {Words {Can} {Be} {Confusing}},
	doi = {10.1007/978-3-031-33383-5_8},
	language = {en},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer Nature Switzerland},
	author = {Shen, Shaofei and Zhang, Mingzhe and Chen, Weitong and Bialkowski, Alina and Xu, Miao},
	editor = {Kashima, Hisashi and Ide, Tsuyoshi and Peng, Wen-Chih},
	year = {2023},
	keywords = {MitigationMethod, stereotypes},
	pages = {99--111},
	file = {Full Text PDF:C\:\\Users\\rgoerge\\Zotero\\storage\\RUUYJYTW\\Shen et al. - 2023 - Words Can Be Confusing Stereotype Bias Removal in.pdf:application/pdf},
}



@misc{bai_measuring_2024,
	title = {Measuring {Implicit} {Bias} in {Explicitly} {Unbiased} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.04105},
	abstract = {Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both challenges by introducing two new measures of bias: LLM Implicit Bias, a promptbased method for revealing implicit bias; and LLM Decision Bias, a strategy to detect subtle discrimination in decision-making tasks. Both measures are based on psychological research: LLM Implicit Bias adapts the Implicit Association Test, widely used to study the automatic associations between concepts held in human minds; and LLM Decision Bias operationalizes psychological results indicating that relative evaluations between two candidates, not absolute evaluations assessing each independently, are more diagnostic of implicit biases. Using these measures, we found pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes (such as race and criminality, race and weapons, gender and science, age and negativity). Our prompt-based LLM Implicit Bias measure correlates with existing language model embedding-based bias methods, but better predicts downstream behaviors measured by LLM Decision Bias. These new prompt-based measures draw from psychology’s long history of research into measuring stereotype biases based on purely observable behavior; they expose nuanced biases in proprietary value-aligned LLMs that appear unbiased according to standard benchmarks.},
	language = {en},
	urldate = {2024-09-12},
	publisher = {arXiv},
	author = {Bai, Xuechunzi and Wang, Angelina and Sucholutsky, Ilia and Griffiths, Thomas L.},
	month = may,
	year = {2024},
	note = {arXiv:2402.04105 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Computation and Language, ToRead, BiasModelEvaluation, ImplicitBias, Evaluation of Downstreambias, R3},
	file = {Bai et al. - 2024 - Measuring Implicit Bias in Explicitly Unbiased Lar.pdf:C\:\\Users\\rgoerge\\Zotero\\storage\\ZFW32DWJ\\Bai et al. - 2024 - Measuring Implicit Bias in Explicitly Unbiased Lar.pdf:application/pdf},
}

@inproceedings{wadhwa2023revisiting,
  title={Revisiting relation extraction in the era of large language models},
  author={Wadhwa, Somin and Amir, Silvio and Wallace, Byron C},
  booktitle={Proceedings of the conference. Association for Computational Linguistics. Meeting},
  volume={2023},
  pages={15566},
  year={2023},
  organization={NIH Public Access}
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
