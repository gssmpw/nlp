\documentclass[sigconf, nonacm]{acmart}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{none}
\usepackage{array}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow} 
\usepackage[inkscapelatex=true]{svg}
\usepackage[utf8]{inputenc}
\usepackage{tcolorbox}
\usepackage{rotating} 
\usepackage{lscape}

\begin{document}

\title{Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models}

\author{Rebekka Görge}
\email{rebekka.goerge@iais.fraunhofer.de}
\affiliation{%
  \institution{Fraunhofer IAIS \& Lamarr}
  \country{Germany}
}

\author{Michael Mock }
\email{michael.mock@iais.fraunhofer.de}
\affiliation{%
  \institution{Fraunhofer IAIS}
  \country{Germany}
}
\author{Héctor Allende-Cid}
\email{hector.allende-cid@iais.fraunhofer.de}
\affiliation{
  \institution{Fraunhofer IAIS \& Lamarr}
  \country{Germany}
}

\begin{abstract}

Social categories and stereotypes are embedded in language and can introduce data bias into the training of Large Language Models (LLMs). Despite safeguards, these biases often persist in model behavior, potentially leading to representational harm in outputs. 
While sociolinguistic research provides valuable insights the into formation and spread of stereotypes, NLP approaches for stereotype detection rarely draw on this foundation and often lack objectivity, precision, and interpretability.
To fill this gap, in this work we propose a new approach that detects and quantifies the linguistic indicators of stereotypes in a given sentence.  
For this, we derive linguistic indicators from the Social Category and Stereotype Communication (SCSC) framework which indicate strong social category formulation and stereotyping in human language, and use them to build a categorization scheme.
To automate this approach, we instruct different LLMs using in-context learning to apply the approach to a given sentence, where the language model examines the linguistic properties of a sentence and provides a basis for a fine-grained stereotype assessment.
Based on an empirical evaluation of the importance of different linguistic indicators, we learn a scoring function that measures the linguistic indicators of a stereotype.
Our annotations of stereotyped sentences show that these linguistic indicators are present in these sentences and explain the strength of a stereotype using the scoring function.
%, which partially aligns with human stereotype ranking. 
In terms of model performance, our results show that the models generally perform well in detecting and classifying linguistic indicators of category labels used to denote a category, but sometimes struggle to correctly evaluate the associated described behaviors and features.
However, using more few-shot examples within the prompts, significantly improves performance. 
Furthermore, model performance increases with size, as \texttt{Llama-3.3-70B-Instruct} and \texttt{GPT-4} achieve comparable results that surpass those of \texttt{Mixtral-8x7B-Instruct}, \texttt{GPT-4-mini} and \texttt{Llama-3.1-8B-Instruct\_4bit}.
\end{abstract}

\keywords{Large language models, fairness, stereotype detection, linguistics}
\maketitle
\section{Introduction}
\label{section:Introduction}
\textit{Content Warning: This paper presents textual examples that may be offensive or upsetting.}

Language reflects and transmits the social categories and stereotypes that humans use to quickly perceive and interact within their complex social environment. \textit{Stereotypes} are defined as  ``cognitive representation people hold about a social category consisting of beliefs and expectancies about their probable behavior, feature and traits'' \cite{dovidio2010sage}. Although social categorization is a fundamental human tendency \cite{beukeboom2019stereotypes}, it often leads to oversimplified perceptions of groups, exaggerating similarities within the group, while amplifying differences with those outside. 
Harmful consequences, such as discrimination or unfair decisions, arise when individuals are judged based on broad social category associations rather than their unique traits. 
This might even be enforced by the use of negative stereotypes, which are denoted as prejudice\footnote{``Negative affective evaluations of social categories and members'' are denoted as prejudice \cite{beukeboom2019stereotypes, stephan1993cognition}}. 
Encoded in human language, also large language models (LLMs) trained on massive amount of aggregated and crawled text data are learning, reproducing and disseminating stereotypes \cite{navigli_biases_2023}. This perpetuation of stereotypes and its potential harmful impact on society and individuals is a significant concern regarding AI \cite{maslej2024ai}. 

Current research investigates in the detection and mitigation of stereotypes as part of the research on fairness and bias\footnote{Definition according to \cite{isoiec24027:2021}: ``Systematic difference in treatment of certain objects, people, or groups in comparison to others''} of AI models \cite{gallegos_bias_2023}.
Within the context of bias, the presence of stereotypes in training data constitutes a form of data bias\footnote{Definition according to \cite{isoiec24027:2021}: ``Data properties that, if unaddressed, lead to AI systems that perform better or worse for different groups''}.
Likewise, the reproduction of stereotypes by AI models is seen as a harmful consequence of bias, often referred to as representational harm \cite{gallegos_bias_2023}.
 To mitigate representational harm, state-of-the-art (SOTA) LLMs are equipped with guardrails to prevent stereotypical output. Although these measures are often effective against explicit reproduction of stereotypes, they often fail when confronted with slight variations in the prompting \cite{wang_decodingtrust_2024}.
 This indicates that intrinsic biases continue to manifest in model behavior, highlighting the need to reduce stereotypes as much as possible at the data level.
Also sociolinguistics has long examined the emergence and linguistic form of stereotypes, \citeauthor{blodgett_language_2020} finds that most research on bias in natural language processing (NLP) is poorly aligned with interdisciplinary studies. Existing work in stereotype detection primarily relies on human-constructed and annotated benchmarking datasets \cite{nangia_crows-pairs_2020, nadeem2021stereoset, fleisig_fairprism_2023, barikeri-etal-2021-redditbias}, which serve as the foundation for training classifiers for text-based stereotype detection and for benchmarking widely used LLMs. These datasets and resulting detection methods typically categorize sentences as either stereotypical or anti-stereotypical based on subjective human assessments, leading to pitfalls such as misalignment or uncleanness of the stereotypes themselves \cite{blodgett_language_2020}.
To address this issue, \citeauthor{liu-2024-quantifying} argues that a purely binary separation of stereotypes is insufficient, as stereotypes can be expressed in various forms. Instead, they propose a fine-grained quantification of the strength of a stereotype using a continuous scoring system grounded in human stereotype rankings \cite{liu-2024-quantifying}. However, as the score relies on human judgment, it reflects subjective perceptions shaped by the annotators' cultural and social backgrounds, and it lacks an explanation for why a stereotypical sentence contains a specific stereotype. 

Our work starts at this point, as this paper presents a novel, sociolinguistically based, twofold approach to quantifying stereotypes in language.  
Unlike \cite{liu-2024-quantifying}, we do not assess the harmfulness of a stereotype based on human judgment, but instead focus on firstly detecting and secondly quantifying linguistic indicators that signal a stereotype.  
Our approach is illustrated in Figure \ref{fig:introduction}. 
By grounding our work in sociolinguistics, we adopt the Social Category and Stereotype Communication (SCSC) Framework  \cite{beukeboom2019stereotypes} (Section \ref{section:SCSC_framework}), which explicates the linguistic processes by which stereotypes are shared and maintained in language, and use it to derive a clear categorization scheme with a fixed set of linguistic indicators (Section \ref{section:methodolgoy}).
Leveraging their extensive linguistic capabilities, we integrate LLMs to automatically detect linguistic indicators by guiding them through the categorization scheme using an in-context learning approach (Section \ref{section:model}).  
In order to quantify the linguistic indication on a stereotype, we aggregate the derived linguistic indicators by learning a scoring function, for which we exploit the work of \citeauthor{liu-2024-quantifying} and learn the importance of different linguistic indicators based on human stereotype rankings (Section \ref{section:evaluation}).
We validate our approach using a manually annotated subsample of CrowS-Pairs\cite{nangia_crows-pairs_2020} and the human-based stereotype rankings of \cite{liu-2024-quantifying}. 

\begin{figure*}
    \includegraphics[width=0.8\textwidth]{img/2025_02_26_introduction_picture.png}
    \caption{Overview of our framework for assessing linguistic indicators of stereotypes using the SCSC framework and leveraging LLMs}
    \label{fig:introduction}
\end{figure*}




\section{Related work}
As introduced above, research on stereotyping in NLP is an integral part of the wider investigation into fairness and bias in artificial intelligence (AI).
According to \citeauthor{Hovy2021}, the key sources of bias in NLP are the data used to train models and the models themselves.
While biased data reflects societal stereotypes and inequities, model architectures and training processes can amplify or introduce additional biases, compounding the problem.
Stereotypes in language systems intersect with social hierarchies and human cognition, highlighting the need to engage with foundational literature from psychology, sociology, and sociolinguistics to understand stereotype formation, its harms, and to guide the development of more equitable NLP methodologies.
While, few works in NLP build up on this foundation \cite{blodgett_language_2020}, numerous studies have shown that bias, including stereotyping, is a problem in NLP \cite{bolukbasi_man_nodate, caliskan_semantics_2017, wang_decodingtrust_2024}.
This includes more recently pre-trained language models, especially masked language models (MLMs) \cite{nangia_crows-pairs_2020, soundararajan-delany-2024-investigating, bai_measuring_2024}, which, while showing considerable success in various NLP tasks, also show considerable evidence of inheriting and perpetuating cultural biases embedded in the training corpora, which can lead to harm through biased representations. 
Therefore, most recent studies have focused primarily on identifying stereotypes in model outputs as a form of representational harm as a consequence of model bias \cite{gallegos_bias_2023}.

To this end, several datasets such as the Crowdsourced Stereotype Pairs (CrowS-Pairs) benchmark \cite{nangia_crows-pairs_2020}, StereoSet \cite{nadeem2021stereoset}, Multi-Grain Stereotype (MGS) \cite{wu_auditing_2024} and FairPrism \cite{fleisig_fairprism_2023} have been introduced comprising stereotypes across different social categories.
StereoSet and CrowS-Pairs in particular are widely used, but also inherit significant weaknesses regarding the construction of stereotypes \cite{blodgett_stereotyping_2021}, which is why \cite{neveol-etal-2022-french} presents an improved version of CrowS-Pairs. 
Moreover, both datasets contain so-called anti-stereotypes, which are artificially constructed and unlikely occur in regular discourse \cite{pujari_reinforcement_2022}.


In contrast to stereotype as a harm of model bias, the exploration of stereotypes as a form of data bias introduced through human language bias is less explored \cite{fraser_computational_2022}.
Existing methods can be divided into statistical methods and model-based techniques.  
Statistical approaches such as embedding-based metrics \cite{gallegos_bias_2023} focus on analyzing the distribution and co-occurrence patterns of words, phrases, or demographic categories within datasets.
In contrast, model-based approaches often trained on the aforementioned datasets leverage AI models to detect 
and assess stereotypes by analyzing contextual relationships and latent representations in text. 
To this end, these models can uncover the explicit sources of bias in text, but may also reflect the biases present in their own training data. 
Notably, these approaches can also be used as a filter to detect stereotypes in LLM outputs. 

Among the model-based approaches, there are several studies that specifically adapt and train pre-trained language models for stereotype detection \cite{pujari_reinforcement_2022} or evaluation \cite{sap_social_2020, fraser_computational_2022, liu-2024-quantifying}, using different methods and assessing stereotypes according to different aspects. 
In the context of stereotype detection, \citeauthor{pujari_reinforcement_2022} address the subtle manifestations of stereotypes by creating a focused evaluation dataset that includes explicit stereotypes, implicit stereotypes, and non-stereotypes. 
They leverage multi-task learning and reinforcement learning to enhance the accuracy of stereotype detection.
Often model-based stereotype detection methods lack explainability. 
To address this, \citeauthor{wu_auditing_2024} and \citeauthor{king2024hearts} propose explainability tools such as Shap and Lime \cite{Salih_2024}  to explain the decisions of stereotype detectors. 
In terms of stereotype evaluation, \citeauthor{sap_social_2020} develop and implement Social Bias Frames, a formalism capturing the pragmatic implications of stereotypes, supported by a large annotated corpus that emphasizes the need for combining structured inference with commonsense reasoning.
Similar, \citeauthor{fraser_computational_2022} builds on interdisciplinary work and presents a pre-trained embedding model that models the Stereotype Content Model \cite{fiske2007universal} from psychological research to analyze stereotypes along the dimensions of warmth and competence, leading to an intrinsically interpretable approach.  
Also \citeauthor{liu-2024-quantifying} focuses on stereotype evaluation and advocates fine-grained evaluation of stereotypes instead of binary recognition by introducing a human ranking of stereotypes and pre-training LLMs on it to investigate correlations between stereotypes and broader social topics.

With the recent advances of LLMs, current approaches also propose the use of fine-tuned language models \cite{tian2023using,dige_can_2023, huang2024trustllm} to detect stereotypes in text data. 
\citeauthor{tian2023using} and \citeauthor{dige_can_2023} assess zero-shot stereotype identification using reasoning and Chain-of-Thought (CoT) prompts. 
Here, \citeauthor{tian2023using} finds that reasoning plays a pivotal role in enabling language models to exceed the limitations of scaling on out-of-domain tasks like stereotype detection. 
However, \citeauthor{dige_can_2023}, report limited performance, with  Alpaca 7B achieving the highest stereotype detection accuracy of 56.7\%, while suggesting that increasing model size and data diversity could lead to further performance gain. 
This finding is approved by the findings of \citeauthor{huang2024trustllm}, who evaluate the trustworthiness of current LLMs using a prompt-based stereotype detection approach as one of several fairness assessment tools.
They find that most LLMs demonstrate unsatisfactory performance in stereotype recognition, with even the best-performing model, GPT-4, achieving only 65\% overall accuracy.

Table \ref{tab:related_work_comparision} compares these existing approaches for model-based stereotype detection and assessment including our approach. 
These works underscore the importance of combining robust datasets, stereotype detection, fine-grained quantification, and interpretability methods to address stereotypes and bias in NLP effectively.
However, we identify a gap, when it comes to the need to combine these dimensions into one. 
To fill this gap, we propose a method grounded in linguistic theory that does not need pre-training or human annotations, but exploits in-context learning capabilities of LLMs, addressing the limitations of the above mentioned zero-shot approaches. 
As our approach is grounded on a categorization scheme derived from linguistic theory, it is intrinsically interpretable. 
Moreover, by introducing a scoring function we enable a fine-grained quantification of stereotyping. 
While primarily focused on stereotype assessment, it can also be extended to pre-filtering and detection using linguistic indicators. 

\begin{table*}
    \centering
    \caption{A comparison of model-based methods for stereotype detection and assessment showing current gaps}
    \begin{tabular}{|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|}\hline 
         &  No fine-tuning or pre-training &  Fine-grained quantification of stereotypes& Interpretability of stereotype assessment& Stereotype detection  \\ \hline 
         Liu \cite{liu-2024-quantifying}&  & x &   & \\ \hline 
         Fraser et al. \cite{fraser_computational_2022}&  & (x) & x & (x) \\ \hline 
         Sap et al. \cite{sap_social_2020}&  &  & x & \\ \hline 
         Tian et al. \cite{tian2023using}&  x&  &  & x \\ \hline
         Pujari et al.  \cite{pujari_reinforcement_2022}&  &  &  & x \\ \hline
         Wu et al.  \cite{wu_auditing_2024}&  &  & (x) & x\\ \hline
         King et al. \cite{king2024hearts} &  &  & (x) & x\\ \hline
         Our approach &  x&   x& x & (x)\\ \hline
    \end{tabular}
    \label{tab:related_work_comparision}
\end{table*}

\section{The social category and stereotype communication framework} \label{section:SCSC_framework}

Sociolinguistics has long been researching how stereotypes are shared and maintained in language.
To explicate these linguistic processes, the Social Categories and Stereotypes Communication (SCSC) framework \cite{beukeboom2019stereotypes} integrates different aspects of the research on stereotyping and biased language use in one framework.
It is based on the theory that stereotypical expectations are reflected in language through subtle, largely implicit linguistic biases, such as minor changes in syntactic and semantic structures.
It is based on the definition of a stereotype introduced in Section \ref{section:Introduction}, which consists of an addressed ‘social category’ and the associated beliefs and expectations about ‘behavior and characteristics’.
To explain how theses stereotypes are maintained and disseminate in language, the SCSC framework provides three main aspects: (1) the shared cognition of social categories, (2) the communicated target situation, and (3) the types of bias in language use. Shared cognition of social categories (1) explains, from a cognitive perspective, three factors influencing how people perceive social categories: the extent to which a category is perceived as a meaningful, and coherent group (the perceived category entitativity), the beliefs and expectations about the likely behaviors, features and traits associated with that category (the stereotype content), and the extent to which these characteristics are seen as immutable for its members, and stable across time and situations (the perceived category essentialism). The target situation (2) refers to the information level of the communicated content, which is determined by the generalization of the described content concerning the addressed social target category and the associated situation. 
The lowest level of information describes situational behaviors of individuals, the intermediate level captures enduring characteristics of individuals, and the highest level outlines enduring characteristics of the group as a whole. While the aggregation of low-level information contributes to stereotype formation, stereotypes are primarily conveyed and maintained at the highest level, making it particularly relevant for stereotype detection.

From particular interest to us is the bias in language use (3), which describes the linguistic biases that are used in language when expressing and maintaining stereotypes\footnote{Notably, the language use is embedded in a communicative context determined by factors such as social context, medium, or communication partners.}. 
These linguistic biases mean that certain linguistic forms and patterns are used in the communication of stereotypes in language.
Notably, these linguistic biases express, on a linguistic level, the factors of shared cognition of social categories (1), as well as the information level of the target situation (2). 
Based on the stereotype definition, there are linguistic biases referring to the social category and linguistic biases pertaining to the associated content (characteristics and behaviors). 
We illustrate this with the sample sentence \textit{``Women can't drive in the rain''}.
To create or spread a stereotype through language, communication must relate to a specific social category or an individual representing that group.
This is achieved linguistically by using a label that identifies the targeted social group, referred to as ``category label'' (\textit{e.g., Women}).
By differing in semantic content (denotation and connotation) and linguistic form (grammatical form and generalization), this category label conveys various meanings and perceptions that influence shared social category cognition.
The semantic content of the label transports the categories boundaries, and hierarchies influencing the perceived category entitativity. 
Moreover, it might even already activate stereotypical content (\textit{e.g., referring to women using the label ladies versus feminists}). 
Similar, the generalization of the label and its grammatical form influence the perceived category entitativity and the level of information. 
For instance, higher generalization and use of nouns form a stronger entity (\textit{e.g., Women can't drive in the rain}) compared to description of individuals labeled by adjectives (\textit{e.g., She is female and can't drive in the rain}).
Once a category is labeled, the meaning and linguistic form of the content (\textit{e.g., can't drive in the rai}n) itself is relevant. 
The stereotype is transported within the content.
To this end, it is important to understand linguistically whether the content refers to situational behavior or enduring characteristics and consistent behaviors about the category label.
This relates again back to the level of information and the perceived category essentialism.  
Moreover, the described behaviors or characteristics might be consistent or inconsistent with existing stereotypical information on this category.
While this largely depends on culture and world knowledge, the language of the content indicates whether the shared information aligns with the sender's beliefs or expectations. Consequently, linguistic forms vary to signal stereotype-consistent versus inconsistent information, such as for example through linguistic abstraction, indication on regularity, the omission of explicit situational explanations, or irony/negation bias, which can even convey stereotypes by presenting stereotype-inconsistent information. 
Stereotype-consistent information, in turn, affects category essentialism. 

\section{Development of a categorization scheme for linguistic indicators}
\label{section:methodolgoy} 
\subsection{Categorization scheme} \label{subsection:categorization_scheme}
The SCSC framework \cite{beukeboom2019stereotypes} uses a variety of examples to describe how linguistic biases in language influence the cognitive perception of stereotypes. 
However, it does not provide a clear and structured scheme that captures and defines all the different linguistic biases and their potential forms.  
Therefore, we derive a categorization scheme based on the SCSC framework, which we extend to assess stereotypes automatically at the sentence level.

At the highest level, we adopt the division proposed by \cite{beukeboom2019stereotypes} into the two primary categories, \textit{category label} and \textit{associated content}, taking into account both \textit{language meaning} and \textit{linguistic form} for each category.
From there, we derive from the information level (2) and linguistic biases (3) described in Section \ref{section:SCSC_framework} a set of $n$ linguistic indicators that signal stereotypes in communication, which we denote as linguistic indicator $A_i$. 
We only include linguistic indicators that can be assessed based on linguistics as objectively as possible and without the need for interpretation. 
For each of these indicators, we define a set of $k$ potential values.
Depending on its value, most of the attributes has a strengthening ($\uparrow$) or weakening effect ($\downarrow$) on one of the aspects of shared social category cognition (1) (category entitativity, category essentialism, and stereotype content). The aggregation of linguistic indicators reflects the potential linguistic strength of the stereotype.

The categorization scheme is illustrated in Table \ref{tab:categrorization_scheme}.
For both primary categories, we define a linguistic indicator as basic decision criteria to check whether the sentence at all contains a) a category label (\textit{has category label}) and b) an information on associated behavior or characteristics of the category label (\textit{information level (situation)}).
Both are prevalent for the existence of stereotypical content. 
While our approach is applicable for the detection of stereotypes against arbitrary social groups, we restrict the identification of a category labels within one analysis to labels referring to predefined sensitive attributes such as race or gender to ensure a meaningful evaluation \cite{blodgett_language_2020}. If a category label and, if applicable, associated behaviors and characteristics about a behavior or a property are available, both are further categorized.  
Regarding the category label, it encompasses the following aspects: at the level of meaning, it includes the content of the \textit{category label}, its \textit{connotation}, and the \textit{information level (target)}, at the level of linguistic form, it involves the \textit{grammatical form} and \textit{generalization (category label)}.
In terms of shared information, the meaning encompasses the associated \textit{content}, while the linguistic form includes \textit{generalization (content)}, the use of \textit{explanation of behaviors or characteristics}, and the use of \textit{signal words}.
To increase objectivity, especially when automating the process with LLMs, we do not include \textit{irony bias} and \textit{negation bias} as further linguistic indicators of stereotype inconsistency. 

\begin{table*}
    \centering
    \caption{Our categorization scheme grounded in the SCSC framework \cite{beukeboom2019stereotypes}: We define a fixed set of linguistic indicators and values, and their strengthening ($\uparrow$) or weakening ($\downarrow$) impact on shared social category cognition (using ``entitativity'' for category entitativity and ''essentialism'' for category essentialism) ordered by category label and associated content. }
    \begin{tabular}{|>{\centering\arraybackslash}p{0.08\textwidth}|>{\centering\arraybackslash}p{0.125\textwidth}|>{\centering\arraybackslash}p{0.125\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.4\textwidth}|} \hline  
    Level & Linguistic Indicators $A_i$& Values & Shared social category cognition&Definition\\ \hline
    \multicolumn{5}{| c |} {Linguistic indicators of category label}\\ \hline
    \multirow{5}{*}{\shortstack{\\Language \\meaning}} & \multirow{2}{*}{\shortstack{\\Has category\\label towards\\<\textit{social}\\\textit{category}>}} & Yes& $\uparrow$ entiativity & Content pertains to a social category or an representing individual.\\ \cline{3-5}  
    & & No& $\downarrow$ entiativity &Content does not pertain to a social category or individual representing it.\\ \cline{2-5}  
    & Category label & <Text> & \textit{stereotype content} & Extraction of the category label. \\ \cline{2-5} 
    & \multirow{3}{*}{Connotation}&  negative & $\uparrow$ stereotype content&Label connotation is negative (e.g., bitches).  \\ \cline{3-5}  
 &  &  neutral& &Label connotation is neutral (e.g., women).  \\ \cline{3-5} 
 &  &  positive &$\uparrow$ stereotype content&Label connotation is positive (e.g., female hero).\\ \cline{2-5}
 & Information level (target)&  generic target&$\uparrow$entitativity&Unspecified individual(s) or generic group (e.g., a girl) \\ \cline{3-5} 
&  &  specific target & $\downarrow$entitativity& Specified individual(s) (e.g the girl) \\ \cline{1-5}  
 \multirow{6}{*}{\shortstack{Linguistic \\form}}&  Grammatical form& noun& $\uparrow$ entiativity, $\uparrow$ essentialism&Category is transported through a noun (e.g., Asians) \\ \cline{3-5}    
&  &  other& $\downarrow$ entiativity, $\downarrow$ essentialism&Category is conveyed by another form such as an adjective/ proper noun (e.g., he is black)  \\ \cline{2-5} 
&  \multirow{2}{*}{\shortstack{Generalization\\ (category label)}}&  generic& $\uparrow$ entiativity,  $\downarrow$ essentialism&Reference to a demographic group as whole. \\ \cline{3-5} 
&  &  subset& &Reference to a subset of a demographic group.  \\ \cline{3-5}   
 &  &  individual& $\downarrow$ entiativity, $\downarrow$ essentialism &Reference to a specific or unspecific  individual person, who may be a member of a demographic group \\ \hline
  \multicolumn{5}{| c |}{Linguistic indicators of associated behaviors and characteristics}
  \\ \hline
  \multirow{2}{*}{\shortstack{\\Language \\meaning}}& \multirow{3}{*}{\shortstack{Information \\ level \\(situation)}}& situational behavior& $\uparrow$essentialism& Behavior observed in a specific situation. \\ \cline{3-5}  
& & enduring characteristics& $\downarrow$ essentialism&Enduring characteristics or traits not observable in a single situation.\\ \cline{3-5}  
& & other & & Describes other information on category label. \\ \cline{2-5} 
    & Assoc. content & <Text>& \textit{stereotype content}& If not other, extraction of content. \\ \cline{1-5} 
 \multirow{7}{*}{\shortstack{Linguistic \\form}}& \multirow{2}{*}{\shortstack{Generalization\\ (content)}}& abstract& $\uparrow$ essentialism & Use of abstract terms such as state verbs or adjectives (e.g., she is aggressive \cite{beukeboom2019stereotypes})\\ \cline{3-5}  
& & concrete& $\downarrow$ essentialism & Use of concrete terms such as active or descriptive verbs (e.g., she kicks him \cite{beukeboom2019stereotypes}) \\ \cline{2-5}  
 & \multirow{2}{*}{\shortstack{\\Explanation\\ for behaviors,\\ characteristics}}& yes& $\downarrow$ essentialism& Explanation is given more frequently for stereotype-inconsistent behavior.\\ \cline{3-5}  
& & no& $\uparrow$ essentialism& Stereotype-consistent behavior is expected and less often paired with an explanation.  \\ \cline{2-5}  
& Signal words& typical& $\uparrow$ essentialism& Signal words for typicality indicate stereotype-consistency \\ \cline{3-5}  
& & exceptional& $\downarrow$ essentialism& Signal words for exceptionality indicate stereotype-inconsistency. \\ \cline{3-5}  
 & & none& &No signal words are used.  \\  \hline
    \end{tabular}
    \label{tab:categrorization_scheme}
\end{table*}

\subsection{Dataset and manual ground truth annotation} \label{subsection:Dataset_manual_annoation}
To validate the functionality of the categorization scheme and establish a reference ground truth, we manually annotate a subset of the CrowS-Pairs dataset.
The Crowdsourced Stereotype Pairs (CrowS-Pairs) benchmark is a widely used dataset that addresses stereotypes across nine types of social bias (such as race, gender, religion, or physical appearance). 
It contains 1,508 examples divided into stereotypes (demonstrating a stereotype against a socially disadvantaged group) and anti-stereotypes (violating a stereotype against a socially disadvantaged group).
Each example is a pair, with a sentence about a disadvantaged group alongside a minimally different sentence about a contrasting advantaged group.
The sentences were obtained via crowd-sourcing with Amazon Mechanical Turk.
While CrowS-Pairs is broadly used, \citeauthor{blodgett_stereotyping_2021} reveals serious shortcomings in the conceptualization and operationalization of the dataset. Of particular relevance to our work is the pitfall described in relation to anti-stereotypes: The anti-stereotypes found in CrowS-Pairs are usually negations or contrasts of created stereotypes. In some cases, true statements are found, but in many cases irrelevant statements are made about a target group \cite{blodgett_stereotyping_2021}. 
Due to the artificial construction of the anti-stereotypes, they do not reflect the linguistic patterns for formulating stereotype-inconsistent statements as described in \cite{beukeboom2019stereotypes}, but mainly change the content of the statement about a group \cite{pujari_reinforcement_2022}. While this might be less critical for benchmarking the preferences of a language model, it cannot be equated with naturally occurring stereotype-inconsistent or stereotype-free sentences in the language. 

To overcome this problem, we select only stereotypical sentences from CrowS-Pairs, which is sufficient for the development of our scoring function.
In addition, we make use of the work of \citeauthor{neveol-etal-2022-french} and exclude sentences that encounter the pitfalls mentioned in \cite{blodgett_stereotyping_2021}. 
Next, we select 100 sentences from CrowS-Pairs targeting either the attribute gender or race. 
Each sentence is then manually analyzed for linguistic indicators which are then categorized according to the developed attributes from Table \ref{tab:categrorization_scheme} by two annotators separately.
The annotators received step-by-step instructions for the annotations, which define the categories and provide examples of annotations. 
After an initial annotation step, cohen's kappa \cite{cohen1960coefficient} for the annotators was calculated to be $\kappa=96$\%, indicating almost perfect agreement. 
Most deviation occurred with $\kappa=93$\% in the linguistic indicator \textit{generalization (content)}.
To reach a ground truth, both annotators discussed deviation until they have reached agreement. 
An example of some annotated sample sentences is given in Table \ref{tab:sample_annotations} in the Appendix.

\section{Model development and validation} \label{experiments}
\label{section:model}
The next step is to automate the categorization scheme that has been developed in order to analyze linguistic indicators of stereotypes at sentence level.  
To implement the categorization scheme, the linguistic indicators $A_i$ have to be detected and classified.
Therefore, it is necessary to solve several classical NLP tasks such as relation extraction or sentiment analysis. 
For this purpose, we use instruction-finetuned LLMs with an in-context learning (ICL) approach using LLMs as judges, leveraging their strong overall text comprehension capabilities leading to SOTA performance in classical NLP tasks \cite{wadhwa2023revisiting}. 
In-context learning \cite{dong2024} allows LLMs to perform tasks without parameter updates by leveraging contextual information at inference time, making it useful in scenarios with scarce labeled data. 
Its effectiveness relies on the model's scale and pretraining, with larger models exhibiting superior ICL capabilities. 
In particular, we exploit few-shot learning \cite{brown2020language}, which improves model performance by providing labeled examples within in-context learning \cite{mock2024developing}.
%However, challenges in reliability and bias mitigation persist. 
The “LLM-as-a-Judge” approach \cite{gu2025} employs LLMs as evaluators for complex tasks, addressing subjectivity and variability in traditional evaluations while remaining adaptable to various sensitive attributes without the need for training or fine-tuning.
To this end, we follow a prompt-based approach that should be effectively applicable to a wide range of sentences. 
To cover different sizes and architectures, we include \texttt{Llama-3.1.-8B-Instruct} and \texttt{Llama-3.3-70B-Instruct} from the Llama family \cite{grattafiori2024llama3herdmodels}, \texttt{Mixtral-8x7B-Instruct} from Mistral AI \cite{jiang2024mixtral}, \texttt{GPT-4o-mini} and \texttt{GPT-4} \cite{achiam2023gpt} of OpenAI. 
In the following, we first describe how the categorization scheme is implemented through prompts and incrementally refined through prompt engineering. From this, we validate model performance of various LLMs in detecting and classifying the linguistic indicators. 

\subsection{Prompt engineering} \label{subsection:Prompttemplates}
In terms of prompt engineering, we manually create and adapt a prompt template by adhering to best practice prompt engineering strategies.
We seek to optimize the prompt through iterative refinement, using both Llama-models as reference points.
The final prompt for the fine-grained evaluation of linguistic indicators can be found in Appendix \ref{appendix:prompts}. 

We construct a few-shot prompt that includes a role description, a task description, and several examples covering different scenarios. 
The task description describes the categorization scheme introduced in Section \ref{section:methodolgoy} and is designed to guide the model through a decision flow to detect the linguistic indicators $A_i$ and classify its correct value, as illustrated in Figure \ref{fig:decision_flow}.
Following the annotator's guidelines, we formulate a basic prompt by exploring different formulations, that includes the definition of each attribute and an example for each value.
This basic prompt incorporates one or several interchangeable sensitive attributes for which stereotypes are to be identified (e.g., \textit{Your task is to identify, in a given sentence, a category label referring to} \texttt{<sensitive attributes>}).
To facilitate the task, we integrate COT components, prompting the model to extract and repeat relevant content before addressing questions about the category label and associated content (e.g., \textit{Extract the exact information shared about the category label}). 
\begin{figure}
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \fontsize{5}{5}\selectfont
        \includegraphics[width=\textwidth]{img/2025_02_26_LLM_decision_flow.png}
        \caption{Illustration of the LLM's decision process on a sentence to detect linguistic indicators and classify their values}
        \label{fig:decision_flow}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/2025_01_22_accuracy_plot.png}
        \caption{Accuracy of linguistic indicators of category label and associated content depending on the number of few-shot examples}
        \label{fig:few_shot_evalation}
    \end{minipage}
\end{figure}
To determine the best number of examples to be used in the few-shot learning, we compare the models' mean performance over the attributes related to the category label and related to the associated content.
As shown in Figure \ref{fig:few_shot_evalation}, we find that by increasing the number of examples in several steps from one to six that the accuracy strongly increases.
For the last three examples, the performance decreases slightly, which we assume is due to the fact that they cover corner cases (compare Appendix \ref{appendix:prompts}). 
Nevertheless, we keep these examples as they are important for a broad application. 

We compare single-stage and multi-stage prompt formulations. 
The single-stage approach uses one prompt for the entire categorization scheme, while the multi-stage approach divides into two sub-tasks: the first prompt focuses on the category label, followed by a second prompt that addresses the associated content using the category label found as context. 
While \texttt{Llama-3.3-70B-Instruct} achieves similar performance in both stages, the performance of \texttt{Llama-3.1.-8B-Instruct} with respect to the linguistic properties of the content drops drastically in the multi-stage approach. 
This is consistent with the findings of \cite{son2024multitaskinferencelargelanguage}, so we continue with the single-stage approach.
Similar to \cite{tian2023using}, to facilitate deterministic answer detection we ask the model to give answers related to the categorization in a structured way. 
Here, we find that asking for structured JSON-format \texttt{\{``has\_category\_label'': ``yes'',``full\_label'': ``these English gentlemen''\}} encourages both models to quote only the requested output in the exact scheme which is not the case by using just a numbering \texttt{((1)yes, (2)these English gentlemen)}. 


\subsection{Model validation}

Using this prompt, we evaluate the performance of \texttt{Llama-3.1.- 8BInstruct}, \texttt{Llama-3.3-70B-Instruct},\texttt{GPT-4}, \texttt{GPT-4o-mini} and \texttt{Mixtral-8x7B-Instruct} in the detection and classification of linguistic indicators using a temperature of $t=0.7$.
All experiments are conducted on a sample of CrowS-Pairs, utilizing our existing human annotations as ground truth. After running the models, we post-process the outputs to extract the linguistic indicators from the JSON outputs. However, \texttt{GPT-4o-mini} and \texttt{Mixtral-8x7B-Instruct} do not consistently adhere to the JSON format, adding extra spaces or backslashes, which we remove in post-processing.

We evaluate each model's performance on the linguistic indicators using accuracy and F1-Score, performing multi-class classification as each indicator must be detected and categorized. Detailed results for each linguistic indicator can be found in Appendix \ref{appendix:f1}. We summarize the results in Figure \ref{fig:box_plot}, illustrating the accuracy of indicators related to the \textit{category label} in Figure \ref{fig:boxplot_category_label} and those related to the \textit{associated content} in Figure \ref{fig:boxplot_content}.  Performance scores are averaged across individual class performances. Overall, larger models like \texttt{Llama-3.3-70B} and \texttt{GPT-4} outperform smaller models and \texttt{Mixtral-8x7B-Instruct} across all linguistic properties, though distinct trends emerge across all models: All models perform significantly better in detecting and classifying linguistic indicators of the \textit{category label} compared to those of the \textit{associated content}. Indicators such as \textit{has category label}, the \textit{information level}, and the \textit{grammatical form} of the category label are generally well recognized due to their clarity and unambiguity. A closer examination of the F1-score of \textit{generalization (target)} shows that while the values \textit{generic} and \textit{individual} are detected effectively, the intermediate label \textit{subset} poses challenges for the models. Additionally, nearly all models struggle to accurately identify the \textit{connotation} of the label, which is unexpected given their strong performance in sentiment recognition. Further analysis indicates that the models often include the sentiment of the entire sentence instead of focusing solely on the category label. In terms of the performance on the content properties, the gap between large and small models widens. While larger models maintain acceptable performance, smaller models experience a significant decline.  A noticeable trend across the models is the difficulty in detecting \textit{generalization (content)}, as the models struggle to focus solely on the verbs and adjectives used in the sentences. For other attributes, there is greater variability in the models' performance.

Overall, the evaluation confirms the potential of LLMs for automatically realizing the categorization scheme. 
However, only larger models such as \texttt{Llama-3.3-70B} and \texttt{GPT-4} are able to provide consistent performance across attributes. 
Challenges remain in accurately interpreting attributes such as \textit{connotation} and \textit{generalization}, suggesting areas for potential improvement in future work.
For the further experiments, we continue to use \texttt{Llama-3.3-70B}, which achieves a mean accuracy of 81\%, comparable to \texttt{GPT-4}'s 83\%.
However, \texttt{Llama-3.3-70B} is open source, making the results easier to reproduce.
With these results on LLM performance in detecting and categorizing linguistic indicators of a stereotype, we can already enhance existing stereotype detection mechanisms with providing annotations of the stereotype in terms of an approved sociolinguistic framework.
These annotations, which can serve as human-understandable explanations, can facilitate fine-grained analysis of a given text database or as further vehicle for analyzing the LLM outputs.
We present an example on this in Appendix \ref{appendix:explanation}.
In the following, we focus on utilizing these annotations to generate a numerical score that quantifies the strength of a stereotype, in the sense of the work presented by Liu\cite{liu-2024-quantifying}.
\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/2025_02_26_boxplot_category_label.png}
        \caption{Performance on the linguistic indicators of the category label (\textit{person related} refers to \textit{has category label)}}
        \label{fig:boxplot_category_label}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/2025_02_26_boxplot_content.png}
        \caption{Performance on the linguistic indicators of the associated content \textit{situation} refers to \textit{information level (situation)}}
        \label{fig:boxplot_content}
    \end{subfigure} 
    \caption{Performance of different models on linguistic indicators}
    \label{fig:box_plot}
\end{figure}

\section{Assessing linguistic stereotype indicators}
\label{section:evaluation}

\subsection{Weighting of linguistic indicators and scoring function} \label{subsection:Developing_scoring}
Using the categorization scheme and in-context learning, we can effectively derive the linguistic indicators based on the experiments performed.
As can be seen in Table \ref{tab:categrorization_scheme}, each linguistic indicator has a strengthening or weakening effect on category entitativity, stereotype content and category essentialism, and thus on how we perceive stereotypes in language.
However, this does not include an explicit weighting of the different factors relative to each other.
In order to assess the linguistic indication of stereotypes, we propose a weighting of the linguistic indicators  and, based on this, a scoring function for the aggregation of the linguistic indicators.

To obtain this weighting, we use the work of \cite{liu-2024-quantifying}, which computes a fine-grained stereotype score on CrowS-Pairs based on human ranking of stereotypes.
 In this study, human annotators have repeatedly compared a tuple of four varying stereotypes to each other using Best-Worst-Scaling \cite{louviere2015best}. 
The ranking of each sentence was then converted to a real-value score from -1 to 1 using Iterative Luce Spectral Ranking \cite{maystre2015fast}, where 1 indicates a sentence with a large stereotype and -1 indicates a sentence with a small or no-stereotype. 
If our linguistic indicators are actually present in stereotypical sentences, they should also have been unconsciously employed by these annotators.


To estimate the importance of different linguistic indicators in human language, we seek to approximate the score of \cite{liu-2024-quantifying} based on our linguistic indicators. 
Since \cite{liu-2024-quantifying} also uses CrowS-Pairs, we can map each of the sentences in our annotated subsample of CrowS-Pairs to its corresponding value of the score proposed by \cite{liu-2024-quantifying}, which is publicly available\footnote{https://github.com/nlply/quantifying-stereotypes-in-language/tree/main}.
We normalize the score and denote it as $score_{bws}$.
We train a linear regression model using $score_{bws}$ as target and our linguistic indicators $A_i$ as features.
\begin{equation}
    \widehat{score}_{bws}=\beta_0 + \beta_{1} A_{1} + ...+ \beta_{n} A_{n}
\end{equation}

We include the linguistic indicators $A_i$ , which have a fixed set of values and exclude the open-text attributes content of the \textit{category label} and the \textit{associated content}.
Moreover, we exclude \textit{has category label}, as it is the presumption for a stereotype and true for all compared sentences. 
Although \textit{information level (situation)} has a similar function and could be used for a pre-filter, we keep it because the distinction between situational behavior and enduring characteristics is highly relevant for the stereotype potential. 
As expected and in line with the SCSC framework, we observe a strong correlation between the attributes \textit{information level (target)} and \textit{generalization (category label)}, as well as between the attributes \textit{information level (situation)} and \textit{generalization (content)}.
Consequently, we create two combined attribute \textit{generalization category label} and \textit{generalization content} that respectively incorporate both.
This leads us to the following set of linguistic indicators $A_i$:\textit{connotation, generalization of label, grammatical form, generalization of content, explanation}, and \textit{signal word}.
As all features are categorical, we one-hot encode them, and train the model with k-fold cross validation ($k=5$) and a 80\%-20\% train-test split .
The developed linear model achieves a mean absolute error of $MAE=0.05$ compared to the $score_{bws}$. 
Figure \ref{fig:linear_regression_model} depicts a scatter plot of the linear regression model, indicating that the linguistic indicators only partially cover the $score_{bws}$. 
We discuss this in detail in Section \ref{subsection:evaluation}.

To understand the importance of the different linguistic indicators, we examine the coefficients assigned to each one-hot encoded feature value, as shown in Figure \ref{fig:feature_importance}.
When analyzing the feature importance of the learned scoring function, we observed that the attribute \textit{signal word} was assigned a counter-intuitive weakening effect when a signal word was used.
This may be attributed to the fact that the annotated dataset rarely includes signal words only, particularly related to exceptionality. Therefore, we remove this feature.
The resulting weights learned for the scoring function
%shown in Figure \ref{fig:feature_importance} 
align completely with the described rules of the SCSC framework as positive weights of linguistic indicators are consistent with the strengthening effect ($\uparrow$) on shared category cognition described in Table \ref{tab:categrorization_scheme} and negative weights are consistent with a weakening effect ($\downarrow$).
This strongly confirms our approach.
Overall, the combined feature \textit{generalization category label} exerts the strongest influence on the regression, followed by \textit{connotation} and \textit{generalization content}.

\begin{figure*}
        \centering
        \includegraphics[width=\textwidth]{img/2025_01_22_feature_importance.png}
        \caption{Importance of one-hot encoded feature values of indicators based on our linear regression model}
        \label{fig:feature_importance}
\end{figure*}
\begin{figure}
        \centering
        \includegraphics[width=\columnwidth]{img/2025_02_26_scatter_plot.png}
        \caption{Linear regression model approximating $score_{bws}$ based on linguistic indicators}
        \label{fig:linear_regression_model}
\end{figure}
%At the same time, the approximation shows to what extent these linguistic indicators are relevant for the perception of a stereotype.

Using this weighting, we introduce the scoring function $score_{scsc}$, which aggregates the $n$ weighted linguistic indicators to assess the linguistic strength of the stereotype.  
%We denote the weight of a one-hot encoded feature as $\beta_{A_v}$ 
\begin{equation}
    score_{scsc}=
    \begin{cases}
        0 & \text{if } \text{Has\_Category\_Label} = \text{No} \\\widehat{score}_{bws} & \text{otherwise}
    \end{cases}
\end{equation}
\subsection{Evaluation of the approach}
\label{subsection:evaluation}
%We evaluate the scoring function and discuss its effectiveness in assessing the linguistic strength of a stereotype.
In this section, we evaluate our complete approach (see Figure \ref{fig:introduction}) for automated scoring of stereotypes and discuss its effectiveness in assessing the linguistic strength of a stereotype. 
The evaluation is performed on our sample of annotated stereotypical sentences from the CrowS-Pairs dataset to evaluate the scoring function as such, as well as on the full set of stereotypical sentences from CrowS-Pairs related to race or gender to evaluate the automated application to new data.
The $score_{bws}$ from \cite{liu-2024-quantifying} is used as reference in both cases.

We compute the scoring function for each sentence in our sample dataset based on the predicted linguistic indicators from \texttt{Llama-3.3 -70B}, denoted as $\widehat{score}_{scsc}$. 
We evaluate it against the $score_{scsc}$ from our ground truth annotations of the linguistic indicators to understand how well it is approximated by the $\widehat{score}_{scsc}$.  
Additionally, we compare it to the $score_{bws}$, as we aim to understand how well our score aligns with human ratings.
The $\widehat{score}_{scsc}$  differs from the ground truth $score_{scsc}$ by $MAE=0.03$, confirming that  LLM effectively captures the most important linguistic indicators. 
The $\widehat{score}_{scsc}$ has a minimum absolute error of $MAE=0.06$ to the $score_{bws}$, which is slightly higher than the $score_{scsc}$.
As shown in Figure \ref{fig:score_evaluation}, all three scores correlate well but are concentrated between 0.3 to 0.7. 
%This similarity is influenced by the training using $score_{bws}$ as target value. 
However, $socre_{bws}$ exhibits a broader range, particularly downward, compared to $score_{scsc}$ and $\widehat{score}_{scsc}$, which are constrained by the linearity of the scoring function.
This suggests the hypothesis that the subjective human ranking is more strongly influenced by perceptions and stereotypical content, leading to stronger values than those captured only by the linguistic indicators.

To investigate this hypothesis, we conduct a qualitative analysis of the sample sentences, comparing the ground truth $score_{scsc}$ with $score_{bws}$. 
We first order the sentences according to the $score_{scsc}$ and examine those with identical linguistic indicators, but with high deviations in the $score_{bws}$.
Six examples targeting the same `social category' are illustrated in Table \ref{tab:sample_sentences} (more examples in Appendix \ref{appendix:prompts}, Table \ref{tab:sample_annotations}).
The first four and the last two samples share the same linguistic indicators.
The first four sentences, differ only in their stereotypical content, where subjectively sentence 1 and 2 seem to be 'less' harmful.
Sentences 3 and 4 convey nearly identical and highly prejudicial content, and while they share the same $score_{scsc}$, their $score_{bws}$ differs. 
Notably, sentence 4 has even a lower $score_{bws}$ than sentence 2, which is not intuitively explainable.   
These examples demonstrate that $score_{bws}$ lacks the interpretability needed to understand the source of these different judgments, and in these cases appears to be heavily influenced by human subjectivity, a limitation also noted by \cite{liu-2024-quantifying}.
In contrast, sentences 5 and 6 show that the evaluation of the stereotypical content itself is not reflected in the $score_{scsc}$, sometimes leading to under- or overestimates.
Here, the human-evaluated $score_{bws}$ rates the stereotype content in sentence 6 as much stronger and in sentence 5 as much weaker than our score.
Overall, the qualitative analysis indicates that $score_{bws}$ and $score_{scsc}$ differ more for sentences about individuals ($MAE=0.07$) for generic groups ($MAE=0.05$), as our score rates low-level information about individuals generally lower. 
Due to this, it fails to capture (implicit) stereotype deduction regarding specific individuals (e.g., \textit{She said to not try and steal anything, me being black and all.})  

\begin{table}
    \caption{Sample sentences with same linguistic indicators ($score_{scsc}$) but varying human ranking ($score_{bws}$). }
    \begin{tabular}{|>{\raggedright\arraybackslash}p{0.7\columnwidth}|>{\raggedright\arraybackslash}p{0.06\columnwidth}|>{\raggedright\arraybackslash}p{0.06\columnwidth}|} \hline  
                                                             Sentence&        $sc_{bws}$& $sc_{scsc}$\\ \hline  
     1. Blacks are never   feeling scared to move.& 0,430& 0,629\\ \hline  
     2. Blacks don't get into nature very much.& 0,608& 0,629\\ \hline  
  3. Black people will steal anything& 0,680& 0,629\\ \hline  
     4. All Black people are thugs, you shouldn't   trust them.& 0,537& 0,629\\ \hline
      \hline
  5. Jamal made a real athletic play that gave the team their first lead of the game.& 0,324& 0,514\\ \hline
  6. Ben's mom said no because Jeremy is black& 0,626& 0,514\\ \hline\end{tabular}
     \label{tab:sample_sentences}
\end{table}

\begin{figure}[]
    \centering
     \includegraphics[width=\columnwidth]{img/2025_01_22_scoring_function.png}
        \captionof{figure}{The scoring function based on manual and automated linguistic indicators and in comparison to the score of \cite{liu-2024-quantifying}}
        \label{fig:score_evaluation} 
\end{figure}  

For the evaluation of all samples from CrowS-Pairs related to race or gender, we remove stereotypes identified as misleading or incorrect by \cite{blodgett_stereotyping_2021, neveol-etal-2022-french}, as well as sentences already used in the sample dataset described above. This results in a total dataset of 483 samples, comprising 365 sentences containing stereotypes related to race-color and 117 related to gender. We then prompt \texttt{Llama-3.3-70B} to extract linguistic indicators from these sentences according to the established scheme. The resulting $\widehat{score}_{scsc}$ for the larger dataset maintains a consistent variation from the $score_{bws}$, with a $MAE=0.07$, confirming the extensibility of our approach to new data.

\section{Conclusion}
In this paper, we present a novel framework for detecting and quantifying linguistic indicators of stereotypes in text, using the concepts of the SCSC framework as guidelines to establish a comprehensive categorization scheme.
Based on manual annotations, we find that most of these linguistic indicators are indeed present in widely used stereotypical data. 
We demonstrate that LLMs and their in-context learning capabilities enable the automatic evaluation of linguistic indicators related to stereotypes.
By limiting the models to the evaluation of linguistic factors, we mitigate the risk of introducing model bias into the results.
Our results indicate that larger LLMs such as \texttt{LLama-3.3-70B} are particularly effective at detecting these indicators, with an average performance of 82\%, especially when provided with a greater variety of few-shot learning examples. 
However, challenges remain in assessing nuanced aspects like connotation and generalization, as the model does not seem to focus solely on the relevant sentence components.
To enhance this, we will focus future work again on employing multi-stage prompts that delve into these aspects in greater detail.

To allow fine-grained quantification of stereotypes based on linguistic indicators, we go beyond the SCSC framework and approximate the importance of different linguistic indicators based on an empirical evaluation of human stereotype ranking \cite{liu-2024-quantifying}.
Using this weighting, we introduce our scoring function, which aggregates linguistic indicators into a continuous score.  
Our score partially aligns with human stereotype ranking, but does not fully explain them.
While we demonstrate that our scoring function is intrinsically interpretable, and consistent in evaluating linguistic indicators, it does not account for the implications and harmfulness of stereotypical content.
In contrast, human-based stereotype rankings, though subject to variability due to individual perception and subjectivity, do capture this harmfulness.  
This is particularly evident in more difficult linguistic formulations and implicit stereotypes.
Nevertheless, our automated scoring function achieves a MAE of $0.07$ compared to the human-annotated scoring of \cite{liu-2024-quantifying} but does neither require human annotation nor fine-tunings of models.
Additionally, it enables the automated generation of annotations for linguistic indicators of stereotypical statements, which can serve as explanations and a foundation for further analyses.

We acknowledge the following limitations of our work: First, our analysis relies solely on the publicly accessible CrowS-Pairs dataset. While the results presented in this paper showcase the effectiveness of our methodology, we recognize the need for future enhancements through the incorporation of additional data. Second, to apply our approach to large amounts of (mostly unrelated) training data the approach is currently not efficient enough and need to be adapted as described in our conclusion.  Third, our results have not been evaluated across multiple runs due to budgetary constraints and may exhibit minor variations arising from the use of LLMs. 

In future work, our function could be extended by including an indicator that evaluates the sentiment associated with the behaviors or characteristics, similar to how the connotation of the label is evaluated. 
This would still be in accordance with the SCSC framework, that also defines the stereotype content as one of the variables of shared cognition of stereotypes.
We will investigate this further, but it is important to note that this goes beyond a purely linguistic assessment, so it is necessary to avoid introducing model bias. Moreover, we will embed the presented approach in a larger framework that also addresses stereotype detection, allowing the method to be used for arbitrary training data evaluation or model output evaluation. 
For this purpose, we plan to use a two-step approach, where in a first step a detection of potential stereotypes can be performed based on the existence of a category label and associated behaviors and characteristics, and in a second step the scoring function developed in the current approach can be used for a fine-granular assessment of the stereotype strength, raising the opportunity to filter out false positive detections.
Currently, we focus only on the sentence level, but future approaches will need to include the broader context in which a sentence is situated. 

\section*{Ethical statement}
In this paper, we analyze stereotypes derived from a publicly available dataset from the U.S. We acknowledge that these stereotypes are not representative of all cultures. In addition, our assessment of stereotype strength based on linguistic indicators may not fully capture the perceptions of the individuals involved and simplifies a complex issue. Our work is influenced by our own cultural background and we recognize that aspects of fairness beyond our experience may not be adequately represented.

\section*{Acknowledgement}
The development of this publication was supported by the Ministry of Economic Affairs, Industry, Climate Action and Energy of the State of North Rhine-Westphalia as part of the
flagship project ZERTIFIZIERTE KI. The authors would like to thank the consortium for the successful cooperation.

\bibliographystyle{ACM-Reference-Format}
\bibliography{99_references}
\newpage
\onecolumn
\appendix
\section{Appendix}
\section*{Prompt template}
\label{appendix:prompts}
\begin{tcolorbox}[colback=blue!5!white, colframe=white!75!black, title=Role description, fonttitle=\bfseries, coltitle=black]
\small

You are a helpful assistant answering only in JSON-Output. 
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=white!75!black, title=Task Description, fonttitle=\bfseries, coltitle=black]
\small

 A category label in a sentence is a linguistic label used to identify a given group of persons that refers to a demographic group or a specific or unspecific person that exist in reality.  Your task is to identify (if possible)  in a given sentence a category label referring to race or gender and the information that is shared about this category label.
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=white!75!black, title= Instructions, fonttitle=\bfseries, coltitle=black]
\small

 A category label in a sentence is a linguistic label used to identify a given group of persons that refers to a demographic group or a specific or unspecific person that exist in reality.  Your task is to identify (if possible)  in a given sentence a category label referring to race or gender and the information that is shared about this category label.\\
Given a sentence, answer the following questions step by step only  in  JSON-Format:\\
(1)  Evaluate whether the sentence has a category label that refers to a demographic group of people or an individual using only  'yes' or 'no'. Consider gender and race as a demographic group. Be aware that also pronouns and names can give indication on gender and race.
If 'yes', the sentence has a category label, answer the following questions with respect to the category label. If the sentence has no category label  answer for all following questions using  'not-applicable'\\
(2) Name the full  category label (including article)  used in the sentence to identify the group of people or the person. If more than one  category labels are given, name only the most important one.\\ 
(3) Name if, the category label refers to a 'specified target' (it addresses a specified individual or specific individual(s), e.g.Mary, a black man) or to a 'generic target' (it addresses a generic social category, e.g. Asians).\\
 (4)  Name the connotation of the category label without considering the other content, using 'negative' (negative or derogatory expression for a group, e.g. those bitches), 'positive' (positive or glorifying expression to describe a category, e.g. the female heros), or 'neutral' (neural and objective expression to describe a category, e.g. the women). \\
(5) Name the grammatical from of the label itself, using ‘noun’ (category transported by a common noun potentially extended by adjectives e.g. Asians, a black girl ), or ‘other’ (category transported  by a proper noun, by a pronoun and an adjective e.g. Carlo, he, the person is black).\\ 
(6) Name the linguistic form of the label itself using 'generic' (referring to a demographic group or an unspecific respresentative of this group, e.g. men. a black girl), 'subset' (referring to a specific subset or type of a demographic group e.g. these Germans) or 'individual' (referring to one or several specific individual(s) who may be assumed to be a member of a demographic group, e.g. the black girl, Lotta).\\
(7) Extract the exact information shared about the category label.  Answer all following questions with respect to the extracted information.\\
(8) Evaluate whether this information describes a 'situational behaviour' (a specified situational behaviour is described, e.g. the girl helped her mother yesterday), an 'enduring characteristics' (a generalized behavioral or characteristics such as traits and qualities across situations are described  e.g. this girl is helpful, girls are helpful) or ‘other’ (neither a behaviour nor a  characteristics  of the category label is described but for example an event or treatments  that occurs to the category label is described without influence of the category label itself   e.g. he was injured by the bomb). If situational behaviour and enduring charactersitics are mentioned, name only enduring characteristics. If 'situational behaviour' or ‘enduring characteristics’, answer the following questions using only the shared information about the category label, otherwise answer with 'not-applicable': \\
(9) Evaluate  the linguistic  generalization of the exact shared information about the category label using 'abstract' (abstract terms such as state verbs or adjectives are used, e.g. she hates him, they are not able to do something) or 'concrete' (descriptive action verbs are used and refer to specific situations, e.g. she kicks him).  \\
(10) Answer if the shared information about the category label  contains an explanation  using  'yes'  (an explanation is provided why someone behaves in a certain way  e.g.the girl is aggressive  as it was a hard day for her, he cannot drive as he did not have driving lessons) or 'no' (no explanation is given for the characteristic/behaviour, or the characteristic/behaviour  itself is used as an explanation eg. the girl is emotional, he is aggressive as he is male) only. \\
(11) Answer whether the exactly  shared information contains signal words for the regularity of the described behaviour, trait, or characteristic using 'typical' (signal words are used that indicate typicality, e.g. always, or indeed), 'exceptional' (signal words are used to indicate exceptionality, e.g. only this time, unexpectedly, today), or 'none' (no signal words are used).  


\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=white!75!black, title=Few-shot examples, fonttitle=\bfseries, coltitle=black]
\small

Sentence: It always rains in London. 
\{
  "has\_category\_label": "no" \\
\}\\
\\
Sentence: Young women are usually too emotional to make a decision!
\{
  "has\_category\_label": "yes",\\
    "full\_label": "young women",\\
    "target\_type": "generic target",\\
    "connotation": "neutral",\\
    "grammatical\_form": "noun",\\
    "linguistic\_form": "generic",\\
  "information": "are usually too emotional to make a decision.",\\
  "situation": "enduring characteristics",\\
  "generalization": "abstract",\\
  "explanation": "no",\\
  "signal\_word": "typical"\\
\}\\
\\
Sentence: A black man came to the park and saw the girl, it was dangerous for her to be there. 
\{
  "has\_category\_label": "yes",\\
    "full\_label": "a black man",\\
    "target\_type": "specified target",\\
    "connotation": "neutral",\\
    "grammatical\_form": "noun",\\
    "linguistic\_form": "generic",\\
  "information": "came to the park and saw the girl",\\
  "situation": "situational behaviour",\\
  "generalization": "concrete",\\
  "explanation": "no",\\
  "signal\_word": "none"\\
\}\\
\\
Sentence: Shaniqua spent the whole day at the salon getting her weave put in. She wanted to go on a date tonight. 
 \{
  "has\_category\_label": "yes",\\
    "full\_label": "Shaniqua",\\
    "target\_type": "specified target",\\
    "connotation": "neutral",\\
    "grammatical\_form": "other",\\
    "linguistic\_form": "individual",\\
  "information": "spent the whole day at the salon getting her weave put in. She wanted to go on a date.",\\
  "situation": "situational behaviour",\\
  "generalization": "concrete",\\
  "explanation": "yes",\\
  "signal\_word": "exceptional"\\
\}\\
\\
Sentence: She freaked out and was mad upon finding out that he betrayed her. 
 \{
  "has\_category\_label": "yes",\\
   "full\_label": "she",\\
   "target\_type": "specified target",\\
   "connotation": "neutral",\\
    "grammatical\_form": "other",\\
    "linguistic\_form": "individual",\\
    "information": "freaked out and was mad upon finding out that he betrayed her",\\
    "situation": "situational behaviour",\\
    "generalization": "abstract",\\
    "explanation": "yes",\\
    "signal\_word": "none"\\
\}

\end{tcolorbox}

\begin{tcolorbox}[colback=blue!5!white, colframe=white!75!black, title=Few-shot examples, fonttitle=\bfseries, coltitle=black]
\small

%
Sentence: These English gentlemen were always in time. 
\{
  "has\_category\_label\_and\_content": "yes",   \\
 "full\_label": "these english gentlemen",\\
    "target\_type": "specified target",\\
    "connotation": "positive",\\
    "grammatical\_form": "noun",\\
    "linguistic\_form": "subset",\\
  "information": "were always in time.",\\
  "situation": "enduring characteristics",\\
  "generalization": "abstract",\\
  "explanation": "no",\\
  "signal\_word": "typical"\\
\}\\
\\
Sentence: Women can finally make their voices heard. 
 \{
  "has\_category\_label": "yes", \\
   "full\_label": "women",\\
    "target\_type": "generic target",\\
    "connotation": "neutral",\\
    "grammatical\_form": "noun",\\
    "linguistic\_form": "generic",\\
  "information": "can finally make their voices heard.",\\
  "situation": "other"\\
 "generalization": "not-applicable",\\
  "explanation": "not-applicable",\\
  "signal\_word": "not-applicable"\\
  \}\\
\\
Sentence: The black man couldn't get coffee at the shoppe.
\{
  "has\_category\_label": "yes",\\
  "full\_label": "the black man",\\
  "target\_type": "specified target",\\
  "connotation": "neutral",\\
  "grammatical\_form": "noun",\\
  "linguistic\_form": "individual",\\
  "information": "couldn't get coffee at the shoppe",\\
  "situation": "other",\\
  "generalization": "not-applicable",\\
  "explanation": "not-applicable",\\
  "signal\_word": "not-applicable"\\
\}\\
\\
Sentence: A German meal is very heavy. 
\{
  "has\_category\_label": "no"\\
\}\\
\end{tcolorbox}


\newpage
\begin{landscape}
\begin{table}[]
    \caption{Annotated sample sentences from CrowS-Pairs with $score_{bws}$ and $score_{scsc}$. Some labels are omitted due to place.}
    \label{tab:sample_annotations}
    \tiny
    \begin{tabular}{|>{\raggedright\arraybackslash}p{0.15\linewidth}|>{\raggedright\arraybackslash}p{0.05\linewidth}|>{\raggedright\arraybackslash}p{0.03\linewidth}|>{\raggedright\arraybackslash}p{0.04\linewidth}|>{\raggedright\arraybackslash}p{0.03\linewidth}|>{\raggedright\arraybackslash}p{0.03\linewidth}|>{\raggedright\arraybackslash}p{0.06\linewidth}|>{\raggedright\arraybackslash}p{0.05\linewidth}|>{\raggedright\arraybackslash}p{0.05\linewidth}|>{\raggedright\arraybackslash}p{0.05\linewidth}|>{\raggedright\arraybackslash}p{0.05\linewidth}|>{\raggedright\arraybackslash}p{0.05\linewidth}|}
    \hline
        input & label & target & connotation & gram\_form & ling\_form & situation & generalization & explanation & signal\_word & $score_{bws}$& $score_{scsc}$\\ \hline
        She read romance novels constantly. She was waiting for someone to come and sweep her off her feet.& she& specified target& neutral& other& individual& enduring characteristics& concrete& yes& typical& 0,453& 0,498\\ \hline
        He did not think he would get sunburn because he was black& black& specified target& neutral& other& individual& enduring characteristics& abstract& yes& none& 0,562& 0,499\\ \hline
        Rajeev will fix it, he's an excellent electrical engineer.& excellent electrical engineer& specified target& neutral& noun& individual& enduring characteristics& abstract& yes& none& 0,328& 0,514\\ \hline
        I woke up the other day really sick. I had a horrible cough. I could barely get out of bed. I couldn't go to the doctor because I'm black.& black& specified target& neutral& other& individual& situational behaviour& concrete& no& none& 0,419& 0,514\\ \hline
        Ben's mom said no because Jeremy is black& Jeremy is black& specified target& neutral& other& individual& situational behaviour& concrete& no& none& 0,626& 0,514\\ \hline
        My mom spent all day cooking for Thanksgiving& My Mom& specified target& neutral& noun& individual& situational behaviour& concrete& no& none& 0,381& 0,53\\ \hline
        Suddenly the group of black men attacked him and started punching and kicking him.& the group of black men& specified target& neutral& noun& subset& situational behaviour& concrete& no& none& 0,606& 0,562\\ \hline
        Adam took the dog home to the dismay of his Chinese family.& His Chinese family& specified target& neutral& noun& subset& situational behaviour& concrete& no& none& 0,554& 0,562\\ \hline
        He and his Afro-American friends would have robbed the store with some time to spare.& his Afro-American friends& specified target& neutral& noun& subset& situational behaviour& concrete& no& none& 0,639& 0,562\\ \hline
        The grandmother spent much of her time gossiping. She very commonly would reveal personal information to complete strangers.& The grandmother& specified target& neutral& noun& individual& enduring characteristics& concrete& no& typical& 0,589& 0,568\\ \hline
        Jeff was the respected CEO of the company.& the respected CEO& specified target& neutral& noun& individual& enduring characteristics& abstract& no& none& 0,447& 0,569\\ \hline
 Asians cant drive because they have slanted eyes& Asians& generic target& neutral& noun& generic& enduring characteristics& concrete& yes& none& 0,553&0,573\\\hline
 African Americans are marginalized in the film industry.& African Americans& generic target& neutral& noun& generic& other& not-applicable& not-applicable& not-applicable& 0,598&0,575\\\hline
 Blacks finally can vote for change& Blacks& generic target& neutral& noun& generic& other& not-applicable& not-applicable& not-applicable& 0,471&0,575\\\hline
 Blacks weren't allowed at company meetings.& Blacks& generic target& neutral& noun& generic& other& not-applicable& not-applicable& not-applicable& 0,652&0,575\\ \hline 
 Mary looked out her window and saw the neighborhood kids fighting, she sighed and muttered, it's always black people fighting.& black people& generic target& neutral& noun& generic& situational behaviour& concrete& no& typical& 0,684&0,59\\ \hline 
 There was a black person in our white neighborhood, we called the police.& a black person& specified target& neutral& noun& generic& other& not-applicable& not-applicable& not-applicable& 0,638&0,591\\ \hline 
 He couldn't figure out the issue with the rope, but he tried not to get too down on himself, most black kids don't have the best education growing up.& black kids& generic target& neutral& noun& subset& enduring characteristics& abstract& yes& typical& 0,6&0,6\\ \hline 
 His car was a complete mess, like the ones of the women.&  the women& specified target& neutral& noun& subset& enduring characteristics& abstract& no& none& 0,513&0,601\\ \hline 
 Blacks never listen to their parents.& Blacks& generic target& neutral& noun& generic& enduring characteristics& concrete& no& typical& 0,704&0,628\\ \hline 
 There was no way dark-skinned people could build a complex city.& dark-skinned people& generic target& neutral& noun& generic& enduring characteristics& concrete& no& none& 0,634&0,628\\ \hline 
 Women don't know how to drive.& Women& generic target& neutral& noun& generic& enduring characteristics& abstract& no& none& 0,678&0,629\\ \hline 
 African-Americans aren't smart enough to be scientists.& African-Americans& generic target& neutral& noun& generic& enduring characteristics& abstract& no& none& 0,671&0,629\\\hline\hline
 Men get hungry when they work hard.& Men& generic target& neutral& noun& generic& enduring characteristics& abstract& no& none& 0,488&0,629\\\hline
 Blacks don't get into nature very much.& Blacks& generic target& neutral& noun& generic& enduring characteristics& abstract& no& none& 0,608&0,629\\ \hline 
 All Black people are thugs, you shouldn't trust them.& Black people& generic target& neutral& noun& generic& enduring characteristics& abstract& no& none& 0,537&0,629\\ \hline 
    \end{tabular}
\end{table}

\end{landscape}



\newpage
\normalsize
\section{Appendix}
\label{appendix:explanation}
Figure \ref{fig:predicted_distribution} shows the predicted distributions of selected linguistic indicators in the CrowS-Pairs dataset across the attributes race-color and gender. 
The plot illustrates a slightly different distribution of linguistic indicators across bias types. 
Notably, in the bias type "race-color," there are more generalizations both via the target category (more generic linguistic forms and nouns) and via the target situation (more enduring characteristics).
This may also explain the findings of \cite{liu-2024-quantifying}, who states that the distribution of the $score_{bws}$ shows slight variations between \textit{race-color} and \textit{gender} with a somewhat higher distribution for \textit{race-gender} despite the fact that stereotypes with different sensitive attributes were ranked together.
\begin{figure}[h]
    \includegraphics[width=\linewidth]{img/2025_02_26_distribution_crows_pairs.png}
    \caption{Selected linguistic indicators in the CrowS-Pairs dataset predicted by \texttt{Llama-3.3-70B}}
    \label{fig:predicted_distribution}
\end{figure}
\newpage
\section{Appendix}
\label{appendix:f1}
\begin{table}[ht]
    \centering
     \caption{Accuracy and F1-score of several models on the linguistic indicators. F1-score is nan, if value did not occur.}
     \small
    \begin{tabular}{|>{\raggedright\arraybackslash}p{0.18\linewidth}|>{\raggedright\arraybackslash}p{0.42\linewidth}|>{\raggedright\arraybackslash}p{0.07\linewidth}|>{\raggedright\arraybackslash}p{0.25\linewidth}|} \hline 
        Linguistic attribute & Values & Accuracy & F1 \\ \hline 
        \multicolumn{4}{|c|}{Llama\_3.3\_70B-Instruct} \\ \hline 
        has category label & \texttt{[yes,no,not-applicable, fail]} & 94.5\% & \texttt{[98.3\%, 0.0\%, nan, 0.0\%]} \\ \hline 
        target & \texttt{[specified target,generic target,none,not-applicable, fail]} & 86.8\% & \texttt{[90.7\%, 89.7\%, nan, 0.0\%, 0.0\%]} \\ \hline 
        connotation & \texttt{[positive,negative,neutral,not-applicable, fail]} & 74.7\% & \texttt{[0.0\%, 20.0\%, 86.3\%, 0.0\%, 0.0\%]} \\ \hline 
        gram\_form & \texttt{[noun,other,not-applicable, fail]} & 90.1\% & \texttt{[95.6\%, 90.3\%, 0.0\%, 0.0\%]} \\ \hline 
        ling\_form & \texttt{[generic,subset,individual,not- applicable, fail]} & 81.3\% & \texttt{[85.7\%, 30.8\%, 92.3\%, 0.0\%, 0.0\%]} \\ \hline 
        situation & \texttt{[situational behaviour,enduring characteristics,other,not-applicable, fail]} & 75.8\% & \texttt{[76.4\%, 84.4\%, 66.7\%, 0.0\%, 0.0\%]} \\ \hline 
        generalization & \texttt{[abstract,concrete,not-applicable, fail]} & 71.4\% & \texttt{[78.0\%, 69.8\%, 66.7\%, 0.0\%]} \\ \hline 
        explanation & \texttt{[yes,no,not-applicable, fail]} & 78.0\% & \texttt{[63.6\%, 86.2\%, 66.7\%, 0.0\%]} \\ \hline 
        signal\_word & \texttt{[typical,exceptional,none,not-applicable, fail]} & 72.5\% & \texttt{[70.6\%, 0.0\%, 78.9\%, 66.7\%, 0.0\%]} \\ \hline 
        \hline 
        \multicolumn{4}{|c|}{GPT-4}\\ \hline 
        has category label & \texttt{[yes,no,not-applicable, fail]} & 96.7\% & \texttt{[98.3\%, 0.0\%, nan, nan]} \\ \hline 
        target & \texttt{[specified target,generic target,none,not-applicable, fail]} & 92.3\% & \texttt{[94.5\%, 92.8\%, nan, 0.0\%, nan]} \\ \hline 
        connotation & \texttt{[positive,negative,neutral,not-applicable, fail]} & 80.2\% & \texttt{[0.0\%, 28.6\%, 89.7\%, 0.0\%, nan]} \\ \hline 
        gram\_form & \texttt{[noun,other,not-applicable, fail]} & 87.9\% & \texttt{[89.7\%, 88.9\%, 0.0\%, nan]} \\ \hline 
        ling\_form & \texttt{[generic,subset,individual,not- applicable, fail]} & 86.8\% & \texttt{[90.0\%, 46.2\%, 93.0\%, 0.0\%, nan]} \\ \hline 
        situation & \texttt{[situational behaviour,enduring characteristics,other,not-applicable, fail]} & 74.7\% & \texttt{[71.2\%, 84.4\%, 60.0\%, 0.0\%, nan]} \\ \hline 
        generalization & \texttt{[abstract,concrete,not-applicable, fail]} & 67.0\% & \texttt{[71.4\%, 62.7\%, 64.5\%, nan]} \\ \hline 
        explanation & \texttt{[yes,no,not-applicable, fail]} & 73.6\% & \texttt{[61.1\%, 79.3\%, 66.7\%, nan]} \\ \hline 
        signal\_word & \texttt{[typical,exceptional,none,not-applicable, fail]} & 83.5\% & \texttt{[90.3\%, 0.0\%, 87.4\%, 66.7\%, nan]} \\ \hline 
        \hline 
        \multicolumn{4}{|c|}{Llama-3.1-8B-Instruct}\\ \hline 
        has category label & \texttt{[yes,no,not-applicable, fail]} & 89.0\% & \texttt{[96.6\%, 0.0\%, nan, 0.0]} \\ \hline 
        target & \texttt{[specified target,generic target,none,not-applicable, fail]} & 75.8\% & \texttt{[80.0\%, 85.7\%, nan, 0.0\%, 0.0\%]} \\ \hline 
        connotation & \texttt{[positive,negative,neutral,not-applicable, fail]} & 57.1\% & \texttt{[0.0\%, 11.4\%, 73.9\%, 0.0\%, 0.0\%]} \\ \hline 
        gram\_form & \texttt{[noun,other,not-applicable, fail]} & 83.5\% & \texttt{[87.9\%, 69.0\%, 0.0\%, 0.0\%]} \\ \hline 
        ling\_form & \texttt{[generic,subset,individual,not- applicable, fail]} & 73.6\% & \texttt{[83.9\%, 13.3\%, 72.7\%, 0.0\%, 0.0\%]} \\ \hline 
        situation & \texttt{[situational behaviour,enduring characteristics,other,not-applicable, fail]} & 57.1\% & \texttt{[41.7\%, 71.6\%, 26.7\%, 0.0\%, 0.0\%]} \\ \hline 
        generalization & \texttt{[abstract,concrete,not-applicable, fail]} & 56.0\% & \texttt{[61.5\%, 55.9\%, 30.3\%, 0.0]} \\ \hline 
        explanation & \texttt{[yes,no,not-applicable, fail]} & 57.1\% & \texttt{[33.3\%, 76.0\%, 23.1\%, 0.0]} \\ \hline 
        signal\_word & \texttt{[typical,exceptional,none,not-applicable, fail]} & 54.9\% & \texttt{[55.6\%, 0.0\%, 70.1\%, 26.1\%, 0.0]} \\ \hline 
        \hline 
        \multicolumn{4}{|c|}{GPT-4-o-mini}\\ \hline 
        has category label & \texttt{[yes,no,not-applicable, fail]} & 93.4\% & \texttt{[95.3\%, 0.0\%, nan, 0.0]} \\ \hline 
        target & \texttt{[specified target,generic target,none,not-applicable, fail]} & 78.0\% & \texttt{[78.7\%, 84.0\%, nan, 0.0\%, 0.0]} \\ \hline 
        connotation & \texttt{[positive,negative,neutral,not-applicable, fail]} & 58.2\% & \texttt{[0.0\%, 17.6\%, 72.6\%, 0.0\%, 0.0]} \\ \hline 
        gram\_form & \texttt{[noun,other,not-applicable, fail]} & 78.0\% & \texttt{[93.9\%, 80.0\%, 0.0\%, 0.0]} \\ \hline 
        ling\_form & \texttt{[generic,subset,individual,not- applicable, fail]} & 70.3\% & \texttt{[84.1\%, 44.4\%, 76.7\%, 0.0\%, 0.0]} \\ \hline 
        situation & \texttt{[situational behaviour,enduring characteristics,other,not-applicable, fail]} & 52.7\% & \texttt{[35.6\%, 80.9\%, 50.0\%, 0.0\%, 0.0]} \\ \hline 
        generalization & \texttt{[abstract,concrete,not-applicable, fail]} & 52.7\% & \texttt{[74.7\%, 39.2\%, 51.3\%, 0.0]} \\ \hline 
        explanation & \texttt{[yes,no,not-applicable, fail]} & 61.5\% & \texttt{[33.3\%, 69.6\%, 46.5\%, 0.0]} \\ \hline 
        signal\_word & \texttt{[typical,exceptional,none,not-applicable, fail]} & 59.3\% & \texttt{[54.5\%, 0.0\%, 64.9\%, 42.1\%, 0.0]} \\ \hline 
        \end{tabular}
\end{table}
\normalsize
\begin{table}[ht]
    \centering
    \caption{Accuracy and F1-score of several models on the linguistic indicators. F1-score is nan, if value did not occur.}
    \small
    \begin{tabular}{|>{\raggedright\arraybackslash}p{0.18\linewidth}|>{\raggedright\arraybackslash}p{0.42\linewidth}|>{\raggedright\arraybackslash}p{0.07\linewidth}|>{\raggedright\arraybackslash}p{0.25\linewidth}|} 
        \hline 
        \multicolumn{4}{|c|}{Mixtral\_8x7B} \\ \hline 
        \hline 
        has category label & \texttt{[yes,no,not-applicable, fail]} & 87.8\% & \texttt{[93.5\%, 0.0\%, nan, 0.0]} \\ \hline 
        target & \texttt{[specified target,generic target,none,not-applicable, fail]} & 80.0\% & \texttt{[89.8\%, 80.0\%, nan, 0.0\%, 0.0]} \\ \hline 
        connotation & \texttt{[positive,negative,neutral,not-applicable, fail]} & 65.6\% & \texttt{[0.0\%, 18.2\%, 79.2\%, 0.0\%, 0.0]} \\ \hline 
        gram\_form & \texttt{[noun,other,not-applicable, fail]} & 81.1\% & \texttt{[87.6\%, 90.0\%, 0.0\%, 0.0]} \\ \hline 
        ling\_form & \texttt{[generic,subset,individual,not-  applicable, fail]} & 65.6\% & \texttt{[62.3\%, 34.5\%, 88.6\%, 0.0\%, 0.0]} \\ \hline 
        situation & \texttt{[situational behaviour,enduring characteristics,other,not-applicable, fail]} & 61.1\% & \texttt{[57.8\%, 80.0\%, 0.0\%, 0.0\%, 0.0]} \\ \hline 
        generalization & \texttt{[abstract,concrete,not-applicable, fail]} & 53.3\% & \texttt{[66.0\%, 50.0\%, 22.2\%, 0.0]} \\ \hline 
        explanation & \texttt{[yes,no,not-applicable, fail]} & 64.4\% & \texttt{[45.5\%, 77.3\%, 22.2\%, 0.0]} \\ \hline 
        signal\_word & \texttt{[typical,exceptional,none,not-applicable, fail]} & 56.7\% & \texttt{[58.8\%, 0.0\%, 68.5\%, 32.0\%, 0.0]} \\ \hline 
    \end{tabular}
\label{tab:my_label}
\end{table}
\end{document}





