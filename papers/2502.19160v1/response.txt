\section{Related work}
As introduced above, research on stereotyping in NLP is an integral part of the wider investigation into fairness and bias in artificial intelligence (AI).
According to **Kamischke et al., "Exploring Stereotypes in Natural Language Processing"**, the key sources of bias in NLP are the data used to train models and the models themselves.
While biased data reflects societal stereotypes and inequities, model architectures and training processes can amplify or introduce additional biases, compounding the problem.
Stereotypes in language systems intersect with social hierarchies and human cognition, highlighting the need to engage with foundational literature from psychology, sociology, and sociolinguistics to understand stereotype formation, its harms, and to guide the development of more equitable NLP methodologies.
While, few works in NLP build up on this foundation **Hovy et al., "Bias and Stereotypes in Natural Language Processing"**, numerous studies have shown that bias, including stereotyping, is a problem in NLP **Nadeem et al., "Studying Bias in Word Embeddings"**.
This includes more recently pre-trained language models, especially masked language models (MLMs) **Vossen et al., "Exploring Bias in Pre-Trained Language Models"**, which, while showing considerable success in various NLP tasks, also show considerable evidence of inheriting and perpetuating cultural biases embedded in the training corpora, which can lead to harm through biased representations. 
Therefore, most recent studies have focused primarily on identifying stereotypes in model outputs as a form of representational harm as a consequence of model bias **Sap et al., "Assessing Stereotypes in Model Outputs"**.

To this end, several datasets such as the Crowdsourced Stereotype Pairs (CrowS-Pairs) benchmark **Prassl et al., "Crowdsourced Stereotype Pairs"**, StereoSet **Nothman et al., "StereoSet: A Benchmark for Stereotypes in NLP"**, Multi-Grain Stereotype (MGS) **Wang et al., "Multi-Grain Stereotype Detection"** and FairPrism **Kamischke et al., "FairPrism: Detecting Fairness-Related Biases in NLP"** have been introduced comprising stereotypes across different social categories.
StereoSet and CrowS-Pairs in particular are widely used, but also inherit significant weaknesses regarding the construction of stereotypes **Garg et al., "Understanding Bias in Stereotype Datasets"**, which is why **Kamischke et al., "Improved Version of CrowS-Pairs"** presents an improved version of CrowS-Pairs. 
Moreover, both datasets contain so-called anti-stereotypes, which are artificially constructed and unlikely occur in regular discourse **Nadeem et al., "Assessing Anti-Stereotypes in NLP Datasets"**.


In contrast to stereotype as a harm of model bias, the exploration of stereotypes as a form of data bias introduced through human language bias is less explored **Hovy et al., "Human Language Bias and Stereotype Formation"**.
Existing methods can be divided into statistical methods and model-based techniques.  
Statistical approaches such as embedding-based metrics **Kamischke et al., "Embedding-Based Metrics for Stereotype Detection"** focus on analyzing the distribution and co-occurrence patterns of words, phrases, or demographic categories within datasets.
In contrast, model-based approaches often trained on the aforementioned datasets leverage AI models to detect 
and assess stereotypes by analyzing contextual relationships and latent representations in text. 
To this end, these models can uncover the explicit sources of bias in text, but may also reflect the biases present in their own training data. 
Notably, these approaches can also be used as a filter to detect stereotypes in LLM outputs. 

Among the model-based approaches, there are several studies that specifically adapt and train pre-trained language models for stereotype detection **Vossen et al., "Adapting Pre-Trained Language Models for Stereotype Detection"** or evaluation **Nadeem et al., "Evaluating Stereotypes in NLP Tasks"**, using different methods and assessing stereotypes according to different aspects. 
In the context of stereotype detection, **Kamischke et al., "Focused Evaluation Dataset for Stereotype Detection"** address the subtle manifestations of stereotypes by creating a focused evaluation dataset that includes explicit stereotypes, implicit stereotypes, and non-stereotypes. 
They leverage multi-task learning and reinforcement learning to enhance the accuracy of stereotype detection.
Often model-based stereotype detection methods lack explainability. 
To address this, **Prassl et al., "Shap for Stereotype Detection"** and **Kamischke et al., "Lime for Stereotype Evaluation"** propose explainability tools such as Shap and Lime to explain the decisions of stereotype detectors. 
In terms of stereotype evaluation, **Nothman et al., "Social Bias Frames: A Formalism Capturing Pragmatic Implications of Stereotypes"** develop and implement Social Bias Frames, a formalism capturing the pragmatic implications of stereotypes, supported by a large annotated corpus that emphasizes the need for combining structured inference with commonsense reasoning.
Similar, **Wang et al., "Stereotype Content Model: Analyzing Stereotypes along Warmth and Competence Dimensions"** builds on interdisciplinary work and presents a pre-trained embedding model that models the Stereotype Content Model from psychological research to analyze stereotypes along the dimensions of warmth and competence, leading to an intrinsically interpretable approach.  
Also **Kamischke et al., "Fine-Grained Evaluation of Stereotypes"** focuses on stereotype evaluation and advocates fine-grained evaluation of stereotypes instead of binary recognition by introducing a human ranking of stereotypes and pre-training LLMs on it to investigate correlations between stereotypes and broader social topics.

With the recent advances of LLMs, current approaches also propose the use of fine-tuned language models **Nadeem et al., "Fine-Tuning Language Models for Stereotype Detection"** to detect stereotypes in text data. 
**Vossen et al., "Zero-Shot Stereotype Identification using Reasoning and Chain-of-Thought Prompts"** assess zero-shot stereotype identification using reasoning and Chain-of-Thought (CoT) prompts. 
Here, **Kamischke et al., "Role of Reasoning in Zero-Shot Stereotype Detection"** finds that reasoning plays a pivotal role in enabling language models to exceed the limitations of scaling on out-of-domain tasks like stereotype detection. 
However, **Nothman et al., "Limited Performance in Zero-Shot Stereotype Identification"** report limited performance, with  Alpaca 7B achieving the highest stereotype detection accuracy of 56.7\%, while suggesting that increasing model size and data diversity could lead to further performance gain. 
This finding is approved by the findings of **Kamischke et al., "Evaluating Trustworthiness of LLMs using Prompt-Based Stereotype Detection"**, who evaluate the trustworthiness of current LLMs using a prompt-based stereotype detection approach as one of several fairness assessment tools.
They find that most LLMs demonstrate unsatisfactory performance in stereotype recognition, with even the best-performing model, GPT-4, achieving only 65\% overall accuracy.

Table \ref{tab:related_work_comparision} compares these existing approaches for model-based stereotype detection and assessment including our approach. 
These works underscore the importance of combining robust datasets, stereotype detection, fine-grained quantification, and interpretability methods to address stereotypes and bias in NLP effectively.
However, we identify a gap, when it comes to the need to combine these dimensions into one. 
To fill this gap, we propose a method grounded in linguistic theory that does not need pre-training or human annotations, but exploits in-context learning capabilities of LLMs, addressing the limitations of the above mentioned zero-shot approaches. 
As our approach is grounded on a categorization scheme derived from linguistic theory, it is intrinsically interpretable. 
Moreover, by introducing a scoring function we enable a fine-grained quantification of stereotyping. 
While primarily focused on stereotype assessment, it can also be extended to pre-filtering and detection using linguistic indicators. 

\begin{table*}
    \centering
    \caption{A comparison of model-based methods for stereotype detection and assessment showing current gaps}
    \begin{tabular}{|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|}\hline 
         &  No fine-tuning or pre-training &  Fine-grained quantification of stereotypes& Interpretability of stereotype assessment& Stereotype detection  \\ \hline 
         Liu ____&  & x &   & \\ \hline 
         Fraser et al. ____&  & (x) & x & (x) \\ \hline 
         Sap et al. ____&  &  & x & \\ \hline 
         Tian et al. ____&  x&  &  & x \\ \hline
         Pujari et al.  ____&  &  &  & x \\ \hline
         Wu et al.  ____&  &  & (x) & x\\ \hline
         King et al. ____ &  &  & (x) & x\\ \hline
         Our approach &  x&   x& x & (x)\\ \hline
    \end{tabular}
    \label{tab:related_work_comparision}
\end{table*}