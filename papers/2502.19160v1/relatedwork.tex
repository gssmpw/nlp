\section{Related work}
As introduced above, research on stereotyping in NLP is an integral part of the wider investigation into fairness and bias in artificial intelligence (AI).
According to \citeauthor{Hovy2021}, the key sources of bias in NLP are the data used to train models and the models themselves.
While biased data reflects societal stereotypes and inequities, model architectures and training processes can amplify or introduce additional biases, compounding the problem.
Stereotypes in language systems intersect with social hierarchies and human cognition, highlighting the need to engage with foundational literature from psychology, sociology, and sociolinguistics to understand stereotype formation, its harms, and to guide the development of more equitable NLP methodologies.
While, few works in NLP build up on this foundation \cite{blodgett_language_2020}, numerous studies have shown that bias, including stereotyping, is a problem in NLP \cite{bolukbasi_man_nodate, caliskan_semantics_2017, wang_decodingtrust_2024}.
This includes more recently pre-trained language models, especially masked language models (MLMs) \cite{nangia_crows-pairs_2020, soundararajan-delany-2024-investigating, bai_measuring_2024}, which, while showing considerable success in various NLP tasks, also show considerable evidence of inheriting and perpetuating cultural biases embedded in the training corpora, which can lead to harm through biased representations. 
Therefore, most recent studies have focused primarily on identifying stereotypes in model outputs as a form of representational harm as a consequence of model bias \cite{gallegos_bias_2023}.

To this end, several datasets such as the Crowdsourced Stereotype Pairs (CrowS-Pairs) benchmark \cite{nangia_crows-pairs_2020}, StereoSet \cite{nadeem2021stereoset}, Multi-Grain Stereotype (MGS) \cite{wu_auditing_2024} and FairPrism \cite{fleisig_fairprism_2023} have been introduced comprising stereotypes across different social categories.
StereoSet and CrowS-Pairs in particular are widely used, but also inherit significant weaknesses regarding the construction of stereotypes \cite{blodgett_stereotyping_2021}, which is why \cite{neveol-etal-2022-french} presents an improved version of CrowS-Pairs. 
Moreover, both datasets contain so-called anti-stereotypes, which are artificially constructed and unlikely occur in regular discourse \cite{pujari_reinforcement_2022}.


In contrast to stereotype as a harm of model bias, the exploration of stereotypes as a form of data bias introduced through human language bias is less explored \cite{fraser_computational_2022}.
Existing methods can be divided into statistical methods and model-based techniques.  
Statistical approaches such as embedding-based metrics \cite{gallegos_bias_2023} focus on analyzing the distribution and co-occurrence patterns of words, phrases, or demographic categories within datasets.
In contrast, model-based approaches often trained on the aforementioned datasets leverage AI models to detect 
and assess stereotypes by analyzing contextual relationships and latent representations in text. 
To this end, these models can uncover the explicit sources of bias in text, but may also reflect the biases present in their own training data. 
Notably, these approaches can also be used as a filter to detect stereotypes in LLM outputs. 

Among the model-based approaches, there are several studies that specifically adapt and train pre-trained language models for stereotype detection \cite{pujari_reinforcement_2022} or evaluation \cite{sap_social_2020, fraser_computational_2022, liu-2024-quantifying}, using different methods and assessing stereotypes according to different aspects. 
In the context of stereotype detection, \citeauthor{pujari_reinforcement_2022} address the subtle manifestations of stereotypes by creating a focused evaluation dataset that includes explicit stereotypes, implicit stereotypes, and non-stereotypes. 
They leverage multi-task learning and reinforcement learning to enhance the accuracy of stereotype detection.
Often model-based stereotype detection methods lack explainability. 
To address this, \citeauthor{wu_auditing_2024} and \citeauthor{king2024hearts} propose explainability tools such as Shap and Lime \cite{Salih_2024}  to explain the decisions of stereotype detectors. 
In terms of stereotype evaluation, \citeauthor{sap_social_2020} develop and implement Social Bias Frames, a formalism capturing the pragmatic implications of stereotypes, supported by a large annotated corpus that emphasizes the need for combining structured inference with commonsense reasoning.
Similar, \citeauthor{fraser_computational_2022} builds on interdisciplinary work and presents a pre-trained embedding model that models the Stereotype Content Model \cite{fiske2007universal} from psychological research to analyze stereotypes along the dimensions of warmth and competence, leading to an intrinsically interpretable approach.  
Also \citeauthor{liu-2024-quantifying} focuses on stereotype evaluation and advocates fine-grained evaluation of stereotypes instead of binary recognition by introducing a human ranking of stereotypes and pre-training LLMs on it to investigate correlations between stereotypes and broader social topics.

With the recent advances of LLMs, current approaches also propose the use of fine-tuned language models \cite{tian2023using,dige_can_2023, huang2024trustllm} to detect stereotypes in text data. 
\citeauthor{tian2023using} and \citeauthor{dige_can_2023} assess zero-shot stereotype identification using reasoning and Chain-of-Thought (CoT) prompts. 
Here, \citeauthor{tian2023using} finds that reasoning plays a pivotal role in enabling language models to exceed the limitations of scaling on out-of-domain tasks like stereotype detection. 
However, \citeauthor{dige_can_2023}, report limited performance, with  Alpaca 7B achieving the highest stereotype detection accuracy of 56.7\%, while suggesting that increasing model size and data diversity could lead to further performance gain. 
This finding is approved by the findings of \citeauthor{huang2024trustllm}, who evaluate the trustworthiness of current LLMs using a prompt-based stereotype detection approach as one of several fairness assessment tools.
They find that most LLMs demonstrate unsatisfactory performance in stereotype recognition, with even the best-performing model, GPT-4, achieving only 65\% overall accuracy.

Table \ref{tab:related_work_comparision} compares these existing approaches for model-based stereotype detection and assessment including our approach. 
These works underscore the importance of combining robust datasets, stereotype detection, fine-grained quantification, and interpretability methods to address stereotypes and bias in NLP effectively.
However, we identify a gap, when it comes to the need to combine these dimensions into one. 
To fill this gap, we propose a method grounded in linguistic theory that does not need pre-training or human annotations, but exploits in-context learning capabilities of LLMs, addressing the limitations of the above mentioned zero-shot approaches. 
As our approach is grounded on a categorization scheme derived from linguistic theory, it is intrinsically interpretable. 
Moreover, by introducing a scoring function we enable a fine-grained quantification of stereotyping. 
While primarily focused on stereotype assessment, it can also be extended to pre-filtering and detection using linguistic indicators. 

\begin{table*}
    \centering
    \caption{A comparison of model-based methods for stereotype detection and assessment showing current gaps}
    \begin{tabular}{|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|>{\centering\arraybackslash}p{0.15\textwidth}|}\hline 
         &  No fine-tuning or pre-training &  Fine-grained quantification of stereotypes& Interpretability of stereotype assessment& Stereotype detection  \\ \hline 
         Liu \cite{liu-2024-quantifying}&  & x &   & \\ \hline 
         Fraser et al. \cite{fraser_computational_2022}&  & (x) & x & (x) \\ \hline 
         Sap et al. \cite{sap_social_2020}&  &  & x & \\ \hline 
         Tian et al. \cite{tian2023using}&  x&  &  & x \\ \hline
         Pujari et al.  \cite{pujari_reinforcement_2022}&  &  &  & x \\ \hline
         Wu et al.  \cite{wu_auditing_2024}&  &  & (x) & x\\ \hline
         King et al. \cite{king2024hearts} &  &  & (x) & x\\ \hline
         Our approach &  x&   x& x & (x)\\ \hline
    \end{tabular}
    \label{tab:related_work_comparision}
\end{table*}