@article{sun2024unveiling,
  title={Unveiling Incomplete Modality Brain Tumor Segmentation: Leveraging Masked Predicted Auto-Encoder and Divergence Learning},
  author={Sun, Zhongao and Li, Jiameng and Wang, Yuhan and Cheng, Jiarong and Zhou, Qing and Li, Chun},
  journal={arXiv preprint arXiv:2406.08634},
  year={2024}
}

@inproceedings{bai2023masked,
  title={Masked autoencoders enable efficient knowledge distillers},
  author={Bai, Yutong and Wang, Zeyu and Xiao, Junfei and Wei, Chen and Wang, Huiyu and Yuille, Alan L and Zhou, Yuyin and Xie, Cihang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={24256--24265},
  year={2023}
}
@inproceedings{zou2023reliable,
  title={Reliable multimodality eye disease screening via mixture of student’st distributions},
  author={Zou, Ke and Lin, Tian and Yuan, Xuedong and Chen, Haoyu and Shen, Xiaojing and Wang, Meng and Fu, Huazhu},
  booktitle={MICCAI},
  year={2023},
}
@inproceedings{jue2019integrating,
  title={Integrating cross-modality hallucinated MRI with CT to aid mediastinal lung tumor segmentation},
  author={Jue, Jiang and Jason, Hu and Neelam, Tyagi and Andreas, Rimner and Sean, Berry L and Joseph, Deasy O and Harini, Veeraraghavan},
  booktitle={MICCAI},
  year={2019},
}

@article{liu2021face,
  title={Face anti-spoofing via adversarial cross-modality translation},
  author={Liu, Ajian and Tan, Zichang and Wan, Jun and Liang, Yanyan and Lei, Zhen and Guo, Guodong and Li, Stan Z},
  journal={IEEE Transactions on Information Forensics and Security},
  year={2021},
  publisher={IEEE}
}

@inproceedings{wang2023learnable,
  title={Learnable cross-modal knowledge distillation for multi-modal learning with missing modality},
  author={Wang, Hu and Ma, Congbo and Zhang, Jianpeng and Zhang, Yuan and Avery, Jodie and Hull, Louise and Carneiro, Gustavo},
  booktitle={MICCAI},
  year={2023},
}

@inproceedings{liu2023m3ae,
  title={M3AE: multimodal representation learning for brain tumor segmentation with missing modalities},
  author={Liu, Hong and Wei, Dong and Lu, Donghuan and Sun, Jinghan and Wang, Liansheng and Zheng, Yefeng},
  booktitle={AAAI},
  year={2023}
}

@article{shi2024passion,
  title={PASSION: Towards Effective Incomplete Multi-Modal Medical Image Segmentation with Imbalanced Missing Rates},
  author={Shi, Junjie and Shang, Caozhi and Sun, Zhaobin and Yu, Li and Yang, Xin and Yan, Zengqiang},
  journal={arXiv preprint arXiv:2407.14796},
  year={2024}
}

@inproceedings{wang2023fundus,
  title={Fundus-enhanced disease-aware distillation model for retinal disease classification from OCT images},
  author={Wang, Lehan and Dai, Weihang and Jin, Mei and Ou, Chubin and Li, Xiaomeng},
  booktitle={MICCAI},
  year={2023},
}

@inproceedings{wang2023prototype,
  title={Prototype knowledge distillation for medical segmentation with missing modality},
  author={Wang, Shuai and Yan, Zipei and Zhang, Daoan and Wei, Haining and Li, Zhongsen and Li, Rui},
  booktitle={ICASSP},
  year={2023},
  organization={IEEE}
}

@inproceedings{li2024correlation,
  title={Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities},
  author={Li, Mingcheng and Yang, Dingkang and Zhao, Xiao and Wang, Shuaibing and Wang, Yan and Yang, Kun and Sun, Mingyang and Kou, Dongliang and Qian, Ziyun and Zhang, Lihua},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{ma2021smil,
  title={Smil: Multimodal learning with severely missing modality},
  author={Ma, Mengmeng and Ren, Jian and Zhao, Long and Tulyakov, Sergey and Wu, Cathy and Peng, Xi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={2302--2310},
  year={2021}
}

@inproceedings{wang2023multi,
  title={Multi-modal learning with missing modality via shared-specific feature modelling},
  author={Wang, Hu and Chen, Yuanhong and Ma, Congbo and Avery, Jodie and Hull, Louise and Carneiro, Gustavo},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{ma2022multimodal,
  title={Are multimodal transformers robust to missing modality?},
  author={Ma, Mengmeng and Ren, Jian and Zhao, Long and Testuggine, Davide and Peng, Xi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18177--18186},
  year={2022}
}

@inproceedings{liu2023sfusion,
  title={SFusion: Self-attention based n-to-one multimodal fusion block},
  author={Liu, Zecheng and Wei, Jia and Li, Rui and Zhou, Jianlong},
  booktitle={MICCAI},
  year={2023},
}
@article{duan2024towards,
  title={Towards Arbitrary-Scale Histopathology Image Super-resolution: An Efficient Dual-branch Framework via Implicit Self-texture Enhancement},
  author={Duan, Minghong and Qu, Linhao and Yang, Zhiwei and Wang, Manning and Zhang, Chenxi and Song, Zhijian},
  journal={arXiv preprint arXiv:2401.15613},
  year={2024}
}
@article{zhang4,
  title={Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs},
  author={Zhang, Xiaofeng and Quan, Yihao and Gu, Chaochen and Shen, Chen and Yuan, Xiaosong and Yan, Shaotian and Cheng, Hao and Wu, Kaijie and Ye, Jieping},
  journal={arXiv preprint arXiv:2411.09968},
  year={2024}
}

@inproceedings{qu2023openal,
  title={Openal: An efficient deep active learning framework for open-set pathology image classification},
  author={Qu, Linhao and Ma, Yingfan and Yang, Zhiwei and Wang, Manning and Song, Zhijian},
  booktitle={MICCAI},
  pages={3--13},
  year={2023},
  organization={Springer}
}
@article{wang2023knowledge,
  title={Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models},
  author={Wang, Shuo and Zhu, Yan and Luo, Xiaoyuan and Yang, Zhiwei and Zhang, Yizhe and Fu, Peiyao and Wang, Manning and Song, Zhijian and Li, Quanlin and Zhou, Pinghong and others},
  journal={arXiv preprint arXiv:2310.11173},
  year={2023}
}
@inproceedings{li2024tp,
  title={TP-DRSeg: improving diabetic retinopathy lesion segmentation with explicit text-prompts assisted SAM},
  author={Li, Wenxue and Xiong, Xinyu and Xia, Peng and Ju, Lie and Ge, Zongyuan},
  booktitle={MICCAI},
  year={2024},
}
@article{xiong2024sam2,
  title={Sam2-unet: Segment anything 2 makes strong encoder for natural and medical image segmentation},
  author={Xiong, Xinyu and Wu, Zihuang and Tan, Shuangyi and Li, Wenxue and Tang, Feilong and Chen, Ying and Li, Siying and Ma, Jie and Li, Guanbin},
  journal={arXiv preprint arXiv:2408.08870},
  year={2024}
}

@inproceedings{peng2023usage,
  title={Usage: A unified seed area generation paradigm for weakly supervised semantic segmentation},
  author={Peng, Zelin and Wang, Guanchun and Xie, Lingxi and Jiang, Dongsheng and Shen, Wei and Tian, Qi},
  booktitle={ICCV},
  year={2023}
}

@article{trinh2024sight,
  title={Sight for sore heads--using cnns to diagnose migraines},
  author={Trinh, Matt and Tang, Feilong and Ly, Angelica and Duong, Annita and Stapleton, Fiona and Ge, Zongyuan and Razzak, Imran},
  journal={Investigative Ophthalmology \& Visual Science},
  year={2024},
}
@article{tang2024discriminating,
  title={Discriminating retinal microvascular and neuronal differences related to migraines: Deep Learning based Crossectional Study},
  author={Tang, Feilong and Trinh, Matt and Duong, Annita and Ly, Angelica and Stapleton, Fiona and Chen, Zhe and Ge, Zongyuan and Razzak, Imran},
  journal={arXiv preprint arXiv:2408.07293},
  year={2024}
}
@inproceedings{chen2024towards,
  title={Towards generalizable tumor synthesis},
  author={Chen, Qi and Chen, Xiaoxi and Song, Haorui and Xiong, Zhiwei and Yuille, Alan and Wei, Chen and Zhou, Zongwei},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{ding2021rfnet,
  title={RFNet: Region-aware fusion network for incomplete multi-modal brain tumor segmentation},
  author={Ding, Yuhang and Yu, Xin and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={3975--3984},
  year={2021}
}

@article{tripathi2024multimodal,
  title={Multimodal transformer model improves survival prediction in lung cancer compared to unimodal approaches},
  author={Tripathi, Aakash and Waqas, Asim and Yilmaz, Yasin and Rasool, Ghulam},
  journal={Cancer Research},
  year={2024},
}
@inproceedings{li2023decoupled,
  title={Decoupled multimodal distilling for emotion recognition},
  author={Li, Yong and Wang, Yuanzhi and Cui, Zhen},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{zhao2022decoupled,
  title={Decoupled knowledge distillation},
  author={Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
  booktitle={CVPR},
  year={2022}
}

@article{Karri_Chakraborty_Chatterjee_2017,   title={Transfer learning based classification of optical coherence tomography images with diabetic macular edema and dry age-related macular degeneration},  url={http://dx.doi.org/10.1364/boe.8.000579},  DOI={10.1364/boe.8.000579},  journal={Biomedical Optics Express},  author={Karri, S. P. K. and Chakraborty, Debjani and Chatterjee, Jyotirmoy},  year={2017},  month={Feb},  pages={579},  language={en-US}  }

@article{ebrahimi2023lanistr,
  title={Lanistr: Multimodal learning from structured and unstructured data},
  author={Ebrahimi, Sayna and Arik, Sercan O and Dong, Yihe and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.16556},
  year={2023}
}

@article{de2018clinically,
  title={Clinically applicable deep learning for diagnosis and referral in retinal disease},
  author={De Fauw, Jeffrey and Ledsam, Joseph R and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O’Donoghue, Brendan and Visentin, Daniel and others},
  journal={Nature medicine},
  volume={24},
  number={9},
  pages={1342--1350},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{watanabe2022combining,
  title={Combining optical coherence tomography and fundus photography to improve glaucoma screening},
  author={Watanabe, Tomoyuki and Hiratsuka, Yoshimune and Kita, Yoshiyuki and Tamura, Hiroshi and Kawasaki, Ryo and Yokoyama, Tetsuji and Kawashima, Motoko and Nakano, Tadashi and Yamada, Masakazu},
  journal={Diagnostics},
  volume={12},
  number={5},
  pages={1100},
  year={2022},
  publisher={MDPI}
}

@article{Burlina2017AutomatedGO,
  title={Automated Grading of Age-Related Macular Degeneration From Color Fundus Images Using Deep Convolutional Neural Networks},
  author={Philippe Burlina and Neil J. Joshi and Michael J. Pekala and Katia D. Pacheco and David E. Freund and Neil M. Bressler},
  journal={JAMA Ophthalmology},
  year={2017},
  volume={135},
  pages={1170–1176},
  url={https://api.semanticscholar.org/CorpusID:36464289}
}
@article{Septiarini2018AutomaticGD,
  title={Automatic Glaucoma Detection Method Applying a Statistical Approach to Fundus Images},
  author={Anindita Septiarini and Dyna Marisa Khairina and Awang Harsa Kridalaksana and Hamdani Hamdani},
  journal={Healthcare Informatics Research},
  year={2018},
  volume={24},
  pages={53 - 60},
  url={https://api.semanticscholar.org/CorpusID:3625741}
}

@article{Hua_Kim_Huynh-The_You_Yu_Le-Tien_Bae_Lee_2021,   title={Convolutional Network With Twofold Feature Augmentation for Diabetic Retinopathy Recognition From Multi-Modal Images},  url={http://dx.doi.org/10.1109/jbhi.2020.3041848},  DOI={10.1109/jbhi.2020.3041848},  journal={IEEE Journal of Biomedical and Health Informatics},  author={Hua, Cam-Hao and Kim, Kiyoung and Huynh-The, Thien and You, Jong In and Yu, Seung-Young and Le-Tien, Thuong and Bae, Sung-Ho and Lee, Sungyoung},  year={2021},  month={Jul},  pages={2686–2697},  language={en-US}  }

@article{wang2022learning,
  title={Learning two-stream CNN for multi-modal age-related macular degeneration categorization},
  author={Wang, Weisen and Li, Xirong and Xu, Zhiyan and Yu, Weihong and Zhao, Jianchun and Ding, Dayong and Chen, Youxin},
  journal={JBHI},
  year={2022},
}

@inproceedings{yan2023vf,
  title={VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia},
  author={Yan, Zipei and Liang, Dong and Xu, Linchuan and Li, Jiahang and Liu, Zhengji and Wang, Shuai and Cao, Jiannong and Kee, Chea-su},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={649--659},
  year={2023},
  organization={Springer}
}
@inproceedings{sun2024redcore,
  title={RedCore: Relative Advantage Aware Cross-Modal Representation Learning for Missing Modalities with Imbalanced Missing Rates},
  author={Sun, Jun and Zhang, Xinxin and Han, Shoukang and Ruan, Yu-Ping and Li, Taihao},
  booktitle={AAAI},
  year={2024}
}
@inproceedings{wang2023distribution,
  title={Distribution-consistent modal recovering for incomplete multimodal learning},
  author={Wang, Yuanzhi and Cui, Zhen and Li, Yong},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{deldari2024crossl,
  title={Crossl: Cross-modal self-supervised learning for time-series through latent masking},
  author={Deldari, Shohreh and Spathis, Dimitris and Malekzadeh, Mohammad and Kawsar, Fahim and Salim, Flora D and Mathur, Akhil},
  booktitle={Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
  pages={152--160},
  year={2024}
}
@inproceedings{li2023missing,
  title={Missing-Modal Face Anti-Spoofing: A Benchmark},
  author={Li, Xuan and Li, Chen and Hu, Jia},
  booktitle={2023 8th International Conference on Image, Vision and Computing},
  year={2023},
  organization={IEEE}
}
@inproceedings{miao2023multimodal,
  title={Multimodal Sentiment Analysis based on Supervised Contrastive Learning and Cross-modal Translation under Modalities Missing},
  author={Miao, Yuqing and Zhu, Gaowei and Liu, Tonglai and Zhang, Wanzhen and Wen, Yimin and Zhou, Ming},
  booktitle={2023 IEEE 14th International Symposium on Parallel Architectures, Algorithms and Programming},
  year={2023},
  organization={IEEE}
}
@article{poudel2024car,
  title={CAR-MFL: Cross-Modal Augmentation by Retrieval for Multimodal Federated Learning with Missing Modalities},
  author={Poudel, Pranav and Shrestha, Prashant and Amgain, Sanskar and Shrestha, Yash Raj and Gyawali, Prashnna and Bhattarai, Binod},
  journal={arXiv preprint arXiv:2407.08648},
  year={2024}
}
@inproceedings{xiong2023client,
  title={Client-Adaptive Cross-Model Reconstruction Network for Modality-Incomplete Multimodal Federated Learning},
  author={Xiong, Baochen and Yang, Xiaoshan and Song, Yaguang and Wang, Yaowei and Xu, Changsheng},
  booktitle={ACMMM},
  year={2023}
}

@article{chandrashekar2023deepgami,
  title={DeepGAMI: deep biologically guided auxiliary learning for multimodal integration and imputation to improve genotype--phenotype prediction},
  author={Chandrashekar, Pramod Bharadwaj and Alatkar, Sayali and Wang, Jiebiao and Hoffman, Gabriel E and He, Chenfeng and Jin, Ting and Khullar, Saniya and Bendl, Jaroslav and Fullard, John F and Roussos, Panos and others},
  journal={Genome Medicine},
  volume={15},
  number={1},
  pages={88},
  year={2023},
  publisher={Springer}
}

@inproceedings{zhang2024dept,
  title={Dept: Decoupled prompt tuning},
  author={Zhang, Ji and Wu, Shihan and Gao, Lianli and Shen, Heng Tao and Song, Jingkuan},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{cong2024decoupled,
  title={Decoupled optimisation for long-tailed visual recognition},
  author={Cong, Cong and Xuan, Shiyu and Liu, Sidong and Zhang, Shiliang and Pagnucco, Maurice and Song, Yang},
  booktitle={AAAI},
  year={2024}
}

@inproceedings{udandarao2023sus,
  title={Sus-x: Training-free name-only transfer of vision-language models},
  author={Udandarao, Vishaal and Gupta, Ankush and Albanie, Samuel},
  booktitle={ICCV},
  year={2023}
}

@article{liang2022mind,
  title={Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning},
  author={Liang, Victor Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James Y},
  journal={NeurIPS},
  year={2022}
}

@article{meng2024continuous,
  title={Continuous K-space Recovery Network with Image Guidance for Fast MRI Reconstruction},
  author={Meng, Yucong and Yang, Zhiwei and Duan, Minghong and Shi, Yonghong and Song, Zhijian},
  journal={arXiv preprint arXiv:2411.11282},
  year={2024}
}

@article{yang2024tackling,
  title={Tackling Ambiguity from Perspective of Uncertainty Inference and Affinity Diversification for Weakly Supervised Semantic Segmentation},
  author={Yang, Zhiwei and Meng, Yucong and Fu, Kexue and Wang, Shuo and Song, Zhijian},
  journal={arXiv preprint arXiv:2404.08195},
  year={2024}
}

@article{WU2023102938,
title = {GAMMA challenge: Glaucoma grAding from Multi-Modality imAges},
journal = {MIA},
year = {2023},
doi = {https://doi.org/10.1016/j.media.2023.102938},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523001986},
author = {Junde Wu and Huihui Fang and Fei Li and Huazhu Fu and Fengbin Lin and Jiongcheng Li and Yue Huang and Qinji Yu and Sifan Song and Xinxing Xu and Yanyu Xu and Wensai Wang and Lingxiao Wang and Shuai Lu and Huiqi Li and Shihua Huang and Zhichao Lu and Chubin Ou and Xifei Wei and Bingyuan Liu and Riadh Kobbi and Xiaoying Tang and Li Lin and Qiang Zhou and Qiang Hu and Hrvoje Bogunović and José Ignacio Orlando and Xiulan Zhang and Yanwu Xu},
keywords = {Glaucoma, Colour fundus photography, Multi-modality, Optical coherence tomography, GAMMA challenge},
abstract = {Glaucoma is a chronic neuro-degenerative condition that is one of the world’s leading causes of irreversible but preventable blindness. The blindness is generally caused by the lack of timely detection and treatment. Early screening is thus essential for early treatment to preserve vision and maintain life quality. Colour fundus photography and Optical Coherence Tomography (OCT) are the two most cost-effective tools for glaucoma screening. Both imaging modalities have prominent biomarkers to indicate glaucoma suspects, such as the vertical cup-to-disc ratio (vCDR) on fundus images and retinal nerve fiber layer (RNFL) thickness on OCT volume. In clinical practice, it is often recommended to take both of the screenings for a more accurate and reliable diagnosis. However, although numerous algorithms are proposed based on fundus images or OCT volumes for the automated glaucoma detection, there are few methods that leverage both of the modalities to achieve the target. To fulfil the research gap, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA) Challenge to encourage the development of fundus & OCT-based glaucoma grading. The primary task of the challenge is to grade glaucoma from both the 2D fundus images and 3D OCT scanning volumes. As part of GAMMA, we have publicly released a glaucoma annotated dataset with both 2D fundus colour photography and 3D OCT volumes, which is the first multi-modality dataset for machine learning based glaucoma grading. In addition, an evaluation framework is also established to evaluate the performance of the submitted methods. During the challenge, 1272 results were submitted, and finally, ten best performing teams were selected for the final stage. We analyse their results and summarize their methods in the paper. Since all the teams submitted their source code in the challenge, we conducted a detailed ablation study to verify the effectiveness of the particular modules proposed. Finally, we identify the proposed techniques and strategies that could be of practical value for the clinical diagnosis of glaucoma. As the first in-depth study of fundus & OCT multi-modality glaucoma grading, we believe the GAMMA Challenge will serve as an essential guideline and benchmark for future research.}
}

@misc{
luo2024eye,
title={Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye Diseases Screening and Fair Identity Scaling},
author={Yan Luo and Yu Tian and Min Shi and Tobias Elze and Mengyu Wang},
year={2024},
url={https://openreview.net/forum?id=Lv9KZ5qCSG}
}


@article{hua2020convolutional,
  title={Convolutional network with twofold feature augmentation for diabetic retinopathy recognition from multi-modal images},
  author={Hua, Cam-Hao and Kim, Kiyoung and Huynh-The, Thien and You, Jong In and Yu, Seung-Young and Le-Tien, Thuong and Bae, Sung-Ho and Lee, Sungyoung},
  journal={JBHI},
  year={2020},
}

@inproceedings{woo2018cbam,
  title={Cbam: Convolutional block attention module},
  author={Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  booktitle={ECCV},
  year={2018}
}

@article{zheng2023casf,
  title={CASF-Net: Cross-attention and cross-scale fusion network for medical image segmentation},
  author={Zheng, Jianwei and Liu, Hao and Feng, Yuchao and Xu, Jinshan and Zhao, Liang},
  journal={Computer Methods and Programs in Biomedicine},
  volume={229},
  pages={107307},
  year={2023},
  publisher={Elsevier}
}


@article{ZOU2024103214,
title = {Confidence-aware multi-modality learning for eye disease screening},
journal = {Medical Image Analysis},
volume = {96},
pages = {103214},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103214},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524001397},
author = {Ke Zou and Tian Lin and Zongbo Han and Meng Wang and Xuedong Yuan and Haoyu Chen and Changqing Zhang and Xiaojing Shen and Huazhu Fu},
keywords = {Uncertainty estimation, Eye disease, Multi-modality learning},
abstract = {Multi-modal ophthalmic image classification plays a key role in diagnosing eye diseases, as it integrates information from different sources to complement their respective performances. However, recent improvements have mainly focused on accuracy, often neglecting the importance of confidence and robustness in predictions for diverse modalities. In this study, we propose a novel multi-modality evidential fusion pipeline for eye disease screening. It provides a measure of confidence for each modality and elegantly integrates the multi-modality information using a multi-distribution fusion perspective. Specifically, our method first utilizes normal inverse gamma prior distributions over pre-trained models to learn both aleatoric and epistemic uncertainty for uni-modality. Then, the normal inverse gamma distribution is analyzed as the Student’s t distribution. Furthermore, within a confidence-aware fusion framework, we propose a mixture of Student’s t distributions to effectively integrate different modalities, imparting the model with heavy-tailed properties and enhancing its robustness and reliability. More importantly, the confidence-aware multi-modality ranking regularization term induces the model to more reasonably rank the noisy single-modal and fused-modal confidence, leading to improved reliability and accuracy. Experimental results on both public and internal datasets demonstrate that our model excels in robustness, particularly in challenging scenarios involving Gaussian noise and modality missing conditions. Moreover, our model exhibits strong generalization capabilities to out-of-distribution data, underscoring its potential as a promising solution for multimodal eye disease screening.}
}
@inproceedings{cheng2020club,
  title={Club: A contrastive log-ratio upper bound of mutual information},
  author={Cheng, Pengyu and Hao, Weituo and Dai, Shuyang and Liu, Jiachang and Gan, Zhe and Carin, Lawrence},
  booktitle={ICML},
  year={2020},
}

@article{liu2023improved,
  title={An improved hybrid network with a transformer module for medical image fusion},
  author={Liu, Yanyu and Zang, Yongsheng and Zhou, Dongming and Cao, Jinde and Nie, Rencan and Hou, Ruichao and Ding, Zhaisheng and Mei, Jiatian},
  journal={IEEE Journal of Biomedical and Health Informatics},
  volume={27},
  number={7},
  pages={3489--3500},
  year={2023},
  publisher={IEEE}
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  year={2002},
}
@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={ICML},
  year={2017},
}
@inproceedings{wei2022mitigating,
  title={Mitigating neural network overconfidence with logit normalization},
  author={Wei, Hongxin and Xie, Renchunzi and Cheng, Hao and Feng, Lei and An, Bo and Li, Yixuan},
  booktitle={ICML},
  year={2022},
}
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@article{warner2024multimodal,
  title={Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects},
  author={Warner, Elisa and Lee, Joonsang and Hsu, William and Syeda-Mahmood, Tanveer and Kahn Jr, Charles E and Gevaert, Olivier and Rao, Arvind},
  journal={IJCV},
  year={2024},
}


@inproceedings{shi2019probabilistic,
  title={Probabilistic face embeddings},
  author={Shi, Yichun and Jain, Anil K},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{chun2021probabilistic,
  title={Probabilistic embeddings for cross-modal retrieval},
  author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{hatamizadeh2022unetr,
  title={Unetr: Transformers for 3d medical image segmentation},
  author={Hatamizadeh, Ali and Tang, Yucheng and Nath, Vishwesh and Yang, Dong and Myronenko, Andriy and Landman, Bennett and Roth, Holger R and Xu, Daguang},
  booktitle={WACV},
  year={2022}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={ICCV},
  year={2017}
}

@article{zhang2024prototypical,
  title={Prototypical information bottlenecking and disentangling for multimodal cancer survival prediction},
  author={Zhang, Yilan and Xu, Yingxue and Chen, Jianqi and Xie, Fengying and Chen, Hao},
  journal={arXiv preprint arXiv:2401.01646},
  year={2024}
}

@inproceedings{han2020ghostnet,
  title={Ghostnet: More features from cheap operations},
  author={Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1580--1589},
  year={2020}
}

@article{han2021improving,
  title={Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis},
  author={Han, Wei and Chen, Hui and Poria, Soujanya},
  journal={arXiv preprint arXiv:2109.00412},
  year={2021}
}


@inproceedings{tang2024hunting,
  title={Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation},
  author={Tang, Feilong and Xu, Zhongxing and Qu, Zhaojun and Feng, Wei and Jiang, Xingjian and Ge, Zongyuan},
  booktitle={CVPR},
  year={2024}
}
@inproceedings{tang2023duat,
  title={DuAT: Dual-aggregation transformer network for medical image segmentation},
  author={Tang, Feilong and Xu, Zhongxing and Huang, Qiming and Wang, Jinfeng and Hou, Xianxu and Su, Jionglong and Liu, Jingxin},
  booktitle={PRCV},
  year={2023},
}
@inproceedings{zhao2024sfc,
  title={Sfc: Shared feature calibration in weakly supervised semantic segmentation},
  author={Zhao, Xinqiao and Tang, Feilong and Wang, Xiaoyang and Xiao, Jimin},
  booktitle={AAAI},
  year={2024}
}
@inproceedings{wang2022stepwise,
  title={Stepwise feature fusion: Local guides global},
  author={Wang, Jinfeng and Huang, Qiming and Tang, Feilong and Meng, Jia and Su, Jionglong and Song, Sifan},
  booktitle={MICCAI},
  year={2022},
  organization={Springer}
}

@inproceedings{xu2024polyp,
  title={Polyp-Mamba: Polyp Segmentation with Visual Mamba},
  author={Xu, Zhongxing and Tang, Feilong and Chen, Zhe and Zhou, Zheng and Wu, Weishan and Yang, Yuyao and Liang, Yu and Jiang, Jiyu and Cai, Xuyue and Su, Jionglong},
  booktitle={MICCAI},
  year={2024},
  organization={Springer}
}
@inproceedings{hu2025ophnet,
  title={Ophnet: A large-scale video benchmark for ophthalmic surgical workflow understanding},
  author={Hu, Ming and Xia, Peng and Wang, Lin and Yan, Siyuan and Tang, Feilong and Xu, Zhongxing and Luo, Yimin and Song, Kaimin and Leitner, Jurgen and Cheng, Xuelian and others},
  booktitle={ECCV},
  year={2025},
}

@article{hu2024ophclip,
  title={OphCLIP: Hierarchical Retrieval-Augmented Learning for Ophthalmic Surgical Video-Language Pretraining},
  author={Hu, Ming and Yuan, Kun and Shen, Yaling and Tang, Feilong and Xu, Xiaohao and Zhou, Lin and Li, Wei and Chen, Ying and Xu, Zhongxing and Peng, Zelin and others},
  journal={arXiv preprint arXiv:2411.15421},
  year={2024}
}

@article{yang2023action,
  title={Action recognition with multi-stream motion modeling and mutual information maximization},
  author={Yang, Yuheng and Chen, Haipeng and Liu, Zhenguang and Lyu, Yingda and Zhang, Beibei and Wu, Shuang and Wang, Zhibo and Ren, Kui},
  journal={arXiv preprint arXiv:2306.07576},
  year={2023}
}


@inproceedings{peng2024parameter,
  title={Parameter efficient fine-tuning via cross block orchestration for segment anything model},
  author={Peng, Zelin and Xu, Zhengqin and Zeng, Zhilin and Xie, Lingxi and Tian, Qi and Shen, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3743--3752},
  year={2024}
}

@inproceedings{peng2024sam,
  title={Sam-parser: Fine-tuning sam efficiently by parameter space reconstruction},
  author={Peng, Zelin and Xu, Zhengqin and Zeng, Zhilin and Yang, Xiaokang and Shen, Wei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={5},
  pages={4515--4523},
  year={2024}
}
