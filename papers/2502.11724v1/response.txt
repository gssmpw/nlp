\section{Related Work}
\subsubsection{Incomplete Multimodal Learning.} 
Recent methods in incomplete multimodal learning emphasize data reconstruction and latent subspace methods. Traditional data reconstruction methods utilize Generative Adversarial Networks**Goodfellow et al., "Generative Adversarial Networks"** to enable models to simulate complete datasets but incur high computational costs. To address this, advanced techniques**Mirza and Osindero, "Conditional Generative Adversarial Nets"** reduce computational overhead. Additionally, statistical models have been introduced to align distributions of reconstructed and available data**Rezende et al., "Stochastic Backpropagation through Stochastic Neuron Activation"**. Latent subspace methods map different data modalities into a shared latent space, enabling the learning of intra-modality relationships**Kingma et al., "Semi-Supervised Learning with Deep Generative Models"**. This work employs explicit constraints to remove redundant information and capture discriminative features, hence relaxing the implicit representation constraint.

\begin{figure*}[!ht]
\centering
\normalsize
\includegraphics[width=0.95\linewidth]{Model_network.pdf}
    % \vspace{-0.2cm}
    \caption{Overview of our proposed framework. \textbf{(a)}: 
    We train a teacher model using complete modality data, followed by co-training with a student model on incomplete inputs for knowledge distillation. The distillation is supervised by feature loss $\mathcal{L}_{\text{Feat}}$ and logit loss $\mathcal{L}_{\text{Logit}}$.
    During the training of the teacher model, the encoder outputs the single-modality feature $e^f$ and $e^O$.  We build a set of proxies for a modality, with each set representing a class. Positive proxies are selected by a similarity matrix between $\hat{e}$ and $e$. All proxies are optimized through the proxy loss $\mathcal{L}_{\text{Prox}}$. Consequently, $\hat{e}^{f,+}$ and $\hat{e}^{O,+}$, together with features $e^f$ and $e^O$ are then passed to the IMDR.
    \textbf{(b)}: Details for IMDR strategy. 
    We estimate the distributions of $\hat{e}^{f,+}$ and $\hat{e}^{O,+}$, then combine them using Eq.~\ref{eq: poe} to obtain the joint distribution $\mathcal{P}(\hat{e} \vert  x^{f}, x^{O})$. The modality-shared feature $s$ is sampled from this distribution. This feature $s$ guides the decoupling via an attention layer, supervised by the loss $\mathcal{L}_{\text{MI}}$ to minimize the mutual information between extracted shared features $\hat{s}$ and specific features $(\mathcal{R}^f, \mathcal{R}^O)$, as well as between $\mathcal{R}^f$ and $\mathcal{R}^O$.
    }
    \label{fig:enter-label}
\end{figure*}
\noindent\textbf{Decoupled Multimodal Representation.}
Decoupled representation techniques can be categorized into Modality-Specific Learning and Training-Inference Decoupling, which are vital for optimizing performance across diverse data types and operational phases.
Modality-Specific Learning ensures that the features of each data type are processed in a way that maintains their unique characteristics while being effectively integrated into a unified model**Long et al., "Learning Transferable Features with Simultaneous Deep Supervision"**. 
Training-Inference Decoupling involves using different models during the training phase compared to the inference phase to better adapt to varying task demands**Gupta et al., "Covariant Denoising Autoencoders"**. 
Although these single-modality-based methods achieved remarkable improvements in feature extraction, they do not explicitly remove redundant information from inter-modality or intra-modality features.
This work utilizes joint distribution across modalities to further guide disentangling multimodal data while preserving diversity in modality shared and specific representations.