%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
\usepackage{commath}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage{graphicx}
% \usepackage{amsthm}
% \usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{bbm}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
%\newtheorem{assumption}[theorem]{Assumption}
%\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}{Assumption}
\theoremstyle{plain}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{bbm}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Federated Linear Dueling Bandits}

\begin{document}

\twocolumn[
\icmltitle{Federated Linear Dueling Bandits}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xuhan Huang}{cusz}
\icmlauthor{Yan Hu}{cusz}
\icmlauthor{Zhiyan Li}{cusz}
\icmlauthor{Zhiyong Wang}{cuhk}
\icmlauthor{Benyou Wang}{cusz}
\icmlauthor{Zhongxiang Dai}{cusz}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{cusz}{The Chinese University of Hong Kong, Shenzhen}
\icmlaffiliation{cuhk}{The Chinese University of Hong Kong}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Zhongxiang Dai}{daizhongxiang@cuhk.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Contextual linear dueling bandits have recently garnered significant attention due to their widespread applications in important domains such as recommender systems and large language models. Classical dueling bandit algorithms are typically only applicable to a single agent. However, many applications of dueling bandits involve multiple agents who wish to collaborate for improved performance yet are unwilling to share their data. This motivates us to draw inspirations from \emph{federated learning}, which involves multiple agents aiming to collaboratively train their neural networks via gradient descent (GD) without sharing their raw data. Previous works have developed federated linear bandit algorithms which rely on closed-form updates of the bandit parameters (e.g., the linear function parameter) to achieve collaboration. However, in linear dueling bandits, the linear function parameter lacks a closed-form expression and its estimation requires minimizing a loss function. This renders these previous methods inapplicable. In this work, we overcome this challenge through an innovative and principled combination of online gradient descent (for minimizing the loss function to estimate the linear function parameters) and federated learning, hence introducing the first \emph{federated linear dueling bandit} algorithms. Through rigorous theoretical analysis, we prove that our algorithms enjoy a sub-linear upper bound on its cumulative regret. We also use empirical experiments to demonstrate the effectiveness of our algorithms and the practical benefit of collaboration.
\end{abstract}

\section{Introduction}
\emph{Contextual dueling bandits} \citep{NeurIPS21_saha2021optimal,ALT22_saha2022efficient,ICML22_bengs2022stochastic,arXiv24_li2024feelgood} have received significant attention recently, due to its widespread adoption in important real-world applications such as recommender systems \citep{JCSS12_yue2012k} and large language models (LLMs) \cite{lin2024prompt,ji2024reinforcement}. 
In every iteration of a contextual dueling bandit problem, an agent receives a $d$-dimensional \emph{context} vector and $K$ \emph{arms}, selects a pair of arms, and collects a binary observation indicating the relative preference between the selected pair of arms \cite{ICML22_bengs2022stochastic}.
For example, when applying a contextual dueling bandit algorithm to optimize the response from an LLM, we receive a context (i.e., a prompt), use the LLM to generate $K$ responses (i.e., arms), select from them a pair of responses, and then ask the user which response is preferred \cite{lin2024prompt}.
In order to intelligently select the pairs of arms, we often adopt a surrogate function to model the \emph{latent reward function} governing the preference feedback observations (more details in Sec.~\ref{sec:background}), for which we typically adopt a linear function.
That is, we assume that the reward function value at arm $x$ can be expressed as $f(x) = \theta^{\top} \phi(x)$ for some unknown $\theta\in\mathbb{R}^d$ and known feature mapping $\phi(\cdot)\in\mathbb{R}^d$.
This is often referred to as the contextual \emph{linear dueling bandit} problem.

Classical contextual dueling bandit algorithms, as discussed above, are only applicable to single-agent scenarios.
However, many real-world applications involve multiple agents, which creates opportunities to enhance performance via \emph{collaboration among agents}.
Of note, in such applications, the agents are often concerned with privacy and are often \emph{unwilling to share their data}, including the selected arms and the observed preference feedback.
For example, users adopting contextual dueling bandits to optimize the response of LLMs may want to collaborate with each other without sharing their personal data involving their selected responses and preference feedback.
These requirements naturally align with the paradigm of \emph{federated learning} (FL), which enables multiple agents to collaboratively train their neural networks (NNs) without requiring them to share their raw data \cite{mcmahan2017communication}.
In every round of FL, every agent calculates the local gradient (or parameters) of their NNs and sends them to the central server, who aggregates all received local gradients (typically via simple averaging) and then broadcasts the aggregated information back to the agents for further local updates of their NN parameters \cite{mcmahan2017communication}.

To extend contextual linear dueling bandits to the federated setting, we can draw inspirations from previous works on federated contextual bandits \cite{shi2021federated}.
Notably, the work of \citet{wang2019distributed} has proposed a federated contextual linear bandit algorithm, which only requires the agents to share some sufficient statistics, including a $d$-dimensional vector and a $(d\times d)$-dimensional matrix.
% for calculating the linear upper confidence bound (Lin-UCB) policy.
Importantly, the two terms in the linear upper confidence bound (Lin-UCB) policy for arm selection, including (a) the \emph{estimated linear function parameter} (i.e., the estimate of $\theta$) and (b) an exploration term, \emph{can be expressed in closed forms} in terms of the summation of these sufficient statistics.
As a result, in order to aggregate the information from all agents, the central server only needs to aggregate the sufficient statistics from all agents via a simple summation.
% Thanks to the clean closed-form expression of the Lin-UCB policy in terms of the summation of these sufficient statistics, the aggregation at the central server can be achieved by simply adding up these sufficient statistics.
This allows every agent to effectively leverage the additional observations from the other agents to accelerate their bandit algorithm without requiring the exchange of raw observations.
This paradigm from \citet{wang2019distributed} has been widely adopted and extended to other bandit problems, such as federated neural bandits \cite{dai2022federated}.

However, in contextual linear dueling bandits, \emph{the estimated linear function parameter $\theta$ lacks a closed-form expression} \cite{ICML22_bengs2022stochastic}.
Instead, we often need to estimate $\theta$ by minimizing a loss function via \emph{gradient descent} (GD).
This renders the federated bandit paradigm from \citet{wang2019distributed} inapplicable to our problem of contextual linear dueling bandits.
To resolve this challenge, we propose to allow the agents to \emph{collaboratively use GD to estimate the linear function parameter}.
Specifically, the joint loss function (to be minimized for parameter estimation) for all agents can be expressed in terms of a summation among all agents (more details in Sec.~\ref{sec:algo}). Therefore, the gradient of the joint loss function can be \emph{decoupled into the contributions from individual agents}. 
As a result, we let every agent calculate the local gradient of its own loss function and send it to the central server.
The central server then aggregates (i.e., sums) all local gradients to attain the gradient of the joint loss function, using which GD can be performed.
Interestingly, in contrast to previous works on federated bandits \cite{shi2021federated,dai2022federated}, this novel approach bears a closer resemblance to the original federated learning (FL) paradigm which also involves the exchange of local gradients \cite{mcmahan2017communication}.

We firstly propose an algorithm adopting federated GD (Algo.~\ref{algo:parameter:estimation}) to estimate the linear function parameter $\theta$, which conducts multiple rounds gradient exchange between the central server and the agents in every iteration.
Next, we develop a more practical algorithm which employs federated online GD (Algo.~\ref{algo:parameter:estimation:ogd}) for estimating $\widehat{\theta}$, in which only a single round of gradient exchange is needed in every iteration.
% Based on these two algorithms for parameter estimation, our high 
Our high-level federated linear dueling bandit (FLDB) algorithm can adopt either federated GD or federated online GD (OGD) for parameter estimation, 
and we refer to the resulting algorithms as the \texttt{FLDB-GD} algorithm and \texttt{FLDB-OGD} algorithm, respectively.

We perform rigorous theoretical analysis of the regret of our \texttt{FLDB-GD} and \texttt{FLDB-OGD} algorithms and show that they both enjoy sub-linear upper bounds on the cumulative regret (Sec.~\ref{sec:theory}).
Our theoretical results show that our \texttt{FLDB-GD} algorithm enjoys a tighter regret upper bound, yet it requires a larger number of communication rounds. On the other hand, our \texttt{FLDB-OGD} algorithm is significantly more communication-efficient (hence more practical) despite having a worse regret upper bound.
Through extensive empirical experiments using synthetic functions, we demonstrate that both our algorithms consistently outperform single-agent linear dueling bandits (Sec.~\ref{sec:experiments}).
In addition, the performance of our algorithm becomes improved as the number of agents is increased, and \texttt{FLDB-GD}, which requires more communication rounds, achieves smaller regrets than \texttt{FLDB-OGD} for the same number of agents.

% Federated learning (FL), an emerging machine learning
% paradigm, involves collaborative learning among multiple
% agents. In this process, selected agents share their local
% updates with a central server, which then aggregates these
% updates and sends the consolidated output back to each
% participating agent \cite{konevcny2016federated, mcmahan2017communication, li2020federated, elgabli2022fednew}.



\section{Background and Problem Setting}
\label{sec:background}
% \paragraph{Contextual dueling bandits.} We consider a contextual dueling bandit problem where, given a context, a learner selects two arms (referred to as a \emph{duel}) and observes preference feedback between them. The learner's objective is to identify the best arm for each context. Let \(\mathcal{C} \subset \mathbb{R}^{d_c}\) denote the context set and \(\mathcal{A} \subset \mathbb{R}^{d_a}\) represent the finite arm set, where \(d_c \geq 1\) and \(d_a \geq 1\). At the beginning of round \(t\), the environment generates a context \(c_t \in \mathcal{C}\), and the learner selects two arms, \(a_{t,1}\) and \(a_{t,2}\), from the arm set \(\mathcal{A}\). After selecting the arms, the learner observes stochastic preference feedback \(y_t\), where \(y_t = 1\) indicates that arm \(a_{t,1}\) is preferred over arm \(a_{t,2}\), and \(y_t = 0\) otherwise. We assume that the preference feedback is governed by an unknown reward function \(f: \mathcal{C} \times \mathcal{A} \rightarrow \mathbb{R}\). For brevity, we denote the set of all context-arm feature vectors in round \(t\) by \(\mathcal{X}_t \subset \mathbb{R}^d\), and use \(x_t^a\) to represent the context-arm feature vector corresponding to context \(c_t\) and arm \(a\).

In a contextual dueling bandit problem, at each iteration $t$, the environment generates a set of $K$ arms, denoted as $\mathcal{X}_t\subset\mathcal{X} \subset \mathbb{R}^d$, where $\mathcal{X}$ represents the domain of all possible arms. The agent then selects a pair of arms $x_{t,1},x_{t,2} \in \mathcal{X}_t$ and receives feedback $y_t$, which is $1$ if $x_{t,1}$ is preferred over $x_{t,2}$ and $0$ otherwise.

\paragraph{Preference Model.}
% We assume the preference has a Bernoulli distribution that follows the Bradley-Terry-Luce (BTL) model \citep{AS04_hunter2004mm, Book_luce2005individual}, which is commonly used in the dueling bandits \citep{NeurIPS21_saha2021optimal, ICML22_bengs2022stochastic, arXiv24_li2024feelgood}. 
Following the common practice in dueling bandits \citep{NeurIPS21_saha2021optimal, ICML22_bengs2022stochastic, arXiv24_li2024feelgood}, we assume that the preference feedback follows the Bradley-Terry-Luce (BTL) model \citep{AS04_hunter2004mm, Book_luce2005individual}.
Specifically, the utility of the arms are represented by a \emph{latent reward function} $f:\mathbb{R}^{d}\rightarrow \mathbb{R}$, which maps any arm $x$ to its corresponding reward value $f(x)$.
Here we assume that $f$ is a linear function $f(x) = \theta^{\top} \phi(x),\forall x$, in which $\theta\in\mathbb{R}^d$ is an unknown parameter and $\phi(\cdot) \in\mathbb{R}^d$ is a known feature mapping. This reduces to standard linear dueling bandits when $\phi(\cdot)$ is the identity mapping.
Then, the probability that the first selected arm $(x_{t,1})$ is preferred over the second selected arm $(x_{t,2})$ for the given reward function $f$ is given by
\begin{align*}
\mathbb{P}(x \succ x') =\mathbb{P} \{ y_t = 1| x_{t,1}, x_{t,2}\}= \mu (f(x_{t,1}) - f(x_{t,2})). 
\end{align*}
Here $x_1 \succ  x_2$ indicates that $x_{t,1}$ is preferred over $x_{t,2}$, $\mu(x) = 1/(1 + \mathrm{e}^{-x} )$ is the \emph{link function} for which we adopt the logistic function.
% , and $f(x_{t,i})$ is the latent reward of the $i$-th selected arm for the given context $c_t$. 

We list below the assumptions needed for our theoretical analysis, all of which are standard assumptions commonly adopted by previous works on dueling bandits
% We need the following standard assumptions on function $\mu$ (also known as a {\em link function} in the literature 
\citep{ICML22_bengs2022stochastic,ICML17_li2017provably}).
\begin{assumption}\label{assumption:basic}
    \begin{itemize}
	\setlength{\itemsep}{3pt}
	\setlength{\parskip}{0pt}
		\item $\kappa_\mu \doteq \inf_{x,x' \in \mathcal{X}} \dot{\mu}(f(x) - f(x')) > 0$ for all pairs of context-arm.

		\item The link function $\mu : \mathbb{R} \rightarrow [0,1]$ is continuously differentiable and Lipschitz with constant $L_\mu$. For logistic function, $L_\mu \le 1/4$.

        \item The difference between feature maps is bounded $\norm{\phi(x_{t,1}) - \phi(x_{t,2})}_2 \leq 1$ for all contexts. \\
	\end{itemize}
\end{assumption}
We also need the following assumption on the context vectors.
\begin{assumption}\label{assumption:fed}
For $x_1, x_2 \in\mathcal{X}_{t,i}$, $\widetilde{\phi}_{t,i} = \phi(x_1) - \phi(x_2) $
   Denote $\Sigma = \widetilde{\phi}_{t,i} {\widetilde{\phi}_{t,i}} ^T$ and $\lambda_f = \displaystyle{\inf_{i, t, x_{1}, x_{2} \in\mathcal{X}_{t,i}}} \lambda_{\min} (\Sigma)$. We assume $\lambda_f$ is a positive constant.
\end{assumption}
% While Assumptions~\ref{assumption:basic} and
Assumption~\ref{assumption:fed} is also commonly used in the analysis of generalized linear bandits and dueling bandits \cite{li2017provablyoptimalalgorithmsgeneralized}.
% linear dueling bandit, 
For example, 
% similar assumptions have also been adopted by 
this assumption is in a similar vein to Assumption 1.5 from \citet{wu2020stochasticlinearcontextualbandits}
% ~\citet{li2017provablyoptimalalgorithmsgeneralized} 
and Assumption 3 from~\citet{ding2021efficientalgorithmgeneralizedlinear}. 
Intuitively, Assumption~\ref{assumption:fed} is reasonable since it ensures that the context vectors sampled from the environment are sufficiently diverse to guarantee a non-degenerate difference between feature vectors.
% While Assumption.~\ref{assumption:basic} and~\ref{assumption:fed} are common assumptions for linear dueling bandit settings, Assumption.~\ref{assumption:fed} is similar to the assumption used by~\citet{li2017provablyoptimalalgorithmsgeneralized}, which is also adopted by~\citet{ding2021efficientalgorithmgeneralizedlinear}. Intuitively, it is reasonable because the assumption reflects that context information are diverse enough to guarantee a non-degenerate difference between feature vectors.
% Assumption.~\ref{assumption:basic} are common assumption in linear dueling bandit settings, while Assumption.~\ref{assumption:fed} is similar to 
\paragraph{Performance measure.}~
The goal of an agent in contextual dueling bandits is to minimize its \emph{regret}.
After selecting a pair of arms, denoted by $x_{t,1}$ and $x_{t,2}$, in iteration $t$, the learner incurs an instantaneous regret. In our theoretical analysis, we aim to analyze the following regret:
\begin{align*}
   r_t = 2f(x^*_t) - f(x_{t,1}) - f(x_{t,2})
\end{align*}
in which $x^*_t = \arg\max_{x\in \mathcal{X}_t} f(x)$ denotes the optimal arm in iteration $t$. 
After observing preference feedback for $T$ pairs of arms, the cumulative regret (or regret, in short) of a sequential policy is given by 
\begin{align*}
    R_T = \sum^T_{t=1} r_t = \sum^T_{t=1} \left(2 f(x^*_t) - f(x_{t,1}) - f(x_{t,2})\right), 
\end{align*}
in which $x^*_t = \arg\max_{x\in \mathcal{X}_t} f(x)$ denotes the optimal arm in iteration $t$. 
Any good policy should have sub-linear regret, i.e., $\lim_{T \to \infty}{R_T}/T = 0.$
A policy with a sub-linear regret implies that the policy will eventually find the best arm and recommend only the best arm in the duel for the given contexts. 

\paragraph{Federated Contextual Dueling Bandits.}
In federated contextual dueling bandits involving $N$ agents, we assume that all agents share the same latent reward function $f(x)=\theta^{\top} \phi(x)$.
This is consistent with the previous works on federated bandits \cite{shi2021federated,dai2022federated}.
In iteration $t$, every agent $i$ receives a separate set of arms $\mathcal{X}_{t,i} \subset \mathcal{X}$ and chooses from them a pair of arms denoted as $x_{t,1,i}$ and $x_{t,2,i}$.
In this case, we analyze the total cumulative regret from all $N$ agents:
\begin{align*}
    R_{T,N} = \sum^T_{t=1}\sum^N_{i=1} \left(2 f(x^*_{t,i}) - f(x_{t,1,i}) - f(x_{t,2,i})\right), 
\end{align*}
in which $x^*_{t,i}={\arg\max}_{x\in\mathcal{X}_{t,i}}f(x)$.


\section{Federated Linear Dueling Bandits}
\label{sec:algo}

We firstly introduce our Algo.~\ref{algo:fed:linear:dueling:bandits}, which is the high-level framework for our federated linear dueling bandit (\emph{FLDB}) algorithm.
When Algo.~\ref{algo:fed:linear:dueling:bandits} employs \emph{GD} (Algo.~\ref{algo:parameter:estimation}) for estimating the parameter $\theta_{\text{sync}}$ (see line 9 of Algo.~\ref{algo:fed:linear:dueling:bandits}), we refer to the resulting algorithm as \texttt{FLDB-GD}; 
when Algo.~\ref{algo:fed:linear:dueling:bandits} adopts \emph{OGD} (Algo.~\ref{algo:parameter:estimation:ogd}) to estimate $\theta_{\text{sync}}$, we refer to the algorithm as \texttt{FLDB-OGD}.

% To begin with, we consider a simplified version of our algorithm, in which \textbf{communication happens after every iteration}.

\begin{algorithm}[h]
	\begin{algorithmic}[1]
		\FOR{$t= 1, \ldots, T$}
		\FOR{Agent $i=1,\ldots,N$ \textbf{in parallel}}
            \STATE {\color{red}[Agent $i$]} Receive contexts $\mathcal{X}_{t,i}$
            % \STATE $\overline{V}_{t,i} = \frac{\lambda}{\kappa_\mu}\mathbf{I} + W_{\text{sync}} + W_{\text{new},i}$
            % \STATE Find $\theta_t = \arg\min_{\theta'} \mathcal{L}_t(\theta')$~\eqref{eq:loss:func}
            \STATE {\color{red}[Agent $i$]} Choose the first arm  $x_{t,1,i} = \arg\max_{x\in\mathcal{X}_{t,i}}\theta_{\text{sync}}^\top \phi(x)$
            % \STATE Choose the second arm $x_{t,2} = \arg\max_{x\in\mathcal{X}_t} \theta_{\text{sync}}^\top \left( \phi(x) - \phi(x_{t,1}) \right) + \frac{\beta_t}{\kappa_\mu}\norm{\phi(x) - \phi(x_{t,1})}_{\overline{V}_{t,i}^{-1}}$
            \STATE {\color{red}[Agent $i$]} Choose the second arm $x_{t,2,i} = \arg\max_{x\in\mathcal{X}_{t,i}} \theta_{\text{sync}}^\top \left( \phi(x) - \phi(x_{t,1,i}) \right) + \frac{\beta_t}{\kappa_\mu}\lVert{\phi(x) - \phi(x_{t,1,i})}\rVert_{W_{\text{sync}}^{-1}}$            
            \STATE {\color{red}[Agent $i$]} Observe the preference feedback $y_{t,i} = \mathbbm{1}(x_{t,1,i}\succ x_{t,2,i})$ %, and update history: $\mathcal{D}_t=\{x_{s,1}, x_{s,2}, y_s\}_{s=1,\ldots,t}$
            \STATE {\color{red}[Agent $i$]} Calculate $W_{\text{new},i} = \left(\phi(x_{t,1}) - \phi(x_{t,2})\right) \left(\phi(x_{t,1}) - \phi(x_{t,2})\right)^{\top}$, and send it to central server
        \ENDFOR
            % \IF{}
            %     \STATE Send a synchonization signal to central server
            % \ENDIF
            % \IF{Communication round started}
            %     \STATE Receive $\{\theta_{\text{sync}}, W_{\text{sync}}\}$ from central server
            % \ENDIF
        \STATE {\color{blue}[Central Server]} Run Algorithms \ref{algo:parameter:estimation} or \ref{algo:parameter:estimation:ogd} to obtain $\theta_{\text{sync}} $
        %= \theta^{(M)} , such that $\mathcal{L}_t^{\text{fed}}(\theta_{\text{sync}}) = 0$
        \STATE {\color{blue}[Central Server]} Update $W_{\text{sync}} \leftarrow W_{\text{sync}} + \sum^N_{i=1} W_{\text{new},i}$
        \STATE {\color{blue}[Central Server]} Broadcast $\{\theta_{\text{sync}}, W_{\text{sync}}\}$ to all agents
		\ENDFOR
	\end{algorithmic}
\caption{Federated Linear Dueling Bandits}
\label{algo:fed:linear:dueling:bandits}
\end{algorithm}


In iteration \( t \), each agent \( i \in [N] \) receives a set of \( K \) contexts, denoted as \( \mathcal{X}_{t,i} \) (line 3 of Algo.~\ref{algo:fed:linear:dueling:bandits}), and selects two arms using the synchronized parameters \( \theta_{\text{sync}} \) and \( W_{\text{sync}} \) following the classic linear dueling bandit (\texttt{LDB}) algorithm \citep{ICML22_bengs2022stochastic}. 
Specifically, the first arm $x_{t,1,i}$ is selected greedily based on \( \theta_{\text{sync}} \), which is the current estimated linear function parameter \emph{based on the data from all agents} (line 4 of Algo.~\ref{algo:fed:linear:dueling:bandits}).
Next, the second arm $x_{t,2,i}$ is chosen following an upper confidence bound strategy to balance exploration and exploitation (line 5 of Algo.~\ref{algo:fed:linear:dueling:bandits}).
In this way, we encourage $x_{t,2,i}$ to both achieve high predicted reward and be different from $x_{t,1,i}$ as well as the previously selected arms.
After observing the preference feedback $y_{t,i}$, the agent computes the updated information matrix $W_{\text{new},i}$ and sends it to a central server (line 6-7 of Algo.~\ref{algo:fed:linear:dueling:bandits}).
% Note that only the matrix $W_{\text{new},i}$ is sent to the central server at iteration $t$.
Next, the central server coordinates all agents to estimate the parameter $\theta_{\text{sync}}$ (to be used in selecting the pair of arms in the next iteration) through either Algo.~\ref{algo:parameter:estimation} or Algo.~\ref{algo:parameter:estimation:ogd} (line 9 of Algo.~\ref{algo:fed:linear:dueling:bandits}). 
Finally, the central server aggregates the updated information matrix \(W_{\text{new},i}\) from all agents (lines 10 of Algo.~\ref{algo:fed:linear:dueling:bandits}) 
and broadcasts the aggregated information matrix \(W_{\text{sync}}\) and the estimated $\theta_{\text{sync}}$ back to all agents (line 11 of Algo.~\ref{algo:fed:linear:dueling:bandits}) to start the next iteration.

In the next two sections, we introduce how we estimate the parameter $\theta_{\text{sync}}$ via either federated GD (Sec.~\ref{subsec:algo:gd}) or federated OGD (Sec.~\ref{subsec:algo:ogd}).

\subsection{Federated GD for Estimating $\theta_{\text{sync}}$ (Algo.~\ref{algo:parameter:estimation})}
\label{subsec:algo:gd}
In a communication round of our algorithm to estimate $\theta_{\text{sync}}$, the goal of the central server is to fine $\theta_{\text{sync}} = {\arg\min}_{\theta'} \mathcal{L}_t^{\text{fed}}(\theta')$, in which the loss function is given by:
% minimize the following loss function:
\begin{equation}
\mathcal{L}_t^{\text{fed}}(\theta') = \sum^N_{i=1} \mathcal{L}_t^{i}(\theta') + \frac{1}{2}\lambda \lVert \theta'\rVert_2^2,
\label{eq:loss:func:fed}
\end{equation}
where
\begin{equation}
\begin{split} 
 \mathcal{L}_t^{i}(\theta') = & - \sum^{t-1}_{s=1} \left( y_s^{i}\log \mu\left({\theta'}^{\top}\left[\phi(x_{s,1}^{i}) - \phi(x_{s,2}^{i})\right]\right) \right. \\
 & \left. + (1-y_s^{i})\log \mu\left({\theta'}^{\top}\left[\phi(x_{s,2}^{i}) - \phi(x_{s,1}^{i})\right]\right) \right).
\end{split}
\label{eq:loss:func:fed:local}
\end{equation}
Equivalently, $\theta_{\text{sync}} = {\arg\min}_{\theta'} \mathcal{L}_t^{\text{fed}}(\theta')$ is the maximum likelihood estimate (MLE) of the unknown parameter $\theta$ \emph{given the data from all $N$ agents up to iteration $t-1$} \cite{ICML22_bengs2022stochastic}.
% That is, the loss function is minimized w.r.t.~the data from all $N$ agents.

Since the loss function \eqref{eq:loss:func:fed} is convex for any $t$, it is natural to apply gradient descent (GD) to optimize~\eqref{eq:loss:func:fed}. 
Interestingly, the loss function \eqref{eq:loss:func:fed} is naturally decoupled across different agents, which allows the central server to estimate the gradient of \eqref{eq:loss:func:fed} using the gradients of the individual local loss functions \eqref{eq:loss:func:fed:local}.
As a result, the agents only need to send their local gradients to the central server and are hence not required to share their local data $\{x_{t,1,i}, x_{t,2,i}, y_{t,i}\}$.
This is formalized by Algo.~\ref{algo:parameter:estimation}.
% The difficulty is to obtain the gradient of~\eqref{eq:loss:func:fed} while avoiding directly communicating the information of \{ $x_{t,1,i}, x_{t,2,i}, y_{t,i}$\} with the agent. We therefore present the algorithm~\ref{algo:parameter:estimation} for $\theta_{\text{sync}}$ estimation.
\begin{algorithm}[h]
	\begin{algorithmic}[1]
        % \STATE Output $\theta^{(M)}$
		\FOR{$s= 0, \ldots, M-1$}
            \STATE {\color{blue}[Central Server]} Central Server broadcasts $\theta^{(s)}$ to all agents
        	\FOR{Every Agent $i= 1, \ldots, N$}
                \STATE {\color{red}[Agent $i$]} Calculate local gradient $\nabla\mathcal{L}_t^{i}(\theta^{(s)})$ \eqref{eq:loss:func:fed:local}
                \STATE {\color{red}[Agent $i$]} Send $\nabla\mathcal{L}_t^{i}(\theta^{(s)})$ back to Central Server
            \ENDFOR
            \STATE {\color{blue}[Central Server]} Central Server aggregates local gradients: $\nabla\mathcal{L}_t^{\text{fed}}({\theta}^{(s)}) = \sum^N_{i=1} \nabla\mathcal{L}_t^{i}(\theta^{(s)}) + \lambda \theta^{(s)}$ \eqref{eq:loss:func:fed}
            \STATE {\color{blue}[Central Server]} Central Server updates parameter: $\theta^{(s+1)} = \theta^{(s)} - \eta \nabla\mathcal{L}_t^{\text{fed}}({\theta}^{(s)})$
            % \STATE Receive $\{W_{\text{new},i}\}_{i=1,\ldots,N}$
            % \STATE Run $M$ rounds of federated learning to calculate $\theta_{\text{sync}}$ such that $\mathcal{L}_t^{\text{fed}}(\theta_{\text{sync}}) = 0$ (to be relaxed later)
            % \STATE Update $W_{\text{sync}} \leftarrow W_{\text{sync}} + \sum^N_{i=1} W_{\text{new},i}$
            % \STATE Broadcast $\{\theta_{\text{sync}}, W_{\text{sync}}\}$ to all agents
		\ENDFOR
        \STATE {\color{blue}[Central Server]} $\theta_{\text{sync}} = \theta^{(M)}$
        % \STATE Central Server broadcasts $\theta_{\text{sync}}=\theta^{(M)}$ to all agents
	\end{algorithmic}
\caption{Federated GD for Estimating $\theta_{\text{sync}}$}
\label{algo:parameter:estimation}
\end{algorithm}

\textbf{Details of Algo.~\ref{algo:parameter:estimation}.}
In every step of Algo.~\ref{algo:parameter:estimation}, the central server broadcasts its current estimate $\theta^{(s)}$ to all $N$ agents (line 2 of Algo.~\ref{algo:parameter:estimation}).
Next, each agent $i$ calculates its own gradient $\nabla\mathcal{L}_t^{i}(\theta^{(s)})$, and sends it back to the central server (line 4-5 of Algo.~\ref{algo:parameter:estimation}).
Then, the central server aggregates (i.e., sums) the gradients from all agents (line 7 of Algo.~\ref{algo:parameter:estimation}) to obtain 
% the gradient of \eqref{eq:loss:func:fed} to obtain 
$\nabla\mathcal{L}_t^{\text{fed}}({\theta}^{(s)})$, and uses it to perform a step of GD (line 8 of Algo.~\ref{algo:parameter:estimation}): $\theta^{(s+1)} = \theta^{(s)} - \eta \nabla\mathcal{L}_t^{\text{fed}}({\theta}^{(s)})$.
After that, the central server broadcasts the updated $\theta^{(s+1)}$ to all agents to initiate the next step $s+1$.
After repeating these procedures for $M$ steps,
% After running $M$ steps of GD, 
the central server obtains 
% broadcasts 
$\theta_{\text{sync}}=\theta^{(M)}$(line 10 of Algo.~\ref{algo:parameter:estimation}) and broadcasts it to all agents (line 11 of Algo.~\ref{algo:fed:linear:dueling:bandits}), who then use it to select their pairs of arms in the next iteration $t+1$ (lines 4-5 of Algo.~\ref{algo:fed:linear:dueling:bandits}).

% The assumption we adopt in the theoretical analysis of \texttt{FLDB-GD} is that $\theta^{\text{sync}}$ can exactly minimize the loss function in \eqref{eq:loss:func:fed}, i.e., $\mathcal{L}_t^{\text{fed}}(\theta^{\text{sync}}) = 0$ (Assumption \ref{assumption:zero:gradient}).

However, although the objective function~\eqref{eq:loss:func:fed} is convex, finding the optimal solution of the loss function at time $t$ requires multiple rounds of gradient descent. This necessitates multiple rounds of gradient exchanges between the central server and the agents in each iteration. To address this, we propose Algo.~\ref{algo:parameter:estimation:ogd} with OGD to estimate $\theta_{\text{sync}}$, in which only a single round of GD is needed in every iteration.  

\subsection{Federated OGD for Estimating $\theta_{\text{sync}}$ (Algo.~\ref{algo:parameter:estimation:ogd})}
\label{subsec:algo:ogd}
\begin{algorithm}[h]
	\begin{algorithmic}[1]
        % \STATE Output $\theta^{(M)}$
            \IF{t = 1}
         %    \STATE {\color{blue}[Central Server]} Central Server broadcasts $\theta^{(1)}$ to all agents
        	% \FOR{Every Agent $i= 1, \ldots, N$}
         %        \STATE {\color{red}[Agent $i$]} Calculate local gradient $\nabla\mathcal{L}_t^{i}(\theta^{(1)})$ \eqref{eq:loss:func:fed:local}
         %        \STATE {\color{red}[Agent $i$]} Send $\nabla\mathcal{L}_t^{i}(\theta^{(1)})$ back to Central Server
         %    \ENDFOR
         %    \STATE {\color{blue}[Central Server]} Central Server aggregates the local gradients: $\nabla\mathcal{L}_t^{\text{fed}}({\theta}^{(s)}) = \sum^N_{i=1} \nabla\mathcal{L}_t^{i}(\theta^{(s)})$ \eqref{eq:loss:func:fed}
         %    \STATE {\color{blue}[Central Server]} Central Server updates parameter: $\theta^{(s+1)} = \theta^{(s)} - \eta \nabla\mathcal{L}_t^{\text{fed}}({\theta}^{(s)})$
            
            \STATE {\color{blue}[Central Server]} Calculate the optimal value $\widehat{\theta}^{(t)}$ of the loss function~\eqref{eq:loss:func:fed} using algorithm \ref{algo:parameter:estimation}, let $\widetilde{\theta}^{(t)} = \widehat{\theta}^{(t)}$
        
            \STATE {\color{blue}[Central Server]} Compute $r$ and maintain convex set $\mathcal{S}=\left\{\theta:\left\|\theta-\widehat{\theta}^{(t)}\right\| \leq 2 r\right\}$.

            \ELSE
            \STATE {\color{blue}[Central Server]} Central Server broadcasts $\widehat{\theta}^{(t)}$ to all agents
            \FOR{Every Agent $i= 1, \ldots, N$}
            \STATE {\color{red}[Agent $i$]} Calculate local gradient $\nabla l_t^{i}(\widehat{\theta}^{(t)})$ \eqref{eq:loss:func:fed:local:ogd}
            \STATE {\color{red}[Agent $i$]} Send $\nabla l_t^{i}(\widehat{\theta}^{(t)})$ back to Central Server
            \ENDFOR
            \STATE {\color{blue}[Central Server]} Central Server aggregates local gradients: $\nabla f_t^{\text{fed}}(\widehat{\theta}^{(t)}) = \sum^N_{i=1} \nabla l_t^{i}(\widehat{\theta}^{(t)})$ (\eqref{eq:loss:func:fed:ogd} and \eqref{eq:loss:func:fed:local:sum:ogd})
            
            \STATE {\color{blue}[Central Server]} Central Server updates parameter: $\eta_t = \frac{1}{\alpha t}, \widehat{\theta}^{(t+1)} =  \pi_S \left( \widehat{\theta}^{(t)} - \eta_{t} \nabla f_t^{\text{fed}}({ \widehat{\theta}}^{(t)}) \right)$
            \STATE {\color{blue}[Central Server]} Central Server updates parameter: $\widetilde{\theta}^{(t+1)} = \frac{1}{t+1} \sum_{j = 1} ^{t+1} \widehat{\theta}^{(j)}$
            \ENDIF
            \STATE {\color{blue}[Central Server]} $\theta_{\text{sync}} = \widetilde{\theta}^{(t+1)}$
            % \STATE Receive $\{W_{\text{new},i}\}_{i=1,\ldots,N}$
            % \STATE Run $M$ rounds of federated learning to calculate $\theta_{\text{sync}}$ such that $\mathcal{L}_t^{\text{fed}}(\theta_{\text{sync}}) = 0$ (to be relaxed later)
            % \STATE Update $W_{\text{sync}} \leftarrow W_{\text{sync}} + \sum^N_{i=1} W_{\text{new},i}$
            % \STATE Broadcast $\{\theta_{\text{sync}}, W_{\text{sync}}\}$ to all agents
        % \STATE Central Server broadcasts $\theta_{\text{sync}}=\theta^{(M)}$ to all agents
	\end{algorithmic}
\caption{Federated Online GD for Estimating $\theta_{\text{sync}}$}
\label{algo:parameter:estimation:ogd}
\end{algorithm}

The loss function which Algo.~\ref{algo:parameter:estimation:ogd} aims to minimize is the same as~\eqref{eq:loss:func:fed} but reformulated into the following form:
% in different arrangement as shown in~\eqref{eq:loss:func:fed:ogd}
\begin{equation}
\mathcal{L}_t^{\text{fed}}(\theta') = \sum^t_{s=1} f_s(\theta'),
\label{eq:loss:func:fed:ogd}
\end{equation}
where 
\begin{equation}
f^{\text{fed}}_s(\theta') =
\begin{cases} 
\displaystyle \sum_{i=1}^{N} l_s^{i}(\theta'), & \quad \text{if } s \neq 1, \\[15pt]
\displaystyle  \sum_{i=1}^{N} l_1^{i}(\theta') + \frac{\lambda}{2} \| \theta' \|_2^2, & \quad \text{if } s = 1.
\end{cases}
\label{eq:loss:func:fed:local:sum:ogd}
\end{equation}
\begin{equation}
\begin{split}
    l_s^{i}(\theta') = & - \left( y_s^{i}\log\mu\left({\theta'}^{\top}\left[\phi(x_{s,1}^{i}) - \phi(x_{s,2}^{i})\right]\right)  \right. \\
 & \left. + (1-y_s^{i})\log\mu\left({\theta'}^{\top}\left[\phi(x_{s,2}^{i}) - \phi(x_{s,1})\right]\right) \right)
\end{split}
\label{eq:loss:func:fed:local:ogd}
\end{equation}
Intuitively, every individual loss function $f^{\text{fed}}_s$ \eqref{eq:loss:func:fed:local:sum:ogd} corresponds to losses of $N$ agents in iteration $s$.
% We can observe that the function $f_s$ is the loss function of $N$ agents at iteration $s$. 
Inspired by the work of~\citet{ding2021efficientalgorithmgeneralizedlinear} who have used OGD to estimate the parameters in generalized linear bandits, in iteration $t$ of our Algo.~\ref{algo:parameter:estimation:ogd}, we 
% , at iteration $t$, Algo.~\ref{algo:parameter:estimation:ogd} 
only use the latest information (i.e., the gradient of $f^{\text{fed}}_t$ \eqref{eq:loss:func:fed:local:sum:ogd}) to update our estimation of $\theta_{\text{sync}}$.
That is, instead of GD, we use OGD to estimate $\theta_{\text{sync}}$.
% while maintain correctness, which is the online GD setting.

\textbf{Details of Algo.~\ref{algo:parameter:estimation:ogd}.}
At the beginning of the estimation ($t=1$), the central server calculates the minimizer of~\eqref{eq:loss:func:fed:ogd} at iteration 1 (line 2 of Algo.~\ref{algo:parameter:estimation:ogd}), which is also the minimizer of $f^{\text{fed}}_1(\theta')$.
This ensures the subsequent estimated $\theta_{\text{sync}}$ always lies in a bounded ball centered at the groundtruth parameter $\theta$, which is needed in our theoretical analysis (Sec.~\ref{sec:theory}). 
The server also maintains a ball centered at $\widehat{\theta}^{(t)}$ as the 
% mapping 
projection
set during OGD (line 3 of Algo.~\ref{algo:parameter:estimation:ogd}).

% Begin with the second round of communication, which is also the second iteration of the whole algorithm, the server computes the gradient at the current \(\widehat{\theta}^{(t)}\) and performs online gradient descent. The central server first broadcasts its current parameter $\widehat{\theta}^{(t)}$ to all $N$ agents (line 5 of Algo.~\ref{algo:parameter:estimation:ogd}); each agent $i$ calculates its own gradient $\nabla l_t^{i}(\theta^{(t)})$ of~\eqref{eq:loss:func:fed:local:ogd}, and sends it back to the central server (line 7-8 of Algo.~\ref{algo:parameter:estimation:ogd});  then the central server adds up the gradient of \(f_t\) from all agents (line 10 of Algo.~\ref{algo:parameter:estimation:ogd}) using~\eqref{eq:loss:func:fed:local:sum:ogd} and performs one step of projected gradient descent with step size \(\eta = \frac{1}{\alpha t}\) (line 11 of Algo.~\ref{algo:parameter:estimation:ogd}). Finally, to take advantage of the past history, the server takes the average over the historical $\widehat{\theta}^{(s)}$ and uses the averaged $\widetilde{\theta}^{(t+1)}$ as the estimate $\theta_{\text{sync}}$ at round \(t\) and broadcasts it to all agents (line 12-14 of Algo.~\ref{algo:parameter:estimation:ogd}).
Starting from the second round of communication ($t\ge 2$), which corresponds to the second iteration of the overall algorithm, the server \emph{computes the gradient at the current \( \widehat{\theta}^{(t)} \) and performs online gradient descent}. 
Specifically, the central server firstly broadcasts its current parameter \( \widehat{\theta}^{(t)} \) to all \( N \) agents (line 5 of Algo.~\ref{algo:parameter:estimation:ogd}). Each agent \( i \) then computes its own gradient \( \nabla l_t^{i}(\widehat{\theta}^{(t)}) \) of~\eqref{eq:loss:func:fed:local:ogd} and sends it back to the central server (lines 7-8 of Algo.~\ref{algo:parameter:estimation:ogd}). 
After that, the central server aggregates the individual gradients \( \nabla l_t^{i}(\widehat{\theta}^{(t)}) \) from all agents to obtain \( \nabla f_t^{\text{fed}}(\widehat{\theta}^{(t)}) \)
% the gradients of \( f_t \) from all agents 
following~\eqref{eq:loss:func:fed:local:sum:ogd} (line 10 of Algo.~\ref{algo:parameter:estimation:ogd}).
Then, the central server performs one step of projected gradient descent with step size \( \eta = \frac{1}{\alpha t} \) (line 11).  
Finally, to leverage historical information, the server averages (i.e., averages) the past estimates \( \widehat{\theta}^{(s)} \) and uses the resulting \( \widetilde{\theta}^{(t+1)} \) as the updated estimate \( \theta_{\text{sync}} \) in round \( t \) (lines 12-14 of Algo.~\ref{algo:parameter:estimation:ogd}). This estimate \( \theta_{\text{sync}} \) is then broadcast to all agents (line 11 of Algo.~\ref{algo:fed:linear:dueling:bandits}).
Compared to Algo.~\ref{algo:parameter:estimation}, Algo.~\ref{algo:parameter:estimation:ogd} requires only one round of communication per iteration for \( t > 1 \). Consequently, Algo.~\ref{algo:parameter:estimation:ogd} is more communication-efficient.

\textbf{Communication Efficiency.}
In both \texttt{FLDB-GD} and \texttt{FLDB-OGD}, communication between the central server and the agents occurs in every iteration, which is unlike some previous works on federated bandits \cite{wang2019distributed,dai2022federated}.
This is because of the inherent difficulty in the problem setting of contextual dueling bandits.
Specifically, if we start a communication round only after multiple iterations, then when an agent selects arms in an intermediate iteration, it has collected some additional local observations since the last communication round. 
As a result, the algorithm and analysis from \citet{wang2019distributed} require a mechanism to \emph{combine the newly collected local information with the synchronized information from all agents in the last communication round}.
This can be easily achieved in (non-dueling) linear bandits \emph{thanks to the closed-form expression of the estimated $\theta$} in terms of a summation across iterations \cite{wang2019distributed}, which allows us to combine the new local information and synchronized global information via a simple summation.
However, this becomes infeasible in our problem of contextual dueling bandits where \emph{a closed-form expression of the estimated $\theta$ is unavailable}.
This makes 
% it highly challenging to 
combining the local and global information, and hence designing an algorithm not requiring communication in every iteration,
% more communication-efficient algorithm, 
highly challenging, which we leave to future work.

% We have adopted this design to simplify the theoretical analysis (Sec.~\ref{sec:theory}). However, our algorithms can be easily generalized to start a communication round after multiple iterations of arm selection.
% This more communication-efficient algorithm is significantly more challenging to analyze mainly due to the lack of closed-form expression of estimated linear function parameter.
% We leave it to future works to analyze this algorithm.


\section{Theoretical Analysis}
\label{sec:theory}
% In our federated linear dueling bandit algorithm (Algorithm \ref{algo:fed:linear:dueling:bandits}), communication happens after every iteration.
% We use $N$ to denote the number of agents, and $T$ to represent the number of iterations (horizon). In every iteration, every agent (in parallel) chooses its pair of arms.
% \label{sec:fed:analysis}
Here we analyze the regret of our \texttt{FLDB-GD} and \texttt{FLDB-OGD} algorithms.

\subsection{Analysis of \texttt{FLDB-GD}}
\label{subsec:theory:gd}
We make the following assumption in our analysis of \texttt{FLDB-GD}.
\begin{assumption}\label{assumption:zero:gradient}
In our analysis here, we assume that when running Algo.~\ref{algo:parameter:estimation}, the resulting $\theta_{\text{sync}}=\theta^{(M)}$ can exactly minimize \eqref{eq:loss:func:fed}, i.e., $\nabla\mathcal{L}_t^{\text{fed}}(\theta^{(M)})=0$. 
% Later we can try to come up with ways to relax this to account for the error of gradient descent. A (potentially) simple way is to assume that after running  $M$ steps of gradient descent, the norm of the gradient is small enough (but may not be $0$): $\norm{\nabla\mathcal{L}_t^{\text{fed}}(\theta^{(M)})}_2 \leq \varepsilon_t$, and as a result, the $\varepsilon_t$'s will appear in the final regret bound.
\end{assumption}
This assumption is commonly adopted by previous works on (non-federated) generalized linear bandits and dueling bandits \cite{li2017provablyoptimalalgorithmsgeneralized,NeurIPS21_saha2021optimal,ICML22_bengs2022stochastic}.
That is, it is a common practice in the literature to assume that the maximum likelihood estimation of the parameter $\theta$ is obtained exactly.
In our problem of federated dueling bandits, this may incur a large number of communication rounds between the central server and the agents.
We will remove the need for this assumption in our analysis of our \texttt{FLDB-OGD} algorithm (Sec.~\ref{subsec:theory:ogd}).
The theorem below provides an upper bound on the cumulative regret of our \texttt{FLDB-GD} algorithm.

Denote $\theta_t\triangleq {\arg\min}_{\theta'} \mathcal{L}_t^{\text{fed}}(\theta')$, i.e., $\theta_t$ is the minimizer of the loss function $\mathcal{L}_t^{\text{fed}}$.
Note that according to Assumption \ref{assumption:zero:gradient}, we have that $\theta_{\text{sync}}= \theta_{t}$.
In our proof of the regret upper bound for \texttt{FLDB-GD}, a crucial step is to derive the following concentration bound.
% for $\theta$.
\begin{lemma}\label{lemma:concentration:theta}
% Denote $\theta_t\triangleq {\arg\min}_{\theta'} \mathcal{L}_t^{\text{fed}}(\theta')$.
Let
% \begin{align*}
    $$\beta_t \triangleq \sqrt{2\log(1/\delta) + d\log\left( 1 + tN\kappa_{\mu}/(d\lambda) \right)}$$
% \end{align*}
Then with probability of at least $1-\delta$, we have that for all $t=1,\ldots,T$
\[
\norm{\theta - \theta_{t}}_{V_t} \leq \frac{\beta_t}{\kappa_\mu}.
\]
\end{lemma}
Lemma \ref{lemma:concentration:theta} suggests that after running the federated GD algorithm in Algo.~\ref{algo:parameter:estimation}, the resulting $\theta_{\text{sync}}=\theta_t$ is a good approximation of the groundtruth parameter $\theta$.
% Note that in \texttt{FLDB-GD}, $\theta_{\text{sync}}= \theta_{t}$
The regret of \texttt{FLDB-GD} can then be upper-bounded by the following theorem.
\begin{theorem}[\texttt{FLDB-GD}]
\label{theorem:gd}
% In \texttt{FLDB-GD}, if we choose $\beta_t \triangleq \sqrt{2\log(1/\delta) + d\log\left( 1 + t N\kappa_{\mu}/(d\lambda) \right)}$, then 
With probability at least $1-\delta$, the overall regrets of all agents in all iterations satisfy:
\begin{equation}
\begin{split}
R_{T,N} &\leq 2N d\log \left( 1 + T N \kappa_{\mu}/(d\lambda) \right)  \\ & + 3 \sqrt{2} \frac{\beta_T}{\kappa_\mu} \sqrt{T 2 d\log \left( 1 + T N\kappa_{\mu}/(d\lambda) \right)} \\
\end{split}
\end{equation}
Ignoring all log factors, we have that
\begin{equation}
\begin{split}
    R_{T,N} = \widetilde{O}\left(N d + \frac{d}{\kappa_\mu} \sqrt{T}\right)
\end{split}
\end{equation}
% \begin{equation}
% \begin{split}
%     R_{T,N} & = \widetilde{O}\left(N d + \frac{\sqrt{d}}{\kappa_\mu} \sqrt{Td}\right) \\ &= \widetilde{O}\left(N d + \frac{d}{\kappa_\mu} \sqrt{T}\right)
% \end{split}
% \end{equation}
\end{theorem}
The regret upper bound for \texttt{FLDB-GD} (Theorem \ref{theorem:gd}) is sub-linear in $T$.
Theorem \ref{theorem:gd} suggests that the average regret of $N$ agents in our \texttt{FLDB-GD} algorithm is upper-bounded by $\widetilde{O}\left(d + \frac{d}{N \kappa_\mu} \sqrt{T}\right)$, which becomes smaller with a larger number $N$ of agents.
This demonstrates \emph{the benefit of collaboration}, because it shows that if a larger number $N$ of agents join the federation of our algorithm, they are guaranteed (on average) to achieve a smaller regret compared with running their contextual dueling bandit algorithms in isolation (i.e., $N=1$).
When there is a single agent, the regret upper bound from Theorem \ref{theorem:gd} reduces to $\widetilde{O}(d + d\sqrt{T}/\kappa_\mu) = \widetilde{O}(d\sqrt{T}/\kappa_\mu)$, which is of the same order as the classical contextual linear dueling bandit algorithm \cite{ICML22_bengs2022stochastic}.
% Compared to the regret upper bound of single-agent linear dueling bandits, which is of the order $\widetilde{O}(d\sqrt{T}/\kappa_\mu)$, the average regret upper bound of \texttt{FLDB-GD} only incurs an extra additive factor of $d$.



\subsection{Analysis of \texttt{FLDB-OGD}}
\label{subsec:theory:ogd}
% Lemma 5.2~\ref{lemma:ogd:init}

% Prop 5.4~\ref{prop:eigenvalue}: to prove strong convexity, to show $\lambda_{min} \geq B$

In the analysis of \texttt{FLDB-OGD}, we do not require Assumption \ref{assumption:zero:gradient}. In other words, the $\theta_{\text{sync}}=\widetilde{\theta}^{(t)}$ returned by Algo.~\ref{algo:parameter:estimation:ogd} is no longer the minimizer of $\mathcal{L}_t^{\text{fed}}$ in \eqref{eq:loss:func:fed:ogd}.
% : $\theta_t\triangleq {\arg\min}_{\theta'} \mathcal{L}_t^{\text{fed}}(\theta')$.
However, here we prove that as long as we have a sufficiently large number of agents, the difference between $\theta_{\text{sync}}=\widetilde{\theta}^{(t)}$ and $\theta_t$ is still bounded.

% The following lemma is an important intermediate step in the proof of the regret upper bound for \texttt{FLDB-OGD},
To begin with, the following proposition proves that 
% which proves that 
as long as the number $N$ of agents is large enough, 
every loss function $f_s^{\text{fed}}(\theta^{\prime})$ in OGD \eqref{eq:loss:func:fed:local:sum:ogd} is strongly convex.
\begin{proposition}[$\alpha$-strongly convex]\label{prop:alpha:convex}
    Denote $\mathbbm{B}_{\eta} := \{ \theta^{\prime}: \|\theta^{\prime}-\theta\| \leq \eta\}$.
    For a constant $\alpha > 0$, let 
    \begin{equation*}
        N \geq \left( \frac{C_1 \sqrt{d} + C_2 \sqrt{\log \frac{2}{\delta}}}{\lambda_f} \right)^2 + \frac{2\alpha}{\kappa_{\mu} \lambda_f},
    \end{equation*}
    where $C_1$ and $C_2$ are two universal constants.
     Then $f_s^{\text{fed}} (\theta^{\prime})$ is an $\alpha$-strongly convex function in $\mathbbm{B}_{3r}$, with probability at least $1-\delta$. 
\end{proposition}
Recall that every $f_s^{\text{fed}}(\theta^{\prime})=\sum_{i=1}^{N} l_s^{i}(\theta')$ corresponds to an individual loss function in our overall loss function \eqref{eq:loss:func:fed:ogd} during our OGD algorithm (Algo.~\ref{algo:parameter:estimation:ogd}).
Therefore, 
% as a result of 
we can make use of 
Proposition \ref{prop:alpha:convex} and the convergence result of OGD for strongly convex functions \cite{hazan2016introduction} to derive the following lemma.
\begin{lemma}\label{lemma:ogd:convergence}
    % For a constant $\alpha > 0$, let 
    % \begin{align}\label{lowerbound:N}
    %     N \geq L^4 \left( \frac{C_1 \sqrt{d} + C_2 \sqrt{\log \frac{2}{\delta}}}{\lambda_f} \right)^2 + \frac{2\alpha}{\kappa_{\mu} \lambda_f},
    %     % \tau & = \lceil \max \{ ( \frac{C_1 \sqrt{d} + C_2 \sqrt{2\log T}}{\sigma_0^2} )^2 + \frac{32 R^2 [d+2\log T]}{c_1^2 \sigma_0^2},  \nonumber \\
    %     % & ( \frac{C_1 \sqrt{d} + C_2 \sqrt{3\log T}}{\lambda_f} )^2 + \frac{2\alpha}{c_3 \lambda_f}    \} \rceil,
    % \end{align}
    % where $C_1$ and $C_2$ are two universal constants.
    Under the same condition on $N$ as Proposition \ref{prop:alpha:convex}, with probability at least $1-\delta$, the following holds for all $t\geq 1$, % where $\hat\theta_{j\tau}$ is the MLE at round $t=j\tau$.
    \begin{equation*}
         \norm{\widetilde{\theta}^{(t)} - \theta_t}_{V_t} \leq \frac{N \sqrt{N + \frac{\lambda}{\kappa_{\mu}}}}{\alpha} \sqrt{1+\log t}.
    \end{equation*}
\end{lemma}
% The following lemma is an important intermediate step in the proof of the regret upper bound of \texttt{FLDB-OGD}, which shows that 
Lemma \ref{lemma:ogd:convergence} shows that the difference between $\theta_{\text{sync}}=\widetilde{\theta}^{(t)}$ (returned by Algo.~\ref{algo:parameter:estimation:ogd}) and $\theta_t\triangleq {\arg\min}_{\theta'} \mathcal{L}_t^{\text{fed}}(\theta')$ is bounded.
% Specifically, we prove that as long as the number $N$ of agents is large enough (as specified in \eqref{lowerbound:N}), 
% the minimum eigenvalue of 
% in iteration $s$
As a result, Lemma \ref{lemma:ogd:convergence}, combined with Lemma \ref{lemma:concentration:theta} which has provided an upper bound on the difference between $\theta_t$ and $\theta$, allows us to bound the difference between $\widetilde{\theta}^{(t)}$ and $\theta$.
% every loss function in OGD \eqref{eq:loss:func:fed:local:sum:ogd} is strongly convex.
That is, the parameter $\widetilde{\theta}^{(t)}$ returned by Algo.~\ref{algo:parameter:estimation:ogd} is an accurate estimation of the groundtruth linear function parameter $\theta$.

% \begin{equation}
%     \|\widetilde{\theta}^{(t)} - \theta_t\| \le \frac{NL}{\alpha} \sqrt{\frac{1+\log t}{t}}
% \end{equation}


With these important supporting lemmas, the following theorem can be proved, which gives an upper bound on the cumulative regret of \texttt{FLDB-OGD}.
% To prove Lemma~\ref{lemma:ogd:convergence}, we first need the following proposition:
% First is the Proposition~\ref{prop:eigenvalue} in \cite{li2017provablyoptimalalgorithmsgeneralized}.
\begin{theorem}[\texttt{FLDB-OGD}]\label{theorem:ogd}
    % The overall regrets is bounded by
    % \begin{equation}
    % \begin{split}
    % &R_{T,N} \leq  2N d\log \left( 1 + T N L^2\kappa_{\mu}/(d\lambda) \right) \\ & + 3 \sqrt{2} (\frac{\beta_T}{\kappa_\mu} + \frac{N L \sqrt{N L^2 + \frac{\lambda}{\kappa_{\mu}}}}{\alpha} \sqrt{1+\log T} )\sqrt{T 2 d\log \left( 1 + T N L^2\kappa_{\mu}/(d\lambda) \right)} 
    % \end{split}
    % \end{equation}
    Ignoring all log factors, with probability of at least $1-\delta$, we have the overall regret can be bounded by
    % \begin{equation}
    % \begin{split}
    % R_{T,N} = & \widetilde{O}\left(N d + (\frac{\sqrt{d}}{\kappa_\mu} + \frac{N^{\frac{3}{2}}}{\alpha})\sqrt{Td}\right) \\  = &\widetilde{O}\left(N d + (\frac{d}{\kappa_\mu} +  \frac{N^{\frac{3}{2}} \sqrt{d}}{\alpha})\sqrt{T}\right)
    % \end{split}
    % \end{equation}
    % \begin{equation}
    % \begin{split}
    % R_{T,N} = \widetilde{O}\left(N d + \left(\frac{d}{\kappa_\mu} +  \frac{N^{\frac{3}{2}} \sqrt{d}}{\alpha}\right)\sqrt{T}\right)
    % \end{split}
    % \end{equation}
    \begin{equation}
    \begin{split}
    R_{T,N} = \widetilde{O}\left(N d + \frac{d}{\kappa_\mu} \sqrt{T} +  \frac{N^{\frac{3}{2}} \sqrt{d}}{\alpha} \sqrt{T}\right)
    \end{split}
    \end{equation}
\end{theorem}
The regret upper bound of our \texttt{FLDB-OGD} algorithm is also sub-linear in $T$.
Compared with \texttt{FLDB-GD} (Theorem \ref{theorem:gd}), the regret upper bound for \texttt{FLDB-OGD} has an additional term of $\widetilde{O}(\frac{N^{3/2} \sqrt{d}}{\alpha}\sqrt{T})$.
This is the loss we suffer for not ensuring that the $\theta_{\text{sync}}=\widetilde{\theta}^{(t)}$ returned by Algo.~\ref{algo:parameter:estimation:ogd} could achieve the minimum of $\mathcal{L}_t^{\text{fed}}$ \eqref{eq:loss:func:fed:ogd}.
% However, note that in return, 
On the other hand, by paying this cost in terms of regret, \texttt{FLDB-OGD} gains a considerably smaller communication cost than \texttt{FLDB-GD}, because \texttt{FLDB-OGD} only needs a single round of communication whereas \texttt{FLDB-GD} requires a potentially large number of communication rounds to find the minimum of $\mathcal{L}_t^{\text{fed}}(\theta')$ \eqref{eq:loss:func:fed}.



Unlike the theoretical result for \texttt{FLDB-GD} (Theorem \ref{theorem:gd}), the regret upper bound in (Theorem \ref{theorem:ogd}) is not improved as the number $N$ of agents is increased.
This is because in the proof of Lemma \ref{lemma:ogd:convergence} (App.~\ref{app:sec:proof:lemma:ogd:convergence}), we have made use of the convergence of OGD for strongly convex functions  \cite{hazan2016introduction}, which requires an upper bound on the expected norm of the gradient: $G^2 \geq E\|\nabla f_{s}\|^2$.
We have shown that the worst-case choice of $G$ is $G=N$ (see \eqref{eq:G:upper:bound} in App.~\ref{app:sec:proof:lemma:ogd:convergence}), which contributed a dependency of $N$ to the last term in the regret upper bound in Theorem \ref{theorem:ogd}.
In other words, if we additionally assume that there exists an upper bound $G$ on expected gradient norm which is independent of $N$, then the last term in Theorem \ref{theorem:ogd} can be replaced by $\frac{G\sqrt{N} \sqrt{d}}{\alpha} \sqrt{T}$. This would then make our average regret upper bound (averaged over $N$ agents) become tighter with a larger number $N$ of agents.



\begin{figure*}[h!]
\vspace{-2mm}
     \centering
     \begin{tabular}{ccc}
         \includegraphics[width=0.33\linewidth]{ICML_2025/figures/mean_std_cumulative_regret_from_npy_500_N50_K10_D5.png} & \hspace{-5mm} 
         \includegraphics[width=0.33\linewidth]{ICML_2025/figures/mean_std_cumulative_regret_from_npy_500_N100_K10_D5.png}& \hspace{-3mm}
         \includegraphics[width=0.33\linewidth]{ICML_2025/figures/mean_std_cumulative_regret_from_npy_500_N150_K10_D5.png} \\
         {\hspace{-2mm} (a) $N=50, K=10, d=5$} & {\hspace{-5mm} (b) $N=100, K=10, d=5$} & {\hspace{-6mm} (c) $N=150, K=10, d=5$}
     \end{tabular}
\vspace{-2.8mm}
     \caption{
     Cumulative regret for different methods with varying numbers of agents (a) $N=50$, (b) $N=100$ and
     (c) $N=150$ under the number of arms $K=10$ and dimension $d=5$.}
     \label{fig:exp:synth_1}
\vspace{-1.4mm}
\end{figure*}

\begin{figure*}[h!]
\vspace{-1.2mm}
     \centering
     \begin{tabular}{ccc}
         \includegraphics[width=0.33\linewidth]{ICML_2025/figures/optimal_different_N_K10_D5.png} & \hspace{-5mm} 
         \includegraphics[width=0.33\linewidth]{ICML_2025/figures/ogd_different_N_K10_D5.png}& \hspace{-3mm}
         \includegraphics[width=0.33\linewidth]{ICML_2025/figures/ogd_different_N_K50_D5.png} \\
         {\hspace{-2mm} (a) \texttt{FLDB-GD }$(K=10, d=5)$} & {\hspace{-5mm} (b) \texttt{FLDB-OGD  }$(K=10, d=5)$} & {\hspace{-6mm} (c) \texttt{FLDB-OGD }$(K=50, d=5)$} 
     \end{tabular}
\vspace{-2.8mm}
     \caption{
     Cumulative regret with varying number of agents for the (a) \texttt{FLDB-GD}, (b) \texttt{FLDB-OGD} method.
     (c) Illustration of the performance of \texttt{FLDB-OGD} under large number of arms $K=50$.}
     \label{fig:exp:synth_2}
\vspace{-1.4mm}
\end{figure*}

\vspace{-1.5mm}
\section{Experiments}
\label{sec:experiments}
\vspace{-1.5mm}

% All figures in this section plot the average cumulative regret across all N clients up to an iteration, which allows us to inspect the benefit that the federated setting brings to an agent (on average). All curves stand for the mean and standard error from 3 independent runs. 
To corroborate our theoretical results and demonstrate the empirical effectiveness of our algorithms, we evaluate the performance of our algorithms using synthetic experiments. 

\subsection{Experimental Settings.}
We generate the groundtruth linear function parameter $\theta$ in each experiment by random sampling from the standard Gaussian distribution. 
In each round, every agent receives $K$ arms (i.e., contexts) randomly generated from the standard Gaussian distribution with dimension $d$. 
In order to ensure fair comparisons, for all methods, we use the same set of hyperparameter of $\lambda = \frac{1}{T}$, where $T$ represents the horizon. In this paper, we chose the horizon $T = 500$. 
In our simulation of \texttt{FLDB-GD}, the server is required to find the minimizer of the loss function~\eqref{eq:loss:func:fed}. As we have assumed in Assumption.~\ref{assumption:zero:gradient}, the \(\theta_{\text{sync}}\) exactly solve the minimization problem. And since~\eqref{eq:loss:func:fed} is convex, gradient descent procedure is guaranteed to converge to its optimum. Thus, when we run our \texttt{FLDB-GD} algorithm (Algo.~\ref{algo:parameter:estimation}) and compute the cumulative regret, we simply use the \texttt{LBFGS} method from \texttt{PyTorch}~\citep{paszke2019pytorchimperativestylehighperformance} to efficiently solve for the optimum. For the simulation of \texttt{FLDB-OGD}, we choose $\alpha=1000$.
All figures in this section plot the average cumulative regret of all $N$ clients up to a given iteration, which allows us to inspect the benefit brought by the federated setting to an agent on average. Each curve depicts the mean and standard error computed over three independent runs.
We verify the benefit of the federated setting, we compare our algorithms with the single-agent baseline of linear dueling bandits (LDB) \cite{ICML22_bengs2022stochastic}, which we denote as \texttt{LDB}. Specifically, the baseline of \texttt{LDB} simply performs a local dueling bandit algorithm in isolation.

% We first compared the performance of different methods (\texttt{LDB} by~\citet{ICML22_bengs2022stochastic}, \texttt{FLDB-GD}, \texttt{FLDB-OGD}) under the same setting of $N, K, D$. Figure.~\ref{fig:exp:synth_1}a,~\ref{fig:exp:synth_1}b,~\ref{fig:exp:synth_1}c present the simulation results of different methods under (a) $N=50$, (b) $N=100$ and (C) $N=150$, with the number of contexts $K=10$ and dimension $D=5$, respectively. The greed curve in Figure.~\ref{fig:exp:synth_1} represents \texttt{LDB}, while the blue curve corresponds to \texttt{FLDB-OGD} and the orange curve refers to \texttt{FLDB-GD}. The federated algorithms~(\texttt{FLDB-GD} and \texttt{FLDB-OGD}) remarkably outperform the baseline \texttt{LDB}. Moreover, these two methods both incur sub-linear regret, while the regret obtained by \texttt{FLDB-OGD} is slightly higher than the regret obtained by \texttt{FLDB-GD}, which is suggested by our theoritical results in Section~\ref{sec:fed:analysis}. 




\subsection{Experimental Results.}
% We first compared the performance of different methods (\texttt{LDB} by~\citet{ICML22_bengs2022stochastic}, \texttt{FLDB-GD}, and \texttt{FLDB-OGD}) under the same settings of \( N, K, D \).
We evaluate our \texttt{FLDB-GD} and \texttt{FLDB-OGD} algorithms, as well as the baseline of \texttt{LDB}, under the same settings of the number of agents $N$, the number of arms $K$ and the input dimension $d$.
% \( N, K, d \).
To begin with, we fix \( K = 10 \) and \( d = 5 \), and evaluate the impact of different number of agents $N$.
The results in Figures~\ref{fig:exp:synth_1}a,~\ref{fig:exp:synth_1}b, and~\ref{fig:exp:synth_1}c demonstrate that in all settings, our \texttt{FLDB-GD} and \texttt{FLDB-OGD} algorithms significantly outperform the baseline of \texttt{LDB}.
In addition, our \texttt{FLDB-GD} consistently achieves smaller regrets than \texttt{FLDB-OGD}, which aligns with our theoretical results (Sec.~\ref{subsec:theory:ogd}) because \texttt{FLDB-GD} enjoy a tighter regret upper bound that \texttt{FLDB-OGD}.
However, as we have pointed out in Sec.~\ref{subsec:algo:ogd} and Sec.~\ref{subsec:theory:ogd}, this performance advantage in terms of regret comes at the expense of increased communication costs.
% Figures~\ref{fig:exp:synth_1}a,~\ref{fig:exp:synth_1}b, and~\ref{fig:exp:synth_1}c present the results for different values of \( N \): (a) \( N = 50 \), (b) \( N = 100 \), and (c) \( N = 150 \), with the number of arms set to \( K = 10 \) and the input dimension fixed at \( d = 5 \), respectively.

% The green curve in Figure~\ref{fig:exp:synth_1} represents \texttt{LDB}, while the blue curve corresponds to \texttt{FLDB-OGD}, and the orange curve refers to \texttt{FLDB-GD}. The federated algorithms (\texttt{FLDB-GD} and \texttt{FLDB-OGD}) significantly outperform the baseline \texttt{LDB}. Moreover, both methods exhibit sub-linear regret. However, the regret incurred by \texttt{FLDB-OGD} is slightly higher than that of \texttt{FLDB-GD}, which aligns with our theoretical results presented in Section~\ref{sec:theory}.



\subsection{Impact of the Number of Agents $N$.}
Next, we use Figure~\ref{fig:exp:synth_2} to provide a better illustration of the impact of the number of agents $N$.
In particular, we fix \( K=10 \) and \( d=5 \), and vary the value of $N$ for both \texttt{FLDB-GD} and \texttt{FLDB-OGD}.
The results in Figures~\ref{fig:exp:synth_2}a and~\ref{fig:exp:synth_2}b show that as the number of agents $N$ is increased, both \texttt{FLDB-GD} and \texttt{FLDB-OGD} achieve smaller regrets.
As a further verification of robustness of the performance of our \texttt{FLDB-OGD}, which is a more practical algorithm than \texttt{FLDB-GD}, we adopt a more challenging setting by increasing the number of arms $K$ to $50$ in Figure~\ref{fig:exp:synth_2}c. The results show that the performance improvement of \texttt{FLDB-OGD} due to a larger number of agents $N$ is still consistent in this more challenging setting.

These results serve as validation for the capability of our algorithms to benefit from more collaboration, and provide incentives to encourage the agents to join the federated of our algorithms.
The benefit of a larger $N$ for \texttt{FLDB-GD} is consistent with our theoretical results in Sec.~\ref{subsec:theory:gd}, because according to Theorem \ref{theorem:gd}, a larger number of agents $N$ leads to a tighter upper bound on the average regret across all agents.
% the average regret of all agents .
Of note, the empirical dependency of \texttt{FLDB-OGD} on $N$ appears inconsistent with the theoretical guarantee offered by Theorem \ref{theorem:ogd}.
This may imply that the regret upper bound in Theorem \ref{theorem:gd} is likely overly conservative and hence provide justifications for our extra assumption of a $G$ that is independent of $N$ (the last paragraph of Sec.~\ref{subsec:theory:ogd}).
Because this extra assumption leads to an average regret upper bound for \texttt{FLDB-OGD} which is improved with a larger $N$ and hence results in a better alignment between our theoretical and empirical results.

% To better illustrate the performance of our algorithm as \( N \) increases, we present Figure~\ref{fig:exp:synth_2}. Figures~\ref{fig:exp:synth_2}a and~\ref{fig:exp:synth_2}b show the cumulative regret of \texttt{FLDB-GD} and \texttt{FLDB-OGD} with \( K=10 \) and \( D=5 \), while Figure~\ref{fig:exp:synth_2}c presents the cumulative regret of \texttt{FLDB-OGD} when a larger number of contexts is available at each round (\( K=50 \)).  

% From Figure~\ref{fig:exp:synth_2}, we observe that a larger \( N \) generally leads to a smaller regret, despite the theoretical result in Section~\ref{sec:theory}. 

\vspace{-1.5mm}
\section{Related work}
\vspace{-1.5mm}
\textbf{Federated Bandits.}
%A number of recent works have extended the classic $K$-armed bandits to the federated setting. \citet{li2022privacy} and \citet{li2020federated} focused on incorporating privacy guarantees into federated $K$-armed bandits in both centralized and decentralized settings. \citet{shi2021federated} proposed a setting where the goal is to minimize the regret of a global bandit whose reward of an arm is the average of the rewards of the corresponding arm from all agents, which was later extended by adding personalization such that every agent aims to maximize a weighted combination between the global and local rewards \citep{shi2021federatedwithpersonalization}. Subsequent works on federated $K$-armed bandits have focused on other important aspects such as decentralized communication via the gossip algorithm~\citep{zhu2021federated}, the security aspect via cryptographic techniques~\citep{ciucanu2022samba}. Regarding federated linear contextual bandits, \citet{wang2019distributed} proposed a distributed linear contextual bandit algorithm which allows every agent to use the observations from the other agents by only exchanging the sufficient statistics to calculate the Linear UCB policy. Subsequently, \citet{dubey2020differentially} extended the method from \citet{wang2019distributed} to consider differential privacy and decentralized communication, \citet{huang2021federated} considered a setting where every agent is associated with a unique context vector, \citet{li2021asynchronous} focused on asynchronous communication. Federated kernelized/GP bandits (also named federated Bayesian optimization) have been explored by \citet{dai2020federated,dai2021differentially}, which focused on the practical problem of hyperparameter tuning in the federated setting. The recent works of \citet{li2022communicationkcb,li2022communicationglb} have, respectively, focused on deriving communication-efficient algorithms for federated kernelized and generalized linear bandits. In addition to federated bandits, other similar sequential decision-making problems have also been extended to the federated setting, such as federated neural bandits \cite{dai2022federated}, federated reinforcement learning \citep{fan2021fault,zhuo2019federated} and federated hyperparameter tuning \citep{holly2021evaluation,khodak2021federated,zhou2021flora}.
Recent studies have extended the classical $K$-armed bandit problem to the federated setting. 
\citet{li2022privacy,li2020federated} introduced privacy-preserving federated $K$-armed bandits in centralized and decentralized settings, respectively. \citet{shi2021federated} formulated a global bandit model where arm rewards are averaged across agents, which was later extended to incorporate personalization \citep{shi2021federatedwithpersonalization}. 
% Other works addressed decentralized communication via gossip algorithms \citep{zhu2021federated} and security via cryptographic methods \citep{ciucanu2022samba}. 
For federated linear contextual bandits, \citet{wang2019distributed} proposed a distributed algorithm using sufficient statistics to compute the Linear UCB policy, which was later extended to incorporate differential privacy \citep{dubey2020differentially}, agent-specific contexts \citep{huang2021federated}, and asynchronous communication \citep{li2021asynchronous}. 
Federated kernelized 
% and Gaussian process 
and neural bandits have been developed for hyperparameter tuning \citep{dai2020federated,dai2021differentially,dai2022federated}.
% , with recent work improving the communication efficiency of kernelized and generalized linear bandits \citep{li2022communicationkcb,li2022communicationglb}.
In addition, many recent works have extended federated bandits to various settings and applied them to solve real-world problems \cite{li2022communicationkcb,li2022communicationglb,zhu2021federated,ciucanu2022samba,blaser2024federated,fan2024federated,yang2024federated,solanki2024fairness,fourati2024federated,wang2024towards,li2024federated,wei2024incentivized,li2024fedconpe}.
% However, to the best of our knowledge, none of these previous works are able to handle preference feedback.

% Beyond bandits, federated extensions exist for neural bandits \citep{dai2022federated}, reinforcement learning \citep{fan2021fault,zhuo2019federated}, and hyperparameter tuning \citep{holly2021evaluation,khodak2021federated,zhou2021flora}.

\textbf{Dueling Bandits.}
%Learning from pairwise or $K$-wise comparisons has been thoroughly explored in the literature. In the context of dueling bandits, the focus is on minimizing regret using preference feedback \citep{ICML09_yue2009interactively,ICML11_yue2011beat,JCSS12_yue2012k,WSDM14_zoghi2014relative,ICML14_ailon2014reducing,ICML14_zoghi2014relative,COLT15_komiyama2015regret,ICML15_gajane2015relative,UAI18_saha2018battle,AISTATS19_saha2019active,ALT19_saha2019pac,AISTATS22_saha2022exploiting,ICML23_zhu2023principled}. There are also lots of work such as \citep{NeurIPS21_saha2021optimal,ICML22_bengs2022stochastic,arXiv24_li2024feelgood,ALT22_saha2022efficient,arXiv23_di2023variance}, which they only consider the linear reward function. However, \cite{verma2024neural} extends to the non-linear reward functions.
% Learning from pairwise or $K$-wise comparisons has been extensively studied. In dueling bandits, the goal is to minimize regret using preference feedback
Thanks to its ability to learn from pairwise preference feedback, dueling bandits have received considerable attention in recent years
\citep{ICML09_yue2009interactively,ICML11_yue2011beat,JCSS12_yue2012k,WSDM14_zoghi2014relative,ICML14_ailon2014reducing,ICML14_zoghi2014relative,COLT15_komiyama2015regret,ICML15_gajane2015relative,UAI18_saha2018battle,AISTATS19_saha2019active,ALT19_saha2019pac,AISTATS22_saha2022exploiting,ICML23_zhu2023principled}. 
To account for complicated real-world scenarios, a number of contextual dueling bandits have been developed which model the reward function using either a linear function
% Many works focus on the linear reward function 
\citep{NeurIPS21_saha2021optimal,ICML22_bengs2022stochastic,arXiv24_li2024feelgood,ALT22_saha2022efficient,arXiv23_di2023variance} or a neural network \cite{verma2024neural}.
% , while \citet{verma2024neural} extends this framework to non-linear reward functions.

\vspace{-1.2mm}
\section{Conclusion}
\vspace{-1.2mm}
In this work, we introduced the first federated linear dueling bandit algorithms, enabling multi-agent collaboration while preserving data privacy. By integrating online gradient descent with federated learning, our approach effectively estimates linear function parameters in dueling bandit settings. Theoretical analysis guarantees a sub-linear cumulative regret bound, and empirical results demonstrate the benefits of collaboration.  
A key challenge, as discussed in Sec.~\ref{subsec:algo:ogd}, is designing algorithms with less frequent communication. Future work will explore novel strategies to address this. Besides, extending our methods to non-linear reward functions using kernelized and neural bandits is a promising direction.
% In conclusion, this work introduces the first federated linear dueling bandit algorithms that enable multi-agent collaboration without sharing raw data. By combining online gradient descent with federated learning, we establish a method for estimating linear function parameters in multi-agent dueling bandit problems. Our theoretical analysis shows that this approach achieves a sub-linear upper bound on the cumulative regret, and empirical evaluations confirm its practical benefits, including better performance when agents cooperate. This study offers a new way of addressing linear dueling bandit challenges in settings like recommender systems and large language models, highlighting the potential gains from shared model updates under strict data privacy requirements.
% As we have discussed in Sec.~\ref{subsec:algo:ogd}, it is highly non-trivial to design a federated linear dueling bandit algorithm in which communication occurs after multiple iterations.
% We plan to tackle this in future work through novel algorithmic designs.
% Another potential future work is to extend our algorithms to account for non-linear reward functions by incorporating ideas from kernelized bandits and neural bandits.


% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}

\newpage
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{arxiv}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
\section{Proof of Lemma~\ref{lemma:concentration:theta}}
\label{appendix:proof:theta}
We first consider the classic linear dueling bandit setting, namely the single agent(N=1) scenario. In single agent scenario, we ignore the subscript $i$ (we use $\{x_{t,1}, x_{t,2}, y_t, r_t\}$ instead of $\{x_{t,1,i}, x_{t,2,i}, y_{t,i}, r_{t,i}\}$).
\begin{lemma}[Single Agent]
\label{lemma:concentration:theta:single}
Let $\beta_t \triangleq \sqrt{2\log(1/\delta) + d\log\left( 1 + t\kappa_{\mu}/(d\lambda) \right)}$

With probability of at least $1-\delta$, we have that for all $t=1,\ldots,T$
\[
\norm{\theta - \theta_{t}}_{V_t} \leq \frac{\beta_t}{\kappa_\mu}.
\]
\end{lemma}
\begin{proof}
Let $\phi: \mathbb{R}^{d'} \rightarrow \mathbb{R}^{d}$ is a feature map such that $f(x) = \theta^\top\phi(x)$ and $d \geq d^\prime$. 
In the case of linear bandits, $\phi(x) = x$ and $d=d'$.
In iteration $s$, define $\widetilde{\phi}_s = \phi(x_{s,1}) - \phi(x_{s,2})$.
Define $\widetilde{f}_{s} = f(x_{s,1}) - f(x_{s,2}) =\theta_f^{\top} \widetilde{\phi}_s$.

For any $\theta_f \in\mathbb{R}^{d}$, define 
\[
G_t(\theta_{f^\prime}) = \sum_{s=1}^t \left(\mu(\theta_{f^\prime}^\top\widetilde{\phi}_s ) - \mu(\theta^\top\widetilde{\phi}_s) \right) \widetilde{\phi}_s  + \lambda \theta_{f'}.
\]
For $\lambda'\in(0, 1)$, setting $\theta_{\bar{f}} = \lambda' \theta_{f^\prime_1} + (1 - \lambda')\theta_{f^\prime_2}$.
and using mean-value theorem, we get:
\begin{align}
    \label{eqn:glb}
    G_t(\theta_{f^\prime_1}) - G_t(\theta_{f^\prime_2}) &= \left[\sum_{s=1}^t \dot\mu(\theta_{\bar{f}}^\top\widetilde{\phi}_s)\widetilde{\phi}_s \widetilde{\phi}_s^\top + \lambda \mathbf{I} \right](\theta_{f^\prime_1} - \theta_{f^\prime_2}) & \left( \theta_f \text{ is constant} \right)  \nonumber \\
   %  & \ge \kappa_\mu \left[\sum_{s=1}^t \widetilde{\phi}_s \widetilde{\phi}_s^\top + \frac{\lambda}{\kappa_\mu} \mathbf{I} \right] (\theta_{f^\prime_1} - \theta_{f^\prime_2}) & \left(\text{from A2: } \dot\mu(\theta_{\bar{f}}^\top\phi(x)) \ge \kappa_\mu \right) \nonumber \\
   %  % \implies G_t(\theta_{f^\prime_1}) - G(\theta_{f^\prime_2})
   % &= \kappa_\mu V_t(\theta_{f^\prime_1} - \theta_{f^\prime_2}) & \left(\text{setting } V_t = \sum_{s=1}^t \widetilde{\phi}_s \widetilde{\phi}_s^\top + \frac{\lambda}{\kappa_\mu} \mathbf{I} \right)
\end{align}
Define $M_t = \left[\sum_{s=1}^t \dot\mu(\theta_{\bar{f}}^\top\widetilde{\phi}_s)\widetilde{\phi}_s \widetilde{\phi}_s^\top + \lambda \mathbf{I} \right]$, and define $V_t = \sum_{s=1}^t \widetilde{\phi}_s \widetilde{\phi}_s^\top + \frac{\lambda}{\kappa_\mu} \mathbf{I}$.
Then we have that $M_t \geq \kappa_\mu V_t$ and that $V^{-1}_t \geq \kappa_\mu M^{-1}_t$.
% It is easy to verify that $M_t V^{-1/2} \geq M'_t V^{-1/2}$.

\begin{align*}
    \norm{G_t(\theta_{t})}_{V_t^{-1}}^2 &=  \norm{G_t(\theta) - G_t(\theta_{t})}_{V_t^{-1}}^2 = \norm{M_t (\theta - \theta_t)}_{V_t^{-1}}^2 & \left( G_t(\theta) = 0 \text{ by definition} \right)\\
    & = (\theta - \theta_t)^{\top} M_t V_t^{-1} M_t (\theta - \theta_t)\\
    &\geq (\theta - \theta_t)^{\top} M_t \kappa_\mu M_t^{-1} M_t (\theta - \theta_t)\\
    & = \kappa_\mu(\theta - \theta_t)^{\top} M_t (\theta - \theta_t)\\
    & \geq \kappa_\mu(\theta - \theta_t)^{\top} \kappa_\mu V_t (\theta - \theta_t)\\
    & = \kappa_\mu^2 (\theta - \theta_t)^{\top} V_t (\theta - \theta_t)\\
    % & \ge (\kappa_\mu V_t(\theta - \theta_{t}))^\top V_t^{-1} \kappa_\mu V_t(\theta - \theta_{t}) & \left(\text{as } ||x||_A^2 = x^\top A x \right) \\
    % & = \kappa_\mu^2 (\theta - \theta_{t})^\top V_t V_t^{-1}  V_t(\theta - \theta_{t}) & \left(\text{as } V_t^\top = V_t \text{ and } \kappa_\mu \text{ is constant} \right) \\
    & = \kappa_\mu^2 \norm{\theta - \theta_{t}}^2_{V_t}  & \left(\text{as } ||x||_A^2 = x^\top A x \right)
\end{align*}
The first inequality is because $V^{-1}_t \geq \kappa_\mu M^{-1}_t$, and the second inequality follows from $M_t \geq \kappa_\mu V_t$.

\noindent
% Let $f_t$ be the estimate of $f$ at the beginning of the iteration $t$ and $f_t(x) = \theta_{f_t}^\top\phi(x)$.
Let $f_{t,s} = \theta_t^{\top} \widetilde{\phi}_{s}$, in which $\theta_t$ is our empirical estimate of the parameter $\theta$.
Now using \ref{eqn:glb}, we have that
\begin{align*}
    \norm{G_t(\theta_{t})}_{V_t^{-1}}^2 &=  \norm{G_t(\theta) - G_t(\theta_{t})}_{V_t^{-1}}^2 & \left( G_t(\theta) = 0 \text{ by definition} \right) \\
    & \ge (\kappa_\mu V_t(\theta - \theta_{t}))^\top V_t^{-1} \kappa_\mu V_t(\theta - \theta_{t}) & \left(\text{as } ||x||_A^2 = x^\top A x \right) \\
    & = \kappa_\mu^2 (\theta - \theta_{t})^\top V_t V_t^{-1}  V_t(\theta - \theta_{t}) & \left(\text{as } V_t^\top = V_t \text{ and } \kappa_\mu \text{ is constant} \right) \\
    & = \kappa_\mu^2 \norm{\theta - \theta_{t}}^2_{V_t}  & \left(\text{as } ||x||_A^2 = x^\top A x \right)
\end{align*}

% \begin{align*}
%     \implies \norm{\theta - \theta_{t}}^2_{V_t} &\le \frac{1}{\kappa_\mu^2} \norm{G_t(\theta_{t})}_{V_t^{-1}}^2 \\
%     &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t (\mu(\theta_{t}^\top \widetilde{\phi}_s ) - \mu(\theta^\top \widetilde{\phi}_s) ) \widetilde{\phi}_s + \lambda \theta_t}_{V_t^{-1}}^2 & \left(\text{by definition of } G_t(\theta_{t}) \right) \\
%     &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t (\mu(f_{t,s}) - \mu(\widetilde{f}_s) ) \widetilde{\phi}_s + \lambda \theta_t}_{V_t^{-1}}^2 & \left(\text{see definitions of } f_{t,s}\, \text{and } \widetilde{f}_s \right) \\
%     &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t (\mu(f_{t,s}) - (y_s - \epsilon_s) ) \widetilde{\phi}_s + \lambda \theta_t}_{V_t^{-1}}^2 & \left(\text{as } y_s = \mu(\widetilde{f}_s) + \epsilon_s \right) \\
%     &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t \left(\mu(f_{t,s}) - y_s\right) \widetilde{\phi}_s + \sum_{s=1}^t\epsilon_s \widetilde{\phi}_s  + \lambda \theta_t}_{V_t^{-1}}^2 \\
%     &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t\epsilon_s \widetilde{\phi}_s}_{V_t^{-1}}^2.
% \end{align*}
\begin{align*}
    \implies \norm{\theta - \theta_{t}}^2_{V_t} &\le \frac{1}{\kappa_\mu^2} \norm{G_t(\theta_{t})}_{V_t^{-1}}^2 \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t (\mu(\theta_{t}^\top \widetilde{\phi}_s ) - \mu(\theta^\top \widetilde{\phi}_s) ) \widetilde{\phi}_s + \lambda \theta_t}_{V_t^{-1}}^2 & \left(\text{by definition of } G_t(\theta_{t}) \right) \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t (\mu(f_{t,s}) - \mu(\widetilde{f}_s) ) \widetilde{\phi}_s + \lambda \theta_t}_{V_t^{-1}}^2 & \left(\text{see definitions of } f_{t,s}\, \text{and } \widetilde{f}_s \right) \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t (\mu(f_{t,s}) - (y_s - \epsilon_s) ) \widetilde{\phi}_s + \lambda \theta_t}_{V_t^{-1}}^2 & \left(\text{as } y_s = \mu(\widetilde{f}_s) + \epsilon_s \right) \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t \left(\mu(f_{t,s}) - y_s\right) \widetilde{\phi}_s + \sum_{s=1}^t\epsilon_s \widetilde{\phi}_s  + \lambda \theta_t}_{V_t^{-1}}^2 \\
    &\leq \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t\epsilon_s \widetilde{\phi}_s}_{V_t^{-1}}^2.
\end{align*}
The last step follows from the fact that $\theta_{t}$ is computed using MLE by solving the following equation:
\begin{equation}
    \sum_{s=1}^t \left(\mu\left( \theta_t^{\top} \widetilde{\phi}_s \right) - y_s\right) \widetilde{\phi}_s + \lambda \theta_t = 0,
\end{equation}
which is ensured by Assumption~\ref{assumption:zero:gradient}.

With this, we have
\begin{equation}
    \label{eqn:parUB}
    \norm{\theta - \theta_{t}}_{V_t}^2 \le \frac{1}{\kappa_\mu^2} \norm{\sum_{s=1}^t\epsilon_s \widetilde{\phi}_s}_{V_t^{-1}}^2.
\end{equation}

% \end{proof}

% \begin{lemma}\label{lemma:upper:bound:from:abbasi}
% Assume that $\norm{\phi(x_{s,1}) - \phi(x_{s,2})}_2 \leq L, \forall s=1,\ldots,T$. 
% Let $\beta_t \triangleq \sqrt{2\log(1/\delta) + d\log\left( 1 + tL^2\kappa_{\mu}/(d\lambda) \right)}$.
% Then with probability of at least $1-\delta$, we have that
% \[
% \norm{\sum_{s=1}^t\epsilon_s \widetilde{\phi}_s}_{V_t^{-1}}^2 \leq \beta_t^2.
% \]
% \end{lemma}
% \begin{proof}
Denote $V_t \triangleq \sum_{s=1}^t \widetilde{\phi}_s \widetilde{\phi}_s^\top + \frac{\lambda}{\kappa_\mu} \mathbf{I}$ and $V \triangleq \frac{\lambda}{\kappa_\mu} \mathbf{I}$.
Denote the observation noise $\epsilon_s = y_s - \mu(f(x_{s,1}) - f(x_{s,2}))$. Note that the sequence of observation noises $\{\epsilon_s\}$ is $1$-sub-Gaussian (justification omitted).

Next, we can apply Theorem 1 from \cite{NIPS11_abbasi2011improved}, to obtain
\begin{equation}
\norm{\sum_{s=1}^t\epsilon_s \widetilde{\phi}_s}_{V_t^{-1}}^2 \leq 2\log\left( \frac{\det(V_t)^{1/2}}{\delta\det(V)^{1/2}} \right),
\end{equation}
which holds with probability of at least $1-\delta$.

Next, based on our assumption that $\norm{\widetilde{\phi}_s}_2 \leq 1$ (Assumption \ref{assumption:basic}), according to Lemma 10 from \cite{NIPS11_abbasi2011improved}, we have that
\begin{equation}\label{eqn:single}
\det(V_t) \leq \left( \lambda/\kappa_\mu +t/ d \right)^d.
\end{equation}
Therefore, 
\begin{equation}
\sqrt{\frac{\det{V_t}}{\det(V)}} \leq \sqrt{\frac{\left( \lambda/\kappa_\mu +t / d \right)^d}{(\lambda/\kappa_\mu)^d}} = \left( 1 + t\kappa_{\mu}/(d\lambda) \right)^{\frac{d}{2}}
\label{eq:upper:bound:det:Vt:V}
\end{equation}
This gives us
\begin{equation}
\norm{\sum_{s=1}^t\epsilon_s \widetilde{\phi}_s}_{V_t^{-1}}^2 \leq 2\log\left( \frac{\det(V_t)^{1/2}}{\delta\det(V)^{1/2}} \right) \leq 2\log(1/\delta) + d\log\left( 1 + t\kappa_{\mu}/(d\lambda) \right)
\label{eq:upper:bound:proof:theta:interm}
\end{equation}

Combining \eqref{eqn:parUB} and \eqref{eq:upper:bound:proof:theta:interm}, we have that
\begin{equation}
\norm{\theta - \theta_{t}}_{V_t}^2 \le \frac{1}{\kappa_\mu^2} \left(2\log(1/\delta) + d\log\left( 1 + t\kappa_{\mu}/(d\lambda) \right)\right) = \frac{\beta_t^2}{\kappa_\mu^2},
\end{equation}
which completes the proof.

\end{proof}

To establish the multi-agent scenario, it suffices to replace~\eqref{eqn:single} with  

\begin{equation}\label{eqn:multi}  
\det(V_t) \leq \left( \lambda/\kappa_\mu + tN / d \right)^d, 
\end{equation}  

while keeping the rest unchanged.

\section{Proof of Theorem~\ref{theorem:gd}}\label{appendix:theorem:gd}
To prove Theorem~\ref{theorem:gd}, we first need the following lemma:
\begin{lemma}
\label{lemma:concentration:square:std}
\[
\sum^T_{t=1}\norm{\phi(x_{t,1}) - \phi(x_{t,2})}_{V_{t-1}^{-1}}^2 \leq 2 d\log \left( 1 + TN\kappa_{\mu}/(d\lambda) \right).
\]
\end{lemma}

Similar to the proof of Lemma~\ref{lemma:concentration:theta} in Appendix~\ref{appendix:proof:theta}, we first prove the single agent case:
\begin{lemma}[Single Agent]
\label{lemma:concentration:square:std:single}
\[
\sum^T_{t=1}\norm{\phi(x_{t,1}) - \phi(x_{t,2})}_{V_{t-1}^{-1}}^2 \leq 2 d\log \left( 1 + T\kappa_{\mu}/(d\lambda) \right).
\]
\end{lemma}
\begin{proof}
We denote $\widetilde{\phi}_t = \phi(x_{t,1}) - \phi(x_{t,2})$.
Recall that we have assumed that $\norm{\phi(x_{t,1}) - \phi(x_{t,2})}_2 \leq 1$.
It is easy to verify that $V_{t-1} \succeq \frac{\lambda}{\kappa_\mu} I$ and hence $V_{t-1}^{-1} \preceq \frac{\kappa_\mu}{\lambda}I$.
Therefore, we have that $\norm{\widetilde{\phi}_t}_{V_{t-1}^{-1}}^2 \leq \frac{\kappa_\mu}{\lambda} \norm{\widetilde{\phi}_t}_{2}^2 \leq  \frac{\kappa_\mu}{\lambda}$. We choose $\lambda$ such that $\frac{\kappa_\mu}{\lambda} \leq 1$, which ensures that $\norm{\widetilde{\phi}_t}_{V_{t-1}^{-1}}^2 \leq 1$.
Our proof here mostly follows from Lemma 11 of \cite{NIPS11_abbasi2011improved}. To begin with, note that $x\leq 2\log(1+x)$ for $x\in[0,1]$. Then we have that 
\begin{equation}
\begin{split}
\sum^T_{t=1}\norm{\widetilde{\phi}_t}_{V_{t-1}^{-1}}^2 &\leq \sum^T_{t=1} 2\log\left(1 + \norm{\widetilde{\phi}_t}_{V_{t-1}^{-1}}^2\right)\\
&= 2 \left( \log\det V_T - \log\det V \right)\\
&= 2 \log \frac{\det V_T}{\det V}\\
&\leq 2\log \left( \left( 1 + T \kappa_{\mu}/(d\lambda) \right)^{d} \right)\\
&= 2 d\log \left( 1 + T\kappa_{\mu}/(d\lambda) \right).
% &\leq 2 \left(d\log( (\text{trace}(V) + tL^2) / d) - \log\det V \right)\\
% &= 2 \left(d\log( ( \frac{\lambda}{\kappa_\mu} d + tL^2) / d) - \log\det V \right)\\
\end{split}
\end{equation}
The second inequality follows from \eqref{eq:upper:bound:det:Vt:V}.
This completes the proof.
\end{proof}

Lemma~\ref{lemma:concentration:square:std:single} can be applied to~\eqref{eqn:multi} to extend the conclusion to the \( N \)-agent case, resulting in Lemma~\ref{lemma:concentration:square:std}.

Then we can derive the regret bound of single agent case:
\begin{lemma}
\label{lemma:ucb:diff:single}
In any iteration $t=1,\ldots,T$, for all $x,x'\in\mathcal{X}_t$, with probability of at least $1-\delta$, we have that
\[
|\left(f(x) - f(x')\right) - \theta_t^{\top}\left( \phi(x) - \phi(x') \right)| \leq \frac{\beta_t}{\kappa_\mu}\norm{\phi(x) - \phi(x')}_{V_{t-1}^{-1}}.
\]
\end{lemma}
\begin{proof}
\begin{equation}
\begin{split}
|\left(f(x) - f(x')\right) - \theta_t^{\top}\left( \phi(x) - \phi(x') \right)| &= |\theta^{\top} \left[(\phi(x) - \phi(x')\right] - \theta_t^{\top}\left[ \phi(x) - \phi(x') \right]|\\
&= | \left(\theta - \theta_t\right)^{\top} \left[\phi(x) - \phi(x') \right]|\\
&\leq \norm{\theta - \theta_t}_{V_{t-1}} \norm{\phi(x) - \phi(x')}_{V_{t-1}^{-1}}\\
&\leq \frac{\beta_t}{\kappa_\mu}\norm{\phi(x) - \phi(x')}_{V_{t-1}^{-1}},
\end{split}
\end{equation}
in which the last inequality follows from Lemma \ref{lemma:concentration:theta:single}.
\end{proof}

\begin{theorem}[Single Agent]
Let $\beta_t \triangleq \sqrt{2\log(1/\delta) + d\log\left( 1 + t\kappa_{\mu}/(d\lambda) \right)}$.
With probability of at least $1-\delta$, we have that
% \[
% R_T \leq \frac{3}{2} \frac{\beta_T}{\kappa_\mu} \sqrt{T 2 d\log \left( 1 + T L^2\kappa_{\mu}/(d\lambda) \right)}.
% \]
\[
R_T \leq \frac{3}{2} \frac{1}{\kappa_\mu} \sqrt{2\log(1/\delta) + d\log\left( 1 + T\kappa_{\mu}/(d\lambda) \right)} \sqrt{T 2 d\log \left( 1 + T\kappa_{\mu}/(d\lambda) \right)}.
\]
\end{theorem}
\begin{proof}
\begin{equation}
\begin{split}
2 r_t &= f(x^*_t) - f(x_{t,1}) + f(x^*_t) - f(x_{t,2})\\
&\stackrel{(a)}{\leq} \theta_t^\top \left( \phi(x^*_t) - \phi(x_{t,1}) \right) + \frac{\beta_t}{\kappa_\mu}\norm{\phi(x^*_t) - \phi(x_{t,1})}_{V_{t-1}^{-1}} +  \theta_t^\top \left( \phi(x^*_t) - \phi(x_{t,2}) \right) + \frac{\beta_t}{\kappa_\mu}\norm{\phi(x^*_t) - \phi(x_{t,2})}_{V_{t-1}^{-1}}\\
&= \theta_t^\top \left( \phi(x^*_t) - \phi(x_{t,1}) \right) + \frac{\beta_t}{\kappa_\mu}\norm{\phi(x^*_t) - \phi(x_{t,1})}_{V_{t-1}^{-1}} + \\
&\qquad \theta_t^\top \left( \phi(x^*_t) - \phi(x_{t,1}) \right) + \theta_t^\top \left( \phi(x_{t,1}) - \phi(x_{t,2}) \right) + \frac{\beta_t}{\kappa_\mu}\norm{\phi(x^*_t) - \phi(x_{t,1}) + \phi(x_{t,1}) - \phi(x_{t,2})}_{V_{t-1}^{-1}}\\
&\stackrel{(b)}{\leq} 2 \theta_t^\top \left( \phi(x^*) - \phi(x_{t,1}) \right) + 2 \frac{\beta_t}{\kappa_\mu}\norm{\phi(x^*) - \phi(x_{t,1})}_{V_{t-1}^{-1}} + \\
&\qquad \theta_t^\top \left( \phi(x_{t,1}) - \phi(x_{t,2}) \right) + \frac{\beta_t}{\kappa_\mu}\norm{\phi(x_{t,1}) - \phi(x_{t,2})}_{V_{t-1}^{-1}}\\
&\stackrel{(c)}{\leq} 2 \theta_t^\top \left( \phi(x_{t,2}) - \phi(x_{t,1}) \right) + 2 \frac{\beta_t}{\kappa_\mu}\norm{\phi(x_{t,2}) - \phi(x_{t,1})}_{V_{t-1}^{-1}} + \\
&\qquad \theta_t^\top \left( \phi(x_{t,1}) - \phi(x_{t,2}) \right) + \frac{\beta_t}{\kappa_\mu}\norm{\phi(x_{t,1}) - \phi(x_{t,2})}_{V_{t-1}^{-1}}\\
&\leq \theta_t^\top \left( \phi(x_{t,2}) - \phi(x_{t,1}) \right) + 3 \frac{\beta_t}{\kappa_\mu}\norm{\phi(x_{t,2}) - \phi(x_{t,1})}_{V_{t-1}^{-1}} \\
&\stackrel{(d)}{\leq} 3 \frac{\beta_t}{\kappa_\mu}\norm{\phi(x_{t,1}) - \phi(x_{t,2})}_{V_{t-1}^{-1}} \\
\end{split}
\label{eq:upper:bound:inst:regret:single}
\end{equation}
Step $(a)$ follows from Lemma \ref{lemma:ucb:diff:single}. Step $(b)$ makes use of the triangle inequality.
Step $(c)$ follows from the way in which we choose the second arm $x_{t,2}$: $x_{t,2} = \arg\max_{x\in\mathcal{X}_t} \theta_t^\top \left( \phi(x) - \phi(x_{t,1}) \right) + \frac{\beta_t}{\kappa_\mu}\norm{\phi(x) - \phi(x_{t,1})}_{V_{t-1}^{-1}}$.
Step $(d)$ results from the way in which we select the first arm: $x_{t,1} = \arg\max_{x\in\mathcal{X}_t}\theta_t^\top \phi(x)$.


Recall that $\beta_T = \sqrt{2\log(1/\delta) + d\log\left( 1 + T \kappa_{\mu}/(d\lambda) \right)}$.

\begin{equation}
\begin{split}
R_T = \sum^T_{t=1} r_t &\leq \sum^T_{t=1} \frac{3}{2} \frac{\beta_t}{\kappa_\mu}\norm{\phi(x_{t,1}) - \phi(x_{t,2})}_{V_{t-1}^{-1}}\\
&\leq \frac{3}{2} \frac{\beta_T}{\kappa_\mu} \sqrt{T \sum^T_{t=1}\norm{\phi(x_{t,1}) - \phi(x_{t,2})}_{V_{t-1}^{-1}}^2}\\
&\leq \frac{3}{2} \frac{\beta_T}{\kappa_\mu} \sqrt{T 2 d\log \left( 1 + T \kappa_{\mu}/(d\lambda) \right)}.
\end{split}
\end{equation}
\end{proof}



Now we begin to prove Theorem~\ref{theorem:gd}.

We use $V_t$ to denote the covariance matrix after iteration $t$: $V_t \triangleq \sum^t_{\tau=1}\sum^N_{i=1} \widetilde{\phi}_{\tau,i} \widetilde{\phi}_{\tau,i}^{\top} + \frac{\lambda}{\mu} \mathbf{I}$, in which $\widetilde{\phi}_{t,i} = \phi(x_{t,1,i}) - \phi(x_{t,2,i})$.
Consider a hypothetical agent which chooses all $T \times N$ pairs of arms in a round-robin fashion, i.e., it chooses $\{(x_{1,1,1},x_{1,2,1}), (x_{1,1,2},x_{t,2,2}), \ldots, (x_{1,1,N},x_{t,2,N}), \ldots, (x_{T,1,N},x_{T,2,N})\}$.
Define the covariance matrix for this hypothetical agent as $\widetilde{V}_{t,i} \triangleq V_{t} + \sum^i_{j=1} \widetilde{\phi}_{t,j} \widetilde{\phi}_{t,j}^{\top}$.
That is, the covariance matrix $\widetilde{V}_{t,i}$ consists of the information from the previous $t$ iterations, as well as the information from first $i$ agents in iteration $t+1$.

Define the set of "bad iterations" as $\mathcal{C} = \{ t=1,\ldots,T | \frac{\det V_{t}}{ \det V_{t-1}} >2 \}$, and we use $|\mathcal{C}|$ to represent the size of the set $\mathcal{C}$. Denoting $V_0=V=\frac{\lambda}{\kappa_\mu} \mathbf{I}$, we can show that 
\begin{equation}
\frac{\det V_T}{\det V} = \prod_{t=1,\ldots,T} \frac{\det V_t}{\det V_{t-1}} \geq \prod_{t \in \mathcal{C}} \frac{\det V_t}{\det V_{t-1}} > 2^{|\mathcal{C}|}.
\end{equation}
This implies that $|\mathcal{C}| \leq \log \frac{\det V_T}{\det V}$.

Next, we analyze the instantaneous regret in a good iteration, i.e., an iteration $t$ for which $\frac{\det V_{t}}{ \det V_{t-1}} \leq 2$.
Let $\beta_t \triangleq \sqrt{2\log(1/\delta) + d\log\left( 1 + t N\kappa_{\mu}/(d\lambda) \right)}$.
{ Note that we still have $\norm{\theta - \theta_{t}}_{V_{t-1}} \leq \frac{\beta_t}{\kappa_\mu}$, where $\theta_t$ refers to $\theta_{sync}$ at time t}.Following the analysis of \ref{eq:upper:bound:inst:regret:single}, we can show that with probability of at least $1-\delta$,
\begin{equation}
\begin{split}
r_{t,i} &= f(x_{t,i}^*) - f(x_{t,1,i}) + f(x_{t,i}^*) - f(x_{t,2,i})\\
&\leq 3 \frac{\beta_t}{\kappa_\mu}\norm{\phi(x_{t,1,i}) - \phi(x_{t,2,i})}_{V_{t-1}^{-1}}.
\end{split}
\end{equation}

Next, making use of Lemma 12 of \cite{NIPS11_abbasi2011improved}, we can show that
\begin{equation}
\begin{split}
\norm{\phi(x_{t,1,i}) - \phi(x_{t,2,i})}_{V_{t-1}^{-1}} &\leq \norm{\phi(x_{t,1,i}) - \phi(x_{t,2,i})}_{\widetilde{V}_{t-1}^{-1}} \sqrt{ \frac{\det \widetilde{V}_{t-1,i}}{\det V_{t-1}}}\\
&\leq \norm{\phi(x_{t,1,i}) - \phi(x_{t,2,i})}_{\widetilde{V}_{t-1}^{-1}} \sqrt{ \frac{\det V_{t}}{\det V_{t-1}}}\\
&\leq \sqrt{2} \norm{\phi(x_{t,1,i}) - \phi(x_{t,2,i})}_{\widetilde{V}_{t-1}^{-1}}.
\end{split}
\end{equation}
This allows us to show that
\begin{equation}
r_{t,i} \leq 3 \sqrt{2} \frac{\beta_t}{\kappa_\mu} \norm{\phi(x_{t,1,i}) - \phi(x_{t,2,i})}_{\widetilde{V}_{t-1}^{-1}}
\end{equation}

Now the overall regrets of all agents in all iterations can be analyzed as:
\begin{equation}
\begin{split}
R_{T,N} &= \sum^T_{t=1}\sum^N_{i=1} r_{t,i} = |\mathcal{C}| N 2 + \sum_{t\notin\mathcal{C}} \sum^N_{i=1} r_{t,i} \\
&\leq 2N \log \frac{\det V_T}{\det V} + \sum_{t\notin\mathcal{C}} \sum^N_{i=1} r_{t,i}\\
&\leq 2N \log \frac{\det V_T}{\det V} + \sum^T_{t=1} \sum^N_{i=1} r_{t,i}\\
&\leq 2N \log \frac{\det V_T}{\det V} + 3 \sqrt{2} \frac{\beta_T}{\kappa_\mu} \sum_{t=1}^T \sum^N_{i=1} \norm{\phi(x_{t,1,i}) - \phi(x_{t,2,i})}_{\widetilde{V}_{t-1}^{-1}} \\
&\leq 2N d\log \left( 1 + T N \kappa_{\mu}/(d\lambda) \right) + 3 \sqrt{2} \frac{\beta_T}{\kappa_\mu} \sqrt{T 2 d\log \left( 1 + T N \kappa_{\mu}/(d\lambda) \right)} \\
\end{split}
\end{equation}
Ignoring all log factors, we have that
\begin{equation}
R_{T,N} = \widetilde{O}\left(N d + \frac{\sqrt{d}}{\kappa_\mu} \sqrt{Td}\right) = \widetilde{O}\left(N d + \frac{d}{\kappa_\mu} \sqrt{T}\right)
\end{equation}



\section{Proof of Lemma \ref{lemma:ogd:convergence} }
\label{app:sec:proof:lemma:ogd:convergence}
Lemma~\ref{lemma:ogd:init} ensures all the $\theta_t=\arg\max_{\theta'} $(optimal value of the~\ref{eq:loss:func:fed} at iteration t)  lie in a ball centered with the ground truth $\theta$
\begin{lemma}\label{lemma:ogd:init}
    For horizon T, let $r \triangleq \sqrt{ \frac{2\log(1/\delta) + d\log\left( 1 + T N \kappa_{\mu}/(d\lambda) \right)}{\lambda \kappa_{\mu}}}$. With probability at least $1- \delta$, we have $\|\theta_t - \theta \| \leq r$ for $t= 1 \dots  T$
\end{lemma}

\begin{proof}
    Since  $\norm{\theta - \theta_{t}}_{V_{t-1}} \leq \frac{\beta_t}{\kappa_\mu}$ with probability at least $1-\delta$, we have 
    \begin{equation}
           \frac{\beta_T}{\kappa_\mu} \geq \norm{\theta - \theta_{t}}_{V_{t-1}}  \geq  \sqrt{\lambda_{min}({V_{t-1}})} \norm{\theta - \theta_{t}} \geq \sqrt{\frac{\lambda}{\kappa_{\mu}}}\norm{\theta - \theta_{t}}
    \end{equation}
    Thus, 
    \begin{equation}
    \norm{\theta - \theta_{t}} \leq \frac{\beta_T}{\sqrt{\kappa_\mu \lambda}} =  \sqrt{ \frac{2\log(1/\delta) + d\log\left( 1 + T N \kappa_{\mu}/(d\lambda) \right)}{\lambda \kappa_{\mu}}}
    \end{equation}   
\end{proof}
We will also use the Proposition 1 in \cite{li2017provablyoptimalalgorithmsgeneralized}:
\begin{proposition}[Proposition 1 in \cite{li2017provablyoptimalalgorithmsgeneralized}]
\label{prop:eigenvalue}
Define $V_{n+1} = \sum_{t=1}^n X_tX_t^T$, where $X_t$ is drawn IID from some distribution in unit ball $\mathbbm{B}^d$. Furthermore, let $\Sigma := E[X_t X_t^T]$ be the second moment matrix, let $B, \delta>0$ be two positive constants. Then there exists positive, universal constants $C_1$ and $C_2$ such that $\lambda_{\min} (V_{n+1}) \geq B$ with probability at least $1-\delta$, as long as
\begin{equation*}
    n \geq \left( \frac{C_1 \sqrt{d} + C_2 \sqrt{\log(1/\delta)}}{\lambda_{\min} (\Sigma)} \right)^2 + \frac{2B}{\lambda_{\min} (\Sigma)}.
\end{equation*}
\end{proposition}

To prove Lemma~\ref{lemma:ogd:convergence}, we also need to show Propsition~\ref{prop:alpha:convex}.

% \begin{proposition}[$\alpha$-strongly convex]
%     Denote $\mathbbm{B}_{\eta} := \{ \theta^{\prime}: \|\theta^{\prime}-\theta\| \leq \eta\}$. If 
%     \begin{equation*}
%         N \geq L^4 \left( \frac{C_1 \sqrt{d} + C_2 \sqrt{\log \frac{1}{\delta_2}}}{\lambda_f} \right)^2 + \frac{2\alpha}{\kappa_{\mu} \lambda_f},
%     \end{equation*}
%      then $f_s(\theta^{\prime})$ is an $\alpha$-strongly convex function in $\mathbbm{B}_{3r}$, with probability at least $(1-\delta_1)(1-\delta_2)$. 
% \end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop:alpha:convex}]

    Recall  
    \begin{equation*}
    f_s^{\text{fed}}(\theta') =
    \begin{cases} 
    \displaystyle \sum_{i=1}^{N} l_s^{i}(\theta'), & \quad \text{if } s \neq 1, \\[15pt]
    \displaystyle  \sum_{i=1}^{N} l_1^{i}(\theta') + \frac{\lambda}{2} \| \theta' \|_2^2, & \quad \text{if } s = 1.
    \end{cases}
    \label{eq:loss:func:fed:local:sum:ogd:2}
    \end{equation*}
    we have 
    \begin{align}\label{eq:hessian}
    \nabla^2 l^i_{s} (\theta^{\prime}) &= \mu^{\prime} {(\theta^{\prime}}^T \widetilde{\phi}_{s,i})\widetilde{\phi}_{s,i}  \widetilde{\phi}_{s,i}^T \nonumber, \\
    \nabla^2 f_{s}^{\text{fed}} (\theta^{\prime}) &= \sum_{i=1}^{N} 
 \mu^{\prime} {(\theta^{\prime}}^T \widetilde{\phi}_{s,i}) \widetilde{\phi}_{s,i}  \widetilde{\phi}_{s,i}^T + \lambda \mathbbm{1}(s=1) \succeq \kappa_{\mu} \sum_{i=1}^{N} \widetilde{\phi}_{s,i}  \widetilde{\phi}_{s,i}^T ~~\text{for}~\forall~\theta^{\prime}\in \mathbbm{B}_{3r}. 
    \end{align}
    % Since $\widehat{\theta}_t \in \mathcal{C}$, we have $\| \tilde\theta_j - \theta^*\| \leq 3$. Denote $\mathbbm{B}_{\eta} := \{ \theta: \|\theta-\theta^*\| \leq \eta\}$, we have $\tilde\theta_j, \hat\theta_{j\tau} \in \mathbbm{B}_3$. 
    % For any $v>0$, define $\bar\theta = v\tilde\theta_j + (1-v)\hat\theta_{j\tau}$, since $\mathbbm{B}_3$ is convex, we have $\bar\theta \in \mathbbm{B}_3$. Therefore, we have from Assumption
    The last inequality comes from Assumption~\ref{assumption:basic} and $\lambda > 0$.
    Since we update $\widetilde\theta_t$ every $N$ rounds (for clients), for the next $N$ rounds (for client), the pulled arms are only dependent on $\widetilde\theta_t$. 
Therefore, the feature vectors of pulled arms among the next $N$ rounds are IID. Thus, $\widetilde{\phi}_i$ is drawn IID in unit ball $\mathbbm{B}^d$. By applying Proposition~\ref{prop:eigenvalue} letting $B=\frac{\alpha}{\kappa_{\mu}}$ and with Assumption~\ref{assumption:fed}, we have with probability at least $1-\delta$,
$$
\nabla^2 f_s\left(\theta^{\prime}\right) \succeq \alpha I_d \text { if } N \geq\left(\frac{C_1 \sqrt{d}+C_2 \sqrt{\log \left(1/\delta\right)}}{\lambda_f}\right)^2 +\frac{2 \alpha}{\kappa_{\mu} \lambda_f}
$$

Namely, $f_s^{\text{fed}}\left(\theta^{\prime}\right)$ is $\alpha$-stronly convex for $\forall~ 1 \leqslant s \leqslant t$.
 \end{proof}

Then we will prove Lemma~\ref{lemma:ogd:convergence}.

\begin{proof}
    % By Proposition~\ref{prop:alpha:convex}, we know $f_s(\theta^{\prime})$ is $\alpha$-strongly convex for $\forall~ 1 \leq s \leq t$ with probability at least $1-\delta_2$. 
    Now we have:
    \begin{align*}
        \theta_t &=\arg\min_{\theta'}\mathcal{L}_t^{\text{fed}}(\theta') \\
        \widehat{\theta}^{(t+1)} &= \pi_S \left( \widehat{\theta}^{(t)} - \eta_{t} \nabla f_t^{\text{fed}}({ \widehat{\theta}}^{(t)}) \right) ~~\text{for}~t\geq1 \\
        \widetilde{\theta}^{(t)} &= \frac{1}{t} \sum_{j = 1} ^{t} \widehat{\theta}^{(j)} 
    \end{align*}
    From Lemma~\ref{lemma:ogd:init} and Algorithm~\ref{algo:parameter:estimation:ogd} we have $\theta_t, \widehat{\theta}^{(t+1)}, \widetilde{\theta}^{(t)} \in \mathbbm{B}_{3r}$ with probability at least $1-\frac{\delta}{2}$. Then by Jensen's inequality, we have 
    \begin{equation}\label{eq:jensen}
        \sum_{s=1}^{t}f_s^{\text{fed}}(\widehat{\theta}^{(s)}) \ge \sum_{s=1}^{t}f_s^{\text{fed}}(\widetilde{\theta}^{(t)}).
    \end{equation}
    Since $\mathcal{S}$ is convex, we apply Theorem 3.3 of Section 3.3.1 in \cite{hazan2023introductiononlineconvexoptimization} and get
    \begin{equation*}
             \sum_{s=1}^t \left( f_s^{\text{fed}}(\widehat{\theta}^{(s)}) - f_s^{\text{fed}}(\theta_t) \right) \leq \frac{G^2}{2\alpha} (1+\log t) ~~\forall ~t \ge1
    \end{equation*}

where $G$ satisfies $G^2 \geq E\|\nabla f_{s}^{\text{fed}}\|^2$. Additionally, we have 
\begin{equation}
\|\nabla f_{s}^{\text{fed}}(\theta')\| =  \| \sum_{i=1}^N \left(\mu\left( {\theta'}^{\top} \widetilde{\phi}_{s,i}\right) - y_{s,i}\right) \widetilde{\phi}_{s,i} \| \le N ,~\text{for} s > 1.
\label{eq:G:upper:bound}
\end{equation}
Thus, we have 
    \begin{equation*}
              \frac{N^2}{2\alpha} (1+\log t) \ge \sum_{s=1}^t \left( f_s^{\text{fed}}(\widehat{\theta}^{(s)}) - f_s^{\text{fed}}(\theta_t) \right) \ge\sum_{s=1}^{t} \left(f_s^{\text{fed}}(\widetilde{\theta}^{(t)}) - f_s^{\text{fed}}(\theta_t) \right) 
    \end{equation*}
By Proposition~\ref{prop:alpha:convex}, we know $f_s^{\text{fed}}(\theta^{\prime})$ is $\alpha$-strongly convex for $\forall~ 1 \leq s \leq t$ with probability at least $1-\frac{\delta}{2}$. Meanwhile, $\theta_t, \widehat{\theta}^{(t+1)}, \widetilde{\theta}^{(t)} \in \mathbbm{B}_{3r}$ with probability at least $1-\frac{\delta}{2}$. Then using the property of $\alpha$-strongly convex we have
\begin{equation}
    f_s^{\text{fed}}(\widetilde{\theta}^{(t)}) \ge f_s^{\text{fed}}(\theta_t) + \nabla f_{s}^{\text{fed}}(\theta_t)^{\top}\left(\widetilde{\theta}^{(t)} - \theta_t\right) + \frac{\alpha}{2}\|\widetilde{\theta}^{(t)} - \theta_t\|^2
\end{equation}
Sum over s we have
\begin{equation}\label{eq:convex:ineq}
    \sum_{s=1}^{t}f_s^{\text{fed}}(\widetilde{\theta}_t) \ge \sum_{s=1}^{t}f_s^{\text{fed}}(\theta_t) + \left(\sum_{s=1}^{t}\nabla f_{s}^{\text{fed}}(\theta_t)^{\top}\right)\left(\widetilde{\theta}_t - \theta_t\right) + \frac{\alpha t}{2}\|\widetilde{\theta}_t - \theta_t\|^2
\end{equation}
Since$ \sum_{s=1}^{t}\nabla f_{s}(\theta_t) = 0$ we get
\begin{equation}
    \frac{\alpha t}{2}\|\widetilde{\theta}^{(t)} - \theta_t\|^2 \le \sum_{s=1}^{t}f_s^{\text{fed}}(\widetilde{\theta}^{(t)}) - \sum_{s=1}^{t}f_s^{\text{fed}}(\theta_t) 
    = \sum_{s=1}^{t} \left(f_s^{\text{fed}}(\widetilde{\theta}^{(t)}) - f_s^{\text{fed}}(\theta_t)\right) 
    \le \frac{(N)^2}{2\alpha} (1+\log t) \nonumber
\end{equation}
That is 
\begin{equation}
    \|\widetilde{\theta}^{(t)} - \theta_t\|
    \le \frac{N}{\alpha} \sqrt{\frac{1+\log t}{t}}
\end{equation}
From$ \|\widetilde{\phi}_{s,i}\| \le 1$ we get $\lambda_{max} (V_{t}) \le t N + \frac{\lambda}{\kappa_{\mu}}$. Therefore,
\begin{align}
    \norm{\widetilde{\theta}^{(t)} - \theta_t}_{V_t} &\le
    \sqrt{\lambda_{max} (V_{t})}\|\widetilde{\theta}_t - \theta_t\| \\
    & \le \frac{\sqrt{t N + \frac{\lambda}{\kappa_{\mu}}}N}{\alpha} \sqrt{\frac{1+\log t}{t}}\\
    &\le \frac{N\sqrt{N + \frac{\lambda}{\kappa_{\mu}}}}{\alpha} \sqrt{1+\log t}
\end{align}
The last inequality holds since $t\ge1$.

\end{proof}

\section{Proof of Theorem \ref{theorem:ogd} }

\begin{proof}
Following the definition in Section~\ref{sec:background}, we will analyze the instantaneous regret in a good iteration. First we have
\begin{equation*}
    % \label{eqn}\widetilde{\theta}_t
    \norm{\theta - \widetilde{\theta}^{(t)}}_{V_t} \le  \norm{\theta - \theta_{t}}_{V_t} + \norm{\theta_t - \widetilde{\theta}^{(t)}}_{V_t}\le \frac{\beta_t}{\kappa_{\mu}} + \norm{\theta_t - \widetilde{\theta}^{(t)}}_{V_t} \le \frac{\beta_t}{\kappa
    _{\mu}}+\frac{N\sqrt{N + \frac{\lambda}{\kappa_{\mu}}}}{\alpha} \sqrt{1+\log t}
\end{equation*}
And
\begin{equation}
\begin{split}
|\left(f(x) - f(x')\right) - {\left(\widetilde{\theta}^{(t)}\right)}^{\top}\left( \phi(x) - \phi(x') \right)| &= |\theta^{\top} \left[(\phi(x) - \phi(x')\right] - {\left(\widetilde{\theta}^{(t)}\right)}^{\top}\left[ \phi(x) - \phi(x') \right]|\\
&= | \left(\theta - \widetilde{\theta}^{(t)}\right)^{\top} \left[\phi(x) - \phi(x') \right]|\\
&\leq \norm{\theta - \widetilde{\theta}^{(t)}}_{V_{t-1}} \norm{\phi(x) - \phi(x')}_{V_{t-1}^{-1}}\\
&\leq \left( \frac{\beta_t}{\kappa
    _{\mu}}+\frac{N\sqrt{N + \frac{\lambda}{\kappa_{\mu}}}}{\alpha} \sqrt{1+\log t}\right)\norm{\phi(x) - \phi(x')}_{V_{t-1}^{-1}} \nonumber
\end{split}
\end{equation}
Thus, for the instantaneous regret in good iteration, with probability at least $1-\delta$, we have 
\begin{equation}
\begin{split}
r_{t,i} &= f(x_{t,i}^*) - f(x_{t,1,i}) + f(x_{t,i}^*) - f(x_{t,2,i})\\
&\leq 3 \left( \frac{\beta_t}{\kappa
    _{\mu}}+\frac{N\sqrt{N + \frac{\lambda}{\kappa_{\mu}}}}{\alpha} \sqrt{1+\log t}\right)\norm{\phi(x_{t,1,i}) - \phi(x_{t,2,i})}_{V_{t-1}^{-1}}.
\end{split}
\end{equation}
By following the proof in Appendix~\ref{appendix:theorem:gd},  the overall regrets of all agents in all iterations can be analyzed as:
\begin{equation}
\begin{split}
R_{T,N} 
% &= \sum^T_{t=1}\sum^N_{i=1} r_{t,i} = |\mathcal{C}| N 2 + \sum_{t\notin\mathcal{C}} \sum^N_{i=1} r_{t,i} \\
% &\leq 2N \log \frac{\det V_T}{\det V} + \sum_{t\notin\mathcal{C}} \sum^N_{i=1} r_{t,i}\\
% &\leq 2N \log \frac{\det V_T}{\det V} + \sum^T_{t=1} \sum^N_{i=1} r_{t,i}\\
% &\leq 2N \log \frac{\det V_T}{\det V} + 3 \sqrt{2} \frac{\beta_T}{\kappa_\mu} \sum_{t=1}^T \sum^N_{i=1} \norm{\phi(x_{t,1,i}) - \phi(x_{t,2,i})}_{\widetilde{V}_{t-1}^{-1}} \\
&\leq 2N d\log \left( 1 + T N \kappa_{\mu}/(d\lambda) \right) + 3 \sqrt{2} \left( \frac{\beta_T}{\kappa_{\mu}}+\frac{N\sqrt{N  + \frac{\lambda}{\kappa_{\mu}}}}{\alpha} \sqrt{1+\log T}\right)\sqrt{T 2 d\log \left( 1 + T N \kappa_{\mu}/(d\lambda) \right)} \\
\end{split}
\end{equation}
Ignoring all log factors, we have that
    \begin{equation}
    R_{T,N} = \widetilde{O}\left(N d + (\frac{\sqrt{d}}{\kappa_\mu} + \frac{N^{\frac{3}{2}}}{\alpha})\sqrt{Td}\right) = \widetilde{O}\left(N d + (\frac{d}{\kappa_\mu} +  \frac{N^{\frac{3}{2}}\sqrt{d}}{\alpha})\sqrt{T}\right)
    \end{equation}
% \begin{equation*}
% \begin{split}
% \norm{\theta - \theta_{t}}_{V_{t-1}} \leq \frac{\beta_t}{\kappa_\mu}
% r_{t,i} &= f(x_{t,i}^*) - f(x_{t,1,i}) + f(x_{t,i}^*) - f(x_{t,2,i}) \\
% &\leq \norm{\theta - \theta_{t}}_{V_{t-1}} 
% \end{split}
% \end{equation*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
