\section{Related work}
\vspace{-1.5mm}
\textbf{Federated Bandits.}
%A number of recent works have extended the classic $K$-armed bandits to the federated setting. Mnih, "Federated Multi-Armed Bandits" and Chen et al., "Differentially Private Federated K-Armed Bandit" focused on incorporating privacy guarantees into federated $K$-armed bandits in both centralized and decentralized settings. Zhang et al., "Federated Bandit with Global Reward" proposed a setting where the goal is to minimize the regret of a global bandit whose reward of an arm is the average of the rewards of the corresponding arm from all agents, which was later extended by adding personalization such that every agent aims to maximize a weighted combination between the global and local rewards ____. Subsequent works on federated $K$-armed bandits have focused on other important aspects such as decentralized communication via the gossip algorithm Zhang et al., "Federated Bandit with Decentralized Communication" , the security aspect via cryptographic techniques Wang, "Secure Federated K-Armed Bandit". Regarding federated linear contextual bandits, Zhang, "Distributed Linear Contextual Bandit Algorithm" proposed a distributed linear contextual bandit algorithm which allows every agent to use the observations from the other agents by only exchanging the sufficient statistics to calculate the Linear UCB policy. Subsequently, Chen et al., "Federated Linear Contextual Bandit with Differential Privacy and Decentralized Communication" extended the method from Zhang, "Distributed Linear Contextual Bandit Algorithm" to consider differential privacy and decentralized communication, Zhang et al., "Federated Linear Contextual Bandit with Agent-Specific Contexts" considered a setting where every agent is associated with a unique context vector, Wang et al., "Asynchronous Federated Linear Contextual Bandits" focused on asynchronous communication. Federated kernelized/GP bandits (also named federated Bayesian optimization) have been explored by Li, "Federated Kernelized Bandit Algorithm for Hyperparameter Tuning", which focused on the practical problem of hyperparameter tuning in the federated setting. The recent works of Zhang et al., "Communication-Efficient Federated Kernelized and Generalized Linear Bandits" have, respectively, focused on deriving communication-efficient algorithms for federated kernelized and generalized linear bandits. In addition to federated bandits, other similar sequential decision-making problems have also been extended to the federated setting, such as federated neural bandits Wang et al., "Federated Neural Bandit with Distributed Learning", federated reinforcement learning Chen, "Federated Reinforcement Learning for Recommendation Systems" and federated hyperparameter tuning Li, "Federated Hyperparameter Tuning via Bayesian Optimization".
Recent studies have extended the classical $K$-armed bandit problem to the federated setting.  Zhang et al., "Private-Federated K-Armed Bandits with Linear Rewards" introduced privacy-preserving federated $K$-armed bandits in centralized and decentralized settings, respectively. Li et al., "Global Federated Bandit Model for Averaged Arm Rewards" formulated a global bandit model where arm rewards are averaged across agents, which was later extended to incorporate personalization ____. 
% Other works addressed decentralized communication via gossip algorithms Zhang et al., "Federated Bandit with Decentralized Communication" and security via cryptographic methods Wang, "Secure Federated K-Armed Bandit". 
For federated linear contextual bandits, Zhang, "Distributed Linear Contextual Bandit Algorithm" proposed a distributed algorithm using sufficient statistics to compute the Linear UCB policy, which was later extended to incorporate differential privacy Chen et al., "Federated Linear Contextual Bandit with Differential Privacy and Decentralized Communication", agent-specific contexts Wang et al., "Federated Linear Contextual Bandit with Agent-Specific Contexts" , and asynchronous communication Wang et al., "Asynchronous Federated Linear Contextual Bandits". 
Federated kernelized 
% and Gaussian process 
and neural bandits have been developed for hyperparameter tuning Li, "Federated Kernelized Bandit Algorithm for Hyperparameter Tuning".
% , with recent work improving the communication efficiency of kernelized and generalized linear bandits Zhang et al., "Communication-Efficient Federated Kernelized and Generalized Linear Bandits".
In addition, many recent works have extended federated bandits to various settings and applied them to solve real-world problems Wang, "Real-World Applications of Federated Bandits".
% However, to the best of our knowledge, none of these previous works are able to handle preference feedback.

% Beyond bandits, federated extensions exist for neural bandits Wang et al., "Federated Neural Bandit with Distributed Learning", reinforcement learning Chen, "Federated Reinforcement Learning for Recommendation Systems" , and hyperparameter tuning Li, "Federated Hyperparameter Tuning via Bayesian Optimization".

\textbf{Dueling Bandits.}
%Learning from pairwise or $K$-wise comparisons has been thoroughly explored in the literature. In the context of dueling bandits, the focus is on minimizing regret using preference feedback ____. There are also lots of work such as Zhang et al., "Linear Reward Dueling Bandit Algorithm" , which they only consider the linear reward function. However, Chen et al., "Non-Linear Reward Dueling Bandit Algorithm" extends to the non-linear reward functions.
% Learning from pairwise or $K$-wise comparisons has been extensively studied. In dueling bandits, the goal is to minimize regret using preference feedback
Thanks to its ability to learn from pairwise preference feedback, dueling bandits have received considerable attention in recent years Zhang et al., "Dueling Bandit Algorithm with Preference Feedback".
To account for complicated real-world scenarios, a number of contextual dueling bandits have been developed which model the reward function using either a linear function Li, "Contextual Dueling Bandit with Linear Reward" or a neural network Chen, "Contextual Dueling Bandit with Neural Network Reward".
% Many works focus on the linear reward function 
____ extends this framework to non-linear reward functions.