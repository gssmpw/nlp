\section{Related work}
\vspace{-1.5mm}
\textbf{Federated Bandits.}
%A number of recent works have extended the classic $K$-armed bandits to the federated setting. \citet{li2022privacy} and \citet{li2020federated} focused on incorporating privacy guarantees into federated $K$-armed bandits in both centralized and decentralized settings. \citet{shi2021federated} proposed a setting where the goal is to minimize the regret of a global bandit whose reward of an arm is the average of the rewards of the corresponding arm from all agents, which was later extended by adding personalization such that every agent aims to maximize a weighted combination between the global and local rewards \citep{shi2021federatedwithpersonalization}. Subsequent works on federated $K$-armed bandits have focused on other important aspects such as decentralized communication via the gossip algorithm~\citep{zhu2021federated}, the security aspect via cryptographic techniques~\citep{ciucanu2022samba}. Regarding federated linear contextual bandits, \citet{wang2019distributed} proposed a distributed linear contextual bandit algorithm which allows every agent to use the observations from the other agents by only exchanging the sufficient statistics to calculate the Linear UCB policy. Subsequently, \citet{dubey2020differentially} extended the method from \citet{wang2019distributed} to consider differential privacy and decentralized communication, \citet{huang2021federated} considered a setting where every agent is associated with a unique context vector, \citet{li2021asynchronous} focused on asynchronous communication. Federated kernelized/GP bandits (also named federated Bayesian optimization) have been explored by \citet{dai2020federated,dai2021differentially}, which focused on the practical problem of hyperparameter tuning in the federated setting. The recent works of \citet{li2022communicationkcb,li2022communicationglb} have, respectively, focused on deriving communication-efficient algorithms for federated kernelized and generalized linear bandits. In addition to federated bandits, other similar sequential decision-making problems have also been extended to the federated setting, such as federated neural bandits \cite{dai2022federated}, federated reinforcement learning \citep{fan2021fault,zhuo2019federated} and federated hyperparameter tuning \citep{holly2021evaluation,khodak2021federated,zhou2021flora}.
Recent studies have extended the classical $K$-armed bandit problem to the federated setting. 
\citet{li2022privacy,li2020federated} introduced privacy-preserving federated $K$-armed bandits in centralized and decentralized settings, respectively. \citet{shi2021federated} formulated a global bandit model where arm rewards are averaged across agents, which was later extended to incorporate personalization \citep{shi2021federatedwithpersonalization}. 
% Other works addressed decentralized communication via gossip algorithms \citep{zhu2021federated} and security via cryptographic methods \citep{ciucanu2022samba}. 
For federated linear contextual bandits, \citet{wang2019distributed} proposed a distributed algorithm using sufficient statistics to compute the Linear UCB policy, which was later extended to incorporate differential privacy \citep{dubey2020differentially}, agent-specific contexts \citep{huang2021federated}, and asynchronous communication \citep{li2021asynchronous}. 
Federated kernelized 
% and Gaussian process 
and neural bandits have been developed for hyperparameter tuning \citep{dai2020federated,dai2021differentially,dai2022federated}.
% , with recent work improving the communication efficiency of kernelized and generalized linear bandits \citep{li2022communicationkcb,li2022communicationglb}.
In addition, many recent works have extended federated bandits to various settings and applied them to solve real-world problems \cite{li2022communicationkcb,li2022communicationglb,zhu2021federated,ciucanu2022samba,blaser2024federated,fan2024federated,yang2024federated,solanki2024fairness,fourati2024federated,wang2024towards,li2024federated,wei2024incentivized,li2024fedconpe}.
% However, to the best of our knowledge, none of these previous works are able to handle preference feedback.

% Beyond bandits, federated extensions exist for neural bandits \citep{dai2022federated}, reinforcement learning \citep{fan2021fault,zhuo2019federated}, and hyperparameter tuning \citep{holly2021evaluation,khodak2021federated,zhou2021flora}.

\textbf{Dueling Bandits.}
%Learning from pairwise or $K$-wise comparisons has been thoroughly explored in the literature. In the context of dueling bandits, the focus is on minimizing regret using preference feedback \citep{ICML09_yue2009interactively,ICML11_yue2011beat,JCSS12_yue2012k,WSDM14_zoghi2014relative,ICML14_ailon2014reducing,ICML14_zoghi2014relative,COLT15_komiyama2015regret,ICML15_gajane2015relative,UAI18_saha2018battle,AISTATS19_saha2019active,ALT19_saha2019pac,AISTATS22_saha2022exploiting,ICML23_zhu2023principled}. There are also lots of work such as \citep{NeurIPS21_saha2021optimal,ICML22_bengs2022stochastic,arXiv24_li2024feelgood,ALT22_saha2022efficient,arXiv23_di2023variance}, which they only consider the linear reward function. However, \cite{verma2024neural} extends to the non-linear reward functions.
% Learning from pairwise or $K$-wise comparisons has been extensively studied. In dueling bandits, the goal is to minimize regret using preference feedback
Thanks to its ability to learn from pairwise preference feedback, dueling bandits have received considerable attention in recent years
\citep{ICML09_yue2009interactively,ICML11_yue2011beat,JCSS12_yue2012k,WSDM14_zoghi2014relative,ICML14_ailon2014reducing,ICML14_zoghi2014relative,COLT15_komiyama2015regret,ICML15_gajane2015relative,UAI18_saha2018battle,AISTATS19_saha2019active,ALT19_saha2019pac,AISTATS22_saha2022exploiting,ICML23_zhu2023principled}. 
To account for complicated real-world scenarios, a number of contextual dueling bandits have been developed which model the reward function using either a linear function
% Many works focus on the linear reward function 
\citep{NeurIPS21_saha2021optimal,ICML22_bengs2022stochastic,arXiv24_li2024feelgood,ALT22_saha2022efficient,arXiv23_di2023variance} or a neural network \cite{verma2024neural}.
% , while \citet{verma2024neural} extends this framework to non-linear reward functions.

\vspace{-1.2mm}