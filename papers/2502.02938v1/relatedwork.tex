\section{Background and Related Work}
\subsection{Multimodal Sentiment Analysis}
MSA is an evolving research field focused on analyzing sentiments in various data types, such as images, videos, audio, and text \citep{cheema-etal-2021-fair}. By combining computer vision, natural language processing, and machine learning, MSA has shown success in areas such as social media, customer service, and product reviews \citep{gandhi-etal-2023-multi}.

Previous work has focused on fusing multiple modalities with pre-trained models to predict a multimodal label that encapsulates sentiment labels for each modality. For example, \citet{cheema-etal-2021-fair} concatenated image and text features initialized from CLIP \citep{radford-etal-2021-learning} and RoBERTa \citep{liu-etal-2019-roberta}, respectively. \citet{wang-etal-2023-exploring} fused both features through Convolutional Neural Networks (CNNs) along with Convolutional Block Attention Module (CBAM) \citep{woo-etal-2018-cbam} where the image and text features were initialized from Residual Networks (ResNet) \citep{he-etal-2015-deep} and BERT \citep{devlin-etal-2019-bert}. 

In addition, recent works such as \citet{sanchez-villegas-etal-2024-improving} introduced auxiliary losses to align and minimize discrepancies between text and image representations, while \citet{chen-etal-2024-holistic} utilized several pre-trained models in combination to enhance sentiment analysis performance.

Although these approaches have achieved state-of-the-art performance, they primarily focus on the multimodal label, often overlooking the contributions of unimodal labels from image and text modalities. Furthermore, existing methods, such as those proposed by \citet{wang-etal-2023-exploring,chen-etal-2024-holistic}, rely on complex fusion strategies, leaving the potential of MLLMs for MSA remains largely unexplored. To the best of our knowledge, the role of MLLMs as a classifier for MSA has not been adequately examined, prompting this study to explore their effectiveness in addressing this task.

\subsection{MLLMs as a Classifier}
Recent advances in LLMs have demonstrated their effectiveness in NLP tasks, inspiring the development of MLLMs capable of integrating multiple modalities, such as images, videos, and audio \citep{naveed-etal-2023-comprehensive}. These models combine the strengths of vision, language, and other modalities, enabling more comprehensive and context-aware understanding across multimodal data. 

However, the use of MLLMs as classifiers for MSA remains relatively unexplored. For instance, \citet{sun-etal-2023-text} utilized RoBERTa and GPT-3 with prompts to build a classifier for unimodal sentiment analysis but did not extend their approach to MSA. To address this limitation, we aim to develop a classifier for MSA by leveraging a MLLM, specifically LLaVA \citep{liu-etal-2023-improved,liu-etal-2023-visual-arxiv}.