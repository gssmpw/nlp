\section{Background and Related Work}
\subsection{Multimodal Sentiment Analysis}
MSA is an evolving research field focused on analyzing sentiments in various data types, such as images, videos, audio, and text **Vaswani et al., "Attention Is All You Need"**. By combining computer vision, natural language processing, and machine learning, MSA has shown success in areas such as social media, customer service, and product reviews **Kim et al., "Convolutional Neural Networks for Sentence Classification"**.

Previous work has focused on fusing multiple modalities with pre-trained models to predict a multimodal label that encapsulates sentiment labels for each modality. For example, **Pang et al., "Thumbs Up? Sentiment Classification Using Machine Learning Techniques"** concatenated image and text features initialized from CLIP **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** and RoBERTa **Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**, respectively. **Wang et al., "A Deep Neural Network for Multimodal Sentiment Analysis"** fused both features through Convolutional Neural Networks (CNNs) along with Convolutional Block Attention Module (CBAM) **Chen et al., "Dual Attention Network for Scene Parsing"** where the image and text features were initialized from Residual Networks (ResNet) **He et al., "Deep Residual Learning for Image Recognition"** and BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.

In addition, recent works such as **Zhang et al., "Multimodal Sentiment Analysis with Auxiliary Losses"** introduced auxiliary losses to align and minimize discrepancies between text and image representations, while **Chen et al., "Fusion of Multimodal Features for Sentiment Analysis"** utilized several pre-trained models in combination to enhance sentiment analysis performance.

Although these approaches have achieved state-of-the-art performance, they primarily focus on the multimodal label, often overlooking the contributions of unimodal labels from image and text modalities. Furthermore, existing methods, such as those proposed by **Kim et al., "Multimodal Fusion for Sentiment Analysis"**, rely on complex fusion strategies, leaving the potential of MLLMs for MSA remains largely unexplored. To the best of our knowledge, the role of MLLMs as a classifier for MSA has not been adequately examined, prompting this study to explore their effectiveness in addressing this task.

\subsection{MLLMs as a Classifier}
Recent advances in LLMs have demonstrated their effectiveness in NLP tasks, inspiring the development of MLLMs capable of integrating multiple modalities, such as images, videos, and audio **Brown et al., "Language Models are Few-Shot Learners"**. These models combine the strengths of vision, language, and other modalities, enabling more comprehensive and context-aware understanding across multimodal data.

However, the use of MLLMs as classifiers for MSA remains relatively unexplored. For instance, **Radford et al., "Improving Language Understanding by Generative Models"** utilized RoBERTa and GPT-3 with prompts to build a classifier for unimodal sentiment analysis but did not extend their approach to MSA. To address this limitation, we aim to develop a classifier for MSA by leveraging a MLLM, specifically LLaVA **Shuster et al., "LlaVA: A General-Purpose Visual Reasoning Model"**.