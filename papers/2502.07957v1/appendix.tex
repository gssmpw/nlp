\appendix

\section{Data}
\label{sec:apd_data}
All code and data used in this study will be made available publicly.

\subsection{Datasets Used by Contrastive Language Image Pre-training Models}


Details concerning the datasets that were used for pre-training the CLIP models we study are provided below.

\subsubsection{OpenAI WebImageText}
The OpenAI WIT dataset \cite{Radford2021LearningSupervision} consists of image-caption pairs sourced from the Internet. The dataset only includes pairs whose text included at least one member of a list of common words, bi-grams, and names, taken from Wikipedia and WordNet. Each member in the list of common words, bi-grams, and names was only allowed to be observed in 20,000 image-text pairs, to ensure that they were not over represented. OpenAI WIT contains 400 million pairs, and was not released publicly. We test 9 models trained on the OpenAI WIT dataset. 

\subsubsection{LAION 5 Billion, 2 Billion, 400 Million, 80 Million, and Aesthetics}
The LAION 5B dataset \cite{Schuhmann2022LAION-5B:Models} consists of approximately five billion image-caption pairs. Pairs in the dataset were originally images and alt-text from Common Crawl. After being downloaded, they were passed into the ViT-B/32 CLIP model released by OpenAI (which was trained on OpenAI WIT), and any pairs whose images were not close in cosine distance to their text were filtered out. Images in the dataset that were suspected of containing illegal content were removed, however other potentially harmful images (which make up an estimated 3\% of the dataset) were tagged but kept in the dataset. Of the total pairs, 2.26 billion have English-based captions, while the remaining have captions in other languages or whose language could not be identified. The pairs with English captions make up the LAION 2B dataset, which the LAION 400M and LAION 80M datasets are subsets of. The LAION Aesthetics dataset that was used for pre-training the models we consider consists of approximately 900 million pairs, selected from the LAION 5B dataset for being aesthetically pleasing to human viewers.

\subsubsection{Yahoo-Flickr Creative Commons 15 Million}
The Yahoo-Flickr Creative Commons 15 Million (YFCC15M) dataset \cite{Radford2021LearningSupervision} is a subset of the larger YFCC100M dataset \cite{Thomee2016YFCC100M:Research}. YFCC100M consists of photos and videos uploaded to Flickr between 2004 and 2014, along with titles and descriptions. Around 11 million pairs in the YFCC100M dataset were predicted by a classifier as being related to the concept of "People," (while the most commonly observed concept was "Outdoor," with around 44 million observations), a frequency which we hypothesize may make learning biases related to humans difficult. YFCC15M includes image-text pairs from YFCC100M whose whose title contains natural language, and/or whose description is in English, as many of the text components in YFCC100M seem to consist of generic filenames or descriptions of camera settings  \cite{Radford2021LearningSupervision}. It is unclear the extent to which the distribution of photos in YFCC100M would be reflected in the YFCC15M subset. 

\subsubsection{Conceptual 12 Million}
The Conceptual 12 Million dataset (CC 12M) dataset  \cite{Changpinyo2021ConceptualConcepts} consists of image-text pairs sourced from the Internet. The pairs are filtered based on image format, offensiveness of content, as well as language, capitalization, and other text features. The dataset was hypernymed, whereby names of people were replaced with a special [PERSON] token. Authors of the dataset also state that they examined the dataset for distributional differences between demographic related words, and did not observe any large differences. 

\subsubsection{Commonpool and Datacomp} The Commonpool and Datacomp datasets were constructed as part of the DATACOMP benchmark, aimed at facilitating rigorous research on multimodal dataset design. Commonpool consists of 12.8 billion image-text pairs sourced from Common Crawl, with multiple scales derived by random subsampling, such as Commonpool-XL, -L, and -S. Filtering strategies played a significant role in dataset curation, focusing on removing NSFW content, deduplication, and face blurring to address safety and privacy concerns. DATACOMP benchmarks incorporate subsets like Datacomp-1B, where these filters were applied to create high-quality datasets from the Commonpool index. Content quality was assessed by leveraging metadata like CLIP similarity scores, enabling high zero-shot performance of models trained on these subsets on downstream tasks like ImageNet, often outperforming proprietary datasets like CLIP's WIT \cite{gadre2024datacomp}.

\subsubsection{WebLI} The WebLI dataset was introduced in the development of the PaLI model, with the aim of creating a high-volume, multilingual dataset to train large vision-language models effectively. It consists of 10 billion images with corresponding text in over 100 languages, collected from a range of public web sources. The multilingual nature of WebLI helps test and extend the model’s capabilities across diverse vision and language tasks beyond English-centric training data. This dataset underwent an extensive filtering process to handle noisy data from the internet while allowing a multilingual mix of image-text pairs, which contributed to state-of-the-art results in tasks such as captioning and visual question-answering \cite{chen2022pali}.

\subsubsection{MetaCLIP} MetaCLIP, or Metadata-Curated Language-Image Pre-training, leverages a metadata-driven curation approach to assemble high-quality image-text training datasets. It starts with a raw data pool from Common Crawl and balances the data distribution based on metadata derived from CLIP’s original curation concepts, ensuring a diverse yet informative subset of training pairs. The use of metadata to curate the dataset, rather than solely relying on black-box filtering, enables superior model training outcomes. MetaCLIP has demonstrated competitive performance in zero-shot classification, outperforming CLIP's original WIT dataset on ImageNet when trained on equivalent model architectures, such as ViT-B and ViT-G \cite{xu2023demystifying}.

\subsubsection{Data Filtering Networks (DFN)} The DFN project focused on developing neural networks to filter large-scale, uncurated datasets effectively. The DFN datasets, specifically DFN-2B and DFN-5B, are constructed from Commonpool, using data filtering networks to select high-quality image-text pairs. These networks were optimized for filtering rather than downstream task performance, demonstrating that models trained on filtered datasets could achieve state-of-the-art results on standard benchmarks. For example, DFN-5B enabled a ViT-H model to achieve an 84.4\% zero-shot accuracy on ImageNet, outperforming datasets like DataComp-1B and LAION-2B, thus highlighting the efficacy of neural network-driven filtering over traditional heuristic-based approaches \cite{fang2023data}.

\subsubsection{Merged2B} The Merged2B dataset, used in the EVA-CLIP project, is a carefully curated dataset combining multiple data sources to improve the efficiency and effectiveness of CLIP training. The dataset incorporates two billion image-text pairs from a mix of public sources, filtered to maximize informativeness while minimizing noise. Merged2B is used to train EVA-CLIP models, which include several techniques to reduce computational costs, such as initializing CLIP training with pre-trained EVA representations, and applying optimizations like flash attention. The resulting models, such as EVA-02-CLIP, achieved competitive zero-shot accuracy with significantly fewer training samples compared to models trained on other large datasets like LAION-2B \cite{sun2023eva}.

\subsubsection{Choice of Open-Clip Models}
We consider all pre-trained models available in OpenCLIP (open-clip-torch version 2.16.0) and supported by timm 0.6.12 as of August of 2024, when we performed our measurement. Some models from \citet{Cherti2023ReproducibleLearning} also appear in OpenCLIP, although the models are not labeled as such, so we filter out any models from OpenCLIP that output identical EAT effect sizes with a model from \citet{Cherti2023ReproducibleLearning}. We also remove two models that were trained on identical datasets, used identical architectures, and trained on identical numbers of samples (either measured in batch size or number of epochs, depending on what information was made available) as those given by \citet{Cherti2023ReproducibleLearning}. This leaves us with 131 models from the OpenCLIP repository. The CLIP and OpenCLIP repositories use MIT licenses.

\subsection{Data for measuring EATs}
\label{apd:eat_data}
\subsubsection{SEAT and iEAT tests and stimuli}

The specific SEAT and iEAT tests we replicate are presented in Figure \ref{fig:ieat-seat-tests}. For further details necessary for reproducibility, we refer readers to the respective original papers, which provide comprehensive descriptions of these tests and their methodologies.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{artifacts/ieat_seat_tests.png}
    \caption{List of Test Categories and Stimuli selected from \citet{May2019OnEncoders} and \citet{Steed2021}.}
    \label{fig:ieat-seat-tests}
\end{figure*}

\subsubsection{Open Affective Standardized Image Set (OASIS) Database}

The Open Affective Standardized Image Set (OASIS) \cite{kurdi2017introducing} contains 900 color images that depict a wide range of themes, including humans, animals, objects, and scenes. These images were rated on valence (positivity or negativity) and arousal (intensity of response) by 822 participants. In our experiments, the 900 images provide non-group-specific valence signals derived from naturalistic image content. All images were standardized to a resolution of 500 x 400 pixels using scaling and cropping processes similar to those described by \citet{wolfe2022evidence}. We selected the top 25 pleasant and top 25 unpleasant images based on valence ratings to construct the two attribute sets used in our analyses. A detailed analysis revealed the aggregate positive valence was equivalent in magnitude to the aggregate negative valence of the filtered images.

\subsection{Textual Templates and Lexica: NRC-VAD Lexicon Valence Stimuli}
Our study employs controlled and balanced textual datasets to investigate how psycholinguistic characteristics of language content influence biases in VLMs. We specifically examine the effects of words and phrases that have psycholinguistic ground-truth ratings in terms of valence. To achieve this, we adopt the sentence template approach used by \citet{May2019OnEncoders} incorporating 6 "semantically bleached templates." These templates are designed to be semantically neutral, providing a consistent syntactic frame for target words without introducing new semantic content. This method ensures that the psycho-semantic attributes of the target words influence the overall sentence meaning. For the experiments, we use the top 25 pleasant and top 25 unpleasant words, sorted by valence ratings, to form the two attribute sets and create the full sets of attributes by using each word in the 6 semantically bleached templates. A detailed analysis revealed the aggregate positive valence was equivalent in magnitude to the aggregate negative valence of the filtered words.

\begin{table*}[htbp]
\centering
\caption{Sentence templates from \citet{May2019OnEncoders}, valence words from \citet{mohammad2018obtaining} and valence images from \citet{kurdi2017introducing} used for the new controlled bias measurement in VLMs.}
\label{tab:templates-and-words}
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Type} & \textbf{Content} \\ \hline
Templates & ``This is the word [WORD]", ``That is the word [WORD]", ``There is the word [WORD]", ``Here is the word [WORD]", ``They are the word [WORD]", ``Those are the word [WORD]" \\ \hline
Positive Words & very positive, enjoyable, generous, happily, happy, love, magnificent, extremely positive, sweetie, passionate, cheerful, happier, feelgood, brotherhood, greatness, happiest, joyful, brilliance, smiling, friendliness, joys, laugh, hugs, awesome, superb \\ \hline
Negative Words & shit, nightmare, toxic, horrifying, murderer, homicide, afraid, mistreated, disheartening, angered, bankruptcy, pain, chaos, decayed, murderous, terrorist, cholera, deceit, suffocation, dangerous, shitload, homicidal, hell, genocide, misbehave \\ \hline
Positive Images & Dog 6, Lake 9, Lake 2, Lake 12, Beach 1, Beach 2, Lake 14, Dog 12, Fireworks 2, Rainbow 2, Lake 1, Lake 15, Rainbow 1, Cat 5, Penguins 2, Lake 8, Dog 4, Siblings 1, Dog 18, Baby 1, Lake 13, Fireworks 1, Lake 10, Baby 5, Sunset 3 \\ \hline
Race Words & Destruction 4, Explosion 5, Scary face 1, War 1, Fire 11, Fire 7, Fire 5, War 8, Severed finger 1, Garbage dump 4, Animal carcass 5, Dirt 1, Garbage dump 2, Fire 9, Tumor 1, Injury 4, War 6, KKK rally 1, Dead bodies 3, Dog 26, KKK rally 2, Dead bodies 2, Dead bodies 1, Dummy 1, Miserable pose 3 \\ \hline
\end{tabular}
\end{table*}

\section{Estimation of Mixed Effects Models}
\label{apd:mixed_effects}

The mixed effects model was chosen to explore the relationship between intrinsic bias (EAT effect size) and upstream factors while accounting for group-level variability across modality and test orders. Initial attempts to fit the model with individual dataset and architecture categories led to convergence issues due to high parameter complexity. To address this, we simplified the analysis by grouping datasets and architectures into broader families, resulting in a more stable and interpretable model. 

The final model included fixed effects for log-transformed parameters ($\log(\text{param})$), architecture family, dataset family, and log-transformed dataset size ($\log(\text{dataset size})$). Random effects were specified for group-level intercepts and slopes for $\log(\text{param})$ and $\log(\text{dataset size})$, capturing differences across modality and test order groups. Including random slopes significantly improved the model's log-likelihood, demonstrating the importance of accounting for the varying influence of parameters and dataset size across groups.

Preprocessing involved removing missing values, log-transforming continuous variables, and scaling using `StandardScaler' to improve numerical stability. Categorical variables were appropriately converted to categorical types for the model. The model was fitted using Restricted Maximum Likelihood (REML) with the `lbfgs' optimizer, achieving a log-likelihood of approximately -2173.32, indicating that the inclusion of random intercepts, slopes and a simplified dataset analysis contributed to better model fit and generalizability. Statistical significance was computed by comparing the Wald t-values to the z-distribution \citep{luke2017evaluating}. Figure \ref{fig:mixed_effects_model} contains the full results of the mixed effects model.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{artifacts/app_upstream_mixed_effects_camera_ready.png}
    \caption{Full mixed effects regression results to measure impact of upstream factors on intrinsic bias.}
    \label{fig:mixed_effects_model}
\end{figure*}


\section{Ablation Results}
\subsubsection{Baseline Results for Old Stimuli}
\label{apd:old_aggregate}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{artifacts/aggregate_effect_size_old_camera_ready.pdf}
    \caption{Aggregate Effect Size ($d$) with error bars representing standard deviation by Test Category Across Modality Orders from SEAT and iEAT stimuli. The direction of effect sizes are largely consistent with the effect sizes obtained from new stimuli in Figure \ref{fig:eat_aggregate}. }
    \label{fig:old_effect_size}
\end{figure}


\section{Implicit Association Test Effect Sizes}
\label{apd:iat}

\begin{table*}[]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Association Test} & \textbf{Mean IAT Effect Size}         & \textbf{\begin{tabular}[c]{@{}l@{}}EAT effect size \\ ranges across \\ modality \\ combinations\end{tabular}}
\\ \hline
Insect-Flower/Valence     & \begin{tabular}[c]{@{}l@{}}1.35 \\ \citep{Greenwald1998}\end{tabular}     & 1.341 ± 0.446 
\\ \hline
Instrument-Weapon/Valence     & \begin{tabular}[c]{@{}l@{}}1.66 \\ \citep{Greenwald1998}\end{tabular}     & 1.490 ± 0.390                                                                                                                                                                                                 \\ \hline
Race/Valence              & \begin{tabular}[c]{@{}l@{}}1.17 \\ \citep{Greenwald1998}\end{tabular}     & 0.248 ± 0.552                                                                                                 \\ \hline
Gender/Valence            & \begin{tabular}[c]{@{}l@{}}(0.02, 0.44) \\ \citep{baron2013}\end{tabular} & 0.361 ± 0.463                                                                                                \\ \hline
Age/Valence               & \begin{tabular}[c]{@{}l@{}}1.42 \\ \citep{Greenwald1998}
\end{tabular}     & 0.007 ± 0.743                                                                                                \\ \hline
\end{tabular}
\caption{Comparison of commonly reported effect sizes found when performing implicit association tests with human subjects vs. the mean and standard deviation of effect sizes observed across modalities for EATs targeting associations between the same concepts.}
\end{table*}









