@inproceedings{Berg2022ALearning,
    title = {{A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning}},
    year = {2022},
    booktitle = {Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    author = {Berg, Hugo and Hall, Siobhan and Bhalgat, Yash and Kirk, Hannah and Shtedritski, Aleksandar and Bain, Max},
    month = {11},
    pages = {806--822},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.aacl-main.61},
    address = {Online only}
}

@article{Caliskan2017SemanticsBiases,
    title = {{Semantics derived automatically from language corpora contain human-like biases}},
    year = {2017},
    journal = {Science},
    author = {Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
    number = {6334},
    pages = {183--186},
    volume = {356},
    url = {https://www.science.org/doi/abs/10.1126/science.aal4230},
    doi = {10.1126/science.aal4230}
}

@inproceedings{Guo2021DetectingBiases,
    title = {{Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases}},
    year = {2021},
    booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
    author = {Guo, Wei and Caliskan, Aylin},
    month = {7},
    pages = {122--133},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    isbn = {9781450384735},
    doi = {10.1145/3461702.3462536},
    arxivId = {2006.03955},
    keywords = {AI ethics, bias, intersectionality, language models, social psychology, word embeddings}
}

@inproceedings{Janghorbani2023Multi-ModalModels,
    title = {{Multi-Modal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Visionâ€“Language Models}},
    year = {2023},
    booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
    author = {Janghorbani, Sepehr and De Melo, Gerard},
    month = {5},
    pages = {1725--1735},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2023.eacl-main.126},
    address = {Dubrovnik, Croatia}
}

@inproceedings{May2019OnEncoders,
    title = {{On Measuring Social Biases in Sentence Encoders}},
    year = {2019},
    booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    author = {May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R. and Rudinger, Rachel},
    month = {3},
    pages = {622--628},
    publisher = {Association for Computational Linguistics},
    url = {http://arxiv.org/abs/1903.10561},
    address = {Minneapolis, Minnesota},
    doi = {10.18653/v1/N19-1063}
}

@misc{Radford2018ImprovingPre-Training,
    title = {{Improving Language Understanding by Generative Pre-Training}},
    year = {2018},
    author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya}
}

@inproceedings{Radford2021LearningSupervision,
    title = {{Learning Transferable Visual Models From Natural Language Supervision}},
    year = {2021},
    booktitle = {Proceedings of the 38th International Conference on Machine Learning},
    author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
    editor = {Meila, Marina and Zhang, Tong},
    month = {6},
    pages = {8748--8763},
    series = {Proceedings of Machine Learning Research},
    volume = {139},
    publisher = {PMLR},
    url = {https://proceedings.mlr.press/v139/radford21a.html}
}

@inproceedings{Steed2021,
    title = {{Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases}},
    year = {2021},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    author = {Steed, Ryan and Caliskan, Aylin},
    pages = {701--713},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    isbn = {9781450383097},
    doi = {10.1145/3442188.3445932},
    arxivId = {2010.15052},
    keywords = {Computer vision, Implicit bias, Unsupervised learning}
}

@inproceedings{Wolfe2022AmericanAI,
    title = {{American == White in Multimodal Language-and-Image AI}},
    year = {2022},
    booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
    author = {Wolfe, Robert and Caliskan, Aylin},
    pages = {800--812},
    series = {AIES '22},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3514094.3534136},
    address = {New York, NY, USA},
    isbn = {9781450392471},
    doi = {10.1145/3514094.3534136},
    keywords = {bias in ai, multimodal models, racial bias, visual semantics}
}

@inproceedings{Wolfe2022EvidenceAI,
    title = {{Evidence for Hypodescent in Visual Semantic AI}},
    year = {2022},
    booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
    author = {Wolfe, Robert and Banaji, Mahzarin R and Caliskan, Aylin},
    pages = {1293--1304},
    series = {FAccT '22},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3531146.3533185},
    address = {New York, NY, USA},
    isbn = {9781450393522},
    doi = {10.1145/3531146.3533185},
    keywords = {bias in AI, hypodescent, language-image models, multimodal, racial bias, visual semantics}
}

@inproceedings{Wolfe2022MarkednessAI,
    title = {{Markedness in Visual Semantic AI}},
    year = {2022},
    booktitle = {ACM International Conference Proceeding Series},
    author = {Wolfe, Robert and Caliskan, Aylin},
    month = {6},
    pages = {1269--1279},
    publisher = {Association for Computing Machinery},
    isbn = {9781450393522},
    doi = {10.1145/3531146.3533183},
    keywords = {age bias, bias in AI, language-and-vision AI, markedness, multimodal, visual semantics}
}

@inproceedings{Wolfe2023ContrastiveBias,
    title = {{Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias}},
    year = {2023},
    booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
    author = {Wolfe, Robert and Yang, Yiwei and Howe, Bill and Caliskan, Aylin},
    pages = {1174--1185},
    series = {FAccT '23},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3593013.3594072},
    address = {New York, NY, USA},
    isbn = {9798400701924},
    doi = {10.1145/3593013.3594072},
    keywords = {AI bias, AI bias in applications, AI bias propagation, gender bias, generative AI, language-vision AI, representation learning, sexualization, text-to-image generators}
}

@inproceedings{cabello-etal-2023-evaluating,
    title = "Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models",
    author = "Cabello, Laura  and
      Bugliarello, Emanuele  and
      Brandl, Stephanie  and
      Elliott, Desmond",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.525/",
    doi = "10.18653/v1/2023.emnlp-main.525",
    pages = "8465--8483",
    abstract = "Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task performance."
}

@article{greenwald1998measuring,
  title={Measuring individual differences in implicit cognition: the implicit association test.},
  author={Greenwald, Anthony G and McGhee, Debbie E and Schwartz, Jordan LK},
  journal={Journal of personality and social psychology},
  volume={74},
  number={6},
  pages={1464},
  year={1998},
  publisher={American Psychological Association}
}

@inproceedings{tan-bansal-2019-lxmert,
    title = "{LXMERT}: Learning Cross-Modality Encoder Representations from Transformers",
    author = "Tan, Hao  and
      Bansal, Mohit",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1514/",
    doi = "10.18653/v1/D19-1514",
    pages = "5100--5111",
    abstract = "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22{\%} absolute (54{\%} to 76{\%}). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: \url{https://github.com/airsplay/lxmert}"
}

