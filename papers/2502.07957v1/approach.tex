\section{Approach}
We describe the method we use for quantifying biases in CLIP models, as well as the regression models we use for exploring the relationship between biases, upstream pre-training factors, and downstream performance.

\noindent\textbf{Measuring Intrinsic Bias}
We measure intrinsic bias using EATs \cite{Caliskan2017SemanticsBiases, Guo2021DetectingBiases,Steed2021,Wolfe2022AmericanAI,Wolfe2023ContrastiveBias}, which provide a generalizable and principled method for quantifying biases related to a variety of concepts, such as race and gender, grounded in literature in cognitive and experimental psychology \cite{Blodgett2020LanguageNLP}. An EAT compares similarities between four sets of embeddings created by a model: Two sets of target embeddings which represent social groups, denoted $X$ and $Y$, and two sets of attribute embeddings which represent valence denoted $A$ and $B$ as described in Section \ref{sec:data}. Each EAT gives an effect size $d$, whose magnitude indicates the strength of the bias, calculated as follows: 


\[d=\frac{mean_{x \in X}s(x,A,B)-mean_{y \in Y}s(y,A,B)}{std\_dev_{w \in X \cup Y}s(w,A,B)}\]

\noindent where $s$ is given by:

\begin{equation*}
    \begin{split}
        s(w,A,B) =  mean_{a \in A}&{cos(w,a)} \\
        &- mean_{b \in B}{cos(w,b)}\\
    \end{split}
    % \label{eqn:s}
\end{equation*}

\noindent and $cos$ refers to cosine similarity, a distance metric used in quantifying associations by capturing information overlap between embeddings. We order the sets of stimuli such that a positive $d$ value indicates a bias that is congruent with a stereotype that has been documented in society (i.e. \textit{flowers}, \textit{instruments}, \textit{women}, \textit{European American}, and \textit{young} are more associated with \textit{pleasantness}). 

\noindent \textbf{Intrinsic Bias and Upstream Factors} To investigate the relationship between intrinsic bias, measured by the EAT effect size ($d$), and various upstream factors, we employ a mixed effects regression model. The upstream factors considered include the log of parameter size (\textit{log(param)}), model architecture (\textit{arch}), pre-training dataset (\textit{dataset}), and the log of dataset size (\textit{log(dataset size)}). The model is specified as follows:

{ \small \begin{equation*} \begin{split} {d}_{ij} = \beta_0 + \beta_1 \log({param})_{ij} \\ + \beta_2 {arch}_{ij} 
+ \beta_3 {dataset}_{ij} + \beta_4 \log({dataset size})_{ij} \\ + \ u{0j} + u_{1j} \log({param})_{ij} + u{2j} \log({dataset size}){ij} + \epsilon_{ij} \end{split} \end{equation*}}

where $i$ indexes individual observations and $j$ indexes groups defined by modality and test order combinations. Here, $\beta_0$ is the fixed intercept, while $\beta_1$ to $\beta_4$ are fixed coefficients for the predictors. The terms $u_{0j}$, $u_{1j}$ and $u{2j}$ represent random intercepts and slopes for \textit{log(param)} and \textit{log(dataset\_size)}, capturing group-specific baseline $d$ and variability in the effect of model size. The residual error is denoted by $\epsilon_{ij}$.

Significant fixed effects for upstream factors indicate their contribution to intrinsic bias.  For example, a significant positive $\beta_3$ suggests that models trained on certain pre-training datasets exhibit higher intrinsic bias. The inclusion of random effects allows the model to account for unobserved heterogeneity across different groups, thereby enhancing the accuracy and generalizability of the estimates. Reproducibility details are provided in Appendix \ref{apd:mixed_effects}.

\noindent\textbf{Intrinsic Bias and Downstream Performance}
We compute Pearson's correlation between intrinsic bias (EAT effect size $d$) and performance on the VTAB+ benchmark, considering zero-shot classification and captioning tasks relevant to each modality. Correlations are computed separately for each test category and modality combination in order to reveal modality-specific trends in the relationship between intrinsic bias and performance.


