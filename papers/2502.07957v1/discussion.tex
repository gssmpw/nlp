\section{Discussion}
%\textit{Paragraph summarizing study and key findings}
In this work, we explore how intrinsic biases and associations in VLMs are influenced by the interaction between modalities and upstream pretraining factors, and their impact on downstream task performance. Our findings highlight that the magnitude of bias effect sizes depends on the modality combination and test category being observed. However, across the board, the effect sizes are significantly influenced by dataset selection and correlate with model performance on downstream tasks. These insights highlight existing pitfalls in the data and training pipelines of VLMs with respect to fairness considerations and provide important implications for mitigating biases in the future development of these models.

\noindent\textbf{Bias in Cross-Modal Interactions} Our analysis reveals that intrinsic biases manifest differently across combinations of text and image modalities. The `Flower-Insect/Valence' and `Instrument-Weapon/Valence' tests show consistently high effect sizes across all modality combinations, indicating a strong association that is unaffected by modality. `Gender/Valence' biases show positive effect sizes in the \textit{All Image} setting, aligning with previous findings that women are associated with more positive terms than men \cite{caliskan2022gender, charlesworth2024extracting}. 

The representation of bias in models also varies substantially depending on the category of bias and the modality combination. Notably, for `Age/Valence', the direction of the effect sizes differs based on the modality. When analyzing images, older individuals are associated with more positive valence, whereas in text, younger individuals tend to be more positively associated. This discrepancy highlights a critical aspect of modality-specific bias propagation, where the amount and type of information conveyed through visual modalities can differ from that in textual modalities, leading to distinct biases. Such modality-dependent differences in the representation of `Age/Valence' suggest that crossmodal VLMs are influenced by biases specific to each modality. Text stimuli often use older names that are less frequent and therefore may not be represented accurately \cite{wilson2024gender}, while images can convey richer, more nuanced information about age, potentially leading to different bias patterns.


\noindent \textbf{Impacts of Upstream Factors} We focus on identifying which upstream factors—such as dataset characteristics and model design decisions—most significantly influence the intrinsic biases in CLIP models. By examining these biases from both unimodal and crossmodal perspectives, we aim to understand how different training inputs and model architectures contribute to intrinsic bias. This includes an in-depth analysis of how various datasets, including filtering strategies used to curate the selection of datasets, impact the emergence of biases. 


In our analysis presented in Figure \ref{fig:rq2}, we demonstrate that the choice of training dataset significantly impacts intrinsic bias, independent of other upstream factors such as model architecture or parameter count. We observe models curated with both automated (e.g. `dfn' \cite{fang_data_2023}) and heuristic filtering strategies (e.g., dataset versions of `commonpool' and `datacomp' \cite{gadre2024datacomp}) to ensure high data quality and subsequent high downstream performance on tasks such as ImageNet accuracy exhibited significantly higher levels of bias, which is likely due to lack of consideration for equitable group identity representation in the dataset curation process. 

Filtering methods that rely on automated neural network-driven decisions \cite{fang_data_2023}, while outperforming heuristic-based approaches in downstream tasks, tend to exacerbate societal biases even further. These results provide strong evidence that bias amplification often originates from the decisions made during the data curation phase, underscoring the need for more ethically-conscious dataset curation practices. 

Our findings align with suggestions from \citet{gadre2024datacomp} to exercise caution when using models trained on these datasets to actively make decisions that impact people. One potential avenue for dataset-related bias mitigation in CLIP models could be replacing names with a generic "person" token, like in CC 12M \cite{Changpinyo2021ConceptualConcepts} which removed some social group signals contained in the dataset. We hypothesize this may have contributed to lesser bias observed in Figure \ref{fig:rq2}, but the full impact of hypernymization is still unclear and left for future work.

Architecture choice was found to be less impactful compared to dataset selection, which aligns with expectations since most of the text and image encoders in our study were transformer-based, with a few cases being CNN-based image encoders. The synthetic processing in these models was not extensive enough to introduce significant additional bias amplification, and the parameter count remained within a reasonable range without incorporating components, unlike more complex architectures involved in applications like text-to-image generation. 

Dataset size was also not a significant contributor to bias, which contradicts the findings of \citet{Berg2022ALearning}. Our findings indicate that simply increasing the model size or the size of the training dataset does not inherently mitigate or exacerbate intrinsic bias. Instead, other factors such as the composition and characteristics of the dataset are more critical in determining the level of bias.


\noindent \textbf{Effects on Downstream Performance} Our investigation into the correlation between intrinsic biases and downstream task performance, as assessed by VTAB+, reveals significant modal dependencies. We demonstrate that higher intrinsic bias levels correlate with increased performance in downstream tasks across unimodal and crossmodal settings. 

The `Flower-Insect/Valence' and `Instrument-Weapon/Valence' bias shows a high positive correlation across modality combinations suggesting that biases linked to non-human concepts may benefit from consistent training signals, improving model performance and that some associations are universally amplified in conjunction with downstream task performance improvement. For `Gender/Valence' in the \textit{Image as Target} (-0.506 $r$) and  \textit{All Text} (-0.273 $r$) settings, we observed negative correlations, implying that the associations with positive valence increased for the stereotype incongruent `Men' group while model performance improves. This suggests that biases shift as models are further optimized, potentially reinforcing gender-specific stereotypes.

These findings indicate significant modal dependencies in how biases affect downstream task performance. The stark contrast between image-only and text-only settings, particularly test categories that involve social groups such as race and age, suggests that biases are not uniformly propagated across modalities but are instead highly dependent on the type of data and the specific tasks. 
