\section{Experiments and Results}

Following \citet{May2019OnEncoders} and \citet{Steed2021}, we compute the EAT effect sizes following the SEAT and iEAT methods across the 131 models and 26 tests for a total of 3,406 data points. Our analysis spans four modality combinations: \textit{All Text}, \textit{All Image}, \textit{Image as Target}, and \textit{Text as Target}, across various bias tests including  `Flower-Insect/Valence', `Instrument-Weapon/Valence', `Gender/Valence', `Race/Valence', and `Age/Valence' (as introduced in Section \ref{sec:data}). The EAT effect sizes computed using stimuli from the original SEAT and iEAT studies demonstrate a high overall variance of 0.62. In an effort to offer more control in our experiments thereby making the effect sizes more comparable across models and reducing the impact of outliers, we recompute them across using the newly controlled and grounded attribute stimuli for our tests, drawn from the NRC-VAD lexicon \cite{mohammad2018obtaining} and OASIS \cite{kurdi2017introducing} datasets. 

We observe reduced effect size variance across models and tests to 0.59 overall (a 4.8\% decrease) when replacing SEAT and iEAT attributes with our new attribute stimuli. The decrease in variance was particularly pronounced for the \textit{All Text} modality (a 33.96\% reduction in variance), which suggests that our set is less susceptible to noise and idiosyncrasies that may have plagued previous test sets. Furthermore, changing the stimuli shows better alignment with human stereotypes, showing a significant effect size ($d>=0.2$) in 70.23\% of the 3406 instances using the new stimuli, while this number is lesser at 67.88\% using the old stimuli. 

We note that even after using more controlled stimuli, a high aggregate variance is not surprising, due to the scale and nature of the study. Our experiments in subsequent sections investigate this variation from known dataset choice and model architectural sources. Instead, by employing these carefully curated and grounded stimuli, we gain a clearer lens through which to examine the underlying biases present in various models. Consequently, we present all further results using these new stimuli, providing more robust and generalizable insights into bias in VLMs.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{artifacts/aggregate_effect_size_camera_ready.pdf}
    \caption{Aggregate Effect Size ($d$) by Test Category Across Modality Orders (NRC-OASIS) along with error bars in black representing standard deviation.}
    \label{fig:eat_aggregate}
\end{figure}

\noindent\textbf{EATs as an Aggregate Measure of Bias} As shown in Figure \ref{fig:eat_aggregate}, 17 out of 20 EATs reveal a pattern of associations which aligns in directionality to results from Implicit Association Tests taken by humans (shown by a positive effect size).\footnote{The magnitude of IAT and EAT effect sizes is not directly comparable because the two tests differ in robustness and contextual dependency. In \ref{apd:iat}, we provide an overview of commonly reported effect sizes in IAT literature for the readers' convenience.} Only in the \textit{All Image} modality for `Age/Valence' and the \textit{Image as Target} for `Race/Valence' and `Age/Valence' are the effect sizes negative, representing associations opposite from those of humans. 

As with humans, `Flower-Insect/Valence,' and `Instrument-Weapon/Valence' show the largest effect sizes across modalities ($d>1$), and associations between valence and social groups are weaker but still present. In all cases, the magnitude of effect sizes varies depending on the modality. Our findings indicate that biases in CLIP models generally align with those found in human assessments in 78.86\% of the 3,406 cases (where $d>0$). In Appendix \ref{apd:old_aggregate}, we show that the direction of effect size across groups is consistent with the original SEAT and iEAT stimuli. 

Figure \ref{fig:eat_aggregate} also contains error bars that represent the standard deviation of effect sizes for the different test categories and modality combinations. We note that there is consistently lower variance for the non-human baselines, such as `Flower-Insect/Valence' and `Instrument-Weapon/Valence', compared to the variance observed in social bias categories like `Age/Valence' and `Race/Valence' indicating that these categories are inherently more susceptible to variability, likely due to the complexity and diversity of social concepts across different training datasets. 

\noindent\textbf{Relationship between Intrinsic Bias and Upstream Factors} We conducted a comprehensive mixed effects regression across the 3,406 observations within 16 different combinations of modality and EAT test category to understand how various upstream factors influence intrinsic bias, measured through the EAT effect size $d$. The model included random slopes and intercepts, effectively capturing high ($\beta=0.46$) group-level variability insights into both fixed and random effects across different combinations of modalities and test categories. For detailed and reproducible information regarding model specifications, variable definitions, and the experimental setup, see Appendix \ref{apd:mixed_effects}.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{artifacts/upstream_figure_10pc_sig_camera_ready.pdf}
    \caption{Fixed effects coefficients with 95\% confidence intervals for upstream factors affecting intrinsic bias. The plot illustrates the impact of dataset family, architecture family, dataset size, and model parameters, highlighting statistically significant predictors ($p < 0.01$) in \textbf{red}, while factors that are not significant are greyed.}
    \label{fig:rq2}
\end{figure}
%

As shown in Figure \ref{fig:rq2}, our findings reveal that dataset family plays a crucial role in determining the magnitude of intrinsic bias. Specifically, several dataset families, including `dfn' ($\beta_{3}=0.608$), `commonpool' ($\beta_{3}=0.399$), `merged2b' ($\beta_{3}=0.396$), `webli' ($\beta_{3}=0.387$), `datacomp' ($\beta_{3}=0.360$), `openai\_wit' ($\beta_{3}=0.351$), `laion' ($\beta_{3}=0.333$), and `metaclip' ($\beta_{3}=0.314$) showed significant positive associations ($p<0.01$) with intrinsic bias effect size with respect to the reference dataset of `CC12m', chosen because we hypothesized its curation strategy would lead to the lowest levels of intrinsic bias. Marginal associations observed in `yfcc15m' and `CC12m' suggests that training on certain datasets contributes more to bias compared to others. This highlights the substantial influence of pretraining data on the biases present in the models.

In contrast, variations in model architecture (although having a positive direction of influence) had no statistically significant impact on intrinsic bias. None of the architectural families demonstrated a significant impact on effect size of bias compared to the reference category, suggesting that, at least within the scope of our study, architectural differences do not play a primary role in influencing bias. Additionally, `log\_params' and `log\_dataset\_size'â€”did not exhibit significant effects on effect size of bias either. 



\noindent\textbf{Relationship between Intrinsic Bias and Downstream Performance}

We investigated the relationship between intrinsic biases measured in CLIP models and their performance on downstream tasks using the VTAB+ benchmark \cite{Schuhmann2022LAION-5B:Models} to understand how intrinsic biases in the models relate to their downstream performance across different modality combinations. Previous research has suggested that biases can influence model performance, particularly as models optimized for accuracy tend to learn and amplify societal biases \cite{hall2022systematic}.

Among the different test categories and modality combinations, we observed positive associations between intrinsic bias and downstream performance (meaning increased bias correlates to improved performance) for the non-human categories of `Flower-Insect/Valence' and `Instrument-Weapon/Valence' for   \textit{All Image} ($r=0.55, 0.81$), \textit{All Text} ($r=0.59, 0.71 $), \textit{Image as Target} ($r=0.69, 0.75$), and \textit{Text as Target} ($r=0.44, 0.82$) in addition to the human category `Race/Valence' in the \textit{All Image} combination ($r=0.35$). These are shown in Figure \ref{fig:eat-correlations}. We found negative correlations (meaning increased intrinsic bias correlates to worse performance) for the `Gender/Valence' for \textit{Image as Target} ($r=-0.51$) and \textit{All Text} ($r=-0.27$).  Insignificant correlations were observed primarily for `Race/Valence' and `Age/Valence' in various modalities and `Gender/Valence' in the \textit{Text as Target} modality.



\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{artifacts/correlation_plot_camera_ready.pdf}
    \caption{Measure of Pearson's correlation $r$ between effect size magnitude and downstream VTAB+ performance across test categories and modality combinations. Significant values are marked with an asterisk.}
    \label{fig:eat-correlations}
\end{figure*}





