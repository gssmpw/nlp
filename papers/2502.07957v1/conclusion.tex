\section{Conclusion}

In this work, we conducted the largest analysis to date on the biases in vision-language models, examining 131 unique CLIP models across 26 datasets and 55 architectures. Our study highlights that the choice of dataset during pre-training, particularly those curated using automatic and heuristic-based filtering approaches that optimise downstream VLM performance, significantly influences intrinsic bias, reinforcing existing disparities. Additionally, we found that biases in models often correlate with improved downstream task performance, across modality settings, suggesting that the possibility that performance optimization can inadvertently amplify certain intrinsic biases as VLMs learn stronger associations between concepts. These findings emphasize the need for more ethically informed dataset curation and bias mitigation strategies to ensure fairer AI models. We release
our code and data at \url{https://github.com/kshitishghate/CLIP_bias}. 