\section{Background and Related Work}
\label{sec:rw}
We now introduce background information on CLIP models and methods for measuring intrinsic bias in these models.

\noindent\textbf{Unimodal Embedding Association Tests}
\citet{Caliskan2017SemanticsBiases} introduced EATs to measure the associations between static word embeddings which encode concepts related to targets (typically social groups) and attributes, similar to Implicit Association Tests (IATs) for human associations \citep{greenwald1998measuring}. As contextual word embeddings replaced static word embeddings, alternate methods to measure intrinsic bias and associations were also developed \citep{Guo2021DetectingBiases, May2019OnEncoders}. The Sentence Encoder Association Test (SEAT) introduced by \citet{May2019OnEncoders} measures intrinsic bias by operating over a set of  target sentences and attribute sentences which are semantically bleached excluding the words which represent the concepts of interest.  

Additionally, EATs have also been developed for modalities other than text, including vision. \citet{Steed2021} introduce the Image Embedding Association Test (iEAT), which is similar to static word embedding EATs except that it operates over embeddings which represent single images rather than words. EATs in both the textual and visual domains have been shown to replicate biases which are observed in humans \cite{Caliskan2017SemanticsBiases, Steed2021}, making them valuable tools for investigating learned associations in both unimodal and cross-modal models. 

\noindent\textbf{Contrastive Language-Image Pre-training} 
CLIP models are some of the most widely used vision-language models due to their success in zero-shot classification tasks \citep{Radford2021LearningSupervision} as well as their usage as components in popular text-to-image generation systems such as DALL-E and Stable Diffusion. CLIP models have separate image and text encoders, which are connected in a joint cross-modal embedding space \citep{Radford2018ImprovingPre-Training}.  During pre-training, datasets of image-caption pairs comprising hundreds of millions of observations are fed into the models. The model's objective maximizes the cosine similarity between an image and its paired caption, while minimizing the cosine similarity between the image and all other captions in the pre-training batch. Several variations on the original model architecture and training dataset have been proposed to improve CLIP; see Section~\ref{sec:data}.


\noindent\textbf{Biases in Vision Language Models}
\citet{cabello-etal-2023-evaluating} investigate the mechanisms of gender bias amplification in pre-training and fine-tuning stages with vision-language models based on the LXMERT architecture \cite{tan-bansal-2019-lxmert}. This work builds upon theirs by investigating intrinsic biases within models based on the CLIP architecture, which is more commonly used, and through non-human, race, and age bias tests as well as gender bias. Additionally, our method does not rely on having access to training corpora or curating lexicons, making it more flexible to apply to a wider variety of models and biases.

Most work investigating biases in CLIP models does so only for individual models. For example \citet{Janghorbani2023Multi-ModalModels} study a  CLIP model and find that it tends to associate images representing homosexuality with text such as ``offending'' and ``vulgar,'' while for heterosexual images with words such ``blissful'' and ``awesome,'' among other associations. Further biases in CLIP's embedding space related to race in 3 CLIP models \cite{Wolfe2022MarkednessAI,Wolfe2022EvidenceAI,Wolfe2022AmericanAI}, and gender in 9 CLIP models \cite{Wolfe2023ContrastiveBias}.  

The only comparative approach that studies bias in CLIP models to our knowledge \citet{Berg2022ALearning} compares gender bias across 9 CLIP models. They find that larger pre-training datasets tend to lead to decreased bias and hence associated with better zero-shot classification performance, they examine only a small set of biases, model architectures, and training datasets. It is not clear from this work the extent to which these trends would extend to other biases or to other CLIP models.


