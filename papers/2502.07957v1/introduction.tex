\section{Introduction}
Neural network models are prone to learning patterns based on statistical associations between concepts within their training data that might lead to harmful bias when it relates to social groups or model performance \citep{fabbrizzi2022survey}. This phenomenon has been observed in a number of vision and language models, each of which are unimodal and learn information within a single modality \citep{Caliskan2017SemanticsBiases, Guo2021DetectingBiases, Steed2021, Wolfe2022VAST:Models, omrani2023evaluating}. Cross-modal models, which learn information from both vision and language modalities, also learn biased information relating to social group associations \cite{Goh2021MultimodalNetworks,Wolfe2022AmericanAI,  Wolfe2022EvidenceAI,
Wolfe2022MarkednessAI, Wolfe2023ContrastiveBias, Janghorbani2023Multi-ModalModels, Berg2022ALearning, mandal_multimodal_2023, hall_visogender_2023}.

These results have largely been found using intrinsic bias tests adapted from Natural Language Processing (NLP): evaluations that compare relative distances between a model's representations of stimuli representing different concepts and social groups. Despite their prevalence in model evaluation, however, there is limited work connecting them to other factors of model design and optimization. For example,  upstream factors such as training datasets, model architectures, and model sizes directly determine the representations learned and consequently may be reflected in intrinsic bias tests. These representations are then directly used for downstream tasks such as zero-shot image classification, which suggests a potential connection between the intrinsic bias of a model and its performance on downstream tasks. By investigating intrinsic bias as it explicitly relates to these two upstream pre-training and downstream zero-shot performance factors, we are able to draw novel insights about the ways in which models can be optimized to reduce harmful or undesirable biases.

We measure the associations between social groups and valence, the pleasantness or unpleasantness of a concept \citep{toney2020valnorm}. Valence is a robust dimension of human cognition as it relates to shaping attitudes and biases \citep{harmon2013does}. Social groups and valence associations also exist in unimodal models \citep{ Wolfe2022VAST:Models}. In this work, we examine whether these associations are observed cross-modally and how they relate to upstream factors and downstream model performance. 

To our knowledge, our work is the first which connects 26 tests of intrinsic bias, including those related to race, gender, age, and baseline associations with respect to non-social group concepts such as flowers and insects, to upstream factors of model training (including 26 training datasets, 55 model architectures, and model sizes ranging between 100 million and 5 billion parameters). While variations on all of these features have been proposed to either mitigate biases or improve performance on downstream tasks, we are the first to address the variance in the magnitude of the effect of these features in CLIP models. Additionally, we connect intrinsic bias tests to a suite of 35 zero-shot image classification and retrieval tasks \cite{Schuhmann2022LAION-5B:Models}. A novel contribution of our work is that we show that optimizing for performance is not sufficient to mitigate intrinsic biases. The scale of our experiments allows us to obtain high statistical power in analyzing the relationship between intrinsic bias and both upstream factors and downstream performance and making the following generalizable knowledge contributions about cross-modal models\footnote{We release our code and data at \url{https://github.com/kshitishghate/CLIP\_bias/}.}:

\textbf{1}. By improving the application of EATs via controlling the valence of images and text used in EATs, we decreased variance in effect size of bias on average by 4.8\% across unimodal and cross-modal tests, and demonstrated significant intrinsic bias in 131 models across 26 EATs. Aggregate intrinsic bias is consistent with human associations in 78.86\% of the 3,406 cases and varies by modality combination and test categories. 
    
\textbf{2}. We demonstrate that the choice of training dataset significantly impacts intrinsic bias, independent of other upstream factors such as model architecture or parameter count. Notably, while current dataset filtering techniques \cite{gadre2024datacomp, fang_data_2023, xu_demystifying_2023} have been successful in optimizing performance metrics like ImageNet classification accuracy, they fall short in addressing fairness. Moreover, filtering methods driven by automated neural network decisions \cite{fang_data_2023}, despite yielding better downstream results compared to heuristic-based approaches \cite{gadre2024datacomp}, tend to exacerbate societal biases even further (e.g. with a $\beta=0.608$ over baselines). Our findings provide strong evidence that bias amplification often originates from choices made during the upstream data curation process.

\textbf{3}. We show that intrinsic bias measures are correlated with downstream performance. Across modality settings, higher intrinsic bias often correlates with improved performance for non-human associations as seen with `Flower-Insect/Valence' (aggregate $r=0.56$) and `Instrument-Weapon/Valence' (aggregate $r=0.78$), suggesting consistent training signals may amplify certain associations. For `Gender/Valence' tests ($r=-0.51$, $r=-0.27$ in two modality settings), improved performance increased positive associations for men, indicating non-congruent emergent stereotypes. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{artifacts/figure_1_clip_bias.pdf}
    \caption{We use Embedding Association Tests, bias evaluation methods for representational or generative models, to quantify biases in 131 CLIP models. Images shown are a subset of the stimuli used to represent the concepts in the Image Embedding Association Test \cite{Steed2021} and our controlled attribute stimuli taken from \citet{kurdi2017introducing}. Distances shown illustrate a stereotype congruent bias (similar to that found in humans), where images representing the concept \textit{Flower} are closer to images representing the concept \textit{Pleasant}, and images representing the concept \textit{Insect} are closer to images representing the concept \textit{Unpleasant}.}
    \label{fig:eat-graphic}
\end{figure}





