\section{Experimental Setup }
\label{sec:data}

We now describe the experimental setup and data for upstream pre-training factors, intrinsic bias, and downstream performance.


\noindent\textbf{CLIP Models and Upstream Factors}
The models we study include the original nine CLIP models released by OpenAI \cite{Radford2021LearningSupervision}; 29 models introduced by \citet{Cherti2023ReproducibleLearning}, which were pre-trained on variable-sized English-only subsets of the LAION 5B dataset \cite{Schuhmann2022LAION-5B:Models}; and 93 additional models from the OpenCLIP project \cite{Ilharco2021OpenCLIP}.

The 131 models studied range in size from 102 million to 5 billion parameters, and all use transformer-based text encoders. While most use transformer-based image encoders as well, 17 of the 131 models we study use ResNets or ConvNeXts, convolutional architectures. The pre-training factors that we consider are the size of the model (measured in the number of parameters), model architecture, pre-training dataset, and size of the pretraining dataset (measured in the number of samples). Model architecture has been tied to bias in other modalities \cite{Ladhak2023WhenSummarization}, and model size and pre-training dataset have been tied to bias in nine CLIP models by \citet{Berg2022ALearning}. 


The datasets used for pre-training these CLIP models consist of image-caption pairs sourced from the Internet and created under varying levels of supervised curation. For example, the OpenAI WebImageText dataset \cite{Radford2021LearningSupervision} includes pairs whose text contains an element from a set of pre-defined phrases, while the LAION 5B dataset \cite{Schuhmann2022LAION-5B:Models} was filtered to remove images suspected of containing illegal content. Some datasets, such as CC 12M, are revised even further to remove or mentions of social groups or names in order to minimize biases which can be learned from the data. The datasets range in size from 12 million to 5 billion pairs; further statistics for pre-training datasets and architectures are available in the Appendix section \ref{sec:apd_data}.

\noindent \textbf{Embedding Association Test Stimuli}
To measure bias in CLIP models, we use a controlled approach concerning a broad set of concepts across both language and vision modalities. We consider five sets of association tests: non-human (flowers-insects; instruments-weapons), race (European American-African American), gender (women-men), and age (young-old). For all of these target categories, the attribute categories are positively or negatively valenced concepts, due to their strong associations with social groups both in human cognition and unimodal models \citep{toney2020valnorm, harmon2013does}. For each domain, the expected outcome is that the first group is more positively valenced than the second. 

We use the EATs introduced in \citet{Steed2021} and \citet{May2019OnEncoders} for vision (iEAT) and language (SEAT) modalities, respectively, in order to test associations between groups and valence. In SEAT, the human groups are represented both with names and highly associated words in individual tests, meaning there are seven textual EATs and four image EATs. We want to note that the `Gender/Valence' category was not tested in the original SEAT and iEAT studies. We follow previous work \cite{caliskan2022gender, charlesworth2024extracting} that establishes women as being more associated with positive valence compared to men, and thus consider women to be the first group in the gender comparison, to represent the stereotype-congruent direction in our analysis.  Additionally, the `instruments' group was not included in the original iEAT study. Following the text stimuli from \citet{May2019OnEncoders} we carefully curated new image stimuli that satisfy the iEAT requirements. 

Furthermore, we introduce a variation in the iEAT and SEAT attribute stimuli in order to use text and images which are more principled and grounded. Specifically, we use new image stimuli from the OASIS dataset \cite{kurdi2017introducing} and new text stimuli from the NRC-VAD lexicon \cite{mohammad2018obtaining} which contain images and words/phrases respectively that are rated and validated by humans and offer more control and human-grounded valence inputs. Figure \ref{fig:eat-graphic} contains a visualisation of the non-human category EAT using our new stimuli. Further details of these stimuli and how they are selected are provided in the Appendix \ref{apd:eat_data}.


EATs are computed across all modality combinations. iEAT consists only of image targets and image attributes, while SEAT consists only of text targets and text attributes. To perform the cross-modal analysis, we combine image and text stimuli, resulting in additional combinations: images as a target with textual attributes, and text as a target with image attributes. Biases are thus computed for four modality combinations: five \textit{All Image}, eight \textit{All Text}, five \textit{Image as Target}, and eight \textit{Text as Target}, totaling 26 tests.


\noindent\textbf{Downstream Performance (VTAB+)}
Because work from \citet{Berg2022ALearning} found an association between zero-shot performance and bias in nine CLIP models, we also test the relationship between performance and bias for models that have performance data available. We employ performance measured on VTAB+ \cite{Schuhmann2022LAION-5B:Models}, a suite of 35 image classification and retrieval tasks, which includes broad sets of images such as ImageNet \cite{Deng2009Imagenet:Database}, sets of natural images captured with standard or specialized equipment such as Caltech-101 \cite{Li2022Caltech101} or Diabetic Retinopathy \cite{Gulshan2016DevelopmentPhotographs}, as well as structural images, such as SmallNORB \cite{LeCun2004LearningLighting}. 