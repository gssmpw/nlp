\section{Related Work}
\paragraph{Deductive Reasoning with Search Algorithms}
LLMs have demonstrated remarkable potential in tackling complex reasoning tasks~\citep{achiam2023gpt,touvron2023llama,jiang2023mistral}.
However, they frequently fail on even simple reasoning steps, significantly degrading performance.
To mitigate this, researchers have proposed inference-time scaling methods that enhance computational effort during the reasoning process before finalizing answers.
Early approaches, such as Self-Consistency~\citep{wang2022self}, scaled computations by sampling multiple solutions.
This approach is inefficient because it requires exploring full solution paths, even if a mistake has occurred early on.
Later work introduced advanced tree search algorithms, including Beam Search~\citep{yao2024tree,zhu2024deductive,yu2024ovm}, MCTS~\citep{tian2024toward,zhang2024accessing}, and A*~\citep{wang2024q,wang2024litesearch}, to strategically allocate computational resources to critical reasoning steps, thereby improving efficiency.

Despite these advancements, most existing methods fail to address the challenges of over-exploration and under-exploration, leading to suboptimal efficiency.
To our knowledge, only \citet{tian2024toward} explicitly tackle over-exploration by pruning repeated reasoning states. However, their solution relies on either edit distance, which is prone to robustness issues (Table \ref{tab:human_eval}), or LLM-based similarity checks, which incur significant computational overhead.
This underscores the absence of a lightweight and robust mechanism to address this problem.

\iffalse
In recent years, Large Language Models (LLMs) have shown remarkable potential in addressing complex reasoning tasks \citep{achiam2023gpt,touvron2023llama,jiang2023mistral}.
Early studies \citep{kojima2022large,wang2022self} concentrated on utilizing the policy model itself, eliciting improved reasoning trajectories through chain-of-thought (CoT) prompting.
\citet{wang2022self} proposed Self-Consistency to perform majority voting on diverse sampled trajectories, demonstrating significant improvement.

Due to the considerable challenge posed by the high sensitivity to individual mistakes in mathematical reasoning \citep{shen2021generate}, later research explored training verifiers at the solution-level or step-level and integrated tree search algorithms \citep{yao2024tree,kang2024mindstar} to simulate the slow thinking process of System 2 \citep{kahneman2011thinking}.
Step-level beam search \citep{zhu2024deductive,yu2024ovm} was first investigated, maintaining a beam of trajectories and effectively pruning poor-quality branches.
Inspired by the success of sophisticated Monte Carlo Tree Search (MCTS) on AlphaGo \citep{silver2016mastering}, some studies \citep{tian2024toward,zhang2024accessing} also demonstrated its effectiveness in LLM reasoning tasks.
However, this method is computationally expensive due to its simulation steps and emphasis on search diversity.
Therefore, \citet{wang2024litesearch} proposed Tree Search, which dynamically assigns expansion budgets, improving efficiency with minimal impact on performance.

Nevertheless, most studies overlook the issue of repeated search states, resulting in inefficient exploration.
To the best of our knowledge, only \citet{tian2024toward} considered pruning repeat states to enhance efficiency.
However, they either employed edit distance, which lacks robustness (as shown in Table \ref{tab:human_eval}), or utilized an LLM to identify similar states, an approach that is inefficient.
In contrast, this work conducts a more practical and comprehensive study of state merging to address this problem.

% We have demonstrated that edit distance is not robust and significantly worse than our embedding models. Furthermore, employing LLMs to assert pairwise similarity is costly and can significantly decrease efficiency.
\fi

\paragraph{Training Verfiers to Guide Searching}
Previous studies have mainly utilized two types of verifiers: value networks \citep{yu2024ovm,tian2024toward} and process reward models (PRMs, \citealt{lightman2023let,wang2023math}).
Typically, both approaches leverage LLMs augmented with classification or regression heads to produce scalar scores to guide searching.

To collect training labels, \citet{lightman2023let} employed human annotators to identify reasoning errors in intermediate steps.
However, this approach is cost-prohibitive, hindering the scalability of training datasets.
Therefore, subsequent research \citep{yu2024ovm,wang2023math,tian2024toward,wang2024litesearch} has instead adopted MC methods to automate label collection.
While effective, they overlook the high variance associated with MC estimation, which influences the stability of trained verifiers and can lead to under-exploration.
% a well-documented challenge in reinforcement learning~\citep{schulman2015high}.
In this study, we are the first to incorporate TD($\lambda$) and verifier ensembling to address this concern.

% The value network aims to approximate the expected reward a policy can obtain from a specific state.
% In contrast, PRMs focus on evaluating the decision-making process rather than the cumulative return from the future. 
% Both models have demonstrated their effectiveness in guiding tree search.