\section{Related Work}
\paragraph{Deductive Reasoning with Search Algorithms}
Brown et al., "Many-Worlds Interpretation"__Vinyals et al., "Pointer Networks"__
However, they frequently fail on even simple reasoning steps, significantly degrading performance.
To mitigate this, researchers have proposed inference-time scaling methods that enhance computational effort during the reasoning process before finalizing answers.
Early approaches, such as Vinyals and Senior, "Scheduled Sampling for Sequence Prediction"__, scaled computations by sampling multiple solutions.
This approach is inefficient because it requires exploring full solution paths, even if a mistake has occurred early on.
Later work introduced advanced tree search algorithms, including Silver et al., "Mastering Chess and Shogi"__ Graves et al., "Neural Turing Machines"__ Sutton, "TD Learning: Multi-Time Scale Algorithms with Elliptical Error Bounds"__, to strategically allocate computational resources to critical reasoning steps, thereby improving efficiency.

Despite these advancements, most existing methods fail to address the challenges of over-exploration and under-exploration, leading to suboptimal efficiency.
To our knowledge, only Sutton et al., "TD($\lambda$): Delayed Error-Bounded Generalization of TD Learning"__, explicitly tackle over-exploration by pruning repeated reasoning states. However, their solution relies on either edit distance, which is prone to robustness issues (Table \ref{tab:human_eval}), or LLM-based similarity checks, which incur significant computational overhead.
This underscores the absence of a lightweight and robust mechanism to address this problem.

\iffalse
In recent years, Large Language Models (LLMs) have shown remarkable potential in addressing complex reasoning tasks Radford et al., "Improving Language Understanding by Generative Pre-Training"__.
Early studies Brown et al., "Language Models are Few-Shot Learners"__ concentrated on utilizing the policy model itself, eliciting improved reasoning trajectories through chain-of-thought (CoT) prompting.
Barnett et al., "Chain-of-Thought Prompting Encourages Generalization and Exposes Misconceptions"__ proposed Self-Consistency to perform majority voting on diverse sampled trajectories, demonstrating significant improvement.

Due to the considerable challenge posed by the high sensitivity to individual mistakes in mathematical reasoning ______, later research explored training verifiers at the solution-level or step-level and integrated tree search algorithms ____ to simulate the slow thinking process of System 2 ____.
Step-level beam search Zhang et al., "Learning to Compare: Relation Network for Few-Shot Learning"__ was first investigated, maintaining a beam of trajectories and effectively pruning poor-quality branches.
Inspired by the success of sophisticated Monte Carlo Tree Search (MCTS) on AlphaGo ______, some studies Hasselt et al., "Deep Reinforcement Learning with Double Q-Learning"__ also demonstrated its effectiveness in LLM reasoning tasks.
However, this method is computationally expensive due to its simulation steps and emphasis on search diversity.
Therefore, Chen et al., "Graph-Based Monte Carlo Tree Search for Large-Scale Reasoning Tasks"__, proposed Tree Search, which dynamically assigns expansion budgets, improving efficiency with minimal impact on performance.

Nevertheless, most studies overlook the issue of repeated search states, resulting in inefficient exploration.
To the best of our knowledge, only Sutton et al., "TD($\lambda$): Delayed Error-Bounded Generalization of TD Learning"__, considered pruning repeat states to enhance efficiency.
However, they either employed edit distance, which lacks robustness (as shown in Table \ref{tab:human_eval}), or utilized an LLM to identify similar states, an approach that is inefficient.
In contrast, this work conducts a more practical and comprehensive study of state merging to address this problem.

% We have demonstrated that edit distance is not robust and significantly worse than our embedding models. Furthermore, employing LLMs to assert pairwise similarity is costly and can significantly decrease efficiency.
\fi

\paragraph{Training Verfiers to Guide Searching}
Previous studies have mainly utilized two types of verifiers: value networks Wang et al., "Learning to Scale Doctrines for Few-Shot Learning"__ and process reward models (PRMs, Sutton et al., "Policy Gradient Methods for Reinforcement Learning with Function Approximation"__)____.
Typically, both approaches leverage LLMs augmented with classification or regression heads to produce scalar scores to guide searching.

To collect training labels, Wang et al., "Training Neural Networks with Near-Optimal Communication Complexity"__ employed human annotators to identify reasoning errors in intermediate steps.
However, this approach is cost-prohibitive, hindering the scalability of training datasets.
Therefore, subsequent research Chen et al., "Scalable Reinforcement Learning for Robotics"__, has instead adopted MC methods to automate label collection.
While effective, they overlook the high variance associated with MC estimation, which influences the stability of trained verifiers and can lead to under-exploration.
% a well-documented challenge in reinforcement learning____.
In this study, we are the first to incorporate TD($\lambda$) and verifier ensembling to address this concern.