\section{Method}

\subsection{Problem Statement}
Our problem setting involves a natural language task instruction $\task$ and a scene graph $\gG = (\gV, \gE)$, where $\gV$ and $\gE$ denote vertices and edges, respectively. Each node $\gV_i$ represents an object along with its attributes, such as coordinates or colors, while each edge indicates a type of spatial relationship, such as inside or on top of. Additionally, we assume access to the \textit{scene graph schema} $\gs$, which is a textual description of types, formats, and the semantics of the graph vertices and edges. Our objective is to generate the solution of $\task$ using LLMs, based on the available information above, expressed as $\sol = f(\task, \gG, \gs; LLMs)$.

%%%%% figure %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
 
  % \vspace*{-0.1in}
  \centering
  \scalebox{0.92}{
    \begin{tikzpicture}
     \node[anchor=north west] at (0in,0in)
      {{\includegraphics[width=1.0\textwidth,clip=true,trim=0
      0pt 0 0]{figs/method.pdf}}};
%     \node[yshift=-0pt,anchor=north west] at (0.1in,0.0in) {\bf \small (a)};
%     \node[anchor=north west] at (0.92in,-0.05in) {\textbf{(a)}};
%   \node[anchor=north west] at (2.00in,-0.05in) {\textbf{(b)}};
%     \node[anchor=north west] at (3.09in,-0.05in) {\textbf{(c)}};
    \end{tikzpicture}
  }
  \vspace*{-0.12in}
  \caption{\textbf{\RwR Workflow}.
  It solves tasks on scene graphs through the cooperation of two LLM agents: Reasoner and Retriever. Reasoner iteratively queries Retriever for graph information and reasons based on the received data from the Retriever. 
  The scene graph schema is prompted to synergize the reasoning and retrieval.
  Additionally, both agents employ the code-writing skill: Retriever programs to retrieve graph information based on the schema, while the Reasoner writes code to utilize external tools for solving complex atomic problems. In the graph, 
  \protect{\raisebox{-.05cm}{\includegraphics[height=.30cm]{figs/code.png}}} and 
  \protect{\raisebox{-.05cm}{\includegraphics[height=.30cm]{figs/code_exe.png}}}
  represent code writing and execution, respectively.
  They retrieve graph information $\bm{\gG}^\prime$ or enhance the analysis $\bm{\anly}$.
  }
  \vspace*{-0.15in}
 \label{fig:Method}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Overview of \RwR}
%TODO: remove repeated statements and add two-agent framework formula.
We explore grounding the reasoning process to scene graphs based on the scene graph schema $\gs$ and the code-writing ability of LLMs.
We develop \RwR, a two-agent framework that iteratively reasons through the next steps and retrieves necessary information from the graph.
As shown in Figure \ref{fig:Method}, our method contains two LLM agents: a \textit{Reasoner} and a \textit{Retriever}. 
The system initializes with the Scene Graph Schema, the Environment Description, general Guidance to direct the cooperation process, and task-dependent information such as the description of Agent Actions and Reasoning Tools. 
Given a task, the Reasoner determines the next substep to approach the task and identifies necessary scene graph information. It then raises a natural language query to the Retriever for this information. Upon receiving the query, the Retriever processes the scene graph through code-writing and sends the data back to Reasoner. By iteratively performing these steps, both agents collaborate to solve the task. 
Formally, at each time step $t$:
\begin{align}
    \anly_t, \query_t &= Reasoner(\{\anly_0, \query_0, \gG^\prime_0\}, \{\anly_{1}, \query_{1}, \gG^\prime_{1}\}, \cdots; \gs) \\
    \code_t &= Retriever(\query_t; \gs) \\
    \gG^\prime_{t} &= \code_t(\gG)
\end{align}
where $\anly$ denotes the current analysis by the Reasoner; $\query$ denotes natural language query for the graph information; $\code$ denotes the retrieval code following the query; and $\gG^\prime$ denotes the retrieved information by executing the code on the scene graph $\gG$.

Importantly, unlike previous iterative methods \cite{react, iterRG} that uses a single LLM to process the entire history, the two agents in \RwR only exchange the query $\query$ and the corresponding graph data $\gG^\prime$, excluding the underlying thought process, such as $\anly$ and $\code$. 
As we will show, this agent-level context filtering, enabled by our two-agent design, is critical for eliminating the interference from irrelevant conversation history, thereby ensuring a seamless and automated cooperative task-solving process.

% including the {\em Explanation} of intermediate conclusions in language and the {\em Reasoning Code} for tool using or sub-task solving;

% Our system initializes with the Scene Graph Schema, the Environment Description, general Guidance to direct the cooperation process, and task-dependent information such as the description of Agent Actions and Reasoning Tools. Then, given the Task, the Reasoner outputs analysis in natural language labeled as {\em Explanation}, and {\em Query} the Retriever. In turn, given the Scene Graph and a Query, the Retriever provides structured responses grounded in the Scene Graph. This process iterates until the Reasoner outputs a plan.

% The next subsections explain workflows of each agent, as well as techniques that ensure a fluent and automated task-solving process.

\subsection{Reasoner}

Reasoner is the central agent steering the task-solving iterations. We prompt it with the schema $\gs$, environment and task information (such as action description for the planning task), annotations of reasoning tools, general guidance to ensure automated task-solving conversation, and optionally, few-shot task-level examples. Reasoner then initiates the conversation with Retriever to solve a given task.

Concretely, without any knowledge about the graph data initially, the Reasoner analyzes only the task $\task$ and graph schema $\gs$, generates the first analysis $\anly_0$, and sends out the first associated query $\query_0$ to the Retriever. At the $t^{th}$ round of conversation, the Reasoner consumes past analyses, queries, and retrieved information: $\{(\anly_0, \query_0, \gG^\prime_0), \cdots, (\anly_{t-1}, \query_{t-1}, \gG^\prime_{t-1})\}$.
It then generates the next corresponding analysis $\anly_t$ and query $\query_{t}$, where $\anly_t$ involves intermediate conclusions and the next subtask to be solved, which informs and justifies $\query_{t}$.
For example, in the $2^{nd}$ round of conversation shown in Figure  \ref{fig:Method}, Reasoner processes previously retrieved agent and red box room and location ($\{(\anly_0, \query_0, \gG^\prime_0), (\anly_{1}, \query_{1}, \gG^\prime_{1})$),
identifies that the next subtask is to find \texttt{\small "the path between two rooms"} ($\anly_2$), and then query for the \texttt{\small "door IDs and attributes"} that connect two rooms ($\query_2$) for solving the subtask. In this way, each reasoning step is grounded to the environment by factoring in the retrieved information. %, and the graph data processed by LLMs is filtered by the reasoning.

The analysis $\anly_t$ might involve solving complex spatial sub-problems, such as navigation and object search. Past literature shows that LLMs give unreliable solutions to quantitative problems \citep{llmMathReason}. To circumvent the deficiency, we follow prior work \citep{toolformer, ART} to enable code-writing and tool-use for the Reasoner. We provide programmatic functions to address atomic problems critical to the given task family. As shown in Figure \ref{fig:Method}, at the $t^{th}$ round of conversation, the Reasoner uses the provided pathfinding tool \texttt{\small traverse\_room} to identify obstacles that need to be removed to traverse to the key, a problem beyond the capacity of LLMs. We include tool annotations in the prompt to guide the Reasoner in querying for the information necessary. 
The introduction of tools prevents hallucination on complex problems and reduces the burden of LLMs by leveraging known algorithms.

%% Below is redundant
% Since the Reasoner controls the iterative process to address a task, it is critical to control its behavior to ensure a smooth flow of the conversation. We control the message exchange between the Reasoner and the Retriever through both prompt guidance and manual interference. Specifically, we prompt the Reasoner with the graph schema and the guidance to \texttt{\small "Communicate using the terms in the graph schema"} to avoid confusion. 
% \red{full prompt linked to appendix?}
% \red{The part below can be updated based on the rebuttal.}
% We also filter out only the next query $\query_{t+1}$ to send to the Retriever, removing the analysis $\anly_t$ and the past conversation. We find that without doing so, the Retriever might attempt to realize all plan steps in the language analysis in the conversation, while omitting the actual desired information, which leads to a failure eventually.

\subsection{Retriever}

The Retriever assists the Reasoner by processing its free-form queries and returning the requested information from the graph. 
Specifically, given a query $\query$, the Retriever generates code $\code$ that executes on the scene graph to retrieve the required information $\gG^\prime$. Here, $\gV^\prime$ and $\gE^\prime$ denote subsets of graph nodes and edges, respectively. While the Reasoner may query for either the entire node or edge or just a subset of their attributes, we use $\gV^\prime$ and $\gE^\prime$ as the general representation for either case.
The code-writing strategy offers significant advantages over traditional API-calling methods.
As shown in Fig. \ref{fig:Method}, it enables efficient graph traversal by iterating through nodes, edges, or attributes using loops. 
It also supports query-oriented information filtering through logical structures such as conditional statements. 
These capabilities ensure that the retrieved information is well-aligned with the reasoning demands.

Similar to the prompt for the Reasoner, the prompt for the  Retriever includes the environment description, the scene graph schema $\gs$, and general guidance. The key difference is that $\gs$ guides the Retriever in writing the information retrieval code. Confusion is avoided by ensuring that both agents communicate using the same terms from the schema.

\subsection{Self-debugging and Error prevention in code-writing} 

Even with adequate context, LLMs are not guaranteed to write executable code in a single attempt. Therefore, we introduce a self-debugging mechanism to both the Retriever and the Reasoner to ensure the successful code execution \citep{selfdebug}. Specifically, we establish an inner iteration between the code-writing LLM and the code executor. At each round, we prompt the history of attempts, including the initial query $\query$, previous programs ${\code_{0}, \cdots, \code_{i-1}}$, and execution outcomes ${\code_{0}(\gG), \cdots, \code_{i-1}(\gG)}$, back to the LLM. If execution errors exist, the code-writing LLM corrects the code and repeats the process. Conversely, if the code execution is successful, then the debugging iteration terminates.

What's more, we observe hallucination in the code written by LLMs as prior work \citep{liu2024exploring}. In our case, the Reasoner might hallucinate about scene information without querying for it from the Retriever. To prevent this, we design a reprompting technique based on keyword detection. Specifically, we detect the keywords \textit{"assuming"} and \textit{"assume"} in the code written by LLMs, and prompt the code back to the Reasoner with the query to remove any assumptions in the code. We observe that the simple technique prevents scene information hallucination in most cases.