
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Prompts
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prompt Templates for \RwR}
\label{app:PromptTemplate}
\RwR adopts template-based prompt generation for both the Reasoner and Retriever. The templates for them are shown in Table \ref{tab:ReasonerPrompt} and Table \ref{tab:RetrieverPrompt}. The prompt is generated by populating the red contents in the template with the specific graph information.


\begin{longtable}{|p{0.96\linewidth}|}
    \caption{Retriever Prompt Template}
    \label{tab:RetrieverPrompt} \\
    \hline
    \textbf{Retriever Prompt Template} \\
    \hline
    \endfirsthead
    \hline
    \endhead
    \hline
    \endfoot

    \begin{lstlisting}[style=prompt]
You are a excellent graph information retrieval agent. Given the environnment description and the schema of the graph representaiton of the environment, you are good at writing code to obtain information from a graph following language queries.
Environment Description:
(*@\textcolor{red}{\textbf{\{ENVIRONMENT PROMPT\}}}@*)

Scene Graph Schema:
(*@\textcolor{red}{\textbf{\{SCENE GRAPH SCHEMA PROMPT\}}}@*)

Please follow the guidance below:
* Please write python code to retrieve information from the graph. Please include node id in your result and print out the result in your code.
* If there is no required information stored in the graph, print None in your code.
* The code execution result will be send back to you. Please check the result. If the information is retrieved, summarize the information and replay 'INFO RETRIEVED' in a separate paragraph following the format below:
    [Summary]
    Summarize the required information
    INFO RETRIEVED
\end{lstlisting}
\\
\end{longtable}



\begin{longtable}{|p{0.96\linewidth}|}
    \caption{Reasoner Prompt Template}
    \label{tab:ReasonerPrompt} \\
    \hline
    \textbf{Reasoner Prompt Template} \\
    \hline
    \endfirsthead
    \hline
    \endhead
    \hline
    \endfoot

    \begin{lstlisting}[style=prompt]
You are a planning agent that is excellent at collaboration and code writing. Given the environment description, schema of the graph representaiton of that environment, a retriever agent that can retrieve information from the graph, and a set of user defined reasoning tool(s), you know what information to ask from the retriever and how to use them as well as the reasoning tool(s) to solve a planning task. Then you can generate a plan executable by the agent to achieve the given mission.

Environment Description:
(*@\textcolor{red}{\textbf{\{ENVIRONMENT PROMPT\}}}@*)

Scene Graph Schema:
(*@\textcolor{red}{\textbf{\{SCENE GRAPH SCHEMA PROMPT\}}}@*)

Agent Actions:
(*@\textcolor{red}{\textbf{\{AGENT ACTIONS\}}}@*)

Please follow the guidance below:
* Solve tasks step-by-step. Figure out the next step that can help you get closer to the solution.
* If you need any information from the graph based on the graph schema, raise a language query. A retriever will return the information to you.
* If you have enough information to solve the next substep, use your reasoning and code writing skill to solve it. If you write code, print out the result with succint explanation. The code execution output will be sent back to you.
* You might be provided with reasoning tools. They are a set of python functions for solving an atomic subproblem, which might be helpful for your task. Please use the tools whenever suitable. The annotation of the tools will be provided at end of the guidance. 
* When asking the retriever for information:
    - Raise language queries that are clear, self-contained, and addressable by traversing through the graph.
    - Communicate using the terms in the graph schema. 
    - Please break questions into simpler queries and raise them one-by-one. Avoid asking for all necessary information at once.
* When the task is solved, summarize the solution and reply `TASK TERMINATE` in a separate paragraph. Do this ONLY when you obtain the complete solution.
* Format your information query message in the following way:
    [Explanation] 
    Explane why querying for the information.
    [Query]
    The information retrieval query to the retriever.
* Format your code writing message in the following way:
    [Explanation]
    Explain what your code does.
    [Code]
    Python code that solves a subproblem. Wrap the code in the python code block.
* Format your entire solution summary message in the following way:
    [Summary]
    Summarize the enire solving process.
    [Actions]
    [ACTION1, ACTION2, ...]
    TASK TERMINATE
\end{lstlisting}
\\
\end{longtable}
\pagebreak


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Environment Details
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Environment Details}
\label{app:EnvDetail}
\subsection{BabyAI Environment and Scene Graph Details}
\label{app:babyAIDetail}
\paragraph{Node attributes} The node attributes in BabyAI scene graph involve:
\begin{itemize}
    \item \textbf{"type"}: String. The type of the element type. Choices: \[root,                room, agent, key, door, box, ball\]
    \item \textbf{"color"}: String. For doors and items. The color of the element.
    \item \textbf{"coordinateâ€œ}: List of integer. Exist for all types of nodes except for the root node. For room nodes, the top left corner coordinate. For other nodes, the 2D coordinate in the grid.
    \item \textbf{"is\_locked"}: Binary. For door. State indicating if a door is locked or not.
    \item \textbf{"size"}: List of integer. For room. The size of a room. 
\end{itemize}


\subsection{VirtualHome Environment and Scene Graph Details}
\label{app:VHDetail}
\paragraph{Node attributes} The node attributes in VirtualHome involve:
\begin{itemize}
    \item \textbf{'id'}: Int. Node id.
    \item \textbf{'category'}: Str. Meta category. E.g. "Room".
    \item \textbf{'class\_name'}: Str. Specific class name. E.g. "bathroom". 
    \item \textbf{'prefab\_name'}: Str. Instance name.
    \item \textbf{'obj\_transform'}: Dict. {'position': 3D vector, 'rotation': Quaternion form as 4D vector, 'scale': 3D vector} 
    \item \textbf{'bounding\_box'}: Dict. {'center': 3D vector, "size": 3D vector} 
    \item \textbf{'properties'}: List. Object properties. Determine the action that can act upon it. 
    \item \textbf{'states'}: List. Object states. Full list of available states: ['CLOSED', 'OPEN', 'ON', 'OFF', 'SITTING', 'DIRTY', 'CLEAN', 'LYING', 'PLUGGED\_IN', 'PLUGGED\_OUT', 'HEATED', 'WASHED']
\end{itemize}

\paragraph{Edge attributes} The edge attributes in VirtualHome involve:
\begin{itemize}
    \item \textbf{'from\_id'}: Int. Id of node in the from relationship.
    \item \textbf{'to\_id'}: Int. Id of node in the to relationship.
    \item \textbf{'relationships'}: Str. Relationship between the 2 objects. Available relationships:
    \begin{itemize}
        \item \textbf{'ON'}: Object from\_id is on top of object to\_id.
        \item \textbf{'INSIDE'}: Object from\_id is inside of object to\_id.
        \item \textbf{'BETWEEN'}: Used for doors. Door connects with room to\_id.
        \item \textbf{'CLOSE'}: Object from\_id is close to object to\_id (< 1.5 metres).
        \item \textbf{'FACING'}: Object to\_id is visible from objects from\_id and distance is < 5 metres. If object1 is a sofa or a chair it should also be turned towards object2.
        \item \textbf{'HOLDS\_RH'}: Character from\_id holds object to\_id with the right hand.
        \item \textbf{'HOLD\_LH'}: Character from\_id holds object to\_id with the left hand.
        \item \textbf{'SITTING'}: Character from\_id is sitting in object to\_id.
    \end{itemize}
\end{itemize}

\paragraph{Action Space}
\begin{itemize}
    \item \textbf{[walk] <class\_name> (id)}: Walk to an object. 
    \item \textbf{[grab] <class\_name> (id)}: Grab an object. Requires that the agent has walked to that object first.
    \item \textbf{[open] <class\_name> (id)}: Open an object. Requires that the agent has walked to that object first.
    \item \textbf{[close] <class\_name> (id)}: Close an object. Requires that the agent has walked to that object first.
    \item \textbf{[switchon] <class\_name> (id)}: Turn an object on. Requires that the agent has walked to that object first.
    \item \textbf{[switchoff] <class\_name> (id)}: Turn an object off. Requires that the agent has walked to that object first.
    \item \textbf{[sit] <class\_name> (id)}: Sit on an object. Requires that the agent has walked to that object first.
    \item \textbf{[putin] <class\_name1> (id1) <class\_name2> (id1)}: Put object 1 inside object 2. Requires that the agent is holding object 1 and has walked to the object 2.
    \item \textbf{[putback] <class\_name1> (id1) <class\_name2> (id1)}: Put object 1 on object 2. Requires that the agent is holding object 1 and has walked to the object 2.
\end{itemize}

\paragraph{Example Task and State-based Specifications in VH-1}
We show the 5 example tasks and their desired final state in the VH-1 environment in Table \ref{tab:VHEgTasks}.
\begin{table}[h!]
    \centering
    % \setlength\tabcolsep{3.2pt}
    \begin{tabular}{r l}
        \toprule[1.5pt]
              \textbf{Task Name} & \textbf{State Specification }
              \\
              \hline
             Watch TV & tv ON \\
             Turn off tablelamp & tablelamp OFF \\
             put the soap in the bathroomcabinet & barsoap INSDIE bathroomcabinet \\
             throw away plum & plum INSIDE garbagecan \\
             make toast & breadslice INSIDE toaster; breadslice HEATED \\
         \bottomrule[1.5pt]
    \end{tabular}
    \caption{\textbf State-based Task Specification in VirtualHome}
    \label{tab:VHEgTasks}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Baseline Details
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Baseline Details}
\label{app:BaseDetail}
\subsection{ReAct}
For ReAct, we create the following graph information retrieval APIs in the list below. Each of them is a wrapper of a basic NetworkX \citep{networkx} operation:
\begin{itemize}
    \item \textbf{\texttt{get\_nodes()}}: Get all node IDs in the scene graph.
    \item \textbf{\texttt{get\_links()}}: Get all links in the scene graph.
    \item \textbf{\texttt{get\_attrs(node\_id)}}: Get the all attributes of a target node;
    \item \textbf{\texttt{get\_neighbors(node\_id)}}: get all neighbor node IDs of a target node.
\end{itemize}

\subsection{SayPlan}
SayPlan \citep{sayplan} is tested in BabyAI tasks. We follow the original work to create the following APIs for the room-level graph traversal purpose: 
\begin{itemize}
  \item \textbf{\texttt{collapse($\gG$)}} for retaining only room and root nodes;
  \item \textbf{\texttt{expand(node\_id)}} for revealing all nodes rooted from a given room node;
  \item \textbf{\texttt{contract(node\_id)}} for removing all nodes rooted from a given room node;
\end{itemize}
We don't assume a graph simulator available for validating and refining the solution as is done in the original paper. Instead, we evaluate the LLM-generated plan by executing it directly in the BabyAI.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Quatitative Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Qualitative results}

%%%%% figure %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
 
  % \vspace*{-0.1in}
  \centering
  \scalebox{0.97}{
    \begin{tikzpicture}
     \node[anchor=north west] at (0in,0in)
      {{\includegraphics[width=1.0\linewidth,clip=true,trim=0
      250pt 0pt 0]{figs/RwR_eg_converse.pdf}}};
    \end{tikzpicture}
  }
  \caption{\textbf{Example \RwR Traversal task solving process (Reasoner-side)}.  It shows the queries or analysis generated by the Reasoner (in black), information obtained from the Retriever (in \yellow{yellow}), the intermediate conclusion obtained through code-writing that processes the graph information (in \green{green}), and the derived plan (in \red{red}). The final plan can successfully achieve the mission shown on the left.
  }
  \vspace*{-0.0in}
 \label{fig:rwrTrvDemo}
\end{figure*}
\begin{figure*}[t!]
 
  % \vspace*{-0.1in}
  \centering
  \scalebox{0.95}{
    \begin{tikzpicture}
     \node[anchor=north west] at (0in,0in)
      {{\includegraphics[width=1.0\linewidth,clip=true,trim=0
      180pt 00pt 0]{figs/vh_qual.pdf}}};
    \end{tikzpicture}
  }
  \caption{\textbf{VirtualHome Qualitative Demonstration}. 
  \textbf{Top row}: Plan Execution;
  \textbf{Middle row}: Generated plan in the VirtualHome action format.
  \textbf{Bottom row}: \RwR Snippet of the Reasoner-side generation leading to the plan. 
  }
  \vspace*{-0.0in}
 \label{fig:vhQual}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Qualitative Results in BabyAI Traversal Task}
\label{app:RwRTrvDemo}

We qualitatively demonstrate how \RwR addresses a challenging BabyAI traversal task in Figure \ref{fig:rwrTrvDemo}. It shows the task solving process from the Reasoner's perspective, including the information queried from the Retriever as well as the intermediate solution obtained through its own code writing. It clearly demonstrates that \RwR is able to ground the plan to the environment by iteratively retrieving graph information based on the task solving process and establishing the next step towards solution based on the past retrieved information.

\subsection{Qualitative results in VirtualHome}
\label{app:RwRVHDemo}
The exemplar result in VirtualHome is shown in Figure~\ref{fig:vhQual}. Due to the prolonged output from the Retriever, we only show the core outputs from the Reasoner-side. The result shows that the Reasoner is able to generate the correct reasoning trace solely based on the graph schema, raising corresponding queries, and use the returned information to generate the correct plan for a given task.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Dynamic scene graph results.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results in Partially Observable Environments with Dynamic Scene Graphs.}
\label{app:DynExp}

\begin{longtable}[h!]{|p{0.96\linewidth}|}
    \caption{Retriever Prompt Template}
    \label{tab:DynPrompt} \\
    \hline
    \textbf{Retriever Prompt Template} \\
    \hline
    \endfirsthead
    \hline
    \endhead
    \hline
    \endfoot

    \begin{lstlisting}[style=prompt]
* Agent commanding action. Command the agent to execute an action. Format:
    [Explanation]
    Explain in one sentence why executing this action.
    [Action]
    Action for the agent to execute. Must be one of the Agent Actions. e.g. pickup(10)
\end{lstlisting}
\\
\end{longtable}
We show that our iterative framework can naturally extend to partially observable environments with dynamically changing scene graphs. To show this, we modify the BabyAI scene graph settings, where the scene graph visible to the agent only contains visited rooms in the past and the newly revealed room by the door opening actions. After the execution of each action, the scene graph is updated to reflect the revised visibility. The environment also provides action feedback, including the execution result (successful or failed) and the agent's updated location. 

We update the prompt of \RwR to allow generating individual BabyAI action to interact with the environment, as opposed to only generate the action sequence at the end without interaction in the main manuscript. Specifically, we remove the \texttt{\small solution summary} message from the Reasoner prompt, and add an additional BabyAI action command message type, as shown in Table \ref{tab:DynPrompt}. The dynamic version of \RwR is tested on the BabyAI traversal tasks under the zero-shot settings. The results are shown in Table. \ref{tab:DynResults}, together with the performance in the static settings as a reference. We do not compare with any baselines as designing the optimal strategy for the dyanmic scene graphs is not the focus of this paper. Nonetheless, even by simple updating the prompts, \RwR performs well in the dynamic settings, even better than that in the static settings due to the opportunity of exploring the environments and processing the feedback. This shows that our method can be adopted for dynamic scene graphs.

\begin{table}[h!]
    \centering
    \setlength\tabcolsep{10.pt}
    \setlength\extrarowheight{2pt}
    \begin{tabular}{l  c c  }
        \toprule[1.5pt]
         & Dynamic & Static \\
        \hline 
         Trv1  & 76\% & 61\%  \\
         Trv2  & 58\% & 56\% \\
         % \cmidrule(r){1-2} \cmidrule(lr){3-14} \cmidrule{15-16}
        \bottomrule[1.5pt]
    \end{tabular}
    \caption{
        \textbf{Results in Partially Observable BabyAI}. 
    }\label{tab:DynResults}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Compute Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis on the Computational Cost}
\label{app:ComputeAnly}

We show the number of the token processed by our method by iterations and average conversation rounds required to solve a query for the BabyAI tasks in Figure \ref{fig:CompAnlyz}. We also plot the token counts of the scene graph and the CoT baseline input.
As a direct whole-graph prompting method, the compute required by CoT is determined by the graph size. So the processed token for NumQ\&A is 4 times larger than that for the Trv-1, despite that the former is a simpler task requiring less reasoning steps.

On the other hand, \RwR processed token number monotonically increase along the iteration, as it processes the cumulative conversation history. Hence, the compute required by \RwR also depends on the task difficulty. However, thanks to the code-writing-based retrieval design, \RwR only processes limited tokens in early iterations. Thus, for simpler task such as NumQ\&A, \RwR process less tokens compared to graph prompting method such as CoT at each iteration, which is helpful for reducing hallucination over redundant information.
For the traversal task, the processed token count of our method grows beyond even the graph size. This trade-off in compute cost yields superior performance, as demonstrated in Table \ref{tab:BabyAI}. 

\begin{figure}[h!]
    \centering
    \scalebox{0.9}{
    % First subfigure
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/NumQA_compute_analysis.pdf}
        \caption{NumQ\&A}
        \label{fig:CompNumQA}
    \end{subfigure}
    \hfill % Optional: add horizontal space between the subfigures
    % Second subfigure
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/Trv_compute_analysis.pdf}
        \caption{Trv-1}
        \label{fig:CompTrv}
    \end{subfigure}
    }
    % Overall figure caption
    \caption{\textbf{Compute Analysis}. We show average conversation rounds and processed token counts at each iteration by \RwR Reasoner for both NumQ\&A (left) and Trv-1 (right) tasks. We also demonstrate the average token counts of the textualized environment scene graph and CoT input for reference.}
    \label{fig:CompAnlyz}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Baseline Failure Examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exemplar Baseline Hallucinations and how \RwR Avoids Them via Code-writing}
\label{app:BaselineFailures}

\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth,clip=true,trim=0pt 275pt 0pt 0pt]{figs/qual_baselineError_1.pdf}
        \caption{Baseline Hallucination: Spatial layout understanding.}
        \label{fig:baseHall_Spatial}
    \end{subfigure}

    \vspace{1cm}

    % Second subfigure
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth,clip=true,trim=0pt 250pt 65pt 0pt]{figs/qual_baselineError_2.pdf}
        \caption{Baseline Hallucination: Spatial counting problem.}
        \label{fig:baseHall_Count}
    \end{subfigure}

    \caption{
    \textbf{Qualitative demonstration on how \RwR avoids hallucination.}
    We show how baselines might hallucinate under the following subtasks:
    \textbf{(a)} Interpreting spatial layout from the scene graphs, where they identify the incorrect neighbor rooms;
    \textbf{(b)} Addressing the counting problem under spatial constraint, where they miscount the number of a target item type in the room.
    \RwR is able to avoid the hallucination via code-writing, which filters and processes the graph information more reliably.
    }
    \label{fig:baseHall}
\end{figure}
In Figure \ref{fig:baseHall}, we qualitative show how \RwR avoids hallucination problems happened on baselines under several scenarios from our tasks. We use the zero-shot \textbf{0-CoT} and the few-shot \textbf{CoT} as comparison. To focus on the key difference, we only show snippets of reasoning processes for each referent subtask.
We show that when reasoning in language, baselines have the tendency to hallucinate in the interpretation of the spatial layout from the scene graph structure, and in address simple quantitative reasoning (e.g. counting) tasks. 
On the other hand, based on the scene graph schema understanding, \RwR is able to solve these subtasks more reliably via code-writing.


