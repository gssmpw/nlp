\section{Related Literature}
\label{sec:lit}

\paragraph{Language models for Task and Motion Planning}
% With the advance of large language or multimodal models, many earlier
Many existing efforts harness the power of large language models for decision making \cite{xi2023rise, leap, llm+p} and robotic control \cite{plan-seq-learn, zhang2023bootstrap, text2motion, CGNet, hatori2018interactively}. With rich built-in knowledge and in-context learning ability,
%trained from the large internet-scale text corpora, 
language models are used for generating task-level plans \cite{raman2022planning, gao2024physically}, action selection \cite{saycan, pivot}, processing environmental or human feedback \cite{CLAIRIFY}, training or finetuning language-conditioned policy models \cite{octo, rt-x, szot2023large}, and more. 
To factor in the environment during planning, recent studies have explored using LLMs for programmatic plan generation \cite{progprompt}, combining knowledge from external perception tools \cite{CaP, voxposer} or grounded decoding \cite{groundedDecoding}, and value function generation \cite{language2reward}. While proven effective, those methods are limited to small scale environments, and rely on expert perception models to extract task-related states from the scene representation with implicit spatial structure. In this work, we study using pretrained LLMs to process the the global representation of large environments with explicit structure.%, and generate the solution that is grounded in the environment. 

\paragraph{Graph as the Scene Representation}
The scope of the solvable task is largely determined by the state representation. Compare to sensory representation such as images or point clouds, scene graphs are compact thus scalable to large environments \cite{greve2024collaborative}, structured to represent spatial layout explicitly \cite{hydra, wu2021scenegraphfusion}, and efficient in representing diverse states of the environment \cite{3dsg}. Therefore, they have been used in various manipulation or navigation tasks \cite{3dsgNav, hierarchicalSg}. In this paper, we exploit these favorable features of the scene graph representation to ground the reasoning process of LLMs to the environment. 

\paragraph{LLMs for Reasoning on Graph}
Leveraging language models to reason with graphs is a growing area. While prior works trains to integrates graph and language knowledge \cite{instructGLM, GRID}, recent study explores serializing graph-structured data as prompts for pretrained LLMs \cite{NLGraph, talkLikeGraph}. This strategy has been successfully used in knowledge-graph-enhanced LLMs reasoning \cite{thinkOnGraph, reasonOnGraph} and scene-graph-based robotic task planning \cite{conceptgraphs}. Closest to our work, SayPlan \cite{sayplan} prompts scene graphs to LLMs and designs a Retrieve-then-Reason framework for robotic planning. However, its room-by-room retrieval heuristic is only effective in the object search task. Instead, we design the Reason-while-Retrieve framework for general spatial reasoning with scene graphs.

%We further incorporate the code-writing and tool-use ability to LLMs, so that our proposed method can effectively retrieve information based on scene graphs and address numerical tasks that fall beyond the expertise of LLMs \cite{AliceInWonderland}.