\section{Conclusion and Future Work}
In this work, we have proposed \RwR: an iterative, two-agent framework that grounds LLMs in a physical environment through scene graphs, and enables them to reason using both natural and, crucially, programming languages. Specifically, \RwR facilitates reasoning on large scene graphs by enabling LLMs to write code that retrieves task-related information \emph{during} the reasoning process.

Our ablation study shows that all the core designs, involving the iterative reason-retrieval, the two-agent cooperation, \emph{and} the code-writing are crucial to the framework's enhanced performance. Iterative reason-retrieval ensures that the environment information enters the planning process in a just-in-time manner, two-agent framework reduces interference across the planning, and code-writing enables prompting with a data \emph{schema} rather than the data itself. In short, all of these limit ``information overload'' in the Reasoner.

Future work could explore the flexibility of \RwR framework to seamlessly integrate  additional agents with new specialties. Potential new agents involve a verifier agent to correct the solution using graph information and new modality agent to process richer information. Reasoning trace optimization could also be explored, as the conversation rounds scale with task difficulty and agent numbers.

% One unexplored benefit of the \RwR framework is its inherent flexibility: new agents with new specialties can be added to the framework with ease. In future work, we plan to experiment with a third agent, the Verifier, to correct mistakes in the Reasoner's plan based on the graph information. Another promising direction is to add new agent expert on new modalities to integrate richer information into our method. The iterative nature of \RwR, however, can lead to longer task-solving times: The number of conversation rounds increases with task complexity and the number of agents. This suggests future work investigating additional agents must be accompanied with methods to steer the LLMs to minimize the required conversation rounds. 


% \ben{We still need a proper conclusion sentence here!}


%In this work, we propose a new iterative Reason-while-Retrieve (\RwR) framework based on code-writing LLMs for reasoning on scene graphs. Unlike previous natural language graph prompting methods, \RwR reasons accurately on large scene graphs by writing code to retrieve task-related information based on the reasoning process. Additionally, through code-writing, LLMs are able to leverage external tools and address spatial problems more reliably. Through a series of numerical Q\&A and planning tasks, we demonstrate that \RwR outperforms existing baselines in both zero-shot and few-shot settings. Furthermore, our ablation study proves that both the iterative cooperation process and the code-writing design are crucial to the framework's enhanced performance.

%Despite its demonstrated capabilities, we observe a few limitations of \RwR. Firstly, although \RwR is more efficient in one-time graph processing via code, the number of conversation rounds required increases with task complexity, leading to longer task-solving times. In contrast, graph prompting methods query LLMs only once or fixed amount of times per task, and therefore do not experience as significant a slowdown with increased task difficulty. This suggests one potential improvement, which is to steer the LLMs to minimize the required conversation rounds. Additionally, the intermediate conclusions obtained during the conversation could be used as the training data, which can be distilled into a language model with enhanced graph understanding ability.

%In addition, we found that the Reasoner does not always generate the correct reasoning trace, sometimes ignoring some or all environmental constraints during task solving. For example, in the BabyAI traversal task, failure cases include the Reasoner querying for the target item ID and directly outputting \texttt{\small pickup(TARGET\_ITEM\_ID)} as the solution, without checking for the existence of a closed door between the agent and the target. We believe this occurs because we provide environmental information \textit{after} the Reasoner queries for it. Future work could investigate integrating the graph information into LLMs \textit{before} token generation. Alternatively, an LLM-based verifier could be introduced to correct the plan based on the graph information.

