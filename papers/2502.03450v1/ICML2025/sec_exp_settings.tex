%%%%% figure %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
 
  % \vspace*{-0.1in}
  \centering
  \scalebox{0.8}{
    \begin{tikzpicture}
     \node[anchor=north west] at (0in,0in)
      {{\includegraphics[width=1.0\linewidth,clip=true,trim=0
      220pt 60pt 0]{figs/exp_settings.pdf}}};
    \end{tikzpicture}
  }
  \vspace*{-0.1in}
  \caption{\textbf{Experiment Settings}. (Best viewed in color) The environment and tasks for evaluation.
  \textbf{(a)} BabyAI Trv-1 task with single-side door obstacle;
  \textbf{(b)} BabyAI Trv-2 task with double-side door obstacles;
  \textbf{(c)} BabyAI Numerical Q\&A task;
  \textbf{(d)} Two VirtualHome household environments (left: VH-1; right: VH-2) and an examplar task.
  }
  \vspace*{-0.15in}
 \label{fig:expSettings}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Settings}

We evaluate our methods on a series of numerical Q\&A (NumQ\&A) and planning tasks in the BabyAI \citep{babyai, minigrid} and VirtualHome (VH) \citep{virtualhome} environments. For each environment, we provide an unified scene graph schema consistent across epoches with potentially distinct scene graphs. Each task requires reasoning on both the spatial structure and the semantic information encoded in the graph. We use the \textbf{success rate} as our evaluation metric, where success is defined as either providing the correct answer or achieving the desired outcome with simulation, depending on the task. 
Note that all experiments in this paper are conducted in the static setting, where the tested methods generate solutions solely based on the initial scene graph without interacting with the environment that will modify the graph. We also provide preliminary results under the dynamic settings in Appendix~\ref{app:DynExp}.

We use GPT-4o for both \RwR and baselines. \RwR is implemented using AutoGen \citep{autogen}. The detailed prompts are shown in Appendix~\ref{app:PromptTemplate}. 

\paragraph{Baselines} Following NLGraph \citep{NLGraph}, we compare our approach against direct reasoning methods based on whole graph prompting, including three zero-shot approaches: \textbf{zero-shot prompting} (\textsc{zero-shot}), \textbf{Zero-Shot Chain-of-Thought} (\textsc{0-cot}) \citep{zeroShotCot}, \textbf{Least-to-Most} (\textsc{ltm}) \citep{LTM}; and three few-shot methods: \textbf{Chain-of-Thought} (\textsc{cot}) \citep{CoT}, \textbf{Build-a-Graph} (\textsc{bag}) \citep{NLGraph}, \textbf{Algorithmic Prompting} (\textsc{algorithm}) \citep{NLGraph}. In addition to the few-shot examples, \textsc{algorithm} also require a language description of the task solving method. 
We also compare against \textbf{SayPlan} \citep{sayplan}, a retrieve-then-reason baseline specifically designed for the scene graphs, and \textbf{ReAct} \citep{react}, a generic iterative reasoning and acting approach that invokes database APIs to aggregate information. We test two versions of ReAct, one with graph-traversal actions only (\textsc{ReAct}) and the other with an additional \texttt{\small traversal\_room} function in BabyAI traversal task as explained in Sec. \ref{sec:BabyAITrv} (\textsc{ReAct-Trv}).
Both SayPlan and ReAct are provided with graph APIs for retrieving graph data. Please refer to Appendix~\ref{app:BaseDetail} for more details.

\paragraph{Few-shot \RwR}
We investigate the performance of \RwR in both zero-shot and few-shot settings. For the latter, we examine two types of few-shot prompting: \textbf{\RwR+FewShot(\RwR-FS)}, which incorporates additional in-context learning examples, and \textbf{\RwR+Algorithm(\RwR-A)}: which adds both in-context examples and algorithmic prompts following \textsc{algorithm}. Notably, we do not provide any fine-grained agent-level exemplar operations as in SayPlan \cite{sayplan}, which can be impractical to collect and may constrain the reasoning flexibility of LLMs. Instead, we examine whether our framework can leverage task-level annotations to enhance its reasoning capacity.


\begin{figure}[t!]
    \centering
    \vspace*{-0pt}
    \scalebox{0.67}{
    	\begin{tikzpicture}
         \node[anchor=north west] at (0in,0in)
          {{\includegraphics[width=1.0\linewidth,clip=true,trim=0
          260pt 580pt 15pt]{figs/babyai_sg.pdf}}};
        \end{tikzpicture}
    }
    \vspace*{-15pt}
    \caption{
        \textbf{BabyAI Scene Graph Representation}. Graph nodes represent \green{items}, \red{agents}, \orange{rooms}, and \yellow{doors}. Edges indicate items or agents located inside a  room, or doors that connect rooms. Room nodes are connected to a \gray{root} node.
    }
    \label{fig:BabyAISG}
    \vspace*{-10pt}
\end{figure}

Next few subsections describe the environment and task details. The scene graph node and edge information described in the schema are shown in Appendix~\ref{app:babyAIDetail}.

\subsection{2D Grid World Numerical Q\&A}
Our first experiment is on a numerical Q\&A task in a customized 9-room 2D BabyAI \citep{babyai} environment, as shown in Figure \ref{fig:expSettings}(c).
We generate scene graph representation of the environment following the hierachical graph design from 3DSG \citep{3dsg}, illustrated in Figure \ref{fig:BabyAISG}. Specifically, the graph represents the spatial scene layout through three levels: root, rooms, and objects, with additional door nodes connecting room pairs.
Please check Appendix \ref{app:babyAIDetail} for more details.
 
Following SayPlan \citep{sayplan}, we design the following question template: \texttt{\small find the color of the \string{TARGET\_OBJECT\string} in a room next to the room with \string{NUM\_IDENTIFIER\string} \string{COLOR\_IDENTIFIER\string} \string{IDENTIFIER\_OBJECT\string}}, where contents in curley brackets are populated based on each environment instance. 
The environment and question pairs are designed to ensure that there is only one answer.

We test each method in 100 task instances. For few-shot methods, we sample two instances and manually annotate the solutions as the in-context prompt. 

\subsection{2D Grid World Traversal Planning}
\label{sec:BabyAITrv}
We also test on the traversal planning in BabyAI, where the task is to generate a sequence of node-centric actions to pick up a target item. We design three atomic actions, including (1) \texttt{\small pickup(nodeID)}: Walk to and pickup an object by the node ID; (2) \texttt{\small remove(nodeID)}: Walk to and remove an object by the node ID; (3) \texttt{\small open(nodeID)}: Walk to and open a door by the node ID. 
%We directly query \RwR and all baselines to generate the actions in the format above.

As shown in Figure \ref{fig:expSettings}(a)(b), the traversal planning task is tested in two related double-room environments, both of which require the agent to pick up the key of the correct color to unlock the door, remove any obstacle that blocks the door, open the door, and pick up the target. The difference is that the first environment, dubbed \textbf{Trv\-1}, contains only the agent-side obstacle, whereas the second environment, dubbed \textbf{Trv\-2}, contains another target-side obstacle. We generate the in-context examples \textit{only in Trv\-1} , and test if the methods can extrapolate to Trv\-2. As before, we evaluate each method in 100 times in different instance of both types of the environment. For \RwR, we provide the reasoning function \texttt{\small traversal\_room} programmed based on the $A^*$ algorithm, which identifies the item to remove in order to reach from an initial to a desired location within the same room.  As we will show, \RwR is able to leverage this external tool to compensate for the limited mathematical problem solving ability of LLMs.

\subsection{Household Task Planning}
The last evaluation is in two VirtualHome (VH) \citep{virtualhome} environments shown in Figure \ref{fig:expSettings}(d), denoted as \textbf{VH-1} and \textbf{VH-2}, respectively. We use the built-in environmental graph as the scene graph. Compared to BabyAI, VH environments have larger state space and action space, containing 115 object instances, 8 relationship types, and multiple object properties and states. Hence, VH environments are more challenging with richer information in the graphs.
% The action space is: $\mathcal{A}$ = \texttt{\{\}}. 
For each environment, we adopt the 10 household tasks from ProgPrompt \citep{progprompt}, such as \texttt{\small "put the soap in the bathroom cabinet"}, and query each method for the action sequence in the VH action format to accomplish the task. 
% As before, we task each method to directly generate the plan in the VH action format. It includes \texttt{\small [action\_name]<object\_name>(object\_id)} for one argument actions, and \texttt{\small [action\_name]<object\_name1>(object\_id1)<object\_name2>(object\_id2)} for two argument actions.
We use two of the tasks, together with the ground truth actions, as the few-shot examples, and test with the other eight. We follow CoELA \cite{coopEmbod} to specify the task as the desired states. For example, the task of above is specified as \texttt{\small soap INSIDE bathroomcabinet}. 
% To achieve the desired state, LLMs need to reason over the current state of the environment in order to identify the sequence of actions that ultimately achieve the achieve the desired outcome. A plan is considered successful if the desired states are reached after simulation. 
For more details, please refer to Appendix \ref{app:VHDetail}.
