% ONLINE SGDA
% \begin{figure}
\begin{wrapfigure}{L}{0.55\textwidth}
    %\vspace*{-0.1cm}
    \begin{minipage}{0.55\textwidth}
    \begin{algorithm}[H]
    \caption{Two time-scale 
    %stochastic 
    simultaneous 
    %gradient descent ascent
    SGDA (TTSSGDA)}
    \textbf{Inputs:} $\mgame, (\policy, \depolicy, \params, \deparams), \learnrate[\param][ ], \learnrate[\deparam][ ],  \param[][][(0)], \deparam[][][(0)], \numiters$ \\
    \textbf{Outputs:} $(\param[][][(\numhorizon)], \deparam[][][(\numhorizon)])_{\numhorizon = 0}^\numiters$
    \label{alg:two_time_sgda}
    \begin{algorithmic}[1]
    
    \State Build gradient estimator $\estG$ associated with $\mgame$ 
    
    \For{$\numhorizon = 0, \hdots, \numiters - 1$}
        % \For{$k = 0, \hdots, \numiters[\strat] - 1$}
            
            \State 
            % Sample trajectories 
            $\hist \sim \histdistrib[][\param]$, $\hist[][\prime] \sim \bigtimes_{\player \in \players} \histdistrib[][{(\deparam[\player] (\param[-\player]), \param[-\player])}]$ 
            
            \State  $\param[][][(\numhorizon + 1)] \gets 
            \param[][][(\numhorizon)] - \learnrate[\param][ ]  \estG[\param] (\param[][][(\numhorizon)], \deparam[][][(\numhorizon)]; \hist, \hist[][\prime]) $
        
            \State  $\deparam[][][(\numhorizon + 1)]  \gets  
            \deparam[][][(\numhorizon)] + \learnrate[\deparam][ ] \estG[\deparam] (\param[][][(\numhorizon)], \deparam[][][(\numhorizon)]; \hist, \hist[][\prime]) $
    
        % \EndFor
        % \State 
        %     % Sample trajectories 
        %     $\hist \sim \bigtimes_{\player \in \players} \histdistrib[][{(\otherpolicy[\player], \truepolicy[-\player])}]$, $\hist[][\prime][] \sim \histdistrib[][\truepolicy]$
            
    \EndFor
    \State \Return $(\param[][][(\numhorizon)], \deparam[][][(\numhorizon])_{\numhorizon = 0}^\numiters$
    \end{algorithmic}
    \end{algorithm}
    \end{minipage}
    %\vspace*{-0.1cm}
\end{wrapfigure}
% \end{figure}
