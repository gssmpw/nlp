\begin{algorithm}[H]
% \caption{Nested Stochastic Gradient Descent Ascent / Max-Oracle Stochastic Gradient Descent}
\textbf{Inputs:} $\outerset, \innerset, \util, \outer[][][][0], \inner[][][][0], \numiters, \numiters[\inner], \left\{\learnrate[\outer][\iter] \right\}_{\iter = 0}^{\numiters-1}, \left\{\learnrate[\inner][\iter] \right\}_{\iter = 0}^{\numiters-1}$\\
\textbf{Outputs:} $(\outer[][][][\iter], \inner[][][][\iter], \langmult[][][\iter])_{\iter = 0}^{\numiters-1}$ 
\begin{algorithmic}[1]
\caption{Policy Gradient Descent Ascent 
\label{alg:nested_pgda}}
\For{$\iter = 0, \hdots, \numiters - 1$}
        % \State Find $\weight \in \R^\omega$ s.t. $\min_{\weight \in \R^\omega} \sum_{\numsample}\left\| \rewards[\numsample][\iter] - \qfunc[ ][\weight](\state, \outeraction, \inneraction; \outer[][][][\iter], \inner[][][][\iter])]\right\|^2_2$
        \State Sample trajectory $\hist[][]  \sim \histdistrib[][{(\outer[][][][\iter], \inner[][][][\iter])}]$ and observe rewards $\rewards = (\rewards[][\numhorizon])_{\numhorizon}$
        \State $\inner[][][][\iter + 1] \gets  \project[\innerset] \left[\inner[][][][\iter] + \grad[\inner] \cumulutil[\hist](\outer[][][][\iter], \inner[][][][\iter])\right]$
        % \State \textbf{IISE:}
        % \IndState[0] $\inner[][][] \gets  \project[\{\innerset: \constr(\outer, \inner) \geq \zeros \}]\left[\inner[][][] + \grad[\inner] \lang[\obji, \constri](\inner[][][], \langmult[][]; \outer[][][][\iter]) \right]$
        % \IndState[0] $\langmult[][][] \gets  \project[\R^\numconstrs_+]\left[\langmult[][][] - \grad[\langmult] \lang[\obji, \constri](\inner[][][][\iter], \langmult[][][\iter]; \outer[][][][\iter]) \right]$
        % \State Find $\weight \in \R^\omega$ s.t. $\min_{\weight \in \R^\omega} \sum_{\numsample}\left\| \rewards[\numsample][\iter] - \qfunc[ ][\weight](\state, \outeraction, \inneraction; \outer[][][][\iter], \inner[][][][\iter])]\right\|^2_2$
        \State $\outer[][][][\iter + 1] \gets \project[\outerset]\left[\outer[][][][\iter] +  \grad[\outer] \cumulutil[\hist](\outer[][][][\iter], \inner[][][][\iter]) \right]$
\EndFor
\State \Return $(\outer[][][][\iter], \inner[][][][\iter])_{\iter = 0}^{\numiters-1}$ 
\end{algorithmic}
\end{algorithm}


The following theorem shows that under affine parametrization of the policy classes, policy gradient descent ascent converges to Markov perfect Nash equilibrium in polynomial-time.


\begin{restatable}[Computational and Sample Complexity for Policy Gradient]{theorem}{thmvalueiter}\label{thm:value_iter}
Suppose that \Cref{alg:value_iter_fisher} is run on a stochastic Fisher market where \Cref{assum:smooth} holds. Assume in addition that the policies $(\policy[\outer], \policy[\inner])$ are affinely parameterized, i.e., $\policy[\outer](\state) \doteq \phi_\outer(\state) \cdot \outer$ and $\policy[\inner](\state) \doteq \phi(\state) \cdot \inner$ for some feature maps $\phi_\outer, \phi_\inner$ of the state space. Then, the following convergence bound holds:
\begin{align}
    \|\statevalue[{\outer[][][][\iter], \inner[][][][\iter]}] - \statevalue[][*]\| \leq O\left(\frac{1}{\sqrt{\numiters}}\right)
\end{align}
\end{restatable}


\textcolor{cyan}{Alec says: it is critical to have a discussion about how the convergence analysis that gives rise to this result differs from the standard proofs. Which specific equations are different, and which steps in the analysis are dissimilar/technically innovative to deal with these points of departure? A reviewer will be carefully looking for this, so it's important to spoon feed it here in detail. Moreover, we give some overview to these points of distinction at the outset of this section, as well as section 1.2. } 


\textcolor{cyan}{Alec says: I will give another pass to the appendix once you make a stab at writting such a discussion here.}


\begin{remark}
Suppose that the policy classes are instead assumed to be any function classes which are Lipschitz-smooth in their parameters. Then, $\cumulutil$ becomes the composition of a convex-concave Lipschitz-continuous function and Lipschitz-smooth functions, and as such is weakly-convex-weakly-concave (see Section 1 of \cite{davis2019stochastic}). In such settings, any random iterate of the inexact proximal point method (see Algorithm 1 of \cite{liu2021first}) with policy gradient descent ascent used in its inner loop will converge to a first order stationary point of the expected cumulative utility function, i.e., to a point $(\outer[][][*], \inner[][][*])$ such that $\subgrad \left(\cumulutil(\outer[][][*], \inner[][][*]) + \setindic[(\outerset \times \innerset)](\outer[][][*], \inner[][][*])\right) = 0$ 
\end{remark}
