\section{Omitted Results and Proofs}

% \subsection{Omitted Results and Proofs from \Cref{sec:prelim}}

% \if 0
% \deni{Maybe add lemma on optimal value function being concave but no time for now.}
% % \lemmasavingsfoc*

% \begin{proof}[Proof of \Cref{lemma:savings_foc}]
% Fix a buyer $\buyer \in \buyers$, and state-contingent prices $\price^*: \states \to \R^\numgoods \times \R^\numassets$. Let $(\allocation[\buyer][][][*], \assetalloc[\buyer][][][*], \saving[\buyer][][*]): \states \to \consumptions \times \assetallocs \times \savings$ be an optimal consumption-saving policy. Throughout, any expectation is taken w.r.t. $\staterv \doteq ({\typerv}^\prime, {\budgetrv}^\prime, {\supplyrv}^\prime) \sim \trans(\cdot \mid \state, \assetalloc[\buyer], \saving[\buyer])$. We also recall that we use throughout the short-hand notation $\state \doteq (\budget[\buyer], \type[\buyer])$ and $\state[][][\prime] \doteq ({\budget[\buyer]}^\prime, {\type[\buyer]}^\prime)$. By Bellman's optimality principle, the actions at state $\state$ taken by the optimal policy are given by the solutions to the right hand side optimization problem in the following recursive Bellman equation for the consumption-saving problem: 
% % Throughout we use $\budget + \assetalloc[\buyer]$ as shortcut for $\budget + (\assetalloc[\buyer], \zeros[-\buyer])$. Suppose that $\budgetval[\buyer]^*$ solves the following recursive Bellman equation:
% \begin{align}
%     \budgetval[\buyer](\state) =
%     %\budgetval[\buyer]^*(\type, \budget, \supply) =
%     \max_{\substack{(\allocation[\buyer],\saving[\buyer]) \in \consumptions \times \savings:\\ 
%     \allocation[\buyer] \cdot \price^*(\state)  \leq \budget[\buyer] + \saving[\buyer]}} \left\{ \util[\buyer]\left(\allocation[\buyer], \type[\buyer]\right) 
%     % \\ \notag   & 
%     + \discount \mathop{\E} \left[ \budgetval[\buyer]({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right] \right\}
% \end{align}

% Define the Lagrangian associated with the optimal actions at state $\state$ defined by the optimal policy for the consumption-saving problem: 
% \begin{align}   
% \max_{\substack{(\allocation[\buyer], \saving[\buyer]) \in \consumptions \times \savings:\\ 
%     \allocation[\buyer] \cdot \price^*(\state) \leq \budget[\buyer] + \saving[\buyer]}} \left\{ \util[\buyer]\left(\allocation[\buyer], \type[\buyer]\right) 
%     % \\ \notag   & 
%     + \discount \mathop{\E} \left[ \budgetval[\buyer][*]({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right] \right\}
% \end{align}
% as follows:
% \begin{align}
%     \lang(\allocation[\buyer], \saving[\buyer], \langmult[ ]; \price) &\doteq \util[\buyer]\left(\allocation[\buyer]; \type[\buyer] \right) 
%     % \\ \notag   & 
%     + \discount \mathop{\E} \left[ \budgetval[\buyer][*]({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right] + \langmult[ ] (\budget[\buyer] + \saving[\buyer]  - \allocation[\buyer] \cdot \price^*(\state)   ) 
% \end{align}
%     % $$
%     % \budgetval[\buyer][*](\type, \budget, \supply) = 
%     %     \max_{(\allocation[\buyer], \assetalloc[\buyer]) \in \R^{\numgoods + 1}_+: \allocation[\buyer] \cdot \price^*(\budget)  \leq \budget[\buyer]}  \util[\buyer](\allocation[\buyer], \type[\buyer]) + \discount  \mathop{\E} \left[ \budgetval[\buyer][*]({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right] \enspace .
%     % $$
    
%     Notice that since constraints are affine the linear constraint qualification holds \deni{Add citation}, and hence there exists a saddle point\footnote{We omit the dependency on $\price(\state)$ for conciseness.} $((\allocation[\buyer][][][*](\state), \saving[\buyer][][*](\state)) , \langmult[ ][*](\state))$ to the following min-max optimization problem:
%     \begin{align}
%         \max_{(\allocation[\buyer], \in \consumptions \times  \savings} \min_{\langmult[ ] \in \R_+} \lang(\allocation[\buyer],  \saving[\buyer], \langmult[ ]; \price, \state)
%     \end{align}
%     where recall, $(\allocation[\buyer][][][*](\state), \saving[\buyer][][*](\state))$ is the action decided at state $\state$ by any optimal policy $(\allocation[\buyer][][][*], \saving[\buyer][][*])$ for the consumption-saving problem. 

%     Now, notice that under the assumptions of the lemma, by this optimization problem is concave in $(\allocation[\buyer][][][], \saving[\buyer][][])$ . As such the KKT conditions \cite{kuhn1951kkt} apply and the following conditions holding for all $\state \in \states$ are necessary and sufficient for a policy of consumptions, holdings, and assets $(\allocation[\buyer][][][*], \saving[\buyer][][*])$ to be optimal:
%     % That is, we have:
%     % first order optimality conditions for an allocation $\allocation[\buyer][][][*](\state) \in \R_+^{\numgoods}$, saving $\assetalloc[\buyer][][][*](\state) \in \R_+$ and associated Lagrangian multipliers $\langmult[ ][*](\state) \in \R_+$, $\bm \mu^*(\state) \in \R^{\numgoods + 1}$ to be optimal for any prices $\price(\state) \in \R^\numgoods_+$ and state $\state \in \states$ are given by the following pair of KKT conditions  for all $\good \in \goods$:
% \begin{align}
%     \subgrad[{\allocation[\buyer]}] \util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) 
%     % + \discount \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right]
%     - \langmult[ ][*](\state) \price^*(\state) \doteq 0 && \text{(First-order optimality)} \label{eq:foc_single_buyer_1}\\ 
%      % \gamma  \subgrad[{\assetalloc[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]  - {\langmult[\buyer]}^*(\state) \assetprice[]^*(\state)  \doteq 0 && \text{(First-order optimality)}\\
%      \gamma  \subgrad[{\saving[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]  + \langmult[\buyer][*](\state) \doteq 0 &&\text{(First-order optimality)}\label{eq:foc_single_buyer_3}\\
%     \langmult[ ][*](\state) (\budget[\buyer] + \saving[\buyer][][*]  - \allocation[\buyer][][][*] \cdot \price^*(\state) ) \doteq 0\label{eq:slack_compl1} && \text{(Slack Complementarity)} \\
%     \allocation[\buyer][][][*] \cdot \price^*(\state) \leq \budget[\buyer] + \saving[\buyer][][*] && \text{(Primal Feasibility)} \\
%     \langmult[ ][*](\state) \geq 0  && \text{(Dual Feasibility)} \label{eq:dual_feasibility}
% \end{align}

% We will now manipulate some of these conditions, to obtain more interpretable ones.
% % Additionally, by the KKT complimentarity conditions, we have for all $\good \in \good$, $\mu_\good^* \allocation[\buyer][\good][][*] = 0$ and $\mu_{\numgoods + 1}^* \assetalloc[\buyer][][][*] = 0$, which gives us: 
% % \begin{align}
% %     \frac{\partial \util[\buyer]}{\partial \allocation[\buyer][\good]}(\allocation[\buyer][][][*](\state); \type[\buyer]) 
% %     % + \discount \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right]
% %     - \langmult[ ][*](\state) \price[\good](\state)  = 0 && \forall \good \in \goods\\
% %     \assetalloc[\buyer][][][*](\state) > 0 \implies \gamma \frac{\partial}{\partial \assetalloc[\buyer]}\mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime [][][*], {\supply}^\prime) \right] -\langmult[ ][*](\state) = 0 && \forall \good \in \goods 
% % \end{align}
% % Taking \Cref{eq:foc_single_buyer_1}, and re-organizing expressions, for all $\good \in \goods$ we have:
% % \begin{align}
% %     \allocation[\buyer][\good][][*](\state) > 0 \implies  \langmult[ ][*]  = \frac{\frac{\partial \util[\buyer]}{\partial \allocation[\buyer][\good]}(\allocation[\buyer][][][*](\state); \type[\buyer]) 
% %     % + \discount \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right]
% %     }{\price[\good](\state)} && \forall \good \in \goods\label{eq:implies_foc_single_buyer_1}\\
% %     \assetalloc[\buyer][][][*](\state) > 0 \implies \langmult[ ][*](\state) =\gamma \frac{\partial}{\partial \assetalloc[\buyer]}\mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime [][][*], {\supply}^\prime) \right]   \label{eq:implies_foc_single_buyer_2}
% % \end{align}
% First, we obtain a closed-form formula for $\langmult[ ][*](\state)$. To do so, take \Cref{eq:foc_single_buyer_1} and apply $\allocation[\buyer][\good][][*](\state)$ with the dot product, we obtain:
% \begin{align}
%     &\allocation[\buyer][][][*](\state) \cdot \subgrad[{\allocation[\buyer]}] \util[\buyer] (\allocation[\buyer][][][*](\state); \type[\buyer]) 
%     % + \discount \sum_{\good \in \goods}\allocation[\buyer][\good]^*(\state)  \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right] 
%     - \langmult[ ][*](\state) \left[\price(\state) \cdot \allocation[\buyer][][][*](\state) \right]= 0
% \end{align}
% Using Euler's theorem for homogeneous functions on the partial derivatives of the utility functions \cite{border2017euler}, we then get:
% \begin{align}
%     &\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) 
%     % + \discount \sum_{\good \in \goods}\allocation[\buyer][\good]^*(\state)  \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right] 
%    - \langmult[ ][*](\state) \left[ \price(\state) \cdot \allocation[\buyer][][][*](\state) \right] = 0
% \end{align}
% From the KKT Slack complementarity conditions (\Cref{eq:slack_compl1}), we have $\langmult[ ][*](\state) (\budget[\buyer] - \saving[\buyer][][*]) = \langmult[ ][*](\state) \left( \price(\state) \cdot \allocation[\buyer][][][*](\state) \right)$, which combined with the above Equation gives us:
% \begin{align}
%     &\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) 
%     % + \discount \sum_{\good \in \goods}\allocation[\buyer][\good]^*(\state)  \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right] 
%     - \langmult[ ][*](\state) (\budget[\buyer]- \saving[\buyer][][*])= 0 \\
%     &\langmult[ ][*](\state) = \frac{\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) 
%     % + \discount \sum_{\good \in \goods}\allocation[\buyer][\good]^*(\state)  \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right]
%     }{\budget[\buyer] - \saving[\buyer][][*]}\label{eq:bang_per_buck}
% \end{align}

% Further, note that primal feasibility and Slack complementarity are redundant, since we have by local non-satiation of the utility functions, that the budget constraint must be binding, i.e.:
% \begin{align}
%     \budget[\buyer] + \saving[\buyer][][*]  - \allocation[\buyer][][][*] \cdot \price^*(\state) = 0\label{eq:budgets_binding}
% \end{align}

% Additionally, since the optimal state-value function is concave in $ \budget[\buyer]$, the state value function is subdifferentiable via an envelope theorem \cite{afriat1971envelope, milgrom2002envelope} w.r.t. $\budget[\buyer]$ and any arbitrary subgradient $\subgrad[{\budget[\buyer]}]\budgetval[\buyer][*]$ is given by the subdifferential envelope theorem \cite{goktas2021minmax}, as follows:
% \begin{align}\label{eq:value_env_buyer_budget}
%     \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state) = \langmult[ ][*](\state) 
% \end{align}

% Hence, plugging \Crefrange{eq:bang_per_buck}{eq:value_env_buyer_budget} into \Crefrange{eq:foc_single_buyer_1}{eq:dual_feasibility}, removing redundant equations and re-organizing terms, we obtain:
% \begin{align}
%     \frac{\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) }{\budget[\buyer] - \saving[\buyer][][*]}  &= \frac{\subgrad[{\allocation[\buyer][\good]}] \util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer])}{\price[\good]^*(\state)} && \forall \good \in \goods\\ 
%     % \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state)  &= \frac{\gamma  \subgrad[{\assetalloc[\buyer][\asset]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]}{\assetprice[\asset]^*(\state)}  && \forall \asset \in \assets \\
%     \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state) &= - \gamma  \subgrad[{\saving[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]  \\
%     \allocation[\buyer][][][*] \cdot \price^*(\state)  &= \budget[\buyer] + \saving[\buyer][][*] 
% \end{align}

% Finally, we show that the last equation implied by the first:

% \begin{align}
%     \frac{\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) }{\budget[\buyer] + \saving[\buyer][][*]}  &= \frac{\subgrad[{\allocation[\buyer]}] \util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer])}{\price[\good]^*(\state)} && \forall \good \in \goods\\
%     \frac{\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) \price[\good]^*(\state)}{\budget[\buyer] + \saving[\buyer][][*]}  &= \subgrad[{\allocation[\buyer]}] \util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) && \forall \good \in \goods
% \end{align}

% Multiplying by $\allocation[\buyer][\good][][*](\state)$ on both sides and summing up across $\good \in \goods$, and applying Euler's theorem for homogeneous functions, we obtain:
% \begin{align}
%     \frac{\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) \sum_{\good \in \goods}\price[\good]^*(\state)\allocation[\buyer][\good][][*](\state)}{\budget[\buyer] + \saving[\buyer][][*]}  &= \sum_{\good \in \goods} \subgrad[{\allocation[\buyer]}] \util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) \allocation[\buyer][\good][][*](\state)\\
%     \frac{\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) \sum_{\good \in \goods}\price[\good]^*(\state)\allocation[\buyer][\good][][*](\state)}{\budget[\buyer]  + \saving[\buyer][][*]}  &= \util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer])\\
%     \price^*(\state) \cdot \allocation[\buyer][][][*](\state)  &= \budget[\buyer]  + \saving[\buyer][][*]
% \end{align}

% Hence, the following equations become necessary and sufficient condition:

% \begin{align}
%     \frac{\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) }{\budget[\buyer] + \saving[\buyer][][*]}  &= \frac{\subgrad[{\allocation[\buyer][\good]}] \util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer])}{\price[\good]^*(\state)} && \forall \good \in \goods\\ 
%     % \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state)  &= \frac{\gamma  \subgrad[{\assetalloc[\buyer][\asset]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]}{\assetprice[\asset]^*(\state)}  && \forall \asset \in \assets \\
%     \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state) &= - \gamma  \subgrad[{\saving[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]
% \end{align}
% % Additionally, using the envelope theorem \cite{afriat1971envelope, milgrom2002envelope}, since the optimal state-value function is concave in $ \budget[\buyer]$, the state value function is subdifferentiable w.r.t. $\budget[\buyer]$ and any arbitrary subgradient $\subgrad[{\budget[\buyer]}]\budgetval[\buyer][*]$ is given by the subdifferential envelope theorem \cite{goktas2021minmax}, as follows:
% % \begin{align}\label{eq:value_env_buyer_budget}
% %     \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state) = \langmult[ ][*](\state) 
% %     % \assetprice^*(\state)\\
% %     % \subgrad[{\saving[\buyer]}] \budgetval[\buyer][*] = \langmult[ ][*](\state)
% % \end{align}
% % % we can also compute $\subgrad[{\assetalloc[\buyer]}] \budgetval[\buyer][*]$ as follows:
% % % \begin{align}
% % %     \frac{\partial \budgetval[\buyer]^*(\state)}{\partial \budget[\buyer]}(\state) = \langmult[ ][*](\state) \assetprice^*(\state)
% % %     % text{\sadie{I'm confused here}}
% % % \end{align}
% % % We note that for all states $\state \in \states$, $\frac{\partial \budgetval[\buyer]^*}{\partial \budget[\buyer]}(\state)$ is well-defined since $\langmult[ ][*](\state)$ is uniquely defined for all states by \Cref{eq:implies_foc_single_buyer_1}.
% % Hence, combining the above with \Crefrange{eq:foc_single_buyer_1}{eq:foc_single_buyer_3}, we get:
% % \begin{align}
% %     \subgrad[{\allocation[\buyer]}] \util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) 
% %     % + \discount \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right]
% %     - \langmult[ ][*](\state) \price^*(\state) \doteq 0\\ 
% %      \gamma  \subgrad[{\assetalloc[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]  - \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state) \assetprice[]^*(\state)  \doteq 0\\
% %      \gamma  \subgrad[{\saving[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]  - \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state) \doteq 0 
% % \end{align}
% % % \begin{align}
% % %     \allocation[\buyer][\good][][*](\state) > 0 \implies  \langmult[ ][*]  = \frac{\frac{\partial \util[\buyer]}{\partial \allocation[\buyer][\good]}(\allocation[\buyer][][][*](\state); \type[\buyer]) 
% % %     % + \discount \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right]
% % %     }{\price[\good]} && \forall \good \in \goods\\
% % %     \assetalloc[\buyer][][][*](\state) > 0 \implies \frac{\partial \budgetval[\buyer]^*}{\partial \budget[\buyer]}(\state)  =\gamma \frac{\partial}{\partial \assetalloc[\buyer]}\mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime [][][*], {\supply}^\prime) \right]  \label{eq:foc_single_buyer_2}
% % % \end{align}
% % % 
% % Finally, 
% % Combining the above conditions, with
% % \Cref{eq:implies_foc_market_1}, and adding to it
% % \Cref{eq:foc_single_buyer_2}, and ensuring that the KKT primal feasibility conditions hold as well, we obtain the following necessary conditions that need to hold for all states $\state \in \states$:
% % % 
% % \begin{align}
% %     \allocation[\buyer][\good][][*](\state) > 0 &\implies   \frac{\frac{\partial \util[\buyer]}{\partial \allocation[\buyer][\good] }(\allocation[\buyer][][][*](\state); \type[\buyer]) 
% %     % + \discount \sum_{\good \in \goods}\allocation[\buyer][\good]^*(\state)  \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right]
% %     }{\price[\good](\state)} =
% %     % \notag\\ 
% %     % &= 
% %     \frac{\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) 
% %     % + \discount \sum_{\good \in \goods}\allocation[\buyer][\good]^*(\state)  \frac{\partial }{\partial \allocation[\buyer][\good]} \mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer]^*({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right]
% %     }{\budget[\buyer] - \assetalloc[\buyer][][][*](\state)} && \forall \good \in \goods\\
% %     \assetalloc[\buyer][][][*](\state) > 0 &\implies \frac{\partial \budgetval[\buyer][*]}{\partial \budget[\buyer]}(\state)  =\gamma \frac{\partial}{\partial \assetalloc[\buyer]}\mathop{\E}_{(\staterv[][][\prime]) } \left[ \budgetval[\buyer][*]({\type}^\prime, {\budget}^\prime , {\supply}^\prime) \right] 
% % \end{align}

% % If additionally the transition  probability function $\trans$ is CSD concave in $
% % % (\allocation[\buyer],
% % \assetalloc[\buyer]$, then $\budgetval[\buyer]$ is concave and the utility maximization problem is concave, which in turn implies that the above conditions are also sufficient.

% % \deni{and as the objective is concave (by the Weierstass M-test and the uniform limit theorem), the KKT first order necessary and sufficient}
% % Combining the last two equations, we obtain, we can re-express the second optimality condition as follows:
% % \begin{align}
% %      \assetalloc[\buyer][][][*](\budget[\buyer]) > 0 \implies \frac{\partial \budgetval[\buyer]^*}{\partial \assetalloc[\buyer]}(\budget[\buyer])  = \gamma (1+\interest) \frac{\partial \budgetval[\buyer]^*}{\partial \assetalloc[\buyer]}(\assetalloc[\buyer][][][*](1+\interest)) 
% % \end{align}
% \end{proof}

% \subsection{Omitted Results and Proofs from \Cref{sec:stoch_fisher_mkt}}
% \lemmaconcavethirdterm*
% \begin{proof}
%     For all $\type \in \typespace$, $\budget, \budget^\prime \in \budgetspace$, $\saving, {\saving}^\prime \in \savings$, $\allocation, {\allocation}^\prime \in \consumptions$, and $\langmult[ ] \in (0,1)$, we have:
%     \begin{align}
%         & \sum_{\buyer \in \buyers} \left[\left(\langmult[ ]\budget[\buyer] + (1- \langmult[ ]){\budget[\buyer]}^\prime - \left[\langmult[ ]\saving[\buyer] + (1-\langmult[ ]) {\saving[\buyer]}^\prime \right]\right) \log \left( \frac{\util[\buyer](\langmult[ ] \allocation[\buyer] + (1-\langmult[ ]) \allocation[\buyer][][][\prime]; \type[\buyer])}{\langmult[ ]\budget[\buyer] + (1- \langmult[ ]){\budget[\buyer]}^\prime - \left[\langmult[ ]\saving[\buyer] + (1-\langmult[ ]) {\saving[\buyer]}^\prime \right]}\right) \right]\\
%         &= \sum_{\buyer \in \buyers} \left[\left(\langmult[ ]\left[\budget[\buyer] - \saving[\buyer]\right] + (1- \langmult[ ]) \left[{\budget[\buyer]}^\prime -  {\saving[\buyer]}^\prime \right] \right) \log \left( \frac{\util[\buyer](\langmult[ ] \allocation[\buyer] + (1-\langmult[ ]) \allocation[\buyer][][][\prime]; \type[\buyer])}{\langmult[ ]\left[\budget[\buyer] - \saving[\buyer]\right] + (1- \langmult[ ]) \left[{\budget[\buyer]}^\prime -  {\saving[\buyer]}^\prime \right] }\right) \right]\\
%         &\geq \sum_{\buyer \in \buyers} \left[\left(\langmult[ ]\left[\budget[\buyer] - \saving[\buyer]\right] + (1- \langmult[ ]) \left[{\budget[\buyer]}^\prime -  {\saving[\buyer]}^\prime \right]  \right) \log \left( \frac{\langmult[ ]\util[\buyer]( \allocation[\buyer]; \type[\buyer]) + (1-\langmult[ ]) \util[\buyer](\allocation[\buyer][][][\prime]; \type[\buyer])}{\langmult[ ]\left[\budget[\buyer] - \saving[\buyer]\right] + (1- \langmult[ ]) \left[{\budget[\buyer]}^\prime -  {\saving[\buyer]}^\prime \right] }\right) \right]\label{eq:util_concave}\\
%         &\geq \sum_{\buyer \in \buyers} \left[\langmult[ ]\left(\budget[\buyer] - \saving[\buyer] \right) \log \left( \frac{\langmult[ ]\util[\buyer]( \allocation[\buyer]; \type[\buyer])}{\langmult[ ]\left[\budget[\buyer] - \saving[\buyer]\right] }\right) \right] +
%         \sum_{\buyer \in \buyers} \left[(1- \langmult[ ])\left( {\budget[\buyer]}^\prime -  {\saving[\buyer]}^\prime  \right) \log \left( \frac{(1-\langmult[ ]) \util[\buyer](\allocation[\buyer][][][\prime]; \type[\buyer])}{(1- \langmult[ ]) \left[{\budget[\buyer]}^\prime -  {\saving[\buyer]}^\prime \right] }\right) \right]\label{eq:log_sum_ineq}\\
%         &= \langmult[ ] \sum_{\buyer \in \buyers} \left[\left(\budget[\buyer] - \saving[\buyer] \right) \log \left( \frac{\util[\buyer]( \allocation[\buyer]; \type[\buyer])}{\budget[\buyer] - \saving[\buyer] }\right) \right] +
%         (1- \langmult[ ]) \sum_{\buyer \in \buyers} \left[\left( {\budget[\buyer]}^\prime -  {\saving[\buyer]}^\prime  \right) \log \left( \frac{\util[\buyer](\allocation[\buyer][][][\prime]; \type[\buyer])}{{\budget[\buyer]}^\prime -  {\saving[\buyer]}^\prime  }\right) \right]
%     \end{align}
%     where \Cref{eq:util_concave} follows from the concavity of the utility function, and \Cref{eq:log_sum_ineq} follows from the log-sum inequality.
% \end{proof}


% \begin{proof}[Proof of \Cref{thm:value_iter}]
%     Line 4 of \Cref{alg:approx_bellman_operator} computes the bellman operator up to an error factor of $O(\nicefrac{1}{\sqrt{\numiters}})$. Since we can fit a concave function using a quadratic program \cite{seijo2011nonparametric}, which can be solved via stochastic gradient descent in $O(\nicefrac{1}{\sqrt{\numiters}})$ iterations/state samples, we can obtain a value function which approximates the true value function within a $O(\nicefrac{1}{\sqrt{\numiters}})$ in $\numiters$ iterations. Combining this with the convergence rate of approximate value iteration \cite{munos2007performance}, we obtain the result.
% \end{proof}
% % \begin{proof}
% %     Let $\type \in \typespace$, $\assetprice \in \assetpricespace$, $\budget, \budget^\prime \in \budgetspace$, $\saving, {\saving}^\prime \in \savings$, $\allocation, \allocation[][][][\prime] \in \consumptions$, $\assetalloc, \assetalloc[][][][\prime] \in \assetallocs$, and $\lambda \in (0,1)$. Define $\overline{\budget} \doteq \lambda\budget + (1- \lambda){\budget}^\prime$, $\overline{\saving} \doteq \lambda\saving[\buyer] + (1-\lambda) {\saving[\buyer]}^\prime$, $\overline{\allocation} \doteq \lambda \allocation[\buyer]  + (1-\lambda) \allocation[\buyer][][][\prime] $ and $\overline{\assetalloc} \doteq \lambda \assetalloc[\buyer]  + (1-\lambda) \assetalloc[\buyer][][][\prime] $ we have:
% %     \begin{align}
% %         & \sum_{\buyer \in \buyers} \left[\left(\overline{\budget[\buyer]} + \overline{\saving[\buyer]} - \assetprice \cdot \overline{\assetalloc[\buyer]} \right) \log \left( \frac{\util[\buyer](\overline{\allocation[\buyer]} ; \type[\buyer])}{\overline{\budget[\buyer]} + \overline{\saving[\buyer]} - \assetprice \cdot \overline{\assetalloc[\buyer]} }\right) \right]\\
% %         % & \sum_{\buyer \in \buyers} \left[\left(\langmult[ ]\budget[\buyer] + (1- \langmult[ ]){\budget[\buyer]}^\prime - \left[\langmult[ ]\saving[\buyer] + (1-\langmult[ ]) {\saving[\buyer]}^\prime \right]\right) \log \left( \frac{\util[\buyer](\langmult[ ] \allocation[\buyer] + (1-\langmult[ ]) \allocation[\buyer][][][\prime]; \type[\buyer])}{\langmult[ ]\budget[\buyer] + (1- \langmult[ ]){\budget[\buyer]}^\prime - \left[\langmult[ ]\saving[\buyer] + (1-\langmult[ ]) {\saving[\buyer]}^\prime \right]}\right) \right]\\
% %         &= \sum_{\buyer \in \buyers} \left[\left(\langmult[ ]\left[\budget[\buyer] + \saving[\buyer] - \assetprice\cdot \assetalloc[\buyer]\right] + (1- \langmult[ ]) \left[{\budget[\buyer]}^\prime +  {\saving[\buyer]}^\prime - \assetprice\cdot \assetalloc[\buyer][][][\prime]\right] \right) \log \left( \frac{\util[\buyer](\langmult[ ] \allocation[\buyer] + (1-\langmult[ ]) \allocation[\buyer][][][\prime]; \type[\buyer])}{\langmult[ ]\left[\budget[\buyer] + \saving[\buyer] - \assetprice\cdot \assetalloc[\buyer]\right] + (1- \langmult[ ]) \left[{\budget[\buyer]}^\prime +  {\saving[\buyer]}^\prime - \assetprice\cdot \assetalloc[\buyer][][][\prime]\right] }\right) \right]\\
% %         &\geq \sum_{\buyer \in \buyers} \left[\left(\langmult[ ]\left[\budget[\buyer] + \saving[\buyer] - \assetprice\cdot \assetalloc[\buyer]\right] + (1- \langmult[ ]) \left[{\budget[\buyer]}^\prime +  {\saving[\buyer]}^\prime - \assetprice\cdot \assetalloc[\buyer][][][\prime]\right]  \right) \log \left( \frac{\langmult[ ]\util[\buyer]( \allocation[\buyer]; \type[\buyer]) + (1-\langmult[ ]) \util[\buyer](\allocation[\buyer][][][\prime]; \type[\buyer])}{\langmult[ ]\left[\budget[\buyer] + \saving[\buyer] - \assetprice\cdot \assetalloc[\buyer]\right] + (1- \langmult[ ]) \left[{\budget[\buyer]}^\prime +  {\saving[\buyer]}^\prime - \assetprice\cdot \assetalloc[\buyer][][][\prime]\right] }\right) \right]\label{eq:util_concave}\\
% %         &\geq \sum_{\buyer \in \buyers} \left[\langmult[ ]\left(\budget[\buyer] + \saving[\buyer] - \assetprice\cdot \assetalloc[\buyer]\right) \log \left( \frac{\langmult[ ]\util[\buyer]( \allocation[\buyer]; \type[\buyer])}{\langmult[ ]\left[\budget[\buyer] + \saving[\buyer] - \assetprice\cdot \assetalloc[\buyer]\right]  }\right) \right] +
% %         \sum_{\buyer \in \buyers} \left[(1- \langmult[ ])\left( {\budget[\buyer]}^\prime +  {\saving[\buyer]}^\prime  - \assetprice\cdot \assetalloc[\buyer][][][\prime] \right) \log \left( \frac{(1-\langmult[ ]) \util[\buyer](\allocation[\buyer][][][\prime]; \type[\buyer])}{(1- \langmult[ ]) \left[{\budget[\buyer]}^\prime -  {\saving[\buyer]}^\prime - \assetprice\cdot \assetalloc[\buyer][][][\prime]  \right] }\right) \right]\label{eq:log_sum_ineq}\\
% %         &= \langmult[ ] \sum_{\buyer \in \buyers} \left[\left(\budget[\buyer] + \saving[\buyer] - \assetprice\cdot \assetalloc[\buyer]\right) \log \left( \frac{\util[\buyer]( \allocation[\buyer]; \type[\buyer])}{\budget[\buyer] + \saving[\buyer] - \assetprice\cdot \assetalloc[\buyer] }\right) \right] +
% %         (1- \langmult[ ]) \sum_{\buyer \in \buyers} \left[\left( {\budget[\buyer]}^\prime +  {\saving[\buyer]}^\prime - \assetprice \cdot \assetalloc[\buyer][][][\prime] \right) \log \left( \frac{\util[\buyer](\allocation[\buyer][][][\prime]; \type[\buyer])}{{\budget[\buyer]}^\prime +  {\saving[\buyer]}^\prime - \assetprice \cdot \assetalloc[\buyer][][][\prime]}\right) \right]
% %     \end{align}
% %     where \Cref{eq:util_concave} follows from the concavity of the utility function, and \Cref{eq:log_sum_ineq} follows from the log-sum inequality.
% % \end{proof}


% \lemmaconvexgame*
% \begin{proof}[Proof of \Cref{lemma:convex_game}]
%     It is clear that for all $\state \in \states, (\allocation,  \saving) \in \consumptions \times  \savings$, $\reward(\state, \price, (\allocation, \saving))$ us affine in $\price$. 
%     Regarding the concavity in $(\allocation, \saving)$ for all $\state \in \states, \price \in \pricespace $, first observe that the first two sums in $\reward(\state, \price, (\allocation,  \saving))$ are affine in $(\allocation, \saving)$ and the third term is concave in $(\allocation, \saving)$ by \Cref{lemma:concave_third_term}. Hence,  
%     Regarding the concavity in $(\allocation, \saving)$ for all $\state \in \states, \price \in \pricespace $, first observe that the first two sums in $\reward(\state, \price, (\allocation,  \saving))$ are affine in $(\allocation, \saving)$ and the third term is concave in $(\allocation, \saving)$ by \Cref{lemma:concave_third_term}. Hence,  
%     % simply the negated KL divergence $(p, q) \mapsto p \log\left(\frac{q}{p}\right) = -  p \log\left(\frac{p}{q}\right)$, composed with $(p, \allocation[\buyer]) \mapsto (p, \util[\buyer](\allocation[\buyer]))$ and  $(\budget[\buyer], \saving[\buyer], \allocation[\buyer]) \mapsto (\budget[\buyer] - \saving[\buyer], \allocation[\buyer])$ respectively. Now, the first composition results in the mapping $(p, \allocation[\buyer]) \mapsto p \log \left(\frac{\util[\buyer](\allocation[\buyer])}{p}\right)$ which is jointly concave in $(p, \allocation[\buyer])$ since the negated KL divergence is a non-decreasing function and for all buyers $\buyer \in \buyers$ and types $\type \in \typespace$, $\util[\buyer](\allocation[\buyer]; \type[\buyer])$ is concave in $\allocation[\buyer]$ (see page 86 of \cite{boyd2004convex}). The composition of this resulting function with the third mapping, then results in the mapping $(\budget[\buyer], \saving[\buyer], \allocation[\buyer]) \mapsto \left(\budget[\buyer] - \saving[\buyer] \right) \log \left(\frac{\util[\buyer](\allocation[\buyer])}{\budget[\buyer] - \saving[\buyer]}\right)$ which is jointly concave $(\budget[\buyer], \saving[\buyer], \allocation[\buyer])$ because it is the composition of a jointly concave function, with an affine function.\footnote{This point can alternatively be proven using the log-sum inequality and the concavity of the utility function, however this particular proof technique highlights the importance of the KL divergence due to its joint concavity in the reward function. Hence, it provides an intuition why our formulation allows for convex-concave rewards, while \citeauthor{goktas2022zero}'s \cite{goktas2022zero} previous formulation did not.} Hence, the third sum must also be jointly concave in $(\budget[\buyer], \saving[\buyer], \allocation[\buyer])$.

%     To prove the second part of the theorem, recall that by Bellman's optimality principle for zero-sum stochastic games \cite{shapley1953stochastic}, the optimal state value function of the game is defined as:
%     \begin{align}
%         \statevalue[][*](\state) \doteq
%         \min_{\substack{\price \in \pricespace}} \max_{\substack{\allocation \in \consumptions\\ \saving  \in \savings}} \price \cdot \left( \supply - \sum_{\buyer \in \buyers} \allocation[\buyer] \right) + \sum_{\buyer \in \buyers} \left(\budget[\buyer] + \saving[\buyer]  \right) \log \left( \frac{\util[\buyer](\allocation[\buyer]; \type[\buyer])}{\budget[\buyer] + \saving[\buyer] }\right) + \discount \Ex\left[\statevalue[][*](\staterv[][][\prime])\right]
%     \end{align}
%     where the expectation is with respect to $\staterv[][][\prime] \sim \trans(\cdot \mid \state, \saving)$.
%     By \Cref{lemma:concave_third_term}, the reward is concave in $(\budget, \allocation, \saving)$ and hence by the stochastic concavity of the probability transition function in $(\budget, \allocation, \saving)$, the entire objective is concave in $(\budget, \allocation, \saving)$ fixing all other variables. Hence, the maximum of this objective is concave in $\budget$, and this concavity is preserved through the exterior minimization by Danskin's theorem \cite{danskin1966thm}.
% \end{proof}


% \theoremzerosumfisher*
% \begin{proof}[Proof of \Cref{theorem:zero_sum_fisher}]
%     Let $(\price^*, \allocation[][][][*], \saving[][][*])$ be an optimal stationary policy for the stochastic Fisher market game. Throughout, any expectation is taken w.r.t. $\staterv[][][\prime] \doteq (\worldstaterv[][][\prime], {\typerv}^\prime, {\budgetrv}^\prime, {\supplyrv}^\prime) \sim \trans(\cdot \mid \state,  \saving)$. By Bellman's optimality principle for zero-sum games \cite{bellman1952theory, shapley1953stochastic}, the actions $(\price^*, \allocation[][][][*], \saving[][][*])(\state)$ at state $\state$ taken by the optimal policy are given by the solutions to the right hand side optimization problem in the following recursive Bellman equation for the consumption-saving problem: 
    
%         \begin{align}\label{eq:rec_eq_game}
%         \statevalue[][*](\state) \doteq
%         \min_{\substack{\price \in \pricespace}} \max_{\substack{\allocation \in \consumptions\\ \saving  \in \savings}} \price \cdot \left( \supply - \sum_{\buyer \in \buyers} \allocation[\buyer] \right) + \sum_{\buyer \in \buyers} \left(\budget[\buyer] + \saving[\buyer] \right) \log \left( \frac{\util[\buyer](\allocation[\buyer]; \type[\buyer])}{\budget[\buyer] + \saving[\buyer]}\right) + \discount \Ex\left[\statevalue[][*](\staterv[][][\prime])\right]
%     \end{align}
    
%     The optimality conditions for the above optimization are given by the KKT theorem \cite{kuhn1951kkt}:

%     \begin{align}
%         - \price^*(\state) + \left( {\budget[\buyer]}^* + \saving[\buyer][][*] \right)\frac{\subgrad[{\allocation[\buyer]}] \util[\buyer](\allocation[\buyer][][][*]; \type[\buyer])}{\util[\buyer](\allocation[\buyer][][][*]; \type[\buyer])}\doteq 0 \\ 
%         % \log \left(\frac{{\budget[\buyer]}^* + \saving[\buyer][][*]}{\util[\buyer](\allocation[\buyer][][][*]; \type[\buyer])}\right) + 1 + \gamma  \subgrad[{\assetalloc[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]\doteq 0 \\
%         \log \left(\frac{\util[\buyer](\allocation[\buyer][][][*]; \type[\buyer])}{{\budget[\buyer]}^* + \saving[\buyer][][*] }\right) - 1 + \gamma  \subgrad[{\saving[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right] \doteq 0 \\
%         \price \cdot \left( \supply - \sum_{\buyer \in \buyers} \allocation[\buyer] \right) \doteq 0 \\
%         \sum_{\buyer \in \buyers} \allocation[\buyer] \leq \supply 
%     \end{align}

%    Now, since by \Cref{lemma:convex_game} the optimal value function is concave, by applying the subdifferential envelope theorem to the left hand side of \Cref{eq:rec_eq_game} we obtain:
%    \begin{align}
%        \subgrad[{\budget[\buyer]}]\budgetval[\buyer][*](\state) \doteq \log \left(\frac{\util[\buyer](\allocation[\buyer][][][*]; \type[\buyer])}{{\budget[\buyer]}^* + \saving[\buyer][][*] }\right) - 1
%    \end{align}

% Plugging this back in above:
%     \begin{align}
%         - \price^*(\state) + \left( {\budget[\buyer]}^* + \saving[\buyer][][*] \right)\frac{\subgrad[{\allocation[\buyer]}] \util[\buyer](\allocation[\buyer][][][*]; \type[\buyer])}{\util[\buyer](\allocation[\buyer][][][*]; \type[\buyer])}\doteq 0 \\ 
%         % - \subgrad[{\budget[\buyer]}]\budgetval[\buyer][*](\state)  + \gamma  \subgrad[{\assetalloc[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]\doteq 0 \\
%         \subgrad[{\budget[\buyer]}]\budgetval[\buyer][*](\state)  + \gamma  \subgrad[{\saving[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right] \doteq 0 \\
%         \price \cdot \left( \supply - \sum_{\buyer \in \buyers} \allocation[\buyer] \right) \doteq 0 \\
%         \sum_{\buyer \in \buyers} \allocation[\buyer] \leq \supply 
%     \end{align}

%     Finally, re-organizing terms, we obtain:

%     \begin{align}
%         \frac{\util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer]) }{\budget[\buyer] + \saving[\buyer][][*]}  &= \frac{\subgrad[{\allocation[\buyer][\good]}] \util[\buyer](\allocation[\buyer][][][*](\state); \type[\buyer])}{\price[\good]^*(\state)} && \forall \good \in \goods \label{eq:bang_per_buck_game}\\ 
%         % \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state)  &= \frac{\gamma  \subgrad[{\assetalloc[\buyer][\asset]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]}{\assetprice[\asset]^*(\state)}  && \forall \asset \in \assets \\
%         \subgrad[{\budget[\buyer]}] \budgetval[\buyer][*](\state) &= - \gamma  \subgrad[{\saving[\buyer]}]\mathop{\E} \left[ \budgetval[\buyer][*](\staterv[][][\prime]) \right]\\
%         \price \cdot \left( \supply - \sum_{\buyer \in \buyers} \allocation[\buyer] \right) \doteq 0 \\
%         \sum_{\buyer \in \buyers} \allocation[\buyer] \leq \supply 
%     \end{align}

%     Additionally, note that re-organizing \Cref{eq:bang_per_buck_game}, we have:
%     \begin{align}
%         \allocation[\buyer] \cdot \price^*(\state) +   \budget[\buyer] - \saving[\buyer][][*]  = 0
%     \end{align}

%     Summing it up across all buyers, we then have:
%     \begin{align}
%         \sum_{\buyer \in \buyers} \left(\allocation[\buyer] \cdot \price^*(\state)   -  \budget[\buyer] - \saving[\buyer][][*] \right) = 0
%     \end{align}
% \end{proof}

% \thmvalueiter*
% \begin{proof}[Proof of \Cref{thm:value_iter}]
%     Line 4 of \Cref{alg:approx_bellman_operator} computes the bellman operator up to an error factor of $O(\nicefrac{1}{\sqrt{\numiters}})$. Since we can fit a concave function using a quadratic program \cite{seijo2011nonparametric}, which can be solved via stochastic gradient descent in $O(\nicefrac{1}{\sqrt{\numiters}})$ iterations/state samples, we can obtain a value function which approximates the true value function within a $O(\nicefrac{1}{\sqrt{\numiters}})$ in $\numiters$ iterations.
% \end{proof}
% % \begin{theorem}

% \thmvalueiter*
% \begin{proof}[Proof of \Cref{thm:value_iter}]
%     By \Cref{lemma:convex_game}, the action value function is concave. Hence, under affine parametrization, the state-value function becomes convex-concave and the convergence rate of stochastic gradient descent ascent in Lipschitz-smooth convex-concave min-max optimization applies \cite{nemirovski2009robust}.
% \end{proof}
% \fi




\subsection{Omitted Results and Proofs from \Cref{sec:gmg}}

\thmexistmpgne*
\begin{proof}
    % First, for our policy subspace $\subpolicies\subseteq \markovpolicies$, we define the \mydef{supporting action space} for each player $\player \in \players$ as $\subactionspace[\player] = \{ \action[\player] \in \actionspace[\player] \mid \exists \pi\in \subpolicies, \state \in \states\text{ s.t. } \policy[\player] (\state) = \action[\player] \}$ and the \mydef{supporting action profile space} by $\bigtimes_{\player \in \players} \subactionspace[\player]$. Moreover, for each player $\player \in \players$, we define the \mydef{supporting feasible action correspondence} $\subactions: \state \times \subactionspace[-\player] \rightrightarrows \subactionspace[\player]$  by $\subactions[\player] (\state, \action[-\player]) = \{ \action[\player] \in \subactionspace[\player] \mid \action[\player] \in \actions (\state, \action[-\player]) \}$, and we denote the \mydef{supporting feasible action profile correspondence} $\subactions (\state, \action) \doteq \bigtimes_{\player \in \players} \subactions[\player] (\state, \action[-\player])$.

    First, by Part 3 of \Cref{assum:existence_of_mpgne}, we know that for any $\player\in \players$, $\fpolicies[\player][{\mathrm{sub}}](\policy[-\player])$ is non-empty, convex, and compact, for all $\policy[-\player]\in \policies[-\player]$. Moreover, 2 of \Cref{assum:existence_of_mpgne}, $\fpolicies[][{\mathrm{sub}}]$ is upper-hemicontinuous.
    Therefore, by the Fan's fixed-point theorem \cite{Fan1952FixedPoint}, the set $\fpolicies[][{\mathrm{sub}}]\doteq \{\policy\in \policies[][{\mathrm{sub}}]\mid \policy\in \fpolicies[][{\mathrm{sub}}](\policy)\}$ is non-empty.
    
    For any player $\player \in \players$ and state $\state \in \states$, we define the \mydef{individual state best-response correspondence} $\brmap[\player][\state]: \subpolicies \rightrightarrows \actionspace[\player]$ by 
    \begin{align}
        % \brvalue[\player][\state] (\vfunc[\player][\policy], \action[-\player])
        % &\doteq \max_{\action[\player] \in \actions[\player] (\state, \action[-\player])} 
        % \reward[\player] (\state, \action[\player], \action[-\player]) +\Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \action[\player], \action[-\player])}[ \discount \vfunc[\player][\policy] (\staterv[][][\prime]) ] \\
        % &= \max_{\action[\player] \in \actions[\player] (\state, \action[-\player])} \qfunc[\player][\policy] (\state, \action[\player], \action[-\player]) \\
        \brmap[\player][\state] (\policy)
        &\doteq 
        \argmax_{\action[\player] \in \actions[\player] (\state, \policy[-\player] (\state))}  
        \reward[\player] (\state, \action[\player], \policy[-\player] (\state)) +\Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \action[\player], \policy[-\player] (\state))}[ \discount \vfunc[\player][\policy] (\staterv[][][\prime]) ] \\
        &= \argmax_{\action[\player] \in \actions[\player] (\state, \policy[-\player] (\state))}
        \qfunc[\player][\policy] (\state, \action[\player], \policy[-\player] (\state))
    \end{align}
    
    Then, for any player $\player \in \players$, we define the \mydef{restricted individual best-response correspondence} $\brmap[\player]: \subpolicies \rightrightarrows \subpolicies[\player]$ as the Cartesian product of individual state best-response correspondences across the states restricted to $\subpolicies$: 
    \begin{align}
        % \brvalue[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime])
        % &= \bigtimes_{\state \in \states} \brvalue[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)) \\
        \brmap[\player] (\policy)
        &= \left(\bigtimes_{\state \in \states} \brmap[\player][\state] (\policy) \right) \bigcap \subpolicies[\player]
        \\
        &= \{ \policy[\player] \in \subpolicies[\player] \mid \policy[\player] (\state) \in \brmap[\player][\state] (\policy), \forall\; \state \in \states \}
    \end{align}

    Finally, we can define the \mydef{multi-player best-response correspondence} $\brmap: \subpolicies \rightrightarrows \subpolicies$ as the Cartesian product of the individual correspondences, i.e., $\brmap(\policy) \doteq \bigtimes_{\player \in \players} \brmap[\player] (\policy)$.

    To show the existence of \MPGNE{}, we first want to show that there exists a fixed point $\policy[][][*] \in \subpolicies$ such that $\policy[][][*] \in \brmap(\policy[][][*])$. 
    To this end, we need to show that 1.~for any $\policy\in \subpolicies$, $\brmap(\policy)$ is non-empty, compact, and convex; 2.~$\brmap$ is upper hemicontinuous.
    
    Take any $\policy\in \subpolicies$. 
    Fix $\player \in \players, \state \in \states$, 
     we know that $\action[\player] \mapsto \qfunc[\player][{\policy}] (\state, \action[\player], \policy[-\player] (\state))$ is concave over $\actions[\player] (\state, \policy[-\player] (\state))$, and $\actions[\player] (\state, \policy[-\player] (\state))$ is non-empty, convex, and compact by \Cref{assum:existence_of_mpgne}, then by Proposition 4.1 of \citet{fiacco1986convexity}, $\brmap[\player][\state] (\policy)$ is non-empty, compact, and convex. 
    
    Now, notice $\bigtimes_{\state \in \states} \brmap[\player][\state] (\policy)$ is compact and convex as it is a Cartesian product of compact, convex sets. Thus, as $\subpolicies$ is also compact and convex by \Cref{assum:policy_class_exist}, we know that $\brmap[\player] (\policy) = \left(\bigtimes_{\state \in \states} \brmap[\player][\state] (\policy) \right) \bigcap \subpolicies[\player]$ is compact and convex. 
    By the assumption of \emph{closure under policy improvement} under \Cref{assum:policy_class_exist}, we know that since $\policy \in \subpolicies$, there exists $\policy[][][+] \in \subpolicies$ such that 
    $$\policy[\player][][+] \in \argmax_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])} \qfunc[\player][\policy] (\state, \policy[\player][][\prime] (\state), \policy[-\player] (\state))$$ for all $\state \in \states$, and that means $\policy[\player][][+] (\state) \in \brmap[\player][\state] (\policy)$ for all $\state \in \states$. Thus, $\brmap[\player] (\policy)$ is also non-empty. 
     Since Cartesian product preserves non-emptiness, compactness, and convexity, we can conclude that 
    $\brmap(\policy) = \bigtimes_{\player \in \players} \brmap[\player] (\policy)$ is non-empty, compact, and convex. 
    
     Similarly, fix $\player \in \players, \state \in \states$, for any $\policy\in \subpolicies$, since $\actions[\player] (\state, \cdot)$ is continuous (i.e. both upper and lower hemicontinuous), by the Maximum theorem, $\brmap[\player][\state]$ is upper hemicontinuous. 
    $\policy\mapsto \bigtimes_{\state \in \states} \brmap[\player][\state] (\policy)$ is upper hemicontinuous as it is a Cartesian product of upper hemicontinuous correspondence, and consequently, $\policy\mapsto \left(\bigtimes_{\state \in \states} \brmap[\player][\state] (\policy) \right) \bigcap \subpolicies$ is also upper hemicontinuous. 
    Therefore, $\brmap$ is also upper hemicontinuous. 

    
    
    
    
    Since $\brmap(\policy)$ is non-empty, compact, and convex for any $\policy\in \subpolicies$ and $\brmap$ is upper hemicontinuous, by Fan's fixed-point theorem \cite{Fan1952FixedPoint}, $\brmap$ admits a fixed point. 

    Finally, say $\policy[][][*] \in \subpolicies$ is a fixed point of $\brmap$, and we want to show that $\policy[][][*]$ is a generalized Markov perfect equilibrium (\MPGNE{}) of $\mgame$.  Since $\policy[][][*] \in \brmap(\policy[][][*]) = \bigtimes_{\player \in \players} \brmap[\player] (\policy[][][*])$, we know that for all $\player \in \players$, $\policy[\player][][*] (\state) \in \brmap[\player][\state] (\policy[][][*])=\argmax_{\action[\player] \in \actions[\player] (\state, \policy[-\player][][*] (\state))}
        \qfunc[\player][{\policy[][][*]}] (\state, \action[\player], \policy[-\player][][*] (\state))$. 
    We now show that for any $\player \in \players$, for any $\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])$, $\vfunc[\player][{\policy[][][*]}] (\state) \geq \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)$ for all $\state \in \states$. Take any policy $\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])$. Note that $\policy[\player]$ may not be Markov, so we denote $\{ \policy[\player] (\hist[: \numhorizon]) \}_{\numhorizon\in \N}=\{ \action[\player][][\numhorizon] \}_{\numhorizon\in \N}$. Then,
    for all $\state[0] \in \states$,
    \begin{align}
        &\vfunc[\player][{\policy[][][*]}] (\state[0]) \notag\\
        &= \qfunc[\player][{\policy[][][*]}] (\state[0], \policy[\player][][*] (\state[0]), \policy[-\player][][*] (\state[0])) \notag\\
        &=\max_{\action[\player] \in \actions[\player] (\state[0], \policy[-\player][][*] (\state[0]))}
        \qfunc[\player][{\policy[][][*]}] (\state[0], \action[\player], \policy[-\player][][*] (\state[0])) \notag \\
        &= \max_{\action[\player] \in \actions (\state[0], \policy[-\player][][*] (\state[0]))} 
        \reward[\player] (\state[0], \action[\player], \policy[-\player][][*] (\state[0])) 
        + \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player], \policy[-\player][][*] (\state[0]))}[\discount \vfunc[\player][{\policy[][][*]}] (\state[1]) ] \notag \\
        &\geq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) + \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}[\discount \vfunc[\player][{\policy[][][*]}] (\state[1]) ] \label{eq:recursive_form_v_prime}
    \end{align}
    For any $\state[0] \in \states$, define $\vfunc[\player][\prime] (\state[0]) \doteq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) 
    + \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}[\discount \vfunc[\player][{\policy[][][*]}] (\state[1]) ]$ . 
    Since $\vfunc[\player][{\policy[][][*]}] (\state) \geq \vfunc[\player][\prime] (\state)$ for all $\player \in \players$, $\state \in \states$, we have for any $\state[0] \in \states$
    \begin{align}
        &\vfunc[\player][{\policy[][][*]}] (\state[0]) \notag\\ 
        &\geq 
        \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) +
        \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))} [\discount \vfunc[\player][{\policy[][][*]}] (\state[1]) ] \notag \\
        & \geq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) +
        \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}[ \discount \vfunc[\player][\prime] (\state[1])] \notag\\
        & \geq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) \notag \\
        &+
        \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}
        \bigg[ \discount \bigg (\reward[\player] (\state[1], \action[\player][][1], \policy[-\player][][*] (\state[1])) \notag\\
        &+\Ex_{\state[2] \sim \trans (\cdot\mid \state[1], \action[\player][][1], \policy[-\player][][*] (\state[1])}[ \discount \vfunc[\player][{\policy[][][*]}] (\state[2]) ] \bigg) \bigg] \notag \\
         & \geq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) \notag \\
         &+
        \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}
        \bigg[ \discount \bigg (\reward[\player] (\state[1], \action[\player][][1], \policy[-\player][][*] (\state[1]))  \notag\\
        &+\Ex_{\state[2] \sim \trans (\cdot\mid \state[1], \action[\player][][1], \policy[-\player][][*] (\state[1])}[ \discount \vfunc[\player][\prime] (\state[2]) ] \bigg) \bigg] \notag\\
       & \vdots \label{eq:expand_v_prime}\\
       &\geq \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state) \notag
    \end{align}
    where in \Cref{eq:expand_v_prime}, we recursively expand $\vfunc[\player][\prime]$ and eliminate $\vfunc[][{\policy[][][*]}]$ using \Cref{eq:recursive_form_v_prime}. 
    We therefore conclude that for all states $\state \in \states$, and for all $\player \in \players$, $$\vfunc[\player][{\policy[][][*]}] (\state) \geq \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state) .$$ 
\end{proof}

\lemmaexploitGNE*
\begin{proof}[Proof of \Cref{lemma:exploit_GNE}]
We first prove the result for state exploitability.

($\policy[][][*]$ is a \MPGNE{} $\implies$ $\sexploit (\state, \policy[][][*]) =0$ for all $\state \in \states$): Suppose that $\policy[][][*]$ is a \MPGNE{}, i.e., for all players $\player \in \players$, $\vfunc[\player][{\policy[][][*]}] (\state) \geq \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)$ for all state $\state \in \states$. Then, for all state $\state \in \states$, we have
\begin{align}
    \forall \player \in \players, \; \; \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state) - \vfunc[\player][{\policy[][][*]}] (\state) =0
\end{align}
Summing up across all players $\player \in \players$, we get
\begin{align}
    \sexploit (\state, \policy[][][*])
    = \sum_{\player \in \players} \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state) - \vfunc[\player][{\policy[][][*]}] (\state) =0
\end{align}

($\sexploit (\state, \policy[][][*]) =0$ for all $\state \in \states$ $\implies$ $\policy[][][*]$ is a \MPGNE{}):
Suppose we have $\policy[][][*] \in \fmarkovpolicies (\policy[][][*])$ and $\sexploit (\state, \policy[][][*]) =0$ for all $\state \in \states$. That is, for any $\state \in \states$
\begin{align}
     \sexploit (\state, \policy[][][*])
    = \sum_{\player \in \players} \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state) - \vfunc[\player][{\policy[][][*]}] (\state) =0.
\end{align}
Since for any $\player \in \players$, $\policy[\player][][*] \in \fmarkovpolicies[\player] (\policy[-\player][][*])$,  $\max_{\policy[\player] \in \fmarkovpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state) - \vfunc[\player][{\policy[][][*]}] \geq \vfunc[\player][{\policy[][][*]}] (\state)-\vfunc[\player][{\policy[][][*]}] (\state) =0$. As a result, we must have for all player $\player \in \players$,
\begin{align}
    \vfunc[\player][{\policy[][][*]}] (\state) = \max_{\policy[\player] \in \fpolicies (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state), \; \; \forall \state \in \states
\end{align}
Thus, we can conclude that $\policy[][][*]$ is a \MPGNE{}.

Then, we can prove results for exploitability in an analogous way. 

($\policy[][][*]$ is a GNE $\implies$ $\gexploit (\policy[][][*]) =0$ ): Suppose that $\policy[][][*]$ is a GNE, i.e., for all players $\player \in \players$, $\payoff[\player] (\policy[][][*]) \geq \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \payoff[\player] (\policy[\player], \policy[-\player][][*])$. Then, we have
\begin{align}
    \forall \player \in \players, \; \; \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \payoff[\player] (\policy[\player], \policy[-\player][][*]) - \payoff[\player] (\policy[][][*]) =0
\end{align}
Summing up across all players $\player \in \players$, we get
\begin{align}
    \gexploit (\policy[][][*])
    = \sum_{\player \in \players} \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \payoff[\player] (\policy[\player], \policy[-\player][][*]) - \payoff[\player] (\policy[][][*]) =0
\end{align}

($\gexploit (\state, \policy[][][*]) =0$ $\implies$ $\policy[][][*]$ is a GNE):
Suppose we have $\policy[][][*] \in \fpolicies (\policy[][][*])$ and $\gexploit (\policy[][][*]) =0$. That is, 
\begin{align}
     \gexploit (\policy[][][*])
    = \sum_{\player \in \players} \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \payoff[\player] (\policy[\player], \policy[-\player][][*]) - \payoff[\player] (\policy[][][*]) =0.
\end{align}
Since for any $\player \in \players$, $\policy[\player][][*] \in \fpolicies[\player] (\policy[-\player][][*])$,  $\max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \payoff[\player] (\policy[\player], \policy[-\player][][*]) - \payoff[\player] (\policy[][][*]) \geq \payoff[\player] (\policy[][][*]) - \payoff[\player] (\policy[][][*]) =0$. As a result, we must have for all player $\player \in \players$,
\begin{align}
    \payoff[\player] (\policy[][][*]) = \max_{\policy[\player] \in \fpolicies (\policy[-\player][][*])} \payoff[\player] (\policy[\player], \policy[-\player][][*])
\end{align}
Thus, we can conclude that $\policy[][][*]$ is a GNE.
\end{proof}


\obsminmax*
\begin{proof}
    The per-player maximum operator can be pulled out of the sum in the definition of state-exploitability, because the $\player$th player's best-response policy is independent of the other players' best-response policies, given a fixed policy profile $\policy$:
    \begin{align}
        \forall\; \state \in \states, 
        \; \; \sexploit (\state, \policy)
        &= \sum_{\player \in \players}
        \max_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])} \vfunc[\player][{(\policy[\player][][\prime], \policy[-\player])}] (\state) - \vfunc[\player][\policy] (\state) \\
        &= \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \sum_{\player \in \players} \vfunc[\player][{(\policy[\player][][\prime], \policy[-\player])}] (\state) - \vfunc[\player][\policy] (\state) \\
        &= \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \scumulreg (\state, \policy, \policy[][][\prime])
    \end{align}
The argument is analogous for exploitability:
    \begin{align}
        \gexploit (\policy)
        &= \sum_{\player \in \players}
        \max_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])} \payoff[\player] (\policy[\player][][\prime], \policy[-\player]) - \payoff[\player] (\policy) \\
        &= \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \sum_{\player \in \players} \payoff[\player] (\policy[\player][][\prime], \policy[-\player]) - \payoff[\player] (\policy) \\
        &= \max_{\policy[][][\prime] \in \fpolicies (\policy)} \gcumulreg (\policy, \policy[][][\prime])
    \end{align}
\end{proof}




\lemmafullsupport*
\begin{proof}
% Suppose that $\max_{\param[][][\prime] \in \params} \langle \grad \gexploit(\param), \param - \param[][][\prime] \rangle \leq  0$, then, for all $\param[][][\prime] \in \params$, we have:
% \begin{align*}
%     0 &\geq \max_{\param[][][\prime] \in \params}\langle \grad \gexploit(\param), \param - \param[][][\prime] \rangle\\
%     &= \max_{\param[][][\prime] \in \params} \langle \grad[\param] \Ex_{\state \sim \initstates} \left[  \sexploit (\state, \param)\right], \param - \param[][][\prime] \rangle\\
%     &= \max_{\param[][][\prime] \in \params} \Ex_{\state \sim \initstates} \left[ \langle \grad[\param] \sexploit (\state, \param), \param - \param[][][\prime] \rangle\right]\\
%     &=  \Ex_{\state \sim \initstates} \left[ \max_{\param[][][\prime] \in \params} \langle \grad[\param] \sexploit (\state, \param), \param - \param[][][\prime] \rangle\right]
% \end{align*}
% where the last equality follows from the existence  \deni{COMPLETE!!!}

% $\max_{\param[][][\prime] \in \params} \langle \grad \sexploit(\state, \param), \param - \param[][][\prime] \rangle \leq 0$,  $\initstates$-almost surely, i.e., $\initstates (\{ \state \in \states \mid \max_{\param[][][\prime] \in \params} \langle \grad \sexploit(\state, \param), \param - \param[][][\prime] \rangle \leq 0 \}) = 1$. 

% Moreover, for any $\varepsilon > 0$ and $\alpha \in [0, 1]$,  if $\supp (\initstates) = \states$ and $\max_{\param[][][\prime] \in \params} \langle \grad \gexploit(\param), \param - \param[][][\prime] \rangle \leq \varepsilon$, then for all states $\state \in \states$, with probability at least $1-\alpha$, $\max_{\param[][][\prime] \in \params} \langle \grad \sexploit(\state, \param), \param - \param[][][\prime] \rangle  \leq \nicefrac{\varepsilon}{\alpha}$. 

    First, using Jensen's inequality, by the convexity of the $2$-norm $\| \cdot \|$, we have: 
    \begin{align*}
    \Ex_{\state \sim \initstates} \left[ \| \grad[\param] \sexploit (\state, \param) \| \right] 
    &\leq \left\| \Ex_{\state \sim \initstates} \left[ \grad[\param] \sexploit (\state, \param) \right] \right\| \, \\
    &= \left\|   \grad[\param] \Ex_{\state \sim \initstates} \left[\sexploit (\state, \param) \right] \right\| \, \\
    &= \| \grad[\param] \exploit (\param) \|  \enspace .   
    \end{align*}
    
    The first claim follows directly from the fact that for all $\state \in \states$, $\|\grad[\param] \gexploit (\state, \param)\| \geq 0$, and hence for the expectation $ \Ex_{\state \sim \initstates} \left[ \| \grad[\param] \gexploit (\state, \param) \| \right]$ to be equal to $0$, its value should be equal to zero on a set of measure 1.

    Now, for the second part, by Markov's inequality, we have:
%\begin{align}
    $\Pr \left( \| \grad[\param] \sexploit (\state, \param) \| \, \geq \nicefrac{\varepsilon}{\delta} \right) \leq \frac{\Ex_{\state \sim \initstates} \left[ \| \grad[\param] \sexploit (\state, \policy) \| \right]} {\nicefrac{\varepsilon}{\delta}} \leq \frac{\varepsilon} {\nicefrac{\varepsilon}{\delta}} = \delta$.
%    \enspace .
%\end{align}
\end{proof}



\lemmabrmismatch*
\begin{proof}
In this proof, for any $\player\in \players$,
we define $\deparam[\player](\param)=\depolicy[\player](\cdot, \policy(\cdot;\param); \deparam)$ as player $\player$'s policy in the policy profile $\deparam(\param)=\depolicy(\cdot, \policy(\cdot;\param); \deparam)$. Similarly, we define $\param[\player]=\policy[\player](\cdot;\param)$ as player $\player$'s policy in the policy profile $\param=\policy(\cdot; \param)$.

Given a policy parametrization scheme $(\policy, \depolicy, \params, \deparams)$,  consider any two parameters $\param \in \params, \deparam \in \deparams$, and any two initial state distributions  $\initstates, \diststates \in \simplex(\states)$, we know that
\begin{align}
    &\left\|\grad[\param] \scumulreg(\diststates, \param, \deparam) \right\|\\
    &=\left\|\grad[\param] \sum_{\player\in \players} \payoff[\player]( \deparam[\player](\param), \param[-\player])
    -\payoff[\player](\param)\right\|\\
    &=  \left\|\sum_{\player\in \players} \grad[\param] (\payoff[\player](\deparam[\player](\param), \param[-\player])-\payoff[\player](\param))\right\|\\
    &=\left\|\sum_{\player\in \players} \grad[\param]\left[
    \E_{\substack{\state[][][\prime]\sim \statedist[\diststates][{(\deparam[\player](\param), \param[-\player])}]\\
    \state \sim \statedist[\diststates][\param]}}
    \left[ \reward[\player](\state[][][\prime], \depolicy[\player](\state[][][\prime], \policy(\state;\param); \deparam), 
    \policy[-\player](\state[][][\prime]; \param) 
    - \reward[\player](\state, \policy(\state; \param))
    \right] \right]\right\|\\
    &= \bigg\|\sum_{\player\in \players}
    \E_{\substack{\state[][][\prime]\sim \statedist[\diststates][{(\deparam[\player](\param), \param[-\player])}]\\
    \state \sim \statedist[\diststates][\param]}}
    \bigg[
    \grad[{\action[-\player]}] \qfunc[\player][{\deparam[\player](\param), \param[-\player]}]    (\state[][][\prime], 
    \depolicy[\player](\state[][][\prime], \policy(\state[][][\prime];\param); \deparam), \policy[-\player](\state[][][\prime];\param)) 
    \grad[{\param}] \left( \depolicy[\player](\state[][][\prime], \policy[-\player](\state[][][\prime]; \param); \param), \policy(\state[][][\prime]; \param) \right)
    \nonumber\\
    &- \grad[\action] \qfunc[\player][\param](\state, \policy(\state; \param)) \grad[\param]\policy(\state; \param)
    \bigg] \label{eq:first_dpg} \bigg\|\\
    &\leq \max_{\player\in \players}
    \max_{\statep, \state\in \states}
    \frac{\statedist[\diststates][{(\deparam[\player](\param), \param[-\player])}] (\statep)  
    \statedist[\diststates][\param](\state)}{\statedist[\initstates][{(\deparam[\player](\param), \param[-\player])}](\statep) \statedist[\initstates][\param](\state)} %%%% constant ends here
    \bigg\|\E_{\substack{\state[][][\prime]\sim \statedist[\initstates][{(\deparam[\player](\param), \param[-\player])}]\\
    \state\sim \statedist[\initstates][\param]}}
    \bigg[
    \grad[{\action[-\player]}] \qfunc[\player][{\deparam[\player](\param), \param[-\player]}]    (\state[][][\prime], 
    \depolicy[\player](\state[][][\prime], \policy(\state[][][\prime];\param); \deparam), \policy[-\player](\state[][][\prime];\param)) 
    \nonumber\\
    &\quad\quad 
    \grad[{\param}] \left( \depolicy[\player](\state[][][\prime], \policy[-\player](\state[][][\prime]; \param); \param), \policy(\state[][][\prime]; \param) \right)
    -  \grad[\action] \qfunc[\player][\param](\state, \policy(\state; \param)) \grad[\param]\policy(\state; \param)
    \bigg] \bigg\|\\
    & \leq \max_{\player\in \players}\max_{\statep, \state\in \states}
    \frac{\statedist[\diststates][{(\deparam[\player](\param), \param[-\player])}](\statep) \statedist[\diststates][\param](\state)}{\statedist[\initstates][{(\deparam[\player](\param), \param[-\player])}](\statep) \statedist[\initstates][\param](\state)}%%%% constant ends here
    \left\|\grad[\param] \left[
    \vfunc[\player][{\deparam[\player](\param), \param[-\player]}](\initstates)- \vfunc[\player][\param](\initstates)\right] \right\|
    \label{eq:second_dpg}\\
    % &\leq \max_{\player\in \players}\max_{\statep, \state\in \states}
    % \frac{\statedist[\diststates][{(\deparam[\player](\param), \param[-\player])}](\statep) \statedist[\diststates][\param](\state)}{\statedist[\initstates][{(\deparam[\player](\param), \param[-\player])}](\statep) \statedist[\initstates][\param](\state)}%%%% constant ends here
    % \sum_{\player\in\player}\grad[\param] \left[
    % \vfunc[\player][{\deparam[\player](\param), \param[-\player]}](\initstates)- \vfunc[\player][\param](\initstates)\right]\\
   %  &=\max_{\player\in \players}\max_{\statep, \state\in \states}
   %  \frac{\statedist[\diststates][{(\deparam[\player](\param), \param[-\player])}](\statep) \statedist[\diststates][\param](\state)}{\statedist[\initstates][{(\deparam[\player](\param), \param[-\player])}](\statep) \statedist[\initstates][\param](\state)}
   % \grad[\param] \sum_{\player\in\player} \left[
   %  \vfunc[\player][{\deparam[\player](\param), \param[-\player]}](\initstates)- \vfunc[\player][\param](\initstates)\right]\\
    & \leq \left(\frac{1}{1-\discount}\right)^2 
    \max_{\player\in \players}\max_{\statep, \state\in \states}\frac{\statedist[\diststates][{(\deparam[\player](\param), \param[-\player])}](\statep) \statedist[\diststates][\param](\state)}{\initstates(\statep)\initstates(\state)} %%% constant end here
    \left\|\grad[\param] \scumulreg(\initstates, \param, \deparam) \right\|
    \label{eq:back_to_initial_dist}\\
    &= \left(\frac{1}{1-\discount}\right)^2 
    \max_{\player\in \players}
    \left\Vert\frac{\statedist[\diststates][{(\deparam[\player](\param), \param[-\player])}]}{\initstates}\right\Vert_{\infty}
    \left\Vert\frac{\statedist[\initstates][\param]}{\initstates}\right\Vert_\infty
    \left\|\grad[\param] \scumulreg(\initstates, \param, \deparam)\right\|
\end{align}
where \Cref{eq:first_dpg} and \Cref{eq:second_dpg} are obtained by deterministic policy gradient theorem \cite{silver2014deterministic}, and \Cref{eq:back_to_initial_dist} is due to the fact that $\statedist[\initstates][\param](\state)\geq (1-\discount)\initstates(\state)$ for any $\policy\in \policies$, $\state\in \states$. 

Given condition (1) of \Cref{assum:param_gradient_dominance}, fix any $\param\in\params$, there exists $\deparam[][][*]\in \deparams$ s.t. for all $\player\in \players$, $\state\in \states$: $$\qfunc[\player][\param](\state, \depolicy[\player](\state, \policy(\state;\param);\deparam[][][*]), \policy[-\player](\state;\param)) 
= \max_{\policy[\player][][\prime]\in \fpolicies[\player](\policy(\cdot;\param))} 
\qfunc[\player][\param](\state, \policy[\player][][\prime](\state), \policy[-\player](\state;\param)) \enspace .
$$  Thus, $\sexploit(\state, \param)=\scumulreg(\state, \param, \deparam[][][*])$ for all $\state\in \states$. Hence, plugging in the optimal best-response policy $\deparam = \deparam[][][*]$, we obtain that \begin{align}
    \|\grad[\param] \sexploit(\diststates, \param)\|
    &\leq  \left(\frac{1}{1-\discount}\right)^2 
    \max_{\player\in \players}
    \left\Vert\frac{\statedist[\diststates][{(\deparam[\player][][*](\param), \param[-\player])}]}{\initstates}\right\Vert_{\infty}
    \left\Vert\frac{\statedist[\initstates][\param]}{\initstates}\right\Vert_\infty
    \|\grad[\param]  \sexploit(\initstates, \param)\|\\
    &\leq \left(\frac{1}{1-\discount}\right)^2 
    \max_{\player\in \players}
    \max_{\policyp[\player] \in \brmap[\player](\policy[-\player](\cdot;\param) )}
    \left\Vert\frac{\statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player](\cdot;\param))}]}{\initstates}\right\Vert_{\infty}
    \left\Vert\frac{\statedist[\initstates][\param]}{\initstates}\right\Vert_\infty
     \|\grad[\param] \sexploit(\initstates, \param)\| \label{eq:max_over_best_response}
\end{align}
where \cref{eq:max_over_best_response} is due to the fact that $\deparam[\player][][*](\param)\in \brmap[\player](\policy[-\player](\cdot;\param) )$.



\if 0
\begin{align*}
    \| \grad[\param] \regsexploit(\diststates, \param)\| = \| \grad[\param] \min_{\param[][][\prime] \in \params} \{ \sexploit(\diststates, \param[][][\prime]) + \lipschitz[{\grad \scumulreg}] \| \param - \param[][][\prime] \|^2 \}\| \\
    = \| \grad[\param] \min_{\param[][][\prime] \in \params} \{ \Ex_{\state \sim \diststates} \left[\sexploit(\state, \param[][][\prime]) \right] + \lipschitz[{\grad \scumulreg}] \| \param - \param[][][\prime] \|^2 \}\| \\
    \leq \| \grad[\param] \Ex_{\state \sim \diststates} \left[ \min_{\param[][][\prime] \in \params} \{ \sexploit(\state, \param[][][\prime])  + \lipschitz[{\grad \scumulreg}] \| \param - \param[][][\prime] \|^2 \} \right]\| \\
    = \|\grad[\param] \max_{\deparam \in \deparams} \scumulreg(\diststates, \param, \deparam)\|
\end{align*}
Further, notice that by Jensen's inequality, and the convexity of the $2$-norm $\| \cdot \|$, we have: 
    \begin{align*}
    \Ex_{\state \sim \initstates} \left[ \| \grad[\param] \regsexploit (\state, \param) \| \right] 
    &\leq \left\| \Ex_{\state \sim \initstates} \left[ \grad[\param] \regsexploit (\state, \param) \right] \right\| \, \\
    &= \left\|   \grad[\param] \Ex_{\state \sim \initstates} \left[\regsexploit (\state, \param) \right] \right\| \, \\\\
    &= \left\|   \grad[\param] \Ex_{\state \sim \initstates} \left[\min_{\param[][][\prime] \in \params} \left\{ \gexploit (\state, \param[][][\prime]) + \lipschitz[{\grad \gcumulreg}] \left\| \param - \param[][][\prime] \right\|^2\right\} \right] \right\|\\
    &\leq \left\|   \grad[\param] \min_{\param[][][\prime] \in \params} \left\{\Ex_{\state \sim \initstates} \left[  \gexploit (\state, \param[][][\prime])\right] + \lipschitz[{\grad \gcumulreg}] \left\| \param - \param[][][\prime] \right\|^2\right\}  \right\|\\
    &\leq \left\| \grad[\param] \min_{\param[][][\prime] \in \params} \left\{\gexploit (\param[][][\prime]) + \lipschitz[{\grad \gcumulreg}] \left\| \param - \param[][][\prime] \right\|^2\right\}  \right\|\\
    &= \| \grad[\param] \regexploit (\param) \|  \enspace .   
    \end{align*}

Finally, 
\fi

\end{proof}

\lemmauncoupledminmax*
\begin{proof}
    Fix $\policy[][][*] \in \fmarkovpolicies(\policy[][][*])$.
    We want to show that $$\max_{\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])} \gexploit (\policy[][][*], \policy[][][\prime]) = \max_{\depolicy \in \depolicies} \gexploit (\policy[][][*], \depolicy (\cdot, \policy (\cdot))) \enspace .$$ 
    Define $\policies[][{\depolicies, \policy[][][*]}] \doteq \{ \policy: \state\mapsto \depolicy (\state, \policy[][][*] (\state)) \mid \depolicy \in \depolicies \} \subseteq \markovpolicies$. 
    
    First, for all $\policy[][][\prime] \in \policies[][{\depolicies, \policy[][][*]}]$, $\policy[][][\prime] (\state) = \depolicy (\state, \policy[][][*] (\state)) \in \actions (\state, \policy[][][*] (\state))$, for all $\state \in \states$, by the definition of $\depolicies$.
    Thus, $\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*]) = \{ \policy \in \markovpolicies\mid \forall\state \in \states, \policy (\state) \in \actions (\state, \policy[][][*] (\state)) \}$.
    Therefore, $\policies[][{\depolicies, \policy[][][*]}] \subseteq \fmarkovpolicies (\policy[][][*])$, which implies that $\max_{\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])} \gexploit (\policy[][][*], \policy[][][\prime])
    \geq \max_{\policy[][][\prime] \in \policies[][{\depolicies, \policy[][][*]}]} \gexploit (\policy[][][*], \policy[][][\prime])
    = \max_{\depolicy \in \depolicies} \gexploit (\policy[][][*], \depolicy (\cdot, \policy (\cdot)))$. 

    Moreover, for all $\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])$, $\policy[][][\prime] (\state) \in \actions (\state, \policy[][][*] (\state))$, for all $\state \in \states$, by the definition of $\fmarkovpolicies$. 
    %\sadie{$\fmarkovpolicies(\policy[][][*])=\{\policy\in \markovpolicies\mid \policy(\state)\in \actions(\state, \policy[][][*](\state))\})$ embodies the constraints by definition!}
    Define $\depolicy[][][\prime]$ such that for all $\state \in \states$, $\depolicy[][][\prime] (\state, \action) = \policy[][][\prime] (\state)$ if $\action = \policy[][][*] (\state)$, and $\depolicy[][][\prime] (\state, \action) = \action[][][][\prime]$ for some $\action[][][][\prime] \in \actions (\state, \action)$ otherwise.
    %if $\action\neq\policy[][][*] (\state)$.
    Note that $\depolicy[][][\prime] \in \depolicies$, since $\forall (\state, \action) \in \states\times \actionspace, \; \depolicy (\state, \action) \in \actions (\state, \action)$. Thus, as $\policy[][][\prime] (\state) = \depolicy[][][\prime] (\state, \policy[][][*] (\state))$, for all $\state \in \states$, it follows that $\policy[][][\prime] \in \policies[][{\depolicies, \policy[][][*]}]$. 
    Therefore, $\fmarkovpolicies (\policy[][][*]) \subseteq \policies[][{\depolicies, \policy[][][*]}]$, which implies that $\max_{\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])} \gexploit (\policy[][][*], \policy[][][\prime])
    \leq \max_{\policy[][][\prime] \in \policies[][{\depolicies, \policy[][][*]}]} \gexploit (\policy[][][*], \policy[][][\prime])
    = \max_{\depolicy \in \depolicies} \gexploit (\policy[][][*], \depolicy (\cdot, \policy (\cdot)))$. 

    Finally, we conclude that $\max_{\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])} \gexploit (\policy[][][*], \policy[][][\prime]) = \max_{\depolicy \in \depolicies} \gexploit (\policy[][][*], \depolicy (\cdot, \policy (\cdot)))$. 
\end{proof}


\thmconvergence*
\begin{proof}
As is common in the optimization literature (see, for instance, \citet{davis2018subgradient}), %instead of minimizing exploitability, to obtain a Lipschitz-smooth merit function for GNE, 
we consider the Moreau envelope of the exploitability, which we simply call the \mydef{Moreau exploitability}, i.e., $$\regexploit (\param) \doteq \min_{\param[][][\prime] \in \params} \left\{ \gexploit (\param[][][\prime]) + \lipschitz[{\grad \scumulreg}] \left\| \param - \param[][][\prime] \right\|^2\right\} \enspace .$$ 
Similarly, we also consider the \mydef{state Moreau exploitability}, i.e., the Moreau envelope of the state exploitability: $$\regsexploit (\state, \param) \doteq \min_{\param[][][\prime] \in \params} \left\{ \sexploit (\state, \param[][][\prime]) + \lipschitz[{\grad \scumulreg}] \left\| \param - \param[][][\prime] \right\|^2\right\} \enspace .$$
We recall that in these definitions, by our notational convention, $\lipschitz[{\grad \scumulreg}] \geq 0$, refers to the Lipschitz-smoothness constants of the state exploitability which in this case we take to be the largest across all states, i.e., for all $\state \in \states$, $(\param, \deparam) \mapsto \scumulreg(\state, \param, \deparam)$ is $\lipschitz[{\grad \scumulreg}]$-Lipschitz-smooth, respectively, and which we note is guaranteed to exist under \Cref{assum:param_lipschitz}. Further, we note that since $\gcumulreg(\param, \deparam) = \Ex_{\state \sim \initstates} \left[\scumulreg(\state, \param, \deparam) \right]$ is a weighted average of $\scumulreg$, $(\param, \deparam) \mapsto \gcumulreg(\param, \deparam)$ is also $\lipschitz[{\grad \scumulreg}]$-Lipschitz-smooth.

We invoke Theorem 2 of \citeauthor{daskalakis2020independent}
(\citeyear{daskalakis2020independent}).
Although their result is stated for gradient-dominated-gradient-dominated functions, their proof applies in the more general case of non-convex-gradient-dominated functions.
    
First, \Cref{assum:param_lipschitz} guarantees that the cumulative regret $\gcumulreg$ is Lipschitz-smooth w.r.t.\@ $(\param, \deparam)$.
    Moreover, under \Cref{assum:param_lipschitz}, which guarantees that $\deparam \mapsto \qfunc[\player][{\param[][][\prime]}] (\state, \depolicy[\player] (\state, \policy[-\player] (\state; \param); \deparam), 
    \policy[-\player] (\state; \param) 
    )$ is continuously differentiable for all $\state \in \states$ and $\param, \param[][][\prime] \in \params$, and \Cref{assum:param_gradient_dominance}, we have that $\gcumulreg$ is $\left(\nicefrac{\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|_\infty}{1-\discount} \right)$-gradient-dominated in $\deparam$, for all $\param \in \params$, by Theorems 2 and 4 of \citeauthor{bhandari2019global} (\citeyear{bhandari2019global}). 
    Finally, under \Cref{assum:param_lipschitz}, since the policy, the reward function, and the transition probability function are all Lipschitz-continuous, $\estpayoff$, $\estgcumulreg$, and hence $\estG$ are also Lipschitz-continuous, since $\states$ and $\actionspace$ are compact. 
    Their variance must therefore be bounded, i.e., there exists $\varconst[\param], \varconst[\deparam] \in \R$ s.t.\@ $\Ex_{\hist, \hist[][\prime]}[\estG[\param] (\param, \deparam; \hist, \hist[][\prime])- \grad[\param] \gcumulreg(\param, \deparam; \hist, \hist[][\prime])] \leq \varconst[\param]$ and $\Ex_{\hist, \hist[][\prime]}[\estG[\deparam] (\param, \deparam; \hist, \hist[][\prime])- \grad[\deparam] \gcumulreg(\param, \deparam; \hist, \hist[][\prime])] \leq \varconst[\deparam]$. 

    Hence, under our assumptions, the assumptions of Theorem 2 of \citeauthor{daskalakis2020independent} are satisfied.
    Therefore,
    %\begin{align}
        $\nicefrac{1}{\numiters  + 1} \sum_{\numhorizon = 0}^\numiters \|\grad \regulexploit(\param[][][(\numhorizon)]) \|\leq \varepsilon$.
    %\enspace .
    %\end{align}
    Taking a minimum across all $\numhorizon \in [\numiters]$, 
    %\samy{}{to identify the best iterate}, 
    we conclude $\left\|\grad \regulexploit (\bestiter[{\param}][\numiters]) \right\| \leq \varepsilon$.
    %\begin{align}
        %$\min_{t = 0, 1, \cdots, \numiters} \|\grad \regulexploit(\param[][][(\numhorizon)]) \|\leq \varepsilon$.
    %\enspace .
    %\end{align}
    % The second part is then a direct consequence of the hoeffding bound, whose assumptions are satisfied since the objective is bounded from above and from below by 0, as the objective is continuous and its domain is non-empty, and compact.
    
    Then, by the Lemma 3.7 of \cite{lin2020gradient}, there exists some $\param[][][*]\in \params$ such that $\|\bestiter[{\param}][\numiters]-\param[][][*]\|\leq \frac{\varepsilon}{2\lipschitz[\gcumulreg]}$ and $\param[][][*]\in \params_\varepsilon\doteq\{\param\in \params\mid \exists \alpha\in \subdiff\gexploit(\param), \|\alpha\|\leq \varepsilon\}$. 
    That is, $\bestiter[{\param}][\numiters]$ is a $(\varepsilon, \frac{\varepsilon}{2\lipschitz[\gcumulreg]})$-stationary point of $\gexploit$.

    \if 0
    Note that since $\gcumulreg$ is Lipschitz-smooth, $\gexploit$ is weakly convex and $\subdiff\gexploit(\param)=\subdiff \obj(\param) - \lipschitz[\gcumulreg] \param$ where $\obj(\param) \doteq \max_{\deparam\in \deparams}\{\gcumulreg(\param, \deparam) + \nicefrac{\lipschitz[\gcumulreg]}{2}\|\param\|^2\}$. Since  $\gcumulreg(\param, \deparam) + \nicefrac{\lipschitz[\gcumulreg]}{2}\|\param\|^2$ is convex in $\param$ for each $\deparam\in \deparams$ and $\deparams$ is compact, Danskin's theorem implies that $\grad[\param]\gcumulreg(\param, \deparam[][][*]) + \lipschitz[\gcumulreg]\param \in \subdiff \obj(\param)$, for any $\deparam[][][*]\in \argmax_{\deparam\in \deparams}\gcumulreg(\param, \deparam)$. Putting these piece together yields that $\grad[\param]\gcumulreg(\param, \deparam[][][*])\in \subdiff \gexploit(\param)$ for any $\deparam[][][*]\in \argmax_{\deparam\in \deparams}\gcumulreg(\param, \deparam)$. Hence, 
    \begin{align}
        \param[][][*]\in \params_\varepsilon
        &\doteq\{\param\in \params\mid \exists \alpha\in \subdiff\gexploit(\param), \|\alpha\|\leq \varepsilon\}\\
        &\supseteq\{\param\in \params\mid \exists \deparam[][][*]\in \argmax_{\param\in \params}\gcumulreg(\param, \deparam) s.t. \|\grad[\param]\gcumulreg(\param, \deparam[][][*])\|\leq \varepsilon \}
    \end{align}
    \fi

    Furthermore, if we assume that $\sexploit(\statedist, \cdot)$ is differentiable at $\param[][][*]$ for any state distribution $\statedist\in \Delta(\states)$, $\gexploit$ is also differentiable at $\param[][][*]$.
    Hence, by the proof of \Cref{lemma:br_mismatch_coef}, we know that for any state distribution $\diststates\in \simplex(\states)$,
    \begin{align}
        \|\grad[\param]\sexploit(\diststates, \param)\|
        &\leq 
        \max_{\deparam[][][*]\in \argmax_{\deparam\in \deparams}\scumulreg(\diststates, \param,\deparam)}\|\grad[\param]\scumulreg(\diststates, \param, \deparam[][][*])\|\\
        &\leq \max_{\player\in \players}\max_{\deparam[][][*]\in \argmax_{\deparam\in \deparams}\scumulreg(\diststates, \param,\deparam)}\\
        &\left(\frac{1}{1-\discount}\right)^2 \left\Vert\frac{\statedist[\diststates][{\deparam[\player][][*](\param), \param[-\player]}]}{\initstates}\right\Vert_\infty
        \left\Vert\frac{\statedist[\diststates][{\param}]}{\initstates}\right\Vert_\infty \|\grad[\param]\gcumulreg(\param, \deparam[][][*])\|\\
       &=\brmismatch(\param, \initstates, \diststates)\|\grad[\param]\gcumulreg(\param, \deparam[][][*])\|
       \end{align}
       \begin{align}
       \frac{1}{\brmismatch(\param, \initstates, \diststates)} \|\grad[\param]\sexploit(\diststates, \param)\|
       \leq \|\grad[\param]\gcumulreg(\param, \deparam[][][*])\| 
    \end{align}
Therefore, 
\begin{align}
        \param[][][*]\in \params_\varepsilon
        &\doteq\{\param\in \params\mid \exists \alpha\in \subdiff\gexploit(\param), \|\alpha\|\leq \varepsilon\}\\
        &\supseteq\{\param\in \params\mid \exists \deparam[][][*]\in \argmax_{\param\in \params}\gcumulreg(\param, \deparam) s.t. \|\grad[\param]\gcumulreg(\param, \deparam[][][*])\|\leq \varepsilon\}\\
        &\supseteq \{\param\in \params\mid \nicefrac{1}{\brmismatch(\param, \initstates, \diststates)} \|\grad[\param]\sexploit(\diststates,\param)\| 
        \leq \varepsilon\}\\
        &=\{\param\in \params\mid \|\grad[\param]\sexploit(\diststates,\param)\| 
        \leq \delta\}
    \end{align}
    Therefore, we can conclude that there exists $\param[][][*]$ such that $\|\bestiter[\param][T]-\param[][][*]\|\leq \frac{\varepsilon}{2\lipschitz[\gcumulreg]}$ and $\|\grad[\param]\sexploit(\diststates, \param)\|\leq \delta$ for any $\diststates$. Thus, $\bestiter[\param][T]$ is a $(\varepsilon, \delta)$-stationary point of $\sexploit(\diststates, \cdot)$ for any $\diststates\in \simplex(\states)$.
    
\end{proof}



\subsection{Omitted Results and Proofs from \Cref{sec:inf_eqa}}\label{sec_app:stochastc_exchange_economy}


\thmexistRRE*
\begin{proof}
    Let $\policy[][][*]=(\consumption[][][][*], \portfolio[][][][*], \price[][][*], \assetprice[][][*]): \states \to \consumptions \times \portfoliospace \times \pricespace \times \assetpricespace $ be an \MPGNE{} of the Radner Markov pseudo-game $\mgame$ associated with $\economy$. 
    We want to show that it is also an RRE of $\economy$.

    First, we want to show that $\policy[][][*]$ is Markov perfect for all consumers. 
    We can make some easy observations: the state value for the player $\buyer\in \buyers$ in the Radner Markov pseudo-game at state $\state\in \states$ induced by the policy $\policy[][][*]$
    \begin{align}
        \vfunc[\buyer][{\policy[][][*]}] (\state)&=
        \Ex_{\histrv\sim \histdistrib[][{\policy[][][*]}]} \left[
        \sum_{\numhorizon=0}^{\infty}\discount^\numhorizon \newreward(\staterv[\numhorizon], \actionrv[][][\numhorizon]) \mid \staterv[0]=\state)
        \right]
        \\&= 
        \Ex_{\histrv\sim \histdistrib[][{\policy[][][*]}]} \left[
        \sum_{\numhorizon=0}^{\infty}\discount^\numhorizon \util[\buyer] (\consumption[\buyer][][][*] (\staterv[\numhorizon]); \typerv[\buyer][][\numhorizon]) \mid \staterv[0]=\state)
        \right]
    \end{align}
    is equal to the consumption state value induced by $(\consumption[][][][*], \portfolio[][][][*], \price[][][*], \assetprice[][][*])$
    \begin{align}
        \vfunc[\buyer][{(\consumption[][][][*], \portfolio[][][][*], \price[][][*], \assetprice[][][*])}] (\state) \doteq \Ex_{\histrv \sim \histdistrib[][{( \consumption[][][][*], \portfolio[][][][*], \price[][][*], \assetprice[][][*])}]} \left[ \sum_{\numhorizon = 0}^\infty \discount^\numhorizon \util[\buyer] \left( \allocation[\buyer][][][*] (\histrv[:\numhorizon][][]); \typerv[][][\numhorizon] \right) \mid \staterv[0] = \state \right]  \enspace .
    \end{align}
    as $\consumption[\player][][][*]$ is Markov. Since $\policy[][][*]$ is a \MPGNE{}, we know that for any $\buyer\in \buyers$:
    $$(\allocation[\buyer][][][*], \portfolio[\buyer][][][*]) \in \argmax_{\substack{(\allocation[\buyer], \portfolio[\buyer]): \states \to \consumptions[\buyer] \times \portfoliospace[\buyer]: \forall \state \in \states, \\(\allocation[\buyer], \portfolio[\buyer])(\state) \in \budgetset[\buyer] (\consendow[\buyer], \price[][][*] (\state), \assetprice[][][*] (\state))
    % \allocation[\buyer] (\state) \cdot \price(\state) + \portfolio[\buyer] (\state) \cdot \assetprice(\state)  \leq \consendow[\buyer] \cdot \price(\state) + \portfolio[\buyer][][] (\state) \cdot \returns[{\worldstate}]} 
    }} \left\{ \vfunc[\buyer][{(\allocation[\buyer][][][], \allocation[-\buyer][][][*], 
    \portfolio[\buyer][][], \portfolio[-\buyer][][][*], \price[][][*], \assetprice[][][*])}] (\state)  \right\}$$ for all $\state\in \states$, so $(\consumption[][][][*], \portfolio[][][][*], \price[][][*], \assetprice[][][*])$ is Markov perfect. 

    Next, we want to show that $(\consumption[][][][*], \portfolio[][][][*], \price[][][*], \assetprice[][][*])$ satisfies the Walras's law. First, we show that for any $\buyer\in \buyers$, $\state\in \states$, $\consumption[\buyer][][][*] (\state) \cdot\price[][][*] (\state)+ \portfolio[\buyer][][][*] (\state) \cdot \assetprice[][][*] (\state)- \consendow[\buyer] \cdot \price[][][*] (\state)=0$. By way of contradiction, assume that there exists some $\buyer\in \buyers$, $\state\in \states$ such that $\consumption[\buyer][][][*] (\state) \cdot\price[][][*] (\state)+ \portfolio[\buyer][][][*] (\state) \cdot \assetprice[][][*] (\state)- \consendow[\buyer] \cdot \price[][][*] (\state) \neq 0$. Note that $(\consumption[\buyer][][][*] (\state), \portfolio[\buyer][][][*] (\state)) \in \newbudgetset(\state, \action[-\buyer])=\budgetset(\consendow[\buyer], \price[][][*] (\state), \assetprice[][][*] (\state))=\{(\consumption[\buyer], \portfolio[\buyer]) \in \consumptions[\buyer] \times \portfoliospace[\buyer] \mid  \consumption[\buyer] \cdot \price[][][*] (\state) + \portfolio[\buyer] \cdot \assetprice[][][*] (\state)  \leq \consendow[\buyer] \cdot \price[][][*] (\state) \}$, so we must have  $\consumption[\buyer][][][*] (\state) \cdot\price[][][*] (\state)+ \portfolio[\buyer][][][*] (\state) \cdot \assetprice[][][*] (\state)- \consendow[\buyer] \cdot \price[][][*] (\state)< 0$. By the (no saturation) condition of \Cref{assum:existence_RRE}, there exists $\consumption[\buyer][][][+] \in \consumptions[\buyer]$ s.t. $\util[\buyer] (\consumption[\buyer][][][+]; \type[\buyer])>\util[\buyer] (\consumption[\buyer][][][*] (\state); \type[\buyer])$. Moreover, since $\consumption[\buyer] \mapsto \util[\buyer] (\consumption[\buyer]; \type[\buyer])$ is concave, for any $0<t<1$, $\util[\buyer] (t\consumption[\buyer][][][+]+(1-t) \consumption[\buyer][][][*] (\state); \type[\buyer])>\util[\buyer] (\consumption[\buyer][][][*] (\state); \type[\buyer])$. Since $\consumption[\buyer][][][*] (\state) \cdot\price[][][*] (\state)+ \portfolio[\buyer][][][*] (\state) \cdot \assetprice[][][*] (\state)- \consendow[\buyer] \cdot \price[][][*] (\state)< 0$, we can pick $t$ small enough such that $\consumption[\buyer][][][\prime]=t\consumption[\buyer][][][+]+(1-t) \consumption[\buyer][][][*] (\state)$ satisfies $\consumption[\buyer][][]['] (\state) \cdot\price[][][*] (\state)+ \portfolio[\buyer][][][*] (\state) \cdot \assetprice[][][*] (\state)- \consendow[\buyer] \cdot \price[][][*] (\state) \leq 0$ but $\consumption[\buyer][][][\prime] \in \consumptions[\buyer]$ s.t. $\util[\buyer] (\consumption[\buyer][][][+]; \type[\buyer])>\util[\buyer] (\consumption[\buyer][][][*] (\state); \type[\buyer])$. Thus,
    \begin{align}
        &\qfunc[\buyer][{\policy[][][*]}](\state, \consumption[\buyer][][][\prime], \consumption[-\buyer][][][*](\state), \portfolio[][][][*](\state), \price[][][*](\state), \assetprice[][][*](\state))\\
        &= \newreward[\buyer](\state, \consumption[\buyer][][][\prime], \consumption[-\buyer][][][*](\state), \portfolio[][][][*](\state), \price[][][*](\state), \assetprice[][][*](\state)) + \Ex_{\staterv[][][\prime] \sim \trans(\staterv[][][\prime] \mid \state, \portfolio[][][][*](\state))} [\discount \vfunc[\buyer][{\policy[][][*]}](\staterv[][][\prime])]\\
        &=\util[\buyer](\consumption[\buyer][][][\prime];\type[\buyer]) + \Ex_{\staterv[][][\prime] \sim \trans(\staterv[][][\prime] \mid \state, \portfolio[][][][*](\state))} [\discount \vfunc[\buyer][{\policy[][][*]}](\staterv[][][\prime])]\\
        &> \util[\buyer](\consumption[\buyer][][][*](\state);\type[\buyer]) + \Ex_{\staterv[][][\prime] \sim \trans(\staterv[][][\prime] \mid \state, \portfolio[][][][*](\state))} [\discount \vfunc[\buyer][{\policy[][][*]}](\staterv[][][\prime])]\\
        &=\qfunc[\buyer][{\policy[][][*]}](\state, \consumption[][][][*](\state), \portfolio[][][][*](\state), \price[][][*](\state), \assetprice[][][*](\state))
    \end{align}
    This contradicts that fact that $\policy[][][*]$ is a \MPGNE{} since an optimal policy is supposed to be greedy optimal (i.e., maximize the action-value function of each player over its action space at all states) respect to optimal action value function. Thus, we know that for all $\buyer\in \buyers$, $\state\in \states$, $\consumption[\buyer][][][*] (\state) \cdot\price[][][*] (\state)+ \portfolio[\buyer][][][*] (\state) \cdot \assetprice[][][*] (\state)- \consendow[\buyer] \cdot \price[][][*] (\state)=0$. Summing across the buyers, we get $\price[][][*](\state) \cdot \left( \sum_{\buyer \in \buyers} \consumption[\buyer][][][*] (\state) - \sum_{\buyer \in \buyers} \consendow[\buyer] \right)  +  \assetprice[][][*] (\state) \cdot \left(\sum_{\buyer \in \buyers} \portfolio[\buyer][][][*] (\state) \right)=0 $ for any $\state\in \states$, which is the Walras' law.
    

    Finally, we want to show that $(\consumption[][][][*], \portfolio[][][][*], \price[][][*], \assetprice[][][*])$ is feasible. We first show that $\sum_{\buyer \in \buyers} \consumption[\buyer][][][*] (\state) - \sum_{\buyer \in \buyers} \consendow[\buyer][][] \leq \zeros[\numcommods]$ for any $\state\in \states$. 
    We proved that for any state $\state\in \states$, $\newreward[\numbuyers+1](\state, \consumption[][][][*](\state), \portfolio[][][][*](\state), \price[][][*](\state), \assetprice[][][*](\state))=\price[][][*](\state) \cdot \left( \sum_{\buyer \in \buyers} \consumption[\buyer][][][*] (\state) - \sum_{\buyer \in \buyers} \consendow[\buyer] \right)  +  \assetprice[][][*] (\state) \cdot \left(\sum_{\buyer \in \buyers} \portfolio[\buyer][][][*] (\state) \right)=0$, which implies $\vfunc[\numbuyers+1][{\policy[][][*]}](\state)=0$. For any $\good\in \goods$, consider a $\price:\states\to \pricespace$ defined by $\price(\state)=\j_{\good}$ for all $\state\in \states$ and a $\assetprice: \state\to \assetpricespace$ defined by $\assetprice(\state)=\zeros[\numassets]$ for all $\state\in \states$. Then, we know that
    \begin{align}
        0 &= \vfunc[\numbuyers+1][{\policy[][][*]}]\\
        &= \qfunc[\numbuyers+1][{\policy[][][*]}](\state, \consumption[][][][*](\state), \portfolio[][][][*](\state), \price[][][*](\state), \assetprice[][][*](\state))\\
        &\geq \qfunc[\numbuyers+1][{\policy[][][*]}](\state, \consumption[][][][*](\state), \portfolio[][][][*](\state), \price(\state), \assetprice(\state))\\
        &=\newreward[\numbuyers+1](\state,  \consumption[][][][*](\state), \portfolio[][][][*](\state), \price(\state), \assetprice(\state)) + \Ex_{\staterv[][][\prime] \sim \trans(\staterv[][][\prime] \mid \state, \portfolio[][][][*](\state))} [\discount \vfunc[\buyer][{\policy[][][*]}](\staterv[][][\prime])]\\
        &= \j_\good\cdot \left( \sum_{\buyer \in \buyers} \consumption[\buyer][][][*] (\state) - \sum_{\buyer \in \buyers} \consendow[\buyer] \right) && \forall \good \in \goods\\
        &= \sum_{\buyer \in \buyers} \consumption[\buyer][\good][][*] (\state) - \sum_{\buyer \in \buyers} \consendow[\buyer][\good] && \forall \good \in \goods
    \end{align}
Thus, we know that $\sum_{\buyer \in \buyers} \consumption[\buyer][][][*] (\state) - \sum_{\buyer \in \buyers} \consendow[\buyer][][] \leq \zeros[\numcommods]$ for any $\state\in \states$. 
Finally, we show that $\sum_{\buyer\in \buyers}\portfolio[\buyer][][][*](\state)\leq \zeros[\numassets]$ for all $\state\in\states$. By way of contradiction, suppose that for some asset $\asset \in [\numassets]$, and some state $\state \in \states$, $\sum_{\buyer\in \buyers}\portfolio[\buyer][\asset][][*](\state)> 0$. Then,  
the auctioneer can increase its cumulative payoff by increasing $\assetprice[\asset][][*](\state)$, which contradicts the definition of a \MPGNE{}.  
    
Therefore, we can conclude that $\policy[][][*]=(\consumption[][][][*], \portfolio[][][][*], \price[][][*], \assetprice[][][*]): \states \to \consumptions \times \portfoliospace \times \pricespace
\times \assetpricespace $ is a RRE of $\economy$.

Finally, notice that the transition functions set in our game are all stochastically concave and as such give rise action-value functions which are concave in the actions each of player \cite{atakan2003stochastic}, and it is easy to verify that the game also satisfies all conditions that guarantee the existence of a \MPGNE{} (see Section 4 of \cite{atakan2003stochastic} for detailed proofs). Hence, by \Cref{thm:existence_of_mpgne} which guarantees the existence of \MPGNE{} in generalized Markov games, we can conclude that there exists an RRE $(\consumption[][][][*], \portfolio[][][][*], \price[][][*], \assetprice[][][*])$ in any Radner economy $\economy$.
\end{proof}


\subsection{Omitted Results and Proofs from \Cref{sec:computation}}

\if 0
\lemmaexploitRRE*
\begin{proof}
    In the proof of \Cref{thm:existence_RRE}, we showed that the set of recursive Radner equilibria (RRE) of any infinite horizon Markov exchange economy $\economy$ that satisfies \Cref{assum:existence_RRE} is equal to the set of \MPGNE{} of the associated generalized Markov exchange economy game $\mgame$. 
    
    Moreover, we can observe that, for any infinite horizon Markov exchange economy $\economy$ and its associated generalized Markov exchange economy game $\mgame$, 
    the exploitability of an outcome $(\consumption, \portfolio, \price, \assetprice)$ in $\economy$ is equivalent to the exploitability of a policy $\policy=(\consumption, \portfolio, \price, \assetprice)$ in $\mgame$. 
    Similarly,
    the state exploitability of an outcome $(\consumption, 
    \portfolio, \price, \assetprice)$ in $\economy$ is equivalent to the state exploitability of a policy $\policy=(\consumption, \portfolio, \price, \assetprice)$ in $\mgame$ given any state $\state\in \state$. 

    Therefore, this results follows readily from \Cref{lemma:exploit_GNE}.
\end{proof}
\fi
\thmcomputeSRE*
\begin{proof}
     In the proof of \Cref{thm:existence_RRE}, we can observe that, for any infinite horizon Markov exchange economy $\economy$ and its associated generalized Markov exchange economy game $\mgame$, 
    the exploitability of an outcome $(\consumption, \portfolio, \price, \assetprice)$ in $\economy$ is equivalent to the exploitability of a policy $\policy=(\consumption, \portfolio, \price, \assetprice)$ in $\mgame$. 
    Similarly,
    the state exploitability of an outcome $(\consumption, 
    \portfolio, \price, \assetprice)$ in $\economy$ is equivalent to the state exploitability of a policy $\policy=(\consumption, \portfolio, \price, \assetprice)$ in $\mgame$ given any state $\state\in \state$. 

    Therefore, this results follows readily from \Cref{thm:convergence_GNE}.
\end{proof}