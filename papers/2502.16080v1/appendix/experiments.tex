
\section{Experiments}\label{sec_app:experiments}

\subsection{Neural Projection Method}
\label{sec_app:npm}

The projection method \cite{judd_projection_1992}, also known as the weighted residual methods, is a numerical technique often used to approximate solutions to complex economic models, particularly those involving dynamic programming and dynamic stochastic general equilibrium (DSGE) models. 
These models are common in macroeconomics and often don't have analytical solutions due to their non-linear, dynamic, and high-dimensional nature. The projection method helps approximate these solutions by projecting the problem into a more manageable, lower-dimensional space.

The main idea of the projection method is to express equilibrium of the dynamic economic model as a solution to a functional equation $\operator(\functional)=\zeros$, where 
$\functional: \states \to \R^\numfactions$ is a function that represent some unknown policy, 
$\operator: (\states\to \R^\numfactions) \to (\states \to \R^\numplayers)$, 
and
$\zeros$ is the constant zero function. Some classic examples of the operator $\operator$ includes Euler equations and Bellman equations.
A canonical project method consists of four steps:~1) Define a set of basis functions $\{\basisfunc[i]: \states \to \R^\numfactions\}_{i\in [\numbasis]}$ and approximate each each function $\functional\in \functionals$
through a linear combination of basis functions: $\approxfunc(\cdot;\fparam)=\sum_{i=1}^\numbasis \fparam[i]\basisfunc[i](\cdot)$;
~2) Define a residual equation as a functional equation evaluated at the approximation: $\residual(\cdot;\fparam)\doteq \operator(\approxfunc(\cdot; \fparam))$;
~3) Choose some weight functions $\{\weightfunc[i]: \states \to \R\}_{i\in [\numweights]}$ over the states and
find $\fparam$ that solves $\weightedr(\fparam)\doteq\int_{\states} \weightfunc[i](\state)\residual(\state; \fparam) d\state=0$ for all $i\in [\numweights]$. This gets the residual ``close" to zero in the weighted integral sense;
~4) Simulate the optimal decision rule based on the chosen parameter $\fparam$ and basis functions.


Recently, the neural projection method was developed to extend the traditional projection method \cite{maliar_deep_2021, azinovic2022deep, sauzet_projection_2021}. In the neural projection method, neural networks are used as the functional approximators for policy functions instead of traditional basis function approximations. 
In this section, we show how we can approximate generalized Markov Perfect Nash equilibrium of generalized Markov game, and consequently Recursive Radner Equilibrium of infinite-horizon Markov exchange economies, through the neural projection method.

% \begin{assumption}[Euler Class]\label{assum:euler_class}
%     Let $\mgame$ be a generalized Markov game. Assume that 1.~The state space can be viewed as the Cartesian product of an endogenous state space $\ystates$ and an exogenous state space $\zstates$, i.e., $\states=\ystates \times \zstates$; 
%     2.~for any $\player\in \players$, $\action[-\player]\in \actionspace[-\player]$, $(\ystate,\zstate)\mapsto \actions[\player]((\ystate, \zstate), \action[-\player])$ is upper-hemicontinuous and $\ystate\mapsto \actions[\player]((\ystate, \zstate), \action[-\player])$ is increasing, i.e., $\ystate\leq \ystate[][][\prime]\implies \actions[\player]((\ystate, \zstate), \action[-\player]) \subseteq \actions[\player]((\ystate[][][\prime], \zstate), \action[-\player])$;
%     3.~for any $\player\in \players$, $\action[-\player]\in \actionspace[-\player]$, $\ystate\mapsto \actions[\player]((\ystate, \zstate), \action[-\player])$ is convex;
%     4.~for any $\state=(\ystate,\zstate), \state[][][\prime]=(\ystate[][][\prime], \zstate[][][\prime])\in \states$, $\action\in \actionspace$, the transition probability $\trans(\state[][][\prime]\mid \state,\action)$ can be factored as
%     $\trans(d\state[][][\prime]\mid \state,\action)
%     =\trans(d(\ystate[][][\prime], \zstate[][][\prime])\mid (\ystate,\zstate),\action)
%     =\indic\{\ystate=\ytrans(\ystate, \zstate, \zstate[][][\prime], \action\})\ztrans(d\zstate[][][\prime]\mid \zstate)$;
%     5.~For any $(\ystate, \zstate, \zstate[][][\prime], \action)\in \ystates\times \zstates\times \zstates\times \actionspace$, $\player\in \players$, there exists $h_i(\ystate, \zstate, \action)$ s.t. $\nicefrac{\partial \ytrans}{\partial \ystate}(\ystate,\zstate,\zstate[][][\prime], \action)=\nicefrac{\partial\ytrans}{\partial\action[\player]}(\ystate,\zstate,\zstate[][][\prime], \action) h_i(\ystate, \zstate, \action)$;
%     6.~For all $\player\in \players$, $\action[-\player]\in \actionspace[-\player]$, and $\zstate\in \zstates$, $\ystate\mapsto \reward[\player]((\ystate, \zstate), \action[\player], \action[-\player])$ is concave for all $\action[\player]\in \actionspace[\player]$ and $\action[\player]\mapsto \reward[\player]((\ystate, \zstate), \action[\player], \action[-\player])$ is concave for all $\ystate\in \ystates$.
% \end{assumption}
% \begin{remark}
%     The fourth condition of \Cref{assum:euler_class} can be interpreted as the transition of the exogenous state variable $\zstate$ is not affected by players' action, so it is ``exogenous"; the transition of the endogenous state variable is deterministically affected by the current state $(\ystate, \zstate)$, the next exogenous state $\zstate[][][\prime]$, and players' actions $\action$.
% \end{remark}

\begin{assumption}\label{assum:neural_proj}
Given a generalized Markov game $\mgame$, assume that 1.~for any $\player\in \players$, $\state\in\states$, $\action[-\player]\in \actionspace[-\player]$,
$\actions[\player](\state, \action[-\player])\doteq \{\action[\player]\in \actionspace[\player]\mid \actionconstr[\player][\numconstr](\state, \action[\player], \action[-\player])\geq 0\text{ for all $\numconstr\in [\numconstrs]$}\}$ for a collection of \mydef{constraint functions} $\{\actionconstr[\player][\numconstr]: \states\times \actionspace \mid \numconstr\in [\numconstrs]\}$, where $\action[\player]\mapsto \actionconstr[\player][\numconstr](\state, \action[\player], \action[-\player])$ is concave\sadie{is this a reasonable assumption?} for every $\numconstr\in [\numconstrs]$. 
\end{assumption}

\begin{restatable}{theorem}{thmbellman_firstorder}
\label{thm:firstorder+bellman}
    Let $\mgame$ be a generalized Markov game that satisfies \Cref{assum:neural_proj}. For any policy profile $\policy\in \fmarkovpolicies$, $\policy$ is a \MPGNE{} if and only if there exists Lagrange multiplier policy $\langmult: \states\to \R_+^{\numplayers\times \numconstrs}$ such that $(\policy, \langmult)$ solves the following functional equation: for all $\player\in \players$, $\state\in \states$,
    \begin{align}
         &0\in \partial_{\action[\player]} \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))+ \sum_{\numconstr\in [\numconstrs]} \langmult[\player,\numconstr](\state) \partial_{\action[\player]}\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[-\player](\state))\\
    &\forall \numconstr\in [\numconstrs], \;\;0=\langmult[\player\numconstr](\state)\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[-\player](\state))\\
    &\forall \numconstr\in [\numconstrs],\;\;0\leq \actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state))\\
    % 0=\vfunc[\player][\policy](\state)-
    % \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))
    \end{align}
    and for all $\player\in \players$, $\state\in \states$,
    \begin{align}
        \vfunc[\player][\policy](\state) =
    \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))
    \end{align}
\end{restatable}
\begin{proof}
    First, we know that a policy profile $\policy\in \fmarkovpolicies$ is a \MPGNE{} if and only if it satisfies the following generalized Bellman Optimality equations, i.e., for all $\player\in \players$, $\state\in \states$,
    \begin{align}
        \vfunc[\player][\policy](\state)
        &=\max_{\action[\player]\in \actions[\player](\state, \policy[-\player](\state))}
        \reward[\player](\state, \action[\player], \policy[-\player](\state))
        +\E_{\state[][][\prime]\sim \trans(\cdot\mid \state, \action[\player], \policy[-\player](\state))}[\gamma \vfunc[\player][\policy](\state[][][\prime])]\label{eq:bellman1}\\
        &=\max_{\action[\player]\in \actions[\player](\state, \policy[-\player](\state))}
        \qfunc[\player][\policy](\state, \action[\player], \policy[-\player](\state))\label{eq:bellman2}
    \end{align}
    Then since $\action[\player]\mapsto \qfunc[\player][\policy](\state, \action[\player], \policy[-\player](\state))$ is concave over $\actions[\player](\state, \policy[-\player](\state))$ by \Cref{assum:existence_of_mpgne}, the KKT conditions provides sufficient and necessary optimality conditions for the constrained maximization problem \begin{align}
        \max_{\action[\player]\in \actions[\player](\state, \policy[-\player](\state))}
        \qfunc[\player][\policy](\state, \action[\player], \policy[-\player](\state)) \label{eq:bellman_opt}
    \end{align}
    That is, $\action[\player][][][*]\in \actions[\player](\state, \policy[-\player](\state))$ is a solution to \cref{eq:bellman_opt} if and only if there exists  $\{\langmult[\player\numconstr][*]:\states\to \R_+\}_{\numconstr\in [\numconstrs]}$ s.t.
    \begin{align}
        &0\in \partial_{\action[\player]} \qfunc[\player][\policy](\state, \action[\player][][][*], \policy[-\player](\state))+ \sum_{\numconstr\in [\numconstrs]} \langmult[\player\numconstr][*](\state) \partial_{\action[\player]}\actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state)) \label{eq:bellman_first_order}\\
        &\forall \numconstr\in [\numconstrs], \;\;0=\langmult[\player\numconstr][*](\state)\actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state))\\
        &\forall \numconstr\in [\numconstrs],\;\;0\leq \actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state))
        % &\forall \numconstr\in [\numconstrs],\;\;\langmult[\player,\numconstr](\state)\geq 0 
    \end{align}
    Therefore, we can conclude that $\policy\in \fmarkovpolicies$ is a \MPGNE{} if and only if there exists $\{\langmult[\player\numconstr]:\states\to \R_+\}_{\player\in \players,\numconstr\in [\numconstrs]}$ s.t. for all $\player\in \players$, $\state\in \states$,
    \begin{align}
         &0\in \partial_{\action[\player]} \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))+ \sum_{\numconstr\in [\numconstrs]} \langmult[\player,\numconstr](\state) \partial_{\action[\player]}\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[\player](\state))\\
    &\forall \numconstr\in [\numconstrs], \;\;0=\langmult[\player\numconstr](\state)\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[\player](\state))\\
    &\forall \numconstr\in [\numconstrs],\;\;0\leq \actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state))
    \end{align}
        and for all $\player\in \players$, $\state\in \states$,
    \begin{align}
        \vfunc[\player][\policy](\state) =
    \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))
    \end{align}
\end{proof}

Therefore, for a policy profile $\policy\in \fmarkovpolicies$ and a Lagrange multiplier policy $\langmult: \states\to \R_+^{\numplayers\times \numconstrs}$ such that $(\policy, \langmult)$, 
consider the \mydef{total first order violation} \begin{align}
    \firsterror(\policy, \langmult)
    &=\sum_{\player\in \players}
\left\|\int_{\state\in \states}
\partial_{\action[\player]} \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))+ \sum_{\numconstr\in [\numconstrs]} \langmult[\player,\numconstr](\state) \partial_{\action[\player]}\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[-\player](\state)) ds\right\|_2^2 
\end{align} 
and the \mydef{average Bellman error}
\begin{align}
    \bellmanerror(\policy, \langmult)
    &=\sum_{\player\in \players}
\left\|\int_{\state\in \states}
\vfunc[\player][\policy](\state) -
    \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))
 ds\right\|_2^2.
\end{align} 
We can directly approximate the \MPGNE{} through minimizing the sum of these two errors. 

Typically, approximating the \MPGNE{} using the neural projection method requires optimizing both the policy profile and the Lagrange multiplier policy. However, in exchange economy Markov pseudo-games, we derive a closed-form solution for the optimal Lagrange multiplier, allowing us to focus solely on optimizing the policy profile. 
% In specific, for any infinite horizon Markov exchange economy $\economy$ and its associated exchange economy Markov pseudo-game $\mgame = (\numbuyers+1, %numplayer
% \numcommods+\numassets, %numactions
% \states, 
% \bigtimes_{\player\in \players}(\consumptions[\player] \times \portfoliospace[\player]) \times (\pricespace[\numgoods] \times \assetpricespace)), %actionspace
% \newbudgetset, %\actions, 
% \newreward, 
% \newtrans, 
% \discount', 
% \initstates')$, the optimal Lagrangian multiplier policy is defined as for any $\state\in \states$, $\buyer\in \buyers$,
% $\langmult[\buyer][][*](\state)=\frac{\reward[\player](\consumption[\player][][][*](\state); \type[\player])}{\consendow[\player]\cdot \price[][][*](\state)-\portfolio[\player][][][*](\state)\cdot \assetprice[][][*](\state)}$.
\subsection{More Results}


\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/linear_nonstochastic.png}
        %\caption{Normalized Metrics for Linear Economy with Deterministic Transition Probability Function}
        \label{fig:linear_nonstochastic}
    \end{subfigure}
    
    % \vspace{0.5cm} % Adjust spacing between subfigures
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/cd_nonstochastic.png}
        %\caption{Normalized Metrics for Cobb-Douglas Economy with Deterministic Transition Probability Function}
        \label{fig:cd_nonstochastic}
    \end{subfigure}
    
    % \vspace{0.5cm} % Adjust spacing between subfigures
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/leontief_nonstochastic.png}
        %\caption{Normalized Metrics for Leontief Economy with Deterministic Transition Probability Function}
        \label{fig:leontief_nonstochastic}
    \end{subfigure}
    \caption{Normalized Metrics for Economies with Deterministic Transition Probability Function}
    \label{fig:nonstochastic}
\end{figure}
\subsection{Implementation Details}
\paragraph{Deterministic Case Training Details}
For deterministic transition probability case, for each reward function class
we randomly sampled one economy with 10 consumers, 10 commodities, 1 asset, and 5 world state. The asset return matrix $\returns$
is sampled in a way such that $\returns[\worldstate][\asset][\commod]\sim \unif([0.5, 1.1])$ for all $\worldstate$, $\asset$, and $\commod$. Moreover, we set the length of the stochastic process to be 30.
For the initial state, we sample each consumer's endowment $\consendow[\player]\sim \unif([0.01, 0.1])^\numcommods$ and normalized so that the total endowment of each commodity add up to 1. We also sample each consumer's type $\type[\player]\sim \unif([1.0, 5.0])^\numcommods$, and set the world state to be 0. 
The transition probability function $\trans$ is defined as $\trans(\state[][][\prime]\mid \state, \portfolio)=1$ for all $\state(\worldstate, \consendow, \type)$ where $\state[][][\prime]=(\worldstate[][][\prime], \consendow[][][][\prime], \type[][][][\prime])$ is defined as $\worldstate[][][\prime]=0$, $\consendow[][][][\prime]=0.01\cdot\ones[\numbuyers\times \numcommods]$, and $\type[][][][\prime]=\type$. 

Then, for both GAPNets method and neural projection method, we run 1000 episodes for each learning rate candidate in a grid search manner and measure the performance in terms of minimizing total first-order violation and average Bellman error. Finally, we pick the best hyperparameter for the final experiments. 

In the final experiments, we run GAPNets for 2000 episodes using learning rates $\learnrate[\param]=1\times 10^{-5}, \learnrate[\deparam]=1\times 10^{-5}$ for the linear economy, 
$\learnrate[\param]=1\times 10^{-5}, \learnrate[\deparam]=1\times 10^{-5}$ for the Cobb-Douglas economy, 
and $\learnrate[\param]=1\times 10^{-5}, \learnrate[\deparam]=1\times 10^{-5}$ for the Leontief economy. 
Similarly, we ran neural projection method for 2000 episodes using learning rates
$\learnrate[\param]=1\times 10^{-4}$ for the linear economy,
$\learnrate[\param]=2.5\times 10^{-5}$ for the Cobb-Douglas economy,
and $\learnrate[\param]=1\times 10^{-4}$ for the Leontief economy.
In this process, we compute the exploitability of computed policy profile through gradient ascent of the adversarial network. In specific, we ran 1000 episodes of gradient ascent with learning rate $\learnrate[\deparam]=5\times 10^{-5}$ for the linear economy, 
$\learnrate[\deparam]=1\times 10^{-4}$ for the Cobb-Douglas economy, 
and $\learnrate[\deparam]=1\times 10^{-4}$ for the Leontief economy. 

Next, for each economy, we randomly sample 50 policy profiles and record their total first-order violations, average Bellman errors, and exploitabilities. Finally, we normalize the results by the average of the sampled values.


\paragraph{Stochastic Case Training Details}
For stochastic transition probability case, for each reward function class
we randomly sampled one economy with 10 consumers, 10 commodities, 1 asset, and 5 world state. The asset return matrix $\returns$
is sampled in a way such that $\returns[\worldstate][\asset][\commod]\sim \unif([0.5, 1.1])$ for all $\worldstate$, $\asset$, and $\commod$. Moreover, we set the length of the stochastic process to be 30.
For the initial state, we sample each consumer's endowment $\consendow[\player]\sim \unif([0.01, 0.1])^\numcommods$ and normalized so that the total endowment of each commodity add up to 1. We also sample each consumer's type $\type[\player]\sim \unif([1.0, 5.0])^\numcommods$, and set the world state to be 0. 
The transition probability function will stochastically transition from state $\state(\worldstate, \consendow, \type)$
to state $\state[][][\prime]=(\worldstate[][][\prime], \consendow[][][][\prime], \type[][][][\prime])$ where $\worldstate[][][\prime]\sim \unif(\{0,1,2,3,4\})$, $\consendow[][][][\prime]\sim 0.002+\unif([0.01, 0.1])^{\numbuyers\times \numcommods}$, and $\type[][][][\prime]=\type$. 

Then, for both GAPNets method and neural projection method, we run 1000 episodes for each learning rate candidate in a grid search manner and measure the performance in terms of minimizing total first-order violation and average Bellman error. Finally, we pick the best hyperparameter for the final experiments. 

In the final experiments, we run GAPNets for 2000 episodes using learning rates $\learnrate[\param]=1\times 10^{-5}, \learnrate[\deparam]=1\times 10^{-5}$ for the linear economy, 
$\learnrate[\param]=2.5\times 10^{-5}, \learnrate[\deparam]=2.5\times 10^{-5}$ for the Cobb-Douglas economy, 
and $\learnrate[\param]=5\times 10^{-5}, \learnrate[\deparam]=5\times 10^{-5}$ for the Leontief economy. 
Similarly, we ran neural projection method for 2000 episodes using learning rates
$\learnrate[\param]=5\times 10^{-5}$ for the linear economy,
$\learnrate[\param]=2.5\times 10^{-5}$ for the Cobb-Douglas economy,
and $\learnrate[\param]=5\times 10^{-4}$ for the Leontief economy.
In this process, we compute the exploitability of computed policy profile through gradient ascent of the adversarial network. In specific, we ran 1000 episodes of gradient ascent with learning rate $\learnrate[\deparam]=7.5\times 10^{-4}$ for the linear economy, 
$\learnrate[\deparam]=1\times 10^{-4}$ for the Cobb-Douglas economy, 
and $\learnrate[\deparam]=1\times 10^{-4}$ for the Leontief economy. When estimating the neural loss function—cumulative regret for the GAPNets method and total first-order violations and average Bellman error for the neural projection method—we use 100 samples for GAPNets and 10 samples for the neural projection method. The primary reason for this difference is the high memory consumption of the neural projection method, which makes larger sample sizes infeasible. 


Next, for each economy, we randomly sample 50 policy profiles and record their total first-order violations, average Bellman errors, and exploitabilities. Finally, we normalize the results by the average of the sampled values.

\subsection{Other Details}

\paragraph{Programming Languages, Packages, and Licensing}
We ran our experiments in Python 3.7 \cite{van1995python}, using NumPy \cite{numpy},  , CVXPY \cite{diamond2016cvxpy}, Jax \cite{jax2018github}, OPTAX \cite{jax2018github}, Haiku \cite{haiku2020github}, and  JaxOPT \cite{jaxopt_implicit_diff}.
All figures were graphed using Matplotlib \cite{matplotlib}. 

Python software and documentation are licensed under the PSF License Agreement. Numpy is distributed under a liberal BSD license. Pandas is distributed under a new BSD license. Matplotlib only uses BSD compatible code, and its license is based on the PSF license. CVXPY is licensed under an APACHE license. 

% \paragraph{Implementation Details}
% In order to project each allocation computed onto the budget set of the consumers, i.e., $\{\allocation \in \R^{\numbuyers \times \numgoods}_+ \mid \allocation\price \leq \budget\}$, we used the alternating projection algorithm for convex sets, and alternatively projected onto the sets $\R^{\numbuyers \times \numgoods}_+$ and $\{\allocation \in \R^{\numbuyers \times \numgoods} \mid \allocation\price \leq \budget\}$. 

% To compute the best-response for the inner play in \Cref{alg:dynamic_max_oracle_gd}, we used the ECOS solver, a CVXPY’s first-order convex-program solvers, but if ever a runtime exception occurred, we ran the SCS solver.

% When computing the distance from the demands $\allocation^{(\iter)}$ computed by our algorithms to the equilibrium demands $\allocation^{(\iter)^\star}$, we normalize both demands to satisfy $\forall \good \in \goods, \;\sum_{\buyer \in \buyers} \allocation[i][j]=1_{\numgoods}$ to reduce the noise caused by changing supplies.

\paragraph{Computational Resources}
The experiments were conducted using Google Colab, which provides cloud-based computational resources. Specifically, we utilized an NVIDIA T4 GPU with the following specifications: GPU: NVIDIA T4 (16GB GDDR6), CPU: Intel Xeon (2 vCPUs), RAM: 12GB, Storage: Colab-provided ephemeral storage. 


\paragraph{Code Repository}
the
full details of our experiments, including hyperparameter search, final experiment configurations, and visualization code,
can be found in our code repository ({\color{blue}\rawcoderepo}).


