\paragraph{Contributions}

%\paragraph{Markov Pseudo-games}

In \Cref{sec:gmg}, we introduce Markov pseudo-games, 
%a generalization of Markov games in which the actions taken by other players determine not only the reward a player receives at a state but also the set of actions available to it, 
and we establish the existence of (pure) \mydef{generalized Markov perfect equilibria (\MPGNE)} in 
%a natural class of such games, called
concave Markov pseudo-games (\Cref{thm:existence_of_mpgne}). 
This result can be seen as a stochastic generalization of \citet{arrow-debreu}'s existence result for (pure) generalized Nash equilibrium in concave pseudo-games \cite{facchinei2010generalized}.
To the best of our knowledge, it also implies the existence of pure (or deterministic) Markov perfect equilibria in a large class of continuous-action Markov games for which existence was heretofore known only in mixed (or randomized) policies \cite{fink1964equilibrium, takahashi1964equilibrium}.

Although the computation of \MPGNE{} is PPAD-hard in general, because \MPGNE{} generalize Nash equilibrium, we reduce this computational problem to generative adversarial learning between a generator, who produces a candidate equilibrium policy profile, and an adversary, who produces a policy profile of best responses to the candidate equilibrium \cite{goktas2023generative} (\Cref{obs:exploit_min_to_min_max}).
Assuming parameterized policies, and taking advantage of the recent progress on solving generative adversarial learning problems (e.g.,~\cite{lin2020gradient, daskalakis2020independent}), we show that a policy profile which is a stationary point of the exploitability (i.e., the playersâ€™ cumulative maximum regret) can be computed in polynomial time under mild assumptions (\Cref{thm:convergence_GNE}). 
This result implies that a policy profile that satisfies necessary first-order stationarity conditions for a \mpgne{} in Markov pseudo-games with a bounded best-response mismatch coefficient (\Cref{lemma:br_mismatch_coef})---i.e., those Markov pseudo-games in which states explored by any \mpgne{} are easily explored under the initial state distribution---can be computed in polynomial time, a result which is analogous known computational results for zero-sum Markov games \cite{daskalakis2020independent}.
%When these policies are represented by neural networks, we call the solutions to this generative adversarial learning problem, \mydef{generative adversarial policy networks (GAPs)}.
As our theoretical computational guarantees apply to policies represented by neural networks, we obtain the first, to our knowledge, deep reinforcement learning algorithm with theoretical guarantees for general-sum games.

% \samy{}{We then set out to compute the recursive com of this game.
% As the state space is continuous, we can only estimate the game's value function from samples.}
% First, we develop a polynomial-time and polynomial-sample complexity value iteration algorithm that \samy{computes}{approximates} a recursive competitive equilibrium. 
% \samy{\samy{}{Because the state space is continuous,} this method requires us to estimate the value function, it can lead to inherent challenges due to the exploration/exploitation tradeoff, as such we}{}
% We then turn our attention to developing policy-gradient-type reinforcement learning methods, which converge to recursive competitive equilibrium (resp.\ first-order \samy{}{recursive} competitive equilibrium) in polynomial time with only a polynomial number of samples, for function approximators which are affine (resp.\ Lipschitz-smooth) in their parameters.
% Finally, we demonstrate the success of our algorithms experimentally, by computing recursive competitive equilibrium, assuming consumers have linear, Cobb-Douglas, or Leontief \samy{}{(all homothetic)} instantaneous utility functions.\amy{what about the intertemporal condition?}


%\paragraph{Infinite horizon Markov exchange economies}

%We then introduce \mydef{infinite horizon Markov exchange economies}, an extension of the infinite horizon stochastic exchange economy model proposed by \citeauthor{magill1994infinite} \cite{magill1994infinite}.
%We provide broad conditions that guarantee the existence of a recursive Radner equilibrium in our model.

In \Cref{sec:infinite}, we introduce an extension of \citet{magill1994infinite}'s infinite horizon exchange economy, which we call the \mydef{infinite horizon Markov exchange economy}. 
On the one hand, our model generalizes \citeauthor{magill1994infinite}'s to a setting with arbitrary, not just financial assets; on the other hand, we restrict the transition model to be Markov.
The Markov restriction allows us to prove the existence of a \mydef{recursive Radner equilibrium (RRE)} \cite{mehra1977recursive} (\Cref{thm:existence_RRE}).
%%(or \mydef{recursive competitive equilibrium})
%meaning a Radner equilibrium that is independent of the initial state distribution, which implies that the domain of the equilibrium policy can be simplified to the space of states rather than the space of histories, which makes its computation tractable.
Our proof reformulates the set of RRE of any infinite horizon Markov exchange economy as the set of GMPE of an associated generalized Markov game (\Cref{thm:existence_RRE}).
To our knowledge, ours is the first result of its kind for such a general setting, as previous recursive competitive equilibrium existence proofs were restricted to economies with one consumer (also called the representative agent), one commodity, or one asset \cite{mehra1977recursive, prescott1980recursive}. 
The aforementioned results allow us to conclude that a stationary point of the exploitability of the Markov pseudo-game associated with any infinite horizon Markov exchange economy can be computed in polynomial time (\Cref{thm:compute_SRE}).

Finally, in \Cref{sec:expts} we implement our policy gradient method in the form of a generative adversarial policy network (GAPNet), and use it to try to find RRE in three infinite horizon Markov exchange economies with three different types of utility functions.
Experimentally, we find that our GAPNet produces approximate equilibrium policies that are closer to \MPGNE{} than those produced by a standard baseline for solving stochastic economies.
