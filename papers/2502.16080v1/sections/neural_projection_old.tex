\subsubsection{Neural Projection Method}
The projection method \cite{judd_projection_1992}, also known as the weighted residual methods, is a numerical technique often used to approximate solutions to complex economic models, particularly those involving dynamic programming and dynamic stochastic general equilibrium (DSGE) models. 
These models are common in macroeconomics and often don't have analytical solutions due to their non-linear, dynamic, and high-dimensional nature. The projection method helps approximate these solutions by projecting the problem into a more manageable, lower-dimensional space.

The main idea of the projection method is to express equilibrium of the dynamic economic model as a solution to a functional equation $\operator(\functional)=\zeros$, where 
$\functional: \states \to \R^\numfactions$ is a function that represent some unknown policy, 
$\operator: (\states\to \R^\numfactions) \to (\states \to \R^\numplayers)$, 
and
$\zeros$ is the constant zero function. Some classic examples of the operator $\operator$ includes Euler equations and Bellman equations.
A canonical project method consists of four steps:~1) Define a set of basis functions $\{\basisfunc[i]: \states \to \R^\numfactions\}_{i\in [\numbasis]}$ and approximate each each function $\functional\in \functionals$
through a linear combination of basis functions: $\approxfunc(\cdot;\fparam)=\sum_{i=1}^\numbasis \fparam[i]\basisfunc[i](\cdot)$;
~2) Define a residual equation as a functional equation evaluated at the approximation: $\residual(\cdot;\fparam)\doteq \operator(\approxfunc(\cdot; \fparam))$;
~3) Choose some weight functions $\{\weightfunc[i]: \states \to \R\}_{i\in [\numweights]}$ over the states and
find $\fparam$ that solves $\weightedr(\fparam)\doteq\int_{\states} \weightfunc[i](\state)\residual(\state; \fparam) d\state=0$ for all $i\in [\numweights]$. This gets the residual ``close" to zero in the weighted integral sense;
~4) Simulate the optimal decision rule based on the chosen parameter $\fparam$ and basis functions.


Recently, the neural projection method was developed to extend the traditional projection method \cite{maliar_deep_2021, azinovic2022deep, sauzet_projection_2021}. In the neural projection method, neural networks are used as the functional approximators for policy functions instead of traditional basis function approximations. 
In this section, we show how we can approximate generalized Markov Perfect Nash equilibrium of generalized Markov game, and consequently Recursive Radner Equilibrium of infinite-horizon Markov exchange economies, through the neural projection method.

% \begin{assumption}[Euler Class]\label{assum:euler_class}
%     Let $\mgame$ be a generalized Markov game. Assume that 1.~The state space can be viewed as the Cartesian product of an endogenous state space $\ystates$ and an exogenous state space $\zstates$, i.e., $\states=\ystates \times \zstates$; 
%     2.~for any $\player\in \players$, $\action[-\player]\in \actionspace[-\player]$, $(\ystate,\zstate)\mapsto \actions[\player]((\ystate, \zstate), \action[-\player])$ is upper-hemicontinuous and $\ystate\mapsto \actions[\player]((\ystate, \zstate), \action[-\player])$ is increasing, i.e., $\ystate\leq \ystate[][][\prime]\implies \actions[\player]((\ystate, \zstate), \action[-\player]) \subseteq \actions[\player]((\ystate[][][\prime], \zstate), \action[-\player])$;
%     3.~for any $\player\in \players$, $\action[-\player]\in \actionspace[-\player]$, $\ystate\mapsto \actions[\player]((\ystate, \zstate), \action[-\player])$ is convex;
%     4.~for any $\state=(\ystate,\zstate), \state[][][\prime]=(\ystate[][][\prime], \zstate[][][\prime])\in \states$, $\action\in \actionspace$, the transition probability $\trans(\state[][][\prime]\mid \state,\action)$ can be factored as
%     $\trans(d\state[][][\prime]\mid \state,\action)
%     =\trans(d(\ystate[][][\prime], \zstate[][][\prime])\mid (\ystate,\zstate),\action)
%     =\indic\{\ystate=\ytrans(\ystate, \zstate, \zstate[][][\prime], \action\})\ztrans(d\zstate[][][\prime]\mid \zstate)$;
%     5.~For any $(\ystate, \zstate, \zstate[][][\prime], \action)\in \ystates\times \zstates\times \zstates\times \actionspace$, $\player\in \players$, there exists $h_i(\ystate, \zstate, \action)$ s.t. $\nicefrac{\partial \ytrans}{\partial \ystate}(\ystate,\zstate,\zstate[][][\prime], \action)=\nicefrac{\partial\ytrans}{\partial\action[\player]}(\ystate,\zstate,\zstate[][][\prime], \action) h_i(\ystate, \zstate, \action)$;
%     6.~For all $\player\in \players$, $\action[-\player]\in \actionspace[-\player]$, and $\zstate\in \zstates$, $\ystate\mapsto \reward[\player]((\ystate, \zstate), \action[\player], \action[-\player])$ is concave for all $\action[\player]\in \actionspace[\player]$ and $\action[\player]\mapsto \reward[\player]((\ystate, \zstate), \action[\player], \action[-\player])$ is concave for all $\ystate\in \ystates$.
% \end{assumption}
% \begin{remark}
%     The fourth condition of \Cref{assum:euler_class} can be interpreted as the transition of the exogenous state variable $\zstate$ is not affected by players' action, so it is ``exogenous"; the transition of the endogenous state variable is deterministically affected by the current state $(\ystate, \zstate)$, the next exogenous state $\zstate[][][\prime]$, and players' actions $\action$.
% \end{remark}

\begin{assumption}\label{assum:neural_proj}
Given a generalized Markov game $\mgame$, assume that 1.~for any $\player\in \players$, $\state\in\states$, $\action[-\player]\in \actionspace[-\player]$,
$\actions[\player](\state, \action[-\player])\doteq \{\action[\player]\in \actionspace[\player]\mid \actionconstr[\player][\numconstr](\state, \action[\player], \action[-\player])\geq 0\text{ for all $\numconstr\in [\numconstrs]$}\}$ for a collection of \mydef{constraint functions} $\{\actionconstr[\player][\numconstr]: \states\times \actionspace \mid \numconstr\in [\numconstrs]\}$, where $\action[\player]\mapsto \actionconstr[\player][\numconstr](\state, \action[\player], \action[-\player])$ is concave\sadie{is this a reasonable assumption?} for every $\numconstr\in [\numconstrs]$. 
\end{assumption}

\begin{restatable}{theorem}{thmbellman_firstorder}
    Let $\mgame$ be a generalized Markov game that satisfies \Cref{assum:neural_proj}. For any policy profile $\policy\in \fmarkovpolicies$, $\policy$ is a MPGNE if and only if there exists Lagrange multiplier policy $\langmult: \states\to \R_+^{\numplayers\times \numconstrs}$ such that $(\policy, \langmult)$ solves the following functional equation: for all $\player\in \players$, $\state\in \states$,
    \begin{align}
         &0\in \partial_{\action[\player]} \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))+ \sum_{\numconstr\in [\numconstrs]} \langmult[\player,\numconstr](\state) \partial_{\action[\player]}\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[-\player](\state))\\
    &\forall \numconstr\in [\numconstrs], \;\;0=\langmult[\player\numconstr](\state)\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[-\player](\state))\\
    &\forall \numconstr\in [\numconstrs],\;\;0\leq \actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state))\\
    % 0=\vfunc[\player][\policy](\state)-
    % \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))
    \end{align}
    and for all $\player\in \players$, $\state\in \states$,
    \begin{align}
        \vfunc[\player][\policy](\state) =
    \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))
    \end{align}
\end{restatable}
\begin{proof}
    First, we know that a policy profile $\policy\in \fmarkovpolicies$ is a MPGNE if and only if it satisfies the following generalized Bellman Optimality equations, i.e., for all $\player\in \players$, $\state\in \states$,
    \begin{align}
        \vfunc[\player][\policy](\state)
        &=\max_{\action[\player]\in \actions[\player](\state, \policy[-\player](\state))}
        \reward[\player](\state, \action[\player], \policy[-\player](\state))
        +\E_{\state[][][\prime]\sim \trans(\cdot\mid \state, \action[\player], \policy[-\player](\state))}[\gamma \vfunc[\player][\policy](\state[][][\prime])]\label{eq:bellman1}\\
        &=\max_{\action[\player]\in \actions[\player](\state, \policy[-\player](\state))}
        \qfunc[\player][\policy](\state, \action[\player], \policy[-\player](\state))\label{eq:bellman1}
    \end{align}
    Then since $\action[\player]\mapsto \qfunc[\player][\policy](\state, \action[\player], \policy[-\player](\state))$ is concave over $\actions[\player](\state, \policy[-\player](\state))$ by \Cref{assum:existence_of_mpgne}, the KKT conditions provides sufficient and necessary optimality conditions for the constrained maximization problem \begin{align}
        \max_{\action[\player]\in \actions[\player](\state, \policy[-\player](\state))}
        \qfunc[\player][\policy](\state, \action[\player], \policy[-\player](\state)) \label{eq:bellman_opt}
    \end{align}
    That is, $\action[\player][][][*]\in \actions[\player](\state, \policy[-\player](\state))$ is a solution to \cref{eq:bellman_opt} if and only if there exists  $\{\langmult[\player\numconstr][*]:\states\to \R_+\}_{\numconstr\in [\numconstrs]}$ s.t.
    \begin{align}
        &0\in \partial_{\action[\player]} \qfunc[\player][\policy](\state, \action[\player][][][*], \policy[-\player](\state))+ \sum_{\numconstr\in [\numconstrs]} \langmult[\player\numconstr][*](\state) \partial_{\action[\player]}\actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state)) \label{eq:bellman_first_order}\\
        &\forall \numconstr\in [\numconstrs], \;\;0=\langmult[\player\numconstr][*](\state)\actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state))\\
        &\forall \numconstr\in [\numconstrs],\;\;0\leq \actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state))
        % &\forall \numconstr\in [\numconstrs],\;\;\langmult[\player,\numconstr](\state)\geq 0 
    \end{align}
    Therefore, we can conclude that $\policy\in \fmarkovpolicies$ is a MPGNE if and only if there exists $\{\langmult[\player\numconstr]:\states\to \R_+\}_{\player\in \players,\numconstr\in [\numconstrs]}$ s.t. for all $\player\in \players$, $\state\in \states$,
    \begin{align}
         &0\in \partial_{\action[\player]} \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))+ \sum_{\numconstr\in [\numconstrs]} \langmult[\player,\numconstr](\state) \partial_{\action[\player]}\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[\player](\state))\\
    &\forall \numconstr\in [\numconstrs], \;\;0=\langmult[\player\numconstr](\state)\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[\player](\state))\\
    &\forall \numconstr\in [\numconstrs],\;\;0\leq \actionconstr[\player][\numconstr](\state, \action[\player][][][*], \policy[-\player](\state))
    \end{align}
        and for all $\player\in \players$, $\state\in \states$,
    \begin{align}
        \vfunc[\player][\policy](\state) =
    \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))
    \end{align}
\end{proof}

Therefore, for a policy profile $\policy\in \fmarkovpolicies$ and a Lagrange multiplier policy $\langmult: \states\to \R_+^{\numplayers\times \numconstrs}$ such that $(\policy, \langmult)$, 
consider the \mydef{first-order Bellman error} \begin{align}
    \firsterror(\policy, \langmult)
    &=\sum_{\player\in \players}
\left\|\int_{\state\in \states}
\partial_{\action[\player]} \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))+ \sum_{\numconstr\in [\numconstrs]} \langmult[\player,\numconstr](\state) \partial_{\action[\player]}\actionconstr[\player][\numconstr](\state, \policy[\player](\state), \policy[-\player](\state)) ds\right\|_2^2 \nonumber\\
&+ \sum_{\player\in \players}
\left\|\int_{\state\in \states}
\vfunc[\player][\policy](\state) -
    \qfunc[\player][\policy](\state, \policy[\player](\state), \policy[-\player](\state))
 ds\right\|_2^2,
\end{align} we can directly approximate the MPGNE through minimizing this error. 

To do so, we first need to parameterize our policy classes \sadie{I propose to have a part for policy parameterization for both neural projection method and exploitability}
\begin{assumption}[Parameterization for Neural Projection Method] \label{assum:param_npm}
    
\end{assumption}

\begin{assumption}[Lipchitz Smooth First-Order Bellman Error]\sadie{conditions to ensure that }
    
\end{assumption}
