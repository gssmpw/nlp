\section{Markov Pseudo-Games}
\label{sec:gmg}

We begin by developing our formal game model.
The games we study are stochastic, in the sense of \citet{shapley1953stochastic}, \citet{fink1964equilibrium}, and \citet{takahashi1964equilibrium}.
Further, they are pseudo-games, in the sense of \citet{arrow-debreu}.
\citeauthor{arrow-debreu} introduced pseudo-games to establish the existence of competitive equilibrium in their seminal model of an exchange economy, where an auctioneer sets prices that determine the consumers' budget sets, and hence their feasible consumptions.
It is this dependency among the players' feasible actions that characterizes pseudo-games.
%
%We use the term ``generalized'' in lieu of ``pseudo.''
We model stochastic pseudo-games, and dub them \mydef{Markov pseudo-games}, as the games are Markov in that the stochastic transitions depend only on the most recent state and player actions.

%\amy{isn't our model a new model? so we should explain how it deviates from others in the literature, instead of citing Shapley, Fink, etc.} \sadie{Yes it is a new model, and I agree that we should have a short paragraph introducing it and explain how it deviates from others in the literature, I will add it later.} 

%\emph{It is this dependence among the feasible actions of each player that makes this model a \samy{generalized Markov game}{Markov pseudo-game}, and not simply a Markov game.} 


\subsection{Model}
\label{sec:gmg_model}

% \deni{Generalizing to differing discount rates!!}

An \mydef{(infinite horizon discounted) Markov pseudo-game}\longversion{%
\footnote{We use the term ``generalized game'' where others before us have used ``pseudo-game'' \cite{facchinei2010generalized}, ``abstract economy'' \cite{arrow-debreu}, and ``social system'' \cite{debreu1952social}. 
In generalized games, because the players' actions are constrained by one another's, a number of authors have argued that it can be hard to imagine a game where the players make their choices simultaneously and it so happens that all constraints are satisfied. 
Our choice of terminology is intended to reflect the fact that a generalized game can be seen as a game with a mediator who proposes an action profile from which the players can deviate, similar to the account given to justify correlated strategies in normal-form games \cite{Aumann1974welfare}.}} 
$\mgame \doteq (\numplayers, \numactions, \states, \actionspace, \actions, \rewards, \trans, \discount, \initstates)$ is an $\numplayers$-player dynamic
%\ssadie{repeated}{dynamic} \amy{repeated?!?! i hope not!} \sadie{I think people use ``repeated" to describe Markov in its wikipedia, but I guess dynamic is better?} \deni{I agree with Sadie, but happy to go either!} \amy{disagree STRONGLY! will have to fix the Wikipedia page someday, myself :)} 
game played over an infinite discrete time horizon. 
\if 0
\amy{i think we can probably cross out the rest of this paragraph, and integrate it into the next.} \sadie{I think right now is fine since we define $\states$, $\actionspace$ here and then define state-action-dependent correspondence $\actions$ in the next paragraph} \amy{but you are taking too much space to define $\states$ and $\actions$. i already (last night) moved those defnitions to the next paragraph (please review). so it is now just a question of where we define the profile notation, and we should define it entirely generically in the prelims, so that we do not have to redefine for actions and then policies and then Markov policies and so on!} \sadie{I think right now looks good to me, and I'm not sure whether we should define generic notation... I agree right now it's a too redundant, but defining  generic notation that incorporate both sets and correspondences can be hard. But if we can think of one way to do that, it would be great!} \amy{right! that is TOO hard. but generic notation for ALL sets, and for ALL correspondences!}
At each time period, the players encounter a state from a set of \mydef{states} $\states \subseteq \R^\numstates$.
Each player $\player \in \players$ then plays an \mydef{action} $\action[\player] \in \actionspace[\player]$ from an action space $\actionspace[\player] \subseteq \R^\numactions$. 
We call a collection $\action \doteq (\action[1], \hdots, \action[\numplayers]) \in \actionspace$ of actions for all players \mydef{an action profile}, where $\actionspace \doteq \bigtimes_{\player \in \players} \actionspace[\player]$ denotes the space of action profiles. 
For convenience, we also define $\actionspace[-\player] \doteq \bigtimes_{\player^\prime \neq \player \in \players} \actionspace[\player]$ as the space of actions of the players other than $\player$. 
% \deni{Add Continuous state, continuous action!} \sadie{Added them after definition of reward, transition function :)}
% Each player $\player \in \players$ is characterized by a type $\type[\player] \in \typespace[\player]$. We refer to any collection of types for all players $\type \doteq (\type[1], \hdots, \type[\numplayers])$ as a \mydef{type profile} and denote the joint type space $\typespace$ 
\fi
The game starts at time $\numhorizon = 0$ in some initial state $\staterv[0] \sim \initstates \in \simplex (\states)$ drawn randomly from a set of states $\states \subseteq \R^{\numstates}$. 
% At each each player $\player \in \players$ simultaneously takes an action from their action space $\actionspace[\player] \subseteq \R^\numactions$ repeatedly, encountering a new state from a set of \mydef{states} $\states \subset \R^{|\states|}$ and moving onto time-period $\numhorizon + 1$ every time they take an action. 
% Players start the game at an initial state determined by an initial state distribution $\initstates: \states \to [0, 1]$ s.t.\ for all states $\state \in \states$, $\initstates (\state) \geq 0$ denotes the probability of the game being initialized at state $\state$.
%\sdeni{via functions $\actionconstr[\player][\numconstr] (\state, \inner[\player], \naction[\player])$,  concave in $\inner[\player]$, for all constraints $\numconstr \in [\numconstrs]$.}{} 
At this and each subsequent time period $\numhorizon = 1, 2, \hdots$, the players encounter a state $\state[\numhorizon] \in \states$, in which each $\player \in \players$ simultaneously takes an \mydef{action} $\action[\player][][\numhorizon] \in \actions[\player] (\state[\numhorizon], \action[-\player][][\numhorizon])$ from a \mydef{set of feasible actions} $\actions[\player] (\state[\numhorizon], \action[-\player][][\numhorizon]) \subseteq \actionspace[\player] \subseteq \R^{\numactions}$, determined by a \mydef{feasible action correspondence} $\actions[\player]: \states\times \actionspace[-\player] \rightrightarrows \actionspace[\player]$, which takes as input the current state $\state[\numhorizon]$ and the other players' actions $\action[-\player][][\numhorizon] \in \actionspace[-\player]$, and outputs a subset of the $\player$th player's action space $\actionspace[\player]$. 
We define $\actions (\state, \action) \doteq \bigtimes_{\player \in \players} \actions[\player] (\state, \action[-\player])$. 
% 
% We call any collection $\action \doteq (\action[1], \hdots, \action[\numplayers]) \in \actionspace$ of actions for all players \mydef{an action profile}, where $\actionspace \doteq \bigtimes_{\player \in \players} \actionspace[\player]$ denotes the space of action profiles. Define $\actionspace[-\player] \doteq \bigtimes_{\player^\prime \neq \player \in \players} \actionspace[\player]$ as the space of actions for the $\player$th player's opponents. Each player's action $\action[\player][][\numhorizon]$ is required to lie within a set of feasible action 
% We also define the space of joint actions $\actionspace = \bigtimes_{\player \in \players} \actionspace[\player]$.
% \sdeni{We define the joint action correspondence as $\actions (\state, \action) = \bigtimes_{\player \in \players} \actions[\player] (\state, \naction[\player])$, \deni{I think we can remove this notation, don't see us using it anywhere!} \amy{is it even correct? why does $\actions$ depend on $\action$ and not $\action[-\player]$?} \deni{It is correct because it is the collection of all action profiles available to the players fixing all the players' strategies} and, as is standard in the literature, by abuse of notation
% where the set of all \mydef{jointly feasible action profiles} by $\actions (\state) = \{ \action \in \actionspace \mid \actionconstr(\state, \action) \geq \zeros \}$.
% We further define $\actionspace = \bigtimes_{\player \in \players} \actionspace[\player]$.
% }{}

Once the players have taken their actions $\action[][][\numhorizon] \doteq (\action[1][][\numhorizon], \hdots, \action[\numplayers][][\numhorizon])$, each player $\player \in \players$ receives a \mydef{reward} $\reward[\player](\state[\numhorizon][], \action[][][\numhorizon])$ given by a \mydef{reward function} $\rewards: \states \times \actionspace \to \R^\numplayers$, after which the game either ends with probability $1-\discount$, where $\discount \in (0,1)$ is called the \mydef{discount factor},\longversion{%
\footnote{
%While for ease of exposition we assume that the discount factor represents the probability of the game ending,
Our results generalize to settings with per-player discount factors $\discount_\buyer \in (0, 1)$, where the discount rates express the players' intertemporal preferences over game outcomes at each time-step. \deni{Elaborate more!}.}}
or continues on to time period $\numhorizon + 1$, transitioning to a new state $\staterv[\numhorizon + 1] \sim \trans (\cdot \mid \state[\numhorizon], \action[][][\numhorizon])$, according to a \mydef{transition} probability function $\trans: \states \times \states \times \actionspace \to \R_+$, where $\trans (\state[\numhorizon + 1] \mid \state[\numhorizon], \action[][][\numhorizon]) \in [0,1]$ denotes the probability of transitioning to state $\state[\numhorizon + 1] \in \states$ from state $\state[\numhorizon] \in \states$ when action profile $\action[][][\numhorizon] \in \actionspace$ is played. 

% \textcolor{cyan}{Alec: who are the players and how do they map to the Arrow-DeBreu economy? We should clarify which player could be interpreted as the firm and is responsible for generation of goods according to demand requirements, and who is the consumer that is susceptible to supply availability? How is the reward of the consumer related to some quantification of demand? How is the utility of the firm related to demand? This should imply some special structure for the objective function, right? }
% We can incorporate finite-action Markov games with $d \in \N_+$ actions into this framework in the usual way, simply by taking the action space for each player to be the probability simplex in $\R^{d}$: i.e., $\forall \player \in \players, \actionspace[\player] = \simplex[{d}]$, and the reward functions of the players to be multilinear in the players' actions, i.e., $\forall \state \in \states, \player \in \players, \reward[\player] (\state, \cdot)$ is multilinear. Similarly, a
% \deni{Revisit}A \mydef{Markov decision process (MDP)} is a Markov game with $\numplayers = 1$ player \cite{bellman1966dynamic}, in which case we omit all subscripts referring to the players, and ommit the any variables refering to the $\inner$-player.

%\deni{This is a technical point but in our definition we require the action correspondences to be continuous-set set valued as is standard in the literature}

% \deni{NOTE: in this paper we work only with joint policies which are deterministic in continuous action space and non-deterministic in discrete action spaces. So in discrete action spaces you have a distribution over outcomes.\amy{what's an outcome? a joint outcome?} 
% We note\amy{doesn't parse? please edit. e.g., ``do not''?} need anywhere in this paper marginal policies since we do note deal with eqm.}

% \deni{I think we should use either stationary/non-stationay or Markovian/non-Markovian but not both because in Markov games they coincide! I think we should remove stationary from our language and stick to Markovian since i think that is clearer... Either way non-Markovian includes non-stationary, but I do not think the inclusion holds the other way around. Since we look for non-markovian, and not non-stationary policies in this paper, we can scrap and referral to non-stationary etc...}
% \amy{i consulted chatGPT on this one:
% Amy: does non-Markovian imply non-stationary?
% ChatGPT: No, a process can be non-Markovian without being non-stationary, and vice versa. \\
% A non-Markovian process is one in which the future state of the process depends on not only the current state, but also on the past states. In other words, it violates the Markov property that the future state only depends on the current state. \\
% A non-stationary process is one in which the statistical properties of the process change over time. It means that the mean, variance, and other statistical properties of the process are dependent on the time index. \\
% So it is possible for a process to be non-Markovian and stationary, non-Markovian and non-stationary, Markovian and non-stationary, or Markovian and stationary. \\
% Amy, to Deni (i wouldn't want to hurt its feelings): of course, it doesn't really know what it is talking about, but it sure seems plausible! also, this is only about processes, not policies!}

% A \mydef{mixed action} $\maction[\player] \in \simplex(\actionspace[\player])$ is a probability distribution over the action space $\actionspace[\player]$ such that for all pure actions $\action \in \actionspace$ $\maction[\player] (\action) \geq 0$ denotes the probability of $\player \in \players$ playing action $\action$.

Our focus is on continuous-state and continuous-action Markov pseudo-games, where
the state and action spaces
%$\states$ and $\actionspace[\player]$, 
%for all players $\player \in \players$, 
are non-empty and compact, and the reward functions are continuous and bounded in each of $\state$ and $\action$, holding the other fixed.
\if 0
: i.e.,
$\state \mapsto \reward (\state, \action)$ is continuous and bounded, i.e., $\| \reward (\cdot, \action) \|_{\infty} 
%\le \reward_{\max} 
< \infty$ and $\action \mapsto \reward (\state, \action)$ is continuous and bounded, i.e., $\| \reward (\state, \cdot) \|_{\infty} 
%%\leq \reward_{\max}
< \infty$.
\fi

\if 0
\samy{}{Our focus is on continuous-state and continuous-action \samy{generalized Markov game}{Markov pseudo-game}s.
In a \mydef{continuous-state} \samy{generalized Markov game}{Markov pseudo-game}, 
1.~$\states$ is non-empty and compact; and 
2.~for all actions $\action \in \actionspace$, the reward function $\state \mapsto \reward (\state, \action)$ is continuous and bounded, i.e., $\| \reward (\cdot, \action) \|_{\infty} 
%\le \reward_{\max} 
< \infty$.
%for some $\reward_{\max} \in \R_{+}$.
In a \mydef{continuous-action} \samy{generalized Markov game}{Markov pseudo-game},
1.~for all states $\state \in \states$, the reward function $\action \mapsto \reward (\state, \action)$ is continuous and bounded, i.e., $\| \reward (\state, \cdot) \|_{\infty} 
%\leq \reward_{\max}
< \infty$;
%for some $\reward_{\max} \in \R_{+}$; 
2.~the action space $\actionspace[\player]$ is non-empty and compact, for all players $\player \in \players$;
and 3.~for all states $\state \in \states$ and actions $\action[-\player] \in \actionspace[-\player]$, the feasible action correspondence $\actions[i] (\state, \action[-\player])$ is continuous, non-empty, and compact.}
\fi

A \mydef{history} $\hist[][][] \in \hists[\numhorizons] \doteq (\states \times \actionspace)^\numhorizons \times \states$ of length $\numhorizons \in \N$ is a sequence of states and action profiles $\hist[][][] = ((\state[\numhorizon], \action[][][\numhorizon])_{\numhorizon = 0}^{\numhorizons-1}, \state[\numhorizons])$
%\amy{ok, now i see how your indexing works. would it be terribly difficult to change to: $\hist[][][] = (\state[0], (\action[][][\numhorizon], \state[\numhorizon])_{\numhorizon = 1}^{\numhorizons})$? the state indexing doesn't change, but you get rid of a $ + 1$ and a $-1$ in the history def'n. so i recommend making the change.} \sadie{I think the problem is that we very often use $(\state[t], \action[][][t])$ together, and this is why we want to group them together.} 
% \sadie{What about $\hist[][][] = ((\state[\numhorizon], \action[][][\numhorizon])_{\numhorizon = 0}^{\numhorizons-1}), \state[\numhorizons])$?} 
%\amy{this is fine, and better than what we have now, i guess. but then i wonder why we bother to start at time 0?} \deni{because we need to know the first state and action taken!} \amy{of course we do! i was just saying we could just as easily call them $s^1$ and $a^1$, instead of $s^0$ and $a^0$, as they are the FIRST state and action taken! but it is also fine to stick w/ their current indices.} 
s.t.\@ a history of length $0$ corresponds only to the initial state of the game. %
%%% SPACE
%\footnote{\amy{candidate for deletion} Throughout, we denote the state and action profile associated with a history at time period $\numhorizon \in \N$ with the same superscripts as that history, e.g., $(\state[\numhorizon], \action[][][\numhorizon  + 1])$ and $(\state[\numhorizon][][\prime], \action[][][\numhorizon][\prime])$ are associated with respective histories $\hist$ and $\hist[][\prime]$.} 
For any history $\hist[][][] = ((\state[\numhorizon], \action[][][\numhorizon])_{\numhorizon = 0}^{\numhorizons-1}, \state[\numhorizons])$ of length $\numhorizons \in \N$, we denote by $\hist[: \othernumhorizons]$ the first $\othernumhorizons \in [\tau^*]$ steps of $\hist$, i.e., $\hist[: \othernumhorizons] = ((\state[\numhorizon], \action[][][\numhorizon])_{\numhorizon = 0}^{\othernumhorizons-1}, \state[\othernumhorizons])$.
Overloading notation, we define the \mydef{history space} $\hists \doteq \bigcup_{\numhorizons = 0}^\infty \hists[\numhorizons]$.
% Overloading notation, we define the collection of all histories of any length $\hists \doteq \bigcup_{\numhorizon = 0}^\infty \hists[\numhorizons]$
% \deni{Might remove stochastic policy definition later, but keeping it in for the moment in case if we need it to explain things later on.}
% 
For any player $\player \in \players$,
a \mydef{policy} $\policy[\player]: \hists \to \actionspace[\player]$ is a mapping from histories of any length to $\player$'s space of (pure) actions.
% A \mydef{stochastic policy} $\policy[\player]: \bigcup_{\numhorizon = 0}^\infty \hists[\numhorizons] \to \simplex(\actionspace[\player])$ for a player $\player \in \players$, is a mapping from histories of any length to a probability distribution over the actions of player $\player$. 
We define the space of all (deterministic) policies as $\policies[\player] \doteq \{ \policy[\player]: \hists \to \actionspace[\player] \}$.\longversion{%
%, and the space of all policy profiles as $\policies \doteq \bigtimes_{\player \in \players} \policies[\player]$, i.e., collection of policies for all players.%
\footnote{
%While to keep the notation we will subsequently introduce simple we will not explicitly define mixed policies,
A mixed policy is simply a distribution over pure policies, i.e., an element of $\simplex (\policies[\player])$.
Moreover, any mixed policy can be equivalently represented as a mapping $\policy[\player][][\mathrm{mixed}]: \hists \to \simplex(\actionspace[\player])$ from histories to distributions over actions s.t. at any history $\hist \in \hists$, player $\player$ plays action $\action[\player] \sim \policy[\player] (\hist)$. 
An analogous definition extends directly to mixed Markov policies as well.}}
% and $\policies[\player][\mathrm{S}] \doteq \{ \policy[\player]: \bigcup_{\numhorizon = 0}^\infty \hists[\numhorizons] \to \simplex(\actionspace[\player]) \}$
% 
A \mydef{Markov policy} \cite{maskin2001markov} $\policy[\player]$ is a 
%\samy{deterministic}{} \amy{i don't think we need to call this out. parentheses and footnote make this explicit enough.} 
policy s.t.\@ $\policy[\player] (\state[\numhorizons]) = \policy[\player] (\hist[: \numhorizons])$, 
%\yc{Minor: Given the way history is defined, the index is off by 1, I think. $\hist[: \numhorizons]$ ends with $\state[\tau  + 1]$.} \amy{if we switch to Sadie's proposed new notation for histories, and that should fix this off-by-one error.}
for all histories $\hist \in \hists[\numhorizons]$ of length $\numhorizons \in \N_+$, where $\state[\numhorizons]$ denotes the final state of history $\hist$. 
% Analogously, we can also define a stochastic Markov policy for stochastic policies. 
As Markov policies are only state-contingent, we can compactly represent the space of all Markov policies for player $\player \in \players$ as $\markovpolicies[\player] \doteq \{ \policy[\player]: \states \to \actionspace[\player] \}$.
%, and the space of all Markov policy profiles as $\markovpolicies \doteq \bigtimes_{\player \in \players} \markovpolicies[\player]$.
% and $\policies[\player][\mathrm{SM}] \doteq \{ \policy[\player]: \states \to \simplex (\actionspace[\player]) \}$. 

%\amy{you do not need to define policies for both Markov and subclass. one is more general than the other, so the other one should just be a special case. i think subclass is more general, so just define that one, and then say that an common choice of subclass is Markov.} \amy{i took care of this.} \sadie{Thank you! Looks nice to me.}

Fixing player $\player \in \players$ and $\policy[-\player] \in \policies[-\player]$, we define the \mydef{feasible policy correspondence} $\fpolicies[\player] (\policy[-\player]) \doteq \{ \policy[\player] \in \policies[\player] \mid \forall \hist \in \hists, \policy[\player] (\hist) \in \actions[\player] (\state[\numhorizons], \policy[-\player] (\hist)) \}$, given history $\hist \in \hists[\numhorizons]$, 
%\samy{the \mydef{feasible Markov policy correspondence} $\fmarkovpolicies[\player] (\policy[-\player]) \doteq \{ \policy[\player] \in \markovpolicies[\player] \mid \forall \state \in \states, \policy[\player] (\state) \in \actions[\player] (\state, \policy[-\player] (\state)) \}$,}{} 
and the \mydef{feasible subclass policy correspondence} $\fsubpolicies[\player] (\policy[-\player]) \doteq \{ \policy[\player] \in \subpolicies[\player] \mid \forall \state \in \states, \policy[\player] (\state) \in \actions[\player] (\state, \policy[-\player] (\state)) \}$, for any $\subpolicies \subseteq \markovpolicies$. 
Of particular interest is $\fmarkovpolicies[\player] (\policy[-\player])$ itself, obtained when $\subpolicies = \markovpolicies$.
%For each player $\player$, the feasible (Markov) policy correspondence takes as input the policies of the other players, and outputs (Markov) policies of player $\player$ which take a feasible action at each state. More generally, we will also consider subsets of Markov policy classes as given by the policy subclass $\subpolicies$, for which the associated set of feasible policies will be given by the feasible subclass policy correspondence.
%\samy{We also define the feasible policy profile correspondence $\fpolicies (\policy) \doteq \bigtimes_{\player \in \players} \fpolicies[\player] (\policy[-\player])$, the the feasible Markov policy profile correspondence $\fmarkovpolicies (\policy) \doteq \bigtimes_{\player \in \players} \fmarkovpolicies[\player] (\policy[-\player])$, and the feasible subclass policy profile correspondence $\fsubpolicies (\policy) \doteq \bigtimes_{\player \in \players} \fsubpolicies[\player] (\policy[-\player])$ for any $\subpolicies\subseteq \markovpolicies$.}{} 
\if 0
\ssadie{Additionally, overloading notation, we define the \mydef{jointly feasible policy space} $\fpolicies\doteq \{ \policy \in \policies\mid \policy \in \fpolicies (\policy) \}$ and 
%the jointly feasible Markov policy profile $\fmarkovpolicies\doteq \{ \policy \in \markovpolicies\mid \policy \in \fmarkovpolicies (\policy) \}$, and 
the \mydef{jointly feasible subclass policy space} $\fsubpolicies\doteq \{ \policy \in \subpolicies\mid \policy \in \fsubpolicies (\policy) \}$ for any $\subpolicies\subseteq \markovpolicies$.
Again, of particular interest is $\fmarkovpolicies (\policy)$ itself, obtained when $\subpolicies = \markovpolicies$.}{}
\fi 
%\sadie{Do we need the non-empty, compact-valued, convex-valued assumption for this to guarantee the existence of fixed point?} \deni{Maybe we can add a footnote later about this, but I want to decide this a bit later} \sadie{we can add one sentence in the proof}

Given a policy profile $\policy \in \policies$ and a history $\hist[][][] \in \hists[\numhorizons]$,
%of length $\numhorizons$,
we define the \mydef{discounted history distribution} assuming initial state distribution $\initstates$ as
%\deni{Make policy here be dependent on history!!} \yc{Right. RHS doesn't depend on $\hist$ at the moment.} \amy{the RHS does depend on history, b/c h = (s0, a1, s1, etc.) is a history. but the POLICY does not. the policy depends only on the final state $s^{(t)}$. we want the policy to depend on $h$, which means using notation like $\pi(h_{:\tau})$, or something like this.}
%
%\begin{align}
    $\histdistrib[\initstates][\policy][\numhorizons] (\hist[][][]) = \initstates (\state[0]) \prod_{\numhorizon = 0}^{\numhorizons-1} \discount^\numhorizon \trans (\state[\numhorizon  + 1] \mid \state[\numhorizon], \action[][][\numhorizon]) \setindic[{\{ \policy (\hist[:\numhorizon]) \}}] (\action[][][\numhorizon])$.
%\endspace .
%\end{align}
%
%\amy{we are given a history, so i don't think we want the rv notation for this expression. above is correct now, i think.} \yc{Agreed with above.}
%\sadie{Like this? $$\histdistrib[\initstates][\policy][\numhorizons] (\hist[][][]) = \initstates (\state[0]) \prod_{\numhorizon = 0}^{\numhorizons-1} \discount^\numhorizon \trans (\staterv[\numhorizon  + 1] \mid \staterv[\numhorizon], \actionrv[][][\numhorizon]) \setindic[{\{ \policy (\hist[:\numhorizon]) \}}] (\actionrv[][][\numhorizon])$$}
%
    % \policy[\outeraction] (\outeraction[][][][\numhorizon], \inneraction[][][][\numhorizon]) \policy[\inneraction] (\outeraction[][][][\numhorizon], \inneraction[][][][\numhorizon]) 
%\end{align}
%
%\deni{Note, we need the indicator function because if the policies do not output the the observed actions than the trajectory is not realizable, i.e., it is occurs with prob. 0.}
%\samy{}{The indicator function ensures that $\hist$ is realizable: i.e., it is not a probability zero event.}
Overloading notation, we also define the set of all realizable trajectories $\hists[\policy][\numhorizons]$ of length $\numhorizons$ under policy $\policy$ as $ \hists[\policy][\numhorizons] \doteq \supp(\histdistrib[\initstates][\policy][\numhorizons])$, i.e., the set of all histories that occur with non-zero probability.
We then denote by
% $\histdistrib[\initstates][\policy][\numhorizons] (\hist[][][]) \doteq \Ex_{\state[0] \sim \initstates} \left[ \histdistrib[\initstates][\policy][\numhorizons] (\hist[][][], \state[0]) \right]$ and
% $\hist[][][\infty] = \hist$, and 
$\histdistrib[\initstates][\policy] \doteq \histdistrib[\initstates][\policy][\infty]$, and by $\histrv[][] = \left(\staterv[0], (\actionrv[][][\numhorizon], \staterv[\numhorizon + 1] )_{\numhorizon = 0}^{\numhorizons - 1} \right)$ any randomly sampled history from $\histdistrib[\initstates][\policy][\numhorizons]$.
%
Finally, we define the \mydef{discounted state-visitation distribution}, again assuming initial state distribution $\initstates$, as 
$\statedist[{\initstates}][\policy] (\state) = \sum_{\numhorizons = 0}^\infty\int_{\hist \in \hists[\policy][\numhorizons]: \state[\numhorizons] = \state} \histdistrib[\initstates][\policy][\numhorizons] (\hist[][][])$. 
%\amy{can you sum over all those histories? they're not drawn from a continuum? you don't need an integral?} \deni{good catch, thank you!!}

For any policy profile $\policy \in \policies$, the \mydef{state-value function} $\vfunc[][\policy]: \states \to \R^\numplayers$ and the \mydef{action-value function} $\qfunc[][\policy]: \states \times \actionspace \to \R^\numplayers$ are defined, respectively, as 
% \deni{Redefine the following.}
%\amy{can we drop the $\discount$'s from these expressions since we are sampling histories from a discounted distribution?}
% \deni{New Version}
\begin{align}
    \vfunc[][\policy] (\state) 
    &\doteq \Ex_{\staterv[\numhorizon + 1] \sim \trans(\cdot \mid \staterv[\numhorizon], \actionrv[][][\numhorizon])} \left[ \sum_{\numhorizon = 0}^\infty \rewards (\staterv[\numhorizon], \actionrv[][][\numhorizon]) \mid \staterv[0] = \state, \actionrv[][][\numhorizon ] = \policy(\staterv[\numhorizon])\right]
    \label{eq:state_value} \\
    \qfunc[][\policy] (\state, \action) 
    &\doteq \Ex_{\staterv[\numhorizon + 1] \sim \trans(\cdot \mid \staterv[\numhorizon], \actionrv[][][\numhorizon])} \left[ \sum_{\numhorizon = 0}^\infty \rewards (\staterv[\numhorizon], \actionrv[][][\numhorizon]) \mid \staterv[0] = \state, \actionrv[][][0] = \action, \actionrv[][][\numhorizon + 1] = \policy(\staterv[\numhorizon + 1 ]) \right]
    \label{eq:action_value} \enspace .
\end{align}
% \deni{Old Version}
% \begin{align}
%     \vfunc[][\policy] (\state) 
%     &\doteq \Ex_{\histrv \sim \histdistrib[\initstates][\policy]} \left[ \sum_{\numhorizon = 0}^\infty \rewards (\staterv[\numhorizon], \actionrv[][][\numhorizon]) \mid \staterv[0] = \state \right]
%     \label{eq:state_value} \\
%     \qfunc[][\policy] (\state, \action) 
%     &\doteq \Ex_{\histrv \sim \histdistrib[\initstates][\policy]} \left[ \sum_{\numhorizon = 0}^\infty \rewards (\staterv[\numhorizon], \actionrv[][][\numhorizon]) \mid \staterv[0] = \state, \actionrv[][][0] = \action \right]
%     \label{eq:action_value}
% \end{align}

%%% SPACE
%\noindent
%We also define the space of state-value and action-value functions respectively as $\vfuncs \subset \{ \vfunc: \states \to \R^\numplayers \}$ and $\qfuncs \subset  \{ \qfunc: \states \times \actionspace \to \R^\numplayers \}$. \amy{what are these spaces for? do we use them?}

% For notational clarity, we denote the state-value function and the action-value function for all states $\state \in \states$, action profiles $\action \in \actions$, and policy profile $\policy \in \actionspace^\states$, as $\statevalue[][\policy] (\state)$ and $\actionvalue[][\policy] (\state, \outeraction, \inneraction)$, respectively.
\sdeni{}{Overloading notation, for any initial state distribution $\diststates \in \simplex (\states)$ and policy profile $\policy$, we denote by $\vfunc[][\policy] (\diststates) \doteq \Ex_{\staterv \sim \diststates} \left[ \vfunc[][\policy] (\staterv) \right]$.}
%{Overloading notation, for any initial state distribution $\initstates \in \simplex (\states)$ and policy profile $\policy$, we denote by $\vfunc[][\policy] (\initstates) \doteq \Ex_{\staterv \sim \initstates} \left[ \vfunc[][\policy] (\staterv) \right]$.}

\amy{how about this instead?}

% \amy{Overloading notation, given a policy profile $\policy \in \policies$ and a history $\hist[][][] \in \hists \doteq \cup_{\numhorizons = 0}^{\infty} \hists[\numhorizons]$, we also define the total return along that history as
% %\begin{align}
%     $\vfunc[][] (\hist) 
%     \doteq \sum_{\numhorizon = 0}^\infty \rewards (\staterv[\numhorizon], \actionrv[][][\numhorizon])$,
%     %\label{eq:state_hist}
% %\end{align}
% %
% based on which we define the \mydef{value function} $\vfunc[][\policy]: \states \to \R^\numplayers$ as
% %\begin{align}
%     $\vfunc[][\policy] (\initstates) 
%     \doteq \Ex_{\histrv \sim \histdistrib[\initstates][\policy]} \left[ \vfunc[][] (\histrv) \right]$.
%     %\label{eq:state_value}
% %\end{align}
% }
%
Finally, the \mydef{(expected cumulative) payoff} associated with policy profile $\policy \in \policies$ is given by
%\amy{can we say in English what this payoff means? something like expected value of rollouts of trajectories under a fixed policy.}
%\deni{Do not want to use utility, because I am ausing it in the economy.}
%
$\payoff (\policy) \doteq \vfunc[][\policy] (\initstates)$.

% \Ex_{\state \sim \initstates} [\vfunc[\player][\policy] (\state)]$.

% \deni{Should be able to remove the Nash eqm def'n} Given a Markov game $\mgame$, we define the following. A $\varepsilon$-\mydef{Nash equilibrium ($\varepsilon$-\nash)} $\policy[][][*] \in (\outeractions \times \inneractions)^\states$ is a policy profile such that $\min_{\policy[\outer] \in \outeractions^\states} \cumulutil(\policy[\outer][][], \policy[\inner][][*]) + \varepsilon \geq \cumulutil(\policy[][][*]) \geq \max_{\policy[\inner] \in \inneractions^\states} \cumulutil(\policy[\outer][][*], \policy[\inner]) - \varepsilon$. 


\subsection{Solution Concepts and Existence}
\label{sec:gmg_exist}

Having defined our game model, we now define two natural solution concepts, and establish their existence.
Our first solution concept is based on the usual notion of Nash equilibrium (\citeyear{nash1950existence}), yet applied to Markov pseudo-games.
Our second is based on the notion of subgame-perfect equilibrium in extensive-form games, a strengthening of Nash equilibrium with the additional requirement that an equilibrium be Nash not just at the start of the game, but at all states encountered during play.
In the context of stochastic games, such equilibria are called ``recursive,'' or ``Markov perfect.''
Following \citet{bellman1966dynamic} and \citet{arrow-debreu}, we identify natural assumptions that guarantee the existence of equilibrium in \sdeni{}{pure} Markov
%%% i'm not sure, "and hence" is correct. not important, so let's just delete.
%(and hence, \sdeni{stationary}{time-invariant} \deni{since we use stationary point as a word, let's avoid stationary here without defining it imo.} \amy{ok, but need a FN, since stationary is standard.}) 
policies, meaning deterministic policies that depend only on the current state, not on the history.
When applied to incomplete stochastic economies, this theorem implies existence of recursive Radner (or competitive) equilibrium, to our knowledge the first result of its kind.

%\amy{PURE? we added ``pure" to the abstract. what are we going to say about pure anywhere else? in the intro, etc.}

An $\varepsilon$-\mydef{generalized Markov perfect equilibrium ($\varepsilon$-\MPGNE)} $\policy[][][*] \in \fmarkovpolicies (\policy[][][*])$ is a Markov policy profile s.t.\@ for all states $\state \in \states$ and players $\player \in \players$, $\vfunc[\player][{\policy[][][*]}] (\state) \geq \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state) - \varepsilon$.  
An $\varepsilon$-\mydef{generalized Nash equilibrium ($\varepsilon$-GNE)} $\policy[][][*] \in \fpolicies (\policy[][][*])$ is a policy profile s.t.\@ for all states $\state \in \states$ and players $\player \in \players$, $\payoff[\player] ({\policy[][][*]}) \geq \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \payoff[\player] (\policy[\player], \policy[-\player][][*]) - \varepsilon$. 
We call a $0$-\MPGNE{} ($0$-GNE) simply a \MPGNE{} (GNE).
As \MPGNE{} is a stronger notion than GNE, every $\varepsilon$-\MPGNE{} is an $\varepsilon$-GNE.

%\yc{Is every $\varepsilon$-\MPGNE{} an $\varepsilon$-GNE? If so, mention it explicitly.} \amy{yes, just like every subgame perfect Nash is Nash.}

% Let $\policy[][][*] \in {(\outeractions \times \inneractions)}^\states$ be an MPGNE, for convenience we define the \mydef{optimal state-value function} and \mydef{the optimal action-value functions} as $\statevalue[][*] \doteq \statevalue[][{\policy[][][*]}]$ and $\actionvalue[][*] \doteq \actionvalue[][{\policy[][][*]}]$ respectively.
% We call a $0$-NE and $0$-MPNE simply NE and MPNE respectively, and note that any NE is a MPNE with probability 1. \deni{Check with Alec if whether when distribution has full support on state space then the two sets are just equal.}  For MDPs, i.e., when $\numplayers = 1$, a $\varepsilon$-MPNE is simply a $\varepsilon$-optimal policy. 
% Given a Markov game $\game$, a \mydef{Markov perfect (Nash) equilibrium (MPE)} $\policy[][][*] \in \simplex(\actionspace)$ is a policy profile such that for all players $\player \in \players$, and states $\state \in \states$, $\policy[\player][][*] \in \argmax_{\policy[\player] \in \actionspace^\states} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)$. We note that if $\game$ is an MDP, then a Nash equilibrium is simply called an \mydef{non-Markovian optimal policy} and a Markov perfect equilibrium is called a \mydef{Markov optimal policy}.
% The 
% % \mydef{regret} $\regret[][\util]: \actionspace \times \actionspace \to \R^\numplayers$ for playing an action profile $\action$ as compared to another action profile $\otheraction$, as follows: for all followers $\player \in \players$,
% % $\regret[\player][] (\action, \otheraction; \outer) = \util[\player] (\outer, (\otheraction[\player], \action[-\player])) - \util[\player] (\outer, \action)$. The 
% \mydef{cumulative regret}, $\gcumulregret[][]: \actionspace^\states \times \actionspace^\states \times \params \to \R$ between two policy profiles $\policy \in \actionspace^\states$ and $\otherpolicy \in \actionspace^\states$, for any parameter $\param \in \params$, across all players in a game as 
% $$\gcumulregret[][] (\policy, \otherpolicy; \param) = \sum_{\player \in \players}  \util[\player] (\otherpolicy[\player], \policy[-\player]; \theta) - \util[\player] (\policy; \theta)
% % = \Ex_{\histrv \sim \histdistrib[\initstates][{(\otherpolicy[\player], \policy[-\player]})]} \left[\sum_{\numhorizon = 0}^\infty \discount^\numhorizon \reward[\player] (\staterv[\numhorizon], \actionrv[][][\numhorizon]) \right] - \Ex_{\histrv \sim \histdistrib[\initstates][{\policy}]} \left[\sum_{\numhorizon = 0}^\infty \discount^\numhorizon \reward[\player] (\staterv[\numhorizon], \actionrv[][][\numhorizon]) \right] 
% \enspace . 
% $$
% Further, the \mydef{exploitability} or (Nikaido-Isoda potential function \cite{nikaido1955note}) of a policy profile $\policy \in \actionspace^\states$ for any parameter $\param \in \params$ is defined as 
% $\exploit[][] (\policy; \param) = \max_{\otherpolicy \in \actionspace^\states} \gcumulregret[][] (\policy, \otherpolicy; \param)$ \cite{goktas2022exploit}. 
% We note that fixing any parameter $\param \in \params$, for all $\action \in \actionspace$, $\exploit (\action; \param) \geq 0$; moreover, $\action[][][][*]$ is a Nash equilibrium of $\game$ iff $\exploit[] (\action[][][][*]; \param) = 0$. 

\begin{assumption}[Existence] \label{assum:existence_of_mpgne}
For all $\player \in \players$, assume
1.~$\actionspace[\player]$ is convex;
%non-empty, convex, and compact;
2.~$\actions[\player] (\state, \cdot)$ is upper- and lower-hemicontinuous, for all $\state \in \states$; 
3.~$\actions[\player] (\state, \action[-\player])$ is non-empty, convex, and compact, for all $\state \in \states$ and $\action[-\player] \in \actionspace[-\player]$; and
4.~for any policy $\policy \in \policies$, $\action[\player] \mapsto \qfunc[\player][\policy] (\state, \action[\player], \action[-\player])$ is continuous and concave over $\actions[\player] (\state, \action[-\player])$, for all $\state \in \states$ and $\action[-\player] \in \actionspace[-\player]$.
\end{assumption}

\begin{assumption}[Policy Class] \label{assum:policy_class_exist}
Given 
%a subspace of the space of Markov policy profiles 
$\subpolicies \subseteq \markovpolicies$, assume
1.~$\subpolicies$ is non-empty, compact, and convex; and
2.~(Closure under policy improvement) for each $\policy \in \subpolicies$, there exists $\policy[][][+] \in \subpolicies$ s.t.\@ $\qfunc[\player][\policy] (\state, \policy[\player][][+] (\state), \policy[-\player] (\state)) = \max_{\policy[\player][][\prime] \in \fpolicies (\policy[-\player])} \qfunc[\player][\policy] (\state, \policy[\player][][\prime] (\state), \policy[-\player] (\state))$, for all $\player \in \players$ and $\state \in \states$. 

%\amy{$\policy[\player][][\prime] \in \fmarkovpolicies$? or $\fsubpolicies$?} \sadie{hmmm actually I think it maybe should be $\fpolicies$ since we want to say your best response is within the subclass!} \amy{so that is $\fsubpolicies$, not $\fpolicies$, correct? i think it should be $\fsubpolicies$.} \sadie{I don't think so, the idea is pi+ can do better than any other policy, even those not in sub class!} \amy{but if that is the case, then how is the class CLOSED under policy improvement? i thought the point was that all best responses should be INSIDE the class, not outside it?} \amy{oh yes, i get it now!!! we can do as well as ANY policy, even outside the class, with policies inside the class. perfect. will fix! thanks!}

%\amy{why do you need both $\policy$ and $\policy[][][\prime]$ in this def'n?} \sadie{I think we do need this in the proof of \Cref{thm:existence_of_mpgne}, but I'm also a little bit not sure here. Do you think the policy fixed by other players doesn't matter?} \amy{the policy of the other players definitely matters! but i don't think the quantification is correct. how could you find a $\policy[][][+]$ that works well against ALL $\policy[][][\prime]$?} \sadie{Never mind I think we can drop the $\policy[][][\prime]$! I updated the proof!}
\end{assumption}

%\amy{does Bhandari justify this assumption? stating that this is NOT an impossible assumption to satisfy.} \sadie{He didn't explicitly justify this, but I agree that we should add some justification. I think the idea is to make sure the policy class is expressive enough so that we can always best-respond within the class.}

Assumption 2, introduced as Condition 1 in \citeauthor{bhandari2019global} (\citeyear{bhandari2019global}), ensures that the policy class under consideration (e.g., $\subpolicies \subseteq \markovpolicies$)
%---in our case, feasible \samy{Markov}{} \amy{?} \sadie{It can be any policy class! In our case is the later parameterized policy class} policies---is expressive enough to include best responses.} 
is expressive enough to include best responses.

\begin{restatable}{theorem}{thmexistmpgne}
\label{thm:existence_of_mpgne}
Let $\mgame$
%$= (\numplayers, \numactions, \states, \actionspace, \actions, \rewards, \trans, \discount, \initstates)$ 
be a Markov pseudo-game for which \Cref{assum:existence_of_mpgne} holds, 
and let $\subpolicies\subseteq \markovpolicies$ be a subspace of Markov policy profiles that satisfies \Cref{assum:policy_class_exist}. 
Then there exists a policy $\policy[][][*] \in \subpolicies$ such that $\policy[][][*]$ is an 
%Markov Perfect generalized Nash equilibrium
\MPGNE{} of $\mgame$.
%, i.e., $\policy[][][*] \in \fmarkovpolicies (\policy[][][*])$ and for all states $\state \in \states$, $\vfunc[\player][{\policy[][][*]}] (\state) \geq \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)$ for all $\player \in \players$.
\end{restatable}

% \begin{proof}
%     % First, for our policy subspace $\subpolicies\subseteq \markovpolicies$, we define the \mydef{supporting action space} for each player $\player \in \players$ as $\subactionspace[\player] = \{ \action[\player] \in \actionspace[\player] \mid \exists \pi\in \subpolicies, \state \in \states\text{ s.t. } \policy[\player] (\state) = \action[\player] \}$ and the \mydef{supporting action profile space} by $\bigtimes_{\player \in \players} \subactionspace[\player]$. Moreover, for each player $\player \in \players$, we define the \mydef{supporting feasible action correspondence} $\subactions: \state \times \subactionspace[-\player] \rightrightarrows \subactionspace[\player]$  by $\subactions[\player] (\state, \action[-\player]) = \{ \action[\player] \in \subactionspace[\player] \mid \action[\player] \in \actions (\state, \action[-\player]) \}$, and we denote the \mydef{supporting feasible action profile correspondence} $\subactions (\state, \action) \doteq \bigtimes_{\player \in \players} \subactions[\player] (\state, \action[-\player])$.
    
%     For any player $\player \in \players$ and state $\state \in \states$, we define the \mydef{individual state best-response correspondence} $\brmap[\player][\state]: \subpolicies \rightrightarrows \actionspace[\player]$ by 
%     \begin{align}
%         % \brvalue[\player][\state] (\vfunc[\player][\policy], \action[-\player])
%         % &\doteq \max_{\action[\player] \in \actions[\player] (\state, \action[-\player])} 
%         % \reward[\player] (\state, \action[\player], \action[-\player]) +\Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \action[\player], \action[-\player])}[ \discount \vfunc[\player][\policy] (\staterv[][][\prime]) ] \\
%         % &= \max_{\action[\player] \in \actions[\player] (\state, \action[-\player])} \qfunc[\player][\policy] (\state, \action[\player], \action[-\player]) \\
%         \brmap[\player][\state] (\policy)
%         &\doteq 
%         \argmax_{\action[\player] \in \actions[\player] (\state, \policy[-\player] (\state))}  
%         \reward[\player] (\state, \action[\player], \policy[-\player] (\state)) +\Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \action[\player], \policy[-\player] (\state))}[ \discount \vfunc[\player][\policy] (\staterv[][][\prime]) ] \\
%         &= \argmax_{\action[\player] \in \actions[\player] (\state, \policy[-\player] (\state))}
%         \qfunc[\player][\policy] (\state, \action[\player], \policy[-\player] (\state))
%     \end{align}
    
%     Then, for any player $\player \in \players$, we define the \mydef{restricted individual best-response correspondence} $\brmap[\player]: \subpolicies \rightrightarrows \subpolicies[\player]$ as the Cartesian product of individual state best-response correspondences across the states restricted to $\subpolicies$: 
%     \begin{align}
%         % \brvalue[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime])
%         % &= \bigtimes_{\state \in \states} \brvalue[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)) \\
%         \brmap[\player] (\policy)
%         &= \left(\bigtimes_{\state \in \states} \brmap[\player][\state] (\policy) \right) \bigcap \subpolicies[\player]
%         \\
%         &= \{ \policy[\player] \in \subpolicies[\player] \mid \policy[\player] (\state) \in \brmap[\player][\state] (\policy), \forall\; \state \in \states \}
%     \end{align}

%     Finally, we can define the \mydef{multi-player best-response correspondence} $\brmap: \subpolicies \rightrightarrows \subpolicies$ as the Cartesian product of the individual correspondences, i.e., $\brmap(\policy) \doteq \bigtimes_{\player \in \players} \brmap[\player] (\policy)$.

%     To show the existence of \MPGNE, we first want to show that there exists a fixed point $\policy[][][*] \in \subpolicies$ such that $\policy[][][*] \in \brmap(\policy[][][*])$. 
%     To this end, we need to show that 1.~for any $\policy\in \subpolicies$, $\brmap(\policy)$ is non-empty, compact, and convex; 2.~$\brmap$ is upper hemicontinuous.
    
%     Take any $\policy\in \subpolicies$. 
%     We can approach from individual state best-response correspondences. Fix $\player \in \players, \state \in \states$, 
%      we know that $\action[\player] \mapsto \qfunc[\player][{\policy}] (\state, \action[\player], \policy[-\player] (\state))$ is concave in $\actions[\player] (\state, \policy[-\player] (\state))$ and $\actions[\player] (\state, \policy[-\player] (\state))$ is non-empty, convex, and compact by \Cref{assum:existence_of_mpgne}, then by Proposition 4.1 of \cite{fiacco1986convexity}, $\brmap[\player][\state] (\policy)$ is non-empty, compact, and convex. 
%     Then, we move to $\brmap[\player] (\policy)$. For any $\player \in \players$, $\bigtimes_{\state \in \states} \brmap[\player][\state] (\policy)$ is compact and convex as it is a Cartesian product of compact, convex sets. Thus, as $\subpolicies$ is also compact and convex by \Cref{assum:policy_class_exist}, we know that $\brmap[\player] (\policy) = \left(\bigtimes_{\state \in \states} \brmap[\player][\state] (\policy) \right) \bigcap \subpolicies[\player]$ is compact and convex. 
%     By (Closure under policy improvement) of \Cref{assum:policy_class_exist}, we know that since $\policy \in \subpolicies$, there exists $\policy[][][+] \in \subpolicies$ such that $\policy[\player][][+] \in \argmax_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])} \qfunc[\player][\policy] (\state, \policy[\player][][\prime] (\state), \policy[-\player] (\state))$ for all $\state \in \states$, and that means $\policy[\player][][+] (\state) \in \brmap[\player][\state] (\policy)$ for all $\state \in \states$. Thus, $\brmap[\player] (\policy)$ is also non-empty. 
%      Since Cartesian product preserves non-emptiness, compactness, and convexity, we can conclude that 
%     $\brmap(\policy) = \bigtimes_{\player \in \players} \brmap[\player] (\policy)$ is non-empty, compact, and convex. 
    
%      Similarly, fix $\player \in \players, \state \in \states$, for any $\policy\in \subpolicies$, since $\actions[\player] (\state, \cdot)$ is continuous (i.e. both upper and lower hemicontinuous), by the Maximum theorem, $\brmap[\player][\state]$ is upper hemicontinuous. 
%     $\policy\mapsto \bigtimes_{\state \in \states} \brmap[\player][\state] (\policy)$ is upper hemicontinuous as it is a Cartesian product of upper hemicontinuous correspondence, and consequently, $\policy\mapsto \left(\bigtimes_{\state \in \states} \brmap[\player][\state] (\policy) \right) \bigcap \subpolicies$ is also upper hemicontinuous. 
%     Therefore, $\brmap$ is also upper hemicontinuous. 
    
%     Since $\brmap(\policy)$ is non-empty, compact, and convex for any $\policy\in \subpolicies$ and $\brmap$ is upper hemicontinuous, by Fan's fixed-point theorem \cite{Fan1952FixedPoint}, $\brmap$ admits a fixed point. 

%     Finally, say $\policy[][][*] \in \subpolicies$ is a fixed point of $\brmap$, and we want to show that $\policy[][][*]$ is a Markov Perfect generalized Nash equilibrium (\MPGNE) of $\mgame$.  Since $\policy[][][*] \in \brmap(\policy[][][*]) = \bigtimes_{\player \in \players} \brmap[\player] (\policy[][][*])$, we know that for all $\player \in \players$, $\policy[\player][][*] (\state) \in \brmap[\player][\state] (\policy[][][*])=\argmax_{\action[\player] \in \actions[\player] (\state, \policy[-\player][][*] (\state))}
%         \qfunc[\player][{\policy[][][*]}] (\state, \action[\player], \policy[-\player][][*] (\state))$. 
%     We want to show that for any $\player \in \players$, for any $\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])$, $\vfunc[\player][{\policy[][][*]}] (\state) \geq \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)$ for all $\state \in \states$. Take any policy $\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])$. Note that $\policy[\player]$ may not be Markov, so we denote $\{ \policy[\player] (\hist[: \numhorizon]) \}_{\numhorizon\in \N}=\{ \action[\player][][\numhorizon] \}_{\numhorizon\in \N}$. Then,
%     for all $\state[0] \in \states$,
%     \begin{align}
%         \vfunc[\player][{\policy[][][*]}] (\state[0])
%         &= \qfunc[\player][{\policy[][][*]}] (\state[0], \policy[\player][][*] (\state[0]), \policy[-\player][][*] (\state[0])) \\
%         &=\max_{\action[\player] \in \actions[\player] (\state[0], \policy[-\player][][*] (\state[0]))}
%         \qfunc[\player][{\policy[][][*]}] (\state[0], \action[\player], \policy[-\player][][*] (\state[0])) \\
%         &= \max_{\action[\player] \in \actions (\state[0], \policy[-\player][][*] (\state[0]))} 
%         \reward[\player] (\state[0], \action[\player], \policy[-\player][][*] (\state[0])) 
%         + \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player], \policy[-\player][][*] (\state[0]))}[\discount \vfunc[\player][{\policy[][][*]}] (\state[1]) ] \\
%         &\geq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) 
%         + \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}[\discount \vfunc[\player][{\policy[][][*]}] (\state[1]) ]
%     \end{align}
%     Define $\vfunc[\player][\prime] (\state[0]) \doteq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) 
%     + \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}[\discount \vfunc[\player][{\policy[][][*]}] (\state[1]) ]$ for any $\state[0] \in \states$. 
%     Since $\vfunc[\player][{\policy[][][*]}] (\state) \geq \vfunc[\player][\prime] (\state)$ for all $\player \in \players$, $\state \in \states$, we have for any $\state[0] \in \states$
%     \begin{align}
%         \vfunc[\player][{\policy[][][*]}] (\state[0]) 
%         &\geq 
%         \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) +
%         \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))} [\discount \vfunc[\player][{\policy[][][*]}] (\state[1]) ] \\
%         & \geq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) +
%         \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}[ \discount \vfunc[\player][\prime] (\state[1])] \\
%         & \geq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) +
%         \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}
%         \bigg[ \discount \bigg (\reward[\player] (\state[1], \action[\player][][1], \policy[-\player][][*] (\state[1])) \\
%         &+\Ex_{\state[2] \sim \trans (\cdot\mid \state[1], \action[\player][][1], \policy[-\player][][*] (\state[1])}[ \discount \vfunc[\player][{\policy[][][*]}] (\state[2]) ] \bigg) \bigg] \\
%          & \geq \reward[\player] (\state[0], \action[\player][][0], \policy[-\player][][*] (\state[0])) +
%         \Ex_{\state[1] \sim \trans (\cdot\mid \state[0], \action[\player][][0], \policy[-\player][][*] (\state[0]))}
%         \bigg[ \discount \bigg (\reward[\player] (\state[1], \action[\player][][1], \policy[-\player][][*] (\state[1])) \\
%         &+\Ex_{\state[2] \sim \trans (\cdot\mid \state[1], \action[\player][][1], \policy[-\player][][*] (\state[1])}[ \discount \vfunc[\player][\prime] (\state[2]) ] \bigg) \bigg] \\
%        & \vdots\\
%        &\geq \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)
%     \end{align}

%     We therefore conclude that for all states $\state \in \states$, $\vfunc[\player][{\policy[][][*]}] (\state) \geq \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)$, for all $\player \in \players$.
% \end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD!!!!!
% \begin{proof}
%     % First, for our policy subspace $\subpolicies\subseteq \markovpolicies$, we define the \mydef{supporting action space} for each player $\player \in \players$ as $\subactionspace[\player] = \{ \action[\player] \in \actionspace[\player] \mid \exists \pi\in \subpolicies, \state \in \states\text{ s.t. } \policy[\player] (\state) = \action[\player] \}$ and the \mydef{supporting action profile space} by $\bigtimes_{\player \in \players} \subactionspace[\player]$. Moreover, for each player $\player \in \players$, we define the \mydef{supporting feasible action correspondence} $\subactions: \state \times \subactionspace[-\player] \rightrightarrows \subactionspace[\player]$  by $\subactions[\player] (\state, \action[-\player]) = \{ \action[\player] \in \subactionspace[\player] \mid \action[\player] \in \actions (\state, \action[-\player]) \}$, and we denote the \mydef{supporting feasible action profile correspondence} $\subactions (\state, \action) \doteq \bigtimes_{\player \in \players} \subactions[\player] (\state, \action[-\player])$.
    
%     For any player $\player \in \players$ and state $\state \in \states$, we define the \mydef{individual state best-response correspondence} $\brmap[\player][\state]: \vfuncs[\player] \times \actionspace[-\player] \rightrightarrows \R
%     \times \actionspace[\player]$ by $\brmap[\player][\state] (\vfunc[\player][\policy], \action[-\player]) = \{(\brvalue[\player][\state] (\vfunc[\player][\policy], \action[-\player]) \}, \brset[\player][\state] (\vfunc[\player][\policy], \action[-\player]))$ where
%     %
%     \begin{align}
%         \brvalue[\player][\state] (\vfunc[\player][\policy], \action[-\player])
%         &\doteq \max_{\action[\player] \in \actions[\player] (\state, \action[-\player])} 
%         \reward[\player] (\state, \action[\player], \action[-\player]) +\Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \action[\player], \action[-\player])}[ \discount \vfunc[\player][\policy] (\staterv[][][\prime]) ] \\
%         &= \max_{\action[\player] \in \actions[\player] (\state, \action[-\player])} \qfunc[\player][\policy] (\state, \action[\player], \action[-\player]) \\
%         \brset[\player][\state] (\vfunc[\player][\policy], \action[-\player])
%         &\doteq 
%         \argmax_{\action[\player] \in \actions[\player] (\state, \action[-\player])}  
%         \reward[\player] (\state, \action[\player], \action[-\player]) +\Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \action[\player], \action[-\player])}[ \discount \vfunc[\player][\policy] (\staterv[][][\prime]) ] \\
%         &= \argmax_{\action[\player] \in \actions[\player] (\state, \action[-\player])}
%         \qfunc[\player][\policy] (\state, \action[\player], \action[-\player])
%     \end{align}
    
%     Then, for any player $\player \in \players$, we define the \mydef{restricted individual best-response correspondence} $\brmap[\player]: \vfuncss[\player] \times \subpolicies[-\player] \rightrightarrows \vfuncss[\player] \times \subpolicies[\player]$ as the Cartesian product of individual state best-response correspondences across the states restricted to $\subpolicies$: $\brmap[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]) =(\brvalue[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]), \brset[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]))$ where
%     \begin{align}
%         \brvalue[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime])
%         &= \bigtimes_{\state \in \states} \brvalue[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)) \\
%         \brset[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime] )
%         &= \left(\bigtimes_{\state \in \states} \brset[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)) \right) \bigcap \subpolicies
%         \\
%         &= \{ \policy[\player] \in \subpolicies\mid \policy[\player] (\state) \in \brset[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)), \forall\; \state \in \states \}
%     \end{align}
%     % = \bigtimes_{\state \in \states} \brmap[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state))
%     % =(\brvalue[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]), \brset[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]))
%     % =(\bigtimes_{\state \in \states} \{ \brvalue[\player][\state] (\vfunc[\policy], \policy[-\player][][\prime] (\state)) \}, \bigtimes_{\state \in \states} \brset[\player][\state] (\vfunc[\policy], \policy[-\player][][\prime] (\state)))$. 
%     We need to be careful to ensure that $\brmap[\player]$ is well-defined by showing that $\bigtimes_{\state \in \states} \{ \brvalue[\player][\state] (\vfunc[\policy], \policy[-\player][][\prime] (\state)) \} \subseteq \vfuncss[\player]$, i.e., $\bigtimes_{\state \in \states} \brvalue[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)) \in \vfuncss[\player]$.
%     In fact, for any state $\state \in \states$, pick any $\policy[\player][][+] (\state) \in \brset[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state))$ and define $\policy[\player][][+] = \bigtimes_{\state \in \states} \policy[\player][][+] (\state)$,
%     it is easy to verify that $\brvalue[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)) = \vfunc[\player][{(\policy[\player][][+], \policy[-\player][][\prime])}] (\state)$, which implies that $\bigtimes_{\state \in \states} \brvalue[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)) \in \vfuncs$. 

%     Finally, we can define the \mydef{multi-player best-response correspondence} $\brmap: \vfuncs \times \subpolicies \rightrightarrows \vfuncs \times \subpolicies$ as the Cartesian product of the individual correspondences, i.e., $\brmap\doteq \bigtimes_{\player \in \players} \brmap[\player]$.

%     To show the existence of \MPGNE, we first want to show that there exists a fixed point $(\vfunc[][*], \policy[][][*]) \in \vfuncss\times \subpolicies$ such that $(\vfunc[][*], \policy[][][*]) \in \brmap(\vfunc[][*], \policy[][][*])$. 
%     To this end, we need to show that 1.~for any $(\vfunc[][\policy], \policy[][][\prime]) \in \vfuncss\times \subpolicies$, $\brmap(\vfunc[][\policy], \policy[][][\prime])$ is non-empty, compact, and convex; 2.~$\brmap$ is upper hemicontinuous.
    
%     Take any $(\vfunc[][\policy], \policy[][][\prime]) \in \vfuncss\times \subpolicies$. 
%     We can approach from individual state best-response correspondences. Fix $\player \in \players, \state \in \states$, 
%      we know that $\action[\player] \mapsto \qfunc[\player][{\policy}] (\state, \action[\player], \policy[-\player][][\prime] (\state))$ is concave in $\actions[\player] (\state, \policy[-\player][][\prime] (\state))$ and $\actions[\player] (\state, \policy[-\player][][\prime] (\state))$ is non-empty, convex, and compact by \Cref{assum:existence_of_mpgne}, then by Proposition 4.1 of \cite{fiacco1986convexity}, $\brset[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state))$ is non-empty, compact, and convex. 
%      Moreover, as a singleton, $\{ \brvalue[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)) \}$ is also non-empty, compact, and convex. Thus, $\brmap[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state))$ is non-empty, compact, and convex. 
%     Then, we move to $\brmap[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime])$. For any $\player \in \players$, $\bigtimes_{\state \in \states} \brmap[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state))$ is compact and convex as it is a Cartesian product of compact, convex sets. Thus, as $\subpolicies$ is also compact and convex by \Cref{assum:policy_class_exist}, we know that $\brset[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]) = \left(\bigtimes_{\state \in \states} \brset[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state)) \right) \bigcap \subpolicies$ is compact and convex. 
%     By (Closure under policy improvement) of \Cref{assum:policy_class_exist}, we know that since $\policy \in \subpolicies$, there exists $\policy[][][+] \in \subpolicies$ such that $\policy[\player][][+] \in \argmax_{\policy[\player][][\prime\prime] \in \fmarkovpolicies[\player] (\policy[-\player][][\prime])} \qfunc[\player][\policy] (\state, \policy[\player][][\prime\prime] (\state), \policy[-\player][][\prime] (\state))$ for all $\state \in \states$, and that means $\policy[\player][][+] (\state) \in \brset[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state))$ for all $\state \in \states$. Thus, $\brset[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime])$ is also non-empty. 
%     Additionally, $\brvalue[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]) = \bigtimes_{\state \in \states} \brvalue[\player][\state] (\vfunc[\player][\policy], \policy[-\player][][\prime] (\state))$ is non-empty, compact, and convex as it is simply a Cartesian product of non-empty, compact, and convex sets. Therefore, $\brmap[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]) =(\brvalue[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]), \brset[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime]))$ is non-empty, compact, and convex for any $\player \in \players$. 
%      Since Cartesian product preserves non-emptyness, compactness, and convexity, we can conclude that 
%     $\brmap(\vfunc[][\policy], \policy[][][\prime]) = \bigtimes_{\player \in \players} \brmap[\player] (\vfunc[\player][\policy], \policy[-\player][][\prime])$ is non-empty, compact, and convex. 
    
%      Similarly, fix $\player \in \players, \state \in \states$, for any $(\vfunc[\player], \action[-\player]) \in \vfuncss[\player] \times \actionspace[-\player]$, since $\actions[\player] (\state, \cdot)$ is continuous (i.e. both upper and lower hemicontinuous), by the Maximum theorem, $\brmap[\player][\state]$ is upper hemicontinuous. 
%     $(\vfunc[\player], \policy[-\player]) \mapsto \bigtimes_{\state \in \states} \brset[\player][\state] (\vfunc[\player], \policy[-\player] (\state))$ is upper hemicontinuous as it is a Cartesian product of upper hemicontinuous correspondence, and consequently, $(\vfunc[\player], \policy[-\player]) \mapsto \left(\bigtimes_{\state \in \states} \brset[\player][\state] (\vfunc[\player], \policy[-\player] (\state)) \right) \bigcap \subpolicies$ is also upper hemicontinuous. 
%     Therefore, $\brmap$ is also upper hemicontinuous. 
    
%     Since $\brmap(\vfunc[][\policy], \policy[][][\prime])$ is non-empty, compact, and convex for any $(\vfunc[][\policy], \policy[][][\prime]) \in \vfuncss\times \subpolicies$ and $\brmap$ is upper hemicontinuous, by Fan's fixed-point theorem \cite{Fan1952FixedPoint}, $\brmap$ admits a fixed point. 

%     Finally, say $(\vfunc[][*], \policy[][][*]) \in \vfuncss\times \subpolicies$ is a fixed point of $\brmap$, and we want to show that $\policy[][][*]$ is a Markov Perfect generalized Nash equilibrium (\MPGNE) of $\mgame$.  Since $(\vfunc[][*], \policy[][][*]) = \brmap(\vfunc[][*], \policy[][][*]) = \bigtimes_{\player \in \players} \brmap[\player] (\vfunc[\player][][*], \policy[-\player][][*])$, we know that for any $\player \in \players$, $\state \in \states$, $\vfunc[\player][*] (\state) = \brvalue[\player][\state] (\vfunc[\player][*], \policy[-\player][][*] (\state))$ and $\policy[\player][][*] (\state) \in \brset[\player][\state] (\vfunc[\player][*], \policy[-\player][][*] (\state))$. 

%     First, we claim that for any $\player \in \players$, for any $\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])$, $\vfunc[\player][*] (\state) \geq \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)$ for all $\state \in \states$. Take any policy $\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])$, for all $\state \in \states$,
%     \begin{align}
%         \vfunc[\player][*] (\state)
%         &= \brvalue[\player][\state] (\vfunc[\player][*], \policy[-\player][][*] (\state)) \\
%         &= \max_{\action[\player] \in \actions (\state, \policy[-\player][][*] (\state))} 
%         \Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \action[\player], \policy[-\player][][*] (\state))}[\reward[\player] (\state, \action[\player], \policy[-\player][][*] (\state)) + \discount \vfunc[\player][*] (\staterv[][][\prime]) ] \\
%         &\geq \Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \policy[\player] (\state), \policy[-\player][][*] (\state))}[\reward[\player] (\state, \policy[\player] (\state), \policy[-\player][][*] (\state)) + \discount \vfunc[\player][*] (\staterv[][][\prime]) ]
%     \end{align}
%     Define $\vfunc[\player][\prime] (\state) \doteq \reward[\player] (\state, \policy[\player] (\state), \policy[-\player][][*] (\state)) +\Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \policy[\player] (\state), \policy[-\player][][*] (\state))}[ \discount \vfunc[\player][*] (\staterv[][][\prime]) ]$. Since $\vfunc[\player][*] (\state) \geq \vfunc[\player][\prime] (\state)$ for all $\player \in \players$, $\state \in \states$, we have
%     \begin{align}
%         \vfunc[\player][*] (\state) 
%         &\geq 
%         [\reward[\player] (\state, \policy[\player] (\state), \policy[-\player][][*] (\state)) +
%         \Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \policy[\player] (\state), \policy[-\player][][*] (\state))} \discount \vfunc[\player][*] (\staterv[][][\prime]) ] \\
%         & \geq \reward[\player] (\state, \policy[\player] (\state), \policy[-\player][][*] (\state)) +
%         \Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \policy[\player] (\state), \policy[-\player][][*] (\state))}[ \discount \vfunc[\player][\prime] (\staterv[][][\prime])] \\
%         & \geq \reward[\player] (\state, \policy[\player] (\state), \policy[-\player][][*] (\state)) +
%         \Ex_{\staterv[][][\prime] \sim \trans (\cdot\mid \state, \policy[\player] (\state), \policy[-\player][][*] (\state))}
%         [ \discount (\reward[\player] (\staterv[][][\prime], \policy[\player] (\staterv[][][\prime]), \policy[-\player][][*] (\staterv[][][\prime])) +\Ex_{\staterv[][][\prime\prime] \sim \trans (\cdot\mid \staterv[][][\prime], \policy[\player] (\staterv[][][\prime]), \policy[-\player][][*] (\staterv[][][\prime])}[ \discount \vfunc[\player][\prime] (\staterv[][][\prime\prime]) ])] \\
%        & \vdots\\
%        &\geq \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)
%     \end{align}

%     Then, we want to show that $\vfunc[\player][{\policy[][][*]}] (\state) \geq \vfunc[\player][*] (\state)$ for any $\player \in \players$, $\state \in \states$. 
%     % Since $\vfunc[][*] \in \vfuncss$, we can say that $\vfunc[][*] = \vfunc[][\policy]$ for some $\policy \in \policies$. 
%     For any $\player \in \players$, $\state \in \states$, since $\policy[\player][][*] (\state) \in \brset[\player][\state] (\vfunc[\player][*], \policy[-\player][][*] (\state))$, $\qfunc[\player][*] (\state, \policy[\player][][*] (\state), \policy[-\player][][*] (\state)) \geq \vfunc[\player][*] (s)$, then by the Policy Improvement Theorem, $\vfunc[\player][{\policy[][][*]}] (\state) \geq \vfunc[\player][*] (\state)$ for any $\player \in \players$, $\state \in \states$.

%     Finally, we can conclude that for all states $\state \in \states$, $\vfunc[\player][{\policy[][][*]}] (\state) \geq \max_{\policy[\player] \in \fpolicies[\player] (\policy[-\player][][*])} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}] (\state)$ for all $\player \in \players$.
% \end{proof}
\if 0 
\begin{assumption}[Concave \samy{generalized Markov game}{Markov pseudo-game}]
\label{assum:concave_q}
1.~(Concave rewards) for all $\player \in \players$, $(\state, \action[\player]) \mapsto \reward[\player] (\state, \action[\player], \action[-\player])$ is concave, for all $\action[-\player] \in \actionspace[-\player]$; 
2.~(Stochastically concave transitions) for all $\player \in \players$,
%\samy{}{and $\action[\player] \in \actionspace[\player]$} \sadie{I think we don't need $\action[\player]$ since it's part of the input of the map} \amy{but we need to say to which $\action[\player]$'s the input map applies! this seems part of a bigger problem then, but i guess this is our convention!}
$(\state, \action[\player]) \mapsto \trans (\state[][][\prime] \mid \state, \action[\player], \action[-\player])$ is stochastically concave, for all $\state[][][\prime] \in \states$ and  $\action[-\player] \in \actionspace[-\player]$.
\end{assumption}

\begin{lemma}[Concavity of the action-value function]
    If $\mgame$
    %$= (\numplayers, \numactions, \states, \actionspace, \actions, \rewards, \trans, \discount, \initstates)$ 
    is a 
    %continuous-state continuous-action 
    \samy{generalized Markov game}{Markov pseudo-game} for which \Cref{assum:concave_q} holds, 
    then, for any policy $\policy \in \policies$, $\action[\player] \mapsto \qfunc[\player][\policy] (\state, \action[\player], \action[-\player])$ is continuous and concave, for all $\player \in \players$, $\state \in \states$, and $\action[-\player] \in \actionspace[-\player]$.
\end{lemma}
\sadie{TODO: Need to prove! Or cite!}
\fi


\subsection{Equilibrium Computation}
%\subsection{Exploitability Minimization}
\label{sec:gmg_minmax}

\if 0
\amy{add def'n: stat'y pt of the Moreau envelope is our solution concept.}

\amy{min exploitability is non-convex + non-diff. to deal w/ non-diff, we use the Moreau envelope of the exploitability. and then we find a stat'y pt, but cannot guarantee it is a global solution, or even a local one, only that it is a stat'y pt.}
\fi

\if 0
\amy{not relevant right now:}
\sdeni{}{As the computation of a \MPGNE{} in a Markov pseudo-game with a singleton state space is equivalent \amy{why equivalent? it's more general, n'est-ce pas? i mean, maybe you could convince me that this is true for finite games, but i have a much harder time digesting it in the infinite case.} to the computation of a Nash equilibrium in one-shot games, the computation of a \MPGNE{} is PPAD-hard \cite{chen2009settling, daskalakis2009complexity}.
As such, it is likely that we can at best hope to compute a policy profile which satisfies necessary conditions of a \MPGNE{} in polynomial time.}
\fi

Our approach to computing a \MPGNE{} in a Markov pseudo-game $\mgame$ is to minimize a \mydef{merit function} associated with $\mgame$, i.e., a function whose minima coincides with the pseudo-game's \MPGNE.
%\deni{The merit function is a property of the game and the solution concept both!}
Our choice of merit function, a common one in game theory, is \mydef{exploitability} $\gexploit: \policies \to \R_+$, defined as $\gexploit (\policy) \doteq \sum_{\player \in \players} \left[ \max_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])}
\payoff[\player] (\policy[\player][][\prime], \policy[-\player]) - \payoff[\player] (\policy) \right]$.
In words, exploitability is the sum of the players' maximal unilateral payoff deviations. 
%\deni{The merit function is a property of the game and the solution concept both!}

Exploitability, however, is a merit function for GNE, \emph{not\/} \MPGNE; \mydef{state exploitability}, $\sexploit (\state, \policy) = \sum_{\player \in \players} [\max_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])} \vfunc[\player][{(\policy[\player][][\prime], \policy[-\player])}] (\state) - \vfunc[\player][\policy] (\state)]$ at all states $\state \in \states$, is a merit function for \MPGNE.
Nevertheless, as we show in the sequel, for a large class of Markov pseudo-games, namely those with a bounded best-response mismatch coefficient (see Section~\ref{sec:mismatch}), 
%in which it is easy to visit the states visited by any \MPGNE[} of the pseudo-game even without knowing any of the \MPGNE{}
the set of Markov policies that minimize exploitability 
%\sdeni{i.e., the set of GNE,}{} \deni{Not necessarily true, a non-stationary policy can stil be part of the set of GNE}
equals the set of \MPGNE, making our approach a sensible one.

We are not out of the woods yet, however, as exploitability is non-convex in general, even in one-shot finite games \cite{nash1950bargaining}.
Although Markov pseudo-games can afford a convex exploitability (see, for instance \cite{flam1994gne}), it is unlikely that all do, as GNE computation is PPAD-hard \cite{chen2009settling, daskalakis2009complexity}. 
Accordingly, we instead set our sights on computing a \mydef{stationary point} of the exploitability, i.e., a policy profile $\policy[][][*] \in \fmarkovpolicies(\policy[][][*])$ s.t. for any other policy $\policy[][][] \in \fmarkovpolicies(\policy[][][*])$, it holds that $\min_{\h\in \subdiff \gexploit(\policy[][][*])} \langle \h, \policy[][][*] - \policy[][][] \rangle \leq 0$.%
\footnote{While we provide a definition of a(n approximate) stationary point for expositional purposes at present, an observant reader might have noticed the exploitability $\gexploit$ is a mapping from a function space to the positive reals, and its Fr\^echet (sub)derivative is ill-specified without a clear definition of the normed vector space of policies on which exploitability is defined. Further, even when clearly specified, such a (sub)derivative might not exist. The precise meaning of a derivative of the exploitability and its stationary points will be introduced more rigorously once we have suitably parameterized the policy spaces.}
%\deni{ADD REFERENCE TO PRECISE DEFINITION HERE.}
Such a point satisfies the necessary conditions of a \MPGNE. 

In this paper, we study Markov pseudo-games with possibly continuous state and action spaces. 
%Due to the bounded accuracy of computational methods in solving search problems over continuous sets, in practice,
As such, we can only hope to compute an \emph{approximate\/} stationary point of the exploitability in finite time. 
Defining a notion of approximate stationarity for exploitability is, however, a challenge, because exploitability is non-differentiable in general (once again, even in one-shot finite games).

Given an approximation parameter $\varepsilon \geq 0$, a natural definition of an $\varepsilon$-stationary point might be a policy profile $\policy[][][*] \in \fmarkovpolicies(\policy[][][*])$ s.t. for any other policy $\policy[][][] \in \fmarkovpolicies(\policy[][][*])$, it holds that $\min_{\h\in \subdiff \gexploit(\policy[][][*])} \langle \h, \policy[][][*] - \policy[][][] \rangle \leq \varepsilon$.
Exploitability is not necessarily Lipschitz-smooth, however, so in general it \samy{is}{may} not \samy{}{be} possible 
\amy{is Lipschitz smoothness really a necessary condition for poly time convergence?} \deni{I guess the weakest condition is weak-concavity. so not necessary but sufficient.} 
to compute an $\varepsilon$-stationary point in $\poly(\nicefrac{1}{\varepsilon})$ evaluations of the (sub)gradient of the exploitability.%
\footnote{To see this, consider the convex minimization problem $\min_{x \in \R} f(x) = |x|$. 
The minimum of this optimization occurs at $x = 0$, which is a stationary point since a (sub)derivative of $f$ at $x= 0$ is $0$. 
However, for $x< 0$, we have $\frac{\partial f(x)}{\partial x} = -1$, and for $x> 0$, we have $\frac{\partial f(x)}{\partial x} = 1$. 
Hence, any $x \in \R \setminus \{ 0 \}$ can at best be a $1$-stationary point, i.e., $\left| \frac{\partial f(x)}{\partial x} \right| = 1$. 
Hence, for this optimization problem, it is not even possible to guarantee the existence of an $\varepsilon$-stationary point distinct from $x= 0$, assuming $\varepsilon \in (0, 1)$, let alone the computation of an $\varepsilon$-stationary point $x^*$ s.t. $\left| \frac{\partial f(x^*)}{\partial x} \right| \leq \varepsilon$.}

To address, this challenge, a common approach in the optimization literature (see, for instance Appendix H, Definition 19 of \citet{liu_first-order_2021}) is to consider an alternative definition known as $(\varepsilon, \delta)$-stationarity. 
Given approximation parameters $\varepsilon, \delta \geq 0$, an $(\varepsilon, \delta)$-stationary point of the exploitability is a policy profile  $\policy[][][*] \in \fmarkovpolicies(\policy[][][*])$ for which there exists a $\delta$-close policy $\policy[][][\dagger] \in \policies$ with $\| \policy[][][\dagger] - \policy[][][*]\| \leq \delta$ s.t.\@ for any other policy $\policy[][][] \in \fmarkovpolicies(\policy[][][\dagger])$, it holds that $\min_{\h\in \subdiff \gexploit(\policy[][][\dagger])} \langle \h, \policy[][][\dagger] - \policy[][][] \rangle \leq \varepsilon$. 
The exploitability minimization method we introduce can compute such an approximate stationary point in polynomial time.
Furthermore, asymptotically, our method is guaranteed to converge to an exact stationary point of the exploitability.

More precisely, following \citet{goktas2022exploit}, who minimize exploitability to solve for variational equilibria in (one-shot) pseudo-games, we first formulate our problem as the quasi-optimization problem of minimizing exploitability,%
\footnote{Here, ``quasi'' refers to the fact that a solution to this problem is both a minimizer of exploitability and a fixed point of an operator, such as $\fpolicies$ or $\fmarkovpolicies$.}
and then transform this problem into a coupled min-max optimization (i.e., a two-player zero-sum game) whose objective is cumulative regret, rather than the potentially ill-behaved exploitability.
Under suitable parametrization, such problems are amenable to polynomial-time solutions via simultaneous gradient descent ascent~\cite{arrow1958studies}, assuming the objective is Lipschitz smooth in both players' decision variables and gradient dominated in the inner player's.
We thus formulate the requisite assumptions to ensure these properties hold of cumulative regret in our game, which in turn allows us to show that two time scale simultaneous stochastic gradient descent ascent (TTSSGDA)
%\Cref{alg:two_time_sgda}) 
converges to an $(\varepsilon, O(\varepsilon))$-stationary point of the exploitability in $\poly(\nicefrac{1}{\varepsilon})$ gradient steps.
%evaluations of the game's payoff function.


\if 0
%%% OLD APPROACH
Our approach to computing \MPGNE{} in Markov pseudo-games is to minimize a \mydef{merit function} $\gexploit: \policies \to \R$ associated with the \MPGNE{} of the Markov pseudo-game---i.e., a function whose minima coincides with the pseudo-game's \MPGNE.
%\deni{The merit function is a property of the game and the solution concept both!}
A common choice of merit function is exploitability, i.e., the sum across players of their maximal unilateral payoff deviations.
Exploitability, however, is non-convex and non-differentiable in general, even in normal-form games.%
\footnote{Indeed, computing \MPGNE{} in Markov pseudo-games can only be harder than computing Nash equilibrium in normal-form games, so it is at least PPAD-hard \cite{chen2009settling, daskalakis2009complexity}.}

While some Markov pseudo-games may afford a convex merit function, it is unlikely that all do.
It is nonetheless possible to devise a Lipschitz-smooth merit function for the \MPGNE{} of an arbitrary Markov pseudo-game simply by taking the Moreau envelope \cite{moreau1965proximite} of the exploitability, thereby obtaining the \mydef{Moreau exploitability} $\regulexploit$.
Consequently, as our approximation of (MP)GNE, we seek a stationary point of the Moreau exploitability, i.e., $\policy[][][*] \in \policies$ s.t. $\| \grad[\policy] \regulexploit (\policy[][][*]) \| \, = 0$, meaning a point at which the distance to a \MPGNE{} cannot be further minimized via first-order improvements.
%We refer to such points as \samy{}{\mydef{first-order GNE}}.}%
%\footnote{\amy{explain that someone else stole this name, which is perfectly reasonable for this purpose! add reference.}

%We now embark upon our endeavor to compute such stationary points.
Following \citet{goktas2022exploit}, who minimize exploitability to solve for variational equilibria in pseudo-games (i.e., one-shot Markov pseudo-games), we first formulate our problem as the quasi-optimization problem of minimizing exploitability.%
\footnote{Here, ``quasi'' refers to the fact that a solution to this problem is both a minimizer of exploitability and a fixed point of an operator, such as $\fpolicies$ or $\fmarkovpolicies$.}
We then transform it into a coupled min-max optimization problem whose
%---also called two-player zero-sum Stackelberg (i.e., sequential) games \cite{goktas2021minmax}
objective is cumulative regret.

While solving for a GNE only requires solving one (global) coupled min-max optimization problem, solving for a \MPGNE{} requires solving such problems at all states simultaneously, an impossible task assuming a continuous state space.
But all is not lost.
We prove that the stationary points of exploitability (\emph{\`a la\/} GNE) correspond to the stationary points of per-state exploitability (\emph{\`a la\/} \MPGNE) in many cases, e.g., when the initial state distribution has full support (\Cref{lem:full_support}). 
Furthermore, for any initial state distribution even without full support, points near the stationary points of exploitability are also near the stationary points of per-state exploitability (\Cref{lem:arb_dist}).
Therefore, it often suffices to minimize exploitability (i.e., to compute GNE in Markov policies), even when the ultimate goal is to find a generalized \emph{Markov perfect\/} equilibrium.
\fi


\if 0
\sadie{Ok, so the story in my mind is be like:
Our ultimate goal is to solve \MPGNE.
First, we find solve minimize state exploitability.
Second, we reformulate so that minimize state exploitability = min max state cumulative regret.
We can't converge to real saddle point for either min max state cumulative regret or min max cumulative regret.
Our alternative solution concept is stationary point of value function (which is equal to stationary point of state exploitability).
Finding the stationary point of all state exploitability simultaneously is still not directly doable, but we find that stationary point of exploitability corresponds to the stationary point of all state exploitability for many cases (aka initial state distribution has full support (Lemma 4). Moreover, even without full support, for any initial state distribution, when we are close to the stationary point of exploitability, we are also close to the stationary point of all state exploitability (Lemma 5).
Therefore, our computational goal is to find the stationary point of exploitability through min-max!
To do that, we do 
1. dependent policy class to decouple the min-max optimization. 
2. policy parameterization
}
\fi

\if 0
Given a Markov pseudo-game $\mgame$ and approximation parameters $\varepsilon, \delta \geq 0$, a $(\varepsilon, \delta)$-\mydef{stationary point of the exploitability} $\exploit$ (resp. \mydef{expected exploitability} w.r.t.\@ $\diststates \in \simplex(\states)$) is a policy $\policy[][][*] \in \policies$ for which there exists $\param[][][\dagger] \in \policies$ s.t $\| \policy[][][\dagger] - \policy[][][*]\| \leq \delta$, and for all other policies $\policy \in \policies$, it holds that $\langle \grad \gexploit (\policy[][][\dagger]), \policy[][][\dagger] - \policy[][][] \rangle \leq \varepsilon$ (resp. $\langle \grad \sexploit (\diststates, \policy[][][\dagger]), \policy[][][\dagger] - \param[][][] \rangle \leq \varepsilon)$. 
We call a $(0, 0)$-stationary point of the exploitability (resp. expected exploitability w.r.t.\@ $\diststates \in \simplex(\states)$) simply a \mydef{stationary point}.
\fi

\if 0
Given a Markov pseudo-game $\mgame$, a parameterization scheme $(\policy, \depolicy, \params, \deparams)$, and approximation parameters $\varepsilon, \delta \geq 0$, a $(\varepsilon, \delta)$-\mydef{stationary point of the exploitability} (resp. \mydef{expected exploitability} w.r.t\@ $\diststates \in \simplex(\states)$) is a policy parameter $\param[][][*] \in \params$ for which there exists $\param[][][\dagger] \in \params$ s.t $\| \param[][][\dagger] - \param[][][*]\| \leq \delta$ and for any other parameters $\param \in \params$, we have $\langle \grad \gexploit(\param[][][\dagger]), \param[][][\dagger] - \param[][][] \rangle \leq \varepsilon$ (resp. $\langle \grad \sexploit(\diststates, \param[][][\dagger]), \param[][][\dagger] - \param[][][] \rangle \leq \varepsilon)$. We call a $(0, 0)$-stationary point of the exploitability (resp. expected exploitability of $\diststates \in \simplex(\states)$) simply a stationary point. We assume that the gradients in the above definitions are well-defined under \Cref{assum:param_lipschitz} which we will introduce in the sequel, the exploitability will be guaranteed to be Lipschitz-continuous and thus almost everywhere differentiable.
\fi


\subsubsection{Exploitability Minimization}

Given a Markov pseudo-game $\mgame$ and two policy profiles $\policy, \policy[][][\prime] \in \policies$, 
we define the \mydef{state cumulative regret} at state $\state \in \states$ as $\scumulreg (\state, \policy, \policy[][][\prime]) = \sum_{\player \in \players} \left[\vfunc[\player][{(\policy[\player][][\prime], \policy[-\player])}] (\state) - \vfunc[\player][\policy] (\state) \right]$;
the \mydef{expected cumulative regret} \sdeni{}{for any initial state distribution $\diststates \in \simplex(\states)$} as $\scumulreg (\diststates, \policy, \policy[][][\prime]) = \Ex_{\state \sim \diststates} \left[ \scumulreg (\state, \policy, \policy[][][\prime]) \right]$,
and the \mydef{cumulative regret} as $\gcumulreg (\policy, \policy[][][\prime]) = \scumulreg (\initstates, \policy, \policy[][][\prime])$.
Additionally, we define the \mydef{state exploitability} of a policy profile $\policy$ at state $\state \in \states$ as 
$\sexploit (\state, \policy) = \sum_{\player \in \players} \max_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])} \vfunc[\player][{(\policy[\player][][\prime], \policy[-\player])}] (\state) - \vfunc[\player][\policy] (\state)$;
, the \mydef{expected exploitability} of a policy profile $\policy$ \sdeni{}{for any initial state distribution $\diststates \in \simplex(\states)$} as $\sexploit (\diststates, \policy) = \Ex_{\state \sim \diststates} \left[ \sexploit (\state, \policy) \right]$, and \mydef{exploitability} as $\gexploit(\policy)=\sum_{\player\in \players} \max_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])} \payoff[\player](\policy[\player][][\prime], \policy[-\player])$. 

In what follows, we restrict our attention to the subclass $\markovpolicies \subseteq \policies$ of (pure) Markov policies.
This restriction is without loss of generality, because finding an optimal policy that maximizes a state-value or payoff function, while the other players' policies remain fixed, reduces to solving \sdeni{an MDP}{a Markov decision process (MDP)}, and an optimal (possibly history-dependent) policy in an MDP is guaranteed to exist in the space of (pure) Markov policies
%in which case an optimal Markov policy is guaranteed to exist \amy{wait, what? for it be without loss, what we need to say is that an optimal policy can be found among Markov policies. rewrite.} \deni{Isn't that what this says ``an optimal markov policy exists'', an optimal policy is history dependent in-general.} \amy{that's not how i read it. to me it says, an optimal policy among Markov policies, not among possibly history-dependent policies, which is what we both agree it should say.} 
under very mild continuity and compactness assumptions \cite{puterman2014markov}. 
Indeed, the next lemma justifies this restriction.

\begin{restatable}{lemma}{lemmaexploitGNE}\label{lemma:exploit_GNE}
    Given a Markov pseudo-game $\mgame$, a Markov policy profile $\policy[][][*] \in \fmarkovpolicies (\policy[][][*])$ is a \MPGNE{} if and only if $\sexploit (\state, \policy[][][*]) = 0$, for all states $\state \in \states$.
    Similarly, a policy profile $\policy[][][*] \in \fpolicies (\policy[][][*])$ is an GNE if and only if $\gexploit (\policy[][][*]) =0$.
\end{restatable}

This lemma tells us that we can reformulate the problem of computing a \MPGNE{} as the quasi-minimization problem of minimizing state exploitability, i.e., $\min_{\policy \in \fmarkovpolicies (\policy)} \sexploit (\state, \policy)$, at all states $\state \in \states$ simultaneously.
The same is true of computing a GNE and exploitability.

This straightforward reformulation of \MPGNE{} (resp.\@ GNE) in terms of state exploitability (resp.\@ exploitability) does not immediately lend itself to computation, as exploitability minimization is non-trivial, because
%\amy{hmmm...this seems a bit too much to me. what we are going to show is that two-time scale SGDA is such an algo. so there is a known algo; it's just that no one used it yet, because our GMG model is new! so no one even tried to compute \MPGNE{} or perhaps even GNE for GMGs before.}\sadie{The idea here is we want min max cumulreg instead of min exploitability. I just don't know how to phrase it :((}
%with convergence rate guarantees.
%The unexploitability (!) of 
exploitability is neither convex nor differentiable in general.
Following \citet{goktas2022exploit},
we can reformulate these problems yet again, this time as coupled quasi-min-max optimization problems \cite{wald1945statistical}.
We proceed to do so now; however, we restrict our attention to exploitability, and hence GNE, knowing that we will later show that minimizing exploitability suffices to minimize state exploitability, and thereby find \MPGNE.

\begin{restatable}{observation}{obsminmax}
\label{obs:exploit_min_to_min_max}
Given a Markov pseudo-game $\mgame$,
    \begin{align}
        \min_{\policy \in \fpolicies (\policy)} \gexploit (\policy)
        &= \min_{\policy \in \fpolicies (\policy)} \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \gcumulreg (\policy, \policy[][][\prime])
        \label{eq:min_max_cumul_regret_formulation}
%        \\
%        \min_{\policy \in \fmarkovpolicies (\policy)} \sexploit (\state, \policy)
%        &= \min_{\policy \in \fmarkovpolicies (\policy)} \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \scumulreg (\state, \policy, \policy[][][\prime]), \; \; \forall\; \state \in \states
%        \label{eq:min_max_regret_formulation}
    \enspace .
    \end{align}   
\end{restatable}

While the above observation makes progress towards our goal of reformulating 
%state 
exploitability minimization in a tractable manner, the problem remains challenging to solve for two reasons: first, a fixed point computation is required to solve the outer player's minimization problem; second, the inner player's policy space depends on the choice of outer policy.
We overcome these difficulties by choosing suitable policy parameterizations.


% \begin{proof}
%     The per-player maximum operator can be pulled out of the sum in the definition of state-exploitability, because the $\player$th player's best-response policy is independent of the other players' best-response policies, given a fixed policy profile $\policy$:
%     \begin{align}
%         \forall\; \state \in \states, 
%         \; \; \sexploit (\state, \policy)
%         &= \sum_{\player \in \players}
%         \max_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])} \vfunc[\player][{(\policy[\player][][\prime], \policy[-\player])}] (\state) - \vfunc[\player][\policy] (\state) \\
%         &= \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \sum_{\player \in \players} \vfunc[\player][{(\policy[\player][][\prime], \policy[-\player])}] (\state) - \vfunc[\player][\policy] (\state) \\
%         &= \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \scumulreg (\state, \policy, \policy[][][\prime])
%     \end{align}
% The argument is analogous for exploitability:
%     \begin{align}
%         \gexploit (\policy)
%         &= \sum_{\player \in \players}
%         \max_{\policy[\player][][\prime] \in \fmarkovpolicies[\player] (\policy[-\player])} \payoff[\player] (\policy[\player][][\prime], \policy[-\player]) - \payoff[\player] (\policy) \\
%         &= \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \sum_{\player \in \players} \payoff[\player] (\policy[\player][][\prime], \policy[-\player]) - \payoff[\player] (\policy) \\
%         &= \max_{\policy[][][\prime] \in \fpolicies (\policy)} \gcumulreg (\policy, \policy[][][\prime])
%     \end{align}
% \end{proof}


\if 0
This lemma tells us that we can reformulate the problem of computing a \MPGNE{} as the quasi-optimization problem of minimizing state-exploitability, i.e., $\min_{\policy \in \fmarkovpolicies (\policy)} \sexploit (\state, \policy)$, for all states $\state \in \states$.
The same is true of computing a GNE and exploitability.
\fi

\if 0
This straightforward reformulation of \MPGNE{} (resp.\@ GNE) in terms of state exploitability (resp.\@ exploitability) does not immediately lend itself to computation, as exploitability minimization is non-trivial, because exploitability is neither convex nor differentiable.
Following \citet{goktas2022exploit}, we achieve convergence guarantees by reformulating the \MPGNE{} and GNE problems yet again, now as coupled quasi-min-max optimization problems \cite{wald1945statistical}.
\fi

\if 0
\amy{btw, Sadie, i don't see how the next two bounds help us right here? what i mean is, Lemma 5 was true even before you proved these bounds. so how do they fit into the story? maybe they are technical and go in the appendix? maybe they are needed to strengthen the convergence result to all states?}
\sadie{I think it's more like justifying why we try to minimize exploitability instead of state exploitability! In the last theorem, we will say how close to \MPGNE{} based on Lemma 3, but I put them here mainly to complete the story: computing \MPGNE{} is really hard, but computing GNE is also approximating \MPGNE{} in most cases, so our algorithm actually is approaching \MPGNE{} instead of just GNE!}
\amy{so we only minimize exploitability, NOT state-exploitability? Yes, we can't minimize state exploitability EXACTLY! NOW WE ARE TELLING YOUR STORY WHICH IS BETTER! THANK YOU! FEEL FREE TO GO BACK TO WHATEVER YOU WERE DOING. :) excellent! thanks!
oh right -- of course!!! sorry!!! too many states! but it's like a side effect, we almost get state-exploitability. that was my whole point the other day. gotcha! cool!!!}
YOU'RE WELCOME :)
\fi

% \begin{proof}
% Consider any policy $\policy, \policy[][][\prime]\in \policies$ and initial state distribution $\initstates, \diststates\in\simplex[\states]$, we know that
% \begin{align}
%     &\grad[\policy] \scumulreg(\diststates, \policy, \policy[][][\prime])\\
%     &=\grad[\policy] \sum_{\player\in \players} \payoff[\player](\policy[\player][][\prime], \policy[-\player])-\payoff[\player](\policy)\\
%     &=  \sum_{\player\in \players} \grad[\policy] (\payoff[\player](\policy[\player][][\prime], \policy[-\player])-\payoff[\player](\policy))\\
%     &=\sum_{\player\in \players} \grad[\policy]\left[
%     \E_{\substack{\state[][][\prime]\sim \statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}]\\
%     \state \sim \statedist[\diststates][\policy]}}
%     \left[ \reward[\player](\state[][][\prime], \policy[\player][][\prime](\state[][][\prime]), \policy[-\player](\state[][][\prime]) - \reward[\player](\state, \policy(\state))
%     \right] \right]\\
%     &= \sum_{\player\in \players}
%     \E_{\substack{\state[][][\prime]\sim \statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}]\\
%     \state\sim \statedist[\diststates][\policy]}}
%     \left[
%     \grad[{\action[-\player]}] \qfunc[\player][{\policy[\player][][\prime], \policy[-\player]}]    (\state[][][\prime], \policy[\player][][\prime](\state[][][\prime]), \policy[-\player](\state[][][\prime])) \grad[{\policy[-\player]}]\policy(\state[][][\prime])
%     - \grad[\action] \qfunc[\player][\policy](\state, \policy(\state)) \grad[\policy](\state)
%     \right] \label{eq:first_dpg}\\
%     &\leq \sum_{\player\in \players}
%     \max_{\statep, \state\in \states}
%     \frac{\statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}] (\statep)  
%     \statedist[\diststates][\policy](\state)}{\statedist[\initstates][{(\policy[\player][][\prime], \policy[-\player])}](\statep) \statedist[\initstates][\policy](\state)} %%%% constant ends here
%     \E_{\substack{\state[][][\prime]\sim \statedist[\initstates][{(\policy[\player][][\prime], \policy[-\player])}]\\
%     \state\sim \statedist[\initstates][\policy]}}
%     \nonumber\\
%     &\quad \quad\left[
%     \grad[{\action[-\player]}] \qfunc[\player][{\policy[\player][][\prime], \policy[-\player]}](\state[][][\prime], \policy[][][\prime](\state[][][\prime]), \policy[-\player](\state[][][\prime])) \grad[{\policy[-\player]}]\policy(\state[][][\prime])
%     - \grad[\action] \qfunc[\player][\policy](\state, \policy(\state)) \grad[\policy](\state)
%     \right]\\
%     &= \sum_{\player\in \players}
%     \min_{\statep, \state\in \states}
%     \frac{\statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}](\statep) \statedist[\diststates][\policy](\state)}{\statedist[\initstates][{(\policy[\player][][\prime], \policy[-\player])}](\statep) \statedist[\initstates][\policy](\state)} %% constant ends here 
%     \grad[\policy] \left[
%     \vfunc[\player][{\policyp[\player], \policy[-\player]}](\diststates)- \vfunc[\player][\policy](\diststates)\right] \label{eq:second_dpg}\\
%     &\leq \max_{\player\in \players}\max_{\statep, \state\in \states}
%     \frac{\statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}](\statep) \statedist[\diststates][\policy](\state)}{\statedist[\initstates][{(\policy[\player][][\prime], \policy[-\player])}](\statep) \statedist[\initstates][\policy](\state)}%%%% constant ends here
%     \sum_{\player\in\player}\grad[\policy] \left[
%     \vfunc[\player][{\policyp[\player], \policy[-\player]}](\diststates)- \vfunc[\player][\policy](\diststates)\right]\\
%     &=\max_{\player\in \players}\max_{\statep, \state\in \states}
%     \frac{\statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}](\statep) \statedist[\diststates][\policy](\state)}{\statedist[\initstates][{(\policy[\player][][\prime], \policy[-\player])}](\statep) \statedist[\initstates][\policy](\state)}
%    \grad[\policy] \sum_{\player\in\player} \left[
%     \vfunc[\player][{\policyp[\player], \policy[-\player]}](\diststates)- \vfunc[\player][\policy](\diststates)\right]\\
%     & \leq \left(\frac{1}{1-\discount}\right)^2 
%     \max_{\player\in \players}\max_{\statep, \state\in \states}\frac{\statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}](\statep) \statedist[\diststates][\policy](\state)}{\initstates(\statep)\initstates(\state)} %%% constant end here
%     \grad[\policy] \scumulreg(\diststates, \policy, \policyp) \label{eq:back_to_initial_dist}\\
%     &= \left(\frac{1}{1-\discount}\right)^2 
%     \max_{\player\in \players}
%     \left\Vert\frac{\statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}]}{\initstates}\right\Vert_{\infty}
%     \left\Vert\frac{\statedist[\diststates][\policy]}{\initstates}\right\Vert_\infty
%     \grad[\policy] \scumulreg(\diststates, \policy, \policyp)
% \end{align}
% where \Cref{eq:first_dpg} and $\Cref{eq:second_dpg}$ are gained by deterministic policy gradient theorem, and \Cref{eq:back_to_initial_dist} is due to the fact that $\statedist[\initstates][\policy](\state)\geq (1-\discount)\initstates(\state)$ for any $\policy\in \policies$, $\state\in \states$.
% \end{proof}

\if 0
\samy{}{OLD: 
The Stackelberg game is ...
played sequentially among two players with the leader's choice of policy constraining the follower's choice set.
Third, we eliminate this coupling, by incorporating it into the follower's choice set, thereby arriving at a 
%typical uncoupled 
min-max optimization problem to which standard convergence algorithms 
%(i.e., stochastic gradient descent-ascent) 
apply.}
%We thus focus on minimizing exploitability by solving this coupled min-max optmization problem: $\min_{\policy \in \fmarkovpolicies} \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \gcumulreg (\policy, \policy[][][\prime])$.
\fi


\subsubsection{Policy Parameterization}
\label{sec:policy_parameterization}

In a coupled min-max optimization problem, any solution to the inner player's maximization problem is implicitly parameterized by the outer player's decision. 
We restructure the jointly feasible Markov policy class to represent this dependence explicitly.

Define the class of \mydef{dependent policies} $\depolicies \doteq \{ \depolicy: \states \times \actionspace \to \actionspace \mid \forall (\state, \action) \in \states \times \actionspace, \; \depolicy (\state, \action) \in \actions (\state, \action) \}= \bigtimes_{\player \in \players} \{ \depolicy[\player]: \states \times \actionspace[\player] \to \actionspace[-\player] \mid \forall(\state, \action[-\player]) \in \states \times \actionspace[-\player], \; \depolicy[\player] (\state, \action[-\player]) \in \actions[\player] (\state, \action[-\player]) \}$. 
With this definition in hand we arrive at an \emph{uncoupled\/} quasi-min-max optimization problem:

\begin{restatable}{lemma}{lemmauncoupledminmax}
\label{lemma:independent_min_max}
    Given a Markov pseudo-game $\mgame$,
    \begin{align}
        \min_{\policy \in \fpolicies (\policy)} \max_{\policy[][][\prime] \in \fmarkovpolicies (\policy)} \gcumulreg (\policy, \policy[][][\prime])
        =
        \min_{\policy \in \fpolicies (\policy)}
        \max_{\depolicy \in \depolicies} \gcumulreg (\policy, \depolicy (\cdot, \policy (\cdot)))
        \label{eq:uncoupled_min_max}
    \enspace .
    \end{align}
\end{restatable}

It can be expensive to represent the aforementioned dependence in policies explicitly.
This situation can be naturally rectified, however, by a suitable policy parameterization. 
A suitable policy parameterization can also allow us to represent the set of fixed points s.t. $\policy \in \fmarkovpolicies (\policy)$ more efficiently in practice \cite{goktas2023generative}. 

Define a \mydef{parameterization scheme} $(\policy, \depolicy, \params, \deparams)$ as comprising a unconstrained parameter space $\params$ and parametric policy profile function $\policy: \states \times \params \to \actionspace$ for the outer player, and an unconstrained parameter space $\deparams$ and parametric policy profile function $\depolicy: \states\times \actionspace \times \deparams \to \actionspace$ for the inner player.
Given such a scheme, we restrict the players' policies to be parameterized: i.e., the outer player's space of policies $\policies[][\params] = \{ 
%\policy (\cdot; \param) 
\policy: \states \times \params \to \actionspace \mid \param \in \params \} \subseteq \markovpolicies$, while the inner player's space of policies $\depolicies[][\deparams] = \{ \depolicy: \states\times \actionspace \times \deparams \to \actionspace
%\depolicy (\cdot; \deparam) 
\mid \deparam \in \deparams \}$.
%
Using these parameterizations, we redefine $\vfunc[][\param] \doteq \vfunc[][{\policy (\cdot; \param)}]$, $\qfunc[][\param] \doteq \qfunc[][{\policy (\cdot; \param)}]$, $\payoff (\param) = \payoff ({\policy (\cdot; \param)})$, and $\histdistrib[\initstates][\param] = \histdistrib[\initstates][{\policy (\cdot; \param)}]$; and
$\vfunc[][\deparam(\param)] \doteq \vfunc[][{\depolicy (\cdot, \policy (\cdot; \param); \deparam)}]$; 
$\qfunc[][\deparam(\param)] \doteq \qfunc[][{\depolicy (\cdot, \policy (\cdot; \param); \deparam)}]$; $\payoff (\deparam(\param)) = \payoff ({\depolicy (\cdot, \policy (\cdot; \param); \deparam)})$; $\histdistrib[\initstates][\deparam(\param)] = \histdistrib[\initstates][{\depolicy (\cdot, \policy (\cdot; \param); \deparam)}]$; and so on.

%Furthermore, we redefine cumulative regret as $\gcumulreg (\param, \deparam) \doteq \gcumulreg (\policy (\cdot; \param), \depolicy (\cdot, \policy (\cdot; \param); \deparam))$.

\begin{assumption}[Parameterization for Min-Max Optimization] 
\label{assum:param_min_max}
Given a Markov pseudo-game $\mgame$ and a parameterization scheme $(\policy, \depolicy, \params, \deparams)$, assume 
1.~for all $\param \in \params$, 
%\samy{}{there exists} %NON-EMPTY
$\policy (\state; \param) \in \actions (\state, \policy (\state; \param))$, for all $\state \in \states$; and
2.~for all $\deparam \in \deparams$, 
%\samy{}{there exists} %NON-EMPTY
$\depolicy (\state, \action; \deparam) \in \actions (\state, \action)$, for all $(\state, \action) \in \states\times \actionspace$. 
\end{assumption}

%\amy{this assumption seems crazy strong. how can our policies be parameterized s.t.\ all constraints are satisfied in advance of play? we would have to satisfy budget constraints before prices are set? i added THERE EXISTS. b/c maybe all you meant is that the correspondences are non-empty?} \sadie{Hmmm let me think about it, this is exactly the same as Deni's assumptions in the GAES paper. I think satisfying constraints is not a problem since both $\fmarkovpolicies$ and $\depolicies$ are fixed sets, and we are trying to represent these two fixed sets. This assumption just says all of our parameterized functions are within the origin sets since we don't want to optimize beyond the set, so I feel it's necessary.}

Assuming a policy parameterization scheme that satisfies \Cref{assum:param_min_max}, we restate our goal, state exploitability minimization, one last time as the following min-max optimization problem:
\begin{align}
\label{eq:new_min_max_opt}
    \min_{\param \in \params}
        \max_{\deparam \in \deparams} 
        \gcumulreg (\param, \deparam)
        \doteq \gcumulreg (\policy (\cdot; \param), \depolicy (\cdot, \policy (\cdot; \param); \deparam)) \enspace .
\end{align}

\amy{can you please add some English summarizing how the parameterization scheme gets around the need to solve a quasi-opt'n problem.}

%\amy{this assumption seems crazy strong. how can our policies be parameterized s.t.\ all constraints are satisfied in advance of play? we would have to satisfy budget constraints before prices are set? i added THERE EXISTS. b/c maybe all you meant is that the correspondences are non-empty?} \sadie{Hmmm let me think about it, this is exactly the same as Deni's assumptions in the GAES paper. I think satisfying constraints is not a problem since both $\fmarkovpolicies$ and $\depolicies$ are fixed sets, and we are trying to represent these two fixed sets. This assumption just says all of our parameterized functions are within the origin sets since we don't want to optimize beyond the set, so I feel it's necessary.}

Now, given unconstrained parameter space, we are able to simplify our definition of \mydef{$(\varepsilon, \delta)$-stationary point of the exploitability}: a policy parameter $\param[][][*]\in \params$ for which there exists a $\delta$-close policy parameter $\param[][][\dagger]\in \params$ with $\|\param[][][*]-\param[][][\dagger]\|\leq \delta$ s.t. $\min_{\h\in \subdiff \exploit(\param[][][\dagger])} \|\h\| \leq \varepsilon$.

\subsubsection{State Exploitability Minimization}
\label{sec:mismatch}

Returning to our stated objective, namely \emph{state\/} exploitability minimization, we turn our attention to obtaining a tractable characterization of this goal.
Specifically, we argue that it suffices to minimize exploitability, rather than state exploitability, as any policy profile that is a stationary point of exploitability is also a stationary point of state exploitability across all states simultaneously, under suitable assumptions.

Our first lemma states that a stationary point of the exploitability is almost surely also a stationary point of the state exploitability at all states.
Moreover, if the initial state distribution has full support, then any $(\varepsilon, \delta)$-stationary point of the exploitability can be converted into an $(\nicefrac{\varepsilon}{\alpha}, \delta)$-stationary point of the \emph{state\/} exploitability, with probability at least $1-\alpha$.

% \begin{restatable}{lemma}{lemmafullsupport}
% \label{lem:full_support}
% Given Markov pseudo-game $\mgame$ and policy parameter $\param[][][*] \in \params$, if $\param[][][*]$ is a stationary point, i.e.,
% if $\max_{\param \in \params} \langle \grad \gexploit (\param[][][*]), \param[][][*] - \param \rangle \leq  0$,
% then $\max_{\param \in \params} \langle \grad \sexploit(\state, \param[][][*]), \param[][][*] - \param \rangle \leq 0$, for all states $\state \in \states$, $\initstates$-almost surely, 
% i.e., $\initstates (\{ \state \in \states \mid \max_{\param \in \params} \langle \grad \sexploit(\state, \param[][][*]), \param[][][*] - \param \rangle \leq 0 \}) = 1$.

% Moreover, for any $\varepsilon > 0$ and $\alpha \in [0, 1]$, if $\supp (\initstates) = \states$ and $\param[][][*]$ is a $\varepsilon$-stationary point, i.e., if $\max_{\param \in \params} \langle \grad \gexploit (\param[][][*]), \param[][][*] - \param \rangle \leq \varepsilon$, then $\max_{\param \in \params} \langle \grad \sexploit (\state, \param[][][*]), \param[][][*] - \param \rangle \leq \nicefrac{\varepsilon}{\alpha}$, for all states $\state \in \states$, with probability at least $1-\alpha$.
% \end{restatable}

%%% OLD LEMMA
\begin{restatable}{lemma}{lemmafullsupport}
\label{lem:full_support}
Given a Markov pseudo-game $\mgame$,
for $\param \in \params$, suppose that $\sexploit(\state, \cdot)$ is differentiable at $\param$ for all $\state\in \states$. If 
 $\| \grad[\param] \gexploit (\param) \| \, = 0$, then, for all states $\state \in \states$, $\| \grad[\param] \sexploit (\state, \param) \| \, = 0$  $\initstates$-almost surely, i.e., $\initstates (\{ \state \in \states \mid \|\grad[\param] \sexploit (\state, \param) \| \, = 0 \}) = 1$. 
Moreover, for any $\varepsilon > 0$ and $\delta \in [0, 1]$, if $\supp (\initstates) = \states$ and $\| \grad[\param] \gexploit (\param) \| \, \leq \varepsilon$, then with probability at least $1-\delta$, $\| \grad[\param] \sexploit (\state, \param) \| \, \leq \nicefrac{\varepsilon}{\delta}$. 
\end{restatable}

In fact, we can strengthen this probabilistic equivalence to a deterministic one by restricting our attention to Markov pseudo-games with bounded best-response mismatch coefficients.
Our best-response mismatch coefficient generalizes the minimax mismatch coefficient in two-player settings \cite{daskalakis2020independent} and the distribution mismatch coefficient in single-agent settings \cite{agarwal2020optimality}.

Given $\mgame$ with initial state distribution $\initstates$ and alternative state distribution $\diststates \in \simplex (\states)$, 
and letting $\brmap[\player] (\policy[-\player]) \doteq \argmax_{\policy[\player][][\prime] \in \fmarkovpolicies[\player](\policy[-\player])} \payoff[\player] (\policy[\player][][\prime], \policy[-\player])$ denote the set of best response policies for player $\player$ when the other players play policy profile $\policy[-\player]$,
we define the \mydef{best-response mismatch coefficient} for policy profile $\policy$ as $\brmismatch (\policy, \initstates, \diststates) \doteq \max_{\player\in \players} \max_{\policyp[\player] \in \brmap[\player](\policy[-\player])} \left( \nicefrac{1}{1-\discount} \right)^2 \Vert \nicefrac{\statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}]}{\initstates} \Vert_{\infty} \Vert \nicefrac{\statedist[\diststates][\policy]}{\initstates} \Vert_\infty$.


\begin{restatable}{lemma}{lemmabrmismatch}
\label{lemma:br_mismatch_coef}
Let $\mgame$ be a Markov pseudo-game with initial state distribution $\initstates$. 
Given policy parameter $\param \in \params$ and arbitrary state distribution $\diststates \in \simplex (\states)$, suppose that both $\sexploit(\initstates, \cdot)$ and $\sexploit(\diststates, \cdot)$ are differentiable at $\param$, then we have:
$\|\grad \sexploit(\diststates, \param)\| 
\leq 
\brmismatch (\policy(\cdot; \param[][][*]), \initstates, \diststates)
\|\grad \gexploit(\param)\|$.
\label{lem:arb_dist}
\end{restatable}

% \begin{restatable}{lemma}{lemmabrmismatch}
% \label{lemma:br_mismatch_coef}
% Let $\mgame$ be a Markov pseudo-game with initial state distribution $\initstates$. 
% Then given policy parameter $\param[][][*] \in \params$ and arbitrary state distribution $\diststates \in \simplex (\states)$,
% $\max_{\param \in \params} \langle \grad \sexploit (\diststates, \param[][][*]), \param[][][*] - \param \rangle \leq \brmismatch (\policy(\cdot; \param[][][*]), \initstates, \diststates) \max_{\param \in \params} \langle \grad \gexploit (\param[][][*]), \param[][][*] - \param \rangle $.
% \label{lem:arb_dist}
% \end{restatable}

Once again, \Cref{lem:full_support} states that any approximate stationary point of exploitability is also an approximate stationary point of state exploitability with high probability, while \Cref{lemma:br_mismatch_coef} upper bounds state exploitability in terms of exploitability, when the best-response mismatch coefficient is bounded.
Together, these two lemmas imply that finding a policy profile this is a stationary point of exploitability is sufficient for find a policy profile this is a stationary point of state exploitability, and hence one that satisfies the necessary conditions of a \MPGNE.


\subsubsection{Algorithmic Assumptions}

We are nearly ready to describe our \sdeni{deep}{} reinforcement learning algorithm for \sdeni{solving}{computing a stationary point of} \Cref{eq:new_min_max_opt}, and thereby finding a policy profile that satisfies the necessary conditions of a \MPGNE.
As \Cref{eq:new_min_max_opt} is a two-player zero-sum game, our method is a variant of simultaneous gradient descent ascent (GDA) \cite{arrow1958studies}, meaning it adjusts its parameters based on first-order information until it reaches a (first-order) stationary point.
Polynomial-time convergence of GDA typically requires that the objective be Lipschitz smooth in both decision variables, and gradient dominated in the inner one, which in our application, translates to the cumulative regret $\gcumulreg (\param, \deparam)$ being Lipschitz smooth in $(\param, \deparam)$ and gradient dominated in $\deparam$. 
These conditions are ensured, under the following assumptions on the Markov pseudo-game.

%By Assumption~\ref{assum:param_lipschitz}, the cumulative regret is guaranteed to be Lipschitz-smooth in $(\param, \deparam)$.
%Moreover, by Assumption~\ref{assum:param_gradient_dominance}, it is also gradient dominated in $\deparam$.

\begin{assumption}[Lipschitz Smooth Payoffs]
\label{assum:param_lipschitz}
Given a Markov pseudo-game $\mgame$ and a parameterization scheme $(\policy, \depolicy, \params, \deparams)$, assume
1.~$\params$ and $\deparams$ are non-empty, compact, and convex,
2.~$\param \mapsto \policy (\state; \param)$ is twice continuously differentiable, for all $\state \in \states$, and $\deparam \mapsto \depolicy (\state, \action; \deparam)$ is twice continuously differentiable, for all $(\state, \action) \in \states \times \actionspace$;
3.~$\action \mapsto \reward (\state, \action)$ is twice continuously differentiable, for all $\state \in \states$;
4.~$\action \mapsto \trans
(\state[][][\prime] \mid \state, 
\action)$ 
%\amy{is this supposed to be an assumption about the transition function? $\trans$ is rendering just as an unbolded $\depolicy$.} \deni{This is supposed to be the transition function, there was a notation clash. I changed it to ``pee'' rather a ``rho'' let me know if it's clashing with anything.} 
is twice continuously differentiable, for all $\state, \state[][][\prime] \in \states$.
\end{assumption}

\begin{assumption}[Gradient Dominance Conditions]
\label{assum:param_gradient_dominance}
Given a Markov pseudo-game $\mgame$ together with a parameterization scheme $(\policy, \depolicy, \params, \deparams)$, assume
1.~(Closure under policy improvement) 
For each $\param\in \params$, there exists $\deparam\in \deparams$ s.t. $\qfunc[\player][\param](\state, \depolicy[\player](\state, \policy(\state;\param);\deparam), \policy[-\player](\state;\param)) 
= \max_{\policy[\player][][\prime]\in \fpolicies[\player](\policy(\cdot;\param))} 
\qfunc[\player][\param](\state, \policy[\player][][\prime](\state), \policy[-\player](\state;\param)) 
$ for all $\player\in \players$, $\state\in \states$.
2.~(Concavity of action-value) $\deparam \mapsto \qfunc[\player][{\param[][][\prime]}] (\state, 
    \depolicy[\player] (\state, \policy[-\player] (\state; \param); \deparam), 
    \policy[-\player] (\state; \param) 
    )$ is concave, for all $\state \in \states$ and $\param, \param[][][\prime] \in \params$. 
\end{assumption}

%Under \Cref{assum:param_lipschitz}, cumulative regret is guaranteed to be Lipschitz continuous, and thus almost everywhere differentiable.
%Nonetheless, our definition of stationary point requires only local differentiability, namely that derivatives exist \sdeni{}{almost} everywhere within a $\delta$ neighborhood of any candidate solution.

\if 0
%%% OLD: simGDA motivation
We now proceed to develop a first-order reinforcement learning algorithm (i.e., a policy gradient method) for finding policies that solve \Cref{eq:new_min_max_opt}, thereby finding policies that satisfy the necessary conditions of an (MP)GNE.
More specifically, we seek an algorithm that adjusts its parameters based on first-order information until it reaches a (first-order) stationary point: i.e., a version of simultaneous gradient descent ascent (simGDA). \amy{add your two std GDA cites. Uzawa? and who else?}
Polynomial-time convergence of simGDA typically \amy{???} requires that the objective be Lipschitz smooth in both decision variables, and gradient dominated in the inner one, which in our application, translates to
$\gcumulreg (\param, \deparam)$ being Lipschitz smooth in $(\param, \deparam)$ and gradient dominated in $\deparam$.

\amy{CAN WE PUT ASSUMPTION 5 RIGHT HERE, RIGHT NOW? why wait?}

\amy{ALSO, CAN WE ADD MORE EXPLANATION OF WHY WE MOVE FROM CUMULATIVE REGRET, AS IN EQ'N 3, TO STATE EXPLOITABILITY?}

One approach to developing such a method would be to try to compute first-order stationary points of the (state) exploitability, as such points satisfy the necessary conditions for \MPGNE{} by design.
However, as the space of policies is continuous, one could only ever hope to find an \emph{approximate\/} stationary point of the (state) exploitability in practice.
But as (state) exploitability is not a Lipschitz-smooth merit function, computing an approximate stationary point to an arbitrary desired accuracy can be intractable.%
%%% important footnote used to be here !!!

%%% OLD: motivation for the Moreau envelope
As is common in the optimization literature (see, for instance, \citet{davis2018subgradient}), instead of minimizing exploitability, to obtain a Lipschitz-smooth merit function for GNE, we consider the Moreau envelope of the exploitability, which we simply call the \mydef{Moreau exploitability}, i.e., $\regexploit (\param) \doteq \min_{\param[][][\prime] \in \params} \left\{ \gexploit (\param[][][\prime]) + \lipschitz[{\grad \scumulreg}] \left\| \param - \param[][][\prime] \right\|^2\right\}$. 
Similarly, we also consider the \mydef{state Moreau exploitability}, i.e., the Moreau envelope of the state exploitability: $\regsexploit (\state, \param) \doteq \min_{\param[][][\prime] \in \params} \left\{ \sexploit (\state, \param[][][\prime]) + \lipschitz[{\grad \scumulreg}] \left\| \param - \param[][][\prime] \right\|^2\right\}$.
We recall that in these definitions, by our notational convention, $\lipschitz[{\grad \scumulreg}] \geq 0$, refers to the Lipschitz-smoothness constants of the state exploitability which in this case we take to be the largest across all states, i.e., for all $\state \in \states$, $(\param, \deparam) \mapsto \scumulreg(\state, \param, \deparam)$ is $\lipschitz[{\grad \scumulreg}]$-Lipschitz-smooth, respectively, and which we note is guaranteed to exist under \Cref{assum:param_lipschitz}. \sdeni{}{Further, we note that since $\gcumulreg(\param, \deparam) = \Ex_{\state \sim \initstates} \left[\scumulreg(\state, \param, \deparam) \right]$ is a weighted average of $\scumulreg$, $(\param, \deparam) \mapsto \gcumulreg(\param, \deparam)$ is also $\lipschitz[{\grad \scumulreg}]$-Lipschitz-smooth.} \amy{hmmm...Assumption 4 is like 2 pages from now? re-org? PUT IT HERE INSTEAD?}
\fi


\if 0
% DENI: remove this subsection entirely
\subsubsection{Moreau State Exploitability Minimization}

An important property of the Moreau envelope of any function is that the set of minima (resp. stationary points) of the function and its Moreau envelope coincide.
As such, the (state) Moreau exploitability is a merit function for (MP)GNE.
While finding a stationary point of the state Moreau exploitability at each state simultaneously sounds cumbersome, we show that finding a stationary point of the Moreau exploitability for an appropriately chosen initial state distribution suffices for finding a stationary point of the state Moreau exploitability at all states simultaneously.

The next two lemmas suggest that if we can find a stationary point of the exploitability, then we can also approximate a stationary point of the state exploitability with high probability (\Cref{lem:full_support}), and of the expected state exploitability (\Cref{lemma:br_mismatch_coef}).
How good this approximation is depends on the initial state distribution $\initstates$.

\deni{SADIE: I checked the proofs for \Cref{lem:full_support} and \Cref{lemma:br_mismatch_coef}, it seems to me they go through for the (state) Moreau exploitability. Can you double check and make the changes from (state) exploitability to Moreau exploitability?}

\amy{THESE LEMMAS --as stated right now-- SEEM TO HAVE NOTHING TO DO WITH THE MOREAU ENVELOPE. SO SHOULDN'T WE MOVE THEM UP, AND INTRODUCING THE MOREAU ENVELOPE ONLY AFTER STATING THEM?}

\deni{AMY: Read the following two lemma as if they are for the Moreau exploitability because they will be.}

\amy{JUST SAW THIS! DO THEY ONLY APPLY TO MOREAU EXPLOITABIITY? THEY DON'T APPLY TO ALL general EXPLOITABIITY?}

\begin{restatable}{lemma}{lemmafullsupport}
\label{lem:full_support}
Given a Markov pseudo-game $\mgame$,
for $\param \in \params$, if $\| \grad[\param] \gexploit (\param) \| \, = 0$, then, for all states $\state \in \states$, $\| \grad[\param] \sexploit (\state, \param) \| \, = 0$  $\initstates$-almost surely, i.e., $\initstates (\{ \state \in \states \mid \|\grad[\param] \sexploit (\state, \param) \| \, = 0 \}) = 1$. 
Moreover, for any $\varepsilon > 0$ and $\delta \in [0, 1]$, if $\supp (\initstates) = \states$ and $\| \grad[\param] \gexploit (\param) \| \, \leq \varepsilon$, then with probability at least $1-\delta$, $\| \grad[\param] \sexploit (\state, \param) \| \, \leq \nicefrac{\varepsilon}{\delta}$. 
\end{restatable}

\begin{restatable}{lemma}{lemmafullsupport}
\label{lem:full_support}
Given a Markov pseudo-game $\mgame$,
for all policy profiles $\policy \in \policies$, if $\| \grad[\policy] \gexploit (\state) \| \, = 0$, then $\initstates$-almost surely, $\| \grad[\policy] \sexploit (\state, \policy) \| \, = 0$, for all $\state\in \states$, i.e., $\initstates (\{ \state \in \states \mid \|\grad[\policy] \sexploit (\state, \policy) \| \, = 0 \}) = 1$. 
Moreover, for any $\varepsilon, \delta > 0$, if $\supp (\initstates) = \states$ and $\| \grad[\policy] \gexploit (\policy) \| \, \leq \varepsilon$, then with probability at least $1-\delta$, $\| \grad[\policy] \sexploit (\state, \policy) \| \, \leq \nicefrac{\varepsilon}{\delta}$. 
\end{restatable}

\Cref{lem:full_support} states, if the initial state distribution has full support, then a stationary point of the Moreau exploitability is almost surely also a stationary point of the state Moreau exploitability.
%at all states.
Moreover, we can bound the gradient of the state Moreau exploitability w.r.t.\@ the policy \samy{}{parameters}  with high probability, given a bounded gradient of the Moreau exploitability itself.

\deni{SADIE CAN YOU CHECK OUT THE FOLLOWING EDIT.} \sadie{Checked out!}
Given $\mgame$ with initial state distribution $\initstates$ and alternative state distribution $\diststates \in \simplex (\states)$, 
and letting $\brmap[\player] (\policy[-\player]) \doteq \argmax_{\policy[\player][][\prime] \in \fmarkovpolicies[\player](\policy[-\player])} \payoff[\player] (\policy[\player][][\prime], \policy[-\player])$ denote the set of best response policies for player $\player$ when the other players play policy profile $\policy[-\player]$,
we define the \mydef{best-response mismatch coefficient} for policy profile $\policy$ as $\brmismatch (\policy, \initstates, \diststates) \doteq \max_{\player\in \players} \max_{\policyp[\player] \in \brmap[\player](\policy[-\player])} \left( \nicefrac{1}{1-\discount} \right)^2 \Vert \nicefrac{\statedist[\diststates][{(\policy[\player][][\prime], \policy[-\player])}]}{\initstates} \Vert_{\infty} \Vert \nicefrac{\statedist[\diststates][\policy]}{\initstates} \Vert_\infty$.

\begin{restatable}{lemma}{lemmabrmismatch}
\label{lemma:br_mismatch_coef}
Let $\mgame$ be a Markov pseudo-game with initial state distribution $\initstates$. 
Then, for all policy parameters $\param \in \params$ and arbitrary state distributions $\diststates \in \simplex (\states)$, 
$\| \grad[\param] \sexploit (\diststates, \param) \|
% \| \grad[\policy] \int\sexploit (\state, \policy) \diststates(d\state) \|
\, \leq \brmismatch (\policy(\cdot; \param), \initstates, \diststates) \| \grad[\param] \gexploit (\param) \|$.
\label{lem:arb_dist}
\end{restatable}
\fi

% % \begin{proof}
% %     The first claim follows directly from Lemma 1 of \cite{bhandari2019global}.
% %     We prove the second part using Jensen's and Markov's inequality. 
% %     First, by Jensen's inequality,
% %     $\Ex_{\state \sim \initstates} \left[ \| \grad[\policy] \sexploit (\state, \policy) \| \right] \leq \left\| \Ex_{\state \sim \initstates} \left[ \grad[\policy] \sexploit (\state, \policy) \right] \right\| \, = \| \grad[\policy] \gexploit (\policy) \|$.
% %     Therefore, by Markov's inequality, 
% % %\begin{align}
% %     $\Pr \left( \| \grad[\policy] \sexploit (\state, \policy) \| \, \geq \nicefrac{\varepsilon}{\delta} \right) \leq \frac{\Ex_{\state \sim \initstates} \left[ \| \grad[\policy] \sexploit (\state, \policy) \| \right]} {\nicefrac{\varepsilon}{\delta}} \leq \frac{\varepsilon} {\nicefrac{\varepsilon}{\delta}} = \delta$.
% % %    \enspace .
% % %\end{align}
% % \end{proof}


% \begin{assumption}[Stochastic Gradient Estimator]
% \label{assum:gradient_estimator_variance}
%     For variance constants $\varconst[\param], \varconst[\deparam]>0$, the stochastic oracle $\goracle(\param, \deparam, \event) =(\goracle[\param] (\param, \deparam, \event), \goracle[\deparam] (\param, \deparam, \event))$ satisfies:
%     \begin{align}
%         \Ex&[\goracle(\param, \deparam, \event) = \grad \gcumulreg (\param, \deparam)], \\
%         \Ex&[\|\goracle[\param] (\param, \deparam, \event)-\grad[\param] \gcumulreg (\param, \deparam) \|^2] \leq \varconst[\param]^2, \sadie{Need this?} \\
%         \Ex&[\|\goracle[\deparam] (\param, \deparam, \event)-\grad[\deparam] \gcumulreg (\param, \deparam) \|^2] \leq \varconst[\deparam]^2.
%     \end{align}
% \end{assumption}


\subsubsection{Algorithm and Convergence}

\if 0
%% OLD
Finally, we present our algorithm for finding the approximate stationary points of the (state) Moreau exploitability. 
Since the first-order stationary points of the (state) exploitability and (state) Moreau exploitability coincide, any policy which is a first-order stationary point of the Moreau envelope satisfies necessary conditions of an (MP)GNE. 
\fi

Finally, we present our algorithm for finding an approximate stationary point of exploitability, and thus state exploitability.
The algorithm we use is two time-scale stochastic simultaneous gradient descent-ascent (TTSSGDA), first analyzed by
\citet{lin2020gradient, daskalakis2020independent}, for which we prove best-iterate convergence to an $(\varepsilon, O(\varepsilon))$-stationary point of exploitability after taking $\poly(\nicefrac{1}{\varepsilon})$ gradient steps 
%(i.e., making this many calls to a first-order oracle) 
under Assumptions \ref{assum:param_lipschitz} and \ref{assum:param_gradient_dominance}.


\input{algos/sgda}


% \begin{proof}
%     Fix $\policy[][][*] \in \fmarkovpolicies$.
%     We want to show that $\max_{\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])} \gexploit (\policy[][][*], \policy[][][\prime]) = \max_{\depolicy \in \depolicies} \gexploit (\policy[][][*], \depolicy (\cdot, \policy (\cdot)))$. 
%     Define $\policies[][{\depolicies, \policy[][][*]}] \doteq \{ \policy: \state\mapsto \depolicy (\state, \policy[][][*] (\state)) \mid \depolicy \in \depolicies \} \subseteq \markovpolicies$. 
    
%     First, for all $\policy[][][\prime] \in \policies[][{\depolicies, \policy[][][*]}]$, $\policy[][][\prime] (\state) = \depolicy (\state, \policy[][][*] (\state)) \in \actions (\state, \policy[][][*] (\state))$, for all $\state \in \states$, by the definition of $\depolicies$.
%     Thus, $\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*]) = \{ \policy \in \markovpolicies\mid \forall\state \in \states, \policy (\state) \in \actions (\state, \policy[][][*] (\state)) \}$.
%     Therefore, $\policies[][{\depolicies, \policy[][][*]}] \subseteq \fmarkovpolicies (\policy[][][*])$, which implies that $\max_{\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])} \gexploit (\policy[][][*], \policy[][][\prime])
%     \geq \max_{\policy[][][\prime] \in \policies[][{\depolicies, \policy[][][*]}]} \gexploit (\policy[][][*], \policy[][][\prime])
%     = \max_{\depolicy \in \depolicies} \gexploit (\policy[][][*], \depolicy (\cdot, \policy (\cdot)))$. 

%     Moreover, for all $\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])$, $\policy[][][\prime] (\state) \in \actions (\state, \policy[][][*] (\state))$, for all $\state \in \states$, by the definition of $\fmarkovpolicies$. 
%     %\sadie{$\fmarkovpolicies (\policy[][][*])=\{\policy\in \markovpolicies\mid \policy(\state)\in \actions(\state, \policy[][][*](\state))\})$ embodies the constraints by definition!}
%     Define $\depolicy[][][\prime]$ such that for all $\state \in \states$, $\depolicy[][][\prime] (\state, \action) = \policy[][][\prime] (\state)$ if $\action = \policy[][][*] (\state)$, and $\depolicy[][][\prime] (\state, \action) = \action[][][][\prime]$ for some $\action[][][][\prime] \in \actions (\state, \action)$ otherwise.
%     %if $\action\neq\policy[][][*] (\state)$.
%     Note that $\depolicy[][][\prime] \in \depolicies$, since $\forall (\state, \action) \in \states\times \actionspace, \; \depolicy (\state, \action) \in \actions (\state, \action)$. Thus, as $\policy[][][\prime] (\state) = \depolicy[][][\prime] (\state, \policy[][][*] (\state))$, for all $\state \in \states$, it follows that $\policy[][][\prime] \in \policies[][{\depolicies, \policy[][][*]}]$. 
%     Therefore, $\fmarkovpolicies (\policy[][][*]) \subseteq \policies[][{\depolicies, \policy[][][*]}]$, which implies that $\max_{\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])} \gexploit (\policy[][][*], \policy[][][\prime])
%     \leq \max_{\policy[][][\prime] \in \policies[][{\depolicies, \policy[][][*]}]} \gexploit (\policy[][][*], \policy[][][\prime])
%     = \max_{\depolicy \in \depolicies} \gexploit (\policy[][][*], \depolicy (\cdot, \policy (\cdot)))$. 

%     Finally, we conclude that $\max_{\policy[][][\prime] \in \fmarkovpolicies (\policy[][][*])} \gexploit (\policy[][][*], \policy[][][\prime]) = \max_{\depolicy \in \depolicies} \gexploit (\policy[][][*], \depolicy (\cdot, \policy (\cdot)))$. 
% \end{proof}

\if 0
\begin{assumption}[Lipschitz Smooth Payoffs]
\label{assum:param_lipschitz}
Given a Markov pseudo-game $\mgame$ and a parameterization scheme $(\policy, \depolicy, \params, \deparams)$, assume
1.~$\params$ and $\deparams$ are non-empty, compact, and convex,
2.~$\param \mapsto \policy (\state; \param)$ is twice continuously differentiable, for all $\state \in \states$, and $\deparam \mapsto \depolicy (\state, \action; \deparam)$ is twice continuously differentiable, for all $(\state, \action) \in \states \times \actionspace$;
3.~$\action \mapsto \reward (\state, \action)$ is twice continuously differentiable, for all $\state \in \states$;
4.~$\action \mapsto \trans
(\state[][][\prime] \mid \state, 
\action)$ 
%\amy{is this supposed to be an assumption about the transition function? $\trans$ is rendering just as an unbolded $\depolicy$.} \deni{This is supposed to be the transition function, there was a notation clash. I changed it to ``pee'' rather a ``rho'' let me know if it's clashing with anything.} 
is twice continuously differentiable, for all $\state, \state[][][\prime] \in \states$.
\end{assumption}


\begin{assumption}[Gradient Dominance Conditions]
\label{assum:param_gradient_dominance}
Given a Markov pseudo-game $\mgame$ together with a parameterization scheme $(\policy, \depolicy, \params, \deparams)$, assume
1.~(Closure under policy improvement) 
For each $\param\in \params$, there exists $\deparam\in \deparams$ s.t. $\qfunc[\player][\param](\state, \depolicy[\player](\state, \policy(\state;\param);\deparam), \policy[-\player](\state;\param)) 
= \max_{\policy[\player][][\prime]\in \fpolicies[\player](\policy(\cdot;\param))} 
\qfunc[\player][\param](\state, \policy[\player][][\prime](\state), \policy[-\player](\state;\param)) 
$ for all $\player\in \players$, $\state\in \states$.
2.~(Concavity of cumulative regret) $\deparam \mapsto \qfunc[\player][{\param[][][\prime]}] (\state, 
    \depolicy[\player] (\state, \policy[-\player] (\state; \param); \deparam), 
    \policy[-\player] (\state; \param) 
    )$ is concave, for all $\state \in \states$ and $\param, \param[][][\prime] \in \params$.
\end{assumption}
\fi
% \begin{corollary}[Gradient Dominance Condition in $\deparam$]
%      Suppose Assumptions \ref{assum:param_min_max}--\ref{assum:param_policy_class} holds, $\deparam \mapsto \gcumulreg (\param, \deparam)$ is gradient dominated with degree one for any $\param \in \params$, i.e., $\max_{\deparam[][][\prime] \in \deparams}
%     \langle \deparam[][][\prime]-\deparam, \grad[\deparam] \gcumulreg (\param, \deparam) \rangle \geq \gdconst[\deparam] \cdot (\max_{\deparam[][][\prime] \in \deparams} \gcumulreg (\param, \deparam[][][\prime]) - \gcumulreg (\param, \deparam))$ for some constant $\gdconst[\deparam]>0$. \sadie{We need this $\gdconst[\deparam]$ for convergence bounds but we don't have it?}
% \end{corollary}

Recall that \Cref{assum:param_lipschitz} guarantees Lipschitz smoothness w.r.t.\@ to both $\param$ and $\deparam$, while \Cref{assum:param_gradient_dominance} guarantees gradient dominance w.r.t\@ $\deparam$.
As the gradient of cumulative regret involves an expectation over histories,
we assume that we can simulate trajectories of play $\hist \sim \histdistrib[\initstates][\policy]$ according to the history distribution $\histdistrib[\initstates][\policy]$, for any policy profile $\policy$, and that doing so provides both value and gradient information for the rewards and transition probabilities along simulated trajectories.
That is, we rely on a differentiable game simulator (see, for instance \citet{suh2022differentiable}), meaning a stochastic first-order oracle that returns the gradients of the rewards $\reward$ and transition probabilities $\trans$, which we query to estimate deviation payoffs, and ultimately cumulative regrets.

%$\hist \sim \histdistrib[\initstates][{(\deparam[\player] (\param[-\player]), \param[-\player])}]$ and $\hist[][\prime] \sim \histdistrib[\initstates][\param]$, 

Under this assumption, we estimate these values using realized trajectories from the 
%\ssadie{}{non-deviating}
history distribution $\hist[][] \sim \histdistrib[\initstates][\param]$ induced by the outer player's policy, and the
%\ssadie{}{non-deviating}
deviation history distribution $\hist[][\deparam] 
%\doteq \left(\hist[][1], \hdots, \hist[][\numplayers] \right) 
\sim \bigtimes_{\player \in \players} \histdistrib[\initstates][{(\deparam[\player] (\param[-\player]), \param[-\player])}]$ induced by the inner player's policy.
More specifically, for all policies $\policy \in \markovpolicies$ and histories $\hist \in \hists[\numhorizons]$,
%of length $\numhorizons$,
the \mydef{payoff estimator} 
for player $\player \in \players$ is given by
%\begin{align}
$\estpayoff[\player] (\policy; \hist) \doteq \sum_{\numhorizon = 0}^{\numhorizons - 1} \initstates (\state[0]) \reward[\player] (\state[t], \policy[][][\prime] (\state[t]))
\prod_{k = 0}^{\numhorizon-1} \discount^k \trans (\state[k  + 1] \mid \state[k], (\state[k]))$.
%\enspace .
%\end{align}
%
Furthermore, for all $\param \in \params$, $\deparam \in \deparams$, $\hist[][][] \sim \histdistrib[\initstates][\param]$, and $\hist[][\deparam] = (\hist[1][\deparam], \cdots, \hist[\numplayers][\deparam]) \sim \bigtimes_{\player \in \players} \histdistrib[\initstates][{(\deparam[\player] (\param[-\player]), \param[\player])}]$, the \mydef{cumulative regret estimator} is given by
%\begin{align}
$\estgcumulreg (\param, \deparam; \hist, \hist[][\prime]) \doteq 
    \sum_{\player \in \players}
    \estpayoff[\player] (\depolicy[\player] ({} \cdot{}, \policy[-\player] (\cdot; \param); \deparam),
    \policy[-\player] ({} \cdot{}, \param); \hist[\player][\deparam])
    - \estpayoff[\player] (\policy (\cdot; \param); \hist[][])$, 
%\enspace ,
%\end{align}
%\amy{i still don't understand why you are using $h'_i$ instead of $h'$ in the payoff estimator? how can we find out $i$'s payoffs if we don't know all players' trajectories? it does not seem like enough to me to know only $i$'s.} \sadie{Payoff estimator takes in history generated by a policy profile, and for each $h_i'$, it is generated by policy $(\deparam[-\player], \param[\player])$.} \amy{thx!!!}
while the \mydef{cumulative regret gradient estimator} is given by $\estG (\param, \deparam; \hist, \hist[][\deparam]) \doteq 
%(\estG[\param] (\param, \deparam; \hist, \hist[][\deparam]), \estG[\deparam] (\param, \deparam; \hist, \hist[][\deparam])) = 
(\grad[\param] \estgcumulreg (\param, \deparam; \hist, \hist[][\prime]), \grad[\deparam] \estgcumulreg (\param, \deparam; \hist, \hist[][\deparam]))$.

Our main theorem requires one final definition, namely the \mydef{equilibrium distribution mismatch coefficient} $\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \|_\infty$, defined as the Radon-Nikodym derivative of the state-visitation distribution of the GNE $\policy[][][*]$ w.r.t.\@ the initial state distribution $\initstates$.
This coefficient, which measures the inherent difficulty of visiting states under the equilibrium policy $\policy[][][*]$---without knowing $\policy[][][*]$---is closely related to other distribution mismatch coefficients used in the analysis of policy gradient methods \citep{agarwal2020optimality}. 

We now state our main theorem, namely that, under the assumptions outlined above, \Cref{alg:two_time_sgda} computes values for the policy parameters that nearly satisfy the necessary conditions for an MGPNE 
%in a Markov pseudo-game 
in polynomially many gradient steps, or equivalently, calls to the differentiable simulator.

\deni{New shortened theorem added here, move the one after it to the appendix instead. Need to work around the restatable env.}

\begin{restatable}{theorem}{thmconvergence} \label{thm:convergence_GNE} 
Given a Markov pseudo-game $\mgame$, and a parameterization scheme $(\policy, \depolicy, \params, \deparams)$, suppose \Cref{assum:existence_of_mpgne}, \ref{assum:param_lipschitz}, and \ref{assum:param_gradient_dominance} hold. For any $\delta > 0$, set $\varepsilon = \delta \|\brmismatch (\cdot, \initstates, \cdot)\|_\infty^{-1}$.
If \Cref{alg:two_time_sgda} is run with inputs that satisfy,  
$\learnrate[\param][ ], \learnrate[\deparam][ ] \asymp  \poly(\varepsilon, \| \nicefrac{\partial \statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \|_\infty, \frac{1}{1-\discount}, \lipschitz[\grad \gcumulreg]^{-1}, \lipschitz[\gcumulreg]^{-1})$, then for some $\numiters \in \poly \left(\varepsilon^{-1}, (1-\discount)^{-1}, \| \nicefrac{\partial \statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \|_\infty, \lipschitz[{\grad \gcumulreg}], \lipschitz[\gcumulreg], \diam (\params \times \deparams), \learnrate[\param][ ]^{-1}\right)$, there exists $\bestiter[{\param}][\numiters]=\param[][][(k)]$ with $k\leq T$ that
 is a $(\varepsilon, \nicefrac{\varepsilon}{2 \lipschitz[\gcumulreg]})$-stationary point of the exploitability, i.e., there exists $\param[][][*]\in \params$ s.t. $\|\bestiter[\param][T]-\param[][][*]\|\leq \nicefrac{\varepsilon}{2 \lipschitz[\gcumulreg]}$ and $\min_{\h\in \subdiff \gexploit(\param[][][*])}\|\h\|\leq \varepsilon$.

Further, for any arbitrary state distribution $\diststates \in \simplex(\states)$, if $\sexploit(\diststates, \cdot)$ is differentiable at $\param[][][*]$, $\| \grad[\param] \gexploit (\diststates, \param[][][*]) \| \leq \delta$, i.e., $\bestiter[{\param}][\numiters]$ is a $(\varepsilon, \delta)$-stationary point for the expected exploitability $\sexploit(\diststates, \cdot)$.
\end{restatable}


% \begin{restatable}{theorem}{thmconvergence} \label{thm:convergence_GNE} 
% Given a Markov pseudo-game $\mgame$, and a parameterization scheme $(\policy, \depolicy, \params, \deparams)$, suppose \Cref{assum:existence_of_mpgne}, \ref{assum:param_lipschitz}, and \ref{assum:param_gradient_dominance} hold. For any $\delta > 0$, set $\varepsilon = \delta \|\brmismatch (\cdot, \initstates, \cdot)\|_\infty^{-1}$.
% If \Cref{alg:two_time_sgda} is run with inputs that satisfy,  
% $\learnrate[\param][ ], \learnrate[\deparam][ ] \asymp  \poly(\varepsilon, \| \nicefrac{\partial \statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \|_\infty, \frac{1}{1-\discount}, \lipschitz[\grad \gcumulreg]^{-1}, \varconst[\deparam]^{-1}, \lipschitz[\gcumulreg]^{-1})$, then for some $\numiters \in \poly \left(\varepsilon^{-1}, (1-\discount)^{-1}, \| \nicefrac{\partial \statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \|_\infty, \lipschitz[{\grad \gcumulreg}], \lipschitz[\gcumulreg], \diam (\params \times \deparams), \learnrate[\param][ ]^{-1}\right)$, there exists $\bestiter[{\param}][\numiters]=\param[][][(k)]$ with $k\leq T$ that
%  is a \ssadie{}{$(\varepsilon, \nicefrac{\varepsilon}{2 \lipschitz[\gcumulreg]})$-}stationary point of the \ssadie{Moreau}{} exploitability, i.e., \ssadie{$\| \grad \regulexploit (\bestiter[{\param}][\numiters])\| \leq \varepsilon$.} {there exists $\param[][][*]\in \params$ s.t. $\|\bestiter[\param][T]-\param[][][*]\|\leq \nicefrac{\varepsilon}{2 \lipschitz[\gcumulreg]}$ and $\min_{\h\in \subdiff \gexploit(\param[][][*])}\|\h\|\leq \varepsilon$.}


% Further, for any arbitrary state distribution $\diststates \in \simplex(\states)$, \ssadie{}{if $\sexploit(\diststates, \cdot)$ is differentiable at $\param[][][*]$, }$\| \grad[\param] \gexploit (\diststates, \param[][][*]) \| \leq \delta$. \ssadie{}{This suggests that $\bestiter[{\param}][\numiters]$ is a $(\varepsilon, \delta)$-stationary point for the expected exploitability $\sexploit(\diststates, \cdot)$.}

% \ssadie{Additionally, there exists $\param[][][*] \in \params$ s.t.\@ $\| \bestiter[\param][T] - \param[][][*] \| \leq \nicefrac{\varepsilon}{2 \lipschitz[\gcumulreg]}$ and $\| \grad[\param] \sexploit (\diststates, \bestiter[{\param}][\numiters]) \| \leq \delta$, for any arbitrary state distribution $\diststates\in \Delta(\states)$.}{}
% \end{restatable}
% \begin{restatable}{theorem}{thmconvergence} 
% \label{thm:convergence_GNE_full} 
% \deni{Revisit for the definitions of the variance which are rn in the theorem proofs.}
% Consider a Markov pseudo-game $\mgame$ \Cref{assum:existence_of_mpgne}, \ref{assum:param_lipschitz}, and \ref{assum:param_gradient_dominance} hold. 
% For any $\delta > 0$, set $\varepsilon = \left(\frac{1}{\max_{\param\in \params} \max_{\diststates\in \simplex(\states) \brmismatch (\param, \initstates, \diststates)}} \right) \delta$. 
% If \Cref{alg:two_time_sgda} is run with inputs that satisfy for all $\numhorizon \in [\numiters]$,  
% $\learnrate[\param][] \asymp  \frac{\epsilon^4 \left( 
% \nicefrac{\left\| \nicefrac{\partial \statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right \|_\infty}{1-\discount} \right)^2}{\lipschitz[\grad \gcumulreg]^3 \left( \lipschitz[\grad \gcumulreg]^2 + \varconst[\deparam]^2 \right) \left( \nicefrac{\lipschitz[\gcumulreg]}{\lipschitz[\grad \gcumulreg]} + 1\right)^2}$,
% $\learnrate[\deparam][] \asymp \frac{\varepsilon^8 \left( \nicefrac{\left\| \nicefrac{\partial \statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right \|_\infty}{1-\discount} \right)^4}{\lipschitz[\grad \gcumulreg]^5 \lipschitz[\gcumulreg] \left( \nicefrac{\lipschitz[\gcumulreg]}{\lipschitz[\grad \gcumulreg]} + 1 \right)^4 \left( \lipschitz[\gcumulreg]^2 + \varconst[\deparam]^2 \right)^{\nicefrac{3}{2}}} \land \frac{\varepsilon^2}{\lipschitz[\grad \gcumulreg] \left( \lipschitz[\gcumulreg]^2 + \varconst[\deparam]^2 \right)}$,
% and $\numiters \geq \left( \nicefrac{(1-\discount)}{\left\| \nicefrac{\partial \statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right \|_\infty} + \nicefrac{\lipschitz[\gcumulreg]}{2 \lipschitz[{\grad \gcumulreg}]} \right)^{-1} \frac{\lipschitz[\gcumulreg] \diam (\params \times \deparams)}{\varepsilon^{2} \learnrate[\param][]}$,
% then the best-iterate policies $\bestiter[{\param}][\numiters] \in \argmin_{\numhorizon = 0, 1, \cdots, \numiters} \left\| \grad \gexploit (\param[][][(\numhorizon)]) \right\|$ converge to a stationary point of the Moreau envelope of the exploitability, i.e., $\left\| \grad \regulexploit (\bestiter[{\param}][\numiters]) \right\| \leq \varepsilon$. 
% Additionally, there exists $\param[][][*] \in \params$ s.t.\@ $\| \bestiter[\param][T] - \param[][][*] \| \leq \nicefrac{\varepsilon}{2 \lipschitz[\gcumulreg]}$ and $\| \grad[\param] \sexploit (\diststates, \bestiter[{\param}][\numiters]) \| \leq \delta$, for any arbitrary state distribution $\diststates\in \Delta(\states)$.
% \end{restatable}

\if 0
\amy{so what does this result say in English about convergence to a recursive RE? add English that reminds the reader the extent to which a small gradient of the expected state-exploitability implies a recursive RE -- up to being an inflection point.}
\fi
% \begin{proof}
% We invoke Theorem 2 of \citeauthor{daskalakis2020independent}
% (\citeyear{daskalakis2020independent}).
% Although their result is stated for gradient-dominated-gradient-dominated functions, their proof applies in the more general case of non-convex-gradient-dominated functions.
    
% First, \Cref{assum:param_lipschitz} guarantees that the cumulative regret $\gcumulreg$ is Lipschitz-smooth w.r.t.\@ $(\param, \deparam)$.
%     Moreover, under \Cref{assum:param_lipschitz}, which guarantees that $\deparam \mapsto \qfunc[\player][{\param[][][\prime]}] (\state, \depolicy[\player] (\state, \policy[-\player] (\state; \param); \deparam), 
%     \policy[-\player] (\state; \param) 
%     )$ is continuously differentiable for all $\state \in \states$ and $\param, \param[][][\prime] \in \params$, and \Cref{assum:param_gradient_dominance}, we have that $\gcumulreg$ is $\left(\nicefrac{\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|_\infty}{1-\discount} \right)$-gradient-dominated in $\deparam$, for all $\param \in \params$, by Theorems 2 and 4 of \citeauthor{bhandari2019global} (\citeyear{bhandari2019global}). 
%     Finally, under \Cref{assum:param_lipschitz}, since the policy, the reward function, and the transition probability function are all Lipschitz-continuous, $\estpayoff$, $\estgcumulreg$, and hence $\estG$ are also Lipschitz-continuous, since $\states, \params$, and $\deparams$ are compact. 
%     Their variance must therefore be bounded, i.e., there exists $\varconst[\param], \varconst[\deparam] \in \R$ s.t.\@ $\Ex_{\hist, \hist[][\prime]}[\estG[\param] (\param, \deparam; \hist, \hist[][\prime])- \grad[\param] (\param, \deparam; \hist, \hist[][\prime])] \leq \varconst[\param]$ and $\Ex_{\hist, \hist[][\prime]}[\estG[\deparam] (\param, \deparam; \hist, \hist[][\prime])- \grad[\deparam] (\param, \deparam; \hist, \hist[][\prime])] \leq \varconst[\deparam]$. 

%     Hence, under our assumptions, the assumptions of Theorem 2 of \citeauthor{daskalakis2020independent} are satisfied.
%     Therefore,
%     %\begin{align}
%         $\nicefrac{1}{\numiters  + 1} \sum_{\numhorizon = 0}^\numiters \|\grad \regulexploit(\param[][][(\numhorizon)]) \|\leq \varepsilon$.
%     %\enspace .
%     %\end{align}
%     Taking a minimum across all $\numhorizon \in [\numiters]$, 
%     %\samy{}{to identify the best iterate}, 
%     we conclude $\left\|\grad \regulexploit (\bestiter[{\param}][\numiters]) \right\| \leq \varepsilon$.
%     %\begin{align}
%         %$\min_{t = 0, 1, \cdots, \numiters} \|\grad \regulexploit(\param[][][(\numhorizon)]) \|\leq \varepsilon$.
%     %\enspace .
%     %\end{align}
%     % The second part is then a direct consequence of the hoeffding bound, whose assumptions are satisfied since the objective is bounded from above and from below by 0, as the objective is continuous and its domain is non-empty, and compact.
    
%     \ssadie{}{Then, by the Lemma 3.7 of \cite{lin2020gradient}, there exists some $\param[][][*]\in \params$ such that $\|\bestiter[{\param}][\numiters]-\param[][][*]\|\leq \frac{\varepsilon}{2\lipschitz[\gcumulreg]}$ and $\param[][][*]\in \params_\varepsilon\doteq\{\param\in \params\mid \exists \alpha\in \subdiff\gexploit(\param), \|\alpha\|\leq \varepsilon\}$. 
%     Note that since $\gcumulreg$ is Lipschitz-smooth, $\gexploit$ is weakly convex and $\subdiff\gexploit(\param)=\subdiff \obj(\param) - \lipschitz[\gcumulreg] \param$ where $\obj(\param) \doteq \max_{\deparam\in \deparams}\{\gcumulreg(\param, \deparam) + \nicefrac{\lipschitz[\gcumulreg]}{2}\|\param\|^2\}$. Since  $\gcumulreg(\param, \deparam) + \nicefrac{\lipschitz[\gcumulreg]}{2}\|\param\|^2$ is convex in $\param$ for each $\deparam\in \deparams$ and $\deparams$ is compact, Danskin's theorem implies that $\grad[\param]\gcumulreg(\param, \deparam[][][*] + \lipschitz[\gcumulreg] \in \subdiff \obj(\param)$, for any $\deparam[][][*]\in \argmax_{\deparam\in \deparams}\gcumulreg(\param, \deparam)$. Putting these piece together yields that $\grad[\param]\gcumulreg(\param, \deparam[][][*])\in \subdiff \gexploit(\param)$ for any $\deparam[][][*]\in \argmax_{\deparam\in \deparams}\gcumulreg(\param, \deparam)$. Hence, 
%     \begin{align}
%         \param[][][*]\in \params_\varepsilon
%         &\doteq\{\param\in \params\mid \exists \alpha\in \subdiff\gexploit(\param), \|\alpha\|\leq \varepsilon\}\\
%         &\supseteq\{\param\in \params\mid \|\grad[\param]\gcumulreg(\param, \deparam[][][*])\|\leq \varepsilon, \; \forall \deparam[][][*]\in \argmax_{\param\in \params}\gcumulreg(\param, \deparam)\}
%     \end{align}
%     Moreover, by the proof of \Cref{lemma:br_mismatch_coef}, we know that for any state distribution $\diststates\in \simplex(\states)$,
%     \begin{align}
%         \|\grad[\param]\sexploit(\diststates, \param)\|
%         &\leq 
%         \max_{\deparam[][][*]\in \argmax_{\deparam\in \deparams}\scumulreg(\diststates, \param,\deparam)}\|\grad[\param]\scumulreg(\diststates, \param, \deparam[][][*])\|\\
%         &\leq \max_{\player\in \players}\max_{\deparam[][][*]\in \argmax_{\deparam\in \deparams}\scumulreg(\diststates, \param,\deparam)}
%         \left(\frac{1}{1-\discount}\right)^2 \left\Vert\frac{\statedist[\diststates][{\deparam[-\player][][*], \param[-\player]}]}{\initstates}\right\Vert_\infty
%         \left\Vert\frac{\statedist[\diststates][{\param}]}{\initstates}\right\Vert_\infty \|\grad[\param]\gcumulreg(\param, \deparam[][][*])\|\\
%        &=\brmismatch(\param, \initstates, \diststates)\|\grad[\param]\gcumulreg(\param, \deparam[][][*])\|\\
%        \frac{1}{\brmismatch(\param, \initstates, \diststates)} \|\grad[\param]\sexploit(\diststates, \param)\|
%        &\leq \|\grad[\param]\gcumulreg(\param, \deparam[][][*])\| 
%     \end{align}
% Therefore, 
% \begin{align}
%         \param[][][*]\in \params_\varepsilon
%         &\doteq\{\param\in \params\mid \exists \alpha\in \subdiff\gexploit(\param), \|\alpha\|\leq \varepsilon\}\\
%         &\supseteq\{\param\in \params\mid \|\grad[\param]\gcumulreg(\param, \deparam[][][*])\|\leq \varepsilon, \; \forall \deparam[][][*]\in \argmax_{\param\in \params}\gcumulreg(\param, \deparam)\}\\
%         &\supseteq \{\param\in \params\mid \nicefrac{1}{\brmismatch(\param, \initstates, \diststates)} \|\grad[\param]\sexploit(\diststates,\param)\| 
%         \leq \varepsilon\}\\
%         &=\{\param\in \params\mid \|\grad[\param]\sexploit(\diststates,\param)\| 
%         \leq \delta\}
%     \end{align}
%     Therefore, we can conclude that there exists $\param[][][*]$ such that $\|\bestiter[\param][T]-\param[][][*]\|\leq \frac{\varepsilon}{2\lipschitz[\gcumulreg]}$ and $\|\grad[\param]\sexploit(\diststates, \param)\|\leq \delta$ for any $\diststates$. 
%     }
% \end{proof}