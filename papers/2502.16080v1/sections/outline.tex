\longversion{
\subsection{Outline}

This paper is organized as follows.
In Section~\ref{sec:prelim}, we define our notation and some requisite mathematical terminology.
%
Next, in Section~\ref{sec:gmg}, we develop our model of generalized Markov games (Section~\ref{sec:gmg_model}), for which we define appropriate solution concepts, i.e., equilibria, in Section~\ref{sec:gmg_exist}.
We then (Section~\ref{sec:gmg_conv}) present a gradient descent-ascent-based reinforcement learning algorithm (\Cref{alg:two_time_sgda}; TTSSGDA) which provably converges to these equilibria via a coupled min-max optmization formulation of the problem (Section~\ref{sec:gmg_minmax}).
%
Then, in Section~\ref{sec:dsge}, we apply our theory to infinite horizon stochastic exchange economies.
First, in Section~\ref{sec:static}, we develop static exchange economies, i.e., spot markets, as generalized (one-shot) games;
then, in Section~\ref{sec:infinite}, we develop infinite horizon---in general, incomplete---stochastic economies, by instantiating our model of generalized Markov games.
Finally, we invoke our main theorems for generalized Markov games to establish the existence of recursive competitive equilibria in incomplete stochastic economies, the first such result to our knowledge, as well as convergence of TTSSGDA to this equilibrium.
This latter result is notable because the equilibria that we learn are equilibria in Markov policies, as opposed to unwieldy non-stationary policies; and recursive, meaning they exhibit subgame-perfect-like behavior.}
