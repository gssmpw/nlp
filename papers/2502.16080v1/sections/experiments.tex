\section{Experiments}
\label{sec:expts}

Given an infinite horizon Markov exchange economy $\economy$, we associate with it an exchange economy Markov pseudo-game $\mgame$, and we then construct a deep learning network to solve $\mgame$.
To do so, we assume a parametrization scheme $(\policy, \depolicy, \params, \deparams)$, where the parametric policy profiles $(\policy, \depolicy)$ are represented by neural networks with $(\params, \deparams)$ as the corresponding network weights.
Computing an RRE via \Cref{alg:two_time_sgda} can then be seen as the result of training a generative adversarial neural network \cite{goodfellow2014gan}, where $\policy$ (resp.\@ $\depolicy$) is the output of the generator (resp.\@ adversarial) network.
We call such a neural representation a \mydef{generative adversarial policy network (GAPNet)}.

Following this approach, we built GAPNets to approximate the RRE in two types of infinite-horizon Markov exchange economies: one with a deterministic transition probability function and another with a stochastic transition probability function. 
Within each type, we experimented with three randomly sampled economies, each with 10 consumers, 10 commodities, 1 asset, 5 world states, and characterized by a distinct class of reward functions, which impart different smoothness properties onto the state-value function: 
\mydef{linear}: $\util[\buyer](\allocation[\buyer]; \type[\buyer]) = \sum_{\good \in \goods} \type[\buyer][\good] \allocation[\buyer][\good]$; 
\mydef{Cobb-Douglas}:  $\util[\buyer](\allocation[\buyer]; \type[\buyer]) = \prod_{\good \in \goods} {\allocation[\buyer][\good]}^{\type[\buyer][\good]}$; 
and \mydef{Leontief}:  $\util[\buyer](\allocation[\buyer]; \type[\buyer]) = \min_{\good \in \goods} \left\{ \frac{\allocation[\buyer][\good]}{\type[\buyer][\good]}\right\}$. 
%
\footnote{Full details of our experimental setup appear in \Cref{sec_app:experiments}, including hyperparameter search values, final experimental configurations, and visualization code.
See also our code repository: \texttt{\coderepo}.} 

We compare our results with a classic neural projection method (also known as deep equilibrium nets \cite{azinovic2022deep}), which macroeconomists and others use to solve stochastic economies.
In this latter method, one seeks a policy profile that minimizes the norm of the system of first-order necessary and sufficient conditions that characterize RRE
%\sadie{Theoretically MPGNE, but that corresponds to RRE}
(see for instance, \cite{FernandezVillaverde2023CompMethodsMacro}).%
\footnote{We describe the neural projection method in \Cref{sec_app:npm}.}
We use the same network architecture for both methods, and select hyperparameters through grid search.
In all experiments, we evaluate the performance of the computed policy profiles using three metrics: total first-order violations, average Bellman errors,%
\footnote{The definitions of these two metrics can be found in \Cref{sec_app:npm}.} 
and exploitability. 

\amy{move to appendix:}
\ssadie{For each metric, we randomly sample 50 policy profiles, record their corresponding values, and normalize the results by dividing it by the average.}{}

%To make sure that this baseline is on equal footing with our method, we optimized this objective over a family of neural networks, namely a family which is exactly the same as the one defining the class of possible generators for our network, GAP. 
% In other words, neural projection methods seek to minimize the first-order Bellman error, an upper bound on the Bellman error, i.e., the distance to the optimal value function in value space \cite{puterman2014markov}.

\amy{what is the x-axis of these figures? 8 is a very small number. is that the total number of iterations?!}\sadie{Total is 2000, we record every 200 iterations, should we change the x-axis?}
\amy{yes, i would change the x-axis. 2000 makes more sense than 8. and label it ``Iterations" somewhere if you can.}

\amy{if it is possible to regenerate the figures, they should not say ``Normalized" in the titles, because it sounds like the economies themselves are normalized, when it is really the metrics that are being normalized...so it is fine to leave ``Normalized metrics'' in the captions.}\sadie{OK I can do it}





% \begin{figure}
%     \begin{subfigure}{\textwidth}
%         \centering
%         \includegraphics[width=0.6\textwidth]{figures/linear_nonstochastic.png}
%         %\caption{Normalized Metrics for Linear Economy with Deterministic Transition Probability Function}
%         \label{fig:linear_nonstochastic}
%     \end{subfigure}
    
%     % \vspace{0.5cm} % Adjust spacing between subfigures
    
%     \begin{subfigure}{\textwidth}
%         \centering
%         \includegraphics[width=0.6\textwidth]{figures/cd_nonstochastic.png}
%         %\caption{Normalized Metrics for Cobb-Douglas Economy with Deterministic Transition Probability Function}
%         \label{fig:cd_nonstochastic}
%     \end{subfigure}
    
%     % \vspace{0.5cm} % Adjust spacing between subfigures
    
%     \begin{subfigure}{\textwidth}
%         \centering
%         \includegraphics[width=0.6\textwidth]{figures/leontief_nonstochastic.png}
%         %\caption{Normalized Metrics for Leontief Economy with Deterministic Transition Probability Function}
%         \label{fig:leontief_nonstochastic}
%     \end{subfigure}
%     \caption{Normalized Metrics for Economies with Deterministic Transition Probability Function}
%     \label{fig:nonstochastic}
% \end{figure}


\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/linear_stochastic.png}
        %\caption{Normalized Metrics for Linear Economy with Stochastic Transition Probability Function}
        \label{fig:linear_stochastic}
    \end{subfigure}
    
    % \vspace{0.5cm} % Adjust spacing between subfigures
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/cd_stochastic.png}
        %\caption{Normalized Metrics for Cobb-Douglas Economy with Stochastic Transition Probability Function}
        \label{fig:cd_stochastic}
    \end{subfigure}
    
    % \vspace{0.5cm} % Adjust spacing between subfigures
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/leontief_stochastic.png}
        %\caption{Normalized Metrics for Leontief Economy with Stochastic Transition Probability Function}
        \label{fig:leontief_stochastic}
    \end{subfigure}

    \caption{Normalized Metrics for Economies with Stochastic Transition Probability Function}
    \label{fig:stochastic}
\end{figure}



\Cref{fig:nonstochastic} (\Cref{sec_app:experiments}) depicts our results for economies with deterministic transition functions.
Perhaps unsurprisingly, while GAPNets demonstrates a clear advantage in minimizing exploitability in all three economies, the neural projection method (NPM) slightly outperforms GAPNets in minimizing total first order violations and average Bellman error, the metrics they are specifically designed to minimize.
Furthermore, in all three economies, exploitability is near 0, highlighting GAPNet's ability to approximate at least a Radner equilibrium. 
%\sadie{Should we say RE or RRE?} 
\Cref{fig:stochastic} presents our results for economies with stochastic transition functions. 
These results indicate that stochasticity hinders NPM's ability to minimize the three metrics, even the method is explictly designed to minimize two of them.
In contrast, GAPNet successfully minimizes all three metrics across all economies.

\yc{Wow! GAP performed so well.}
