\section{Related Work}
We now position our contributions in relation to other works on uncertainty-aware quality estimation, faster inference, and reranking.


\paragraph{Model uncertainty.}
Quantifying uncertainty in learned automated machine translation metrics was first proposed by ____, who use the variance from ensembles or Monte Carlo dropout as a measure of uncertainty.
A different approach by ____ introduces a secondary confidence estimation model to complement the original quality estimation model.\footnote{
They also propose an approach whereby one model predicts both quantities, similar to our instant confidence.
See \Cref{sec:fail_hts} documenting our attempts at replicating this model.
}
Instead, we optimize a single model that produces two scores: a quality estimate and a measure of uncertainty.
We optimize this model jointly in each training step, optionally optimizing for predictions at each layer and optionally with respect to the last layer's prediction.
Finally, we note that the focus of ____ is on analyzing the source of uncertainty, while our focus is on using uncertainty to make quality estimation more reliable and efficient.

\paragraph{Faster quality estimation.}
Multiple previous works investigated improving the efficiency of calculating trained metrics.
For large scales, ____ use length-batching to speed up inference.
At the same time, they statically prune the quality estimation model, which is similar to our constant-exit approach.
____ use a smaller baseline language model rather than the default XLM-Roberta for COMET.
____ find that simply quantizing the model to half precision has almost no effect on the final quality estimation performance while halving the compute costs.
____ port COMET to a faster inference engine for massive speed gains.
Finally, ____ explore pruning, distillation, and quantization for a very large quality estimation model, xCOMET ____.
All of these approaches are orthogonal to our method
and could be used in combination.

\paragraph{Early-Exit.}
Previous works have proposed methods to stop computation at a particular Transformer block layer ____, but these methods are largely applicable to classification tasks.
____ propose \textit{learning-to-exit}, which we loosely follow in our work.
However, instead of predicting a probability of success, we predict the absolute error of the model, which is directly interpretable.

\paragraph{Reranking.}
Reranking improves translation quality ____, but scoring large candidate sets is computationally expensive. One common approach is minimum Bayes risk (MBR) decoding ____, which selects the translation candidate with the lowest expected risk. Recent work has made MBR more efficient ____, including methods that pre-select candidates with cheaper, noisier scoring functions ____.

Other approaches improve efficiency through token-level reranking ____ or by framing reranking as a Bayesian Optimization problem, where a cheaper scoring model assists in identifying high-quality candidates before applying the more expensive scoring model ____.