\section{Related Work}
We now position our contributions in relation to other works on uncertainty-aware quality estimation, faster inference, and reranking.


\paragraph{Model uncertainty.}
Quantifying uncertainty in learned automated machine translation metrics was first proposed by \citet{glushkova-etal-2021-uncertainty-aware}, who use the variance from ensembles or Monte Carlo dropout as a measure of uncertainty.
A different approach by \citet{zerva-etal-2022-disentangling} introduces a secondary confidence estimation model to complement the original quality estimation model.\footnote{
They also propose an approach whereby one model predicts both quantities, similar to our instant confidence.
See \Cref{sec:fail_hts} documenting our attempts at replicating this model.
}
Instead, we optimize a single model that produces two scores: a quality estimate and a measure of uncertainty.
We optimize this model jointly in each training step, optionally optimizing for predictions at each layer and optionally with respect to the last layer's prediction.
Finally, we note that the focus of \citet{zerva-etal-2022-disentangling} is on analyzing the source of uncertainty, while our focus is on using uncertainty to make quality estimation more reliable and efficient.

\paragraph{Faster quality estimation.}
Multiple previous works investigated improving the efficiency of calculating trained metrics.
For large scales, \citet{rei-etal-2022-searching} use length-batching to speed up inference.
At the same time, they statically prune the quality estimation model, which is similar to our constant-exit approach.
\citet{cheng2024bayesianoptimizationapproachmachine} use a smaller baseline language model rather than the default XLM-Roberta for COMET.
\citet{zouhar-etal-2024-pitfalls} find that simply quantizing the model to half precision has almost no effect on the final quality estimation performance while halving the compute costs.
\citet{gowda-etal-2023-cometoid,gowda-etal-2024-pymarian} port COMET to a faster inference engine for massive speed gains.
Finally, \citet{larionov2024xcometlitebridginggapefficiency} explore pruning, distillation, and quantization for a very large quality estimation model, xCOMET \citep{guerreiro-etal-2024-xcomet}.
All of these approaches are orthogonal to our method
and could be used in combination.

\paragraph{Early-Exit.}
Previous works have proposed methods to stop computation at a particular Transformer block layer \citep{bertpatience}, but these methods are largely applicable to classification tasks.
\citet{xin-etal-2021-berxit} propose \textit{learning-to-exit}, which we loosely follow in our work.
However, instead of predicting a probability of success, we predict the absolute error of the model, which is directly interpretable.

\paragraph{Reranking.}
Reranking improves translation quality \citep{freitag-etal-2022-high}, but scoring large candidate sets is computationally expensive. One common approach is minimum Bayes risk (MBR) decoding \citep[MBR;][]{eikema-aziz-2020-map}, which selects the translation candidate with the lowest expected risk. Recent work has made MBR more efficient \citep{cheng-vlachos-2023-faster,deguchi-etal-2024-centroid,trabelsi2024efficientminimumbayesrisk,vamvas-sennrich-2024-linear}, including methods that pre-select candidates with cheaper, noisier scoring functions \citep{fernandes-etal-2022-quality, eikema-aziz-2022-sampling}.

Other approaches improve efficiency through token-level reranking \citep{singhal-etal-2023-eel} or by framing reranking as a Bayesian Optimization problem, where a cheaper scoring model assists in identifying high-quality candidates before applying the more expensive scoring model \citep{cheng2024bayesianoptimizationapproachmachine}.