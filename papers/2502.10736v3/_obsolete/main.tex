% \documentclass[acmsmall,screen,review,anonymous]{acmart}
\documentclass[screen,acmsmall]{acmart}

\usepackage{CJKutf8}
\usepackage{array}
\usepackage{fontawesome5}
\usepackage{xcolor}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

\acmConference[CSCW'25]{The 28th ACM SIGCHI Conference on Computer-Supported Cooperative Work \& Social Computing}{October 18--22,
  2025}{Bergen, Norway}


\sloppy

\begin{document}

\newcommand{\system}{SpeechCap}

\newcommand{\altcolor}[1]{{#1}}
% \newcommand{\altcolor}[1]{\textcolor{blue}{#1}}

\title{SpeechCap: Leveraging Playful Impact Captions to Facilitate Interpersonal Communication in Social Virtual Reality}
\renewcommand{\shorttitle}{SpeechCap}


\author{Yu Zhang}
\affiliation{%
  \institution{City University of Hong Kong}
  \city{Hong Kong SAR}
  \country{China}
}
\email{yui.zhang@my.cityu.edu.hk}

\author{Yi Wen}
\affiliation{%
  \institution{Texas A\&M University}
  \city{College Station, Texas}
  \country{United States}
}
\email{cyberwenyi2357@tamu.edu}

\author{Siying HU}
\affiliation{%
  \institution{City University of Hong Kong}
  \city{Hong Kong SAR}
  \country{China}
}
\email{siyinghu-c@my.cityu.edu.hk}

\author{Zhicong Lu}
\affiliation{%
  \institution{George Mason University}
  \city{Fairfax, Virginia}
  \country{United States}
}
\email{zlu6@gmu.edu}

\renewcommand{\shortauthors}{Zhang, et al.}


% \begin{CJK*}{UTF8}{gbsn}


\begin{abstract}
Social Virtual Reality (VR) emerges as a promising platform bringing immersive, interactive, and engaging mechanisms for collaborative activities in virtual spaces. 
However, interpersonal communication in social VR is still limited with existing mediums and channels.
\altcolor{To bridge the gap, we propose a novel method for mediating real-time conversations in social VR, which leverages \textit{impact captions}, a type of typographic visual effect widely used in videos, to encode both verbal and non-verbal information.}
We first investigated the design space of impact captions by content analysis and a co-design session with four experts.
We then implemented \system{}, a proof-of-concept system with which users can communicate with each other using speech-driven impact captions in VR.
Through a user study (N=14), we evaluated the effectiveness of the visual and interaction design of speech-driven impact captions, highlighting their strengths in the interactivity and integrating verbal and non-verbal information in communication mediums.
Finally, we discussed our main findings regarding visual rhetoric, interactivity, and ambiguity, and further provided design implications for facilitating interpersonal communication in social VR.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003124.10010866</concept_id>
       <concept_desc>Human-centered computing~Virtual reality</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003129</concept_id>
       <concept_desc>Human-centered computing~Interactive systems and tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003123.10011759</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in interaction design</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Virtual reality}
\ccsdesc[500]{Human-centered computing~Interactive systems and tools}
\ccsdesc[500]{Human-centered computing~Empirical studies in interaction design}

\keywords{Social Virtual Reality, Interpersonal Communication, Impact Captions}

\begin{teaserfigure}
  \includegraphics[width=\textwidth]{img/cover_revision.png}
    \caption{
    \altcolor{
    SpeechCap showcases a novel way to mediate voice conversations in social VR by converting speech into playful impact captions.
      (A) From real-time speech, SpeechCap generates impact captions with customized visual design that conveys both verbal and non-verbal information. The caption ``happy'' is in a warm and bright color for the positive emotions, and a ``smiling'' emoji further highlight the pleasant and relaxing atmosphere.
      (B) Users can play with impact captions like real-world objects, e.g., throwing a ``cake''.
      (C) Once the flying ``cake'' crashes on another user, it will explode like a firework.
    }}
  \Description{.}
  \label{fig:teaser}
\end{teaserfigure}


\maketitle


\section{Introduction}
With the increasing availability of consumer-level virtual reality head-mounted displays (HMDs) ~\cite{anthes2016state, li2021social}, Social Virtual Reality (VR) emerges as a promising platform that brings immersive, interactive, and engaging experiences to users for a wide range of collaborative activities. 
Social VR now supports remote conferencing ~\cite{abdullah2021videoconference}, immersive learning ~\cite{thanyadit2022xr, peng2021exploring}, healthcare ~\cite{li2020designing, udapola2022social}, collaborative design ~\cite{mei2021cakevr}, music composing ~\cite{men2022supporting} and daily activities such as sleeping ~\cite{maloney2020falling} and even drinking ~\cite{chen2024drink}.
Among these diverse application areas, social VR helps users overcome physical dispersion, enabling them to communicate and collaborate with each other in the same virtual environment ~\cite{li2021social, palmer1995interpersonal} by offering channels supporting both verbal and non-verbal communication ~\cite{dzardanova2022virtual, mcveigh2021case, wei2022communication}.
Text messaging and voice chat are two channels commonly used for verbal communication. 
Pictures ~\cite{li2019measuring}, avatars ~\cite{fu2023mirror, baker2021avatar, kolesnichenko2019understanding, freeman2021body}, and embodied interactions ~\cite{maloney2020talking, wieland2022non} are channels that can convey non-verbal information in social VR.

\altcolor{Yet, these existing communication channels in social VR are preliminary, as they are either borrowed from traditional communication media (e.g., texts, voices, and pictures) lacking adaptations to the nature of social VR ~\cite{montoya2023wordsphere, rzayev2021reading, dzardanova2022virtual, hsieh2020bridging}, or difficult to be reproduced very well by current VR technologies (e.g., gestures, facial expressions, embodied actions) ~\cite{sykownik2023vr, tanenbaum2020make, wei2022communication}.
Specifically, texts as a communication medium in VR still suffer from difficulties in user inputting ~\cite{montoya2023wordsphere} and readability issues in presentations ~\cite{rzayev2021reading, hsieh2020bridging}. 
Voice-based conversation in VR can easily be disturbed by sound intrusions from the environment ~\cite{akselrad2023body}. 
Furthermore, non-verbal communication mediums, such as gestures and facial expressions, cannot be well-simulated with the avatars in current VR applications ~\cite{baker2021avatar, aseeri2021influence, freeman2021body}, and users may find it hard to achieve specific actions or movements using controllers with joysticks and buttons ~\cite{sykownik2023vr, tanenbaum2020make, li2019measuring}.}

Except for social VR, previous research in HCI and CSCW has explored ways to enhance interpersonal communication in different scenarios. Those scenarios include video conferencing ~\cite{liu2023visual, xia2023crosstalk}, augmented reality (AR) ~\cite{lee2023exploring, liao2022realitytalk, leong2022wemoji}, and text messaging interfaces ~\cite{aoki2022emoballoon}. 
These different scenarios demonstrate the power of the integration of both verbal and non-verbal cues in the mediums to support interpersonal communication. 
On the other hand, researchers have also envisioned the potential of social VR in providing situational awareness and enabling new social interactions with novel communication mediums that are unattainable in traditional computer-mediated communication (CMC) platforms ~\cite{mcveigh2021case, mcveigh2022beyond}. 

\altcolor{On top of the prior work, we proposed a new approach to support interpersonal communication in social VR. Our approach is inspired by \textit{Impact Captions}, a type of expressive typographic 2D visual effect with engaging visual elements and animated motions. Impact captions are traditionally used in entertaining television shows and online videos ~\cite{o2010japanese, sasamoto2014impact} to attract viewers' attention and create engaging watching experiences ~\cite{sasamoto2021hookability}. 
Particularly, we introduced impact captions as a communication medium to facilitate interpersonal communication and interactions in social VR. In our design, impact captions encode both verbal and non-verbal information extracted from real-time speech; they also offer interactivity that allows users to intentionally play with the captions for communicative purposes.}

\altcolor{To implement and evaluate our impact-caption-inspired approach, we first explored the design space of impact captions regarding the visual appearances and interactivity through a content analysis of a collection of TV show videos.
Based on the design space, we held a co-design session with 4 experts (i.e., HCI researchers) to further derive specific design goals for building a system to demonstrate our idea.
Following the findings so far, we developed \system{}, a proof-of-concept system that takes real-time speech as input to generate interactive impact captions with customized visual appearances in a multi-user VR environment.
Using \system{}, we conducted an in-lab study with 14 participants to assess impact captions as a medium for supporting interpersonal communication in social VR.} 

\altcolor{The user study results highlighted the integration of verbal and non-verbal information offered by impact captions, indicating the design can make social VR communication experiences not only clear but also engaging, while the risks of miscommunication caused by ambiguity inherent from the complex visual design were noticed.
Moreover, we found that impact captions can also facilitate interactions between users beyond enhancing speech conversations in social VR.
Furthermore, we illustrated three application scenarios to demonstrate the generalizability of our impact-caption-inspired approach. 
Finally, this research ends up with discussions on the findings from the user study, in which we provided insights and implications for future work and explained the limitations and possible improvements on the scalability of the current \system{} system.}

\altcolor{In summary, this research contributes:
(i) A comprehensive design space of impact captions in terms of visual and interaction design for supporting interpersonal communication;
(ii) A proof-of-concept system, \system{}, that enables users to conduct real-time conversations in social VR with the help of interactive impact captions;
(iii) An evaluation study of \system{} that demonstrates the effectiveness of our impact-caption-inspired approach for communication, highlighting the value of employing creative mediums for supporting interpersonal communication in social VR.}




\section{Related Work}
Starting from the background of interpersonal communication in social virtual reality (VR), we highlight the needs for better ways to support communication with both verbal and non-verbal information, and then introduce previous research systems to augment computer-mediated communication (CMC), which inspired our work. Finally, we introduce impact captions and captioning systems for communication purposes in HCI, showing our motivation for this research.

\subsection{Interpersonal Communication in Social Virtual Reality}
Social Virtual Reality (VR) refers to immersive virtual spaces where multiple users can synchronously interact with each other through VR head-mounted displays (HMDs) ~\cite{mcveigh2019shaping, freeman2021body}. It provides high-fidelity physical presence to facilitate various forms of interpersonal communication with both verbal and non-verbal cues over other computer-mediated communication channels ~\cite{yassien2020design, mcveigh2021case}. 
With an immersive and synchronous manner, social VR can effectively support collaborative tasks and promote social interactions in a wide range of application scenarios, including collaborative prototyping ~\cite{mei2021cakevr}, healthcare and treatment ~\cite{li2020designing, udapola2022social}, immersive learning ~\cite{thanyadit2022xr, peng2021exploring, jensen2018review}, intimate relationship building ~\cite{wang2023designing, freeman2021hugging}, and inter-generational communication ~\cite{shen2024legacysphere, wei2023bridging, baker2019exploring}.

Although social VR has been increasingly applied in various domains, it remains far from being a socially and emotionally fulfilling digital space for users' socialization needs ~\cite{wei2022communication, tanenbaum2020make, maloney2020talking}. Previous research identified several limitations regarding interpersonal communication in social VR.
A known issue is that current social VR applications do not allow users to easily express emotions (i.e., moods and excitements) and intentions through non-verbal cues that are common in real-world conversations ~\cite{maloney2020talking, sykownik2023vr, wu2023interactions}, because virtual avatars reduce or even filter out several important non-verbal signals such as facial expressions and body language ~\cite{baker2021avatar, freeman2021body, zhang2022s, lee2022understanding, fu2023mirror}. As an extreme case, imperfect avatars with realism will lead to the uncanny valley effect, bringing discomfortable social experiences to users ~\cite{latoschik2017effect, kyrlitsias2022social}. 
Besides the lack of expressiveness, the voice-based communication, as a common way to communicate in social VR, encounters the difficulty of coordinating speakers in multi-user virtual spaces  ~\cite{yan2023conespeech}.

Recent research proposed several computational methods to improve interpersonal communication experiences in social VR, such as generating realistic avatars that could capture users' facial expressions and body movements using deep learning models ~\cite{van2022deep}, adding haptic technologies to facilitate co-presence to allow affective communication ~\cite{fermoselle2020let, ahmed2016reach}, and simulating spatial sound effects to enhance the convenience and flexibility of voice-based communication ~\cite{yan2023conespeech}. 
In summary, all of these efforts aim to make the interpersonal communication in social VR close to the situations of the realistic world. 

\altcolor{However, people appreciate social VR not because it can simulate the real world. Instead, a significant reason is that 
the unique affordances of social VR can offer users immersive and unrealistic social experiences ~\cite{freeman2021body, maloney2020talking}. In other words, social VR can augment social signaling and unlock new social interactions that are unattainable in the realistic world ~\cite{mcveigh2022beyond, mcveigh2021case}.
At present, the potential of virtual reality to create unrealistic experiences for enhancing interpersonal communication for socialization needs is still underexplored. This leaves us opportunities to dive deep into the potentials of virtual reality and build novel tools with the idea of ``superpowers'' ~\cite{mcveigh2022beyond, mcveigh2021case} to support interpersonal communication in social VR.}



\subsection{\altcolor{Combining Verbal and Non-verbal Information to Augment Communication}}
Both verbal and non-verbal information is crucial in interpersonal communication in social VR ~\cite{palmer1995interpersonal}.
Besides verbal cues that mainly convey semantic meanings, non-verbal cues convey emotions ~\cite{liebman2016s, luo2024emotion}, enhance understanding ~\cite{aburumman2022nonverbal}, regulate interactions ~\cite{maloney2020talking}, reflect cultural norms ~\cite{freeman2021hugging}, and overall enrich the communication experiences no matter in offline face-to-face scenarios or online digital spaces.

\altcolor{Recent research in the HCI and CSCW fields has explored ways to involve non-verbal signals into verbal-centric communications (e.g., live presentations) to facilitate interpersonal communication and build social connections through computational approaches ~\cite{liao2022realitytalk, liu2023visual, cao2024elastica, an2024emowear, chen2021bubble, aoki2022emoballoon, choi2019emotype}.}
A thread of research aims to provide visual aids for computer-mediated spoken presentations. ``RealityTalk'' ~\cite{liao2022realitytalk} explores a speech-driven approach that allows users to give a speech with predefined visual elements while mentioning particular key words. ``Visual Captions'' ~\cite{liu2023visual} and ``CrossTalk'' ~\cite{xia2023crosstalk} further automate the process by introducing machine-learning-based models to predict users' intentions and automatically provide potential visual elements while processing speech input. 
``Elastica'' ~\cite{cao2024elastica} further enables adaptive animations on the associated visuals to effectively enhance the expressiveness of presentations.

Beyond visual aids, affective enhancement is another important dimension explored by previous research. For voice messaging interfaces, ``Emowear'' ~\cite{an2024emowear} invents a concept named ``Emotional Teasers'', which refers to a collection of animated emoticons, to explicitly show the emotional tone of an upcoming voice message for smartwatch users. 
For text-centered communication mediums such as text messaging interfaces, previous research has explored how the color and shape of speech bubbles ~\cite{chen2021bubble, aoki2022emoballoon}, and the typeface of textual content ~\cite{choi2019emotype, de2023visualization} can be purposefully designed and computationally generated to convey the speaker's moods in conversations. 

\altcolor{However, still less research explores how communication mediums in social VR can borrow the ideas from other CMC forms to enhance the interpersonal communication experiences, especially with the considerations of integrating both verbal and non-verbal information.
So far, previous research revealed that non-verbal cues play an important and unique role in interpersonal communication in social VR. }
In unrealistic virtual spaces, users tend to engage in social interactions with more active and bold non-verbal behaviors ~\cite{maloney2020falling, chen2024drink}. 
This phenomenon reflects a ``proteus effect'' in which users intentionally manipulate virtual avatars to express themselves beyond pure verbal-language-based communication ~\cite{maloney2020talking}. 
\altcolor{With more application scenarios and novel user activities emerging in social VR, strong demands for tools for facilitating interpersonal communication and interactions arise ~\cite{tanenbaum2020make}. }


\subsection{\altcolor{Impact Captions: A Typographic-Centered Visual Design Beyond Pure Verbal Communication}}
Impact captions refer to a type of typographic-centered visual effect that is prevalent in TV shows and online videos to engage viewers ~\cite{sasamoto2014impact}. Traditionally, impact captions are used as a supplementary channel that provides information outside of the screen (e.g., a commentary message from the TV show's editor ~\cite{o2010japanese}) or visualizes the implicit non-verbal information in frames (e.g., the characters' moods ~\cite{o2010japanese} or environmental sound effects ~\cite{wang2016visualizing}). Unlike visual cues in previous related works for improving interpersonal communication ~\cite{liao2022realitytalk, liu2023visual, cao2024elastica, an2024emowear, chen2021bubble, aoki2022emoballoon, choi2019emotype}, impact captions always use text as the main visual component that conveys verbal information and apply artistic modifications with additional visual decorations to textual elements to convey non-verbal cues simultaneously. The integration of both verbal and non-verbal information in the visual design of impact captions makes the captions a powerful medium that can provide rich semantics, afford viewers' affective reactions, and provoke thoughts ~\cite{o2010japanese, sasamoto2021hookability, chow2023impact}.

\altcolor{HCI researchers have noticed the potential of captioning mechanisms in supporting communication and have built captioning systems to help people with deaf and hard-of-hearing (DHH) situations perceive in-depth information in videos ~\cite{kim2023visible, de2023visualization, bragg2017designing, wang2016visualizing, seto2010subtitle}. In these cases, modifications on textual elements (e.g., irregular typefaces, coloring, and introducing dynamic visual effects) of subtitles have been proved to be effective in conveying semantic information of both verbal words and implicit non-verbal content of videos ~\cite{kim2023visible, de2023visualization, seto2010subtitle}. Yet, their explorations have not reached the scope of immersive media, such as VR.
When considering textual captions in VR, another line of research indicates that the reading experience of texts in VR varies depending on the displaying conditions ~\cite{rzayev2021reading}, and prolonged reading would reduce the comfort of using VR ~\cite{ubur2024easycaption}. 
Yet, it is also unknown whether typographic-centered impact captions can effectively mediate communication in social VR.}

\altcolor{To fill the research gap on tools for interpersonal communication in social VR and to respond to the needs of social VR users, this work explores how impact captions, as a type of elegant typographic design, can be applied to mediate and facilitate interpersonal communication in social VR with the inspirations from previous research on augmenting computer-mediated communication using visual cues and captioning mechanisms.
By investigating a design space for using impact captions as a communication tool and conducting a study with the proof-of-concept system, we demonstrate a concrete example envisioning the potentials of social VR in interpersonal communication and socialization.}



\section{Design Space of Impact Captions}
\label{section_design_space}
As a form of typographic design as well as visual rhetoric, impact captions can convey both verbal and non-verbal information on multiple dimensions. 
To introduce impact captions into social VR as a communication medium, we investigated the design space of impact captions with a concentration on the visual components and interactions. 

\subsection{Methods: Analyzing Videos with Impact Captions}
To obtain a comprehensive understanding of impact captions currently used in videos (e.g., TV shows and online videos), we conducted a content analysis on a collection of videos of TV show recordings retrieved from popular online video-sharing platforms.

\subsubsection{Data Collection}
We collected relevant videos using a two-stage approach.
In the first stage, we followed the top TV series rankings (i.e., IMDb\footnote{https://www.imdb.com/?ref\_=nv\_home}, TV Time\footnote{https://www.tvtime.com/}, and Douban\footnote{https://m.douban.com/tv/tvshow}) to collect and review videos of three representative variety show series: ``Arashi'' (from Japan), ``Running Man'' (from South Korea), and ``Who is the Murderer'' (from China). These videos helped us get an initial sense and identify preliminary dimensions of the design space.
In the second stage, we iteratively enhanced the design space by analyzing videos from an enlarged sample video set collected from online video-sharing platforms including YouTube and Bilibili. Specifically, we used the keywords ``variety show with captions'' and ``popular variety shows'' to search on these platforms with a newly registered account on YouTube and Bilibili, respectively. For each keyword, we focused on the top 30 videos of the raw results and filtered out irrelevant or redundant videos. As a result, a total of 46 sample videos were included in the dataset.


\subsubsection{Content Analysis}
Three authors collaboratively conducted content analysis on the video collection to build the design space. 
First, three authors randomly selected a subset of videos (N=10) to individually code the visual elements and behaviors of the impact captions in the videos, with the aim of determining initial dimensions.
Next, three authors sat together to reflect and discuss their initial insights and ultimately generated a consensus codebook. Finally, three authors reviewed and analyzed the complete data set.
Consequently, we categorized the impact caption design space into visual space and interaction space, in which visual space focuses on textual and non-textual elements (\autoref{fig:ds_visual_elements}), and interaction space (\autoref{fig:ds_interactions}) relates to motions, physicalization, and embodied interactive operations.

\subsection{Visual Design Space of Textual Elements}
\emph{Textual elements} are normally the primary visual component in an impact caption, including words, phrases, sentences, and symbols with linguistic meaning. 
In TV shows and videos, the content of textual elements comes from different sources such as narrative voiceover, conversations, onomatopoetic words, and commentary with ironic or entertaining implications added during the post-editing process of TV show production ~\cite{sasamoto2014impact}.

\emph{Typeface (font type)}, \emph{size}, and \emph{color} are three primary visual dimensions for designing textual elements. Depending on the contexts and purposes, all of these dimensions can be used to encode the importance of information (e.g., highlighting keywords) or represent a character's emotion (e.g., explicitly showing moods).

\begin{figure}[htb]
    \includegraphics[width=10cm]{img/design_space_visual_revision.png}
    \caption{Visual Design Space of Impact Captions. Impact captions consist of textual and non-textual visual elements. 
    Textual elements include the typeface, color, and size of texts.
    Non-textual elements include emoji, ornament, and speech bubble.}
    \Description{.}
    \label{fig:ds_visual_elements}
\end{figure}

\subsubsection{Typeface}
Typeface denotes the fundamental visual style of text and affects the reader's perceptions and emotional responses ~\cite{bianchi2021emotional, amare2012seeing}. Impact captions in videos widely utilize typeface to make impressions for audiences ~\cite{sasamoto2021hookability}. However, there is no strict standard that defines which typeface should be used under which circumstance. In our design space, we summarized the empirical rules based on content analysis on TV show videos.

A key perspective of the typeface used in the impact caption is the regularity of word characters' strokes. Artistic typefaces with irregular strokes are usually used to exaggerate characters' moods or to emphasize surprising situations. The rich shapes of the strokes allow the typeface to match changing emotions and vivacious performance.
Relatively, formal and regular typefaces (e.g., serif fonts) commonly appear in impact captions that are used as annotations with neutral information. This kind of caption provides additional knowledge to the original video content and should be optimized for viewers to read.
In addition, the visual style of the texts can also be designed to fit the overall visual theme of the video in TV shows. In other words, various deeply customized typefaces were created for impact captions in current TV shows beyond standard typefaces used in common computer interfaces such as desktop and mobile phones.

\subsubsection{Size}
The size of captions is another dimension commonly used to encode semantic meanings.
Size can be used for several purposes, depending on the creator's design intent and the overall atmosphere and theme of the context. 
Texts with changeable sizes attract attention more effectively, making the subtitles stand out on the screen and piquing viewers' interest. The use of larger fonts helps to emphasize key information, punchlines, or significant events. Additionally, larger fonts and flashy designs enhance the show's entertainment value, contributing to a lighthearted and enjoyable experience for the viewers. Text size can also help create a specific atmosphere, with larger fonts conveying liveliness and excitement, while smaller fonts may evoke warmth and intimacy. 
In terms of readability, larger text is easier to read on television screens, ensuring that the information is easily accessible to the audience. 
Additionally, highly abstract semantic meanings, such as strength, effectiveness, and severity, can be encoded by text size in impact captions.

\subsubsection{Color}
\label{sec_space_color}
In TV shows and videos, the color design of impact captions aims to create the show's atmosphere, emphasize key information, and attract the audience's attention. Thus, the color of impact captions usually features vibrant and eye-catching styles. For example, bright colors are usually used to add liveliness and appeal to entertainment-oriented shows, making captions more noticeable on the screen.

\altcolor{Besides, the color of impact captions is also used to imply the moods of characters, which reflects the color-emotion associations in human perceptions and self-expressions ~\cite{plutchik2013theories, hanada2018correspondence}.
Empirical evidence shows that bright and highly-saturated colors tend to associate with positive feelings, while dark and low-saturated colors may associate with negative feelings ~\cite{wilms2018color, hanada2018correspondence, plutchik2013theories, chen2021bubble}.
However, there are no standard rules to tell which mood should be represented by which color.
Because the color-emotion associations are not strict one-to-one mapping rules. A single color may reflect more than one type of emotion and vice versa ~\cite{hanada2018correspondence, wilms2018color}. Moreover, the perceptions of emotion conveyed by the same color also vary among different groups of people ~\cite{wilms2018color, hanada2018correspondence, chen2021bubble}.}

\altcolor{Nevertheless, the color-emotion associations still highlight the potential of using color to convey emotional information for interpersonal communication bi-directionally. Color can help users receive emotional information from others and express emotional feelings of themselves.}


\subsection{Visual Design Space for Non-verbal Elements}
Beside textual elements, several \emph{non-verbal elements}, such as typographic ornaments, speech balloons, emojis (emoticons), are involved in impact caption.
Those elements provide visual cues to convey invisible information, enhance expressiveness, and leverage engaging communication experiences.

\subsubsection{Typographic ornaments}
Typographic ornaments refer to decorative elements or symbols used to embellish and enhance the appearance of impact captions. They are typically small, non-alphabetic characters that add artistic flair and visual interest to the text.
Common examples of ornaments include swashes, which are elegant flourishes on letters often seen in decorative scripts; fleurons, floral or leaf-like designs used as separators or accents; bullets, various shapes used for lists and highlighting points; dingbats, pictorial symbols representing a wide range of subjects; and borders and rule lines for separating text.
In current impact captions, the content of ornaments is flexible and can be deeply customized to fit text contexts.

\subsubsection{Speech bubbles}
A speech bubble (a.k.a. speech balloon, ornamental frame) is the decorative visual element around text as a container of impact caption. This concept comes from comics and graphic novels and is used to represent the speech or thoughts of characters ~\cite{aoki2022emoballoon}. It is a rounded or elliptical shape with a tail or pointer that connects to the character's mouth, indicating the source of the speech. Inside the bubble, the character's dialogue or thoughts are written in text to convey their words or inner monologue.
In addition to speech, a speech bubble can also represent a character's inner thoughts, visualize loud or intense sounds, and provide narrative captions (additional narration or context to the story). Speech balloons play a crucial role in visual storytelling, as they allow audiences to understand and follow the characters' conversations and emotions. Different styles of speech balloons can be used to depict various emotions or tones of speech.

\subsubsection{Emojis and Emoticons}
Emojis are actual graphical images that depict a wide range of subjects, including facial expressions, animals, food, weather, etc.
Similarly, an emoticon is a combination of characters, usually made from keyboard symbols, that represents a facial expression or an emotion ~\cite{lo2008nonverbal}.
Emoticons are commonly used in text-based communication, such as texting, messaging, social media posts, and online forums, to convey feelings or reactions that may not be easily expressed through text alone.
Emojis and emoticons can be integrated into impact captions to enhance the emotional tone, clarity, and expressiveness.

\subsection{Interactivity Design Space}
When bringing impact captions from video-based media to VR, a significant change is that impact captions can be ``alive'' beyond unchangeable visual appearances and motions.
In virtual reality, impact captions can be made interactive to respond to user input, including voice and embodied actions such as gestures and body movements.
By playing with these virtual textual-visual elements, users can make fun, express themselves, and interact with others.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/design_space_interaction_revision.png}
    \caption{
        \altcolor{Interactivity Design Space of Impact Captions.
        The design space include three dimensions: \textbf{Physicalization}, \textbf{Motion}, and \textbf{Interaction}.
        With \textbf{Physicalization}, impact captions can appear like physical objects to be affected by the gravity with mass, have velocity for movement, and take spaces with volume.
        With \textbf{Motion}, impact captions appear to be ``alive'' and be responsive to user actions.
        \textbf{Interaction} describes how users can play with impact captions using embodied interaction.
    }}
    \Description{.}
    \label{fig:ds_interactions}
\end{figure*}

\subsubsection{Physicalization}
Using the physical engine provided by current VR technology, impact captions can be endowed with various physical effects. These physical effects allow impact captions to behave like objects in the real world, helping users establish perception beyond language.
For impact captions with specific meanings, physical sensations can be employed to enhance the semantics of the text. Our design space includes two of the most common physical quantities: mass and velocity \autoref{fig:ds_interactions}.
Mass directly relates to people's perception of ``light'' and ``heavy''. When impact captions are assigned mass, gravity effects can be applied to them so that they would naturally convey the degree of abstraction in terms of weight.
For example, assigning gravitational acceleration to an impact caption with the content ``heavy-hearted'' results in a dropping motion, physically conveying the feeling of ``heaviness''.
The concept of velocity is related to time. When adjusting the motion speed of impact captions, it can enhance the sense of urgency in expressing the textual content.

\subsubsection{\altcolor{Motion}}
Motions make impact captions alive and attractive by endowing changing appearances. Motions can be independent (i.e., only affecting an impact caption itself) or dependent (i.e., relying on other objects to take effect). 
In our design space, we include three commonly observed independent animated motions: shivering, blinking, and explosion \autoref{fig:ds_interactions}.
When shivering, primary visual components (i.e., textual and non-textual) of an impact caption will swing periodically. When blinking, the color of visual components (mainly text) could change rapidly and keep glowing.
Explosion is an effect where an original caption splits into several pieces of fragments or replicas spread around the whole space. 
As for dependent motions, character (or avatar in VR) is the most common object with which impact captions can interact. Surrounding is a useful motion where impact captions move following an orbit. Surrounding impact captions usually represent the unique personalities or properties of avatars.

\subsubsection{Interaction}
Embodied interaction emphasizes the integration of the human body and physical actions, such as gestures ~\cite{luo2024emotion} and bodily movements ~\cite{mueller2018experiencing}, as integral components of interaction with digital technology ~\cite{smith2018communication, kirsh2013embodied}.
Specifically, to support interpersonal communication using impact captions in VR, we recognized four common gestures (i.e., drag, drop, stretch, and throw) and one interaction with the virtual body (avatar) involved: attaching a caption with the avatar's body.
These embodied interactions can be used for either caption appearance customization or triggering particular actions. In this way, users can intuitively manipulate captions by their own hands rather than relying on a floating menu and clicking virtual buttons.


\section{Co-designing an Impact-Caption-Inspired Communication Tool in Social VR}
To concretize the design space to a feasible system design for our impact-caption-inspired approach, we conducted a co-design session with four HCI researchers and the four authors of this paper. 
From the session, we obtained primary directions and design goals for building the proof-of-concept system.

\subsection{Procedure}
The co-design procedure contains two sub-sessions. The first sub-session focused on brainstorming and open-ended discussion, and the second sub-session aimed at paper prototyping.

\subsubsection{Participants}
Besides the four authors of this paper, we recruited four extra HCI researchers from our university to our co-design session.
During the process, the first author played the role of instructor to host the sessions and also participated in the activities to contribute ideas. 
The eight contributors are either developers or designers of digital products. They all have rich experiences in VR gaming and social VR (e.g., VRChat\footnote{https://hello.vrchat.com/}).


\subsubsection{Sub-Session 1: Brainstorming and Discussion}
In this sub-session, the main objective was to understand the scenarios, needs, and purposes of users' interpersonal communication behaviors in social VR settings. This session was followed by initial brainstorming and design discussions. 
Participants were first asked to share their experiences and pain points to communicate with other VR players and discuss possible ways to address the issues. 
The instructor then presented several examples of impact captions (sourced from example videos collected during the design space investigation stage) to illustrate the concept and potential use cases of these impact captions. 
Next, the participants were asked to discuss their views on applying impact captions to social VR scenarios and brainstorm any desired ways to apply impact caption's features to design communication tools in VR.
The results of this session were eventually summarized by the instructor.

\subsubsection{Sub-Session 2: Prototyping and Presentation}
After brainstorming, the main objective was to continue the co-design process, concretizing ideas in the creation of low-fidelity prototypes. 
At the beginning, the instructor led a review of the results of the previous session and then informed the participants that they had 30 minutes to create their own prototype using white paper by drawing or handcrafting. Participants were encouraged to discuss their ideas with others and to think aloud during the session. However, participants still needed to create their own prototypes individually. When the time was up, each participant presented his/her prototype to the group, explaining the design concept, and discussing the prototypes.

% \begin{figure}[htb]
%     \includegraphics[width=\linewidth]{img/co_design.png}
%     \caption{}
%     \label{fig:co_design}
% \end{figure}

We recorded the entire process of the co-design session with permissions from the contributors, transcribed it into text, and collected the prototypes they designed. 
Using the method proposed by Chen and Zhang ~\cite{chen2015remote}, three of our authors conducted open coding and categorization of the design prototypes of the participants. 
In particular, components involved in the design of each prototype (such as input format, output results), information flow, context, and other visual elements (such as present style, text content, and colors) were coded by referring to existing work regarding Visual Captions ~\cite{liu2023visual} and Augmented Presentation ~\cite{liao2022realitytalk}.
Finally, we revealed qualitative findings regarding how impact captions can support interpersonal communication and derived design goals for implementing a prototype system.

\subsection{Findings}
From the co-design activities, we identified three common patterns of using impact captions to facilitate interpersonal communication.

\subsubsection{Enhancing Emotional Expressing}
Emotions are abstract and inconspicuous. Impact captions could be an effective way to visualize and concretize human emotions as visible and touchable shapes in VR.
This allows information senders in communication to share their emotional feelings in perceptible forms, so that the receivers could also easily understand the sender's moods and intentions in the VR space.

The textual elements in the design space can encode various perspectives of emotional expressions.
Text color is often used to represent positive or negative emotions in avatar dialogues. Generally, cool colors are used to convey more negative emotions, whereas warm colors represent more positive and enthusiastic emotions. For example, the word ``indifference'' could be rendered in deep blue, while ``happy'' is often depicted in bright orange or yellow colors.
In addition to color, the typeface of text is also associated with emotions. More regular fonts are typically linked to serious and earnest emotions, whereas more artistic and irregular fonts convey a sense of openness and freedom.
As for non-textual visual elements, emojis and emoticons are already commonly used elements for conveying people's emotions in social media and instant messaging. Previous research has also found that different shapes of bubbles can carry different emotional information to express calm or intense expressions.

\subsubsection{Highlighting Key Information in Speech}
In conversations, impact captions can be used to highlight key information and, with their positioning or bubble identifiers, attribute them to the speakers.
Typeface, color, and text size are useful dimensions for highlighting key information in a lency speech. Texts in distinctive colors or larger sizes are more likely to be read first by the recipient. Additionally, simpler and more regular fonts are more readable compared to overly complex artistic typefaces. In practical scenarios, multiple visual dimensions can be used individually or simultaneously to emphasize key information while aligning with the artistic style of the context.

\subsubsection{Identifying Speakers}
Although 3D audio and spatial sound effects allow users in social VR to trace the source of the sound and identify who is speaking, not everyone can accurately discern direction through simulated sound. Moreover, when multiple people speak simultaneously, the situation becomes more challenging. Impact captions, as visible virtual objects, are generated near the speaking avatars with a bubble tail pointing to the corresponding character in social VR. This allows participants to easily identify the current speaker through visual perception. Furthermore, when impact captions generated during conversations are allowed to persist in the environment for a prolonged period, newcomers can also see previous conversation content and gauge the level of activity of other participants based on the number of impact captions.

\subsection{Design Goals}
\label{section_design_goals}
Based on the results of the co-design, we propose the following design goals to implement a proof-of-concept tool to support interpersonal communication in social VR through impact captions.

\subsubsection{G1: Generating impact caption in time}
It was agreed by the participants that the impact captions should be generated when users were speaking, as voice-based ``face-to-face'' conversation is the most common way to interpersonal communication in social VR. Ideally, impact captions should appear simultaneously with the speech.

\subsubsection{G2: Maintaining readability and clarity}
Once impact captions were generated, a key aspect in making them useful for communication is to maintain readability and clarity so that other users in the VR space can clearly perceive the information carried out by impact captions. This requires each impact caption to appear in a proper location with a correct facing direction. In addition, multiple impact captions should not overlap each other.

\subsubsection{G3: Enabling interpersonal interactions with playful impact captions}
Interactive impact captions could enhance or introduce new forms of interaction between users, making these captions a type of new medium that facilitates connections among multiple users in social VR. 
Taking advantage of the interactivity capabilities provided by VR techniques, users should be able to intuitively handle and play with the impact captions generated by themselves. 


\section{SpeechCap: System Design and Implementation}
We implemented \system{} as a proof-of-concept system to demonstrate and evaluate our impact-caption-inspired approach to support interpersonal communication in social VR. According to the design goals (\autoref{section_design_goals}), \system{} takes the user's real-time speech to drive the generation of impact captions in VR (G1), uses rule-based methods to filter out non-important words to be rendered as impact captions in VR (G2), and provides multiple impact-caption-mediated interactions for communication (G2, G3).

% Additionally, voice input prevents users' hands from being occupied with complicated text input operations, leaveing the spare capacities for interactions with impact captions.

\subsection{Technical Pipeline: Algorithms, Software, and Hardware Implementations}
\label{sec_pipeline}
\altcolor{The core technical pipeline of \system{} (\autoref{fig:pipeline}) consists of three main software modules:
(1) a \textit{Voice Interface} that transcribes speech to text in real time and stores the transcribed text for subsequent processing; 
(2) a \textit{Text Processor} that decides which words should be made as impact captions and the appearance of each impact caption to be rendered in VR based on the analysis of user speech.
(3) a \textit{VR Application} that provides a multi-user VR environment for users to realistically enjoy communication using interactive impact captions.}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{img/pipeline_revision.png}
    \caption{
        Technical Pipeline of \system{}. 
        The system consists of Voice Interface, Text Processor, and VR Application. 
        The Voice Interface processes users' real-time voice input and stores the transcribed texts.
        The VR Application keeps polling the Text Processor for fetching upcoming impact captions, driving the Text Processor to convert transcribed texts into impact captions with customized visual design.
    }
    \Description{.}
    \label{fig:pipeline}
\end{figure*}

\subsubsection{\altcolor{Voice Interface}}
\label{sec_voice_interface}
\altcolor{We implemented a Python module with basic real-time speech-to-text capability using OpenAI's Whisper \footnote{https://github.com/openai/whisper} ~\cite{radford2023robust} as the core model for speech recognition.
As a state-of-the-art AI model designed for speech recognition tasks, Whisper's accuracy is close to that of professional human transcribers ~\cite{radford2023robust}, and its speed is above 10 words per second when using the minimized pruned model (i.e., ``tiny.en'') for transcribing English on an ordinary hardware platform ~\cite{haz2023study}. }

\altcolor{Since Whisper was not originally designed for real-time transcribing, we employed a self-developed multi-thread algorithm to approximately process voice input in a real-time manner. 
When starting, the algorithm recurrently chunks the input voice stream into fragments with a fixed duration (e.g. 1 second by default). Once a fragment is created, the algorithm would allocate a thread to call Whisper for transcribing, and then save the text result with a sequence number. 
When running the algorithm, the sequence number starts from 0 and increases by 1 each time for a newly processed fragment. The sequence number mechanism ensures that the downstream Text Processor and VR Application can always keep alignment with the original speech when they process texts and render impact captions in VR. }

\altcolor{In terms of accuracy, our method could be slightly lower than the core model because we rely on the multi-threading strategy to transcribe the chunks of speech input one-by-one. Thus, a single word could possibly be separated into two successive chunks and then fail to be recognized, even if the chance is small.
For speeding up, Whisper provides a sort of pruned model with smaller sizes to reduce computational consumptions. 
And in our algorithm, the maximum number of simultaneous working threads is configurable. More available threads can accelerate transcribing while requiring more computational power.
Ideally, the delay between a word being recorded and transcribed can be calculated by summing up the chunk duration (e.g. 1 second), Whisper processing time, and the time of waiting for an available thread. 
Particularly, in our user study, we adopted the ``tiny.en'' model of Whisper (with the minimum number of parameters and specified for English) and a maximum of 4 threads to maintain an acceptable transcribing speed for running the Voice Interface on Apple MacBook Pro with the M1 Pro chipset.}

\altcolor{Our real-time speech recognition algorithm is scalable in terms of both accuracy and efficiency. 
By employing Whisper models with different sizes or other SOTA speech recognition models, both dimensions could possibly be improved, although the trade-off between accuracy and efficiency always holds. 
Except for the core model, by configuring different chunk durations and the number of available threads (i.e. thread pool size), the efficiency is optimizable.
In addition, advanced hardware resources other than laptops can bring significant improvements to the performance of the Voice Interface module. }


\subsubsection{\altcolor{Text Processor}}
\label{sec_text_processor}
To decide which words should be made as impact captions and the specific visual design of each individual caption, we developed the Text Processor module to analyze transcribed speech using NLTK \footnote{https://www.nltk.org/} and Pydub \footnote{https://pydub.com/}.
Text Processor and upper-stream Voice Interface are connected by a shared database (SQLite) in which transcribed texts are stored by Voice Interface. And to respond to the needs of the downstream VR Application, we encapsulate the Text Processor in a back-end web server using the Flask framework \footnote{https://flask.palletsprojects.com/}. The server is responsible for providing processed data (i.e., the content and visual design of impact captions) for the VR Application through HTTP APIs.

\altcolor{For keyword extraction in Text Processor, we followed a strategy using the part-of-speech (POS) tag to filter out function words (e.g., \textit{conjunction}, \textit{particle}, and \textit{adposition}) as these categories of words mainly convey grammar functionalities with less semantics, and to keep words from the tags \textit{noun}, \textit{verb}, \textit{adjective}, and \textit{adverb}. In addition, we also keep the words of \textit{interjections} as they indicate emotional expressions in conversations, and emotional expression is one of the primary usages of impact captions based on our previous findings.}

\altcolor{For the visual design of impact captions in VR, Text Processor applies a rule-based approach to generate a customized visual appearance regarding the design space (\autoref{section_design_space}) for each individual impact caption. The rule-based approach utilizes semantic and acoustic features extracted from the user's speech to determine the appearances on each dimension of the design space for an impact caption instance. Details about the rules are discussed in the next section \autoref{section_mapping}.}

\subsubsection{\altcolor{VR Application}}
\label{sec_vr_application}
We implemented a VR application to render interactive impact captions and provide multi-user virtual reality environments to simulate social VR conversation scenarios. Technically, the VR application is built with Unreal Engine 5.1~\footnote{https://www.unrealengine.com/en-US/unreal-engine-5} and Meta XR Tools~\footnote{https://developers.meta.com/horizon/develop/}, as we adopted Meta Quest 2 as the target VR hardware device.
Besides the overall settings, we utilized the built-in 3D Text Actor of Unreal Engine~\cite{unrealtext2024} to implement the impact captions in VR.

\altcolor{To achieve a multi-user VR environment for at least two players (i.e., running VR simulation in the same virtual session on multiple HMDs), we developed our VR application based on the ``Listen Server'' mode offered by Unreal Engine ~\cite{unrealmultiplayer2024}. In this mode, the first player who starts the application will take the responsibility as a ``server'' to create and host a session over a local area network (LAN), while the ``server'' itself is also a ``client''. Once a session has started, other players in the same LAN can join the session and play as a pure ``client''.
Technically, whether or not to play the role of ``server'', each individual VR HMD in the multi-player session needs to simulate the entire virtual reality environment independently. 
To make the independently simulated virtual worlds have the same appearances for all the players, the ``clients'' have to share the same parameters for rendering objects and simulating interactions through a replication mechanism.
The replication mechanism continuously synchronizes the parameters from the ``server'' to the ``clients'' and reports changes made by the ``clients'' to the ``server''. In this way, multiple players can eventually experience the same VR environment using their own devices.}

\altcolor{Nevertheless, the replication mechanism does not force every ``client'' to strictly have the same appearance all the time, since each individual ``client'' always reserves full control of the objects in its own simulation.
This allows us to make local modifications to the objects before they are finally rendered on each individual ``client''.
In this way, we adjust the rotation parameters of the impact captions on each ``client'' HMD, making the impact captions always face towards the HMD's owner player in a readable angle (i.e., the replicants of the ``same'' impact caption are rendered to face different directions in different ``clients'').}


\subsubsection{Hardware Settings}
\label{sec_hardware}
Hardware settings include a laptop, a wireless microphone, and a VR head-mounted device with controllers for a single user. 
Specifically, we employed an Apple MacBook Pro (with an M1 Pro chipset) laptop for running the Voice Interface and Text Processor modules with the associated database and web server, and a Bluetooth wireless microphone connected to the laptop for collecting the user's voice input. Meta Quest 2 with controllers is used to run the \system{} VR Application. 

%TODO: a figure showing the hardware settings with software modules.

To create a stable network environment for serving the multi-player session among the devices, we configured a router to provide a local wireless network with fixed IP addresses for each quest device and the laptops. Additionally, during a session, we use a remote meeting software (e.g., ZOOM) on the laptops to transmit the voices of the users so that they can hear each other in the VR environment.


\subsection{\altcolor{Speech-driven Impact Caption Generation}}
\label{section_mapping}
In \system{}, we adopted a rule-based method to determine the specific visual appearance for impact captions on each dimension of the design space.
Our method takes verbal content, affective factors (e.g., valence) and prosodic factors (e.g., loudness) extracted from the speech to compute the specific visual design of each impact caption (\autoref{fig:real_mapping}), making impact captions appropriate for the corresponding conversations.
% Particularly, we employ fixed word lists for each of the dimenstions, see supplemantary

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{img/mapping_v3.png}
    \caption{
        Mappings Semantics to Impact Caption Design for Proof of the Concept.
        Valence links to text color where a warm color for positive moods and a cold color for negative moods.
        Loudness links to the size of captions. The larger the louder. 
        Formality links to typeface. ``Time New Roman'' is used for formal and ``Comic Sans'' is used for casual words.
        Emoji is applied for words regarding happy, embarrassed, and sad feelings.
        Speech Bubble and a ``shivering'' motion is applied for words with excitement.
        Ornaments is applied for words representing specific entities.
    }
    \Description{.}
    \label{fig:real_mapping}
\end{figure*}

\subsubsection{Text Color}
\altcolor{Based on our design space (\autoref{sec_space_color}) and the empirical knowledge of color-emotion associations ~\cite{wilms2018color, hanada2018correspondence, plutchik2013theories}, we selected a bright and warm color ({\textcolor[rgb]{1.0,0.82,0.26}{\faSquare}}, close to ``light orange'') for representing captions with positive emotional expressions, a dark and cold color ({\textcolor[rgb]{0.09,0.27,0.61}{\faSquare}}, close to ``dark blue'') for negative captions, and white for neutral captions. 
Technically, the valence of an impact caption is determined by NLTK's ``opinion\_lexicon'', which refers to lists of positive and negative words for sentiment analysis.}

\subsubsection{Caption Size}
The size of an impact caption is determined by the loudness (volume) of its belonging speech recording fragment. We estimate the loudness by decibels relative to full scale (dBFS) because dBFS measures volume on the same scale so that comparable results can be generated from different audio recording fragments. Using dBFS, the value 0.0 represents the maximum loudness, and negative numbers are used for lower loudness. For example, -20 in dBFS means that the volume is 20 dBs less than the maximum.
In \system{}, we implemented a rule that allows impact captions to be rendered in three sizes: ``small'', ``medium'', and ``large''. Small size is applied when the dBFS value is under -40. Medium size is applied when the dBFS value is between -40 and -20. Large size applies when the dBFS value is above -20. The louder the speech is, the larger the impact captions will be.

\subsubsection{Typeface}
\altcolor{Referring to the design space and previous studies on human perceptions of typeface ~\cite{bianchi2021emotional, amare2012seeing}, we use typeface to encode and represent the formality of speech. 
As pointed out by previous research, the formality of texts is not a dimension that can be directly calculated but inferred by the words ~\cite{heylighen1999formality}.
Thus, we decided to classify words in speech as either ``formal'' or ``casual'' in terms of their formality and apply a typeface for the two groups, respectively.}

\altcolor{To identify the formality of words, we prepared a pre-defined word list of formal words so that we can determine an incoming word by checking if it is in the list.
As for the typefaces, we referred to the literature to guide our design choices.
For formal words, we applied ``Times New Roman'' \footnote{https://en.wikipedia.org/wiki/Times\_New\_Roman}, which is a serif typeface originally designed for serious publications. It is also known to make people feel formal and serious ~\cite{mackiewicz2004people}.
For casual words, we adopted ``Comic Sans'' \footnote{https://en.wikipedia.org/wiki/Comic\_Sans}, as it is also studied by previous research as a casual, sans-serif typeface with a playful and informal look ~\cite{amare2012seeing}. Its visual characteristics feature rounded edges and irregular strokes.}

% we used rule-based methods using special word lists to determine whether a word should be considered formal or causal. 
% Formal words will be rendered in VR using a straight typeface, while causal words were associated with an artistic typeface\cite{de2024caption}.

\subsubsection{Emoji and Emoticon}
\altcolor{As a non-textual element that directly links to emotions, we selected three common moods to be explicitly displayed by emojis in our impact captions: a ``smiling face'', a ``sad face'', and an ``embarrassed face'', as faces with common expressions belong to a category of emojis that are known to be commonly used across different cultural backgrounds ~\cite{czkestochowska2022context}.}
Similarly to the rules for deciding typefaces, we employed three special word lists for the three moods, respectively. Once a word in the lists is mentioned, the corresponding emoji will be attached aside from the textual part of an impact caption. As the most straightforward example, the word ``happy'' in impact captions would be accompanied by the ``smiling face'' emoji, while ``crying'' will have a ``sad face'' emoji.

\subsubsection{Ornament}
According to the design space, typographic ornaments include various types of non-alphabetic characters or icons that are relevant to the words within the impact caption. In \system{}, we focus on one of the main purposes of using the ornament, which is to visually highlight objects or entities (e.g., ``cake'') that were mentioned in speech. 
Particularly, we specified a list of keywords of common concepts in daily life for ornaments. The graphic icons for these words are retrieved from Noun Project \footnote{https://thenounproject.com/}, a free open-source icon library. 
The assets of the icons were pre-installed in the VR Application module of \system{}, and the back-end Text Processor will make decisions for each word based on the word list. 


\subsubsection{Speech Bubble}
Speech bubble in \system{} is used for two types of words: greetings (e.g., ``hello'') and interjections (e.g., ``HHHHHH'' for laughing), since they indicate attention requirements or speech with excitement.
For impact captions with greeting words, the balloon is rounded-edged to make the captions different from others and attractive. For interjections, a spiky-edged balloon is used to simulate surprising sounds or speech.

\subsubsection{Motion Effects}
A ``shivering'' motion effect will be added to the words with excitement, such as ``shocking'' and ``surprised'', when spawned. 
Rather than adding more motions to the impact captions while they are generated, we leave the potential of triggering motion effects to the users, since VR naturally provides such interactivity over videos where impact captions originate from. In \system{}, the impact captions can always be alive and interactive.


\subsection{Communication with Interactive Impact Captions in VR}
\label{sec_interactivity_functions}
\system{} allows users to have conversations in a shared VR space with the support of interactive impact captions. 
In \system{}, impact captions can only be triggered by speech input. 
\altcolor{When a particular word is mentioned in the speech, \system{} will create a corresponding impact caption at a position roughly in front of the speaker’s avatar at the height of the chest so that users can easily grab the captions. To avoid overlapping, the position is slightly randomized each time for a newly incoming caption  (\autoref{sec_visual_clutter}).}

The visual appearance of each caption instance is determined by the semantics and affective factors of the corresponding speech fragment (\autoref{section_mapping}). 
\altcolor{During a conversation, both the speaker and the listener can see and play with the captions, making their communication process visible, interactive, and playful beyond the traditional voice-only experience.}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{img/system_revision2.png}
    \caption{
    \altcolor{
        Interactions with Impact Caption in \system{}. 
        (A) \textbf{Grab} to hold and place a caption to a new position;
        (B) \textbf{Stretch} with two hands to resize an caption;
        (C) \textbf{Attach} an caption to (another player's) head;
        (D) \textbf{Shoot} an caption to trigger the \textbf{Explosion} effect.
    }}
    \Description{.}
    \label{fig:system}
\end{figure*}


\subsubsection{\altcolor{Fundamental Interactions}}
\altcolor{\system{} enables users to play with impact captions in VR intuitively using a set of embodied actions (G3), which achieves fundamental interactions between user and the captions.
\textbf{Grab} (\autoref{fig:system} A.) is an action that allows users to hold a caption with their hands using controllers, change its orientation, and place it elsewhere just like taking a physical item in the real world. 
When grabbing an impact caption, users can further \textbf{Shake} the caption to make it start shivering and blinking.
When both hands grab the same impact caption simultaneously, the caption can be \textit{Stretched} (\autoref{fig:system} B.) for resizing. By resizing, users may create huge captions larger than their avatars, or tiny captions like a bullet. 
The scalability of size extends the usage of impact captions beyond speech-driven subtitles. Users may combine captions of different sizes with other actions to achieve interactions with others. For example, making a huge caption and shaking it to trigger the blinking effects to enhance the visual impact to emphasize the key points of conversation.}

\altcolor{Furthermore, when holding an impact caption, users can \textbf{Throw} it away by releasing the grab button while moving their arms, just like the natural ``throw'' action. 
As an enhanced version of ``throwing'', users can \textbf{Shoot} a caption, letting it emit towards a direction in a straight line. The shooting action is bonded with the front trigger button on the controllers. Users can hold the button for up to 3 seconds to charge the action. Longer charging time rewards a stronger initial force for emission, resulting in a high initial velocity of the ``bullet'' caption.}

\subsubsection{\altcolor{Decorate Avatars for Self Expression}}
\altcolor{Avatar plays an important role in social VR that supports self-presentation and reflects users' emotions and personalities ~\cite{sykownik2022something, freeman2021hugging}. 
In \system{}, users can \textbf{Attach} (\autoref{fig:system} C.) an impact caption to the top of their virtual head by tapping or lightly touching the head while holding the caption. Once attached, the caption could be fixed continuously like a ``tag'' or ``hat''. 
This feature is inspired by the typical usage of impact captions in TV shows where impact captions are used to tag the characters, revealing their emotional status. 
Besides the head, \system{} also allows users to attach an impact caption to the body part of avatars, in which the attached caption will start to periodically revolve around the avatar, achieving the surrounding motion effect in the design space.}

\subsubsection{\altcolor{Mediating Interpersonal Interactions}}
\altcolor{In \system{}, impact captions can react to the actions and avatars of multiple users so that they can be used as a medium to facilitate the interactions between the users.
When thrown or shot by users, impact captions start flying in the virtual space.
Once the flying impact caption crashes with another impact caption or avatar, an explosion effect will be triggered (\autoref{fig:system} D.), in which several duplications of the flying caption will be generated at the crashing point and emit outwards fast, like the explosion of fireworks.
Such a dynamic effect may be used as a non-verbal cue to attract attention and add more fun to the social VR experience.}

\subsubsection{\altcolor{Reducing Visual Clutter and Overlapping}}
\label{sec_visual_clutter}
\altcolor{To maintain the readability and clarity of floating impact captions in conversations (G2), \system{} introduces a ``time-to-live (TTL)'' mechanism to softly control the number of captions.
The ``TTL'' mechanism allows each caption to live for 5 seconds once generated.
If users do not interact with a newly generated impact caption within the time period, the caption will automatically disappear afterwards. Otherwise, if the user wants to keep the impact caption for any purpose, the simplest action to do so is to touch the caption or to grab it at least once before it disappears. 
Once the caption has interactions with a user, it will no longer be constrained by the living time limitation until users intentionally delete it by holding it and clicking a button on the VR controller (i.e., ``X'' button for the left-hand controller and ``A'' button for the right-hand controller).}

\altcolor{Additionally, to avoid multiple impact captions overlapping with each other when a user keeps speaking, \system{} randomizes the positions for spawning. Specifically, the exact spawn position of a caption is calculated by a fixed position plus random offsets on $x$, $y$, and $z$ dimensions. As a result, this strategy makes multiple impact captions look like a ``word cloud'' emerging in front of the speaker's avatar. }

%Although it is not perfect, this strategy effectively reduces the chance of overlapping.

% utilizes the technical features of VR multi-user application (\autoref{sec_vr_application}) to ensure that different users can always see the captions at an appropriate angle at which the captions face users' eye sights perpendicularly.


\section{User Study}
To understand the effectiveness of the design space and \system{} system in supporting interpersonal communication in social VR, we conducted a user study, inviting 14 participants of various backgrounds from local universities to experience simulated conversations in VR with interactive impact captions using \system{}. User feedback was evaluated from both quantitative and qualitative perspectives through a post-study survey and semi-structured interviews. 

% RQ1: What is the user experience of conversation with speech-driven impact captions in social VR scenarios using \system{}?
% RQ2: Does the (visual and interactive) design of impact captions afford meanings in conversations in social VR?

\subsection{Participants}
\altcolor{We recruited 14 participants (6 female and 8 male, aged between 18 and 35) from local universities by advertising on social media. There is no overlap between this group of participants and the four experts who contributed to the previous co-design activity. In terms of social VR experiences, five participants (annotated P1 to P5) were novice users who had never used VR HMDs before, while the other nine participants (P6 to P14) self-reported that they have rich experiences in VR games, social VR features, and VR application developments.}

\subsection{Procedure}
The user study was conducted individually for each participant. The study session for a participant was run in a one-on-one manner in which an experimenter (either the first or second author of this paper) serves the participant to go through the entire procedure. The study sessions lasted 75 to 90 minutes and were held in a lab space in our university. After the study, the participant received a gift card valued \$50 in local currency. For analysis, the study processes were fully audio-taped and the activities in VR were screen-recorded.
Regarding hardware settings, two sets of Meta Quest 2 devices and two laptop computers (paired with Bluetooth microphones) were used for the experimenter and the participant in a session. An additional router was configured to host a local area network to serve the multi-user session of the \system{}'s VR application (\autoref{sec_hardware}). 

% Figure showing how the is study is conducted?

The core design of this user study is to evaluate \system{} through two typical conversation scenarios in social VR: (1) a private conversation between two close friends and (2) a public discussion on a debatable topic with multiple participants. 
First, the dyad conversation scenario simulates a private conversation in which rich emotional expressions occur. In this scenario, the experimenter and the participant play as close friends to talk about their daily life and personal feelings with the help of impact captions. 
In the multi-person conversation scenario, the participant will join an ongoing discussion to listen to others and then provide his/her own opinion. 
In the two scenarios, participants will have opportunities to play the role of both speaker (i.e., information sender) and listener (i.e., information receiver) in turns, allowing them to play with various impact caption instances in very different visual and interactive appearances.
Specifically, a full user study session consists of four sections as follows: 

\subsubsection{Introduction and System Walk-through (\textasciitilde20 min)}
At the beginning, the experimenter briefly introduced the background and main purpose of \system{} (i.e., facilitating interpersonal communication in social VR). Next, the experimenter guided the participant to get familiar with basic usages of the hardware devices if they were not, and then walked through the \system{} system. During the walk-through, the experimenter provided verbal and hand-in-hand guidance, ensuring that the participant could see and generate impact captions with their own speech, and know how to interact with the impact captions in VR. After the walk-through, the experimenter would inform the participants about the next dyad and multi-person scenarios.

\subsubsection{Dyad Conversation Scenario (\textasciitilde15-20 min)}
Once the participant and the hardware settings were ready, the experimenter would take another set of devices, go to a separate room (avoiding conflict with the speech-to-text program running on the laptop), and initiate the conversation in VR by telling a pre-defined sad story about losing her cat recently. During the storytelling, the experimenter employed several impact captions to enhance the speech and seek emotional support from the participant. The participant was then asked to comfort the experimenter by telling some happy moments from recent daily life, using impact captions generated along with the conversation's development. 

\subsubsection{Public Discussion Scenario (\textasciitilde15-20 min)}
In this scenario, the participant was asked to join a discussion of the topic ``\textit{what privacy and security issues will arise around AI?}''
The discussion was started between the experimenter and a ``NPC'' character who was configured to speak based on a pre-defined script and timeline, due to the limitation of hardware resources. Once joining the session, the participant was asked to first listen to the ongoing conversation for a while and then present her/his own opinions with the help of impact captions.

\subsubsection{Survey and Semi-structured Interview (\textasciitilde25-30 min)}
After the two conversation scenarios, the participant was asked to complete a survey about their experience with \system{}. The participant was also encouraged to think aloud while completing the questionnaire. When the survey was done, the experimenter would conduct a semi-structured interview with the participant, diving into their experiences and thoughts in depth.


\subsection{Results and Findings of Post-study Survey}
The post-study survey questionnaires consist of two sections. The first section employs four questions that ask whether the impact caption design space is understandable, interesting, and meaningful.
The second section contains six questions about the user experience with \system{}, considering enjoyment, emotional expression, clarity of presentation, and overall experience of having conversations with impact captions in VR. The questions are designed in a 7-point Likert scale format ~\cite{joshi2015likert}, requiring the participants to respond with their level of agreement on the statements.

\begin{figure}[htb]
    \includegraphics[width=\linewidth]{img/survey_revision.png}
    \caption{
        Post-study Survey with Results. 
        The survey consists of two sections in which the first section contains four questions (Q1-Q4) about the design space of impact captions and the second section contains six questions (Q6-Q10) about the experience with SpeechCap. 
        Overall, participants believed the design space of impcat captions was meaningful and the experiences of using \system{} is enjoyable and helpful.
    }
    \Description{.}
    \label{fig:survey}
\end{figure}

In the first section (\autoref{fig:survey} upper), the participants agreed that the overall design of impact captions to be used for communication in VR was understandable (Q1, M=6.36, SD=0.74) and interesting (Q2, M=6.14, SD=0.95). 
For the visual appearances and design of individual impact captions, 12 participants agreed that the integration of non-textual elements (ornaments, emojis, speech bubbles) with texts was meaningful (Q3, M=5.71, SD=1.07). And 13 participants believed that the visual design (i.e., color, typeface, and size) of textual elements was reasonable (Q4, M=5.79, SD=0.89). No participants responded with disagreement on the design of impact captions.

In the second section (\autoref{fig:survey} lower), most of the participants (11/14) agreed that their experiences with \system{} were somewhat enjoyable (Q5, M=5.50, SD=1.22). 
From an information sender's perspective, participants diverged on the effectiveness of using impact captions for emotional expression in social VR (Q6, M=4.29, SD=1.20). Six participants agreed that impact captions can help express their emotions effectively, while the other five committed objections. Two participants responded with a neutral opinion. This may relate to the ambiguity caused by the mapping rules for non-verbal visual elements. 
For the next question about whether impact captions were helpful for presentation clarity (Q7, M=4.43, SD=0.76), six participants responded with agreement, while five participants stayed neutral and one responded ``somewhat disagreed''.
From a receiver's perspective, the majority of the participants (13/14) agreed that they could perceive the emotions of others by seeing and playing with the impact captions in VR (Q8, M=5.43, SD=0.65). Most of the participants (12/14) also agreed that impact captions could improve presentation clarity and help them understand other people in VR (Q9, M=5.36, SD=0.74). In addition, ten participants agreed that impact captions were useful in setting the tone of conversation (Q10, M=5.00, SD=0.96).


\subsection{Results and Findings of Semi-structured Interviews}
During the user study, all participants fully experienced the two scenarios and played the roles of both information sender and receiver in conversations. Creative usages of impact captions in VR emerged from their play-through. 
In the semi-structured interviews, we discussed questions regarding the impact captions' design space and user experiences of \system{} with the participants based on their individual performance.
Overall, the participants responded positively, indicating that the impact captions were ``\textit{interesting and enjoyable to play with.}'' (P1, P2, P4, P10, P13).
Two authors analyzed the transcribed scripts of the interviews by iterative open coding ~\cite{corbin2014basics} and finally revealed the following findings showing the meaningfulness of our impact captions' design space, and the usefulness of \system{} system.


\subsubsection{\altcolor{F1: Textual Elements Can Effectively Support Conversations in Social VR}}
\label{finding_text}
\altcolor{Participants generally believed that visible texts in impact captions enhance their conversational experience in VR (P2, P3, P4, P6, P7, P11, P12, P13, P14). 
On one hand, text elements help users accurately express complex or profound meanings. P12 emphasized the expressive capability of impact captions: ``\textit{The texts within impact captions have a high potential in expressiveness, because texts can convey complex, abstract concepts as well as accurately represent ironic or literary content, which non-textual elements cannot achieve.}''}

On the other hand, some participants (P2, P7, P8, P13, P14) believed impact captions help extract and emphasize key information from dialogues. 
In \system{}, impact captions automatically disappear if not touched by users in several seconds (\autoref{sec_visual_clutter}). Therefore, besides filtering trivial words in the back-end processing (\autoref{sec_text_processor}), \system{} also allows users to actively select impact captions with keywords they consider important to remain in VR. ``\textit{...if I was sharing some knowledge with others with \system{}, I could easily make the keywords of my speech persistent and let them floating around my avatar so that my audiences won't lose focus...}'', said P7. 
And when talking about the multi-person conversation scenario, P14 said ``\textit{I'd appreciate \system{} if it can always correctly recognize and show the meaningful words by impact captions. This prevents me from listening to lengthy speeches in meetings.}''

Additionally, one participant (P7), a non-native English speaker, felt that impact captions lower the barrier for her to engage in English conversations. P7 said, ``\textit{English is not my native language, and when speaking with native speakers, I often struggle to understand due to different accents and fast speed of speech... but impact captions allow me to clearly and accurately see what others are saying, and they can remain in the VR space as a conversation record for later review.}''



\subsubsection{\altcolor{F2: Non-textual Elements are Engaging but Possibly Ambiguous}}
\label{finding_ambiguity}
\altcolor{Overall, participants agreed that the non-textual visual elements in impact captions are engaging and making the speech-driven captions really ``impactful'' (P1, P2, P3, P4, P5, P6, P8, P10, P11, P12, P13)}. P1 said ``\textit{I love the idea you combine those cute emojis with the colorful texts. It makes those captions not only interesting but also meaningful.}'' 
Among the multiple non-textual dimensions in the design space, the colors, ornaments, emojis, and motion effects are perceived by participants as the most noticeable features (P1, P3, P4, P7, P10, P11, P12). 

\altcolor{Nevertheless, the non-textual elements in impact captions may not be able to convey meanings as clearly as texts can do.} First, the current colors of impact captions in \system{} were reported to be confusing and ambiguous by some participants (P1, P8, P9). P1 asked ``\textit{I noticed that those impact captions had different colors, but actually I didn't figure out why a word should be rendered in blue while others were not?}'', and P9 made another similar comment saying that ``\textit{I don't quite understand why you choose orange color for the word "happy" while other words are in white.}''. 
Similar issues arise for emojis. P4 commented, ``\textit{Expressing with visual elements alone can lead to misunderstandings; some people like to use a smiling face for implicit sarcasm or be \'ironic\'}.
These cases may relate to findings of previous research that different people may have different perceptions of the same visual elements when conveying non-verbal meanings ~\cite{hanada2018correspondence, wilms2018color, czkestochowska2022context, miller2017understanding}, revealing the existence of ambiguity in our design of impact captions.

\altcolor{However, on the other hand, three participants (P3, P10, P12) believed that the integration of textual and non-textual elements in impact captions can also mitigate the potential ambiguity.} P3 highlighted, ``\textit{I might misunderstand the meaning when only seeing the smiling face, but if there was a word "happy" nearby, it would be much clearer,}''. In other words, the meanings of different components of a single impact caption can mutually reinforce each other, reducing the chances of misinterpretation by the recipient. 
When discussing the color-mapping rules in \system{}, P10 commented, ``\textit{I think using different colors to show the emotions in a conversation is a good idea, and I don't worry about misunderstandings because when the overall tone is positive, I can see most words are in orange. The number of colored captions not only can avoid the ambiguity produced by a single word, but also can reflect how strong the emotional expression is.}''



\subsubsection{\altcolor{F3: Interactivity is a Key to Engaging Communication Experience in Social VR}}
\label{finding_interaction}
\altcolor{Most participants (P1, P2, P4, P5, P6, P7, P8, P10, P11, P13, P14) agreed that the interactivity of impact captions in \system{} could bring novel collaborative social experiences and help develop a sense of intimacy between the users in social VR.} ``\textit{Throwing a caption to people in social VR makes me think of the "squat and strait" movement before starting a game when playing "Fall Guys", in which everybody feels fun together.}'', said P6. She believed the interpersonal actions mediated by impact captions in \system{} were meaningful for building connections with others in social VR, even such interactive capabilities were somehow ``\textit{unnecessary}'' for a speech-driven captioning system. 

Besides the fundamental interactions, more creative usages of impact captions were explored by the participants.
Note-taking is one method commonly proposed. P7 recalled his experience of the multi-person discussion scenario in our study and said, ``\textit{During the group discussion, I would like to grab the keywords mentioned by other members, enlarge them, and place them in the virtual space, just like using a whiteboard,}''. P4 also talked about similar use cases, saying that ``\textit{I used to capture the key points mentioned by others and record them with notes in meetings. In \system{}, impact captions can be the notes, I can just take the captions aside ... this could also be used in multiplayer reasoning games, right? You can note what others have said (as a basis for reasoning),}''.
Note-taking allows impact captions to persistently present the conversation in the virtual space like dialogue records, achieving a means of asynchronous communication. ``\textit{Impact captions can be like email ... other users can read the meeting record whenever they join the session, as long as the impact captions remain there.} said P12.

Another creative use case was conducted by P10. When conducting the dyad conversation scenario in which there was a sad story about a missing cat, he purposefully generated several impact captions with negative words and used these captions as blocks to construct a small shelter in VR by resizing and placing. P10 explained, ``\textit{When I felt sad and uncomfortable, I always find somewhere to hide myself avoiding meeting with other people. So I take these many "sad" words to build a blue room for taking a break.}''. He also invited the researcher to join his room made by impact captions to relax in VR.


\subsubsection{\altcolor{F4: Needs for Identifying Keywords in Conversations}}
\label{finding_keywords}
\altcolor{Although the participants mostly provided us with positive feedback to \system{}, they also pointed out some drawbacks.
One of the most commonly asked questions by the participants when they started using \system{} was about how we decided the words to be rendered as impact captions, as they usually found there were too many impact captions emerging when speaking continuously.
After knowing our simple POS-tag-based filtering method to remove functional words, most participants suggested an improvement on it. 
P2 suggested, ``\textit{Don not turn every word into an impact caption. Please extract the key information (of the speech) and only show impact captions of the relevant keywords.}'' 
Other participants, like P11, thought too much text content increased the reading and comprehension costs and then reduced the overall experience of having a conversation with \system{}. P11 complained, ``\textit{No one wants to read that much text when using VR.}'' 
Particularly, in the multi-person conversation scenario, as more impact captions were spawned and accumulated in limited space, some participants (P1, P2) reported that they felt the field of vision was partially obstructed. }


% Talk about user's feed back on the possible visual clutter and the insufficient of keywords extraction.

% -> implication on keywords extraction: take user's preference as input to consider 
% e.g./ for English learners, the system can allows them to record which words are more important to be learnt. sth.//.





\section{Application Scenarios}
\label{sec_apps}
\altcolor{Besides the scenarios and potential creative use cases committed in the user study, \system{} further supports a variety of communicative needs under different contexts. We briefly introduce the following additional application scenarios to illustrate the generalizability of \system{}. }

\subsection{\altcolor{Making Social VR Accessible to Deaf and Hard-of-Hearing (DHH) People}}
\altcolor{Recent research revealed that current VR applications fail to provide sufficient accessibility support for deaf and hard-of-hearing (DHH) people to neither experience immersive digital content nor conduct remote communication and socialization ~\cite{jain2021towards, borna2024applications}. 
Captioning systems, known as useful tools for making content accessible in other digital media such as videos, are still under-explored in VR contexts ~\cite{kim2023visible, de2023visualization, de2024caption}.
\system{} not only have similar functionalities as previous systems that turn speech into visible captions for DHH people ~\cite{kim2023visible, de2023visualization, li2022soundvizvr}, but also extend the idea by incorporating multiple visual cues and adding interactivity to convey rich non-verbal information (i.e., valence, excitement, etc.) in virtual reality.}

\altcolor{For example, when participating in a conversation with someone who is talking about a vacation travel plan in social VR with \system{}, a DHH user can perceive the speaker's positive mood through the captions in a bright color with a laughing emoji. An additional shivering motion can notify the user that the speaker is also really excited about the trip. By dragging and placing impact captions of the names of the must-visit spots mentioned in speech, the speaker can demonstrate the plan with clarity. Further, when the speaker picks up a caption containing ``volcano'' and talks about her passion for hiking and exploration, the impact caption with a corresponding volcano icon can give clear reference to the ongoing speech.
Overall, informative captions, engaging visuals, and interactive playfulness provided by \system{} can enlarge the space for DHH individuals to communicate with other people and the virtual world in social VR.}


\subsection{\altcolor{Enhancing Interactions for Teaching and Learning in Social VR}}
\altcolor{VR has shown a great potential in education for the increased social presence it provides in remote learning, and the access to a wide range of learning contexts that may not be accessible in reality ~\cite{thanyadit2022xr, peng2021exploring, jensen2018review}. However, it is still far from being a commonly used tool due to the challenges of creating educational content that is easy for reviewing, providing inclusive functionalities, and supporting collaborative learning ~\cite{jin2022will}.   
In a virtual lecture, \system{} allows instructors to create instructional cues (e.g., notes) by placing impact captions of keywords in the space. The shared interactive captions can further act as a medium for the interactions between the instructor and the students, facilitating learning activities like question answering or hands-on demonstration.}

\altcolor{For example, when teaching the structures of plants in a botany class, the instructor would usually introduce the roots, stems, leaves, and flowers in sequence. With \system{}, once these words were mentioned, the relevant impact captions would be generated with texts accompanied by symbolic icons.
Then, the instructor can grab and stretch to enlarge the caption for emphasizing, and further captures the students' attention by shaking the caption to trigger the blinking effect.
If a student has questions about a specific concept mentioned before, he could also make an impact caption of words for that concept and shoot the words, creating an explosion of the words so that the instructor can easily know which part the student is confused about. 
In addition, further use cases can be explored to adapt to different contexts in teaching and learning. \system{} in the current stage provides a starting point to a way to achieve engaging and playful interpersonal interactions for social VR users in the future.}


\subsection{\altcolor{Facilitating Engaging Live Streaming Experiences in Social VR }}
\altcolor{Recent research suggests that VR streamers face challenges in building emotional connections since their facial expressions can be obscured by VR headsets, making it difficult for viewers to perceive the emotions of the streamer directly ~\cite{wu2023interactions}, while emotional connection is a key factor in live streaming ~\cite{lu2018you}.
\system{} can provide visualized emotions for streamers by detecting the expressed moods within the speech and adding appropriate emoticons, motions to relevant captions.
\system{} can also enhance the communication between the streamer and the viewers by rendering the viewers' reactions as interactive captions in the streamer's virtual space, mediating the interactions between the streamer and the viewers.
This could be a way to satisfy the demands of visible and spatialized objects in engaging live streaming experiences ~\cite{wu2023interactions, lu2018you}.}

\altcolor{For example, in a typical VR live streaming scenario, a streamer was self-evaluating his playing performance on Beat Saber, a popular VR rhythm game. He didn't do well in the game but would like to tell the audience that he had a lot of fun playing it and felt like a champion. This strong emotion can be communicated and augmented by \system{} with colored captions of words like ``Fun'' and ``Exciting'' accompanied by laughing emojis. 
He then made a ``Champion'' caption with a trophy icon and attached it to his avatar's head like a funny crown to show self-mockery.
By shooting the caption ``Exciting'' outwards, the explosion motion effect can be triggered once the caption crashes with an object in the VR scene, making the atmosphere more interesting and engaging. From the viewers' perspective, they can also send their feedback and comments as impact captions, using the visible and interactive captions to make humorous responses to the streamer's poor performance.}


% \subsection{Enhancing the Feeling of Intimacy between existing close-ties}
% Social VR offers a new way to mediate and support interpersonal relationships – it leverages both the anonymity and fexibility of virtual spaces and the physical and bodily experiences in offine face-to-face interactions, making the development of such relationships more 
% immersive and realistic.\cite{freeman2021hugging}. However, 
% \subsection{Relieving the Uneasiness of Interacting with Strangers in social VR}





\section{Discussion}
\altcolor{
The user study findings demonstrate the effectiveness of our impact caption design space and the usefulness of our \system{} system. 
Related to findings and the literature, we first discuss the potentials of using impact captions as a form of visual rhetoric with interactivity to enhance communication and interactions in social VR.
Next, we present an in-depth understanding of the ambiguity in \system{} and proposed future research directions regarding it for designing communication tools in social VR.
Then, we provide three design implications for future tools for interpersonal communication in social VR based on the study results.
Finally, we explain the scalability of \system{}, showing the limitations and corresponding future directions to improve our \system{}}.

% Study Findings:
% Textual Elements Can Effectively Support Conversations in Social VR.
% Non-textual Elements are Engaging but Possibly Ambiguous.
% Impact Captions is a Powerful Medium for Facilitating Interactions between Users

% Specifically, textual elements can clearly convey the information in speech-based conversations. Non-textual elements make the experiences engaging and informative, while they may also introduce unexpected risks of miscommunication caused by ambiguity. Moreover, we found that impact captions can also facilitate social interactions between users beyond speech conversations in social VR. Additionally, we illustrate three application scenarios to further explore the generalizability of our impact-caption-mediated method for diverse contexts in social VR.


\subsection{\altcolor{Designing Visual Rhetoric with Interactivity for Facilitating Communication in Social VR}}

\altcolor{As suggested by previous research, mediating interpersonal communication in virtual spaces is actually about supporting people in constructing cognitive and affective connections ~\cite{mcveigh2021case, palmer1995interpersonal} and discovering self-identity beyond the real world ~\cite{freeman2021body, sykownik2022something, maloney2020talking}. Although most of the known communication methods in social VR are still imperfect ~\cite{wei2022communication, dzardanova2022virtual, akselrad2023body, sykownik2023vr, tanenbaum2020make, baker2021avatar}}.

\subsubsection{\altcolor{Highlighting the Visual Rhetoric and Interactivity}}
\altcolor{To fill this gap, our work incorporates speech-driven captioning mechanisms into social VR with improvements on captions' visual design and interactivity.
Inspired by the original impact captions used in entertaining TV shows ~\cite{sasamoto2014impact}, we design and implement a form of visual rhetoric with an integration of textual and non-textual elements to convey verbal and non-verbal information simultaneously and provide several human-caption interactions to support speech conversations in social VR.
The user study results confirmed that our impact-caption-mediated method for communication can effectively enhance clarity (\autoref{finding_text}) and reinforce non-verbal cues (\autoref{finding_ambiguity}) in social VR conversations. Moreover, we also found that the rich interactivity offered by impact captions can promote interpersonal interactions between users (\autoref{finding_interaction}). 
Particularly, \system{} presents a concrete example of how expressive visual objects with interactivity can mediate communication and further support social interactions in social VR.}


\subsubsection{\altcolor{Considering Generalizability}}
\altcolor{As for the generalizability of our impact-caption-mediated method, the aforementioned application scenarios (\autoref{sec_apps}) demonstrate that \system{} can be widely applied to different cases to support user needs in social VR. It can enhance emotional expressions to create engaging and intimate atmospheres, support live presentations for teaching scientific knowledge in educational scenarios, and further provide a promising captioning mechanism that can bring immersive social VR experiences to deaf and hard-of-hearing (DHH) individuals. }

\subsubsection{\altcolor{Research Directions for Future Work}}
\altcolor{In summary, our work emphasizes the value of visual rhetoric integrating with interactivity for facilitating communication in social VR, showing how invisible verbal and non-verbal information can be made perceptible and playable. \system{} expands on the ``superpowers'' of social VR ~\cite{mcveigh2022beyond, mcveigh2021case}, leading the way towards unique immersive communication experiences and fostering novel interactions not only between two human users, but also between humans and the digital media.}

\altcolor{As the design space of impact captions in \system{} is generalizable, future work can borrow from or build on our impact captions to create novel digital mediums to support communication needs in a wide range of social VR applications, including facilitating distinctive activities ~\cite{chen2024drink, maloney2020falling}, enhancing self-presentations ~\cite{freeman2021body, sykownik2022something, maloney2020talking}, and constructing social connections ~\cite{li2019measuring}. Besides, further studies are required to investigate the long-term influence of using \system{} for communication on users and explore whether such a method can interfere with the ``proteus effect'' in social VR ~\cite{maloney2020talking} to reveal opportunities for expanding and enriching the application scenarios.
}

\subsection{\altcolor{Understanding the Ambiguity in \system{}}}
\altcolor{In computer-mediated communication (CMC) systems, ambiguity is inevitable due to multiple socio-technical factors, such as cultural differences, personal preferences, and technical limitations ~\cite{stacey2003against, aoki2005making}.
In \system{}, impact captions also face the risks of miscommunication caused by the ambiguity inherent in the complex design space.
Through the user study, a concern raised by participants is that the current heuristic rule-based method for deciding the visual appearances may fail to accommodate the diverse needs of users with different preferences, since they may have different perceptions when seeing the same impact captions (\autoref{finding_ambiguity}). Previous research pointed out that visual elements in the design space, such as color-emotion association ~\cite{hanada2018correspondence, wilms2018color} and emoji ~\cite{czkestochowska2022context, miller2017understanding}, could imply ambiguous interpretations by different people, which also supports our study findings.}

\altcolor{However, the inevitable ambiguity is a double-edged sword in digital media design. While bringing risks of miscommunication, it can also facilitate creativity ~\cite{stacey2003against}, support negotiation ~\cite{gaver2003ambiguity}, and even enhance the human-likeness of AI agents for human-AI communication ~\cite{liu2024let}. 
Therefore, a trade-off arises: for CMC tools, designers must balance the clarity of communication while maintaining certain ambiguity to meet other design goals ~\cite{stacey2003against, aoki2005making}. This trade-off leads to two directions of dealing with the ambiguity: mitigating and utilizing.} 

\altcolor{Yet, the way to mitigate ambiguity for communication in social VR has not been fully explored. Though previous work revealed the potential of using biosignal visualizations ~\cite{lee2022understanding}.
As a complement, our impact caption design space and \system{} illustrate another way to reduce the risks of miscommunication caused by the ambiguity in social VR conversations. Particularly, our impact captions can reduce ambiguity by providing contextual information, which is a known effective method for communication in contexts other than social VR ~\cite{cottone2009solving, dey2005designing, miller2017understanding}.}

\altcolor{For a single impact caption in \system{}, the integration of textual and non-textual visual elements allows the elements to complement each other and provide mutual confirmation in terms of their meanings. This can reduce ambiguity conveyed by a single element (i.e., a single emoji) (\autoref{finding_ambiguity}). 
At the speech level, the speech-driven approach produces impact captions sequentially aligning with the voice input, in which the conversation contexts could be naturally visualized and presented. 
Moreover, users can persistently present an impact caption by simply dragging and placing it at an arbitrary position in the virtual space (\autoref{sec_visual_clutter}) to intentionally create environmental cues ~\cite{cottone2009solving} for providing contextual information with flexibility.
Overall, these features embedded in \system{} pave the way towards designing communication tools in social VR with the consideration of mitigating ambiguity. 
Future work can refer to the self-explanatory visual design of impact captions to construct visual cues that can convey meanings with clarity and can also leave space for users to actively create environmental cues in the virtual space to support their communication activities.}

% \subsubsection{Utilizing Ambiguity in Future Work}
\altcolor{As for utilizing the ambiguity for particular design aims, which is not covered in the current stage of \system{} and the design space of impact captions, we believe it would be a promising research direction for future work that aims to support interpersonal communication and social interactions in social VR.
With the unique affordances ~\cite{mcveigh2022beyond, freeman2021body, freeman2021hugging, wei2022communication} offered by social VR and growing application scenarios with communication needs ~\cite{maloney2020falling, chen2024drink, mei2021cakevr}, ambiguity should have great potential to foster valuable design and mediums to satisfy specific user needs ~\cite{stacey2003against, gaver2003ambiguity}.}



\subsection{\altcolor{Design Implications}}
\altcolor{Based on \system{} and the findings, we identify the following potential design implications for designing digital mediums and tools to support interpersonal communication in social VR.
With the fast evolution of VR and related computing technologies, the contexts of conducting interpersonal communication in VR may change rapidly. However, we hope these implications can highlight the invariant knowledge learned from \system{} to inspire future work.}

\subsubsection{\altcolor{Presenting Verbal Information Selectively}}
\altcolor{Verbal information is fundamental in interpersonal communication ~\cite{liu2023visual}. 
However, when enhancing verbal information for social VR conversations, we should consider selective presentations rather than making all the words in speech visible.
In our user study, participants suggested avoiding presenting every word in speech, since too many insignificant words could be distracting and cause unexpected visual clutter (\autoref{finding_keywords}). 
Although we have already designed a mechanism to softly control the number of impact captions, trying to minimize the possibility of visual clutter (\autoref{sec_visual_clutter}), the POS-tag-based filtering mechanism in the text processor of \system{} is too simple to be an effective keyword extraction method.
With the fact evolution of AI and NLP technologies, future work can explore the ways to involve advanced NLP technologies such as Large Language Models (LLMs) ~\cite{maragheh2023llm} to achieve keyword extraction with accurate understandings of the speech content and the user's intentions.}


% For improving \system{}, the keyword extraction method should be able to retrieve meaningful words from lengthy speeches and reduce the number of words to be presented, so that our visual-centered communicative mediums can avoid visual clutter in virtual space, avoid overwhelming caused by too much insignificant information, and make the presentation concise and meaningful.
% Although successful keyword extraction relies on accurate understandings of the speech content and the user's intentions, it is possible now with the advancements of NLP technologies such as Large Language Models (LLMs) ~\cite{maragheh2023llm}. 

\subsubsection{\altcolor{Using Visual Rhetoric to Present Non-verbal Information}}
\altcolor{Non-verbal information plays an important role in communication, social interactions ~\cite{mcveigh2022beyond, maloney2020talking, aburumman2022nonverbal, liebman2016s}, and self-presentations ~\cite{freeman2021body, sykownik2022something, zhang2022s} in social VR.
Our work demonstrates that rich visual elements can effectively convey non-verbal information in social VR, showcasing the potential of visual rhetoric design for communication tools.
However, we currently only use simple mapping rules to decide how captions correspond to specific visual elements in \system{}. These rules are inadequate for sufficiently and accurately expressing the various types of non-verbal information, as pointed out by participants in the user study.
Therefore, future research can start from seeking in-depth understandings of the associations between specific visual elements and non-verbal information (such as color-emotion association ~\cite{hanada2018correspondence, wilms2018color}), and create novel visual rhetoric designs with effectiveness and elegance for supporting various applications of social VR.}


\subsubsection{\altcolor{Enhancing the Interactivity of Communication Medium}}
\altcolor{The impact captions in \system{} embed rich interactivity, enabling users to create multiple new forms of interpersonal interactive actions (\autoref{sec_interactivity_functions}).
This again demonstrates the unique interactive and immersive nature of social VR ~\cite{maloney2020talking}, highlighting the value of the interactivity for supporting creative, engaging, and emotional communication experiences.
On top of \system{} and previous work, future research can explore more on the interactivity afforded by the digital mediums in social VR, investigating their influences and creating novel designs and technologies to enhance the existing communication channels or even introduce new channels to support interpersonal communication and interactions in social VR.}




% \subsubsection{Providing Personalized Digital Mediums to Support}
% When introducing affective factors to computationally generate content for human needs, we should not neglect personalized needs.

% There is a strong user demand for customization options. This need stems from the diverse ways individuals interpret and interact with visual elements ~\autoref{finding_ambiguity}. Different users have unique preferences and cultural backgrounds that influence their perception of impact captions. By allowing customization, users can tailor their experiences, making interactions more meaningful and comfortable.

% Integrating with advanced generative-AI technology
% To implement customization effectively, platforms should offer flexible design options, providing a range of choices for typefaces, sizes, colors, etc.

% Currently, generative AI technology has shown great potential to assist individuals in rapidly and effectively creating rich computer graphics. Previous research has shown that AI can collaborate with humans in creative endeavors ~\cite{wan2023gancollage}, producing digital products that are expressive ~\cite{iluz2023word, xie2023wakey} and serve personalized needs. 
% Combining generative AI technology with the design space of impact captions can lead to more flexibility and expressive capabilities, enabling the creation of content that better supports communication and interaction among users in social VR.

% AI-powered adaptive personalized recommendations and configurations could be a direction to be explored. 

% By supporting customization, social VR platforms can enhance user satisfaction, improve accessibility, and foster a more inclusive and engaging environment.

% Furthermore, to differentiate the parameters for rendering an impact caption based on users' own preferences is technically feasible. 

 

\subsection{\altcolor{Technical Scalability and Future Improvements}}
\altcolor{\system{} is theoretically scalable in terms of architecture and algorithms (\autoref{sec_pipeline}). However, the current implementation of \system{} as a proof-of-concept system has limited scalability when considering real-world deployment. We identified three main aspects of the limitations and discuss possible solutions for future improvements. }

\subsubsection{\altcolor{Seeking for Better Speech-to-text Solution}}
\altcolor{First, our custom speech transcribing algorithm introduces limitations.
The algorithm integrates the Whisper model ~\cite{whisper2023} with multi-thread processing for approximate real-time processing, which is inferior to the cloud-based end-to-end speech-to-text services provided by business companies in terms of performance (i.e., speed and accuracy). 
Running on laptops further restricts its performance due to the limited computational resources.
For future improvement, we can replace the voice interface module with a well-developed real-time speech transcribing solution, such as a cloud-based business solution or a locally-deployable SDK.}

\subsubsection{\altcolor{Reducing Network Load by Simplifying Architecture}}
\altcolor{Second, the architecture of \system{} heavily relies on network communication.
The back-end web server works as a centric hub that generates parameters of impact captions and exposes APIs to feed VR applications.
The VR application, as the user end, keeps polling the back-end server to continuously fetch newly generated impact captions through the network. In the meantime, multiple VR application instances also continuously sync with each other to achieve the multi-user session across the network.
As pointed out by recent research, the growing number of users will significantly increase the network load for social VR applications ~\cite{cheng2022we}. This challenge also applies to \system{}.
For improvement, through the perspective of architecture, the back-end server could be reduced, because ideally, all the computations, including speech transcribing and caption generation, can be processed in place in a single computational node (i.e., the VR HMDs). Though this relies on the growth of computational power provided by advanced chips in the future. 
Without the back-end server, only the synchronization mechanism among VR devices requires network resources.}

\subsubsection{\altcolor{Simplifying the Hardware Requirements}}
\altcolor{Third, the hardware setting is too complex. Currently, we employ a laptop, a microphone, and a VR HMD to serve a single user. This setting requires coordination among the devices to smoothly run \system{}. Even though the setting is manageable under lab settings for user study, it is far from being used by ordinary people out of the lab.
This limitation can also be addressed with the simplification of architecture, as the laptop and external microphone are used for running the back end for speech input and processing. Once those computational tasks are embedded into VR HMDs, no extra hardware will be needed for \system{}.}


% \subsubsection{Limitations on User Study}
% % \subsubsection{Study in Real-World Contexts with Participants with Diverse Backgrounds}
% One limitation of the user study is the lack of consideration for participants' varying levels of VR expertise. This oversight could impact the study's findings, as experienced users might interact with and interpret impact captions differently than those new to VR. The disparity in familiarity can influence engagement levels and the effectiveness of communication, potentially skewing the results.

% Another limitation is the insufficient diversity in the participant sample. The study's conclusions may not be broadly applicable due to the limited range of cultural and personal backgrounds represented. Cultural differences can greatly affect how impact captions are perceived and understood, impacting their overall effectiveness. Addressing this limitation in future research would provide a more comprehensive understanding of user experiences across diverse populations.

% By acknowledging these limitations, future studies can be designed to capture a wider range of perspectives, ultimately leading to more universally applicable insights into the use of impact captions in social VR.

% The scope of our study is limited in terms of its scale and lab setup. Thus, further in-depth and long-term evaluations in real-world contexts would be necessary to gain a
% better understanding of its impact in such settings. For future work, deploying the \system{} tool in work meetings, classrooms, and informal social gatherings could provide valuable insights. 
%However, studying \system{} in different contexts may require different


% \subsubsection{It's Age of AI}

% Might be useful
% A growing body of research is exploring enhancing communication using AI agents:
% https://dl.acm.org/doi/10.1145/3613904.3642163
% https://dl.acm.org/doi/full/10.1145/3544549.3585651

% How People Prompt Generative AI to Create Interactive VR Scenes ~\cite{aghel2024people}


\section{Conclusion}
This research demonstrates the potential of impact captions as a novel tool to facilitate interpersonal communication in social VR. 
\altcolor{By integrating both verbal and non-verbal cues} through dynamic and playful typographic elements, impact captions enrich the interpersonal interactions in real-time conversations in virtual spaces. Our proof-of-concept system, \system{}, showcases the feasibility and benefits of this approach, while also identifying areas for improvement. 
Future work could focus on refining the design of impact captions to minimize ambiguity and exploring customization features to better meet diverse user needs and enhance inclusiveness.
Overall, our impact-caption-inspired approach offers new avenues for engaging and meaningful interactions among social VR users, paving the way for more immersive and effective communication experiences.



\bibliographystyle{ACM-Reference-Format}
\bibliography{citations}


\clearpage


\appendix

% \section{User Study Participants}

% %TODO: participant table? can be ignored.
% \begin{table}[!ht]
%   \caption{Participants Backgrounds}
%   \label{tab:paricipants}
%   \small
%     \begin{tabular}{llllll}
%     \toprule
%      ID & Gender & Age & Occupation & Experience of VR \\
%     \midrule
%     P1 & female & 24-30 27 & Master's Student & Novice \\ 
%     %% HX
%     P2 & female & 18-24 23 & Master Student & Novice \\ 
%     %% LYN 
%     P3 & male & 18-24 20 & Undergraduate Student & Novice \\ 
%     %% SCX
%     P4 & male & 25-30 27 & Ph.D. Candidate & Novice \\ 
%     %% LW
%     P5 & male & 24-30 26 & Ph.D. Candidate & Novice \\ 
%     %% oyy
%     P6 & female & 18-24 21 & Undergraduate Student & Expert \\ 
%     %% YXY
%     P7 & female & 18-24 21 & Undergraduate Student & Expert \\ 
%     %% WSS
%     P8 & female & 24-30 27 & Ph.D. Candidate & Expert \\ 
%     %% sxy
%     P9 & female & 31-35 35 & Senior Researcher & Expert \\ 
%     %% wy
%     P10 & male & 18-24 22 & Undergraduate Student & Expert \\ 
%     %% QYG
%     P11 & male & 18-24 24 & Research Assistant & Expert \\ 
%     %% HYB
%     P12 & male & 25-30 25 & Ph.D. Candidate & Expert \\ 
%     %% WQ
%     P13 & male & 24-30 28 & Software Engineer & Expert \\ 
%     %% CXS
%     P14 & male & 24-30 30 & Postdoctoral Researcher & Expert \\ 
%     %% lht
%     \bottomrule
%     \end{tabular}
% \end{table}

% \end{CJK*}

\end{document}
\endinput
