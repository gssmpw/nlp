\section{SpeechCap: System Design and Implementation}
We implemented \system{} as a proof-of-concept system to demonstrate and evaluate our impact-caption-inspired approach to support interpersonal communication in social VR. According to the design goals (\autoref{section_design_goals}), \system{} takes the user's real-time speech to drive the generation of impact captions in VR (G1), uses rule-based methods to filter out non-important words to be rendered as impact captions in VR (G2), and provides multiple impact-caption-mediated interactions for communication (G2, G3).

% Additionally, voice input prevents users' hands from being occupied with complicated text input operations, leaveing the spare capacities for interactions with impact captions.

\subsection{Technical Pipeline: Algorithms, Software, and Hardware}
\label{sec_pipeline}
The technical pipeline of \system{} (\autoref{fig:pipeline}) consists of three main software modules namely \textbf{Voice Interface}, \textbf{Text Processor}, and \textbf{VR Application}.
Voice Interface transcribes speech to text in real time and stores the transcribed text for subsequent processing.
Text Processor determines which words should be made as impact captions and decide the appearance of captions based on the analysis of speech texts. VR Application provides VR environment for users to engage in communication through interactive impact captions.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{img/pipeline_minor_abc.png}
    \caption{
    \altcolor{
        \system{} System Overview. 
        The system consists of three key modules: (A) \textbf{Voice Interface} that processes real-time voice input and stores the transcribed texts to a shared database,
        (B) \textbf{Text Processor} that distills transcribed texts into impact captions and decides the design of each caption, and 
        (C) \textbf{VR Application} that keeps polling the Text Processor for fetching upcoming impact captions and renders the VR space.
        As for hardware settings, Voice Interface and Text Processor run on a laptop that is paired with a Bluetooth Microphone for collecting voice input. A local area network (LAN) is configured to support the connection between the laptop and VR headsets and among multiple sets of hardware devices for multiple users.
    }}
    \Description{.}
    \label{fig:pipeline}
\end{figure*}


\subsubsection{Voice Interface}
\label{sec_voice_interface}
We implemented a Python module with basic real-time speech-to-text capability using OpenAI's Whisper \footnote{https://github.com/openai/whisper} ~\cite{radford2023robust} as the core model for speech recognition.
As a state-of-the-art AI model designed for speech recognition tasks, Whisper's accuracy is close to that of professional human transcribers ~\cite{radford2023robust}, and a speed exceeding 10 words per second when using the minimal pruned model (i.e., ``tiny.en'') for transcribing English on an ordinary hardware platform ~\cite{haz2023study}.

Since Whisper was not originally designed for real-time transcribing, we employed a custom multi-thread algorithm to process voice input in near real time.
At the start, the algorithm divides the input voice stream into fragments of fixed duration (e.g., 1 second by default). Once a fragment is created, the algorithm would allocate a thread to call Whisper for transcribing, and then save the text result with a sequence number. 
When running the algorithm, the sequence number starts from zero and increases by one each time for a newly processed fragment. The sequence number mechanism ensures that the downstream Text Processor and VR Application can always maintain alignment with the original speech when they process texts and render impact captions in VR.

In terms of accuracy, our method may slightly underperform compared to the core model due to the use of a multi-threading strategy that processes speech chunks sequentially
Thus, a single word might be split into two successive chunks and fail to be recognized, although this is unlikely.
For speeding up, Whisper provides a sort of pruned model with smaller sizes to reduce computational consumptions. 
And in our algorithm, the maximum number of simultaneous working threads is configurable. More available threads can accelerate transcribing while requiring more computational power.
Ideally, the delay between a word being recorded and transcribed can be calculated by summing up the chunk duration (e.g. 1 second), Whisper processing time, and the time of waiting for an available thread. 
Particularly, in our user study, we adopted the ``tiny.en'' model of Whisper (with the minimum number of parameters and specified for English) and a maximum of 4 threads to maintain an acceptable transcribing speed for running the Voice Interface on Apple MacBook Pro with the M1 Pro chipset.

Our speech recognition algorithm is scalable in terms of both accuracy and efficiency. 
By employing Whisper models with different sizes or other SOTA speech recognition models, both dimensions could be improved, although the trade-off between accuracy and efficiency remains. 
Except for the core model, by configuring different chunk durations and the number of available threads (i.e. thread pool size), the efficiency is optimizable.
In addition, advanced hardware resources other than laptops can bring significant improvements to the performance of the Voice Interface module.


\subsubsection{Text Processor}
\label{sec_text_processor}
To decide which words should be made as impact captions and the specific visual design of each individual caption, we developed the Text Processor module to analyze transcribed speech using NLTK \footnote{https://www.nltk.org/} and Pydub \footnote{https://pydub.com/}.
Text Processor and upper-stream Voice Interface are connected by a shared database (SQLite) in which transcribed texts are stored by Voice Interface. And to respond to the needs of the downstream VR Application, we encapsulate the Text Processor in a back-end web server using the Flask framework \footnote{https://flask.palletsprojects.com/}. The server is responsible for providing processed data (i.e., the content and visual design of impact captions) for the VR Application through Web APIs.

For keyword extraction in Text Processor, we followed a strategy using the part-of-speech (POS) tag to filter out function words (e.g., \textit{conjunction}, \textit{particle}, and \textit{adposition}) as these categories of words mainly convey grammar functionalities with less semantics, and to keep words from the tags \textit{noun}, \textit{verb}, \textit{adjective}, and \textit{adverb}. In addition, we also keep the words of \textit{interjections} as they indicate emotional expressions in conversations, and emotional expression is one of the primary usages of impact captions based on our previous findings.

For the visual design of impact captions in VR, Text Processor applies a rule-based approach to generate a customized visual appearance regarding the design space (\autoref{section_design_space}) for each individual impact caption. The rule-based approach utilizes semantic and acoustic features extracted from the user's speech to determine the appearances on each dimension of the design space for an impact caption instance. Details about the rules are discussed in the next section \autoref{section_mapping}.

\subsubsection{VR Application}
\label{sec_vr_application}
We implemented a VR application to render interactive impact captions and provide multi-user virtual reality environments to simulate social VR conversation scenarios. Technically, the VR application is built with Unreal Engine 5.1~\footnote{https://www.unrealengine.com/en-US/unreal-engine-5} and Meta XR Tools~\footnote{https://developers.meta.com/horizon/develop/}, as we adopted Meta Quest 2 as the target VR hardware device.
Besides the overall settings, we utilized the built-in 3D Text Actor of Unreal Engine~\cite{unrealtext2024} to implement the impact captions in VR.

To achieve a multi-user VR environment for at least two players (i.e., running VR simulation in the same virtual session on multiple HMDs), we developed our VR application based on the ``Listen Server'' mode offered by Unreal Engine ~\cite{unrealmultiplayer2024}. In this mode, the first player who starts the application will take the responsibility as a ``server'' to create and host a session over a local area network (LAN), while the ``server'' itself is also a ``client''. Once a session has started, other players in the same LAN can join the session and play as a pure ``client''.
Technically, whether or not to play the role of ``server'', each individual VR HMD in the multi-player session needs to simulate the entire virtual reality environment independently. 
To make the independently simulated virtual worlds have the same appearances for all the players, the ``clients'' have to share the same parameters for rendering objects and simulating interactions through a replication mechanism.
The replication mechanism continuously synchronizes the parameters from the ``server'' to the ``clients'' and reports changes made by the ``clients'' to the ``server''. In this way, multiple players can eventually experience the same VR environment using their own devices.

Nevertheless, the replication mechanism does not force every ``client'' to strictly have the same appearance all the time, since each individual ``client'' always reserves full control of the objects in its own simulation.
This allows us to make local modifications to the objects before they are finally rendered on each individual ``client''.
In this way, we adjust the rotation parameters of the impact captions on each ``client'' HMD, making the impact captions always face towards the HMD's owner player in a readable angle (i.e., the replicas of the ``same'' impact caption are rendered to face different directions in different ``clients'').


\subsubsection{Hardware Settings}
\label{sec_hardware}
Hardware settings include a laptop, a wireless Bluetooth microphone, and a VR head-mounted device with controllers for a single user (\autoref{fig:system}). 
Specifically, we employed an Apple MacBook Pro (with an M1 Pro chipset) laptop for running the Voice Interface and Text Processor modules with the associated database and web server, and a Bluetooth wireless microphone connected to the laptop for collecting the user's voice input. Meta Quest 2 with controllers is used to run the \system{} VR Application. 

%TODO: a figure showing the hardware settings with software modules.

To create a stable network environment for serving the multi-player session among the devices, we configured a router to provide a local wireless network with fixed IP addresses for each quest device and the laptops. Additionally, during a session, we use a remote meeting software (e.g., ZOOM) on the laptops to transmit the voices of the users so that they can hear each other in the VR environment.


\subsection{Speech-driven Impact Caption Generation}
\label{section_mapping}
In \system{}, we used a rule-based method to determine the visual appearance of impact captions across each dimension of the design space.
Our method takes verbal content, affective factors (e.g., valence) and prosodic factors (e.g., loudness) extracted from the speech to compute the visual design of each impact caption (\autoref{fig:real_mapping}), ensuring that the captions align with the context of the conversation.
% Particularly, we employ fixed word lists for each of the dimenstions, see supplemantary


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{img/mapping_v3.png}
    \caption{
        Mappings Semantics to Impact Caption Design for Proof of the Concept.
        Valence links to text color where a warm color for positive moods and a cold color for negative moods.
        Loudness links to the size of captions. The larger the louder. 
        Formality links to typeface. ``Time New Roman'' is used for formal and ``Comic Sans'' is used for casual words.
        Emoji is applied for words regarding happy, embarrassed, and sad feelings.
        Speech Bubble and a ``shivering'' motion is applied for words with excitement.
        Ornaments is applied for words representing specific entities.
    }
    \Description{.}
    \label{fig:real_mapping}
\end{figure*}

\subsubsection{Text Color}
Based on our design space (\autoref{sec_space_color}) and the empirical knowledge of color-emotion associations ~\cite{wilms2018color, hanada2018correspondence, plutchik2013theories}, we selected a bright and warm color ({\textcolor[rgb]{1.0,0.82,0.26}{\faSquare}}, close to ``light orange'') for representing captions with positive emotional expressions, a dark and cold color ({\textcolor[rgb]{0.09,0.27,0.61}{\faSquare}}, close to ``dark blue'') for negative captions, and white for neutral captions. 
Technically, the valence of an impact caption is determined by NLTK's ``opinion\_lexicon'', which refers to lists of positive and negative words for sentiment analysis.

\subsubsection{Caption Size}
The size of an impact caption is determined by the loudness (volume) of its belonging speech recording fragment. We estimate the loudness by decibels relative to full scale (dBFS) because dBFS measures volume on the same scale so that comparable results can be generated from different audio recording fragments. Using dBFS, the value 0.0 represents the maximum loudness, and negative numbers are used for lower loudness. For example, -20 in dBFS means that the volume is 20 dBs less than the maximum.
In \system{}, we implemented a rule that allows impact captions to be rendered in three sizes: ``small'', ``medium'', and ``large''. Small size is applied when the dBFS value is under -40. Medium size is applied when the dBFS value is between -40 and -20. Large size applies when the dBFS value is above -20. The louder the speech is, the larger the impact captions will be.

\subsubsection{Typeface}
Referring to the design space and previous studies on human perceptions of typeface ~\cite{bianchi2021emotional, amare2012seeing}, we use typeface to encode and represent the formality of speech. 
As pointed out by previous research, the formality of texts is not a dimension that can be directly calculated but inferred by the words ~\cite{heylighen1999formality}.
Thus, we decided to classify words in speech as either ``formal'' or ``casual'' in terms of their formality and apply a typeface for the two groups, respectively.

To identify the formality of words, we prepared a pre-defined word list of formal words so that we can determine an incoming word by checking if it is in the list.
As for the typefaces, we referred to the literature to guide our design choices.
For formal words, we applied ``Times New Roman'' \footnote{https://en.wikipedia.org/wiki/Times\_New\_Roman}, which is a serif typeface originally designed for serious publications. It is also known to make people feel formal and serious ~\cite{mackiewicz2004people}.
For casual words, we adopted ``Comic Sans'' \footnote{https://en.wikipedia.org/wiki/Comic\_Sans}, as it is also studied by previous research as a casual, sans-serif typeface with a playful and informal look ~\cite{amare2012seeing}. Its visual characteristics feature rounded edges and irregular strokes.

% we used rule-based methods using special word lists to determine whether a word should be considered formal or causal. 
% Formal words will be rendered in VR using a straight typeface, while causal words were associated with an artistic typeface\cite{de2024caption}.

\subsubsection{Emoji and Emoticon}
As a non-textual element that directly links to emotions, we selected three common moods to be explicitly displayed by emojis in our impact captions: a ``smiling face'', a ``sad face'', and an ``embarrassed face'', as faces with common expressions belong to a category of emojis that are known to be commonly used across different cultural backgrounds ~\cite{czkestochowska2022context}.
Similarly to the rules for deciding typefaces, we employed three special word lists for the three moods, respectively. Once a word in the lists is mentioned, the corresponding emoji will be attached aside from the textual part of an impact caption. As the most straightforward example, the word ``happy'' would be accompanied by the ``smiling face'' emoji, while ``crying'' will have a ``sad face'' emoji.

\subsubsection{Ornament}
According to the design space, typographic ornaments include various types of non-alphabetic characters or icons that are relevant to the words within the impact caption. In \system{}, we focus on one of the main purposes of using the ornament, which is to visually highlight objects or entities (e.g., ``cake'') that were mentioned in speech. 
Particularly, we specified a list of keywords of common concepts in daily life for ornaments. The graphic icons for these words are retrieved from Noun Project \footnote{https://thenounproject.com/}, a free open-source icon library. 
The assets of the icons were pre-installed in the VR Application module of \system{}, and the back-end Text Processor will make decisions for each word based on the word list. 


\subsubsection{Speech Bubble}
Speech bubble in \system{} is used for two types of words: greetings (e.g., ``hello'') and interjections (e.g., ``HHHHHH'' for laughing), since they indicate attention requirements or speech with excitement.
For impact captions with greeting words, the balloon is rounded-edged to make the captions different from others and attractive. For interjections, a spiky-edged balloon is used to simulate surprising sounds or speech.

\subsubsection{Motion Effects}
A ``shivering'' motion effect will be added to the words with excitement, such as ``shocking'' and ``surprised'', when spawned. 
Rather than adding more motions to the impact captions while they are generated, we leave the potential of triggering motion effects to the users, since VR naturally provides such interactivity over videos where impact captions originate from. In \system{}, the impact captions can always be alive and interactive.


\subsection{Communication with Interactive Impact Captions in VR}
\label{sec_interactivity_functions}
\system{} allows users to have conversations in a shared VR space with the support of interactive impact captions. In \system{}, impact captions are exclusively triggered by speech input. 
\altcolor{
When a particular word (i.e., identified by the rules in Text Processor, \autoref{sec_text_processor}) is mentioned in the speech,
}
\system{} will create a corresponding impact caption at the position roughly in front of the speaker’s avatar at the height of the chest so that users can easily grab the captions. To avoid overlapping, the position will be randomized each time for newly generated captions (\autoref{sec_visual_clutter}).

The visual appearance of each caption is determined by the semantics and affective factors of the corresponding speech (\autoref{section_mapping}). In a conversation, both the speaker and the listener can see and play with the captions, allowing the communication process to be visible, interactive, and playful beyond traditional voice-only experiences.


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{img/system_revision3.png}
    \caption{
        Interactions with Impact Caption. 
        (A) \textbf{Grabbing} allows users to hold and place a caption to an arbitrary position;
        (B) \textbf{Stretching} needs two hands to resize an impact caption;
        (C) \textbf{Attaching} allows an impact caption to be attached on the head or body of the virtual avatar;
        (D) \textbf{Shooting} can eject an impact caption forward and trigger the \textbf{Explosion} effect when a collision occurs.
    }
    \Description{.}
    \label{fig:system}
\end{figure*}


\subsubsection{Fundamental Interactions}
\system{} allows users to intuitively interact with impact captions in VR through a set of embodied actions (G3), enabling basic human-caption interactions.
\textbf{Grab} (\autoref{fig:system} A) allows users to hold a caption using VR controllers, adjust its orientation, and move it to a new position, mimicking real-world interactions with a physical item.
When grabbing an impact caption, users can \textbf{Shake} it to trigger shivering and blinking effects.

If both hands grab the same impact caption simultaneously, users can \textit{Stretch} it to resize (\autoref{fig:system} B).
By resizing, users may create huge captions larger than their avatars, or tiny captions like a bullet. 
The ability to adjust size broadens the functionality of impact captions, making them more than just speech-driven subtitles. 
Users can combine captions of different sizes with other actions to enhance interactions with others. For example, making a huge caption and shaking it to trigger the blinking effects to enhance the visual impact to emphasize the key points of conversation.

Furthermore, when holding an impact caption, users can \textbf{Throw} it away by releasing the grab button while moving their arms, just like the natural ``throw'' action. 
As an enhanced version of ``throwing'', users can \textbf{Shoot} a caption, letting it emit towards a direction in a straight line. The shooting action is bonded with the front trigger button on the controllers. Users can hold the button for up to 3 seconds to charge the action. Longer charging time rewards a stronger initial force for emission, resulting in a high initial velocity of the ``bullet'' caption.

\subsubsection{Decorate Avatars for Self Expression}
Avatars play an important role in social VR that supports self-presentation and reflects users' emotions and personalities ~\cite{sykownik2022something, freeman2021hugging}. 
In \system{}, users can \textbf{Attach} an impact caption to their avatar’s head by tapping or touching it while holding the caption (\autoref{fig:system} C). Once attached, the caption remains fixed like a ``tag'' or ``hat''. 
This feature is inspired by the typical usage of impact captions in TV shows where impact captions are used to tag the characters, revealing their emotional status. 
Besides the head, users can also attach an impact caption to a body part of the avatar, causing it to revolve around the avatar periodically and creating a circling motion effect.

\subsubsection{Mediating Interpersonal Interactions}
In \system{}, impact captions can react to user actions and avatars, serving as a medium to facilitate interactions.
When thrown or shot by users, impact captions start flying in the virtual space.
Once a flying impact caption collides with another caption or avatar, it triggers an explosion effect (\autoref{fig:system} D), generating multiple replicas that emit outward like fireworks.
Such a dynamic effect may be used as a non-verbal cue to attract attention and add more fun to the social VR experience.

\subsubsection{Reducing Visual Clutter and Overlapping}
\label{sec_visual_clutter}
To maintain the readability and clarity of floating impact captions in conversations (G2), \system{} introduces a time-to-live (TTL) mechanism to manage the number of captions.
The TTL mechanism allows each caption to live for 5 seconds once generated.
If users do not interact with a newly generated impact caption within the time period, the caption will automatically disappear afterwards. 
To keep an impact caption, users can simply touch or grab it before it disappears.

Once the caption has interactions with a user, it will no longer be constrained by the living time limitation until users intentionally delete it by holding it and clicking a button on the VR controller (i.e., ``X'' button for the left-hand controller and ``A'' button for the right-hand controller).

Additionally, to avoid multiple impact captions overlapping with each other when a user keeps speaking, \system{} randomizes the positions for spawning. Specifically, the exact spawn position of a caption is calculated by a fixed position plus random offsets on $x$, $y$, and $z$ dimensions. This strategy creates a ``word cloud'' effect in front of the speaker’s avatar.


