%File: formatting-instructions-latex-2025.tex
%release 2025.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[table]{xcolor}
\usepackage{tabularx}  
\usepackage{amsmath} 
\usepackage{amsthm}  
\usepackage{graphicx}
\usepackage{booktabs} % 用于创建漂亮的表格线  
% \usepackage{wrapfig}  
\usepackage{enumitem}
\usepackage{bm}
\newtheorem{theorem}{Theorem}  
\newtheorem{corollary}{Corollary}  
\usepackage{amssymb}  
\usepackage{multirow}
\input{math_commands.tex}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Disentangling Long-Short Term State Under Unknown Interventions for Online Time Series Forecasting}
\author{
  Ruichu Cai\textsuperscript{\rm1, \rm2},
  Haiqin Huang\textsuperscript{\rm1},
  Zhifan
  Jiang\textsuperscript{\rm1},
  Zijian Li\textsuperscript{\rm3}\footnote{corresponding author: Zijian Li (leizigin@gmail.com)},
  Changze Zhou\textsuperscript{\rm1},\\
  Yuequn Liu\textsuperscript{\rm1},
  Yuming Liu\textsuperscript{\rm1},
  Zhifeng Hao\textsuperscript{\rm4}
}
\affiliations{
    \textsuperscript{\rm 1}School of Computer Science, Guangdong University of Technology, China\\
    \textsuperscript{\rm 2}Peng Cheng Laboratory, Shenzhen, China\\
    \textsuperscript{\rm 3}Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates\\
     \textsuperscript{\rm 4}Shantou University\\
    % email address must be in roman text type, not monospace or sans serif

}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
% While massive data-driven deep models boost the development of time series forecasting, they can hardly be employed in the online forecasting scenario where data arrives sequentially, since it is hard to preserve long-term dependency and update short-term changes simultaneously. Although some recent methods solve this problem by controlling the updates of latent states, they cannot disentangle the long/short-term states, leading to suboptimal performance of 
Current methods for time series forecasting struggle in the online scenario, since it is difficult to preserve long-term dependency while adapting short-term changes when data are arriving sequentially. Although some recent methods solve this problem by controlling the updates of latent states, they cannot disentangle the long/short-term states, leading to the inability to effectively adapt to nonstationary. To tackle this challenge, we propose a general framework to disentangle long/short-term states for online time series forecasting. Our idea is inspired by the observations where short-term changes can be led by unknown interventions like abrupt policies in the stock market. Based on this insight, we formalize a data generation process with unknown interventions on short-term states. Under mild assumptions, we further leverage the independence of short-term states led by unknown interventions to establish the identification theory to achieve the disentanglement of long/short-term states. Built on this theory, we develop a \textbf{L}ong \textbf{S}hort-\textbf{T}erm \textbf{D}isentanglement model (\textbf{LSTD}) to extract the long/short-term states with long/short term encoders, respectively. Furthermore, the \textbf{LSTD} model incorporates a smooth constraint to preserve the long-term dependencies and an interrupted dependency constraint to enforce the forgetting of short-term dependencies, together boosting the disentanglement of long/short-term states. Experimental results on several benchmark datasets show that our \textbf{LSTD} model outperforms existing methods for online time series forecasting, validating its efficacy in real-world applications.
%Code is
%available at https://github.com/DMIRLAB-Group/LSTD .


% To further enforce disentanglement, the \textbf{LSTD} model incorporates a minimal update constraint for long-term states and a sparse dependency constraint for short-term states. 
 



% To solve this problem, we observe that short-term changes can be led by unknown interventions like abrupt policies in the stock market, which inspires us to formalize a data generation process with unknown interventions on short-term states. Under mild assumptions, we further establish the identification theory to guarantee the disentanglement of long short-term states by 

% Under mild assumptions, we establish the identification theory to guarantee the disentanglement of long short-term states.  Based on this theory, we develop a \textbf{L}ong \textbf{S}hort-\textbf{T}erm \textbf{D}isentanglement model (\textbf{LSTD}) to extract the long/short-term states with long/short term encoders, respectively.


% To solve this problem, we propose a general framework to disentangle long short-term states for online time series forecasting. Specifically, we first formalize a data generation process where the short-term changes are led by unknown interventions. Under mild assumptions, we establish the identification theory to guarantee the disentanglement of long short-term states. Based on this theory, we develop a \textbf{L}ong \textbf{S}hort-\textbf{T}erm \textbf{D}isentanglement model (\textbf{LSTD}) to extract the long/short-term states with long/short term encoders, respectively. To further enforce disentanglement, the \textbf{LSTD} model incorporates a minimal update constraint for long-term states and a sparse dependency constraint for short-term states.
 
% To solve this problem, we observe that the short-term changes are usually led by unknown interventions like abrupt policies in the stock market. Based this observation, we 
% online 
% 基于深度学习的时间序列预测发展得很好了，但是难以处理数据流，因为少量的数据不足以刻画多模态的变化。现有的方法虽然一定程度上通过控制记忆来缓解分布偏移，但是并不能根本将长期和短期解耦。为了解决这个挑战，我们观察得非稳态的产生的原因在于不可知的干预，例如政策对金融的影响。基于这个观测我们设计了xxx数据生成过程，其中xxxxx。基于这个数据生成过程，我们提出了xxx理论，基于这个理论，我们提出了xxxx模型。在xxxx上取得好的效果。
\end{abstract}
\begin{links}
    \link{Code}{https://github.com/DMIRLAB-Group/LSTD}
\end{links}
\begin{figure*}[t]
\centering
\includegraphics[width=1.9\columnwidth]{figs/motivation.png}
    \caption{Illustration of sequentially arriving exchange rate data, which is influenced by short-term customs duties and long-term financial revenue. Moreover, the short-term customs duties are intervened by sudden customs tariff policies. (a) If the estimated short-term customs duties and long-term financial revenue are entangled, short-term influence from Environments (e.g., $e_1, e_2, e_3$) may affect the effectiveness of the models to adapt to the changing environments, leading to suboptimal forecasting performance. (b) When the long/short-term states are disentangled, the model can quickly adapt to environmental changes and hence achieve correct forecasting results. (c) Data generation process for time-series data. $\bm{z}_{t}^s$ and $\bm{z}_{t}^d$ denotes the long/short-term states. Note that the short-term states $\bm{z}_{t}^d$ are intervened randomly.}
    \label{fig:motivation}
\end{figure*}

\section{Introduction}
% 起：基于深度学习的时间序列预测发展得很好了，但是难以处理数据流，因为少量的数据不足以刻画多模态的变化
% 承：现有的方法虽然一定程度上通过控制记忆来缓解分布偏移，但是并不能根本将长期和短期解耦。
% 转：我们观察得非稳态的产生的原因在于不可知的干预，例如政策对金融的影响。基于这个观测我们设计了xxx数据生成过程，其中xxxxx。
% 合：基于这个数据生成过程，我们提出了xxx理论，基于这个理论，我们提出了xxxx模型。在xxxx上取得好的效果。

% 作为时间序列分析的一个基础认为，时间序列预测已经在各方面得到应用。在工业界中，时间序列数据往往是顺序到来，伴随着分布的变化，现有的方法难以在顺序数据中适应分布的变化，因此难以做出准确的预测。

As one of the most fundamental tasks in time series analysis \cite{hamilton2020time,liu2023itransformer}, time series forecasting \cite{zhou2021informer,zeng2023transformers,kitaev2020reformer,liu2021pyraformer,wu2021autoformer,zhou2021informer} plays a critical role in various fields such as finance \cite{clements2004forecasting,cao2019financial}, and traffic \cite{lippi2013short}. However, in the industry, since time series data often arrives sequentially and is accompanied by temporal distribution shifts \cite{wang2022koopman,li2024and}, existing methods \cite{wu2021autoformer,nie2022time,lopez2017gradient} that heavily rely on the mini-batch training paradigm can hardly adapt to these changing distributions, leading to suboptimal prediction results in the online scenario.

% and generate accurate predictions in the online scenario.

% Onenet, FSNet, CIKM,
% Several methods are devised for the online time series forecasting. 
To solve this problem, several recent methodologies \cite{cai2023memda,guo2024online,mejri2024novel,lin1992self} are proposed to adapt the short-term nonstationarity and long-term dependencies. FSNet \cite{pham2022learning} leverages the partial derivative to characterize the short-term information and an associative memory to preserve the long-term dependencies. To better combine long-term and short-term historical information, OneNet \cite{wen2024onenet} uses a reinforcement learning-based to dynamically adjust the combination of temporal correlation and cross-variable dependency models. Recently, Zhang et al. \cite{zhang2024addressing} propose the  Concept Drift Detection and Adaptation framework (D$^3$A), which first detects the temporal distribution shift and then employs an aggressive manner to update the model. In summary, these methods aim to address online time series forecasting via two steps: 1) disentangling long/short-term states; and 2) adapting short-term states and reusing long-term states for forecasting. Please refer to Appendix A for further discussion about online time series forecasting and causal representation learning.

% time series data usually arrive sequentially, following with temporal distribution shift


% 实际上，非稳态由于短期变量受到的干预引起的，而且干预什么时候发生，作用在什么隐变量身上是未知的，因此没有额外的假设和约束加，解耦是很困难的。
% 图a展示了一个金融的例子，汇率受到关税和财政收入影响，突发的关税政策作用于关税导致非稳态的发生，。如果没有


Although current methods achieve non-trivial contributions on how to update short-term states or how to efficiently combine the long/short-term states, 
% they cannot disentangle the long/short-term states under the online scenarios, making the models hardly adapt to the nonstationary environments.
they implicitly assume that the long/short-term states have been well-disentangled from nonstationary time series data. However, this assumption is hard to meet, and without disentanglement of the long/short-term states, existing methods can hardly adapt to the nonstationary environments. 
% In practice, nonstationarity is brought by the interventions on short-term states, and when the interventions occur is even unknown. Therefore, it is difficult to achieve disentanglement in long/short-term states without further assumptions or constraints. 
Figure \ref{fig:motivation} provides a finance example, where the monetary exchange rate is influenced by the short-term variables (e.g. customs duties) and the long-term variables (e.g., financial revenue). Nonstationarity occurs due to unknown customs tariff policies. 
% And different environments are divided due to different customs tariff policies. 
As shown in Figure \ref{fig:motivation} (a), when the long-term and short-term latent variables are not disentangled, the financial revenues are entangled with the customs duties. As a result, existing methods can be hard to effectively adapt to the changes in financial revenues and may obtain an inaccurate forecasting performance even if they use a masterly strategy to update the short-term states and preserve the long-term states. 

% 基于以上例子，我们发现解耦xxx很重要。正如图b所示，当解耦了xxx和xxx，模型可以适应xxx引起的分布变化的同时保留长期信息。


% 分布偏移由于未知
% In practice, nonstationarity is brought by the interventions on short-term states, and when the interventions occur is even unknown. Therefore, it is difficult to achieve disentanglement in long/short-term states without further assumptions or constraints.

Based on the aforementioned example, we observe that nonstationarity is brought by the unknown interventions on short-term states. Moreover, to address the online forecasting task, it is intuitive to find that we should disentangle the long/short-term states from the time series with unknown interventions as shown in Figure \ref{fig:motivation} (b). Under this intuition, we first consider that the sequentially arriving data follow a data generation process in Figure \ref{fig:motivation} (c), where the latent short-term states are influenced by unknown interventions. Under mild assumptions, we establish disentanglement results on long/short-term states by leveraging the independence of intervened short-term states. To bridge the gap between theory and practice, we further develop a \textbf{L}ong \textbf{S}hort-\textbf{T}erm \textbf{D}isentanglement model (\textbf{LSTD}) to solve the online time series forecasting problem. Specifically, the proposed \textbf{LSTD} model includes a minimal update constraint to preserve the long-term dependencies and an interrupted dependency constraint to enforce the forgetting of short-term dependencies, which facilitates the disentanglement of long-term and short-term latent states.  Empirical results on several real-world benchmark datasets show that the proposed \textbf{LSTD} method outperforms existing state-of-the-art methods for online time series forecasting, highlighting its effectiveness in real-world applications.

% existing methods can hardly guarantee that the long-term and short-term latent variables are disentangled, so 

% two latent variables: financial revenue and customs duties. 




% \section{Identifying Distribution of Time Series Data with Unknown Interventions}
\section{Data Generation Process for Time Series Data}

To show how to disentangle the long-term and short-term latent states in the online time series forecasting scenario, we first introduce the data generation process of time series data as shown in Figure \ref{fig:motivation} (c). Mathematically, we let $\rvx=\{\rvx_1,\rvx_2,\cdots,\rvx_t,\cdots\}$ be time series data with discrete time steps, in which each observation $\rvx_t$ is generated from latent variables $\rvz_t$ through an invertible and nonlinear mixing function $g$ as formalized in Equation (\ref{equ:mixing}).
\begin{equation}
\label{equ:mixing}
    \rvx_t=g(\rvz_t).
\end{equation}

At each time step $t$, $\rvz_t \in \mathbb{R}^{n}$ are divided into the long-term latent states $\rvz_t^s \in \mathbb{R}^{n_s}$ and short-term latent states $\rvz_t^d \in \mathbb{R}^{n_d}$, and $n=n_s+n_d$. Moreover, the $i$-th component of $\rvz_t^s$ is generated by some components of historical long-term latent states $\rvz_{t-\tau}^s$ with the time lag of $\tau$ via a nonparametric function as shown in Equation (\ref{equ:long_term_gen}).
\begin{equation}
\label{equ:long_term_gen}
\begin{split}
    z^s_{t,i}=f^s_i(\{z^s_{t-\tau,k}|& z^s_{t-\tau,k}\in \textbf{Pa}(z^s_{t,i})\}, \varepsilon^s_{t,i}), \\
    &\text{with}\quad \varepsilon^s_{t,i} \sim p_{\varepsilon^s_{t,i}},
\end{split}
\end{equation}
where $\textbf{Pa}(z^s_{t,i})$ denotes the set of latent variables that directly cause $z^s_{t,i}$ and $\varepsilon^s_{t,i}$ denotes the temporally
and spatially independent noise extracted from a distribution $p_{\varepsilon^s_{t,i}}$.

Moreover, according to the observation in the example in Figure \ref{fig:motivation}, we assume that the nonstationarity in time series data is led by the interventions on the short-term latent variables (e.g., the truncation between $\rvz_{t-1}^d$ and $\rvz_t^d$ in Figure \ref{fig:motivation} (c)). It is noted that when the interventions occur is unknown. To illustrate the randomness of the interventions, we let $I$ be an indicator to decide if an intervention occurs and $I$ comes from a Bernoulli distribution $\mathbf{B}(I, \theta )$ with the probability of $\theta $. When $I=0$, it indicates no intervention, whereas when $I=1$, it signifies intervention. When intervention occurs, the data is generated solely by noise. Formally, the generation process of the short-term latent variables is shown as follows:
\begin{equation}
\label{equ:short_term_gen}
\begin{split}
    z_{t,j}^d&=\left\{ 
    \begin{aligned}
         f^d_j(\{z^d_{t-\tau,k}|& z^d_{t-\tau,k}\in \textbf{Pa}(z^d_{t,j})\}, \varepsilon^d_{t,j}) \;\;\text{, if I = 0}   \\
        &f^d_j(\varepsilon^d_{t,j}) \quad\quad\quad\quad\quad\qquad,\text{if I = 1}
    \end{aligned}
    \right.\\
    &\quad\quad\text{where}\quad \varepsilon^d_{t,j} \sim p_{\varepsilon^d_{t,j}} \quad \text{and} \quad I\sim \mathbf{B}(I, \theta),
\end{split}
\end{equation}
where $\textbf{Pa}(z^d_{t,j})$ denotes the set of latent variables that directly cause $z^d_{t,j}$ and $\varepsilon^d_{t,j}$ denotes the temporally and spatially independent noise extracted from a distribution $p_{\varepsilon^d_{t,j}}$. 

The data generation process as shown in Equation (\ref{equ:mixing})-(\ref{equ:short_term_gen}) can be well interpreted by the aforementioned financial example. First, the exchange rate can be considered as the observation time series data. Sequentially, the financial revenue and the customs duties denote the long-term and short-term latent variables, respectively. Finally, $I=1$ denotes that the customs tariff policy intervenes with customs duties and leads to temporal distribution changes. In the context of online time series forecasting, where the time series data arrive sequentially, we first predict the value of $\rvx_{L+1:H}$ given $\rvx_{1:L}$ at $t$-th time step. Then at $t+1$-th time step, we have access to the true value of $\rvx_{L+1:H}$ to update the model and then use $\rvx_{2:L+1}$ to predict the value of $\rvx_{L+2:H+1}$. 



% \subsection{Identifying Distribution of Time Series Data for Online Time Series Forecasting}
\section{Disentanglement of Long-Term and Short-Term States}


% \subsection{Block-wise Identification}
% Based on the this data generation process \\
To disentangle the long-term latent variables $\textbf{z}^s_t$ and the short-term latent variables $\textbf{z}^d_t$, we propose the block-wise identification theory in Theory \ref{theorem:1}. Mathematically, the block-wise identification means that for the ground-truth $\rvz_t^*$, there exists $\hat{\rvz}_t^*$ and an invertible function $h^*_z:\mathbb{R}^{n^*} \rightarrow \mathbb{R}^{n^*}$, such that $\hat{\rvz}_t^*=h^*_z(\rvz_t^*)$. And $*$ can be $d$ or $s$.
\begin{theorem}
\label{theorem:1}
(\textbf{Subspace Identification of the long-term and short-term Latent Variables}) Suppose that the observed data from long/short-term is generated following the data generation process in Figure\ref{fig:motivation} (c), and we further make the following assumptions:
\begin{itemize}[leftmargin=*,  itemsep=5pt]  
    % \item A1 \underline{(Minimum Intervention Threshold:)} Assuming intervention occurs within at least $\tau$ ($\tau > 2$) steps  
    \item A1 \underline{(\textbf{Smooth, Positive and Conditional independent Den-}} \\
    \underline{\textbf{sity:})}
    \cite{yao2022temporally,yao2021learning} The probability density function of latent variables is smooth and positive, i.e., $p(\rvz_{t-\tau+1:t}|\rvz_{t-\tau})>0$ over $\textbf{Z}_{t-\tau}$ and $\textbf{Z}_{t-\tau+1:t}$.
    Conditioned on $\rvz_{t-\tau}$ each $z_i$ is independent of any other $z_j$ for $i,j\in {1,...,n}, i\neq j,\ i.e$, $\log{p(\rvz_{t-\tau+1:t}|\rvz_{t-\tau})} = \sum_{k=1}^{n_s} \log {p(z_{t-\tau+1:t,k}|\rvz_{t-\tau})}$
    % \item A3\underline{ (Conditional independent:)} Conditioned on $\textbf{z}_{t-\tau}$ each $z_i$ is independent of any other $z_j$ for $i,j\in {1,...,n}, i\neq j,\ i.e$, $\log{p(\textbf{z}_{t-\tau+1:t}|\textbf{z}_{t-\tau})} = \sum_{k=1}^{n_s} \log {p(z_{t-\tau+1:t,k}|\textbf{z}_{t-\tau})}$
    \item A2 \underline{(\textbf{non-singular Jacobian}):} \cite{kong2023understanding} Each generating function $g$ has non-singular Jacobian matrices almost anywhere and $g$ is invertible. 
    \item A3\underline{ (\textbf{Linear Independence}:) }\cite{yao2022temporally} For any $\rvz^d\in \textbf{Z}^d_{t-\tau+1:t}\subseteq {R}^{n_d},\bar{v}_{t-\tau,1},...,\bar{v}_{t-\tau,n_d}$ as $n_d$ vector functions in $z^d_{t-\tau,1},...,z^d_{t-\tau.l},...,z^d_{t-\tau,n_d}$ are linear independent, where $\bar{v}_{t-\tau,l}$ are formalized as follows:
     \begin{align}
        \bar{\textbf{v}}_{t-\tau,l}=\frac{\partial^2 \log{p(\rvz^d_{t-\tau+1:t}|\rvz^d_{t-\tau})}  }{\partial z^d_{t-\tau+1:t ,k} \partial z^d_{t-\tau,l} } 
    \end{align}
\end{itemize}
Suppose that we learn $(\hat{g}, {\hat{f}}_i^s, {\hat{f}}_i^d)$ to achieve Equation (\ref{equ:mixing})-(\ref{equ:short_term_gen}) with the minimal number of transition edge among short term latent variables $\rvz^d_1, \cdots, \rvz^d_t, \cdots$, then the long-term and short-term latent variables are block-wise identifiable.

% When intervention occurs, the short-term component will exhibit discontinuities. Through this disconnection, the symmetry between the long-term and short-term can be broken. Therefore, the long-term and short-term components can be distinctly identified. However, the timing of the intervention is unknown, so we enforce the sparsity of the modules through L1 regularization. By considering partial derivatives, we ensure the existence of discontinuities, thus learning the discontinuities. Finally, through independence proofs, we establish the block-wise identifiable of the components: if $\hat{g}_1: \textbf{Z}^s \times \textbf{Z}^d_1  \longrightarrow \textbf{x}_{t-\tau-1}$ and  $\textbf{Z}^s \times \textbf{Z}^d_2 \longrightarrow \textbf{x}_{t-\tau}\ \cup\ \textbf{x}_{t-\tau+1:t}$, assume the generating
% process of the true model $(g_1, g_2)$ and match the joint distribution $p_{(\textbf{x}_{t-\tau-1},\ \textbf{x}_{t-\tau}\ \cup\ \textbf{x}_{t-\tau+:t})}$, then there is a one-to-one mapping between the estimate $\hat{\textbf{Z}}^s_{t-\tau-1:t}$ and the ground truth $\textbf{Z}^s_{t-\tau-1:t}$ over $\textbf{Z}^s \times \textbf{Z}^d_1 \times \textbf{Z}^d_2,$ that is, $\textbf{Z}^s_{t-\tau-1:t}$ is block-identifiable. Then, by learning the data generation process, $\textbf{Z}_{t-\tau+1:t}^d$ are block-wise identifiable.
\end{theorem}

\paragraph{Proof Sketch:} The proof can be found in Appendix \textbf{B}. First, we construct an invertible transformation $h_z$ between the ground-truth latent variables and estimated ones. Sequentially, we prove that the ground truth of long-term latent variables is not the function of short-term latent variables by leveraging the pairing time series from different influences. Sequentially, we leverage sufficient variability of historical information to show that the short-term latent variables are not the function of the estimated long-term latent variables. Moreover, by leveraging the invertibility of transformation $h_z$, we can obtain the Jacobian of $h_z$ as shown in Equation (\ref{equ:Jh1}), where $B = 0$ and $C = 0$, since the ground truth long-term latent variables are not the functions of short-term latent variables and the short-term latent variables are not the function of the estimated long-term latent variables.
% \begin{wrapfigure}{r}{0.5\textwidth} % 左侧环绕，宽度为 0.5 倍的文本宽度
% \begin{equation}
%  \small
% \begin{gathered}
% \label{equ:Jh1}
%     \mJ_{h_m}=\begin{bmatrix}
%     \begin{array}{c|c}
%         \textbf{A}:=\frac{\partial z_t^{c}}{\partial \hat{z}_t^{c}} & \textbf{B}:=\frac{\partial z_t^c}{\partial \hat{z}_t^{s_m}}=0 \\ \midrule
%         \textbf{C}:=\frac{\partial z_t^{s_m}}{\partial \hat{z}_t^c}=0 & \textbf{D}:=\frac{\partial z_t^{s_m}}{\partial \hat{z}_t^{s_m}},
%     \end{array}
%     \end{bmatrix}
% \end{gathered}
% \end{equation}
% \end{wrapfigure}
% % \lipsum[2]
% \begin{minipage}{0.5\textwidth}   
     
% \end{minipage}  
\begin{equation}  
    \begin{gathered}  
    \label{equ:Jh1}  
        \textbf{J}_{h_z}=\begin{bmatrix}  
        \begin{array}{c|c}  
            \textbf{A}:=\frac{\partial \rvz_t^{s}}{\partial \hat{\rvz}_t^{s}} & \textbf{B}:=\frac{\partial \rvz_t^s}{\partial \hat{\rvz}_t^{d}}=0 \\ \midrule  
            \textbf{C}:=\frac{\partial \rvz_t^{d}}{\partial \hat{\rvz}_t^s}=0 & \textbf{D}:=\frac{\partial \rvz_t^{d}}{\partial \hat{\rvz}_t^{d}}
        \end{array}  
        \end{bmatrix}  
    \end{gathered}  
    \end{equation} 

%\paragraph{Discussion of the Assumptions:}
\paragraph{Discussion of the Identification Results:}
We would like to highlight that the theoretical results provide sufficient conditions for the identification of our model. That implies: 1) our model can be correctly identified when all the assumptions hold. 2) at the same time, even if some of the above assumptions do not hold, our method may still learn the correct model. From an application perspective, these assumptions rigorously defined a subset of applicable scenarios of our model. Thus, we provide detailed explanations of the assumptions, how they relate to real-world scenarios, and in which scenarios they are satisfied. 

\paragraph{Smooth, Positive and Conditional independent Density.} 
This assumption is common in the existing identification results \cite{yao2022temporally,yao2021learning,yao2022temporally,yao2021learning}. In real-world scenarios, smooth and positive density implies continuous changes in historical information, such as temperature variations in weather data. To achieve this, we should sample as much data as possible to learn the transition probabilities more accurately. Moreover, The conditional independent assumption is also common in identifying temporal latent processes  \cite{li2024subspace}. Intuitively, it means there are no immediate relations among latent variables. To satisfy this assumption, we can sample data at high frequency to avoid instantaneous dependencies caused by subsampling. 

\paragraph{Non-singular Jacobian of $g$.} This assumption is also common in \cite{kong2023understanding, li2024and, li2024identification,xie2023multi,kong2023identification}. Mathematically, it denotes that the Jacobian from the latent variables to the observed variables is full rank. %In real-world scenarios, it means that for each observation variable, there exists at least one latent variable that influences it, i.e., each observation variable is related to the temporal latent process. 
In real-world scenarios, it means that there is at least one observation for each latent variable. To meet this assumption, we can ignore such independent latent variables since they have no influence on the observations. 

% at least one latent variable influences each observation variable. 

\paragraph{Linear Independence.}



\paragraph{Data generation process with unknown interventions.} In real-world time series data, there are many unknown interventions that lead to nonstationarity like the financial example in Figure 1. Therefore, this assumption is reasonable. Besides, we need to impose discontinuities in the short-term components to break the symmetry between the long and short terms in the causal graph. This ensures that the long and short terms are identifiable. Through the identifiable theory, we can explain whether the module learns long-term or short-term components, thereby theoretically guaranteeing the disentanglement of long and short terms. In practice, we may investigate the nonstationarity of the data to test whether this assumption is valid. 
% \textbf{Discussion of the Assumptions:} The proof can be found in the Appendix. The first assumptions are common in the existing identification results \cite{yao2022temporally,yao2021learning}, implying that $p(\rvz_{t-\tau+1:t}|\rvz_{t-\tau})$ should change sufficiently over $\rvz_{t-\tau}$. The second assumption is also common in \cite{kong2022partial}, meaning that the influence from each latent source to observation is independent. The third assumption means that the historical information changes sufficiently, which can be easily satisfied with sufficient time series data. %Through the identifiable theory, we can explain whether the module learns long-term or short-term components, thereby theoretically guaranteeing the disentanglement of long and short terms.
 
%\textbf{Discussion of the Constraint:}  
% In this part, we discuss intervention   in detail. We need to impose discontinuities in the short-term components. If there are no discontinuities, the transition between long-term and short-term processes is symmetrical in the causal graph, so the long-term and short-term cannot be disentangled. Therefore, when learning the short-term components, it is necessary to constrain their continuity to ensure the existence of discontinuities. Intuitively, constraining the short-term components ensures the model adapts quickly without being interfered by past information from the long-term components. At the same time, to ensure the continuity of the long-term components, we guarantee their continuity to distinguish between the long-term and short-term components. Through the identifiable theory, we can explain whether the module learns long-term or short-term components, thereby theoretically guaranteeing the disentanglement of long and short terms.



% \subsection{Component-wise Identification}
% Based on Theorem\ref{theorem:1}, we further establish the component-wise identifiability result as follows.
% \begin{corollary}
% {(\textbf{Component-wise Identification of the Long-term and Short-term Latent Variables})} Suppose that the observed data from different patterns is generated following the data generation process in Figure , and we further make the assumption A2, A3 and the  following assumptions:
% \begin{itemize}
%     \item $A4$  \underline{(Linear Independent:)} For each value of $z_t$, 
%     $v_{1t},\ \hat{v}_{1t},\ v_{2t},\ \hat{v}_{2t},...,v_{nt},\ \hat{v}_{nt}$ , as 2n vector functions in $z_{1,t-1},\ z_{2,t-1},...,z_{n,t-1}$, are linearly independent.
%     %, then $z_t$ must be an invertible, component-wise transformation of a
%     %permuted version of $\hat{z}_t$ %
%     \begin{align}
%     \small
%     v_{k,t} = (\frac{\partial^2 \log{p(z^s_{t,k}|z^s_{t-1})} }{\partial z^s_{k,t} \partial z^s_{1,t-1}},%\frac{\partial^2 \log{p(z^s_{t,k}|z^s_{\tau -1}})} {\partial z^s_{k,t} \partial z^s_{2,t-1}},\\ \nonumber
%     ...,\frac{\partial^2 \log{p(z^s_{t,k}|z^s_{t-1}})} {\partial z^s_{k,t} \partial z^s_{n,t-1}} )^T\\
%     \hat{v}_{k,t} = (\frac{\partial^3 \log{p(z^s_{t,k}|z^s_{t-1})} }{\partial^2 z^s_{k,t} \partial z^s_{1,t-1}}, %\frac{\partial^3 \log{p(z^s_{t,k}|z^s_{t-1})} }{\partial^2 z^s_{k,t} \partial z^s_{2,t-1}},\\ \nonumber
%     ..., \frac{\partial^3 \log{p(z^s_{t,k}|z^s_{t-1})} }{\partial^2 z^s_{k,t} \partial z^s_{n,t-1}})^T
%     \end{align}
% \end{itemize}
% % Then, if $\hat{g}_1: \textbf{Z}^s \times \textbf{Z}^d_1  \longrightarrow \textbf{x}_{t-\tau-1}$ and  $\textbf{Z}^s \times \textbf{Z}^d_2 \longrightarrow \textbf{x}_{t-\tau}\ \cup\ \textbf{x}_{t-\tau+:t}$ , assume the generating
% % process of the true model $(g_1, g_2)$ and match the joint distribution $p_{(\textbf{x}_{t-\tau-1},\ \textbf{x}_{t-\tau}\ \cup\ \textbf{x}_{t-\tau+:t})}$ , then there is a one-to-one mapping between the estimate $\hat{\textbf{Z}}^s_{t-\tau-1:t}$ and the ground truth $\textbf{Z}^s_{t-\tau-1:t}$ over $\textbf{Z}^s \times \textbf{Z}^d_1 \times \textbf{Z}^d_2,$ that is , $\textbf{Z}^d_{t_\tau+1:t}$ and $\textbf{Z}^s_{t-\tau-1:t}$ is  component-wise identifiable.
%  Then, by learning the data generation process, $\textbf{Z}_{t-\tau+1:t}^d$ and $\textbf{Z}_{t-\tau-1:t}^s$ are component-wise identifiable.
% \end{corollary}

% \textbf{Proof Sketch and Discussion:}  The proof can be found in Appendix . Based on Theorem 1, we employ similar assumptions like \cite{yao2021learning,yao2022temporally} to construct a full-rank linear system with only zero solution, which ensures the component-wise identifiability of latent variables, i.e., the estimated and ground truth latent variables are one-to-one corresponding. 

% % \subsection{Disentangling Constraints}
% % \subsubsection{Interrupted Dependency Constraint}
% % \newline\newline\newline\newline\newline\newline\newline\newline

% \begin{figure*}[t]
%     \centering
% \includegraphics[width=2\columnwidth]{CameraReady/LaTeX/figs/online_model.pdf}
%     \caption{The framework of the proposed \textbf{LSTD} model. The long/short-term latent variables $\rvz_{1:L}^d$ and $\rvz_{1:L}^s$ are extracted from the encoder. And the latent transition module is used to estimated the $\rvz_{L+1:H}^d$ and the $\rvz_{L+1:H}^s$ from $\rvz_{L+1:H}^d$ and $\rvz_{L+1:H}^s$, respectively. The long-term and short-term prior networks are used to estimate the prior distributions.} 
%     \label{fig:model}
% \end{figure*}
\section{Long Short-Term Disentanglement Model}
\begin{figure*}[t]
    \centering
\includegraphics[width=1.8\columnwidth]{figs/online_model.png}
    \caption{The framework of the proposed \textbf{LSTD} model. The long/short-term latent variables $\rvz_{1:L}^d$ and $\rvz_{1:L}^s$ are extracted from the encoder. And the latent transition module is used to estimated the $\rvz_{L+1:H}^d$ and the $\rvz_{L+1:H}^s$ from $\rvz_{1:L}^d$ and $\rvz_{1:L}^s$, respectively. The long-term and short-term prior networks are used to estimate the prior distributions.} 
    \label{fig:model}
\end{figure*}
\subsection{Model Overview}
In this section, we introduce the implementation of the long/short-term disentanglement model as shown in Figure \ref{fig:model}. Specifically, it uses a variational sequential autoencoder as a backbone architecture and further employs long-term and short-term prior architectures with smooth constraint and sparse dependency constraint for long-term and short-term latent variable disentanglement. 
% Please refer to Appendix X for details on the implementation of the proposed model.

% and further disentangles   

\subsection{Variational Sequential Autoencoder}
To model the time series data, we follow the data generation process in Figure \ref{fig:motivation} (c) and derive the evidence lower bound (ELBO) as shown in Equation (\ref{equ:elbo}).
\begin{equation}
\small
\label{equ:elbo}
\begin{split}
ELBO=&\underbrace{\mathbb{E}_{q(\rvz^s_{1:H}|\rvx_{1:H})} \mathbb{E}_{q(\rvz^d_{1:H}|\rvx_{1:H})}\ln p(\rvx_{1:H}|\rvz^s_{1:H},\rvz^d_{1:H})}_{L_{R}+L_{P}}\\&- \underbrace{D_{KL}(q(\rvz^s_{1:H}|\rvx_{1:H})||p(\rvz^s_{1:H}))}_{L^s_{K}}\\&
\underbrace{-D_{KL}(q(\rvz^d_{1:H}|\rvx_{1:H})||p(\rvz^d_{1:H}))}_{L^d_{K}}
\end{split}
\end{equation}
where $L_R$ and $L_P$ denote the reconstructed and prediction loss, respectively:
\begin{equation}
\small
    \begin{split}
    L_R&=\frac{1}{L} \sum^L_{i=1} (\hat{\rvz}_i-\rvz_i)^2
    \\L_P&=\frac{1}{H-L} \sum^H_{i=L+1} (\hat{\rvz}_i-\rvz_i)^2
    \end{split}
\end{equation}
$D_{KL}$ denotes the KL divergence. Specifically, $q(\rvz^s_{1:H}|\rvx_{1:H}),\ q(\rvz^d_{1:H}|\rvx_{1:H})$, which includes the encoder and the latent transition module in Figure \ref{fig:model}, is used to approximate the prior distribution. $p(\rvx_{1:H}|\rvz^s_{1:H},\rvz^d_{1:H})$ is used to reconstruct the historical observations and forecast the future values. The aforementioned two distributions can be formalized as follows:
\begin{equation}
\small
    \hat{\rvz}^s_{1:H},\hat{\rvz}^d_{1:H}=\phi(\rvx_{1:H}), \quad \hat{\rvx}_{1:H}=\psi(\rvz_{1:H}),
\end{equation}
Where $\rvz_{1:H}$ denotes the combination of $\hat{\rvz}^s_{1:H}$ and $\hat{\rvz}^d_{1:H}$. For the implementation of $\phi$, we follow the backbone of FSNet \cite{pham2022learning}. For the implementation of $\psi$, we employ an MLP (Multilayer Perceptron). Please refer to Appendix C for more implementation details of the \textbf{LSTD} model.

\subsection{Long-Term and Short-Term Prior Networks 
% with Smooth Constraint for Long-Term Disentanglement
}
To model the prior distribution of the long-term latent variables, we propose the long-term prior networks. Similar to the existing methods for causal representation learning \cite{yao2021learning,yao2022temporally}, we let $\{r_i^s\}$ be a set of learned inverse transition functions that take the estimated long-term latent variables and output the noise term, i.e.,
$\hat{\epsilon}_{t,i}^s=r_i^s(\hat{z}_{t,i}^s, \hat{\rvz}^s_{t-1})$ \footnote{We use the superscript symbol to denote estimated variables.} and each $r_i^s$ is modeled with MLPs. Then we devise a transformation $\kappa^s:=\{\hat{\rvz}^s_{t-1},\hat{\rvz}^s_t\}\rightarrow\{\hat{\rvz}^s_{t-1}, \hat{\epsilon}^s_t\}$, and its Jacobian is $\small{\mathbf{J}_{\kappa^s}=
    \begin{pmatrix}
        \mathbb{I}&0\\
        M & \text{diag}\left(\frac{\partial r^s_i}{\partial \hat{z}_{t,i}^s}\right)
    \end{pmatrix}}$,
where $M$ denotes a matrix. By applying the change of variables formula, we have the following equation:
\begin{equation}
% \small
\label{equ:short_temp_transition}
    \log p(\hat{\rvz}^s_{t-1},\hat{\rvz}^s_t)=\log p(\hat{\rvz}^s_{t-1},\hat{\epsilon}_t^s) + \log|\text{det}(\mathbf{J}_{\kappa^s})|.
\end{equation}
Since we assume that the noise term in Equation (\ref{equ:short_temp_transition}) is independent with $\rvz_{t-1}^s$, we can enforce the independence of the estimated noise $\hat{\epsilon}_t^s$ and further have:
\begin{equation}
% \small
\log p(\hat{\rvz}^s_{t}|\hat{\rvz}^s_{t-1})=\log p(\hat{\epsilon}^s_{t}) + \sum_{i=1}^{n_s}\log|\frac{\partial r_i^s}{\partial \hat{z}^s_{t,i}}|.
\end{equation}
Therefore, the long-term prior can be estimated as follows:
\begin{equation}
\small
\log p(\hat{\rvz}_{1:t}^s)=\log p(\hat{\rvz}^s_1)+\sum_{\tau=2}^t \left( \sum_{i=1}^{n_s}\log p(\hat{\epsilon}^s_{\tau,i}) +\sum_{i=1}^{n_s} \log|\frac{\partial r_i^s}{\partial \hat{z}^s_{\tau,i}}| \right),
\end{equation}
where $p(\hat{\epsilon}^s_i)$ follow Gaussian distributions. Similarly, we can further estimate the short-term prior as follows:
\begin{equation}
\small
\log p(\hat{\rvz}_{1:t}^d)=\log p(\hat{\rvz}^d_1)+\sum_{\tau=2}^t \left( \sum_{i=1}^{n_d}\log p(\hat{\epsilon}^d_{\tau,i}) +\sum_{i=1}^{n_d} \log|\frac{\partial r_i^d}{\partial \hat{z}^d_{\tau,i}}| \right),
\end{equation}
\subsection{Smooth Constraint for Long-Term Disentanglement}
To preserve the long-term dependencies in the long-term latent variables, we propose the smooth constraint. Since the causal relationships of the long-term dependencies are stable, the association of the long-term dependencies is also stable. Based on this insight, we consider the attention weights as associations and extract the association with the help of the self-attention mechanism. Specifically, we first split the $\rvz_{1:H}^s$ into two equal-size segmentation $\rvz_{1:H/2}^s$ and $\rvz_{H/2:H}^s$. And then the association of $\rvz_{1:H/2}^s$ and $\rvz_{H/2:H}^s$ can be formalized as follows:
\begin{equation}
\begin{split}
    A_{\rvz_h^s}&=\text{Softmax}(\frac{\rvz_{1:H/2}^s { \rvz_{1:H/2}^s}^{\mathsf{T}}}{\sqrt{n_s}}),\\
    A_{\rvz_e^s}&=\text{Softmax}(\frac{\rvz_{H/2:H}^s { \rvz_{H/2:H}^s}^{\mathsf{T}}}{\sqrt{n_s}}),
\end{split}
\end{equation}
in which $A_{\rvz_h^s}$ and $A_{\rvz_e^s}$ denote the association matrices of the start half and the end half segments. Hence, we can restrict the long-term dependencies by restricting the similarity of these two matrices as shown in Equation (\ref{equ:smooth_con})
\begin{equation}
\label{equ:smooth_con}
    \mathcal{L}_m = ||A_{\rvz_h^s} - A_{\rvz_e^s}||_2,
\end{equation}
where $||\cdot||_2$ denotes the L2 norm of matrices.

\subsection{Interrupted Dependency Constraint for Short-Term Disentanglement}
Since the nonstationarity is assumed to be led by the interventions to the short-term latent variables, given $\rvz_{1:H}^d$, if intervention occurs at $\tau$-th time step, and $2<\tau<H-1$, then $\frac{\partial \varepsilon_{H, i}^d}{\partial z_{\tau-1,j}^d}=0$, where $i,j \in \{1,\cdots,n_d\}$. Based on this intuition, we aim to enforce the interruption of the estimated short-term dependencies to meet the unknown interventions. To achieve this, we propose the interrupted dependency constraint for the short-term variables. Specifically, given the estimated short-term variables $\rvz_{1:H}^d$, we have:
\begin{equation}
    \mathcal{L}_s = \sum_{(i,j)\in \{1,\cdots,n_d\}}\sum_{\tau\in\{2,\cdots,H-1\}}||\frac{\partial \hat{\varepsilon}_{H, i}^d}{\partial \hat{z}_{\tau-1,j}^d}||_1,
\end{equation}
where $||\cdot||_1$ denote the L1 norm.

By using the aforementioned interrupted dependency constraint, the intervention on the short-term latent variables can be automatically detected, which finally enforces the disentanglement of the short-term latent variables.

\subsection{Model Summary}
By combining the aforementioned variational sequentially autoencoder with the restriction of smooth constraint and interrupted dependency constraint, we can finally formalize the total loss of the proposed \textbf{LSTD} model as follows:
\begin{equation}
    \mathcal{L} =L_{R} + L_{P} + \beta L_K + \alpha L_m + \gamma L_s,
\end{equation}
where $L_{K}=L^s_{K}+L^d_{K}$ . And
$\alpha,\beta,\gamma$ are hyper-parameters. 




% \clearpage
\section{Experiment}

\subsection{Datasets}
% \subsubsection{Dataset Description}\label{app:dataset}
% The detailed descriptions of the real-world datasets are shown as follows:
% \vspace{-3mm}
To evaluate the performance of our method, we consider the following datasets. \textbf{ETT}  is an electricity transformer temperature dataset collected from two separate counties in China, which contains two separate datasets $\{\text{ETTh2, ETTm1}\}$ for one hour level and minutes level, respectively. \textbf{Exchange} is the daily exchange rate dataset from eight foreign countries including Australia, British, Canada, Switzerland, China, Japan, New Zealand, and Singapore ranging from 1990 to 2016. \textbf{Weather }\footnote{\url{https://www.bgc-jena.mpg.de/wetter/}} is recorded at the Weather Station at the Max Planck Institute
for Biogeochemistry in Jena, Germany. \textbf{ECL} \footnote{\url{https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014}} is an electricity-consuming load dataset with the electricity consumption (kWh) collected from 321 clients. \textbf{Traffic} \footnote{\url{https://pems.dot.ca.gov/}} is a dataset of traffic speeds collected from the California Transportation Agencies (CalTrans) Performance Measurement System (PeMS). For each dataset, we follow the standard preprocessing and setting in OneNet \cite{wen2024onenet}.
% which contains data collected from 325 sensors located throughout the Bay Area.

\subsection{Baselines}

% 介绍一下有什么对比方法（一段）
% 介绍一下不同的消融模型（一段）

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}





\begin{table*}[]

\centering


\renewcommand{\arraystretch}{0.65}
\setlength{\tabcolsep}{4pt}

%\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|cccccccccc}
\toprule
Models                     & Len & LSTD                                  & OneNet         & FSNet & OneNet-T       & DER++ & ER    & MIR   & TFCL   & Online-T & Informer \\ \midrule
                           & 1   & \textbf{0.377}                        & 0.380          & 0.466 & 0.411          & 0.508 & 0.508 & 0.486 & 0.557  & 0.502    & 7.571    \\
                           & 24  & 0.543                        & \textbf{0.532} & 0.687 & 0.772          & 0.828 & 0.808 & 0.812 & 0.846  & 0.830    & 4.629    \\
\multirow{-3}{*}{ETTh2}    & 48  & 0.616                        & \textbf{0.609} & 0.846 & 0.806          & 1.157 & 1.136 & 1.103 & 1.208  & 1.183    & 5.692    \\ \midrule
                           & 1   & \textbf{0.081}                        & 0.082 & 0.085 & 
                           0.082 & 0.083 & 0.086 & 0.085 & 0.087  & 0.214    & 0.456    \\
                           & 24  & 0.102                       & \textbf{0.098} & 0.115 & 0.212          & 0.196 & 0.202 & 0.192 & 0.211  & 0.258    & 0.478    \\
\multirow{-3}{*}{ETTm1}    & 48  & 0.115                                 & \textbf{0.108} & 0.127 & 0.223          & 0.208 & 0.220 & 0.210 & 0.236  & 0.283    & 0.388    \\ \midrule
                           & 1   & \textbf{0.153}                        & 0.156          & 0.162 & 0.171          & 0.174 & 0.180 & 0.179 & 0.177  & 0.206    & 0.426    \\
                           & 24  & \textbf{0.136}                        & 0.175          & 0.188 & 0.293          & 0.287 & 0.293 & 0.291 & 0.301  & 0.308    & 0.380    \\
\multirow{-3}{*}{WTH}      & 48  & \textbf{0.157}                        & 0.200          & 0.223 & 0.310          & 0.294 & 0.297 & 0.297 & 0.323  & 0.302    & 0.367    \\ \midrule
                           & 1   & \textbf{2.112}                        & 2.351          & 3.143 & 2.470          & 2.657 & 2.579 & 2.575 & 2.732  & 3.309    & 3.813    \\
                           & 24  & \textbf{1.422}                        & 2.074          & 6.051 & 4.713          & 8.996 & 9.327 & 9.265 & 12.094 & 11.339   & 9.185    \\
\multirow{-3}{*}{ECL}      & 48  & \textbf{1.411}                        & 2.201          & 7.034 & 4.567          & 9.009 & 9.685 & 9.411 & 12.110 & 11.534   & 11.183   \\ \midrule
                           & 1   & \textbf{0.231}                        & 0.241          & 0.312 & 0.236          & 0.271 & 0.284 & 0.298 & 0.306  & 0.334    & 0.234    \\
                           & 24  & \textbf{0.398}                        & 0.438          & 0.426 & 0.425          & 0.476 & 0.461 & 0.451 & 0.441  & 0.481    & 0.451    \\
\multirow{-3}{*}{Traffic}  & 48  & \textbf{0.426}                        & 0.473          & 0.445 & 0.451          & 0.486 & 0.510 & 0.502 & 0.438  & 0.503    & 0.496    \\ \midrule
                           & 1   & {\color[HTML]{212121} \textbf{0.013}} & 0.017          & 0.094 & 0.031          & 0.106 & 0.097 & 0.095 & 0.106  & 0.113    & 0.102    \\
                           & 24  & \textbf{0.039}                        & 0.047          & 0.113 & 0.060          & 0.111 & 0.162 & 0.104 & 0.098  & 0.116    & 0.107    \\
\multirow{-3}{*}{Exchange} & 48  & \textbf{0.043}                        & 0.062          & 0.156 & 0.065          & 0.183 & 0.181 & 0.101 & 0.101  & 0.168    & 0.116    \\ \bottomrule
\end{tabular}%
%}
\caption{Mean Square Error (MSE) results on the different datasets. TCN is abbreviated as T}
\label{MSE}
\end{table*}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% \begin{table*}[]
%  \vspace{-0.85cm}
% \centering
% \caption{Mean Square Error (MSE) results on the different datasets. TCN is abbreviated as T}
% \label{MSE}
% \vspace{-0.25cm}

% \setlength{\tabcolsep}{5pt}
% %\resizebox{\textwidth}{!}{%
% \begin{tabular}{c|c|cccccccccc}
% \hline
% Models                     & Len & LSTD                                           & OneNet                  & FSNet          & OneNet-T       & DER++          & ER             & MIR            & TFCL            & Online-T        & Inforner        \\ \hline
%                            & 1   & \textbf{0.377±6.05E-04}                        & 0.38±7.29E-05           & 0.466±3.43E-01 & 0.411±5.74E-05 & 0.508±2.65E-03 & 0.508±1.72E-02 & 0.486±1.51E-02 & 0.557±3.33E-02  & 0.502±2.23E-04  & 7.571±2.59E-03  \\
%                            & 24  & 0.543±6.79E-04                                 & \textbf{0.532±1.87E-04} & 0.687±7.85E-01 & 0.772±2.44E-02 & 0.828±2.21E-02 & 0.808±3.17E-03 & 0.812±2.62E-02 & 0.846±4.89E-03  & 0.83±9.52E-05   & 4.629±1.52E-03  \\
% \multirow{-3}{*}{ETTh2}    & 48  & 0.616±8.70E-04                                 & \textbf{0.609±6.92E-04} & 0.846±8.84E-03 & 0.806±6.87E-03 & 1.157±1.45E-02 & 1.136±2.74E-03 & 1.103±6.84E-03 & 1.208±1.74E-02  & 1.183±2.94E-04  & 5.692±1.29E-03  \\ \hline
%                            & 1   & \textbf{0.081±9.35E-07}                        & 0.082±6.26E-06          & 0.085±1.69E-03 & 0.082±7.26E-05 & 0.083±1.75E-03 & 0.086±8.90E-03 & 0.085±1.91E-02 & 0.087±1.58E-02  & 0.214±8.73E-05  & 0.456±1.71E-04  \\
%                            & 24  & 0.102±1.05E-07                                 & \textbf{0.098±3.10E-06} & 0.115±6.37E-04 & 0.212±6.10E-06 & 0.196±1.75E-02 & 0.202±2.69E-02 & 0.192±1.23E-02 & 0.211±2.70E-02  & 0.258±6.35E-05  & 0.478±2.52E-04  \\
% \multirow{-3}{*}{ETTm1}    & 48  & 0.115±2.46E-04                                 & \textbf{0.108±1.45E-06} & 0.127±4.66E-04 & 0.223±4.67E-05 & 0.208±1.27E-02 & 0.22±2.44E-02  & 0.21±1.37E-03  & 0.236±5.83E-03  & 0.283±2.01E-04  & 0.388±1.43E-03  \\ \hline
%                            & 1   & \textbf{0.153±7.15E-07}                        & 0.156±3.81E-07          & 0.162±8.82E-07 & 0.171±2.38E-07 & 0.174±1.44E-02 & 0.18±2.19E-02  & 0.179±1.37E-03 & 0.177±1.54E-02  & 0.206±2.61E-04  & 0.426±1.86E-04  \\
%                            & 24  & \textbf{0.136±5.45E-06}                        & 0.175±6.76E-06          & 0.188±3.50E-06 & 0.293±5.44E-06 & 0.287±1.01E-02 & 0.293±1.18E-02 & 0.291±1.47E-02 & 0.301±3.18E-02  & 0.308±2.78E-04  & 0.38±3.02E-04   \\
% \multirow{-3}{*}{WTH}      & 48  & \textbf{0.157±3.07E-05}                        & 0.2±1.69E-05            & 0.223±1.11E-05 & 0.31±1.01E-04  & 0.294±1.14E-03 & 0.297±2.15E-02 & 0.297±1.79E-02 & 0.323±3.20E-02  & 0.302±1.17E-04  & 0.367±8.04E-05  \\ \hline
%                            & 1   & \textbf{2.112±3.87E-04}                        & 2.351±2.02E-03          & 3.143±3.17E-02 & 2.47±1.18E-03  & 2.657±2.90E-03 & 2.579±3.06E-04 & 2.575±1.42E-04 & 2.732±6.65E-03  & 3.309±2.32E-04  & 3.813±1.60E-03  \\
%                            & 24  & \textbf{1.422±6.57E-04}                        & 2.074±4.19E-04          & 6.051±2.39E-01 & 4.713±1.06E-03 & 8.996±1.43E-04 & 9.327±1.67E-04 & 9.265±2.56E-05 & 12.094±1.07E-04 & 11.339±3.17E-04 & 9.185±3.25E-03  \\
% \multirow{-3}{*}{ECL}      & 48  & \textbf{1.411±4.26E-02}                        & 2.201±1.00E-02          & 7.034±4.15E-01 & 4.567±2.30E-04 & 9.009±2.27E-02 & 9.685±3.27E-04 & 9.411±1.58E-04 & 12.11±6.49E-05  & 11.534±8.39E-05 & 11.183±1.54E-04 \\ \hline
%                            & 1   & \textbf{0.231±7.33E-06}                        & 0.241±1.06E-06          & 0.312±9.96E-06 & 0.236±2.30E-06 & 0.271±1.14E-04 & 0.284±6.80E-05 & 0.298±2.21E-04 & 0.306±2.67E-05  & 0.334±2.14E-03  & 0.234±2.42E-04  \\
%                            & 24  & \textbf{0.398±4.89E-05}                        & 0.438±4.59E-05          & 0.426±2.70E-05 & 0.425±2.13E-05 & 0.476±3.87E-03 & 0.461±1.35E-04 & 0.451±2.18E-04 & 0.441±1.06E-04  & 0.481±1.80E-04  & 0.451±1.19E-04  \\
% \multirow{-3}{*}{Traffic}  & 48  & \textbf{0.426±5.93E-06}                        & 0.473±8.38E-04          & 0.445±6.02E-05 & 0.451±2.22E-06 & 0.486±9.09E-05 & 0.51±1.16E-04  & 0.502±2.52E-04 & 0.438±3.05E-04  & 0.503±3.23E-04  & 0.496±6.74E-05  \\ \hline
%                            & 1   & {\color[HTML]{212121} \textbf{0.013±2.85E-07}} & 0.017±1.22E-06          & 0.094±8.55E-04 & 0.031±4.23E-06 & 0.106±1.68E-05 & 0.097±3.50E-05 & 0.095±1.69E-04 & 0.106±2.49E-04  & 0.113±7.35E-05  & 0.102±2.12E-04  \\
%                            & 24  & \textbf{0.039±1.66E-08}                        & 0.047±4.10E-05          & 0.113±2.13E-04 & 0.06±9.82E-07  & 0.111±5.92E-05 & 0.162±2.40E-04 & 0.104±4.92E-05 & 0.098±1.92E-04  & 0.116±1.88E-04  & 0.107±5.37E-05  \\
% \multirow{-3}{*}{Exchange} & 48  & \textbf{0.043±2.95E-06}                        & 0.062±2.91E-04          & 0.156±1.35E-04 & 0.065±4.79E-06 & 0.183±7.38E-05 & 0.181±4.58E-05 & 0.101±1.47E-05 & 0.101±1.59E-04  & 0.168±4.48E-05  & 0.116±1.81E-04  \\ \hline
% \end{tabular}%

% \end{table*}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}


We consider nine state-of-the-art as follows: OneNet \cite{wen2024onenet} which considered the temporal and feature relationships and used reinforcement learning to update their relationships in real-time. At the same time, we compared with a very excellent backbone model FSNet \cite{pham2022learning} which considered gradient updates to optimize fast new
as well as retained information and be used in OneNet. Besides, we also compared the OneNet model with TCN as its backbone named OnetNet-TCN, and the regular usage of TCN named Online-TCN \cite{zinkevich2003online} for online learning. The Experience Replay (ER) \cite{chaudhry2019tiny} stored the previous data in a buffer and interleaved with newer samples during learning. Meanwhile, ER has many advanced variants: TFCL \cite{aljundi2019task} used a task-boundary detection
mechanism and a knowledge consolidation strategy; MIR \cite{aljundi2019online} selected
samples that cause the most forgetting; and DER++ \cite{buzzega2020dark} incorporated a
knowledge distillation strategy. Additionally, we have incorporated a long-term time-series forecasting model, Informer \cite{zhou2021informer}, to investigate the performance of conventional forecasting models in online time-series forecasting problems.% In the tables, TCN is abbreviated as T.
% To reduce noise from single samples
% and achieve faster and better convergence, we followed the experimental setup of method OneNet
\subsection{Quantitative Results and Discussion}
% 主表实验结果
 Experiment results on each dataset are shown in Table \ref{MSE} and Table \ref{MAE}. Since some methods report the best results on the original paper, we also show
the best results on the aforementioned tables.  Please refer to Appendix D for the experiment results with mean and variance over three random seeds. Our \textbf{LSTD} model significantly outperforms all other baselines on most online forecasting tasks. Specifically, our method outperforms the most
competitive baselines by a clear margin of $44\%$ on the Exchange, which verifies the example in the introduction. Moreover, our method also greatly reduced prediction errors in the WTH and ECL datasets. However,
our method achieves the second-best but still comparable
results in the ETT dataset, this might be because there are a few unknown interventions in the ETT datasets. How to address other types of nonstationarity will be an interesting future direction. In addition, we conduct performance analysis experiments and visualization in Appendix D. Compared with other models, we can find that the proposed \textbf{LSTD} has the best model performance and relatively good model efficiency.
% We conducted a visual experiment in the WTH dataset as shown in Figure \ref{fig:Visualization}.% From the prediction plot, it is also evident that our model can rapidly adapt to sudden changes and correctly forecast time-series.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/ablation.png}
    \caption{ Ablation study on the Exchange datasets. We explore the impact of different loss terms}
    \label{fig:ablation}
\end{figure}

\subsection{Qualitative Results and Discussion}
% 可视化结果，不同方法的预测折线图 （建议两行5列） + 分析
 We further conduct visualization results in the WTH and Exchange dataset in Figure \ref{fig:Visualization}. Remarkably, our method detects interventions well and achieves better visualization results than that of OneNet and FSNet, which do not explicitly disentangle the short-term and long-term variables. This is because the long/short term variables of these methods might be entangled, hindering the rapid adaptation to the changing environment of the data streams, and finally resulting in suboptimal predictions%, especially when the prediction length is long
 . In the meanwhile, our method disentangles the long/short term variables by sparsity dependency constraint, and can efficiently adapt to the new environment. At the same time, the smooth constraint further maintains the long-term variables behind the time series data. Therefore, the prediction curve of our method can well align with the ground truth even if the prediction length is long.
 
 % elative stability in the long-term module
 
 % , maintaining relative stability in the long-term module, thus resolving the  \textit{stability-plasticity}  dilemma, hence it can achieve the ideal online forecasting performance.
 % Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[]

\centering
\renewcommand{\arraystretch}{0.65}
\setlength{\tabcolsep}{4pt}
%\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|cccccccccc}
\toprule
Models                     & Len & LSTD                                  & OneNet         & FSNet & OneNet-T & DER++ & ER    & MIR   & TFCL  & Online-T & Informer \\ \midrule
                           & 1   & \textbf{0.347}                        & 0.348          & 0.368 & 0.374    & 0.375 & 0.376 & 0.410 & 0.472 & 0.436    & 0.850    \\
                           & 24  & 0.411                                 & \textbf{0.407} & 0.467 & 0.511    & 0.540 & 0.543 & 0.541 & 0.548 & 0.547    & 0.668    \\
\multirow{-3}{*}{ETTh2}    & 48  & \textbf{0.423}                        & 0.436          & 0.515 & 0.543    & 0.577 & 0.571 & 0.565 & 0.592 & 0.589    & 0.752    \\ \midrule
                           & 1   & \textbf{0.187}                                 & \textbf{0.187} & 0.191 & 0.191    & 0.192 & 0.197 & 0.197 & 0.198 & 0.085    & 0.512    \\
                           & 24  & \textbf{0.217}                        & 0.225          & 0.249 & 0.319    & 0.326 & 0.333 & 0.325 & 0.341 & 0.381    & 0.525    \\
\multirow{-3}{*}{ETTm1}    & 48  & 0.249                                 & \textbf{0.238} & 0.263 & 0.371    & 0.340 & 0.351 & 0.342 & 0.363 & 0.403    & 0.460    \\ \midrule
                           & 1   & \textbf{0.200}                        & 0.201          & 0.216 & 0.221    & 0.235 & 0.244 & 0.244 & 0.240 & 0.276    & 0.458    \\
                           & 24  & \textbf{0.223}                        & 0.225          & 0.276 & 0.345    & 0.351 & 0.356 & 0.355 & 0.363 & 0.367    & 0.417    \\
\multirow{-3}{*}{WTH}      & 48  & \textbf{0.242}                        & 0.279          & 0.301 & 0.356    & 0.359 & 0.363 & 0.361 & 0.382 & 0.362    & 0.419    \\ \midrule
                           & 1   & \textbf{0.226}                        & 0.254          & 0.472 & 0.411    & 0.421 & 0.506 & 0.504 & 0.524 & 0.635    & 0.549    \\
                           & 24  & \textbf{0.292}                        & 0.333          & 0.997 & 0.513    & 1.035 & 1.057 & 1.066 & 1.256 & 1.196    & 1.198    \\
\multirow{-3}{*}{ECL}      & 48  & \textbf{0.294}                        & 0.348          & 1.061 & 0.534    & 1.048 & 1.074 & 1.079 & 1.303 & 1.235    & 1.164    \\ \midrule
                           & 1   & \textbf{0.225}                        & 0.240          & 0.278 & 0.236    & 0.251 & 0.256 & 0.284 & 0.297 & 0.284    & 0.258    \\
                           & 24  & \textbf{0.316}                        & 0.346          & 0.365 & 0.346    & 0.409 & 0.417 & 0.443 & 0.493 & 0.385    & 0.365    \\
\multirow{-3}{*}{Traffic}  & 48  & \textbf{0.332}                        & 0.371          & 0.378 & 0.355    & 0.386 & 0.294 & 0.397 & 0.531 & 0.380    & 0.394    \\ \midrule
                           & 1   & {\color[HTML]{212121} \textbf{0.070}} & 0.085          & 0.174 & 0.117    & 0.173 & 0.124 & 0.118 & 0.153 & 0.169    & 0.115    \\
                           & 24  & \textbf{0.132}                        & 0.148          & 0.206 & 0.166    & 0.227 & 0.210 & 0.204 & 0.227 & 0.213    & 0.196    \\
\multirow{-3}{*}{Exchange} & 48  & \textbf{0.142}                        & 0.170          & 0.254 & 0.173    & 0.243 & 0.241 & 0.209 & 0.183 & 0.258    & 0.217    \\ \bottomrule
\end{tabular}%
%}
\caption{Mean Absolute Error (MAE) results on the different datasets. TCN is abbreviated as T}

\label{MAE}
\end{table*}



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]
    {figs/predict.png}
    \caption{The figure (a) represents the visualization of the proposed LSTD and other baselines. The blue lines denote the ground-truth time series data and the lines with other colors denote the predicted results of different methods. The figure (b) shows the visualization of the LSTD method for detecting interventions. The yellow lines represent the real-time series data, and the red lines represent the gradient.  Black dotted lines denote intervention occurs. \textit{(Best view in color)}
    }
    \label{fig:Visualization}
\end{figure*}
\subsection{Ablation Study}
% \vspace{-0.1cm}
% 消融实验结果，柱状图，一行+分析
We further devise three model variants. a) \textbf{LSTD}-L1: we remove the interrupted dependency constraint for
short-term disentanglement. b) \textbf{LSTD}-L2: we remove the smooth constraint for long-term
disentanglement. c) \textbf{LSTD}-KL: we remove the long/short-term prior and the corresponding Kullback-Leibler divergence term. Experiment results on the Exchange dataset are shown in Figure \ref{fig:ablation}. We find that 1) the performance of \textbf{LSTD}-L1 drops without an accurate forgetting of the information, implying that the accurate forgetting benefits the quickly adapting to changes in the data domain and improves the disentanglement and forecasting performance. 2) the performance of \textbf{LSTD}-L2 drops without retention of the information, implying that the retention benefits the preserving of the long-term effects and improves the forecasting performance. 3) Both long-term and short-term priors play an important role in forecasting, implying that these priors can capture temporal information. 



% \vspace{-0.2cm}
\section{Summary}
This paper presents a long/short-term state disentanglement model to address the challenges of online time-series forecasting in the presence of nonstationarity led by unknown interventions. Unlike existing methods, this model can theoretically identify both long-term and short-term latent variables, enhancing its relevance to real-world data. Technologically, the LSTD model employs the smooth constraint and sparse dependency constraint to enforce the disentanglement of long/short-term variables. %Experiments on benchmark datasets demonstrate its effectiveness. 
In summary, this paper offers valuable insights into enhancing online time-series forecasting via causal representation learning.


\section{Acknowledgments}

This research was supported in part by National Science and Technology Major Project (2021ZD0111501), National Science Fund for Excellent Young Scholars (62122022) and Natural Science Foundation of China (U24A20233, 62206064, 62206061).

% This paper introduces a long short-term state disentanglement model designed to tackle the challenge of online time-series forecasting in the presence of unknown interventions causing nonstationarity. Unlike existing approaches, this method can theoretically identify both long-term and short-term latent variables, enhancing its applicability to real-world data. Experiments on prominent benchmark datasets demonstrate the model's effectiveness. Overall, this paper offers valuable insights into addressing online time-series forecasting from a causal perspective.

% This paper presents a data generation process for time series data, where unknown interventions lead to nonstationarity. Based on this data generation process, we propose a long short-term state disentanglement model that identifies the long-term and short-term latent variables. 
\bibliography{aaai24}

% \section*{Checklist}

% \textbf{This paper:} \newline
% \textbullet \ Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes) \newline
% \textbullet \ Clearly delineates statements that are opinions, hypotheses, and speculation from objective facts and results (yes) \newline
% \textbullet \ Provides well-marked pedagogical references for less-familiar readers to gain the background necessary to replicate the paper (yes)


% \noindent \textbf{Does this paper make theoretical contributions? (yes)} 
% \noindent If yes, please complete the list below.
% \newline
% \textbullet \ All assumptions and restrictions are stated clearly and formally. (yes) \newline
% \textbullet \ All novel claims are stated formally (e.g., in theorem statements). (yes) \newline
% \textbullet \ Proofs of all novel claims are included. (yes) \newline
% \textbullet \ Proof sketches or intuitions are given for complex and/or novel results. (yes) \newline
% \textbullet \  Appropriate citations to theoretical tools used are given. (yes) \newline
% \textbullet \  All theoretical claims are demonstrated empirically to hold. (yes) \newline
% \textbullet \  All experimental code used to eliminate or disprove claims is included. (yes) \newline


% \noindent \textbf{Does this paper rely on one or more datasets? (yes)}

% \noindent If yes, please complete the list below.\newline 
% \textbullet \ A motivation is given for why the experiments are conducted on the selected datasets. (yes)  \newline
% \textbullet \ All novel datasets introduced in this paper are included in a data appendix. (NA)  \newline
% \textbullet \ All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (NA)  \newline
% \textbullet \ All datasets drawn from the existing literature (potentially including authors’ own previously published work) are accompanied by appropriate citations. (yes)  \newline
% \textbullet \ All datasets drawn from the existing literature (potentially including authors’ own previously published work) are publicly available. (yes)  \newline
% \textbullet  \ All datasets that are not publicly available are described in detail, with an explanation of why publicly available alternatives are not scientifically satisfying. (NA)  \newline


% \noindent \textbf{Does this paper include computational experiments? (yes)}

% \noindent If yes, please complete the list below.\newline
% \textbullet \ Any code required for pre-processing data is included in the appendix. (yes)   \newline
% \textbullet \ All source code required for conducting and analyzing the experiments is included in a code appendix. (yes)   \newline
% \textbullet \ All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)   \newline
% \textbullet \ All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from. (yes)   \newline
% \textbullet \ If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes)   \newline
% \textbullet \ This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; and names, and versions of relevant software libraries and frameworks. (yes)   \newline
% \textbullet \ This paper formally describes the evaluation metrics used and explains the motivation for choosing these metrics. (yes)   \newline
% \textbullet \ This paper states the number of algorithm runs used to compute each reported result. (yes)   \newline
% \textbullet \ Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average); median to include measures of variation, confidence, or other distributional measures. (yes)   \newline
% \textbullet \ The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes)   \newline
% \textbullet \ This paper lists all final (hyper-)parameters used for each model/algorithm in the paper’s experiments. (yes)   \newline
% \textbullet \ This paper states the number and range of values tried per (hyper-) parameter during the development of the paper, along with the criterion used for selecting the final parameter setting. (yes)   \newline



% Online time series forecasting is a widely used technique in the real world due to the continuity of the data and the frequent drift of concepts. In this approach, the learning process occurs over a series of rounds. The model receives a look-back window, predicts the forecasting window, and then displays the valid values to improve the model's performance in the next round. Recently, a brunch of online time series forecasting work got excellent results, including considering gradient updates to optimize fast new as well as retained information \cite{pham2022learning} and models that consider both temporal and feature dimensions \cite{wen2024onenet}. Nevertheless, the fast adaptation and information retention of the models mentioned above are simultaneous. It needs to decouple the long and short term, which can lead to confounding results and suboptimal predictions. To solve this problem, the LSTD decouples the data first to isolate the long and short-term effects on the prediction, with the long-term effects being used to preserve the characteristics of the historical data and the short-term effects being used to quickly adapt to changes in the data for better online prediction.
% \subsection{Continual Learning}
% Continual learning is a novel topic and aims to set up intelligent agency by learning the sequence of tasks to perform with restricted access to experience \cite{lopez2017gradient}. A continual learning model must balance the knowledge of the current task and the prompt of the future learning process, as known in the stability-plasticity dilemma \cite{lin1992self,grossberg2013adaptive}. Due to their connection to how humans learn, several neuroscience frameworks have prompted the development of various continual learning algorithms. The continual learning model corresponds to the requirement of online time series forecasting. The constant learning can enable real-time updates upon receiving the new data to adapt the data dynamics better, improving the model accuracy.
% The proposed LSTD incorporates continual learning into an online time series forecasting model, which mitigates the stability-plasticity problem by decoupling the long and short-term effects, retaining the knowledge of previous tasks through the long-term effects, and facilitating the learning of future tasks through the short-term effects.
% \subsection{Causal Representation Learning}
% To recover the latent variable with identification guarantees\cite{yao2023multi,scholkopf2021toward,liu2023causal,gresele2020incomplete}, independent component analysis (ICA) has been used in a number of studies to determine the casual representation \cite{rajendran2024learning,mansouri2023object,wendong2024causal}. 
% Conventional approaches presuppose a linear mixing function for latent and observable variables. \cite{comon1994independent,hyvarinen2013independent,lee1998independent,zhang2007kernel}. However, determining the linear mixing function is a difficult problem in real-world situations. For the identifiability, many assumptions are made throughout the nonlinear ICA process, including the sparse generation process and the usage of auxiliary variables\cite{zheng2022identifiability,hyvarinen1999nonlinear,hyvarinen2024identifiability,khemakhem2020ice,li2023identifying}.\\
% Specifically, Aapo et al.'s study confirms identifiability first. The exponential family is assumed to consist of latent sources in Ref. \cite{khemakhem2020variational,hyvarinen2016unsupervised,hyvarinen2017nonlinear, hyvarinen2019nonlinear}, where auxiliary variables such as domain indexes, time indexes, and class labels are added. Furthermore, Zhang et al.'s study \cite{kong2022partial, xie2023multi,kong2023identification,yan2024counterfactual} demonstrates that the exponential family assumption is not necessary to accomplish component-wise identification for nonlinear ICA. \\
% Sparsity assumptions were used in several study endeavors to attain identifiability without supervised signals\cite{zheng2022identifiability,hyvarinen1999nonlinear,hyvarinen2024identifiability,khemakhem2020ice,li2023identifying}. For example, Lachapelle et al. \cite{lachapelle2023synergies, lachapelle2022partial} presented mechanism sparsity regularization as an inductive bias to find causative latent components. Zhang et al. selected the latent variable sparse structures in Ref. \cite{zhang2024causal} to achieve identifiability under distribution shift. Furthermore, nonlinear ICA was utilized in Ref. \cite{hyvarinen2016unsupervised,yan2024counterfactual,huang2023latent,halva2020hidden,lippe2022citris} to get the identifiability of time series data. 
% Premise and capitalization of variance variations across data segments based on separate sources were utilized in the study by Aapo et al.\cite{hyvarinen2016unsupervised} to detect nonstationary time series data identifiability. Conversely, the latent variables in stationary time series data are found via permutation-based contrastive learning. Independent noise and variability history information features have been used more recently in TDRL \cite{yao2022temporally} and LEAP \cite{yao2021learning}. \\
% Simultaneously, latent variables were discovered by Song et al. \cite{song2024temporally} without the need to observe the domain variables. Imant et al. \cite{daunhawer2023identifiability} described multimodal comparative learning as having identifiability in terms of modality. Yao et al.\cite{yao2023multi} postulated that multi-perspective causal representations can still be identified when there are incomplete observations. This paper uses multi-modality time series data and leverages historical variability information and multi-modality data fairness to demonstrate identifiability.


% \clearpage
 \appendix
 \onecolumn
 \input{supplementary2}
% \section{Proof}

% \section{Implementation Details}

% \section{More Experiments Results}

\end{document}
