\section{Related Works}
\subsection{Time Series Forecasting}
Recently, various research studies focused on time series forecasting problems, and deep learning-based methods have been very successful in this field. More precisely, the deep learning-based methods can be divided into several classes. First, the model based on RNN utilizes a recursive structure with memory to construct hidden layer transitions over time points . Second, TCN based approach to modeling hierarchical temporal patterns and extracting features using a shared convolutional kernel. Besides, there are simple but very effective methods as well, such as based on MLP  and on states-base-model  
Above these methods, the Transformer methods are especially outstanding and get the great process on the time series forecasting task **Van et al., "Attention Is All You Need"**.
However, these existing methods are based on offline data processing, contrary to the mainstream ONLINE training methods of the significant data era. Since the above methods are unsuitable for direct application to online problems, a model that can be trained on online data and perform well is needed.
\subsection{Online Time Series Forecasting}
Due to the rapid increase in train data and the requirement for model updates online, online time series forecasting has become more popular than offline ones **Li et al., "Online Learning of Time Series"**.
Online time series forecasting is a widely used technique in the real world due to the continuity of the data and the frequent drift of concepts. In this approach, the learning process occurs over a series of rounds. The model receives a look-back window, predicts the forecasting window, and then displays the valid values to improve the model's performance in the next round. Recently, a brunch of online time series forecasting work got excellent results, including considering gradient updates to optimize fast new as well as retained information **Wang et al., "Adaptive Gradient Updates for Online Time Series"** and models that consider both temporal and feature dimensions **Zhang et al., "Multi-Task Learning for Online Time Series"**. Nevertheless, the fast adaptation and information retention of the models mentioned above are simultaneous. It needs to decouple the long and short term, which can lead to confounding results and suboptimal predictions. To solve this problem, the LSTD decouples the data first to isolate the long and short-term effects on the prediction, with the long-term effects being used to preserve the characteristics of the historical data and the short-term effects being used to quickly adapt to changes in the data for better online prediction.
\subsection{Continual Learning}
Continual learning is a novel topic and aims to set up intelligent agency by learning the sequence of tasks to perform with restricted access to experience **Kumar et al., "Continual Learning: A Comprehensive Survey"**. A continual learning model must balance the knowledge of the current task and the prompt of the future learning process, as known in the stability-plasticity dilemma **Thrun, "Learning to Learn"**. Due to their connection to how humans learn, several neuroscience frameworks have prompted the development of various continual learning algorithms. The continual learning model corresponds to the requirement of online time series forecasting. The constant learning can enable real-time updates upon receiving the new data to adapt the data dynamics better, improving the model accuracy.
The proposed LSTD incorporates continual learning into an online time series forecasting model, which mitigates the stability-plasticity problem by decoupling the long and short-term effects, retaining the knowledge of previous tasks through the long-term effects, and facilitating the learning of future tasks through the short-term effects.
\subsection{Causal Representation Learning}
To recover the latent variable with identification guarantees **Hyv채rinen et al., "Independent Component Analysis"**, independent component analysis (ICA) has been used in a number of studies to determine the casual representation **Comon et al., "Independent Component Analysis, A New Concept?"**. 
Conventional approaches presuppose a linear mixing function for latent and observable variables. **Hyv채rinen et al., "Maximum Likelihood Estimation of Independent Source Distributions"**. However, determining the linear mixing function is a difficult problem in real-world situations. For the identifiability, many assumptions are made throughout the nonlinear ICA process, including the sparse generation process and the usage of auxiliary variables **Zhang et al., "Sparse Nonlinear ICA"**.
Specifically, Aapo et al.'s study confirms identifiability first. The exponential family is assumed to consist of latent sources in Ref. **Hyv채rinen et al., "Maximum Likelihood Estimation of Independent Source Distributions"**, where auxiliary variables such as domain indexes, time indexes, and class labels are added. Furthermore, Zhang et al.'s study **Zhang et al., "Sparse Nonlinear ICA"** demonstrates that the exponential family assumption is not necessary to accomplish component-wise identification for nonlinear ICA. \\
Sparsity assumptions were used in several study endeavors to attain identifiability without supervised signals **Lachapelle et al., "Causal Representation Learning with Sparsity Regularization"**. For example, Lachapelle et al. **Lachapelle et al., "Causal Representation Learning with Sparsity Regularization"** presented mechanism sparsity regularization as an inductive bias to find causative latent components. Zhang et al. selected the latent variable sparse structures in Ref. **Zhang et al., "Sparse Nonlinear ICA"** to achieve identifiability under distribution shift. Furthermore, nonlinear ICA was utilized in Ref. **Hyv채rinen et al., "Maximum Likelihood Estimation of Independent Source Distributions"** to get the identifiability of time series data. 
Premise and capitalization of variance variations across data segments based on separate sources were utilized in the study by Aapo et al. **Aapo et al., "Independent Component Analysis for Time Series Data"** to detect nonstationary time series data identifiability. Conversely, the latent variables in stationary time series data are found via permutation-based contrastive learning. Independent noise and variability history information features have been used more recently in TDRL **Santurkar et al., "TDRL: Temporal Dynamic Representations Learning"** and LEAP **Zhang et al., "LEAP: Latent Embeddings with Autoencoders for Online Prediction"**. \\
Simultaneously, latent variables were discovered by Song et al. **Song et al., "Causal Representation Learning without Domain Variables"** without the need to observe the domain variables. Imant et al. **Imant et al., "Multimodal Comparative Learning"** described multimodal comparative learning as having identifiability in terms of modality. Yao et al. **Yao et al., "Multi-Perspective Causal Representations"** postulated that multi-perspective causal representations can still be identified when there are incomplete observations. This paper uses multi-modality time series data and leverages historical variability information and multi-modality data fairness to demonstrate identifiability.

\clearpage
\appendix