\section{Related Works}
\subsection{Time Series Forecasting}
Recently, various research studies focused on time series forecasting problems, and deep learning-based methods have been very successful in this field. More precisely, the deep learning-based methods can be divided into several classes. First, the model based on RNN utilizes a recursive structure with memory to construct hidden layer transitions over time points . Second, TCN based approach to modeling hierarchical temporal patterns and extracting features using a shared convolutional kernel. Besides, there are simple but very effective methods as well, such as based on MLP  and on states-base-model  
Above these methods, the Transformer methods are especially outstanding and get the great process on the time series forecasting task \cite{kitaev2020reformer,liu2021pyraformer,wu2021autoformer,zhou2021informer}.
However, these existing methods are based on offline data processing, contrary to the mainstream ONLINE training methods of the significant data era. Since the above methods are unsuitable for direct application to online problems, a model that can be trained on online data and perform well is needed.
\subsection{Online Time Series Forecasting}
Due to the rapid increase in train data and the requirement for model updates online, online time series forecasting has become more popular than offline ones\cite{liu2016online,gultekin2018online,aydore2019dynamic}.
Online time series forecasting is a widely used technique in the real world due to the continuity of the data and the frequent drift of concepts. In this approach, the learning process occurs over a series of rounds. The model receives a look-back window, predicts the forecasting window, and then displays the valid values to improve the model's performance in the next round. Recently, a brunch of online time series forecasting work got excellent results, including considering gradient updates to optimize fast new as well as retained information \cite{pham2022learning} and models that consider both temporal and feature dimensions \cite{wen2024onenet}. Nevertheless, the fast adaptation and information retention of the models mentioned above are simultaneous. It needs to decouple the long and short term, which can lead to confounding results and suboptimal predictions. To solve this problem, the LSTD decouples the data first to isolate the long and short-term effects on the prediction, with the long-term effects being used to preserve the characteristics of the historical data and the short-term effects being used to quickly adapt to changes in the data for better online prediction.
\subsection{Continual Learning}
Continual learning is a novel topic and aims to set up intelligent agency by learning the sequence of tasks to perform with restricted access to experience \cite{lopez2017gradient}. A continual learning model must balance the knowledge of the current task and the prompt of the future learning process, as known in the stability-plasticity dilemma \cite{lin1992self,grossberg2013adaptive}. Due to their connection to how humans learn, several neuroscience frameworks have prompted the development of various continual learning algorithms. The continual learning model corresponds to the requirement of online time series forecasting. The constant learning can enable real-time updates upon receiving the new data to adapt the data dynamics better, improving the model accuracy.
The proposed LSTD incorporates continual learning into an online time series forecasting model, which mitigates the stability-plasticity problem by decoupling the long and short-term effects, retaining the knowledge of previous tasks through the long-term effects, and facilitating the learning of future tasks through the short-term effects.
\subsection{Causal Representation Learning}
To recover the latent variable with identification guarantees\cite{yao2023multi,scholkopf2021toward,liu2023causal,gresele2020incomplete}, independent component analysis (ICA) has been used in a number of studies to determine the casual representation \cite{rajendran2024learning,mansouri2023object,wendong2024causal}. 
Conventional approaches presuppose a linear mixing function for latent and observable variables. \cite{comon1994independent,hyvarinen2013independent,lee1998independent,zhang2007kernel}. However, determining the linear mixing function is a difficult problem in real-world situations. For the identifiability, many assumptions are made throughout the nonlinear ICA process, including the sparse generation process and the usage of auxiliary variables\cite{zheng2022identifiability,hyvarinen1999nonlinear,hyvarinen2024identifiability,khemakhem2020ice,li2023identifying}.\\
Specifically, Aapo et al.'s study confirms identifiability first. The exponential family is assumed to consist of latent sources in Ref. \cite{khemakhem2020variational,hyvarinen2016unsupervised,hyvarinen2017nonlinear, hyvarinen2019nonlinear}, where auxiliary variables such as domain indexes, time indexes, and class labels are added. Furthermore, Zhang et al.'s study \cite{kong2022partial, xie2023multi,kong2023identification,yan2024counterfactual} demonstrates that the exponential family assumption is not necessary to accomplish component-wise identification for nonlinear ICA. \\
Sparsity assumptions were used in several study endeavors to attain identifiability without supervised signals\cite{zheng2022identifiability,hyvarinen1999nonlinear,hyvarinen2024identifiability,khemakhem2020ice,li2023identifying}. For example, Lachapelle et al. \cite{lachapelle2023synergies, lachapelle2022partial} presented mechanism sparsity regularization as an inductive bias to find causative latent components. Zhang et al. selected the latent variable sparse structures in Ref. \cite{zhang2024causal} to achieve identifiability under distribution shift. Furthermore, nonlinear ICA was utilized in Ref. \cite{hyvarinen2016unsupervised,yan2024counterfactual,huang2023latent,halva2020hidden,lippe2022citris} to get the identifiability of time series data. 
Premise and capitalization of variance variations across data segments based on separate sources were utilized in the study by Aapo et al.\cite{hyvarinen2016unsupervised} to detect nonstationary time series data identifiability. Conversely, the latent variables in stationary time series data are found via permutation-based contrastive learning. Independent noise and variability history information features have been used more recently in TDRL \cite{yao2022temporally} and LEAP \cite{yao2021learning}. \\
Simultaneously, latent variables were discovered by Song et al. \cite{song2024temporally} without the need to observe the domain variables. Imant et al. \cite{daunhawer2023identifiability} described multimodal comparative learning as having identifiability in terms of modality. Yao et al.\cite{yao2023multi} postulated that multi-perspective causal representations can still be identified when there are incomplete observations. This paper uses multi-modality time series data and leverages historical variability information and multi-modality data fairness to demonstrate identifiability.


\clearpage
\appendix