\section{Methodology}
\label{sec:Methodology}
In this Section, we follow the two observations ($\mathbf{O}_1$\&$\mathbf{O}_2$) in Section~\ref{sec:Theoretical Analysis} and split \M into three stages: 
 
\begin{itemize}[leftmargin=*,noitemsep,topsep=2pt]
\item \textbf{Stage 1: Construct $\mathbf{D_{\text{ik}}}$ \& $\mathbf{D_{\text{idk}}}$}, which obtain $\mathbf{D_{\text{ik}}}$ \& $\mathbf{D_{\text{idk}}}$ by querying the internal state of LLMs and modifying the label of the incorrect set to `I don't know'.

\item \textbf{Stage 2: Dataset Construction based on  $\mathbf{O}_1$}, select \texttt{idk} samples from the first observation.

\item \textbf{Stage 3: Influence-directed Refusal-aware Instruction Tuning based on $\mathbf{O}_2$}, which allocates different weight when Refusal-aware Instruction Tuning from the second observation.
\end{itemize}
Figure~\ref{fig:method} presents a detailed overview of our proposed \M~framework, Algorithm~\ref{alg:RAI_data} details the overall process with subsequent subsections detailing each component.

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figure/method.pdf}
  \caption{\textbf{Overview of our framework.} \M contains three stages: 
  \textbf{(1)} Constructing datasets $\mathbf{D_{\text{ik}}}$ and $\mathbf{D_{\text{idk}}}$ by querying the internal state of LLMs.
  \textbf{(2)} Distilling the datasets to select \texttt{idk} samples based on the first observation $\mathbf{O}_1$.
  \textbf{(3)} Performing Influence-directed Refusal-aware Instruction Tuning using the second observation $\mathbf{O}_2$.
% \textbf{Step 1}: We construct .
% \textbf{Step 2}: We measure the correlation . 
% \textbf{Step 3}: tune.
  }
  \label{fig:method}
\end{figure*}

\subsection{Stage 1: Construct $\mathbf{D_{\text{ik}}}$ \& $\mathbf{D_{\text{idk}}}$}
In the first stage, we compute the accuracy \(\mathcal{C}(x)\) for each sample \(x\). Our research focuses on two distinct tasks: Multiple-Choice Question Answering (MCQA) and Open-Ended Question Answering (OEQA). For the MCQA task, we use the token probability of the ground truth to evaluate the correctness of each sample. In the OEQA task, the model generates answers \(N\) times (with \(N = 10\)) for each question, and we calculate accuracy based on the generated responses. 

Next, we construct the dataset \(\mathbf{D_{\text{ik}}}\) using samples with \(\mathcal{C}(x)\) of the correctness threshold $\mathcal{T}_{\mathcal{C}}$ or higher. For samples with \(\mathcal{C}(x)\) below $\mathcal{T}_{\mathcal{C}}$, we modify their output to ``I don't know'' and construct these samples into a separate dataset, \(\mathbf{D_{\text{idk}}}\).



\subsection{Stage 2: Dataset Construction based on  $\mathbf{O}_1$}
In this module, as outlined in Algorithm\ref{alg:RAI_data}, we select the most efficient data to train the model. \textbf{First, for the \texttt{idk} set $\mathbf{D_{\text{idk}}}$}, we identify the most impactful \texttt{idk} samples that significantly enhance the model's ability to refuse unknown questions based on the conclusion of $\mathbf{O}_1$. Specifically, we approximate \eqref{eq:loss_decomposition_short} and define \textbf{Refusal Influence} of each training sample on the model’s loss as:

\begin{equation} 
\label{eq} 
\small 
\mathcal{I}^{\text{ref}}(x) = \Bigl \langle \nabla \mathcal{L}(x; \theta), \mathbb{E}_{x^{o} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{o}_{\text{idk}}, y^{o}_{\text{idk}}; \theta) \right] \Bigr \rangle,
\end{equation}
where \(x\) denotes the training sample, \(\mathcal{L}\) is the loss function, and \(\theta\) represents the model parameters. This influence metric allows us to assess the contribution of each \texttt{idk} sample towards minimizing the incorrectness.
Building upon the influence scores, we employ a ranking strategy to select the most influential \texttt{idk} samples for fine-tuning:
\begin{equation}
\small
\label{eq:selected}
\mathbf{D_{\text{idk}}^{\text{selected}}} = \{ x \in \mathbf{D_{\text{idk}}} \mid \texttt{Rank}\bigl( \mathcal{I}^{\text{ref}}(x) \bigl) \leq N_{\text{idk}} \}.
\end{equation}
The selected \texttt{idk} samples, which exhibit the highest refusal influence, will form a distilled dataset used for targeted training.
Furthermore, to ensure computational efficiency, we apply two techniques following~\cite{Xia_Malladi_Gururangan_Arora_Chen} to construct valuable low-dimensional gradient features: parameter efficient fine-tuning via LoRA~\cite{hu2021lora} and random projections~\cite{projection}.

\textbf{Secondly, for the dataset $\mathbf{D_{\text{ik}}}$}, we select the subset of data with the highest accuracy. Previous work~\cite{ren2024learning_or_align} has demonstrated that using this subset for fine-tuning does not negatively impact the model's overall performance. The selection of this data can be represented as:
\begin{equation}
\small
\label{eq:selected}
\mathbf{D_{\text{ik}}^{\text{selected}}} = \{ x \in \mathbf{D_{\text{ik}}} \mid \texttt{Rank}\bigl( \mathcal{C}(x) \bigl) \leq N_{\text{ik}} \}.
\end{equation}
% 前文也得改一下C(x)怎么算

\subsection{Stage 3: Influence-directed Refusal-aware Instruction Tuning based on $\mathbf{O}_2$}
In this stage, we introduce an Influence-directed Refusal-aware Instruction Tuning based on the conclusions from $O_{2}$, which mentions that the issue of Over-Refusal is closely related to the difference in influence between training samples. The larger this difference, the more stable the accuracy of the model remains when learning the ability to reject. Therefore, we propose the concept of \textbf{Stable Influence} and assign each sample a weight \(\omega\), which is calculated as follows:
\begin{equation}
\small
\label{eq:loss_decomposition}
\begin{aligned}
\mathcal{I}^{\text{sta}}(x) = & \Bigl \langle \nabla \mathcal{L}(x; \theta), 
  \mathbb{E}_{x^{o} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{o}_{\text{idk}}, y^{o}_{\text{idk}}; \theta) \right] \\
  & - \mathbb{E}_{x^{o} \sim D_{\text{ik}}} \left[ \nabla \mathcal{L}(x^{o}_{\text{ik}}, y^{o}_{\text{idk}}; \theta) \right] 
  \Bigr \rangle,
\end{aligned}
\end{equation}
\vspace{-0.3cm}
\begin{equation}
\small
\label{eq:loss_decomposition}
\begin{aligned} 
  \omega(x^{o}) = & \frac{e^{\mathcal{I}^{\text{sta}}(x^{o}) / \tau}}{\mathbb{E}_{x^{o} \sim D^{\text{selected}}_{\text{idk}}} \left[ e^{\mathcal{I}^{\text{sta}}(x^{o}) / \tau} \right]}, 
\end{aligned}
\end{equation}
where \(x^{o}\) represents a training sample, and \(\tau\) refers to the temperature parameter. As \(\tau\) approaches 0, the differences in weight distribution become more pronounced, while as \(\tau\) approaches 1, the changes in weights become minimal. The constraint \(\sum \omega(x^o) = 1\) ensures that the model maintains its ability to reduce error rates.

Finally, during training, we apply a weight to the loss of the \texttt{idk} samples to mitigate over-refusal. The SFT loss is calculated as follows:

\begin{equation}
\small
\label{eq:loss_decomposition}
\begin{aligned}
\mathcal{L}_{SFT} = & \sum\limits_{(x^o, y^o) \in D^{\text{selected}}_{\text{idk}}} \omega(x^o) \mathcal{L}(x, y^o; \theta)\\
&+ \sum\limits_{(x^o, y^o) \in D^{\text{selected}}_{\text{ik}}} \mathcal{L}(x^o, y^o; \theta).
\end{aligned}
\end{equation}

\begin{algorithm}[t]
% \footnotesize
\caption{\M Process}
\label{alg:RAI_data}
\textbf{Input:}{~$D_{\text{src}} = \{x_{0}, x_{1}, ..., x_{N}\}$}, $\mathcal{T}_{\mathcal{C}}$, $\tau$ $N_{\text{ik}}$, $N_{\text{idk}}$ \\
\textbf{Output:}{~$D_{rait}$}
\begin{algorithmic}[1]
\State $D_{\text{ik}} = \{x \, | \, x \in D_{\text{src}},  \mathcal{C}(x) \geq \mathcal{T}_{\mathcal{C}}\}$
\State $D_{\text{idk}} = \{x \, | \, x \in D_{\text{src}},  \mathcal{C}(x) < \mathcal{T}_{\mathcal{C}}\}$
\State $\overline{g}_{\text{idk}}(D_{\text{idk}}) = \frac{1}{|D_{\text{idk}}|} \sum_{x \in D_{\text{idk}}} x.g_{\text{idk}}$
\State $\overline{g}_{\text{idk}}(D_{\text{ik}}) = \frac{1}{|D_{\text{ik}}|} \sum_{x \in D_{\text{ik}}} x.g_{\text{idk}}$
\For{$x_i$ in $D_{\text{idk}}$}
    \State $x_i.\mathcal{I}^{\text{ref}} = x_i.g_{\text{idk}} \cdot \overline{g}_{\text{idk}}(D_{\text{idk}})$
    \State $x_i.\mathcal{I}^{\text{sta}} = x_i.g_{\text{idk}} \cdot [\overline{g}_{\text{idk}}(D_{\text{idk}}) - \overline{g}_{\text{idk}}(D_{\text{ik}})]$
\EndFor
\State $D_{\text{ik}} = \text{sort}(D_{\text{ik}}, \text{key}=\mathcal{C}, \text{order=descend})$
\State $D_{\text{idk}} = \text{sort}(D_{\text{idk}}, \text{key}=\mathcal{I}^{\text{ref}}, \text{order=descend})$
\State $D_{\text{ik}}^{\text{selected}} = \text{TopK}(D_{\text{ik}}, N_{\text{ik}})$
\State $D_{\text{idk}}^{\text{selected}} = \text{TopK}(D_{\text{idk}}, N_{\text{idk}})$

\State $\text{Initialize: } Z = 0$ 
\For{$x_i$ in $D_{\text{idk}}^{\text{selected}}$}
    \State $Z = Z + e^{x_i.\mathcal{I}^{\text{sta}} / \tau}$
\EndFor

\For{$x_i$ in $D_{\text{idk}}^{\text{selected}}$}
    \State $x_i.\omega_i = \frac{e^{x_i.\mathcal{I}^{\text{sta}} / \tau}}{Z/|D_{\text{idk}}^{\text{selected}}|}$ 
\EndFor

\State $D_{\text{rait}} = D_{\text{ik}} \cup D_{\text{idk}}$
\State \textbf{return} $D_{\text{rait}}$
\end{algorithmic}
\end{algorithm}
