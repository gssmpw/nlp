\section{Introduction}
\label{sec:Introduction}
% p1: 先说大模型成功，会出现幻觉，所以重要性：然后为什么要做RAIT； 
Large Language Models (LLMs), including notable turbos like GPTs~\cite{ChatGPT,OpenAI2023GPT4TR} and LLaMA~\cite{touvron2023llama,dubey2024llama}, have achieved remarkable advances, demonstrating exceptional capabilities across a diverse range of downstream tasks~\cite{kaplan2020scaling,vu2024gptvoicetasker,achiam2023gpt,bai2024rat,jiang2024hykge,jiang2024tc}. Despite this success, critical challenges persist, particularly in the generation of \textit{\textbf{hallucinations}}—the models generate incorrect or fabricated information when confronted with unfamiliar or ambiguous queries~\cite{ji2023survey,dont_hallucinate_abstain,kang2024unfamiliar}, which ultimately limits the reliability and usefulness of LLMs.

\begin{figure}[t]
    \centering
    % \vspace{-1.5cm}
    \includegraphics[width=1.0\linewidth]{figure/intro.pdf}
    % \vspace{-0.74cm}
    \caption{Descriptions of \textbf{\textit{C1} \& \textit{C2}}. After RAIT, the initial LLM model will largely reject unknown questions to avoid errors. However, the overly conservative nature of RAIT also led to a decrease in accuracy.}
    % \vspace{-0.3cm}
    \label{fig:introduction}
\end{figure}


Ideally, the responsible LLM should decline to answer questions beyond its knowledge to minimize hallucinations~\cite{know_your_limits,survey_honesty}.
Recent studies~\cite{alignment_for_honesty, R_Tuning, rejection_improves, cheng2024can, bai2024efficient, zhu2024utilizeflowsteppingriver} have developed Refusal-Aware Instruction Tuning (RAIT), which constructs the refusal-aware dataset and employs Supervised Fine-Tuning (SFT)~\cite{dongabilities,Ouyang_Wu_Jiang_Almeida_Wainwright_Mishkin_Zhang_Agarwal_Slama_Ray_et,luo2024kuaiji} to teach models to appropriately decline responses.
Typically, refusal-aware datasets~\cite{R_Tuning,zhangdefending} categorize training samples into \texttt{ik} (correct) and \texttt{idk} (incorrect) groups based on response correctness. Samples with incorrect responses (\texttt{idk}) are treated as unknown knowledge and the answers are replaced with refusal responses like ``I don't know'', while the correct (\texttt{ik}) remain unchanged. Despite RAIT’s success in reducing hallucinations~\cite{R_Tuning,alignment_for_honesty,zhang2025amulet,wan2024mitigating}, studies like~\cite{varshney2023art} and~\cite{cheng2024can} highlight that models can become overly cautious, leading to over-refusals. 
Therefore, as shown in Figure \ref{fig:introduction} and Figure \ref{fig:case}, the RAIT should address the following two challenges \textit{\textbf{simultaneously}}:
\textbf{\textit{C1}. How to effectively reduce the hallucinations by refusing the unknown questions?}
\textbf{\textit{C2}. How to avoid over-refusal to ensure questions that can be correctly answered are not rejected?}

% 关于这个问题，我们的解决办法是什么？
To address these challenges, we introduce the \textbf{\underline{G}}radient-based \textbf{\underline{R}}efusal-\textbf{\underline{A}}ware \textbf{\underline{I}}nstruction \textbf{\underline{T}}uning Framework~(\textbf{\M}), which has several advantages over previous RAIT methods:
(1) Unlike prior methods that rely solely on the outputs of LLMs, \M utilizes gradients to achieve a more accurate representation of LLMs' internal knowledge states, ensuring better alignment between the constructed training samples and the LLMs' knowledge.
(2) Our gradient-based sample selection process is more efficient, achieving comparable training results with fewer samples \cite{Xia_Malladi_Gururangan_Arora_Chen, xu2024parenting}.
(3) By incorporating gradient information, we account for influences among training samples \cite{ren2024learningdynamics}, effectively minimizing sample conflicts \cite{zhu2024utilizeflowsteppingriver} and better addressing the aforementioned two challenges.

In practical implementation, we first derive two theoretical observations ($\mathbf{O_1}$ \& $\mathbf{O_2}$) by progressively addressing reduced inaccuracies (\textit{\textbf{C1}}) and mitigating over-refusal (\textit{\textbf{C2}}), which form the basis for designing \M. 
% 根据后文内容修正
Then in the framework designing: \ding{182} For \textbf{\textit{C1}}, we leverage the Refusal Influence formula within the \texttt{idk} set to select a small subset, enabling the LLMs to learn the refusal paradigm while filtering out inefficient samples.
\ding{183} For \textbf{\textit{C2}}, we implement an adaptive weighting method derived from the Stable Influence formula between the correct and incorrect sample sets, assigning varying sample weights during the RAIT phase to alleviate the issue of over-refusal. In summary, our contributions are as follows:

%contribution：1）从理论上，用涟漪效应结束ove-refuse的原因； 2）提出了一个正交化数据集的方法；  3）我们SOTA了
\begin{itemize}[leftmargin=*,noitemsep,topsep=2pt]
\item To the best of our knowledge, we are the first to conduct a theoretical analysis of the causes underlying the over-refusal phenomenon in LLMs.
\item The \M framework establishes a comprehensive workflow encompassing data construction and fine-tuning, utilizing two gradient-driven observations to enhance the model's refusal capability while effectively mitigating over-refusal.
\item Through extensive experimental evaluation on both open-ended question answering and multiple-choice tasks, we demonstrate that \M surpasses existing baselines by significantly reducing hallucination rates and enhancing overall performance.
\end{itemize}

\begin{figure}[t]
    \centering
    % \vspace{-1.5cm}
    \includegraphics[width=1.0\linewidth]{figure/case.pdf}
    % \vspace{-0.74cm}
    \caption{Case of mitigating hallucination and avoiding over-refusal.}
    % \vspace{-0.3cm}
    \label{fig:case}
\end{figure}


% 全文的组织线索=>intro（challenge）=> related work => observation => method => exp.
The paper is organized as follows. In Introduction (c.f. Section~\ref{sec:Introduction}), we present our research question and its corresponding two challenges (\textit{\textbf{C1}} \& \textit{\textbf{C2}}). The Related Work (c.f. Section~\ref{sec:Related Work}) provides a literature analysis concerning each challenge. In the Preliminary section (c.f. Section~\ref{sec:Preliminary}), we define the symbols and tasks relevant to \M. Following this, the Theoretical Analysis section (Section~\ref{sec:Theoretical Analysis}) derives two observations, $\mathbf{O_1}$ \& $\mathbf{O_2}$, in a progressive manner corresponding to \textit{\textbf{C1}} \& \textit{\textbf{C2}}. The Method section (Section~\ref{sec:Methodology}) aligns $\mathbf{O_1}$ with data construction and $\mathbf{O_2}$ with the RAIT phase. Section \ref{sec:Experiment} introduces the experiment settings and the experimental results. Finally, Section \ref{sec:Conclusion} concludes this paper and discusses future research directions.
