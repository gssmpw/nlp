\section{Appendix}
\label{sec:appendix}

\subsection{Theoretical Analysis Details}
\subsubsection{More Proof on $\mathbf{O}_1$}
\label{app:More Proof on O1}

\begin{equation}
\scriptsize
\setlength{\jot}{5pt} % 调整行距
\label{eq:loss_decomposition}
\begin{aligned}
&\mathbb{E}_{x^{u}_{\text{idk}} \sim D_{\text{idk}}} \left[ \Delta \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \right] \\
\equiv & -\eta_t \Bigl \langle  
\mathbb{E}_{x^{u}_{\text{idk}} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \right], \\
&\quad \mathbb{E}_{x^{o} \sim D} \left[ \nabla \mathcal{L}(x^{o}, y^{o}; \theta) \right] \Bigr \rangle + \mathcal{O}(\eta^2) \\ 
= & -\eta_t \Bigl \langle  
\mathbb{E}_{x^{u}_{\text{idk}} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \right], \\
&\quad \Bigl( \mathbb{E}_{x^{o}_{\text{ik}} \sim D_{\text{ik}}} \left[ \nabla \mathcal{L}(x^{o}_{\text{ik}}, y^{o}_{\text{ik}}; \theta) \right] + \\
&\quad \mathbb{E}_{x^{o}_{\text{idk}} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{o}_{\text{idk}}, y^{o}_{\text{idk}}; \theta) \right] 
\Bigr) \Bigr \rangle + \mathcal{O}(\eta^2) \\
\approx & -\eta_t \mathbb{E}_{x^{u}_{\text{idk}} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \right] \cdot \\
&\quad \mathbb{E}_{x^{o}_{\text{idk}} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{o}_{\text{idk}}, y^{o}_{\text{idk}}; \theta) \right] \\
= & -\eta_t 
\mathbb{E}_{(x^{u}_{\text{idk}},x^o_{\text{idk}}) \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \cdot \right. \\
&\quad \left. \nabla \mathcal{L} (x^{o}_{\text{idk}},y^{o}_{\text{idk}}; \theta) \right]. \\
& = - \mathbb{E}_{(x^{u}_{idk}, x^o_{idk}) \sim D_{idk}} \left[ \mathcal{I}(x^{o}_{idk}, y^{o}_{idk}, x^{u}_{idk}, y^{u}_{idk}; \theta) \right]
\end{aligned}
\end{equation}


\subsubsection{More Proof on $\mathbf{O}_2$}
\label{app:More Proof on O2}
RAIT can lead to the phenomenon of over-refusal, where the model refuses to answer questions it is inherently capable of addressing, thereby resulting in a decrease in accuracy. Accordingly, for an unlabeled input query $x_{ik}^u$, the output responses will shift from $y_{ik}$ to $y_{idk}$. Assuming the use of a symmetric loss function, the difference in the loss function values for the same input $x_{ik}$ with target labels $y_{ik}$ and $y_{idk}$ is approximately opposite in sign: $\Delta L(x_{ik}, y_{ik}) \approx -\Delta L(x_{ik}, y_{idk})$. Therefore, the proof is as follows:
\begin{equation}
\scriptsize
\setlength{\jot}{5pt} % 调整行距
\label{eq:loss_decomposition}
\begin{aligned}
&\mathbb{E}_{x^{u}_{\text{idk}} \sim D_{\text{idk}}} \left[ \Delta \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \right] 
+ \mathbb{E}_{x^{u}_{\text{ik}} \sim D_{\text{ik}}} \left[ \Delta \mathcal{L}(x^{u}_{\text{ik}}, y^{u}_{\text{ik}}; \theta) \right] \\ 
\approx & ~\mathbb{E}_{x^{u}_{\text{idk}} \sim D_{\text{idk}}} \left[ \Delta \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \right] 
- \mathbb{E}_{x^{u}_{\text{ik}} \sim D_{\text{ik}}} \left[ \Delta \mathcal{L}(x^{u}_{\text{ik}}, y^{u}_{\text{idk}}; \theta) \right] \\ 
\equiv & -\eta_t \Bigl \langle  
\mathbb{E}_{x^{u}_{\text{idk}} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \right], \\
&\quad \mathbb{E}_{x^{o} \sim D} \left[ \nabla \mathcal{L}(x^{o}, y^{o}; \theta) \right] \Bigr \rangle + \eta_t \Bigl \langle  
\mathbb{E}_{x^{u}_{\text{ik}} \sim D_{\text{ik}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{ik}}, y^{u}_{\text{idk}}; \theta) \right], \\
&\quad \mathbb{E}_{x^{o} \sim D} \left[ \nabla \mathcal{L}(x^{o}, y^{o}; \theta) \right] \Bigr \rangle + \mathcal{O}(\eta^2) \\ 
= & - \eta_t \Bigg\langle 
\Bigg\{ \mathbb{E}_{x^{u}_{\text{idk}} \sim D_{\text{idk}}} 
\left[ \nabla \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \right] \\
& \quad - \mathbb{E}_{x^{u}_{\text{ik}} \sim D_{\text{ik}}} 
\left[ \nabla \mathcal{L}(x^{u}_{\text{ik}}, y^{u}_{\text{ik}}; \theta) \right] 
\Bigg\}, \\ 
& \quad 
\Bigg\{ \mathbb{E}_{x^{o}_{\text{idk}} \sim D_{\text{idk}}} 
\left[ \nabla \mathcal{L}(x^{o}_{\text{idk}}, y^{o}_{\text{idk}}; \theta) \right] 
+ \mathbb{E}_{x^{o}_{\text{ik}} \sim D_{\text{ik}}} 
\left[ \nabla \mathcal{L}(x^{o}_{\text{ik}}, y^{o}_{\text{ik}}; \theta) \right] 
\Bigg\} 
\Bigg\rangle \\ 
& \quad + \mathcal{O}(\eta^2)
 \\
\approx & - \eta_t \left \langle  
\left \{
\mathbb{E}_{x^{u}_{\text{idk}} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \right] 
\right. \right. \\
& \quad \left. - \mathbb{E}_{x^{u}_{\text{ik}} \sim D_{\text{ik}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{ik}}, y^{u}_{\text{idk}}; \theta) \right] \right \},  \mathbb{E}_{x^{o}_{\text{idk}} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{o}_{\text{idk}}, y^{o}_{\text{idk}}; \theta) \right] \Big \rangle \\
= & -\eta_t \left \{
\mathbb{E}_{(x^{u}_{\text{idk}}, x^o_{\text{idk}}) \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{idk}}, y^{u}_{\text{idk}}; \theta) \cdot \nabla \mathcal{L}(x^{o}_{\text{idk}}, y^{o}_{\text{idk}}; \theta) \right] \right. \\
&\quad - \left.
\mathbb{E}_{x^{u}_{\text{ik}} \sim D_{\text{ik}}, x^{o}_{\text{idk}} \sim D_{\text{idk}}} \left[ \nabla \mathcal{L}(x^{u}_{\text{ik}}, y^{u}_{\text{idk}}; \theta) \cdot \nabla \mathcal{L}(x^{o}_{\text{idk}}, y^{o}_{\text{idk}}; \theta) \right] \right \} \\
= & - \left \{ \mathbb{E} _{x^{u} _{\text{idk}}, x^o _{\text{idk}} \sim D _{\text{idk}}} \left[ \mathcal{I}(x^{o} _{\text{idk}}, y^{o} _{\text{idk}}, x^{u} _{\text{idk}}, y^{u} _{\text{idk}}; \theta) \right] \right. \\
& \quad - \left . \mathbb{E} _{x^{u} _{\text{ik}} \sim D _{\text{ik}}, x^{o} _{\text{idk}} \sim D _{\text{idk}}} \left[ \mathcal{I}(x^{o} _{\text{idk}}, y^{o} _{\text{idk}}, x^{u} _{\text{ik}}, y^{u} _{\text{idk}}; \theta) \right] \right \}
\end{aligned}
\end{equation}

Here, the first approximation is transformed as follows:
\begin{itemize}[leftmargin=*]
    \item RAIT can lead to the phenomenon of over-refusal, where the model refuses to answer questions it is inherently capable of addressing, thereby resulting in a decrease in accuracy. Accordingly, for an unlabeled input query $x_{ik}^u$, the output responses will shift from $y_{ik}$ to $y_{idk}$.
    \item Assuming the use of a symmetric loss function, the difference in the loss function values for the same input $x_{ik}$ with target labels $y_{ik}$ and $y_{idk}$ is approximately opposite in sign: 
    $$
    \Delta \mathcal{L}(x_{ik}, y_{ik}) \approx -\Delta \mathcal{L}(x_{ik}, y_{idk}).
    $$
\end{itemize}



\subsection{Reasons for Choosing Influence Formula}
\label{app:Reasons for Choosing Influence Formula}
The reasons for choosing the influence formula of Equations (4) and (5) instead of the optimizing of Equation (2):
\begin{itemize}[leftmargin=*]
    \item The left-hand term, $\Delta \mathcal{L}$, in Equations (4) and (5) represents our optimization objective on the test set. Our goal is to identify the training samples that minimize this $\Delta \mathcal{L}$.
    \item \textbf{Computational complexity without approximation is $O(m \cdot n)$}: Directly computing the contribution of each training sample to $\Delta \mathcal{L}$ is computationally expensive. Assuming the training set size is $m$ and the test set size is $n$, we need to compute the loss change for \textbf{each test sample} after training on \textbf{each individual training sample}, with time complexity of $O(m \cdot n)$.
    \item \textbf{Computational complexity is reduced to $O(m + n)$ after approximation}: Inspired by \cite{Pruthi_Liu_Kale_Sundararajan_2020, Xia_Malladi_Gururangan_Arora_Chen}, we approximate $\Delta \mathcal{L}$ using the influence function. Using the influence function to approximate $\Delta \mathcal{L}$ only requires computing the gradients of each training sample and test sample without repeated calculations. Its time complexity is $O(m + n)$, which significantly reduces computational overhead. This approximation arises from omitting the higher-order terms in the Taylor expansion during the derivation of the influence function. 
\end{itemize}


\subsection{Orthogonal Experiment}
\label{subsec:orthogonal_experiment}
In our experiments, we observed that the gradient distributions for \texttt{idk} and \texttt{ik} are nearly orthogonal, as illustrated in Figure~\ref{fig:orth}. Through experiments conducted on the Llama2-7B-Chat model and the MMLU dataset, we found that the inner product distribution between \texttt{idk} gradients and \texttt{ik} gradients is centered around zero, with the computed value being
$
\Bigl \langle \mathbb{E}_{x } \left[ \nabla \mathcal{L}(x,y_{\text{idk}}; \theta) \right], 
\mathbb{E}_{x} \left[ \nabla \mathcal{L}(x,y_{\text{ik}}; \theta) \right] \Bigr \rangle = 0.008.
$
In contrast, the inner product of \textit{ik} with itself is significantly larger,
$
\Bigl \langle \mathbb{E}_{x } \left[ \nabla \mathcal{L}(x,y_{\text{ik}}; \theta) \right], 
\mathbb{E}_{x} \left[ \nabla \mathcal{L}(x,y_{\text{ik}}; \theta) \right] \Bigr \rangle = 0.103,
$
and the inner product of \texttt{idk} with itself is even greater,
$
\Bigl \langle \mathbb{E}_{x } \left[ \nabla \mathcal{L}(x,y_{\text{idk}}; \theta) \right], 
\mathbb{E}_{x} \left[ \nabla \mathcal{L}(x,y_{\text{idk}}; \theta) \right] \Bigr \rangle = 0.513.
$

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figure/orth.jpg}
  \caption{The overview of our proposed \M.
  }
  \label{fig:orth}
\end{figure*}

Our explanation for this phenomenon includes the following points:
\begin{itemize}[leftmargin=*]
    \item \texttt{ik} samples and \texttt{idk} samples train different capabilities of the LLM: From the perspective of the internal knowledge of the LLM, \texttt{ik} samples help the LLM ``\textbf{learn knowledge}'', while \texttt{idk} samples grant the LLM the ability to ``\textbf{reflect on self-knowledge}'', that is, to predict the boundaries of its own knowledge.
    \item \textbf{The different capabilities of the LLM are usually associated with different regions or activation patterns of the transformer}: For example, \citep{dai2022knowledgeneuronspretrainedtransformers,yu2024neuronlevelknowledgeattributionlarge} indicates that knowledge is primarily stored in specific neurons of the LLM, while some studies show that different attention heads perform different functions, such as the Successor Head~\cite{gould2023successorheadsrecurringinterpretable} and the Induction Head~\cite{ren2024identifyingsemanticinductionheads}. Additionally, the multilingual capabilities of the LLM mainly depend on the layers near the input and output ends of the transformer, rather than the middle layers~\cite{wendler2024llamasworkenglishlatent}.
    \item The abilities to ``learn knowledge'' (\texttt{ik} samples) and ``reflect on self-knowledge'' (\texttt{idk} samples) correspond to different regions or activation patterns of the LLM transformer. Therefore, during the training process, the gradients of \texttt{ik} samples and \texttt{idk} samples act on different regions or activation patterns of the transformer respectively. This helps explain why the gradient distributions of \texttt{ik} samples and \texttt{idk} samples exhibit a state of near orthogonality.
\end{itemize}

\subsection{Prompts in \M}
\label{app:imple}
\subsubsection{Prompts for Getting Correctness.}
Prompts for getting correctness on MMLU and TriviaQA datasets are shown in Table 4 and Table 5. They use 5-shot and 3-shot settings respectively.

\input{table/prompt_kq_MMLU}
\input{table/prompt_kq_triviaqa}

\subsubsection{Prompts for training.}
For the \textit{Van-Tuning}, we use the \textit{basic} prompt as shown in Table 4 and Table 5 without in-context example. All other experiments use the \textit{refuse} prompt as shown in Table 6 and Table 7. Loss is only computed on the target answer $\{\text{answer}_\text{rait}\}$.

\input{table/prompt_train_REFUSE_MMLU}
\input{table/prompt_train_REFUSE_TriviaQA}

\subsubsection{Prompts for evaluation.}
The \textit{Init-Basic} method uses the original question format for evaluation, without any prior instructions. For the other methods, the evaluation prompts are shown in Tables 8 and 9.

\input{table/prompt_eval_REFUSE_MMLU}
\input{table/prompt_eval_REFUSE_TriviaQA}


\subsection{Reasons for Choosing Few-Shot Setting}
\label{A5}
We use few-shot prompting to calculate the correctness of LLMs, aiming to ensure that \textbf{LLMs strictly follow instructions to achieve the following objectives}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Easy-to-parse response}: We extract answers from the response using rules and then compare them with the gt to calculate correctness. Both research~\cite{xfinder} and our practice show that accurately extracting answers is challenging. Therefore, we provide examples of easy-to-parse answers through few-shot examples to ensure compliance with instructions.
    \item \textbf{Clear answers}: Existing chat LLMs often have a certain level of self-awareness and may choose not to answer or give vague responses like ``I'm not sure'' or ``for reference only'' when facing uncertain questions. However, we expect the LLM to answer the query directly, regardless of whether the answer is correct.
    \item \textbf{Concise answers to reduce inference costs}: In a 0-shot scenario, LLMs tend to produce longer responses (chain of thought processes or detailed descriptions). Using few-shot prompts helps obtain concise answers.
\end{itemize}
\textbf{Although descriptive instructions can be added to 0-shot prompts to require LLMs to meet the above standards, LLMs generally do not strictly follow them.} Furthermore, we believe that \textbf{the risk of introducing knowledge by few-shot prompting is minimal}. When choosing few-shot samples, we select from \textbf{different dataset splits}: for MCQA, test samples are from the MMLU test split and few-shot samples from the val split; for OEQA, test samples are from the triviaqa train split, with few-shot samples from the dev split.

\subsection{Sensitive Experiment}
\label{A6}
\textbf{Effect of temprature on \texttt{idk} samples' weight.}
We analyzed the Stable Influence of the samples discussed in Stage 3 of \M and found that their values were relatively small. As a result, it was necessary to adjust the temperature to better normalize the weight assigned to each sample. To explore the specific effects of these adjustments, we conducted experiments using the MMLU dataset and the LLaMA3-8B-Instruct model. As shown in Table~\ref{tab:tem_results}, the weight of the \texttt{idk} samples only becomes effective when \( \tau \) is within a reasonable range. When \( \tau \) is set to 1, the weights of all samples remain almost unchanged, being equal to 1.

\input{latex/8_Appendix_Tempture_Table}

\input{latex/8_Appendix_threshold_Table}
\textbf{Effect of correctness threshold.}
In our experiments, we set the correctness threshold \( \mathcal{T}_{\mathcal{C}} \) to 0.5. We conducted detailed experiments to determine this threshold, and as shown in Table~\ref{tab:ths_tc_results}, the model's performance is not highly sensitive to the choice of \( \mathcal{T}_{\mathcal{C}} \). Within a reasonable range, our method consistently delivers strong results. All experiments were performed on the MMLU dataset using the LLaMA3-8B-Instruct model.
