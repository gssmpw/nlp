\section{Related Work}
\paragraph{U-Net and Its Variants}  
U-Net~\cite{unet} introduced skip connections to bridge low-level and high-level features, significantly improving segmentation performance. U-Net++~\cite{unet++} further enhanced this design by incorporating dense connections and deep supervision to refine feature aggregation. AHF-U-Net~\cite{AHF-U-Net} employed attention mechanisms similar to SE~\cite{hu2018squeeze} and CBAM~\cite{woo2018cbam} to compute importance coefficients for fusion connections, enhancing feature selection adaptively. U-Net Transformer~\cite{U-NetTransformer} integrated self-attention in the bottleneck and cross-attention in skip connections.

\paragraph{Explainability Methods}  
Grad-CAM and its variants~\cite{wang2020score,chattopadhay2018grad,selvaraju2017grad} generate saliency maps by weighting feature maps based on gradients, second-order derivatives, or model confidence. SHAP~\cite{lundberg2017unified} estimates feature importance by perturbing inputs and computing Shapley values. LIME~\cite{ribeiro2016whyitrustyou} approximates local decision boundaries by training interpretable linear models around the input space. MHEX (Multi-Head Explainer)~\cite{mhex} enhances both performance and explainability through lightweight modules and fine-tuning while also providing uncertainty estimation.  

\paragraph{Uncertainty Estimation}  
Subjective Logic (SL)~\cite{subjectiveLogic} and Dempster-Shafer Theory (DST)~\cite{DempsterShafer} encourage models to quantify uncertainty in predictions. Monte Carlo Dropout (MC Dropout)~\cite{gal2016dropout} estimates uncertainty by enabling dropout at inference time. Deep Ensembles~\cite{lakshminarayanan2017simplescalablepredictiveuncertainty} aggregate predictions from multiple independently trained models to improve robustness. Test-Time Augmentation (TTA)~\cite{TTA} leverages augmented inputs during inference to indirectly reflect model uncertainty. MHEX\cite{mhex} introduces an alternative uncertainty estimation approach by analyzing gradient consistency.