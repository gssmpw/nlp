\section{Related Work}
\paragraph{U-Net and Its Variants}  
U-Net____ introduced skip connections to bridge low-level and high-level features, significantly improving segmentation performance. U-Net++____ further enhanced this design by incorporating dense connections and deep supervision to refine feature aggregation. AHF-U-Net____ employed attention mechanisms similar to SE____ and CBAM____ to compute importance coefficients for fusion connections, enhancing feature selection adaptively. U-Net Transformer____ integrated self-attention in the bottleneck and cross-attention in skip connections.

\paragraph{Explainability Methods}  
Grad-CAM and its variants____ generate saliency maps by weighting feature maps based on gradients, second-order derivatives, or model confidence. SHAP____ estimates feature importance by perturbing inputs and computing Shapley values. LIME____ approximates local decision boundaries by training interpretable linear models around the input space. MHEX (Multi-Head Explainer)____ enhances both performance and explainability through lightweight modules and fine-tuning while also providing uncertainty estimation.  

\paragraph{Uncertainty Estimation}  
Subjective Logic (SL)____ and Dempster-Shafer Theory (DST)____ encourage models to quantify uncertainty in predictions. Monte Carlo Dropout (MC Dropout)____ estimates uncertainty by enabling dropout at inference time. Deep Ensembles____ aggregate predictions from multiple independently trained models to improve robustness. Test-Time Augmentation (TTA)____ leverages augmented inputs during inference to indirectly reflect model uncertainty. MHEX____ introduces an alternative uncertainty estimation approach by analyzing gradient consistency.