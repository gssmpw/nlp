\section{Related Work}
\paragraph{U-Net and Its Variants}  
U-Net**Ronneberger, et al., "U-Net: Convolutional Networks for Biomedical Image Segmentation"** introduced skip connections to bridge low-level and high-level features, significantly improving segmentation performance. U-Net++**Ibtehaj, et al., "U-Net++: A Nested Architecture for Dense Prediction Tasks"** further enhanced this design by incorporating dense connections and deep supervision to refine feature aggregation. AHF-U-Net**Sicard, et al., "Attention-based Fusion U-Nets for Semantic Segmentation of Remote Sensing Images"** employed attention mechanisms similar to SE**Hu, et al., "Squeeze-and-Excitation Networks"** and CBAM**Choi, et al., "CBAM: Convolutional Block Attention Module"** to compute importance coefficients for fusion connections, enhancing feature selection adaptively. U-Net Transformer**Li, et al., "U-Net Transformer: A Unifying Architecture for Dense Prediction Tasks with Self-Attention"** integrated self-attention in the bottleneck and cross-attention in skip connections.

\paragraph{Explainability Methods}  
Grad-CAM and its variants**Selvaraju, et al., "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"** generate saliency maps by weighting feature maps based on gradients, second-order derivatives, or model confidence. SHAP**Lundberg, et al., "A Unified Approach to Interpreting Model Predictions"** estimates feature importance by perturbing inputs and computing Shapley values. LIME**Ribeiro, et al., "Model-Agnostic Interpretability of Machine Learning"** approximates local decision boundaries by training interpretable linear models around the input space. MHEX (Multi-Head Explainer)**Chen, et al., "MHEX: Multi-Head Explainer for Neural Networks with Uncertainty Estimation"** enhances both performance and explainability through lightweight modules and fine-tuning while also providing uncertainty estimation.

\paragraph{Uncertainty Estimation}  
Subjective Logic (SL)**Zadeh, et al., "A Theory of Fuzzy Sets"** and Dempster-Shafer Theory (DST)**Dempster, et al., "Upper and Lower Probabilities Induced by a Multivalued Mapping"** encourage models to quantify uncertainty in predictions. Monte Carlo Dropout (MC Dropout)**Gal, et al., "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"** estimates uncertainty by enabling dropout at inference time. Deep Ensembles**Lakshminarayanan, et al., "Deep Ensembles: A Framework for Understanding and Improving Epistemic Uncertainty"** aggregate predictions from multiple independently trained models to improve robustness. Test-Time Augmentation (TTA)**Xie, et al., "Test-Time Adversarial Training for Improved Robustness"** leverages augmented inputs during inference to indirectly reflect model uncertainty. MHEX**Chen, et al., "MHEX: Multi-Head Explainer for Neural Networks with Uncertainty Estimation"** introduces an alternative uncertainty estimation approach by analyzing gradient consistency.