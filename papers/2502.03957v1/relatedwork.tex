\section{Related Work}
\begin{figure*}[t]
\centering
\includegraphics[width=.96\textwidth]{fig/pipeline.png}
\caption{The processing pipeline of the proposed explanation approach; dashed lines indicate iterative processes. (Input Deepfake Image source: FaceForensics++ dataset).}
\label{fig:pipeline}
\end{figure*}

Despite the ongoing interest in developing AI-based models for detecting deepfake images and videos, the explanation of the output of these models has been investigated only to a small extent, thus far. In one of the first attempts towards explaining a deepfake detector's decision, Malolan \etal \cite{9092227}, examined the use of the LIME \cite{10.1145/2939672.2939778} and LRP \cite{10.1371/journal.pone.0130140} methods to produce visual explanations (in the form of heatmaps) about the output of an XceptionNet-based \cite{8099678} detector that was trained on a subset of the FaceForensics++ dataset \cite{9010912}. Pino \etal \cite{DBLP:journals/corr/abs-2105-05902} employed adaptations of SHAP \cite{10.5555/3295222.3295230}, Grad-CAM \cite{8237336} and self-attention methods, to explain the output of deepfake detectors (based on EfficientNet-B4 and B7 \cite{tan2020efficientnet}) that were trained on the DFDC dataset \cite{DFDC2020}. Silva \etal \cite{SILVA2022100217}, applied the Grad-CAM method \cite{8237336} and focused on the computed gradients for the attention map to spot the image regions that influence the most the predictions of an ensemble of CNNs (XceptionNet \cite{8099678}, EfficientNet-B3 \cite{DBLP:conf/icml/TanL19}) and attention-based models for deepfake detection. Xu \etal \cite{9707568}, utilized learned features to explain the detection performance of a trained linear deepfake detector on the FaceForensics++ dataset \cite{9010912}, with the help of heatmap visualizations and uniform manifold approximation and projection. Jayakumar \etal \cite{9993294}, employed the Anchors \cite{10.5555/3504035.3504222} and LIME \cite{10.1145/2939672.2939778} methods to produce visual explanations for a trained model (based on EfficientNet-B0 \cite{DBLP:conf/icml/TanL19}) that detects five different types of deepfakes. Aghasanli \etal \cite{10350382}, used SVM and xDNN \cite{ANGELOV2020185} classifiers to understand the behavior of a Transformer-based deepfake detector. Finally, Jia \etal \cite{Jia_2024_CVPR} examined the capacity of multimodal LLMs in detecting deepfakes and providing textual explanations about their decisions.

In terms of evaluation, most of the aforementioned works evaluate the produced explanations qualitatively using a small set of samples \cite{9092227,9707568,SILVA2022100217,9993294,10350382,Jia_2024_CVPR}. Towards a quantitative, and thus more objective, evaluation, Pino \etal \cite{DBLP:journals/corr/abs-2105-05902} used tailored metrics that relate to low-level features of the obtained visual explanations, such as inter/intra-frame consistency, variance and centredness. Gowrisankar \etal \cite{GOWRISANKAR2024103684} described a framework that applies a number of adversarial attacks, adding noise in regions of a deepfake image that correspond to the spotted salient regions after explaining the (correct) classification of its real counterpart, and evaluates the performance of an explanation method based on the observed drop in the detector's accuracy. Building on \cite{GOWRISANKAR2024103684}, Tsigos \etal \cite{10.1145/3643491.3660292} proposed a simpler evaluation framework, which uses the produced explanation after detecting a deepfake image and does not require access to its original counterpart. Finally, the ROAD framework \cite{pmlr-v162-rong22a} for evaluating attribution-based explanation methods for image classifiers, applies a similar noise-based imputation strategy. Nevertheless, the efficiency of the applied imputation has not been investigated thus far on deepfake classifiers. 

Our paper is most closely related with works that utilize \cite{9092227,9993294} or evaluate \cite{GOWRISANKAR2024103684,10.1145/3643491.3660292} perturbation-based explanation methods. Nevertheless, contrary to these works, we propose the use of adversarially-generated perturbation masks and investigate their impact on inferring the importance of input features using modified versions of four SOTA perturbation-based explanation methods (RISE \cite{DBLP:conf/bmvc/PetsiukDS18}, SHAP \cite{10.5555/3295222.3295230}, LIME \cite{10.1145/2939672.2939778}, SOBOL \cite{fel2021sobol}). Moreover, in contrast to \cite{9092227,9707568,SILVA2022100217,9993294,10350382}, that evaluate explanations only qualitatively using a small set of samples, we assess the performance of explanation methods also using a quantitative evaluation framework.