
\section{Prior Work}

Intent understanding and comparison have been widely studied in natural language processing (NLP) and human-computer interaction (HCI). Traditional approaches rely on text similarity metrics such as BLEU \cite{papineni2002bleu} and ROUGE \cite{lin2004rouge}, which measure lexical overlap but struggle to capture semantic equivalence. More advanced embedding-based methods, such as BERTScore \cite{zhang2019bertscore}, improve upon this by leveraging contextualized representations, though they may still fail to distinguish functional differences in intent.

Beyond text-based methods, Natural Language Inference (NLI) models \cite{bowman2015large} have been employed to determine logical relationships between intents, but their effectiveness in UI-based intent comparison remains limited. Recent work has also introduced \textit{automated evaluation frameworks} \cite{berkovitch2024identifying} that leverage heuristic and learned scoring methods to assess intent similarity, though challenges remain in aligning automated metrics with human judgments.

In UI-based tasks, datasets like Mind2Web \cite{mind2web2024} and AitW \cite{aitw2024} provide benchmarks for evaluating agent performance in structured web and mobile environments. However, comparing intents within these datasets remains an open challenge, as standard NLP metrics fail to capture the nuances of user trajectories and execution paths. Our work builds upon these efforts by introducing a more structured, fact-based comparison metric designed specifically for downstream UI automation and memory-based retrieval tasks.
