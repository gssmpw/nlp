
\subsection{Dataset and Annotation}

We annotated a dataset of intent pairs (gold and predicted) labeling whether they are likely to produce similar UI trajectories and achieve the same goal.  Examples include "Play the top rated movie" vs. "Play The Shawshank Redemption" (different trajectories) and "Show top rated hotels" vs. "Show 5 star rated hotels" (similar trajectories).  A smaller dataset was also labeled for fact correctness. The full dataset has 2000 examples, annotated by 6 raters with a consolidated optimal label. 1000 examples have a consolidated label based on 7 labels (6 raters + scratch), and another 1000 examples have a consolidated label based on 6 raters. Out of these, 138 examples have expert annotation (Tal).

\section{Data Contribution}

We contribute a higher-quality subset of SCRATCH, based on manual annotation, and an improved, automatically cleaned version.  We provide detailed annotation guidelines and methodology, especially for cases where gold intents are not available upfront.  We also explore synthetic gold generation via automatic consolidation.


\subsection{Evaluation Methodology}

We evaluate AutoFact in two main experiments:

\subsubsection{Experiment 1: Evaluation Methodology}

We compare AutoFact to other metrics using a full-match labeled dataset. We compare AutoEval Full Match prompt, double-sided NLI, and AutoFact using F1-score and AUC-ROC.

\subsubsection{Experiment 2: Optimal Annotation Strategy}

We evaluate AutoFact's performance on datasets with different annotation strategies: cleaned Scratch, annotations from individual raters, single prompt optimal annotation, pyramid method, Socratic method, self-critique, 3 random raters, and the best 3 raters.
