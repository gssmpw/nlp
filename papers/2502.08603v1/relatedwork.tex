\section{Related work}
There is a large body of theoretical research on NGD~\cite{amari1998natural, martens2020new, bottou2018optimization} arguing that it requires fewer iterations than SGD to converge to the same value of the loss in specific settings. K-FAC~\cite{martens2015optimizing} aims to reduce this complexity and invokes a block-wise approximation of the curvature matrix, which may not always hold. While first introduced for multi-layer perceptrons, K-FAC has been applied to more complex architectures, such as recurrent neural networks~\cite{martens2018kronecker} and transformers~\cite{eschenhagen2024kronecker}, where additional approximations have to be made and where the associated computational overhead can vary. While the K-FAC approximation is uncontrolled, there is a large body of empirical evidence showing its faster convergence per step with respect to Adam and its variants~\cite{martens2015optimizing, martens2018kronecker, eschenhagen2024kronecker, ren2019efficient,gargiani2020promise}. It has also been shown that K-FAC extends the critical batch size for a variety of tasks~\cite{zhang2019algorithmic}, decreasing the diminishing returns usually seen by scaling up the batch size in neural network training.

However, because of the per-step overhead of K-FAC, it remains roughly on-par with first-order methods~\cite{eschenhagen2024kronecker} in terms of per-wall clock time performance. In this work we focus on reducing the runtime per step of the K-FAC optimizer, which directly makes it more competitive. 





At the core of our approach are thermodynamic algorithms~\cite{aifer2024_TLA} for solving linear systems and inverting matrices. We remark that alternative analog methods for solving these kinds of problems can be found in Refs.~\cite{sun2019solving,
sun2020time}. In addition, alternative approaches to thermodynamic computing have been proposed~\cite{hylton2020thermodynamic,ganesh2017thermodynamic,lipka2024thermodynamic,whitelam2024thermodynamic}, applications beyond linear algebra have explored such as Bayesian inference~\cite{aifer2024_TBI} and quadratic programming~\cite{bartosik2024thermodynamic}, and closely related to thermodynamic computing is probabilistic computing~\cite{aadit2022massively,kaiser2022life} and reversible computing~\cite{frank2020reversible}. 


While several approaches have been proposed to accelerate training of AI models using novel hardware, these efforts typically aim at reducing the constant coefficients appearing in the time cost of computation. For example, analog computing devices have been proposed to achieve reduced
time and energy costs of training relative to available digital technology~\cite{kim2017analog,ambrogio2018equivalent,cristiano2018perspective,aguirre2024hardware}. These devices are generally limited to training a neural network that has a specific architecture (corresponding to the
structure of the analog device). 

\begin{figure*}[t]%
\centering
% \includegraphics[width=0.7\textwidth]{figures/Flowchart.png}
\scalebox{1}{\input{figures/scheme.tikz}}
\caption{\justifying \textbf{Overview of the thermodynamic algorithm for K-FAC.} On the left is shown a two-layer neural network with weight matrices $W_1$ and $W_2$ and activations $a_1, a_2, a_3$ that are stored on a digital device. From these quantities Kronecker factors $A_\ell$ and $B_\ell$ are computed and sent to the thermodynamic solver, which inverts them or solves a linear system where they enter as the positive semi-definite matrix. Then, the result is sent back to the digital device and the weights are updated. Note that this algorithm is easily parallelized, e.g., many thermodynamic solvers can be used to compute the K-FAC update rule (Eq.~\eqref{eq:K-FAC_update}) for one or more layers each. 
}\label{fig:scheme}
\end{figure*}


A strength of our approach is its flexibility with respect to model architecture. Although this same strength appeared in the Thermodynamic NGD algorithm of Ref.~\cite{donatella2024thermodynamic}, that algorithm would either require (1) a large-scale hardware (with a number of physical components scaling linearly with the number of model parameters) for which important scalability challenges have yet to be solved, or (2) restricting the training tasks only to fine-tuning. In this sense, Ref.~\cite{donatella2024thermodynamic} did not fully solve the issue of training large-scale foundational models. Thus, a key insight of the current paper is to make the training of large-scale AI models practical and scalable for thermodynamic hardware. Moreover, our complexity analysis (Table~\ref{tab:complexities}) suggests that the per-iteration complexity of K-FAC can be made similar to that of a first-order training method.