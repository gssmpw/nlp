%-----------------------------------------------------------------------
\documentclass[final,3p,times]{elsarticle}
%-----------------------------------------------------------------------
\usepackage[latin1]{inputenc}
%\usepackage{comment}  % Required for the comment environment
\usepackage{amsfonts,amssymb,amsmath,exscale} % math packages
%\usepackage[dvips]{graphicx,color,psfrag}     % graphic packages
\usepackage{textcomp}
\usepackage{pifont}
\usepackage{rotating}
\usepackage{wasysym}
\usepackage{defcml_fonts}

\setcounter{tocdepth}{4} 
\setcounter{secnumdepth}{4}

\usepackage{graphicx,dblfloatfix}
\usepackage{color}
\usepackage{amsfonts,amssymb,amsmath}
\usepackage{psfrag}
\usepackage[belowskip=2pt]{caption}
\usepackage{booktabs}
\setlength{\captionmargin}{0.05\textwidth}

\usepackage[numbers]{natbib}
\usepackage{tikz}                              % tikz is used for drawing inside latex.
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}

\renewcommand{\topfraction}      {1}% Seitenanteil von Objekten oben
\renewcommand{\bottomfraction}   {1}% Seitenanteil von Objekten unten
\renewcommand{\textfraction}  {0.01}% Mindestanteil Text pro Seite

%-----------------------------------------------------------------------
% Neue definitionen
%-----------------------------------------------------------------------
\usepackage{dsfont}
%
\newcommand{\macro}{\overline}%
\newcommand{\micro}[1]{{\stackrel{\vspace{-2.0mm}{\displaystyle\sim}}{\mathnormal{#1}}}}%
\newcommand{\microsub}[1]{{\stackrel{\vspace{-2.0mm}{\sim}}{\mathnormal{#1}}}}%
\newcommand{\ltilde}[1]{{\stackrel{\vspace{-2.0mm}{\displaystyle\sim}}{#1}}}
\DeclareRobustCommand{\BfrakF}{{\Bgothic F}}
\DeclareRobustCommand{\BfrakP}{{\Bgothic P}}
\DeclareRobustCommand{\BfrakR}{{\Bgothic R}}
\DeclareRobustCommand{\Bfrakp}{{\Bgothic p}}
\DeclareRobustCommand{\frakf}{{\gothic f}}
\DeclareRobustCommand{\Bfrakn}{{\Bgothic n}}
\DeclareRobustCommand{\BfrakS}{{\Bgothic S}}
\newcommand\exceq{\stackrel{\smash{\scriptscriptstyle\mathrm{!}}}{=}}
\newcommand{\tensor}[2]{\left[\begin{array}{#1}#2\end{array}\right]}
\newcommand\assembly{\raisebox{-1ex}{\huge\textsf{A}}}


\journal{Computational Mechanics - Special Issue (Volume I: Multiscale Mechanics of Materials)}

%**********************************************************************
\begin{document}

\begin{frontmatter}

\title{An Enhanced Deep Learning Approach for Vascular Wall Fracture Analysis}

\author{Fadi Aldakheel\corref{cor}\(^{a}\)}
\ead{fadi.aldakheel@ibnm.uni-hannover.de}
\ead[url]{https://orcid.org/0000-0003-4074-4576}
\cortext[cor]{Corresponding author.} 
%
\author{Alexandros Tragoudas\(^{a}\)}
%
\author{Marta Alloisio\(^{b}\)}
%
\author{Elsayed S. Elsayed\(^{a}\)}
%
\author{T. Christian Gasser\(^{b}\)}
%

\address{ \(^a\) Institute of Mechanics and Computational Mechanics, Leibniz Universit\"at Hannover, Appelstrasse 9a, 30167 Hannover, Germany} 

\address{ \(^b\) Solid Mechanics, Department of Engineering Mechanics, KTH Royal Institute of Technology, Sweden} 

\begin{abstract}
%
Vascular disease remains the primary cause of death globally to this day. Tissue damage in these vascular disorders is closely tied to how the diseases develop, requiring careful study. Therefore, the scientific community has dedicated significant efforts to capture the properties of vessel wall fractures. The symmetry-constrained Compact Tension (symconCT) test and combined with Digital Image Correlation (DIC), enabled the study of tissue fracture in various aorta specimens under different conditions. The purpose of this experimental data was to support the development and verification of computational models aimed at capturing the material and fracture properties of vascular tissue.

Traditionally, the analysis of fracture processes in biological tissues involves extensive
computational and experimental efforts due to the complex nature of tissue behavior under
stress. These high costs have posed significant challenges, demanding efficient solutions to
accelerate research progress.

Deep learning techniques have shown promise in streamlining this process by leveraging
neural networks to learn intricate patterns and relationships from datasets [4,5]. In this study,
we integrate deep learning methodologies with the U p -Net architecture[3] to predict fracture
responses in porcine aorta specimens. By training the network on a comprehensive datasetof fracture behaviors and associated parameters, the model learns to capture the intricate
interplay of factors influencing fracture progression. These parameterized datasets consist
of pictures describing the evolution of tissue fracture path along with the DIC
measurements and simulation data. The integration of U p -Net not only enhances predictive
accuracy but also significantly reduces the computational/experimental burden, thereby
enabling a more streamlined and efficient analysis of fracture processes.

\bigskip
\bigskip

\begin{comment}
Material modeling using modern numerical methods accelerates the design process and reduces the costs of developing new products. However, for multiscale modeling of heterogeneous materials, the well-established {\it homogenization techniques} remain  {\it  computationally expensive} for high accuracy levels. In this contribution, a machine learning approach, Convolutional 
Neural Networks (CNNs), is proposed as a computationally  efficient solution method that is capable of  providing  a high level of accuracy. In this work,  the data-set used for the training process, as well as the numerical tests, consists of artificial/real microstructural  images (``input''). Whereas, the output is the homogenized stress of a given representative volume element $\cal{RVE}$. The model performance is demonstrated by means of examples and compared with traditional homogenization methods. As the examples illustrate, high accuracy in predicting the homogenized stresses, along with a significant reduction in the computation time, were achieved using the developed CNN model.
\end{comment}

\end{abstract}


\begin{keyword}
Deep Learning; Convolutional Neural Networks; Computational Micro-to-Macro Approach; Heterogeneous Materials.
\end{keyword}

\end{frontmatter}


%=======================================================================
\section{Introduction}
\label{sec1-CNN}
%=======================================================================

\begin{comment} %---------------------------------------
Engineering design is an optimization process in which engineers identify and solve problems. The key elements of product design are functionality, cost, form, material properties, and behavior. In this context, heterogeneous materials that consist of different constituents such as polymer matrix composites, metallic-materials, wood, concrete, and ceramic composites, play a major role in the optimization process. Such advanced materials have been in use for a few decades in the automotive, aerospace industry, and constructions due to their superior properties, compared to their homogeneous counterparts. As the macroscopic properties of such heterogeneous materials are greatly influenced by their micro-constituents properties, the understanding of the microstructure of such materials is an essential task to fully utilize their potential in the design process. In this regard, the direct use of the classical multiscale techniques for solving such problems is unfeasible due to the huge computational costs. Thus, a need  for a computationally less expensive approach exists. One of the current possibilities introduced in literature is the employment of data-driven models to reduce the computation costs \cite{nguyen2018data,eggersmann2019model,carrara2020data,lu2021stochastic,bahmani2021kd}. Machine learning, deep learning, or neural networks are examples of such data-driven models that help to reduce the model complexity and may surpass conventional constitutive modeling, see \cite{bessa2019bayesian,fuchs2021dnn2,zohdi2022machine,fernandez2020application,Zhang2020homogenization,lu2021deepxde,vlassis2022component,lopez2018manifold,heider2021multi,henkes2022physics,zohdi2022note,fuhg2022physics,fernandez2022material,as2022mechanics,benaimeche2022k,cueto2022thermodynamics,stocker2022novel,zhang2022hidenn,niekamp2023surrogate,wessels2022computational,kalina2023fe} and the citations therein.

The objective of this work is to introduce a computationally efficient and accurate solution to speed up multiscale modeling processes. The ability of the Convolutional Neural Networks (CNN) to capture the macroscopic constitutive properties from images of the microstructure will be utilized to predict the homogenized elastic stress. In comparison with published works \cite{rao2020three,yang2018deep,beniwal2019deep,eidel2023deep}, we highlight the remarkable efficiency of the proposed CNN model by employing different microstructures with various phases and comparing the results with the classical homogenization approaches. Furthermore, a key point is the illustration of advanced transfer learning of the CNN model to new realistic microstructures. These detailed investigations have not been  considered so far.
\\
\\
\begin{figure}[!b]
	\centering
	{\includegraphics[width=0.85\textwidth]{./Figuers/realBVP.pdf}}  %left, bottom, right and top 
	\caption{{Concrete real specimen with different microstructures (CT-images source: \texttt{www.baustoff.uni-hannover.de} related to \cite{aldakheel2020microscale,wriggers2020water,aldakheel2021simulation,noii2022probabilistic}).}}
	\label{multiscale}% 
\end{figure} 
%


From the modeling point of view, the multiscale approach can be classified in two distinct classes denoted as {\it concurrent} and {\it hierarchical} multiscale techniques. These are defined by differentiation of the macro characteristic length scale $\mathfrak{L}_{macro}$ with its micro domain counterpart $\mathfrak{L}_{micro}$. The concurrent multiscale method implies $\mathfrak{L}_{micro}\equiv\mathfrak{L}_{macro}$, as classified in \cite{Lloberas,Fish2014,Fish3,aldakheel2021simulation,aldakheel2020global}. Whereas in
the hierarchical multiscale method, the average size of the heterogeneous
microdomain is much smaller than its macro specimen size,
i.e. $\mathfrak{L}_{micro}\ll\mathfrak{L}_{macro}$, see \cite{Michel, Fish2014,aldakheel2023machine}. This
is often denoted as {scale separation law}, see computational homogenization
approaches based on the Hill-Mandel principle; outlined for
instance in \cite{Hill, Michel} among others. The aim of all those mentioned multiscale simulation techniques is the reduction of uncertainties and empirical assumptions while simultaneously increasing the accuracy of the solution. For a better understanding of  scale bridging, we consider a real concrete specimen with different microstructures, illustrated in Figure \ref{multiscale}.
%
%
On the micro-scale, the representative volume elements $\cal{RVE}$s consist of aggregates, pores and a cement matrix. At the macro-scale, construction applications in building and energy technologies are of interest, including the microscopic material behavior needed to develop the required system-output.


For determining the effective properties of heterogeneous materials at the micro-scale, there exist many approaches. For periodic composites with linear constitutive behavior, those properties can be analyzed by solving a sufficient number of unit cell problems along with the corresponding boundary conditions. Furthermore, the asymptotic homogenization approach can be considered in this category related to scale separation, as well documented in \citet{bensoussan+lions+papanicolaou78} and
\citet{sanchez-palencia80}. In the case of irregular microstructures, the required effective properties cannot be computed exactly. In the literature, the available methods in this category are limited to the computation of upper and lower bounds for the effective stiffness, as outlined in \citet{voigt1887} and \citet{reuss29}. This was further extended in the work of \citet{hashin+shtrikman62a,hashin+shtrikman63} by considering variational principles, leading to better estimates. \citet{hill65} developed a self-consistent method by embedding a single inclusion into an infinite domain of the initially unknown effective matrix material. Another important scheme is the well-known two-scale computational homogenization (FE$^2$) which determines the effective properties by two nested BVPs (boundary-value-problems) along with the corresponding scale transition law, see for instance \citet{smit+brekelmans+meijer98},
\citet{miehe+schotte+schroeder99,miehe+schroeder+schotte99,miehe02},
\citet{feyel+chaboche00}, \citet{terada+kikuchi01},
\citet{kouznetsova+geers+brekelmans02}, \citet{geers+etal10}, \citet{schroeder+keip12,schroeder00}, \citet{chatzigeorgiou+javili+steinmann14}, \citet{schmidt2022computational} and
\citet{javili+chatzigeorgiou+steinmann13}
%
In this case, the material behavior at the microscopic level is analyzed by employing the representative volume element concept, whereas a homogenization technique is considered to compute the macroscopic response. We refer to \citet{hill72}, \citet{suquet87} and \citet{nemat-nasser+hori99} 
for fundamental homogenization principles of local mechanical responses. The above-introduced analytical and physically motivated mathematical models lead to pronounced computational costs. To accelerate micro-macro simulations of materials with complex micro-structures, the search for advanced technologies that reduce expensive simulation time is critical.
\\
\\
%
In the present contribution, the focus will be put on the computational micro-to-macro approach of heterogeneous materials where the characteristic length of the microscale is  much smaller than its macro counterpart  \citet{ZoOdRo96,ZoWr99,Zo08,HaWr08,TeWr11,TeWr08,TeWr07,WrMo06,TeZo07,ZoWr05}. In this contribution, a time-efficient and robust convolutional neural network (CNN) model will be utilized. A composite with 3 different phases is considered within this work (e.g. a hybrid composite laminates consisting of shape memory alloy and glass fiber as reinforced phases and epoxy resin as the host material of the composite; or concrete microstructure consisting of aggregates, voids and cement-paste matrix). The training data is generated by applying standard finite element simulations (FEM) on a cubical sample of a microstructure with different numbers of inclusions/phases. Such geometrical heterogeneities are obtained through random spatial distributions and arbitrary sizes of  inclusions/phases, leading to random volume fractions in each microstructure. The network is then trained using the microstructures as an input and the homogenized stresses as labels. As an advantage, the proposed method has the potential for providing accurate and feasible approximations for various engineering applications. To this end, promising results that approach the exact solution are achieved. 

The paper is organized as follows:
%
In Section~\ref{sec2-CNN} a brief overview of the micro-to-macro transition concept is introduced. 
%
Next, the theory of the convolutional neural networks (CNNs) is presented in detail in Section~\ref{sec3-CNN}.
%
The CNN model is then employed to predict the homogenized macroscopic stress of a microstructure representing a heterogeneous composite in Section~\ref{sec4-CNN}.
%
The model capability is illustrated through various representative examples in Section~\ref{sec5-CNN} and compared with the traditional multiscale methods. The trained network is then applied to learn the constitutive behavior of elastic materials within the finite element application. Thereafter, the trained model is used to predict the macroscopic stress and through transfer learning, it is applied to a new structure. Section~\ref{sec6-CNN} presents a summary and outlook for extensions of  this work.
%
\end{comment} %---------------------------------------



%=======================================================================
\section{Micro-to-macro transition concept}
\label{sec2-CNN}
%=======================================================================

\begin{comment} %---------------------------------------
The aim of this section is to give an overview of the multiscale mechanics of materials. A material point $\bar\Bx$ within the solid $\bar\calB$ at the macro level is considered. In addition to kinematics and balance relations, given by continuum mechanics, the constitutive behavior is needed to solve a boundary value problem in $\bar\calB$. In cases when the material body consists of a heterogeneous microstructure that determines its behavior, a constitutive assumption posed a priori can only provide a coarse estimate of the effective constitutive behavior. The homogenization approach relies on the assumption that the two considered scales are well separated, i.e.\ a typical size on the macroscale is much larger than a typical size of the underlying microstructure, $l_{macro} \gg l_{micro}$ as depicted in Figure~\ref{homo}. At the microscale, the so-called representative volume element $\cal{RVE}$ models the microstructure that corresponds to the macroscopic material point. Under boundary conditions which are determined by the macroscopic state of deformation, a boundary-value-problem is defined on this representative volume element. At this level, both the governing balance relations and the constitutive behavior are known. The goal is to return this constitutive information from a finer scale to the macro level. The $\cal{RVE}$ acts as a statistically representative portion of the heterogeneous microstructure (grains separated by grain boundary, voids, inclusion, crack, and other similar defects), see \citet{nemat-nasser+hori99}. Its size must be chosen such that it is large enough to be representative or rather such that it sufficiently accounts for the character and distribution of heterogeneities. Nevertheless, it should be much smaller than the specimen considered at the macro level to ensure a scale separation on the one hand, and to achieve an increased efficiency on the other hand. If the material or geometric properties of the underlying microstructures vary spatially within a macro specimen, as e.g. in functionally graded materials, the representative volume element can be chosen differently in different macro regions if only a local periodicity is required.  

\end{comment} %---------------------------------------
%
\begin{figure} [t]%
	%
	\centering%
	%
	\includegraphics[width=.8\textwidth]{./Figuers/multiscale2.pdf}
	%
	\caption{Computational micro-to-macro transition approach of heterogeneous materials.}
	%
	\label{homo}%
\end{figure}%
%
%
%=======================================================================
\subsection{Macroscopic boundary value problem}
%=======================================================================
%
Let $\bar\calB\subset\calR^\delta$ denote a macroscopic body with dimension
$\delta\in [2,3]$, as sketched in Figure \ref{homo}.
We study the mechanical deformation of the body under quasi-static loading in the time interval $\bar\calT \subset \calR_+$. In what follows, 
$\bar\nabla (\cdot) := \partial_{\bar\Bx}(\cdot)$ and 
$\dot{(\cdot)} := \partial_{t}(\cdot)$ denote the gradient and time derivative
of the macroscopic field $(\cdot)$, respectively. The primary variable field is the {displacement field} $\bar\Bu$ of the material
point $\bar\Bx\in\bar\calB$ at time $\bar t \in \bar\calT$
\begin{equation}
\bar\Bu : 
\begin{cases}
\bar\calB \times \bar\calT \rightarrow \calR^\delta \\
(\bar\Bx,\bar t) \mapsto \bar\Bu(\bar\Bx,\bar t)
\end{cases} 
\label{eq:homo1}
\end{equation}
%
The kinematic relation arising from the considered framework is the linear strain tensor $\bar\Bve
:= \bar\nabla_s\bar\Bu = \half (\bar\nabla\bar\Bu
+ \bar\nabla^T\bar\Bu)$. The strains are assumed to be small, i.e.\ $| \bar\nabla\bar\Bu | <
\bar\epsilon$ is bounded by a small number $\bar\epsilon$.
\\
\\
In what follows we assume the existence of a macroscopic potential density functional $\bar\Pi$, which is obtained by a variational principle of homogenization for the underlying microstructure. With this assumption, we can formulate a variational structure resulting with the minimization problem for the determination of macroscopic primary variable  
%
\begin{equation}
\{ \bar\Bu\}
= \mbox{Arg} \Big\{ \, \inf_{\bar\Bu}  \,
\bar \Pi(\bar\Bu) \, \Big\} \; ,
\label{eq:homo2}
\end{equation}
%
where $\bar\Pi$ is defined as
%
\begin{equation}
\bar\Pi(\bar\Bu) = \int_{\bar\calB} \bar \Psi(\bar\Bve)\; dV
- \bar\calP_{ext}(\bar\Bu) \; ,
\label{eq:homo3}
\end{equation}
%
hereby, the potential energy $\bar \Psi$ is fully determined at the microscopic $\cal{RVE}$ by a variational formulation principle and the mechanical loading contribution
%
\begin{equation}
\bar\calP_{ext}(\bar\Bu)
= \int_{\bar\calB} \bar\Bgamma\cdot \bar\Bu\; dV
+ \int_{\partial\bar\calB_{\bar\Bt}} \bar\Bt_N\cdot \bar\Bu \; dA \; .
\label{eq:homo4}
\end{equation}
%
The given macroscopic body force per unit volume is introduced as
$\bar\Bgamma$, whereas the tractions on the Neumann boundary
$\partial\bar\calB_{\bar\Bt}$ are depicted as $\bar\Bt_N$. Furthermore, consider a decomposition of the surface $\partial\bar\calB
=\partial\bar\calB_{\bar\Bu} \cup \partial\bar\calB_{\bar\Bt}$ into a part
$\partial\bar\calB_{\bar\Bu}$ where the displacements are prescribed
and a part $\partial\bar\calB_{\bar\Bt}$ with given tractions, along with $\partial\bar\calB_{\bar\Bu} \cap \partial\bar\calB_{\bar\Bt}
= \emptyset$. 


The necessary condition of \req{eq:homo2} results in the equilibrium equations
describing the macroscopic problem for the quasi-static case under consideration. The Euler-Lagrange equation is the balance of linear momentum
%
\begin{equation}
\div [\bar\Bsigma] + \bar\Bgamma= \Bzero \quad \mbox{in} \ \bar\calB \; ,
\label{eq:homo5}
\end{equation}
%
along with the Neumann-type boundary conditions $\bar\Bsigma \cdot\bar\Bn = \bar\Bt_N$ on
$\partial\bar\calB_{\bar\Bt}$. The macroscopic stresses $\bar\Bsigma$ are
%
\begin{equation}
\bar\Bsigma = \p{\bar\Bve} \bar \Psi (\bar\Bve)  \; ,
\label{eq:homo6}
\end{equation}
governed by the macroscopic potential energy function $\bar\Psi$ obtained by the
homogenization of the microstructure.

%=======================================================================
\subsection{Microscopic boundary value problem}
%=======================================================================
Let $\calB\subset\calR^\delta$ denote a periodic microstructure ($\cal{RVE}$) as depicted in Figure~\ref{homo}.  
In what follows, $\nabla (\cdot)
:= \partial_{\Bx}(\cdot)$ and $\dot{(\cdot)} := \partial_{t}(\cdot)$
denote the gradient and the time derivative of the microscopic field
$(\cdot)$, respectively. The primary variable field is the {displacement field} $\Bu$ of the material point $\Bx\in\calB$ at time $t \in \calT$
\begin{equation}
\Bu : 
\begin{cases}
\calB \times \calT \rightarrow \calR^\delta \\
(\Bx, t) \mapsto \Bu(\Bx, t)
\end{cases} 
\label{eq:micro-homo1}
\end{equation}
%
The microscopic linear strain tensor $\Bve$ is the symmetric part of the displacement gradient
$\Bve:= \nabla_s\Bu = \half (\nabla\Bu
+ \nabla^T\Bu)$. We now postulate a variational principle of homogenization that determines the macroscopic potential energy $\bar\Psi$ introduced above, as follows
\begin{equation}
%
\bar\Psi(\bar\Bve) = \inf_{\Bu}  \, \frac{1}{|\calB|} \int_{\calB}  \Psi(\Bve) \; dV \; .
\label{eq:micro-homo2}
\end{equation}
%
This definition is conceptually in line with the formulations outlined in \citet{miehe02}.
%
At the micro-level, we identify the stresses as 
\begin{equation}
\Bsigma(\Bve) := \p{\Bve} \Psi(\Bve) \; ,
\label{eq:micro-homo3}
\end{equation}
%
governed by the constitutive functions $\Psi$. With this definition at hand, the
variation of the principle (\ref{eq:micro-homo2}) gives the condition
%
\begin{equation}
\frac{1}{|\calB|} \int_\calB \big(- \div [\p{\Bve}\Psi]\big)
\cdot \delta\dot\Bu \,dV + \frac{1}{|\calB|} \int_{\partial\calB}
\big(\p{\Bve}\Psi\cdot\Bn\big) \cdot\delta\dot\Bu \,dA = 0 \; .
\label{eq:micro-homo4}
\end{equation}
%
Hence, the Euler-Lagrange equation for the variational principle (\ref{eq:micro-homo2}) is
%
\begin{equation}
\div[ \Bsigma] = \Bzero \qquad \mbox{in} \; \;\calB
\label{eq:micro-homo5}
\end{equation}
%
on the microstructure $\calB$. 
%




%=======================================================================
\subsection{Homogenization quantities and macro-homogeneity conditions}
%=======================================================================

The microscopic boundary-value-problem is linked to the macroscopic framework by a scale bridging approach accounting for homogenized quantities defined on the boundary/volume of the representative volume element. The macroscopic strain $\bar\Bve$ and microscopic strain $\Bve$ along with their associated work-conjugate stress measures $\bar\Bsigma$ and $\Bsigma$ are defined as
%
\begin{equation}
\bar\Bve:=\frac{1}{|\calB|} \int_{\p{}\calB} \ \mbox{sym} (\Bu \otimes \Bn)\; dA = 
\frac{1}{|\calB|} \int_{\calB} \Bve \;dV 
\AND 
\bar\Bsigma:=\frac{1}{|\calB|} \int_{\p{}\calB} \ \mbox{sym} (\Bt_N \otimes \Bx)\; dA
=  \frac{1}{|\calB|} \int_{\calB} \Bsigma \; dV
\ ,
\label{eq:homo7}
\end{equation}
% 
where $|\calB|$ is the volume of the micro-structure $\calB$.
\\
\\
In order to derive appropriate boundary conditions for the $\cal{RVE}$, we consider a \textit{ Hill-Mandel macro-homogeneity condition} proposed
by \citet{hill72} stating the equivalence of the macroscopic stress power with the volume average of its microscopic counterpart
%
\begin{equation}
\bar\Bsigma:\dot{\bar\Bve}	= \frac{1}{|\calB|} \int_{\calB} \Bsigma : \dot\Bve \; dV = \frac{1}{|\calB|} \int_{\partial\calB} \Bt_N \cdot\dot\Bu \; dA 
\ .
\label{eq:homo8}
\end{equation}
%
To solve the microscopic boundary-value-problem we need to set up appropriate boundary conditions for the $\cal{RVE}$. For the micro-elastic model under consideration we focus on three types of boundary constraints:
%
\begin{itemize}
	\item[(i)] \textit{Dirichlet-type boundary conditions:} linear displacement constraint on the boundary $\partial\calB$. In the first approach, a homogeneous strain is imposed in the full microstructure, i.e.\
	$\Bve=\bar\Bve$ in $\calB$. Hence, the microscopic strain is identical
	to the macro-strain at any point $\Bx\in\calB$ of the
	microstructure. This is called \textit{Voigt-Taylor assumption}
	referring to \citet{voigt1887} and
	yields an upper bound of the stiffness of the $\cal{RVE}$.
	\item[(ii)] \textit{Neumann-type boundary conditions:} constant stress on the boundary $\partial\calB$, zero micro-tractions. The so-called \textit{Reuss-Sachs bound}, \citet{reuss29}, gives a lower bound of the stiffness of the microstructure. Here, a homogeneous stress is applied in the full domain $\calB$ in the form $\Bsigma=\bar\Bsigma$.
	\item[(iii)] \textit{Periodic boundary conditions:} periodicity of all primary
	fields on opposite surfaces $\partial\calB^+$ and
	$\partial\calB^-$ of the microstructure.
\end{itemize}
%
\begin{figure}[b]%
\centering%
%
\includegraphics[width=0.75\textwidth]{./Figuers/P-BC.eps}	
%
\caption{Periodic microstructure, where the surface of the $\cal{RVE}$ in (a) decomposes into two parts $\partial\calB=\partial\calB^+\cup\partial\calB^-$ with normals $\Bn^+$ and $\Bn^-=-\Bn^+$ at associated points $\Bx^+\in\partial\calB^+$ and $\Bx^-\in\partial\calB^-$. (b)	Periodic boundary conditions for the displacement.}
%
\label{Periodic-BCs}%
\end{figure}%
%
The upper Voigt-Taylor and lower Reuss-Sachs bounds are only mentioned
for completeness and do not play a crucial role in the subsequent
treatment of the boundary conditions. The periodic constraints $(iii)$ are only applicable for perfectly periodic microstructures, but for many cases it turns out that they provide better results than either the Dirichlet-type constraints $(i)$ or the Neumann-type conditions $(ii)$. Here, we refer to the work
of \citet{ZoWr05} and the references cited therein.
\\
\\
For the periodic boundary conditions plotted in Figure \ref{Periodic-BCs}, the surface of the $\cal{RVE}$ decomposes into two parts $\partial\calB=\partial\calB^+\cup\partial\calB^-$ with normals $\Bn^+$ and $\Bn^-=-\Bn^+$ at associated points $\Bx^+\in\partial\calB^+$ and $\Bx^-\in\partial\calB^-$. The deformation will be extended by a fine scale
fluctuation field marked in what follows with a tilde $(\tilde{\cdot} )$
%
%
\begin{equation}
\begin{array}{r@{\ }c@{\ }l}
\Bu &=& \bar\Bve\cdot\Bx+\tilde\Bu 
\end{array}
\ ,
\label{eq:homo9}
\end{equation}%
around the macro-modes.  As an additional constraint for the fluctuation fields, the Hill condition reads
%
\begin{equation}
\frac{1}{|\calB|} \int_{\partial\calB}
\Bt_N\cdot\dot{\tilde\Bu}  \; dA = 0
\ .
\label{eq:homo10}
\end{equation}
This constraint can be satisfied for \textit{periodic fluctuations} and
\textit{anti-periodic tractions} at the boundary $\partial\calB$
resulting in

\begin{equation}
\tilde\Bu^+ = \tilde\Bu^- 
\AND
\Bt_N^+ = -\Bt_N^-
\quad \mbox{on} \quad
\partial\calB \ ,
\label{eq:homo11}
\end{equation}
%
at associated points $\Bx^+\in\partial\calB^+$ and
$\Bx^-\in\partial\calB^-$. Such boundary
conditions are the most reasonable choice for the homogenization
analysis under consideration, even for microstructures which are
non-periodic. 
\\
\\
{As a {\it summary of the computational framework}:
\begin{enumerate} 
\item Evaluate the macroscopic deformation at each material point. \item The necessary boundary conditions are applied at the microscopic $\cal{RVE}$. \item The microscopic boundary value problem is computed under the macroscopic loading. \item The volumetric averaged microscopic quantities are transferred to the dedicated material macroscopic points. \item The macroscopic boundary value problem is solved. 
\end{enumerate} 
For further details on computational homogenization method for different engineering applications, the interested reader is referred to the references in the introduction part of this work.}





%=======================================================================
\section{Theory of Convolutional Neural Network (CNN)}
\label{sec3-CNN}
%=======================================================================
To overcome the issue of large computational costs to solve the multiscale problem on the micro-to-macro level, summarized in Section \ref{sec2-CNN}, an approach to replace the calculation with a machine learning model is introduced. Applying deep learning (DL) is an empirical, highly iterative process that requires training several models in order to reach satisfactory results. During this process, a combination of different parameters and hyper-parameters is tested.

The purpose of this section is to give a brief insight into the structure and operation of the Convolutional Neural Networks (CNN), without going into specific variants and manifestations of this technique. CNN is a kind of Artificial Neural Network and is associated with deep learning techniques. More specific, the CNN model is a specialized Feed Forward Neural Network (FFNN) that excels at working with data that have a grid like structure, such as images or time-series data. CNN has success at computer vision applications, such as image classification, object detection, text detection and recognition, see \citet{gu2018recent}. The weight-sharing techniques achieved by convolutional and pooling layers are the main differentiator between CNN and FFNN. Those two essential operations, along with other deep learning functions will be explained next.

\begin{figure}[b]
	\centering 
	\includegraphics[width=.6\columnwidth]{./Figuers/Convolution_operation.png} 
	\caption{An example of a convolution operation.}
	\label{convolution_operation}
\end{figure}



\subsection{The convolution operation}
The convolution operation could be defined as a linear multiplication between an input and a set of weights. In this work, the input is a gray-scale image and the set of weights is a matrix, known as a \textbf{filter} or \textbf{kernel}. Multiplying the filter with the input image at different points allows it to capture the same feature regardless of its location in the image. The output of the convolutional operation is usually referred to as a \textbf{feature map}. For an image the feature value at a position ($i,j$) in a feature map is given by the equation
\begin{equation}
z_{i,j} = \sum_m\sum_n \; \BI_{i+m,j+n} \; \Bk_{m,_n}\; ,
\end{equation}
where $\BI$ is the image, and $\Bk$ is the applied filter with $m,n$ dimensions. This convolution operation is visualized in Fig \ref{convolution_operation}.
%
Nonlinearity is introduced to each feature value in the feature map element-wise by the activation function
\begin{equation}
a_{i,j} = f(z_{i,j}) \; ,
\end{equation}
Note that, the activation functions, also known as transfer functions, have a big impact on the performance of deep learning models. They introduce nonlinearity to computations done in the neuron and determine whether or not to activate it. Hereby, we employed two different non-linear activation functions, namely the \textbf{Rectified linear unit} (ReLU) \citet{nair2010} and \textbf{Swish function} \citet{eger2019time}. As they both produced similar results we selected ReLU in this contribution. ReLU is considered to be the default activation function in modern neural networks architecture, usually used in the hidden layers with another activation function at the output layer. As it represents an almost linear function, it is computationally efficient while allowing for backpropagation. ReLU can be represented mathematically as  
\begin{equation}
f(z) = max(0,z) =
\begin{cases}
z   &   z \ge 0 \\[2mm]
0 & z < 0 \; . 
\end{cases}
\end{equation}



\subsection{Pooling layers}

\begin{figure}[b]
	\centering 
	\includegraphics[width=.6\columnwidth]{./Figuers/Average_pooling.png} 
	\caption{Example of Average Pooling operation}
	\label{Average_pooling}
\end{figure}
\begin{figure}[t]
	\centering 
	\includegraphics[width=.6\columnwidth]{./Figuers/Max_pooling.png} 
	\caption{Example of Max Pooling operation}
	\label{Max_poolingg}
\end{figure}






Another main component of convolutional neural networks is pooling layers, usually placed between two convolutional layers. They reduce the dimensions of the feature map resulting from the convolution operation, thus reducing the learning parameters as well as the computational cost and speeding up the training process. The pooling layer does not have learning parameters, but the size of the pooling window and the type of the pooling function performed on the pixels in this window are hyper-parameters that need to be tuned during the training process. There are many kinds of pooling operations used in CNN models, max pooling and average pooling being the most commonly used ones. In an average pooling operation, the average value of the pixels in the pooling window is calculated as sketched in Fig \ref{Average_pooling}, while in the max pooling operation, the highest pixel value in the pooling window is considered, as shown in Fig \ref{Max_poolingg}.


\subsection{Loss Function}
The optimum parameters for the CNN model are found by optimization of an objective function. The \textbf{loss function} is a term referring to the objective function which is the case for an optimization process using minimization. A loss function measures the error between the model prediction and the ground truth. Thus, it indicates how well the model is performing and should be able to represent this error. That's why different predicting problems require different types of loss functions. For example, in the case of binary classification, binary cross-entropy is a suitable choice. In the case of regression prediction problems, an appropriate loss function could be square error loss or absolute error loss. The loss function is known as the cost function when applied to the whole data set. In this contribution, we use the mean squared error loss (MSE). It is the sum of squared differences between predicted and ground truth values, which are expressed by the equation
\begin{equation}
MSE =  \frac{1}{N}\sum_{i=1}^{N} (y_{i} - \widehat{y}_i)^2 \; ,
\end{equation}
where $N$ is the number of training examples, $y_i$ is the ground truth, and $\widehat{y}_i$ are the predicted values. 
\\
\\
A back-propagation and optimization algorithm are considered in this work. Here, gradient descent is one of the most commonly used optimization algorithms in neural networks that minimize a loss function. Furthermore, the adaptive momentum estimation (Adam) has shown successful results while dealing with learning rate problems occurring with the adaptive gradient algorithm (Adagrad).





%=======================================================================
\section{CNN aided multiscale mechanics of materials }
\label{sec4-CNN}
%=======================================================================

In this section, a convolutional neural network model is used to predict the homogenized macroscopic stress $\boldsymbol{\Bar{\sigma}}$ of a microstructure, which represents a heterogeneous composite, consisting of three-phases, under a predefined macroscopic strain $\boldsymbol{\Bar{\Bve}}$. The proposed model falls under the {\it supervised-learning} category, i.e. the data used for the training and testing processes are labeled. The data generation process, the design of the CNN model architecture, and the results will be discussed in the following subsections.

\begin{figure}[t]
	\centering 
	\includegraphics[width=0.95\textwidth]{./Figuers/workflow_data_generation.png} 
	\caption{The work-flow for data-generation process.}
	\label{data_generation}
\end{figure}

\subsection{Dataset generation}

The macroscopic performance of materials often relies on their microscopic structures, which can not be seen by the naked eye. In recent years, actual complex microstructures can be captured with high resolution using modern non-destructive imaging techniques such as a micro-computed tomography scan $\mu$-CT or scanning electron microscopes. In this work, the microstructures are synthetically generated. A reinforced composite that consists of three different materials is considered (e.g. a hybrid composite laminates consisting of shape memory alloy and glass fibers as reinforced phases and epoxy resin as host material of the composite; or a concrete microstructure consisting of aggregates, voids and cement-paste matrix), see \citet{noii2022probabilistic,segurado2002numerical,wessels2020neural}. The workflow of generating the dataset is depicted in Fig \ref{data_generation}.

The generated dataset consists of $10800$ square-shaped $\cal{RVE}$s with a side length of $1$~mm. It is divided into $11$ groups of $900$ $\cal{RVE}$s depending on various volume fractions and inclusions. Each $\cal{RVE}$ contains a different number of inclusions with circular and ellipsoidal shapes that are randomly distributed: The circles vary in radius between $0.10-0.15$~mm, whereas the ellipsoids have a small radius of $0.008$~mm and a maximum radius of $0.08$~mm; resulting in different volume fractions ranging from a minimum of $10\%$ to a maximum of $35\%$.

\begin{figure}[b]
	\centering 
	\includegraphics[width=\textwidth]{./Figuers/geom.eps} 
	\caption{Data set generation: Images of the idealized microstructure.}
	\label{idealized_microstructure}
\end{figure}


\subsection{Images of the microstructure}

\begin{figure}[t]
	\centering 
	\includegraphics[width=\textwidth]{./Figuers/geo-m.eps} 
	\caption{Data set generation: Transfer learning.}
	\label{TL_RVEs}
\end{figure}

These $\cal{RVE}$s are randomly generated using various center-coordinates ($x$,$y$) and radius ($r$) of the circles and ellipsoids that represent the inclusions in a certain predefined bound, which represents the edges of the $\cal{RVE}$. The resulting geometrical information is then used to produce gray-scale images, where the inclusions are assigned a pixel value of $128$ for ellipsoids and $255$ for the circular inclusions, and the matrix is assigned a pixel value of $0$. The dimensions of the images are (256, 256, 1), where the first two numbers represent height and width and the last number represents the color channels. These gray-scale images are the input data of the CNN model. Fig. \ref{idealized_microstructure} depicts representative volume elements ($\cal{RVE}$) of the microstructure with various volume fraction distributions. To demonstrate the efficiency of the proposed CNN model, we transfer the learning model to a new $\cal{RVE}$ structure and compare the results and computation costs with learning from scratch. For that reason, we started with simple two-phases microstructure as plotted in Fig \ref{TL_RVEs}, then we test a complex structure at the end of this work. For the construction of those artificial microstructures, we refer to our work \cite{noii2022probabilistic} related to the random allocation of the heterogeneities.


\subsection{Calculation of macroscopic stresses}
Acquiring the labels for each one of the generated $\cal{RVE}$ is the second step of generating the dataset, on which the proposed CNN model will be trained and tested. These labels are the components of the macroscopic stress $\boldsymbol{\Bar{\sigma}}$
\def\c{\begin{bmatrix} \Bar{\sigma}_{11}& \Bar{\sigma}_{22}& \Bar{\sigma}_{12}\end{bmatrix}}
\begin{equation}
\boldsymbol{\Bar{\sigma}}= \boldsymbol{\widehat{y}} =\c^T \; .
\end{equation}
%
The computational multiscale framework is applied using the standard finite element method (FEM) as outlined in \citet{wriggers06}. In this contribution, the generation of the idealized microstructures was done in Matlab and Gmsh, see \cite{noii2022probabilistic}. Then the meshed microstructure was exported to Abaqus for solving the boundary value problem of the microstructure. The computations are carried out under the plane strain assumption while imposing periodic deformation and antiperiodic traction, which was introduced in Section \ref{sec2-CNN}. The composite consists of isotropic linear elastic constituents with properties shown in Table \ref{table:Material_proprieties}.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h!]
	\centering
	\begin{tabular}{||c c c ||} 
		\hline
		Materials & Young's modulus (GPa) & Poisson's ratio  \\ [0.5ex] 
		\hline\hline
		Matrix & 3 & 0.38 \\ 
		\hline
		Phase I (Ellipsoid) & 70 & 0.20 \\
		\hline
		Phase II (Circles) & 50 & 0.35 \\
		\hline
	\end{tabular}
	\caption{Material proprieties of $\cal{RVE}$ constituents}
	\label{table:Material_proprieties}	
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The micro-to-macro approach is based on a deformation-driven microstructure with a {\it constant} macroscopic strain. The dataset is then divided into three subsets training, validation, and testset 8640:1080:1080. The model uses the training set to learn the parameters (weights, biases). The validation set is used to fine-tune the model's hyper-parameters  (learning rate, number of hidden layers,...) and as an indicator of over-fitting. Finally, the test set gives an unbiased estimation of how well the model generalizes on cases that it has not encountered before. Normalizing the input features and the output of neural networks is usually done to stabilize and speed up the training process. One of the common ways is to rescale the data to be in a range between $[0,1]$.
In the case of the presented dataset, the input data are already normalized in the range $[0,1]$, the output data has noticeable differences in values between the first two components, which represent the normal stresses, and the third one representing the shear stress, so they need to be scaled separately through the following equation
\begin{equation}
\Bar{y} = \frac{y - min(y)}{max(y)-min(y)} \; ,
\end{equation}
with the normalized $\Bar{y}$ components of $y$.

Neural networks are known for their strong ability to memorize the training data examples, which leads to over-fitting, especially in the cases of small datasets. To overcome this problem, several regularization techniques are applied, such as ($L_2$ regularization, dropout layers, and early stopping). $L_2$ regularization, also known as weight decay regularization, imposes a squared $L_2$ norm on the weights. The regularized cost function in this case is 
\begin{equation}
MSE =  {\frac{1}{N}\sum_{i=1}^{N} \big(y_{i} - \widehat{y}_i\big)^2} + \lambda \sum_{i=1}^{N} w_i^2 \; ,
\end{equation}
where $\lambda$ is a regularization parameter. Early stopping is another regularization technique, which works by stopping the training process after a predefined number of epochs (once the error of the validation set starts to increase, as it is a sign of over-fitting). Then, the parameters of the model with the best performance on the validation set are restored. Dropout regularization works by randomly deactivating a certain number of the nodes in the fully connected layers at each iteration during the training process.


\subsection{CNN-model architecture}
Applying Deep learning is a highly iterative process, in which the first step is to define an idea for solving the task at hand (such as deciding the model type and structure). The second step is to implement this idea and the last step is to experiment and see if the results are satisfactory. If this is not the case one has to iterate until reaching the desired results. Although having good domain knowledge helps to go through this cycle efficiently, it is very unlikely to get the right parameters without iterations. 

Transfer learning, where a model that works well on the same task already exists, is used directly or retrained to tune the hyper-parameters. This is one method to speed up the model design process and will be investigated in this work. Another method is to borrow ideas from models that proved to be successful in similar tasks. The proposed model architecture is implemented using Keras 2.7 and Python 3.7.12. The training process was done using Google virtual machines through Google-COLAB, the machine was equipped with a NVIDIA Tesla P100-PCIE-GBU with 16 GB memory.

Usually, CNN models are applied in fields like image classification or object detection, but in this application, it is used for a regression task. That's why the mean square error (MSE) between the CNN predictions and the ground truth (results from the finite element simulations) is chosen as a cost function. CNN models consist of convolutional, pooling, and fully connected layers. The choice of these layer's parameters decides the capacity and the power of representation of the model. To have a sufficient representation, different filter sizes up to (5,5) are chosen through the convolutional layers for the evaluated model. Furthermore, a stride of $1$ and implicit zero padding (same padding) is applied to reduce the effect of narrower output dimension resulting from the convolution operation. As regularization techniques, $L_2$ regularization with a factor of $0.01$, and early stop with $40$ epochs as a predefined number to stop the training if the validation set error does not decrease, are considered. The activation function used through all the layers is ReLU due to its computational efficiency. The Adam optimizer with a learning rate of $0.01$ is chosen for training the model while applying a learning rate decay factor of $0.1$ if the validation error does not decrease for $20$ consecutive epochs. 

As a starting point of this work, we employed already existing CNNs with deeper network architectures \cite{rao2020three,yang2018deep,beniwal2019deep} furthermore, we utilize additional operations, including skip connections in e.g. ResNet50, DenseNet, and MobileNetV2, filter concatenation in InceptionV3, and other operations like squeeze and excitation blocks in MobileNetV3 (consume more time in contrast to AlexNet with only convolution and pooling operations).

Deeper architectures were designed to handle complex features in a dataset containing several hundred class labels. However, in our study, the dataset contains fewer variations in pixel intensities between samples and no color channels were involved. Therefore a simpler architectural construction for the material property prediction would be appropriate. Hence, it is valuable to develop a model from scratch based on the insights we gathered from initial experimentation, which will certainly add concrete advantage in comparing the performance of different models for the dataset and application used in the study. The proposed model architecture is sketched in Fig \ref{Architecture}.



\begin{figure}[t]
	\centering 
	\includegraphics[width=0.95\textwidth]{./Figuers/architecture.eps} 
	\caption{CNN-model architecture for predicting homogenized stress.}
	\label{Architecture}
\end{figure}


\begin{figure}[t]
	\centering 
	\includegraphics[width=0.95\textwidth]{./Figuers/conv-feature.eps} 
	\caption{Visualisation of the input image (three-phases idealized microstructure) and the feature maps learned through the convolutional layers of the CNN-model.}
	\label{Feature_maps}
\end{figure}




Neural networks are regarded as black boxes, as it is hard to understand the reasoning behind the specific decisions they make and the values of the weights they choose. The filters in the convolutional neural networks are weights that the network learns. Since CNN works with images these filters allow for visualizing the feature maps, which could lead to a better understanding of the features that the model learns. Fig. \ref{Feature_maps} shows the different feature maps through the model. It can be seen that they represent most of the details of the microstructure, but as they go deeper, the learned features seem to be more abstract and hard to be interpreted.



%=======================================================================
\section{Results and Discussions}
\label{sec5-CNN}
%=======================================================================

\begin{figure}[t]
	\centering 
	\includegraphics[width=0.4\textwidth]{./Figuers/stress_xx.png} \qquad\qquad\qquad
	\includegraphics[width=0.4\textwidth]{./Figuers/stress_yy.png}\\[4mm]
 	\includegraphics[width=0.4\textwidth]{./Figuers/stress_xy.png} \qquad
	\includegraphics[width=0.55\textwidth]{./Figuers/geom.eps} 
	\caption{CNN-predictions (y-axis) vs. ground-truth values (FEM x-axis) of the stress components: $\bar{\sigma}_{xx}, \bar{\sigma}_{yy}, \bar{\sigma}_{xy} $.}
	\label{example1-1}
\end{figure}


\begin{table}[t]
\centering
	\begin{tabular}{||c c c c ||} 
		\hline
		Stress components & $\bar{\sigma}_{11}$ & $\bar{\sigma}_{22}$& $\bar{\sigma}_{12}$  \\ [0.5ex] 
		\hline\hline
		MAPE & 0.87\% & 0.87\% &0.36\%  \\ 
		\hline
		$R^2$& 0.98 &0.98  &0.91 \\
		\hline
	\end{tabular}
	\caption{MAPE and $R^2$ values on the test dataset.}
	\label{table:Mape and r2}
\end{table}
%
\begin{table}[t]
	
	\centering
	\begin{tabular}{||c c c  ||} 
		\hline
		Volume Fraction & FEM-Model & CNN-Model \\ [0.5ex] 
		\hline\hline
		12.6 & 82.0 s & 0.07 s \\ 
		\hline
		20.7 & 98.0 s & 0.07 s \\
		\hline
		25.4 & 152.0 s & 0.07 s \\
		\hline
		31.8 & 200.0 s & 0.07 s \\
		\hline
	\end{tabular}
	\caption{Average CPU time needed for computing the homogenized stress of $\cal{RVE}$s containing different numbers of inclusions.}
	\label{table:time comparison}
\end{table}

\subsection{Three-phases microstructure}
\label{sec5-1-CNN}


The first model problem is concerned with predicting the macroscopic stresses of the three-phases idealized microstructures introduced in Fig. \ref{idealized_microstructure} with various volume fractions. The details of the CNN-model along with the required data for the FEM-model are described in Section \ref{sec4-CNN}. The CNN model will be evaluated in an unbiased way using a test dataset consisting of 1080 $\cal{RVE}$s, which was randomly picked from the whole dataset. The model predictions are shown in Fig \ref{example1-1} as a scatter plot, where the red-line represents the actual values obtained by finite element simulations (FEM-model). The figure shows the ability of the model to successfully predict the three components of the homogenized stresses. 

The model performance is measured in a quantitative way by the mean absolute percentage error (MAPE) for each component of the homogenized stresses. The MAPE is defined as 
\begin{equation}
MAPE = \frac{1}{n}\sum_{j=1}^{n}\left|\frac{\widehat{y}_j- y_j}{y_j}\right| \; ,
\end{equation}
where $\widehat{y}_j$ is the predicted value and $y_j$ is  the actual value obtained by FEM simulations. Another method to evaluate the performance of the model is the $R^2$-score, also known as the coefficient of determination, which is a statistical measure that shows how well the CNN-model approximates the actual data. $R^2$-score usually  has a value in range between $0$ and $1$, defined as 
\begin{equation}
	R^2 = 1 - \frac{\sum_{j=1}^{n} \big(y_j - \widehat{y}_j \big)^2}{\sum_{j=1}^{n} \big(y_j - \left.\overline{y_j}\right. \big)^2} \; , 
\end{equation}
in terms of the mean value $\left.\overline{y_j}\right.$, where the values closer to $1$ represent a model with better performance. The MAPE and $R^2$-score values for each component of the homogenized stresses are given in Table \ref{table:Mape and r2}. The results of the proposed CNN-model are very promising with accurate macroscopic stress prediction, that have a good MAPE and coefficient of determination close to $1$.

One of the main aspects to consider implementing deep learning in the mechanical field is the high computational efficiency, thus the ability to speed up the design process. Recent advancements in the computational power, specially the GPU parallelization ability, allow shorter training times for deep learning models. In this work, the training process of the model took around {$30.0$~min for a total of $300$ epochs}. Once the model is trained, the CNN-model advantages start to kick in. It can be seen in Table \ref{table:time comparison} that the computation time of the standard finite element method increases with the increase of the volume fraction of inclusions (increase number of inclusions). This is due to the increasing number of elements needed to accurately compute the homogenized stresses. On the other hand, the increase in number of inclusions or volume fraction does not affect the prediction time of the CNN model, see Table \ref{table:time comparison}. The CNN-model is $1171$ times faster than the FEM-model in the case of a $\cal{RVE}$ with $12.6$ volume fraction of inclusions, and up to $2857$ times in the case of computationally demanding $\cal{RVE}$s with volume fraction of $31.8$. The computations shown in Table \ref{table:time comparison} were executed on an Intel(R) Core(TM) i7-8750H CPU @ 2.2 GHZ and 16 GB of RAM. 





\subsection{Transfer learning model vs. training from scratch model}

Finally, we demonstrate the capacity of the CNN-model further by transferring the trained model of Section \ref{sec5-1-CNN} to a new geometry (data). The goal here is to predict the macroscopic stresses using the best trained model and compare that with a training from scratch model (same procedure as described in the previous example). To this end, we used less number of data (new microstructures) as sketched in Fig \ref{TL_RVEs} and only $100$ epochs were considered in the learning process, to demonstrate the CNN transfer-learning efficiency. 

As a first comparison, we plot the macroscopic stresses predictions of both models in Fig. \ref{example2-1} together with the actual values obtained by FEM-model. Hereby, the transfer learning model shows a better performance close to the FEM red-line compared with the training from scratch model. The good prediction of the transfer learning model required much less number of epochs (around $20$ epochs) and smaller MSE when compared with the training from scratch model as illustrated in Fig. \ref{example2-2}. Thus, we were able to further accelerate the micro-to-macro simulations of materials with complex micro-structures using the transfer learning approach. Next, we also compare the mean absolute percentage error (MAPE) and the $R^2$-score for both models in Table \ref{table:Transfer-learning}. As expected, the transfer learning model illustrates a better MAPE and coefficient of determination compared with the scratch training model.

\begin{figure}[t]
	\centering 
	\includegraphics[width=0.4\textwidth]{./Figuers/Tr_xx.png} \qquad\qquad\qquad
	\includegraphics[width=0.4\textwidth]{./Figuers/Tr_yy.png}\\[4mm]
 	\includegraphics[width=0.4\textwidth]{./Figuers/Tr_xy.png} \qquad
	\includegraphics[width=0.55\textwidth]{./Figuers/geo-m.eps} 
	
   \caption{Performance comparison between transfer learning model and training from scratch model: CNN-predictions vs. ground-truth values of the stress components.}
	\label{example2-1}
\end{figure}





\begin{table}[t]
\centering
	\begin{tabular}{||c c c c ||} 
		\hline
		Training from scratch model & $\bar{\sigma}_{11}$ & $\bar{\sigma}_{22}$& $\bar{\sigma}_{12}$  \\ [0.5ex] 
		\hline\hline
		MAPE & 1.0\% & 0.9\% & 0.5\%  \\ 
		\hline
		$R^2$& 0.87 &0.87  &0.55 \\
		\hline
	\end{tabular}
	
\bigskip

	\begin{tabular}{||c c c c ||} 
		\hline
		Transfer learning model & $\bar{\sigma}_{11}$ & $\bar{\sigma}_{22}$& $\bar{\sigma}_{12}$  \\ [0.5ex] 
		\hline\hline
		MAPE & 0.21\% & 0.22\% &0.35\%  \\ 
		\hline
		$R^2$& 0.99 &0.99  &0.80 \\
		\hline
	\end{tabular}
	
	
	\caption{Performance comparison between transfer learning model and training from scratch model: MAPE and $R^2$ values on the test dataset.}
	\label{table:Transfer-learning}
\end{table}
%
\begin{figure}[t]
	\centering 
	\includegraphics[width=0.56\textwidth]{./Figuers/epochs-TL.png} 
			\caption{Transfer learning losses (in blue) vs. training from scratch losses (in green).}
	\label{example2-2}
\end{figure}


%=======================================================================
\section{Conclusion}
\label{sec6-CNN}
%=======================================================================

In this contribution, a Convolutional Neural Networks (CNN) model was developed to predict the homogenized stress in the case of elastic small deformation for a synthetically generated microstructure representing a heterogeneous material. 
%
The considered material was a composite with 3 different phases (e.g. a hybrid composite laminate consisting of shape memory alloy and glass fiber as reinforced phases and epoxy resin as the host material of the composite; or concrete microstructure consisting of aggregates, voids and cement-paste matrix). The positions of inclusions and their sizes varied randomly in each $\cal{RVE}$ of the generated microstructure. 
%
The data set used for the training process and the numerical tests consisted of images of the generated microstructures, which were considered the CNN model's input, and the homogenized stresses resulting from applying finite element simulations were considered to be the labels. 
%
The proposed CNN model was chosen after several numerical tests performed on different model architectures, these were inspired by the work in different papers taking the CNN approach to solve computational mechanics tasks. The architecture of the CNN model consists of five sets of 2 convolutional layers followed by a max-pooling layer then a flatten layer and two fully connected layers, while utilizing $L_2$ and early stopping as regularization methods. The obtained results when employing the CNN model and its transfer learning show high accuracy in predicting the homogenized stress and a significant decrease in computation time. 


In this work, we focused our investigations on linear elastic material response, however, the proposed CNN model can efficiently be extended toward inelastic (time-dependent) material response by coupling the CNN structure with Recurrent Neural Networks (RNN) or adding the deformed configurations as an input. These topics await investigation. Furthermore, our model performance was evaluated using numerical results. As a future step, real experimental data of concrete microstructures (DFG Priority Program SPP 2020 Experimental-Virtual-Lab) will be employed as future training data in the CNN approach.



\smallskip
\smallskip

{\bf Acknowledgment.}
%
{Fadi Aldakheel} (FA) gratefully acknowledges support for this research by the ``German Research Foundation'' (DFG) in the {\sc Priority Program SPP 2020} within its second funding phase. FA would like to thank N. Noii and A. Khodadadian for the support with mesh generation related to our paper \cite{noii2022probabilistic}.



%
%
\bibliographystyle{model1-num-names}
\bibliography{manuscript}
\end{document}


















