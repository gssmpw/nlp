%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[table]{xcolor}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{utfsym}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


% \usepackage{pbox}
\usepackage{caption}


\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{soul}
\usepackage{listings}
\usepackage{graphicx,subfigure}

\definecolor{tablecell1}{RGB}{242, 242, 242}
\definecolor{tablecell2}{RGB}{205, 232, 248}
\definecolor{tablecell3}{RGB}{255, 242, 204}
\newcommand{\colorcell}{\cellcolor{tablecell1}}
\newcommand{\hcolorcell}{\cellcolor{tablecell2}}

\definecolor{cella}{rgb}{1.0, 0.92, 0.92}
\newcommand{\colorcella}{\cellcolor{cella}}
\renewcommand\algorithmiccomment[1]{\hspace{0.4em} $\triangleright$ #1}
\sethlcolor{tablecell2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\riashat}[1]{\textcolor{red}{\{Riashat: #1\}}}
\newcommand{\zeyu}[1]{\textcolor{blue}{\{Zeyu: #1\}}}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Learning Fused State Representations for Control from Multi-View Observations}

\begin{document}

\twocolumn[
\icmltitle{Learning Fused State Representations for Control from Multi-View Observations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zeyu Wang}{equal,bit}
\icmlauthor{Yao-Hui Li}{equal,bit}
\icmlauthor{Xin Li}{bit}
\icmlauthor{Hongyu Zang}{meituan}
\icmlauthor{Romain Laroche}{wayve}
\icmlauthor{Riashat Islam}{mila}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{bit}{Beijing Institute of Technology, Beijing, China}
\icmlaffiliation{meituan}{Meituan, Beijing, China}
\icmlaffiliation{wayve}{Wayve, London, UK}
\icmlaffiliation{mila}{Mila - Quebec AI Institute, Montreal, Canada}

\icmlcorrespondingauthor{Xin Li}{xinli@bit.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{reinforcement learning, multi-view learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Multi-View Reinforcement Learning (MVRL) seeks to provide agents with multi-view observations, enabling them to perceive environment with greater effectiveness and precision. Recent advancements in MVRL focus on extracting latent representations from multiview observations and leveraging them in control tasks. However, it is not straightforward to learn compact and task-relevant representations, particularly in the presence of redundancy, distracting information, or missing views. In this paper, we propose \underline{\textbf{M}}ulti-view \underline{\textbf{F}}usion \underline{\textbf{S}}tate for \underline{\textbf{C}}ontrol (\textbf{MFSC}), firstly incorporating bisimulation metric learning into MVRL to learn task-relevant representations. Furthermore, we propose a multiview-based mask and latent reconstruction auxiliary task that exploits shared information across views and improves MFSCâ€™s robustness in missing views by introducing a mask token. Extensive experimental results demonstrate that our method outperforms existing approaches in MVRL tasks. Even in more realistic scenarios with interference or missing views, MFSC consistently maintains high performance.

% \riashat{test}

\end{abstract}

\section{Introduction}
\label{introduction}
In robotic manipulation tasks, acquiring accurate 3D scene information, including understanding target position, orientation, occlusions, and stacking relationships among objects in complex environments, is crucial for effective grasping and interaction with objects \cite{3D-1, 3D-2}. However, directly using 3D inputs is often bottlenecked by high data collection costs and increased computational complexity. MVRL has recently emerged as a promising alternative, leveraging 2D observations from multiple perspective cameras to enhance spatial perception while reducing computational demands \cite{mvrl}. Despite these advantages, MVRL still struggles with two common challenges: i) \textit{the difficulty in learning and fusing compact representation from redundant and high-dimensional multi-view observations}, and ii) \textit{the problem of dealing with missing or interfering views in real-world scenarios}.

% \citet{mvrl, f2c} learn distinct representations for each view using a variant autoencoders (VAEs)-based architecture and \citet{f2c} derives the final fused representation for MVRL via a weighted fusion mechanism. \citet{lookcloser} introduces the use of the cross-view attention mechanism, a more flexible method for representation fusion. However, none of the aforementioned multi-view representation learning methods possess a clear objective to focus on features directly relevant to the task, often incorporating irrelevant information into the final representation. Recent works on bisimulation metric learning \cite{castro2021mico, DBC, SimSR} capture task-relevant representations by measuring behavioral similarities between states based on their rewards and transition distributions, and appear to be a promising solution to mitigate this issue. 

% \citet{keypoint} learns 3D keypoints from multiple cameras as fused representations through pixel-level reconstruction. 

\citet{keypoint} averages keypoint estimations from different camera views to obtain a single world coordinate prediction. \citet{f2c} learns distinct representations for each view using a VAE-based architecture and derives the final fused representation for MVRL via a weighted fusion mechanism. \citet{lookcloser} introduces the use of a cross-view attention mechanism, offering a more flexible approach for representation fusion. However, these multi-view representation learning methods lack explicit mechanisms to focus on task-relevant features, often incorporating irrelevant information into the final representation. Recent works on bisimulation metric learning \cite{castro2021mico, DBC, SimSR} capture task-relevant representations by measuring behavioral similarities between states based on their rewards and transition distributions, and appear to be a promising solution to mitigate this issue. 

In this work, we integrate the self-attention mechanism with bisimulation metric, offering a powerful approach to multi-view representation learning and fusion. Inspired by BERT \cite{bert} and ViT \cite{ViT}, a learnable fusion \texttt{[state]} embedding for MVRL tasks is introduced to avoid biases from fixed views, and dynamically capture the globally optimal fusion of multi-view information. By aligning the fusion of multi-view representations based on the similarity of task-relevant information, we enable robust multi-view integration and efficient control. However, bisimulation metric learning heavily depends on rewards to fuse state representations, which may cause it to overlook fine-grained information or interactions unique to each view that are not directly reflected in the reward signal. Secondly, bisimulation assumes complete and high-quality observations, making it vulnerable to scenarios where observations are incomplete, as it lacks mechanisms to compensate for missing views. 

\begin{table*}[t!]
\caption{Comparison of different MVRL algorithms.}
\vskip -0.1in
\label{comparison}
\centering
\small
\setlength{\tabcolsep}{2.4pt}
\begin{tabular}{lcccccc}
\toprule
\textbf{Method} & \textbf{Masking} & \textbf{Applying Demos} & \textbf{Reconstruction} & \textbf{Framework} & \textbf{Multi-view Methods}  \\
\midrule
\colorcell Keypoint3D \cite{keypoint} & \colorcell \usym{2717} & \colorcell \usym{2717} & \colorcell Pixel & \colorcell Encoder-Decoder & \colorcell 3D keypoints\\
LookCloser \cite{lookcloser}& \usym{2717} & \usym{2717} & \usym{2717} & Encoder only & Cross-attention  \\
\colorcell F2C \cite{f2c} & \colorcell \usym{2717} & \colorcell \usym{2717} & \colorcell \usym{2717} & \colorcell Encoder only & \colorcell CVIBs \\
MV-MWM \cite{mvmwm} & Feature+View & \usym{2713} & Pixel & Encoder-Decoder & Self-attention  \\
\colorcell MVD \cite{mvd}& \colorcell \usym{2717} & \colorcell \usym{2717} & \colorcell \usym{2717} & \colorcell Encoder only & \colorcell Contrastive learning  \\
\midrule
\hcolorcell \textbf{MFSC} (ours) & \hcolorcell Pixel & \hcolorcell \usym{2718} & \hcolorcell Latent & \hcolorcell Encoder only & \hcolorcell Self-attention+Bisimulation \\
\bottomrule
\end{tabular}
\vskip -0.1in
\end{table*}

To overcome these limitations, we propose an enhanced approach that integrates multiview-based masking and latent reconstruction along with bisimulation metric learning. The bisimulation-driven reconstruction helps ensure that the learned state representations are not only aligned with the task's global objectives but also retain crucial details that are specific to each individual view. Furthermore, the reconstruction process encourages representation learning to leverage cross-view dependencies to recover the masked information, rather than simply depending on the reward signal alone for state representation learning. Through the auxiliary task of multiview-based masking and latent reconstruction, we naturally introduce a learnable mask token to replace missing views, bolstering the model's capability to handle missing views. The key contributions of our work are summarized below:

\begin{itemize}
    \item We propose a novel multi-view representation learning and fusion algorithm MFSC, which is the first to incorporate bisimulation into the MVRL framework. This integration enables the extraction of task-relevant representations from redundant multi-view observations. Our method can be combined with any existing RL algorithms with minimal adjustments.
    \item We propose a multiview-based masking and latent reconstruction auxiliary task that significantly enhances multi-view representation fusion.  By introducing a learnable mask token, our method improves robustness against missing views.
    \item Extensive experiments on motion and robotic manipulation tasks demonstrate that our method outperforms existing MVRL algorithms, consistently achieving superior performance even in challenging scenarios with missing or noisy views.
\end{itemize}

\section{Related Work}
\textbf{Multi-View Representation Learning.} Multi-view representation learning uses multiple sources (or views) of a shared context. These views may include multiple camera views of the same scene (as in our work), as well as multimodal inputs (e.g., audio and video) or synthetic views of the unimodal measurements (e.g., time-sequenced data). \citet{li2018survey} categorized existing methods into two approaches: representation alignment and representation fusion. Representation alignment aims to capture the relationships among multiple different views through feature alignment, such as minimizing the distance between representations of different views \cite{feng2014cross, li2003multimedia}, maximizing similarity between views \cite{bachman2019learning, frome2013devise}, and maximizing the correlation of variables across views \cite{andrew2013deep}. Representation fusion integrates representations from different views to form a compact representation for downstream tasks \cite{geng2022multimodal, xie2020joint, karpathy2015deep}. Both strategies aim to harness the complementary information provided by multiple views for more comprehensive data representation. In this work, we propose a multi-view representation fusion method designed for control. Our approach leverages the properties of sequential decision-making tasks and multi-view data to achieve task-relevant and robust representation fusion.

\textbf{Multi-View Reinforcement Learning.} Effective state representation learning in MVRL aims to transform high-dimensional observations into a compact latent space. \citet{mvrl} propose a VAE-based algorithm that minimizes Euclidean distance between states encoded from different views, assuming a consistent primary view. Keypoint3D \cite{keypoint} uses 3D reconstruction to learn keypoints from third-person views, requiring camera calibration. Lookcloser \cite{lookcloser} introduces a cross-view attention mechanism for aggregating egocentric and third-person representations without calibration, though at higher computational cost. F2C \cite{f2c} uses conditional variational information bottlenecks (CVIBs) to model state space, showing robustness to missing views. MV-MWM \cite{mvmwm}, based on Dreamer-v2 \cite{Dreamer-v2}, applies dual masking and pixel-level reconstruction tasks during pretraining. Since MV-MWM relies on expert demonstrations, we did not include a direct comparison.  MVD \cite{mvd} employs contrastive learning to disentangle multi-view representations for control tasks. For a detailed comparison of MFSC with other algorithms, refer to Table \ref{comparison} and Appendix \ref{appendix:related work}.

\section{Preliminaries}
This section establishes the foundational concepts underlying our method, i.e., the Multi-View Markov Decision Process (MV-MDP) and the bisimulation metric. 

\subsection{Multi-View Markov Decision Processes}
A Multi-View Markov Decision Process is defined as the following tuple \( \mathcal{M}=\langle \mathcal{S}, \mathcal{A}, \mathcal{\vec{O}}, \mathcal{P}, \Omega, \mathcal{R}, \gamma \rangle \). \( \mathcal{S} \) represents the set of states \( s \) in the environment, \( \mathcal{A} \) is a set of actions \( a \), \( \mathcal{\vec{O}} = \{\mathcal{O}^k\}_{k=1}^{K} \) represents the set of \( K \) observations \(\vec{o} = \{o^k\}_{k=1}^{K}\). With the assumption that each multi-view observation \( \vec{o} \in \mathcal{\vec{O}} \) uniquely determines its generating state \( s \in S \), we can obtain the latent state regarding its multi-view observation by a projection function \(\phi \left( \vec{o} \right) :\vec{\mathcal{O}}\rightarrow \mathcal{S}\). Therefore, \(s\) and \( \phi \left( \vec{o} \right) \) can be used interchangeably. \( \mathcal{P}(s'|s,a) = \Pr(s_{t+1}=s' | s_t=s, a_t=a) \) is the transition dynamics distribution. The corresponding transition function under the multi-view observation space is defined \( \vec{o}' \sim \mathcal{\hat{P}}(\vec{o}'|\vec{o},a) \), where \( \hat{\mathcal{P}}\left( \vec{o}'|\vec{o},a \right) =\Omega \left( \vec{o}'|s' \right) \mathcal{P}\left( s'|s,a \right) \) and \( \Omega \left( \vec{o}|s \right) =\prod_{k=1}^K{\text{Pr}}\left( o_{t}^{k}=o^k|s_t=s \right) \) is the probability distribution of joint observation. \( \mathcal{R}(s, a) \in \mathbb{R} \) is the immediate reward function for taking action \( a \) at state \( s \), \( \gamma \in \left[ 0, 1 \right) \) is the discount factor. The goal of the agent is to find the optimal policy $\pi(a|s)$ to maximize the expected reward: \( E_{s_0, a_0, \dots}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)\right] \).


\subsection{Bisimulation}
Bisimulation is used to formalize the concept of state similarity. The core principle is that two states are deemed equivalent if their future behaviors, i.e. state transitions and rewards given the same actions, are indistinguishable under the same policy. \citet{castro2021mico} introduced the MICo update operator as a mathematical tool for constructing a $\pi$-bisimulation metric. This metric evaluates state similarity by focusing exclusively on the actions dictated by a specific policy $\pi$, rather than considering the entire action space.
\begin{definition}\citet{castro2021mico}
\label{mico-definition}
Let $\mathcal{M}$ be the set of all measures over $S$. The MICo update operator $\mathcal{F}^{\pi}: \mathcal{M} \to \mathcal{M}$ is defined as:
\[
\mathcal{F}^{\pi}(g)(s_i, s_j) = (1-c) \cdot|r^{\pi}_{s_i} - r^{\pi}_{s_j}| + c \cdot\mathcal{W}(g)(\mathcal{P}^{\pi}_{s_i}, \mathcal{P}^{\pi}_{s_j}),
\]
where $s_i, s_j \in S$, $r^{\pi}_{s_i} = \sum_{a \in A}\pi(a|s_i) r_{s_i}^a$, $\mathcal{P}^{\pi}_{s_i} = \sum_{a \in A} \pi(a|s_i) \mathcal{P}_{s_i}^a$, and $\mathcal{W}(g)$ denotes the Wasserstein distance between distributions, computed with cost function $g$. $c$ controls the trade-off between prioritizing immediate reward differences and accounting for long-term future state dynamics.
\end{definition}

\begin{theorem}\citet{castro2021mico}
\label{pai-bisimulation}
The MICo update operator $\mathcal{F}^{\pi}$ has a unique fixed point $g^{\pi}_{\sim}$.
\end{theorem}


\begin{figure*}[t!]
% \vskip -0.2in
\begin{center}
\includegraphics[width=1.0\linewidth, trim={0 0 0 20mm}, clip]{results/framework.pdf}
\end{center}
\vskip -0.6in
\caption{Framework of MFSC. (a) The \textbf{left} part illustrates the process of MVRL, where the agent receives observations from multiple views, learns a fused latent state, and interacts with the environment through an actor-critic framework. (b) The \textbf{middle} part provides a detailed overview of the MFSC architecture. Each view is encoded into a latent embedding via a CNN, followed by state fusion using the Self-Attention Fusion Module. Metric learning is guided by bisimulation loss, and a mask-based self-supervised auxiliary task is employed to enhance the model's cross-view learning capabilities. (c) The \textbf{right} part presents the inner workings of the Self-Attention Fusion Module, which integrates embeddings from different views through attention mechanisms to produce a unified state representation.}
\vskip -0.1in
\label{framework}
\end{figure*}


\section{Theoretical Analysis}
\label{section4}
MVRL introduces the challenge of dealing with observations that may contain redundant or irrelevant information across different views. To address this, we employ bisimulation metrics to capture task-relevant information from multi-view observations. This section formalizes our theoretical framework, extending the bisimulation metric from single-view RL to multi-view setting and demonstrating its effectiveness in simplifying MVRL tasks.

\subsection{Bisimulation Metric in Multi-View Setting}
Formally, we define the bisimulation metric for policy $\pi$ on a multi-view setting as: 
\begin{align}
\mathcal{F}^{\pi} G(\vec{o}_i, \vec{o}_j) = &(1-c) \cdot|r^{\pi}_{\vec{o}_i} - r^{\pi}_{\vec{o}_j}| \nonumber \\
&+ c \cdot \mathbb{E}_{\vec{o}'_i \sim \hat{\mathcal{P}}^{\pi}_{\vec{o}_i}, \vec{o}'_j \sim \hat{\mathcal{P}}^{\pi}_{\vec{o}_j}}[G(\vec{o}'_i, \vec{o}'_j)],
\label{ep4.1-1}
\end{align}
where $\vec{o}_i, \vec{o}_j\in \mathcal{O}^1\times \mathcal{O}^2\times\cdots\times \mathcal{O}^k$, and the operator $\mathcal{F}^{\pi}$ has a unique fixed point $G^{\pi}$. Learning the exact bisimulation metric is computationally challenging. In this paper, we employ a ``base metric'' by setting $G^{\pi}$ as a simple cosine distance and guide the representation learning, a learnable aggregator $\phi$, to align with the behavioral pattern demonstrated in Eq.\eqref{ep4.1-1}. Consequently, Eq.\eqref{ep4.1-1} is rewritten as:
%approximate it in the embedding space using a learned aggregator $\phi: \mathcal{O}^1\times \mathcal{O}^2\times\cdots\times \mathcal{O}^k\rightarrow \mathbb{R}^d$: 
\begin{align}
    \mathcal{F}^{\pi} G^{\pi}(\phi &\left( \vec{o}_i \right), \phi \left( \vec{o}_j \right)) = (1-c) \cdot|r^{\pi}_{\phi \left( \vec{o}_i \right)} - r^{\pi}_{\phi \left( \vec{o}_j \right)}| \nonumber \\
    &+ c \cdot \mathbb{E}_{\vec{o}'_i \sim \hat{\mathcal{P}}^{\pi}_{\vec{o}_i}, \vec{o}'_j \sim \hat{\mathcal{P}}^{\pi}_{\vec{o}_j}}[G^{\pi}(\phi \left( \vec{o}'_i \right), \phi \left( \vec{o}'_j \right))].
    \label{ep4.1-2}
\end{align}
The aggregator $\phi$ maps multi-view observations into a more compact space of summarizations $\mathcal{Z}$, defined as: $\phi: \mathcal{O}^1\times \mathcal{O}^2\times\cdots\times \mathcal{O}^k\rightarrow \mathcal{Z}\in \mathbb{R}^d$. By clustering observations to be similar under the bisimulation metric, the original MVRL problem can be approximated as solving a latent MDP \( \bar{\mathcal{M}}=\langle \mathcal{Z}, \mathcal{A}, \bar{\mathcal{P}}, \Omega, \bar{\mathcal{R}}, \gamma, p_0 \rangle \).

\subsection{Latent MDP Construction Assumptions}
We establish the following assumption to support the construction of the latent MDP:
\begin{assumption}
\label{assumption1}
There exists a latent space $\mathcal{Z}$ with dimension lower than that of $\mathcal{O}^1 \times \mathcal{O}^2 \times \cdots \times \mathcal{O}^k$, and a bounded approximation error $\eta > 0$, such that the induced latent MDP  \( \bar{\mathcal{M}}=\langle \mathcal{Z}, \mathcal{A}, \bar{\mathcal{P}}, \Omega, \bar{\mathcal{R}}, \gamma, p_0 \rangle \) satisfies:
\begin{enumerate}
    \item \textbf{Markovian Property}: $\bar{\mathcal{P}}(z'|z,a) = \int_{\vec{o}\,'} \hat{\mathcal{P}}(\vec{o}\,'|\vec{o},a)$ for all $\vec{o}=(o_1,o_2,...,o_k)\in \mathcal{O}^1\times \mathcal{O}^2\times\cdots\times \mathcal{O}^k$ such that $\phi(\vec{o})=z\in \mathcal{Z}$, and same for $\bar{\mathcal{R}}$.
    \item \textbf{Value Function Approximation}: For every $\vec{o} \in \mathcal{\vec{O}}$, there exists $z \in \mathcal{Z}$ such that $|V^*(\vec{o})-V^*(z)|\leq\eta.$
\end{enumerate}
\end{assumption}
This assumption ensures that $\mathcal{Z}$ retains sufficient information for value estimation while reducing redundancy.
\subsection{Bounding Value Function Differences}
To quantify the impact of the above transformation, we derive the following lemma:
\begin{lemma}[Value Function Bound] 
\label{lemma1}
For a latent MDP $\bar{\mathcal{M}}$ constructed by an aggregator $\phi: \mathcal{O}^1\times \mathcal{O}^2\times\cdots\times \mathcal{O}^k\rightarrow \mathcal{Z}$ that clusters multi-view observations in a $\varepsilon$-neighborhood. The difference between the optimal value functions of the original and latent MDPs is bounded as:
\begin{equation}
    |V^*(\vec{o})-V^*(\phi(\vec{o}))|\leq\frac{2\varepsilon}{(1-\gamma)(1-c)},
\end{equation}
where $c \in [0, 1)$ is the trade-off parameter in the bisimulation metric, and $c \geq \gamma$.
\end{lemma}
\vskip -0.1in
The proof can be found in the Appendix \ref{proof}. Lemma \ref{lemma1} serves to establish a bound on the difference between the optimal value functions of multi-view observations and their corresponding clusters in a simplified MDP. By leveraging the learned aggregator, we can effectively reduce the complexity of the multi-view MDP's state space while maintaining a predictable level of accuracy in value function estimation. 

\section{Learning Fused State Representations}
In this section, we detail how the aggregator $\phi$ is constructed to produce fused state representations. Fig.\ref{framework} outlines the overall framework, which comprises two key components: \textbf{(i)} \textbf{Self-Attention Fusion Module}, which integrates bisimulation metrics to guide the aggregator in extracting task-relevant features from multi-view observations; and \textbf{(ii)} \textbf{Multiview-based Masking and Latent Resconstruction}, which serves as an auxiliary task to further promote cross-view state aggregation.

\subsection{Self-Attention Fusion Module}
In this section, we will introduce how bisimulation can be incorporated into multi-view representation learning and aggregation. Specifically, our method comprises two core submodules: \textbf{(i)} \textbf{Convolutional Feature Embedding}, generating embeddings of the original high-dimensional multi-view observations; \textbf{(ii)} \textbf{Self-Attention Fusion Module}, integrating multi-view representations based on bisimulation metric.

\textbf{Convolutional Feature Embedding.} The feature encoding module uses a Convolutional Neural Network (CNN) encoder to encode single-view image observations into fixed-dimensional embeddings. Given a multi-view observations $\vec{o} = \{o^{1}, o^{2}, \ldots, o^{k}\}$, where $o^{i} \in \mathbb{R}^{H \times W \times C}$, the CNN encodes each image into a single-view representation $x^{i}$, where $x^{i} \in \mathbb{R}^{d}$.

\textbf{Self-Attention Fusion Module.} Similar to the [\texttt{class}] token used in BERT \cite{bert} and ViT \cite{ViT}, we prepend a learnable fusion \texttt{[state]} embedding $x^{0} \in \mathbb{R}^{d}$ to the sequence of multi-view embedded representations. The state fusion representation $x^{0}$, trained via self-attention and the bisimulation metric, serves as the final fused representation of multi-view observations and is used for downstream RL tasks. Additionally, akin to ViT, a trainable positional embedding 1D tensor $E_{pos}$ is incorporated into the multi-view embeddings to encode view-specific information:
\begin{equation}
z_{0} = [x^{0}, x^{1}, x^{2}, \ldots, x^{k}] + E_{pos}, \: E_{pos}\in \mathbb{R}^{\left( k+1 \right) \times d}.
\end{equation}
The embedded sequence is then fed into the Self-Attention Fusion Module. Specifically, the Self-Attentioni Fusion Module consists of $L$ attention layers. Each layer is composed of a Multi-Headed Self-Attention (MHSA) layer, a layer normalization (LN), and Multi-Layer Perceptron (MLP) blocks. The process can be described as follows:
\begin{equation}
z'_\ell = \text{MHSA}(\text{LN}(z_{\ell-1})) + z_{\ell-1}, \quad \ell = 1 \dots L
\end{equation}
\begin{equation}
z_\ell = \text{MLP}(\text{LN}(z'_{\ell})) + z'_{\ell}. \quad \ell = 1 \dots L
\end{equation}
The output after $L$ attention layers is $z_L = \{z^{0}_L, z^{1}_{L},$ $\dots, z^{k}_{L}\}$, where $z^{0}_L$ represents the final state fusion embedding. To extract task-relevant representations from multi-view observations, bisimulation metric learning is integrated into the state fusion process. Specifically, the bisimulation metric for a given policy $\pi$ (Eq.\ref{ep4.1-2}) employs the measurement $G$, as defined in SimSR \cite{SimSR}. Here, $G$ is based on cosine distance, which offers lower computational complexity compared to the Wasserstein distance while effectively mitigating representation collapse.

% To capture the task-relevant representations from multi-view observations, bisimulation metric learning is introduced in the process of state fusion. Consider the bisimulation metric on policy $\pi$ in Equation \ref{ep4.1-2}, the measurement $G$, as in SimSR \cite{SimSR}, is defined using the cosine distance, which has lower computational complexity compared to the Wasserstein distance and effectively prevents the collapse of the representation. 

In RL, the critic in actor-critic algorithms like SAC \cite{SAC}, can be decomposed into two function approximators, $\psi$ and $\phi$, with parameters $\theta$ and $\omega$ respectively: $Q_{\theta ,\omega}=\psi _{\theta}\left( \phi _{\omega}\left( \vec{o} \right) \right)$. Here, $\psi_{\theta}$ serves as the value function approximator, while $\phi_{\omega}$ is the state aggregator, with the goal of aligning the distances between representations to match the cosine distance. Therefore, the parameterized representation distance $G_{\phi _{\omega}}$ can be defined as an approximation to the original observation distance $G^{\pi}$: 
\begin{align}
    G^\pi(\vec{o}_i,\vec{o}_j)&\approx G_{\phi _{\omega}}(\vec{o}_i,\vec{o}_j):= 1-\cos _{\phi _{\omega}}(\vec{o}_i,\vec{o}_j) \nonumber \\
    &=1-\frac{\phi _{\omega}(\vec{o}_i)^T\cdot\phi _{\omega}(\vec{o}_j)}{\|\phi _{\omega}(\vec{o}_i)\|\cdot\|\phi _{\omega}(\vec{o}_j)\|}.
\end{align}
Therefore, the objective of state fusion with bisimulation metric is:
\begin{align}
    &\mathcal{L}_{fus} =\mathbb{E}_{\left( \vec{o}_i, r, a, \vec{o}_{i}^{'} \right) ,\left( \vec{o}_j,r,a,\vec{o}_{j}^{'} \right) \sim \mathcal{D}}\left( G_{\phi _{\omega}}\left( \vec{o}_i,\vec{o}_j \right) -G_{\odot} \right) ^2.\nonumber\\
    &\left\{\begin{array}{l}
    G_{\odot}=\left| r_{\vec{o}_i}^{\pi}-r_{\vec{o}_j}^{\pi} \right|+\gamma G_{\phi _{\omega}} (s_{i}',s_{j}') \\
    s_{i}'\sim \mathcal{P}\left( \cdot \left| \phi _{\omega} \right. \left( \vec{o}_i' \right) ,a \right) ,s_{j}'\sim \mathcal{P}\left( \cdot \left| \phi _{\omega}\left( \vec{o}_j' \right) ,a \right. \right)
    \end{array}\right.  
    \label{eq:fusion}
\end{align}
$\mathcal{P}$ is latent state dynamics model and $\mathcal{D}$ is the replay buffer. For detailed explanations of the latent state dynamics model and the reward scaling mechanism, please refer to the Appendices \ref{sec:dyn} and \ref{sec:reward}. By incorporating bisimulation metrics during state aggregation, our method is able to focus on the causal features that directly influence rewards, effectively integrating information from multi-view observations.

\begin{figure*}[t!]
% \vskip -0.4in
\begin{center}
\includegraphics[width=1.0\linewidth]{results/meta_world_5.pdf}
\end{center}
\vskip -0.1in
\caption{Performance comparison on five robotic arm manipulation tasks from Meta-World. All curves show the mean and its 95\% Confidence Intervals (CIs) of performance across four independent seeds. The black dashed line represents the final convergence result of F2C in Meta-World.}
\vskip -0.2in
\label{fig:metaworld}
\end{figure*}

\subsection{Multiview-based Masking and Latent Reconstruction}
Although bisimulation learning provides an effective approach to aggregating task-relevant information across multiple views, it overlooks the local details within each view, as it focuses on global state transitions and task reward rather than the details of each individual view. To address these challenges, we introduce a multiview-based masking and latent reconstruction auxiliary task, fostering the learning of cross-view dependencies. The masking and reconstruction mechanism strengthens bisimulation's capacity to capture richer multi-view information, enabling more robust fusion and control. 

% To learn more compact and task-relevant representations from multi-view observations, we employ a Mask-based Latent Reconstruction strategy in addition to bisimulation metric learning. In visual RL tasks, previous works \cite{MLR, mask} have shown that the significant spatio-temporal redundancy can be eliminated by mask-based reconstruction methods. Consequently, we reconstruct spatially masked pixels in the latent space by leveraging potential correlations between multiple views. Compared to reconstruction in the original pixel space, reconstructing the inferred state representations from the unmasked frames preserves essential control information while reducing unnecessary spatial redundancy.

Specifically, we randomly mask a portion of the original multi-view image observations $\{o^{1}, o^{2}, \ldots, o^{k}\}$. The masked observation sequences $\{\tilde{o}^1,\tilde{o}^2,...,\tilde{o}^k\}$ are then processed through the CNN Encoder and the Self-Attention Fusion Module, resulting in a set of masked state embeddings $\{\tilde{z}_{L}^{0},\tilde{z}_{L}^{1},...,\tilde{z}_{L}^{k}\}$. Motivated by the success of SimSiam \cite{simsiam} in self-supervised learning, we use an asymmetric architecture to calculate the distance between the reconstructed latent states and the target states. The state embeddings of the masked observations are passed through a prediction head to get the final reconstructed/predicted states $\{\hat{z}_{L}^{0},\hat{z}_{L}^{1},...,\hat{z}_{L}^{k}\}$. We construct the reconstruction loss using cosine similarity, ensuring that the final predicted result closely approximates its corresponding target. The final objective function of masking and latent reconstruction can be formulated as:
\begin{equation}
    \mathcal{L}_{rec}=1-\frac{1}{k+1}\sum_{i=0}^{k}{\frac{\left( \hat{z}_{L}^{i} \right) ^T\cdot z_{L}^{i}}{\lVert \hat{z}_{L}^{i} \rVert \cdot \lVert z_{L}^{i} \rVert}}.
    \label{eq:rec}
\end{equation}

The masking and latent reconstruction serves as an auxiliary task and is optimized together with the multi-view state fusion module. Thus, the overall loss function of MFSC is:
\begin{equation}
\label{MFSC_objective}
\mathcal{L}_{mfsc}=\mathcal{L}_{fus} + \lambda\mathcal{L}_{rec}
\end{equation}
$\lambda$ represents the weighting coefficient for the two loss functions.

\section{Experiment}
Through our experiments, we aim to investigate the following questions: (1) How does the performance of MFSC compare to existing MVRL algorithms? (2) Has MFSC successfully captured task-relevant representations? And, how does the Self-Attention Fusion Module operate across different views? (3) How does the MFSC perform in the presence of noise interference? (4) To what extent can MFSC handle tasks with missing views? (5) What is the relative importance and contribution of each part of MFSC?
\subsection{Setup}
\textbf{Experimental Setup.} We evaluate our method on a set of 3D manipulation environments Meta-World \cite{yu2020meta} and a high degree of freedom 3D locomotion environment PyBullet's Ant \cite{coumans2022python}. These environments are originally designed for state-based RL, posing significant challenges for pixel-based RL. Furthermore, we also evaluate MFSC on the widely used single-view continuous control benchmark DMControl \cite{dmc}, comparing it with recent visual RL baselines, as well as in the more realistic multi-view highway driving scenario, CARLA \cite{carla}. More details about the experiments can be found in the Appendix \ref{add_exp_details}.

For fair comparison, we adopt the same Meta-World experimental setup as described in \citet{keypoint}. Details regarding the specific task settings of Meta-World can be found in the Appendix \ref{env_details}. PyBullet's Ant is highly dynamic, with different colors assigned to adjacent limbs to enhance the agent's fine-grained understanding of movable joints and components. To further validate whether our algorithm can still capture task-relevant information, we also evaluate it in a more practical \textit{`Ant No Color'} task, where no explicit visual cues are provided.

\textbf{Baselines.} We compare MFSC with several baseline methods. All baselines, including MFSC, are implemented with PPO \cite{PPO}. The baselines include: (1) Keypoint3D \cite{keypoint}, which learns 3D keypoints from multiple third-person view cameras; (2) LookCloser \cite{lookcloser}, which applies cross-attention between pairs of views to integrate multi-view information; (3) Fuse2Control (F2C) \cite{f2c}, which employs an information-theoretic approach to learn a state space model and extract information independently from each view; (4) MVD \cite{mvd}, a contrastive learning-based algorithm for disentangling multi-view representations. Additionally, for common RL algorithms, we stack images from all three views to form the observation: (5) RAD \cite{RAD}, which achieves high sample efficiency through data augmentation; (6) Vanilla PPO, the original PPO algorithm \cite{PPO}.

\subsection{Evaluation on Meta-World and PyBullet's Ant}
\textbf{Meta-World.} As shown in Fig.\ref{fig:metaworld}, our method consistently outperforms other MVRL algorithms across all five tasks, exhibiting significantly higher sample efficiency and more stable performance. In particular, Vanilla PPO shows scarcely any indication of performance improvement, indicating the difficulty in extracting meaningful fused representations without any special designs. While RAD integrates data augmentation and performs well in simpler tasks, it struggles to learn effective fused representations in more complex tasks, such as \textit{`Open Window'} and \textit{`Close Box'}. Keypoint3D demonstrates competitive performance in certain tasks, particularly in \textit{`Close Box'}, but its overall training efficiency and final performance remain suboptimal. LookCloser, which is also based on Transformer architecture, is also effective. However, its overall performance has not yet exceeded that of MFSC. Moreover, our method outperforms the previous SOTA algorithm, F2C, in terms of convergence performance across the majority of tasks.

\begin{figure}[htbp]
\vskip -0.1in
\begin{center}
\includegraphics[width=0.8\linewidth]{results/ant.pdf}
\end{center}
\vskip -0.2in
\caption{Performance on PyBullet's Ant.}
\vskip -0.1in
\label{fig:ant}
\end{figure}

\begin{figure*}[t!]
\begin{center}
\includegraphics[width=1.0\linewidth, trim={5mm 40mm 5mm 40mm}, clip]{results/vis.pdf}
\end{center}
\vskip -0.1in
\caption{Visualization of multi-view fusion for task-relevant representations. The first column of each frame represents snapshots from different views. The second column of each frame displays Grad-CAM heatmaps for each view, generated from the task loss and normalized via min-max scaling. In the third column of each frame, we unify the gradient maps from all three views. We select the top 2.5\% of gradient values across all views, highlight those corresponding pixels in white within each view, and show all other pixels in black.}
\vskip -0.1in
\label{vis}
\end{figure*}

\textbf{PyBullet's Ant.} As shown in Fig.\ref{fig:ant}, the performance of our method in the \textit{`Ant'} environment is comparable to Keypoint3D. During training, our algorithm exhibits stable and consistent performance improvement. Even in the more practical \textit{`Ant No Color'} task, our method exhibits the highest performance, demonstrating its ability to effectively capture and aggregate task-relevant information from multi-view observations. In contrast, Keypoint3D exhibits a significant performance decline after losing key color annotations. 

\subsection{Visualization and Analysis}
To verify that MFSC is indeed capable of capturing task-relevant features, we utilize the Gradient-weighted Class Activation Mapping (Grad-CAM) technique \cite{selvaraju2017grad} to visualize the learned representations of MFSC. As shown in Fig.\ref{vis} (middle column of each frame), MFSC consistently focuses on task-relevant features, such as the target position, robotic arm, or ant's legs, while minimizing focus on less relevant elements, like window edges or the ant's body. This analysis demonstrates MFSC's ability to effectively identify and extract task-relevant representations from each view.

In the \textit{`Open Window'} task (first row), during the initial stage where the goal is to position the robotic arm correctly, all three views provide critical information, and the model distributes its attention accordingly. However, when the goal shifts to opening the window, occlusion in \textbf{View 3} leads the model to focus more on \textbf{View 1} and \textbf{View 2}, which contain more relevant details. In the \textit{`Ant No Color'} task, the importance of information from all three views is relatively balanced, as shown by the uniform distribution of top-gradient pixels, indicating that MFSC allocates attention more evenly.

The visualization above demonstrates that MFSC can effectively extract and fuse task-relevant representations from multiple views. The fused representations enhance the performance of downstream RL tasks by offering a more robust and comprehensive understanding of the environment.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\linewidth, trim={10mm 0mm 20mm 4mm}, clip]{results/robust_view.pdf}
\end{center}
\vskip -0.2in
\caption{Performance on noisy view.}
\vskip -0.2in
\label{robust_view}
\end{figure}

\subsection{Robustness Against Noisy Views}
To evaluate the robustness of MFSC under interference views, we introduce an additional disruptive view, composed of images entirely irrelevant to the task. As depicted in Fig.\ref{robust_view}, both RAD and LookCloser suffer notable performance decline when subjected to noise, particularly in the \textit{`Closer Drawer'} task. Both simple concatenation of observations from different views (RAD) and fused representations derived through attention mechanisms (LookCloser) prove susceptible to the disruptive effects of noise. In contrast, MFSC preserves its high performance despite the introduction of irrelevant information. 

\begin{figure}[htbp]
\vskip -0.1in
\begin{center}
\includegraphics[width=1.0\linewidth, trim={8mm 0mm 8mm 2mm}, clip]{results/robust_vis_2.pdf}
\end{center}
\vskip -0.1in
\caption{Partially visualization of the learned representation on the noisy view.}
\vskip -0.1in
\label{robust_vis_2}
\end{figure}


\begin{figure*}[t!]
    \centering
    \subfigure[]{%
        \includegraphics[width=0.58\textwidth]{results/missing_view.pdf}
        \label{fig:missing_view}
    }
    \hfill
    \subfigure[]{%
        \includegraphics[width=0.39\textwidth]{results/ablation.pdf}
        \label{fig:ablation}
    }
    \vskip -0.1in
    \caption{(a) Performance comparison of MFSC under full-view and missing-view. The black and gray lines represent the final convergence results of F2C. (b) Performance of ablation study.}
    \vskip -0.2in
    \label{fig:main}
\end{figure*}


From the visualization analysis in Fig.\ref{robust_vis_2}, by incorporating bisimulation into the representation learning process, MFSC continues to capture task-relevant features, thereby significantly enhancing its robustness to interference views. More details on visualization and ablation study are in the Appendix \ref{sec:robust_vis}


\subsection{Robustness Against Missing Views}
To explore whether MFSC has the ability to counteract missing views, we systematically evaluate the performance of MFSC under conditions of missing views across three tasks from Meta-World. During training, we explicitly introduced a mask token and leveraged the masking and latent reconstruction to learn shared representations across views. In the testing phase, we simulate real-world scenarios with incomplete view information by randomly omitting observations from one view and replacing the missing view with the mask token. Fig.\ref{fig:missing_view} summarizes the performance comparison between MFSC and two other competitive MVRL algorithms, LookCloser and F2C, under both missing view and full view conditions. The results indicate that, although missing views can affect certain task-specific details, such as object position or robotic arm posture, MFSC still demonstrates improved convergence speed and results across the majority of tasks. Compared to LookCloser, the performance degradation of MFSC is relatively limited across all tested tasks, and it even achieves performance comparable to the previous SOTA algorithm for missing views, F2C. This robustness to missing views is largely attributed to the introduction of multiview-based masking and latent reconstruction, which leverages cross-view dependencies to promote robust shared representation learning.

\subsection{Ablation Study}
Fig.\ref{fig:ablation} illustrates the returns of the algorithm across two benchmarks, Meta-World and PyBullet's Ant, comparing three variations: MFSC (the proposed method), MFSC without bisimulation constraints (`MFSC w/o Bis'), MFSC without Masking and Latent Reconstruction (`MFSC w/o MLR'), and MFSC without Self-Attention Fusion Module (`MFSC w/o SAFM'), which means an average of the embeddings for each view. 

MFSC (red line), as the complete method, achieves the highest returns throughout the process. In contrast, removing the bisimulation constraint in MFSC (orange line) not only significantly degrades performance but also increases variance, which highlights the importance of bisimulation in MFSC. `MFSC w/o MLR' (blue line) performs better than `MFSC w/o Bis' (orange line) but still falls short of the full MFSC method, emphasizing the significance of learning cross-view information. Finally, replacing the Self-Attention Fusion Module (green line) with feature averaging leads to a noticeable performance drop, demonstrating that the self-attention mechanism's dynamic integration of cross-view information is more effective than simple averaging.

% The results indicate that models lacking these components perform poorly compared to the complete model, highlighting their critical importance within our framework. 


\section{Discussion}
\textbf{Limitations and Future Work.} One limitation of our approach lies in its reliance on masking techniques to handle missing views. For certain views containing crucial information, accurately reconstructing the true environmental state remains challenging, even when inferring from other observations. This is due to the incomplete complementarity of information between multiple views, particularly in scenarios involving complex state transitions or occlusions. To address this limitation, future research could explore the integration of state-space models \cite{lee2020stochastic} to better capture temporal dependencies, enabling more accurate state estimation even in the absence of certain views. 

\textbf{Conclusion.} We propose a novel framework, Multi-view Fusion State for Control (MFSC), to address the challenge of representation learning in MVRL. MFSC integrates the self-attention mechanism with bisimulation metric learning, guiding the fusion of task-relevant representations from multi-view observations. Additionally, we introduce an auxiliary task, multiview-based masking and latent reconstruction, to capture cross-view information and enhance robustness to missing views. Experimental results demonstrate that MFSC effectively aggregates task-relevant details and shows robustness in scenarios with missing views. Finally, visualization analyses confirm the capability of MFSC to capture task-relevant information and dynamically fuse multi-view observations.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\newpage
\section*{Impact Statement}
The proposed MFSC framework advances MVRL by tackling key challenges in learning task-relevant representations from multi-view observations. By integrating bisimulation metric learning with a masking-based latent reconstruction auxiliary task, MFSC enhances robustness and adaptability in environments with redundant, distracting, or incomplete inputs. These improvements are particularly beneficial for applications such as autonomous robotics, self-driving systems, and intelligent surveillance, where robust perception under real-world uncertainties is critical.

From an ethical standpoint, biases in sensor placement, data modalities, or patterns of missing observations could introduce unintended disparities in learned policies. Future research should explore strategies to mitigate these biases, ensuring consistent and reliable deployment across diverse scenarios. Moreover, as reinforcement learning gains traction in safety-critical domains, rigorous validation and improved interpretability of learned policies are essential to prevent unintended consequences and enhance trustworthiness.

\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proofs}
\label{proof}
\textbf{Lemma \ref{lemma1}.} \textit{Given a latent MDP $\bar{\mathcal{M}}$ constructed by a learned aggregator $\phi: \mathcal{O}^1\times \mathcal{O}^2\times\cdots\times \mathcal{O}^k\rightarrow \mathcal{Z}$ that clusters multi-view observations in a $\varepsilon$-neighborhood. The optimal value functions of the original MDP and the latent MDP are bounded as:}
\begin{equation}
    |V^*(\vec{o})-V^*(\phi(\vec{o}))|\leq\frac{2\varepsilon}{(1-\gamma)(1-c)}.
\end{equation}

\textit{Proof.} The proof follows straightforwardly from DBC \cite{DBC}. From Theorem 5.1 in \citet{finiteMDP} we have:
\begin{align}
    (1-c)|V^*(\vec{o})-V^*(\phi(\vec{o}))|\leq g(\vec{o},\tilde{d})+\frac{\gamma}{1-\gamma}\max _{u\in \mathcal{O}^1\times \mathcal{O}^2\times\cdots\times\mathcal{O}^k}g(u,\tilde{d}), 
\end{align}
where $g$ is the average distance between a multi-view observation and all other multi-view observations in its equivalence class under the bisimulation metric $\tilde{d}$. By specifying a $\varepsilon$-neighborhood for each cluster of multi-view observations, we can replace $g$:
\begin{align*}
(1-c)|V^*(\vec{o})-V^*(\phi(\vec{o}))| & \leq 2\varepsilon + \frac{\gamma}{1-\gamma}2\varepsilon \\
|V^*(\vec{o})-V^*(\phi(\vec{o}))| & \leq \frac{1}{1-c} \left( 2\varepsilon + \frac{\gamma}{1-\gamma}2\varepsilon \right) \\
& = \frac{2\varepsilon}{(1-\gamma)(1-c)}.
\end{align*}
As $\varepsilon \rightarrow 0$, the optimal value function of the aggregated MDP converges to the original value function. By defining a learning error for $\phi$, $\mathcal{L}:=\sup_{\vec{o}_i,\vec{o}_j\in\mathcal{O}^1\times \mathcal{O}^2\times\cdots\times\mathcal{O}^k}\big|||\phi(\vec{o}_i)-\phi(\vec{o}_j)||_1-{\tilde{d}}(\vec{o}_i,\vec{o}_j)\big|$, we can also update the bound in Lemma \ref{lemma1} to incorporate $\mathcal{L}: |V^*(\vec{o})-V^*(\phi(\vec{o}))| \leq\frac{2\varepsilon + 2\mathcal{L}}{(1-\gamma)(1-c)}$.


\section{Additional Related Work Discussion}
In this section, we provide a more detailed discussion on state representation learning in RL and a detailed comparison between our method and other related works.

\subsection{State Representation Learning in RL} Well-constructed state representations enable agents to better understand and adapt to complex environments, thus improving task performance and decision-making efficiency. For example, methods such as CURL \cite{laskin2020curl} and DrQ series \cite{kostrikov2020image, yarats2021mastering}, leverage data augmentation techniques such as cropping and color jittering to enhance model generalization. However, their performance is highly dependent on the specific augmentations applied, leading to variability in results. Masking-based approaches \cite{mvmwm, yu2022mask, seo2023masked, liu2022masked}, selectively obscure parts of the input to mitigate redundant information and improve training efficiency. Although these methods show promise in filtering out irrelevant data, they carry the risk of unintentionally discarding task-critical information, potentially affecting overall agent performance. Bisimulation-based methods \cite{DBC, SimSR} focus on constructing reward-sensitive state representations to ensure that states with similar values are close in the representation space, promoting sample efficiency and consistent decision-making. Another line of research explores the causal relationships between state representations and control \cite{wang2022causal, lamb2022guaranteed, efroni2021provably, efroni2022sample, fu2021learning}. By analyzing the causal links between states and actions, these methods aim to improve agents' understanding and control of the environment, further optimizing RL performance.


\subsection{Comparison with Additional Related Work} 
\label{appendix:related work}
\textbf{MV-MWM \cite{mvmwm}.} MV-MWM proposed an RL framework that trained a multi-view masked autoencoder for representation learning and a world model to enhance the model-based RL algorithm's robustness to view variations in robotic manipulation tasks. MV-MWM adopts a dual masking strategy at both the convolutional feature and view levels, requiring an additional decoder to perform pixel-level reconstruction, which aims to recover task-specific details from raw observations. Our method applies masking at the pixel level but conducts reconstruction in the latent space, thereby avoiding the reconstruction of task-irrelevant details and focusing on high-level task-related information. Furthermore, MV-MWM incorporates other robust components to enhance performance. For example, it is built on the powerful model-based RL baseline, Dreamer-v2 \cite{Dreamer-v2}, and utilizes expert demonstrations to guide policy learning. In contrast, our method does not employ any of these components. As MV-MWM requires additional expert demonstration data for training, we did not conduct a direct comparison with it in our work.

\textbf{MLR \cite{MLR}.} Although both MLR and our method leverage a mask and latent reconstruction auxiliary task to learn more compact representations, there are notable differences in their implementation details. Firstly, MLR predicts complete state representations in the latent space from observations with spatially and temporally masked pixels. It prioritizes extracting inherent information from images, often overlooking task-relevant information. In contrast, our method integrates bisimulation metric learning, enabling the extraction of task-relevant information from multi-view observations. Secondly, our method employs the self-attention module as a fusion mechanism to learn a fused state representation that directly contributes to downstream RL tasks. Instead, MLR uses the self-attention mechanism merely as an auxiliary loss module to encourage the CNN encoder to capture temporal dependencies within sequences.

\section{Experimental Details}
\label{exp_details}
\subsection{Hyperparameters}
Table \ref{ppo:hparams} and \ref{sac:hparams} provide detailed information regarding the experimental setup and hyperparameter configurations. In the Meta-World and PyBullet environments, we utilize a latent representation size of 128, following the Keypoint3D framework outlined by \citet{keypoint}. In the CARLA and DMControl environment, we utilize a representation size of 50, adhering to the SAC framework described in \citet{DBC}. All networks in both the policy and the representation models are optimized using the Adam optimizer \cite{kingma2014adam}, ensuring consistent performance across various environments.
\begin{table}[h!]
  \caption{MFSC's hyperparameters, based on PPO.
  }
  \centering
  \begin{tabular}{lll}
  \toprule
  \textbf{Hyperparameter} & Meta-World & PyBullet \\
  \midrule
  \multicolumn{3}{l}{\textbf{General}} \\
  \midrule
  Batch size & 6400 & 16000\\
  Rollout buffer size & 100000 & 100000\\ 
  Frame stack & 1 & 2\\ 
  Num of views & 3 & 3\\ 
  Epochs per update & 8 & 10\\
  Discount & 0.99 & 0.99\\
  GAE lambda & 0.95 & 0.95\\
  Clip range & 0.2 & 0.2\\ 
  Entropy coefficient & 0.0 & 0.0\\
  Value function coefficient & 0.5 & 0.5\\
  Gradient clip & 0.5 & 0.5\\
  Target KL & 0.12 & 0.12\\
  Learning rate & $2\times10^{-4}$ & $2\times10^{-4}$\\
  State representation dimension & 128 & 128 \\
  Weight of loss $\lambda$ & 1.0 & 1.0 \\
  \midrule
  \multicolumn{3}{l}{\textbf{MFSC}} \\
  \midrule
  Representation learning rate & $2\times10^{-4}$ & $2\times10^{-4}$\\
  Weight of fusion loss & 1.0 & 1.0\\
  Weight of reconstruction loss & 1.0 & 1.0\\
  Number of dynamics models & 5 & 5 \\
  Mask ratio & 0.8 & 0.8 \\
  Cube spatial size & $12 \times 12$ & $12 \times 12$ \\
  Cube depth & 3 & 3 \\
  Self-attention fusion module depth & 2 & 2\\
  \bottomrule
  \end{tabular}
  \label{ppo:hparams}
  \end{table}

\begin{table}[h!]
  \caption{MFSC's hyperparameters, based on SAC.
  }
  \centering
  \begin{tabular}{lll}
  \toprule
  \textbf{Hyperparameter} & Carla & DMControl\\
  \midrule
  \multicolumn{2}{l}{\textbf{General}} \\
  \midrule
  Batch size & 128 & 128\\
  Replay buffer size & 100000 & 100000\\ 
  Frame stack & 3 & 3\\ 
  Num of views & 3 & 4\\ 
  Discount & 0.99 & 0.99 \\
  Critic Q-function soft-update rate & $0.01$ & $0.01$\\
  Critic target update frequency & $2$ & $2$\\
  Actor update frequency & $2$ & $2$\\
  Init temperature & $0.1$ & $0.1$\\
  Learning rate & $1\times10^{-3}$ & $2\times10^{-4}$\\
  Temperature learning rate & $1\times10^{-4}$ & $1\times10^{-4}$\\
  State representation dimension & 50 & 50 \\
  Weight of loss $\lambda$ & 1.0 & 1.0 \\
  \midrule
  \multicolumn{2}{l}{\textbf{MFSC}} \\
  \midrule
  Representation learning rate & $2\times10^{-4}$ & $2\times10^{-4}$\\
  Weight of fusion loss & 1.0 & 1.0\\
  Weight of reconstruction loss & 1.0 & 0.5\\
  Number of dynamics models & 5 & 5 \\
  Mask ratio & 0.5 & 0.5 \\
  Cube spatial size & $12 \times 12$ & $12 \times 12$\\
  Cube depth & 3 & 3\\
  Self-attention fusion module depth & 2 & 2\\
  \bottomrule
  \end{tabular}
  \label{sac:hparams}
  \end{table}
  
\subsection{Latent State Dynamics Modeling}
\label{sec:dyn}
Following our approach, we develop an ensemble version of deterministic dynamics models $\mathcal{P}_k(\cdot | \phi_{\omega}(x), a)\}_{k=1}^{K}$. Unlike probabilistic dynamics models, our transition models are deterministic and their outputs are consistent with the encoderâ€™s output, both of which are subjected to $l_2$-normalization. Instead of using a probabilistic transition, we calculate the distance using cosine similarity. Specifically, at the training step, we update the parameters of the dynamics models based on the cosine similarity loss function:
\begin{equation}
\begin{aligned}\mathcal{L}_{dyn}&=\frac{1}{K}\sum_{k=1}^{K}\left[1-\frac{\mathcal{P}_k(\cdot|\phi_\omega(\vec{o}),a)\cdot\phi_\omega(\vec{o}')}{\|\mathcal{P}_k(\cdot|\phi_\omega(\vec{o}),a)\|\|\phi_\omega(\vec{o}')\|}\right], \end{aligned}
\label{eq:dyn}
\end{equation}
where $i \in \{1, 2, \ldots, K\}$. Since deterministic models share the same gradient but are initialized randomly, they may still acquire different parameters after training. This ensemble model allows us to estimate the latent dynamics of the environment effectively while ensuring that the output remains consistent across the encoder and dynamics model. At the inference step, we randomly sample one of $K$ deterministic dynamics models to compute the transition to the next latent state $s'$.
% NEURIPS2023_5a166745
\subsection{Reward Normalization}
\label{sec:reward}
Reward normalization is a crucial component of our representation learning approach, as it relies directly on the reward function to guide the extraction and learning of learning. In experimental tasks, the rewards used for representation learning are consistent with those used in policy learning. Following Keypoint3D \cite{keypoint}, to ensure stable learning dynamics, we apply a moving average normalization method to dynamically normalize the reward values. This method calculates the moving average of historical rewards and adjusts the rewards to have a mean of 0 and a standard deviation of 1. This normalization process helps mitigate fluctuations in reward values caused by variations in task difficulty, environmental changes, or exploration strategies, enabling the model to more effectively learn meaningful representations from stable reward signals. Additionally, since the scale of rewards influences the bisimulation metric and the upper bound of value function errors, we adopt reward scaling to avoid feature collapse and reduce bisimulation measurement errors. Following the work of \citet{NEURIPS2023_5a166745}, we apply \(c_r = 1 - \gamma\) and \(c_t = \gamma\) to compute bisimularity.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=1.0\linewidth]{results/view_vis.pdf}
\end{center}
\vskip -0.1in
\caption{Visualization of three view observations in the Meta-World, PyBullet'Ant and CARLA environment. \textbf{Left \& Middle:} Third-person view observations from three different cameras in the \textit{`Close Window'} and PyBullet's Ant task. \textbf{Right:} First-person observations from three different views in the CARLA. It can be observed that in the Meta-World and PyBullet environments, each view exhibits a relatively high degree of coupling, encapsulating a substantial portion of the task-relevant information. However, due to factors such as occlusions (e.g., the robotic arm obstructing the target object or the leg joints in the PyBullet's Ant task), no single view captures the complete state information. In contrast, in the CARLA environment, the information contained in each view is mutually independent, and the three views collectively constitute the environmentâ€™s state.}
\vskip -0.2in
\label{view_vis}
\end{figure}

\section{Environment Details}
\label{env_details}
\textbf{Meta-World.} To evaluate whether our model can accelerate policy optimization when jointly trained with the policy, we perform five complex robotic arm manipulation tasks in the Meta-World environment \cite{yu2020meta}. Each task involves 50 randomized configurations, such as the initial pose of the robot, object locations, and target positions. As shown in Fig.\ref{view_vis}, for each task, we use three third-person cameras from different angles to observe the robot arm and relevant objects. Since the gripper state at the end of the robotic arm may not be clearly visible from any of the three camera angles, following the settings of \citet{keypoint} and \citet{f2c}, an indicator is introduced in the Meta-World tasks to signify whether the gripper is open or closed. This indicator is concatenated with the learned latent state and is fed into the actor network. Due to experimental variations, we adopt the results reported in the F2C paper for comparison.

\textbf{PyBullet's Ant.} The PyBullet's Ant \cite{coumans2022python} task is designed to simulate the motion control of a quadruped robot in a highly dynamic 3D-locomotion environment. The objective of this task is to control the joints of the robot's legs, enabling it to learn how to balance and move as quickly and stably as possible. The Ant robot has a highly dimensional state and action space, which includes physical quantities such as joint angles, angular velocities, and linear velocities. The robot's movement is generated by controlling the torque or force applied to its joints, making a fine-grained understanding of the movable joints and parts essential. As locomotion environments require temporal reasoning, we use a frame stack of 2. The reward function in this task is typically based on the robot's forward velocity while accounting for control costs (energy consumption) to incentivize efficient movement. Due to the complexity of the environment and the high-dimensional action space, the Ant task provides a significant challenge for training and testing RL algorithms.

\begin{wrapfigure}{r}{0.5\textwidth}
\centering
\setlength{\abovecaptionskip}{0cm}
\vskip -0.2in
\includegraphics[width=0.5\textwidth]{results/env_vis.pdf}
\caption{The birdâ€™s-eye view of the \texttt{`Town04'} map and snapshots of training period.}
\label{carla_vis}
\vskip -0.1in
\end{wrapfigure}
\textbf{CARLA.} To evaluate the performance of our approach in more realistic multi-view tasks, we applied MFSC to the vision-based autonomous driving domain based on CARLA \cite{carla}. In this experiment, we select the official \texttt{`Town04'} map, featuring a circular highway with an intersection, as illustrated in Fig.\ref{carla_vis}. The agentâ€™s goal is to drive as safely as possible for a limited 1,000-time steps, avoiding collisions with other moving vehicles or obstacles. Following the experimental setup of DBC \cite{DBC}, the reward function is designed as $r_t\left( s,a \right) =\text{v}^{\text{T}} \hat{\text{u}}_{\textit{highway}}\cdot \varDelta t-\lambda _i\cdot \textit{impulse}-\lambda _s\cdot \left| \textit{steer} \right|$, where $\text{v}^{\text{T}}$ represents the velocity vector of the ego vehicle projected onto the unit vector of the highway $\hat{\text{u}}_{\textit{highway}}$, and then is multiplied by time discretization $\varDelta t$ to represent the effective distance. \textit{Impulse} is the clash force, measured in Newton seconds, and \textit{Steer} is the output of the steering amplitude. As shown in Fig.\ref{view_vis}, we use three cameras on the vehicleâ€™s roof, capturing left, front, and right views of the driver, each with a 60-degree field of vision. The agent must operate the vehicleâ€™s movement through first-person vision under varying conditions of wind, rain, clouds, or sunlight.

\textbf{DeepMind Control Suite.} The DeepMind Control Suite (DMControl) \cite{dmc} is a widely used benchmark environment for evaluating RL algorithms, offering a collection of continuous control tasks with interpretable rewards and standardized structures. Although DMControl is not inherently designed for multi-view inputs, we adapt it by utilizing historical time-series data as a proxy to simulate multi-view observations. This adaptation allows us to explore how temporal and multi-view information can enhance policy optimization. In our experiments, we select representative tasks from DMControl, including balance control, robotic manipulation, and motion planning, covering a range of complexities from simple to moderately challenging continuous control scenarios. 

Multi-view observations in the Meta-World and PyBullet environments exhibit a greater degree of redundancy. This redundancy allows the attention mechanism to effectively focus on shared, useful information across views. However, in CARLA, although multiple views are provided, the differences between views can be much more pronounced. As shown in the right part of Fig.\ref{view_vis}, observations from different views are highly variable (e.g., different roads, and moving vehicles), leading to visual and semantic dissimilarity between different views. The larger differences between views make it harder to fuse the information effectively, and relying solely on attention mechanisms may not capture sufficient cross-view correlations. Therefore, concatenating image observations from multiple views is a straightforward yet effective approach.

\section{Additional Experimental Results}
\label{add_exp_details}
\subsection{Results on DeepMind Control Suite}
\begin{wrapfigure}{r}{0.45\textwidth}
\centering
\vskip -0.2in
\includegraphics[width=0.45\textwidth, trim={5mm 0mm 5mm 0mm}, clip]{results/dmc_input.pdf}
\vskip -0.1in
\caption{``Multi-view" observation in DMControl. Each observation consists of a stack of three consecutive image frames.}
\label{dmc_multiview}
\vskip -0.2in
\end{wrapfigure}
To compare our method with the latest single-view RL methods, we downgrade MFSC to a single-view setting on the DeepMind Control Suite. Specifically, as shown in Fig.\ref{dmc_multiview}, we construct ``multi-view" observations by selecting observations $O_{t-3}, O_{t-2}, O_{t-1}, O_{t}$ from four consecutive time steps. 

We selected the previously high-performing model-free RL algorithms SAC \cite{SAC}, CURL \cite{laskin2020curl}, DrQ-v2 \cite{drq-v2} and TACO \cite{taco}, along with the model-based RL algorithms Dreamer-v3 \cite{dreamerv3} and TD-MPC2 \cite{tdmpc2}. The results of DrQ-v2, Dreamer-v3 and TD-MPC2 are obtained from \citet{drq-v2}, \citet{dreamerv3}, \citet{tdmpc2}. The reproduced results for TACO are implemented by the official code \footnote{\url{https://github.com/FrankZheng2022/TACO}}. The results of SAC and CURL are downloaded from this link \footnote{\url{https://github.com/danijar/dreamerv3/tree/main/scores}}. 

The results, as summarized in Table \ref{dmc:results}, indicate that MFSC, when degraded to a single-view setting, performs competitively with established single-view RL algorithms. Although there is a slight performance gap compared to the SOTA algorithm TACO, MFSC consistently achieves results on par with first-tier methods such as Dreamer-v3 and TD-MPC2 across the evaluated tasks, and it surpasses that of SAC, CURL, and DrQ-v2 across most tasks. Furthermore, these results demonstrate the efficacy of MFSCâ€™s representation learning framework, even when reduced to a sequential single-view scenario. Despite the constrained single-view conditions, MFSCâ€™s ability to remain competitive reaffirms the robustness of its design and its suitability for a wide range of RL tasks.


\begin{table}[htbp]
\caption{Comparison results on DeepMind Control Suite. The results of MFSC and TACO are presented as the mean and standard deviation across four random seed runs. The best score is marked with \hl{\textbf{colorbox}}.}
% \vskip -0.1in
\label{dmc:results}
\centering
\setlength{\tabcolsep}{7.5pt}
\begin{tabular}{l|ccccc|cc}
    \multicolumn{1}{l}{} & \multicolumn{5}{c}{\textit{Model-free RL}}                & \multicolumn{2}{c}{\textit{Model-based RL}} \\
    \midrule
    Task (1M Steps) &  \textbf{MFSC(ours)} & \textbf{SAC}    & \textbf{CURL}  & \textbf{DrQ-v2} & \textbf{TACO}  & \textbf{Dreamer-v3} & \textbf{TD-MPC2}\\
    \midrule
    Cartpole Swingup &  832$\pm$29          & 717$\pm$134     & 758$\pm$155     & \hl{\textbf{865$\pm$15}}     & 864$\pm$2     & 862$\pm$9 & $-$ \\
    Cheetah Run &  679$\pm$33     & 19$\pm$14         & 502$\pm$120     & 710$\pm$52     & \hl{\textbf{915$\pm$6}}     & 841$\pm$87 & 537$\pm$60 \\
    Finger Spin &  977$\pm$7     & 289$\pm$104         & 873$\pm$145     & 860$\pm$104     & 978$\pm$10     & 594$\pm$136 & \hl{\textbf{985$\pm$2}} \\
    Walker walk &  909$\pm$40     & 41$\pm$17         & 818$\pm$261     & 770$\pm$371     & \hl{\textbf{968$\pm$3 }}    & 959$\pm$27 & 940$\pm$8 \\
    \bottomrule
    \end{tabular}%
\vskip -0.1in
\end{table}

\subsection{Results on CARLA} 
The results of CARLA are shown in Table \ref{carla}. MFSC achieves the highest performance on three out of six driving metrics. Our method outperforms SAC on both crucial comparisons of episodic reward and episodic distance. Furthermore, in terms of metrics reflecting vehicular stability during operation, such as average steering amplitude variation, average braking intensity, and collision severity, all algorithms exhibited relatively modest performance. This is attributed to the reward function being primarily crafted to validate representation learning rather than specifically tailored to address safety considerations. Overall, in the CARLA task, our algorithm does not exhibit a significant improvement over the baseline algorithm SAC. This further supports our previous viewpoint that, in multi-view tasks like CARLA, where there is almost no shared information between views, concatenating image observations from multiple views, as in SAC, is a straightforward yet effective approach.

\begin{table}[htbp]
\caption{Comparison results of different driving metrics in CARLA. Results of DrQ \cite{drq} and CURL \cite{laskin2020curl} are obtained from \citet{SAR}. The best score is marked with \hl{\textbf{colorbox}}. $\pm$ corresponds to a standard deviation of three random seeds, after 400k training steps. The direction of the arrow indicates whether the desired metric is large or small.}
% \vskip -0.1in
\label{carla}
\centering
\setlength{\tabcolsep}{14pt}
\begin{tabular}{lcccc}
    \toprule
    Metrics & \textbf{SAC}   & \textbf{DrQ}   & \textbf{CURL}  & \textbf{MFSC(ours)} \\
    \midrule
    Distance (m) $\uparrow$ & $179.60\pm 76.29$ & $158.10\pm1.54$ & $172.50\pm7.60$ & $\hl{\textbf{183.30}}\pm67.39$ \\
    Successes episodes $\uparrow$ & $\hl{\textbf{21\%}}$  & 21\%  & 14\%  & 13\% \\
    Average steer $\downarrow$ & $10.24\%$ & $12.02\%$ & $9.76\%$ & $\hl{\textbf{8.94\%}}$ \\
    Avergae brake $\downarrow$ & $\hl{\textbf{0.94\%}}$ & $1.66\%$ & $1.75\%$ & $1.02\%$ \\
    Crash intensity (N/s) $\downarrow$ & 3750  & \hl{\textbf{3313}}  & 4311  & 4211 \\
    Episode return $\uparrow$ & $152.73\pm4.54$ & $122.80\pm2.58$ & $149.10\pm14.10$ & $\hl{\textbf{168.01}}\pm9.62$ \\
    \bottomrule
    \end{tabular}
\vskip -0.1in
\end{table}

\subsection{Visualization and Analysis on Noisy View}
\label{sec:robust_vis}
\begin{wrapfigure}{r}{0.45\textwidth}
\centering
\setlength{\abovecaptionskip}{0cm}
\vskip -0.2in
\includegraphics[width=0.45\textwidth, trim={5mm 0mm 5mm 0mm}, clip]{results/nosiy_view.pdf}
\caption{Multi-view observations in noisy view setting.}
\label{noisy_view}
\vskip -0.1in
\end{wrapfigure}
To evaluate the robustness of our method under interference views, we introduced a noisy-views setting. Specifically, as illustrated in Fig.\ref{noisy_view}, in addition to the clean views $O_1, O_2, O_3$, that are normally acquired, we incorporate an additional viewpoint $O_4$ that is entirely irrelevant to the task. The noisy view $O_4$ consists of distracting images, all sourced from the \texttt{video\_hard} datasets in \textit{DMControl Generalization Benchmark} (DMControl-GB) \footnote{\url{https://github.com/nicklashansen/dmcontrol-generalization-benchmark}}. We employed the same Grad-CAM technique as in Fig.\ref{vis} to visualize the features of the four views. As shown in the visualization results of Fig.\ref{noisy_view}, our method remains capable of capturing task-relevant details even under the noisy interference setting.


\begin{figure}[htbp]
\begin{center}
\vskip -0.1in
\includegraphics[width=1.0\linewidth]{results/robust_vis.pdf}
\end{center}
\vskip -0.2in
\caption{Visualization of learned representation on noisy view. Pixels with the highest gradient values (top 2.5\%) across all views are marked white while others are marked black.}
\label{robust_vis}
\end{figure}

To investigate whether bisimulation can assist MFSC in countering the interference of noise, we conducted ablation experiments under the setting of interference views. The results confirmed that bisimulation effectively aids MFSC in mitigating the impact of noise, allowing it to maintain its performance even in the presence of disruptive noise. This demonstrates the robustness of MFSC when enhanced with bisimulation, highlighting its ability to focus on relevant features while disregarding irrelevant noise.
\begin{figure}[htbp]
\begin{center}
% \vskip -0.15in
\includegraphics[width=0.35\linewidth, trim={3mm 3mm 3mm 3mm}, clip]{results/robust_view_ablation.pdf}
\end{center}
\vskip -0.1in
\caption{Ablation study on robust view.}
\label{robust_view_ablation}
\end{figure}


\subsection{Hyperparameter Sensitivity Analysis}
\begin{wrapfigure}{r}{0.45\textwidth}
\centering
% \setlength{\abovecaptionskip}{0cm}
\vskip -0.25in
\includegraphics[width=0.45\textwidth]{results/weight.pdf}
\vskip -0.1in
\caption{Results of loss weight tuning experiment. The vertical lines on the barchart represent the standard deviation of the results from the three seeds.}
\label{loss_weight}
% \vskip -0.1in
\end{wrapfigure}
In the overall loss function of MFSC in Eq.\ref{MFSC_objective}, the weighting coefficient $\lambda$ is set to 1, signifying that both loss components contribute equally to the total loss. We also conducted a hyperparameter sensitivity analysis to evaluate the impact of different fusion loss and reconstruction loss contributions on the algorithm's performance. Specifically, we compared three configurations: $\lambda=1$ (original setting), indicating equal contributions from both losses, $\lambda=2$, signifying a greater contribution from the fusion loss, and $\lambda=0.5$, denoting a greater influence from the reconstruction loss. The results are visualized in Fig.\ref{loss_weight} for two tasks: \textit{`Close Box'} and \textit{`Ant No Color'}.

The experimental results indicate that the model performs relatively better when the weights of the two losses are equal (i.e., $\lambda=1$). Increasing or decreasing the weight of either loss may result in performance variations, likely due to the model overemphasizing one loss component while neglecting the importance of the other. This finding underscores the critical importance of balancing the fusion loss and the reconstruction loss to optimize both the overall performance and stability of the model.

\subsection{Results on Different Network Architectures} 
To verify the robustness of our method in different network architectures, we implemented two distinct MFSC network designs and performed evaluations in the Meta-World environment. As illustrated in the left part of Fig.\ref{model_architecture}, the network employed in the paper adopts a structure which is similar to the SimSiam \cite{simsiam} architecture. The encoder includes all layers that can be shared between both branches. To avoid collapsing, we applied a stop-gradient operation to one branch of the network. Furthermore, we also implemented MFSC with a network architecture based on a momentum encoder. As shown in the right half of Fig.\ref{model_architecture}, in addition to the stop-gradient operation, the momentum encoder is updated using the exponential moving average method.
\begin{figure}[t!]
\begin{center}
% \vskip -0.15in
\includegraphics[width=1.0\linewidth]{results/model_architecture.pdf}
\end{center}
\vskip -0.2in
\caption{Comparison on network architectures. \textbf{Left:} SimSiam architecture. \textbf{Right:} Momentum encoder architecture. 
\texttt{'$\textbf{//}$'} represents a stop-gradient operation.}
% \vskip -0.1in
\label{model_architecture}
\end{figure}

In Fig.\ref{metaworld:ema}, it is evident that although the performance of the MFSC with two distinct network architectures differs on certain tasks, such as \textit{`Open Window'} and \textit{`Push w/Wall'}, overall, the algorithms exhibit comparable performance across both architectures. This outcome suggests that our method demonstrates a certain degree of robustness with respect to the design of the network architecture.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\linewidth]{results/ema.pdf}
\end{center}
\vskip -0.1in
\caption{Performance comparison of two different network architectures on Meta-World.}
\vskip -0.1in
\label{metaworld:ema}
\end{figure}

\subsection{Comparison of training resources}
We compared the training time, inference time, and resource requirements of MFSC with the following baselines: single-view methods RAD and Vanilla-PPO, as well as multi-view methods Keypoint3D and LookCloser. The experiments were conducted on a server equipped with two NVIDIA A800 GPUs (Ampere architecture) with 80GB of VRAM each, running on CUDA 12.3 and NVIDIA driver version 545.23.06. The server is powered by dual Intel Xeon Platinum 8370C CPUs, each featuring 32 cores and 64 threads, with a base clock speed of 2.80GHz and a maximum clock speed of 3.5GHz. The system has a total of 128 logical processors and a NUMA configuration with two nodes. The hardware setup ensures sufficient computational resources to train and evaluate the proposed method.
\begin{table}[h]
\caption{Comparison of training resources for different algorithms. The last column represents the improvement in the average score of five tasks in Meta-World compared to the baseline algorithm RAD. Optimal results in the MVRL algorithms are marked as bold and underlined.}
\vskip -0.1in
\label{training_resources}
\centering
\begin{tabular}{llcccc}
\toprule
Category & Method & Training time & Inference time & GPU memory  & Performance improvement\\
\midrule
\colorcella View Concatenating & \colorcella Vanilla PPO & \colorcella 3.8h & \colorcella 2.67ms & \colorcella 850MB & \colorcella $-$\\
\colorcella & \colorcella RAD & \colorcella 3.8h & \colorcella 2.69ms & \colorcella 758MB & \colorcella $-$ \\
\hcolorcell  & \hcolorcell Keypoint3D & \hcolorcell \underline{\textbf{12.2h}} & \hcolorcell 6.01ms & \hcolorcell \underline{\textbf{1406MB}} & \hcolorcell  $21.17\%\uparrow$ \\
\hcolorcell View Fusion & \hcolorcell LookCloser & \hcolorcell 13.9h & \hcolorcell 14.35ms & \hcolorcell 3530MB & \hcolorcell $45.66\% \uparrow$ \\
\hcolorcell & \hcolorcell \textbf{MFSC (ours)} & \hcolorcell 14.9h & \hcolorcell \underline{\textbf{4.26ms}} & \hcolorcell 4098MB & \hcolorcell \underline{\textbf{97.93\%} $\uparrow$}\\
\bottomrule
\end{tabular}
\vskip -0.1in
\end{table}

As shown in Table \ref{training_resources}, due to the complexities of information fusion, model architecture, and cross-view feature extraction, multi-view fusion algorithms typically require more resources than simpler multi-view image concatenating methods. Compared to other multi-view fusion algorithms, our method demands greater training time and GPU memory. However, fortunately, our algorithm significantly outperforms others in terms of inference time and performance improvement. This indicates that while our method may require more resources during the training phase, it offers much more efficient performance during the inference phase, which is a crucial advantage in practical applications.

\section{Algorithm}
Our pseudocode in Algorithm \ref{algorithm1} and Algorithm \ref{algorithm2}, where Algorithm \ref{algorithm1} is based on Proximal Policy Optimization (PPO) and Algorithm \ref{algorithm2} is based on Soft Actor-Critic (SAC). Furthermore, we illustrate the representation learning component of MFSC in Algorithm \ref{algorithm3} using a PyTorch-like style for enhanced clarity and comprehension.

\begin{algorithm}[htbp]
   \caption{Pseudocode of MFSC, based on PPO}
   \label{algorithm1}
\begin{algorithmic}[1] 
   \STATE {\bfseries Input:} iterations to repeat entire processes $N_{\text{Repeat}}$, batch size $B$, rollout length $T$.
   \FOR{$\text{iter} = 1$ {\bfseries to} $N_{\text{Repeat}}$}
   \STATE \textbf{Initialize} $\mathcal{B}_{rollout}$
   \FOR{{$b = 1$ {\bfseries to} $B$}}
    \STATE Run policy $\pi_{\theta_{\text{old}}}$ to collect $(\vec{o}, a, r, \vec{o}')_{1:T}$
    \STATE $\mathcal{B}_{rollout} \gets \mathcal{B}_{rollout} \cup (\vec{o}, a, r, \vec{o}')_{1:T}$
   \ENDFOR
   \STATE Estimate advantage values $\hat{A}_{1:T,1:N}$ on $\mathcal{B}_{rollout}$
   \FOR{{$t = 1$ {\bfseries to} $T$}}
        \STATE Sample $(\vec{o}, a, r, \vec{o}') \sim \mathcal{B}_{rollout}$
        \STATE Patch masking the multi-view observation $\vec{o}$
        \STATE Calculate $\mathcal{L}_{policy}$ \algorithmiccomment{PPO algorithm}
        \STATE Calculate $\mathcal{L}_{fus}$ according to Eq.\ref{eq:fusion}
        \STATE Calculate $\mathcal{L}_{rec}$ according to Eq.\ref{eq:rec}
        \STATE Calculate $\mathcal{L}_{dyn}$ according to Eq.\ref{eq:dyn} 
        \STATE Optimize $\mathcal{L}_{policy} + \mathcal{L}_{rec} + \mathcal{L}_{fus} + \mathcal{L}_{dyn}$ throughout $\mathcal{B}_{rollout}$    
   \ENDFOR
        \STATE $\pi_{\text{old}} \gets \pi$
   \ENDFOR 
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
    \caption{Pseudocode of MFSC, based on SAC}
    \label{algorithm2}
    \begin{algorithmic}[1]
    \STATE {\bfseries Input:} maximum time steps $T$, batch size $B$, replay buffer size $|\mathcal{D}|$, target network update frequency $\tau$.
    \FOR{Time $t=0$ to $T$}
        \STATE{Encode state: $\mathbf{z_t} = \phi(\mathbf{\vec{o}})$}
        \STATE{Execute action: $\mathbf{a_t} \sim \pi(\mathbf{z_t})$}
        \STATE{Record data: $\mathcal{D} \leftarrow \mathcal{D} \cup \{ \vec{o}_{t}, a_t, r_t, \vec{o}'_{t+1}\}$}
        \STATE{Sample batch: $B_t \sim \mathcal{D}$}
        \STATE{Update value network: $\mathbb{E}_{\mathcal{B}_t} \big[\mathcal{L}(V) \big]$} \algorithmiccomment{SAC algorithm}
        \STATE{Update policy network: $\mathbb{E}_{\mathcal{B}_t} \big[ \mathcal{L}(\pi)\big]$} \algorithmiccomment{SAC algorithm}
        \STATE{Update fusion network $\phi$: $\mathbb{E}_{\mathcal{B}_t} \big[ \mathcal{L}(\phi) \big]$} according to Eq.\ref{MFSC_objective} 
        \STATE{Update the dynamics: $\mathbb{E}_{B_i}[\mathcal{L}_\mathcal{P}(\theta_j)], \text{ where } j \in \{1, \dots, K\}$} according to Eq.\ref{eq:dyn} 
    \ENDFOR
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[]
\caption{Pseudocode for the representation learning component of MFSC, PyTorch-like}
\label{algorithm3}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.5pt}{7.5pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.5pt}{7.5pt}\color{codeblue},
  keywordstyle=\fontsize{7.5pt}{7.5pt}\color{codekw},
}
\begin{lstlisting}[language=python]
# encoder: cnn + mlp + self-attention, fusion network, the output is L2-normalized
# transition_model: mlps, ensemble of transition models, the output is L2-normalized
# global_predictor: mlp, prediction head network
# lmb: weighting coefficient

def cosine_dis(feature_a, feature_b):
    feature_a_norm = nn.functional.normalize(feature_a, dim=1)
    feature_b_norm = nn.functional.normalize(feature_b, dim=1)
    temp = torch.matmul(feature_a_norm, feature_b_norm.T)
    dis = torch.diag(temp)
    return dis

def spr_loss(masked_z_a, target_masked_z_a):
    global_latents = self.global_predictor(masked_z_a)
    with torch.no_grad():
        global_targets = target_masked_z_a
    loss = norm_mse_loss(global_latents, global_targets, mean=False).mean()
    return loss

def compute_mfsc_loss(obs, action, reward, next_obs):

    obs = obs.view(batch_size, self.num_cameras, self.frame_stack, self.image_channels, self.image_size, self.image_size)
    next_obs = next_obs.view(batch_size, self.num_cameras, self.frame_stack * self.image_channels, self.image_size, self.image_size)
    
    # reconstruction loss
    masked_z_a = self.encoder.mask_encode(obs, mask=True)
    with torch.no_grad():
        target_masked_z_a = self.encoder.mask_encode(obs, mask=False)

    masked_z_a = masked_z_a.flatten(0, 1)
    target_masked_z_a = target_masked_z_a.flatten(0, 1)

    res_loss = spr_loss(masked_z_a, target_masked_z_a)

    # fusion loss
    z_a = self.encoder(obs)
    with torch.no_grad():
        pred_a = self.transition_model.sample_prediction(torch.cat([z_a, action], dim=1))

    next_diff = compute_dis(pred_a, pred_a)
    r_diff = torch.abs(reward.T - reward)
    z_diff = compute_dis(z_a, z_a)

    bisimilarity = self.c * r_diff + (1 - self.c) * next_diff
    fus_loss = torch.nn.HuberLoss()(z_diff, bisimilarity.detach())

    # mfsc loss
    mfsc_loss = fus_loss + self.lmb * res_loss

    # transition loss
    h = self.encoder(obs)
    next_h = self.encoder(next_obs).unsqueeze(0)
    pred_next_h = self.transition_model(torch.cat([h, action], dim=1))

    cos_sim = F.cosine_similarity(next_h.detach(), pred_next_h, dim=-1)
    transition_loss = 1 - cos_sim.mean()
    
    return mfsc_loss, transition_loss
    
\end{lstlisting}
\end{algorithm}


\end{document}