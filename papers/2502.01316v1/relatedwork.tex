\section{Related Work}
\textbf{Multi-View Representation Learning.} Multi-view representation learning uses multiple sources (or views) of a shared context. These views may include multiple camera views of the same scene (as in our work), as well as multimodal inputs (e.g., audio and video) or synthetic views of the unimodal measurements (e.g., time-sequenced data). \citet{li2018survey} categorized existing methods into two approaches: representation alignment and representation fusion. Representation alignment aims to capture the relationships among multiple different views through feature alignment, such as minimizing the distance between representations of different views \cite{feng2014cross, li2003multimedia}, maximizing similarity between views \cite{bachman2019learning, frome2013devise}, and maximizing the correlation of variables across views \cite{andrew2013deep}. Representation fusion integrates representations from different views to form a compact representation for downstream tasks \cite{geng2022multimodal, xie2020joint, karpathy2015deep}. Both strategies aim to harness the complementary information provided by multiple views for more comprehensive data representation. In this work, we propose a multi-view representation fusion method designed for control. Our approach leverages the properties of sequential decision-making tasks and multi-view data to achieve task-relevant and robust representation fusion.

\textbf{Multi-View Reinforcement Learning.} Effective state representation learning in MVRL aims to transform high-dimensional observations into a compact latent space. \citet{mvrl} propose a VAE-based algorithm that minimizes Euclidean distance between states encoded from different views, assuming a consistent primary view. Keypoint3D \cite{keypoint} uses 3D reconstruction to learn keypoints from third-person views, requiring camera calibration. Lookcloser \cite{lookcloser} introduces a cross-view attention mechanism for aggregating egocentric and third-person representations without calibration, though at higher computational cost. F2C \cite{f2c} uses conditional variational information bottlenecks (CVIBs) to model state space, showing robustness to missing views. MV-MWM \cite{mvmwm}, based on Dreamer-v2 \cite{Dreamer-v2}, applies dual masking and pixel-level reconstruction tasks during pretraining. Since MV-MWM relies on expert demonstrations, we did not include a direct comparison.  MVD \cite{mvd} employs contrastive learning to disentangle multi-view representations for control tasks. For a detailed comparison of MFSC with other algorithms, refer to Table \ref{comparison} and Appendix \ref{appendix:related work}.