\section{Related Work}
\textbf{Multi-View Representation Learning.} Multi-view representation learning uses multiple sources (or views) of a shared context. These views may include multiple camera views of the same scene (as in our work), as well as multimodal inputs (e.g., audio and video) or synthetic views of the unimodal measurements (e.g., time-sequenced data). ____ categorized existing methods into two approaches: representation alignment and representation fusion. Representation alignment aims to capture the relationships among multiple different views through feature alignment, such as minimizing the distance between representations of different views ____, maximizing similarity between views ____, and maximizing the correlation of variables across views ____. Representation fusion integrates representations from different views to form a compact representation for downstream tasks ____. Both strategies aim to harness the complementary information provided by multiple views for more comprehensive data representation. In this work, we propose a multi-view representation fusion method designed for control. Our approach leverages the properties of sequential decision-making tasks and multi-view data to achieve task-relevant and robust representation fusion.

\textbf{Multi-View Reinforcement Learning.} Effective state representation learning in MVRL aims to transform high-dimensional observations into a compact latent space. ____ propose a VAE-based algorithm that minimizes Euclidean distance between states encoded from different views, assuming a consistent primary view. Keypoint3D ____ uses 3D reconstruction to learn keypoints from third-person views, requiring camera calibration. Lookcloser ____ introduces a cross-view attention mechanism for aggregating egocentric and third-person representations without calibration, though at higher computational cost. F2C ____ uses conditional variational information bottlenecks (CVIBs) to model state space, showing robustness to missing views. MV-MWM ____, based on Dreamer-v2 ____, applies dual masking and pixel-level reconstruction tasks during pretraining. Since MV-MWM relies on expert demonstrations, we did not include a direct comparison.  MVD ____ employs contrastive learning to disentangle multi-view representations for control tasks. For a detailed comparison of MFSC with other algorithms, refer to Table \ref{comparison} and Appendix \ref{appendix:related work}.