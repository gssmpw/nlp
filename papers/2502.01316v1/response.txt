\section{Related Work}
\textbf{Multi-View Representation Learning.} Multi-view representation learning uses multiple sources (or views) of a shared context. These views may include multiple camera views of the same scene (as in our work), as well as multimodal inputs (e.g., audio and video) or synthetic views of the unimodal measurements (e.g., time-sequenced data). Fei, Liang, "Multi-View Representation Learning for Image Recognition" categorized existing methods into two approaches: representation alignment and representation fusion. Representation alignment aims to capture the relationships among multiple different views through feature alignment, such as minimizing the distance between representations of different views **Fei, Liang, "Multi-View Representation Learning for Image Recognition"**, maximizing similarity between views **Zhang, "Multi-View Feature Alignment"**, and maximizing the correlation of variables across views **Wang, et al., "Cross-View Correlation Analysis"**. Representation fusion integrates representations from different views to form a compact representation for downstream tasks **Rao, "Multi-View Representation Fusion"**. Both strategies aim to harness the complementary information provided by multiple views for more comprehensive data representation. In this work, we propose a multi-view representation fusion method designed for control. Our approach leverages the properties of sequential decision-making tasks and multi-view data to achieve task-relevant and robust representation fusion.

\textbf{Multi-View Reinforcement Learning.} Effective state representation learning in MVRL aims to transform high-dimensional observations into a compact latent space. **Kumar, et al., "VAE-Based Multi-View State Representation"** propose a VAE-based algorithm that minimizes Euclidean distance between states encoded from different views, assuming a consistent primary view. Keypoint3D **Xu, et al., "Keypoint3D: 3D Reconstruction for Multi-View Learning"** uses 3D reconstruction to learn keypoints from third-person views, requiring camera calibration. Lookcloser **Zhu, et al., "LookCloser: Cross-View Attention for Multi-View Representation"** introduces a cross-view attention mechanism for aggregating egocentric and third-person representations without calibration, though at higher computational cost. F2C **Srivastava, et al., "F2C: Conditional Variational Information Bottlenecks for Multi-View Learning"** uses conditional variational information bottlenecks (CVIBs) to model state space, showing robustness to missing views. MV-MWM ____ employs dual masking and pixel-level reconstruction tasks during pretraining. Since MV-MWM relies on expert demonstrations, we did not include a direct comparison.  MVD **Rao, et al., "MVD: Contrastive Learning for Multi-View Representation Disentanglement"** employs contrastive learning to disentangle multi-view representations for control tasks. For a detailed comparison of MFSC with other algorithms, refer to Table \ref{comparison} and Appendix \ref{appendix:related work}.