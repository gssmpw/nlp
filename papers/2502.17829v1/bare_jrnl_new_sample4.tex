\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue, pdfborder={0 0 0}]{hyperref}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Silent Speech Sentence Recognition with Six-Axis Accelerometers using Conformer and CTC Algorithm}

\author{Yudong Xie, Zhifeng Han, Qinfan Xiao, Liwei Liang, 

Lu-Qi Tao*, \textit{Member}, \textit{IEEE}, and Tian-Ling Ren*, \textit{Fellow}, \textit{IEEE}
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.} 
\thanks{This work is supported by the National Key R\&D Program (2022YFB3204100, 2021YFC3002200), the National Natural Science Foundation (U20A20168) of China, the Research Fund from Tsinghua University Initiative Scientific Research Program, and a grant from the Guoqiang Institute, Tsinghua University. }
\thanks{Yudong Xie, Zhifeng Han and Qinfan Xiao are with the Department of Electronic Engineering, Tsinghua University, Beijing 100084, China}
\thanks{Liwei Liang is with the School of Integrated Circuits, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing 100084, China}
\thanks{Lu-Qi Tao and Tian-Ling Ren are with the School of Integrated Circuits, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing 100084, China(e-mail: taoluqi@tsinghua.edu.cn; RenTL@tsinghua.edu.cn)}
}

% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Silent speech interfaces (SSI) are being actively developed to assist individuals with communication impairments who have long suffered from daily hardships and a reduced quality of life.  However, silent sentences are difficult to segment and recognize due to elision and linking. A novel silent speech sentence recognition method is proposed to convert the facial motion signals collected by six-axis accelerometers into transcribed words and sentences. A Conformer-based neural network with the Connectionist-Temporal-Classification algorithm is used to gain contextual understanding and translate the non-acoustic signals into words sequences, solely requesting the constituent words in the database. Test results show that the proposed method achieves a 97.17\% accuracy in sentence recognition, surpassing the existing silent speech recognition methods with a typical accuracy of 85\%-95\%, and demonstrating the potential of accelerometers as an available SSI modality for high-accuracy silent speech sentence recognition.
\end{abstract}

\begin{IEEEkeywords}
Silent-speech sentence recognition, Six-axis accelerometer, Connectionist Temporal Classification (CTC), Conformer-based neural network.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{S}{peech} has been one of the most essential ways for humans to communicate with each other for thousands of years. Acoustic signals convey much of the important information in our daily lives. However, speech-based communication cannot be performed well under certain physiological constraints, especially for people who are bothered with defects in the acoustic system, such as laryngects who suffer from surgical treatments.  For the voiceless population, the inability to communicate effectively can lead to profound social isolation and psychological distress. In emergency situations, the lack of a reliable communication method can even be life-threatening, as they are unable to call for help or express urgent needs. Therefore, the development of a non-invasive, portable, and highly accurate silent speech recognition system \cite{ref1,ref2} is not just a technological advancement but a necessity for improving the quality of life and safety of these individuals.

The technique for the devices mentioned above is the “silent speech interface” (SSI)\cite{ref3,ref4}, which has been widely used to communicate with each other for people with vocal defects\cite{ref5,ref6}. The performance of SSI mainly depends on the techniques of silent speech signal acquisition and data decoding. The modalities of SSI used in signal acquisition include the modalities based on electromagnetic, mechanical, ultrasonic, visual, and mixed types\cite{ref7,ref8,ref9,ref10,ref11,ref12,ref13,ref14,ref15,ref16}. Sensors of EEG and surface electromyography (sEMG) are critical components of the electromagnetic-based SSI\cite{ref17,ref18,ref19,ref20,ref21,ref22,ref23,ref24,ref25,ref26}. They are often invasive and unsuitable for improving the quality of life for the patients. Mechanical SSI uses strain and infrared sensors, while ultrasonic and visual-based SSI focuses on lip-reading from mouth shapes\cite{ref27,ref28,ref29,ref30,ref31,ref32,ref33,ref34}. Non-invasive modalities are generally complicated, expensive, impractical, and lack portability. Mixed modalities combine these techniques but often suffer from instability and unreliable signal collection due to complex structures\cite{ref11,ref29}. Therefore, accelerometers have been chosen for their non-invasiveness, anti-noise, portability, and high sensitivity\cite{ref35,ref36,ref37,ref38,ref39}.

A 95\% accuracy has been achieved in word classification using three-axis accelerometers and a deep neural network\cite{ref8,ref40}. However, the practicality of these three-axis accelerometers is constrained by their large size and the relatively low dimensionality of the data they produce. The six-axis accelerometers provide additional gyroscope data alongside x-, y-, and z-axis data, significantly improving the portability and feature extraction.

Most speech recognition approaches only consider word classification\cite{ref41,ref42}, and not many consider continuous sentence recognition. Sentence recognition remains challenging due to elision (loss of syllables) and linking (word boundaries blending). These phenomena are especially common in English compared to Chinese or Korean \cite{ref43}. The phonetic systems of Chinese and Korean feature a one-to-one correspondence between characters and phonemes, which simplifies word segmentation compared to English\cite{ref43,ref44}. None of the prior studies has effectively addressed the challenges of silent speech sentence recognition in dynamic and accurate segmentation and recognition (especially in English). While some researchers reported high accuracy in sentence recognition, their approaches treated entire sentences as single entities, like the word-level classification. The nuanced segmentation and dynamic contextual understanding required for accurate sentence-level recognition were overlooked\cite{ref22,ref45}.

On the other hand, various approaches based on machine learning\cite{ref46,ref47,ref48} have been introduced to solve the segment matching problems\cite{ref49}. The Conformer\cite{ref50}, a Transformer\cite{ref51} combined with convolutional layers and Connectionist Temporal Classification (CTC) algorithms\cite{ref52} has gained attention for its exceptional performance in end-to-end automatic speech recognition (ASR) systems\cite{ref53}. Transformers leverage self-attention mechanisms for context understanding and the end-to-end learning, while CTC has advantages in processing sequential data without requiring strict alignment. Although CTC treats tokens as independent and lacks contextual awareness, integration with Transformers may compensate for this limitation.

In this study, a new method for SSI is presented to capture the facial motion features. The proposed method employs the six-axis accelerometers together with the Conformer and CTC algorithms to decode the captured signals and to recognize sentences solely requesting the constituent words in the database. It can segment and learn dynamically from context from context to obtain high accuracy in segmenting and recognizing complex combinations of words and sentences. To the best of our knowledge, no prior study has reported the SSI method to segment and recognize sentences dynamically using a word database.


\section{Method}

\subsection{Participants}

One laryngect and three students from Tsinghua University participated in the test to demonstrate the performance of the proposed technique. All the participants were informed of the details of our experiment before the experiment began.

\begin{figure*}[ht]\centering
	\includegraphics[width=18.1cm]{Location.pdf}
	\caption{Location of the accelerometers used to collect facial motion signals: (a) Location of sensors; (b) A patient wearing the sensor system; (c) Flowchart of the speech recognition system.}\label{FIG_1}
\end{figure*}

\subsection{Data Recording}

\subsubsection{Microelectromechanical System}

The test was conducted using a microelectromechanical system, which is composed of six accelerometers and a chip ESP32 (Espressif). The communication protocol between the host and the devices is SPI, which can handle up to six devices using two buses at the same time. The accelerometer is the six-axis MPU6500 (Invensense). It records three-axis acceleration data and three-axis gyro data at the sampling frequency of 50 Hz.

Fig. 1 shows the means of employing the accelerometer to record the facial motion data. Fig. 1(a) depicts the location of sensors. Fig. 1(b) shows the designed flexible printed circuit board with six accelerometers at the top of the twenty-centimeter-long “tentacles”. This design is helpful for the portability of our device and brings comfort to patients such as laryngects. Fig. 1(c) shows the flowchart of the system operation. The sensors transfer motion data to the mainboard and then to the upper monitor, at which a neural network decodes the signals and converts them into outputs.

\vspace{10pt}

\subsubsection{Data Paradigm}

The accelerometers are stuck to the volunteer’s face and throat. Fig. 1(a) depicts the location of accelerometers\cite{ref54,ref55}. For example, channel 1 is positioned at the jaw, channel 2 is placed under the jaw, channel 3 at the throat, channel 4 and channel 5 at the upper and lower lip, respectively, while channel 6 is at the cheek.

Words and sentences in both English and Chinese were collected. Each word was recorded 100 times repeatedly, and each sample contained 80 sampling points. The size of one sample is 80 (window size) $\times$ 6 (channels) $\times$ 6 (axes). As for a sentence or phrase, the size is 180 (window size) $\times$ 6 (channels) $\times$ 6 (axes). The sentences are repeated 30 times in the experiment.

In total, sixteen English words, two English sentences, and eight Chinese phrases were recorded. These words and sentences are widely used in our daily lives, especially for laryngects. To test the generalization ability of the proposed technique, \textit{"what"}, \textit{"want"}, \textit{"wait"}, \textit{"wonder"}, and \textit{"water"} are selected deliberately as easily confused words. Since test sentences contain words in the dataset, sentence recognition can be done using the existing data. All the tested words and sentences as well as their meanings are listed in Table 1.

Data augmentation, such as word concatenation and random Gaussian noise, is required in that there is a strong need for sentence data to train the CTC model. Words are randomly concatenated into “sentences”, although they might be linguistically meaningless, and used for training to improve the segmentation and recognition ability of the model. The Gaussian noise with a standard deviation of one-third of the original data is added to expand the dataset size tenfold.



\begin{table*}[htbp]
	\caption{List of words, phrases, and sentences}
	\centering
	\label{table_1}
	%\centering
	\resizebox{2\columnwidth}{!}{
		\begin{tabular}{l l l l}
			\hline\hline \\[-3mm]
			\multicolumn{1}{c}{Words} & \multicolumn{1}{c}{Words} & \multicolumn{1}{c}{Sentences} & \multicolumn{1}{c}{Chinese Phrases}  \\%[1.6ex] 
			\hline
			Afternoon & What & Drink water & Tengtong (Hurt) \\
			Beautiful & Wait &  Hello, please wait & Fanshen (Turn over) \\
			Breakfast & Want &  & Xiachuang (Getting out of bed) \\
            Drink & Water &  & Henkaixin (Very happy) \\
            Hello & Welcome &  & Xiexieni (Thank you) \\
            Please & Wonder &  & Woyaoheshui (I want to drink water) \\
            Sorry & Wonderful &  & Woxiangchifan (I want to have a meal) \\
            Thanks &  &  & Tianqizhenhao (What nice weather) \\
			\hline\hline
		\end{tabular}
	}
\end{table*}

\subsection{Data Preprocessing}

The data preprocessing techniques, including moving average, high-pass filtering, and normalization, were used to enhance the quality of the dataset for further analysis or modeling.

The moving average was used to smooth out high-frequency data (such as instantaneous spikes). In this work, the adjacent three data are averaged as the new result. For an input signal $X$, the output after the moving average is:

\begin{equation}
Y_t = \frac{1}{3}\sum_{i=t}^{t+2}X_t
\end{equation}

A slight movement by a human might be recorded in the data due to the high sensitivity of the accelerometers. Compared to facial motion features, the movements of the head and body are often in the low frequency range. To eliminate the biases and low-frequency noise, a high-pass filter is introduced. The filter is a fourth-order Butterworth filter with the cutoff frequency $f_c=2 Hz$. For input $X$, the high-pass output $Y$ is:

\begin{equation}
Y=h_{high-pass}*X
\end{equation}

The z-score normalization is used to standardize the raw data to enhance the generalization ability of the model and avoid overfitting. The data of one sample is standardized into 36 (dimensions) $\times$ window size (80 for words and 180 for sentences) in the end.

\subsection{Machine Learning Model}

Sentence recognition in silent speech interfaces (SSI) is challenging due to the complexities such as elision and linking, especially in English. The complexities often make the traditional word-by-word approach ineffective. This study addresses this challenge with a transformer-based model, conformer, integrated with local multi-head self-attention. The conformer combines the ability of CNNs to capture local features with the contextual understanding of the transformer. This design enhances adaptability to varying sentence lengths and speaking rates, especially suitable for silent speech recognition.

The Connectionist Temporal Classification (CTC) algorithm addresses the challenge of aligning accelerometer data with spoken words despite variations in speech rate. By introducing a “blank” label, CTC enables the model to ignore irrelevant input frames and map multiple frames to a single output. Its loss function, based on the negative log-likelihood of the most probable output sequence, ensures robust recognition across diverse speakers. It is expressed as:


\begin{equation}
L_{CTC} = -\log{\sum_{\pi \in \Pi}P(\pi|X)}
\end{equation}

In (3), where $\Pi$ is the set of all possible alignments of the target sequence $Y$ with the input sequence $X$, and $P(\pi|X)$ is the probability of alignment $\pi$ given the input sequence $X$. The probability of an alignment $\pi$ is calculated as:

\begin{equation}
P(\pi|X) = \prod_{t=1}^T P(\pi_t|X_t)
\end{equation}

Where $\pi_t$ is the label at time step $t$ in the alignment $\pi$, and $P(\pi_t|X_t)$ is the probability of label $\pi_t$ at time step $t$ given the input $X_t$.

\vspace{10pt}

\subsubsection{Network Structure}

An end-to-end neural network based on CTC loss and Conformer with a multi-head local self-attention mechanism is used to convert accelerometer data into word sequences. Fig. 2(a) shows the complete model structure. The model mainly consists of three parts: A convolution head that processes the accelerometer data by extracting local patterns and down-sampling the data; local self-attention blocks that aggregate the context information; and the CTC algorithm that decodes the features extracted from the previous network.

Conformer is a stacked architecture combining convolutional layers and Transformer blocks. Input features pass through three convolutional blocks, each consisting of a convolution layer, batch normalization, ReLU activation, and dropout. The first block uses 30 input channels and outputs 128 hidden dimensions. A residual connection is applied in the second block, while the third block down-samples data to enhance computational efficiency.

\begin{figure*}[ht]\centering
	\includegraphics[width=18.1cm]{Network.pdf}
	\caption{Architecture and operating principle of the network. (a) Architecture of the network with three convolutional blocks, three local self-attention blocks, and linear layers. (b) Diagrammatic sketch of the local self-attention mechanism. The left image stands for full context attention and each token can have the global information. The middle image is for the limited context attention and each token only has access to the local information. The right image means local attention combined with global token. The global token shares its information to the other tokens. (c) Illustration of the CTC algorithm. CTC algorithm introduces “blank” token to split the sequence. The signal is cut into tokens, decoded into words and then simplified into a continuous sentence based on the “blank” token.}\label{FIG_2}
\end{figure*}

Local self-attention was applied which enforces the attention matrix to be a band matrix. This makes each token only obtain information from a fixed-size surrounding window (which is about 0.4s in this work). To maintain the model’s ability to gain access to the global context, a global token at the beginning of each sequence is appended to aggregate overall information.

The CTC algorithm is introduced for its excellent performance in sequence-to-sequence tasks. CTC works by segmenting the words into thinner slices (smaller window size) and recognizing them as well as considering the entire sequence of inputs and outputs, rather than requiring a one-to-one alignment between input frames and output labels.

\vspace{10pt}

\subsubsection{Training Parameters}

The model was trained to minimize the sum of CTC loss and Cross-Entropy loss, using the Adams optimizer. The learning rate was set to 0.001, with a decay rate of 0.00001. Eighty percent of the dataset was used for blind training, and the remaining twenty percent was used for testing. The batch size was 32 and the number of epochs was set to 50. PyTorch is used for all data training processes of the neural network mentioned above.


\section{Results}

\subsection{Accuracy of Recognition}

The proposed sentence recognition method trains the model to learn from sentence data and then uses the existing words to identify each part of an unknown sentence.

Before the training process, waveforms for the accelerometer signals are checked to see if it has reproducibility and stability, as shown in Fig. 3(a). Different test trials with the same word and sentence exhibit high consistency. As illustrated in Fig. 3(b), the data has strong clustering effects after Principal Component Analysis (PCA) and indicates obvious distinctions between different words. The results indicate that the same word has similar and stable features while different words hold distinctive differences.

The model shows an excellent performance in sentence segmentation and recognition based on the good quality of the accelerometer data. The Conformer performs well in extracting the features and learning the contextual information. The CTC algorithm segments the sentence and decodes the result. Accuracy is calculated as the ratio of correctly recognized words to the total words in a sentence or dataset. Fig. 3(c) shows the accuracy and the loss curve during the training process. It can be seen from Fig. 3(c) that the total accuracy in the whole dataset is 97.17\%, which is higher than previous studies \cite{ref8}, which showed an accuracy of 94.65$\pm$2.54\%. As shown in Fig. 3(c), the average classification accuracy is 97.24\% for words and 97.16\% for the sentences.


\begin{figure*}[ht]\centering
	\includegraphics[width=18.1cm]{Train_Results.pdf}
	\caption{Recognition related performance. (a) The overlapping waveform of four respective samples of the same word(left) or sentence(right). (b) Results of data after Principal Component Analysis (PCA) in a six-word database. (c) The accuracy and loss curves to training epochs; error bars indicate standard deviations. (d) The confusion matrix of the classification task on five words.}\label{FIG_3}
\end{figure*}

\subsection{Performance of Easily Confused Words}

The classification accuracy on five easily confused words (\textit{what}, \textit{want}, \textit{wait}, \textit{wonder}, and \textit{water}) is analyzed to evaluate the discriminative ability of the proposed method in distinguishing phonetically similar words. These words were chosen due to their high potential for misclassification, given their resemblance in articulation and motion patterns. 

Fig. 3(d) is the confusion matrix of the five easily confused words. It offers a visual representation of how the model differentiates the easily confused words. It can be seen from Fig. 3(d) that the model performs well in classifying these ambiguous words, achieving a high level of precision in distinguishing their subtle differences in facial motion and pronunciation. The mean accuracy exceeds 99\%, demonstrating the robustness of the system. Results also showed that only roughly 0.2\% of the testing samples were incorrectly recognized. This implies that misclassification occurs at a very low rate. This impressive performance highlights the effectiveness of the accelerometer-based system combined with the sequence-to-sequence algorithm. The results suggest that the proposed approach not only captures gross facial movements and mouth shape variations but also extracts fine-grained motion features from the accelerometer data. Such capability is crucial for improving the accuracy of silent speech recognition and ensuring that highly similar words can be correctly classified with minimal errors.

\subsection{Performance of Sentence Recognition}

In addition to word classification, the proposed model has demonstrated excellent performance in sentence segmentation and recognition, further validating its capability in processing continuous speech data. The average sentence recognition accuracy reaches 97.16\%, indicating a high level of reliability in translating facial motion patterns into corresponding textual representations. This superior performance is largely attributed to the Conformer model’s ability to effectively capture both local dependencies and global contextual information. By integrating self-attention mechanisms with convolutional layers, the model ensures that even subtle variations in facial motion are accurately mapped to words and sentences, thereby improving the overall recognition precision.

\begin{figure*}[ht]\centering
	\includegraphics[width=18.1cm]{Sentence_segment.pdf}
	\caption{Performance considering the specificity of the sentences. (a) An example of segmentation of a long sentence and its matching with the corresponding words. The waveform exhibits similarity between each part of the sentence and the words. (b) Histogram of the accuracy related to different participants. The \textit{Blind} bar means the model is trained without the person’s data and tested immediately. The \textit{Few Shots} means that we add small amount of samples (10 pieces) into the training dataset and test the accuracy. The \textit{Normal} means 70\% of the person’s data is involved in the training process and 30\% for testing. The error bars indicate standard deviations. (c) Histogram of the accuracy related to the length of the sentences; error bars indicate standard deviations. The accuracy remains steady at around 97\% even when the length increases.}\label{FIG_4}
\end{figure*}

Fig. 4(a) illustrates the similarity between a portion of sentence data and the corresponding individual words in the original time domain. This visualization confirms that the model successfully maintains structural integrity and consistency when processing longer speech sequences. Additionally, Fig. 4(c) presents the relationship between the average recognition accuracy and sentence length. The results show a relatively stable trend, with accuracy consistently remaining above 95\% across different sentence lengths. This demonstrates that the use of CTC decoding effectively mitigates the impact of sentence length on recognition performance. Unlike traditional methods that may struggle with longer sequences, the CTC-based approach enables the model to maintain high accuracy regardless of sentence complexity.


\subsection{Performance with respect to Different Individuals}

The model’s performance across different participants is illustrated in Fig. 4(b). The red bars in the figure represent the accuracy achieved by each participant under standard conditions, while the green bars depict “blind” testing results. In the “blind” scenario, the model was trained using data from other participants and tested on a new individual without prior exposure to their specific motion patterns. This evaluation was conducted to assess the generalization capability of the model when applied to unseen subjects.

The results indicate that, on average, participants achieved over 95\% accuracy when the model was trained using their own data, demonstrating its effectiveness in adapting to individual speech patterns. However, the “blind” accuracy varied among individuals, reflecting differences in speaking habits, facial muscle movements, and pronunciation styles. While some participants achieved relatively high recognition rates even in the blind testing scenario, others exhibited a noticeable drop in accuracy due to unique articulation characteristics.

To address this variability and enhance the model’s generalization ability, a “few-shot” learning approach was applied. By incorporating a small number of participant-specific samples into the training dataset, the recognition accuracy was significantly improved. The blue bars in Fig. 4(b) indicate that with only a limited amount of personalized data, the model’s accuracy increased to over 90\%, effectively reducing the performance gap observed in blind testing. This result demonstrates the model’s capacity to adapt to individual differences and highlights the importance of incorporating minimal personalized training data to achieve more reliable recognition results across diverse users.

\subsection{Performance with respect to Different Channels}

The results reported above are based on six channels. To simplify the system, each channel was evaluated independently, achieving mean accuracies ranging from 90.22\% to 93.19\%, as shown in Fig. 5(a). Channel 2 demonstrated the highest single-channel accuracy, while all channels showed similar performance, reflecting a balance in features captured by facial motions.

\begin{figure*}[ht]\centering
	\includegraphics[width=18.1cm]{Acc_MPU_Axis_Com.pdf}
	\caption{Performance considering different channels and axes. (a) The accuracy of a single channel. (b) The accuracy with respect to the number of channels. (c) Accuracy matrix of the combinations of the double channels. (d) The accuracy of a single axis. (e) The accuracy with respect to the number of axes. (f) Accuracy matrix of the combinations of the double axes. Error bars indicate standard deviations.}\label{FIG_5}
\end{figure*}

Pairwise combinations of channels were also tested, as shown in Fig. 5(c), with the highest accuracy of 97.03\% achieved using channels 1 and 4, likely due to complementary features from the jaw and upper lip. Fig. 5(b) shows that using two or three channels can maintain accuracy above 95\%, with only a 1.5\% reduction compared to six channels. This suggests the system can be simplified without significant performance loss.


\subsection{Performance with respect to Different Axes}

The model was trained using six axes. Axes 1-3 are for acceleration data and axes 4-6 for gyroscope data. Single axis performance evaluation, shown in Fig. 5(d), reveals that gyroscope data generally outperformed accelerometer data, with accuracies of 92.39\% to 93.39\% versus 87.62\% to 92.42\%. Fig. 5(e) shows the accuracy with the use of more axes. It can be seen from Fig. 5(e) that a mean accuracy of 95.37\% is achieved using two axes, while the accuracy is 95.57\% when all six axes are used. This indicates that data from two axes may be sufficient for accuracy performance. Fig. 5(f) illustrates the accuracy performance using a pair of two axes. It is observed from Fig. 5(f) that the range of accuracy is from 94.04\% to 97.07\%.

\section{Discussion}

The presented work demonstrates a significant advancement in silent speech sentence recognition using six-axis accelerometers and a Conformer-based neural network with CTC algorithm.  The proposed silent speech recognition system has the potential to significantly improve the quality of life for individuals with speech impairments, helping them regain the ability to communicate with others in normal lives. The portability of the six-axis accelerometer-based system makes it particularly suitable for real-world applications, ensuring that patients can rely on it in their daily lives. While the achieved accuracy of 97.17\% is impressive, several aspects warrant further discussion and exploration.

The proposed method addresses the spatiotemporal correlation of facial motion acceleration data by utilizing convolutional layers to extract features from adjacent time and space domains. The Transformer is used for contextual learning, and the CTC algorithm for the interpretation and representation of word segmentation.

The strength of the proposed method lies in its ability to dynamically recognize arbitrary silent sentences by autonomous segmentation and identification based on a word database, without prior knowledge of the sentences. This overcomes the limitations of previous studies that treated word classification or treated sentences as a whole entirety similar to the word \cite{ref22,ref45}. The previous methods are only capable of static recognition and lack flexibility and expressiveness. The users of those approaches are limited to fixed databases and unable to convey extra meanings. This approach allows for a more versatile and practical SSI system, capable of recognizing a vast number of sentence combinations within the database. The patient can express whatever he/she wants by using different combinations of words to build a sentence, just like normal speaking.

The method demonstrates robust performance, excelling in recognizing easily confused words and adapting across individuals. Its few-shot learning capability allows quick adaptation to new users with minimal data. The compact, non-invasive six-axis accelerometers enhance portability and usability, making the system ideal for daily use by individuals with speech impairments.

The few-shot learning shows promising results, although the performance is still affected by the habits of individual speakers. Further work is required to improve its generalization capability. Combining accelerometers with microphones could provide additional acoustic information to address this issue. Additionally, the relatively small dataset used in this study limits the model’s ability to handle complex linguistic phenomena. Expanding the database with diverse sentences and linguistic features could further enhance robustness and accuracy performance.

The proposed method offers a promising solution for individuals with speech and communication impairments, paving the way for more practical and accessible SSI systems. Future research would focus on addressing the challenges mentioned above and exploring advancements to achieve better accuracy and generalization.


\section{Conclusion}

In this paper, a new method based on six-axis accelerometers has been presented for silent sentence recognition. The system is designed to let individuals with speech impairments regain the ability to communicate normally and enhance their expressive capabilities, focusing not only on word-level expression but also on complete sentence output. Six accelerometers are used to extract the motion features during the silent speaking period. A Conformer-based neural network integrated with the CTC algorithm was applied to translate mechanical motions into word and sentence transcriptions accurately. The average accuracy in sentence recognition and word classification is 97.17\% based on the proposed method. The proposed method performs well on easily confused word recognition and shows robustness and generalization ability on the few-shot task.

This work is the first to recognize silent sentences dynamically and promotes the practical implementation and utilization of the system. The system is portable and applicable for patients in their daily usage due to the advantages of the accelerometers. In the future, the integration of accelerometers and other modalities, such as microphones, will be explored.



\section*{Acknowledgments}
The authors would like to thank Cancer Hospital Chinese Academy of Medical Sciences and volunteering participants for help in obtaining the dataset. We also thank Heng-Ming Tai and Kaigui Xie for their help on refining our manuscript.



\begin{thebibliography}{1}
\bibliographystyle{IEEEtran}

\bibitem{ref1}
W. Lee, J. J. Seong, B. Ozlu, B. S. Shim, A. Marakhimov, and S. Lee, {\it{Biosignal sensors and deep learning-based speech recognition: A review}}, Sensors, vol. 21, no. 4, p. 1399, 2021.

\bibitem{ref2}
T. Schultz, M. Wand, T. Hueber, D. J. Krusienski, C. Herff, and J. S. Brumberg, {\it{Biosignal-based spoken communication: A survey}}, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 12, pp. 2257-2271, 2017.

\bibitem{ref3}
B. Denby, T. Schultz, K. Honda, T. Hueber, J. M. Gilbert, and J. S. Brumberg, {\it{Silent speech interfaces}}, Speech Communication, vol. 52, no. 4, pp. 270-287, 2010.

\bibitem{ref4}
J. A. Gonzalez-Lopez, A. Gomez-Alanis, J. M. M. Doñas, J. L. Pérez-Córdoba, and A. M. Gomez, {\it{Silent speech interfaces for speech restoration: A review}}, IEEE access, vol. 8, pp. 177995-178021, 2020.

\bibitem{ref5}
M. J. Fagan, S. R. Ell, J. M. Gilbert, E. Sarrazin, and P. M. Chapman, {\it{Development of a (silent) speech recognition system for patients following laryngectomy}}, Medical engineering \& physics, vol. 30, no. 4, pp. 419-425, 2008.

\bibitem{ref6}
M. Llorente, A. Podhorski, and S. Fernandez, {\it{Wearable Voice Dosimetry System}}, Applied Sciences, vol. 14, no. 13, p. 5806, 2024.

\bibitem{ref7}
Y. Kunimi, M. Ogata, H. Hiraki, M. Itagaki, S. Kanazawa, and M. Mochimaru, {\it{E-MASK: a mask-shaped interface for silent speech interaction with flexible strain sensors}}, in Proceedings of the Augmented Humans International Conference 2022, 2022, pp. 26-34. 

\bibitem{ref8}
J. Kwon, H. Nam, Y. Chae, S. Lee, I. Y. Kim, and C.-H. Im, {\it{Novel three-axis accelerometer-based silent speech interface using deep neural network}}, Engineering Applications of Artificial Intelligence, vol. 120, p. 105909, 2023.

\bibitem{ref9}
Y. Igarashi, K. Futami, and K. Murao, {\it{Silent Speech Eyewear Interface: Silent Speech Recognition Method Using Eyewear with Infrared Distance Sensors}}, in Proceedings of the 2022 ACM International Symposium on Wearable Computers, 2022, pp. 33-38. 

\bibitem{ref10}
Z. Che, X. Wan, J. Xu, C. Duan, T. Zheng, and J. Chen, {\it{Speaking without vocal folds using a machine-learning-assisted wearable sensing-actuation system}}, Nature Communications, vol. 15, no. 1, p. 1873, 2024.

\bibitem{ref11}
Q. Yang et al., {\it{Mixed-modality speech recognition and interaction using a wearable artificial throat}}, Nature Machine Intelligence, vol. 5, no. 2, pp. 169-180, 2023.

\bibitem{ref12}
H. Begum, O. Chowdhury, M. S. R. Hridoy, and M. M. Islam, {\it{AI-based Sensory Glove System to Recognize Bengali Sign Language (BaSL)}}, IEEE Access, 2024.

\bibitem{ref13}
M. R. Cassim, J. Parry, A. Pantanowitz, and D. M. Rubin, {\it{Design and construction of a cost-effective, portable sign language to speech translator}}, Informatics in Medicine Unlocked, vol. 30, p. 100927, 2022.

\bibitem{ref14}
P. Dong, Y. Li, S. Chen, J. T. Grafstein, I. Khan, and S. Yao, {\it{Decoding silent speech commands from articulatory movements through soft magnetic skin and machine learning}}, Materials Horizons, vol. 10, no. 12, pp. 5607-5620, 2023.

\bibitem{ref15}
Y. Jin et al., {\it{EarCommand: " Hearing" Your Silent Speech Commands In Ear}}, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 6, no. 2, pp. 1-28, 2022.

\bibitem{ref16}
R. B. Rafiq, S. A. Karim, and M. V. Albert, {\it{An LSTM-based Gesture-to-Speech Recognition System}}, in 2023 IEEE 11th International Conference on Healthcare Informatics (ICHI), 2023: IEEE, pp. 430-438. 

\bibitem{ref17}
J. S. Brumberg, A. Nieto-Castanon, P. R. Kennedy, and F. H. Guenther, {\it{Brain–computer interfaces for speech communication}}, Speech communication, vol. 52, no. 4, pp. 367-379, 2010.

\bibitem{ref18}
G. S. Meltzner, J. T. Heaton, Y. Deng, G. De Luca, S. H. Roy, and J. C. Kline, {\it{Development of sEMG sensors and algorithms for silent speech recognition}}, Journal of neural engineering, vol. 15, no. 4, p. 046031, 2018.

\bibitem{ref19}
E. Donchin, K. M. Spencer, and R. Wijesinghe, {\it{The mental prosthesis: assessing the speed of a P300-based brain-computer interface}}, IEEE transactions on rehabilitation engineering, vol. 8, no. 2, pp. 174-179, 2000.

\bibitem{ref20}
T. Schultz and M. Wand, {\it{Modeling coarticulation in EMG-based continuous speech recognition}}, Speech Communication, vol. 52, no. 4, pp. 341-353, 2010.

\bibitem{ref21}
L. Bi, A. G. Feleke, and C. Guan, {\it{A review on EMG-based motor intention prediction of continuous human upper limb motion for human-robot collaboration}}, Biomedical Signal Processing and Control, vol. 51, no. MAY, pp. 113-127, 2019.

\bibitem{ref22}
J. Wang, A. Samal, J. R. Green, and F. Rudzicz, {\it{Sentence recognition from articulatory movements for silent speech interfaces}}, in 2012 IEEE international conference on acoustics, speech and signal processing (ICASSP), 2012: IEEE, pp. 4985-4988. 

\bibitem{ref23}
D. Vorontsova et al., {\it{Silent EEG-speech recognition using convolutional and recurrent neural network with 85\% accuracy of 9 words classification}}, Sensors, vol. 21, no. 20, p. 6744, 2021.

\bibitem{ref24}
R. Hofe et al., {\it{Small-vocabulary speech recognition using a silent speech interface based on magnetic sensing}}, Speech Communication, vol. 55, no. 1, pp. 22-32, 2013.

\bibitem{ref25}
M. Rekrut, A. M. Selim, and A. Krüger, {\it{Improving Silent Speech BCI Training Procedures Through Transfer from Overt to Silent Speech}}, in 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2022: IEEE, pp. 2650-2656. 

\bibitem{ref26}
M. Kim, B. Cao, T. Mau, and J. Wang, {\it{Speaker-independent silent speech recognition from flesh-point articulatory movements using an LSTM neural network}}, IEEE/ACM transactions on audio, speech, and language processing, vol. 25, no. 12, pp. 2323-2336, 2017.

\bibitem{ref27}
T. Hueber, G. Chollet, B. Denby, and M. Stone, {\it{Acquisition of ultrasound, video and acoustic speech data for a silent-speech interface application}}, Proc. of ISSP, pp. 365-369, 2008.

\bibitem{ref28}
R. El‐Bialy et al., {\it{Developing phoneme‐based lip‐reading sentences system for silent speech recognition}}, CAAI Transactions on Intelligence Technology, vol. 8, no. 1, pp. 129-138, 2023.

\bibitem{ref29}
T. Hueber, E.-L. Benaroya, G. Chollet, B. Denby, G. Dreyfus, and M. Stone, {\it{Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips}}, Speech Communication, vol. 52, no. 4, pp. 288-300, 2010.

\bibitem{ref30}
M. Hao, M. Mamut, N. Yadikar, A. Aysa, and K. Ubul, {\it{A Survey of Research on Lipreading Technology}}, IEEE Access, vol. 8, pp. 204518-204544, 2020.

\bibitem{ref31}
R.-C. Zheng, Y. Ai, and Z.-H. Ling, {\it{Speech reconstruction from silent tongue and lip articulation by pseudo target generation and domain adversarial training}}, in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023: IEEE, pp. 1-5. 

\bibitem{ref32}
J. Hong, M. Kim, S. J. Park, and Y. M. Ro, {\it{Speech reconstruction with reminiscent sound via visual voice memory}}, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3654-3667, 2021.

\bibitem{ref33}
T. Kefalas, Y. Panagakis, and M. Pantic, {\it{Large-scale unsupervised audio pre-training for video-to-speech synthesis}}, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024.

\bibitem{ref34}
X. Ai and B. Fang, {\it{Cross-Modal Language Modeling in Multi-Motion-Informed Context for Lip Reading}}, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 2220-2232, 2023.

\bibitem{ref35}
V. Farrahi, M. Niemelä, M. Kangas, R. Korpelainen, and T. Jämsä, {\it{Calibration and validation of accelerometer-based activity monitors: A systematic review of machine-learning approaches}}, Gait \& posture, vol. 68, pp. 285-299, 2019.

\bibitem{ref36}
R. P. Troiano, J. J. McClain, R. J. Brychta, and K. Y. Chen, {\it{Evolution of accelerometer methods for physical activity research}}, British journal of sports medicine, vol. 48, no. 13, pp. 1019-1023, 2014.

\bibitem{ref37}
M. Varanis, A. Silva, A. Mereles, and R. Pederiva, {\it{MEMS accelerometers for mechanical vibrations analysis: A comprehensive review with applications}}, Journal of the Brazilian Society of Mechanical Sciences and Engineering, vol. 40, pp. 1-18, 2018.

\bibitem{ref38}
I.-M. Lee and E. J. Shiroma, {\it{Using accelerometers to measure physical activity in large-scale epidemiological studies: issues and challenges}}, British journal of sports medicine, vol. 48, no. 3, pp. 197-201, 2014.

\bibitem{ref39}
F. Han, P. Yang, H. Du, and X.-Y. Li, {\it{Accuth: Anti-spoofing voice authentication via accelerometer}}, in Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems, 2022, pp. 637-650. 

\bibitem{ref40}
J. Kwon, J. Hwang, J. E. Sung, and C.-H. Im, {\it{Speech synthesis from three-axis accelerometer signals using conformer-based deep neural network}}, Computers in Biology and Medicine, vol. 182, p. 109090, 2024.

\bibitem{ref41}
S. R. Kadiri, F. Javanmardi, and P. Alku, {\it{Investigation of self-supervised pre-trained models for classification of voice quality from speech and neck surface accelerometer signals}}, Computer Speech \& Language, vol. 83, p. 101550, 2024.

\bibitem{ref42}
U. Shafiq, A. Waris, J. Iqbal, and S. Gilani, {\it{A Fusion of EMG and IMU for an Augmentative Speech Detection and Recognition System}}, IEEE Access, 2024.

\bibitem{ref43}
M. Wang, C. Yang, and C. Cheng, {\it{The contributions of phonology, orthography, and morphology in Chinese–English biliteracy acquisition}}, Applied Psycholinguistics, vol. 30, no. 2, pp. 291-314, 2009.

\bibitem{ref44}
X. Luo, Y. Yang, J. Sun, and N. Chen, "Correspondence between the Korean and Mandarin Chinese pronunciations of Chinese characters: A comparison at the sub-syllabic level," Buckeye East Asian Linguistics, vol. 4, pp. 46-56, 2019.

\bibitem{ref45}
J. Luo, J. Wang, N. Cheng, G. Jiang, and J. Xiao, {\it{End-to-end silent speech recognition with acoustic sensing}}, in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021: IEEE, pp. 606-612. 

\bibitem{ref46}
A. Graves, S. Fernández, and J. Schmidhuber, {\it{Bidirectional LSTM networks for improved phoneme classification and recognition}}, in International conference on artificial neural networks, 2005: Springer, pp. 799-804. 

\bibitem{ref47}
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, {\it{Gradient-based learning applied to document recognition}}, Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.

\bibitem{ref48}
V. Vapnik, The nature of statistical learning theory. Springer science \& business media, 2013.

\bibitem{ref49}
S. Kashiwagi, K. Tanaka, Q. Feng, and S. Morishima, {\it{Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning}}, arXiv preprint arXiv:2305.14203, 2023.

\bibitem{ref50}
A. Vaswani, {\it{Attention is all you need}}, Advances in Neural Information Processing Systems, 2017.

\bibitem{ref51}
A. Graves and N. Jaitly, {\it{Towards end-to-end speech recognition with recurrent neural networks}}, in International conference on machine learning, 2014: PMLR, pp. 1764-1772. 

\bibitem{ref52}
Y. Jiang, J. Yu, W. Yang, B. Zhang, and Y. Wang, "Nextformer: A convnext augmented conformer for end-to-end speech recognition," arXiv preprint arXiv:2206.14747, 2022.

\bibitem{ref53}
A. Gulati et al., {\it{Conformer: Convolution-augmented transformer for speech recognition}}, arXiv preprint arXiv:2005.08100, 2020.

\bibitem{ref54}
J. C. Lucero and K. G. Munhall, {\it{Analysis of facial motion patterns during speech using a matrix factorization algorithm}}, The Journal of the Acoustical Society of America, vol. 124, no. 4, pp. 2283-2290, 2008.

\bibitem{ref55}
M. Zhu et al., {\it{Towards optimizing electrode configurations for silent speech recognition based on high-density surface electromyography}}, Journal of neural engineering, vol. 18, no. 1, p. 016005, 2021.
 


\end{thebibliography}


% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{Yudong Xie}
% is currently working towards the B.S. degree in electronic engineering at Tsinghua University. His research interests include human-computer interactions, flexible sensing, and intelligent sensors.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiographynophoto}{Zhifeng Han}
% is currently working towards the B.S. degree in electronic engineering at Tsinghua University. His research interests include Wireless communication, CSI feedback and deep learning.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiographynophoto}{Qinfan Xiao}
% is currently working towards the B.S. degree in electronic engineering at Tsinghua University. His research interests include deep learning and brain-computer interface.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiographynophoto}{Liwei Liang}
% is working towards the master's degree in electronic information engineering at Tsinghua University. His research mainly includes human-computer interaction and flexible sensing.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiographynophoto}{Luqi Tao}
% is currently an associate professor at National Center for Information Research, Tsinghua University. He received the B.S. and Ph.D. degrees in 2012 and 2017 from Shandong University and Tsinghua University respectively. His major research interests include advanced micro-nano electron devices and wearable sensors.
% \end{IEEEbiographynophoto}




\vfill

\end{document}


