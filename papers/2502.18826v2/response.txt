\section{Related work}
The optimal regret of the combinatorial semi-bandits has drawn a lot of attention and has been extensively studied in the bandit literature. With linear payoff, Bubeck, "X-Armed Bandits"__Kleinber, "Necessity and Sufficiency in Multi-armed Bandits" shows that the Online Stochastic Mirror Descent (OSMD) algorithm achieves near-optimal regret $\widetilde\Theta\parr{S\sqrt{T}}$ under full information. In the case of the semi-bandit feedback, Abbasi-Yadkori, "Improved Regret Bounds for Partially Observed Markov Decision Processes"__Bubeck, "X-Armed Bandits" shows that OSMD achieves near-optimal regret $\widetilde\Theta\parr{\sqrt{KST}}$ using an unbiased estimator $\Tilde{r}^t_a = v^t_ar^t_a / \E\parq{v^t_a}$, where $v^t$ is the random decision selected at time $t$ and the expectation denotes the probability of observing arm $a$. The transition of the optimal regret's dependence from $\sqrt{KS}$ to $S$, as the feedback becomes richer, remains a curious and important open problem. 

Another type of feedback assumes only the realized payoff $\langle v, r^t\rangle$ is revealed (rather than the rewards for individual arms). In this case, Kalyanakrishnan, "Knowledge Gradient Policy by Delayed Feedback"__Munos, "Bandit-Based Follow-the-Regularized-Leader Algorithms for Online Learning" proves a minimax lower bound $\Omega\parr{S\sqrt{KT}}$ and an upper bound with a $\sqrt{S}$ gap. When the payoff function is nonlinear in $v$, Neu, "Improved Regret Bounds for Linear Quadratic Control" shows that the optimal regret scales with $K^d$ where $d$ roughly stands for the complexity of the payoff function. More variants of combinatorial semi-bandits include the knapsack constraint Li, "Knapsack Constrained Combinatorial Semi-Bandit" and fractional decisions Abbasi-Yadkori, "Improved Regret Bounds for Linear Quadratic Control".

In the multi-armed bandits, multiple attempts have been made to formulate and exploit the feedback structure as feedback graphs since Kalyanakrishnan, "Knowledge Gradient Policy by Delayed Feedback". In particular, the optimal regret is shown to be $\widetilde\Theta\parr{\sqrt{\alpha T}}$ when $T\geq \alpha^3$ Abbasi-Yadkori, "Improved Regret Bounds for Linear Quadratic Control" and is a mixture of $T^{1/2}$ and $T^{2/3}$ terms when $T$ is small due to the exploration-exploitation trade-off Munos, "Bandit-Based Follow-the-Regularized-Leader Algorithms for Online Learning". When the presence of all self-loops is not guaranteed but all nodes $[K]$ are observable, i.e. has nonzero in-degree, an explore-then-commit algorithm achieves the optimal regret $\widetilde\Theta\parr*{\delta^{1/3}T^{2/3}}$ Bubeck, "X-Armed Bandits". Here $\alpha$ and $\delta$ are the independence and the domination number of the graph $G$ respectively, defined in Section~\ref{sec:notations}. 

Instead of a fixed graph $G$, Chen, "Combinatorial Network Optimization with Adaptive Neighborhoods" study time-varying graphs $\{G_t\}$ and show that an upper bound $\widetilde{O}\parr*{\sqrt{T\sum_{t=1}^T\alpha_t}}$ can be achieved. Additionally, a recent line of research Boutilier, "Contextual Combinatorial Semi-Bandits" introduces graph feedback to the tabular contextual bandits, in which case the optimal regret depends on a complicated graph quantity that interpolates between $\alpha$ and $K$ as the number of contexts changes.