% Template for the submission to:
%   The Annals of Probability           [aop]
%   The Annals of Applied Probability   [aap]
%   The Annals of Statistics            [aos]
%   The Annals of Applied Statistics    [aoas]
%   Stochastic Systems                  [ssy]
%
%Author: In this template, the places where you need to add information
%        (or delete line) are indicated by {???}.  Mostly the information
%        required is obvious, but some explanations are given in lines starting
%Author:
%All other lines should be ignored.  After editing, there should be
%no instances of ??? after this line.

% use option [preprint] to remove info line at bottom
% journal options: aop,aap,aos,aoas,ssy
% natbib option: authoryear

%\documentclass[onecolumn,journal]{IEEEtran}

\documentclass[letterpaper,11pt]{article}
\usepackage[margin=1in]{geometry}  % set the margins to 1in on all sides

\input{defs}
% \usepackage{times}
% \usepackage{lineno,hyperref}
% \usepackage{xcolor}
% \usepackage{algorithm}
% \usepackage[ruled]{algorithm2e}
% % \usepackage{algpseudocode}
% \usepackage{sidecap, caption}
% \usepackage[nice]{nicefrac}
% \usepackage{mathtools}
% \usepackage{amsmath, bbm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NOTATIONS
\newcommand{\Acal}{\mathcal{A}}
\newcommand {\Bcal} {{\mathcal{B}}}
\newcommand {\Dcal} {{\mathcal{D}}}
\newcommand {\Ocal} {{\mathcal{O}}}
\newcommand {\Fcal} {{\mathcal{F}}}
\newcommand{\indic}{\mathbbm{1}}
\newcommand{\Tr}{\mathsf{tr}}
\newcommand{\Var}{\mathsf{Var}}
\newcommand{\hr}{\hat{r}}
\newcommand{\br}{\bar{r}}
\newcommand{\cb}{\check{b}}
\newcommand {\E} {{\mathbb{E}}}
\newcommand {\R} {{\mathbb{R}}}

\newcommand{\conv}{\mathrm{Conv}}
\newcommand{\reg}{\mathsf{R}}
\newcommand{\Csa}{\mathcal{C}_{\mathsf{SA}}}
\newcommand{\notto}{\not\rightarrow}
\newcommand{\Nout}{N_{\mathrm{out}}}
\newcommand{\dout}{d_{\mathrm{out}}}

\newcommand{\Aact}{\mathcal{A}_{\mathrm{act}}}

\newcommand{\YW}[1]{\textcolor{orange}{[YW: #1]}}
\newcommand{\YH}[1]{\textcolor{blue}{[YH: #1]}}
\newcommand\numberthis{\addtocounter{equation}
{1}\tag{\theequation}}


\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\nor}{\|}{\|}
\DeclarePairedDelimiter{\parr}{(}{)}
\DeclarePairedDelimiter{\parq}{[}{]}
\DeclarePairedDelimiter{\parqq}{\llbracket}{\rrbracket}
\DeclarePairedDelimiter{\bra}{\lbrace}{\rbrace}


\begin{document}
%\begin{frontmatter}

% "Title of the paper"
\title{Adversarial Combinatorial Semi-bandits with Graph Feedback}

\author{Yuxiao Wen\thanks{Yuxiao Wen is with the Courant Institute of Mathematical Sciences, New York University, email: \url{yuxiaowen@nyu.edu}.}}


%
\maketitle
%\thankstext{T1}{Footnote to the title with the ``thankstext'' command.}

\begin{abstract}
In combinatorial semi-bandits, a learner repeatedly selects from a combinatorial decision set of arms, receives the realized sum of rewards, and observes the rewards of the individual selected arms as feedback. In this paper, we extend this framework to include \emph{graph feedback}, where the learner observes the rewards of all neighboring arms of the selected arms in a feedback graph $G$. We establish that the optimal regret over a time horizon $T$ scales as $\widetilde{\Theta}\parr{S\sqrt{T}+\sqrt{\alpha ST}}$, where $S$ is the size of the combinatorial decisions and $\alpha$ is the independence number of $G$. This result interpolates between the known regrets $\widetilde\Theta\parr{S\sqrt{T}}$ under full information (i.e., $G$ is complete) and $\widetilde\Theta\parr{\sqrt{KST}}$ under the semi-bandit feedback (i.e., $G$ has only self-loops), where $K$ is the total number of arms. A key technical ingredient is to realize a convexified action using a random decision vector with negative correlations. 

\end{abstract}


\section{Introduction}
\label{sec:intro}
Combinatorial semi-bandits are a class of online learning problems that generalize the classical multi-armed bandits \cite{Robbins1952SomeAO} and have a wide range of applications including multi-platform online advertising \cite{avadhanula2021stochastic}, online recommendations \cite{wang2017efficient}, webpage optimization \cite{liu2021map}, and online shortest path \cite{gyorgy2007line}. In these applications, instead of taking an individual action, a set of actions is chosen at each time \cite{cesa2012combinatorial, audibert2014regret, chen2013combinatorial}. Mathematically, over a time horizon of length $T$ and for a fixed combinatorial budget $S$, a learner repeatedly chooses a (potentially constrained) combination of $K$ individual arms within the budget, i.e. from the following decision set
\[
\Acal_0 \subseteq \Acal \equiv  \bra*{v\in \{0,1\}^K: \|v\|_1 = S},
\]
and receives a linear payoff $\langle v, r^t\rangle$ where $r^t\in[0,1]^K$ denotes the reward associated to each arm at time $t$. After making the decision at time $t$, the learner observes $\{v_ar^t_a: a\in[K]\}$ as the \textit{semi-bandit feedback} or the entire reward vector $r^t$ under \textit{full information}. When $S=1$, it reduces to the multi-armed bandits with either the bandit feedback or full information. 

Under the adversarial setting for bandits \cite{auer1995gambling}, no statistical assumption is made about the reward vectors $\{r^t\}_{t\in[T]}$. Instead, they are (potentially) generated by an adaptive adversary. The objective is to minimize the expected \textit{regret} of the learner's algorithm $\pi$ compared to the best fixed decision in hindsight, defined as follows:
\begin{equation}\label{eq:reg_def}
\E\parq{\reg(\pi)} = \E\parq*{\max_{v_*\in\Acal_0}\sum_{t=1}^T\langle v_*-v^t, r^t\rangle}
\end{equation}
where $v^t\in\Acal_0$ is the decision chosen by $\pi$ at time $t$. The expectation is taken over any randomness in the learner's algorithm and over the rewards, since the reward $r^t$ is allowed to be generated adaptively and hence can be random. Note that while the adversary can generate the rewards $r^t$ adaptively, i.e. based on the learner's past decisions, the regret in \eqref{eq:reg_def} is measured against a fixed decision $v_*$ assuming the adversary would generate the same rewards.

While the semi-bandit feedback has been extensively studied, the current literature falls short of capturing additional information structures on the rewards of the individual arms, except for the full information case. As a motivating example, consider the multi-platform online advertising problem, where the arms represent the (discretized) bids. At each round and on each platform, the learner makes a bid and receives zero reward on losing the auction and her surplus on winning the auction. In many ads exchange platforms, the winning bid is always announced, and hence the learner can compute the counterfactual reward for any bids higher than her chosen bid \cite{han2024optimal}. This additional information is not taken into account in the semi-bandit feedback.

Another example is the online recommendation problem, where the website plans to present a combination of recommended items to the user. The semi-bandit feedback assumes that the user's behavior on the displayed items will reveal no information about the undisplayed items. However, this assumption often ignores the semantic relationship between the items. For instance, suppose two items $i$ and $j$ are both tissue packs with similar prices. If item $i$ is displayed and the user clicks on it, a click is likely to happen if item $j$ were to be displayed. On the other hand, if item $i$ is a football and item $j$ is a wheelchair, then a click on one probably means a no-click on the other. Information of this kind is beneficial for the website planner and yet overlooked in the semi-bandit feedback.

To capture this rich class of combinatorial semi-bandits with additional information, we consider a more general feedback structure described by a directed graph $G=([K], E)$\footnote{In this paper, we assume that the edge set $E$ contains all self-loops.} among the $K$ arms. After making the decision $v\in\Acal_0$ at each time, the learner now observes the rewards associated to all neighboring arms of the selected arms in $v$:
\[
\bra*{v_ir^t_i: \text{$\exists a\in[K]$ such that $v_a=1$ and $a\rightarrow i\in E$}}.
\]
This graph formulation allows us to leverage information that is unexploited in the semi-bandit feedback. 

Note that when $G$ is complete, the feedback structure corresponds to having full information; when $G$ contains only the self-loops, it becomes the semi-bandit feedback. In the presence of a general $G$, the exploration-exploitation trade-off becomes more complicated, and the goals of this paper are (1) to fully exploit this additional structure in the regret minimization and (2) to understand the fundamental learning limit in this class of problems.




\subsection{Related work}
The optimal regret of the combinatorial semi-bandits has drawn a lot of attention and has been extensively studied in the bandit literature. With linear payoff, \cite{koolen2010hedging} shows that the Online Stochastic Mirror Descent (OSMD) algorithm achieves near-optimal regret $\widetilde\Theta\parr{S\sqrt{T}}$ under full information. In the case of the semi-bandit feedback, \cite{audibert2014regret} shows that OSMD achieves near-optimal regret $\widetilde\Theta\parr{\sqrt{KST}}$ using an unbiased estimator $\Tilde{r}^t_a = v^t_ar^t_a / \E\parq{v^t_a}$, where $v^t$ is the random decision selected at time $t$ and the expectation denotes the probability of observing arm $a$. The transition of the optimal regret's dependence from $\sqrt{KS}$ to $S$, as the feedback becomes richer, remains a curious and important open problem. 

Another type of feedback assumes only the realized payoff $\langle v, r^t\rangle$ is revealed (rather than the rewards for individual arms). In this case, \cite{audibert2014regret} proves a minimax lower bound $\Omega\parr{S\sqrt{KT}}$ and an upper bound with a $\sqrt{S}$ gap. When the payoff function is nonlinear in $v$, \cite{han2021adversarial} shows that the optimal regret scales with $K^d$ where $d$ roughly stands for the complexity of the payoff function. More variants of combinatorial semi-bandits include the knapsack constraint \cite{sankararaman2018combinatorial} and fractional decisions \cite{pmlr-v37-wen15}.

In the multi-armed bandits, multiple attempts have been made to formulate and exploit the feedback structure as feedback graphs since \cite{mannor2011bandits}. In particular, the optimal regret is shown to be $\widetilde\Theta\parr{\sqrt{\alpha T}}$ when $T\geq \alpha^3$ \cite{alon2015online, eldowa2024minimax} and is a mixture of $T^{1/2}$ and $T^{2/3}$ terms when $T$ is small due to the exploration-exploitation trade-off \cite{kocak2023online}. When the presence of all self-loops is not guaranteed but all nodes $[K]$ are observable, i.e. has nonzero in-degree, an explore-then-commit algorithm achieves the optimal regret $\widetilde\Theta\parr*{\delta^{1/3}T^{2/3}}$ \cite{alon2015online}. Here $\alpha$ and $\delta$ are the independence and the domination number of the graph $G$ respectively, defined in Section~\ref{sec:notations}. 

Instead of a fixed graph $G$, \cite{cohen2016online, alon2017nonstochastic} study time-varying graphs $\{G_t\}$ and show that an upper bound $\widetilde{O}\parr*{\sqrt{T\sum_{t=1}^T\alpha_t}}$ can be achieved. Additionally, a recent line of research \cite{balseiro2023contextual, han2024optimal, wen2024stochastic} introduces graph feedback to the tabular contextual bandits, in which case the optimal regret depends on a complicated graph quantity that interpolates between $\alpha$ and $K$ as the number of contexts changes.


\subsection{Our results}
\label{sec:results}
\begin{table*}[t]
\caption{Minimax regret bounds up to polylogarithmic factors. Our results are in bold.}
\label{table:summary}
\vskip 0.15in
\begin{center}
\begin{tabular}{lccc}
\toprule
 & Semi-bandit ($\alpha=K$) &\textbf{General feedback graph $G$} & Full information ($\alpha=1$) \\
\midrule
Regret    & $\widetilde\Theta(\sqrt{KST})$ & $\mathbf{\widetilde\Theta\boldsymbol(S\sqrt{T} + \sqrt{\boldsymbol{\alpha} ST}\boldsymbol)}$ & $\widetilde\Theta(S\sqrt{T})$ \\

\bottomrule
\end{tabular}
% \end{sc}
% \end{small}
\end{center}
\vskip -0.1in
\end{table*}


In this paper, we present results on combinatorial semi-bandits with a feedback graph $G$ and the full decision set $\Acal_0 = \Acal$, while results on general $\Acal_0$ are discussed in \cref{sec:general_subset}.
Our results are summarized in Table~\ref{table:summary}, and the main contribution of this paper is three-fold:
\begin{enumerate}
    \item We introduce the formulation of a general feedback structure using feedback graphs in combinatorial semi-bandits.

    \item On the full decision set $\Acal$, we establish a minimax regret lower bound $\Omega(S\sqrt{T} + \sqrt{\alpha ST})$ that correctly captures the regret dependence on the feedback structure and outlines the transition from $\widetilde\Theta(S\sqrt{T})$ to $\widetilde\Theta(\sqrt{KST})$ as the feedback gets richer.


    \item We propose a policy OSMD-G (OSMD under graph feedback) that achieves near-optimal regret under general directed feedback graphs and adversarial rewards, while remaining the same computational efficiency as OSMD.
\end{enumerate}
It is straightforward to extend our results to all strongly observable graphs\footnote{In the language of \cite{alon2015online}, $G$ is strongly observable if, for every $a\in[K]$, either $a\rightarrow a$ or $a'\rightarrow a$ for all $a'\neq a$.}. When the feedback graphs $\bra{G_t}_{t\in[T]}$ are allowed to be time-varying, we can also obtain a corresponding upper bound. The upper bound results are summarized in the following theorem.
\begin{theorem}\label{thm:upper_bound}
Consider the full decision set $\Acal$. For $1\leq S\leq K$ and any directed graph $G=([K],E)$, there exists an algorithm $\pi$ that achieves regret
\[
\E\parq{\reg(\pi)} = \widetilde{O}\parr*{S\sqrt{T} + \sqrt{\alpha ST}}.
\]
When the feedback graphs $\bra{G_t}_{t\in[T]}$ are time-varying, the same algorithm $\pi$ achieves
\[
\E\parq{\reg(\pi)} = \widetilde{O}\parr*{S\sqrt{T} + \sqrt{S\sum_{t=1}^T\alpha_t}}
\]
where $\alpha_t=\alpha(G_t)$ is the independence number of $G_t$.
\end{theorem}

This algorithm $\pi$ is OSMD-G proposed in Section~\ref{sec:osmdg}. In OSMD-G, the learner solves for an optimal convexified action $x\in\conv(\Acal)$ via mirror descent at each time $t$, using the past observations, and then realizes it (in expectation) via selecting a random decision vector $v^t$. In the extreme cases of full information and semi-bandit feedback, the optimal regret is achieved as long as $v^t$ realizes the convexified action $x$ in expectation \cite{audibert2014regret}. However, under a general graph $G$, the regret analysis for a tight bound crucially requires this random decision vector to have negative correlations among the arms, i.e. $\mathsf{Cov}(v^t_i, v^t_j)\leq 0$ for $i\neq j$, in addition to the realization of $x$. Consequently, the following technical lemma plays a key role in our upper bound analysis: 

\begin{lemma}\label{lem:prob_properties}
Fix any $1\le S \le K$ and $x\in\conv(\Acal)$. There exists a probability distribution $p$ over $\Acal$ that satisfies:
\begin{enumerate}
    \item \textbf{(Mean)} $\forall i\in[K]$, $\E_{v\sim p}\parq*{v_i} = x_i$.

    \item \textbf{(Negative correlations)} $\forall i\neq j$, $\E_{v\sim p}\parq*{v_iv_j}\leq x_ix_j$, i.e. any pair of arms $(i,j)$ is negatively correlated.
\end{enumerate}
In particular, there is an efficient scheme to sample from $p$.
\end{lemma}
This lemma is a direct corollary of Theorem 1.1 in \cite{chekuri2009dependent}, and the sampling scheme is the randomized swap rounding. The mean condition guarantees that the convexified action is realized in expectation. The negative correlations essentially allow us to control the variance of the observed rewards in OSMD-G, thereby decoupling the final regret into two terms. Intuitively, the negative correlations imply a more exploratory sampling scheme; a more detailed discussion is in Section~\ref{sec:osmdg}.

To show that OSMD-G achieves near-optimal performance, we consider the following minimax regret:
\begin{equation}
\reg^* = \inf_{\pi}\sup_{\bra{r^t}}\E\parq{\reg(\pi)}
\end{equation}
where the inf is taken over all possible algorithms and the sup is taken over all potentially adversarial reward sequences. The following lower bound holds:
\begin{theorem}\label{thm:lower_bound}
Consider the full decision set $\Acal$. When $T\geq \alpha^3/S$ and $S<K$, it holds that
\[
\reg^* = \Omega\parr*{S\sqrt{T\log(K/S)} + \sqrt{\alpha ST}}.
\]
\end{theorem}
\noindent Note that the lower bound construction in Section~\ref{sec:lower_bound} is stochastic, as is standard in the literature, and thus stochastic combinatorial semi-bandits will not be easier. 


\subsection{Notations}
\label{sec:notations}
For $n\in\mathbb{N}$, denote $[n] = \bra{1,2,\dots,n}$. The convex hull of $\Acal$ is denoted by $\conv(\Acal)$, and the truncated convex hull is defined by
\[
\conv_\epsilon(\Acal) = \bra*{x\in \conv(A): x_i\geq \epsilon\textnormal{ for all }i\in[K]}.
\]
We use the standard asymptotic notations $\Omega, O, \Theta$ to denote the asymptotic behaviors up to constants, and $\widetilde\Omega, \widetilde{O}, \widetilde\Theta$ up to polylogarithmic factors respectively. Our results will concern the following graph quantities:
\begin{align*}
\alpha &= \max\bra*{|I|: I\subseteq [K]\text{ is an independent subset in $G$}},\\
\delta &= \min\bra*{|D|: D\subseteq [K]\text{ is a dominating subset in $G$}}.
\end{align*}
For a binary vector $v\in\Acal$ that represents an $S$-arm subset of $[K]$, we denote its out-neighbors in $G$ by
\[
\Nout(v) = \bra*{i\in[K]: \text{$\exists a\in[K]$ with $v_a=1$ and $a\rightarrow i$}}.
\]
Let $D\subseteq \R^d$ be an open convex set and $F:\bar D\rightarrow\R$ be a differentiable, strictly convex function. We denote the Bregman divergence defined by $F$ as
\[
D_F(x,y) = F(x) - F(y) - \langle \nabla F(y), x-y \rangle.
\]
The dual of $F$ is defined by
\[
F^*(z) = \sup_{x\in\bar D}\langle x, z\rangle - F(x).
\]





\section{Regret lower bound}
\label{sec:lower_bound}
In this section, we present the proof for the minimax regret lower bound in Theorem~\ref{thm:lower_bound}. For ease of notation, we let $v\in\Acal$ denote both the binary vector and the subset $\bra{i\in[K]: v_i=1}$ it represents when the context is clear. 
\begin{proof}[Proof of Theorem~\ref{thm:lower_bound}]
Under the full information setup (i.e. when $G$ is a complete graph), a lower bound $\Omega\parr{S\sqrt{T\log(K/S)}}$ was given by \cite{koolen2010hedging}, which implies that $\reg^*(G) = \Omega\parr{S\sqrt{T\log(K/S)}}$ for any general graph $G$.

To show the second part of the lower bound, without loss of generality, we may assume $\alpha = nS$ for some $n\in\mathbb{N}_{\geq 2}$. Consider a maximal independent set $I\subseteq [K]$ and partition it into $I_1,\dots, I_S$ such that $|I_m|=n=\frac{\alpha}{S}$ for $m\in[S]$. Index each subset by $I_m = \bra*{a_{m,1},\dots,a_{m,n}}$. To construct a hard instance, let $u\in[n]$ be a parameter and the product reward distribution be $P^u = \prod_{a\in[K]}\mathsf{Bern}(\mu_a)$ where
\[
\mu_a = 
\begin{cases}
\frac{1}{4} + \Delta & \quad \text{if $a = a_{m,u}\in I_m$ for $m\in[S]$;}\\
\frac{1}{4} & \quad \text{if $a\in I\backslash\{a_{m,u}\}_{m\in[S]}$;}\\
0 & \quad \text{if $a\not\in I$.}
\end{cases}
\]
The reward gap $\Delta\in(0, 1/4)$ will be specified later. Also let $P^0 = \prod_{a\in[K]}\mathsf{Bern}(\mu_a^0)$ with $\mu_a^0=\frac{1}{4}\indic[a\in I]$. Then the following observations hold:
\begin{enumerate}
    \item For each $u\in\Omega$, the optimal combinatorial decision is $v_*(u) = \{a_{m,u}\}_{m\in[S]}$, and any other $v\in\Acal$ suffers an instantaneous regret at least $\Delta|v \backslash v_*(u)|$;

    \item Under any $P^u$ or $P^0$, a decision $v\in\Acal$ suffers an instantaneous regret at least $\frac{1}{4}|v\cap I^c|$;
\end{enumerate}
Fix any policy $\pi$ and denote by $v_t=\{v_{t,1},\dots,v_{t,S}\}$ the arms pulled by $\pi$ at time $t$. Let $N_m$ be the total number of pulls in set $I_m$, $T_m(j)$ be the number of times $a_{m,j}$ is pulled, and $N_0$ be the total number of pulls outside $I$.
Let $u$ be uniformly distributed over $[n]$ and $\E_u$ be the expectation under environment $P^u$. We can lower bound the worst-case regret by the Bayes regret:
\begin{align*}
\max_{u\in[n]}\E_u\parq*{\reg(\pi)} &\geq \frac{1}{n}\sum_{u=1}^n\E_{u}\parq*{\frac{1}{4}N_0 + \Delta \sum_{m=1}^SN_m - T_m(u)}\\
&\geq \Delta\parr*{ST - \frac{1}{n}\sum_{u=1}^n\E_u\parq*{\sum_{m=1}^ST_m(u)}}\numberthis\label{eq:lower_eq2}
\end{align*}
since $N_0 + \sum_{m=1}^S N_m = ST$ and $\Delta \leq \frac{1}{4}$. Note that for any generated reward sequence $R^T = \bra*{r^t_a}_{t\in[T], a\in[K]}$, the policy's behavior is determined in the sense that $\E_u\parq*{T_m(j)|R^T} = \E_0\parq*{T_m(j)|R^T}$ for any $j\in[n]$, which implies for each $u\in[n]$ in the summand:

\begin{align*}
\E_u\parq*{\sum_{m=1}^ST_m(u)} - \E_0\parq*{\sum_{m=1}^ST_m(u)} &= \sum_{R^T}\Prob_u(R^T)\E_u\parq*{\sum_{m=1}^ST_m(u)\bigg|R^T} - \Prob_0(R^T)\E_0\parq*{\sum_{m=1}^ST_m(u)\bigg|R^T}\\
&= \sum_{R^T}\Prob_u(R^T)\E_u\parq*{\sum_{m=1}^ST_m(u)\bigg|R^T} - \Prob_0(R^T)\E_u\parq*{\sum_{m=1}^ST_m(u)\bigg|R^T}\\
&\leq ST\sum_{R^T:\Prob_u(R^T) \geq \Prob_0(R^T)}\parr*{\Prob_u(R^T) - \Prob_0(R^T)}\\
&\stepa{\leq} ST\sqrt{\frac{1}{2}\mathsf{KL}\parr*{\Prob_0(R^T)\|\Prob_u(R^T)}}\\
&\stepb{=} ST\sqrt{\frac{1}{2}\sum_{t=1}^T\sum_{R^{t-1}}\Prob_0(R^{t-1})\mathsf{KL}\parr*{\Prob_0(r^t|R^{t-1})\|\Prob_u(r^t|R^{t-1})}}
\end{align*}

where (a) uses Pinsker's inequality and (b) follows from the chain rule of KL divergence. By our construction of the reward distribution $P^u$, there is only a difference in the observed reward distribution when $a_{m,u_m}\in \Nout(v^t)$ for any $m\in[S]$. Via this observation and chain rule again, we have
\begin{align*}
&\sum_{R^{t-1}}\Prob_0(R^{t-1})\mathsf{KL}\parr*{\Prob_0(r^t|R^{t-1})\|\Prob_u(r^t|R^{t-1})}\\
&= \sum_{m=1}^S\Prob_0[a_{m,u}\in \Nout(v^t)]\mathsf{KL}\parr*{\mathsf{Bern}(1/4) \| \mathsf{Bern}(1/4+\Delta)}\\
&\leq \frac{64\Delta^2}{3}\sum_{m=1}^S\Prob_0[a_{m,u}\in \Nout(v^t)]
\end{align*}
where we use the inequality $\mathsf{KL}(\mathsf{Bern}(p) \| \mathsf{Bern}(q))\leq \frac{(p-q)^2}{q(1-q)}$ and $\Delta\in(0,1/4).$ Plugging this back, we get
\begin{align*}
\E_u\parq*{\sum_{m=1}^ST_m(u)} - \E_0\parq*{\sum_{m=1}^ST_m(u)} &\leq 8\Delta ST\sqrt{\frac{1}{6}\E_0\parq*{\sum_{t=1}^T\sum_{m=1}^S\indic[a_{m,u}\in \Nout(v^t)]}}\\
&\leq 8\Delta ST\sqrt{\frac{1}{6}\parr*{S\E_0\parq*{T_0} + \E_0\parq*{\sum_{m=1}^ST_m(u)}}}.\numberthis\label{eq:lower_eq1}
\end{align*}
Now suppose $\E_0\parq*{T_0} \geq \sqrt{\alpha ST}$. The policy $\pi$ will incur a large regret under $P^0$:
\begin{align*}
\E_0\parq*{\reg(\pi)} \geq \frac{1}{4}\E_0\parq*{T_0} = \frac{1}{4}\sqrt{\alpha ST}.
\end{align*}
Therefore, we may proceed with the assumption that $\E_0\parq*{T_0} < \sqrt{\alpha ST}$ in \eqref{eq:lower_eq1}:
\begin{align*}
\E_u\parq*{\sum_{m=1}^ST_m(u)} - \E_0\parq*{\sum_{m=1}^ST_m(u)} \leq 8\Delta ST\sqrt{\frac{1}{6}\parr*{S\sqrt{\alpha ST} + \E_0\parq*{\sum_{m=1}^ST_m(u)}}}.
\end{align*}
Plugging the above inequality back in \eqref{eq:lower_eq2} yields
\begin{align*}
\max_{u\in[n]}\E_u\parq*{\reg(\pi)}&\geq \Delta\parr*{ST - \frac{1}{n}\sum_{u=1}^n\E_0\parq*{\sum_{m=1}^ST_m(u)}} - \frac{1}{n}\sum_{u=1}^n8\Delta^2 ST\sqrt{\frac{1}{6}\parr*{S\sqrt{\alpha ST} + \E_0\parq*{\sum_{m=1}^ST_m(u)}}}\\
&\stepc{\geq} \Delta ST\parr*{\frac{1}{2} - 8\Delta\sqrt{\frac{1}{6}\parr*{S\sqrt{\alpha ST} + \frac{1}{n}ST}}}
\end{align*}
where (c) comes from the concavity of $x\mapsto\sqrt{x}$. By assumption, $T\geq \frac{\alpha^3}{S}$, so $\frac{ST}{n} = \frac{S^2T}{\alpha}\geq S\sqrt{\alpha ST}$. We can then lower bound the regret by
\begin{align*}
\max_{u\in[n]}\E_u\parq*{\reg(\pi)} &\geq \Delta ST\parr*{\frac{1}{2} - 8\Delta\sqrt{\frac{S^2T}{3\alpha}}}.
\end{align*}
Finally, choosing $\Delta=\frac{1}{32}\sqrt{\alpha/(ST)}\in (0,1/4)$ gives
\[
\max_{u\in[n]}\E_u\parq*{\reg(\pi)}\geq \frac{1}{64}\sqrt{\alpha ST} - \frac{1}{128}\sqrt{\frac{\alpha}{3T}} \geq \frac{1}{128}\sqrt{\alpha ST}.
\]
\end{proof}
Note that the term $\sqrt{\alpha ST}$ manifests in the bound only when $\alpha \geq S$. The key in the construction above is to partition the maximum independent set, when $\alpha\geq S$, into $S$ subsets of equal size $\alpha/S$. Then we may treat each subset as a multi-armed bandit sub-instance, as constructed in \cite{alon2015online}, which intuitively gives regret $\Omega(\sqrt{\alpha T/S})$. Summing over the $S$ sub-instances then yields the desired lower bound.


\section{A near-optimal algorithm}
\label{sec:upper_bound}
This section is structured as follows: In Section~\ref{sec:osmdg}, we present our OSMD-G algorithm and highlight the choice of reward estimators and sampling distributions that allow us to deal with general feedback graphs. Then we show that OSMD-G indeed achieves near-optimal regret $\widetilde{O}(S\sqrt{T}+\sqrt{\alpha ST})$ in Section~\ref{sec:upper_bound_proof}. 



\subsection{Online stochastic mirror descent with graphs}
\label{sec:osmdg}
The overall idea of OSMD-G (Algorithm~\ref{alg:osmdg}) is to perform a gradient descent step, based on the observed graph feedback and unbiased estimators, at each time $t$ in a dual space defined by a mirror mapping $F$ that satisfies the following:
\begin{definition}
Given an open convex set $D\subseteq \R^d$, a mirror mapping $F:\bar D\rightarrow \R$ satisfies
\begin{itemize}
    \item $F$ is strictly convex and differentiable on $D$;
    \item $\lim_{x\rightarrow\partial D}\|\nabla F(x)\|=+\infty$.
\end{itemize}
\end{definition}
While OSMD-G works with any well-defined mirror mapping, we will prove the desired upper bound in Section~\ref{sec:upper_bound_proof} for OSMD-G with mapping $F(x) = \sum_{i=1}^K(x_i\log(x_i) - x_i)$ defined on $D=\R^K_+$. For this choice of $F$, the dual space $D^* = \R^K$ and hence \eqref{eq:adv_dual_gradient_ascent} is always valid. In fact, \eqref{eq:adv_dual_gradient_ascent} admits the explicit form
\[
w^{t+1} = x^t\exp(\eta\Tilde{r}^t).
\]


\begin{algorithm}[ht!]\caption{Online Stochastic Mirror Descent under Graph Feedback (OSMD-G)}
\label{alg:osmdg}
\textbf{Input:} time horizon $T$, decision set $\Acal$, arms $[K]$, combinatorial budget $S$, feedback graph $G$, a truncation rate $\epsilon\in(0,1)$, a learning rate $\eta>0$, a mirror mapping $F$ defined on a closed convex set $\bar{\Dcal}\supseteq \conv_\epsilon(\Acal)$.

\textbf{Initialize:} $x^1 \gets \argmin_{x\in\conv_\epsilon(\Acal)} F(x)$.

\For{$t=1$ \KwTo $T$}
{
Generate a combinatorial decision $v^t$ by \cref{alg:random_swap_rounding} with target $x^t$.

Observe the feedback $\bra*{r^t_a: a\in\Nout(v^t)}$ and build the reward estimator for each $a\in[K]$:

\begin{equation}\label{eq:adv_reward_est}
    \Tilde{r}^t_a = \frac{\sum_{i\in[K]:i\rightarrow a}\indic[v^t_i=1]r^t_a}{\sum_{i\in[K]:i\rightarrow a}x^t_i}.
\end{equation}

Find $w^{t+1}\in\Dcal$ such that
\begin{equation}\label{eq:adv_dual_gradient_ascent}
    \nabla F(w^{t+1}) = \nabla F(x^t) + \eta \Tilde{r}^t.
\end{equation}

Project $w^{t+1}$ to the truncated convex hull $\conv_\epsilon(\Acal)$:
\begin{equation}\label{eq:adv_bregman_projection}
    x^{t+1} \gets \argmin_{x\in\conv_\epsilon(\Acal)} D_F(x, w^{t+1}).
\end{equation}
}
\end{algorithm}

Recall at each time $t$, for a selected decision $v^t\in\Acal$, the learner observes graph feedback $\bra{v^t_ir^t_i: i\in\Nout(v^t)}$. Based on this, we define the reward estimator for each arm $a\in[K]$ at time $t$ in \eqref{eq:adv_reward_est}. As we invoke a sampling scheme to realize $x^t$ in expectation, i.e. $\E[v^t]=x^t$, our estimator in \eqref{eq:adv_reward_est} is unbiased. 

A crucial step in OSMD-G is to sample a decision $v^t$ at each time $t$ that satisfies both the mean condition $\E[v^t]=x^t$ and the negative correlation $\E[v^t_iv^t_j]\leq x_ix_j$. Thanks to Lemma~\ref{lem:prob_properties}, both conditions are guaranteed for \textit{all} possible target $x^t\in\conv(\Acal)$ when we invoke \cref{alg:random_swap_rounding} as our sampling subroutine. The description and details of \cref{alg:random_swap_rounding} are deferred to \cref{app:rsr}.

While seemingly intuitive given that $\|v^t\|_1=S$, we emphasize that the negative correlations $\E[v^t_iv^t_j]\leq x_ix_j$ do not necessarily hold and can be non-trivial to achieve. Consider the case $S=2$. When $x^t=\frac{2}{K}\mathbf{1}$ is the uniform vector, a uniform distribution over all pairs satisfies the correlation condition, seeming to suggest the choice of $p(i,j)\propto x_i^t x_j^t$. However, when $x^t = (1, 0.8, 0.2)$, the only solution is to sample the combination $\{1,2\}$ with probability $0.8$ and $\{1,3\}$ with probability $0.2$, suggesting a zero probability for sampling $\{2,3\}$. A general strategy must be able to generalize both scenarios. From the perspective of linear programming, the correlation condition adds $\binom{K}{2}$ constraints to the original $K$ constraints (from the mean condition) in finding $p^t$, making it much harder to find a feasible solution.


\subsection{Regret upper bound}
\label{sec:upper_bound_proof}
In the following theorem, we show that OSMD-G achieves near-optimal regret for a time-invariant feedback graph. The proof for time-varying feedback graphs $\bra{G_t}_{t\in[T]}$ only takes a one-line change in \eqref{eq:change_time_varying}. It is clear that \cref{thm:upper_bound_detailed} implies \cref{thm:upper_bound}.

\begin{theorem}\label{thm:upper_bound_detailed}
Let the mirror mapping be $F(x) = \sum_{i=1}^K(x_i\log x_i - x_i)$. When the correlation condition for $p^t$ is satisfied, the expected regret of Algorithm~\ref{alg:osmdg} is upper bounded by
\[
\E[\reg(\mathrm{Alg~\ref{alg:osmdg}})] \leq \epsilon KT + \frac{S\log(K/S)}{\eta} + \frac{\eta}{2}(S+4\alpha\log(4K/(\epsilon\alpha))) T.
\]
In particular, with learning rate $\eta = \sqrt{\frac{2S\log(K/S)}{(S+4\alpha\log(4K^2T/\alpha)) T}}$, and truncation $\epsilon=\frac{1}{KT}$, it becomes
\[
\E[\reg(\mathrm{Alg~\ref{alg:osmdg}})] \leq S\sqrt{T\log(K/S)} + 2\sqrt{\alpha ST\log(K/S)\log(4K^2T/\alpha)}.
\]
\end{theorem}
\begin{proof}
Fix any $v\in\Acal$. Let 
\[
v_\epsilon = \argmin_{v'\in\conv_\epsilon(\Acal)}\|v-v'\|_1
\]
which satisfies $(v-v_\epsilon)^Tr^t \leq \|v-v_\epsilon\|_1 \leq K\epsilon$ since $r^t\in[0,1]^K$. We can decompose the regret as
\begin{align*}
\E\parq*{\sum_{t=1}^T\parr*{v-v^t}^Tr^t} &= \E \parq*{\sum_{t=1}^T\parr*{v-v_\epsilon}^Tr^t + \parr*{v_\epsilon-v^t}^Tr^t}\\
&\leq \epsilon TK + \E \parq*{\sum_{t=1}^T\parr*{v_\epsilon-v^t}^Tr^t}\numberthis\label{eq:breg_div_reg_decomp1}
\end{align*}
To upper bound the second term, note that the reward estimator in \eqref{eq:adv_reward_est} is unbiased: for any $t\in[T]$ and $a\in[K]$,
\begin{align*}
\E\parq*{\Tilde{r}^t_a} = \frac{\E\parq*{\sum_{i\in[K]:i\rightarrow a}\indic[v^t_i=1]}}{\sum_{i\in[K]:i\rightarrow a}x^t_i}r^t_a = \frac{\sum_{i\in[K]:i\rightarrow a}\E\parq*{\indic[v^t_i=1]}}{\sum_{i\in[K]:i\rightarrow a}x^t_i}r^t_a \stepa{=} r^t_a
\end{align*}
where (a) follows from $x^t_i=\E_{v^t\sim p^t}[v^t_i]=\E\parq*{\indic[v^t_i=1]}$ by the choice of $p^t$. Given that $\Tilde{r}^t$ is unbiased and nonnegative, we have $\E\parq*{\parr*{v_\epsilon-v^t}^Tr^t} = \E\parq*{\parr*{v_\epsilon-x^t}^T\Tilde{r}^t}$, and the following decomposition is inspired by \cite{audibert2014regret}: by \eqref{eq:adv_dual_gradient_ascent} and the definition of Bregman divergence,
\begin{align*}
\eta\parr*{v_\epsilon-x^t}^T\Tilde{r}^t &= \parr*{v_\epsilon-x^t}^T\parr*{\nabla F(w^{t+1})-\nabla F(x^t)} \\
&= D_F(v_\epsilon, x^t) + D_F(x^t, w^{t+1})  - D_F(v_\epsilon, w^{t+1})\\
&\stepb{\leq} D_F(v_\epsilon, x^t) + D_F(x^t, w^{t+1})  - D_F(v_\epsilon, x^{t+1}) - D_F(x^{t+1}, w^{t+1})
\end{align*}
where (b) follows from the Pythagorean theorem for Bregman divergences \cite{cesa2006prediction}[Lemma 11.3] since $x^{t+1}$ is the projection of $w^{t+1}$ onto $\conv_\epsilon(\Acal)$ in \eqref{eq:adv_bregman_projection}. Summing over time $t$ gives
\begin{align*}
\E \parq*{\sum_{t=1}^T\parr*{v_\epsilon-v^t}^Tr^t}&\leq \frac{1}{\eta}\E\parq*{D_F(v_\epsilon, x^1)- D_F(v_\epsilon, x^{T+1})}  + \frac{1}{\eta}\E\parq*{\sum_{t=1}^T D_F(x^t, w^{t+1}) -  D_F(x^{t+1}, w^{t+1})}\\
&\leq \frac{1}{\eta}\E\parq*{D_F(v_\epsilon, x^1) + \sum_{t=1}^T D_F(x^t, w^{t+1})}\numberthis\label{eq:breg_div_reg_decomp2}
\end{align*}
since Bregman divergences are nonnegative. By Lemma~\ref{lem:minimizer_of_convex_func} and the log-sum inequality, the first term is bounded by
\begin{align*}
D_F(v_\epsilon, x^1)\leq F(v_\epsilon)-F(x^1)&\leq \sum_{i=1}^Kx^1_i\log\frac{1}{x^1_i} \leq S\log(K/S).
\end{align*}
To bound the second term in \eqref{eq:breg_div_reg_decomp2}, by straightforward computation and Lemma~\ref{lem:breg_div_dual}, we have
\[
D_{F^*}(\nabla F(y), \nabla F(x)) \leq \frac{1}{2}\sum_{i=1}^K x_i\parr*{\nabla F(y) - \nabla F(x)}_i^2
\]
which, together with \eqref{eq:adv_dual_gradient_ascent} in OSMD-G, gives
\begin{align*}
\E\parq*{\sum_{t=1}^T D_F(x^t, w^{t+1})} &\stepc{=} \E\parq*{\sum_{t=1}^T D_{F^*}(\nabla F(w^{t+1}), \nabla F(x^t))} \\
&= \E\parq*{\sum_{t=1}^T D_{F^*}(\nabla F(x^t) + \eta\Tilde{r}^t, \nabla F(x^t))}\\
&\leq \frac{1}{2}\E\parq*{\sum_{t=1}^T\sum_{a=1}^Kx^t_a\parr*{\eta\Tilde{r}^t_a}^2}\\
&= \frac{\eta^2}{2}\sum_{t=1}^T\sum_{a=1}^K\E\parq*{x^t_a\parr*{\frac{\sum_{i\in[K]:i\rightarrow a}\indic[v^t_i=1]r^t_a}{\sum_{i\in[K]:i\rightarrow a}x^t_i}}^2}\\
&\stepd{\leq} \frac{\eta^2}{2}\sum_{t=1}^T\sum_{a=1}^Kx^t_a\frac{\E\parq*{\sum_{i\in[K]:i\rightarrow a}v^t_i}^2}{\parr*{\sum_{i\in[K]:i\rightarrow a}x^t_i}^2}\numberthis\label{eq:temp_end_eq1}
\end{align*}
where (c) invokes Lemma~\ref{lem:breg_div_dual} again and (d) follows from $r^t_a\in[0,1]$ and $v^t_i\in\{0,1\}$. By the choice of $v^t$ in Algorithm~\ref{alg:osmdg}, the random variables $v^t_i$ are negatively correlated, and thus we can upper bound the second moment of this sum in \eqref{eq:temp_end_eq1}:
\begin{align*}
\E\parq*{\sum_{i\in[K]:i\rightarrow a}v^t_i}^2 &= \parr*{\sum_{i\in[K]:i\rightarrow a}\E\parq*{v^t_i}}^2 + \Var\parr*{\sum_{i\in[K]:i\rightarrow a}v^t_i}\\
&= \parr*{\sum_{i\in[K]:i\rightarrow a}\E\parq*{v^t_i}}^2 + \sum_{i\in[K]:i\rightarrow a}\Var\parr*{v^t_i}   + \sum_{\substack{i,j\in[K]: i\neq j \\ i,j\rightarrow a}}\mathsf{Cov}\parr*{v^t_i, v^t_j}\\
&\leq \parr*{\sum_{i\in[K]:i\rightarrow a}x^t_i}^2 + \sum_{i\in[K]:i\rightarrow a}x^t_i. \numberthis\label{eq:decompose_second_moment_sum}
\end{align*}
Plugging \eqref{eq:decompose_second_moment_sum} in \eqref{eq:temp_end_eq1} yields
\begin{align*}
\E\parq*{\sum_{t=1}^T D_F(x^t, w^{t+1})} &\leq \frac{\eta^2}{2}\sum_{t=1}^T\sum_{a=1}^Kx^t_a\parr*{1 + \frac{1}{\sum_{i\in[K]:i\rightarrow a}x^t_i}}\\
&\stepe{\leq} \frac{\eta^2}{2}T\parr*{S + 4\alpha\log\parr*{\frac{4K}{\epsilon\alpha}}}\numberthis\label{eq:change_time_varying}
\end{align*}
where (e) is due to $\sum_{a=1}^Kx_a^t\leq S$ and Lemma~\ref{lem:alon_alpha}. Combining \eqref{eq:change_time_varying} with \eqref{eq:breg_div_reg_decomp1} and \eqref{eq:breg_div_reg_decomp2}, we end up with the desired upper bound
\begin{align*}
\E[\reg(\mathrm{Alg~\ref{alg:osmdg}})] \leq &\epsilon KT + \frac{S\log(K/S)}{\eta} + \frac{\eta}{2}\parr*{S+4\alpha\log\parr*{4K/(\epsilon\alpha)}} T.
\end{align*}
\end{proof}
Note that at each time $t$ and for each arm $a\in[K]$, the total number of arms that observe $a$ is a random variable due to the random decision $v^t$. In \eqref{eq:temp_end_eq1} in the proof above, one can naively bound the second moment of this random variable by
\[
\E\parq*{\sum_{i\in[K]:i\rightarrow a}v^t_i}^2 \leq S\E\parq*{\sum_{i\in[K]:i\rightarrow a}v^t_i}
\]
since $\|v^t\|_1\leq S$, which leads to an upper bound $\widetilde{O}(S\sqrt{\alpha T})$. To improve on this bound, we need to further exploit the structures of the feedback graph $G$ and/or the sampling distribution $p^t$ of $v^t$, which motivates Lemma~\ref{lem:prob_properties}. The negative correlations therein allow us to decompose this second moment into the squared mean and a sum of the individual variances, as in \eqref{eq:decompose_second_moment_sum}. By saving on the $O(K^2)$ correlation terms, this decomposition shaves the factor in \eqref{eq:temp_end_eq1} from $S\alpha$ to $S+\alpha$, yielding the desired result $\widetilde{O}(S\sqrt{T}+\sqrt{\alpha ST})$.


\section{Discussions}

\subsection{When negative correlations are impossible}
\label{sec:general_subset}
So far, we have shown the optimal regret $\widetilde\Theta(S\sqrt{T}+ \sqrt{\alpha ST})$ on the full decision set $\Acal$. Our upper bound in Theorem~\ref{thm:upper_bound} fails on general decision subsets $\Acal_0\subseteq \Acal$, because it is not always possible to find a distribution $p^t$ for the decision $v^t$ in OSMD-G that provides the negative correlations in Lemma~\ref{lem:prob_properties}. For example, when there is a pair of arms $(a,b)$ with $v_a = v_b$ for all $v\in\Acal_0$, it is simply impossible to achieve negative correlations. 

This failure, however, is not merely an analysis artifact. In the following, we present an example where moving from the full set $\Acal$ to a proper subset $\Acal_0\subsetneq \Acal$ provably increases the optimal regret to $\widetilde\Theta(S\sqrt{\alpha T})$.

Assuming $S$ divides $K$, we let $V_1,V_2,\dots, V_{K/S}$ be a partition of the arms $[K]$ of equal size $S$. For the feedback graph $G$, let each $V_i$ be a clique for $i=1,\dots,K/S$. Let $H$ be an arbitrary other graph over the cliques such that $V_i\rightarrow V_j$ in $H$ iff $a\rightarrow b$ for all $a\in V_i$ and $b\in V_j$ in $G$. The independence numbers $\alpha(G) = \alpha(H)$ are equal. On the full decision set $\Acal$, Theorem~\ref{thm:upper_bound} and \ref{thm:lower_bound} tell us the optimal regret is $\widetilde\Theta(S\sqrt{T}+ \sqrt{\alpha ST})$.

Now consider a proper decision subset
\begin{equation}\label{eq:subset}
\Acal_{\text{partition}} = \bra{\mathbf{1}_{1:S}, \mathbf{1}_{S+1:2S},\dots,\mathbf{1}_{K-S+1:K}}
\end{equation}
where $(\mathbf{1}_{i:j})_k=\indic[i\leq k\leq j]$ is one on the coordinates from $i$ to $j$ and zero otherwise. Namely, the only feasible decisions are the first $S$ arms in $V_1$, the next $S$ arms in $V_2$, ..., and the last $S$ arms in $V_{K/S}$. It is straightforward to see that this problem is equivalent to a multi-armed bandit with $K/S$ arms and a feedback graph $H$, and the rewards range in $[0,S]$. From the bandit literature \cite{alon2015online}, the optimal regret on this decision subset $\Acal_{\text{partition}}$ is $\widetilde\Theta(S\sqrt{\alpha T})$ which is fundamentally different from the result for the full decision set, even under the same feedback graph.

This example implies that any algorithm for general feedback graphs and general decision subsets can only guarantee $\widetilde{O}(S\sqrt{\alpha T})$, when $S\alpha \leq K$. Hence the following upper bound is of interest:

\begin{theorem}\label{thm:general_subset}
On general decision subset $\Acal_0\subseteq\Acal$ where the correlation condition is not guaranteed, the algorithm OSMD-G achieves
\[
\E\parq{\reg(\mathrm{Alg~\ref{alg:osmdg}})} = \widetilde{O}\parr*{S\sqrt{\alpha T}}.
\]
\end{theorem}
\vspace{-1em}
For any target $x^t\in\conv(\Acal_0)$, there is always a probability distribution $p^t$ such that $\E_{p^t}[v^t]=x^t$, which is used in earlier works \cite{audibert2014regret, koolen2010hedging}. With this choise of $p^t$, OSMD-G achieves the regret in \cref{thm:general_subset}. The proof follows from Section~\ref{sec:upper_bound_proof} and is left to Appendix~\ref{app:upper_bound_proof}. Together with the construction of $\Acal_{\text{partition}}$ in \eqref{eq:subset}, it suggests that leveraging the negative correlations in OSMD-G is crucial to achieving improved regret $\widetilde{O}(S\sqrt{T}+\sqrt{\alpha ST})$ on the full decision set.


Note on general $\Acal_0$, the efficiency of OSMD-G is no longer guaranteed; see discussions in \cite{koolen2010hedging, audibert2014regret}. To compensate, we provide an elimination-based algorithm that is agnostic of the structure of the decision subset $\Acal_0$ and achieves $\widetilde{O}(S\sqrt{\alpha T})$ when the rewards are stochastic. The algorithm and its analysis are left in Appendix~\ref{app:stochastic}.


\subsection{When negative correlations are possible}
By Theorem 1.1 in \cite{chekuri2009dependent}, \cref{lem:prob_properties} and OSMD-G can be generalized directly to any decision subset $\Acal_0'\subseteq \bra{v\in\{0,1\}^K: \|v\|_1\le S}$ that forms a matroid. Notably, matroids require that decisions with size less than $S$ are also feasible, hence they are different from the setup $\Acal_0\subseteq \Acal$ we consider throughout this work. 

In addition, while \cite{chekuri2009dependent} focuses on matroids, the proof of their Theorem 1.1 only relies on the following exchange property of a decision set $\Acal_0$: for any $v,u\in\Acal_0$, there exist $i\in u-v$ and $j\in v-u$ such that $u-i+j, v-j+i\in\Acal_0$. \cref{lem:prob_properties} follows from the fact that our full set $\Acal$ satisfies this property. Here we provide another example of $\Acal_0\subseteq \Acal$ satisfying it:

Consider the problem that the learner operates on $S$ systems in parallel, and on each system $s$ he/she has $K_s$ arms to choose from. Then $K=\sum_{s\in[S]}K_s$ and the feasible decisions are $\Acal_0=\bra{(v_1,\dots, v_S): v_s\in [K_s]}$. It is clear that this $\Acal_0$ satisfies the exchange property above, and hence OSMD-G applies directly to such problems, e.g. multi-platform online advertising.




\subsection{Weakly observable graphs}
The results in previous sections apply to the strongly observable feedback graphs. A natural question would be what regret guarantee we can get when the feedback graph $G=([K],E)$ is weakly observable\footnote{A graph $G$ is weakly observable if every node has nonzero in-degree and $G$ itself is not strongly observable.}.

In the context of stochastic rewards, we can consider the following explore-then-commit (ETC) policy: the learner first explores the arms in a minimal dominating subset\footnote{While finding the minimal dominating subset is NP-hard, there is an efficient $\log(K)$-approximate algorithm, which we include in Appendix~\ref{app:auxiliary} for completeness.} as uniformly as possible for $T_0$ time steps, and then commit to the $S$ empirically best arms for the rest of the time. Its performance is characterized by the following result.
\begin{theorem}
With high probability, the ETC policy achieves regret $\widetilde{O}(ST^{2/3} + \delta^{1/3}S^{2/3}T^{2/3})$.
\end{theorem}
We briefly outline the proof here. When $\delta \geq S$, thanks to the stochastic assumption and concentration, each arm contributes a sub-optimality $\widetilde{O}(\sqrt{\delta/ST_0})$. Trading off $T_0$ in the upper bound
\[
ST_0 + ST\sqrt{\delta/(ST_0)}
\]
gives the bound $\widetilde{O}(\delta^{1/3}S^{2/3}T^{2/3})$. When $\delta <S$, a similar analysis yields the bound $\widetilde{O}(ST^{2/3})$. In multi-armed bandits (when $S=1$), this ETC policy achieves the optimal regret $\widetilde\Theta(\delta^{1/3}T^{2/3})$ \cite{alon2015online}. It remains as an open problem whether ETC is still optimal in the combinatorial semi-bandits with $S>1$.



\section*{Acknowledgements}
The author is grateful to Yanjun Han for helpful discussions and for pointing to the matroid theory literature.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{Preprint}
\bibliographystyle{alpha}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Randomized Swap Rounding}\label{app:rsr}
This section introduces the randomized swap rounding scheme by \cite{chekuri2009dependent} that is invoked in \cref{alg:osmdg}. Note that randomized swap rounding is not always valid for any decision set $A$: its validity crucially relies on the exchange property that for any $u,c\in A$, there exist $a\in u\backslash c$ and $a'\in c\backslash u$ such that $u-\{a\}+\{a'\}\in A$ and $c-\{a'\}+\{a\}\in A$. This property is satisfied by the full decision set $\Acal$ as well as any subset $A\subseteq \bra{v\in\{0,1\}^K: \|v\|_1\le S}$ that forms a matroid. However, for general $A$ this can be violated, and as discussed in Section~\ref{sec:general_subset}, no sampling scheme can guarantee the negative correlations and the learner must suffer a $\widetilde{\Theta}(S\sqrt{\alpha T})$ regret.
\begin{algorithm}[ht!]\caption{Randomized Swap Rounding}
\label{alg:random_swap_rounding}
\textbf{Input:} decision set $A$, arms $[K]$, target $x=\sum_{i=1}^Nw_i v_i$ where $N=|A|$.

\textbf{Initialize:} $u \gets v_1$.

\For{$i=1$ \KwTo $N-1$}
{
Denote $c\gets v_{i+1}$ and $\beta_i\gets \sum_{j=1}^iw_j$.

\While{$u\neq c$}{
Pick $a\in u\backslash c$ and $a'\in c\backslash u$ such that $u-\{a\}+\{a'\}\in A$ and $c-\{a'\}+\{a\}\in A$.

With probability $\frac{\beta_i}{\beta_i + w_{i+1}}$, set $c\gets c-\{a'\}+\{a\}$;

Otherwise, set $u\gets u-\{a\}+\{a'\}$.
}
}
Output $u$.
\end{algorithm}

\section{Arm elimination algorithm for stochastic rewards}
\label{app:stochastic}
As promised in Section~\ref{sec:general_subset}, we present an elimination-based algorithm, called Combinatorial Arm Elimination, that is agnostic to the decision subset $\Acal_0$ and achieves regret $\widetilde{O}(S\sqrt{\alpha T})$. We assume the reward $r^t_i\in[0,1]$ for each arm $i\in[K]$ is i.i.d. with a time-invariant mean $\mu_i$. The algorithm maintains an active set of the decisions and successively eliminates decisions that are statistically suboptimal. It crucially leverages a structured exploration within the active set $\Aact$. In the proof below and in Algorithm~\ref{alg:comb_arm_elim}, for ease of notation, we let $v\in\Acal_0$ denote both the binary vector and the subset of $[K]$ it represents. So $a\in v\subseteq [K]$ if $v_a=1$.

\begin{algorithm}[h!]\caption{Combinatorial Arm Elimination}
\label{alg:comb_arm_elim}
\textbf{Input:} time horizon $T$, decision subset $\Acal_0\subseteq\Acal$, arm set $[K]$, combinatorial budget $S$, feedback graph $G$, and failure probability $\epsilon\in(0,1)$.

\textbf{Initialize:} Active set $\Aact\gets \Acal_0$, minimum count $N\gets 0$.

Let $(\br_a^t, n_a^t)$ be the empirical reward and the observation count of arm $a\in[K]$ at time $t$.

For each combinatorial decision $v\in\Aact$, let $\br_v^t = \sum_{a\in v}\br_a^t$ be the empirical reward and $n_v^t = \min_{a\in v}n_a^t$ be the minimum observation count.

\For{$t=1$ \KwTo $T$}{
Let $\Acal_N\gets \bra*{v\in \Aact: n_v^t = N}$ be the decisions that have been observed least.

Let $G_N$ be the graph $G$ restricted to the set $U_t=\bra*{a\in[K]: \exists v\in\Acal_N \textnormal{ with } a\in v} = \bigcup_{v\in \Acal_N}v$.

Let $a_t\in U_t$ be the arm with the largest out-degree (break tie arbitrarily).

Pull any decision $v_t\in \Acal_N$ with $a_t\in v_t$.

Observe the feedback $\bra*{r^t_a: a\in\Nout(v_t)}$ and update $(\br^t_a, n^t_a)$ accordingly.

\If{$\min_{v\in\Acal_N}n^t_v > N$}{
Update the minimum count $N\gets \min_{v\in\Aact}$.

Let $\br^t_{\max}\gets \max_{v\in\Aact}\br^t_v$ be the maximum empirical reward in the active set.

Update the active set as follows:
\[
\Aact \gets \bra*{v\in\Aact : \br^t_v \geq \br^t_{\max} - 6S\sqrt{\frac{\log(2T)\log(KT/\epsilon)}{N}}}.
\]
}
}
\end{algorithm}

\begin{theorem}
Fix any failure probability $\epsilon\in(0,1)$. For any decision subset $\Acal_0\subseteq \Acal$, with probability at least $1-\epsilon$, Algorithm~\ref{alg:comb_arm_elim} achieves expected regret
\[
\E\parq{\reg(\mathrm{Alg~\ref{alg:comb_arm_elim}})} = \widetilde{O}\parr*{K+ S\sqrt{\log(KT/\epsilon)\alpha T} + \epsilon T}.
\]
\end{theorem}
\begin{proof}
Fix any $\epsilon\in(0,1).$ For any $n\geq 1$, denote $\Delta_n = 3\sqrt{\log(2T)\log(KT/\epsilon)/n}$. During the period of $N=n$, by Lemma~\ref{lem:sto_reward_concentration}, with probability at least $1-\epsilon$, we have $\abs*{\br^t_a-\mu_a}\leq \Delta_n$ for any individual arm $a\in U_t$ at any time $t$. In the remaining proof, we assume this event holds. Then the optimal combinatorial decision $v_*$ is not eliminated at the end of this period, since
\begin{align*}
\br^t_{v_*} \geq \mu_{v_*} - S\Delta_n \geq \mu_{\max} - S\Delta_n \geq \br^t_{\max} - 2S\Delta_n.
\end{align*}
In addition, for any $v\in\Aact$, the elimination step guarantees that
\begin{equation}\label{eq:sto_bounded_suboptimality}
\mu_v \geq \br^t_v - S\Delta_n \geq \br^t_{\max} - 3S\Delta_n \geq \br^t_{v_*} - 3S\Delta_n \geq \mu_{v_*} - 4S\Delta_n.
\end{equation}
Let $T_n$ be the duration of $N=n$. Recall that $a_t\in U_t$ has the largest out-degree in the graph $G$ restricted to $U_t$. By Lemma~\ref{lem:greedy_dom} and Lemma~\ref{lem:alon15}, we are able to bound $T_n$:
\[
T_n \leq (1+\log(K))\delta(G_N) \leq 50\log(K)(1+\log(K))\alpha(G_N) \leq 50\log(K)(1+\log(K))\alpha\equiv M.
\]
By \eqref{eq:sto_bounded_suboptimality}, the regret incurred during $T_n$ is bounded by $4S\Delta_nT_n$. Thus with probability at least $1-\epsilon$, the total regret is upper bounded by
\begin{align*}
\E\parq{\reg(\mathrm{Alg~\ref{alg:comb_arm_elim}})} &\leq T_0 + 4S\sum_{n=1}^\infty \Delta_nT_n\\
&\leq T_0 + 4S\sum_{n=1}^{T/M} \Delta_nM\\
&\leq T_0 + 12SM\sqrt{\log(2T)\log(KT/\epsilon)}\sqrt{T/M}\\
&\leq T_0+ 12S\sqrt{\log(2T)\log(KT/\epsilon)}\sqrt{MT}\\
&\leq K + 60\sqrt{\log(K)(1+\log(K))\log(2T)\log(KT/\epsilon)}S\sqrt{\alpha T}.
\end{align*}
\end{proof}


\section{Proof of Theorem~\ref{thm:general_subset}}
\label{app:upper_bound_proof}
The proof of Theorem~\ref{thm:general_subset} replicates that of Theorem~\ref{thm:upper_bound_detailed}. The only difference is that the correlation condition of $p^t$ is no longer guaranteed on general $\Acal_0$. Thus we have to bound \eqref{eq:temp_end_eq1} differently:
\begin{align*}
\frac{\eta^2}{2}\sum_{t=1}^T\sum_{a=1}^Kx^t_a\frac{\E\parq*{\sum_{i\in[K]:i\rightarrow a}v^t_i}^2}{\parr*{\sum_{i\in[K]:i\rightarrow a}x^t_i}^2} &\stepa{\leq} \frac{\eta^2}{2}\sum_{t=1}^T\sum_{a=1}^Kx^t_a\frac{S\E\parq*{\sum_{i\in[K]:i\rightarrow a}v^t_i}}{\parr*{\sum_{i\in[K]:i\rightarrow a}x^t_i}^2}\\
&=\frac{\eta^2}{2}\sum_{t=1}^T\sum_{a=1}^Kx^t_a\frac{S\sum_{i\in[K]:i\rightarrow a}x^t_i}{\parr*{\sum_{i\in[K]:i\rightarrow a}x^t_i}^2}\\
&=\frac{\eta^2}{2}\sum_{t=1}^T\sum_{a=1}^K S\frac{x^t_a}{\sum_{i\in[K]:i\rightarrow a}x^t_i}\\
&\stepb{\leq} 2S\eta^2\alpha T\log\parr*{\frac{4K}{\alpha\epsilon}}
\end{align*}
where (a) is by $\|v^t\|_1\leq S$ and (b) uses Lemma~\ref{lem:alon_alpha}. Plugging this back to \eqref{eq:temp_end_eq1} in the proof of Theorem~\ref{thm:upper_bound_detailed} yields the desired bound. When the feedback graphs are time-varying, one gets instead $\widetilde{O}\parr*{S\sqrt{\sum_{t=1}^T\alpha_t}}$.



\section{Auxiliary lemmas}
\label{app:auxiliary}


For any directed graph $G=(V,E)$, one can find a dominating set by recursively picking the node with the largest out-degree (break tie arbitrarily) and removing its neighbors. The size of such dominating set is bounded by the following lemma:
\begin{lemma}[\cite{chvatal1979greedy}]
\label{lem:greedy_dom}
For any graph $G=(V,E)$, the above greedy procedure outputs a dominating set $D$ with
\begin{align*}
    |D| \leq (1+\log|V|)\delta(G).
\end{align*}
\end{lemma}


\begin{lemma}[Lemma 5 in \cite{alon2015online}]\label{lem:alon_alpha}
Let $G=([K], E)$ be a directed graph with $i\in \Nout(i)$ for all $i\in[K]$. Let $w_i$ be positive weights such that $w_i\geq \epsilon\sum_{i\in[K]}w_i$ for all $i\in[K]$ for some constant $\epsilon\in (0,\frac{1}{2})$. Then
\[
\sum_{i\in[K]}\frac{w_i}{\sum_{j\in[K]:j\rightarrow i}w_j} \leq 4\alpha\log\parr*{\frac{4K}{\alpha\epsilon}}
\]
\end{lemma}

\begin{lemma}[Lemma 8 of \citep{alon2015online}]
\label{lem:alon15}
    For any directed graph $G=(V,E)$, one has $\delta(G)\leq 50\alpha(G)\log|V|$.
\end{lemma} 


\begin{lemma}\label{lem:minimizer_of_convex_func}
Let $F:X\rightarrow \R$ be a convex, differentiable function and $D\subset\R^d$ be an open convex subset. Let $x_* = \argmin_{x\in D}F(x)$. Then for any $y\in D$, $(y-x_*)^T\nabla F(x_*)\geq 0$.
\end{lemma}
\begin{proof}
We will prove by contradiction. Suppose there is $y\in D$ with $(y-x_*)^T\nabla F(x_*) < 0$. Let $z(t) = F(x_* + t(y-x_*))$ for $t\in [0,1]$ be the line segment from $F(x_*)$ to $F(y)$. We have
\[
z'(t) = (y-x_*)^T\nabla F(x_*+t(y-x_*))
\]
and hence $z'(0) = (y-x_*)^T\nabla F(x_*)<0$. Since $D$ is open and $F$ is continuous, there exists $t>0$ small enough such that $z(t)<z(0) = F(x_*)$, which yields a contradiction.
\end{proof}


\begin{lemma}[Chapter 11 in \cite{cesa2006prediction}]\label{lem:breg_div_dual}
Let $F$ be a Legendre function on open convex set $\Dcal\subseteq \R^d$. Then $F^{**}=F$ and $\nabla F^* = (\nabla F)^{-1}$. Also for any $x,y\in\Dcal$, 
\[
D_F(x,y) = D_{F^*}(\nabla F(y), \nabla F(x)).
\]
\end{lemma}


\begin{lemma}[Lemma 1 in \cite{han2024optimal}]\label{lem:sto_reward_concentration}
Fix any $\epsilon\in(0,1)$. With probability at least $1-\epsilon$, it holds that
\[
\abs*{\br_a^t - \mu_a} \leq 3\sqrt{\frac{\log(2T)\log(KT/\epsilon)}{n_a^t}}
\]
for all $a\in[K]$ and all $t\in[T]$.
\end{lemma}
\end{document}


