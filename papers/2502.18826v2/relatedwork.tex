\section{Related work}
The optimal regret of the combinatorial semi-bandits has drawn a lot of attention and has been extensively studied in the bandit literature. With linear payoff, \cite{koolen2010hedging} shows that the Online Stochastic Mirror Descent (OSMD) algorithm achieves near-optimal regret $\widetilde\Theta\parr{S\sqrt{T}}$ under full information. In the case of the semi-bandit feedback, \cite{audibert2014regret} shows that OSMD achieves near-optimal regret $\widetilde\Theta\parr{\sqrt{KST}}$ using an unbiased estimator $\Tilde{r}^t_a = v^t_ar^t_a / \E\parq{v^t_a}$, where $v^t$ is the random decision selected at time $t$ and the expectation denotes the probability of observing arm $a$. The transition of the optimal regret's dependence from $\sqrt{KS}$ to $S$, as the feedback becomes richer, remains a curious and important open problem. 

Another type of feedback assumes only the realized payoff $\langle v, r^t\rangle$ is revealed (rather than the rewards for individual arms). In this case, \cite{audibert2014regret} proves a minimax lower bound $\Omega\parr{S\sqrt{KT}}$ and an upper bound with a $\sqrt{S}$ gap. When the payoff function is nonlinear in $v$, \cite{han2021adversarial} shows that the optimal regret scales with $K^d$ where $d$ roughly stands for the complexity of the payoff function. More variants of combinatorial semi-bandits include the knapsack constraint \cite{sankararaman2018combinatorial} and fractional decisions \cite{pmlr-v37-wen15}.

In the multi-armed bandits, multiple attempts have been made to formulate and exploit the feedback structure as feedback graphs since \cite{mannor2011bandits}. In particular, the optimal regret is shown to be $\widetilde\Theta\parr{\sqrt{\alpha T}}$ when $T\geq \alpha^3$ \cite{alon2015online, eldowa2024minimax} and is a mixture of $T^{1/2}$ and $T^{2/3}$ terms when $T$ is small due to the exploration-exploitation trade-off \cite{kocak2023online}. When the presence of all self-loops is not guaranteed but all nodes $[K]$ are observable, i.e. has nonzero in-degree, an explore-then-commit algorithm achieves the optimal regret $\widetilde\Theta\parr*{\delta^{1/3}T^{2/3}}$ \cite{alon2015online}. Here $\alpha$ and $\delta$ are the independence and the domination number of the graph $G$ respectively, defined in Section~\ref{sec:notations}. 

Instead of a fixed graph $G$, \cite{cohen2016online, alon2017nonstochastic} study time-varying graphs $\{G_t\}$ and show that an upper bound $\widetilde{O}\parr*{\sqrt{T\sum_{t=1}^T\alpha_t}}$ can be achieved. Additionally, a recent line of research \cite{balseiro2023contextual, han2024optimal, wen2024stochastic} introduces graph feedback to the tabular contextual bandits, in which case the optimal regret depends on a complicated graph quantity that interpolates between $\alpha$ and $K$ as the number of contexts changes.