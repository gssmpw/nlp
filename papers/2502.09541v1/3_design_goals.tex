\section{\THISWORK\ Design Overview} \label{section:design_overview}

We design \THISWORK---a framework that alleviates the IO bottleneck for GPU-accelerated data analytics by tapping into the opportunity discussed in \S\ref{subsection:scaling_gpu_io_independently}.
\THISWORK\ fully engages IO resources on a multi-GPU system. 
%and showcases examples to understand application performance using this idea.
Crucially, \THISWORK\ uses a single GPU for computation, and multiple GPUs for data transfers; CPU is only used for orchestrating these data transfers.
The design of \THISWORK\ is divided into three layers as detailed below.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
We build \THISWORK, a proof-of-concept software stack, to justify our IO redistribution idea.
\THISWORK\ redistributes IO resources on multi-GPU systems to break the IO bottleneck of a single GPU and provides examples to understand the applications' performance characteristics when using this new scheme.  
\textcolor{blue}{\THISWORK\ only use GPU to do the real computation, and CPU is only used for scheduling}
\THISWORK\ includes three layers, each serving a different design goal.
It has an optimized IO primitive at the bottom, an IO-decoupled programming model in the middle, and example implementations of 3 important applications on the top.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

\noindent
\textbf{Optimized IO Primitive (\texttt{Exchange}) (\S\ref{sec:Exchange-IO-primitive})}
Because of non-ideal runtime and hardware behaviors (detailed in \S\ref{sec:io-challenges}), it is challenging to fully leverage the IO bandwidth provided by hardware.
In the presence of these behaviors, we design an efficient IO primitive \textit{\texttt{Exchange}} that allows a target GPU to utilize its neighboring GPUs' SDMA engines and PCIe bandwidth for data transfers.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
\textbf{Optimized IO-primitive, \texttt{Exchange} (\S~\ref{sec:Exchange-IO-primitive}). } 
It is non-trivial to carry out our novel IO redistribution idea using existing GPU runtime and hardware due to a bunch of nonidealities we will explain in \S~\ref{sec:io-challenges}.
\texttt{Exchange} allows a \textit{target GPU} to use its neighboring \textit{forwarding GPUs}' under-utilized SDMA engines and PCIe bandwidth to enhance its own IO throughput.  
With our special design, \texttt{Exchange} unleashes the full potential of the underlying hardware.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

\noindent
\textbf{IO-Decoupled Programming Model (\S\ref{sec:IO-decoupled-model}). }
Unlike prior works~\cite{triton-join, pump-up-volume, gowan-ipdpsw-2018, sioulas-icde-2019, rui-vldb2020} that integrate the design of IO primitives with compute kernels, we advocate for a fundamentally different approach.
We argue that such an integrated approach is challenging and hinders the reuse of GPU code. 
By leveraging our high-throughput \texttt{Exchange} primitive, we introduce an efficient programming model that decouples IO scheduling from on-GPU kernel design. 
This separation facilitates the reuse of existing code designed only for on-core GPU data processing. It also enables the independent design and exploration of next-generation IO primitives without concerns about the compute kernels.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
\textbf{IO-decoupled Programming Model (\S~\ref{sec:IO-decoupled-model}). }
Limited by the IO bandwidth, prior works~\cite{triton-join, pump-up-volume, gowan-ipdpsw-2018, sioulas-icde-2019, rui-vldb2020} have to co-design the IO patterns and compute kernels.
We argue that designing these two together is challenging, and prevents reusing existing GPU code.
Taking advantage of our high-throughput \texttt{Exchange} primitive, we propose an efficient programming model that separates the concern of IO scheduling and on-GPU kernel design and enables reusing code not designed for out-of-core GPU data processing.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%




% \textcolor{red}{One of the insight is that given the high bandwidth achieved by \texttt{Exchange}, there is no need to maintain some CPU-side helper functions to reorganize the data on CPU (like group data into bins or merge sorted array). 
% Every thing can be done by GPU, and CPU is only used as a scheduler.
% On the other hand, I also do not want to leave the reader an impression that our solution is not compatible with the hybrid one.}

\noindent
\textbf{Tailored Query Execution (\S\ref{sec:query-execution}).}
% {
% \color{blue}
With our IO primitive and programming model, we leverage state-of-the-art on-core libraries AMD rocPRIM~\cite{rocprim} and Crystal~\cite{crystal-sigmod-20} to build a set of high-performance out-of-core query operators.
Although we only showcase how to apply our techniques to these libraries, the idea is generally applicable to other execution libraries.

\THISWORK's programming model abstracts away the IO orchestration, but query operators still need to specify their data partitioning strategy for out-of-core processing.
For certain operators, including selection, projection, and join with only one table larger than GPU memory, we can
trivially divide the out-of-core table into multiple chunks and then stream the data onto the GPU.

We illustrate how we implement query operators using the existing on-core GPU libraries with two complex operators that require tailored data partitioning strategy: sort (\S\ref{sec:design-sort}) and hash join with both tables larger than the GPU memory (\S\ref{sec:design-join}).
In \S\ref{sec:design-ssb}, we also introduce a late materialization technique utilizing zero-copy memory accesses as a complementary optimization.
% }

% We now discuss \THISWORK's details in the following sections.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
\textbf{Example Applications. }
While our programming model relieves the burden of orchestrating IO operations manually and allows the reuse of existing code, each application still needs to define how to partition the data and glue simple GPU kernels together to accomplish more complicated tasks.
Under our programming model, we design sort (\S~\ref{sec:design-sort}) and hash join (\S~\ref{sec:design-join}) as examples.
We implement our sort only with existing GPU libraries and hash join with both existing GPU code and our customized kernels.
Finally, we explain how to perform out-of-core query processing on star schema with our framework and discuss how to use zero-copy memory accesses to further improve query processing performance (\S~\ref{sec:design-ssb}).
All applications achieve state-of-the-art performance, and help us understand the difference brought by our IO redistribution idea.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

% Finally, we explain how to adapt the on-GPU query processing library, Crystal~\cite{crystal-sigmod-20}, to handle out-of-core query processing (\S~\ref{sec:design-ssb}), and discuss how to use zero-copy together with \texttt{Exchange} to improve query processing performance further.


