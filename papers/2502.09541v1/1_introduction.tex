\section{Introduction}

GPUs, with their massively parallel architecture, offer high computational power and memory throughput, making them an attractive choice for accelerating large-scale data analytics. 
However, a significant limitation is the memory capacity of GPUs, which is typically constrained to tens or low hundreds of gigabytes in modern hardware. 
In contrast, CPU memory has reached capacities of multiple terabytes. 
Transferring data from CPU memory to GPU memory is often bottlenecked by the limited bandwidth of interconnect links such as PCIe. 
This combination of capacity-limited GPU memory and bandwidth-limited CPU-GPU communication significantly restricts the acceleration potential of GPUs, as real-world data analytics workloads often require processing large datasets that by far exceed the GPU memory capacity.

Prior works addressing this issue can be broadly categorized into two approaches: (1) utilizing multiple GPUs~\cite{heavyai, mg-join-sigmod-2021, multi-gpu-sort-sigmod-2022}, and (2) leveraging CPU-side memory for data processing~\cite{hetexchange-vldb-2019, HERO-VLDB-2017, GDB-TDBSys-2009, mordered-vldb-2022, Ocelot-VLDB-2014, Ocelet-VLDB-2013}. 
The first approach employs multiple GPUs to horizontally scale memory capacity. 
However, this also scales computational resources, maintaining a constant compute-to-memory ratio, and may lead to under-utilization of expensive GPU resources. 
In the second category, some works stream data from CPU memory to GPU~\cite{hetexchange-vldb-2019, HERO-VLDB-2017, GDB-TDBSys-2009}, but still face the bandwidth limitations of PCIe links. 
Additionally, the CPU-GPU hybrid execution approach~\cite{mordered-vldb-2022, Ocelot-VLDB-2014, Ocelet-VLDB-2013} needs to deal with the large disparity in computational power between CPUs and GPUs, risking leaving GPU resources idle when CPU falls on the critical path.
This raises a critical question: \textit{how can we exploit the massively parallel GPU architecture to accelerate data analytics workloads that exceed GPU memory capacity?}

In this paper, we present \THISWORK—a GPU-accelerated framework for large-scale data analytics that addresses memory capacity limitations from a fresh perspective. 
\THISWORK\ has two crucial design goals. 
First, it processes workload sizes that significantly exceed GPU memory capacity. 
\textit{It assumes no data caching in GPU memory before the execution of any query.}
Second, \THISWORK\ is designed to have IO scheduling independent of GPU kernel optimization, alleviating the burden on the GPU programmer.
This unique programming model also maximizes the reuse of optimized GPU code.

The design of \THISWORK\ is structured into three layers. 
With the evolution of multi-GPU systems, we identify a unique opportunity to leverage the IO resources of all GPUs on such systems to transfer data to a single GPU executing data analytics. 
Based on this, at a bottom design layer, we propose an optimized IO primitive that fully exploits the bandwidth available from all PCIe links and inter-GPU communication fabric to transfer data from CPU memory to the GPU at high speeds. 
{\color{black}
This design is motivated by the observation that today's data processing platforms serve both data analytics and AI workloads to support intelligent workflows and diverse needs from users. 
AI workloads tend to be compute-bound, leaving their IO resources underutilized. 
We aim to co-locate compute-bound workloads and data analytics on the same multi-GPU server to process hybrid requests from users.
Our primitive repurposes these idle IO resources from other GPUs to forward data to the target GPU that handles IO-intensive data analytics.

% many workloads running on GPUs, such as AI inferences, 

}
% This design is motivated by the observation that many workloads running on GPUs, such as AI inferences, tend to be compute-bound, leaving their IO resources underutilized. 
% We aim to co-locate these workloads and data analytics on the same multi-tenant multi-GPU system.
% Our primitive repurposes these idle IO resources from other GPUs to forward data to the target GPU that handles IO-intensive data analytics.

We further present the design of an IO-decoupled programming model that clearly separates the development of GPU kernels from IO scheduling at a middle design layer.
In this model, programmers write optimized GPU kernels under the assumption that data is readily available, even for workloads that far exceed GPU memory capacity. 
Our primitive handles IO scheduling and orchestration independently.
This approach significantly simplifies the complex process of GPU code development, promotes the reuse of existing code, and allows for independent exploration of optimization strategies for both kernel execution and IO scheduling. 
Finally, at a top design layer, we implement a set of tailored query operators utilizing our IO primitive and programming model. 
We demonstrate data partitioning and compute orchestration strategies through case studies of sort and hash join, and discuss late materialization based on GPU's zero-copy memory access technique, which further improves query performance.

To demonstrate the effectiveness of \THISWORK, we use AMD's multi-GPU system with 4 GPUs connected to a single CPU socket; however, our techniques are vendor-agnostic. 
We conduct two sets of experiments: (1) a single GPU running data analytics with the other GPUs idle to uncover maximum performance gains, and (2) a single GPU running data analytics while the other three GPUs handle various AI inference tasks to assess the impact on AI workloads when their IO resources are engaged. 
On the end-to-end Star Schema Benchmark~\cite{star-schema-sigmod-02} with a scaling factor of 1000, the first set of experiments shows that \THISWORK\ outperforms the CPU-based DuckDB by 3.4$\times$ and GPU-based Proteus by 5.7$\times$ on average. 
In the second set of experiments, our IO primitive causes only a marginal slowdown of 6.8\% on average for AI workloads. 
Furthermore, we compare the price performance of \THISWORK\ with a CPU-only DuckDB solution that shows benefits ranging from 1.5$\times$--4.2$\times$, depending on the workload.
The key contributions of \THISWORK\ are summarized below.


\begin{itemize}[leftmargin=*]
    \item An optimized IO primitive that utilizes untapped PCIe bandwidth from multiple GPUs for high-speed CPU-GPU data transfer.
    \item An IO-decoupled programming model that separates GPU kernel development from IO scheduling, promoting GPU code reuse.
    \item 
    Tailored query operator implementations with efficient data partitioning and compute orchestration, as well as a late materialization technique based on GPU's zero-copy memory access that further boosts the query performance.
    %Tailored query execution that implements data partitioning and compute orchestration for efficient execution, as well as a late materialization technique based on zero-copy memory access that further boosts the query performance.
    \item \THISWORK—an end-to-end GPU-accelerated framework that enhances the performance and price-performance of DuckDB by 3.4$\times$ and 2.6$\times$, respectively.
\end{itemize}



%%%%%%%%%%%%% OLD TEXT
\begin{comment}
The high compute throughput and memory bandwidth provided by GPUs have drawn people's attention to accelerating data analytics with GPUs in the past decade.
While prior GPU-accelerated data analytic systems demonstrate significant speedup and better cost efficiency over CPU-based systems, the small GPU memory capacity remains a considerable obstacle to their wide adoption.
Despite that the CPU side can be equipped with a large amount of memory, they are shadowed by the narrow CPU-GPU interconnects.
The memory capacity limit poses challenges for efficiently processing large datasets, which are common in today's data-driven world.


While prior works focus on solving this issue by scaling the number of GPUs for larger memory or introducing a CPU-GPU hybrid execution strategy, we take a fundamentally different approach that enables a GPU to access CPU-side memory at high throughput by redistributing IO resources in multi-tenant multi-GPU systems. 
Today's GPUs are heavily demanded by all kinds of deep learning (DL) workloads, which are compute-intensive and under-utilize their CPU-GPU interconnects.
With the prevalence of multi-GPU systems, we notice that when GPU-accelerated analytics is collocated with compute-intensive workloads on different GPUs in the same machine, there is a unique opportunity to recycle the idle IO resources for the GPU running data analytics.
Specifically, we route the traffic of a target GPU through its neighboring GPUs whose IO resources are under-utilized and thus support its off-device access with all the CPU-GPU links in a multi-GPU system.
Such a strategy allows a GPU to access the CPU-side DRAM with throughput comparable to the DRAM bandwidth limit and directly process large datasets in DRAM with high efficiency, challenging the traditional knowledge that IO is a fundamental bottleneck for GPU-based data analytics to use CPU-side memory.

In summary, this work makes the following novel contribution:
\begin{itemize}[leftmargin=*]
    \item To our best knowledge, we are the first to take advantage of idle IO resources in multi-GPU systems to enable efficient GPU-based query processing on large datasets.
    \item \texttt{Exchange}, an optimized IO primitive that saturates all the PCIe links to enhance the IO performance of a GPU, makes it possible for the GPU to effectively utilize CPU memory.
    \item An IO-decoupled programming model that leverages the high IO throughput provided by \texttt{Exchange} to extend existing GPU kernels to process data on CPU, separating the concern of IO management and on-GPU computation.
    \item I do not have good idea now to summarize the section 6.
    \item \THISWORK\ , a XXX (I think probably we need to consider how to position the code we have...)
\end{itemize}

\subsection{Assumptions \& targets}

In another way: Io is long been known as a bottleneck, particularly for co-processor model GPU DBs.
We are proposing a technique that is generally applicable to ...

\begin{enumerate}
    \item cold start. E.g. the user can execute a single query on demand (\textbf{pay the cost to GPUs at query-level granularity.})
    E.g. the DRAM on CPU caches the hot data for a DB which is much larger than GPU memory size. But our solution is generic enough to also speedup other cases, this is more related to our evaluation setup.
    \item Avoid using CPU (except for scheduling) and treat the CPU-DRAM as a passive device.
    To understand the limit of GPU based DB.
    Today's computer architecture is more disaggregated then before, save money on the CPU side.

\end{enumerate}
An imaginary scenario in the future is GPUs are connected to a disaggregated memory pool, and a user enqueues an OLAP query to the system. 
The system then enqueues the operation to a GPU's schedule (it can currently run some other tasks like machine learning. It is likely as ML is the god today).
When the query is at the head of the queue, the result will be returned.
In this case, we only use GPU for a tiny time slice that is actually needed by the query.

\end{comment}