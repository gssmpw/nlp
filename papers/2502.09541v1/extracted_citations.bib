@article{FlinkCL,
author = {Chen, Cen and Li, Kenli and Ouyang, Aijia and Li, Keqin},
title = {FlinkCL: An OpenCL-Based In-Memory Computing Architecture on Heterogeneous CPU-GPU Clusters for Big Data},
year = {2018},
issue_date = {Dec. 2018},
publisher = {IEEE Computer Society},
address = {USA},
volume = {67},
number = {12},
issn = {0018-9340},
url = {https://doi.org/10.1109/TC.2018.2839719},
doi = {10.1109/TC.2018.2839719},
abstract = {Research on in-memory big data management and processing has been prompted by the increase in main memory capacity and the explosion in big data. By offering an efficient in-memory distributed execution model, existing in-memory cluster computing platforms such as Flink and Spark have been proven to be outstanding for processing big data. This paper proposes FlinkCL, an in-memory computing architecture on heterogeneous CPU-GPU clusters based on OpenCL that enables Flink to utilize GPU's massive parallel processing ability. Our proposed architecture utilizes four techniques: a heterogeneous distributed abstract model (HDST), a Just-In-Time (JIT) compiling schema, a hierarchical partial reduction (HPR) and a heterogeneous task management strategy. Using FlinkCL, programmers only need to write Java code with simple interfaces. The Java code can be compiled to OpenCL kernels and executed on CPUs and GPUs automatically. In the HDST, a novel memory mapping scheme is proposed to avoid serialization or deserialization between Java Virtual Machine (JVM) objects and OpenCL structs. We have comprehensively evaluated FlinkCL with a set of representative workloads to show its effectiveness. Our results show that FlinkCL improve the performance by up to <inline-formula><tex-math notation="LaTeX">$11 times$</tex-math><alternatives> <inline-graphic xlink:href="chen-ieq1-2839719.gif"/></alternatives></inline-formula> for some computationally heavy algorithms and maintains minor performance improvements for a I/O bound algorithm.},
journal = {IEEE Trans. Comput.},
month = dec,
pages = {1765–1779},
numpages = {15}
}

@inproceedings{Funke-sigmod18,
author = {Funke, Henning and Bre\ss{}, Sebastian and Noll, Stefan and Markl, Volker and Teubner, Jens},
title = {Pipelined Query Processing in Coprocessor Environments},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3183713.3183734},
doi = {10.1145/3183713.3183734},
abstract = {Query processing on GPU-style coprocessors is severely limited by the movement of data. With teraflops of compute throughput in one device, even high-bandwidth memory cannot provision enough data for a reasonable utilization.Query compilation is a proven technique to improve memory efficiency. However, its inherent tuple-at-a-time processing style does not suit the massively parallel execution model of GPU-style coprocessors. This compromises the improvements in efficiency offered by query compilation. In this paper, we show how query compilation and GPU-style parallelism can be made to play in unison nevertheless. We describe a compiler strategy that merges multiple operations into a single GPU kernel, thereby significantly reducing bandwidth demand. Compared to operator-at-a-time, we show reductions of memory access volumes by factors of up to 7.5x resulting in shorter kernel execution times by factors of up to 9.5x.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1603–1618},
numpages = {16},
keywords = {just-in-time query compilation, massively parallel query processing, olap, operator pipelining, query-coprocessing},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@article{GDB-TDBSys-2009,
author = {He, Bingsheng and Lu, Mian and Yang, Ke and Fang, Rui and Govindaraju, Naga K. and Luo, Qiong and Sander, Pedro V.},
title = {Relational query coprocessing on graphics processors},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {0362-5915},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/1620585.1620588},
doi = {10.1145/1620585.1620588},
abstract = {Graphics processors (GPUs) have recently emerged as powerful coprocessors for general purpose computation. Compared with commodity CPUs, GPUs have an order of magnitude higher computation power as well as memory bandwidth. Moreover, new-generation GPUs allow writes to random memory locations, provide efficient interprocessor communication through on-chip local memory, and support a general purpose parallel programming model. Nevertheless, many of the GPU features are specialized for graphics processing, including the massively multithreaded architecture, the Single-Instruction-Multiple-Data processing style, and the execution model of a single application at a time. Additionally, GPUs rely on a bus of limited bandwidth to transfer data to and from the CPU, do not allow dynamic memory allocation from GPU kernels, and have little hardware support for write conflicts. Therefore, a careful design and implementation is required to utilize the GPU for coprocessing database queries.In this article, we present our design, implementation, and evaluation of an in-memory relational query coprocessing system, GDB, on the GPU. Taking advantage of the GPU hardware features, we design a set of highly optimized data-parallel primitives such as split and sort, and use these primitives to implement common relational query processing algorithms. Our algorithms utilize the high parallelism as well as the high memory bandwidth of the GPU, and use parallel computation and memory optimizations to effectively reduce memory stalls. Furthermore, we propose coprocessing techniques that take into account both the computation resources and the GPU-CPU data transfer cost so that each operator in a query can utilize suitable processors—the CPU, the GPU, or both—for an optimized overall performance. We have evaluated our GDB system on a machine with an Intel quad-core CPU and an NVIDIA GeForce 8800 GTX GPU. Our workloads include microbenchmark queries on memory-resident data as well as TPC-H queries that involve complex data types and multiple query operators on data sets larger than the GPU memory. Our results show that our GPU-based algorithms are 2--27x faster than their optimized CPU-based counterparts on in-memory data. Moreover, the performance of our coprocessing scheme is similar to, or better than, both the GPU-only and the CPU-only schemes.},
journal = {ACM Trans. Database Syst.},
month = dec,
articleno = {21},
numpages = {39},
keywords = {Relational database, graphics processors, join, parallel processing, primitive, sort}
}

@inproceedings{GHive,
author = {Liu, Haotian and Tang, Bo and Zhang, Jiashu and Deng, Yangshen and Yan, Xiao and Zheng, Xinying and Shen, Qiaomu and Zeng, Dan and Mao, Zunyao and Zhang, Chaozu and You, Zhengxin and Wang, Zhihao and Jiang, Runzhe and Wang, Fang and Yiu, Man Lung and Li, Huan and Han, Mingji and Li, Qian and Luo, Zhenghai},
title = {GHive: accelerating analytical query processing in apache hive via CPU-GPU heterogeneous computing},
year = {2022},
isbn = {9781450394147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3542929.3563503},
doi = {10.1145/3542929.3563503},
abstract = {As a popular distributed data warehouse system, Apache Hive has been widely used for big data analytics in many organizations. Meanwhile, exploiting the massive parallelism of GPU to accelerate online analytical processing (OLAP) has been extensively explored in the database community. In this paper, we present GHive, which enhances CPU-based Hive via CPU-GPU heterogeneous computing. GHive is designed for the business intelligence applications and provides the same API as Hive for compatibility. To run SQL queries jointly on both CPU and GPU, GHive comes with three key techniques: (i) a novel data model gTable, which is column-based and enables efficient data movement between CPU memory and GPU memory; (ii) a GPU-based operator library Panda, which provides a complete set of SQL operators with extensively optimized GPU implementations; (iii) a hardware-aware MapReduce job placement scheme, which puts jobs judiciously on either GPU or CPU via a cost-based approach. In the experiments, we observe that GHive outperforms Hive in both query processing speed and operating expense on the Star Schema Benchmark (SSB).},
booktitle = {Proceedings of the 13th Symposium on Cloud Computing},
pages = {158–172},
numpages = {15},
location = {San Francisco, California},
series = {SoCC '22}
}

@inproceedings{GPUQP,
author = {Fang, Rui and He, Bingsheng and Lu, Mian and Yang, Ke and Govindaraju, Naga K. and Luo, Qiong and Sander, Pedro V.},
title = {GPUQP: query co-processing using graphics processors},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247606},
doi = {10.1145/1247480.1247606},
abstract = {We present GPUQP, a relational query engine that employs both CPUs and GPUs (Graphics Processing Units) for in-memory query co-processing. GPUs are commodity processors traditionally designed for graphics applications. Recent research has shown that they can accelerate some database operations orders of magnitude over CPUs. So far, there has been little work on how GPUs can be programmed for heavy-duty database constructs, such as tree indexes and joins, and how well a full-fledged GPU query co-processor performs in comparison with their CPU counterparts. In this work, we explore the design decisions in using GPUs for query co-processing using both a graphics API and a general purpose programming model. We then demonstrate the processing flows as well as the performance results of our methods.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1061–1063},
numpages = {3},
keywords = {query processing, graphics processing units},
location = {Beijing, China},
series = {SIGMOD '07}
}

@article{HERO-VLDB-2017,
author = {Karnagel, Tomas and Habich, Dirk and Lehner, Wolfgang},
title = {Adaptive work placement for query processing on heterogeneous computing resources},
year = {2017},
issue_date = {March 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {7},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/3067421.3067423},
doi = {10.14778/3067421.3067423},
abstract = {The hardware landscape is currently changing from homogeneous multi-core systems towards heterogeneous systems with many different computing units, each with their own characteristics. This trend is a great opportunity for data-base systems to increase the overall performance if the heterogeneous resources can be utilized efficiently. To achieve this, the main challenge is to place the right work on the right computing unit. Current approaches tackling this placement for query processing assume that data cardinalities of intermediate results can be correctly estimated. However, this assumption does not hold for complex queries. To overcome this problem, we propose an adaptive placement approach being independent of cardinality estimation of intermediate results. Our approach is incorporated in a novel adaptive placement sequence. Additionally, we implement our approach as an extensible virtualization layer, to demonstrate the broad applicability with multiple database systems. In our evaluation, we clearly show that our approach significantly improves OLAP query processing on heterogeneous hardware, while being adaptive enough to react to changing cardinalities of intermediate query results.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {733–744},
numpages = {12}
}

@article{Ocelet-VLDB-2013,
author = {Heimel, Max and Saecker, Michael and Pirk, Holger and Manegold, Stefan and Markl, Volker},
title = {Hardware-oblivious parallelism for in-memory column-stores},
year = {2013},
issue_date = {July 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {9},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/2536360.2536370},
doi = {10.14778/2536360.2536370},
abstract = {The multi-core architectures of today's computer systems make parallelism a necessity for performance critical applications. Writing such applications in a generic, hardware-oblivious manner is a challenging problem: Current database systems thus rely on labor-intensive and error-prone manual tuning to exploit the full potential of modern parallel hardware architectures like multi-core CPUs and graphics cards. We propose an alternative design for a parallel database engine, based on a single set of hardware-oblivious operators, which are compiled down to the actual hardware at runtime. This design reduces the development overhead for parallel database engines, while achieving competitive performance to hand-tuned systems.We provide a proof-of-concept for this design by integrating operators written using the parallel programming framework OpenCL into the open-source database MonetDB. Following this approach, we achieve efficient, yet highly portable parallel code without the need for optimization by hand. We evaluated our implementation against MonetDB using TPC-H derived queries and observed a performance that rivals that of MonetDB's query execution on the CPU and surpasses it on the GPU. In addition, we show that the same set of operators runs nearly unchanged on a GPU, demonstrating the feasibility of our approach.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {709–720},
numpages = {12}
}

@article{Ocelot-VLDB-2014,
author = {Bre\ss{}, Sebastian and K\"{o}cher, Bastian and Heimel, Max and Markl, Volker and Saecker, Michael and Saake, Gunter},
title = {Ocelot/HyPE: optimized data processing on heterogeneous hardware},
year = {2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/2733004.2733042},
doi = {10.14778/2733004.2733042},
abstract = {The past years saw the emergence of highly heterogeneous server architectures that feature multiple accelerators in addition to the main processor. Efficiently exploiting these systems for data processing is a challenging research problem that comprises many facets, including how to find an optimal operator placement strategy, how to estimate runtime costs across different hardware architectures, and how to manage the code and maintenance blowup caused by having to support multiple architectures.In prior work, we already discussed solutions to some of these problems: First, we showed that specifying operators in a hardware-oblivious way can prevent code blowup while still maintaining competitive performance when supporting multiple architectures. Second, we presented learning cost functions and several heuristics to efficiently place operators across all available devices.In this demonstration, we provide further insights into this line of work by presenting our combined system Ocelot/HyPE. Our system integrates a hardware-oblivious data processing engine with a learning query optimizer for placement decisions, resulting in a highly adaptive DBMS that is specifically tailored towards heterogeneous hardware environments.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1609–1612},
numpages = {4}
}

@article{cao2024vldb,
author = {Cao, Jiashen and Sen, Rathijit and Interlandi, Matteo and Arulraj, Joy and Kim, Hyesoon},
title = {GPU Database Systems Characterization and Optimization},
year = {2023},
issue_date = {November 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3632093.3632107},
doi = {10.14778/3632093.3632107},
abstract = {GPUs offer massive parallelism and high-bandwidth memory access, making them an attractive option for accelerating data analytics in database systems. However, while modern GPUs possess more resources than ever before (e.g., higher DRAM bandwidth), efficient system implementations and judicious resource allocations for query processing are still necessary for optimal performance. Database systems can save GPU runtime costs through just-enough resource allocation or improve query throughput with concurrent query processing by leveraging new GPU resource-allocation capabilities, such as Multi-Instance GPU (MIG).In this paper, we do a cross-stack performance and resource-utilization analysis of four GPU database systems, including Crystal (the state-of-the-art GPU database, performance-wise) and TQP (the latest entry in the GPU database space). We evaluate the bottlenecks of each system through an in-depth microarchitectural study and identify resource underutilization by leveraging the classic roofline model. Based on the insights gained from our investigation, we propose optimizations for both system implementation and resource allocation, using which we are able to achieve 1.9x lower latency for single-query execution and up to 6.5x throughput improvement for concurrent query execution.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {441–454},
numpages = {14}
}

@inproceedings{crystal-sigmod-20,
author = {Shanbhag, Anil and Madden, Samuel and Yu, Xiangyao},
title = {A Study of the Fundamental Performance Characteristics of GPUs and CPUs for Database Analytics},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3318464.3380595},
doi = {10.1145/3318464.3380595},
abstract = {There has been significant amount of excitement and recent work on GPU-based database systems. Previous work has claimed that these systems can perform orders of magnitude better than CPU-based database systems on analytical workloads such as those found in decision support and business intelligence applications. A hardware expert would view these claims with suspicion. Given the general notion that database operators are memory-bandwidth bound, one would expect the maximum gain to be roughly equal to the ratio of the memory bandwidth of GPU to that of CPU. In this paper, we adopt a model-based approach to understand when and why the performance gains of running queries on GPUs vs on CPUs vary from the bandwidth ratio (which is roughly 16\texttimes{} on modern hardware). We propose Crystal, a library of parallel routines that can be combined together to run full SQL queries on a GPU with minimal materialization overhead. We implement individual query operators to show that while the speedups for selection, projection, and sorts are near the bandwidth ratio, joins achieve less speedup due to differences in hardware capabilities. Interestingly, we show on a popular analytical workload that full query performance gain from running on GPU exceeds the bandwidth ratio despite individual operators having speedup less than bandwidth ratio, as a result of limitations of vectorizing chained operators on CPUs, resulting in a 25\texttimes{} speedup for GPUs over CPUs on the benchmark.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1617–1632},
numpages = {16},
keywords = {in-memory analytics, heterogenous systems, GPU query performance, GPU data analytics},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@misc{heavyai,
    author = {HEAVY.AI},
    year = 2024,
    title = {HEAVY.AI Documentation},
    url = {https://docs.heavy.ai/installation-and-configuration/system-requirements/hardware},
    note = {Accessed: 07/24/2024}
}

@article{hetexchange-vldb-2019,
author = {Chrysogelos, Periklis and Karpathiotakis, Manos and Appuswamy, Raja and Ailamaki, Anastasia},
title = {HetExchange: encapsulating heterogeneous CPU-GPU parallelism in JIT compiled engines},
year = {2019},
publisher = {VLDB Endowment},
volume = {12},
number = {5},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/3303753.3303760},
doi = {10.14778/3303753.3303760},
abstract = {Modern server hardware is increasingly heterogeneous as hardware accelerators, such as GPUs, are used together with multicore CPUs to meet the computational demands of modern data analytics work-loads. Unfortunately, query parallelization techniques used by analytical database engines are designed for homogeneous multicore servers, where query plans are parallelized across CPUs to process data stored in cache coherent shared memory. Thus, these techniques are unable to fully exploit available heterogeneous hardware, where one needs to exploit task-parallelism of CPUs and data-parallelism of GPUs for processing data stored in a deep, non-cache-coherent memory hierarchy with widely varying access latencies and bandwidth.In this paper, we introduce HetExchange-a parallel query execution framework that encapsulates the heterogeneous parallelism of modern multi-CPU-multi-GPU servers and enables the parallelization of (pre-)existing sequential relational operators. In contrast to the interpreted nature of traditional Exchange, HetExchange is designed to be used in conjunction with JIT compiled engines in order to allow a tight integration with the proposed operators and generation of efficient code for heterogeneous hardware. We validate the applicability and efficiency of our design by building a prototype that can operate over both CPUs and GPUs, and enables its operators to be parallelism- and data-location-agnostic. In doing so, we show that efficiently exploiting CPU-GPU parallelism can provide 2.8x and 6.4x improvement in performance compared to state-of-the-art CPU-based and GPU-based DBMS.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {544–556},
numpages = {13}
}

@inproceedings{mg-join-sigmod-2021,
author = {Paul, Johns and Lu, Shengliang and He, Bingsheng and Lau, Chiew Tong},
title = {MG-Join: A Scalable Join for Massively Parallel Multi-GPU Architectures},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3448016.3457254},
doi = {10.1145/3448016.3457254},
abstract = {The recent scale-up of GPU hardware through the integration of multiple GPUs into a single machine and the introduction of higher bandwidth interconnects like NVLink 2.0 has enabled new opportunities of relational query processing on multiple GPUs. However, due to the unique characteristics of GPUs and the interconnects, existing hash join implementations spend up to 66\% of their execution time moving the data between the GPUs and achieve lower than 50\% utilization of the newer high bandwidth interconnects. This leads to extremely poor scalablity of hash join performance on multiple GPUs, which can be slower than the performance on a single GPU. In this paper, we propose MG-Join, a scalable partitioned hash join implementation on multiple GPUs of a single machine. In order to effectively improve the bandwidth utilization, we develop a novel multi-hop routing for cross-GPU communication that adaptively chooses the efficient route for each data flow to minimize congestion. Our experiments on the DGX-1 machine show that MG-Join helps significantly reduce the communication overhead and achieves up to 97\% utilization of the bisection bandwidth of the interconnects, resulting in significantly better scalability. Overall, MG-Join outperforms the state-of-the-art hash join implementations by up to 2.5x. MG-Join further helps improve the overall performance of TPC-H queries by up to 4.5x over multi-GPU version of an open-source commercial GPU database Omnisci.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1413–1425},
numpages = {13},
keywords = {distributed join, hash join, multi-gpu architectures, network utilization},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{mordered-vldb-2022,
author = {Yogatama, Bobbi W. and Gong, Weiwei and Yu, Xiangyao},
title = {Orchestrating Data Placement and Query Execution in Heterogeneous CPU-GPU DBMS},
year = {2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3551793.3551809},
doi = {10.14778/3551793.3551809},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2491–2503},
numpages = {13}
}

@inproceedings{multi-gpu-sort-sigmod-2022,
author = {Maltenberger, Tobias and Ilic, Ivan and Tolovski, Ilin and Rabl, Tilmann},
title = {Evaluating Multi-GPU Sorting with Modern Interconnects},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3514221.3517842},
doi = {10.1145/3514221.3517842},
abstract = {GPUs have become a mainstream accelerator for database operations such as sorting. Most GPU sorting algorithms are single-GPU approaches. They neither harness the full computational power nor exploit the high-bandwidth P2P interconnects of modern multi-GPU platforms. The latest NVLink 2.0 and NVLink 3.0-based NVSwitch interconnects promise unparalleled multi-GPU acceleration. So far, multi-GPU sorting has only been evaluated on systems with PCIe 3.0. In this paper, we analyze serial, parallel, and bidirectional data transfer rates to, from, and between multiple GPUs on systems with PCIe 3.0/4.0, NVLink 2.0/3.0, and NVSwitch. We measure up to 35x higher parallel P2P throughput with NVLink 3.0-based NVSwitch over PCIe 3.0. To study GPU-accelerated sorting on today's hardware, we implement a P2P-based GPU-only (P2P sort) and a heterogeneous (HET sort) multi-GPU sorting algorithm and evaluate them on three modern platforms. We observe speedups over state-of-the-art parallel CPU radix sort of up to 14x for P2P sort and 9x for HET sort. On systems with fast P2P interconnects, P2P sort outperforms HET sort up to 1.65x. Finally, we show that overlapping GPU copy/compute operations does not mitigate the transfer bottleneck when sorting large out-of-core data.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1795–1809},
numpages = {15},
keywords = {multi-GPU sorting, high-speed interconnects, database acceleration},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{robroek2024euromlsys,
author = {Robroek, Ties and Yousefzadeh-Asl-Miandoab, Ehsan and T\"{o}z\"{u}n, P\i{}nar},
title = {An Analysis of Collocation on GPUs for Deep Learning Training},
year = {2024},
isbn = {9798400705410},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3642970.3655827},
doi = {10.1145/3642970.3655827},
abstract = {Deep learning training is an expensive process that extensively uses GPUs. However, not all model training saturates modern powerful GPUs. To create guidelines for such cases, this paper examines the performance of the different collocation methods available on NVIDIA GPUs: na\"{\i}vely submitting multiple processes on the same GPU using multiple streams, utilizing Multi-Process Service (MPS), and enabling the Multi-Instance GPU (MIG). Our results demonstrate that collocating multiple model training runs yields significant benefits, leading to up to three times training throughput despite increased epoch time. On the other hand, the aggregate memory footprint and compute needs of the models trained in parallel must fit the available memory and compute resources of the GPU. MIG can be beneficial thanks to its interference-free partitioning but can suffer from sub-optimal GPU utilization with dynamic or mixed workloads. In general, we recommend MPS as the best-performing and most flexible form of collocation for a single user submitting training jobs.},
booktitle = {Proceedings of the 4th Workshop on Machine Learning and Systems},
pages = {81–90},
numpages = {10},
keywords = {MIG, collocation on GPUs, resource-aware deep learning},
location = {Athens, Greece},
series = {EuroMLSys '24}
}

