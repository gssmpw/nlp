\section{Evaluation Results} \label{section:restuls}

\subsection{Interference-Free Analysis}
\noindent
\textbf{Performance of the \texttt{Exchange} primitive.}
Figure~\ref{fig:io-bandwidth} illustrates a comparison of the IO throughput achieved by our optimized \texttt{Exchange} and the baseline solution, which solely relies on the GPU runtime. 
We vary the total amount of data transferred from 2GB to 16GB and adjust the packet size from 10MB to 80MB. 
The combination of data size and packet size determines the total number of packets, which in turn affects the number of pipeline stages required for data transfer. 
Too few pipeline stages can lead to significant overhead in the prologue and epilogue phases of the pipeline. 
Conversely, utilizing excessively small packets is also inefficient, as each memory copy incurs a fixed overhead from the runtime, regardless of the transferred data volume. 
Therefore, small packet sizes exacerbate this overhead, making it disproportionately large.

Our solution achieves up to 140GB/s throughput when transferring 8GB or more of data. 
When the total amount of data transferred is small, we observe a decrease in throughput due to the reduced number of pipeline stages. 
As previously explained, this issue cannot be alleviated by simply reducing the packet size. 
For instance, while a packet size of 10MB provides better performance compared to an 80MB packet size when transferring 2GB of data, it delivers lower throughput when the data size exceeds 8GB. 
Empirically, we find that a packet size of 20MB strikes a balance, achieving desirable performance for small and large data transfers.
Consequently, we use a packet size of 20MB for all the applications evaluated below.

Compared to the baseline, which fully relies on the GPU runtime, our solution is not only more performant but also more stable. 
Such a baseline fails to fully exploit the full-duplex capabilities of PCIe links, achieving only about 110-130GB/s throughput when transferring data bidirectionally. 
Additionally, its performance is highly unstable due to the irregular PCIe bandwidth, especially when the CPU DRAM bandwidth becomes saturated.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}

Figure \ref{fig:io-bandwidth} compares the IO throughput achieved by our optimized \texttt{Exchange} with the one achieved by the baseline solution only relying on the GPU runtime.
We vary the total amount of data transferred from 2GB to 16GB, and the packet size from 10MB to 80MB.
The amount of data and packet size determines the total number of packets, which consequently determines the number of pipeline stages for the data transfer.
If there are too few pipeline stages, the overhead in the prologue and epilogue of the pipeline becomes considerable.
On the other hand, using tiny packets is also unacceptable.
Each memory copy pays a fixed overhead for the runtime regardless of the amount of data being transferred.
Tiny packets make such overhead significant.

Our solution achieves up to around 140GB/s throughput when transferring 8GB or more data.
When the total amount of data transferred is less, we observe decreased throughput due to fewer pipeline stages.
As explained, this can not be relieved by reducing the packet size.
While the case of 10 MB packet size achieves a better performance than the case of 80MB packet size when the total amount of data being transferred is 2GB, it delivers less throughput when the data size is larger than 8GB.
Empirically, we find 20MB is a sweet point that achieves desirable performance in transferring small and large amounts of data.
Thus, we use 20MB for all the applications evaluated below.

Compared to the baseline that fully depends on the GPU runtime, our solution is not only more performant but also more stable.
The baseline solution fails to take advantage of the full-duplex property of PCIe links properly, thus it only achieves around 110-130GB/s throughput when transferring the traffic in both directions. 
Besides, its performance is highly unstable due to the irregular PCIe bandwidth when the CPU DRAM bandwidth is saturating.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/sort-result.pdf}
%     \caption{Results for Sort. (a) the throughput achieved by different solutions, (b) the time breakdown for the \THISWORK\ sort, and (c) the time taken by on-GPU kernel execution of a typical pipeline stage.}
%     \label{fig:sort-perf}
% \end{figure}

\noindent
\textbf{Performance of Sort.}
We compare our sort implementation with CPU and GPU baselines in Figure~\ref{fig:sort-perf}(a). 
Our implementation achieves a throughput of 2.7 billion elements per second, which is 27.9$\times$ faster than TBB, 6.3$\times$ faster than PARADIS, and 1.7$\times$ faster than the configuration using only one GPU's IO resources. 
Figure~\ref{fig:sort-perf}(b) provides a breakdown of the sort operation, revealing that 65.1\% of the time is consumed by the \texttt{SortExKernel}. 
This occurs because, after enhancing the IO throughput, the sorting operation becomes bounded by the GPU processing throughput, as illustrated in Figure~\ref{fig:sort-perf}(c). 
While it takes the GPU approximately 208ms to sort a partition of 500 million 8-byte integers, transferring that partition to the GPU using four GPUs' IO resources requires only about 113ms. 
This limitation explains why we do not achieve nearly a 4$\times$ speedup compared to the single GPU IO solution. 
Conversely, the \texttt{MergeExKernel} remains IO-bound, with the on-GPU kernel completing in approximately 67ms.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
We compare our sort implementation with the CPU and GPU baselines in Figure~\ref{fig:sort-perf}(a).
Our sort implementation achieves 2.67B elements per second throughput, which is 27.9$\times$ compared to TBB, 6.3$\times$ compared to PARADIS, and 1.7$\times$ compared to the case using only one GPU's IO.
Figure~\ref{fig:sort-perf}(b) is the time breakdown of the sort operation, where 65.1\% of time is spent on the \texttt{SortExOperation}.
The reason is that after we enhance the IO throughput, sorting the array by partition is bounded by GPU-processing throughput, which is showcased in Figure~\ref{fig:sort-perf}(c).
It takes the GPU ~208ms to sort a partition of 500M 8-byte integers, but only ~113ms to transfer that partition to GPU using 4 GBUs' IO resources.
This is why we do not achieve close to 4$\times$ speedup compared to the single GPU IO solution.
On the other hand, \texttt{MergeExOperation} is still IO-bound, which finishes the on-GPU kernel in ~67ms.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/join-result.pdf}
%     \caption{Results for Hash Join. (a) the throughput achieved by different solutions, (b) the time breakdown for the \THISWORK\ hash join, and (c) the time taken by on-GPU kernel execution of a typical pipeline stage.}
%     \label{fig:hash-join-perf}
% \end{figure}

\begin{figure*}[t]
\centerline{\includegraphics[width=\linewidth]{figures/ssb-result.pdf}}
\caption{Star Schema Benchmark execution time and speedup.}
\label{fig:ssb-perf}
\end{figure*}
\begin{figure}
    \centering
    \includegraphics[width=0.86\linewidth]{figures/interference.pdf}
    \caption{Interference between \THISWORK\ on the target GPU and the deep learning applications on the forwarding GPUs. 
    % (a) The slowdown for the deep learning applications (x-axis) when the IO traffic (y-axis) runs in the background. 
    % (b) The slowdown for the \THISWORK\ applications (y-axis) when the deep learning applications (x-axis) run in the background.
    }
    \label{fig:interference}
\end{figure}
\noindent
\textbf{Performance of Hash Join.}
In contrast to sorting, hash join remains an IO-bound kernel even with our IO optimization technique. 
As shown in Figure~\ref{fig:sort-perf}(d), our solution achieves a throughput of 2.3 billion tuples per second. 
This is 24.1$\times$ faster than DuckDB, 2.4$\times$ faster than Triton Join (CPU), 1.3$\times$ faster than the CPU-GPU-NVLink-based Triton Join (GPU), and 3.2$\times$ faster than the single GPU solution using a standard PCIe link.
The speedup over the single GPU IO solution is more pronounced because all phases of hash join are IO-bound. 
This is evident in Figure~\ref{fig:sort-perf}(f). 
The \texttt{HashJoinExKer} requires only 34ms to complete the on-GPU join kernel, which is significantly less than the 61ms required for data transfer.
Similarly, it takes 90ms to partition a chunk of data, which is transferred in around 113ms. 
All phases scale uniformly with the improvement of IO throughput, as depicted in the time breakdown in Figure~\ref{fig:sort-perf}(e), where they consume a comparable amount of time. 
Notably, \THISWORK\ outperforms Triton Join without using proprietary CPU-GPU interconnects by exploiting untapped PCIe bandwidth.
% Notably, while surpassing the performance of Triton Join, our solution relies solely on commodity PCIe links, without utilizing any proprietary CPU-GPU connections to enhance IO throughput.


%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
Unlike sort, hash join is still an IO-bound kernel even with our IO-redistribution technique.
As shown in Figure~\ref{fig:hash-join-perf}(a), our solution achieves 2.3 billion tuples per second throughput.
This is around 24.1$\times$ over DuckDB, 2.4$\times$ over the CPU implementation of Triton Join, 1.3$\times$ over the CPU-GPU-NVlink based GPU Triton Join~\cite{triton-join}, and 3.2$\times$ over the single GPU solution with a common PCIe link.
The speedup over the single GPU IO solution is more significant because all hash join phases are IO-bound.
This can be observed from Figure~\ref{fig:hash-join-perf}(c).
The \texttt{HashJoinExOp} takes only ~34ms to finish the on-GPU join kernel, which is much lower than the ~61ms data transfer time.
Similarly, it only takes ~90ms to partition a chunk of data transferred in ~113ms.
All phases scale uniformly with the improvement of IO throughput, thus the time breakdown in Figure~\ref{fig:hash-join-perf}(b) shows that they take a similar amount of time.
While achieving better results than Triton Join, we do not use any proprietary CPU-GPU links to improve the IO through, but solely based on commodity PCIe links.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

\noindent
\textbf{Performance of SSB queries.}
Figure~\ref{fig:ssb-perf} illustrates the comparison of SSB query performance between our solution and the baseline approaches.
On average, our solution achieves a 3.4$\times$ speedup over DuckDB, with all data dynamically fetched from CPU DRAM.
When examining individual query flights, the speedup is 2.4$\times$ for Q1.*, 3.6$\times$ for Q2.*, 3.9$\times$ for Q3.*, and 3.7$\times$ for Q4.*. 
The higher speedup observed in Q2.*, Q3.*, and Q4.* is attributed to their inclusion of more complex multi-way joins.
The more complex multi-way join demands higher memory throughput for hash table probing, thus favoring GPU-based solutions more as they can operate in high-bandwidth GPU memory.
The CPU-based solution has to use the limited DRAM bandwidth on hash table probing and fact table reading, while our solution only uses DRAM bandwidth for the latter.
Lightweight queries like Q11 only filter the fact table based on some predicates, whose only DRAM traffic is reading the fact table once.
Thus, the benefit of high-bandwidth GPU memory is minimized, and we observe less speedup. 


By comparing the bars of \texttt{navie} and \texttt{Proteus-GPU} with \texttt{DuckDB}, it becomes evident that GPU-based solutions struggle to achieve performance comparable to the CPU-based DuckDB without utilizing our IO optimization technique. 
However, this technique alone is insufficient, as indicated by the comparison between the \THISWORK\ and \texttt{DuckDB} bars. 
It only achieves a 1.6$\times$ speedup against \texttt{DuckDB} because it transfers unused data to the GPU without considering column selectivity. 
While zero-copy can exploit selectivity, it falls short of maximizing throughput because it relies on a single PCIe link. 
Notably, using zero-copy alone results in worse performance than \THISWORK\ .
Our final solution dynamically switches between SDMA-based data transfer for columns with selectivity greater than a threshold \(TH\) and zero-copy data transfer for columns with selectivity less than \(TH\).
Our solution also achieves 5.7$\times$ speedup over \texttt{Proteus-Hybrid}, despite that it uses both CPU and GPU.
It is difficult for such a hybrid solution to divide work between CPU and GPU and efficiently utilize the CPU DRAM bandwidth.
Our solution achieves 6.2$\times$ speedup over \texttt{Proteus-Lazy}, which enhances \texttt{Proteus-GPU} with late materialization techniques.
After we resolve the IO bottleneck and fully utilize CPU-side DRAM, a pure GPU-based solution can achieve highly competitive results.


%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
Figure~\ref{fig:ssb-perf} shows the comparison of SSB query performance between our solution and the baselines.
On average, our solution achieves 3.4$\times$ speedup over DuckDB, with all the data fetched from CPU DRAM on the fly.
Broken down into each query flight, the speedup is 2.4$\times$ for Q1.*, 3.6$\times$ for Q2.*, 3.9$\times$ for Q3.* and 3.7$\times$ for Q4.*.
More speedup is observed in Q2.*, Q3.*, and Q4.* because they include more complicated multi-way joins.

By comparing the bars of \texttt{navie} and \texttt{Proteus-GPU} with \texttt{DuckDB}, note that GPU-based solutions fail to achieve comparable performance to CPU-based DuckDB without using our IO redistribution technique.
However, this technique only is not enough, as we can see by comparing the bars of \texttt{GPU-IO} with \texttt{DuckDB}. 
It only achieves a 1.6$\times$ speedup against DuckDB, as it transfers unused data to GPU ignoring columns' selectivity.
While we can use zero-copy to exploit selectivity, it fails short in maximum throughput because it only uses one PCIe link.
We can notice that using zero-copy alone only delivers worse performance than \texttt{GPU-IO}.
Our final solution switches between SDMA-based data transfer for columns with selectivity larger than a threshold $TH$ and zero-copy data transfer for columns with selectivity lower than $TH$.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/zero-copy-vs-gpu-io.pdf}
%     \caption{Zero copy vs GPU IO}
%     \label{fig:selectivity-perf}
% \end{figure}

% In our study, we set the threshold \(TH = 64\) based on the formula outlined in \S\ref{sec:design-ssb}. 
% To ensure the accuracy and effectiveness of this threshold, we developed the following micro-benchmark specifically designed for validation purposes.
% \begin{verbatim}
% for i in range(16e9)
%   sum += pred[i % 2e9] % SEL == 0 ? v[i] : 0
% \end{verbatim}
% The \texttt{pred} array resides in GPU memory, and \texttt{SEL} is a hyperparameter that is inversely related to selectivity. 
% We implement this micro-benchmark using both GPU-IO and zero-copy data transfer techniques, varying \texttt{SEL} from 1 to 128. 
% The results are presented in Figure~\ref{fig:selectivity-perf}. 
% Notably, when \texttt{SEL} \(> 64\), zero-copy becomes more efficient. 
% This aligns with the threshold \(TH < \frac{1}{64}\), corroborating the results derived from our formula.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
The \texttt{pred} array on GPU memory, and \texttt{SEL} is a hyperparameter that is inverse to the selectivity.
We implement this micro-benchmark using both GPU-IO and zero-copy data transfer and varies \texttt{SEL} from 1 to 128.
The result is presented in Figure~\ref{fig:selectivity-perf}.
We notice when \texttt{SEL} $>64$ zero-copy becomes more efficient. 
This corresponds to $TH < \frac{1}{64}$ and matches the result from our formula.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

\subsection{Interference Analysis}
\label{sec:interference}
\noindent
While \THISWORK\ utilizes additional GPUs and their IO resources to forward data to a target GPU, running AI workloads on these auxiliary GPUs can lead to a slowdown of these workloads.
Figure~\ref{fig:interference}(a) presents the slowdown for the AI applications (x-axis) when the IO traffic (y-axis) runs in the background, and (b) shows the slowdown for the \THISWORK\ applications (y-axis) when the deep learning applications (x-axis) run in the background.
(1) Compared to single-direction IO traffic, bidirectional IO traffic has a more significant impact on the performance of foreground applications. This is likely due to the increased stress placed on the memory subsystems of the forwarding GPUs.
(2) Memory-intensive workloads are more susceptible to interference from data forwarding activities, as their performance is constrained by the memory bandwidth available on the GPUs. 
Background data forwarding consumes a portion of the memory bandwidth, leading to an average slowdown of 6.8\%.
Compared to SD3, text embedding generation, and LLM prefilling, LLM decoding experiences a greater degree of slowdown.

Figure~\ref{fig:interference} illustrates that current hardware may not optimize for our IO optimization techniques due to two key observations.
First, although the memory subsystem is theoretically stressed to the same degree in both scenarios, forwarding IO traffic from the device to the host results in a more significant slowdown compared to traffic from the host to the device.
Second, to support the 140GB/s IO throughput we achieved, each GPU incurs an additional memory bandwidth cost of $\frac{140 \times 2}{4} = 70$GB/s, which constitutes only $\frac{70}{1200} \approx 5.8\%$ of the MI100's total bandwidth.
However, empirical observations reveal slowdowns of 7.2\%, 13.4\%, and 16.9\% for \texttt{SD3}, \texttt{Llama3} decoding with a batch size of 32, and \texttt{Llama3} decoding with a batch size of 1, respectively.
We hypothesize that this discrepancy arises because our programming model generates atypical memory traffic that hinders the GPU memory controller's ability to fully utilize bandwidth for the foreground application.

We analyze the slowdown of data analytics applications caused by DL applications on forwarding GPUs. 
As shown in Figure~\ref{fig:interference}, the target GPU experiences less slowdown, with a maximum of 10.4\%. 
However, the slowdown patterns are more irregular compared to forwarding GPUs. 
Text embedding generation and \texttt{Llama3} prefilling cause more interference than \texttt{SD3}, despite all being compute-bound workloads. 
Interestingly, the memory-bound \texttt{Llama3} decoding shows less interference on the target GPU, contrasting with the significant interference on the forwarding GPUs.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
Besides, Figure~\ref{fig:interference} also shows current hardware may not be able to handle our novel use cases efficiently.
(1) While stressing the memory subsystem to the same degree theoretically, the forwarding IO traffic from device to host causes a greater slowdown than from host to device.
(2) To support the ~140GB/s IO throughput we achieved, each GPU only needs to pay $\frac{140 \times 2}{4} = 70$ GB/s additional memory bandwidth, which is only $\frac{70}{1200} \approx 5.8\%$ of MI100's total bandwidth.
However, we observe 7.2\%, 13.4\%, and 16.9\% slowdown for \texttt{SD3}, \texttt{Llama3} decoding with batch size 32, and \texttt{Llama3} decoding with batch size 1.
We speculate that this is because our new way of programming generates uncommon memory traffic to the GPU memory controller, and prevents it from fully utilizing the maximum memory bandwidth.

Next, we also analyze the slowdown of data analytics applications influenced by DL applications running on the forwarding GPUs.
Less slowdown is observed on the target GPU as shown in Figure~\ref{fig:interference}, where the maximum slowdown is 10.4\%.
However, the slowdown numbers become more irregular compared to the case of forwarding GPUs.
We observe that text embedding generation and \texttt{Llama3} prefilling cause more interference than \texttt{SD3}, although all of them are compute-bound workloads.
Surprisingly, memory-bound \texttt{Llama3} decoding shows less interference on the target GPU, in contrast to the high degree of interference on the forwarding GPUs.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

% \noindent
% \textbf{How are \THISWORK\ applications influenced by the DL applications on the forwarding GPUs?}


\noindent
\textbf{Overall system efficiency.}
Given that our technique can accelerate heavily IO-bound applications by 3 to 4 times, we argue that the system is still more efficient even with a slowdown of up to 16.9\% on the other GPUs.
The improvement of overall system efficiency in a 4-GPU system can be quantified as shown below.
% We discuss how our technique enhances the overall efficiency of a 4-GPU system, as quantified by the following formula.
% Given that our technique can speed up the heavily IO-bounded applications by 3~4$\times$, up to 16.9\% slowdown on the other three GPUs is acceptable. 
% We discuss how much our technique improves the 4-GPU system's efficiency as a whole.
% The improvement of the whole 4-GPU system's efficiency is given by the following formula
$$
\text{speedup}_\text{sys} = \frac{\text{speedup}_\text{t} * \text{slowdown}_\text{t} + 3 * \text{slowdown}_\text{f}}{4}
$$
The subscripts `t' and `f' denote the target GPU and forwarding GPUs, respectively. 
Consider the scenario where \texttt{SD3} and hash join, both with primarily bidirectional IO traffic, are collocated.
The overall system speedup is $\frac{3.2 * (1 - 0.051) + 3 * (1 - 0.072)}{4} \approx 1.45$.
In our setup, the least favorable combination is running \texttt{Llama3} decoding without batching alongside sort. 
Despite this, we still achieve a modest speedup of$\frac{1.7 * (1 - 0.032) + 3 * (1 - 0.169)}{4} \approx 1.03$ speedup.
Note that these speedup values refer to the entire 4-GPU system. 
For a single GPU, they correspond to speedups of 2.8$\times$ and 1.12$\times$, respectively.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
where the subscript ``t'' and ``f'' mean the target GPU and the forwarding GPUs.
Consider the case of collocating \texttt{SD3} and hash join, whose IO traffic is mainly bidirectional.
The whose system speedup is $\frac{3.2 * (1 - 0.051) + 3 * (1 - 0.072)}{4} \approx 1.45$.
In our setup, the worst combination is running \texttt{Llama3} decoding without batching with sort, but we still achieve a minor $\frac{1.7 * (1 - 0.032) + 3 * (1 - 0.169)}{4} \approx 1.03$ speedup.
Note that the speedup here is in terms of all 4 GPUs, and the speedup above translates to 2.8$\times$ and 1.12$\times$ in terms of a single GPU.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%