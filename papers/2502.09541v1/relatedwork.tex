\section{Related Work}
\label{section:related_works}

\textbf{GPU native query processing systems.}
Many GPU-accelerated systems~\cite{heavyai, mg-join-sigmod-2021, multi-gpu-sort-sigmod-2022} optimize for the case that the entire dataset can be stored in GPU memory, and use CPU-based solutions or stream data to GPU only as fallback plans. 
Such systems focus on improving the performance of on-device GPU kernels while addressing the memory capacity problem by using multiple GPUs.
% In contrast, our work breaks the IO wall between CPU and GPU to enable the GPU to process large datasets stored in CPU-side memory.
In contrast, our work enables the GPU to process large datasets in CPU-side memory by borrowing the IO bandwidth from the other GPUs.
% Despite using a multi-GPU system, we only run the query processing application on one of the GPUs, and borrow the IO bandwidth from the other GPUs that under-utilize IO resources through SDMA engines.
\THISWORK\ also directly benefits from techniques that improve on-device GPU kernels~\cite{Funke-sigmod18, crystal-sigmod-20}, as it can reuse existing GPU code through its IO-decoupled programming model.

% The other GPUs run different applications, like DL applications, with a minor degree of slowdown caused by memory subsystem interference.


\noindent
\textbf{CPU-GPU hybrid query processing systems.}
To handle large datasets, multiple systems~\cite{hetexchange-vldb-2019, HERO-VLDB-2017, GDB-TDBSys-2009, mordered-vldb-2022, Ocelot-VLDB-2014, Ocelet-VLDB-2013, GPUQP, FlinkCL} place part or all of the data in CPU memory and use both CPU and GPU to process the query. 
These works include multiple optimizations to enhance operator placement and data placement and reduce the amount of data to be processed.
\THISWORK\ focuses on IO optimization and is orthogonal to prior works in this direction.
\THISWORK's discussion on late materialization emphasizes trading off zero-copy access and our SDMA-based IO primitives and provides a verified analytical formula to benefit from both, setting it apart from the prior late materialization techniques for GPU query processing like the one in GHive~\cite{GHive}.
Hybrid systems can incorporate our optimization to improve their performance further.

\noindent
\textbf{GPU slicing for workload collocation.}
Some prior work~\cite{robroek2024euromlsys, cao2024vldb} explores GPU slicing techniques to collocate workloads that stress different micro-architecture resources on a single GPU.
Resources, like memory bandwidth, that would be left idle by a workload can be used by another complementary workload. 
\THISWORK's IO redistribution idea at the GPU level, can be combined with GPU slicing at sub-GPU granularity for even higher overall system efficiency, which we leave as future work.
For example, one can assign a 25\% GPU slice with all GPUs' IO bandwidth with \THISWORK's techniques to further resolve the IO bottleneck.



%