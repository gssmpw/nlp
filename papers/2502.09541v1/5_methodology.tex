\section{Evaluation Methodology} \label{section:methodology}

We evaluate \THISWORK\ in two distinct scenarios.
We first run \THISWORK\ while keeping all forwarding GPUs idle to understand the upper bound on the improvement from our IO primitive.
We require that all input/output data be retrieved from and stored in CPU's DRAM, with no GPU-side caching, which represents the most challenging setup for GPU execution. 
Then, we concurrently run \THISWORK\ on the target GPU and deep learning workloads on the forwarding GPUs to analyze the interference introduced by our techniques.
While an alternative to partition system resources is by running data analytics on 25\% of four GPUs, we experimentally show in the technical report~\cite{vortex-technical-report} that this is an inferior design choice.


%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
We conduct two sets of evaluations for \THISWORK. 
We first run \THISWORK\ while keeping all forwarding GPUs idle to understand the limit of our IO redistribution techniques.
We take the most challenging experiment setup for GPU execution, where \textit{all experiments require the input and output to be stored at and generated to CPU-side DRAM, and no caching is allowed}.
Then, we concurrently run \THISWORK\ on the target GPU and deep learning workloads on the forwarding GPUs to analyze the interference introduced by our techniques.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

\subsection{Interference-Free Analysis}
\noindent
\textbf{\texttt{Exchange}.}
We evaluate the efficiency of our \texttt{Exchange} IO primitive by varying both the total volume of data transferred and the granularity of the data packets. 
For benchmarking, we compare it against an in-house implementation that relies solely on the GPU runtime.
This baseline is susceptible to the challenges outlined in \S\ref{sec:io-challenges}.

\noindent
\textbf{Sort.}
In this experiment, we sort 8 billion 8-byte integers and use TBB and PARADIS~\cite{paradis-vldb-2015}, which are state-of-the-art CPU sort implementations from industry and academia, as our baselines. 
Due to the absence of open-source out-of-core GPU sort libraries, we compare \THISWORK\ against in-house GPU baseline. 
The only distinction between this and \THISWORK\ is that the former utilizes the IO resources of a single GPU, whereas \THISWORK\ leverages multiple GPUs.

\noindent
\textbf{Hash Join.}
Following the experimental setup outlined in \cite{triton-join}, we focus on the query below
\begin{verbatim}
SELECT SUM(A.val + B.val) FROM A, B WHERE A.key == B.key;
\end{verbatim}
% We follow the experimental setup outlined in \cite{triton-join}, as detailed in \S\ref{sec:design-join}. 
Because we lack access to machines equipped with CPU-GPU NV-Links, we use the results from \cite{triton-join} as our GPU baseline. 
For our CPU baselines, we employ DuckDB and the CPU hash join implementation from \cite{triton-join}, representing state-of-the-art solutions from both industry and academia.

\noindent
\textbf{Star Schema Benchmark (SSB).}
% {
% \color{blue}
SSB is an OLAP benchmark widely used by many prior database research~\cite{ydb-2013, crystal-sigmod-20, mordered-vldb-2022, kaibo-vldb-2014, hetexchange-vldb-2019}, composed of 13 queries grouped in 4 query flights that process over a fact table and a few small dimension tables.
These queries commonly filter the dimension table, join them with the fact table, and aggregate the joined results. 

We integrate \THISWORK\ with the on-core query processing library Crystal~\cite{crystal-sigmod-20} to implement queries in SSB.
As the dimension table is small, we load the dimension tables to GPU memory, filter and build the hash table for them, and keep them inside GPU memory.
Then, we partition the fact table and process these partitions one by one by Crystal.
We utilize the late materialization optimization when transferring the fact table to the GPU.
Because the selectivity of each dimension table is known after their hash tables are built, we can use the selectivity of the dimension table as an accurate estimator for the selectivity of each column in the fact table.
Thus, we can optimize the star schema query by late materializing some columns in the fact table during query execution, as long as their selectivity estimator is less than $TH$.

{\color{blue}


}


% }

We evaluate SSB at a scale factor (SF) of 1000, comprising 6 billion rows in the fact table. 
The entire database spans approximately 600 GB, with 144 GB designated as hot data for query processing. 
For benchmarking, we use DuckDB as our CPU baseline and Proteus~\cite{hetexchange-vldb-2019} as our GPU baseline, which is a leading out-of-core GPU database. 
We compare \THISWORK\ against Proteus in two configurations: CPU-GPU hybrid execution mode, and pure-GPU execution mode using default settings.
\THISWORK\ executes each query independently without caching data between queries.

\subsection{Interference Analysis}
As we are borrowing idle IO resources from neighbors that run compute-intensive applications like Deep Learning (DL), it is important to understand the degree of interference caused by our technique in such a multi-tenant system.
We run a range of deep learning workloads from Hugging Face on the forwarding GPUs to assess the impact of \THISWORK\ on system performance.

\noindent
\textbf{Text-to-image Diffusion Model Inference.} 
We use \textit{stable-diffusion-3-medium} (SD3)~\cite{stable-diffusion}, a widely recognized text-to-image generative model that creates images from user prompts. 
Since the diffusion model operates iteratively, we measure performance based on the number of iterations completed per second.

\noindent
\textbf{Text Embedding Generation.}
Text embeddings convert text into high-dimensional vectors and are crucial for text mining and Retrieval-Augmented Generation (RAG) applications, which enhance LLM.
In our evaluation, we use the state-of-the-art text embedding model \textit{e5-mistral-7b-instruct}~\cite{wang2022text, wang2024improving}. 
This model processes text sequences to generate corresponding embeddings. 
We measure its performance by the number of embeddings produced per second.

\begin{figure*}[t]
\centerline{\includegraphics[width=0.89\linewidth]{figures/io-throughput.pdf}}
\caption{Data transfer throughput achieved by the IO-primitives with different transfer granularity.}
\label{fig:io-bandwidth}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/sort-join-result-prop.pdf}
    \caption{Results for Sort (a-c) and Join (d-e). (a,d) the throughput achieved by different solutions, (b,e) the time breakdown for the \THISWORK\ sort/join, and (c,f) the time taken by on-GPU kernel execution of a typical pipeline stage.}
    \label{fig:sort-perf}
    
\end{figure*}



\noindent
\textbf{LLM Serving.}
Our evaluation of LLM serving focuses on \textit{Meta-Llama-3-8B-Instruct}~\cite{llama3modelcard}. 
LLM inference comprises two distinct phases: the compute-intensive prefill phase, which processes prompts, and the memory-intensive decode phase, which generates text. 
We assess these stages separately, given that they have different workload characteristics and modern LLM serving systems often process them on decoupled GPU pools. 
% \textcolor{red}{
% The workload characteristics of LLM inference are influenced by runtime factors such as sequence length and batch size. 
% Additionally, this work investigates how different batching strategies impact the level of interference introduced by LLM serving.
% }

% We base our evaluation of LLM serving on \texttt{Meta-Llama-3-8B-Instruct}.
% LLM inference includes a compute-intensive prefill phase that processes prompts and a memory-intensive decode phase that generates the text.
% We evaluate these two stages separately as recent LLM serving systems process these two stages on two decoupled GPU pools.
% The workload characteristic of LLM inference depends on a bunch of runtime factors, like sequence length and batch size.
% In this work, we also investigate how LLM batching influences the degree of interference.

\noindent
\textbf{Workload Characteristics}
The DL workloads we analyze exhibit varying demands on GPU memory subsystems and compute units. 
Diffusion models, text embedding generation, and the prefill stage of LLM serving are characterized by high arithmetic intensity and are therefore classified as compute-bound applications~\cite{golden2024generativeaillmsimplications, zhao2024prepackingsimplemethodfast}. 
Conversely, the decode stage of LLM serving is memory-bound when processing small batch sizes but shifts to compute-bound as batch sizes increase~\cite{zhang2024flattenquantbreakinginferencecomputebound}. 
A detailed analysis on how these characteristics affect interference is present in \S\ref{sec:interference}.
% We provide a detailed analysis of how these differing workload characteristics affect the degree of interference in 

\subsection{Hardware Configuration}
All the GPU workloads, except Proteus, are run on a dual-socket server with 8 AMD MI100 GPUs connected to two AMD 7V13 processors in groups of 4.
We only use 4 GPUs and one socket for this work.
The GPUs are directly connected to the CPU through PCIe 4.0 links, which provide ~28GB/s maximum bandwidth in each direction.
% While we use vendor-agnostic CPU-GPU interconnect technology, \THISWORK\ can also be used with vendor-specific interconnects such as NVLink to offer even higher bandwidth, which may further improve performance.
AMD 7V13 CPU has 8 memory channels running at 3200Mhz, and we measure the maximum DRAM bandwidth at 150GB/s using the STREAM benchmark~\cite{stream-benchmark}.
Because Proteus is developed based on Nvidia GPUs, we run it on Nvidia A40, a GPU comparable to AMD MI100.
We run CPU baselines on a dual-socket server with two Intel Xeon Platinum 8380 processors, which also have 8 memory channels per socket and deliver 150GB/s memory bandwidth.
We choose this server for CPU baselines as they achieve better performance compared to the AMD server.


