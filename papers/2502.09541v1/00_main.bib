@article{rui-vldb2020,
author = {Rui, Ran and Li, Hao and Tu, Yi-Cheng},
title = {Efficient join algorithms for large database tables in a multi-GPU environment},
year = {2020},
issue_date = {December 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {4},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/3436905.3436927},
doi = {10.14778/3436905.3436927},
abstract = {Relational join processing is one of the core functionalities in database management systems. It has been demonstrated that GPUs as a general-purpose parallel computing platform is very promising in processing relational joins. However, join algorithms often need to handle very large input data, which is an issue that was not sufficiently addressed in existing work. Besides, as more and more desktop and workstation platforms support multi-GPU environment, the combined computing capability of multiple GPUs can easily achieve that of a computing cluster. It is worth exploring how join processing would benefit from the adaptation of multiple GPUs. We identify the low rate and complex patterns of data transfer among the CPU and GPUs as the main challenges in designing efficient algorithms for large table joins. To overcome such challenges, we propose three distinctive designs of multi-GPU join algorithms, namely, the nested loop, global sort-merge and hybrid joins for large table joins with different join conditions. Extensive experiments running on multiple databases and two different hardware configurations demonstrate high scalability of our algorithms over data size and significant performance boost brought by the use of multiple GPUs. Furthermore, our algorithms achieve much better performance as compared to existing join algorithms, with a speedup up to 25X and 2.8X over best known code developed for multi-core CPUs and GPUs respectively.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {708–720},
numpages = {13}
}

@inproceedings{triton-join,
author = {Lutz, Clemens and Bre\ss{}, Sebastian and Zeuch, Steffen and Rabl, Tilmann and Markl, Volker},
title = {Triton Join: Efficiently Scaling to a Large Join State on GPUs with Fast Interconnects},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517911},
doi = {10.1145/3514221.3517911},
abstract = {Database management systems are facing growing data volumes. Previous research suggests that GPUs are well-equipped to quickly process joins and similar stateful operators, as GPUs feature high-bandwidth on-board memory. However, GPUs cannot scale joins to large data volumes due to two limiting factors: (1)~large state does not fit into the on-board memory, and (2)~spilling state to main memory is constrained by the interconnect bandwidth. Thus, CPUs are often the better choice for scalable data processing.In this paper, we propose a new join algorithm that scales to large data volumes by taking advantage of fast interconnects. Fast interconnects such as NVLink~2.0 are a new technology that connect the GPU to main memory at a high bandwidth, and thus enable us to design our join to efficiently spill its state. Our evaluation shows that our Triton join outperforms a no-partitioning hash join by more than 100\texttimes{} on the same GPU, and a radix-partitioned join on the CPU by up to 2.5\texttimes{}. As a result, GPU-enabled DBMSs are able to scale beyond the GPU memory capacity.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1017–1032},
numpages = {16},
keywords = {GPU, TLB, data transfer bottleneck, modern hardware, out-of-core},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{pump-up-volume,
author = {Lutz, Clemens and Bre\ss{}, Sebastian and Zeuch, Steffen and Rabl, Tilmann and Markl, Volker},
title = {Pump Up the Volume: Processing Large Data on GPUs with Fast Interconnects},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3318464.3389705},
doi = {10.1145/3318464.3389705},
abstract = {GPUs have long been discussed as accelerators for database query processing because of their high processing power and memory bandwidth. However, two main challenges limit the utility of GPUs for large-scale data processing: (1) the on-board memory capacity is too small to store large data sets, yet (2) the interconnect bandwidth to CPU main-memory is insufficient for ad hoc data transfers. As a result, GPU-based systems and algorithms run into a transfer bottleneck and do not scale to large data sets. In practice, CPUs process large-scale data faster than GPUs with current technology. In this paper, we investigate how a fast interconnect can resolve these scalability limitations using the example of NVLink 2.0. NVLink 2.0 is a new interconnect technology that links dedicated GPUs to a CPU@. The high bandwidth of NVLink 2.0 enables us to overcome the transfer bottleneck and to efficiently process large data sets stored in main-memory on GPUs. We perform an in-depth analysis of NVLink 2.0 and show how we can scale a no-partitioning hash join beyond the limits of GPU memory. Our evaluation shows speed-ups of up to 18x over PCI-e 3.0 and up to 7.3x over an optimized CPU implementation. Fast GPU interconnects thus enable GPUs to efficiently accelerate query processing.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1633–1649},
numpages = {17},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@INPROCEEDINGS{gowan-ipdpsw-2018,
  author={Gowanlock, Michael and Karsin, Ben},
  booktitle={2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={Sorting Large Datasets with Heterogeneous CPU/GPU Architectures}, 
  year={2018},
  volume={},
  number={},
  pages={560-569},
  keywords={Graphics processing units;Sorting;Data transfer;Merging;Memory management;Optimization;Central Processing Unit;GPGPU;Heterogeneous architecture;Sorting},
  doi={10.1109/IPDPSW.2018.00095}}

@INPROCEEDINGS{sioulas-icde-2019,
  author={Sioulas, Panagiotis and Chrysogelos, Periklis and Karpathiotakis, Manos and Appuswamy, Raja and Ailamaki, Anastasia},
  booktitle={2019 IEEE 35th International Conference on Data Engineering (ICDE)}, 
  title={Hardware-Conscious Hash-Joins on GPUs}, 
  year={2019},
  volume={},
  number={},
  pages={698-709},
  keywords={Graphics processing units;Partitioning algorithms;Instruction sets;Hardware;Bandwidth;Engines;Performance evaluation;join;GPU;databases;analytics},
  doi={10.1109/ICDE.2019.00068}}

@misc{onesweep-2022,
      title={Onesweep: A Faster Least Significant Digit Radix Sort for GPUs}, 
      author={Andy Adinets and Duane Merrill},
      year={2022},
      eprint={2206.01784},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2206.01784}, 
}

@inproceedings{mergepath-2012,
author = {Green, Oded and McColl, Robert and Bader, David A.},
title = {GPU merge path: a GPU merging algorithm},
year = {2012},
isbn = {9781450313162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/2304576.2304621},
doi = {10.1145/2304576.2304621},
abstract = {Graphics Processing Units (GPUs) have become ideal candidates for the development of fine-grain parallel algorithms as the number of processing elements per GPU increases. In addition to the increase in cores per system, new memory hierarchies and increased bandwidth have been developed that allow for significant performance improvement when computation is performed using certain types of memory access patterns.Merging two sorted arrays is a useful primitive and is a basic building block for numerous applications such as joining database queries, merging adjacency lists in graphs, and set intersection. An efficient parallel merging algorithm partitions the sorted input arrays into sets of non-overlapping sub-arrays that can be independently merged on multiple cores. For optimal performance, the partitioning should be done in parallel and should divide the input arrays such that each core receives an equal size of data to merge.In this paper, we present an algorithm that partitions the workload equally amongst the GPU Streaming Multi-processors (SM). Following this, we show how each SM performs a parallel merge and how to divide the work so that all the GPU's Streaming Processors (SP) are utilized. All stages in this algorithm are parallel. The new algorithm demonstrates good utilization of the GPU memory hierarchy. This approach demonstrates an average of 20X and 50X speedup over a sequential merge on the x86 platform for integer and floating point, respectively. Our implementation is 10X faster than the fast parallel merge supplied in the CUDA Thrust library.},
booktitle = {Proceedings of the 26th ACM International Conference on Supercomputing},
pages = {331–340},
numpages = {10},
keywords = {graphics processors, measurement of multiple-processor systems, parallel algorithms, parallel systems},
location = {San Servolo Island, Venice, Italy},
series = {ICS '12}
}

@misc{rocprim,
  author = {AMD},
  year = 2024,
  title = {rocPRIM documentation},
  url = {https://rocm.docs.amd.com/projects/rocPRIM/en/latest/},
  note = {Accessed: 07/02/2024}
}

@misc{cub,
  author = {NVidia},
  year = 2024,
  title = {cub documentation},
  url = {https://docs.nvidia.com/cuda/cub/index.html},
  note = {Accessed: 07/02/2024}
}

@misc{hip,
    author = {AMD},
    year = 2024,
    title = {HIP documentation},
    url = {https://rocm.docs.amd.com/projects/HIP/en/latest/},
    note = {Accessed: 07/04/2024}
}

@misc{cuda,
    author = {NVidia},
    year = 2024,
    title = {CUDA Programming Guide},
    url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-interface},
    note = {Accessed: 07/04/2024}
}

@misc{oracle-star-join,
   author = {Oracle},
   year = 2003,
   title = {Star Join},
   url = {https://www.orafaq.com/tuningguide/star%20join.html},
   note = {Accessed: 07/22/2024}
}

@misc{mi300x,
    author = {AMD},
    year = 2024,
    title = {AMD Instinct MI300X Accelerators},
    url ={https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html},
    note = {Accessed: 07/23/2024}
}

@misc{aws-p4-topo,
    author = {Neelay Thaker},
    year = 2020,
    title = {Amazon EC2 P4d instances deep dive},
    url = {https://aws.amazon.com/blogs/compute/amazon-ec2-p4d-instances-deep-dive/},
    now = {Accessed: 07/23/2024}
}

@misc{aws-p3-topo,
    author = {Emma White},
    year = 2019,
    title = {Optimizing deep learning on P3 and P3dn with EFA},
    url = {https://aws.amazon.com/blogs/compute/optimizing-deep-learning-on-p3-and-p3dn-with-efa/},
    now = {Accessed: 07/23/2024}
}

@misc{genoa,
    author = {AMD},
    year = 2024,
    title = {4th Generation AMD EPYC Processors},
    url = {https://www.amd.com/en/products/processors/server/epyc/4th-generation-9004-and-8004-series.htmll},
    note = {Accessed: 07/23/2024}
}

@misc{stream-benchmark,
    author = {AMD},
    year = 2024,
    title = {STREAM Benchmark},
    url = {https://www.amd.com/en/developer/zen-software-studio/applications/spack/stream-benchmark.html},
    note = {Accessed: 07/31/2024}
}

@misc{heavyai,
    author = {HEAVY.AI},
    year = 2024,
    title = {HEAVY.AI Documentation},
    url = {https://docs.heavy.ai/installation-and-configuration/system-requirements/hardware},
    note = {Accessed: 07/24/2024}
}


@misc{stable-diffusion,
    author = {Stability AI},
    year = {2024},
    title = {Stable Diffusion 3 Medium Model},
    url = {https://huggingface.co/stabilityai/stable-diffusion-3-medium},
    note = {Accessed: 07/25/2024}

}

@inproceedings{partitioned-join-vldb99,
author = {Boncz, Peter A. and Manegold, Stefan and Kersten, Martin L.},
title = {Database Architecture Optimized for the New Bottleneck: Memory Access},
year = {1999},
isbn = {1558606157},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 25th International Conference on Very Large Data Bases},
pages = {54–65},
numpages = {12},
series = {VLDB '99}
}

@inproceedings{crystal-sigmod-20,
author = {Shanbhag, Anil and Madden, Samuel and Yu, Xiangyao},
title = {A Study of the Fundamental Performance Characteristics of GPUs and CPUs for Database Analytics},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3318464.3380595},
doi = {10.1145/3318464.3380595},
abstract = {There has been significant amount of excitement and recent work on GPU-based database systems. Previous work has claimed that these systems can perform orders of magnitude better than CPU-based database systems on analytical workloads such as those found in decision support and business intelligence applications. A hardware expert would view these claims with suspicion. Given the general notion that database operators are memory-bandwidth bound, one would expect the maximum gain to be roughly equal to the ratio of the memory bandwidth of GPU to that of CPU. In this paper, we adopt a model-based approach to understand when and why the performance gains of running queries on GPUs vs on CPUs vary from the bandwidth ratio (which is roughly 16\texttimes{} on modern hardware). We propose Crystal, a library of parallel routines that can be combined together to run full SQL queries on a GPU with minimal materialization overhead. We implement individual query operators to show that while the speedups for selection, projection, and sorts are near the bandwidth ratio, joins achieve less speedup due to differences in hardware capabilities. Interestingly, we show on a popular analytical workload that full query performance gain from running on GPU exceeds the bandwidth ratio despite individual operators having speedup less than bandwidth ratio, as a result of limitations of vectorizing chained operators on CPUs, resulting in a 25\texttimes{} speedup for GPUs over CPUs on the benchmark.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1617–1632},
numpages = {16},
keywords = {in-memory analytics, heterogenous systems, GPU query performance, GPU data analytics},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{ydb-2013,
author = {Yuan, Yuan and Lee, Rubao and Zhang, Xiaodong},
title = {The Yin and Yang of processing data warehousing queries on GPU devices},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {10},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/2536206.2536210},
doi = {10.14778/2536206.2536210},
abstract = {Database community has made significant research efforts to optimize query processing on GPUs in the past few years. However, we can hardly find that GPUs have been truly adopted in major warehousing production systems. Preparing to merge GPUs to the warehousing systems, we have identified and addressed several critical issues in a three-dimensional study of warehousing queries on GPUs by varying query characteristics, software techniques, and GPU hardware configurations. We also propose an analytical model to understand and predict the query performance on GPUs. Based on our study, we present our performance insights for warehousing query execution on GPUs. The objective of our work is to provide a comprehensive guidance for GPU architects, software system designers, and database practitioners to narrow the speed gap between the GPU kernel execution (the fast mode) and data transfer to prepare GPU execution (the slow mode) for high performance in processing data warehousing queries. The GPU query engine developed in this work is open source to the public.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {817–828},
numpages = {12}
}

@article{kaibo-vldb-2014,
author = {Wang, Kaibo and Zhang, Kai and Yuan, Yuan and Ma, Siyuan and Lee, Rubao and Ding, Xiaoning and Zhang, Xiaodong},
title = {Concurrent analytical query processing with GPUs},
year = {2014},
issue_date = {July 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {11},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/2732967.2732976},
doi = {10.14778/2732967.2732976},
abstract = {In current databases, GPUs are used as dedicated accelerators to process each individual query. Sharing GPUs among concurrent queries is not supported, causing serious resource underutilization. Based on the profiling of an open-source GPU query engine running commonly used single-query data warehousing workloads, we observe that the utilization of main GPU resources is only up to 25\%. The underutilization leads to low system throughput.To address the problem, this paper proposes concurrent query execution as an effective solution. To efficiently share GPUs among concurrent queries for high throughput, the major challenge is to provide software support to control and resolve resource contention incurred by the sharing. Our solution relies on GPU query scheduling and device memory swapping policies to address this challenge. We have implemented a prototype system and evaluated it intensively. The experiment results confirm the effectiveness and performance advantage of our approach. By executing multiple GPU queries concurrently, system throughput can be improved by up to 55\% compared with dedicated processing.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1011–1022},
numpages = {12}
}

@article{mordered-vldb-2022,
author = {Yogatama, Bobbi W. and Gong, Weiwei and Yu, Xiangyao},
title = {Orchestrating Data Placement and Query Execution in Heterogeneous CPU-GPU DBMS},
year = {2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3551793.3551809},
doi = {10.14778/3551793.3551809},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2491–2503},
numpages = {13}
}

@misc{vortex-technical-report,
    author = {Yuan, Yichao and Iyer, Advait and Ma, Lin and Talati, Nishil},
    year = {2024},
    title = {Vortex: Overcoming Memory Capacity Limitations in
GPU-Accelerated Large-Scale Data Analytics (Technical Report)},
    url = {https://figshare.com/s/550db82949fe74dfa41e}
}

@article{hetexchange-vldb-2019,
author = {Chrysogelos, Periklis and Karpathiotakis, Manos and Appuswamy, Raja and Ailamaki, Anastasia},
title = {HetExchange: encapsulating heterogeneous CPU-GPU parallelism in JIT compiled engines},
year = {2019},
publisher = {VLDB Endowment},
volume = {12},
number = {5},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/3303753.3303760},
doi = {10.14778/3303753.3303760},
abstract = {Modern server hardware is increasingly heterogeneous as hardware accelerators, such as GPUs, are used together with multicore CPUs to meet the computational demands of modern data analytics work-loads. Unfortunately, query parallelization techniques used by analytical database engines are designed for homogeneous multicore servers, where query plans are parallelized across CPUs to process data stored in cache coherent shared memory. Thus, these techniques are unable to fully exploit available heterogeneous hardware, where one needs to exploit task-parallelism of CPUs and data-parallelism of GPUs for processing data stored in a deep, non-cache-coherent memory hierarchy with widely varying access latencies and bandwidth.In this paper, we introduce HetExchange-a parallel query execution framework that encapsulates the heterogeneous parallelism of modern multi-CPU-multi-GPU servers and enables the parallelization of (pre-)existing sequential relational operators. In contrast to the interpreted nature of traditional Exchange, HetExchange is designed to be used in conjunction with JIT compiled engines in order to allow a tight integration with the proposed operators and generation of efficient code for heterogeneous hardware. We validate the applicability and efficiency of our design by building a prototype that can operate over both CPUs and GPUs, and enables its operators to be parallelism- and data-location-agnostic. In doing so, we show that efficiently exploiting CPU-GPU parallelism can provide 2.8x and 6.4x improvement in performance compared to state-of-the-art CPU-based and GPU-based DBMS.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {544–556},
numpages = {13}
}

@inproceedings{otterness-rtns-2021,
author = {Otterness, Nathan and Anderson, James H.},
title = {Exploring AMD GPU Scheduling Details by Experimenting With “Worst Practices”},
year = {2021},
isbn = {9781450390019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3453417.3453432},
doi = {10.1145/3453417.3453432},
abstract = {Graphics processing units (GPUs) have been the target of a significant body of recent real-time research, but research is often hampered by the “black box” nature of GPU hardware and software. Now that one GPU manufacturer, AMD, has embraced an open-source software stack, one may expect an increased amount of real-time research to use AMD GPUs. Reality, however, is more complicated. Without understanding where internal details may differ, researchers have no basis for assuming that observations made using NVIDIA GPUs will continue to hold for AMD GPUs. Additionally, the openness of AMD’s software does not mean that their scheduling behavior is obvious, especially due to sparse, scattered documentation. In this paper, we gather the disparate pieces of documentation into a single coherent source that provides an end-to-end description of how compute work is scheduled on AMD GPUs. In doing so, we start with a concrete demonstration of how incorrect management triggers extreme worst-case behavior in shared AMD GPUs. Subsequently, we explain the internal scheduling rules for AMD GPUs, how they led to the “worst practices,” and how to correctly manage some of the most performance-critical factors in AMD GPU sharing.},
booktitle = {Proceedings of the 29th International Conference on Real-Time Networks and Systems},
pages = {24–34},
numpages = {11},
keywords = {real-time systems, parallel computing, graphics processing units},
location = {NANTES, France},
series = {RTNS '21}
}

@INPROCEEDINGS{olmedo-rtas-2020,
  author={Olmedo, Ignacio Sañudo and Capodieci, Nicola and Martinez, Jorge Luis and Marongiu, Andrea and Bertogna, Marko},
  booktitle={2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)}, 
  title={Dissecting the CUDA scheduling hierarchy: a Performance and Predictability Perspective}, 
  year={2020},
  volume={},
  number={},
  pages={213-225},
  keywords={Reverse engineering;Graphics processing units;Parallel processing;Real-time systems;Timing;Resource management},
  doi={10.1109/RTAS48715.2020.000-5}}

@misc{golden2024generativeaillmsimplications,
      title={Generative AI Beyond LLMs: System Implications of Multi-Modal Generation}, 
      author={Alicia Golden and Samuel Hsia and Fei Sun and Bilge Acun and Basil Hosmer and Yejin Lee and Zachary DeVito and Jeff Johnson and Gu-Yeon Wei and David Brooks and Carole-Jean Wu},
      year={2024},
      eprint={2312.14385},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2312.14385}, 
}

@misc{zhao2024prepackingsimplemethodfast,
      title={Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models}, 
      author={Siyan Zhao and Daniel Israel and Guy Van den Broeck and Aditya Grover},
      year={2024},
      eprint={2404.09529},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.09529}, 
}

@misc{zhang2024flattenquantbreakinginferencecomputebound,
      title={FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization}, 
      author={Yi Zhang and Fei Yang and Shuang Peng and Fangyu Wang and Aimin Pan},
      year={2024},
      eprint={2402.17985},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.17985}, 
}

@inproceedings{star-schema-sigmod-02,
author = {Weininger, Andreas},
title = {Efficient execution of joins in a star schema},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/564691.564754},
doi = {10.1145/564691.564754},
abstract = {A star schema is very popular for modeling data warehouses and data marts. Therefore, it is important that a database system which is used for implementing such a data warehouse or data mart is able to efficiently handle operations on such a schema. In this paper we will describe how one of these operations, the join operation --- probably the most important operation --- is implemented in the IBM Informix Extended Parallel Server (XPS).},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {542–545},
numpages = {4},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}
@inproceedings{10.1145/53990.54022,
author = {Lam, M.},
title = {Software pipelining: an effective scheduling technique for VLIW machines},
year = {1988},
isbn = {0897912691},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/53990.54022},
doi = {10.1145/53990.54022},
abstract = {This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.The techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors. This compiler has been used for developing a large number of applications in the areas of image, signal and scientific processing.},
booktitle = {Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation},
pages = {318–328},
numpages = {11},
location = {Atlanta, Georgia, USA},
series = {PLDI '88}
}

@article{softpipe-pldi-1988,
author = {Lam, M.},
title = {Software pipelining: an effective scheduling technique for VLIW machines},
year = {1988},
issue_date = {July 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {7},
issn = {0362-1340},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/960116.54022},
doi = {10.1145/960116.54022},
abstract = {This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.The techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors. This compiler has been used for developing a large number of applications in the areas of image, signal and scientific processing.},
journal = {SIGPLAN Not.},
month = jun,
pages = {318–328},
numpages = {11}
}


@article{tqp-vldb-2022,
author = {He, Dong and Nakandala, Supun C and Banda, Dalitso and Sen, Rathijit and Saur, Karla and Park, Kwanghyun and Curino, Carlo and Camacho-Rodr\'{\i}guez, Jes\'{u}s and Karanasos, Konstantinos and Interlandi, Matteo},
title = {Query processing on tensor computation runtimes},
year = {2022},
issue_date = {July 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3551793.3551833},
doi = {10.14778/3551793.3551833},
abstract = {The huge demand for computation in artificial intelligence (AI) is driving unparalleled investments in hardware and software systems for AI. This leads to an explosion in the number of specialized hardware devices, which are now offered by major cloud vendors. By hiding the low-level complexity through a tensor-based interface, tensor computation runtimes (TCRs) such as PyTorch allow data scientists to efficiently exploit the exciting capabilities offered by the new hardware. In this paper, we explore how database management systems can ride the wave of innovation happening in the AI space.We design, build, and evaluate Tensor Query Processor (TQP): TQP transforms SQL queries into tensor programs and executes them on TCRs. TQP is able to run the full TPC-H benchmark by implementing novel algorithms for relational operators on the tensor routines. At the same time, TQP can support various hardware while only requiring a fraction of the usual development effort. Experiments show that TQP can improve query execution time by up to 10X over specialized CPU- and GPU-only systems. Finally, TQP can accelerate queries mixing ML predictions and SQL end-to-end, and deliver up to 9X speedup over CPU baselines.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2811–2825},
numpages = {15}
}

@inproceedings{tcudb-sigmod-2022,
author = {Hu, Yu-Ching and Li, Yuliang and Tseng, Hung-Wei},
title = {TCUDB: Accelerating Database with Tensor Processors},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3514221.3517869},
doi = {10.1145/3514221.3517869},
abstract = {The emergence of novel hardware accelerators has powered the tremendous growth of machine learning in recent years. These accelerators deliver incomparable performance gains in processing high-volume matrix operators, particularly matrix multiplication, a core component of neural network training and inference. In this work, we explored opportunities of accelerating database systems using NVIDIA's Tensor Core Units (TCUs). We present TCUDB, a TCU-accelerated query engine processing a set of query operators including natural joins and group-by aggregates as matrix operators within TCUs. Matrix multiplication was considered inefficient in the past; however, this strategy has remained largely unexplored in conventional GPU-based databases, which primarily rely on vector or scalar processing. We demonstrate the significant performance gain of TCUDB in a range of real-world applications including entity matching, graph query processing, and matrix-based data analytics. TCUDB achieves up to 288x speedup compared to a baseline GPU-based query engine.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1360–1374},
numpages = {15},
keywords = {tensor cores, database engine, GPU},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{multi-gpu-sort-sigmod-2022,
author = {Maltenberger, Tobias and Ilic, Ivan and Tolovski, Ilin and Rabl, Tilmann},
title = {Evaluating Multi-GPU Sorting with Modern Interconnects},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3514221.3517842},
doi = {10.1145/3514221.3517842},
abstract = {GPUs have become a mainstream accelerator for database operations such as sorting. Most GPU sorting algorithms are single-GPU approaches. They neither harness the full computational power nor exploit the high-bandwidth P2P interconnects of modern multi-GPU platforms. The latest NVLink 2.0 and NVLink 3.0-based NVSwitch interconnects promise unparalleled multi-GPU acceleration. So far, multi-GPU sorting has only been evaluated on systems with PCIe 3.0. In this paper, we analyze serial, parallel, and bidirectional data transfer rates to, from, and between multiple GPUs on systems with PCIe 3.0/4.0, NVLink 2.0/3.0, and NVSwitch. We measure up to 35x higher parallel P2P throughput with NVLink 3.0-based NVSwitch over PCIe 3.0. To study GPU-accelerated sorting on today's hardware, we implement a P2P-based GPU-only (P2P sort) and a heterogeneous (HET sort) multi-GPU sorting algorithm and evaluate them on three modern platforms. We observe speedups over state-of-the-art parallel CPU radix sort of up to 14x for P2P sort and 9x for HET sort. On systems with fast P2P interconnects, P2P sort outperforms HET sort up to 1.65x. Finally, we show that overlapping GPU copy/compute operations does not mitigate the transfer bottleneck when sorting large out-of-core data.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1795–1809},
numpages = {15},
keywords = {multi-GPU sorting, high-speed interconnects, database acceleration},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{mg-join-sigmod-2021,
author = {Paul, Johns and Lu, Shengliang and He, Bingsheng and Lau, Chiew Tong},
title = {MG-Join: A Scalable Join for Massively Parallel Multi-GPU Architectures},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3448016.3457254},
doi = {10.1145/3448016.3457254},
abstract = {The recent scale-up of GPU hardware through the integration of multiple GPUs into a single machine and the introduction of higher bandwidth interconnects like NVLink 2.0 has enabled new opportunities of relational query processing on multiple GPUs. However, due to the unique characteristics of GPUs and the interconnects, existing hash join implementations spend up to 66\% of their execution time moving the data between the GPUs and achieve lower than 50\% utilization of the newer high bandwidth interconnects. This leads to extremely poor scalablity of hash join performance on multiple GPUs, which can be slower than the performance on a single GPU. In this paper, we propose MG-Join, a scalable partitioned hash join implementation on multiple GPUs of a single machine. In order to effectively improve the bandwidth utilization, we develop a novel multi-hop routing for cross-GPU communication that adaptively chooses the efficient route for each data flow to minimize congestion. Our experiments on the DGX-1 machine show that MG-Join helps significantly reduce the communication overhead and achieves up to 97\% utilization of the bisection bandwidth of the interconnects, resulting in significantly better scalability. Overall, MG-Join outperforms the state-of-the-art hash join implementations by up to 2.5x. MG-Join further helps improve the overall performance of TPC-H queries by up to 4.5x over multi-GPU version of an open-source commercial GPU database Omnisci.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1413–1425},
numpages = {13},
keywords = {distributed join, hash join, multi-gpu architectures, network utilization},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{HERO-VLDB-2017,
author = {Karnagel, Tomas and Habich, Dirk and Lehner, Wolfgang},
title = {Adaptive work placement for query processing on heterogeneous computing resources},
year = {2017},
issue_date = {March 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {7},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/3067421.3067423},
doi = {10.14778/3067421.3067423},
abstract = {The hardware landscape is currently changing from homogeneous multi-core systems towards heterogeneous systems with many different computing units, each with their own characteristics. This trend is a great opportunity for data-base systems to increase the overall performance if the heterogeneous resources can be utilized efficiently. To achieve this, the main challenge is to place the right work on the right computing unit. Current approaches tackling this placement for query processing assume that data cardinalities of intermediate results can be correctly estimated. However, this assumption does not hold for complex queries. To overcome this problem, we propose an adaptive placement approach being independent of cardinality estimation of intermediate results. Our approach is incorporated in a novel adaptive placement sequence. Additionally, we implement our approach as an extensible virtualization layer, to demonstrate the broad applicability with multiple database systems. In our evaluation, we clearly show that our approach significantly improves OLAP query processing on heterogeneous hardware, while being adaptive enough to react to changing cardinalities of intermediate query results.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {733–744},
numpages = {12}
}

@article{GDB-TDBSys-2009,
author = {He, Bingsheng and Lu, Mian and Yang, Ke and Fang, Rui and Govindaraju, Naga K. and Luo, Qiong and Sander, Pedro V.},
title = {Relational query coprocessing on graphics processors},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {0362-5915},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/1620585.1620588},
doi = {10.1145/1620585.1620588},
abstract = {Graphics processors (GPUs) have recently emerged as powerful coprocessors for general purpose computation. Compared with commodity CPUs, GPUs have an order of magnitude higher computation power as well as memory bandwidth. Moreover, new-generation GPUs allow writes to random memory locations, provide efficient interprocessor communication through on-chip local memory, and support a general purpose parallel programming model. Nevertheless, many of the GPU features are specialized for graphics processing, including the massively multithreaded architecture, the Single-Instruction-Multiple-Data processing style, and the execution model of a single application at a time. Additionally, GPUs rely on a bus of limited bandwidth to transfer data to and from the CPU, do not allow dynamic memory allocation from GPU kernels, and have little hardware support for write conflicts. Therefore, a careful design and implementation is required to utilize the GPU for coprocessing database queries.In this article, we present our design, implementation, and evaluation of an in-memory relational query coprocessing system, GDB, on the GPU. Taking advantage of the GPU hardware features, we design a set of highly optimized data-parallel primitives such as split and sort, and use these primitives to implement common relational query processing algorithms. Our algorithms utilize the high parallelism as well as the high memory bandwidth of the GPU, and use parallel computation and memory optimizations to effectively reduce memory stalls. Furthermore, we propose coprocessing techniques that take into account both the computation resources and the GPU-CPU data transfer cost so that each operator in a query can utilize suitable processors—the CPU, the GPU, or both—for an optimized overall performance. We have evaluated our GDB system on a machine with an Intel quad-core CPU and an NVIDIA GeForce 8800 GTX GPU. Our workloads include microbenchmark queries on memory-resident data as well as TPC-H queries that involve complex data types and multiple query operators on data sets larger than the GPU memory. Our results show that our GPU-based algorithms are 2--27x faster than their optimized CPU-based counterparts on in-memory data. Moreover, the performance of our coprocessing scheme is similar to, or better than, both the GPU-only and the CPU-only schemes.},
journal = {ACM Trans. Database Syst.},
month = dec,
articleno = {21},
numpages = {39},
keywords = {Relational database, graphics processors, join, parallel processing, primitive, sort}
}

@article{Ocelot-VLDB-2014,
author = {Bre\ss{}, Sebastian and K\"{o}cher, Bastian and Heimel, Max and Markl, Volker and Saecker, Michael and Saake, Gunter},
title = {Ocelot/HyPE: optimized data processing on heterogeneous hardware},
year = {2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/2733004.2733042},
doi = {10.14778/2733004.2733042},
abstract = {The past years saw the emergence of highly heterogeneous server architectures that feature multiple accelerators in addition to the main processor. Efficiently exploiting these systems for data processing is a challenging research problem that comprises many facets, including how to find an optimal operator placement strategy, how to estimate runtime costs across different hardware architectures, and how to manage the code and maintenance blowup caused by having to support multiple architectures.In prior work, we already discussed solutions to some of these problems: First, we showed that specifying operators in a hardware-oblivious way can prevent code blowup while still maintaining competitive performance when supporting multiple architectures. Second, we presented learning cost functions and several heuristics to efficiently place operators across all available devices.In this demonstration, we provide further insights into this line of work by presenting our combined system Ocelot/HyPE. Our system integrates a hardware-oblivious data processing engine with a learning query optimizer for placement decisions, resulting in a highly adaptive DBMS that is specifically tailored towards heterogeneous hardware environments.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1609–1612},
numpages = {4}
}


@article{Ocelet-VLDB-2013,
author = {Heimel, Max and Saecker, Michael and Pirk, Holger and Manegold, Stefan and Markl, Volker},
title = {Hardware-oblivious parallelism for in-memory column-stores},
year = {2013},
issue_date = {July 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {9},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/2536360.2536370},
doi = {10.14778/2536360.2536370},
abstract = {The multi-core architectures of today's computer systems make parallelism a necessity for performance critical applications. Writing such applications in a generic, hardware-oblivious manner is a challenging problem: Current database systems thus rely on labor-intensive and error-prone manual tuning to exploit the full potential of modern parallel hardware architectures like multi-core CPUs and graphics cards. We propose an alternative design for a parallel database engine, based on a single set of hardware-oblivious operators, which are compiled down to the actual hardware at runtime. This design reduces the development overhead for parallel database engines, while achieving competitive performance to hand-tuned systems.We provide a proof-of-concept for this design by integrating operators written using the parallel programming framework OpenCL into the open-source database MonetDB. Following this approach, we achieve efficient, yet highly portable parallel code without the need for optimization by hand. We evaluated our implementation against MonetDB using TPC-H derived queries and observed a performance that rivals that of MonetDB's query execution on the CPU and surpasses it on the GPU. In addition, we show that the same set of operators runs nearly unchanged on a GPU, demonstrating the feasibility of our approach.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {709–720},
numpages = {12}
}


@inproceedings{crotty2022you,
  author       = {Andrew Crotty and
                  Viktor Leis and
                  Andrew Pavlo},
  title        = {Are You Sure You Want to Use {MMAP} in Your Database Management System?},
  booktitle    = {12th Conference on Innovative Data Systems Research, {CIDR} 2022,
                  Chaminade, CA, USA, January 9-12, 2022},
  publisher    = {www.cidrdb.org},
  year         = {2022},
  url          = {https://www.cidrdb.org/cidr2022/papers/p13-crotty.pdf},
  timestamp    = {Mon, 18 Jul 2022 17:13:00 +0200},
  biburl       = {https://dblp.org/rec/conf/cidr/CrottyLP22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{abadi2006materialization,
  title={Materialization strategies in a column-oriented DBMS},
  author={Abadi, Daniel J and Myers, Daniel S and DeWitt, David J and Madden, Samuel R},
  booktitle={2007 IEEE 23rd International Conference on Data Engineering},
  pages={466--475},
  year={2006},
  organization={IEEE}
}

@misc{wang2024improving,
      title={Improving Text Embeddings with Large Language Models}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Linjun Yang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2401.00368},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.00368}, 
}

@misc{wang2022text,
      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2212.03533},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.03533}, 
}

@misc{llama3modelcard,

title={Llama 3 Model Card},

author={AI@Meta},

year={2024},

url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md},
note = {Accessed: 2024-09-25}

}

@book{external-sort,
    author = {Donald Knuth},
    title = {The Art Of Computer Programming, vol. 3: Sorting And
Searching},
    publisher = {Addison-Wesley},
    year = {1973},
    pages = {391-392}
}

@article{paradis-vldb-2015,
author = {Cho, Minsik and Brand, Daniel and Bordawekar, Rajesh and Finkler, Ulrich and Kulandaisamy, Vincent and Puri, Ruchir},
title = {PARADIS: an efficient parallel algorithm for in-place radix sort},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi-org.proxy.lib.umich.edu/10.14778/2824032.2824050},
doi = {10.14778/2824032.2824050},
abstract = {In-place radix sort is a popular distribution-based sorting algorithm for short numeric or string keys due to its linear run-time and constant memory complexity. However, efficient parallelization of in-place radix sort is very challenging for two reasons. First, the initial phase of permuting elements into buckets suffers read-write dependency inherent in its in-place nature. Secondly, load balancing of the recursive application of the algorithm to the resulting buckets is difficult when the buckets are of very different sizes, which happens for skewed distributions of the input data. In this paper, we present a novel parallel in-place radix sort algorithm, PARADIS, which addresses both problems: a) "speculative permutation" solves the first problem by assigning multiple non-continuous array stripes to each processor. The resulting shared-nothing scheme achieves full parallelization. Since our speculative permutation is not complete, it is followed by a "repair" phase, which can again be done in parallel without any data sharing among the processors. b) "distribution-adaptive load balancing" solves the second problem. We dynamically allocate processors in the context of radix sort, so as to minimize the overall completion time. Our experimental results show that PARADIS offers excellent performance/scalability on a wide range of input data sets.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1518–1529},
numpages = {12}
}

@misc{xu2023seadendtoendtexttosqlgeneration,
      title={SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising}, 
      author={Kuan Xu and Yongbo Wang and Yongliang Wang and Zujie Wen and Yang Dong},
      year={2023},
      eprint={2105.07911},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2105.07911}, 
}

@misc{hui2021improvingtexttosqlschemadependency,
      title={Improving Text-to-SQL with Schema Dependency Learning}, 
      author={Binyuan Hui and Xiang Shi and Ruiying Geng and Binhua Li and Yongbin Li and Jian Sun and Xiaodan Zhu},
      year={2021},
      eprint={2103.04399},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.04399}, 
}

@misc{biswal2024text2sqlenoughunifyingai,
      title={Text2SQL is Not Enough: Unifying AI and Databases with TAG}, 
      author={Asim Biswal and Liana Patel and Siddarth Jha and Amog Kamsetty and Shu Liu and Joseph E. Gonzalez and Carlos Guestrin and Matei Zaharia},
      year={2024},
      eprint={2408.14717},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2408.14717}, 
}

@misc{fan2024surveyragmeetingllms,
      title={A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models}, 
      author={Wenqi Fan and Yujuan Ding and Liangbo Ning and Shijie Wang and Hengyun Li and Dawei Yin and Tat-Seng Chua and Qing Li},
      year={2024},
      eprint={2405.06211},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.06211}, 
}

@misc{golatkar2024cprretrievalaugmentedgeneration,
      title={CPR: Retrieval Augmented Generation for Copyright Protection}, 
      author={Aditya Golatkar and Alessandro Achille and Luca Zancato and Yu-Xiang Wang and Ashwin Swaminathan and Stefano Soatto},
      year={2024},
      eprint={2403.18920},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.18920}, 
}

@misc{rag-databricks,
  author = {Databricks},
  year = 2024,
  title = {RAG (Retrieval Augmented Generation) on Databricks },
  url = {https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html},
  note = {Accessed: 2024-09-25}
}

@misc{fargo-llm-assitant,
  author = {Wells Fargo},
  year = 2024,
  title = {Want to see where your money goes? Just ask Fargo},
  url = {https://sites.wf.com/fargo/},
  note = {Accessed: 2024-09-25}
}

@misc{chase-ai,
  author = {Forbes},
  year = 2024,
  title = {JPMorgan Chase Leads AI Revolution In Finance With Launch Of LLM Suite},
  url = {https://www.forbes.com/sites/janakirammsv/2024/07/30/jpmorgan-chase-leads-ai-revolution-in-finance-with-launch-of-llm-suite/},
  note = {Accessed: 2024-09-25}
}

@misc{tencent-text-2-sql,
author = {Tencent},
  year = 2024,
title = {Bring text-to-sql to BI Production in Large Enterprise},
url = {https://www.databricks.com/dataaisummit/session/bring-text-sql-bi-production-large-enterprise} ,
note = {Accessed: 2024-09-26}
}

@misc{databricks-text-2-sql,
    author = {Databricks},
  year = 2024,
    title = {Improving Text2SQL Performance with Ease on Databricks},
    url = { https://www.databricks.com/blog/improving-text2sql-performance-ease-databricks },
    note = {Accessed: 2024-09-26}
}

@misc{databricks-ai-classify,
    author = {Microsoft},
  year = 2024,
    title = {ai\_classify function documentation},
    url = { https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/ai_classify },
    note = {Accessed: 2024-09-26}

}

@inproceedings{Funke-sigmod18,
author = {Funke, Henning and Bre\ss{}, Sebastian and Noll, Stefan and Markl, Volker and Teubner, Jens},
title = {Pipelined Query Processing in Coprocessor Environments},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3183713.3183734},
doi = {10.1145/3183713.3183734},
abstract = {Query processing on GPU-style coprocessors is severely limited by the movement of data. With teraflops of compute throughput in one device, even high-bandwidth memory cannot provision enough data for a reasonable utilization.Query compilation is a proven technique to improve memory efficiency. However, its inherent tuple-at-a-time processing style does not suit the massively parallel execution model of GPU-style coprocessors. This compromises the improvements in efficiency offered by query compilation. In this paper, we show how query compilation and GPU-style parallelism can be made to play in unison nevertheless. We describe a compiler strategy that merges multiple operations into a single GPU kernel, thereby significantly reducing bandwidth demand. Compared to operator-at-a-time, we show reductions of memory access volumes by factors of up to 7.5x resulting in shorter kernel execution times by factors of up to 9.5x.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1603–1618},
numpages = {16},
keywords = {just-in-time query compilation, massively parallel query processing, olap, operator pipelining, query-coprocessing},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{GHive,
author = {Liu, Haotian and Tang, Bo and Zhang, Jiashu and Deng, Yangshen and Yan, Xiao and Zheng, Xinying and Shen, Qiaomu and Zeng, Dan and Mao, Zunyao and Zhang, Chaozu and You, Zhengxin and Wang, Zhihao and Jiang, Runzhe and Wang, Fang and Yiu, Man Lung and Li, Huan and Han, Mingji and Li, Qian and Luo, Zhenghai},
title = {GHive: accelerating analytical query processing in apache hive via CPU-GPU heterogeneous computing},
year = {2022},
isbn = {9781450394147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3542929.3563503},
doi = {10.1145/3542929.3563503},
abstract = {As a popular distributed data warehouse system, Apache Hive has been widely used for big data analytics in many organizations. Meanwhile, exploiting the massive parallelism of GPU to accelerate online analytical processing (OLAP) has been extensively explored in the database community. In this paper, we present GHive, which enhances CPU-based Hive via CPU-GPU heterogeneous computing. GHive is designed for the business intelligence applications and provides the same API as Hive for compatibility. To run SQL queries jointly on both CPU and GPU, GHive comes with three key techniques: (i) a novel data model gTable, which is column-based and enables efficient data movement between CPU memory and GPU memory; (ii) a GPU-based operator library Panda, which provides a complete set of SQL operators with extensively optimized GPU implementations; (iii) a hardware-aware MapReduce job placement scheme, which puts jobs judiciously on either GPU or CPU via a cost-based approach. In the experiments, we observe that GHive outperforms Hive in both query processing speed and operating expense on the Star Schema Benchmark (SSB).},
booktitle = {Proceedings of the 13th Symposium on Cloud Computing},
pages = {158–172},
numpages = {15},
location = {San Francisco, California},
series = {SoCC '22}
}

@misc{data-parallel,
    author = {wikipedia},
  year = 2024,
    title = {Data parallelism},
    url = {https://en.wikipedia.org/wiki/Data_parallelism},
    note = {Accessed: 2024-09-25}
}

@Manual{cudf, 
      title = {RAPIDS: Libraries for End to End GPU Data Science},
      author = {RAPIDS Development Team},
      year = {2023},
      url = {https://rapids.ai},
    note = {Accessed: 2024-09-30}
}

@misc{thrust,
    title = {Thrust},
    author = {NVidia},
    year = {2024},
    url = {https://developer.nvidia.com/thrust},
    note = {Accessed: 2024-09-30}
}

@inproceedings{GPUQP,
author = {Fang, Rui and He, Bingsheng and Lu, Mian and Yang, Ke and Govindaraju, Naga K. and Luo, Qiong and Sander, Pedro V.},
title = {GPUQP: query co-processing using graphics processors},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247606},
doi = {10.1145/1247480.1247606},
abstract = {We present GPUQP, a relational query engine that employs both CPUs and GPUs (Graphics Processing Units) for in-memory query co-processing. GPUs are commodity processors traditionally designed for graphics applications. Recent research has shown that they can accelerate some database operations orders of magnitude over CPUs. So far, there has been little work on how GPUs can be programmed for heavy-duty database constructs, such as tree indexes and joins, and how well a full-fledged GPU query co-processor performs in comparison with their CPU counterparts. In this work, we explore the design decisions in using GPUs for query co-processing using both a graphics API and a general purpose programming model. We then demonstrate the processing flows as well as the performance results of our methods.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1061–1063},
numpages = {3},
keywords = {query processing, graphics processing units},
location = {Beijing, China},
series = {SIGMOD '07}
}

@article{FlinkCL,
author = {Chen, Cen and Li, Kenli and Ouyang, Aijia and Li, Keqin},
title = {FlinkCL: An OpenCL-Based In-Memory Computing Architecture on Heterogeneous CPU-GPU Clusters for Big Data},
year = {2018},
issue_date = {Dec. 2018},
publisher = {IEEE Computer Society},
address = {USA},
volume = {67},
number = {12},
issn = {0018-9340},
url = {https://doi.org/10.1109/TC.2018.2839719},
doi = {10.1109/TC.2018.2839719},
abstract = {Research on in-memory big data management and processing has been prompted by the increase in main memory capacity and the explosion in big data. By offering an efficient in-memory distributed execution model, existing in-memory cluster computing platforms such as Flink and Spark have been proven to be outstanding for processing big data. This paper proposes FlinkCL, an in-memory computing architecture on heterogeneous CPU-GPU clusters based on OpenCL that enables Flink to utilize GPU's massive parallel processing ability. Our proposed architecture utilizes four techniques: a heterogeneous distributed abstract model (HDST), a Just-In-Time (JIT) compiling schema, a hierarchical partial reduction (HPR) and a heterogeneous task management strategy. Using FlinkCL, programmers only need to write Java code with simple interfaces. The Java code can be compiled to OpenCL kernels and executed on CPUs and GPUs automatically. In the HDST, a novel memory mapping scheme is proposed to avoid serialization or deserialization between Java Virtual Machine (JVM) objects and OpenCL structs. We have comprehensively evaluated FlinkCL with a set of representative workloads to show its effectiveness. Our results show that FlinkCL improve the performance by up to <inline-formula><tex-math notation="LaTeX">$11 times$</tex-math><alternatives> <inline-graphic xlink:href="chen-ieq1-2839719.gif"/></alternatives></inline-formula> for some computationally heavy algorithms and maintains minor performance improvements for a I/O bound algorithm.},
journal = {IEEE Trans. Comput.},
month = dec,
pages = {1765–1779},
numpages = {15}
}

@inproceedings{robroek2024euromlsys,
author = {Robroek, Ties and Yousefzadeh-Asl-Miandoab, Ehsan and T\"{o}z\"{u}n, P\i{}nar},
title = {An Analysis of Collocation on GPUs for Deep Learning Training},
year = {2024},
isbn = {9798400705410},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3642970.3655827},
doi = {10.1145/3642970.3655827},
abstract = {Deep learning training is an expensive process that extensively uses GPUs. However, not all model training saturates modern powerful GPUs. To create guidelines for such cases, this paper examines the performance of the different collocation methods available on NVIDIA GPUs: na\"{\i}vely submitting multiple processes on the same GPU using multiple streams, utilizing Multi-Process Service (MPS), and enabling the Multi-Instance GPU (MIG). Our results demonstrate that collocating multiple model training runs yields significant benefits, leading to up to three times training throughput despite increased epoch time. On the other hand, the aggregate memory footprint and compute needs of the models trained in parallel must fit the available memory and compute resources of the GPU. MIG can be beneficial thanks to its interference-free partitioning but can suffer from sub-optimal GPU utilization with dynamic or mixed workloads. In general, we recommend MPS as the best-performing and most flexible form of collocation for a single user submitting training jobs.},
booktitle = {Proceedings of the 4th Workshop on Machine Learning and Systems},
pages = {81–90},
numpages = {10},
keywords = {MIG, collocation on GPUs, resource-aware deep learning},
location = {Athens, Greece},
series = {EuroMLSys '24}
}

@article{cao2024vldb,
author = {Cao, Jiashen and Sen, Rathijit and Interlandi, Matteo and Arulraj, Joy and Kim, Hyesoon},
title = {GPU Database Systems Characterization and Optimization},
year = {2023},
issue_date = {November 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3632093.3632107},
doi = {10.14778/3632093.3632107},
abstract = {GPUs offer massive parallelism and high-bandwidth memory access, making them an attractive option for accelerating data analytics in database systems. However, while modern GPUs possess more resources than ever before (e.g., higher DRAM bandwidth), efficient system implementations and judicious resource allocations for query processing are still necessary for optimal performance. Database systems can save GPU runtime costs through just-enough resource allocation or improve query throughput with concurrent query processing by leveraging new GPU resource-allocation capabilities, such as Multi-Instance GPU (MIG).In this paper, we do a cross-stack performance and resource-utilization analysis of four GPU database systems, including Crystal (the state-of-the-art GPU database, performance-wise) and TQP (the latest entry in the GPU database space). We evaluate the bottlenecks of each system through an in-depth microarchitectural study and identify resource underutilization by leveraging the classic roofline model. Based on the insights gained from our investigation, we propose optimizations for both system implementation and resource allocation, using which we are able to achieve 1.9x lower latency for single-query execution and up to 6.5x throughput improvement for concurrent query execution.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {441–454},
numpages = {14}
}