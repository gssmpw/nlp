\section{Background and Motivation} \label{section:background_motivation}
This section discusses the effort of accelerating database workloads with GPUs systems, challenges faced by existing approaches, and improvement opportunities exploited by our design.

\subsection{Background: Capacity-limited GPU Memory}
Data analytics is a complex and resource-intensive task that involves processing large volumes of data, necessitating substantial computational power. 
The massively parallel architecture of GPUs presents a compelling option for data analytics. 
However, a significant limitation is their on-device memory capacity. 
For instance, next-generation high-end GPUs offer up to 192 GB of memory per card, which is considerably less than the 6 TB maximum DRAM supported by CPUs~\cite{mi300x, genoa}. 
The distinct characteristics of GPU and CPU memory technologies, such as HBM versus DDR, impose physical constraints that make scaling GPU memory capacity challenging. 
Consequently, the disparity between CPU and GPU memory capacities is likely to persist, posing a crucial design question: \textit{How can we utilize the high computational bandwidth of GPUs for data analytics given the limited GPU memory available?}

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
Data analytics is time-consuming because we need to process a lot of data.
While GPUs provide high compute throughput and memory bandwidth compared to CPUs, their memory is much smaller and more expensive than CPUs.
The next-generation high-end GPUs contain up to 192 GB of memory per card, but it pales in comparison to the 6TB maximum DRAM a CPU can support.
On GPUs, a memory controller connects to a single GDDR or HBM chip, where each chip typically contains 4GB (GDDR6X) or 16GB (HBM3 with 8 layers).
Due to physical limitations, each GPU die can only connect to a limited number of DRAM chips (e.g. H100 GPU die only connects to 5 16GB HBM3 dies), capping the maximum amount of DRAM each GPU die can use.
On the other hand, each CPU memory controller connects to a channel with two DDR DIMM modules, each with 256GB, and today's CPU can contain 8-12 memory controllers.
As a result of how GPUs and CPUs organize memory, the gap in memory capacity is unlikely to be closed in the future.
This creates a dilemma for GPU-based data analytics.
Despite the high compute throughput provided by a GPU die, it only has access to a small piece of data.
On the other hand, we want to accelerate data analytics because we want to process big data.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

\subsection{Background: Prior Attempts in Processing Large Datasets using GPUs}

% \textcolor{teal}{Lin: I think the flow here needs some revision. It seems that we're mixing up existing work and possible solutions. Have people proposed multi-GPU databases in the literature? Have people proposed co-processor models? We don't need to discuss anything that has not been proposed in the literature. If I recall correctly, there seem to be three types of GPU databases: (1) Only process data up to GPU memory limit, (2) use GPU memory as a cache, and (3) hybrid model where some operators run on GPU, and others run a GPU. We only need to discuss why these existing approaches are not satisfactory.
% \break
% Also, I don't think we need to get into our solution here either. That can come later after we discuss the GPU evolutions.}

While many prior proposals focus on optimizing for the case where the entire dataset can be stored in GPU memory~\cite{tcudb-sigmod-2022, tqp-vldb-2022, crystal-sigmod-20}, they fall short to effectively handle larger datasets due to GPU memory capacity limit.
% This makes them struggle to cope with the large amounts of data common in the real world.
This significantly impairs their ability to handle the vast volumes of data typical in real-world applications.
Prior research explores two approaches to accelerate data analytics with GPUs: (1) utilizing multiple GPUs to increase the size of GPU-side memory and (2) using CPU-side memory to hold data.

\noindent
\textbf{Utilizing multiple GPUs.}
High-bandwidth inter-GPU interconnects enable the treatment of memory in multiple GPUs as a unified, large memory pool, thereby allowing for horizontal scaling of GPU memory capacity. 
Systems can take advantage of this fact to hold more data in GPU-side memory~\cite{heavyai, mg-join-sigmod-2021, multi-gpu-sort-sigmod-2022}.
However, this approach is not without drawbacks. 
It necessitates the acceptance of bundled GPU compute resources when the primary issue is memory capacity. 
Such an inflexible strategy may lead to resource under-utilization and consequently increase overall costs.
Additionally, there is a limit to the number of GPU cards a single node can support, typically up to 8, which means the total aggregated memory capacity remains significantly less than what CPUs can provide.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
The high-bandwidth GPU interconnects allow treating the memory in multiple GPUs as a big memory pool, and we can thus scale the amount of memory each GPU has access to horizontally.
However, it is not a perfect solution because we have to accept the bundled GPU compute resources while capacity is the main problem, resulting in higher total costs.
Plus, there is a limit to the maximum number of cards each node can support, usually 8, so the aggregated memory capacity is still far less than what the CPU can have.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

\noindent
\textbf{Using CPU-side memory to hold data.}
This category includes the systems that target CPU-side DRAM to hold large amounts of data. 
Such systems typically adopt a CPU-GPU hybrid solution that utilizes both GPU cores and CPU cores for high performance.
To keep GPU busy, some of them~\cite{hetexchange-vldb-2019, HERO-VLDB-2017, GDB-TDBSys-2009}
choose to stream data from CPU memory to GPU through the CPU-GPU PCIe link.
Identifying the limited bandwidth of PCIe links as a major bottleneck, some other works opt to place part of the data in GPU memory and the rest in CPU memory~\cite{mordered-vldb-2022, Ocelot-VLDB-2014, Ocelet-VLDB-2013}.
The GPU can first process its local data and then handle the work streamed from the PCIe link, which reduces the amount of data that goes through the narrow off-device IO link.
However, we argue that CPU-GPU IO is still a fundamental issue for such solutions.
While the GPU is much faster than the CPU, it has access to less data due to its smaller memory capacity, which creates load imbalance between the CPU and the GPU.
As a result, after the GPU finishes processing its small portion of data, such a solution has to stream data through the PCIe link to keep the GPU running. 
The low IO bandwidth makes it hard to keep up with the speed GPU ingests data.

% \textit{In this work, we target solving the IO bottleneck for single GPU query execution by taking advantage of multi-GPU systems' IO topology with a novel way of programming.}
% \textcolor{blue}{We consider an extreme case where inputs and outputs need to be in DRAM but only using GPU to process the data}.
% \textcolor{red}{Sometime, people turn to hybrid solution not because they want to improve an already very good GPU-only out-of-core processing system, but because this is the only way they can achieve better results than a highly optimized CPU solution while keeping the GPU within the context (e.g. Proteus).}








% However, GPU memory only has limited capacity, preventing such kind of proposals from processing large amounts of data.
% \begin{enumerate}
%     \item Compared to CPU DIMM, GPU memory capacity is small and it is limited by JEDEC standards.
%     (a) \textbf{a table} compares GPUs' memory capacity with CPUs'
%     \item Unlike DIMM, the user cannot scale GPU memory independent from GPU compute resources. 
%     (\textbf{a figure} comparing how GPU connects to memory and CPU connects to memory)
%     Despite the similar price of GDDR and DDR chips, the GPU die area and compute power bundled with GPU memory means users need to pay more money.
% \end{enumerate}

% talk about why capacity is a problem for GPU DB.

% Another option is to spill part of the data to CPUs' DRAM and bring it to the GPU on-demand.
% However, this approach suffers from the limited single GPU IO bandwidth.
% include the AWS figure here.
% Motivate the paper from a cost perspective.

% Mention the mem / transistor argument

% \textbf{The most cost efficient way to use GPU memory is only to use it as a scratchpad for execution.}
% \textbf{Otherwise, you pay money for the idle transistors}

% \subsection{The implication to GPU-native execution model}
% Given the high price, it is too costly to hold the data in GPU memory when there is no requests, which leaves the bundled compute transistor idle.
% Thus, we cannot charge at per service level granularity.

% \subsection{The implication to the hybrid execution model }

% The main idea is to process data in place (on the CPU) to reduce the amount of traffic going through PCIe links.
% However, given the proportional size of DRAM and GPU memory, it means the weaker CPU side will have more data but the GPU side will have smaller amount of data.
% This imbalance makes CPU-GPU communication inevitable, and the IO will still be a bottleneck.
% Notable, when the size of CPU side DRAM is much larger than GPU side memory or caching is disabled, GPU will quickly finish processing the small piece of data located in GPU, and GPU's execution model is reduced to a co-processor execution model.
% Therefore, although the techniques discussed in the work is built upon the co-processor model, it can benefit hybrid execution model as well.

% Some hybrid GPU-CPU DB solutions also adopt some general purpose optimizations like segment skipping, which has been widely incorporated into existing DB systems, like parquet.
% Such optimization is orthogonal to the focus of this work as it reduces the amount of traffic being transferred from the DRAM data store at an algorithm level, and thus can speedup any specific implementations. 

\begin{figure}
    \centering
    \includegraphics[width=0.43\textwidth, trim={0.25cm 0.0cm 0.25cm 0.05cm}, clip]{figures/architecture_evolution.pdf}
    \caption{Evolution of GPU system topology.}
    \label{fig:gpu-sys-arch}
\end{figure}

\subsection{Background: Evolution of GPU Systems}
To understand the CPU-GPU data transfer bottleneck and identify new optimization opportunities, it is essential to examine the evolution of GPU system hardware architecture. 
Figure~\ref{fig:gpu-sys-arch}(a) illustrates a classic single GPU system topology, which aligns with conventional understanding. 
In this configuration, the GPU is connected to the CPU via a single PCIe link, while the CPU connects to its memory through DDR channels. 
Considering the common standards of PCIe 4.0 and DDR4-3200, which are prevalent in contemporary systems, the CPU typically has eight memory channels. 
The PCIe link provides approximately 28GB/s bandwidth in one direction and up to 56GB/s in both directions due to its full-duplex nature. 
However, this bandwidth is significantly lower than the 150GB/s bandwidth achievable by the CPU, thereby making CPU-GPU data transfer a critical bottleneck.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
To understand the CPU-GPU data transfer bottleneck and uncover new optimization opportunities, we first describe how the hardware architecture of GPU systems has evolved.
Figure~\ref{fig:gpu-sys-arch}(a) shows a classic single GPU system topology that matches the conventional wisdom.
Here, the GPU connects to the CPU with a single PCIe link; the CPU connects to the memory through DDR channels.
Assuming common standards of PCIe 4.0 and DDR4-3200, widespread in systems today, CPU has 8 memory channels.
The PCIe link can deliver about 28GB/s bandwidth in one direction and 56GB/s bandwidth if there is traffic in both directions, due to its full-duplex property.
This is far less than 150GB/s bandwidth the CPU can achieve, making CPU-GPU data transfer a bottleneck.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

To further scale the computing power using multiple GPUs, Figure~\ref{fig:gpu-sys-arch}(b) depicts another system topology found in instances such as AWS p3 and p4~\cite{aws-p3-topo, aws-p4-topo}. 
Due to the limited number of PCIe lanes in older generations of CPUs, supporting four GPUs requires connecting to the CPU through PCIe switches. 
In this configuration, each pair of GPUs shares a PCIe x16 link via these switches, preventing concurrent communication with the CPU at full bandwidth. 
Consequently, the bandwidth between the CPU and GPUs is limited to 56GB/s unidirectionally and 112GB/s bidirectionally. 
While the bidirectional bandwidth approaches the 150GB/s achievable by CPU DRAM, the unidirectional bandwidth remains insufficient by comparison.
In addition to the PCIe links between CPUs and GPUs, modern systems also feature point-to-point high-speed communication between multiple GPUs via technologies such as NVLink (NVIDIA) and Infinity Fabric (AMD).

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
Later, people include multiple GPU cards in each machine to further scale the computing capacity.
Figure~\ref{fig:gpu-sys-arch} (b) presents an old multi-GPU system's architecture, which can be found on machines like AWS p2/3/4.
Older-generation CPUs usually have a limited number of PCIe lanes.
To support 4 GPUs, PCIe switches are necessary.
Each pair of two GPUs shares a PCIe x16 link through PCIe switches, preventing them from concurrently communicating with the CPU at full bandwidth.
In such architecture, the bandwidth between CPU and GPU is only $\sim$56GB/s unidirectionally and $\sim$112GB/s bidirectionally.
While the bidirectional bandwidth is close to $\sim$150GB/s CPU DRAM bandwidth, the unidirectional bandwidth is still incomparable.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

More recently, server-class CPUs such as AMD's Milan, Rome, and Genoa have begun supporting more PCIe lanes, leading to an architecture where each GPU is directly connected to the CPU through separate PCIe links, as shown in Figure~\ref{fig:gpu-sys-arch}(c).
With all four GPUs connected to the CPU, the aggregated unidirectional PCIe bandwidth is comparable to CPU DRAM bandwidth, and the bidirectional bandwidth even exceeds CPU DRAM bandwidth. 
\textit{Given that CPUs often struggle to fully utilize their DRAM bandwidth, it is time to re-evaluate the assumption that CPU-GPU communication bandwidth is always a bottleneck.}

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
In recent years, server-class CPUs (AMD Milan, Rome, Genoa) have started to provide more PCIe lanes, resulting in an architecture where GPUs directly connect to the CPU though separate PCIe links, as illustrated in Figure~\ref{fig:gpu-sys-arch} (c).
When all four GPUs are connected to CPUs, the aggregated unidirectional PCIe bandwidth is comparable to CPU DRAM bandwidth, and the bidirectional bandwidth even surpasses the CPU DRAM bandwidth.
Given that it is usually difficult for CPUs to fully utilize the DRAM bandwidth, it is time to re-evaluate the assumption that CPU-GPU communication bandwidth is always a bottleneck. 
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

% \subsection{The Opportunity to scale GPU IO resources independently from compute resources}
\subsection{Opportunity: Scaling GPU IO Resources Independently from Compute} \label{subsection:scaling_gpu_io_independently}

Following the topology in Figure~\ref{fig:gpu-sys-arch}(c), a straightforward method to increase CPU-GPU communication bandwidth is to use all GPUs simultaneously. 
However, this approach falls short in resolving the IO bottleneck since it scales both IO and computational power equally. 
The GPU still processes data faster than it can be transferred, maintaining the IO as a bottleneck. 
%\textit{This suggests that an ideal solution would leverage the bandwidth of all the PCIe links while utilizing only one GPU for computation.}

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
A straightforward way to increase the CPU-GPU communication bandwidth is to use all the GPUs together.
However, it cannot solve the IO bottleneck because it scales both IO and compute power equally.
The GPU side still consumes data faster than it transfers, keeping IO a bottleneck.
\textit{This suggests that an ideal solution is to take the bandwidth of all the PCIe links but only use one GPU to do the computation.}
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

% \subsubsection{The idle IO resources in GPU running compute-intensive workloads}

Our \textit{key observation} is that, unlike data analytics, not all GPU workloads are IO resource-bound. 
For example, modern AI tasks, such as Large Language Model (LLM) inference, are primarily compute-bound and rarely require substantial CPU-GPU data transfer bandwidth. 
Besides, there is a recent trend that data processing platforms not only host classic data analytics services but also AI applications to support users' AI-empowered workflow~\cite{biswal2024text2sqlenoughunifyingai, fan2024surveyragmeetingllms, golatkar2024cprretrievalaugmentedgeneration,xu2023seadendtoendtexttosqlgeneration, hui2021improvingtexttosqlschemadependency} or diverse computation needs that mix AI and data analytics demands from multiple users~\cite{chase-ai, fargo-llm-assitant}.
% Our \textit{key observation} is that, unlike data analytics, not all GPU workloads are IO resource bound. 
% For example, modern AI tasks, such as Large Language Model (LLM) inference, are primarily compute-bound and rarely require substantial CPU-GPU data transfer bandwidth. 
% {\color{blue}
% Unlike data analytics, modern AI tasks are primarily compute-bound and rarely require substantial CPU-GPU data transfer bandwidth.}

We aim to exploit the increasingly prevalent multi-GPU systems to more efficiently serve this trend in the future.
We observe a \textbf{\textit{unique opportunity}} to re-distribute the IO resources with multiple GPUs that serve hybrid data analytics and AI workloads. 
Such a system can dedicate the compute of a small number of GPUs to IO-bound data analytics workloads using all GPUs' IO bandwidth. 
It can then use the compute of the remaining GPUs for compute-bound AI workloads while utilizing their IO resources as well.

\textit{In this work, we address the IO bottleneck for GPU-accelerated data analytics by utilizing the compute resources of one GPU and the IO resources of all GPUs in a multi-GPU system.} 
Our approach targets a ``cold-start'' scenario for the GPU - all data resides on CPU memory before the execution starts, therefore the utilization of GPU is not constraint by GPU memory size.
Nevertheless, such an approach may also generalize to more than one GPU for data analytics in other settings, which we leave as future work.

%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
Many GPU applications are not thirsty for higher CPU-GPU IO bandwidth like query processing.
The main driving force for the evolution of GPUs is the high computation required by Deep Learning (DL) applications.
The IO resources of the GPUs running DL applications are largely left idle, which can be used to scale the IO throughput for other GPUs.
We can collocate the IO-bound GPU data analytics applications with the compute-intensive DL applications in a multi-GPU system, and \textit{redistribute the idle IO resources on the GPUs that running DL applications to the GPU running data analytics}.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%

\subsection{Background: Data Transfer Modes on GPU}
\label{sec:SDMA}
GPUs have two primary methods for accessing off-device data: (1) System Direct Memory Access (SDMA) engines and (2) zero-copy memory access.
SDMA is a dedicated hardware component separate from the compute units, responsible for performing data transfers. 
It operates using \texttt{Memcpy*()} API calls in CUDA/HIP to initiate I/O operations, without affecting the ongoing compute tasks on the GPU. 
However, SDMA incurs a fixed overhead for transfer initiation and typically requires a coarse-grained transfers for efficiency, usually on the order of 10s of megabytes.
In contrast, zero-copy memory access allows compute units to directly access CPU-side data at cache line granularity (typically 64/128 bytes). 
% To utilize this method, the GPU kernel must include statements that reference CPU memory addresses.

\noindent
\textbf{Terminology.}
In the remainder of the paper, the following terminology will be used.
A multi-GPU system is conceptualized as being divided into two sides by the PCIe links.
The side containing the GPUs is referred to as the \textit{Device} side.
The side containing the CPU and DRAM is referred to as the \textit{Host} side.
The direction of data transfer from the Host to the Device is abbreviated as \textit{H2D}.
The direction of data transfer from the Device to the Host is abbreviated as \textit{D2H}.
We use the term \textbf{on-core} to refer to concepts that apply to the case where all data fits GPU memory, and \textbf{out-of-core} for the cases where the dataset is larger than GPU memory.


%%%%%%%%%%%% OLD TEXT START %%%%%%%%%%%%
\begin{comment}
There are two ways for GPUs to access off-device data.
GPUs primarily use System Direct Memory Access (SDMA) engines, pieces of hardware separate from the compute units, to perform off-device IO operations.
Such IO operations are invoked by \texttt{Memcpy*()} API calls in CUDA/HIP and do not influence the kernel compute activities, except using a fraction of GPU memory bandwidth.
However, it takes some fixed overhead to launch SDMA transfer, so efficient SDMA transfer needs to be coarse-grained, usually at least tens of MB.
When the applications on GPUs do not perform CPU-GPU IO operations, both the SDMA engines and PCIe links are idle.

On the other hand, zero-copy memory access allows the compute units to directly access CPU-side data at cache line granularity. 
To use it, the kernel launched to the GPUs needs to contain statements that access addresses belonging to the CPU.
\end{comment}
%%%%%%%%%%%% OLD TEXT END %%%%%%%%%%%%



\begin{comment}
\subsection{Vision - \textcolor{red}{DISCUSS THIS}}
\textcolor{teal}{Lin: I actually don't think we need to have this subsection. But we can discuss}
\begin{itemize}[leftmargin=*]
    \item With the help of IO redistribution, GPU-CPU IO is no longer a bottleneck.
    \item With our technique, GPU can very efficiently use CPU side DRAM bandwidth. 
    Because there is little bandwidth the CPU cores can use further, the meaning of CPU/GPU hybrid execution becomes marginal.
    GPU-only execution can achieve satisfying result and only need to maintain one set of code base.
    Hybrid execution need to main both high-performance CPU code and high-performance GPU code, and the way to achieve high-performance on CPU and GPU is very different.
    Also, such a strategy usually results in unique design, prevent code reuse.
\end{itemize}


\subsection{Challenges - \textcolor{red}{DISCUSS THIS}}

\begin{enumerate}[leftmargin=*]
    \item It is non-trivial to carry out our novel IO redistribution idea using existing GPU runtime and hardware due to a bunch of nonidealities we will explain in \S~\ref{sec:io-challenges}.
    Naive solutions cannot saturate the underlying hardware.
    \item To bypass the IO bottlenecks, prior works provide ad-hoc solutions that 
    \begin{itemize}[leftmargin=*]
        \item  aa
        \item  bb
    \end{itemize}
    \item no example applications are using our novel IO redistribution idea, so the performance characteristics of applications using this new scheme are unclear.
\end{enumerate}
\end{comment}