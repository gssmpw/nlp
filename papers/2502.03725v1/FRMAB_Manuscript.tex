%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

\RequirePackage{amsthm}


\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
% \usepackage{algorithm}%
% \usepackage{algorithmicx}%
% \usepackage{algpseudocode}%
\usepackage{listings}%

%%%%%Mine Starts Here
\usepackage{lmodern}
     % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{bbm}
\usepackage{physics}
\usepackage{bm}
\usepackage[short]{optidef}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand{\RomanNumeralCaps}[1]
    {\MakeUppercase{\romannumeral #1}}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
% \usepackage{array}
% \usepackage{graphicx}
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage[colorinlistoftodos]{todonotes}

% % \newtheorem{theorem}{Theorem}[section]
% % \newtheorem{lemma}[theorem]{Lemma}
% % \newtheorem{corollary}{Corollary}[theorem]
% % \newtheorem{assumption}{Assumption}[theorem]

\usepackage{tikz}
\usepackage{cases}
%\allowdisplaybreaks 
%\usepackage{amsthm}
\usepackage{xfrac}
\usepackage{acronym}
% \usepackage{rotating}
% \usepackage{pdflscape}

\usepackage{algorithmic}
\usepackage[ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output} 
\usepackage{mathtools}



% % Private macros here (check that there is no clash with the style)

% \usepackage{enumitem}


%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below


\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
\newtheorem{example}{Example}%
\newtheorem{assumption}{Assumption}%
\newtheorem{lemma}{Lemma}%

\theoremstyle{thmstylethree}%
\newtheorem{remark}{Remark}%


\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Optimal Control of Fluid Restless Multi-armed
Bandits: A Machine Learning Approach]{Optimal Control of Fluid Restless Multi-armed
Bandits: A Machine Learning Approach}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Dimitris} \sur{Bertsimas}}\email{dbertsim@mit.edu}
\equalcont{These authors contributed equally to this work.}

\author[2]{\fnm{Cheol Woo} \sur{Kim}}\email{acwkim@mit.edu}
\equalcont{These authors contributed equally to this work.}

\author[3]{\fnm{Jos\'e} \sur{Ni\~no-Mora}}\email{jose.nino@uc3m.es}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Sloan School of Management}, \orgname{Massachusetts Institute of Technology}, \orgaddress{\city{Cambridge}, \postcode{02142}, \state{MA}, \country{USA}}}

\affil[2]{\orgdiv{Operations Research Center}, \orgname{Massachusetts Institute of Technology}, \orgaddress{\city{Cambridge}, \postcode{02142}, \state{MA}, \country{USA}}}

\affil[3]{\orgdiv{Department of Statistics}, \orgname{Carlos \RomanNumeralCaps{3} University of Madrid}, \orgaddress{\city{Getafe}, \postcode{28903}, \state{Madrid},  \country{Spain}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{We propose a machine learning approach to the optimal control of fluid restless multi-armed bandits (FRMABs) with state equations that are either affine or quadratic in the state variables. By deriving fundamental properties of FRMAB problems, we design an efficient machine learning based algorithm. Using this algorithm, we solve multiple instances with varying initial states to generate a comprehensive training set. We then learn a state feedback policy using Optimal Classification Trees with hyperplane splits (OCT-H). We test our approach on machine maintenance, epidemic control and fisheries control problems. Our method yields high-quality state feedback policies and achieves a speed-up of up to 26 million times compared to a direct numerical algorithm for fluid problems.}



\keywords{Restless Multi-Armed Bandit, Fluid Approximation, Optimal Control, Decision Trees}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}
\label{sec:intro}

We study the continuous-time deterministic formulation of restless multi-armed bandit problems, referred to as fluid restless multi-armed bandit (FRMAB) problems. Restless multi-armed bandits, introduced by \citep{whit88}, are stochastic control problems that model sequential resource allocation problems across multiple projects, where each project's state evolves stochastically even when no effort is allocated to it. This model has numerous real-world applications, including healthcare \citep{health}, machine maintenance \citep{abbuMakis19}, and wireless communication \citep{wireless}, among many others. See also the recent survey \citep{nmmath23}.

However, solving restless multi-armed bandit problems to optimality is known to be at least PSPACE-hard in general \citep{paptsi99}. A common approach is to use the Whittle index policy \citep{whit88} as a heuristic. However, even this approach often faces scalability issues. As a result, many studies impose specific assumptions on the problem structure \citep{mmg18, tm19} or propose learning-based approaches to approximate the Whittle index \citep{fnmt19, nghhs24}.

To address the complexity of solving stochastic control problems, their deterministic approximations are commonly explored. A notable example is in the control and analysis of multiclass queueing networks. It has been shown that  the stability of a fluid queueing network implies the stability of its stochastic counterpart \citep{dai1995, Stolyar1995}. Fluid queueing networks also provide a useful framework for building control policies for their stochastic counterparts, often demonstrating strong empirical performance and asymptotic optimality properties  \citep{Mag1999, Mag2000, robustfluid}. See also the book by \citet{queuebook}.

% On the other hand, continuous-time deterministic optimal control problems have historically been proven to be useful to model numerous real-world problems in engineering \citep{optbook2}, natural and social sciences \citep{optbook1, grassetal08}  and operations research \citep{Sethi2019}.  

In the restless multi-armed bandit literature, \cite{larn16} explores fluid approximations for a specific class of problems. They demonstrate that the fluid policy shows strong empirical performance for the associated stochastic problems. However, their analysis relies on strong assumptions about the problem structure and the number of projects. They also relax the coupling resource constraint, a fundamental aspect of restless bandits. Generally, solving optimal control problems without such assumptions or relaxations is computationally challenging. This challenge is particularly relevant in the scenarios where optimal control problems with varying initial states need to be solved repeatedly, a common situation in real-world applications.

Recently, many learning-based approaches have been proposed to overcome the complexity of solving optimal control problems. \citep{LEE2021109421, pmlr-v120-kim20b, lutter23} develop reinforcement learning methods, while \citep{bertsimas2023optimalcontrolmulticlassfluid} propose using a decision tree algorithm, Optimal Classification Trees with Hyperplane Splits (OCT-H) \citep{OCT}, to solve fluid queueing network control problems. 

In this work, we propose a machine learning approach to solve FRMAB problems using OCT-H. Classification tree algorithms are particularly appealing for learning optimal policies in many continuous-time optimal control problems. These problems often have piecewise constant optimal policies, which decision trees with hyperplane splits can effectively learn \citep[Chapters 4 \& 8 in][]{Meyn2007, bertsimas2023optimalcontrolmulticlassfluid}.

Our approach is similar to the class of methods known as imitation learning in the reinforcement learning literature. We generate multiple control instances and solve them using numerical algorithms to generate training data. Then, we use supervised learning algorithm, OCT-H, to imitate the state-control mapping in the optimal trajectories.

\subsubsection*{\textbf{Notational Conventions}} Throughout this paper, we use boldface letters to denote vectors and matrices. The $i_{th} $ entry of a vector $\bm{x}$ is denoted $x_i$. We use $\bm{0}$ to denote the vector of zeros. We use $x(\cdot)$ to denote a real-valued function, and $\bm{x}(\cdot)$ to denote a vector whose entries are real-valued functions. We use $\bm{x}$ instead of $\bm{x}(\cdot)$ when it is clear from the context that $\bm{x}$ is referring to a vector of functions. 


\subsection{Problem Formulation}
\label{s:formulation}

We consider a FRMAB model with $n$ projects with finite time horizon $T < \infty$. Project $i \in [n]$ has state $x_i(t)$ at time $t \geqslant 0,$ moving over the open state space $\mathcal{X}_i \triangleq (0, H_i)$ with $H_i \leqslant \infty$. We write the system state as $\bm{x}(t) = (x_i(t))_{i=1}^n,$ which belongs to the state space $\mathcal{X} \triangleq \prod_{i=1}^n \mathcal{X}_i$.
At each time $t$ the system controller chooses a control $\bm{u}(t) = (u_i(t))_{i=1}^n \in [0, 1]^n$ where $u_i(t) \in [0, 1]$, which is required to be piecewise continuous, models the level of effort allocated to project $i$. The values $1$ and $0$ represent  ``full effort''  and ``least effort'' levels, respectively.
At most $m < n$ projects can be set at each time $t$ to the  ``full effort'' level, so  we have  the  coupling resource constraints $\sum_{i=1}^n u_i(t) \leqslant m$.

The state evolution of  project $i$ follows first-order autonomous ordinary differential equation (ODE) referred to as the state equation: at all
times $t$ where $\bm{u}(\cdot)$ is continuous, for given continuously differentiable functions $\phi_{0,i}(\cdot)$ and $\phi_{1,i}(\cdot),$ 
$$
\dot{x}_i(t) = u_i(t) \phi_{1,i}(x_i(t)) + (1 - u_i(t)) \phi_{0,i}(x_i(t)).
$$
$\phi_{1,i}(\cdot)$ and $\phi_{0,i}(\cdot)$  represent the state equations when $u_i(t) = 1$ and $u_i(t) = 0$, respectively. Similarly, the instantaneous reward rate earned by project $i$ at each time $t$
depends on its current state and control, and is given by $u_i(t)R_{1,i}(x_i(t)) + (1 - u_i(t))R_{0,i}(x_i(t))$ for given continuously differentiable functions $R_{0,i}(\cdot)$ and $R_{1,i}(\cdot)$. 

For a given initial state $\bm{x}_0$, the general FRMAB problem described above can be formulated as the following optimal control problem:
\begin{equation}
\begin{aligned}
\label{eq:genrmabp}
&\max_{\bm{u}(\cdot), \bm{x}(\cdot)} &&\int_0^T \sum_{i=1}^n \bigg[u_i(t) R_{1,i}(x_i(t)) + (1 - u_i(t)) R_{0,i}(x_i(t)) \bigg] \, dt \\
&\text{s.t.} \quad && \dot{x}_i(t) = u_i(t) \phi_{1,i}(x_i(t)) + (1 - u_i(t)) \phi_{0,i}(x_i(t)) , \quad  \forall i \in [n], \forall t \in [0,T], \\
& \qquad \ &&0 < x_i(t) < H_i, \quad \forall i \in [n], \forall t \in [0,T],  \\
& \qquad \ &&\bm{x}(0) = \bm{x}_0, \\
& \qquad \ &&0 \leqslant u_i(t) \leqslant 1, \quad \forall i \in [n], \forall t \in [0,T],  \\
& \qquad \ &&\sum_{i=1}^n u_i(t) \leqslant m, \quad \forall t \in [0,T].
\end{aligned}
\end{equation}

Specifically, in this work, we focus on two fundamental cases that capture many important real-world problems. In the first case, we assume that both the state equations and the reward functions are affine in the state variable: $\phi_{u,i}(x) = \alpha_{u,i} + \beta_{u,i}x, R_{u,i}(x) = r_{u,i}x - c_{u,i},  u \in \{0,1\}$, In the second case, we assume that the state equations are quadratic (without the intercept) and the reward functions are affine in the state variable: $\alpha_{u,i}x + \beta_{u,i}x^2 ,\alpha_{u,i} \neq 0, \beta_{u,i} \neq 0, R_{u,i}(x) = r_{u,i}x - c_{u,i},  u \in \{0,1\}, i \in [n]$. 

\textcolor{black}{Here, we assume that the state equations for the quadratic dynamics do not include intercepts in $\phi_{u,i}$. Our derivation relies on closed-form expressions of state trajectories within intervals where the control variable is fixed to a constant vector. This assumption simplifies these expressions, allowing us to present the key ideas and results more concisely. However, the principles of our approach can be extended to more general state equations.} We also add the following assumption:
\begin{assumption}
\label{ass:concave}
$\phi_{u,i}(\cdot)$ is a concave function for all $u \in \{0, 1\}, i \in [n].$
\end{assumption}

\noindent Note that the models with affine dynamics automatically satisfies this assumption. In the quadratic case, this assumption requires $\beta_{u,i} <0,  u \in \{0,1\}, i \in [n]$. This condition ensures that the algorithm developed in Section \ref{s:affineacot} finds optimal solutions for Problem \eqref{eq:genrmabp}.

In general, enforcing the state constraints such as $0 < x_i(t) < H_i$ makes the problem considerably more challenging to solve. Fortunately, many important problems automatically satisfy these upper and lower bound constraints without explicit enforcement. This is because the state equation often guarantees that the state trajectory remains within a bounded interval, due to the existence of equilibrium points in dynamical systems \citep[Chapter 5-11]{Sethi2019}. Therefore, we focus on the class of problems where we can treat the problem as if state constraints do not exist.

An optimal solution to Problem (\ref{eq:genrmabp}) is given as $$\Big\{\big( \bm{x}^*(t),\bm{u}^*(t)\big): t \in [0,T], \bm{x}^*(0) = \bm{x}_0 \Big\},$$ 
which is a pair of state and control trajectories starting from a fixed initial state $\bm{x}_0$. However, in practice, one often needs to resolve Problem \eqref{eq:genrmabp} with different initial states repeatedly. Therefore, the mapping from any state $\bm{x} \in \mathcal{X}$ to its associated optimal control $\bm{u} \in [0,1]^{n}$ is more useful than a single pair of trajectories from a specific initial state. This mapping is referred to as a state feedback policy, which is typically time-dependent because Problem \eqref{eq:genrmabp} is a finite horizon problem and an optimal stationary policy usually does not exist. Computing such a policy, however, is generally considered very challenging. The goal of this work is to propose using OCT-H to learn a time-dependent state feedback policy
$$
\pi: \mathcal{X} \times [0,T] \mapsto [0,1]^{n}.
$$

\subsection{Contributions}
\label{s:contri}

The contributions of the paper are as follows.
\begin{enumerate}
    \item We initiate the study of fluid restless multi-armed bandits where the state equations are either affine or quadratic in the state. We derive fundamental properties of these systems and use them to efficiently implement a numerical solution algorithm known as the shooting method  \citep{StoerJ2013ItNA}.
    \item We propose the use of the decision tree algorithm, OCT-H, to learn a state feedback policy. To address potential nonlinearities in the training data, we leverage the structural properties of FRMAB problems, developing an efficient technique for nonlinear feature augmentation.
    \item We test our approach on machine maintenance, epidemic control, and fisheries control problems, demonstrating that it produces high-quality feedback policies for these applications.
    \item We show that once a policy is learned, it leads to a significant speed-up compared to solving a problem from scratch using the shooting method.
\end{enumerate}

\subsection{Paper Structure}
\label{s:struc}

Section \ref{sec:background} provides the optimality conditions for general FRMAB problems and includes a brief review of OCT-H. Section \ref{sec:gendyn} analyzes models with affine and quadratic dynamics. The derived results enable efficient implementation of the shooting method. In Section \ref{sec:ML}, we develop a learning approach based on OCT-H. Section \ref{sec:exp} reports the results of computational experiments, analyzing the accuracy and speed of our approach. In Section \ref{sec:conclusion}, we include our conclusions.

 


\section{Background}
\label{sec:background}

In this section, we review Pontryagin's Maximum Principle \citep[Theorem 3.4]{grassetal08} to derive the optimality conditions for Problem \eqref{eq:genrmabp}. Following this, we provide a brief overview of OCT-H.

\subsection{Optimality Conditions of FRMAB Problems}
\label{s:genfpaH}

The Pontryagin's Maximum Principle gives necessary optimality conditions for general optimal control problems \cite[Theorem 3.4]{grassetal08}. Due to Assumption \ref{ass:concave}, these conditions also become sufficient \citep[Chapter 2]{Sethi2019}. To apply Pontryagin's maximum principle to Problem \eqref{eq:genrmabp}, we formulate the \emph{Hamiltonian} which involves the \emph{costate} variable $\bm{y}(\cdot)$, given by  
\begin{align*}
H(\bm{x}, \bm{u}, \bm{y}, t) = &\sum_{i=1}^n \bigg[ u_i(t) R_{1,i}(x_i(t)) + (1 - u_i(t)) R_{0,i}(x_i(t)) \\
&\quad + y_i(t)\Big(u_i(t) \phi_{1,i}(x_i(t)) + (1 - u_i(t)) \phi_{0,i}(x_i(t)) \Big) \bigg].
\end{align*}
The Pontryagin's Maximum Principle applied to Problem \eqref{eq:genrmabp} can be formulated as the following lemma.

\begin{lemma}[Pontryagin Maximum Principle \citep{Bittner1963LSP, grassetal08}]
\label{lemma:pmp}
Under Assumption \ref{ass:concave}, $\bm{x}^*(\cdot)$ and $\bm{u}^*(\cdot)$ are optimal state and control trajectories for Problem (\ref{eq:genrmabp}), if and only if there exists a 
continuous and piecewise continously differentiable costate variable
$\bm{y}(\cdot)$, such that
\begin{itemize}
\item[\textbf{a)}] For all $i \in[n]$, at every time $t$ where $\bm{u}(\cdot)$ is continuous, 
\begin{equation}
\begin{split}
\label{eq:dotyitgen}
\dot{y}_i(t) & = -\mathcal{H}_{x_i}(\bm{x}^*, \bm{u}^*, \bm{y},t) \\
& = -\dot{R}_{0,i}(x_i^*(t)) - y_i(t) \dot{\phi}_{0,i}(x_i^*(t)) \\
& - \big[\dot{R}_{1,i}(x_i^*(t)) - \dot{R}_{0,i}(x_i^*(t))\ + y_i(t) \big(\dot{\phi}_{1,i}(x_i^*(t)) - \dot{\phi}_{0,i}(x_i^*(t))\big)\big] u_i^*(t)\\ 
\end{split}
\end{equation}
\item[\textbf{b)}] The following \emph{transversality condition} holds: 
\begin{equation}
\label{eq:yTgen}
\bm{y}(T) = 0.
\end{equation}
\item[\textbf{c)}] At each time $t$, 
\begin{equation}
\label{eq:hxuystarthamax}
\mathcal{H}(\bm{x}^*, \bm{u}^*, \bm{y}, t) \geqslant
  \mathcal{H}(\bm{x}^*, \bm{u}, \bm{y}, t) \textup{ for all feasible controls } \bm{u}.  \end{equation}
\end{itemize}
\end{lemma}


By Lemma \ref{lemma:pmp} (c), it is straightforward that at any time $t$, a control $\bm{u}^*(t)$ satisfying (\ref{eq:hxuystarthamax}) can be computed by solving the following linear optimization (LO) problem:
\begin{equation}
\label{eq:pmpdmLP}
\begin{aligned}
& \max_{\bm{u}} &&\sum_{i=1}^n  \gamma_i^*(t) \, u_i \\
&s.t. \quad && 0 \leqslant u_i \leqslant 1, \quad \forall i \in [n], \\
& \quad && \sum_{i=1}^n u_i \leqslant m,
\end{aligned}
\end{equation}
where $\gamma_i^*(t) \triangleq R_{1,i}(x_i^*(t)) - R_{0,i}(x_i^*(t)) + y_i (t)\big[\phi_{1,i}(x_i^*(t)) - \phi_{0,i}(x_i^*(t))\big] $. That is, the optimal control at time $t$ follows an index policy, determined by ranking the index functions $\gamma_i^*(t)$.

\subsection{Optimal Classification Trees with Hyperplane Splits}
\label{s:oct}
Optimal Classification Trees (OCT) trains a near-optimal decision tree for classification tasks. Unlike classical decision tree algorithms such as CART \citep{BreiFrieStonOlsh84}, which rely on a greedy algorithm, OCT aims to learn a globally optimal decision tree using advanced optimization techniques. OCT uses a single feature in the node splits, meaning it partitions the feature space with hyperplanes that are perpendicular to the axis and assigns a prediction to each region. This often results in improved accuracy, robustness to noise, and shallower trees compared to CART.

OCT-H, introduced in \citep{OCT}, is a generalization of OCT, where arbitrary linear combinations of the features are used for splits. Unlike OCT, OCT-H can partition the feature space with arbitrary hyperplanes, enabling it to capture more complex patterns in the data and often leading to better prediction accuracy. For more details on this technique, refer to \citep{MLOPT}.


\section{Fluid Restless Multi-Armed Bandits} 
\label{sec:gendyn}

In this section, we first outline the basic properties of general FRMAB problems. Then, we analyze two distinct cases: one where the state equations are affine in the state and another where they are quadratic. Finally, we describe the shooting method \citep{StoerJ2013ItNA}, a classical numerical algorithm used to solve optimal control problems. The version we present is tailored to solve Problem \eqref{eq:genrmabp} using the results we derive.

\subsection{Basic Properties}
\label{s:fhuocp}


The next result elucidates the structure of the optimal control $\bm{u}^*(\cdot)$ to Problem \eqref{eq:genrmabp} using Lemma \ref{lemma:pmp}. 

\begin{proposition}
\label{pro:optindxpol}
There exists an optimal control $\bm{u}^*(\cdot)$ that is piecewise constant in $t$, with each entry being either 1 or 0.
\end{proposition}
\begin{proof} 

As mentioned in Section \ref{s:genfpaH}, at each time $t$, the optimal control $\bm{u}^*(t)$ is determined by solving Problem \eqref{eq:pmpdmLP}. $\bm{u}^*(t)$ is always binary, which follows straightforwardly from the analysis of the dual LO for Problem \eqref{eq:pmpdmLP}:
\begin{equation*}
\label{eq:dpmpdmLP}
\begin{aligned}
& \min_{\bm{v},w} \,&& m w + \sum_{i=1}^n  v_{i} \\
&s.t. \quad && v_{i} \geqslant 0, \quad w \geqslant 0, \\
&\quad && v_{i} + w \geqslant \gamma_i^*(t), \quad i \in [n].
\end{aligned}
\end{equation*}

Furthermore, the index function $\gamma^*_i(t)$ is continuous in $t$ in Lemma \ref{lemma:pmp}. As long as the rank between the index functions does not change, the control vector remains constant. Therefore, the optimal control $\bm{u}^*(t)$ is piecewise constant in $t$ and always a binary vector. 
\end{proof}


Proposition \ref{pro:optindxpol} indicates that in the optimal trajectories of $\bm{u}^*(\cdot)$ and $\bm{x}^*(\cdot)$, the time interval is divided into multiple subintervals. Within each subinterval, the optimal control $\bm{u}(\cdot)$ is a constant binary vector. In the subsequent sections, we derive the closed-form trajectories of the state and the costate variables in a subinterval with a constant control vector. These closed-form expressions will be used to develop an efficient version of the shooting method, as well as the machine learning approach. 

For convenience, we introduce the following notations for the remainder of the paper, defined for all \(i \in [n]\):
\begin{equation*}
\begin{aligned}
\alpha_i(u) & \triangleq \alpha_{0,i} + u(\alpha_{1,i} - \alpha_{0,i}), \\
\beta_i(u) & \triangleq \beta_{0,i} + u(\beta_{1,i} - \beta_{0,i}), \\
r_i(u) & \triangleq r_{0,i} + u(r_{1,i} - r_{0,i}).
\end{aligned}
\end{equation*}



\subsubsection{Affine Dynamics} 
\label{sec:affine}
%A: This is the general alpha-beta model
We next apply the above framework to the case where 
both the state equations and reward functions are affine in the state, given by $\phi_{u,i}(x_i) = \alpha_{u,i} + \beta_{u,i} x_i$ and
$R_{u,i}(x_i) = r_{u,i} x_i - c_{u,i}$ for $u = 0, 1, i \in [n]$.


Suppose we are given a finite partition of the time interval $[0, T]$ that consists of $S$ subintervals $[t_s, t_{s+1})$, along with corresponding binary controls $\bm{u}_s \in \{0, 1\}^n$, for $s = 0, 1, \dots, T-1$, with $t_0 = 0$ and $t_S = T$. Given an initial state $\bm{x}(0) = \bm{x}_0$ and a costate $\bm{y}(0) = \bm{y}_0$, we can consider the resulting trajectories $\{ (\bm{x}(t),\bm{y}(t)): t \in [0,T] \}$ obtained by taking control $\bm{u}_s$ on time interval $[t_s, t_{s+1})$ for each $s$. Recall that both the state and the costate trajectories are continuous in $t$, which allows us to build the entire trajectory with the given information. Hence, without loss of generality, we focus on an interval $[t_s, t_{s+1})$ with constant control. 

The ODEs giving the state and costate evolution for project $i$ are: for  $t \in [t_s, t_{s+1}),$ 
\begin{equation}
\label{eq:affinedotxy}
\begin{split}
\dot{x}_i(t)  & = \alpha_i(u_{s,i})
+ \beta_i(u_{s,i})x_i(t), \\
\dot{y}_i(t) & = -r_i(u_{s,i}) - \beta_i(u_{s,i})y_i(t).
\end{split}
\end{equation}

\noindent The solution to (\ref{eq:affinedotxy}), given $x_i(t_s)$ and $y_i(t_s)$, can be obtained in closed form, as shown in the next result. The proof is not included, as it is straightforward from elementary ODE theory.

\begin{equation}
\begin{aligned}
x_i(t) &= 
\begin{cases}
\displaystyle x_i(t_s) + \left[ - \frac{\alpha_i(u_{s,i})}{\beta_i(u_{s,i})} - x_i(t_s) \right] \left[1 - e^{\beta_i(u_{s,i})(t-t_s)}\right], & \text{if } \beta_i(u_{s,i}) \neq 0, \\ 
\displaystyle x_i(t_s) + \alpha_i(u_{s,i})(t - t_s), & \text{if } \beta_i(u_{s,i}) = 0, 
\end{cases} \\
y_i(t) &= 
\begin{cases}
\displaystyle y_i(t_s) + \left[ - \frac{r_i(u_{s,i})}{\beta_i(u_{s,i})} - y_i(t_s) \right] \left[1 - e^{-\beta_i(u_{s,i})(t-t_s)}\right], & \text{if } \beta_i(u_{s,i}) \neq 0, \\ 
\displaystyle y_i(t_s) - r_i(u_{s,i})(t - t_s), & \text{if } \beta_i(u_{s,i}) = 0. 
\end{cases}
\end{aligned}
\label{eq:affineex}
\end{equation}



\subsubsection{Quadratic Dynamics}
\label{sec:quadratic}
%A: This is the logistic model (fishery, epidemiology)

We proceed similarly to the case where the state equations and the reward functions are quadratic and affine in the state, respectively. That is, the reward function for each project $i \in [n]$ is given by  $R_{1,i}(x_i) = r_{u,i} x_i - c_{u,i}$ and the state equation is  $\phi_{u,i}(x_i) = \alpha_{u,i}x_i + \beta_{u,i} x_i^2, \beta_{u,i} \neq 0, \alpha_{u,i} \neq 0$, for all $u \in \{0,1\}, i \in [n]$. Although not addressed in this work, generalizing the reward functions to quadratic forms is straightforward. Again, we focus on the fixed interval $[t_s, t_{s+1})$ with constant control $\bm{u}(t) = \bm{u}_s$. In this interval, the ODEs governing the state and costate evolution for project $i$ are:
\begin{equation}
\label{eq:quad}
\begin{split}
\dot{x}_i(t) & = \alpha_i(u_{s,i})x_i(t) + \beta_i(u_{s,i})x_i^2(t), \\
\dot{y}_i(t) & = -r_i(u_{s,i}) - y_i(t)\big[\alpha_i(u_{s,i}) + 2\beta_i(u_{s,i})x_i(t) \big].
\end{split}
\end{equation}
The solution to (\ref{eq:quad}) is given as follows. Unlike the case of affine dynamics, we do not use expressions involving the initial conditions in the subinterval $x_i(t_s)$ and $y_i(t_s)$, as this leads to considerably more complicated expressions. Instead, we introduce constants $K$ and $G$ for simplicity.
\begin{equation}
\label{eq:quadex}
\begin{split}
x_i(t) & = \frac{K\alpha_i(u_{s,i})e^{\alpha_i(u_{s,i})t}}{1 - K\beta_i(u_{s,i})e^{\alpha_i(u_{s,i})t}}, \\
 y_i(t) & = G e^{-\alpha_i(u_{s,i})t}(1 - K \beta_i(u_{s,i})e^{\alpha_i(u_{s,i})t})^2 \\
 &\quad - \frac{r_i(u_{s,i})}{K\alpha_i(u_{s,i})\beta_i(u_{s,i})}(1 - K \beta_i(u_{s,i})e^{\alpha_i(u_{s,i})t})e^{-\alpha_i(u_{s,i})t}.
\end{split}
\end{equation}



\subsection{The Shooting Method}
\label{s:affineacot}
We use the results derived above to design an algorithm for computing the optimal trajectories for Problem \eqref{eq:genrmabp} with a fixed initial state $\bm{x}_0$. The algorithm is based on the well-known \emph{shooting method}, and further exploits the structure of the optimal trajectories identified previously. 

The algorithm aims to find an initial costate value $\bm{y}_0$ that achieves the terminal condition $\bm{y}(T) = 0$. Once an initial state $\bm{x}_0$ and initial costate $\bm{y}_0$ are fixed, the associated trajectory $\bm{x}(\cdot), \bm{u}(\cdot), \bm{y}(\cdot)$ up to time $T$ can be computed. This computation follows the index policy that satisfies \eqref{eq:hxuystarthamax}. Specifically, at each time $t$, given $\bm{x}(t)$ and $\bm{y}(t)$, we rank the index functions $\gamma_i(t)$ for all projects to determine the associated control vector. As long as the control vector remains constant, we can roll out the trajectories using the expressions derived in Section \ref{s:fhuocp}. A change in the control vector indicates the start of a new subinterval. According to Lemma \ref{lemma:pmp}, if the resulting trajectories also satisfy $\bm{y}(T) = 0$, they are guaranteed to be optimal.

Formally, let $\bm{g}:\mathbb{R}^n \mapsto \mathbb{R}^n$ be the function that takes an initial costate value $\bm{y}_0$ as an input, and the resulting terminal costate value $\bm{y}(T)$ as an output. The shooting method is an iterative algorithm to solve the $n \times n$ root finding problem $\bm{g}(\bm{y}) = \bm{0}$ numerically.

In the $k_{th}$ iteration of the shooting method, the algorithm starts with a guess $\bm{y}_{k, 0}$ for $\bm{y}(0)$, computes corresponding state and costate trajectories $\bm{x}(\cdot)$ and 
$\bm{y}(\cdot)$ up to time $T$ following the index policy structure. This automatically partitions of the time interval $[0, T]$ into subintervals $[t_s, t_{s+1})$ with a constant control $u^s$, for $s = 0, \ldots, S-1$, 
with $t_0 = 0$ and $t_S = T$, where the number $S$ of intervals is also determined.  Then, the algorithm checks whether $\bm{y}(T) \approx 0$ within a given tolerance level $\epsilon$. If such is the case, the algorithm stops. Otherwise, the initial costate value is updated and the process is repeated. The update follows Broyden's method \citep{broyden65, gay79}, a well-established derivative-free quasi-Newton's method.

In the $k_{th}$ iteration, $k \geqslant 1$, Broyden's method computes iterates $\bm{y}_{k,0}$ and $\bm{J}_k$, a surrogate for the Jacobian of $\bm{g}$, following 
\begin{equation}\label{eq:broyden}
\begin{aligned}
\bm{y}_{k,0} & = \bm{y}_{k-1,0}  - \big(\bm{J}_{k-1}\big)^{-1} \bm{g}(\bm{y}_{k-1,0}), \\
\bm{J}_k & = \bm{J}_{k-1} + \frac{\bm{g}(\bm{y}_{k,0}) - \bm{g}(\bm{y}_{k-1,0}) - \bm{J}_{k-1} (\bm{y}_{k,0} - \bm{y}_{k-1,0})}{\lVert{\bm{y}_{k,0} - \bm{y}_{k-1,0}}\rVert^2} \big(\bm{y}_{k,0} - \bm{y}_{k-1,0}\big)'.
\end{aligned}
\end{equation}
Typically, the method starts with $\bm{J}^0 = \bm{I}$, the identity matrix. Also, when we compute the trajectory starting from $t = 0$, we choose a small step size $\delta$ and proceed by incrementally adding $\delta$ to $t$ until reaching $T$. The details of the algorithm are provided in Algorithm \ref{alg:shooting}.

\begin{remark}
\label{re:shooting}
In general, shooting methods are known to be vulnerable to numerical instability. This instability occurs primarily because the shooting methods involve computing the trajectories of $\bm{x}(\cdot),\bm{y}(\cdot)$, which often require numerical approximations of the solutions to differential equations \citep{Diek76, khal92, StoerJ2013ItNA}. However, for the models with affine and quadratic dynamics discussed in this work, no approximation is required because closed-form expressions of $\bm{x}(\cdot),\bm{y}(\cdot)$ exist, given some initial values. This advantage reduces the risk of numerical errors and enhances the reliability and accuracy of the shooting method.
\end{remark}



\begin{algorithm}
 \KwInput{$\epsilon, \delta, R^0_i(\cdot), R^1_i(\cdot), \phi^0_i(\cdot), \phi^1_i(\cdot), T, m, n, \bm{x}_0$ }
\KwOutput{$\Big\{\big( \bm{x}^*(t),\bm{u}^*(t), \bm{y}(t)\big): t \in [0,T], \bm{x}(0) = \bm{x}_0 \Big\}$ } 
\textbf{Initialization: $\bm{y}_{0,0} \in \mathbb{R}^{n}, \bm{J}_{0} =\bm{I}, k = 0$} 

\vspace{2mm}


\Repeat{$\|\bm{g}(\bm{y}_{k-1,0})\|_\infty \leq {\epsilon}$}{
    Fix the initial state to $\bm{x}_0$ and the initial costate to $\bm{y}_{k,0}$. \\
    Compute the trajectories of $\bm{x}(t),\bm{u}(t), \bm{y}(t)$ starting from $t = 0$ up to $t = T$. \\
    $k \leftarrow k + 1$. \\
    Update $\bm{y}_{k,0}$ and $\bm{J}_k$ as in \eqref{eq:broyden}.
}
    

\textbf{Return} $\Big\{\big( \bm{x}(t),\bm{u}(t), \bm{y}(t)\big): t \in [0,T] , \bm{x}(0) = \bm{x}_0 \Big\}$.
  \caption{Shooting Method}
 \label{alg:shooting}
\end{algorithm}




\section{A Machine Learning Approach}
\label{sec:ML}
%A: This is where we present ML-related theorems. Adam's part.

In this section, we present a machine learning approach to learn a state feedback policy for Problem \eqref{eq:genrmabp}. We begin by providing an overview of the algorithm, then introduce a nonlinear feature augmentation technique to incorporate nonlinearity into the learned policy. Finally, we present an example of our approach, focusing on the case of admission and routing to parallel infinite-server queues. 



\subsection{Algorithm Overview}

Our approach uses OCT-H to imitate the time-dependent state-control mappings in the optimal trajectories. To achieve this, we generate multiple initial states for Problem \eqref{eq:genrmabp} and solve each instance using Algorithm \ref{alg:shooting}. For each instance with an initial state $\bm{x}_0$, we obtain the optimal state and control trajectories $\Big\{ (\bm{x}^*(t), \bm{u}^*(t)): t \in [0,T], \bm{x}(0) = \bm{x}_0\Big\}$. We select $N \in \mathbb{N}$ distinct time points $t_1, \dots, t_N$ from the interval $[0,T]$, and extract the corresponding state values $\bm{x}^*(t_1), \dots, \bm{x}^*(t_N)$ and the control values $\bm{u}^*(t_1), \dots, \bm{u}^*(t_N)$. The training data extracted from this process consists of $\bigg\{\Big(\big(\bm{x}^*(t_\ell),t_\ell\big), \bm{u}^*(t_\ell)\Big)\bigg\}_{\ell=1}^{N}$, where the tuple $\big(\bm{x}^*(t_\ell), t_\ell\big)$ is the feature vector and $\bm{u}^*(t_\ell)$ is the target for the $\ell_{th}$ data point. The time $t_{\ell}$ is included as part of the feature vector to incorporate the time-dependency of the state feedback policy.

As discussed in Section \ref{s:oct}, OCT-H partitions the state space using hyperplanes and assigns predictions to each region. Consequently, OCT-H may struggle to capture complex nonlinear patterns in the data. One straightforward solution could be to use deep neural networks, leveraging their strong approximation capabilities. However, deep neural networks are black box algorithms that obscure how decisions are made, as they automatically learn nonlinearities embedded in their layers. Moreover, simply feeding raw data into a deep neural network makes it challenging to fully exploit the inherent structures of FRMAB problems.

Instead of using deep neural networks, we adhere to OCT-H and address potential nonlinearity in the decision boundaries by augmenting the feature vector with nonlinear transformations of the state variables. Specifically, for each feature vector $\big(\bm{x}^*(t_\ell),t_\ell\big)$ in the training data, we add nonlinear transformations of the state vector $\bm{x}^*(t_\ell)$ as additional entries. Then, we apply OCT-H to the augmented data set. This approach ensures that the resulting policy remains interpretable, maintaining the decision-making process within the framework of decision trees. 


\subsection{Addressing Nonlinearity by Feature Augmentation}

We propose a heuristic approach to choosing nonlinear transformations on the state variables. As discussed in Section \ref{s:genfpaH}, the optimal control at time 
$t$ is determined by the relative ordering of the priority indices $\gamma_i(t), i \in [n]$. This implies that switches in effort (i.e., changes in the control vector) occur either when $\gamma_i(t) = \gamma_j(t)$ for some projects $i \neq j$, or when $\gamma_i(t) = 0$. At these points, it becomes indifferent whether we invest effort in project $i$ or $j$, or whether we invest effort in project $i$ or idle, respectively. The set of points in the state space where such switching of efforts occur are known as the switching curves in the optimal control literature \citep{Sethi2019}. In other regions of the state space, there exists a unique constant optimal control vector. Switching curves can be thought of as discriminating lines in the feature space that separate data points with different labels in a supervised learning setting. However, identifying switching curves or their functional forms is challenging. Switching curves can be found by expressing the equalities $\gamma_i(t) = \gamma_j(t)$ only with respect to the state variables $\bm{x}(t)$. Since $\gamma_i(t)$ involves the costate variable $y_i(t)$, we need a way to express $y_i(t)$ in terms of $\bm{x}(t)$. We propose a heuristic to approximate the relation between $y_i(t)$ and $\bm{x}(t)$.

\subsubsection{Affine Dynamics}
\label{s:adh}
Consider an interval $[t_s, t_{s+1})$ with a constant control $\bm{u}_s$. We first express the index function $\gamma_i(t)$ only in terms $x_i(t)$ in this specific interval. 

\begin{proposition}
\label{pro:switching_affine}
For models with affine dynamics, in an interval $[t_s, t_{s+1})$ with a constant control $\bm{u}_s$, $\gamma_i(t)$ can be expressed as an affine combination of the terms
\begin{equation}
t, x_i(t), x_i^2(t), \frac{1}{{x_i(t) + \frac{\alpha_{0,i}}{\beta_{0,i}}}}, \frac{1}{{x_i(t) + \frac{\alpha_{1,i}}{\beta_{1,i}}}}.
\label{eq:affine_features}
\end{equation}
\end{proposition}

\begin{proof} 
Using the results in \eqref{eq:affineex}, the relation between $x_i(t)$ and $y_i(t)$ can be described as follows:
\begin{equation*}
y_i(t) = 
\begin{cases}
\begin{aligned}
\displaystyle y_{i,1}(t,u_{s,i}) \triangleq & \; -\frac{r_i(u_{s,i})}{\beta_i(u_{s,i})} + 
\frac{\left( x_i(t_s) + \frac{\alpha_i(u_{s,i})}{\beta_i(u_{s,i})} \right) \left( y_i(t_s) + \frac{r_i(u_{s,i})}{\beta_i(u_{s,i})} \right)}{x_i(t) + \frac{\alpha_i(u_{s,i})}{\beta_i(u_{s,i})}}, \\
& \; \text{if } \beta_i(u_{s,i}) \neq 0, 
\end{aligned} \\[3ex]
\begin{aligned}
\displaystyle y_{i,2}(t,u_{s,i}) \triangleq & \; y_i(t_s) - r_i(u_{s,i}) \frac{x_i(t) - x_i(t_s)}{\alpha_i(u_{s,i})}, \\
& \; \text{if } \beta_i(u_{s,i}) = 0, \alpha_i(u_{s,i}) \neq 0, r_i(u_{s,i}) \neq 0, 
\end{aligned} \\[2ex]
\begin{aligned}
\displaystyle y_{i,3}(t,u_{s,i}) \triangleq & \; x_i(t) - x_i(t_s) + y_i(t_s) - r_i(u_{s,i})(t - t_s), \\
& \; \text{if } \beta_i(u_{s,i}) = \alpha_i(u_{s,i}) = 0, r_i(u_{s,i}) \neq 0, 
\end{aligned} \\[2ex]
\begin{aligned}
\displaystyle y_{i,4}(t,u_{s,i}) \triangleq & \; y_i(t_s), \\
& \; \text{if } \beta_i(u_{s,i}) = r_i(u_{s,i}) = 0. 
\end{aligned}
\end{cases}
\end{equation*}

\noindent Given that the optimal control vector is always binary, as stated in Proposition \ref{pro:optindxpol}, this relation leads to the unified expression:
\begin{equation}\label{eq:combined}
\begin{aligned}
y_i(t) &=  \sum_{j=1}^{4}\bigg(C_{1,j}y_{i,j}(t,1) + C_{0,j}y_{i,j}(t,0)\bigg),
\end{aligned}
\end{equation}

\noindent where only one of the coefficients $C_{u,j}, u \in \{0,1\}, j \in [4]$ is non-zero, depending on the values of $\beta_i(u_{s,i}), \alpha_i(u_{s,i}), r_i(u_{s,i})$.

Subsequently, we substitute the expression \eqref{eq:combined} into the index function 
$$\gamma_i(t) = r_{1,i}x_i(t) - c_{1,i} - r_{0,i}x_i(t) + c_{0,i} + y_i(t)[\alpha_{1,i} + \beta_{1,i}x_i(t) - \alpha_{0,i} - \beta_{0,i}x_i(t)],$$ resulting in an expression for $\gamma_i(t)$ that involves an affine combination of the terms only with respect to $x_i(t)$ given in \eqref{eq:affine_features}. Here, the terms $\frac{1}{{x_i(t) + \frac{\alpha_{0,i}}{\beta_{0,i}}}}$ and $\frac{1}{{x_i(t) + \frac{\alpha_{1,i}}{\beta_{1,i}}}}$ correspond to $y_{i,1}(t,u_{s,i})$, while $x_i^2(t)$ corresponds to $y_{i,2}(t,u_{s,i})$ and $y_{i,3}(t,u_{s,i})$, and $t$ corresponds to $y_{i,3}(t,u_{s,i})$.  
\end{proof}

The initial values $y_i(t_s)$ and $x_i(t_s)$ that appear in the coefficients of the affine combination in Proposition \ref{pro:switching_affine} should vary depending on the specific time interval and the initial state $\bm{x}(0)$ that led to that interval. In other words, these coefficients remain constant only within the specific interval considered. Thus, these values should generally be treated as time and state-dependent, and the relation between $\gamma_i(t)$ and $x_i(t)$ described in Proposition \ref{pro:switching_affine} does not globally apply across the entire state space.

We propose simply approximating $\gamma_i(t)$ with the affine combination of the terms in \eqref{eq:affine_features}, regardless of time and state. This approach treats the coefficients of the affine combination as constants to be learned, and allows us to approximate any switching curve using the affine combination of the terms in \eqref{eq:affine_features}, defined for all $i \in [n]$. Therefore, for each $i \in [n]$, we augment the original feature vectors with the terms in  \eqref{eq:affine_features}. Applying OCT-H to these augmented feature vectors enables the learning of nonlinear switching curves within the state space. 

Note that while our approach assumes implicit time-dependence of switching curves through the inclusion of time as part of the feature vector in each original data point, explicit time dependencies are not separately modeled in the approximation of $\gamma_i(t)$.

%%%%%%%%%


%%%%%%%%%

\subsubsection{Quadratic Dynamics}
\label{s:qdh}
For the quadratic case, we apply the same principle as in the affine dynamics, leading to the following proposition. 

\begin{proposition}
\label{pro:switching_quad}

For models with quadratic dynamics, in an interval $[t_s, t_{s+1})$ with a constant control $\bm{u}_s$, $\gamma_i(t)$ can be expressed as an affine combination of the terms
\begin{equation}
x_i(t), \frac{1}{x_i(t)}, \frac{1}{{x_i(t) + \frac{\alpha_i^{0}}{\beta_i^{0}}}}, \frac{1}{{x_i(t) + \frac{\alpha_i^{1}}{\beta_i^{1}}}}.
\label{eq:quad_features}
\end{equation}
\end{proposition}

\begin{proof} 
The results in \eqref{eq:quadex} lead to the following expression:
$$
y_i(t) = \Bigg( \frac{K(\alpha_i(u) + x_i(t)\beta_i(u))}{x_i(t)} - K\beta_i(u) \Bigg) \Bigg(G - G\beta_i(u)\frac{x_i(t)}{\alpha_i(u) + x\beta_i(u)} - \frac{r_i(u)}{K\alpha_i(u)\beta_i(u)} \Bigg)
$$
We plug this expression into the index function 
$$
\gamma_i(t) = u(r^1_ix_i(t) - c_i^1) + (1-u)(r^0_ix_i(t) - c_i^0) + y_i(t)\Bigg(u(\alpha_i^{1}x_i + \beta_i^{1} x_i^2) + (1-u)(\alpha_i^{0}x_i + \beta_i^{0} x_i^2) \Bigg),
$$

to get the results. 
\end{proof}


\noindent Following the approach used for affine dynamics, we augment the original feature vector in each data point with the terms in \eqref{eq:quad_features}, for all $i \in [n]$.



\subsubsection{Data-driven Feature Augmentation}
\label{s:ddfs}
In practice, not all of the terms listed above are needed, as the expression of $y_i(t)$ with respect to $x_i(t)$ depends on the problem parameters ${\beta}_{u_{s,i},i}, {\alpha}_{u_{s,i},i}, {r}_{u_{s,i},i}$ and whether $u_{s,i}$ is 1 or 0. This means some of the expressions in \eqref{eq:affine_features} and \eqref{eq:quad_features} may not be used at all. After we have generated the training data, we can augment the feature vector in a data-driven way, by investigating the control variables that occurred in the generated data. For example, in the models with affine dynamics, assume $u_i = 1$ for all data points generated. If $\beta_{1,i} \neq 0$, then the terms resulting from the cases $y_{i,1}(t,0)$ and $y_{i,j}(t,u_i), j = 2,3,4 $, are unnecessary because only $C_{1,1}$ in \eqref{eq:combined} will be non-zero. Hence, we augment the feature vector only with $\frac{1}{{x_i(t) + \frac{\alpha_{1,i}}{\beta_{1,i}}}}$. This principle generalizes to data-driven feature augmentation process, provided in Algorithm \ref{alg:augmentation}. We describe the entire proposed approach, beginning from data generation to decision tree training, in Algorithm \ref{alg:main}.

\begin{algorithm}
\KwInput{$\mathcal{D}, \mathcal{U}, n$}
\KwOutput{Augmented feature vectors $\mathcal{D}^{'}$} 
\textbf{Initialization: $ \mathcal{D}^{'} = \mathcal{D}$} 



Identify $v_i = \{u_i \in \mathbb{R}: \bm{u} \in \mathcal{U} \}, \; \forall i \in [n]$.\\

\textbf{Affine Dynamics} \\ 
\For{$i \in [n]$}{
  \For{$u_i \in v_i$}{
    \For{$ \big(\bm{x},t\big) \in \mathcal{D}^{'}$}{
        \If{$\beta_i(u_{i}) \neq 0$}{Add $\frac{1}{x_i + \frac{\alpha_i({u_i})}{\beta_i({u_i})}}$ as an additional entry to the feature vector $\big(\bm{x},t\big)$.}
        \ElseIf{$r_i(u_i) \neq 0$}{Add $x_i^2$ as an additional entry to the feature vector $\big(\bm{x},t\big)$.}
    }
  }
}

\vspace{3mm}

\textbf{Quadratic Dynamics} \\

\For{$i \in [n]$}{
  \For{$ \big(\bm{x},t\big) \in \mathcal{D}^{'}$}{
          Add $\frac{1}{x_i}$ as an additional entry to the feature vector $\big(\bm{x},t\big)$. \\
    \For{$u_i \in v_i$}{
         Add $\frac{1}{x_i + \frac{\alpha_i({u_i})}{\beta_i({u_i})}}$ as an additional entry to the feature vector $\big(\bm{x},t\big)$.
    }
  }
}



\caption{Data-Driven Feature Augmentation}
\label{alg:augmentation}
\end{algorithm}


\begin{algorithm}
\KwInput{$M, \{t_1, \dots, t_N\}, \epsilon, \delta, R^0_i(\cdot), R^1_i(\cdot), \phi^0_i(\cdot), \phi^1_i(\cdot), T, m, n, \bm{x}_0$}
\KwOutput{$
\pi: \mathcal{X} \times [0,T] \mapsto [0,1]^{n}
$} 

\textbf{Initialization: $ \mathcal{D} = \emptyset, \mathcal{U} = \emptyset,  j = 1$} 

\vspace{3mm}

\textbf{1. Data Generation} \\

\For{$j = 1, \dots, M$}{
    Sample an initial state ${\bm{x}_0}$ from the state space $\mathcal{X}$. \\
    Solve Problem \eqref{eq:genrmabp} with the initial state $\bm{x}_0$ using Algorithm \ref{alg:shooting}. \\
    $\mathcal{D} \leftarrow \mathcal{D} \cup \bigg\{\big(\bm{x}^*(t_\ell),t_\ell\big)\bigg\}_{\ell=1}^{N}$ \\
    $\mathcal{U} \leftarrow \mathcal{U} \cup \bigg\{\bm{u}^*(t_\ell)\bigg\}_{\ell=1}^{N} $ \\
    $j \leftarrow j + 1$
}




\vspace{3mm}

\textbf{2. Feature Augmentation } \\

Use Algorithm \ref{alg:augmentation} to augment the feature vectors in $\mathcal{D}$ and denote the augmented feature vectors as $\mathcal{D}^{'}$.




\textbf{3. Training} \\
Use OCT-H to train a classification tree. The feature vectors are $\mathcal{D}^{'}$ and the target vectors are $\mathcal{U}$. 

\caption{OCT-H for Fluid Restless Multi-armed Bandits.}
\label{alg:main}
\end{algorithm}



\subsection{Example: Optimal Control of Admission and Routing to Parallel Infinite-Server Queues}
\label{s:cocarpisq}
%A: This is where we present the infinite server routing, where we know the closed form solution

We study the optimal control of admission and routing to parallel infinite-server queues as a special case of models with affine dynamics. Due to its simple structure, we can derive closed-form expressions for the index functions $\gamma_i(t), i\in [n]$. For small $n$, this allows us to obtain a simple closed-form optimal policy. Then, we use Algorithm \ref{alg:main} to learn a decision tree policy and compare it with the closed-form optimal policy.


\subsubsection{Deriving the Closed-form Index Functions}
\label{s:queueingindex}

Consider a system with $n$ parallel fluid queues with infinite buffers. 
Fluid arrives to the system at rate $\lambda$. 
The controller chooses the proportion $u_i(t) \in [0, 1]$ to be routed to each queue $i$ at each time $t$. 
The system equation for the buffer contents $x_i(t)$ of queue $i$ is 
\begin{equation*}
\label{eq:dotxitarpq}
\dot{x}_i(t) = 
\lambda u_i(t) - \mu_i x_i(t),
\end{equation*}
which corresponds to the fluid analog of an infinite-server queue.
The remaining proportion, $1 - \sum_{i=1}^n u_i(t)$, is rejected, incurring a cost rate $R$. Note that $\sum_{i=1}^n u_i(t) \leqslant 1$.
Furthermore, queue $i$ incurs holding costs at rate $C_i$. 

The goal is to  minimize the following cost objective over a finite horizon $[0, T]$:
\[
\min_{\bm{u}(\cdot), \bm{x}(\cdot)} \, \int_0^T \big[R \lambda (1 - \sum_i u_i(t)) + \sum_i C_i x_i(t) \big] \, dt,
\]
which in turn is reformulated in maximization form as 
\begin{equation*}
\label{eq:ocparpq}
\max_{\bm{u}(\cdot), \bm{x}(\cdot)} \, \int_0^T \sum_i [R \lambda u_i(t) - C_i x_i(t)] \, dt.
\end{equation*}
The upper bound in the state space of each queue $i$ is $H_i \triangleq \infty$. We define the optimal control of admission and routing to parallel infinite-server queues as the following:

\begin{equation}
\begin{aligned}
\label{eq:routing}
&\max_{\bm{u}(\cdot), \bm{x}(\cdot)} &&\int_0^T \sum_{i=1}^{n} [R \lambda u_i(t) - C_i x_i(t)] \, dt \\
&\text{s.t.} \quad && \dot{x}_i(t) = \lambda u_i(t) - \mu_i x_i(t), \quad  \forall i \in [n], \forall t \in [0,T], \\
& \qquad \ && x_i(t) > 0, \quad \forall i \in [n], \forall t \in [0,T],  \\
& \qquad \ &&\bm{x}(0) = \bm{x}_0, \\
& \qquad \ &&0 \leqslant u_i(t) \leqslant 1, \quad \forall i \in [n], \forall t \in [0,T],  \\
& \qquad \ &&\sum_{i=1}^n u_i(t) \leqslant 1, \quad \forall t \in [0,T].
\end{aligned}
\end{equation}

We first prove that the constraints $x_i(t) > 0, \forall i \in [n]$, are automatically satisfied regardless of the control trajectory.

\begin{proposition}
\label{pro:routingpositive}

For Problem \eqref{eq:routing}, if $\bm{x}_0 > 0$, then $\bm{x}(t) > 0, \forall t \in [0,T]$ regardless of the control trajectory.
\end{proposition}

\begin{proof} 
For $t \in [t_s, t_{s+1})$ with a constant control vector $\bm{u}_s \in [0,1]^{n}$, 
\begin{equation*}
\begin{split}
{x}_i(t)  & = x_i(t_s)+ [\frac{\lambda u_{s,i} }{\mu_i} - x_i(t_s)] \, \big[1 - e^{-\mu_i 
 (t-t_s)}\big] \\
& = \frac{\lambda u_{s,i} }{\mu_i}(1 - e^{-\mu_i 
 (t-t_s)}) + x_i(t_s)e^{-\mu_i 
 (t-t_s)}
\end{split}
\end{equation*}
Assuming $x_i(t_s) > 0$, it is straightforward that $x_i(t)  > 0$ in the interval $[t_s, t_{s+1})$. It is also clear that $x_i(t_{s+1})  > 0$, again implying $x_i(t)  > 0$ in the interval $[t_{s+1}, t_{s+2})$.  Hence, due to mathematical induction, $\bm{x}_0  > 0$ guarantees that $\bm{x}(t)  > 0, \; \forall t \in [0,T]$. 
\end{proof}

Now, we derive a closed-form expression for the index function.

\begin{proposition}
\label{pro:routingpolicy}

The index function for Problem \eqref{eq:routing} is
\begin{equation}
\label{eq:arcgammait}
\gamma_i(t) = R - {C_i} \frac{\lambda}{\mu_i} \big[1- e^{-\mu_i (T-t)}\big] , \quad i \in [n], t \in [0, T].
\end{equation}
\end{proposition}

\begin{proof} 
First, note that the costate $y_i(t)$ satisfies the following ODE in this model:
\[
\dot{y}_i(t) = C_i + \mu_i y_i(t), \quad t \in [0, T].
\]
The index function derived from Lemma \ref{lemma:pmp} is $\gamma_i(t) = R + \lambda y_i(t)$.

For $t \in [t_s, t_{s+1}),$ 
\begin{equation*}
\label{eq:affinedotxysolarc}
\begin{split}
x_i(t) & =  x_i(t_s)+ [\frac{\lambda u_{s,i} }{\mu_i} - x_i(t_s)] \, \big[1 - e^{-\mu_i 
 (t-t_s)}\big] \\
y_i(t) & = y_i(t_s) + \big[- \frac{C_i}{\mu_i} - y_i(t_s)\big] \big[1 - e^{\mu_i 
 (t-t_s)}\big].
\end{split}
\end{equation*}
Applying the boundary condition $y_i(T) = 0$, $y_i(t)$ is given by 
\begin{equation}
\label{eq:yitsolarc}
y_i(t) = - \frac{C_i}{\mu_i} \big[1- e^{-\mu_i (T-t)}\big], \quad t \in [0, T].
\end{equation}

Therefore, substituting (\ref{eq:yitsolarc}) into the index function $\gamma_i(t)$, we obtain the closed-form index expression. 
\end{proof}


By Proposition \ref{pro:routingpolicy}, solving Problem \eqref{eq:routing} is significantly simplified. At each time $t$, we compute the index functions for all projects and allocate efforts accordingly. 


\subsubsection{Learning Feedback Policy using OCT-H}
\label{s:octhex}

In this section, we present a state feedback policy for Problem \eqref{eq:routing} learned by OCT-H. We consider the problem with $n = 2, m = 1$. For this problem, at each time $t$, we only need to compare $\gamma_1(t), \gamma_2(t)$ and 0 to determine the optimal control. From the derivation above, it is straightforward that the optimal feedback policy depends only on the time $t$ and not on the state variable. Given the parameters $\mu_1 = 0.5, \mu_2 = 1, C_1 = 1, C_2 = 1.5, \lambda = 1, R = 3, T = 10 $, the optimal feedback policy is:
\begin{equation}
\pi(\bm{x}, t) = 
\begin{cases}
\displaystyle (0,1), & \text{if } t < 10 - \log{9} \approx 7.802, \\ 
\displaystyle (1,0), & \text{if } t \geq 10 - \log{9} \approx 7.802.
\end{cases}
\label{eq:queuopt}
\end{equation}

\noindent To generate a training data, we sample 1000 initial states uniformly from the interval $(0,10)^2$ and solve each instance. For each solved instance, we extract 10 feature vectors along with their associated control vectors from each subinterval with constant control. The policy learned by OCT-H is given in Figure \ref{fig:routing}. We observe that this policy is almost identical to the optimal policy in \eqref{eq:queuopt}, with only slight numerical differences.

\begin{figure}
    \centering
    \includegraphics[scale = 0.5]{queue_tree.png}
    \caption{The decision tree OCT-H learned for the infinite server routing problem with $n = 2$.}
    \label{fig:routing}
\end{figure}


\section{Computational Experiments}
\label{sec:exp}


In this section, we present the results of computational experiments. We conduct experiments on three distinct problems with varying sizes and time horizons. The first problem, the machine maintenance problem, has affine dynamics. The other two problems, the epidemic control and the fisheries control problems, have quadratic dynamics. We evaluate the quality of the policy learned using Algorithm \ref{alg:main}, and also assess the relative speed-up compared to Algorithm \ref{alg:shooting}. For all problems considered, the state trajectories automatically satisfy the state constraints $x_i(t) \in (0,H_i), \forall i \in [n],$ regardless of the control trajectories. The proof is provided in Appendix \ref{sec:appendix}.

\subsection{Problem Description}
\label{subsec:desc}




\subsubsection{Machine Maintenance}

We consider the classic machine maintenance problem studied in \citep{ks71}. In this problem, the state variable $x_i(t), i \in [n]$ represents the cumulative probability that machine $i$ has failed by time $t$, while $u_i(t)$ represents the preventive maintenance rate at time $t$. The natural failure rate, cost of maintenance, junk value, revenue of machine $i$ are denoted as $h_i, C_i, L_i, R_i$, respectively. The primary objective is to maximize the total profit generated by the machines, adjusted for any junk values if a machine fails prematurely.
The objective is to maximize 
$$\int_0^T \sum_{i=1}^n \Big[R_i - C_ih_iu_i(t) + L_i(h(1 - u_i(t))(1 - x_i(t)))\Big] \, dt$$ 
subject to the state equation 
$$\dot{x}_i(t) = h(1 - u_i(t))(1 - x_i(t)).$$ 
To align this problem with the formulation in Section \ref{sec:affine}, we can set the parameters as $\alpha_{1,i} =  \beta_{1,i} = 0$, and $\alpha_{0,i} = h_i,  \beta_{0,i} = - h_i$. In this model, the state trajectory stays in $(0,1)^{n}$ regardless of the control policy.

To generate a problem, we sample the parameters $\bm{h}, \bm{C}, \bm{L}, \bm{R}$ uniformly at random from the intervals $[0, 0.5]^n, [1,3]^n, [2,4]^n, [2,4]^n,$ respectively. 


\subsubsection{Epidemic Control}

This problem is based on the SIS epidemic model studied in \citep{pollett24}. It has been shown that the fraction of infected individuals in a stochastic version of the SIS epidemic model, following a continuous-time Markov chain, converges in probability to the solution of a continuous-time deterministic differential equation. The epidemic control problem we consider is derived from this continuous-time deterministic differential equation.

In this problem, the state variable $x_i(t)$ represents the fraction of infected people in subpopulation $i$ and $u_i(t)$ represents the intervention effort for subpopulation $i$ at time $t$. Let $C_i$ be the unit social cost per fraction of infected population, and $P_i$ be the unit intervention cost. The transmission rates $\lambda_{1,i}$ and $\lambda_{0,i}$ correspond to active and inactive intervention cases in subpopulation $i$, respectively. Similarly, $\mu_{1,i}$ and $\mu_{0,i}$ denote the recovery rates under active and inactive intervention, respectively. The objective is to minimize the total cost 
$$\int_0^T \sum_{i=1}^n [C_i(t)x_i(t) + P_iu_i(t)] \, dt$$ 
subject to the state equation 
$$
\dot{x}_i(t) = u_i(t)\bigg[\lambda_{1,i} x(t)\bigg(1 - \frac{\mu_1}{\lambda_{1,i}}- x_i(t)\bigg)\bigg]\ + (1- u_i(t))\bigg[\lambda_{0,i} x_i(t)\bigg(1 - \frac{\mu_0}{\lambda_{0,i}}- x(t)\bigg)\bigg]
.$$ 
In this model, the state trajectory always stays in $(0,1)^{n}$.


To generate a problem, we make two assumptions. First, we assume that in each subpopulation $i$, the intervention cost is always lower than the cost of infection. Second, $\lambda_{1,i} < \mu_{1,i}$ and $\lambda_{0,i} > \mu_{0,i}$, to reflect the effects of active intervention. To implement these assumptions, we first sample $\bm{C}$ uniformly at random from the interval $[0, 1]^n$. We then multiply $\bm{C}$ element-wise  with another vector sampled uniformly at random from $[0, 1]^n$ to compute $\bm{P}$. Next, we sample $\bm{\lambda}_1$ and $\bm{\mu}_0$ uniformly at random from $[2,4]^{n}$. Finally, we randomly sample two vectors uniformly from $[0,0.5]^{n}$ and add them to $\bm{\lambda}_1$ and $\bm{\mu}_0$ to obtain $\bm{\mu}_1$ and $\bm{\lambda}_0,$ respectively.


\subsubsection{Fisheries Control}

This problem is based on the classic logistic model of population growth \citep{Ba2011}, extended to the optimal control of fisheries studied by \citep{schae57, gordon91}. In this example, $x_i(t)$ and $u_i(t)$ represent the size of population $i$ at time $t$ and the fishing effort on population $i$ at time $t$, respectively. $r_i$ denotes the intrinsic growth rate of population $i$, and $H_i$ is the maximum sustainable population size. For each population $i$, its catchability coefficient, the unit price of landed fish, and the unit cost of effort are denoted $q_i$, $p_i$, and $C_i$, respectively. The objective is to maximize 
$$\int_0^T \sum_{i=1}^n (p_i q_i x_i(t) - C_i) u_i(t) \, dt$$
subject to the state equation
\[
\dot{x}_i(t) = r_i \left( 1 - \frac{x_i(t)}{H_i} \right) x_i(t) - q_i x_i(t) u_i(t).
\]
To align this problem with the formulation in Section \ref{sec:quadratic}, we can set the parameters as $\beta_{0,i} = \beta_{1,i} = -\frac{r_i}{H_i}$, $\alpha_{0,i} = r_i$, and $\alpha_{1,i} = r_i - q_i$. In this model, the state trajectory always stays in $\prod_{i=1}^n (0,H_i)$.

To generate a problem, we sample the parameters $\bm{r}, \bm{H},\bm{q}, \bm{p}, \bm{C}$ uniformly at random from the intervals $[0, 0.15]^n, [1,6]^n, [0,0.15]^n, [0,2]^n, [0,0.1]^n$, respectively. 



\subsection{Experimental Setup}
\label{subsec:exp_set}

We randomly generate problem instances with varying initial states and solve them using Algorithm \ref{alg:shooting} to generate training data. We sample 3000 initial states uniformly at random from $\prod_{i=1}^n (0,H_i)$, where $H_i$ represents the bounds specific to each problem. For each solved instance, we extract 10 feature vectors along with their associated control vectors from each subinterval with constant control.  For Algorithm \ref{alg:shooting}, we fix $m = \floor*{0.3n}$, $\epsilon = 0.00001$, and $\delta = 0.0001$. 

We evaluate our approach in two different ways. First, we measure simply how well the learned policy imitates the optimal trajectory generated by Algorithm \ref{alg:shooting}. We evaluate the out-of-sample test accuracy on 1000 data points consisting of state-control pairs that the decision tree has not seen during training.

Second, to evaluate the ultimate quality of the learned policies, we apply them to problems with varying initial states and compute the associated objective cost. We compare this cost with the optimal objective cost acquired by Algorithm \ref{alg:shooting}. To compute the objective cost associated with a policy, we discretize the dynamics to approximate the continuous-time trajectory and the integral objective values. We measure the suboptimality of a policy by subtracting the associated objective cost from the optimal objective cost, and then dividing the result by the absolute value of the associated objective cost. As the goal is maximization, this value is guaranteed to be non-negative. We report the maximum of these values across the test instances to analyze the potential suboptimality of the learned policies for unseen problems. The number of test instances is 100.

When measuring the speed-up compared to the shooting method, we divide the time it takes to solve an instance from scratch using Algorithm \ref{alg:shooting} by the time it takes for the trained decision tree to make an inference. The number of test instances is 100 for this. Finally, we report the time required to train a decision tree using OCT-H, with training times reported in minutes and seconds.

All experiments in this section were conducted on a MacBook Pro with an Apple M2 Pro chip and 16GB of memory. Software for OCT-H is available at \citet{InterpretableAI}. We tune the maximum depth of the tree by grid searching over the list [5,10,15].


\subsection{Experimental Results}

In Tables \ref{table:machine}, \ref{table:epidemic}, and \ref{table:fisheries}, we report the experimental results for the machine maintenance, the epidemic control, and the fisheries control problems, respectively. We draw the following conclusions.

\subsubsection*{\textbf{Observations}} 
\begin{itemize}
    \item The out-of-sample classification accuracy is consistently high, never falling below 98\% and frequently achieving 100\%. This indicates that the policy learned using Algorithm \ref{alg:main} imitates the optimal trajectory very well. 
    \item Even when the out-of-sample test accuracy falls below 100\%, the maximum suboptimality remains very low, never exceeding 1.8\%. 
    \item The proposed approach significantly outspeeds Algorithm \ref{alg:shooting}, with speed-ups reaching up to more than 26 million times. We also note that the relative speed-up increases with larger $n$ and $T$. This enhancement is attributed to the increased computational demand of solving problems with larger number of projects and longer time horizons.
    \item The training times are typically short, up to several hours in a personal laptop.  demonstrating the practicality of our approach.
\end{itemize}






\begin{table*}\centering
\caption{Experiment results for the machine maintenance problem.}
\ra{1.3}
\begin{tabular}{cccccc}
\toprule
$n$ & $T$ &  Training Time (min:s) & Speed-up & Accuracy & Max Suboptimality   \\ 
\midrule


5& \multirow{2}{*}{1}& 2:23& 47625  & 1.00 & 0.0000  \\

10&  & 4:50 & 93751 & 1.00 & 0.0000  \\

\hline

5&  \multirow{2}{*}{5}& 6:49 & $4.90 \times 10^5$ & 1.00 & 0.0000  \\

10&   &19:27& $3.23 \times 10^6$ & 0.98 & 0.0181  \\





\bottomrule
\end{tabular}
\label{table:machine}
\end{table*}

\begin{table*}\centering
\caption{Experiment results for the epidemic control problem.}
\ra{1.3}
\begin{tabular}{cccccc} 
\toprule
$n$ & $T$ &  Training Time (min:s) & Speed-up & Accuracy & Max Suboptimality   \\ 
\midrule


5& \multirow{2}{*}{1} & 7:33 & $2.10 \times 10^5$ & 0.99 & 0.0011  \\

10&  &12:33& $1.08 \times 10^6$ & 1.00 & 0.0000  \\


\hline
5& \multirow{2}{*}{5} & 7:04 & $8.60 \times 10^5$ & 0.99 & 0.0000  \\

10&  & 18:07 &  $2.65 \times 10^7 $& 1.00 & 0.0000  \\



\bottomrule
\end{tabular}
\label{table:epidemic}
\end{table*}


\begin{table*}\centering
\caption{Experiment results for the fisheries control problem.}
\ra{1.3}
\begin{tabular}{cccccc}
\toprule
$n$ & $T$ &  Training Time (min:s) & Speed-up & Accuracy & Max Suboptimality   \\ 
\midrule


5& \multirow{2}{*}{1}& 34:24 &  90653 & 0.98 & 0.0000  \\

10&  & 51:29 & $1.59 \times 10^5$ & 0.99 & 0.0000  \\

\hline

5&  \multirow{2}{*}{5}& 12:40 &  $3.90 \times 10^5$ & 0.99 & 0.0011  \\

10&  & 143:20 & $2.07 \times 10^6$ & 0.98 & 0.0111   \\



\bottomrule
\end{tabular}
\label{table:fisheries}
\end{table*}




\section{Conclusions}
\label{sec:conclusion}

We have proposed a machine learning approach to learn a state feedback policy for fluid restless multi-armed bandit problems with affine and quadratic state equations. Instead of relying on black-box algorithms that automatically learn nonlinear patterns, we introduce a feature augmentation technique and use OCT-H. Our computational experiments demonstrate that this approach effectively learns high-quality policies for a variety of problems across different sizes and time horizons.



% Appendix here
% Options are (1) APPENDIX (with or without general title) or
%             (2) APPENDICES (if it has more than one unrelated sections)
% Outcomment the appropriate case if necessary
%
\begin{appendices}

\section{}\label{sec:appendix}

We prove that for the problems considered in Section \ref{subsec:desc}, the state constraints $0 < x_i(t) < H_i, \forall i \in [n],$ are automatically satisfied. Similar to the logic in the proof of Proposition \ref{pro:routingpositive}, we prove that in each interval $[t_s,t_{s+1})$ with a constant control $\bm{u}_s$, if $x_i(t_s) \in (0,H_i)$, then $x_i(t) \in (0,H_i), \forall t \in [t_s,t_{s+1})$. This implies that if $\bm{x}(0) \in (0,H_i)$, then $\bm{x}(t) \in (0,H_i), \forall t \in [0,T]$.

\begin{proposition}
\label{pro:mainpositive}

For the machine maintenance problem described in Section \ref{subsec:desc}, the state trajectory stays within $(0,1)^{n}$ regardless of the control trajectory.

\end{proposition}

\begin{proof} 
We directly apply the results in \eqref{eq:affineex}. For the machine maintenance problem, $\beta_i(u_{s,i}) = - \alpha_i(u_{s,i})$. If $\beta_i(u_{s,i}) = 0$, then $x_i(t) = x_i(t_s) \in (0,1), \forall t \in [t_s,t_{s+1})$.  If $\beta_i(u_{s,i}) \neq 0$, then
\begin{equation*}
\begin{split}
{x}_i(t)  & = x_i(t_s)+ (1 - x_i(t_s))(1 - e^{-h_i(t-t_s)})\\
& = 1 - e^{-h_i(t-t_s)}(1 - x(t_s)).
\end{split}
\end{equation*}

The first equality proves that $x_i(t) > 0,$ and the second equality proves that $x_i(t) < 1, \forall t \in [t_s, t_{s+1})$. 
\end{proof}

\begin{proposition}
\label{pro:epidemicpositive}

For the epidemic control problem described in Section \ref{subsec:desc}, the state trajectory stays within $(0,1)^{n}$ regardless of the control trajectory.

\end{proposition}

\begin{proof} 

We directly apply the results in \eqref{eq:quadex} that
$$
x_i(t) = \frac{K\alpha_i(u_{s,i})e^{\alpha_i(u_{s,i})t}}{1 - K\beta_i(u_{s,i})e^{\alpha_i(u_{s,i})t}}.
$$

From the above expression, $x_i(t)$ is a monotone function of $t$ in this interval. If $\alpha_i(u_{s,i}) > 0$, then 
\begin{equation*}
\begin{split}
\lim_{t\to\infty}x_i(t)  & = -\frac{\alpha_i(u_{s,i})}{\beta_i(u_{s,i})}\\
& = 1 - \frac{u_{s,i}(\mu_0 - \mu_1) - \mu_0}{u_{s,i}(\lambda_0 - \lambda_1) - \lambda_0} \\
\end{split}
\end{equation*}
Due to the assumption that $\alpha_i(u_{s,i}) > 0$,
$$
u_{s,i}(\mu_0 - \mu_1) - \mu_0 > u_{s,i}(\lambda_0 - \lambda_1) - \lambda_0,
$$
where $u_{s,i}(\mu_0 - \mu_1) - \mu_0 = \mu_0(u_{s,i} - 1) - u_{s,i}\mu_1 < 0$. Hence, $-\frac{\alpha_i(u_{s,i})}{\beta_i(u_{s,i})} \in (0,1)$.

If $\alpha_i(u_{s,i}) < 0$, $\lim_{t\to\infty}x_i(t) = 0 $. This guarantees that $x_i(t) \in (0,1)$ in this interval.
\end{proof}


\begin{proposition}
\label{pro:fisheriespositive}

For the fisheries control problem described in Section \ref{subsec:desc}, the state trajectory stays within $(0,H_i)^{n}$ regardless of the control trajectory.

\end{proposition}

\begin{proof} 

The proof is almost identical to the case of epidemic control. Again, we directly apply the results in \eqref{eq:quadex} that
$$
x_i(t) = \frac{K\alpha_i(u_{s,i})e^{\alpha_i(u_{s,i})t}}{1 - K\beta_i(u_{s,i})e^{\alpha_i(u_{s,i})t}}.
$$

From the above expression, $x_i(t)$ is a monotone function of $t$ in this interval. If $\alpha_i(u_{s,i}) > 0$, then 
\begin{equation*}
\begin{split}
\lim_{t\to\infty}x_i(t)  & = -\frac{\alpha_i(u_{s,i})}{\beta_i(u_{s,i})}\\
& = \frac{H_i(r_i - q_iu_{s,i})}{r_i} \\
\end{split}
\end{equation*}
Due to the assumption that $\alpha_i(u_{s,i}) =  r_i - q_iu_{s,i} > 0$,
$\frac{H_i(r_i - q_iu_{s,i})}{r_i} \in (0,H_i)$.

If $\alpha_i(u_{s,i}) < 0$, $\lim_{t\to\infty}x_i(t) = 0 $. This guarantees that $x_i(t) \in (0,H_i)$ in this interval.
\end{proof}




\end{appendices}

\section*{Statements and Declarations}


\begin{itemize}
\item \textbf{Funding:} The work of Jos\'e Ni\~no-Mora was partially supported by the Spanish State Research Agency (AEI) under grant PID2019-109196GB-I00/AEI/10.13039/501100011033.
\item \textbf{Conflict of interest:} The authors have no competing interests to declare that are relevant to the content of this article.
\end{itemize}
%
%   or
%
% \begin{APPENDICES}
% \section{<Title of Section A>}
% \section{<Title of Section B>}
% etc
% \end{APPENDICES}

%%
%\theendnotes

% Acknowledgments here
%\ACKNOWLEDGMENT{}


% References here (outcomment the appropriate case)

% 
\bibliography{reference} % if more than one, comma separated

% CASE 2: BiBTeX used to generate mypaper.bbl (to be further fine tuned)
%\input{mypaper.bbl} % outcomment this line in Case 2

%If you don't use BiBTex, you can manually itemize references as shown below.





%%%%%%%%%%%%%%%%%
\end{document}