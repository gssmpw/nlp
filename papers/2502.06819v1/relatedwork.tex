\section{Related Work}
\label{sec:relat}

The literature on synthesizing 3D representations conditioned on natural language based text instructions, is constantly growing, in part thanks to advances in generative models~\cite{poole2022dreamfusion}. For our work, two types of approaches are especially relevant: works focusing on indoor 3D scene synthesis; and works on 3D human-scene interaction. 

\subsection{3D scene synthesis} 
Indoor scene synthesis, which focuses on generating reasonable furniture arrangements that satisfy room functionality in a 3D space, has been widely studied in the past decades. Using inputs such as floor plans \cite{paschalidou2021atiss, feng2024layoutgpt}, scene graphs \cite{zhai2024echoscene, wei2024planner3d,GraphDreamer}, and natural language descriptions \cite{tang2024diffuscene, lin2024instructscene}, controllable scene synthesis has shown promising performance. Early works \cite{ritchie2019fast, wang2021sceneformer, paschalidou2021atiss} employ CNN-based or transformer-based autoregressive generative models to progressively estimate 3D layout, usually consisting of object category, position, size and orientation. However, these sequential approaches can lead to the accumulation of prediction errors as they may struggle to accurately represent inter-object relationships. Recent progress \cite{tang2024diffuscene, lin2024instructscene} has been made by leveraging denoising diffusion probabilistic models, achieving visually coherent scenes. While great progress has been made, inherent problems of 3D indoor scene synthesis still exist. Most of the existing methods concentrate on improving the accuracy of 3D layout and/or 3D object generation. However, the method capability in applications are restricted, without taking the object-level functionality of the synthesized scenes into account. A few works \cite{zhang2021fast, min2024funcscene} attempt to emphasize the object functionality, however, suffer from long-tail distribution problems. With the recent advances in foundation models, Vision-language models (VLMs) (e.g., \cite{rombach2022high}) are employed to bridge 2D and 3D representations, such as GenZI \cite{li2024genzi}. Besides, some recent works (e.g., \cite{feng2024layoutgpt, ocal2024sceneteller, zhou2024gala3d}) investigate large language models (LLMs) for visual planning. This motivates us to leverage the reasoning ability of LLMs for object functional priors.  

\subsection{3D human-scene interaction} 
Synthesizing humans interacting with the 3D environment is a key challenge for advancing 3D scene understanding applications. Most attempts have been made to generate human poses and motions given a 3D scene. POSA \cite{hassan2021populating} introduced an ego-centric representation grounded in the surface-based 3D human model SMPL-X \cite{pavlakos2019expressive}, incorporating contact labels and scene semantics. To model potential interactions, they utilized a Conditional Variational Autoencoder (CVAE) based on the SMPL-X vertex positions. 
Dynamic human-object interactions have also been recently explored, e.g., text-to-motion synthesis \cite{li2024controllable, yi2024generating}. However, these aforementioned methods rely on accurate 3D scenes, and a noisy scene mesh can lead to penetration between the human body and scene. Another line of work takes the opposite framework. Pose2Room \cite{nie2022pose2room}, SUMMON \cite{ye2022scene}, MIME \cite{yi2023mime} and SHADE \cite{hong2024human} generate 3D scenes from floor plans and human activities. However, these approaches require human motion sequences during inference, which are not always easily accessible, limiting their applicability in human-conditioned scene synthesis. To summarize, recent developments in 3D human-scene interaction have been separately investigated in two distinct areas: scene-conditioned human synthesis and human-conditioned scene synthesis. Instead, we aim at going beyond the previous paradigms by leveraging geometric human priors to enhance the functionality of synthesized 3D scenes, creating a more integrated approach to human-perceived 3D scene synthesis.