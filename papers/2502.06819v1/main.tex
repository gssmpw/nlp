\documentclass[10pt,twocolumn]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{authblk}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{textcomp}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\perthousand}{\textperthousand}


\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{url}

\iccvfinalcopy 


\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\title{Functional 3D Scene Synthesis through Human-Scene Optimization}

\begin{document}

\author{
    Yao Wei$^{1,2}$\thanks{Work done during an internship at IIT.}, Matteo Toso$^{2}$, Pietro Morerio$^{2}$, Michael Ying Yang$^{3}$, Alessio Del Bue$^{2}$ \\  
    $^1$University of Twente, $^2$Istituto Italiano di Tecnologia (IIT), $^3$University of Bath
}

\makeatletter
\g@addto@macro\@maketitle{
  \begin{figure}[H]
  \setlength{\linewidth}{\textwidth}
  \setlength{\hsize}{\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{illustration.pdf}
  \caption{Given a text prompt and a scene type, our method can synthesize visually and functionally coherent 3D scenes, improving the generation of realistic rooms. Previous approaches optimize \textbf{layout only}, producing rooms that often are not usable by humans, even if they enforce \textbf{physical constraints} among objects (e.g. avoiding collision).}
  \label{fig:illustrate}
  \end{figure}
}
\makeatother

\maketitle

\begin{abstract}
This paper presents a novel generative approach that outputs 3D indoor environments solely from a textual description of the scene. Current methods often treat scene synthesis as a mere layout prediction task, leading to rooms with overlapping objects or overly structured scenes, with limited consideration of the practical usability of the generated environment. 
Instead, our approach is based on a simple, but effective principle: we condition scene synthesis to generate rooms that are usable by humans. This principle is implemented by synthesizing 3D humans that interact with the objects composing the scene. 
If this human-centric scene generation is viable, the room layout is functional and it leads to a more coherent 3D structure. 
To this end, we propose a novel method for functional 3D scene synthesis, which consists of reasoning, 3D assembling and optimization.
We regard text guided 3D synthesis as a reasoning process by generating a scene graph via a graph diffusion network. 
Considering object functional co-occurrence, a new strategy is designed to better accommodate human-object interaction and avoidance, achieving human-aware 3D scene optimization.
We conduct both qualitative and quantitative experiments to validate the effectiveness of our method in generating coherent 3D scene synthesis results.

\end{abstract}

\section{Introduction}
\label{sec:intro}

Generative models for scene synthesis are fundamental tools for accessing, prompting and modifying 3D representations of the world. 
They have immediate applications in AR/VR~\cite{VRCopilot2024,NEURIPS2022_GET3D,Lin_2023_CVPR} and hold great potential to revolutionize workflows for artists and designers~\cite{Artistic2020,Canvas2024}.
One key enabling factor is to generate complete 3D layouts by simply using a text prompt, so bypassing the complexity of manually creating or editing 3D environments. 
The challenges lie in not only the generation of spatial layouts that are visually appealing and coherent but also the production of scenes that are physically and functionally plausible.  

Existing 3D scene synthesis methods are far from user-friendly inputs. For instance, graph-conditioned approaches  \cite{zhai2024echoscene, wei2024planner3d} expect users to carefully design graph structures providing a set of spatial relations. 
Language conditioned scene synthesis methods \cite{zhou2024gala3d, ocal2024sceneteller} typically require long and detailed text descriptions. 
Indeed, previous works only treat 3D scene synthesis as a pure layout prediction task, either in autoregressive or one-shot fashion, regardless of the scene functionality. 
%
In contrast, we advocate the principle that scene layouts should be generated to be usable by human beings, such that a person would be able to perform daily actions in the generated scene.  
As illustrated in Fig.~\ref{fig:illustrate}, previous approaches (\eg InstructScene \cite{lin2024instructscene}) optimize {layout only}, producing rooms that often are not usable by humans, even if they enforce {physical constraints} among object layouts (\eg avoiding collision~\cite{wei2024planner3d}).

Motivated by recent advances in human-conditioned 3D scene synthesis~\cite{yi2023mime,tang2024diffuscene}, we improve room layout generation by constraining the model to create environments that express plausible human-scene interactions. Most of the existing methods rely on well-aligned 3D scenes and human motion sequences for training; this, however, is a time-consuming and memory-intensive process~\cite{tang2024diffuscene}. Besides, these methods require human motions as one of the inputs during inference \cite{yi2023mime}, which limits the model generalization capability given arbitrary prompts from users. In contrast, we introduce priors from pre-trained models, which enable zero-shot learning of 3D human-object interaction.  Moreover, our work takes only text descriptions as inputs during the inference stage, which greatly reduces the burden of user input.

We define our approach with three steps: Reasoning, 3D assembling and Optimization. \textbf{Reasoning} takes as input the scene type (e.g., bedroom) and a textual description that is used to distill spatial co-occurrence of objects, representing by a scene graph with object nodes and their pairwise spatial relations. 
We also embed information into such graphs about whether and how objects can be utilized by a human (e.g., a chair can be sat on, while a ceiling lamp typically does not facilitate direct interaction).
The second step is \textbf{3D assembly}: from the constructed scene graph, we generate a 3D scene layout by estimating the position, size and orientation of objects, which are retrieved from a specific database. 
We also synthesize human poses in the scene given the positions and orientations of contact objects in 3D space. To foster functional coherence of the synthesized scenes, the final \textbf{optimization} stage encourages appropriate human-object contacts while considering functional co-occurrence of objects.

In summary, our \textbf{key contributions} are as follows:
\begin{itemize}
    \item  We propose a novel method for functional 3D scene synthesis, which consists of reasoning, 3D assembling and optimization.
    \item Through distilling object-based human action priors from LLMs, a new commonsense scene graph is constructed from text descriptions using graph diffusion models, without requiring annotated interaction data.
    \item Considering object functional co-occurrence, a new strategy is designed to better accommodate human-object interaction and avoidance, achieving human-aware 3D scene optimization.
\end{itemize}

\begin{figure*}[t]
  \centering
   \includegraphics[width=.9\linewidth]{pipeline_v5.png}
   \caption{\textbf{The three stages of our scene synthesis pipeline.} Given a text prompt $X$ and a scene type $V$, we a) extract a set of object categories $c$, features $f$ and possible human-object actions $a$, and pairwise spatial relationships $R$. These provide respectively the nodes, embeddings and directional edges of a noisy, partial graph that can be denoised by a graph diffusion network. b) A second diffusion process then generates the spatial layout of the object, assigning to each object a translation $t$, size $s$, and orientation $r$ while leaving the previously defined features unaltered; human actions and objects' categories are used to retrieve 3D meshes and place them at poses $(r, s, t)$. c)  Iterating over all predicted human-object actions, we optimize the pose of any object that would results into an intersection of the human mesh and the scene's objects. During this stage all other features are left unaltered. 
   }
   \label{fig:pipeline}
\end{figure*}

\section{Related Work}
\label{sec:relat}

The literature on synthesizing 3D representations conditioned on natural language based text instructions, is constantly growing, in part thanks to advances in generative models~\cite{poole2022dreamfusion}. For our work, two types of approaches are especially relevant: works focusing on indoor 3D scene synthesis; and works on 3D human-scene interaction. 

\subsection{3D scene synthesis} 
Indoor scene synthesis, which focuses on generating reasonable furniture arrangements that satisfy room functionality in a 3D space, has been widely studied in the past decades. Using inputs such as floor plans \cite{paschalidou2021atiss, feng2024layoutgpt}, scene graphs \cite{zhai2024echoscene, wei2024planner3d,GraphDreamer}, and natural language descriptions \cite{tang2024diffuscene, lin2024instructscene}, controllable scene synthesis has shown promising performance. Early works \cite{ritchie2019fast, wang2021sceneformer, paschalidou2021atiss} employ CNN-based or transformer-based autoregressive generative models to progressively estimate 3D layout, usually consisting of object category, position, size and orientation. However, these sequential approaches can lead to the accumulation of prediction errors as they may struggle to accurately represent inter-object relationships. Recent progress \cite{tang2024diffuscene, lin2024instructscene} has been made by leveraging denoising diffusion probabilistic models, achieving visually coherent scenes. While great progress has been made, inherent problems of 3D indoor scene synthesis still exist. Most of the existing methods concentrate on improving the accuracy of 3D layout and/or 3D object generation. However, the method capability in applications are restricted, without taking the object-level functionality of the synthesized scenes into account. A few works \cite{zhang2021fast, min2024funcscene} attempt to emphasize the object functionality, however, suffer from long-tail distribution problems. With the recent advances in foundation models, Vision-language models (VLMs) (e.g., \cite{rombach2022high}) are employed to bridge 2D and 3D representations, such as GenZI \cite{li2024genzi}. Besides, some recent works (e.g., \cite{feng2024layoutgpt, ocal2024sceneteller, zhou2024gala3d}) investigate large language models (LLMs) for visual planning. This motivates us to leverage the reasoning ability of LLMs for object functional priors.  

\subsection{3D human-scene interaction} 
Synthesizing humans interacting with the 3D environment is a key challenge for advancing 3D scene understanding applications. Most attempts have been made to generate human poses and motions given a 3D scene. POSA \cite{hassan2021populating} introduced an ego-centric representation grounded in the surface-based 3D human model SMPL-X \cite{pavlakos2019expressive}, incorporating contact labels and scene semantics. To model potential interactions, they utilized a Conditional Variational Autoencoder (CVAE) based on the SMPL-X vertex positions. 
Dynamic human-object interactions have also been recently explored, e.g., text-to-motion synthesis \cite{li2024controllable, yi2024generating}. However, these aforementioned methods rely on accurate 3D scenes, and a noisy scene mesh can lead to penetration between the human body and scene. Another line of work takes the opposite framework. Pose2Room \cite{nie2022pose2room}, SUMMON \cite{ye2022scene}, MIME \cite{yi2023mime} and SHADE \cite{hong2024human} generate 3D scenes from floor plans and human activities. However, these approaches require human motion sequences during inference, which are not always easily accessible, limiting their applicability in human-conditioned scene synthesis. To summarize, recent developments in 3D human-scene interaction have been separately investigated in two distinct areas: scene-conditioned human synthesis and human-conditioned scene synthesis. Instead, we aim at going beyond the previous paradigms by leveraging geometric human priors to enhance the functionality of synthesized 3D scenes, creating a more integrated approach to human-perceived 3D scene synthesis.

\section{Method}
\label{sec:metho}

\begin{figure}[th!]
  \centering
   \includegraphics[width=\linewidth]{reasoning_v6.jpg}
   \caption{\textbf{From text prompt to scene graph.} Given the scene type $V$ and a text prompt $X$, we predict likely object categories $c$ by a) leveraging the learned distribution of objects in a collection of same-type 3D scenes and b) extracting from $X$ object categories and their spatial relationship $R$. Then, we c) use Llama to predict possible human contact interaction $a$ with the objects. d) We extract additional features $f$ from the text prompt using a pre-trained VQ-VAE; together, $c$, $f$ and $a$ provide node embeddings for an incomplete scene graph $G$, with edges $R$. e) We then use a graph diffusion network, conditioned on CLIP textual features extracted from $X$, to generate the complete scene graph $G$. 
   }
   \label{fig:texttograph}
\end{figure}

Given a text prompt $X$, that describes a scene type $V$, our method synthesizes a 3D scene $S$ which is consistent with $X$ and that is compliant with a set of human activities. As illustrated in Fig. \ref{fig:pipeline}, the proposed method consists of three primary stages: \textit{reasoning}, \textit{3D assembly}, and \textit{optimization}. 

\noindent
\textbf{Reasoning} (Fig.~\ref{fig:pipeline}a, Sec. \ref{subsec:reasoning}): This stage takes care of building an intermediate representation, namely a scene graph, which will be then exploited for the actual 3D generation. Graph nodes represent objects, and relations are encoded by directed edges. Such a graph is estimated through diffusion, and it incorporates human-scene interactions that upgrade previous semantic graphs, which only describe the spatial co-occurrence of objects~\cite{lin2024instructscene}.

\noindent
\textbf{3D Assembly} (Fig.~\ref{fig:pipeline}b, Sec. \ref{subsec:assembly}): Building upon the graph representation, the 3D layout estimation of objects is performed via diffusion of translation, size and orientation parameters. After that, textured 3D mesh models for each object are retrieved from a database, based on node features. 
Meanwhile, 3D contact humans are retrieved from a collection of SMPL-X body models~\cite{pavlakos2019expressive} with static poses from RenderPeople scans \cite{patel2021agora}. As a result, 3D scenes can be generated by fitting the object and human models to the generated layouts.

\noindent
\textbf{Optimization} (Fig.~\ref{fig:pipeline}c, Sec. \ref{subsec:optim}): By defining functional object groups that categorize objects with similar functionality, a new strategy is designed for human-aware 3D scene optimization, where appropriate human-object contact is encouraged, and inappropriate human-object surface interpenetration is discouraged.

\subsection{Reasoning}
\label{subsec:reasoning}
Instead of employing well-crafted scene graphs or detailed language descriptions as inputs for both training and inference, we explore easy-to-use text descriptions as guidance of scene synthesis, which greatly reduces manual efforts.
As exemplified in Fig.~\ref{fig:texttograph}, we construct a scene graph $G=\{O,R\}$ from the text prompt $X$ and information learned from 3D scenes belonging to scene type $V$. 

The graph nodes represent objects $O=\{o_1, o_2, ..., o_N\}$, and the relations $R=\{r_1, r_2, ..., r_M\}$ are formulated by directed edges connecting nodes, where $N$ and $M$ are the number of object categories and relation categories, respectively. The objects present in the scene are both inferred from the scene type by drawing on object distribution learned from existing dataset (Fig.~\ref{fig:texttograph}a); and directly derived from the text prompt (Fig.~\ref{fig:texttograph}b). In the latter case, the prompt also contains explicit spatial relations $R$. This results in a collection of relation triplets, where each directed edge and the pair of nodes it connects form a triplet=$<$subject, predicate, object$>$, e.g., $<$chair, in front of, desk$>$.
Unlike the semantic graph used in previous works such as InstructScene~\cite{lin2024instructscene}, which only reflects the spatial co-occurrence of objects, we introduce a graph representation that incorporates human-scene interactions to achieve synthesized scenes that are functionally coherent. The $i$-th graph node $o_i=\{c_i, f_i, a_i\}$ consists not only of an object category $c$, but also on possible human actions $a$ (Fig.~\ref{fig:texttograph}e) and shape and texture features $f$. 
The object features $f$ are represented by OpenShape \cite{liu2024openshape} features quantized from a codebook $Z$, which is derived through a pre-trained vector-quantized variational autoencoder (VQ-VAE). 

The human actions $a$ are inferred by Llama-2 \cite{touvron2023llama} via in-context learning (Fig.~\ref{fig:texttograph}c). Our prompts to Llama-2 are composed of task specification, query, and in-context exemplars. The task is described using a very compact prompt: ``\textit{In a \$SCENE TYPE\$, please infer the potential human-object interactions given a list of objects}'', where \textit{\$SCENE TYPE\$} will be replaced by the queried scene type $V$. The object list serves as the query condition, and we provide a set of supporting exemplars as, for example, \textit{sofa} typically supports human \textit{sitting} or \textit{lying}. Here, since the objects are not articulated, we focus on three types of human actions: \textit{sitting}, \textit{lying} and \textit{touching}, and $a$ is set to \textit{None} if human-object contacts do not exist.

The objects $o_i$ and edges $R$ obtained from the text define only a partial, noisy graph: for example, not all objects have an explicit relation $R$ in $X$(Fig.~\ref{fig:texttograph}d). To recover a complete scene graph, we train a graph diffusion model as a denoiser (parameterized by $\phi$) to generates clean graphs from noisy, \ie masked graphs (Fig.~\ref{fig:texttograph}e). To guide the diffusion process, we use textual features $\lambda$ extracted from $X$ using a pre-trained and frozen CLIP text encoder \cite{radford2021learning}. The denoiser network is a graph Transformer consisting of graph attention, MLP layers and cross-attention modules, and it is trained to model the conditional distribution $p_\phi(G|X,V)=p_\phi(O,R|X,V,\lambda)=p_\phi(C,F,A,R|X,V,\lambda)$ by minimizing the weighted summation of variational bounds for node and edge conditioned on $X$.

\subsection{3D Assembly}
\label{subsec:assembly}

Now, we have the task to generate a 3D scene that uses the scene graph to locate the objects and the humans performing the given action. This 3D scene synthesis is made using a retrieval mechanism, with which we collect object shapes and SIMPL-X body models~\cite{pavlakos2019expressive} from a given 3D shape database.
This process is outlined in Fig.~\ref{fig:pipeline}b. First, we train a diffusion model parameterized by $\theta$ to predict 8-parameterized 3D layouts $L=\{l_1, l_2, ..., l_n\}$ for graph nodes, where $n$ is the number of object instances within the scene, and the $j$-th layout $l_j=\{t_j, s_j, r_j\}$ consists of translation $t$, size $s$, and rotation represented by $cos(r)$ and $sin(r)$. The diffusion model is trained to represent the conditional distribution $p_\theta(S|G, V)=p_\theta(L|G, V)$.

Then, textured 3D mesh models are retrieved from the 3D-FUTURE~\cite{fu20213d2} database according to the estimated object attributes including category, feature and size, \ie $(c,f,s)$. The retrieval process starts with filtering by object categories, and sorting by feature cosine similarities. After selecting top-K textured 3D models, these selected models are sorted again by size. This way, the model with the most similar geometries and appearances can be retrieved.

Meanwhile, 3D humans are retrieved using contact object category $c$ and human action $a$, from a collection of SMPL-X body models~\cite{pavlakos2019expressive} with static poses from RenderPeople scans \cite{patel2021agora}. Specifically, there are five contact human bodies: sitting with hands at sides, sitting with arms on table, lying down with hands crossed behind head, half lying, and standing and touching objects in front.
Retrieved human body models $h_i^C$ are thus associated to their \textit{contact object} $o_i^C$ (the apex $C$ stands for ``contact''). As a result, 3D scenes can be generated by fitting the object and human models to the generated layouts.

\begin{algorithm}
\caption{Optimization}
\label{alg:optimization}
\begin{algorithmic}[1]
\State \textbf{Input}: 3D scene $S$ consisting of $n$ objects $o_1, o_2, ..., o_n$, with categories $c_1, c_2, ..., c_n$ where $q\leq n$ objects contact with humans $H^C=\{h_1^C, h_2^C, ..., h_q^C\}$, functional object group $\Omega$, threshold $\beta$.
\For{$h_i^C$ in $H^C$}
\For{$j$ in range($n$)}
\If{$h_i^C \cap o_j \neq  \varnothing$ and $o_i^C \neq o_j$}
\If{$o_j \in c_i$}
    \State move $o_j$ away from $h_i^C$
\ElsIf{ $o_j \notin c_i$ \textbf{and} $(o_i^C, o_j) \in \Omega$}
\If{intersect area $\geq \beta$}
\State move $o_j$  away from $h_i^C$
\EndIf
\ElsIf{$o_j \notin c_i$ and $(o_i^C, o_j) \notin \Omega$}
\State remove $o_j$
\EndIf
\EndIf
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Optimization}
\label{subsec:optim}
For human-aware 3D scene synthesis, most traditional solutions involve training neural networks using well-aligned 3D scenes and human motion sequences. However, this is impractical, because the vast number of possible trajectories results in time-consuming and memory-intensive processing. This motivates us to consider \textit{local scenes}, \ie parts of the scene surrounding those objects with which a human is most likely to interact. 

We also observed that furniture objects with similar functionality are usually spatially arranged together. Therefore, we define functional object groups $\Omega$, e.g.  \textit{bed}-and-\textit{nightstand}, \textit{table}-and-\textit{chair}, \textit{coffee table}-and-\textit{sofa}. By categorizing objects into disjoint groups, our method enables modeling both spatial and functional co-occurrence. 

More in detail, our strategy for human-aware 3D scene refinement encourages appropriate human-object contact and discourages human-object surface interpenetration. The overall process is formalized in Algorithm~\ref{alg:optimization} and exemplified in Fig.~\ref{fig:pipeline}c: given a 3D scene with $n$ objects to be optimized, we consider the $q\leq n$ humans $h_i^C$, each in contact with its own \textit{contact object}  $o_i^C$ as defined in Sec. \ref{subsec:assembly}. For all possible human-object interactions we check whether humans intersect with objects which are different from their own \textit{contact object}, since intersections between humans and objects have to be discouraged. Specifically, objects of the same category $c_i$ are moved far apart from the human, while overlapping objects within the same functional object group $\Omega$ are adjusted. Finally, if a human intersects an object which is not in the same functional object group $\Omega$ as its \textit{contact object}, the former is discarded since it is likely misplaced. 

\begin{table*}[!]
\caption{Quantitative evaluation of our and other state-of-the-art indoor Text-to-3D synthesis models. We report the fraction of query relationships preserved by the model (iRecall), the similarity scores between learned and real distribution (FID, FIC-C, KID), and the accuracy of a model discriminating between ground truth and generated scenes (SCA) on scenes representing bed, living and dining rooms. We highlight the \textbf{best} and \underline{second best} performance.\label{tab:main_results}}
\begin{center}
\begin{tabular}{cr|cccccc}
\hline
\multicolumn{2}{c|}{\textbf{Text-to-Scene Synthesis}} & iRecall $\uparrow$ & FID $\downarrow$ & FID-C $\downarrow$ & KID $\downarrow$ & SCA \\
\hline
\multirow{4}{*}{\textbf{Bedroom}} & ATISS \cite{paschalidou2021atiss} & 48.13 & 119.73 & 6.95 & 0.39 & 59.17 \\
& DiffuScene \cite{tang2024diffuscene} & 56.43 & 123.09 & 7.13 & 0.39 & 60.49 \\
& InstructScene \cite{lin2024instructscene} & \underline{73.47} & \underline{115.81} & \textbf{6.45} & \textbf{0.37} & \underline{54.32} \\
& Ours & \textbf{79.18} & \textbf{115.47} & \underline{6.56} & 0.65 & \textbf{53.70} \\
\hline
\multirow{4}{*}{\textbf{Living room}} & ATISS \cite{paschalidou2021atiss} & 29.50 & 117.67 & 6.08 & 17.60 & 69.38 \\
& DiffuScene \cite{tang2024diffuscene} & 31.15 & 122.20 & 6.10 & 16.49 & 72.92 \\
& InstructScene \cite{lin2024instructscene} & \textbf{57.48} & \underline{111.85} & \textbf{5.22} & \underline{10.50} & \underline{67.70} \\
& Ours & \textbf{57.48} & \textbf{111.34} & \underline{5.39} & \textbf{10.28} & \textbf{67.18} \\
\hline
\multirow{4}{*}{\textbf{Dining room}} & ATISS \cite{paschalidou2021atiss} & 37.58 & 137.10 & 8.49 & 23.60 & 67.61 \\
& DiffuScene \cite{tang2024diffuscene} & 37.87 & 145.48 & 8.63 & 24.08 & 70.57 \\
& InstructScene \cite{lin2024instructscene} & \textbf{64.68} & \textbf{128.14} & \textbf{7.67} & \textbf{12.21} & 62.50 \\
& Ours & \underline{64.31} & \underline{130.14} & \underline{7.88} & \underline{12.61} & \textbf{60.22} \\
\hline
\end{tabular}
\end{center}
\end{table*}

\section{Experiments}
\label{sec:exper}

In this section, we test our proposed method and validate the claims that indoor text-to-3D synthesis can be improved by introducing function-informed refinement of the scene graph and optimizing the spatial arrangement of the objects based on human-scene optimization. 
Following the current state-of-the-art~\cite{paschalidou2021atiss, tang2024diffuscene, lin2024instructscene}, we test our approach on the 3D-FRONT dataset \cite{fu20213d, fu20213d2} (Sec.~\ref{sec:exp:setting}). We then provide both quantitative and qualitative evaluation of the approach, comparing it against the current state-of-the-art (Sec.~\ref{sec:exp:eval}). Finally, to provide further insights on the benefit of the \emph{Reasoning} and \emph{Optimization} steps, we provide ablation experiments exploring the effect of using them separately (Sec~\ref{sec:exp:ablation}).    

\subsection{Experimental settings}
\label{sec:exp:setting}

The 3D-FRONT dataset \cite{fu20213d, fu20213d2} is a large-scale repository of indoor synthetic scenes with professionally designed furniture layouts and with textured 3D models available in different styles. The dataset contains $18k$ different rooms, but evaluation protocols focus on \emph{bedroom}, \emph{dining room} and \emph{livingroom} scenes, which amounts to 813, 900 and 4041 synthetic rooms respectively. The number of object classes available varies with the type of scene: bedrooms typically include 3 to 12 instances from 21 object categories, while for living rooms and dining rooms there are from 3 to 21 instances from 24 categories. Following InstructScene~\cite{lin2024instructscene}, the training and testing data are split as follows: 3879/162 for the bedroom scenes, 621/192 for the living room scenes, and 723/177 for the dining rooms scenes. Each scene is paired with a text description obtained from~\cite{lin2024instructscene} as follows. First, the objects are automatically captioned using a combination of BLIP~\cite{blip2022} and ChatGPT~\cite{ouyang2022training}, while their spatial relationships are extracted using a set of hard-coded rules resulting in 11 possible relationships (\eg left of, right of, above, below). Then, a prompt is formed by sampling one or two of the possible $\textless$subject, predicate, object$\textgreater$ triplets. More details can be found in the supplementary material.

\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{result.pdf}
   \caption{Qualitative results. Our method is able to recover from inaccurate layouts (e.g. nightstand tightly placed in between the bed and wardrobe in the first example or overlapping chairs in the second one) which are instead present in InstructScene~\cite{lin2024instructscene}, our baseline. As a reference, we show the ground-truth arrangement.}
   \label{fig:result}
\end{figure*}

\begin{table*}[th!]
\caption{Ablation studies on bedrooms. Our model introduces two key components: functionality-informed \emph{reasoning} to enhance the edges of the scene graph; and \emph{optimization} of the spatial layout of the scene based on human-object interaction. We compare the performance of the full model against those obtained using only one of the two modules. The best results are \textbf{bold}.
\label{exp:ablation}}
\begin{center}
\begin{tabular}{c|cccccc}
\hline
Settings & iRecall $\uparrow$ & FID $\downarrow$ & FID-C $\downarrow$ & KID $\downarrow$ & SCA \\
\hline
Full model (Ours) & \textbf{79.18} &  \textbf{115.47} &  {6.56} & 0.65 &  \textbf{53.70} \\
w/o Optimization & 77.14 & 129.87 & 7.37 & 0.96 & 66.84 \\
w/o Reasoning & 75.51 & 131.23 & 7.75 & 0.96 & 65.43 \\
InstructScene \cite{lin2024instructscene} & 73.47 & 115.81 &  \textbf{6.45} &  \textbf{0.37} & 54.32 \\
\hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Results}
\label{sec:exp:eval}

To assess our model, we compare its performance against InstructScene \cite{lin2024instructscene}, which learns common sense of indoor scenes using graph diffusion models, as well as ATISS \cite{paschalidou2021atiss}, an autoregressive model that sequentially generates unordered object sets; and DiffuScene \cite{tang2024diffuscene}, which treats scene attributes as continuous 2D matrices with a Gaussian diffusion model. For each of the three types of scenes - bedroom, living room and dining room - we report the same evaluation metrics used in InstructScene \cite{lin2024instructscene}. These are instruction recall (iRecall), Fréchet Inception Distance (FID), FID-C which computes FID by CLIP features, Kernel Inception Distance (KID), and scene classification accuracy (SCA). To reflect the quality of constructed scene graphs, iRecall quantifies the fraction of $\textless$subject, relation, object$\textgreater$ triplets present in the prompt that are also present in the synthesized scenes, \ie the higher the metric the more the information provided by the prompt is preserved in the scene. The FID, FID-C and KID scores, computed using the clean-fid library \cite{parmar2022aliased}, measure the similarity between the learned and real distributions of top-down renderings (at $256^2$ resolution), with lower scores indicating greater similarity. Finally, the SCA is obtained by fine-tuning an AlexNet \cite{krizhevsky2012imagenet}, pretrained on ImageNet \cite{deng2009imagenet}, to discriminate between ground truth and synthesized scenes, and hence the closest SCA is to $50\%$ accuracy, the better the synthesized scenes are. As claimed by a recent work \cite{jayasumana2024rethinking}, FID(-C) and KID suffer from the limitations of the underlying CLIP or Inception embeddings, which restrict their ability to represent rich and complex image content. Therefore, iRecall and SCA are considered as main evaluation metrics.

As reported in Table~\ref{tab:main_results}, the results demonstrate that our method increases performance compared to previous methods for text-to-3D scene synthesis. Compared to state-of-the-art method InstructScene \cite{lin2024instructscene}, the proposed method shows superior or equivalent iRecalls in two over three scenes, confirming the overall efficacy of our approach. In addition, our method provides the best SCAs, \ie closest to $50\%$, across the three scene types. On the bedroom scenes, our method obtains improvements on iRecall by $5.71\%$, and comparable FID score. While our method generally achieves good results, it falls short of surpassing InstructScene on FID-C scores. Our method also exhibits slight KID increases on living rooms, which usually contain more objects. 

To provide further insight in the performance of our method with respect to the baseline provided by InstructScene~\cite{lin2024instructscene}, we showcase in Fig.~\ref{fig:result} one example of synthesized room for each type of scene. Thanks to the intermediate graphs, both methods can reason about scene content based on the input prompts. As a result, the prompt usually explicitly describes one or two objects, while the generated scene typically includes more objects. This inconsistency, often perceived as diversity in generative modeling, enables greater flexibility and creativity in synthesizing 3D indoor scenes. 

It is observed that InstructScene fails to synthesize functional 3D scenes, such as overlapped objects (in bedroom and living room), missing functional objects (dining table in living room), and overly-generated objects (three coffee tables in dining room). In contrast, the scenes synthesize by our method exhibit higher quality, for instance, a dining table is surrounded by dining chairs, which belong to the same functional object group, supporting human action \textit{sitting}. Furthermore, our method achieves more similar results to the reference, ground-truth scenes from 3D-FRONT. 

Additionally, the refined scene graph generated in the reasoning stage is capable of diverse downstream tasks without any fine-tuning. We investigate four zero-shot tasks, including stylization, re-arrangement, completion, and unconditional generation. The results of these experiments are reported in the supplementary material.


\subsection{Ablation studies}
\label{sec:exp:ablation}
In the previous sections we have shown the advantages of our model with respect to the previous state-of-the-art. However, the main novel elements of our pipeline intervene in two stages of the scene generation process, by \emph{i)} enhancing the initial scene graph obtained from the text prompt leveraging functional relationships of the objects in the scene; and by \emph{ii)} optimizing the spatial layout of the generated scene in order to guarantee feasible human-object interactions. These two refinement steps act independently on different stages of the pipeline, raising the question of whether both are needed and of their relative importance. To investigate this issue, we provide an ablation test in which only one of the two improvements is applied. The results of this test are reported in Table~\ref{exp:ablation}. By removing one of the proposed steps, our method performance in general decreases in the metrics  iRecall, FID and SCA for the scene bedroom. This result justifies the inclusion of both reasoning and optimization in the pipeline

We also show qualitative results in Fig.~\ref{fig:result_abla_reas} for the reasoning step and Fig.~\ref{fig:result_abla_opti} for the optimization step. It can be seen that the overlapped chairs can be re-arranged to separate layouts after scene graph edges are sampled conditioned on nodes in the reasoning stage. The optimization strategy addresses inappropriate human-object interactions, for instance, (a) the coffee table is slightly moved from the sofa as they fall into the same functional group, (b) the overlapped armchairs become separate.

\begin{figure}[th!]
  \centering
   \includegraphics[width=\linewidth]{ablation1.pdf}
   \caption{Ablations showing the effect of the reasoning stage. By sampling graph edges conditioned on graph nodes, reasoning stage results in more structured layouts. To show the details, the content highlighted in the blue box is zoomed in.}
   \label{fig:result_abla_reas}
\end{figure}


\begin{figure}[th!]
  \centering
   \includegraphics[width=\linewidth]{ablation2.pdf}
   \caption{Ablations showing the effect of the optimization stage. The proposed strategy leads to better human-scene interaction and avoidance. To show the details, the content highlighted in the blue box is zoomed in.}
   \label{fig:result_abla_opti}
\end{figure}

\section{Conclusions}
\label{concl}

We have shown that the generation of room layouts that adhere to human use is key to improve the automatic generation of 3D environments. Our experimental evaluation has shown that learning just from datasets or/and encoding physical constraints might provide object arrangements that are not realistic, possibly creating rooms that are not designed for humans. This improvement was made possible by designing a scene representation that includes object use in its scene graph representation and then consistently optimize layouts taking in consideration the human placement, not known a priori.

Our model leverages language and human foundational models to support the generation of scene graphs and human poses, but our approach is, in general, agnostic to these modules, as the main contributions are related to the scene graph representation and the further optimization stage that consistently improves the scene layout. For this reasons it could be extended and reused to generate custom scenes that are tailored to specific human actions, e.g. a working environment, a factory, or an hospital.

\textit{Limitations.} Our approach can be extended and improved by considering current limitations. Due to the random nature of diffusion processes for scene graph generation, there is no guarantee that the objects, features and relationships included in the text prompt are then present in the output scene. We could devise a mechanism that anchors objects that are more confident given the LLM output. The human generation process is related to a single action that is done with a single item, there are no dynamics that can be expressed by more complex motions inside the area. If viable, generating human motions would increase computation but would further optimize walkable areas and the arrange of furniture and objects that are linked by a sequence of actions.

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\clearpage

\section{Appendix}

\noindent
This document provides additional details on our main paper, providing further insight for understanding and reproducing our work. We provide i) implementation details of the proposed method in Appendix~\ref{sec:Impl}, ii) experimental comparisons on zero-shot applications in Appendix~\ref{sec:Zero}, iii) additional qualitative results across diverse scene types in Appendix~\ref{sec:AddiQual}.

\subsection{Implementation Details}
\label{sec:Impl}
Our pipeline consists of two diffusion models $\phi$ and $\theta$, used respectively in the Reasoning (Sec.$3.1$ of the main paper) and 3D Assembly stages (Sec.$3.2$ of the main paper), which are trained separately. At run time $\phi$ infers graph structures conditioned on commonsense knowledge from the text prompt, and $\theta$ performs graph-to-layout synthesis, predicting the spatial arrangement of objects and humans. All models are trained on a single GPU with a batch size of 128, using as optimizer AdamW with a learning rate of $1\mathrm{e}{-4}$, and exponentially moving average (EMA) technique. 

\noindent
\textbf{Additional details on the Reasoning stage}: Given a text prompt $X$, the frozen text encoder of CLIP-ViT-B-32~\cite{radford2021learning} extracts textual embeddings $\lambda$, which act as conditions for a graph transformer diffusion model with a cross-attention mechanisms. Inspired by~\cite{lin2024instructscene}, the graphs $G$ are diffused independently in terms of object category $C$, features $F$ and spatial relations $R$, through a forward diffusion process denoted as $q(G_t|G_{t-1})$. The timestep $t$ is an integer between $1$ and the maximum timestep $T$, which we set to 100. Through the reverse diffusion process $p(G_{t-1}|G_t)$, the graph Transformer $\phi$, conditioned on $\lambda$, is trained to reconstruct clean graphs $G_0$ by iteratively denoising corrupted graphs $G_t$. This amounts to minimizing the objective,
\begin{equation}
\mathcal{L}^{G|X}_\phi=\mathcal{L}^{G|\lambda}_\phi=\delta_c\mathcal{L}^{C|\lambda}_\phi+\delta_f\mathcal{L}^{F|\lambda}_\phi+\delta_e\mathcal{L}^{R|\lambda}_\phi,
\label{eq:loss_diff1}
\end{equation}
where $\mathcal{L}_\phi$ represents the variational upper bound on $\mathbb{E}_{q(G_0)}[-\log p_\phi(G_0)]$. The weights $\delta_c$ and $\delta_f$ are set to 1.0 for joint optimization and adjusted to 0.0 after 1500 epochs to fine-tune the relationship distribution. The weight $\delta_e$ is fixed as 10.0. We utilize 5-layer Transformers with 8 heads, 512-dimensional attention and a dropout rate of 0.1. Additionally, the scene attributes are enriched by object-based human actions $A$, which are synthesized via a large language model Llama-2-7b-hf ~\cite{touvron2023llama}. Our prompts to Llama-2 include task specification, in-context exemplars, and query as in the following example: ``\textit{In a living room, please infer the potential human-object interactions given a list of objects. For example: Given objects `chair, sofa, tv stand, cabinet, pendant lamp', you should return `sitting, lying, None, touching, None'. These objects are `ceiling lamp, coffee table, multi-seat sofa, dining chair, dining table', the corresponding human actions should be:}''.

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{human.pdf}
   \caption{3D humans collection, including their actions (a) \textit{sitting}, (b) \textit{sitting and touching}, (c) \textit{lying}, (d) \textit{half lying}, and (e) \textit{standing and touching}.}
   \label{fig:res_huma}
\end{figure}

\begin{table*}[t]
\caption{Quantitative evaluation of our and other state-of-the-art indoor Text-to-3D Scene Synthesis models on zero-shot application to downstream tasks - Stylization, Re-arrangement, Completion, and Unconditional generation (shorted in Uncond.). We highlight the \textbf{best} and \underline{second best} performance. \label{tab:zeroshot}}
\begin{center}
\begin{tabular}{cc|cc|cc|cc|c}
\hline
\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Zero-shot Applications}}} & \multicolumn{2}{c|}{\textbf{Stylization}} & \multicolumn{2}{c|}{\textbf{Re-arrangement}} & \multicolumn{2}{c|}{\textbf{Completion}} & \textbf{Uncond.}\\
& & $\triangle \uparrow$ & FID $\downarrow$ & iRecall $\uparrow$ & FID $\downarrow$ & iRecall $\uparrow$ & FID $\downarrow$ & FID $\downarrow$\\ 
\hline
\multirow{4}{*}{\textbf{Bedroom}} & ATISS \cite{paschalidou2021atiss} & \underline{3.44} & 123.91 & 61.22 & 107.67 & 64.90 & 89.77 & 134.51 \\
& DiffuScene \cite{tang2024diffuscene} & 1.08 & 127.35 & 68.57 & 106.15 & 48.57 & 96.28 & 135.46 \\
& InstructScene \cite{lin2024instructscene} & \textbf{6.34} & \underline{122.73} & \underline{79.59} & \underline{105.27} & \underline{69.80} & \underline{82.98} & \textbf{124.97} \\
& Ours & 1.82 & \textbf{120.12} & \textbf{82.04} & \textbf{103.00} & \textbf{69.84} & \textbf{82.05} & \underline{127.99} \\
\hline
\multirow{4}{*}{\textbf{Living room}} & ATISS \cite{paschalidou2021atiss} & -3.57 & 110.85 & 31.97 & 117.97 & 43.20 & 106.48 & 129.23 \\
& DiffuScene \cite{tang2024diffuscene} & -2.69 & 112.80 & 41.50 & 115.30 & 19.73 & 95.94 & 129.75 \\
& InstructScene \cite{lin2024instructscene} & \textbf{0.28} & \underline{109.39} & \textbf{56.12} & \textbf{106.85} & \textbf{46.94} & \underline{92.52} & \underline{117.62} \\
& Ours & \underline{-0.71} & \textbf{107.47} & \underline{54.76} & \underline{107.47} & \underline{45.58} & \textbf{90.64} & \textbf{115.11} \\
\hline
\multirow{4}{*}{\textbf{Dining room}} & ATISS \cite{paschalidou2021atiss} & \underline{-1.11} & 131.14 & 36.06 & 134.54 & \underline{57.99} & 122.44 & 147.52 \\
& DiffuScene \cite{tang2024diffuscene} & -2.98 & 135.20 & 46.84 & 133.73 & 32.34 & 115.08 & 150.81 \\
& InstructScene \cite{lin2024instructscene} & \textbf{1.69} & \textbf{128.78} & \underline{62.08} & \underline{125.07} & \textbf{60.59} & \textbf{107.86} & \textbf{137.52} \\
& Ours & -2.78 & \underline{129.77} & \textbf{66.17} & \textbf{122.64} & 55.39 & \underline{111.89} & \underline{137.87}\\
\hline
\end{tabular}
\end{center}
\end{table*}

\noindent
\textbf{Additional details on the 3D Assembly stage}: Given $G_0$, the assembly module recovers the layout $l=\{t,s,r\}$ of each node using commonsense priors. Given the ground truth layouts $\hat{L}=\{\hat{l_1},...,\hat{l_n}\}$ we append them to the node embeddings to generate a complete ground truth graph $\hat{G_0}$, and train a graph transformer diffusion model $\theta$ - which has the same architecture as $\phi$ - to recover $\hat{G_0}$ by denoising corrupted graphs. These are obtained by concatenating to the nodes' embedding Gaussian noise. Here, the maximum timestep $T$ is set to 10. 
Model $\theta$ is optimized by minimizing the Mean Squared Error (MSE) between ground truth and predicted layouts, formulated as,
\begin{equation}
\mathcal{L}^{L|G}_\theta=\frac{1}{n}\sum_{i=1}^n(\hat{t_i}-t_i)^2+\frac{1}{n}\sum_{i=1}^n(\hat{s_i}-s_i)^2+\frac{1}{n}\sum_{i=1}^n(\hat{r_i}-r_i)^2.
\label{eq:loss_diff2}
\end{equation}

The predicted layouts are used to complete the scene by retrieving 3D meshes of objects and humans. Object retrieval is done by filtering the 3D-FUTURE dataset~\cite{fu20213d2} based on the query category $c$ and filtering models by feature cosine similarities, \ie prioritizing those that more closely match the geometry and appearance of the query feature $f$. Given the top-K most similar textured 3D models, we select the one closest to the predicted query size $s$. Instead, human meshes are retrieved from a collection of SMPL-X models~\cite{pavlakos2019expressive} of static poses from RenderPeople scans~\cite{patel2021agora} (Fig.~\ref{fig:res_huma}). Each \textit{contact object} is assigned one of five \textit{contact humans} according to the human action $a$ and the object category $c$: for example, \textit{sitting} in a leisure context, \eg on a sofa, will match to Fig.~\ref{fig:res_huma}a, while sitting at a \textit{desk} or \textit{table} will match to Fig.~\ref{fig:res_huma}b.
The contact humans share the same poses $(t,r)$ of the matched objects, with the exception of Fig.~\ref{fig:res_huma}e, usually associated with objects like \textit{wardrobe} and \textit{cabinet}. In this case, the human is rotated by  $180^{\circ}$ to place it facing towards the object it is touching (e.g., open or close the door).

\noindent
\textbf{Additional details on the Optimization stage}: In the main paper (Sec.$3.3$) we introduced functional object groups $\Omega$, which aggregate objects with similar
functionality that are often found in close proximity, potentially leading to more human-object interactions. 
For example, \textit{dining chairs} are usually close to \textit{dining tables}, and a human \textit{sitting} on the former will likely be also in contact with the latter. 
We define groups based on functional co-occurrence rules, such as reception-oriented (\eg \textit{coffee table-and-sofa}), or dining-oriented groups (\eg \textit{dining table-and-dining chair}). Such definitions are used to include soft constraints during human-scene optimization. 

\subsection{Zero-shot Applications}
\label{sec:Zero}

The refined scene graph generated in the Reasoning stage can also be used for downstream tasks, without further fine-tuning. To highlight this, we follow \cite{lin2024instructscene} and apply our pre-trained model to four zero-shot tasks - stylization, re-arrangement, completion,and unconditional generation. 
This downstream test did not require further training, and were omitted from the main paper only for space constraints.
The first three tasks generate scenes conditioning them also on some preexisting features, in addition to the text prompt $X$.
Stylization $p_\phi(f|\lambda,c,a,t,s,r)$ predicts the texture and appearance features $f$ given the layout $(t,s,r)$, the human actions $a$ and the object categories $c$. Similarly, rearrangement $p_{\phi,\theta}(t,s,r|\lambda,c,f,a)$ predicts the layout given all other scene's features, and scene completion introduces new objects to the scene while preserving the existing ones and their features. Unconditional generation conditions a scene from null textual features $\lambda$, producing a scene dependent exclusively on the distributions learned from the dataset. For all four applications, we report the quantitative evaluation in Table \ref{tab:zeroshot}, comparing it against InstructScene \cite{lin2024instructscene}, ATISS \cite{paschalidou2021atiss} and DiffuScene \cite{tang2024diffuscene}. This evaluation shows how our method achieves the best performances in all tasks on the Bedroom scenes. Additionally, in the living room scenes - which are usually richer in objects and relationships - our method has the best FIDs on stylization, completion and unconditional generation tasks.

Qualitative examples of the text prompt used and the resulting scenes, rendered from a top-down perspective, are reported in Fig. \ref{fig:res_zero_styl} (stylization), Fig. \ref{fig:res_zero_rear} (re-arrangement), Fig. \ref{fig:res_zero_comp} (completion) and Fig. \ref{fig:res_zero_unco} (unconditional generation).
We point out how InstructScene \cite{lin2024instructscene} can provide content diverging from the given text prompt, resulting in overlapping objects and incorrect functional object groups. For example, in the bottom row of Fig. \ref{fig:res_zero_comp} there is no space to walk between the coffee table and the sofa; or in the last dining room of Fig. \ref{fig:res_zero_unco}: the dining table is not near dining chairs but the sofa.
In contrast, the results synthesized by our method exhibit higher consistency with the input text prompts (e.g., Fig. \ref{fig:res_zero_styl}), more functional arrangements (e.g., Fig. \ref{fig:res_zero_rear}), and less object overlapping (e.g., Fig. \ref{fig:res_zero_comp}, Fig. \ref{fig:res_zero_unco}).

\begin{figure*}[t]
  \centering
   \includegraphics[width=.83\linewidth]{res_zero_styl.pdf}
   \caption{Stylization results using InstructScene~\cite{lin2024instructscene} and our method. Given an original scene, the object texture features are conditioned on the text prompt while keeping the other scene elements unchanged. Our method exhibits higher consistency with the text prompts.}
   \label{fig:res_zero_styl}
\end{figure*}

\begin{figure*}[t]
  \centering
   \includegraphics[width=.83\linewidth]{res_zero_rear.pdf}
   \caption{Re-arrangement results using InstructScene~\cite{lin2024instructscene} and our method. Given an original scene, the object layouts are conditioned on text prompt while keeping the other scene elements unchanged. Our method results in more realistic spatial arrangements.}
   \label{fig:res_zero_rear}
\end{figure*}

\begin{figure*}[t]
  \centering
   \includegraphics[width=.83\linewidth]{res_zero_comp.pdf}
   \caption{Completion results using InstructScene~\cite{lin2024instructscene} and our method. Given an original scene, new objects are incorporated in the existing environment conditioned on a text prompt. Our method is able to generate mode functional 3D scenes with practical object poses, \eg, the bed and the tv stand, as well as coherent object attributes, \eg, the corner table. }
   \label{fig:res_zero_comp}
\end{figure*}

\begin{figure*}[t]
  \centering
   \includegraphics[width=.9\linewidth]{res_zero_unco.pdf}
   \caption{Unconditional generation results using InstructScene~\cite{lin2024instructscene} and our method. Without any text prompts, 3D scenes of the desired scene type are generated using the learned feature distributions. While both methods exhibit diversity of outputs, ours also preserves functionality and ensures practical applicability.}
   \label{fig:res_zero_unco}
\end{figure*}

\subsection{Additional Qualitative Results}
\label{sec:AddiQual}

To provide more context for the results of the paper and support the claims about the quality of our synthetic scenes, we expand on the qualitative results provided in the main paper. We make here available additional scenes' rendering for \textit{bedroom} (Fig.~\ref{fig:res_add_bed}), \textit{living rooms} (Fig.~\ref{fig:res_add_liv}) and \textit{dining rooms} (Fig.~\ref{fig:res_add_din}). 
In addition to the top-down rendering of each scene, we provide zoomed-in additional renderings (at $1024^2$ resolution) from viewpoints at opposite corners of the rooms, \ie a camera at top-left and one at the bottom-right corner of the scenes, looking down from a high vantage point. To further highlight how our scenes are more compatible with human-object interactions, we also show the same synthesized 3D scenes populated with the retrieved 3D human poses. These results highlight how, compared to InstructScene, our method can generate visually appealing and functionally coherent scenes which supports appropriate human-object contacts, enabling the practical usability of the generated environment.

\begin{figure*}[t]
  \centering
   \includegraphics[width=.9\linewidth]{res_bedroom.pdf}
   \caption{Additional 3D bedroom results. InstructScene~\cite{lin2024instructscene} suffers from object collisions between chair and bed, and repeating wardrobes. In contrast, our method achieves more practical scene synthesis, sometimes even higher functional coherence than GT (see the left example where two nightstands are not symmetrically aligned with the bed).}
   \label{fig:res_add_bed}
\end{figure*}

\begin{figure*}[t]
  \centering
   \includegraphics[width=.9\linewidth]{res_livingroom.pdf}
   \caption{Additional 3D living room results. InstructScene~\cite{lin2024instructscene} fails to synthesize appropriate layouts within functional object groups, e.g., desk-and-chair, coffee table-and-sofa. Instead, the scenes synthesized by our method shows more realistic arrangements with less collisions.}
   \label{fig:res_add_liv}
\end{figure*}

\begin{figure*}[t]
  \centering
   \includegraphics[width=.9\linewidth]{res_diningroom.pdf}
   \caption{Additional 3D dining room results. Compared to InstructScene~\cite{lin2024instructscene} which results in overlapped dining chairs and missing armchair, our method demonstrates more reliable performance.}
   \label{fig:res_add_din}
\end{figure*}

\end{document}