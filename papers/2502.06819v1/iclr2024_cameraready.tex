
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\definecolor{myrefcolor}{rgb}{0, 0.367, 0.7}
\usepackage[colorlinks=true, allcolors=myrefcolor]{hyperref}

\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{adjustbox} 
\usepackage{pifont} 
\newcommand{\cmark}{\ding{51}} 
\newcommand{\xmark}{\ding{55}}
\usepackage[font=normalsize,labelfont=bf,tableposition=top]{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage[symbol]{footmisc}

\definecolor{Gray}{gray}{0.85}

% \usepackage{floatrow}
\usepackage{graphicx,wrapfig,lipsum}

\def\shownotes{1} 
 \ifnum\shownotes=1
\newcommand{\authnote}[2]{{[#1: #2]}}
\else 
\newcommand{\authnote}[2]{{}}
\fi
\newcommand{\CS}[1]{{\color{blue}\authnote{CS}{#1}}}
\newcommand{\JM}[1]{{\color{red}\authnote{JM}{#1}}}
\newcommand{\SH}[1]{{\color{magenta}\authnote{SH}{#1}}}
\newcommand{\YC}[1]{{\color{brown}\authnote{YC}{#1}}}
\newcommand{\FX}[1]{{\color{cyan}{#1}}}
\newcommand{\SC}[1]{{\color{orange}\authnote{SC}{#1}}}

\newcommand{\METHOD}{FLATTEN}
\newcommand{\Method}{Flatten}
\newcommand{\method}{flatten}

\title{\method: optical FLow-guided ATTENtion for consistent text-to-video editing}

\iclrfinalcopy 
\begin{document}

\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\vspace{-5em}
\begin{center}
\textbf{Yuren Cong$^{1,2}$\footnote[1]{\footnotesize{Work done during an internship at Meta AI.}}, 
Mengmeng Xu$^{2}$,
Christian Simon$^{2}$,
Shoufa Chen$^{3}$,
Jiawei Ren$^{4}$, \\
Yanping Xie$^{2}$,
Juan-Manuel Perez-Rua$^{2}$,
Bodo Rosenhahn$^{1}$,
Tao Xiang$^{2}$,
Sen He$^{2}$}
\end{center}

\vspace{-2mm}
{\small
$^{1}$Leibniz University Hannover, $^{2}$Meta AI, $^{3}$The University of Hong Kong, $^{4}$Nanyang Technological University}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



 
\begin{center}
\includegraphics[width=0.888\linewidth]{figures/teaser.pdf}
\captionof{figure}{Our method generates visually consistent videos that adhere to different types (style, texture, and category) of textual prompts while faithfully preserving the motion in the source video.
% FLAT supports semantic editing such as style transfer (2nd row), texture change (3rd row), and entity editing (4th row).
}
\label{fig:teaser}
\end{center}


\begin{abstract}

Text-to-video editing aims to edit the visual appearance of a source video conditional on textual prompts.
A major challenge in this task is to ensure that all frames in the edited video are visually consistent. 
Most recent works apply advanced text-to-image diffusion models to this task by inflating 2D spatial attention in the U-Net into spatio-temporal attention.
%\SH{1. what's the limitation of prior methods. 2. Our method and it's motivation. 3. high level description of our method and how it address previous limitation. 4. pros of our methods, e.g., training free. 5. performance of our method.}
% However, this naive design of capturing spatio-temporal context can be impacted by irrelevant information in the video, resulting in lack of consistency in the edited videos.
Although temporal context can be added through spatio-temporal attention, it may introduce some irrelevant information 
% \footnote{Each patch attends to all other patches and aggregates their features in spatio-temporal attention.} 
for each patch and therefore cause inconsistency in the edited video. 
In this paper, for the first time, we introduce optical flow into the attention module in the diffusion model's U-Net to address the inconsistency issue for text-to-video editing.
% Our method, \textbf{FLATTEN}, enforces the same patch across different frames to attend to each other in the attention module and, therefore, improve the visual consistency in the edited videos. 
Our method, \textbf{FLATTEN}, enforces the patches on the same flow path across different frames to attend to each other in the attention module, thus improving the visual consistency in the edited videos.
Additionally, our method is training-free and can be seamlessly integrated into any diffusion-based text-to-video editing methods and improve their visual consistency.
%We propose a novel flow-guided attention that implicitly leverages optical flow and seamlessly integrates with diffusion models. % to avoid mismatch between pixel-wise optical flow and editing semantics.
%Based on the flow-guided attention, we present FLAT, a training-free framework which enables high-quality and highly consistent text-to-video editing.
Experiment results on existing text-to-video editing benchmarks show that our proposed method achieves the new state-of-the-art performance. In particular, our method excels in maintaining the visual consistency in the edited videos.
The project page is available at \footnotesize{ \url{https://flatten-video-editing.github.io/}}.

% A text-to-video diffusion model can be constructed by extending the spatial attention in an text-to-image diffusion model into spatio-temporal attention.
% However, without substantial video data for training, it is challenging to generate high-quality videos with consistent content and preserve the motion dynamics of reference videos.
% To address this challenge, we propose a novel text-guided video editing model, FLAT, that hiding optical flow of reference videos into the attention mechanism of the diffusion model.
% We first extract the optical flow of the reference videos and infer the pixel trajectories within the latent space. 
% Following this, we implement self-attention to the pixels on the same trajectory during DDIM inversion and denoising, which is called as flow-guided attention.
% Our method can be built upon a pre-trained text-to-image diffusion model and is training-free.
% To validate the effectiveness of our approach, we conduct comprehensive experiments and benchmark the task of text-guided video editing on different datasets.
% Our method outperforms previous approaches and demonstrates superior results in motion preservation and content consistency. 

\end{abstract}

\section{Introduction}
\label{sec:introduction}
%\SH{What is the task. Why we study this task?}
Short videos have become increasingly popular on social platforms in recent years. To attract more attention from subscribers, people like to edit their videos to be more intriguing before uploading them onto their personal social platforms. 
%
Text-to-video (T2V) editing, which aims to change the visual appearance of a video according to a given textual prompt,
%
%With the popularity of short videos on the social media platforms, video editing is playing a crucial role in real-world applications like media creation, advertisement, AR/MR, \textit{etc}. 
can provide a new experience for video editing and has the potential to significantly increase flexibility, productivity, and efficiency. It has, therefore, attracted a great deal of attention recently ~\citep{wu2022tune, khachatryan2023text2video, qi2023fatezero,zhang2023controlvideo, ceylan2023pix2video, qiu2023freenoise, ma2023follow}.

%As a result, there is an urgent demand for high-quality T2V editing.

% This task is urgently demanding for real-world applications like social media creation, filming, advertisement, AR/MR, \textit{etc.}.
% %
% Some early promising results~\citep{hertz2022prompt, tumanyan2023plug, parmar2023zero} have been shown recently on the highly related and prerequisite task of text-to-image (T2I) editing.
% They are benefited from the large-scale text-to-image generation diffusion models pre-trained on massive text-image pairs. 
% Text-to-video (T2V) editing aims to edit the visual appearance of a video according to a given textual prompt and can be widely applied to social media apps, film and advertising industry, AR/MR and \textit{etc.}.
% % The output video should preserve the motion pattern of the source video while the generated content is visually consistent.
%  Some early promising results have been shown recently~\citep{ hertz2022prompt, tumanyan2023plug, parmar2023zero} on the highly related and prerequisite tasks of text-to-image (T2I) editing, which only requires to edit a single frame.
% Typically, their impressive results benefit from large-scale text-to-image generation diffusion models pre-trained on massive language-image pairs.
% Diffusion models have made impressive progress on in the field of text-to-image generation (\citep{ramesh2021zero, rombach2022high, saharia2022photorealistic, ramesh2022hierarchical, balaji2022ediffi}).


A critical challenge in text-to-video editing compared to text-to-image (T2I) editing is visual consistency, \textit{i.e.}, the content in the edited video should have a smooth and unchanging visual appearance throughout the video.
Furthermore, the edited video should preserve the motion from the source video with minimal structural distortion.
These challenges are expected to be alleviated by using fundamental models for text-to-video generation~\citep{ho2022imagen, singer2022make, blattmann2023align, yu2023magvit}. 
Unfortunately, these models usually take substantial computational resources and gigantic amounts of video data, and many models are unavailable to the public.


% \SH{What the main challenge in this task}However, compared with T2I editing, T2V editing is much more challenging because it has two extra requirements.
% (1) Visual consistency. The content in the edited video should have a natural and unchanging visual appearance throughout the video.
% (2) Motion preservation. The edited video should preserve the motion from the input with minimal structural distortion.
% These challenges may be addressed by using fundamental models for text-to-video generation \citep{ho2022imagen, singer2022make, blattmann2023align}. 
% Unfortunately, those models usually take substantial computational resources and gigantic amounts of video data, and many models are unavailable to the public.
% These challenges may be addressed by using fundamental models for text-to-video generation \citep{ho2022imagen, singer2022make, blattmann2023align}. % that incorporate motion priors.
% Unfortunately, those models usually takes substantial computational resources, gigantic amount of video data, and many of them are closed to the public.
% % accessing to such models is generally limited, and training a stable video generation model requires substantial computational resources and a large-scale video dataset.

% \vspace{-3mm}
% \begin{figure}[h!]
% \begin{center}
% \includegraphics[width=0.5\linewidth]{figures/difference.pdf}
% \caption{Illustration of spatial attention, spatio-temporal attention and flow-guided attention. 
% The patches marked with the colored spots attend to the same colored patches and aggregate their feature.
% $F_k$ indicates the $k$-th frame in the video. 
% }
% \label{fig:difference}
% \vspace{-3mm}
% \end{center}
% \end{figure}

% \vspace{-3mm}
% \begin{figure}[h!]
% \floatbox[{\capbeside\thisfloatsetup{capbesideposition={right,center},capbesidewidth=6cm}}]{figure}[\FBwidth]
% {\caption{Illustration of spatial attention, spatio-temporal attention and flow-guided attention. 
% The patches marked with the colored spots attend to the same colored patches and aggregate their feature.
% $F_k$ indicates the $k$-th frame in the video. }
% \label{fig:difference}}
% {\includegraphics[width=1\linewidth]{figures/difference.pdf}}
% \vspace{-3mm}
% \end{figure}

\begin{wrapfigure}{r}{7cm}
\includegraphics[width=7cm]{figures/difference.pdf}
\caption{Illustration of spatial attention, spatio-temporal attention, and our flow-guided attention. 
The patches marked with the crosses attend to the colored patches and aggregate their features.
$F_k$ indicates the feature map of the $k$-th video frame.}
\label{fig:difference}
\vspace{-3mm}
\end{wrapfigure} 

Most recent works \citep{wu2022tune, khachatryan2023text2video, qi2023fatezero,zhang2023controlvideo, ceylan2023pix2video} attempt to extend the existing advanced diffusion models for text-to-image generation to a text-to-video editing model by inflating spatial self-attention into spatio-temporal self-attention.
% Specifically, the extended 3D spatio-temporal attention module flattens all frames as a large pseudo image \CS{what is a large pseudo image here? do you have any reference?} to capture the spatial and temporal context in 2D, as shown in Figure~\ref{fig:difference}.
% \SH{strictly speaking, this is not correct. The input to the self attention is the feature maps of the video frames not the the raw video frames.}
% Specifically, all frames are combined into a large pseudo-image in the extended spatio-temporal attention module, as depicted in Figure~\ref{fig:difference}.
Specifically, the features of the patches from different frames in the video are combined in the extended spatio-temporal attention module, as depicted in Figure~\ref{fig:difference}.
By capturing spatial and temporal context in this way, these methods require only a few fine-tuning steps or even no training to accomplish T2V editing.
Nevertheless, this simple inflation operation introduces irrelevant information since each patch attends to all other patches in the video and aggregates their features in the dense spatio-temporal attention.
The irrelevant patches in the video can mislead the attention process, posing a threat to the consistency control of the edited videos.
As a result, these approaches still fall short of the visual consistency challenge in text-to-video editing.
%
% A straightforward solution is to finetune or train the model on video data to learn the temporal consistency\SH{this claim is questionable. Some works did fine-tuning on video data, but they still have visual consistency issue}. However, this comes at the cost of increased resource utilization and complexity.\SH{How recent works deal with the above mentioned challenges in this task? And what are the limitations in existing methods?} % good!
% Most recent works \citep{wu2022tune, khachatryan2023text2video, zhang2023controlvideo, ceylan2023pix2video, tokenflow2023} attempt to extend advanced text-to-image diffusion models, \textit{e.g.}, Stable Diffusion \citep{rombach2022high}, into a T2V editing model by inflating 2D spatial attention modules into 3D.
% % inflating 2D spatial attention in the U-Net into spatio-temporal attention.
% Specifically, the extended 3D spatio-temporal attention module flattens all frames as a large pseudo image to capture spatial and temporal context in 2D, as shown in Figure~\ref{fig:difference}.
% This simple inflation method introduces irrelevant information since each patch in the large pseudo image attends to all other patches and aggregates their features in the dense spatio-temporal attention.
% Also, the irrelevant patches in the pseudo image could mislead the attention process, challenging the consistency control on the output.
% An intuitive solution is to enforce the patches describing the same object moving across different frames to attend to each other.
% Although this attention pattern can be learnt in training, it comes at the cost of increased resource utilization and complexity. % good!


% Most recent works attempt to extend a pre-trained image diffusion model into a text-to-video editing model by inflating 2D spatial attention in the U-Net into spatial-temporal attention.
% By utilizing image generation priors, training large-scale video models can be avoided.
% To modelling the motion from the source video, Tune-a-video \citep{wu2022tune} implements a novel framework by extending the pre-trained Stable Diffusion model~\citep{rombach2022high} into the spatio-temporal domain and refining its performance using the source video as training data.
% Several works~\citep{khachatryan2023text2video, zhang2023controlvideo} attempt to generate videos with  ControlNet~\citep{zhang2023adding}, utilizing a sequence of image structure representations from the source video (e.g. edge or depth maps) as guidance in the video editing process. 
% Moreover, inspired by impressive image editing approaches, DDIM inversion~\citep{song2020denoising, mokady2023null} and per-frame feature injection~\citep{tumanyan2023plug} are also performed in recent video editing methods~\citep{ceylan2023pix2video, tokenflow2023, qi2023fatezero} to preserve the source video structure.
% The aforementioned structure conditions primarily originate from the image domain, lacking motion priors. 
% Although these per-frame conditions can aid in preserving structure, they fall short in effectively controlling visual consistency.




% \textbf{\textit{Is there a motion guidance to facilitate consistency in video editing?}} % directly from source videos 
% Optical flow emerges as a clear solution.
% Optical flow represents the pixel displacement between frames, serving as a valuable motion prior.
% Nonetheless, incorporating optical flow explicitly into the generative video editing workflow is full of challenges.
% There is no readily available video conditioning model based on optical flow. 
% Furthermore, optical flow provides a meticulous account of pixel displacement, but due to the change in semantics, the pixel positions in the edited video are unavoidably different from those in the source video.
% The intricate mismatch between optical flow and semantics makes it difficult to explicitly utilize optical flow in generative video editing.

% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=0.99\linewidth]{figures/teaser.pdf}
% \caption{Our method FLAT enables high-quality and highly consistent text-to-video editing by hiding optical flow in attention. 
% Given a source video and text prompts, FLAT can generate the consistent video that align with semantic content while faithfully preserving the motion dynamics from the source video.
% FLAT supports the semantic editing such as style transfer (2nd row), texture alteration (3rd row), and entity editing (4th row).
% }
% \label{fig:teaser}
% \end{center}
% \end{figure}

%To avoid this, we introduce optical flow for the first time into the attention module in the U-Net for fine-grained consistency controlling. 

%\SH{What is our method? What motivated us to propose this method and how our method addreses the limitations in previous method.}

In this paper, for the first time, we propose \textbf{FLATTEN}, a novel (optical) FLow-guided ATTENtion that seamlessly integrates with text-to-image diffusion models and implicitly leverages optical flow for text-to-video editing to address the visual consistency limitation in previous works.
FLATTEN enforces the patches on the same flow path across different frames to attend to each other in the attention module, thus improving the visual consistency of the edited video.
The main advantage of our method is that enables the information to communicate accurately across multiple frames guided by optical flow, which stabilizes the prompt-generated visual content of the edited videos.
%
More specifically, we first use a pre-trained optical flow prediction model~\citep{teed2020raft} to estimate the optical flow of the source video. 
%
%Then we downsample the optical flow to align with the resolution scales of the latent space in the diffusion model\SH{this is too specific in the introduction}.
%
The estimated optical flow is then used to compute the trajectories of the patches and guide the attention mechanism between patches on the same trajectory.
%
Meanwhile, we also propose an effective way to integrate flow-guided attention into the existing diffusion process, which can preserve the per-frame feature distribution, even without any training. 
%
%The main advantage of our \METHOD~method is to enable the information exchange effectively across multiple frames guided by the optical flow, which stabilizes the prompt-generated visual content on the edited videos.
%
% FLATTEN can easily adapt the existing T2I editing framework for video editing.
%
%\SH{rewrite this sentence}
%Based on FLATTEN, we introduce a T2V editing framework which fully converts a T2I diffusion model into spatio-temporal domain, then adopt T2I editing techniques such as DDIM inversion~\citep{mokady2023null} and feature injection ~\citep{tumanyan2023plug}.
We present a T2V editing framework utilizing FLATTEN as a foundation and employing T2I editing techniques such as DDIM inversion~\citep{mokady2023null} and feature injection ~\citep{tumanyan2023plug}.
%
We observe high-quality and highly consistent text-to-video editing, as shown in Figure~\ref{fig:teaser}.
Furthermore, our proposed method can be easily integrated into other diffusion-based text-to-video editing methods and improve the visual consistency of their edited videos.

\textbf{The contributions} of this work are as follows: (1) We propose a novel flow-guided attention (FLATTEN) that enables the patches on the same flow path across different frames to attend to each other during the diffusion process and present a framework based on FLATTEN for high-quality and highly consistent T2V editing. (2) Our proposed method, FLATTEN, can be easily integrated into existing text-to-video editing approaches without any training or fine-tuning to improve the visual consistency of their edited results. (3) We conduct extensive experiments to validate the effectiveness of our method.
Our model achieves the new state-of-the-art performance on existing text-to-video editing benchmarks, especially in maintaining visual consistency.
% \SH{rewrite the contribution, merge your proposed method and framework into one contribution}
%\begin{itemize}
%{
%\item We propose a novel flow-guided attention (FLATTEN) that enables the same patch across different frames to attend to each other during the diffusion process and present a framework based on FLATTEN for high-quality and highly consistent T2V editing. 

% and an effective way to integrate FLATTEN into the existing diffusion process.

%\item Our proposed method, FLATTEN, can be easily integrated into existing text-to-video editing approaches without any training or fine-tuning to improve the visual consistency of their edited results.

% We introduce a text-to-video editing framework based on flow-guided attention and an effective way to integrate FLATTEN into the existing diffusion process.

%\item We conduct extensive experiments to validate the effectiveness of our method.
%Our model achieves state-of-the-art performance on existing text-to-video editing benchmarks, especially in maintaining the visual consistency.
%}
%\end{itemize}
% Our main contributions can be summarized as follows:
% \begin{itemize}
% {
% \item We propose FLATTEN, a novel optical FLow-guided ATTENtion that enables the information exchange effectively guided by the video motion information.
% \item FLATTEN can be easily applied to existing image/video editing framework without further training or finetuning. It outperforms previous video editing methods and demonstrates superior motion preservation and visual consistency results. 
% }
   %  \item We present a training-free text-to-video editing framework by extending a pre-trained text-to-image diffusion model. 
   %  Our framework can generate a visually consistent and harmonious video based on a source video and textual prompts.
   % \item A novel flow-guided attention is proposed to improve video consistency by using optical flow implicitly. It can be seamlessly integrated into diffusion models.
   %  \item We conduct comprehensive experiments and benchmark the task of text-to-video editing. 
   %  Our method outperforms previous video editing methods and demonstrates superior results in motion preservation and visual consistency. 
% \end{itemize}
% We make the following contributions in this paper: we propose FLATTEN, a training-free method, which can be seamlessly integrated into any diffusion
% based text-to-video editing methods. Our method achieved the state-of-the-art performance on existing text-to-video benchmarks. We conduct extensive experiment \CS{Can we have some examples what kinds of experiment here? e.g., user studies, video editing, ablations} to validate the effective of our proposed method.

\section{Related Work}
\label{sec:related_work}

\paragraph{Image and Video Generation}
Image generation is a popular generative task in computer vision.
Deep generative models, e.g., GAN~\citep{ karras2019style, kang2023scaling} and auto-regressive Transformers~\citep{ ding2021cogview, esser2021taming, yu2022scaling} have demonstrated their capacity. % to generate high-fidelity images.
Recently, diffusion models~\citep{ho2020denoising, song2020denoising, song2020score} have received much attention due to their stability. 
%during training on large-scale datasets. 
Many T2I generation methods based on diffusion models have emerged and achieved superior performance~\citep{ramesh2021zero, ramesh2022hierarchical, saharia2022photorealistic, balaji2022ediffi}. 
Some of these methods operate in pixel space, while others work in the latent space of an auto-encoder. 
% Among them, Stable Diffusion~\citep{rombach2022high} is widely used in further applications because of its open source.
%implements diffusion models in the latent space.
%It is widely used in further applications since it is open source.

Video generation~\citep{le2021ccvs, ge2022long, chen2023gentron, Cong_2023_CVPR, yu2023magvit, luo2023videofusion} can be viewed as an extension of image generation with additional dimension.
Recent video generation models~\citep{singer2022make, zhou2022magicvideo, ge2023preserve} attempt to extend successful text-to-image generation models into the spatio-temporal domain.
VDM~\citep{ho2022video} adopt a spatio-temporal factorized U-Net for denoising while LDM~\citep{blattmann2023align} implement video diffusion models in the latent space.
%VideoFusion~\citep{luo2023videofusion} decomposes the video diffusion process to utilize a pre-trained image diffusion model.
%However, training these video models requires massive resources and high-quality video data.
Recently, controllable video generation~\citep{yin2023dragnuwa, li2023generative, chen2023motion, teng2023drag} guided by optical flow fields facilitates dynamic interactions between humans and generated content.

\vspace{-1mm}
\paragraph{Text-to-Image Editing}
T2I editing is the task of editing the visual appearance of a source image based on textual prompts.
Many recent methods~\citep{avrahami2022blended, couairon2022diffedit, zhang2023adding} work on pre-trained diffusion models. 
SDEdit~\citep{meng2021sdedit} adds noise to the input image and performs denoising through the specific prior.
Pix2pix-Zero~\citep{parmar2023zero} performs cross-attention guidance while Prompt-to-Prompt~\citep{hertz2022prompt} manipulates the cross-attention layers directly.
% PNP-Diffusion~\citep{tumanyan2023plug} performs DDIM inversion first and save diffusion features during image reconstruction. 
PNP-Diffusion~\citep{tumanyan2023plug} saves diffusion features during reconstruction and injects these features during T2I editing.
%While integrating text-to-image editing methods into video editing holds promise, relying solely on these techniques leads to visual inconsistency.
While video editing can benefit from these creative image methods, relying on them exclusively can lead to inconsistent output.

\vspace{-1mm}
\paragraph{Text-to-Video Editing}
Gen-1~\citep{esser2023structure} demonstrates a structure and content-driven video editing model while Text2Live~\citep{bar2022text2live} uses a layered video representation. 
However, training these models is very time-consuming.
Recent works attempt to extend pre-trained image diffusion models into a T2V editing model. % by using image priors.
Tune-A-Video~\citep{wu2022tune} extends a latent diffusion model to the spatio-temporal domain and fine-tunes it with source videos, but still has difficulties in modeling complex motion.
Text2Video-Zero~\citep{khachatryan2023text2video} and ControlVideo~\citep{zhang2023controlvideo} use ControlNet~\citep{zhang2023adding} to help editing.
They can preserve the per-frame structure but relatively lack control of visual consistency.
FateZero~\citep{qi2023fatezero} introduces an attention blending block to enhance shape-aware editing while the editing words have to be specified.
To improve consistency, TokenFlow \citep{tokenflow2023} enforces linear combinations between diffusion features based on source correspondences.
However, the pre-defined combination weights are not adapted to all videos, resulting in high-frequency flickering.


%Inspired by image editing methods, DDIM inversion~\citep{song2020denoising, mokady2023null} and diffusion feature injection~\cite{tumanyan2023plug} are also integrated into recent video editing methods~\citep{ceylan2023pix2video, tokenflow2023, qi2023fatezero} to preserve the source structures.

Different from the aforementioned methods, we propose a novel flow-guided attention, which implicitly uses optical flow to guide attention modules during the diffusion process.
Our framework can improve the overall visual consistency for T2V editing and can also be seamlessly integrated into existing video editing frameworks without any training or fine-tuning.


% To avoid additional training requirements and to minimize conflicts between optical flow and editing semantics, we propose a flow-guided attention that can be seamlessly integrated into diffusion models.
% Our training-free framework can edit videos based on text prompts while maintaining motion dynamics and visual consistency.

\section{Methodology}

\subsection{Preliminaries}
\label{sec:preliminaries}

\paragraph{Latent Diffusion Models}
Latent diffusion models operate in the latent space with an auto-encoder and demonstrate superior performance in text-to-image generation. 
%It performs the diffusion process in the latent space using a pre-trained auto-encoder.
In the forward process, Gaussian noise is added to the latent input $\bm{z}_0$. The density of $\bm{z}_t$ given $\bm{z}_{t-1}$ can be formulated as:
\begin{equation}
\centering
q(\bm{z}_t | \bm{z}_{t-1}) = \mathcal{N}(\bm{z}_t; \sqrt{1-\beta_t}\bm{z}_{t-1}, \beta_t \bm{\text{I}}),
\label{eq:ddpm_forward}
\end{equation}
where $\beta_t$ is the variance schedule for the timestep $t$.
The number of timesteps used to train the diffusion model is denoted by $T$.
The backward process uses a trained U-Net $\epsilon_{\theta}$ for denoising:
\begin{equation}
\centering
p_{\theta}(\bm{z}_{t-1}|\bm{z}_t) = \mathcal{N}(\bm{z}_{t-1}; \mu_{\theta}(\bm{z}_t, \bm{\tau}, t),  \Sigma_{\theta}(\bm{z}_t, \bm{\tau}, t) ),
\label{eq:ddpm_backward}
\end{equation}
where $\bm{\tau}$ indicates the textual prompt. $\mu_{\theta}$ and $\Sigma_{\theta}$ are computed by the denoising model $\epsilon_{\theta}$.
%When generating images, DDIM~\citep{song2020denoising} or DPM~\citep{lu2022dpm} sampling is often performed to speed up the generation process.

\vspace{-2mm}
\paragraph{DDIM Inversion}
DDIM can convert a random noise to a deterministic $\bm{z}_0$ during sampling~\citep{song2020denoising, dhariwal2021diffusion}.
Based on the assumption that the ODE process can be reversed in the small-step limit, the deterministic DDIM inversion can be formulated as:
\begin{equation}
\centering
\bm{z}_{t+1} = \sqrt{\frac{\alpha_{t+1}}{\alpha_{t}}} \bm{z}_{t} + \sqrt{\alpha_{t+1}} \left( \sqrt{\frac{1}{\alpha_{t+1}-1}}-\sqrt{\frac{1}{\alpha_{t}}-1}  \right) \epsilon_{\theta}(\bm{z}_{t}),
\label{eq:ddim_inverse}
\end{equation}
where $\alpha_{t}$ denotes $\prod^t_{i=1}(1-\beta_i) $.
DDIM inversion is employed to invert the input $\bm{z}_{0}$ into $\bm{z}_{T}$, which can be used for reconstruction and further editing tasks.


\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/overview3.pdf}
\caption{Overview of our framework.
We inflate the existing U-Net architecture along the temporal axis
%and incorporate flow-guided attention into the U-Net blocks without introducing new parameters.
and combine flow-guided attention (FLATTEN) with dense spatio-temporal attention to avoid introducing any new parameters.
The outcome of dense spatio-temporal attention $\bm{H}$ is further used for FLATTEN.
The keys and values for FLATTEN are gathered from $\bm{H}$ based on the patch trajectories sampled from the optical flow.
%We activate FLATTEN both in DDIM inversion and sampling.
%$\bm{Q}$, $\bm{K}$, and $\bm{V}$ denote query, key, and value respectively. 
The weights of the U-Net $\epsilon_{\theta}$ are frozen.
}
\label{fig:overview}
\vspace{-4mm}
\end{center}
\end{figure}


\subsection{Overall Framework}
\label{sec:FLAT}
Our framework aims to edit the source video $\mathcal{V}$ according to an editing textual prompt $\bm{\tau}$ and output a visually consistent video.
To this end, we expand the U-Net architecture of a T2I diffusion model along the temporal axis inspired by previous works~\citep{wu2022tune,khachatryan2023text2video,zhang2023controlvideo}. Furthermore, to facilitate consistent T2V editing, we incorporate flow-guided attention (FLATTEN) into the U-Net blocks without introducing new parameters. 
To retain the high-fidelity of the generated video, we employ DDIM inversion in the latent space with our re-designed U-Net to estimate the latent noise $\bm{z}_T$ from the source video. We use empty text for DDIM inversion without the need to define a caption for the source video. Lastly, we generate an edited video using the DDIM process with inputs from the latent noise $\bm{z}_T$ and the target prompt $\bm{\tau}$. 
% We use empty text for DDIM inversion to avoid the need for an additional caption of the source video.
%
% The edited video is sampled by the U-Net using the latent noise $\bm{z}_T$ and the textual prompt $\bm{\tau}$. \CS{this is not correct we do not use unet to sample}
%It is built upon the pre-trained text-to-image generation model, Stable Diffusion.
% The edited video is generated by the U-Net model using the latent noise $\bm{z}_T$ and textual prompt $\bm{\tau}$.
Our framework as illustrated in Figure~\ref{fig:overview} is training-free, thus comfortably reducing additional computation. 
%Our method is training-free, and the overview of our framework is demonstrated in Figure~\ref{fig:overview}. 
%\SC{We only provide one equation about DDIM Inversion. maybe we should give more details about this step in our pipeline.}

\vspace{-2mm}
\paragraph{U-Net Inflation} 

The original U-Net architecture employed in an image-based diffusion model comprises a stack of 2D convolutional residual blocks, spatial attention blocks, and cross-attention blocks that incorporate textual prompt embeddings.
To adapt the T2I model to the T2V editing task, we inflate the convolutional residual blocks and the spatial attention blocks.
Similar to previous works \citep{ho2022video, wu2022tune}, the $3\times3$ convolution kernels in the convolutional residual blocks are converted to $1\times3\times3$ kernels by adding a pseudo temporal channel.
In addition, the spatial attention is replaced with a dense spatio-temporal attention paradigm.
In contrast to the spatial self-attention strategy applied to the patches in a single frame, we adopt all patch embeddings across the entire video as the queries ($\bm{Q}$), keys ($\bm{K}$), and values ($\bm{V}$).
This dense spatio-temporal attention can provide a comprehensive perspective throughout the video.
Note that the parameters of the linear projection layers and the feed-forward networks in the new dense spatio-temporal attention blocks are inherited from those in the original spatial attention blocks. 

\vspace{-2mm}
\paragraph{FLATTEN Integration}
To further improve the visual consistency of the output frames, we integrate our proposed flow-guided attention in the extended U-Net blocks.
We combine FLATTEN with dense spatio-temporal attention since both attention mechanisms are designed to aggregate visual context.
Given the latent video features, we first perform dense spatio-temporal attention. 
Specific linear projection layers are employed to convert the patch embeddings of the latent features into the queries, keys, and values, respectively.
The results of dense spatio-temporal attention are denoted as $\bm{H}$.
To avoid introducing newly trainable parameters and preserve the feature distribution, we do not apply new linear transformations to recompute the queries, keys, and values.
We directly use $\bm{H}$ as the input of flow-guided attention. 
Note that no positional encoding is introduced.
%\SC{confused by the `directly` here before reading Eq 7, 8.  we should explicitly say there is no additional encoding on Q,K,V here}
% Furthermore, the queries ($Q$) and keys ($K$) from the dense spatio-temporal attention are reused in the flow-guided attention to avoid introducing new trainable parameters and to maintain the integrity of the feature distribution.
When a patch embedding serves as a query, the corresponding keys and the values for FLATTEN are gathered from the output of dense spatio-temporal attention $\bm{H}$ based on the patch trajectories sampled from optical flow.
More details are demonstrated in Section~\ref{sec:patch_trajectories}.
% After performing the flow-guided attention, the output is broadcast to the latent feature while the final output is computed by the feed-forward network in the dense spatio-temporal attention block.
After performing flow-guided attention, the output is forwarded to the feed-forward network from the dense spatio-temporal attention block.
% We activate the flow-guided attention not only during DDIM denoising but also DDIM inversion to achieve more precise latent noise.
We activate FLATTEN not only during DDIM sampling but also when performing DDIM inversion since using FLATTEN in DDIM inversion allows a more efficient inversion by introducing additional temporal dependencies. 
More details are discussed in Appendix~\ref{appendix:ddim}.

We also implement the feature injection following the image editing method~\citep{tumanyan2023plug}. 
For efficiency, we do not reconstruct the source video but inject the features from DDIM inversion during sampling.
With these adaptations, our framework establishes and enhances the connections between frames, thus contributing to high-quality and highly consistent edited videos.



\subsection{Flow-guided Attention}
\label{sec:patch_trajectories}


\paragraph{Optical Flow Estimation}
Given two consecutive RGB frames from the source video, we use RAFT~\citep{teed2020raft}  to estimate optical flow.
The optical flow between two frames denotes a dense pixel displacement field $(f_x ,f_y)$. 
The coordinates of each pixel $(x_k, y_k)$ in the $k$-th frame can be projected to its corresponding coordinates in the ($k+1$)-th frame based on the displacement field.
The new coordinates in the ($k+1$)-th frame can be formulated as: 
\begin{equation}
\centering
% \: used for a proper space
(x_{k+1}, \; y_{k+1}) = (x_k + f_x(x_k, y_k), \; y_k + f_y(x_k, y_k)).
\label{eq:flow1}
\end{equation}
In order to implicitly use optical flow to guide the attention modules, we downsample the displacement fields of all frame pairs to the resolution of the latent space. 

\vspace{-2mm}
\paragraph{Patch Trajectory Sampling}
We sample the patch trajectories in the latent space based on the downsampled fields $(\hat{f}_x, \hat{f}_y)$. 
We start iterating from the patches on the first frame. 
For a patch with coordinates $(x_0, y_0)$ on the first frame, its coordinates on all subsequent frames can be derived from the displacement field.
The coordinates are linked, and the trajectory sequence can be presented as:
\begin{equation}
\centering
traj = \{ (x_0, y_0), (x_1, y_1), (x_2, y_2), \cdots , (x_K, y_K) \},
\label{eq:flow1}
\end{equation}
where $K$ denotes the frame number of the source video.
For a latent space with the size $H \times W$, there is ideally a trajectory set denoted as $\{ traj_1, traj_2, ..., traj_N \}$, where $N=HW$.
However, certain patches disappear over time, and new patches appear in the video. 
For each new patch that appears in the video, a new trajectory is created.
As a result, the size of the trajectory set $N$ is generally larger than $HW$.
To simplify the implementation of flow-guided attention, when an occlusion happens, we randomly select a trajectory to continue sampling and stop the other conflicting trajectories.
% \SC{Do we need to explain if this random selecting is reasonable}
This strategy ensures that each patch in the video is uniquely assigned to a single trajectory, and there is no case where a patch is on multiple trajectories.
%\SC{`This operation' refers to `randomly select' or `flow-guided attention'? how does this operation `guarantee'?}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/flowattn.pdf}
\caption{Illustration of FLATTEN. We use RAFT to estimate the optical flow of the source video and downsample them to the resolution of the latent space. 
The trajectories of the patches in the latent space are sampled based on the displacement field.
For each query, we gather the patch embeddings on the same trajectory from the latent feature as the corresponding key and value.
The multi-head attention is then performed, and the patch embeddings are updated.
}
\label{fig:flow_attn}
\vspace{-3mm}
\end{center}
\end{figure}

\vspace{-3mm}
\paragraph{Attention Process}
Flow-guided attention is performed on the sampled patch trajectories. 
The overview of FLATTEN is illustrated in Figure~\ref{fig:flow_attn}.
%We gather the patch embeddings on the same trajectory from the latent feature $\bm{z}$ and put them in the same sequence.
We gather the embeddings of the patches on the same trajectory from the latent feature $\bm{z}$.
The patch embeddings on a trajectory $traj$ can be presented as:
\begin{equation}
\centering
\bm{z}_{traj} = \{ \bm{z}(x_0, y_0), \bm{z}(x_1, y_1), \bm{z}(x_2, y_2), \cdots , \bm{z}(x_K, y_K)\},
\label{eq:flow1}
\end{equation}
where $\bm{z}(x_k, y_k)$ %\CS{is this the same {z} as in 3.1? YR: Yes} 
indicates the patch embedding at the coordinates $(x_k, y_k)$ in the $k$-th frame. 
We perform multi-head attention with the patch embeddings on the same trajectory. 
For a query $\bm{z}(x_k, y_k)$, % in $\bm{z}_{traj}$, 
the corresponding keys and values are the other patch embeddings on the same trajectory $traj$.
%The query $\bm{Q}$, the key $\bm{K}$ and the value $\bm{V}$ are the latent embeddings.
No additional position encoding is introduced.
Our flow-guided attention can be formulated as follows:
\begin{align}
\centering
\bm{Q} &= \bm{z}(x_k, y_k), \\
\bm{K} = \bm{V} &= \bm{z}_{traj} - \{\bm{z}(x_k, y_k)\}, \\ %\setminus
\text{Attn}(\bm{Q},\bm{K},\bm{V}) &= \text{Softmax}(\frac{\bm{Q}\bm{K}^T}{\sqrt{d}})\bm{V},
\label{eq:flow1}
\vspace{-2mm}
\end{align}
where $\sqrt{d}$ is a scaling factor.  
The latent features $\bm{z}$ are updated by flow-guided attention to eliminate the negative effects from feature aggregation of irrelevant patches in dense spatio-temporal attention.
Importantly, we ensure that each patch embedding on the latent feature is uniquely assigned to a single trajectory during patch trajectory sampling. 
This assignment resolves conflicts and allows for a comprehensive update of all patch embeddings.

We utilize optical flow to connect the patches in different frames and sample the patch trajectories. 
% \CS{do we have a more detailed explanation how we draw trajectories and tokens from optical flow information? like $(u,v)$ to the token index position.}
Our flow-guided attention facilitates the information exchange between patches on the same trajectory, thus improving visual consistency in video editing.
We integrate FLATTEN into our framework and implement text-to-video editing without any additional training.
Furthermore, FLATTEN can also be easily integrated into any diffusion-based T2V editing method, as shown in Section~\ref{sec:integrability}.



\section{Experiments}

\subsection{Experimental Settings}

\paragraph{Datasets}
We evaluate our text-to-video editing framework with 53 videos sourced from LOVEU-TGVE\footnote{\scriptsize{\url{https://sites.google.com/view/loveucvpr23/track4}}}.
16 of these videos are from DAVIS~\citep{perazzi2016benchmark}, and we denote this subset as TGVE-D.
The other 37 videos are from Videvo, which are denoted as TGVE-V.
The resolution of the videos is re-scaled to $512\times512$.
Each video consists of 32 frames labeled with a ground-truth caption and 4 creative textual prompts for editing.



% \begin{table}[http!]
% \caption{Quantitative results for TGVE-D.}
% \vspace{-3mm}
% \label{tab:quantative1}
% \begin{center}
% \begin{adjustbox}{max width=0.9\textwidth}
% \begin{tabular}{ccccccccc}
% \bottomrule
% Method & CLIP-F $\uparrow$ & CLIP-T $\uparrow$ & PickScore  $\uparrow$ & Error$_{warp}$ $\downarrow$\\
% \hline
% Tune-A-Video  &0.9105   &0.2733 & 20.58 & 0.02923\\
% FateZero &\textbf{0.9258}   &0.2706 & 20.45 & 0.00579\\
% Text2Video-Zero  &0.9239  & 0.2786 & 20.32 & 0.02207\\
% ControlVideo & 0.9168 &  0.2772 & 20.56 & 0.00681\\
% TokenFlow &0.9245 &0.2691 & 20.93 & 0.00536\\
% FLATTEN (ours) & 0.9249  & \textbf{0.2805} & \textbf{20.95} & 0.00492\\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{center}
% \end{table}

% \begin{table}[http!]
% \caption{Quantitative results for TGVE-V}
% \vspace{-3mm}
% \label{tab:quantative2}
% \begin{center}
% \begin{adjustbox}{max width=0.9\textwidth}
% \begin{tabular}{ccccc}
% \bottomrule
% Method & CLIP-F $\uparrow$ & CLIP-T $\uparrow$ & PickScore  $\uparrow$ &  Error$_{warp}$ $\downarrow$\\
% \hline
% Tune-A-Video  &0.9630   & 0.2584 & 20.20 & 0.01538\\
% FateZero & 0.9664   & 0.2572 & 20.09 & 0.00510\\\
% Text2Video-Zero  &0.9684  & 0.2653 &  20.43 & 0.01155\\
% ControlVideo & 0.9655 &  0.2592 & 20.36 & 0.00632\\
% TokenFlow &0.9672 & 0.2557 & 20.61 & 0.00315\\
% FLATTEN (ours) & \textbf{0.9675}   & \textbf{0.2670} & \textbf{20.63} & 0.00316\\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{center}
% \end{table}


\begin{table}[tp!]
\caption{Quantitative results on TGVE-D and TGVE-V.}
\vspace{-3mm}
\label{tab:quantative}
\begin{center}
\begin{adjustbox}{max width=0.99\textwidth}
% \begin{tabular}{c|cccc>{\columncolor{Gray}}c|cccc>{\columncolor{Gray}}c}
\begin{tabular}{c|ccccc|ccccc}
\hline
 \multirow{2}*{Method} & \multicolumn{5}{c|}{TGVE-D} & \multicolumn{5}{c}{TGVE-V} \\
 
 & CLIP-F $\uparrow$ & PickScore  $\uparrow$ & CLIP-T $\uparrow$  &  E$_{warp}$ $\downarrow$ & $\text{S}_{edit}$ $\uparrow$ & CLIP-F $\uparrow$ & PickScore  $\uparrow$ & CLIP-T $\uparrow$ &  E$_{warp}$ $\downarrow$ & $\text{S}_{edit}$ $\uparrow$\\
\hline
Tune-A-Video & 91.05 & 20.58  &27.33  & 29.23 &\cellcolor[HTML]{D0F0C0} 9.35 & 96.30  & 20.20 & 25.84  & 15.38 &\cellcolor[HTML]{D0F0C0} 16.80\\
Text2Video-Zero&92.39 & 20.32 & 27.86  & 22.07 &\cellcolor[HTML]{D0F0C0} 12.62 & \textbf{96.84}  &  20.43 & 26.53 & 11.55 &\cellcolor[HTML]{D0F0C0} 22.97\\
ControlVideo& 91.68 & 20.56 &  27.72  & 6.81 &\cellcolor[HTML]{D0F0C0} 40.70 & 96.55 & 20.36 &  25.92 & 6.32 &\cellcolor[HTML]{D0F0C0} 41.01\\
FateZero &\textbf{92.58} & 20.45  &27.06  & 5.79 &\cellcolor[HTML]{D0F0C0} 46.74 & 96.64 & 20.09  & 25.72  & 5.10 &\cellcolor[HTML]{D0F0C0} 50.43\\\
TokenFlow &92.45 & 20.93 &26.91  & 5.36 &\cellcolor[HTML]{D0F0C0} 50.21 & 96.72 & 20.61 & 25.57 & \textbf{3.15} &\cellcolor[HTML]{D0F0C0} 81.17\\
FLATTEN (ours)& 92.49 & \textbf{20.95} & \textbf{28.05}  & \textbf{4.92} & \cellcolor[HTML]{D0F0C0} \textbf{57.01} & 96.75 & \textbf{20.63}  & \textbf{26.70}  & 3.16 &\cellcolor[HTML]{D0F0C0} \textbf{84.49}\\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-5mm}
\end{center}
\end{table}

\vspace{-3mm}
\paragraph{Evaluation Metrics}
As per standard \citep{wu2022tune,qi2023fatezero, ceylan2023pix2video, tokenflow2023}, we use the following automatic evaluation metrics:
For textual alignment, we use CLIP~\citep{radford2021learning} to measure the average cosine similarity between the edited frames and the textual prompt, denoted as CLIP-T.
To evaluate visual consistency, we adopt the flow warping error $\text{E}_{warp}$~\citep{lai2018learning}, 
% which warps the edited video frames according to the optical flow of the source video and computes the warping error. 
which warps the edited video frames according to the estimated optical flow of the source video and measures the pixel-level difference.
% However, both CLIP-T and $\text{E}_{warp}$ cannot comprehensively represent editing performance.
Using these metrics independently cannot comprehensively represent editing performance.
For instance, 
% $\text{E}_{warp}$ reports a failed edited video that is extremely similar to the source video is also close to 0.
$\text{E}_{warp}$ reports 0 errors when the edited video is exactly the source video.
Therefore, we propose $S_{edit}$ as our main evaluation metric, which combines
% To address this limitation, we introduce a new metric, editing score, denoted as $S_{edit}$, by combining 
CLIP-T and $\text{E}_{warp}$ as a unified score.
Specifically, the editing score is calculated as $S_{edit}$ = CLIP-T/$\text{E}_{warp}$. 
Following the previous work~\citep{wu2022tune}, we also adopt CLIP-F and PickScore, which computes the average cosine similarity between all frames in a video and the estimated alignment with human preferences, respectively.
% as evaluation metrics.
For brevity, the numbers of CLIP-F/CLIP-T/$\text{E}_{warp}$ shown in this paper are scaled up by 100/100/1000.

\vspace{-3mm}
\paragraph{Implementation Details}
% Our framework is built upon Stable Diffusion~\citep{rombach2022high} and we use the official pre-trained weights without any fine-tuning\footnote{
% \tiny{\url{https://huggingface.co/stabilityai/stable-diffusion-2-1-base}}}.
We inflate a pre-trained text-to-image diffusion model and integrate FLATTEN into the U-Net to implement T2V editing without any training or fine-tuning.
To estimate the optical flow of the source videos, we utilize RAFT~\citep{teed2020raft}.
%If not specified, the optical flow are downsampled to $64\times64$ resolution and the flow-guided attention is performed at this resolution.
We find that applying flow-guided attention in DDIM inversion can also improve latent noise estimation by introducing additional temporal dependencies.
Therefore, we use flow-guided attention both in DDIM sampling and inversion.
More details are shown in Appendix~\ref{appendix:ddim}.
We implement 100 timesteps for DDIM inversion and 50 timesteps for DDIM sampling.
Following the image editing method~\citep{tumanyan2023plug}, the diffusion features are saved during DDIM inversion and are further injected during sampling.
To efficiently perform the dense spatio-temporal attention in the modified U-Net, we use xFormers~\citep{xFormers2022}, which can reduce GPU memory consumption.


\subsection{Quantitative Comparison}
\label{sec:quantitative}
We compare our approach with 5 publicly available text-to-video editing methods:
Tune-A-Video~\citep{wu2022tune}, FateZero~\citep{qi2023fatezero}, Text2Video-Zero~\citep{khachatryan2023text2video}, ControlVideo~\citep{zhang2023controlvideo}, and TokenFlow~\citep{tokenflow2023}.
In these methods, Tune-A-Video requires fine-tuning the source videos. 
Both Tune-A-Video and FateZero need the additional caption of the source video, while our model does not.
% FateZero~\citep{qi2023fatezero} allows video editing based on either Stable Diffusion or a finetuned Tune-A-Video model.
% We use Stable Diffusion for FateZero in our experiments.
Text2Video-Zero and ControlVideo use ControlNet~\citep{zhang2023adding} to preserve the structural information.
Edge maps are used as the condition in our experiments, which have better performance than depth maps.
TokenFlow linearly combines the diffusion features based on the correspondences of the source video features.
%Note that our approach is training-free and can edit the videos without additional conditional model.

Table~\ref{tab:quantative} shows the quantitative comparisons of TGVE-D and TGVE-V.
Our approach outperforms other compared methods in terms of CLIP-T, PickScore, and editing score $\text{S}_{edit}$ on both datasets. 
In terms of the warping error $\text{E}_{warp}$, our method is slightly  $0.1\times10^{-3}$ lower than TokenFlow.
While considering textual faithfulness, our CLIP-T score is significantly higher. 
As a result, our method has a higher editing score overall.
Text2Video-Zero has high CLIP-F and CLIP-T, but performs weakly in terms of visual consistency.
% Although FateZero has the highest CLIP-F on TGVE-D, it is sensitive to the hyper-parameter settings for each editing, which may result in less than optimal results.
Although FateZero has the highest CLIP-F on TGVE-D, its output video is sometimes very similar to the source video due to the hyperparameter setting issue.
% The optimal hyperparameters (e.g. blending words and thresholds) required for editing various videos using FateZero vary considerably.
Our approach demonstrates superior performance on all evaluation metrics. 


% \begin{table}[tp!]
% \caption{Quantitative results for TGVE-D and TGVE-V}
% \vspace{-3mm}
% \label{tab:quantative}
% \begin{center}
% \begin{adjustbox}{max width=0.99\textwidth}
% \begin{tabular}{c|cccc|cccc}
% \bottomrule
%  \multirow{2}*{Method} & \multicolumn{4}{c}{TGVE-D} & \multicolumn{4}{|c}{TGVE-V} \\
%  & CLIP-F $\uparrow$ & CLIP-T $\uparrow$ & PickScore  $\uparrow$ &  Error$_{warp}$ $\downarrow$ & CLIP-F $\uparrow$ & CLIP-T $\uparrow$ & PickScore  $\uparrow$ &  Error$_{warp}$ $\downarrow$\\
% \hline
% Tune-A-Video & 91.05   &27.33 & 20.58 & 29.23 & 96.30   & 25.84 & 20.20 & 15.38\\
% FateZero &\textbf{92.58}   &27.06 & 20.45 & 5.79 & 96.64   & 25.72 & 20.09 & 5.10\\\
% Text2Video-Zero&92.39  & 27.86 & 20.32 & 22.07  & \textbf{96.84}  & 26.53 &  20.43 & 11.55\\
% ControlVideo& 91.68 &  27.72 & 20.56 & 6.81 & 96.55 &  25.92 & 20.36 & 6.32\\
% TokenFlow &92.45 &26.91 & 20.93 & 5.36 & 96.72 & 25.57 & 20.61 & \textbf{3.15}\\
% FLATTEN (ours)& 92.49  & \textbf{28.05} & \textbf{20.95} & \textbf{4.92} & 96.75   & \textbf{26.70} & \textbf{20.63} & 3.16\\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \vspace{-5mm}
% \end{center}
% \end{table}


\subsection{Qualitative Results}
\label{sec:qualitative_result}

The qualitative comparison is presented in Figure~\ref{fig:qualitative}.
The source video at the top is from TGVE-D, and the source video at the bottom is from  TGVE-V.
Tune-A-Video generates videos with high quality per frame, but it struggles to preserve the source structure, \textit{e.g.}, the wrong number of trucks.
FateZero sometimes cannot edit the visual appearance based on the prompt, and the output video is almost identical to the source, as shown in the top example.
Both Text2Video-Zero and ControlVideo rely on pre-existing features (\textit{e.g.}, edge maps) provided by ControlNet.
If the source condition features are of low quality, for example, due to motion blur, this leads to an overall decrease in video editing quality.
TokenFlow samples keyframes and performs a linear combination of features to keep visual consistency.
However, the pre-defined combination weights may not be appropriate for all videos.
In the example at the bottom, a white sun intermittently appears and disappears in the frames edited by TokenFlow.
In contrast, our method can generate consistent videos based on the prompt with flow-guided attention.
More qualitative results are shown in Appendix~\ref{appendix:additional}.


\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/qualitative2.pdf}
\vspace{-3mm}
\caption{Qualitative comparison between advanced T2V editing approaches and our method.
The first column shows the source frames from TGVE-D (top) and TGVE-V (bottom), while the other columns present the corresponding frames edited by different methods.
%We kindly suggest that reviewers take the time to view the complete videos provided in our supplementary material.
The complete videos are provided in the supplementary material.
}
% With the flow-guided attention, FLATTEN can preserve as much of the source motion information as possible while editing the semantics of videos.
% Our approach allows dynamic elements within the video to faithfully preserve their original motion. In contrast, static elements, like the desert background (right), remain stable without any flickering, thus improving the overall realism of the edited video.
%With flow-guided attention, FLATTEN can preserve the source motion while editing the visual appearance.
\label{fig:qualitative}
\vspace{-5mm}
\end{center}
\end{figure}



% \subsection{Ablation Study}
% In the ablation studies, we consider how the following aspects influence the final performance.
% All the ablation experiments are performed on TGVE-D.
% More experiments are given in the Appendix.

% \paragraph{Flow-guided Attention for DDIM Inversion}
% The flow-guided attention not only enhances the generation of consistent videos but also improves the DDIM inversion process.
% We conduct DDIM inversion on the source videos and reconstruct them using the denoising model with and without the flow-guided attention respectively.
% When incorporating flow-guided attention into the inversion and reconstruction,
% more details in the source video can be restored, such as the eyes of the goldfish in Figure \ref{fig:reconstruction}.
% Quantitatively, using flow-guided attention results in higher performance metrics, with \textbf{PSNR} (peak signal-to-noise ratio) and \textbf{SSIM} (structural similarity index measure) reaching values of \textbf{33.89dB} and \textbf{0.9159}, respectively, when compared to the source videos. In contrast, without flow-guided attention, \textbf{PSNR} and \textbf{SSIM} values drop to \textbf{32.74dB} and \textbf{0.8974}.


% \begin{figure}
% \centering
% \begin{minipage}[b]{.49\textwidth}
%   \centering
%   \includegraphics[width=.99\linewidth]{figures/recon.pdf}
%   \caption{Using flow-guided attention during DDIM inversion contributes to the restoration of original details. The fish eyes in the third row are successfully reconstructed, while in the second row some are not represented. }
%   \label{fig:reconstruction}
% \end{minipage}%
% \hfill
% \begin{minipage}[b]{.492\textwidth}
%   \centering
%   \includegraphics[width=.99\linewidth]{figures/ablate.pdf}
%   \caption{The effectiveness of the dense spatio-temporal attention (DST-A) and the flow-guided attention (FG-A). 
%   The video (in the last row) generated using both attention modules are more visually consistent.
%   }
%   \label{fig:ablate}
% \end{minipage}
% \end{figure}

% \paragraph{Module Effectiveness}
% To verify the contribution of each module to the overall performance, we deactivate different modules and the results are shown in Table \ref{tab:modules}.
% Without the dense spatio-temporal attention (DST-A) and the flow-guided attention (FG-A), the output videos have poor visual consistency.
% However, due to the lack of dynamic constraints, CLIP Score (Prompt) is slightly higher in this case.
% The temporal dependencies are captured after the dense spatio-temporal attention is activated and CLIP Score (Frame) increases significantly.
% Visual consistency is further improved by integrating the flow-guided attention.
% A qualitative comparison is shown in Figure \ref{fig:ablate}.


\begin{wrapfigure}{r}{5.9cm}
\vspace{-5mm}
\includegraphics[width=5.9cm]{figures/control_flat.pdf}
\vspace{-6mm}
\caption{
FLATTEN can also improve visual consistency for other methods.}
\label{fig:controlvideo_flow}
\vspace{-4mm}
\end{wrapfigure} 

\subsection{Plug-and-Play FLATTEN}
\label{sec:integrability}
FLATTEN can be seamlessly integrated into other diffusion-based T2V editing methods. 
To verify its compatibility, we incorporate FLATTEN into the U-Net blocks of ControlVideo~\citep{zhang2023controlvideo}.
The visual consistency of the videos edited by ControlVideo with FLATTEN is significantly improved, as shown in Figure~\ref{fig:controlvideo_flow}.
The fish (cyan box) in the bottom frame edited by the original ControlVideo disappears while using FLATTEN ensures a consistent visual appearance.
We evaluate the ControlVideo with FLATTEN on TGVE-D. 
After integrating FLATTEN, the warping error E$_{warp}$ decreases remarkably from $6.81$ to $4.78$, while CLIP-T slightly decreases from $27.72$ to $26.97$.
The editing score $S_{edit}$ is improved from $\bm{40.70}$ to $\bm{56.42}$, 
which shows that FLATTEN can improve visual consistency for other T2V editing methods.
% \textbf{By integrating FLATTEN into U-Net blocks of different scales, a trade-off can be made between visual consistency and textual faithfulness.} 



% \begin{minipage}{.7\textwidth}
% \centering
% \begin{adjustbox}{max width=1\textwidth}
% \includegraphics[width=1\linewidth]{figures/control_flat.pdf}
% \end{adjustbox}
% \captionof{figure}{}
%   \label{tab:modules}
% \end{minipage}%
% \hfill
% \begin{minipage}{.29\textwidth}
% \centering
%   \captionof{table}{ verify the contributions of different modules to the }
%   \label{tab:user_study}
% \begin{adjustbox}{max width=1\textwidth}
% \begin{tabular}{ccccc}
% \bottomrule
% Metric & ControlVideo & ControlVideo+FLATTEN  \\
% \hline
% CLIP-T & 27.72& 26.97 \\
% Error$_{warp}$ &6.81& 4.28  \\
% TC-Score  &20.91& 22.69  \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{minipage}



\subsection{Ablation Study}
To verify the contributions of different modules to the overall performance, we systematically deactivated specific modules in our framework.
Initially, we ablate both dense spatio-temporal attention (DSTA) and flow-guided attention (FLATTEN) from our framework.
The dense spatio-temporal attention is replaced by the original spatial attention in the pre-trained image model.
This is viewed as our baseline model (Base).
% This configuration degrades our framework into an image editing framework similar to PNP~\citep{tumanyan2023plug} and is viewed as our baseline model (Base).
As shown in Figure~\ref{fig:ablate}, the edited structure is sometimes distorted.
%Although structural information is preserved through feature injection, the generated frames exhibit inconsistency due to the lack of temporal dependencies, as shown in Figure \ref{fig:ablate}. 
We individually activate DSTA and FLATTEN. 
%\JM{Can we get small picture-in-picture type of zoom-in so we can see the inconsistencies in the Figure a bit better?}
They both can reason about temporal dependencies and enhance structural preservation and visual consistency.
% The dense spatio-temporal attention introduces global information, resulting in higher-quality output videos, while the flow-guided attention allows precise control over visual consistency at a fine-grained level.
As a further step, we combine DSTA and FLATTEN in two distinct ways and explore their effectiveness:
(I) The output of dense spatio-temporal attention is forwarded to the linear projection layers to recompute the queries, keys, and values for FLATTEN;
(II) The output of DSTA is directly used as queries, keys, and values for FLATTEN.
We find that the first combination sometimes results in blurring, which reduces the editing quality.
The second combination performs better and is adopted as the final solution.
The quantitative results for the ablation study on TGVE-D are presented in Table~\ref{tab:modules}.


\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/ablate_wolf.pdf}
\caption{Qualitative results on the effectiveness of flow-guided attention (FLATTEN) and dense spatio-temporal attention (DSTA). 
%We first remove DST-A and FG-A from the model as the baseline and activate them respectively.
%Both DST-A and FG-A contribute to preserving structural information.
%To further improve the visual consistency, we test two methods to combine DST-A and FG-A. The second combination method can generate higher quality videos.
We also explore two combinations of FLATTEN and DSTA.
To easily compare visual consistency, we zoom in on the area of \texttt{nose} in different frames.
In the lower right frames, both the structure as well as the colorization is temporally consistent.
}
\label{fig:ablate}
\vspace{-1mm}
\end{center}
\end{figure}


% \begin{figure}
% \centering
% \begin{minipage}[b]{.49\textwidth}
%   \centering
%   \includegraphics[width=.99\linewidth]{figures/recon.pdf}
%   \caption{Using flow-guided attention during DDIM inversion contributes to the restoration of original details. The fish eyes in the third row are successfully reconstructed, while in the second row some are not represented. }
%   \label{fig:reconstruction}
% \end{minipage}%
% \hfill
% \begin{minipage}[b]{.492\textwidth}
%   \centering
%   \includegraphics[width=.99\linewidth]{figures/ablate.pdf}
%   \caption{The effectiveness of the dense spatio-temporal attention (DST-A) and the flow-guided attention (FG-A). 
%   The video (in the last row) generated using both attention modules are more visually consistent.
%   }
%   \label{fig:ablate}
% \end{minipage}
% \end{figure}

\begin{table}
\centering
\begin{minipage}{.57988\textwidth}
\centering
  \captionof{table}{Ablation results for dense spatio-temporal attention (DSTA), flow-guided attention (FLATTEN), and their combinations on TGVE-D. }
  \label{tab:modules}
\begin{adjustbox}{max width=0.95\textwidth}
 \begin{tabular}{ccccc}
\hline
% Method & CLIP-F $\uparrow$ & CLIP-T $\uparrow$ & Error$_{warp}$ $\downarrow$\\
% \hline
% Base  & 84.12   & 28.36 & 21.6\\
% Base + DST-A & 91.72   & 27.97 & 6.65\\
% Base + FG-A & 91.39  & 28.02 & 6.27\\
% Base + DST-A + FG-A (I) & 92.07  & 27.96 & 5.60\\
% Base + DST-A + FG-A (II) & 92.49  & 28.05 & 4.92& \\
Method & CLIP-T $\uparrow$ & Error$_{warp}$ $\downarrow$ & $\text{S}_{edit}$ $\uparrow$\\
\hline
Base  & 28.36 & 13.40 & \cellcolor[HTML]{D0F0C0} 21.16\\
Base + DSTA & 27.97 & 6.65 & \cellcolor[HTML]{D0F0C0} 42.06\\
Base + FLATTEN   & 28.02 & 6.27 & \cellcolor[HTML]{D0F0C0} 44.69\\
Base + DSTA + FLATTEN (I)  & 27.96 & 5.60 & \cellcolor[HTML]{D0F0C0} 49.93\\
Base + DSTA + FLATTEN (II)  & 28.05 & 4.92&  \cellcolor[HTML]{D0F0C0} 57.01\\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-3mm}
\end{minipage}%
\hfill
\begin{minipage}{.40\textwidth}
\centering
  \captionof{table}{User study of different T2V editing methods. The numbers indicate the average user preference rating (\%). }
  \label{tab:user_study}
\begin{adjustbox}{max width=0.95\textwidth}
\begin{tabular}{cccc}
\hline
Method  & Semantic  & Consistency & Motion  \\
\hline
Tune-A-Video & 18.43& 7.42& 8.18\\
Text2Video-Zero & 11.01& 4.49 & 4.21 \\
ControlVideo & 12.36& 7.42& 3.97 \\
FateZero & 8.09&  13.26& 17.76 \\
TokenFlow & 18.65&  26.74& 24.30 \\
FLATTEN (ours) & \textbf{31.46}& \textbf{41.12}& \textbf{41.59} \\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-3mm}
\end{minipage}
\end{table}

\vspace{-1mm}
\subsection{User Study}
\vspace{-1mm}
% Although automatic evaluation metrics can reflect the quality of the generated video, they are not fully representative of human perception.
We conduct a user study %to compare our approach with other T2V editing methods 
since automatic metrics cannot fully represent human perception.
We collect 180 edited videos and divide them into 30 groups.
Each group consists of 6 videos edited by different methods with the same source video and prompt. 
We asked 16 participants to vote on their preference from the following perspectives:
(1) \textbf{semantic} alignment (2) visual \textbf{consistency}, and (3) \textbf{motion} and structure preservation.
The average user preference rating is shown in Table~\ref{tab:user_study}. 
Our method achieves higher user preference
in all perspectives.
More details are shown in Appendix~\ref{appendix:user}.

\vspace{-1mm}
\section{Conclusion}
\vspace{-1mm}
% We propose FLATTEN, a novel flow-guided attention to improve the visual consistency for text-to-video editing and present a training-free framework which achieves the new state-of-the-art performance on the existing T2V editing benchmarks.
% Furthermore, FLATTEN can also be seamlessly integrated into other diffusion based T2V editing methods to improve their visual consistency.
% We conduct comprehensive experiments to validate the effectiveness of our method and benchmark the task of T2V editing.
% Our method is designed for highly consistent T2V editing and is therefore limited in dramatic shape editing (\textit{e.g.,} changing sharks into drones).
% This could be solved by combining FLATTEN with other shape-aware editing methods.
% % Our approach demonstrates superior performance, especially in maintaining the visual consistency for edited videos.
% The code will be available after publication.
We propose FLATTEN, a novel flow-guided attention to improve the visual consistency for text-to-video editing, and present a training-free framework that achieves the new state-of-the-art performance on the existing T2V editing benchmarks.
Furthermore, FLATTEN can also be seamlessly integrated into any other diffusion-based T2V editing methods to improve their visual consistency.
We conduct comprehensive experiments to validate the effectiveness of our method and benchmark the task of text-to-video editing.
Our approach demonstrates superior performance, especially in maintaining the visual consistency for edited videos.


\clearpage
% \section*{Ethics Statement}
% As generative AI becomes more widely used in everyday life, it is necessary to face up to the potential social impact of our work.
% Text-to-image and text-to-video generative models bring along various ethical concerns, which are also found and possibly heightened in generative video editing models such as ours.
% Since our text-to-video editing model is built on a pre-trained text-to-image generation model, it inherits the possible limitations and biases of the image generation model.
% Moreover, our model's ability to edit videos with precise semantic content enhances the flexibility of deep fake applications.
% It may be used to violate portrait rights or create false information.
% On the other hand, our model facilitate real-world applications like social media creation, filming, advertisement, AR/MR, \textit{etc.}, and  significantly increase flexibility, productivity, and efficiency.

% \section*{Reproducibility Statement}
% Our text-to-video editing method is training-free and therefore easily reproducible. 
% We utilized the publicly available RAFT model for optical flow prediction~\citep{teed2020raft}. 
% Additionally, we employed the same pre-trained text-to-image generation model used in other T2V editing methods~\citep{wu2022tune,qi2023fatezero,zhang2023controlvideo,khachatryan2023text2video,tokenflow2023}, which is also publicly accessible. 
% To facilitate reproducibility, we have already provided an overview of our method in Figure~\ref{fig:overview} and a comprehensive illustration in Figure~\ref{fig:flow_attn}.

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\appendix

% \section*{Appendix}
% In the appendix, we first demonstrate that FLATTEN can improve DDIM inversion and can contribute to high-quality latent noise estimation in Appendix~\ref{appendix:ddim}. 
% The additional qualitative editing results are demonstrated in Appendix~\ref{appendix:additional}. 
% We kindly suggest that reviewers take the time to view the complete videos provided in our supplementary material.
% The user study details are discussed in Appendix~\ref{appendix:user}. 
% Furthermore, we discuss the limitations of our method in Appendix~\ref{appendix:limitations}.

\section{DDIM Inversion with FLATTEN}
\label{appendix:ddim}

Flow-guided attention (FLATTEN) can also improve the DDIM inversion process, which is critical in our T2V editing framework.
We have validated the effectiveness of FLATTEN for the editing task in the ablation study (see Table~\ref{tab:modules}).
To further demonstrate that FLATTEN can contribute to high-quality latent noise estimation, we perform DDIM inversion on the source videos and reconstruct them using the U-Net with and without FLATTEN, respectively.
When activating FLATTEN during DDIM inversion, more details in the source video can be restored, such as the eyes of the goldfish in Figure \ref{fig:reconstruction}.
Quantitatively, using FLATTEN results in higher scores for reconstruction metrics, with PSNR (peak signal-to-noise ratio) and  SSIM (structural similarity index measure) reaching the values of 33.89dB and 0.9159, respectively. 
In contrast, PSNR and SSIM of the reconstruction without FLATTEN drop to 32.74dB and 0.8974.
The quantitative results are shown in Table \ref{tab:recon}.


\begin{table}[h!]
\caption{The results of DDIM inversion and reconstruction with and without FLATTEN.}
\vspace{-3mm}
\label{tab:recon}
\begin{center}
\begin{adjustbox}{max width=0.99\textwidth}
\begin{tabular}{ccc}
\hline
Method  & PSNR  $\uparrow$  & SSIM $\uparrow$   \\
\hline
\textit{w/o} FLATTEN & 32.74dB & 0.8974 \\
\textit{w/} FLATTEN & 33.89dB & 0.9159 \\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-5mm}
\end{center}
\end{table}

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=1\linewidth]{figures/recon.pdf}
\vspace{-3mm}
\caption{Using FLATTEN during DDIM inversion helps to improve the quality of the estimated latent noise.
This is reflected in video reconstruction.
The fish eyes and other details in the \textbf{third} column are successfully reconstructed, while in the \textbf{second} column, some details are missing. 
}
\label{fig:reconstruction}
\vspace{-3mm}
\end{center}
\end{figure}

\section{Additional Qualitative Results}
\label{appendix:additional}
The additional qualitative results are shown in Figure~\ref{fig:additional1} and Figure~\ref{fig:addition2}.
%\textbf{We kindly suggest that reviewers take the time to view the complete videos provided in our supplementary material}.
With flow-guided attention, our training-free framework enables high-quality and highly consistent T2V editing.
% We use the same text-to-image generation model as TokenFlow~\citep{tokenflow2023}.

To further demonstrate the visual consistency of videos generated by our approach, we provide the additional qualitative comparisons, which are shown in %Figure~\ref{fig:com1} and 
Figure~\ref{fig:com2}.
The videos produced by FLATTEN exhibit superior quality, characterized by a remarkable level of visual consistency and semantic alignment.

\begin{figure}[http!]
\begin{center}
\includegraphics[width=1\linewidth]{figures/addition3.pdf}
\caption{Additional qualitative results. 
The complete videos are provided in the supplementary material.
%We kindly suggest that reviewers take the time to view the complete videos provided in our supplementary material.
}
\label{fig:additional1}
\end{center}
\end{figure}

% \begin{figure}[http!]
% \begin{center}
% \includegraphics[width=1\linewidth]{figures/com1.pdf}
% \caption{Qualitative comparison between advanced text-to-video editing approaches and FLATTEN.}
% \label{fig:com1}
% \end{center}
% \end{figure}

\begin{figure}[http!]
\begin{center}
\includegraphics[width=1\linewidth]{figures/com2.pdf}
\caption{Qualitative comparison between advanced text-to-video editing approaches and FLATTEN.}
\label{fig:com2}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=1\linewidth]{figures/addition2.pdf}
\caption{Our approach can output highly consistent videos conditional on different textual prompts.
}
\label{fig:addition2}
\vspace{-3mm}
\end{center}
\end{figure}





\section{User Study Details}
\label{appendix:user}
We randomly sampled 30 source videos from TGVE-D and TGVE-V then edit them with 6 text-to-video editing approaches, including Tune-A-Video~\citep{wu2022tune}, FateZero~\citep{qi2023fatezero}, Text2Video-Zero~\citep{khachatryan2023text2video},  ControlVideo~\citep{zhang2023controlvideo}, ControlNet~\citep{zhang2023adding}, TokenFlow~\citep{tokenflow2023} and our FLATTEN.
For each group, we asked 16 participants to vote on their preference for 6 edited videos from the following perspectives:
\begin{itemize}
    \item \textbf{Semantic} Alignment: The edited videos should match the given editing prompt.
    \item Visual \textbf{Consistency}: The adjacent frames in the edited videos should be smooth.
    \item \textbf{Motion} and Structure Preservation: The motion/structure of the edited videos should align with the source video.
\end{itemize}
An example of our user study interface is shown in Figure~\ref{fig:user}.


\begin{figure}[ht!]
\begin{center}
\fbox{\includegraphics[width=0.7889\linewidth]{figures/userinterface.png}}
\caption{An example of our user study interface. Given a source video with an editing prompt, users should select their preferred video from 6 videos edited by different T2V editing methods from different perspectives (\textit{e.g.,} visual consistency).
}
\label{fig:user}
\end{center}
\end{figure}


\section{Limitations}
\label{appendix:limitations}
Our approach is designed for highly consistent text-to-video editing utilizing optical flow from the source video.
Therefore, our approach excels in style transfer, coloring, and texture editing but is relatively limited in dramatic structure editing. 
% (\textit{e.g.,} changing sharks into drones, as shown in Figure~\ref{fig:limit}).
A failure case is demonstrated in Figure~\ref{fig:limit}.
The shape of sharks is completely different from quadrotor drones.
The model changes the original sharks into ``mechanical sharks'', but not drones.

% In addition, compared to the T2V editing methods (\textit{e.g.,} ControlVideo \citep{zhang2023controlvideo}) that use random noise, our T2V editing method, like others that require DDIM inversion, takes more time for editing.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.75\linewidth]{figures/limit.pdf}
\caption{Our approach is relatively limited in dramatic structure editing, \textit{e.g.,} turning sharks into drones.
}
\label{fig:limit}
\vspace{-3mm}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/trajectories1.pdf}
\caption{Visualization of the patch trajectories. The trajectories are computed based on the downsampled flow fields ($64\times64$) and the patches on the trajectories are marked with red dots.}
\label{fig:trajectories}
\end{center}
\end{figure}



\section{Trajectory Visualization}
\label{appendix:trajectory}

The flow estimator, Raft~\citep{teed2020raft}, has demonstrated its superior performance in many applications, being able to accurately predict the flow field of dynamic videos.
To demonstrate the robustness of the flow field estimation, we sample several predicted trajectories for video examples with large motion and visualize the trajectories in Figure~\ref{fig:trajectories}. 
RAFT is robust even for videos with large and abrupt motions.
Note that our approach does not rely on any specific flow estimation module. 
The trajectory prediction could be more precise with better flow estimation models in the future.



\section{Robustness to Flows}
\label{appendix:robustness}


One notable advantage of our method is the integration of the flow field into the attention mechanism, significantly enhancing adaptability and robustness. 
To further demonstrate the robustness of FLATTEN to the pre-computed optical flows, we add random Gaussian noise to the pre-computed flow field and use the corrupted flow field for video editing. The qualitative comparison is shown in Figure~\ref{fig:flownoise}. 
The corrupted flow field results in a few artifacts in the edited video (3rd row). 
However, the editing result is still better than the output of the baseline model without using optical flow as guidance.

Moreover, we replace the optical flow from RAFT in flow-guided attention with the flow estimated by another flow prediction model, GMA~\citep{jiang2021learning}. The comparison is shown in Figure~\ref{fig:gma}. There is no obvious difference between the output videos and it shows that our method is robust to small differences in patch trajectories.



\begin{figure}[http!]
\begin{center}
\includegraphics[width=1\linewidth]{figures/flow_noise.pdf}
\caption{Video editing results from the baseline model (1st row), FLATTEN with the Raft flow (2nd row), and FLATTEN with the noised flow (3rd row).}
\label{fig:flownoise}
\end{center}
\end{figure}

\begin{figure}[http!]
\begin{center}
\includegraphics[width=0.8\linewidth]{figures/gma.png}
\caption{Comparison between using the optical flow from Raft (left) and GMA (right).}
\label{fig:gma}
\end{center}
\end{figure}


\section{Runtime Evaluation}

To compare the computational cost of different text-to-video editing models, we measure the runtime required to edit a single video (with 32 frames) by the different models. 
The runtime of the different models at different stages on a single A100 GPU is shown in Table~\ref{tab:runtime}. 
Our model has a relatively short runtime in the sampling stage and there is scope for further improvement.

\begin{table}[h!]
\caption{Runtime evaluation of different T2V editing models.}
\vspace{-3mm}
\label{tab:runtime}
\begin{center}
\begin{adjustbox}{max width=0.99\textwidth}
\begin{tabular}{cccc}
\hline
Method  & Finetuning  & DDIM Inversion & Sampling  \\
\hline
Tune-A-Video~\citep{wu2022tune} & 11min15s & 3min52s & 3min34s \\
Text2Video-Zero~\citep{khachatryan2023text2video} &- & - & 3min17s \\
ControlVideo~\citep{zhang2023controlvideo} &  	- &  	- & 4min36s \\
FateZero~\citep{qi2023fatezero} & - & 4min56s & 4min49s \\
TokenFlow~\citep{tokenflow2023} & - & 3min41s & 3min29s \\
FLATTEN (ours) & - & 3min52s & 3min45s \\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-5mm}
\end{center}
\end{table}

 
\end{document}
