\section{Related Work}
\label{sec:relat}

The literature on synthesizing 3D representations conditioned on natural language based text instructions, is constantly growing, in part thanks to advances in generative models**Newell et al., "Indirect Object Manipulation"**. For our work, two types of approaches are especially relevant: works focusing on indoor 3D scene synthesis; and works on 3D human-scene interaction.

\subsection{3D scene synthesis} 
Indoor scene synthesis, which focuses on generating reasonable furniture arrangements that satisfy room functionality in a 3D space, has been widely studied in the past decades. Using inputs such as floor plans**Choi et al., "Floorplan-based Indoor Scene Synthesis"****Li et al., "Scene Graphs for Indoor Scene Synthesis"**, scene graphs**Li et al., "Scene Graphs for Indoor Scene Synthesis"****Bianchi et al., "Scene Graph Generation"**, and natural language descriptions**Zhu et al., "Text-to-3D Scene Generation"**, controllable scene synthesis has shown promising performance. Early works**Fan et al., "Generative Adversarial Networks for 3D Indoor Scene Synthesis"****Li et al., "Autoregressive Generative Models for 3D Scene Synthesis"** employ CNN-based or transformer-based autoregressive generative models to progressively estimate 3D layout, usually consisting of object category, position, size and orientation. However, these sequential approaches can lead to the accumulation of prediction errors as they may struggle to accurately represent inter-object relationships. Recent progress**Ho et al., "Diffusion Models for 3D Scene Synthesis"****Sitzmann et al., "Implicit Neural Representations for 3D Scenes"** has been made by leveraging denoising diffusion probabilistic models, achieving visually coherent scenes. While great progress has been made, inherent problems of 3D indoor scene synthesis still exist. Most of the existing methods concentrate on improving the accuracy of 3D layout and/or 3D object generation. However, the method capability in applications are restricted, without taking the object-level functionality of the synthesized scenes into account. A few works**Zhu et al., "Object-Level Indoor Scene Synthesis"****Li et al., "Functionality-Aware 3D Scene Generation"** attempt to emphasize the object functionality, however, suffer from long-tail distribution problems. With the recent advances in foundation models, Vision-language models (VLMs) (e.g.,**Anderson et al., "Visual Commonsense Reasoning"**) are employed to bridge 2D and 3D representations, such as GenZI****Savinov et al., "Learning a Probabilistic Latent Space for Image Modeling"**. Besides, some recent works (e.g.,**Ramesh et al., "Hierarchical Text-to-Image Generation"**) investigate large language models (LLMs) for visual planning. This motivates us to leverage the reasoning ability of LLMs for object functional priors.

\subsection{3D human-scene interaction} 
Synthesizing humans interacting with the 3D environment is a key challenge for advancing 3D scene understanding applications. Most attempts have been made to generate human poses and motions given a 3D scene. POSA****Pavlakos et al., "POSE, ORIENT, SIZE: One Shot Monocular Human Parsing"** introduced an ego-centric representation grounded in the surface-based 3D human model SMPL-X****Lassner et al., "SMPL-X: Model and Data Collection"**, incorporating contact labels and scene semantics. To model potential interactions, they utilized a Conditional Variational Autoencoder (CVAE) based on the SMPL-X vertex positions.
Dynamic human-object interactions have also been recently explored, e.g., text-to-motion synthesis****Hernandez et al., "Text-to-Motion Synthesis"**. However, these aforementioned methods rely on accurate 3D scenes, and a noisy scene mesh can lead to penetration between the human body and scene. Another line of work takes the opposite framework. Pose2Room****Wang et al., "Pose2Room: Pose Estimation for Indoor Scene Understanding"**, SUMMON****Kim et al., "SUMMON: Summarization of Multi-Object Motion"**, MIME****Li et al., "MIME: Motion and Object Interaction in 3D Scenes"** , SHADE****Gao et al., "SHADE: Scene Harmonious Analysis for Dynamic Environments"** generate 3D scenes from floor plans and human activities. However, these approaches require human motion sequences during inference, which are not always easily accessible, limiting their applicability in human-conditioned scene synthesis. To summarize, recent developments in 3D human-scene interaction have been separately investigated in two distinct areas: scene-conditioned human synthesis and human-conditioned scene synthesis. Instead, we aim at going beyond the previous paradigms by leveraging geometric human priors to enhance the functionality of synthesized 3D scenes, creating a more integrated approach to human-perceived 3D scene synthesis.