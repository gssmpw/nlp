\section{Related Work}
\label{sec:related-work}
NL2SH translation is a well-studied natural language processing (NLP) task. Table \ref{tab:related} summarizes the contributions of previous work by listing the names of datasets, functional equivalence heuristics (FEHs), and models used in this field. The table is sparsely populated because the majority of previous work focused on improving a NL2SH dataset, FEH, or model in isolation.

\begin{table}[ht!]
  \scriptsize
  \setlength{\tabcolsep}{2pt}
  \centering
  \caption{Summary of NL2SH datasets, FEHs, and models created in previous work.}
  \begin{tabular}{llll}
    \hline
    \textbf{Citation}       & \textbf{Datasets} & \textbf{FEHs} & \textbf{Models}  \\ \hline
    ____         & NL2Bash           & -             & -                \\
    ____           & -                 & -             & AInix            \\
    ____     & -                 & NL2CMD        & Tellina          \\
    ____          & -                 & -             & Magnum           \\
    ____        & NL2CMD            & -             & -                \\
    ____             & -                 & -             & AST              \\
    ____        & -                 & -             & T5, GPT2         \\
    ____        & -                 & -             & ShellGPT         \\
    ____       & InterCode-Bash    & InterCode     & -                \\
    ____        & text\_to\_bash    & -             & -                \\
    ____        & MultiPL-E         & Unit Tests    & -                \\
    ____            & -                 & TSED          & -                \\
    ____        & CodeSift          & CodeSift      & -                \\
    ____ & IBM\_Instana      & Podman        & -                \\
    ____        & LinuxCmds         & -             & -                \\
    ____            & -                 & -             & Warp AI          \\
    ____       & -                 & -             & shell-gpt        \\
    ____         & -                 & -             & Copilot CLI      \\
    ____            & -                 & -             & CodeWhisperer    \\
    ____            & -                 & -             & AI Shell         \\
    ____     & -                 & -             & ScriptSmith      \\
    ____  & -                 & -             & CodeLlama2       \\ \hline
    \textbf{Ours (2025)}    & NL2SH-ALFA        & IC-ALFA       & Llama, Qwen, GPT \\ \hline
  \end{tabular}
  \label{tab:related}
\end{table}

The 2020 NeurIPS NLC2CMD Competition formalized the task of NL2SH translation by providing a human-curated NL2Bash dataset of 9,305 instruction-command pairs and the NL2CMD benchmark for evaluating submitted models ____. The competition resulted in numerous NL2SH models and showed that fine-tuning a pre-trained foundation model could outperform dedicated transformer ____, recurrent neural network ____, abstract syntax tree ____, and sequence to sequence ____ based models for the task of NL2SH translation ____.

The NL2CMD benchmarks's FEH parses commands and assigns a similarity score based on the utilities used, order of utilities, and number of utility flags. This heuristic outperforms conventional string comparison techniques, such as edit distance, for measuring the functional equivalence of commands. However, ____ state that the NL2CMD FEH could be improved by executing commands and measuring the similarity of the outputs and side effects. Verification by execution is preferable because Bash is a Turing-complete language, so verifying the equivalence of two commands before execution is undecidable due to side effects ____. Despite this known shortcoming, the NL2CMD benchmark is widely used for model evaluations ____.

____ address this shortcoming with the InterCode-Bash benchmark. The benchmark uses a subset of 224 instruction-command pairs from the NL2Bash dataset for testing. InterCode's FEH executes the model command and ground truth command in identical Docker containers ____. The results of command execution are then compared using three checks. First, the pre and post-execution states of each container are compared using git-diff. Second, the file contents of each container are compared using MD5 hashes. Third, the standard output of both commands are vectorized and compared using the term frequency, inverse document frequency (TFIDF) method ____. If every check finds the execution results identical, the model and ground truth command are considered functionally equivalent ____. Although this method is more accurate than previous heuristics, it will fail to identify a valid model command that has syntactically different output from the ground truth command.

____ and ____ present similar execution-based frameworks for Jupyter Notebooks and VHDL code, respectively. ____ describe an execution-based framework similar to the InterCode-Bash benchmark using Podman containers. Unfortunately, the code for their FEH and the 50 instruction-command pairs in their test dataset are not public.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.99\textwidth]{figs/icalfa.png}
  \caption{A diagram of NL2SH translation with a comparison of functional equivalence heuristics.}
  \label{fig:icalfa}
\end{figure*}

Focusing on the FEH, ____ present a novel benchmark that uses ____'s GPT-4 model to determine functional equivalence. Their FEH passes the model command and ground truth command to GPT-4 with the prompt, \textit{"Given 2 Bash commands, please generate a similarity score from 0 to 1."} ____ find this method fails to determine functional equivalence because current LLMs are unable to emulate command execution. ____ and ____ confirm this finding with broader evaluations of LLM's ability to determine semantically equivalent or different pairs of programs. They find LLMs show a lack of depth in understanding code semantics.

____ attempt to advance the work presented by ____, by addressing the scalability constraints of execution-based frameworks. Their FEH, CodeSift, uses an LLM to convert the model command to a natural language description. Then they compare this natural language description with the original natural language task using an LLM. While they find CodeSift to be more effective than conventional FEHs, their work lacks comparison with execution based heuristics. Further, their FEH introduces uncertainty by requiring accurate Bash to natural language translation, which is equally as challenging as the task they aim to measure, natural language to Bash translation. Their follow on work concludes that \textit{"executing [Bash] scripts within a controlled environment would offer more reliable assessments"} ____.

The MultiPL-E benchmark is widely used for evaluating code generation models, containing 540 Bash scripting tasks ____. This benchmark uses unit tests to determine if a generated script produces the expected output for a given input. The use of unit tests is sufficient for this benchmark because its tasks, such as string manipulation and math calculations, are deterministic and result in simple outputs. This method fails to assess file manipulation, system administration and network management tasks because they produce more complex outputs than what can reasonably be assessed with unit tests.

Current SOTA NL2SH models use general purpose LLMs for translation. In practice, users can either accept, reject, or edit the model translation ____. A human-in-the-loop approach is necessary because the models may produce incorrect translations ____. Efforts to improve model performance include fine-tuning and prompt engineering. ____ fine-tune the BART, T5, and GPT-2 models on the NL2Bash dataset and find model performance improves as measured by the NL2CMD benchmark. ____ conducts a similar study, fine-tuning the CodeLlama2 model on the NL2Bash dataset. Unfortunately, neither of these studies evaluate their models using a reliable execution-based benchmark.

We begin by creating verified and expanded NL2SH datasets starting from multiple datasets presented in previous work. Next, we combine the InterCode execution FEH presented by ____ with the language model evaluation presented by ____. Using our new datasets and FEH, we evaluate methods for improving model performance. Figure \ref{fig:icalfa} summarizes the shortcomings of FEHs in previous work.