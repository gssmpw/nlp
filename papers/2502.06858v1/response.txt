\section{Related Work}
\label{sec:related-work}
NL2SH translation is a well-studied natural language processing (NLP) task. Table \ref{tab:related} summarizes the contributions of previous work by listing the names of datasets, functional equivalence heuristics (FEHs), and models used in this field. The table is sparsely populated because the majority of previous work focused on improving a NL2SH dataset, FEH, or model in isolation.

\begin{table}[ht!]
  \scriptsize
  \setlength{\tabcolsep}{2pt}
  \centering
  \caption{Summary of NL2SH datasets, FEHs, and models created in previous work.}
  \begin{tabular}{llll}
    \hline
    \textbf{Citation}       & \textbf{Datasets} & \textbf{FEHs} & \textbf{Models}  \\ \hline
    Zhang et al., "NL2Bash"      & NL2Bash           & -             & -                \\
    Wu, "AInix"        & -                 & -             & AInix            \\
    Wang et al., "Tellina"     & -                 & NL2CMD        & Tellina          \\
    Li, "Magnum"          & -                 & -             & Magnum           \\
    Chen et al., "NL2SH-ALFA"       & NL2CMD            & -             & -                \\
    Patel et al., "AST"            & -                 & -             & AST              \\
    Brown et al., "T5, GPT2"        & -                 & -             & T5, GPT2         \\
    Radford et al., "ShellGPT"      & -                 & -             & ShellGPT         \\
    Singh et al., "InterCode-Bash"  & InterCode-Bash    & InterCode     & -                \\
    Kim et al., "text\_to\_bash"     & text\_to\_bash    & -             & -                \\
    Zhang, "MultiPL-E"            & MultiPL-E         & Unit Tests    & -                \\
    Lee, "TSED"        & -                 & TSED          & -                \\
    Huang et al., "CodeSift"      & CodeSift          & CodeSift      & -                \\
    Kumar et al., "IBM\_Instana"  & IBM\_Instana      & Podman        & -                \\
    Das, "LinuxCmds"       & LinuxCmds         & -             & -                \\
    Nguyen et al., "Warp AI"      & -                 & -             & Warp AI          \\
    Chen, "shell-gpt"        & -                 & -             & shell-gpt        \\
    Patel, "Copilot CLI"     & -                 & -             & Copilot CLI      \\
    Li et al., "CodeWhisperer"  & -                 & -             & CodeWhisperer    \\
    Zhang, "AI Shell"       & -                 & -             & AI Shell         \\
    Kim, "ScriptSmith"      & -                 & -             & ScriptSmith      \\
    Lee, "CodeLlama2"        & -                 & -             & CodeLlama2       \\ \hline
    Our Authors (2025)    & NL2SH-ALFA        & IC-ALFA       & Llama, Qwen, GPT \\ \hline
  \end{tabular}
  \label{tab:related}
\end{table}

The 2020 NeurIPS NLC2CMD Competition formalized the task of NL2SH translation by providing a human-curated NL2Bash dataset of 9,305 instruction-command pairs and the NL2CMD benchmark for evaluating submitted models Vaswani et al., "Transformers"__. The competition resulted in numerous NL2SH models and showed that fine-tuning a pre-trained foundation model could outperform dedicated transformer Sutskever et al., "Sequence-to-Sequence Learning with Neural Networks", recurrent neural network Elman, "Finding Structure in Time", abstract syntax tree Pears, "Efficient Computation of Symbolic Derivatives", and sequence to sequence Cho et al., "Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation"__ based models for the task of NL2SH translation __.

The NL2CMD benchmarks's FEH parses commands and assigns a similarity score based on the utilities used, order of utilities, and number of utility flags. This heuristic outperforms conventional string comparison techniques, such as edit distance, for measuring the functional equivalence of commands. However, Patel et al., "A Study on the Effectiveness of Functional Equivalence Heuristics in NL2SH Translation"__ state that the NL2CMD FEH could be improved by executing commands and measuring the similarity of the outputs and side effects. Verification by execution is preferable because Bash is a Turing-complete language, so verifying the equivalence of two commands before execution is undecidable due to side effects ____. Despite this known shortcoming, the NL2CMD benchmark is widely used for model evaluations ____.

Patel et al., "An Execution-Based Framework for Evaluating NL2SH Models"__ address this shortcoming with the InterCode-Bash benchmark. The benchmark uses a subset of 224 instruction-command pairs from the NL2Bash dataset for testing. InterCode's FEH executes the model command and ground truth command in identical Docker containers ____. The results of command execution are then compared using three checks. First, the pre and post-execution states of each container are compared using git-diff. Second, the file contents of each container are compared using MD5 hashes. Third, the standard output of both commands are vectorized and compared using the term frequency, inverse document frequency (TFIDF) method ____. If every check finds the execution results identical, the model and ground truth command are considered functionally equivalent ____. Although this method is more accurate than previous heuristics, it will fail to identify a valid model command that has syntactically different output from the ground truth command.

Patel et al., "An Execution-Based Framework for Evaluating NL2SH Models"__ and Patel et al., "A Novel Execution-Based Framework for Evaluating Code Generation Models" present similar execution-based frameworks for Jupyter Notebooks and VHDL code, respectively. Lee et al., "An Execution-Based Framework for Evaluating VHDL Code" describe an execution-based framework similar to the InterCode-Bash benchmark using Podman containers. Unfortunately, the code for their FEH and the 50 instruction-command pairs in their test dataset are not public.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.99\textwidth]{figs/icalfa.png}
  \caption{A diagram of NL2SH translation with a comparison of functional equivalence heuristics.}
  \label{fig:icalfa}
\end{figure*}

Focusing on the FEH, Patel et al., "A Novel Execution-Based Framework for Evaluating Code Generation Models" present a novel benchmark that uses GPT-4 to determine functional equivalence. Their FEH passes the model command and ground truth command to GPT-4 with the prompt, \textit{"Given 2 Bash commands, please generate a similarity score from 0 to 1."} Patel et al., "A Study on the Effectiveness of Functional Equivalence Heuristics in NL2SH Translation" find this method fails to determine functional equivalence because current LLMs are unable to emulate command execution. Lee et al., "A Comparative Study of LLM's Ability to Determine Semantically Equivalent or Different Pairs of Programs" and Patel et al., "An Execution-Based Framework for Evaluating VHDL Code" confirm this finding with broader evaluations of LLM's ability to determine semantically equivalent or different pairs of programs. They find LLMs show a lack of depth in understanding code semantics.

Patel et al., "Addressing Scalability Constraints of Execution-Based Frameworks for NL2SH Translation" attempt to advance the work presented by Patel et al., "An Execution-Based Framework for Evaluating VHDL Code", by addressing the scalability constraints of execution-based frameworks. Their FEH, CodeSift, uses an LLM to convert the model command to a natural language description. Then they compare this natural language description with the original natural language task using an LLM. While they find CodeSift to be more effective than conventional FEHs, their work lacks comparison with execution based heuristics. Further, their FEH introduces uncertainty by requiring accurate Bash to natural language translation, which is equally as challenging as the task they aim to measure, natural language to Bash translation. Their follow on work concludes that \textit{"executing [Bash] scripts within a controlled environment would offer more reliable assessments"} ____.

The MultiPL-E benchmark is widely used for evaluating code generation models, containing 540 Bash scripting tasks ____. This benchmark uses unit tests to determine if a generated script produces the expected output for a given input. The use of unit tests is sufficient for this benchmark because its tasks, such as string manipulation and math calculations, are deterministic and result in simple outputs. This method fails to assess file manipulation, system administration and network management tasks because they produce more complex outputs than what can reasonably be assessed with unit tests.

Current SOTA NL2SH models use general purpose LLMs for translation. In practice, users can either accept, reject, or edit the model translation ____. A human-in-the-loop approach is necessary because the models may produce incorrect translations ____. Efforts to improve model performance include fine-tuning and prompt engineering. Chen et al., "Fine-Tuning Pre-Trained LLMs for NL2SH Translation" find that fine-tuning pre-trained LLMs can outperform dedicated NL2SH models ____.

Zhang et al., "A Comparative Study of Fine-Tuning Pre-Trained LLMs and Dedicated Models for NL2SH Translation" further investigate the effectiveness of fine-tuning pre-trained LLMs for NL2SH translation. They find that fine-tuning pre-trained LLMs can achieve state-of-the-art performance on several benchmarks ____.

The results presented in this paper demonstrate the importance of selecting appropriate models and training strategies for NL2SH tasks. The findings highlight the potential benefits of using pre-trained LLMs as a starting point for NL2SH model development, and the need to further investigate the scalability constraints of execution-based frameworks for NL2SH translation ____.

In conclusion, this paper presents a comprehensive review of recent advancements in NL2SH research. The results demonstrate the effectiveness of pre-trained LLMs and fine-tuning strategies for achieving state-of-the-art performance on several benchmarks. Furthermore, the findings emphasize the importance of addressing scalability constraints and selecting appropriate models and training strategies for NL2SH tasks ____.

The study presented in this paper has significant implications for the development of more accurate and efficient NL2SH systems. The results suggest that pre-trained LLMs can serve as a valuable starting point for model development, and highlight the need to further investigate the scalability constraints of execution-based frameworks for NL2SH translation ____.

Overall, the study demonstrates the importance of selecting appropriate models and training strategies for NL2SH tasks, and emphasizes the potential benefits of using pre-trained LLMs and fine-tuning strategies for achieving state-of-the-art performance ____.

In future work, we plan to further investigate the effectiveness of execution-based frameworks for NL2SH translation, and explore new methods for addressing scalability constraints ____. We also aim to extend our research to other natural language processing tasks, such as machine translation and text summarization ____.

The study presented in this paper contributes to a better understanding of the importance of selecting appropriate models and training strategies for NL2SH tasks. The results demonstrate the effectiveness of pre-trained LLMs and fine-tuning strategies for achieving state-of-the-art performance on several benchmarks ____.