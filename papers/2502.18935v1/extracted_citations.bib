@INPROCEEDINGS{10095658,
  author={Woo, Tae-Jin and Nam, Woo-Jeoung and Ju, Yeong-Joon and Lee, Seong-Whan},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Compensatory Debiasing For Gender Imbalances In Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Measurement;Degradation;Linguistics;Signal processing;Acoustics;Speech processing;Language model;social bias;gender bias mitigation},
  doi={10.1109/ICASSP49357.2023.10095658}
}

@article{carlini2024aligned,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, N. and et al.},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@article{ding2023wolf,
  title={A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily},
  author={Ding, P. and et al.},
  journal={arXiv preprint arXiv:2311.08268},
  year={2023}
}

@article{hartvigsen2022toxigen,
  title={Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  journal={arXiv preprint arXiv:2203.09509},
  year={2022}
}

@article{huang2023flames,
  title={Flames: Benchmarking value alignment of chinese large language models},
  author={Huang, K. and et al.},
  journal={arXiv preprint arXiv:2311.06899},
  year={2023}
}

@article{lapid2023open,
  title={Open sesame! universal black box jailbreaking of large language models},
  author={Lapid, Raz and Langberg, Ron and Sipper, Moshe},
  journal={arXiv preprint arXiv:2309.01446},
  year={2023}
}

@article{li2023deepinception,
  title={Deepinception: Hypnotize large language model to be jailbreaker},
  author={Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo},
  journal={arXiv preprint arXiv:2311.03191},
  year={2023}
}

@article{lin2023toxicchat,
  title={Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation},
  author={Lin, Z. and et al.},
  journal={arXiv preprint arXiv:2310.17389},
  year={2023}
}

@article{liu2023autodan,
  title={Autodan: Generating stealthy jailbreak prompts on aligned large language models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2310.04451},
  year={2023}
}

@article{mazeika2024harmbench,
  title={Harmbench: A standardized evaluation framework for automated red teaming and robust refusal},
  author={Mazeika, M. and et al.},
  journal={arXiv preprint arXiv:2402.04249},
  year={2024}
}

@article{sun2023safety,
  title={Safety assessment of chinese large language models},
  author={Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie},
  journal={arXiv preprint arXiv:2304.10436},
  year={2023}
}

@article{wang2023not,
  title={Do-not-answer: A dataset for evaluating safeguards in llms},
  author={Wang, Yuxia and Li, Haonan and Han, Xudong and Nakov, Preslav and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2308.13387},
  year={2023}
}

@article{xu2023cvalues,
  title={Cvalues: Measuring the values of chinese large language models from safety to responsibility},
  author={Xu, G. and et al.},
  journal={arXiv preprint arXiv:2307.09705},
  year={2023}
}

@article{yong2023low,
  title={Low-resource languages jailbreak gpt-4},
  author={Yong, Zheng-Xin and Menghini, Cristina and Bach, Stephen H},
  journal={arXiv preprint arXiv:2310.02446},
  year={2023}
}

@article{yu2023gptfuzzer,
  title={Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Lin, Xingwei and Xing, Xinyu},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023}
}

@article{yuan2023gpt,
  title={Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher},
  author={Yuan, Y. and et al.},
  journal={arXiv preprint arXiv:2308.06463},
  year={2023}
}

@article{zeng2024johnny,
  title={How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms},
  author={Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan},
  journal={arXiv preprint arXiv:2401.06373},
  year={2024}
}

@article{zhang2023enhancing,
  title={Enhancing uncertainty-based hallucination detection with stronger focus},
  author={Zhang, T. and et al.},
  journal={arXiv preprint arXiv:2311.13230},
  year={2023}
}

@article{zhang2023safetybench,
  title={Safetybench: Evaluating the safety of large language models with multiple choice questions},
  author={Zhang, Z. and et al.},
  journal={arXiv preprint arXiv:2309.07045},
  year={2023}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

