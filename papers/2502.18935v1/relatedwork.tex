\section{Related Work}
This section reviews existing Chinese benchmarks designed for safety evaluation, as well as the jailbreak attacks that serve as key factors in improving the effectiveness of test data.

\subsection{Benchmarks for Safety Evaluation}
Previous benchmarks have primarily focused on the specific risk assessment of LLMs, ranging from text toxicity~\cite{hartvigsen2022toxigen,lin2023toxicchat} and social bias~\cite{10095658} to hallucination~\cite{zhang2023enhancing}. As the capabilities and complexity of LLMs continue to increase, a growing number of benchmarks have emerged to evaluate the overall safety of these models~\cite{wang2023not,mazeika2024harmbench}. However, these benchmarks primarily focus on English scenarios, whereas JailBench concentrates on the Chinese language context, aiming to provide a deeper assessment of LLM safety in Chinese.

With the rapid advancement of LLMs' Chinese language capabilities, several Chinese-specific benchmarks have also been constructed~\cite{zhang2023safetybench,sun2023safety,huang2023flames,xu2023cvalues}. For instance, SafetyBench evaluates LLM safety through multiple-choice questions in both Chinese and English. While Flames is notable for its adversarial design, pushing the boundaries of evaluating value alignment in Chinese LLMs. Nevertheless, these benchmarks generally suffer limited effectiveness in thoroughly evaluating LLM safety, as increasingly robust defenses against malicious prompts pose challenges for detecting deeper security vulnerabilities~\cite{sun2023safety,wang2023not}. This limitation underscores the necessity of enhancing the harmfulness of test data.

\subsection{Jailbreak Attacks on LLMs}
We introduce jailbreak attacks~\cite{carlini2024aligned} into the construction of JailBench to improve the effectiveness of thorough safety evaluation. Early jailbreak attacks on LLMs primarily relied on manually crafted scenarios specifically designed to bypass the modelsâ€™ safeguards~\cite{li2023deepinception,ding2023wolf}. These approaches also included translating harmful prompts into low-resource languages~\cite{yong2023low} or using cryptography to conceal harmful intentions~\cite{yuan2023gpt}. These carefully designed jailbreak templates can serve as high-quality prompt resources for JailBench construction.

To minimize the human effort and time required to craft jailbreak prompts, researchers have explored various automated red-teaming methods. These approaches range from utilizing search optimization algorithms to generate adversarial prompts~\cite{zou2023universal,lapid2023open} to leveraging LLMs as prompt optimizers~\cite{chao2023jailbreaking,zeng2024johnny}. Particularly relevant to our work are the dynamic prompt optimization~\cite{liu2023autodan} and iteration techniques~\cite{yu2023gptfuzzer}. These techniques maintain a "template pool" of effective jailbreak templates which can be easily combined with standard harmful queries to rapidly generate numerous high-risk prompts. Consequently, we propose the AJPE framework, which leverages the language capabilities of LLMs to perform few-shot learning for generating more targeted and context-aware jailbreak prompts, potentially increasing the effectiveness and efficiency of jailbreak attacks, thereby providing a more rigorous and comprehensive assessment of LLMs safety within the Chinese language context.

Different from existing Chinese benchmarks, JailBench incorporates advanced jailbreak attacks with automatic prompt generation for thorough safety evaluations, offering comprehensive security vulnerability identification for LLMs.