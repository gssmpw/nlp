[
  {
    "index": 0,
    "papers": [
      {
        "key": "hartvigsen2022toxigen",
        "author": "Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece",
        "title": "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection"
      },
      {
        "key": "lin2023toxicchat",
        "author": "Lin, Z. and et al.",
        "title": "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "10095658",
        "author": "Woo, Tae-Jin and Nam, Woo-Jeoung and Ju, Yeong-Joon and Lee, Seong-Whan",
        "title": "Compensatory Debiasing For Gender Imbalances In Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhang2023enhancing",
        "author": "Zhang, T. and et al.",
        "title": "Enhancing uncertainty-based hallucination detection with stronger focus"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2023not",
        "author": "Wang, Yuxia and Li, Haonan and Han, Xudong and Nakov, Preslav and Baldwin, Timothy",
        "title": "Do-not-answer: A dataset for evaluating safeguards in llms"
      },
      {
        "key": "mazeika2024harmbench",
        "author": "Mazeika, M. and et al.",
        "title": "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2023safetybench",
        "author": "Zhang, Z. and et al.",
        "title": "Safetybench: Evaluating the safety of large language models with multiple choice questions"
      },
      {
        "key": "sun2023safety",
        "author": "Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie",
        "title": "Safety assessment of chinese large language models"
      },
      {
        "key": "huang2023flames",
        "author": "Huang, K. and et al.",
        "title": "Flames: Benchmarking value alignment of chinese large language models"
      },
      {
        "key": "xu2023cvalues",
        "author": "Xu, G. and et al.",
        "title": "Cvalues: Measuring the values of chinese large language models from safety to responsibility"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "sun2023safety",
        "author": "Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie",
        "title": "Safety assessment of chinese large language models"
      },
      {
        "key": "wang2023not",
        "author": "Wang, Yuxia and Li, Haonan and Han, Xudong and Nakov, Preslav and Baldwin, Timothy",
        "title": "Do-not-answer: A dataset for evaluating safeguards in llms"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "carlini2024aligned",
        "author": "Carlini, N. and et al.",
        "title": "Are aligned neural networks adversarially aligned?"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "li2023deepinception",
        "author": "Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo",
        "title": "Deepinception: Hypnotize large language model to be jailbreaker"
      },
      {
        "key": "ding2023wolf",
        "author": "Ding, P. and et al.",
        "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yong2023low",
        "author": "Yong, Zheng-Xin and Menghini, Cristina and Bach, Stephen H",
        "title": "Low-resource languages jailbreak gpt-4"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yuan2023gpt",
        "author": "Yuan, Y. and et al.",
        "title": "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      },
      {
        "key": "lapid2023open",
        "author": "Lapid, Raz and Langberg, Ron and Sipper, Moshe",
        "title": "Open sesame! universal black box jailbreaking of large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chao2023jailbreaking",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty queries"
      },
      {
        "key": "zeng2024johnny",
        "author": "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan",
        "title": "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liu2023autodan",
        "author": "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
        "title": "Autodan: Generating stealthy jailbreak prompts on aligned large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yu2023gptfuzzer",
        "author": "Yu, Jiahao and Lin, Xingwei and Xing, Xinyu",
        "title": "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts"
      }
    ]
  }
]