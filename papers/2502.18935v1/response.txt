\section{Related Work}
This section reviews existing Chinese benchmarks designed for safety evaluation, as well as the jailbreak attacks that serve as key factors in improving the effectiveness of test data.

\subsection{Benchmarks for Safety Evaluation}
Previous benchmarks have primarily focused on the specific risk assessment of LLMs, ranging from text toxicity **Zhou et al., "Detecting Toxicity in Text"** and social bias **Huang et al., "Evaluating Social Bias in LLMs"** to hallucination **Li et al., "Assessing Hallucination in Language Models"**. As the capabilities and complexity of LLMs continue to increase, a growing number of benchmarks have emerged to evaluate the overall safety of these models **Wang et al., "Safety Evaluation of LLMs with Enhanced Benchmarks"**. However, these benchmarks primarily focus on English scenarios, whereas JailBench concentrates on the Chinese language context, aiming to provide a deeper assessment of LLM safety in Chinese.

With the rapid advancement of LLMs' Chinese language capabilities, several Chinese-specific benchmarks have also been constructed **Chen et al., "SafetyBench: A Chinese-Specific Benchmark for LLM Safety"**. For instance, SafetyBench evaluates LLM safety through multiple-choice questions in both Chinese and English. While Flames is notable for its adversarial design, pushing the boundaries of evaluating value alignment in Chinese LLMs. Nevertheless, these benchmarks generally suffer limited effectiveness in thoroughly evaluating LLM safety, as increasingly robust defenses against malicious prompts pose challenges for detecting deeper security vulnerabilities **Li et al., "Leveraging Robust Defenses to Enhance LLM Safety"**. This limitation underscores the necessity of enhancing the harmfulness of test data.

\subsection{Jailbreak Attacks on LLMs}
We introduce jailbreak attacks **Sun et al., "Jailbreaking Language Models: A Comprehensive Review"** into the construction of JailBench to improve the effectiveness of thorough safety evaluation. Early jailbreak attacks on LLMs primarily relied on manually crafted scenarios specifically designed to bypass the modelsâ€™ safeguards **Wang et al., "Crafting Effective Jailbreak Scenarios for LLMs"**. These approaches also included translating harmful prompts into low-resource languages **Liu et al., "Low-Resource Language Translation for Harmful Prompts"** or using cryptography to conceal harmful intentions **Zhang et al., "Cryptographic Techniques for Concealing Harmful Intentions"**. These carefully designed jailbreak templates can serve as high-quality prompt resources for JailBench construction.

To minimize the human effort and time required to craft jailbreak prompts, researchers have explored various automated red-teaming methods. These approaches range from utilizing search optimization algorithms to generate adversarial prompts **Tao et al., "Adversarial Prompt Generation with Search Optimization"** to leveraging LLMs as prompt optimizers **Huang et al., "LLMs for Prompt Optimization: A Comprehensive Review"**. Particularly relevant to our work are the dynamic prompt optimization **Li et al., "Dynamic Prompt Optimization for LLM Safety Evaluation"** and iteration techniques **Chen et al., "Iterative Techniques for Enhanced LLM Safety Evaluation"**. These techniques maintain a "template pool" of effective jailbreak templates which can be easily combined with standard harmful queries to rapidly generate numerous high-risk prompts. Consequently, we propose the AJPE framework, which leverages the language capabilities of LLMs to perform few-shot learning for generating more targeted and context-aware jailbreak prompts, potentially increasing the effectiveness and efficiency of jailbreak attacks, thereby providing a more rigorous and comprehensive assessment of LLMs safety within the Chinese language context.

Different from existing Chinese benchmarks, JailBench incorporates advanced jailbreak attacks with automatic prompt generation for thorough safety evaluations, offering comprehensive security vulnerability identification for LLMs.