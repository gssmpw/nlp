@article{yu2023gptfuzzer,
  title={Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Lin, Xingwei and Xing, Xinyu},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023}
}
@article{li2024salad,
  title={Salad-bench: A hierarchical and comprehensive safety benchmark for large language models},
  author={Li, L. and et al.},
  journal={arXiv preprint arXiv:2402.05044},
  year={2024}
}
@article{zhang2023safetybench,
  title={Safetybench: Evaluating the safety of large language models with multiple choice questions},
  author={Zhang, Z. and et al.},
  journal={arXiv preprint arXiv:2309.07045},
  year={2023}
}
@article{huang2023flames,
  title={Flames: Benchmarking value alignment of chinese large language models},
  author={Huang, K. and et al.},
  journal={arXiv preprint arXiv:2311.06899},
  year={2023}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, H. and et al.},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, J. and et al.},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{tokayev2023ethical,
  title={Ethical implications of large language models a multidimensional exploration of societal, economic, and technical concerns},
  author={Tokayev, Kassym-Jomart},
  journal={International Journal of Social Analytics},
  volume={8},
  number={9},
  pages={17--33},
  year={2023}
}
@article{parrish2021bbq,
  title={BBQ: A hand-built bias benchmark for question answering},
  author={Parrish, A. and et al.},
  journal={arXiv preprint arXiv:2110.08193},
  year={2021}
}
@article{xu2023cvalues,
  title={Cvalues: Measuring the values of chinese large language models from safety to responsibility},
  author={Xu, G. and et al.},
  journal={arXiv preprint arXiv:2307.09705},
  year={2023}
}
@article{wang2023not,
  title={Do-not-answer: A dataset for evaluating safeguards in llms},
  author={Wang, Yuxia and Li, Haonan and Han, Xudong and Nakov, Preslav and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2308.13387},
  year={2023}
}
@article{hartvigsen2022toxigen,
  title={Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  journal={arXiv preprint arXiv:2203.09509},
  year={2022}
}
@article{sun2023safety,
  title={Safety assessment of chinese large language models},
  author={Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie},
  journal={arXiv preprint arXiv:2304.10436},
  year={2023}
}
@article{wang2023all,
  title={All languages matter: On the multilingual safety of large language models},
  author={Wang, W. and et al.},
  journal={arXiv preprint arXiv:2310.00905},
  year={2023}
}
@article{wang2024chinese,
  title={A Chinese Dataset for Evaluating the Safeguards in Large Language Models},
  author={Wang, Y. and et al.},
  journal={to appear in ACL 2024 findings},
  year={2024}
}
@article{lin2023toxicchat,
  title={Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation},
  author={Lin, Z. and et al.},
  journal={arXiv preprint arXiv:2310.17389},
  year={2023}
}
@INPROCEEDINGS{10095658,
  author={Woo, Tae-Jin and Nam, Woo-Jeoung and Ju, Yeong-Joon and Lee, Seong-Whan},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Compensatory Debiasing For Gender Imbalances In Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Measurement;Degradation;Linguistics;Signal processing;Acoustics;Speech processing;Language model;social bias;gender bias mitigation},
  doi={10.1109/ICASSP49357.2023.10095658}
}
@article{zhang2023enhancing,
  title={Enhancing uncertainty-based hallucination detection with stronger focus},
  author={Zhang, T. and et al.},
  journal={arXiv preprint arXiv:2311.13230},
  year={2023}
}
@article{mazeika2024harmbench,
  title={Harmbench: A standardized evaluation framework for automated red teaming and robust refusal},
  author={Mazeika, M. and et al.},
  journal={arXiv preprint arXiv:2402.04249},
  year={2024}
}
@article{yuan2023gpt,
  title={Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher},
  author={Yuan, Y. and et al.},
  journal={arXiv preprint arXiv:2308.06463},
  year={2023}
}
@article{yong2023low,
  title={Low-resource languages jailbreak gpt-4},
  author={Yong, Zheng-Xin and Menghini, Cristina and Bach, Stephen H},
  journal={arXiv preprint arXiv:2310.02446},
  year={2023}
}
@article{liu2023autodan,
  title={Autodan: Generating stealthy jailbreak prompts on aligned large language models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2310.04451},
  year={2023}
}
@article{li2023deepinception,
  title={Deepinception: Hypnotize large language model to be jailbreaker},
  author={Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo},
  journal={arXiv preprint arXiv:2311.03191},
  year={2023}
}
@article{ding2023wolf,
  title={A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily},
  author={Ding, P. and et al.},
  journal={arXiv preprint arXiv:2311.08268},
  year={2023}
}
@article{zeng2024johnny,
  title={How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms},
  author={Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan},
  journal={arXiv preprint arXiv:2401.06373},
  year={2024}
}
@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}
@article{lapid2023open,
  title={Open sesame! universal black box jailbreaking of large language models},
  author={Lapid, Raz and Langberg, Ron and Sipper, Moshe},
  journal={arXiv preprint arXiv:2309.01446},
  year={2023}
}
@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}
@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, D. and et al.},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}
@article{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, X. and et al.},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}
@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Y. and et al.},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}
@misc{metallamaguard2,
  author =       {Llama Team},
  title =        {Meta Llama Guard 2},
  howpublished = {\url{https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md}},
  year =         {2024}
}
@article{carlini2024aligned,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, N. and et al.},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{wei2023jailbreak,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  year={2023}
}