@inproceedings{cheng2024edit,
    title = "Can We Edit Multimodal Large Language Models?",
    author = "Cheng, Siyuan  and
      Tian, Bozhong  and
      Liu, Qingbin  and
      Chen, Xi  and
      Wang, Yongheng  and
      Chen, Huajun  and
      Zhang, Ningyu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.856/",
    doi = "10.18653/v1/2023.emnlp-main.856",
    pages = "13877--13888",
    abstract = "In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights."
}

@article{dai2021knowledge,
  title={Knowledge neurons in pretrained transformers},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  journal={arXiv preprint arXiv:2104.08696},
  year={2021}
}

@article{das2024one,
  title={One VLM to Keep it Learning: Generation and Balancing for Data-free Continual Visual Question Answering},
  author={Das, Deepayan and Talon, Davide and Mancini, Massimiliano and Wang, Yiming and Ricci, Elisa},
  journal={arXiv preprint arXiv:2411.02210},
  year={2024}
}

@article{de2021continual,
  title={A continual learning survey: Defying forgetting in classification tasks},
  author={De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ale{\v{s}} and Slabaugh, Gregory and Tuytelaars, Tinne},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={7},
  pages={3366--3385},
  year={2021},
  publisher={IEEE}
}

@inproceedings{goswami2024calibrating,
  title={Calibrating Higher-Order Statistics for Few-Shot Class-Incremental Learning with Pre-trained Vision Transformers},
  author={Goswami, Dipam and Twardowski, Bart{\l}omiej and Van De Weijer, Joost},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4075--4084},
  year={2024}
}

@article{hayou2024lora+,
  title={LoRA+: Efficient Low Rank Adaptation of Large Models},
  author={Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
  journal={arXiv preprint arXiv:2402.12354},
  year={2024}
}

@article{he2023continual,
  title={Continual instruction tuning for large multimodal models},
  author={He, Jinghan and Guo, Haiyun and Tang, Ming and Wang, Jinqiao},
  journal={arXiv preprint arXiv:2311.16206},
  year={2023}
}

@inproceedings{he2023sensitivity,
  title={Sensitivity-aware visual parameter-efficient fine-tuning},
  author={He, Haoyu and Cai, Jianfei and Zhang, Jing and Tao, Dacheng and Zhuang, Bohan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11825--11835},
  year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{liu2024dora,
  title={DoRA: Weight-Decomposed Low-Rank Adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@inproceedings{panos2023first,
  title={First session adaptation: A strong replay-free baseline for class-incremental learning},
  author={Panos, Aristeidis and Kobe, Yuriko and Reino, Daniel Olmeda and Aljundi, Rahaf and Turner, Richard E},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={18820--18830},
  year={2023}
}

@article{srivastava2024improving,
  title={Improving multimodal large language models using continual learning},
  author={Srivastava, Shikhar and Harun, Md Yousuf and Shrestha, Robik and Kanan, Christopher},
  journal={arXiv preprint arXiv:2410.19925},
  year={2024}
}

@article{verwimp2023continual,
  title={Continual learning: Applications and the road forward},
  author={Verwimp, Eli and Aljundi, Rahaf and Ben-David, Shai and Bethge, Matthias and Cossu, Andrea and Gepperth, Alexander and Hayes, Tyler L and H{\"u}llermeier, Eyke and Kanan, Christopher and Kudithipudi, Dhireesha and others},
  journal={arXiv preprint arXiv:2311.11908},
  year={2023}
}

@article{wu2024continual,
  title={Continual learning for large language models: A survey},
  author={Wu, Tongtong and Luo, Linhao and Li, Yuan-Fang and Pan, Shirui and Vu, Thuy-Trang and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2402.01364},
  year={2024}
}

@inproceedings{zhang2023adaptive,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{zhang2024overcoming,
  title={Overcoming Generic Knowledge Loss with Selective Parameter Update},
  author={Zhang, Wenxuan and Janson, Paul and Aljundi, Rahaf and Elhoseiny, Mohamed},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24046--24056},
  year={2024}
}

@article{zhao2024safe,
  title={SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning with Pre-Trained Models},
  author={Zhao, Linglan and Zhang, Xuerui and Yan, Ke and Ding, Shouhong and Huang, Weiran},
  journal={arXiv preprint arXiv:2411.02175},
  year={2024}
}

@article{zhou2024lora,
  title={LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation},
  author={Zhou, Hongyun and Lu, Xiangyu and Xu, Wang and Zhu, Conghui and Zhao, Tiejun},
  journal={arXiv preprint arXiv:2402.07721},
  year={2024}
}

