\section{Experiments}\label{sec:experiments}
We conduct a series of experiments under three different few-shot continual learning (CL) settings (CL-5, CL-20, and CL-50 shots) to thoroughly investigate the performance of LoRSU based on ten VQA datasets. By adopting this paradigm, we aim to assess the adaptability and efficiency of LoRSU under constrained learning conditions, ensuring that it remains both computationally feasible and effective in improving downstream performance. Specifically, we seek to address the following key questions: 1) How does our method, LoRSU, compare to other fine-tuning and CL baselines that use the CLIP loss to update the image encoder? 2) Does updating the image encoder separately and then reintegrating it into the corresponding VLM enhance downstream VQA performance? 3) What is the effect of using the perplexity loss instead of the CLIP loss to update the image encoder? 4) What are the benefits of choosing a subset of attention heads to be fine-tuned using LoRSU? and 5) What are the computational benefits of LoRSU?

\subsection{Datasets}\label{sec:datasets}
We evaluate the performance of \ours on ten visual question answering (VQA) datasets falling in two broad categories: regular VQA datasets and classification datasets converted to VQA datasets.

\textbf{Regular VQA datasets.}~~We consider four standard VQA datasets used for benchmarking VLMs' performance~\cite{duan2024vlmevalkit}: 
\emph{VSR}~\cite{Liu2022VisualSR}, the Visual Spatial Reasoning corpus consists of caption-image pairs labeled as True or False, where each caption describes the spatial relation between two objects in the image. VLMs evaluate whether the caption accurately reflects the image.
\emph{HM}~\cite{kiela2020hateful}, the Hateful Memes dataset designed to detect multimodal hateful memes.
\emph{MMVP}~\cite{tong2024eyes}, the Multimodal Visual Patterns
dataset is a challenging benchmark which has been built on images that CLIP perceives as similar despite their clear visual differences.
\emph{VisOnly}~\cite{kamoi2024visonlyqa}, a novel dataset created to directly assess the visual perception abilities of VLMs in answering questions about geometric and numerical details in scientific figures. This dataset allows us to assess fine-grained visual perception in VLMs independently of other abilities, such as reasoning, making it the most challenging among the previously mentioned datasets.


\textbf{Classification-to-VQA datasets.}~~We convert four popular multi-class classification datasets into multiple-choice VQA problems, where each question has five choices, and the VLM is tasked with selecting the correct answer. These datasets are introduced as examples of scenarios where visual domain shifts are encountered, allowing us to examine the utility of updating the image encoder; a critical consideration often overlooked in many standard VQA datasets.% that primarily focus on reasoning and factual knowledge. 
The datasets include:
%\rahaf{mention that we add these datasets as examples of when visual domain shift is encountered to examine the utility of updating vision encoder since most standard VQA datasets focus on reasoning and factual knowledge? }
\emph{GTS}~\cite{stallkamp2012man}, the German Traffic Sign dataset, which \citet{zhang2024overcoming} considered as an out-of-distribution dataset for CLIP pretraining; \emph{CAn}~\cite{wang2024clips}, a recent dataset created to test CLIP's robustness with animal images containing realistic spurious features such as unexpected backgrounds; \emph{AIR}~\cite{maji13fine-grained}, a fine-grained aircraft classification dataset; \emph{ESAT}~\cite{helber2019eurosat}, a dataset of satellite images used for land cover classification.

\textbf{TSI \& DALLE.}~~In addition to these existing datasets, we introduce two novel VQA datasets: TSI and DALLE, both designed to explore the effects of domain shift.
The \emph{TSI}~\cite{das2019toyota} dataset was preprocessed as a classification dataset, where the goal is to recognize the activity depicted in each image. Frames were extracted from videos to create a training set of approximately 10K images and a test set of approximately 5K images, encompassing 27 distinct activity classes. The \emph{DALLE} dataset, constructed by querying the OpenAI's model DALL·E 2, includes representative images generated from 22 activity classes appearing in TSI. For each activity, we generated 30 images, resulting in a total of 660 images designated exclusively for evaluation purposes. %\rahaf{ I would separate the the introduction of our two datasets in a separate paragraph and add a reference to the specific DALL-E used. } to generate representative images of these activities. We extract 30 images per action totaling 660, all of them are designated for testing. 

We follow the common practice in few-shot continual learning~\cite{panos2023first} to construct the sequences. We divide each dataset into 5 sets of disjoint classes/categories and consider 5/20/50 shot settings where only 5/20/50 images per class in the current set are used for fine-tuning the model. More details on how we split each of these datasets for the CL settings are provided in appendix~\ref{sec_appx:datasets}.

\input{tables/clip_baselines_summary}

\subsection{Experimental Setting}\label{sec:experimental_settings}

\textbf{Metrics.}~~%\rahaf{add an introductory sentence on why standard metrics alone don't suffice. }
While standard metrics in the CL literature exist to evaluate general performance~\cite{lopez2017gradient,chaudhry2018riemannian}, VLMs exhibit generic knowledge across various domains beyond the one being adapted, making it crucial to evaluate how adaptation impacts their overall performance. These metrics do not measure the change in performance relative to the model's initial state prior to the learning process.
To address this, we use the zero-shot accuracy of each VQA dataset as the benchmark baseline and report the change in accuracy on the test split of the target dataset so positive values indicate an improvement in accuracy. This approach enables us to quantify the model's ability to accumulate knowledge, using the pretrained model as the reference point; we name this metric as \emph{Target Improvement} (TI) accuracy. We also calculate the average accuracy change on the test splits of the remaining datasets, when fine-tuning on a specific dataset, to estimate average forgetting of generic knowledge or possible positive backward transfer~\cite{de2021continual}; we call this metric \emph{Control Change} (CC) accuracy where `control' refers to the control datasets we use to calculate the average accuracy change. TI and CC are computed based on the fine-tuned VLM after the last session of CL. We also consider standard CL performance metrics such as \emph{Average Accuracy} (ACC) and \emph{Backward Transfer} (BWT)~\cite{lopez2017gradient} to examine how accuracy and forgetting evolves through continuous adaptation. Notice that these metrics, in contrast to TI and CC, focus on the accuracy and forgetting during continual adaptation and they do not take into account the performance of the fine-tuned model on other datasets.

\textbf{Implementation details.}~~Please see Appendix~\ref{sec_appx:implementation_details}.


\textbf{Models.}~~For our experiments, we consider the popular Vision Language Model LLaVA-v1.5~\cite{liu2024visual} that leverages a frozen CLIP image encoder. Specifically, LLaVA utilizes a frozen OpenAI-CLIP-L-14~\cite{radford2021learning} with a LLM (Vicuna-7b~\cite{chiang2023vicuna}). The two modules are connected through a two-layer MLP projector that aligns image and text features. The LLM and the MLP projector are optimized during the visual instruction tuning while CLIP remains frozen. LLaVA concatenates adjacent tokens from CLIP-L-14 and processes them with an MLP projector as input to LLama-2 (7B-chat)~\cite{touvron2023llama}; the MLP projector and the language model are optimized while the image encoder remains frozen. 

\textbf{Baselines.}~~We compare LoRSU to the following methods that also use the CLIP loss to fine-tune the image encoder:
\begin{itemize}[noitemsep,topsep=1pt,parsep=1pt,partopsep=1pt,leftmargin=*]
    \item \emph{LN}~\cite{perez2018film,panos2023first} is used for both few-shot and CL. Only the  image encoder LayerNorm modules' parameters are optimized.
    \item \emph{F-FT} is the standard fine-tuning technique where all image encoder parameters undergo gradient updates.
    \item \emph{F-EWC} fine-tunes all the image encoder parameters with EWC regularization~\cite{kirkpatrick2017overcoming}.% which has successfully used to many CL problems in the past.
    \item \emph{LoRA}~\cite{hu2021lora} a popular PEFT method which parameterizes incremental updates by two low-dimensional matrices and only fine-tunes them.
    \item \emph{AdaLoRA}~\cite{zhang2023adalora} dynamically adjusts the low-rank update budget allocation during training.
    \item \emph{SPU}~\cite{zhang2024overcoming} is a PEFT baseline, specifically designed to tackle catastrophic forgetting in CL scenarios, that utilizes structured sparsity based on gradient information to fine-tune the most significant parameters of the fc1 module in the transformer block. 
\end{itemize}

\subsection{CLIP-based Updates}

We evaluate the performance of the Vision-Language Model (VLM) when only the image encoder is fine-tuned using the CLIP loss in a CL setting. This experiment compares six strong CLIP-based baselines with our proposed method, \ours. Table~\ref{table:clip_baselines_summary} reports the average accuracies of TI/CC over three runs; detailed results can be found in appendix~\ref{sec_appx:detailed_res}. We observe that \ours consistently achieves superior TI scores across datasets and CL settings, underscoring its ability to enhance task-specific performance effectively. Furthermore, \ours maintains CC accuracies that take consistently small negative or even positive values, highlighting its capacity to preserve or slightly improve performance on control datasets while fine-tuning on target datasets. Even in datasets where other methods struggle (e.g., CAn, ESAT), LoRSU often performs better, maintaining positive or close-to-neutral TI and CC scores. For instance, In ESAT (CL-50) containing challenging satellite images, LoRSU achieves the highest TI (7.0) with a positive CC (0.2), outperforming SPU (TI=5.8, CC=0.1) and all other methods. 

\input{tables/bwt_metrics_clip_reduced}

\input{tables/ppl_vs_clip_summary}

\textbf{Additional metrics.}~~We assess the performance of LoRSU against LoRA and SPU in terms of ACC and BWT across two out-of-domain datasets, GTS and ESAT. Since LoRA and SPU have similar number of trainable parameters as LoRSU and competitive performance in our previous experiment, we choose those for comparison. Table~\ref{table:bwt_metrics_clip_reduced} shows that LoRSU's  performs well with respect to these metrics, following similar patterns as TI and CC in Table~\ref{table:clip_baselines_summary}. LoRSU achieves the best performance on ACC while exhibiting minimal forgetting with the least negative BWT values. Similar patterns are observed on extra datasets in appendix~\ref{sec_appx:extra_bwt_results}. 

\subsection{CLIP-based vs. Perplexity-based Updates}
Traditionally, LLMs and VLMs achieve impressive performance through fine-tuning with the perplexity loss. LoRA is the standard PEFT method for this purpose, and thus, we consider three extra LoRA variants plus \emph{LoRSU-Ppl} which all utilize the perplexity loss to update the model. 
\begin{itemize}[noitemsep,topsep=1pt,parsep=1pt,partopsep=1pt,leftmargin=*]
    \item \emph{LoRA-L} applies LoRA adapters to all weight matrices of the LLM and thus perplexity loss is required.
    \item \emph{LoRA-Ppl} is the same method as LoRA but this time the perplexity loss is used to update the adapters.
    \item \emph{LoRA-F} applies LoRA adapters to all weight matrices of the LLM, the image encoder, and the MLP projector.
\end{itemize}

We aevaluate how LoRSU and LoRA perform compared to their perplexity-based counterparts, LoRSU-Ppl and LoRA-Ppl, respectively. Furthermore, we seek to explore how these methods compare to parameter-efficient fine-tuning approaches when either the entire VLM (LoRA-F) or only the LLM component (LoRA-L) is updated.

The results in Table \ref{table:ppl_vs_clip_summary} highlight the strong and robust performance of LoRSU and LoRSU-Ppl compared to other baseline methods across various settings. Both LoRSU and LoRSU-Ppl achieve minimal negative or even positive changes in CC, indicating reduced catastrophic forgetting and improved retention of generic knowledge compared to baselines. The table reports the average accuracies of TI/CC over three runs with exact results provided in appendix~\ref{sec_appx:detailed_res}.

The use of the perplexity loss in LoRSU-Ppl demonstrates a considerable improvement in TI accuracy over LoRSU when fine-tuned for VQA datasets. For instance, LoRSU-Ppl achieves 10\% higher TI accuracy than LoRSU on VSR. We hypothesize that the perplexity loss acts as an additional signal that optimizes the image encoder to complement the frozen language model more effectively, improving the alignment between visual and textual modalities in VQA.

However, we observe that LoRSU achieves a balance between task-specific improvements and generalization, consistently demonstrating higher CC accuracy compared to LoRSU-Ppl across most datasets. Lastly, although LoRA-F achieves high TI scores on many datasets, it suffers significantly from forgetting, underscoring the importance of LoRSU's structured updates in CL scenarios.


\subsection{The Choice of Attention Heads}
\input{tables/lorsu_attn_vs_rand_summary}
Given that LoRSU's mechanism of choosing attention heads is a key-point to its success, we conduct an ablation study on the different strategies for selecting attention heads during the fine-tuning process. In this experiment, we compare LoRSU's performance to two new variants of LoRSU, namely, \emph{LoRSU-Rand} and \emph{LoRSU-AAH}. LoRSU-Rand randomly chooses the number of attention heads ($k=2$ heads) to be fine-tuned while LoRSU-AAH fine-tunes all the available attention heads (16 in total) in each transformer block. For extra results on the sensitivity of the number of LoRSU's optimal attention heads $k$, see appendix~\ref{sec_appx:ablation_heads_general}.

The results in Table~\ref{table:lorsu_attn_vs_rand_summary} demonstrate that  LoRSU's targeted approach is performant, balancing task-specific improvements (TI) and the retention of generic knowledge (CC).
Random selection (LoRSU-Rand) fails to generalize well, while fine-tuning all attention heads (LoRSU-AHH) adds unnecessary computational overhead with less effective generalization. LoRSU outperforms both of the variants in TI while LoRSU-AHH is marginally better in CC. Additional experiments that investigate the robustness of \ours in terms of the number of training epochs can be found in appendix~\ref{sec_appx:robustness}; ablation studies of other hyperparameters of LoRSU are given in appendix~\ref{sec_appx:extra_ablations}.

\input{figures/tflops_comparisons_fig}
\subsection{Computational Efficiency}
In Figure \ref{fig:tflops_comparisons}, we asses the computational benefits of LoRSU using the CLIP loss comparing to baseline methods. We focus on two key metrics: trainable parameters and floating-point operations per second (TFLOPs). 

\ours requires 25× fewer computation resources than LoRA-F and LoRSU-Ppl, demonstrating the suitability of using CLIP loss when computational resources are limited. Unlike the perplexity loss, which necessitates forward and backward passes through both the vision encoder and LLM, the CLIP loss operates solely on the vision encoder, significantly reducing computational overhead. This makes LoRSU more scalable, enabling efficient continual learning even in resource-constrained environments.
