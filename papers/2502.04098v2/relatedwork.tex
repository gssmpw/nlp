\section{Related Work}
\label{sec:related_work}

\input{figures/lorsu_architecture_fig}
%\paragraph{Model Editing}
% Large language model and vision language models are strong foundation models but are still prone to mistakes and their knowledge can get outdated, consequently it is important to develop efficient updates that preserve unrelated knowledge.  
% The main line of work in this area focuses on LLM editing, where previous factual knowledge has changed and the model must be updated effectively on those changes. 
\looseness=-1\textbf{Continual Learning.} Our work falls within the continual learning literature, where a model needs to be updated incrementally as new data arrive, accumulating knowledge over tasks and reducing forgetting of previously acquired information \cite{de2021continual}.

\textbf{Continual Learning for Multimodal Language Models.} \citet{wu2024continual} provide a survey on continual learning for LLMs, highlighting challenges of computational efficiency and forgetting. \citet{srivastava2024improving} explored continual multi-modal learning on VQA datasets, keeping the vision encoder frozen. \citet{he2023continual} examined continual instruction tuning with sequential VQA datasets, proposing a method where the projection head is expanded for each new task. \citet{das2024one} introduced a pseudo-rehearsal strategy for vision-language models, updating only the language projection layer. Our method adapts only the vision encoder, preserving language capabilities.

\textbf{Continual Learning with Few-Shot Updates.} \citet{verwimp2023continual} posits that an ideal continual learning solution would enable continual correction of model's mistakes at a lower computational cost than retraining from scratch. However, most continual few-shot learning from pre-trained models focuses on classification tasks and introduces solutions that cannot scale to large multimodal models. \citet{panos2023first} update the vision encoder on the first task only, later adapting a covariance matrix for incoming tasks. \citet{goswami2024calibrating} calibrate the covariance matrix for new classes based on semantic similarity. \citet{zhao2024safe} introduce few and slow updates, proposing a transfer loss function and a cross-classification loss to mitigate catastrophic forgetting. Few-shot updates can also be viewed through the lens of model editing~\cite{Sinitsin2020Editable}. MEND~\cite{mitchell2022fast} scales model editing to large language models by transforming the gradient obtained from fine-tuning, through a low-rank decomposition fed to auxiliary networks designed to make fast, local edits to a pre-trained model, requiring a set of unrelated examples to prevent forgetting. ROME~\cite{meng2022locating} applies causal tracing to identify layers where incorrect factual knowledge is stored, applying a low-rank update. However, ROME does not scale to continual updates or non-association types of updates. \citet{cheng2024edit} studied multi-modal editing, showing negligible deterioration in multi-modal task performance when updating language models but severe forgetting when updating vision encoders. To the contrary, our method focuses on adapting the vision encoder rather than updating the factual knowledge in the LLM, yet achieving strong performance gains and negligible forgetting.
%According to~\citet{verwimp2023continual}, an ideal continual learning solution should enable continual correction of model errors at a lower computational cost than full retraining. However, most few-shot continual learning approaches focus on classification and do not scale to large multimodal models. Prior works update the vision encoder only on the first task \cite{panos2023first}, calibrate covariance matrices based on semantic similarity \cite{goswami2024calibrating}, or introduce minimal updates with transfer and cross-classification losses to mitigate forgetting \cite{zhao2024safe}. Few-shot updates can also be framed as model editing \cite{Sinitsin2020Editable}, where MEND \cite{mitchell2022fast} transforms fine-tuning gradients via low-rank decomposition for localized edits, and ROME \cite{meng2022locating} applies causal tracing to update factual knowledge but does not scale to continual learning. Recent work on multimodal editing \cite{cheng2024edit} shows severe forgetting when updating vision encoders. In contrast, our approach adapts the vision encoder rather than modifying LLM factual knowledge, achieving strong performance with negligible forgetting.

\textbf{Continual Learning of Pre-Trained Image Encoders.}~~SPT~\cite{he2023sensitivity} estimates a mask of updates based on parameter sensitivity, performing low-rank or sparse updates. SPU~\cite{zhang2024overcoming} localizes updates to the first feed-forward layer of each transformer block, inspired by knowledge neuron theory~\cite{dai2021knowledge}. Our approach generalizes updates to all layers, selecting relevant parameters and maintaining gradient norms, combined with LoRA on selected attention heads for adaptivity and stability,  achieving SOTA performance on continual fewshot multimodal tasks.

%======================================

% \textbf{Continual Learning.}~~Our work falls within the continual learning literature, where a model needs to be updated incrementally as new data arrive, accumulating knowledge over tasks and reducing forgetting of previously acquired information. We refer to \cite{de2021continual} for a comprehensive survey on continual learning.

% \textbf{Continual Learning for MultiModal Language Models.}~~\citet{wu2024continual} provide a survey on continual learning for large language models, proposing a novel multi-stage categorization scheme that includes continual pretraining, instruction tuning, and alignment, highlighting  the challenges of computational efficiency and forgetting. \citet{srivastava2024improving} explored continual multi-modal learning of large language models on multiple VQA datasets, keeping the vision encoder frozen. Their findings indicate that while continual updates to the language model result in minimal forgetting on language understanding tasks, they can significantly impact language generation tasks. \citet{he2023continual} examined continual instruction tuning with sequential VQA datasets, proposing a method where the projection head is expanded for each new task, while the language model and vision encoder remain frozen. \citet{das2024one} introduced a method for continually updating vision-language models using a pseudo-rehearsal strategy, which only updates the language projection layer, keeping the vision encoder and language model frozen. Our method differs from existing work by offering stable updates and preserving language capabilities by adapting only the vision encoder.

% \textbf{Continual Learning with Few-Shot Updates.}~~A recent position paper \cite{verwimp2023continual} on continual learning posits that an ideal solution would continually correct errors of deployed models at a significantly lower computational cost than retraining from scratch, while preserving previously learned and unaffected information. However, most continual few-shot learning from pre-trained models focuses on classification tasks and introduces solutions that cannot scale to large multimodal models. \citet{panos2023first} update the vision encoder on the first task only, later adapting a covariance matrix for incoming classification tasks, and \citet{goswami2024calibrating} calibrate the covariance matrix for new classes based on semantic similarity to many-shot base classes. \citet{zhao2024safe} introduce few and slow updates, proposing a transfer loss function to leverage general knowledge from pre-trained models and a cross-classification loss with feature alignment to mitigate catastrophic forgetting. Few-shot updates can also be viewed through the lens of model editing, where the model is updated to correct one or a few mistakes \cite{Sinitsin2020Editable}.

% MEND~\cite{mitchell2022fast} scales model editing to large language models by transforming the gradient obtained from standard fine-tuning through a low-rank decomposition fed to auxiliary networks designed to make fast, local edits to a pre-trained model, requiring a set of unrelated examples to prevent forgetting. Another line of model editing, ROME \cite{meng2022locating}, views MLPs as neural associative memory and applies causal tracing to identify the layers where incorrect factual knowledge is stored in the large language model, further applying a low-rank update to insert the new association. However, ROME does not scale to continual updates or non-association types of updates. \citet{cheng2024edit} studied multi-modal editing, applying edit methods only to language models, showing negligible deterioration in multi-modal task performance. However, when updating the vision encoders, severe forgetting is observed. Our method resembles model editing approaches in localizing updates to specific parameters, but we do not require additional examples and focus on adapting the vision encoder rather than updating factual knowledge in the LLM.

% \textbf{Continual Learning of Pre-Trained Vision Encoders.}~~In the context of updating vision models for specific tasks, SPT~\cite{he2023sensitivity} estimates a mask of updates based on parameter sensitivity to the task. Depending on the number of relevant parameters, either low-rank or sparse updates are performed using a threshold. Regarding continual updating of CLIP while maintaining its generalization performance and reducing forgetting, SPU~\cite{zhang2024overcoming} treats layers of the transformers differently. Inspired by knowledge neuron theory~\cite{dai2021knowledge}, SPU localizes updates to the first feed-forward layer of each transformer block, updating only the relevant parameters for the task at hand. In our approach, we select and identify relevant parameters for the current data, generalizing updates to all layers while preserving the specificity of each layer. We choose masks that maintain the gradient norm of parameter updates and combine them with LoRA on selected attention heads, striking a balance between adaptivity and stability.

%==========================================================================

% \textbf{Continual learning}
% Our work falls within the continual learning literature where a model needs to be updated incrementally as new data arrives, accumulating knowledge over tasks and reducing forgetting of previously acquired information. We refer to ~\cite{de2021continual} for a survey on continual learning.

% \textbf{Continual Learning for MultiModal Language models.}
% \citet{wu2024continual} provide a survey on continual learning for large language models, proposing a novel multi-staged categorization scheme that includes continual pretraining, instruction tuning, and alignment, highlighting the challenges of computational efficiency and  forgetting. \citet{srivastava2024improving} explored continual multi-modal learning of large language models on multiple VQA datasets, keeping the vision encoder frozen. Their findings indicate that while continual updates to the language model result in minimal forgetting on language understanding tasks, they can significantly impact language generation tasks. \citet{he2023continual} examined continual instruction tuning with sequential VQA datasets, proposing a method where the projection head is expanded for each new task, while the language model and vision encoder remain frozen. \cite{das2024one} introduced a method for continually updating vision-language models using a pseudo-rehearsal strategy, which only updates the language projection layer, keeping the vision encoder and language model frozen.
% Our method differs from existing works by offering stable updates and preservation of the language capabilities by adapting only the vision encoder.

% \textbf{Continual learning with fewshot updates}
% A recent position paper~\cite{verwimp2023continual} on continual learning, 
% posits that an ideal continual learning solution would continually correct errors of deployed models at a significantly lower computational cost than retraining from scratch, while preserving previously learned and unaffected information. %This approach would optimize computational efficiency and performance
% However, most continual fewshot learning from pretrained models  focus on classification tasks and introduce solutions that cannot scale to large multimodal models. ~\cite{} update the vision encoder on the first task only while later adapting a covariance matrix for the incoming classification tasks and ~\cite{goswami2024calibrating} calibrate the covariance matrix for new classes based on semantic similarity to many-shot base classes.
% \citet{safe} introduce few and slow updates,  propose a transfer loss function to leverage general knowledge from pre-trained models and a cross-classification loss with feature alignment to mitigate catastrophic forgetting.
% Few shot updates, can be also views from the lens of model editing, where the model is updated on to correct one or few mistakes~\cite{Sinitsin2020Editable}
 
% MEND~\cite{mitchell2022fast} scales model editing to large language models and operates by transforming the gradient obtained from standard fine-tuning through a low-rank decomposition that are fed to auxiliary networks designed to make fast, local edits to a pre-trained model's behavior. require set of unrelated examples  to prevent forgetting.  
% Another line of model editing, ROME~\cite{meng2022locating} view MLPs as neural associative memory and apply causal tracing to identify the layers  for which the wrong factual knowledge are stored in the large language model, further applying a one rank update to insert the new association.  However, ROME do not scale to continual updates or to non association type of updates. 
% % These approaches 
% ~\cite{cheng2024edit} studied multi-modal editing, when the edit methods are only applied to language models, showing noneligible deterioration in the performance multi-modal tasks. However, when updating the vision encoders fully sever forgetting is observed.
% Our method resembles model editing approaches in the localization of the updates to specific set of parameters, however we do not require additional examples to acquired and focus on adapting the vision encoder as opposed to updating factual knowledge in the large language model.

% \textbf{Continual learning of pre-trained vision encoders.} In the context of updating vision models for specific tasks, SPT~\citep{he2023sensitivity} estimates a mask of updates based on parameter sensitivity to the task. Depending on the number of relevant parameters, either low-rank or sparse updates are performed (using a threshold).
% With regards to continual updating CLIP while maintaining its generalization performance and reducing forgetting, SPU~\citep{zhang2024overcoming} treats layers of the transformers differently, and inspired by knowledge neuron theory, SPU localizes the updates to the first feedforward layer of each transformer block and then only relevant parameters to the task at hand are updated.  
% In our approach, we select and identify relevant parameters to the current data. However, we generalize the updates to all layers while preserving  the specificity of each layer. We choose masks that maintain the gradient norm of parameter updates and combine them with LoRA on selected attention heads, striking a balance between  adaptivity and stability.

%============================================================================================================

 % \cite{wu2024continual} survey on continual learning for large language models. The authors propose a novel multi-staged categorization scheme for continual learning techniques, which includes continual pretraining, instruction tuning, and alignment. They discuss challenges such as computation-efficient continual learning and controllable forgetting
% ~\cite{srivastava2024improving} investigated continual multi-modal learning of LLM on multiple VQA datasets where the vision encoder remain frozen. While it shows that updating LLM continually has little forgetting on lanugo understanding tasks it can have significant effect on language generation tasks.
% ~\cite{he2023continual} studied continual instruction tunning where VQA datasets are received sequentially and proposed a method where the project head is expanded for each new task, while the LLM and vision encoder remain frozen.
% ~\cite{das2024one} proposes a method for updating continually vision language models using a pseudo rehearsal strategy. The method only updates the language projection layer while the vision encoder and the LLM remain frozen.
%  \textbf{Only LoRA variants - no continual updates.} Another line of work focus on updating the model for a new task or dataset with parameter efficient finetuning. Low Rank Adapters (LoRA)~\citep{hu2021lora} approximates the parameter updates by a low rank matrix, achieving similar performance on the target task by optimizing only 1\% of the parameters compared to the full model. The original version of LoRA updated only attention layers. Subsequently, several extensions have been proposed to enhance LoRA which modify all layers. Various options are available including  adapting the learning rate~\citep{hayou2024lora+} of the low rank matrix, using an adaptive rank~\citep{zhang2023adaptive} or decomposing the update matrix into magnitude and direction~\cite{liu2024dora}. These approaches focus solely on efficiently updating the network without considering the impact on model performance for other unrelated tasks or enforcing any locality to specific layers or parameters. It is worth noting that LoRA drop~\citep{zhou2024lora} attempts to localize the updates to specific layers.  It initially allows a few iterations of LoRA updates and then assesses the impact of each low-rank update on individual layers and selectively updates only those layers where the change exceeds a specified threshold. However, this selectivity remains at the layer level and depends on the change introduced by a few full updates.
% In contrast,  we treat each layer differently based on its structure and assess the relevance of individual parameters to the task at hand. We then holistically combine the importance and relevance of these parameters with low-rank updates.