\section{Discussion}\label{sec:discussion}
%In this work, we have introduced a novel parameter-efficient fine-tuning method, LoRSU, specifically designed for few-shot continual learning scenarios with VLMs. Inspired by the limitations of CLIP on out-of-distribution and fine-grained benchmarks and noted how these weaknesses are inherited by the VLMs that utilize CLIP’s embeddings, we propose to update the image encoder separately, specifically on data where CLIP fails. Remarkably, this strategy in tandem with the use of LoRSU significantly corrects VLM mistakes on previously unseen images from the same data. In our extensive experiments, LoRSU, not only targets efficiency but also ensures the preservation of the model’s generic knowledge through localized and structured updates. Due to limitations in compute, we focus on CLIP and LLaVA but we plan to scale our work to other VLMs and image encoders, as we believe our conclusion scales well since our method is generic to any transformer model.
In this work, we introduced LoRSU, a novel parameter-efficient fine-tuning method, specifically designed for few-shot continual learning scenarios with VLMs. Unlike existing approaches, LoRSU operates without relying on a replay buffer, making it uniquely suited for resource-constrained settings. Through extensive experiments, we demonstrate that LoRSU achieves both computational efficiency and the preservation of the model’s generic knowledge by using localized and structured updates. LoRSU outperforms 12 baselines in over 80\% of evaluations across 10 datasets and 3 settings, achieving the highest TI accuracies in most cases while maintaining stable or even positive CC accuracies. To the best of our knowledge, we are the first to explore few-shot continual learning of VLMs.

Whilst we focus on CLIP and LLaVA due to computational constraints, our method is generic to any transformer model, and we plan to extend it to other VLMs and image encoders. Another promising direction is using a smaller LLM proxy model in perplexity-based methods like LoRSU-Ppl, which has shown strong VQA performance. This could improve scalability and LoRSU's use in resource-limited settings. Finally, LoRSU’s binary mask-based structured updates ensure efficient, precise parameter updates, but scaling to larger architectures like LLMs poses challenges. Replacing binary masks with more scalable solutions for vast parameter spaces will be crucial to manage memory and processing demands, offering opportunities for further refinement.