\section{Introduction}\label{sec:intro}
%Large Language Models (LLMs) have transformed the landscape of natural language understanding and generation, revolutionizing a wide range of domains plications. These advancements bring us closer to creating useful and reliable automated assistants. Since vision and visual understanding play a crucial role in intelligent agents expected to operate in the real world, Vision-Language Models (VLMs) are a key extension~\cite{chen2024internvl,Qwen2VL}. These models either incorporate embeddings from vision-only models or are trained end-to-end with both vision and language input. Remarkably, VLMs consistently achieve impressive performance across question-answering and image-captioning benchmarks. 

Large Language Models (LLMs) have revolutionized natural language understanding and generation, enabling significant advancements across diverse applications. As intelligent agents are increasingly expected to operate in real-world multimodal environments, integrating visual understanding becomes essential. Vision-Language Models (VLMs) extend LLMs by incorporating visual information, either through pre-trained vision encoders or end-to-end multimodal training. These models have demonstrated state-of-the-art performance on vision-language tasks such as visual question answering (VQA) and image captioning, highlighting their potential for general-purpose multimodal reasoning \cite{chen2024internvl,Qwen2VL}.

Approaches that rely on pretrained image encoders typically use variants of the CLIP model \citep{radford2021learning}, which is kept frozen in the vision-language binding process~\cite{liu2024visual}. CLIP is a widely deployed vision transformer that has strong zero-shot capabilities in various tasks and domains. However several  existing works have highlighted various weaknesses of CLIP on out of domain data~\cite{liu2024visual,zhu2023minigpt,chen2023minigpt,li2023blip,tong2024eyes}. When deploying VLMs as visual assistants in new domains, it is then expected that VLMs can be updated using a few images gathered from the target environment whenever deficiencies are noted. 

Continual learning allows a model to be continuously updated as new data from new tasks or domains are encountered.  Recent literature on continual learning (CL) of vision language models focus on updating either the LLM~\cite{srivastava2024improving} or language projection layers~\cite{das2024one}, maintaining a frozen image encoder. 

In vision language models, the LLM component provides reasoning and factual knowledge, while the image encoder's role is to extract robust and accurate visual features. In this work, we argue that adapting VLMs to new visual domains or tasks is more effective and efficient when the image encoder is updated rather than the LLM. Figure~\ref{fig:dalle_tsi} highlights this issue using images from the Toyota Smart Home dataset (TSI)~\cite{das2019toyota} dataset: in the first column, LLaVA~\cite{liu2024llava} struggles to recognize the person's action in the original image but accurately describes the same action in a generated image from OpenAI's DALL·E 2. This example underscores that the visual shift, rather than the LLM's understanding of the action, is the main source of weakness.

Motivated by the above limitations, we introduce a novel parameter-efficient fine-tuning (PEFT) method called \ours (Low-Rank Adaptation with Structured Updates) for selectively updating specific modules of the transformer blocks of image encoders within VLMs. The right column of Figure~\ref{fig:dalle_tsi} 
illustrates the (correct) responses of LLaVA after updating the image encoder separately with our method on a low-number of samples from TSI dataset compared to the pretrained LLaVA's (wrong) response.

Through extensive experiments, we demonstrate that updating the image encoder is essential for improving the performance of the VLM that relies on it. More importantly, this approach is computationally efficient, as the image encoder has significantly fewer parameters compared to the language model and the method is less prone to forgetting, especially the LLM knowledge.

We evaluated our approach on various VQA tasks comparing to state-of-the-art CL methods and the PEFT baseline LoRA\cite{hu2021lora} on various few-shot CL settings. We show significant improvements of the full VLM model on all settings and very low rates of forgetting  without using any replay buffer of data from the previous tasks. By selectively updating the image encoder, our method provides a robust and efficient solution for handling visual shifts. This targeted adaptation strategy avoids the need to modify the entire model, preserving existing knowledge whilst ensuring strong performance in new domains.

The contributions of the paper are as follows:
\begin{itemize}[noitemsep,topsep=1pt,parsep=1pt,partopsep=1pt]
    \item We propose LoRSU, a novel replay-free PEFT method tailored for few-shot continual learning.
    \item We introduce two new VQA datasets, TSI and DALLE, created to expose the limitations of pre-trained image encoders in VLMs.
    \item We conduct the first large-scale study of few-shot CL in VLMs, evaluating \ours across ten diverse VQA datasets and benchmarking against state-of-the-art PEFT and CL methods. \ours consistently outperforms all baselines.
 %   \item To the best of our knowledge, this work is the first to investigate few-shot continual learning in VLMs.
\end{itemize}

%Approaches that rely on pretrained image encoders typically use variants of the CLIP model \citep{radford2021learning}, which is kept frozen in the vision-language binding process~\rahaf{references}. 
%\rahaf{ CLIP is a widely deployed vision transformer that has strong zero-shot capabilities in various tasks and domains. However several  existing works have highlighted various weaknesses of CLIP on out of domain data~\cite{liu2024visual,zhu2023minigpt,chen2023minigpt,li2023blip,tong2024eyes} }.
%\rahaf{When deploying VLMs as visual assistants in new domains ( e.g., a persons home), it is then expected that VLMs can be updated few images gathered from the target environments whenever deficiencies are noted. }
%\rahaf{Continual learning targets continuous updates of  models as new data from new tasks or domain are encountered.  Recent literature on continual learning of multimodal models focus on updating either the LLM or language projection layers, maintaining the vision encoder frozen ( References from Related work).   }

%\rahaf{In vision language models, the LLM module posses the reasoning and factual knowledge, while the vision encoder role is to extract robust accurate visual feature. In this work, we argue that when adapting VLMs to new visual domains or task, it can be more beneficial in terms of both stability and efficiency to update the vision encoder instead. Figure~\ref{fig-Dalle-tsi} first column shows an example of an image from TSI (REF) dataset where LLaVA is not able to recognize the action of the person while on a generated image of the same action (from DALLE.E), LLaVA can perfectly describe the actions. Showing case the visual shift as source of weakness rather than the knowledge about the performed action by the LLM.}
%\rahaf{ Introduce LoRSU here. }
%\rahaf{Figure~\ref{fig-Dalle-tsi} second column shows the responses of LLaVA (REF) after updating the vision encoder separately with our method on fewshot examples from TSI dataset compared to the original LLaVA response. }
%\rahaf{We argue that updating the vision encoder can be necessary to enhance the performance of the entire family of VLMs built upon the encoder and more importantly it is computationally efficient as the image encoder contains far fewer parameters than the language model.}
%\rahaf{We evaluated our approach on various VQA tasks compared to SOTA CL methods and the PEFT baseline LoRA (ref) on various continual fewshot settings. We show significant improvements of the full VLM model on all settings and very low rates of forgetting  without sotring an replay any data of previous tasks. MAYBE ADD MORE POINTS FROM THE LAST PARAGRAPH.}
%\rahaf{Add contributions here.}

%CLIP, a widely deployed vision and text transformer, stands out for its robustness to domain shifts and outstanding capabilities in recognizing a wide range of objects, scenes, and actions. However, our evaluation reveals specific limitations in CLIP’s performance. Specifically, when tested on an action recognition dataset featuring various simple actions with moderate image quality, CLIP exhibits substandard performance and is easily confounded by the image content. Other works~\cite{liu2024visual,zhu2023minigpt,chen2023minigpt,li2023blip,tong2024eyes} reveal similar shortcomings of CLIP for particular use cases. These findings underscore weaknesses in visual understanding of CLIP, especially in challenging and previously unseen domains, and drive the need for continuous model improvements to address these imperfections.

%In the realm of continual learning and few-shot adaptation, enabling VLMs to efficiently adapt to new data or domains from limited examples is a critical challenge. This problem is particularly relevant in scenarios involving domain shifts, where the model encounters unfamiliar visual distributions rather than new knowledge altogether. A prominent example is our TSI-DALLE scenario, where specific visual shifts confound the model's performance. To address this, we propose a realistic and resource-efficient approach: focusing updates solely on the image encoder. This strategy allows for the correction of mistakes while preserving the model’s broader capabilities.

%Given the composite nature of VLMs, which combine vision encoders and language models, a broader question arises: Which components are better suited for targeted updates in such scenarios? To investigate this, we conducted separate fine-tuning experiments on the image encoder \emph{and} the Language Model (LLM) using a dataset where the original VLM exhibited numerous errors. The results were striking: updating the image encoder alone significantly improved performance on the specific data of interest, achieving even better accuracy than updating the LLM. Furthermore, this approach is computationally efficient as the image encoder contains far fewer parameters than the language model. Importantly, it enhances the performance of the entire family of VLMs built upon the encoder.

%Our findings underscore the importance of adapting the image encoder to new domains, especially in few-shot continual learning settings where labeled data is scarce and continuous adaptation creates new challenges. By selectively updating the image encoder, our method provides a robust and efficient solution to visual shifts. This targeted adaptation strategy avoids the need to modify the entire model, preserving existing knowledge while ensuring strong performance in new domains. Our work challenges the assumption that full-model adaptation is necessary in all cases, demonstrating that precise updates to specific components can achieve superior outcomes in continual learning scenarios.

%in summary of contributions say:
%- no memory required, no forgetting
%- adapting only the image encoder does not damage the %reasoning / chat ability of the LLM

\input{figures/tsi_dalle_llava}
