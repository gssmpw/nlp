\newpage
\appendix
\onecolumn

\section{Proof of the optimal mask $\bp^*$}\label{sec_appx:proof_mask}
\begin{definition}\label{def:tops}
The operator TOP-$C: \mathbb{R}^d \rightarrow \mathbb{R}^d$, for $1 \leq C \leq d$ is defined as
\begin{equation}\
    \left( \text{TOP-}C(\bx) \right)_{\pi(i)} := \left\{
\begin{array}{ll}
      x_{\pi(i)}, & i \leq C \\
      0, & \text{otherwise},\\
\end{array} 
\right. \nonumber
\end{equation}
where $\bx = (x_1, \ldots, x_d)^{\top} \in \RR^d$ and $\pi$ is a permutation of $\{1, 2, \ldots, d \}$ such that $|x_{\pi(i)}| \geq |x_{\pi(i+1)}|$, for $i=1, \ldots, d-1$, i.e. the TOP-$S$ operator keeps only the $S$ largest elements of $\bx$ in magnitude and truncates the rest to zero.
\end{definition}

\begin{lemma}\label{lemma:max_norm}
For any $\bx \in \mathbb{R}^d-\{ \mathbf{0}\}$, $1 \leq C \leq d$, the optimal mask
\begin{align}
     \bp^* & =  \argmax_{\bp \in \{0, 1 \}^d} \frac{\norm{\bp \odot \bx }^2}{\norm{\bx}^2},~~\text{s.t.}~\norm{\bp}_0 \leq C,  \nonumber
\end{align}
has zeros everywhere except the $C$ largest elements of $\bx$ in magnitude.
\begin{proof}
    Rewriting the optimization problem as 
    \begin{equation}
        \max_{\bp \in \{0, 1 \}^d} \sum_{i=1}^d p_i x_i^2,~~ \text{s.t.}~\sum_{i=1}^d p_i \leq C \nonumber,
    \end{equation}
    Notice that this is a trivial binary knapsack problem with maximum weight capacity $C$ and weights equal to one. Hence, the maximum is attained when we pick the top $C$ maximal $x_i^2$ elements. 
\end{proof}
\end{lemma}

\begin{remark}

\end{remark} It holds that $\text{TOP-}S(\bx) = \bp^* \odot \bx$.


\begin{corollary}
The optimal mask $\bp^*$ in \eqref{eq:maxim} has zeros everywhere except for the indices $i \in \{j: \exists \ell \in \{1, \ldots, G \},~\text{such that}~j \in \{ \pi_{\ell}(1), \ldots,  \pi_{\ell}(c_{\ell})\}    \}$, where $\pi_{\ell}$ is the same permutation as in Definition $\ref{def:tops}$ for the set of indices $I_{\ell}
$.
\end{corollary}
\begin{proof}
    The result follows from the mutual exclusiveness of $I_{\ell}$ in the constraints of \eqref{eq:maxim} and Lemma \ref{lemma:max_norm}.
\end{proof}

\section{Implementation Details}\label{sec_appx:implementation_details}
We describe below the implementation details of section~\ref{sec:experiments}.
\begin{itemize}
    \item All the experiments are conducted on a single NVIDIA A100 GPU.
    \item We have included error bars over three runs for all experiments.
    \item We use PyTorch~\cite{paszke2019pytorch} to implement all the algorithms.
    \item We use Adam~\citep{kingma2014adam} as an optimizer for the methods that utilize the CLIP loss for fine tuning and AdamW~\citep{loshchilov2017decoupled} for those ones that use the perplexity loss. 
    \item A learning rate scheduler of Cosine Annealing with Warmup is employed for all methods.
    \item For all experiments, we set the learning rate $1 \times 10^{-5}$ and $2 \times 10^{-5}$, for LoRSU and LoRSU-Ppl, respectively.
    \item We set batch size to 16 for all methods that fine-tune the vision encoder through CLIP loss. We reduce the batch size to 8 for those methods that fine-tune the vision encoder through perplexity loss or those that fine-tune the LLM. This was due to GPU memory limitations.
    \item All methods run for 20, 15, and 10 epochs for the CL-5, CL-10, and CL-50 settings, respectively.
    \item For LoRA (-Ppl), we set rank $r=64$ while LoRA-L and LoRA-F use $r=8$, for all experiments.
    \item For AdaLoRA, we set the initial rank to 70 and the final average rank to 64.
    \item The adapters of LoRA and AdaLoRA are applied to all weight matrices of each of the transformer blocks.
    \item For SPU, we use sparsity=15\% for all experiments.
    \item For \ours (-Ppl) we use sparsity=10\%, rank=64, and we pick the top-2 attention heads for all experiments.   
\end{itemize}
The choice of the above hyperparameters ensures that LoRA (-Ppl), LoRA-L, LoRA-F, AdaLoRA. SPU, and LoRSU (-Ppl) have similar number of trainable parameters. 

\section{Datasets}\label{sec_appx:datasets}
Details on all datasets used in section~\ref{sec:experiments} are presented here. 

\subsection{TSI \& DALLE}
We start with the description of how we constructed our newly introduced VQA datasets \emph{TSI} and \emph{DALLE}.

\paragraph{TSI.}  To extract  images from the videos of the Toyota Smart Home dataset (TSI), we discretized each video clip into 2 frames per second and then selected the frame in the middle of the total time duration of the video clip. In Table~\ref{table:tsi_class_names} we describe the actions that were selected and the corresponding prompt used for CLIP classification. We also note dropping few actions to avoid ambiguous classes.
\input{tables/tsi_classes}

\paragraph{DALLE.} We generated images from DALLÂ·E 2 using OpenAI python package and we used the prompt
 ``\textit{A person} $\{a\}$'' where $a \in $ \{ \textit{using a white coffee machine, 
                 eating, 
                 cutting bread, 
                 stirring the pot, 
                 holding a glass, 
                 watching TV, 
                 holding a bottle, 
                 walking, 
                 making tea, 
                 cutting food, 
                 holding a cup, 
                 using a laptop, 
                 lying down, 
                 holding a can, 
                 person holding a black kettle, 
                 reading a book, 
                 cleaning up, 
                 sitting down, 
                 using a tablet, 
                 boiling water in a black kettle, 
                 using a cordless phone, 
                 washing dishes}\}.

In Table~\ref{table:datasets_num_data}, we present the average number of images per session used to update the model for each CL setting. Finally, Table~\ref{table:datasets_num_data_evaluation} provides characteristics of the datasets used for evaluating performance.

\subsection{Continual Learning Splits}
For the continual learning settings of section~\ref{sec:experiments}, we split all datasets into five non-overlapping continual learning (CL) splits based on the classes/categories of each dataset. Unless stated otherwise, we use the training split of each dataset to construct these CL splits.

\paragraph{GTS~\cite{stallkamp2012man}.} We split the 43 classes of GTS as follows:
\begin{itemize}
    \item \emph{Session 1:} $\left[ 25, 2, 11,  1, 40, 27,  5,  9, 17 \right]$.
    \item \emph{Session 2:} $\left[ 32, 29, 20, 39, 21, 15, 23, 10, 3 \right]$.
    \item \emph{Session 3:} $\left[ 18, 38, 42, 14, 22, 35, 34, 19, 33 \right]$.
    \item \emph{Session 4:} $\left[ 12, 26, 41, 0, 37, 6, 13, 24 \right]$.
    \item \emph{Session 5:} $\left[ 30, 28, 31, 7, 16, 4, 36, 8 \right]$.
\end{itemize}

\paragraph{TSI~\cite{das2019toyota}.} We split the 27 action categories of TSI as follows:
\begin{itemize}
    \item \emph{Session 1:} [\textit{WatchTV}, \textit{Laydown}, \textit{Sitdown}, \textit{Pour.Fromkettle}, \textit{Enter}, \textit{Drink.Frombottle}].
    \item \emph{Session 2:} [\textit{Eat.Attable}, \textit{Pour.Frombottle}, \textit{Cook.Cleandishes}, \textit{Maketea.Boilwater}, \textit{Leave}, \textit{Cook.Cleanup}].
    \item \emph{Session 3:} [\textit{Maketea.Insertteabag}, \textit{Makecoffee.Pourwater}, \textit{Drink.Fromcan}, \textit{Readbook}, \textit{Cutbread}].
    \item \emph{Session 4:} [\textit{Drink.Fromcup}, \textit{Drink.Fromglass}, \textit{Usetablet}, \textit{Pour.Fromcan}, \textit{Usetelephone}].
    \item \emph{Session 5:} [\textit{Walk}, \textit{Cook.Stir}, \textit{Makecoffee.Pourgrains}, \textit{Cook.Cut}, \textit{Uselaptop}].
\end{itemize}

\paragraph{CAn~\cite{wang2024clips}.} The 45 classes of CAn are split as follows:
\begin{itemize}
    \item \emph{Session 1:} $\left[ 102, 9, 20, 56, 23, 30, 357, 291, 144 \right]$.
    \item \emph{Session 2:} $\left[ 41, 293, 42, 49, 54, 57, 70, 279, 305 \right]$.
    \item \emph{Session 3:} $\left[ 71, 10, 76, 79, 349, 16, 81, 83, 100 \right]$.
    \item \emph{Session 4:} $\left[ 130, 30, 133, 150, 275, 276, 58, 277, 80 \right]$.
    \item \emph{Session 5:} $\left[ 39, 290, 37, 296, 316, 337, 89, 360, 128 \right]$.
\end{itemize}
The indices of CAn correspond to those of ImageNet~\cite{imagenet} since the dataset was built based on these 45 animal classes of ImageNet.

\paragraph{AIR~\cite{maji13fine-grained}.} We split the 100 aircraft types of AIR as follows:
\begin{itemize}
    \item \emph{Session 1:} $\left[ 23, 8, 11, 7, 48, 13, 1, 91, 94, 54, 16, 63, 52, 41, 80, 2, 47, 87, 78, 66 \right]$.
    \item \emph{Session 2:} $\left[ 19, 6, 24, 10, 59, 30, 22, 29, 83, 37, 93, 81, 43, 99, 86, 28, 34, 88, 44, 14 \right]$.
    \item \emph{Session 3:} $\left[ 84, 70, 4, 20, 15, 21, 31, 76, 57, 67, 73, 50, 69, 25, 98, 46, 96, 0, 72, 35 \right]$.
    \item \emph{Session 4:} $\left[ 58, 92, 3, 95, 56, 90, 26, 40, 55, 89, 75, 71, 60, 42, 9, 82, 39, 18, 77, 68 \right]$.
    \item \emph{Session 5:} $\left[ 32, 79, 12, 85, 36, 17, 64, 27, 74, 45, 61, 38, 51, 62, 65, 33, 5, 53, 97, 49 \right]$.
\end{itemize}

\paragraph{ESAT~\cite{helber2019eurosat}.} We split the 10 different land terrain classes of ESAT as follows:
\begin{itemize}
    \item \emph{Session 1:} $\left[ 0, 1 \right]$.
    \item \emph{Session 2:} $\left[ 2, 3 \right]$.
    \item \emph{Session 3:} $\left[ 4, 5 \right]$.
    \item \emph{Session 4:} $\left[ 6, 7 \right]$.
    \item \emph{Session 5:} $\left[ 8, 9 \right]$.
\end{itemize}

\paragraph{DALLE.} This dataset was only used for performance evaluation (control dataset), and not fine-tuning.

\paragraph{VSR~\cite{Liu2022VisualSR}.} The images of this VQA dataset are labeled according to 36 different categories that describe the dominant object of the image. We create the CL splits as follows:
\begin{itemize}
    \item \emph{Session 1:} [\textit{oven}, \textit{dining table}, \textit{spoon}, \textit{boat}, \textit{cake}, \textit{donut}, \textit{sandwich}].
    \item \emph{Session 2:} [\textit{fire hydrant}, \textit{elephant}, \textit{airplane}, \textit{truck}, \textit{apple}, \textit{hot dog}, \textit{sheep}].
    \item \emph{Session 3:} [\textit{kite}, \textit{baseball glove}, \textit{cow}, \textit{tie}, \textit{scissors}, \textit{toaster}, \textit{tv}].
    \item \emph{Session 4:} [\textit{bicycle}, \textit{banana}, \textit{couch}, \textit{teddy bear}, \textit{bus}, \textit{umbrella}, \textit{bird}].
    \item \emph{Session 5:} [\textit{potted plant}, \textit{bowl}, \textit{broccoli}, \textit{bottle}, \textit{knife}, \textit{orange}, \textit{person}, \textit{pizza}].
\end{itemize}

\paragraph{HM~\cite{kiela2020hateful}.} For the hateful memes dataset, since there was not any labeling information of the images so we can spli the images in a meaningful way, we randomly split the training images into five disjoint sets to create our final CL splits.

\paragraph{MMVP~\cite{tong2024eyes}.} This is the only dataset where no training split is available and it is comprised of just 300 images. For this reason, we only used it for evaluation in our experiments in the main paper. However, for completeness, we included results in Table~\ref{table:fine_tune_llm_mmvp} where we fine-tune on it. We use 150 images for training which are equally split into five sessions and the rest of the 150 images are used for evaluation. Thus, the setting can be considered as a 30-shot CL setting. 

\paragraph{VisOnly~\cite{kamoi2024visonlyqa}.} This dataset categorizes its samples into seven categories describing the nature of the geometric and numerical information in scientific figures. We created the splits as follows:
\begin{itemize}
    \item \emph{Session 1:} \textit{Geometry-Triangle}.
    \item \emph{Session 2:} \textit{Geometry-Quadrilateral}.
    \item \emph{Session 3:} \textit{Geometry-Length}
    \item \emph{Session 4:} \textit{Geometry-Angle}.
    \item \emph{Session 5:} [\textit{Geometry-Area}, \textit{3D-Size}, \textit{3D-Angle}].
\end{itemize}

\input{tables/datasets_num_data}

\section{Detailed Results}\label{sec_appx:detailed_res}

\subsection{CLIP-based Updates+}
The detailed accuracies for all baselines and datasets used to create Table~\ref{table:clip_baselines_summary} of the main paper can be found in Tables~\ref{table:vlm_vqa_acc_gtsrb_clip} through~\ref{table:vlm_vqa_acc_eurosat_clip}.
\input{tables/clip_baselines_all}

\subsection{Extra ACC and BWT results}\label{sec_appx:extra_bwt_results}
\input{tables/bwt_metrics_clip_full}
In Table~\ref{table:bwt_metrics_clip_full} we present results of the ACC and BWT on extra datasets plus the ones in the main paper. The results follow the same patterns as in section~\ref{sec:experiments} with LoRSU demonstrating the most consistent performance in both ACC and BWT compared to the other two baselines. SPU is close to \ours in terms of BWT but it significantly lacks behind in ACC.

\subsection{CLIP-based vs. Perplexity-based Updates+}
The detailed accuracies for all baselines and datasets used to create Table~\ref{table:ppl_vs_clip_summary} of the main paper can be found in Tables~\ref{table:fine_tune_llm_gtsrb} through~\ref{table:fine_tune_llm_eurosat}. We have also included results on fine-tuning the model using \emph{MMVP} dataset in Table~\ref{table:fine_tune_llm_mmvp}.
\input{tables/ppl_vs_clip_all}


\section{Extra Ablation Studies}\label{sec_appx:extra_ablations}

\subsection{Ablation on the rank $r$ of LoRSU}
In Table~\ref{table:ablation_ranks}, we investigate the effect on performance of using different ranks for LoRSU. As the rank $r$ increases, the VQA accuracy on the target dataset slightly improves, peaking at $r=64$. Beyond 
that, performance slightly decreases. Performance on other datasets remains relatively stable with small fluctuations.
\input{tables/ablation_ranks}

\subsection{Ablation on the number of optimal attention heads of LoRSU}\label{sec_appx:ablation_heads_general}
\input{tables/ablation_heads_general}
In Table~\ref{table:ablation_heads}, we examine how the number of attention heads chosen to be fine-tuned affects LoRSU's performance. We notice that more attention heads marginally improve the performance of the model while the extra flexibility can cause more forgetting, e.g. ESAT.

\subsection{Robustness on the Choice of Attention Heads}\label{sec_appx:robustness}
\input{tables/lorsu_attn_rand_epochs}
We show in Table~\ref{table:lorsu_attn_rand_epochs} that LoRSU's mechanism of choosing the most important attention heads provides a clear advantage in terms of robustness over the other two LoRSU's variants, LoRSU-Rand and LoRSU-AAH. We can see that TI and CC decline in a lower rate compared to that of LoRSU-RAnd and LoRSU-AAH, as we increase the number of training epochs.. As expected, LoRSU-Rand appears to be the least robust method since the random choice of the attention heads constitute it more unstable.

\section{TSI vs. DALLE}\label{sec_appx:tsi_vs_dalle}
In Figures~\ref{fig:tsi_example_1} through~\ref{fig:tsi_example_4}, we present examples of images from TSI and DALLE for different actions. In general, we observe that TSI comprised of   natural, unposed images of senior individuals performing daily tasks, reflecting real-life scenarios. The images are broader, showing the surrounding environment, which is crucial for context. On the other hand, DALLE images are idealized or stylized images. The focus is narrower, with emphasis on the object of the action (e.g. tablet, glass, etc.).

\input{figures/tsi_examples_figure}
