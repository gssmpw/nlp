\begin{figure*}
\vskip 0.2in
\centering
\includegraphics[width=0.8\textwidth]{figures/LorSU.png}
%\vspace{-0.5cm}
\caption{LoRSU mechanism: After computing the gradient $\nabla_{\btheta} \cL_t (\btheta)$ over the target dataset at time $t$, LoRSU picks a small number of attention heads and a small number of paremeters from the first linear layer of the MLP module in the transformer block based on the magnitude of the gradients of $\nabla_{W_{\text{Attn}}} \cL_t $ and $\nabla_{W_{\text{fc1}}} \cL_t$, respectively. Computational efficiency is ensured by introducing LoRA adapters to the attention weight matrices.}
\label{fig:LorSU}
%\vspace{-0.5cm}
\vskip -0.2in
\end{figure*}