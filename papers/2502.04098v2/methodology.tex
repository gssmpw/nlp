\section{\texorpdfstring{\underline{Lo}w-\underline{R}ank Adaptation with \underline{S}tructured \underline{U}pdates}{Lg}}\label{sec:lorsu_method}

%To efficiently fine-tune large visual encoders or any transformer-based models, including large language models, without incurring catastrophic forgetting (i.e., a deterioration in model performance on other datasets and tasks), we have developed a new parameter-efficient fine-tuning method called Low-Rank Adaptation with Structured Updates (\textbf{LoRSU}). \\
%LoRSU aims to update specific parameters from each transformer block in a parameter efficient way while circumventing the risk of generic knowledge loss due to fine-tuning on the current task. To achieve this, we update a small part of the parameters of the first linear layer of the MLP block in the transformer module of each layer as in \cite{zhang2024overcoming}. However, this approach might lack flexibility since all the other parameters from the transformer block remain frozen during optimization. To overcome this, we also update the most informative attention heads based on the gradient information of the loss function.

Few-shot continual learning is a highly practical and challenging scenario, where models must incrementally adapt to new tasks with limited supervision while retaining previously acquired knowledge. This setting closely mirrors real-world applications, such as interactive AI assistants and autonomous systems, where models receive a continuous stream of novel data but only sparse supervision per update.

To address the challenge of efficiently fine-tuning large-scale visual encoders and transformer-based models under the few-shot continual learning setting, without causing catastrophic forgetting (i.e., degradation in performance on previously learned tasks), we propose a novel parameter-efficient fine-tuning method called \emph{Low-Rank Adaptation with Structured Updates} (\textbf{LoRSU}).

LoRSU updates specific parameters within each transformer block in a resource-efficient manner, mitigating the risk of generic knowledge loss when fine-tuning for new tasks. Specifically, we selectively update a subset of parameters from the first linear layer in the MLP block of each transformer layer, as proposed in \cite{zhang2024overcoming}. While this approach reduces the fine-tuning burden, it may limit model flexibility as the remaining parameters in the transformer block remain fixed. To enhance flexibility, we further update the most informative attention heads based on the gradient of the task-specific loss.

More specifically, let a dataset $\cD_t = \{\bx_n, \by_n \}_{n=1}^{N_t}$ for the current task $t$ where $\bx_n$ is an image with text description $\by_n$. We define $\cL(\btheta; \cD_t) := \cL_t(\btheta)$ as the  loss used for training the model and $\btheta \in \mathbb{R}^d$ is the full set of model's parameters. The standard Multi-head Self-Attention Mechanism (MSA)~\cite{vaswani2017attention}, comprised of $H$ $D_h$-dimensional heads, is defined as the concatenation of multiple self-attention (SA) blocks where $\bq^{(i)} = W_q^{(i)} Z^{\top}, \bk^{(i)} = W_k^{(i)} Z^{\top}, \bv^{(i)} = W_v^{(i)} Z^{\top} \in \mathbb{R}^{D_h \times N}$, are the query, key and value matrices, which are used to compute the self-attention outputs as follows
\begin{align}
        A^{(i)} & = \text{softmax}( \bq^{(i)^\top} \bk^{(i)} / \sqrt{D_h} )  \in \mathbb{R}^{N \times N}, \\
        \text{SA}_i(Z) & = A^{(i)} \bv^{(i)^\top}  \in \mathbb{R}^{N \times D_h}, ~~i=1, \ldots, H.
\end{align}
$Z \in \mathbb{R}^{N \times D}$ is the input matrix of $N$ tokens of dimension $D$ and $ W_q^{(i)},  W_k^{(i)},$ and $ W_k^{(i)}$ are the query, key, and value matrices of learnable parameters for head $i$, respectively. The final MSA function is defined as 
$\text{MSA}(Z) = \text{Concat}\left[ SA_1(Z), \ldots, SA_H(Z)  \right] W_o \in \mathbb{R}^{N \times D},~W_o \in \mathbb{R}^{H D_h \times D}$.

Since we care to update the parameters of the heads that cause the largest changes in  $\cL_t(\btheta)$, we compute the gradient of the loss with respect to the parameters of each head and then we update only those heads with the largest cumulative contribution to the loss change. Since the matrices $W_q^{(i)},  W_k^{(i)},  W_v^{(i)}$ are all the parameters of head $i$, we can define an importance score for each head by adding the squared values of their corresponding gradients $G_q^{(i)} = \nabla_{W_q^{(i)}} \cL_t$, $G_k^{(i)} = \nabla_{W_k^{(i)}} \cL_t$, and $G_v^{(i)} = \nabla_{W_v^{(i)}} \cL_t$, as follows
\begin{equation}
    s_i = \sum_{m,l} \left( (G_q^{(i)}[m,l])^2 + (G_k^{(i)}[m,l])^2 + (G_v^{(i)}[m,l])^2 \right). \label{eq:s_i} 
\end{equation}
We provide a theoretical justification of \eqref{eq:s_i} in the next section. We update only the top-$k$ heads, based on their importance scores $\{s_1, \ldots, s_H \}$, $I \subset \{1, \ldots, H \}$, to be updated on the current task. Nevertheless, the number of parameters remain high due to the large weight matrices. Therefore, we parametrize the original weights using LoRA~\cite{hu2021lora} to further reduce the computational burden. The matrices $W_q^{(i)}, W_k^{(i)}, W_v^{(i)}, i \in I$ are now defined as
%\begin{align}
%    W_q^{(i)^{\prime}} & = W_q^{(i)} + A_q^{(i)} B_q^{(i)} \\
%    W_k^{(i)^{\prime}} & = W_k^{(i)} + A_k^{(i)} B_k^{(i)} \\
%    W_v^{(i)^{\prime}} & = W_v^{(i)} + A_v^{(i)} B_v^{(i)}.
%\end{align}
\begin{equation}
    W_{\alpha}^{(i)^{\prime}} = W_{\alpha}^{(i)} + A_{\alpha}^{(i)} B_{\alpha}^{(i)},~~~\alpha \in \{q, k, v \}. \label{eq:lora_attn_weights}
\end{equation}
Finally, to ensure that we only update $W_q^{(i)},  W_k^{(i)},  W_v^{(i)}, \forall i \in I$ we use a binary mask on the gradient vector with respect to all parameters of all attention heads. We keep the projection matrix $W_o$ frozen. We note that most modern implementations of transformer blocks concatenate the three attention weight matrices $W_q, W_k, W_v$ into one and thus we only need to apply LoRA once to this concatenated matrix.

Regarding the first linear layer in the MLP module, $W_{\text{fc1}} \in \mathbb{R}^{d \times D}$, we mask the gradients of $W_{\text{fc1}}$ so only the most important parameters for the current task to be updated, i.e.~we use the following biased gradient update.
\begin{equation}
\hat{\nabla}_{W_{\text{fc1}}} \cL_t = M_{\text{fc1}} \odot \nabla_{W_{\text{fc1}}} \cL_t,
\end{equation}
where  $M_{\text{fc1}} \in \{0, 1 \}^{d \times D}$ is a zero-one mask that is built by choosing a proportion of the largest squared values of $\nabla_{W_{\text{fc1}}} \cL_t$ in a similar manner as in~\cite{zhang2024overcoming} and $\odot$ is the Hadamard product.

\textbf{Theoretical justification.}~~The importance scores in \eqref{eq:s_i} can be derived from the following constrained (binary) optimization problem\footnote{For notational simplicity, we assume a single transformer block for this case.}
\begin{align}
 & \bp^*  =  \argmax_{\bp \in \{0, 1 \}^d} \frac{\norm{\bp \odot \nabla_W \cL(\btheta_0) }^2}{\norm{\nabla_W \cL(\btheta_0)}^2},  \label{eq:maxim} 
\\ & \text{s.t.}~~\bigcup_{\ell=1}^G I_{\ell} \subset \{1, 2, \ldots, d\},~I_i \cap I_j = \emptyset,~~\forall i\neq j, \nonumber \\
& \text{and}~~ C = \sum_{\ell=1}^G c_{\ell},~~c_{\ell} \leq |I_{\ell}|~~\forall \ell,~~~\norm{\bp}_0 \leq C, \nonumber
\end{align}
where $\btheta_0$ is the vector of the pretrained parameters before using $\cD_t$ for fine-tuning the model. The groups of parameters $I_i$ correspond to the parameters of a specific module (e.g. Self-Attention or MLP projector) we aim to learn, hence the constraint of mutually exclusiveness, $I_i \cap I_j = \emptyset$, between different pairs of parameter groups. Also note that we allowed to choose a subset $c_{\ell}$ of the parameters of a specific group $I_{\ell}$ which is the underneath mechanism of \ours choosing attention heads and parameters of fc1. The mask $\bp^*$ is chosen so that the gradient norm of the masked gradients is as large as possible under the sparsity constraints. We prove in appendix \ref{sec_appx:proof_mask} that the indices of the non-zero values of $\bp^*$ can be found using the importance scores in \eqref{eq:s_i} and the magnitudes of the gradients with respect to the fc1 parameters. 