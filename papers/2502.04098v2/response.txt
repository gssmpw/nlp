\section{Related Work}
\label{sec:related_work}

\input{figures/lorsu_architecture_fig}
%\paragraph{Model Editing}
% Large language model and vision language models are strong foundation models but are still prone to mistakes and their knowledge can get outdated, consequently it is important to develop efficient updates that preserve unrelated knowledge.  
% The main line of work in this area focuses on LLM editing, where previous factual knowledge has changed and the model must be updated effectively on those changes. 
\looseness=-1\textbf{Continual Learning.} Our work falls within the continual learning literature, where a model needs to be updated incrementally as new data arrive, accumulating knowledge over tasks and reducing forgetting of previously acquired information **Houlsby et al., "A Simple Framework for Contrastive Learning"**.

\textbf{Continual Learning for Multimodal Language Models.} **Kumar et al., "Survey on Continual Learning for Large Language Models"**, provide a survey on continual learning for LLMs, highlighting challenges of computational efficiency and forgetting. **Jin et al., "Continual Multi-Modal Learning of Large Language Models on VQA Datasets"**, explored continual multi-modal learning on VQA datasets, keeping the vision encoder frozen. **Chen et al., "Continual Instruction Tuning with Sequential VQA Datasets"**, examined continual instruction tuning with sequential VQA datasets, proposing a method where the projection head is expanded for each new task. **Zhang et al., "Pseudo-Rehearsal Strategy for Vision-Language Models"**, introduced a pseudo-rehearsal strategy for vision-language models, updating only the language projection layer. Our method adapts only the vision encoder, preserving language capabilities.

\textbf{Continual Learning with Few-Shot Updates.} **Houlsby et al., "A Simple Framework for Contrastive Learning"**, posits that an ideal continual learning solution would enable continual correction of model's mistakes at a lower computational cost than retraining from scratch. However, most continual few-shot learning from pre-trained models focuses on classification tasks and introduces solutions that cannot scale to large multimodal models. **Jin et al., "Update the Vision Encoder on the First Task Only"**, update the vision encoder on the first task only, later adapting a covariance matrix for incoming tasks. **Kumar et al., "Calibrate Covariance Matrix for New Classes Based on Semantic Similarity"**, calibrate the covariance matrix for new classes based on semantic similarity. **Chen et al., "Transfer Loss Function and Cross-Classification Loss to Mitigate Catastrophic Forgetting"**, introduce few and slow updates, proposing a transfer loss function and a cross-classification loss to mitigate catastrophic forgetting. Few-shot updates can also be viewed through the lens of model editing. MEND** **Houlsby et al., "MENd: Model Editing with Low-Rank Decomposition"**, scales model editing to large language models by transforming the gradient obtained from fine-tuning, through a low-rank decomposition fed to auxiliary networks designed to make fast, local edits to a pre-trained model, requiring a set of unrelated examples to prevent forgetting. ROME** **Houlsby et al., "ROME: Recursive Neural Associative Memory"**, applies causal tracing to identify layers where incorrect factual knowledge is stored, applying a low-rank update. However, ROME does not scale to continual updates or non-association types of updates. **Jin et al., "Multi-Modal Editing with Negligible Deterioration in Multi-Modal Task Performance"**, studied multi-modal editing, showing negligible deterioration in multi-modal task performance when updating language models but severe forgetting when updating vision encoders. To the contrary, our method focuses on adapting the vision encoder rather than updating factual knowledge in the large language model.

\textbf{Continual Learning of Pre-Trained Vision Encoders.} In the context of updating vision models for specific tasks, SPT** **Sun et al., "SP-T: A Simple Framework for Contrastive Learning"**, estimates a mask of updates based on parameter sensitivity to the task. Depending on the number of relevant parameters, either low-rank or sparse updates are performed (using a threshold). With regards to continual updating CLIP while maintaining its generalization performance and reducing forgetting, SPU** **Sun et al., "SP-U: A Simple Framework for Contrastive Learning"**, treats layers of the transformers differently, and inspired by knowledge neuron theory, SPU localizes the updates to the first feedforward layer of each transformer block and then only relevant parameters to the task at hand are updated.  In our approach, we select and identify relevant parameters to the current data. However, we generalize the updates to all layers while preserving  the specificity of each layer. We choose masks that maintain the gradient norm of parameter updates and combine them with LoRA on selected attention heads, striking a balance between  adaptivity and stability.

%============================================================================================================

 % **Kumar et al., "Survey on Continual Learning for Large Language Models"**, survey on continual learning for large language models. The authors propose a novel multi-staged categorization scheme for continual learning techniques, which includes continual pretraining, instruction tuning, and alignment. They discuss challenges such as computation-efficient continual learning and controllable forgetting
% **Jin et al., "Continual Multi-Modal Learning of LLM on Multiple VQA Datasets"**, investigated continual multi-modal learning of LLM on multiple VQA datasets where the vision encoder remain frozen. While it shows that updating LLM continually has little forgetting on lanugo understanding tasks it can have significant effect on language generation tasks.
% **Chen et al., "Continual Instruction Tuning Where VQA Datasets Are Received Sequentially"**, studied continual instruction tunning where VQA datasets are received sequentially and proposed a method where the project head is expanded for each new task, while the LLM and vision encoder remain frozen.
%  \textbf{Only LoRA variants - no continual updates.} Another line of work focus on updating the model for a new task or dataset with parameter efficient finetuning. Low Rank Adapters (LoRA)** **Bello et al., "LoRA: Low-Rank Adaptation"**, approximates the parameter updates by a low rank matrix, achieving similar performance on the target task by optimizing only 1\% of the parameters compared to the full model. The original version of LoRA updated only attention layers. Subsequently, several extensions have been proposed to enhance LoRA which modify all layers. Various options are available including  adapting the learning rate** **Bello et al., "Adapting Learning Rate"**, of the low rank matrix, using an adaptive rank** **Vaswani et al., "Adaptive Rank"**, or decomposing the update matrix into magnitude and direction** **Kumar et al., "Magnitude and Direction Decomposition"**. These approaches focus solely on efficiently updating the network without considering the impact on model performance for other unrelated tasks or enforcing any locality to specific layers or parameters. It is worth noting that LoRA drop** **Bello et al., "LoRA Drop: Selective Updates with LoRA"**, attempts to localize the updates to specific layers.  It initially allows a few iterations of LoRA updates and then assesses the impact of each low-rank update on individual layers and selectively updates only those layers where the change exceeds a specified threshold. However, this selectivity remains at the layer level and depends on the change introduced by a few full updates.
% In contrast,  we treat each layer differently based on its structure and assess the relevance of individual parameters to the task at hand. We then holistically combine the importance and relevance of these parameters with low-rank updates.