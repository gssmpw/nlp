We consider a two-time-scale nonlinear NCS as depicted in Figure \ref{fig: Block Diagram}, designed using emulation techniques \cite{dragan_stability}. Specifically, a dynamic continuous output-feedback controller is developed to ensure robustness for both the \emph{reduced} (slow) system and the \emph{boundary-layer} (fast) system, initially without considering the network. Subsequently, the network is designed by establishing bounds on transmission intervals and selecting an appropriate scheduling protocol \cite{dragan_stability}. The resulting continuous-time controller is then deployed over the network, with the objective of providing conditions under which the stability of the SPNCS is guaranteed. Details on the emulation design framework are provided in Section 5.
%
Next, we introduce the model of Figure \ref{fig: Block Diagram}.
%
% We consider a two-time-scale nonlinear NCS, shown in Figure \ref{fig: Block Diagram}, which is designed by emulation \cite{dragan_stability}. 
% \red{In particular, a dynamic output-feedback controller is assumed to be designed to ensure robustness properties for both the \emph{reduced system} (slow) and the \emph{boundary-layer system} (fast) without network constraint. Then we design the network by defining the bounds on transmission intervals and selecting the scheduling protocol \cite{dragan_stability}.
% %
% The controller is then deployed over the network, and 
% our aim is to provide condition on the original closed-loop system and the network under which stability of the SPNCS follows. See Section \ref{Section Emulation design framework} for details one the emulation design framework.}
% %appropriate MATIs are selected to ensure the stabilization of the NCS. 
% Next, we introduce the model of Figure \ref{fig: Block Diagram}.

\begin{figure}[H]
    \centering
    \includegraphics[width = 0.6\linewidth]{Figures/Block_diagram_small_font.pdf}
    \caption{NCS Block Diagram}
    \label{fig: Block Diagram}
\end{figure}

\subsection{Plant ($\mathcal{P}$) and Controller ($\mathcal{C}$)}
%Let $n_{x_p}, n_{z_p}, n_{y_s}, n_{y_f} \in \mathbb{Z}_{\geq 0}$.
We model the plant as the following SPS,
\begin{equation}
%\setlength\abovedisplayskip{4pt}%shrink space
%\setlength\belowdisplayskip{4pt}
    \mathcal{P}:
    \begin{cases}
    \begin{aligned}
    \dot x_p &= f_p(x_p, z_p,\hat u)\\
    \epsilon \dot z_p &= g_p(x_p, z_p, \hat u) \\
    y_p &= \left(y_s, y_f  \right) = \left(k_{p_s}(x_p) , k_{p_f}(x_p, z_p) \right) ,
    \end{aligned}
    \end{cases} 
    \label{eqn:plant}
\end{equation}
where $0 < \epsilon \ll 1$, $x_p \in \mathbb{R}^{n_{x_p}}$, $z_p\in \mathbb{R}^{n_{z_p}}$, $y_s\in \mathbb{R}^{n_{y_s}}$, $y_f \in \mathbb{R}^{n_{y_f}}$ and $n_{x_p}, n_{z_p}, n_{y_s}, n_{y_f} \in \mathbb{Z}_{\geq 0}$.
%
Here, $x_p$ and $z_p$ denote the slow and fast plant states, respectively, while $y_s$ and $y_f$ represent the slow and fast output, respectively. Additionally, $\hat u = (\hat u_s, \hat u_f)$ refers to the latest received control input $u$ in \eqref{eqn:controller} from the network. It is assumed that $k_{p_s}$ and $k_{p_f}$ are continuously differentiable, and $f_p$ and $g_p$ are locally Lipschitz.
%$f_p(0,0,0) = 0$, $g_p(0,0,0) = 0$, $k_{p_s}(0) = 0$, $k_{p_f}(0,0) = 0$ and $k_{p_f}$ is continuously differentiable.
%
%
%
% In our emulation-based approach, we assume that a dynamic controller has been designed to stabilise plant \eqref{eqn:plant} in the absence of network, \cyan{i.e., $\hat{y}_p \equiv y_p$ and $\hat{u} \equiv u$}, \todo{maybe remove this paragraph}
Similarly, the dynamic controller has the following form,
\begin{equation}
    \mathcal{C}:
    \begin{cases}
    \begin{aligned}
    \dot x_c &= f_c(x_c, z_c, \hat{y}_p)\\
    \epsilon \dot z_c &= g_c(x_c, z_c, \hat y_p) \\
    u &= (u_s, u_f) = \left(k_{c_s}(x_c), k_{c_f}(x_c,z_c) \right) ,
    \end{aligned}
    \end{cases}
    \label{eqn:controller}
\end{equation}
where $\epsilon$ comes from (\ref{eqn:plant}), $x_c \in \mathbb{R}^{n_{x_c}}$, $z_c \in \mathbb{R}^{n_{z_c}}$, $u_s \in \mathbb{R}^{n_{u_s}}$, $u_f \in \mathbb{R}^{n_{u_f}}$ and $n_{x_c}, n_{z_c}, n_{u_s}, n_{u_f} \in \mathbb{Z}_{\geq 0}$. Moreover, $\hat y_p = (\hat y_s, \hat y_f)$ refers to the most recently received output of the plant transmitted via the network. It is assumed that $k_{c_s}$ and $k_{c_f}$ are continuously differentiable, $f_c$ and $g_c$ are locally Lipschitz, and $u_s$, $u_f$, $y_s$, $y_f$ have the dimension as $\hat u_s$, $\hat u_f$, $\hat y_s$, $\hat y_f$, respectively.
%
%\sout{We have $n_{x_p} + n_{x_c} \geq 1$ and $n_{z_p} + n_{z_c} \geq 1$ to guarantee the existence of both the slow and fast states. We also have that $n_{y_s} + n_{u_s} \geq 1$ and $n_{y_f} + n_{u_f} \geq 1$, which ensures that both fast and slow signals are present in the system.}
%



%\sout{Our results generalize those from \cite{SPNCS} by considering the transmission of control inputs and plant outputs via the network, as opposed to transmitting the plant state and assuming the control input is transmitted through a perfect network.}
%
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width = 0.6\linewidth]{Figures/Block diagram small font.pdf}
%     \caption{NCS Block Diagram}
%     \label{fig: Block Diagram}
% \end{figure}





\subsection{Network ($\mathcal{N}$)}
A channel may consist of multiple \emph{network nodes}, each representing a group of sensors and/or actuators, see \cite{wang2017observer} for more information. In this paper, we consider that each node can only contain either slow (i.e., $y_s$, $u_s$) or fast (i.e., $y_f$, $u_f$) signals, but not both. Only one node can transmit data at any given transmission time, regulated by the channel scheduling protocol. This implies that slow signals are never transmitted simultaneously with fast signals. In particular, at each transmission time allocated to a slow (resp. fast) node, a group of elements in $y_s$ (resp. $y_f$) and $u_s$ (resp. $u_f$) accessible to that node is sampled and transmitted.

%In this context, we define $\mathcal{T} \coloneqq \{t_0, t_1, t_2, \cdots \}$ as a set of all transmission instants. Let $\mathcal{T}^s \coloneqq \{t_0^s, t_1^s, t_2^s, \cdots \} $ be the subsequence of $\mathcal{T}$ such that all the elements of $\mathcal{T}^s$ are the instances that a slow node gets access to the network. Then we define the set of instances that a fast note gets access to the network to be $\mathcal{T}^f \coloneqq \mathcal{T}-\mathcal{T}^s = \{t_0^f, t_1^f, t_2^f, \cdots \}$

In this context, we define $\mathcal{T} \coloneqq \{t_1, t_2, t_3, \cdots \}$ as the set of all transmission instants. Let $\mathcal{T}^s \coloneqq \{t_1^s, t_2^s, t_3^s, \cdots \}$ be the subsequence of $\mathcal{T}$ consisting of the instances that a slow node gains access to the network. We then define the set of instances that a fast node gets access to the network as $\mathcal{T}^f \coloneqq \mathcal{T} \setminus \mathcal{T}^s = \{t_1^f, t_2^f, t_3^f, \cdots \}$.
%
%
%
% Define $\mathcal{T}^s \coloneqq \{t_0^s, t_1^s, t_2^s, \cdots \} $ as the unbounded set of transmission times at which a slow node is transmitted,
% and $\mathcal{T}^f \coloneqq \{t_0^f, t_1^f, t_2^f, \cdots \}$ as the unbounded set of transmission times at which a fast node is transmitted,
% such that $\mathcal{T}^s \cap \mathcal{T}^f = \emptyset$. Then, let $\mathcal{T} \coloneqq \mathcal{T}^s \cup \mathcal{T}^f =  \{t_0, t_1, t_2, \cdots \} $ denote the set of all transmission instances, with its elements arranged in ascending time order.
%
We impose that for any $k \in \mathbb{Z}_{\geq 1}$, the transmission times satisfy
\begin{subequations}
    \begin{align}
    &\tau_{\text{miati}}^s \leq t_{k+1}^s - t_k^s \leq \tau_{\text{mati}}^s, \; \forall t_k^s,t_{k+1}^s\in \mathcal{T}^s,  \label{eqn: timer eqn1}
    \\
    &\tau_{\text{miati}}^f \leq t_{k+1}^f - t_k^f \leq \tau_{\text{mati}}^f ,  \; \forall t_k^f, t_{k+1}^f  \in \mathcal{T}^f,  
    \label{eqn: timer eqn2}
    \\
    &\tau_{\text{miati}}^f \leq t_{k+1} - t_k, \quad \qquad \; \; \ \forall t_k, t_{k + 1} \in \mathcal{T}, \label{eqn: timer eqn3}
    \end{align}
    \label{eqn: Stefan timer}%
\end{subequations}
\noindent where $0<\tau_{\text{miati}}^f\leq \tau_{\text{mati}}^f$ denote, respectively, the MIATI and MATI between any two consecutive fast transmissions. Similarly, $\tau_{\text{miati}}^s$ and $\tau_{\text{mati}}^s$ are the MIATI and MATI between two consecutive slow updates.
We note that since there might be a slow transmission between two consecutive fast transmissions,
\begin{equation}
    \tau_{\text{miati}}^f \leq  \tfrac{1}{2}\tau_{\text{mati}}^f
    \label{eqn: condition on miati^f}
\end{equation}
must hold to satisfy \eqref{eqn: timer eqn2} and \eqref{eqn: timer eqn3}, as in \cite{Stefan_thesis}.


Let the \emph{network-induced errors} be $e_{y_s} \coloneqq \hat{y}_s - y_s$, $e_{y_f} \coloneqq \hat{y}_f - y_f$, $  e_{u_s} \coloneqq \hat{u}_s - u_s$ and $  e_{u_f} \coloneqq \hat{u}_f - u_f $.
For simplicity, $(\hat{y}_s,\hat{y}_f,\hat{u}_s,\hat{u}_f)$ are assumed to be constant between any two successive transmission times, i.e., zero-order hold devices are used.
%Other type of network-processing may be implemented if desired, see, e.g., \cite{dragan_stability}.
Before we present the behaviour of the system at transmission times, we introduce some useful notation regarding the variables: $x\coloneqq (x_p,x_c)\in\mathbb{R}^{n_x}$, $z \coloneqq ( z_p, z_c) \in \mathbb{R}^{n_z}$, $e_s \coloneqq ( e_{y_s} , e_{u_s})\in \mathbb{R}^{n_{e_s}}$ and $e_f \coloneqq (e_{y_f} , e_{u_f}) \in \mathbb{R}^{n_{e_f}}$, with $n_x\coloneqq n_{x_p}+n_{x_c}$,  $n_z\coloneqq n_{z_p}+n_{z_c}$, $n_{e_s}\coloneqq n_{y_s}+n_{u_s}$ and  $n_{e_f}\coloneqq n_{y_f}+n_{u_f}$. 
 
At each transmission time $t_k^s \in \mathcal{T}^s$ for slow updates, the values $(\hat{y}_s,\hat{y}_f,\hat{u}_s,\hat{u}_f) $ are updated according to
$
\big(\hat{y}_s ( {t_k^s}^{+}),\hat{u}_s ( {t_k^s}^{+} )\big)
    =
    \big(
    y_s(t_k^s), u_s(t_k^s)
    \big)+ h_s(k, e_{s}(t_k^s) )
$
and
$
\big(\hat{y}_f ( {t_k^s}^{+} ), 
    \hat{u}_f ( {t_k^s}^{+})
    \big)
    =
    \left(
    \hat{y}_f(t_k^s), \hat{u}_f(t_k^s)
    \right)
$,
%
% \begin{equation*}
%     \begin{aligned}
%     \big(
%     \hat{y}_s ( {t_k^s}^{+}),
%     \hat{u}_s ( {t_k^s}^{+} )
%     \big)
%     =&
%     \big(
%     y_s(t_k^s), u_s(t_k^s)
%     \big)+ h_s(k, e_{s}(t_k^s) ),
%     \\
%     \big(
%     \hat{y}_f ( {t_k^s}^{+} ), 
%     \hat{u}_f ( {t_k^s}^{+})
%     \big)
%     =&
%     \left(
%     \hat{y}_f(t_k^s), \hat{u}_f(t_k^s)
%     \right) ,
%     \end{aligned}
% \end{equation*}
where the function $h_s: \mathbb{Z}_{\geq 0}\times \mathbb{R}^{n_{e_s}}  \rightarrow \mathbb{R}^{n_{e_s}}$ models the scheduling protocol \cite{dragan_stability} for the slow updates.
%
Similarly, for each $t_k^f \in \mathcal{T}^f$, we have 
$
\big(
    \hat{y}_s ( {t_k^f}^{+} ), 
    \hat{u}_s ( {t_k^f}^{+})
    \big)
    =
    \big(
    \hat{y}_s(t_k^f), \hat{u}_s(t_k^f)
    \big)
$
and
$\big(
    \hat{y}_f ( {t_k^f}^{+} ), 
    \hat{u}_f ( {t_k^f}^{+} )
    \big)
    =
    \big(
    y_f(t_k^f), u_f(t_k^f)
    \big) 
    + h_f\big(k, e_{f}(t_k^f) \big)$,
%
% \begin{align*}
%     \begin{aligned}
%     \big(
%     \hat{y}_s ( {t_k^f}^{+} ), 
%     \hat{u}_s ( {t_k^f}^{+})
%     \big)
%     =&
%     \big(
%     \hat{y}_s(t_k^f), \hat{u}_s(t_k^f)
%     \big),
%     \\
%     \big(
%     \hat{y}_f ( {t_k^f}^{+} ), 
%     \hat{u}_f ( {t_k^f}^{+} )
%     \big)
%     =&
%     \big(
%     y_f(t_k^f), u_f(t_k^f)
%     \big) 
%     + h_f\big(k, e_{f}(t_k^f) \big) ,
%     \end{aligned}
%    % \label{eqn: fast update}
% \end{align*}
where the function $h_f: \mathbb{Z}_{\geq 0}\times \mathbb{R}^{n_{e_f}} \rightarrow \mathbb{R}^{n_{e_f}} $ is the scheduling protocol for the update of fast components. 
%
If a SPNCS has $\ell$ slow nodes, then $e_s$ can be partitioned as $e_s = [e_{s,1}^\top \; e_{s,2}^\top \; \cdots \; e_{s,\ell}^\top]$. If the slow scheduling protocol $h_s$ grants the $i$th slow node access to the network at a transmission instance $t_k^s \in \mathcal{T}^s$, then $e_{s,i}$ experiences a jump. For protocols such as round robin (RR) and try-one-discard (TOD) \cite{dragan_stability}, $e_{s,i}({t_k^s}^+) = 0$ and $e_{s,j}({t_k^s}^+) = e_{s,j}({t_k^s})$ for all $j \neq i$, although this assumption is not generally necessary. The same rule applies to the fast nodes. 




% A variable useful for analysis is the so-called \emph{network-induced error}, which we define as $e_{y_s} \coloneqq \hat{y}_s - y_s$, $e_{y_f} \coloneqq \hat{y}_f - y_f$, $  e_{u_s} \coloneqq \hat{u}_s - u_s$ and $  e_{u_f} \coloneqq \hat{u}_f - u_f $.
% For simplicity, $(\hat{y}_s,\hat{y}_f,\hat{u}_s,\hat{u}_f)$ are assumed to be constant between any two successive transmission times (i.e. zero-order hold behaviour). Other type of network-processing may be implemented if desired, see, e.g., \cite{dragan_stability}.
% Define $x\coloneqq (x_p,x_c)\in\mathbb{R}^{n_x}$, $z \coloneqq ( z_p, z_c) \in \mathbb{R}^{n_z}$, $e_s \coloneqq ( e_{y_s} , e_{u_s})\in \mathbb{R}^{n_{e_s}}$ and $e_f \coloneqq (e_{y_f} , e_{u_f}) \in \mathbb{R}^{n_{e_f}}$, with $n_x\coloneqq n_{x_p}+n_{x_c}$,  $n_z\coloneqq n_{z_p}+n_{z_c}$,  $n_{e_s}\coloneqq n_{y_s}+n_{u_s}$ and  $n_{e_f}\coloneqq n_{y_f}+n_{u_f}$. 