\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage{amsmath}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{amssymb}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{booktabs} 
\usepackage{subcaption}
\usepackage{caption}
\usepackage[ruled, english]{algorithm2e}
\renewcommand{\algorithmcfname}{Algorithm}%
\SetKwInput{KwInput}{Input}%
\SetKwInput{KwOutput}{Output}%
\SetKwFor{For}{for}{do}{end}%
\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{definition}{Definition}
\usepackage{graphicx}


\title{PLS-based approach for fair representation learning}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
\date{} 					% Or removing it

\author{ \href{https://orcid.org/0009-0000-2966-3404}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Elena M.~De-Diego}\\
	Institute of Data Science and Artificial Intelligence (DATAI), \\
        Universidad de Navarra, Ismael Sánchez Bella Building, Campus Universitario\\
	31009 Pamplona, Spain \\
	\texttt{e.dediego.m@gmail.es} \\
	%% examples of more authors
	\And
	\href{https://orcid.org/0000-0002-8258-4454}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Adrián Perez~Suay} \\
	Image Processing Laboratory (IPL)\\
        Universitat de València  \\
	\texttt{Adrian.Perez@uv.es} \\
	\And
	\href{https://orcid.org/0000-0002-0455-1200}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Paula~Gordaliza} \\
	Universidad Pública de Navarra\\
    31009 Pamplona, Spain \\
	\texttt{paula.gordaliza@unavarra.es} \\
	\And
	\href{https://orcid.org/0000-0002-1252-2960}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Jean-Michel~Loubes} \\
	Institut de Mathématiques de Toulouse \\
        INRIA  \\
	\texttt{jean-michel.a.loubes@inria.fr} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{Under review}
\renewcommand{\undertitle}{Under review}
\renewcommand{\shorttitle}{PLS-based approach for fair representation learning}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={PLS-based approach for fair representation learning},
pdfsubject={Machine Learning},
pdfauthor={Elena M.~De-Diego},
pdfkeywords={Fair Representation Learning, PLS, Supervised Learning, Dimension Reduction},
}

\begin{document}
\maketitle

\begin{abstract}
% 
We revisit the problem of fair representation learning by proposing Fair Partial Least Squares (PLS) components. 
% 
PLS is widely used in statistics to efficiently reduce the dimension of the data by providing representation tailored for the prediction. 
% 
We propose a novel method to incorporate fairness constraints in the construction of PLS components.
% 
This new algorithm provides a feasible way to construct such features both in the linear and the non linear case using kernel embeddings. 
% 
The efficiency of our method is evaluated on different datasets, and we prove its superiority with respect to standard fair PCA method.
%
\end{abstract}


\keywords{Fair Representation Learning \and PLS \and Supervised Learning \and Dimension Reduction}


\section{Introduction}
\label{sec:introduction}
% 
Over the past few years, the increasing use of automated decision-making systems has been widely installed in businesses of private companies of all types, as well as government applications. 
% 
Since many of these decisions are made in sensitive domains, including healthcare \citep{Morik2010}, finance \citep{10.5555/573193}, criminal justice \citep{angwin2016machine}, or hiring \citep{Amazon}, society has experienced a significant impact on people's lives. 
% 
This fact has made the intersection between Artificial Intelligence (AI), Ethics and Law a crucial area of current research.
% 
Despite the success demonstrated by Machine Learning (ML) in these decision-making processes, there is a growing concern regarding the potential discriminatory biases in the decision rules. 
% 

% 
One promising approach to mitigate unfair prediction outcomes is fair representation learning proposed by \citet{pmlr-v28-zemel13} (see Section \ref{sec:background} for related work), which seek to learn meaningful representations that maintain the content necessary for a particular task while removing indicators of protected group membership.
% 
Once the fair representation is learned, any prediction model constructed on the top of the fair representation (i.e. using the representation as an input vector) are expected to be fair.
% 
Several works related to ours, such as \citet{pmlr-v206-kleindessner23a, Olfat_Aswani_2019, Lee_Kim_Olfat_Hasegawa-Johnson_Yoo_2022}, tackle the objective using principal component analysis (PCA). However, the new representation tends to be less useful for predicting the target when it is strongly correlated with some directions in the data that have low variance.
% 

% 
We propose an alternative formulation based on the dimensionality reduction statistical technique Partial Least Squares (PLS) \citep{5bf28666-1390-327d-b81f-93157976a3f4}. 
% 
The paper introduces fairness for PLS as doing PLS while minimizing the dependence of the projections with the demographic attribute.
% 
The main objective is to learn a representation that can trade-off some measure of fairness (e.g. statistical parity, equal opportunity) with utility (e.g. covariance with respect to the target, accuracy) and can be kernelized.
% 
Specifically, the goal is to create a representation that: (i) has lower dimension; (ii) preserves information about the input space; (iii) is useful for predicting the target; (iv) is approximately independent of the sensitive variable.
% 
Our formulation has the same complexity as standard Partial Least Squares, or Kernel Partial Least Squares, and have applications on different domains and with different data structures as tabular, image or text embeddings.
% 
To sum up, we address the following questions: (1) How can fairness be defined in the context of PLS? And (2) How to integrate feasible fairness constraints into PLS algorithms?

\paragraph{Outline of Contributions.}
% 
We make both theoretical and practical contributions in the field of fair representation learning and fair machine learning by proposing a dimensionality reduction framework for fair representation.  
% 
More precisely, our contributions can be outlined as follows.
\begin{itemize}
    \item Fair Partial Least Squares: in section \ref{sec:fairPLS} we first review the Partial Least Squares method and then we propose the fair formulation as a regularization in the iterative process of obtaining the weights. 
    \item Kernel Fair Partial Least Squares: in section \ref{sec:KFPLS} we present how the method  of Fair PLS can be extended to the non linear case by using kernel embedding and the Hilbert Schmidt independence criterion (HSIC) as the fairness term.   
    \item Application to different fields and different data: we present diverse experiments in both tasks (classification and regression) for tabular data and we discuss how such framework could improve fairness for Natural Language Processing algorithms.
    Some details and experiments are deferred to the appendix.
\end{itemize}

\paragraph{Notation.}
\label{sec:notation}
% 
For $n \in \mathbb{N}$, let $[n] = \{1, ..., n\}$. 
We generally denote scalars by non-bold letters, vectors by bold lower-case letters and matrices by bold upper-case letters. 
% 
All vectors $\mathbf{x} \in \mathbb{R}^{d} \equiv \mathbb{R}^{d \times 1}$ are column vectors, while $\mathbf{x}^{\mathsf{T}} \in \mathbb{R}^{1 \times d}$ represents its transpose, a row vector. 
% 
For a matrix $\mathbf{X} \in \mathbb{R}^{d_{1} \times d_{2}}$, let $\mathbf{X}^{\mathsf{T}} \in \mathbb{R}^{d_{2} \times d_{1}}$ be its transpose.
$\mathbf{I}_{r}$ denotes the identity matrix of size $r$.
% 
For $\mathbf{X} \in \mathbb{R}^{d \times d}$, let $\text{trace} (\mathbf{X}) = \sum_{i=1}^{d} \mathbf{X}_{i,i}$.
% 
We denote by $\mathbf{A} \succ 0$ and $\mathbf{A} \succeq 0$ if the matrix $\mathbf{A}$ is positive definite and positive semi-definite respectively.


\section{Background}
\label{sec:background}

\paragraph{Algorithmic fairness.}
% 
In the last decade, fairness in ML has established itself as a very active area of research which tries to ensure that predictive algorithms are not discriminatory towards any individual or subgroup of population, based on demographic characteristics such as race, gender, disabilities, sexual orientation, or political affiliation \citep{barocas2018fairness}.
% 
Although fair ML is a relatively new area of concern, the growing amount of evidence of discrimination found in increasingly varied fields, has driven the development of several approaches to this problem. 
% 
We refer to \citet{wang2022brief} for a brief review on algorithmic fairness. 
% 

%
In general, the different formalizations of the concept of fairness in the existing literature can be broadly classified into individual and group fairness.
% 
Let $\mathbf{X} \in \mathcal{X}$, $\mathbf{S} \in \mathcal{S}$ and $\mathbf{Y} \in \mathcal{Y}$ be the non-sensitive input, the sensitive attribute and the ground truth target variable, respectively.
% 
Group (or statistical) fairness emphasizes an equal treatment of individuals with respect to the sensitive attributes $S$, which can be expressed through a measure of statistical independence between the variables involved. 
% 
In particular, the two main notions along this line are Demographic Parity (DP) \citep{kamiran2012data} and Equality of Odds (EO) \citep{NIPS2016_9d268236}. 
% 
The measure DP requires that sensitive attributes should not influence the algorithm's outcome, that is $\hat{Y} \independent S$; while for EO such independence is conditional to the ground-truth, that is $\hat{Y} \independent S | Y$. 
% 
In the particular setting of binary classification, $\mathcal{Y}=\{0,1\}$, a classifier $c: \mathbb{R}^{d} \rightarrow \{0, 1\}$ is said to be DP-fair, with respect to the joint distribution of $(X, S)$, if $P(c(X) = 1 | S = s) = P(c(X) = 1)$. 
% 
On the other hand, $c$ is EO-fair, with respect to $(X, S)$, if $P(c(X) = 1 | S = s, Y = y) = P(c(X) = 1 | Y = y),$ for $s,y=0,1$. 
% 
A relaxed version of EO has been also proposed as Equality of Opportunity \citep{NIPS2016_9d268236} and requires only the equality in TPR, namely $P(c(X) = 1 | S = 0, Y = 1) = P(c(X) = 1 | S = 1, Y = 1)$. 
% 
On the other hand, individual fairness \citep{10.1145/2090236.2090255} examines individual algorithms' predictions and ensures that when two individuals are similar with respect to a specific task, they are classified similarly. 
% 
However, it is defined in terms of certain similarity metric for the prediction task at hand which is generally difficult to obtain. 
% 
Individual fairness is close to the notion of counterfactual fairness which specifies the notion of closeness between individual with a causal framework as shown in \citet{NIPS2017_a486cd07,JMLR:v25:21-1440}.
% 

% 
Regardless of the notion of fairness, methods for fair forecasting can be divided into \textit{(i) pre-processing} the input data from which the algorithms learns in order to remove sensitive dependencies \citep{kamiran2012data, NIPS2017_9a49a25d,gordaliza2019obtaining}; \textit{(ii) in-processing} by incorporating a fairness constraint or penalty in the algorithm's learning objective function \citep{10.1145/3038912.3052660, NEURIPS2018_83cdcec0, risser2022tackling}; and \textit{post-processing}, which modifies the predictions given by the algorithm \citep{JMLR:v22:20-1143}. 


\paragraph{Fair Representation Learning.}
% 
The field of Fair Representation Learning (FRL) focuses on learning data representations from which any information about the protected group  membership has been removed, while simultaneously retaining as much information related to other features as possible.
% 
Hence, any ML model trained on the new representation should not not be able to discriminate based on the demographic information, achieving fair outcomes.
% 

% 
The goal of fair representation is to learn a fair feature representation $r: \mathcal{X} \rightarrow \mathcal{X}^{\prime}$ such that the information shared between $r(\mathbf{X})$ and some sensitive attribute $S \in \mathcal{S}$ is minimal.
% 
This is founded on the data processing inequality, a concept from information theory which states that the models' prediction can not have any more information about $S$ than its input or hidden states \citep{10.5555/2230996.2231000}.
% 
Hence, the idea is to map the inputs $\mathbf{X}$ to $r(\mathbf{X})$ and use this feature space as input, thus ensuring the certain definition of fairness is achieved, inspired by an ethical notion that establishes the way to limit the influence of $S$ on the outcome of an AI system.
% 
As causal dependence is a special kind of statistical dependence \citep{Pearl_2009}, the real aim is to learn a map $r: \mathcal{X} \rightarrow \mathcal{X}^{\prime}$ such that $r(\mathbf{X})$ is (approximately) statistically independent with respect to the sensitive attribute $S$, guaranteeing fairness of any model trained on top of this new representation. 
% 

% 
FRL has been initially considered by \citet{pmlr-v28-zemel13} where they propose to learn a representation that is a probability distribution over clusters where learning the cluster of a sample does not give any information about the sensitive attribute $S$. 
% 
Since then, a variety of methods have been put forward in the recent literature.
% 
A popular approach to address this challenge is the variational auto-encoder (VAE) \citep{Gupta_Ferber_Dilkina_Ver_Steeg_2021, louizos2015variational} which aim to minimize the information encoded in them. 
% 
Other methods \citep{edwards2016censoring, pmlr-v80-madras18a, NIPS2017_8cb22bdd, liao2019gap} obtain learning representations formulating the problem as an adversarial game, learning an encoder and an adversary.
% 
In contrast to the adversarial training scheme, \citet{Olfat_Aswani_2019} introduced the concept of fair PCA, aiming to ensure that no linear classifier can predict demographic information from the projected data. 
% 
This approach has been further extended by \citet{pmlr-v206-kleindessner23a, Lee_Kim_Olfat_Hasegawa-Johnson_Yoo_2022}. 
% 
Additionally, another notion of fair PCA was proposed by \citet{NEURIPS2018_cc4af25f} which seeks to balance the excess reconstruction error across different demographic groups. 
% 
This is extended by approaches such as \citet{pelegrina2021novel, Kamani2022}.
% 

% 
In this work, we propose to introduce fairness constraints for PLS decomposition. 
% 
Actually, in a supervised setting, PLS enables to build features which are more accurate than PCA components since they are directly related to the target to be forecast. 
% 
Hence we propose to extend the PLS feature construction extraction with a fairness constraint, achieving a representation that is both fair but also enables to obtain accurate predictions. 
% 
This work extends the previous vanilla method described in \citet{champion2023human} and applied to a medical dataset that projects a posteriori the components onto the less biased components characterized by weaker correlations with the biased variable. 
% 

% 
In this paper we provide a feasible way to impose fairness constraint on PLS components. 
% 
Hence we provide representations that both enable to achieve a good forecast accuracy with few components while reducing unwanted biases.
% 
% [Revised]
Most FRL methods are unsupervised, and existing supervised techniques do not account for situations where the number of samples is smaller than the number of features.  
% 
Our proposal, Fair PLS, addresses this gap.



\section{Fair Partial Least Squares}
\label{sec:fairPLS}
% 

% 
Let, as before, $\mathbf{X} \in \mathcal{X}$, $\mathbf{S} \in \mathcal{S}$ and $\mathbf{Y} \in \mathcal{Y}$ be the non-sensitive input, the sensitive attribute and the ground truth target variable, respectively. 
% 
The samples are drawn from a distribution $\mathbb{P}$ over $\mathcal{X} \times \mathcal{S} \times \mathcal{Y}$, where $\mathcal{X} \subset \mathbb{R}^{d}$ is the set of possible (non-sensitive) inputs, $\mathcal{Y} \subset \mathbb{R}^{m}$ is the set of possible labels and $\mathcal{S} \subset \mathbb{R}^{n_{s}}$ is the set of possible sensitive variable values. 
% 
In the context of supervised learning, a decision rule, denoted by $f: \mathcal{X} \rightarrow \mathcal{Y}$, is built to perform a specific prediction task from a set of labeled samples $\mathcal{D} = \{\mathbf{x}_{i}, \mathbf{s}_{i}, \mathbf{y}_{i}\}_{i=1}^{n}$. 
% 
We represent the dataset of $n$ points $\mathbf{x}_{1}, \mathbf{x}_{2}, ..., \mathbf{x}_{n} \in \mathbb{R}^{d}$ as a matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$, where the $i$-th row is equal to $\mathbf{x}_{i}$. 
% 
Without loss of generality, we suppose that $\mathbf{X} \in \mathbb{R}^{n \times d}$ is the original centred $d$ variables of $n$ observations and $\mathbf{Y} \in \mathbb{R}^{n \times m}$ be the centred target.
% 

% 
In this section, we begin by reviewing the Partial Least Squares (PLS) technique and then we introduce the first theoretical formulation of Fair Partial Least Squares. 
% 
Our approach involves incorporating a regularization term into the PLS objective.
% 

\subsection{Partial Least Squares}
\label{sec:PLS}
% 
The Partial Least Squares (PLS - \textit{a.k.a.} projection on latent structures) approach is a supervised dimension reduction technique which generates orthogonal vectors, also referred to as latent vectors or components, by maximising the covariance between different sets of variables \citep{https://doi.org/10.1002/cem.1180020306, 10.1007/11752790_2, https://doi.org/10.1002/wics.51}.
% 
In detail, PLS aims to decompose the zero-mean matrix $\mathbf{X} = \mathbf{T} \mathbf{P}^{\mathsf{T}}$ as a product of $k \in [d]$ latent vectors (columns of $\mathbf{T} \in \mathbb{R}^{n \times k}$) and a matrix of weights ($\mathbf{P} \in \mathbb{R}^{d \times k}$), with the constraint that these components explain as much as possible of the covariance between $\mathbf{X}$ and $\mathbf{Y}$. 
% 
In other words, PLS approach attempts to find directions that help explain both the response $\mathbf{Y}$ and the predictors $\mathbf{X}$. 
% 
Indeed, any collection of orthogonal vectors that span the column space of $\mathbf{X}$ could be used as the latent vectors. 
% 
Consequently, to achieve decorrelated components with maximum correlation with $\mathbf{Y}$, additional conditions on the matrices $\mathbf{P}$ and $\mathbf{Q}$ will be required.
% 
Specifically, the PLS method finds two sets of weights vectors denoted as $(\mathbf{w}_{1}, ..., \mathbf{w}_{d})$ and $(\mathbf{c}_{1}, ..., \mathbf{c}_{d})$ such that the linear combination of the columns of $\mathbf{X}$ and $\mathbf{Y}$ have maximum covariance. 
%
The first pair of vectors $\mathbf{w}$ and $\mathbf{c}$ verify the following optimization problem:
\begin{equation}
\label{eq:1}
 Cov(\mathbf{t}, \mathbf{u}) = Cov(\mathbf{X}\mathbf{w}, \mathbf{Y}\mathbf{c}) = \max_{||\mathbf{p}|| = ||\mathbf{q}|| = 1} Cov(\mathbf{X}\mathbf{p}, \mathbf{Y}\mathbf{q}),
\end{equation}
% 
where $Cov(\mathbf{t}, \mathbf{u}) = \frac{\mathbf{t}^{\mathsf{T}} \mathbf{u}}{n}$ denotes the sample covariance between the score vectors.
% 
Once the first latent vector is found, the PLS method undergoes a series of iterations, obtaining the $k \in [d]$ weights vectors such that at each iteration $h$, the vector $\mathbf{w}_{h}$ is orthogonal to all preceding  weight vectors $(\mathbf{w}_{1}, ..., \mathbf{w}_{h-1})$, namely, $\forall l \in [h-1]$ : $\mathbf{t}_{h} \perp \mathbf{t}_{l}$. 
% 
Without loss of generality, we assume that the dependent variables are just one $\mathbf{Y} = \mathbf{y}$, then $\mathbf{c} = \mathbf{1}$ and $\mathbf{u} = \mathbf{y}$.
% 
What this means is that the columns of the weight matrix $\mathbf{W}$ are defined such that the squared sample covariance between the latent components and $\mathbf{Y}$ is maximal, given that these latent components are empirically uncorrelated with each other. 
% 
Moreover, the vectors $(\mathbf{w}_{1}, \cdots , \mathbf{w}_{k})$ are constrained to have a unit length. 
% 
To sum up, the weights vectors verify the following optimization problem:
\begin{equation}
\label{eq:2}
\forall h \in [k], \quad \mathbf{w}_{h} = \argmax_{\mathbf{w} \in \mathcal{W}_{h}} \quad Cov(\mathbf{X}\mathbf{w}, \mathbf{Y}),
\end{equation}
where $ \mathcal{W}_{h} = \{ \mathbf{w} \in \mathbb{R}^{d} \quad | \quad \mathbf{w}^{\mathsf{T}} \mathbf{w} = 1, \quad \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}\mathbf{X} \mathbf{w}_{l} = 0 \quad \forall l \in [h-1] \}$ and the latent vector are defined as $\mathbf{t}_{h} = \mathbf{X}\mathbf{w}_{h}$.
% 

The Nonlinear Iterative Partial Least Squares (NIPALS) was introduced by \citet{wold1975path} as an iterative algorithm for computing the matrices $\mathbf{W}$ and $\mathbf{T}$. 
% 
The pseudo code can be found in Appendix \ref{sec:app_fairPLS}. 
% 
When considering the relationship between vectors at step $h$ and their corresponding vectors at step $h-1$ for a specific dimension, the equations reveal that the NIPALS algorithm performs similarly to the power method used for determining the largest eigenvalue of a matrix.
% 
Hence, PLS is closely related to the eigen and singular value decomposition (refer to \citet{Abdi2006TheE} for an introduction to these notions). 
% 
At convergence of the algorithm, the vector $\mathbf{w}$ satisfies $\mathbf{X}^{\mathsf{T}} \mathbf{Y} \mathbf{Y}^{\mathsf{T}} \mathbf{X} \mathbf{w} = \lambda \mathbf{w} $, indicating that the weight vector $\mathbf{w}$ is the first eigenvector of the symmetric positive semi-definite matrix $\mathbf{X}^{\mathsf{T}} \mathbf{Y} \mathbf{Y}^{\mathsf{T}} \mathbf{X}$, with $\lambda$ the maximum eigenvalue.
% 

% 
As a consequence, the problem of finding the vectors $\mathbf{w}$ and $\mathbf{c}$ such that the components $\mathbf{t}$ and $ \mathbf{u}$ are the ones with maximal covariance among all components in $\mathbf{X}$ and $\mathbf{Y}$ space respectively, is equivalent to the problem of computing the singular vectors of the singular value decomposition (SVD) of the matrix $\mathbf{A} = \mathbf{X}^{\mathsf{T}} \mathbf{Y}$. 
% 
This is, the weight vector $\mathbf{w}_{1}$ is the first left singular vector of the matrix $\mathbf{A}$.
% 
The $\mathbf{A}$ can be decompose using a Singular Value Decomposition (SVD) as: $ \mathbf{A} = \mathbf{X}^{\mathsf{T}} \mathbf{Y} = \mathbf{F} \mathbf{\Sigma} \mathbf{G}^{\mathsf{T}}$, where $\mathbf{F} \in \mathbb{R}^{d \times d}$ contains the left singular vectors, $\mathbf{\Sigma} \in \mathbb{R}^{d \times m}$ is a diagonal matrix with the singular values as diagonal elements, and $\mathbf{G} \in \mathbb{R}^{m \times m}$ contains the right singular vectors. 
% 
Note that $\mathbf{F}$ and $\mathbf{G}$ are orthogonal matrices, which means that $\mathbf{F}^{\mathsf{T}}\mathbf{F} = \mathbf{I}_{d}$ and $\mathbf{G}^{\mathsf{T}}\mathbf{G} = \mathbf{I}_{m}$.
% 
The square of the largest singular value $\sigma_{1}$ is in fact the maximum of \eqref{eq:2} when $\mathbf{p} = \mathbf{f}_{1}$ and $\mathbf{q} = \mathbf{g}_{1}$. 
% 
The vector $\mathbf{F}^{\mathsf{T}} \mathbf{x}_{i} \in \mathbb{R}^{k}$ is the projection of $\mathbf{x}_{i}$ onto the subspace spanned by the columns of $\mathbf{F}$, viewed as a point in the lower-dimensional space $\mathbb{R}^{k}$.
% 
This solution gives the maximum value $\sum_{i = 1}^{d} \sigma_{i}$.
% 
\citet{Hskuldsson1988PLSRM} proved that PLS method is based on the fact that the largest singular value at step $h+1$ is larger that the second largest singular value at step $h$.

\subsection{Our formulation of Fair PLS}
\label{sec:FPLS}
% 
In Fair PLS, we aim to learn a projection of the data matrix $\mathbf{X}$ onto a $k$-dimensional subspace $r_{\eta}(\mathbf{X})$, dependent on the target $Y$ to be forecast, but such that the covariance dependence measure between the new data representation and the demographic attribute $S$ is minimal, according to parameter $\eta > 0$.
% 
Hence, the objective is to learn a map $r_{\eta}: \mathcal{X} \rightarrow \mathcal{X}^{\prime}$ such that $r_{\eta}(\mathbf{X})$, at the same time, enables to estimate accurately the parameter of interest $Y$, but is also statistically independent with respect to the sensitive attribute $S$, ensuring fairness of any model trained on this representation. 
% 
$\eta$ denotes here the parameter that balances the trade-off between the information contained by the representation related to forecast $Y$, and its unbiasedness with respect to the sensitive attribute $S$. 
% 

% 
We formulate the Fair PLS (FPLS) approach as the computation of a matrix of weights $\mathbf{W} \in \mathbb{R}^{d \times k}$ where the column $\mathbf{w}_{h} = [w_{1,h}, \cdots , w_{d,h}]^{\mathsf{T}}$ represents the solution, for $h \in [k]$, of the optimization problem \eqref{eq:2} restricting to vectors such that the projections $\mathbf{w}^{\mathsf{T}} \mathbf{x}_{i}$ and the sensitive $s_{i}$ are statistically independent $\forall i \in [n]$. We modify the initial definition of PLS by  computing the quadratic covariance.
% 
Actually, our method aims at constructing the linear combination of the most correlated components, regardless of the sign.  
% 
According to \cite{NEURIPS2023_d066d21c}, the fact that every linear classifier exhibits demographic parity with respect to $S$ when evaluated on $\mathbf{X}$ is equivalent to the condition that every component of $\mathbf{X}$ has zero covariance with every component of $S$.
%
In summary, Fair PLS is formulated as:
\begin{equation}
\begin{split}
    \label{eq:3}
    & \argmax_{\mathbf{w} \in \mathcal{W}^{\prime}_{h}} \quad Cov^{2}(\mathbf{X}\mathbf{w}, \mathbf{Y}), \quad \quad \text{where } \\ & \mathcal{W}^{\prime}_{h} = \{ \mathbf{w} \in \mathbb{R}^{d} \quad | \quad \mathbf{w}^{\mathsf{T}} \mathbf{w} = 1, \quad \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}\mathbf{X} \mathbf{w}_{l} = 0 \quad \forall l \in [h-1] \text{ and } \\
    & \forall i \in [n]  \quad  Cov^{2}(\mathbf{w}^{\mathsf{T}} \mathbf{x}_{i}, s_{i}) = 0 \}.
\end{split}
\end{equation}
% 
The independence criteria between the projections and the sensitive variable, $Cov^{2} (\mathbf{X}\mathbf{w}, S) = 0$, is added to the optimization problem of the standard PLS technique as a regularization term. 
% 
Hence, the following general objective function has to be optimise:
% 
\begin{equation}
\begin{split}
\label{eq:4}
    % & \forall h \in [k] \quad \mathbf{w}_{h} = \argmax_{\mathbf{w} \in \mathcal{W}_{h}} \quad \sum_{j=1}^{m} Cov^{2}(\mathbf{X}\mathbf{w}, Y_{j}) - \eta \text{ } Cov^{2} (\mathbf{X}\mathbf{w}, S) \equiv \\
    & \forall h \in [k] \quad \mathbf{w}_{h} = \argmax_{\mathbf{w} \in \mathcal{W}_{h}} \quad ( \mathbf{C}_{\mathbf{X}\mathbf{w}, \mathbf{Y}}^{2} - \eta \text{ } \mathbf{C}_{\mathbf{X}\mathbf{w}, S}^{2} ) \equiv \\
    & \forall h \in [k] \quad \mathbf{w}_{h} = \argmax_{\mathbf{w} \in \mathcal{W}_{h}} \quad \left( \frac{1}{n^{2}} \text{ } \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}} \mathbf{Y} \mathbf{Y}^{\mathsf{T}} \mathbf{X} \mathbf{w} - \eta \text{ } \frac{1}{n^{2}} \text{ } \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}} S S^{\mathsf{T}} \mathbf{X} \mathbf{w} \right),
\end{split}
\end{equation}
where $\eta > 0 $ is the regularization parameter, $\mathbf{C_{\mathbf{A}, \mathbf{B}}} = \frac{1}{n} \mathbf{A}^{\mathsf{T}} \mathbf{B}$ is the empirical cross covariance matrix between $\mathbf{A}$ and $\mathbf{B}$ and $ \mathcal{W}_{h} = \{ \mathbf{w} \in \mathbb{R}^{d} \quad | \quad \mathbf{w}^{\mathsf{T}} \mathbf{w} = 1, \quad \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}\mathbf{X} \mathbf{w}_{l} = 0 \quad \forall l \in [h-1] \}$.
% 
This problem is solved in an efficient manner with the Gradient Descent algorithm \citep{understandingML}. 
% 
Then, at each iteration, we take a step in the direction of the negative of the gradient at the current point. That is, the update step is: $ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \varepsilon \text{ } \frac{\partial g_{FPLS}(\mathbf{w}^{t})}{\partial \mathbf{w}}$, where the function to optimize is $g_{FPLS} (\mathbf{w}) = \frac{1}{n^{2}} \text{ } \mathbf{w}^{\mathsf{T}} (\mathbf{X}^{\mathsf{T}} \mathbf{Y} \mathbf{Y}^{\mathsf{T}} \mathbf{X} - \eta \mathbf{X}^{\mathsf{T}} \mathbf{S} \mathbf{S}^{\mathsf{T}} \mathbf{X})  \mathbf{w}$, the respective gradient is $\frac{\partial g_{FPLS}}{\partial \mathbf{w}} = \frac{2}{n^{2}} \text{ } (\mathbf{X}^{\mathsf{T}} \mathbf{Y} \mathbf{Y}^{\mathsf{T}} \mathbf{X} - \eta \mathbf{X}^{\mathsf{T}} \mathbf{S} \mathbf{S}^{\mathsf{T}} \mathbf{X})  \mathbf{w}$, and $\varepsilon > 0$ is the learning rate.
% 

\begin{algorithm}[!htbp]
\caption{Fair PLS algorithm}\label{alg:FairPLS}
\KwInput{$d$ independent variables stored in a centred matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ and $m$ dependent variables stored in a centred matrix $\mathbf{Y} \in \mathbb{R}^{n \times m}$; sensitive centred variable $S$;  $\eta$ parameter; $k$ number of components.}
\KwOutput{$\mathbf{W}$, $\mathbf{T}$.}
% 
Set $\mathbf{X}_{1} = \mathbf{X}$ and $\mathbf{Y}_{1} = \mathbf{Y}$\;
% 
\For{$h \in [k]$}{
% 
Compute the weights $\boldsymbol{w}_{h} \in \mathbb{R}^{d}$ as the maximum of the function $f_{FPLS} (\mathbf{w}) = \frac{1}{n^{2}} \text{ } \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}_{h} \mathbf{Y}_{h} \mathbf{Y}^{\mathsf{T}}_{h} \mathbf{X}_{h} \mathbf{w} - \eta \frac{1}{n^{2}} \text{ } \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}_{h} \mathbf{S} \mathbf{S}^{\mathsf{T}} \mathbf{X}_{h} \mathbf{w}$\;
% 
Scale them to be of length one\;
% 
Project $\mathbf{X}_{h}$ on the singular vectors in order to obtain the scores $\mathbf{t}_{h} = \mathbf{X}_{h} \mathbf{w}_{h}$\;
% 
Compute the loadings $\boldsymbol{\gamma}_{h} \in \mathbb{R}^{d}$ such that the matrix of $1$-rank $\boldsymbol{\kappa}_{h}\boldsymbol{\gamma}_{h}^{\mathsf{T}}$ is as close as possible to $\mathbf{X}_{h}$\;
% 
Compute residual matrices: $\mathbf{X}_{h+1} = \mathbf{X}_{h} - \boldsymbol{\kappa}_{h}\boldsymbol{\gamma}_{h}^{\mathsf{T}}$\;
% 
}
Store the vectors $\mathbf{w}, \mathbf{t}$ in the corresponding matrices\;
\end{algorithm}
% 
Therefore, Fair PLS finds a best approximating projection such that the projected data is statistically independent from the sensitive attribute.
% 
The parameter $\eta$ can be interpreted as the trade-off between fairness and utility.
% 
The algorithm that implements this approach is detailed below, and consists of approximating $\mathbf{X}$ as a sum of $1$-rank matrices $\mathbf{X} = \mathbf{T}\mathbf{W}^{\mathsf{T}}$, where $\mathbf{T} \in \mathbb{R}^{n \times k}$ contains the scores in its columns and $\mathbf{W}$ contains the weights in its columns.
% 

\paragraph{Why cannot Fair PLS be formulated in closed form? }
% 
It is important to note that adapting the Partial Least Squares methodology to achieve fairness as a trade-off is challenging due to the inherent complexity of the PLS method. Contrary to PCA analysis for which a closed form may be found, computing the PLS components is not a direct method. We refer for instance to \citet{blazere2015partial} or \citet{lofstedt2024using} and references therein. When modifying the loss with the fairness penalty makes the computation even less tractable.
% 
Specifically, if we denote as $\sigma_{min,Y}$ the minimum eigenvalue of the matrix $\mathbf{Y} \mathbf{Y}^{\mathsf{T}}$ and $\sigma_{max,S}$ the maximum eigenvalue of the matrix $S S^{\mathsf{T}}$.
% 
The Fair PLS weights $\{\mathbf{w}_{h}\}_{h = 1}^{k}$ are the eigenvectors of the certain matrix if $\eta \leq \sigma_{min,Y} / \sigma_{max,S}$.
% 
Moreover the matrix whose eigenvectors are the Fair PLS weights is $\mathbf{X}^{\mathsf{T}} \mathbf{M} \mathbf{M}^{\mathsf{T}} \mathbf{X}$, with $\mathbf{B} = \mathbf{Y} \mathbf{Y}^{\mathsf{T}} - \eta S S^{\mathsf{T}} = \mathbf{Q}^{\mathsf{T}}\mathbf{D}\mathbf{Q} $ and $\mathbf{M} = \mathbf{Q}^{\mathsf{T}} \mathbf{D}^{1/2}$ . 
% 

% 
\paragraph{Motivation for Fair PLS}
% 
The motivation behind the idea of Fair Representation Learning by a PLS-based approach is interpretability. 
% 
Yet our aim is to provide a method that enables us to recover a linear transformation of the data to promote explainability of the components. 
% 
Hence, the PLS method was a suitable way to achieve interpretability of the new components yet enabling forecasting. 
% 
Fair PLS allows us to learn a new representation that not only is lower in dimension but also a trade-off between fairness and utility performance. 
% 
For instance, for the COMPAS dataset, if we obtain the most relevant features of the learned components, we discover that if $\eta = 0.0$ (i.e. standard PLS), these are: event, decile score, juv misd count, race and decile score; while for $\eta = 1.0$ they are: event, age, juv other count, juv misd count and priors count. 
% 
Hence, the sensitive variable does not impact the Fair PLS components.




\section{Extensions}
\label{sec:extensions}

\subsection{Kernelizing Fair PLS}
\label{sec:KFPLS}
% 
Let us now extend our Fair PLS approach (Section \ref{sec:FPLS}) to the non-linear version of PLS by means of reproducing kernels \citep{10.5555/944790.944806}.
% 
In this section, we formulate Kernel Fair Partial Least Squares by adding the Hilbert Schmidt independence criterion \citep{pmlr-v108-tan20a, NIPS2007_3a077244} as the fairness regularization term in the standard PLS formulation in \eqref{eq:2}.
% 
The proposed Kernel Fair PLS is based on a fair adaptation of the NIPALS procedure to iteratively estimate the desired components which are not linearly related to the input variables.
%
Furthermore, this will allow to use multiple sensitive attributes simultaneously.
% 
To this end, Kernel Fair PLS is a generalization of Fair PLS to feature spaces of arbitrary large dimensionality. 
% 
We additionally provide the pseudo code of kernelized Fair PLS in Appendix \ref{sec:app_fairPLS}.

% 
To do this, we assume a nonlinear transformation of the input variables $\mathbf{X} \in \mathbb{R}^{n \times d_X}$ and $\mathbf{S} \in \mathbb{R}^{n \times d_S}$ into separable feature reproducing kernel Hilbert spaces (RKHSs) $(\mathcal{H}_{K_X}, \langle \cdot, \cdot \rangle_{K_X} )$ and $(\mathcal{H}_{K_S}, \langle \cdot , \cdot \rangle_{K_S} )$, respectively. Recall that in our proposal $d_S\geq 1$ admits more than one sensitive variable. The corresponding mapping functions are defined as $\mathbf{\phi}: \mathbf{x}_{i} \in \mathbb{R}^{d_X} \mapsto \mathbf{\phi}(\mathbf{x}_{i}) \in \mathcal{H}_{K_X}$ and $\mathbf{\psi}: \mathbf{s}_{i} \in \mathbb{R}^{d_S} \mapsto \mathbf{\psi}(\mathbf{s}_{i}) \in \mathcal{H}_{K_S}$, respectively. 
% 
This yields to the matrices $\mathbf{\Phi}$ and $\mathbf{\Psi}$ where the row $i, 1\leq i \leq n$  denotes the vectors $\mathbf{\phi}(\mathbf{x}_{i})$ and $\mathbf{\psi}(\mathbf{z}_{i})$ respectively.
% 
Hence, the corresponding reproducing kernel functions can be written in the form of $K_{\mathbf{X}} (\mathbf{x}_{i} , \mathbf{x}_{j}) = \langle  \mathbf{\phi}(\mathbf{x}_{i}) , \mathbf{\phi}(\mathbf{x}_{j})\rangle_{K_X} = \mathbf{\phi}(\mathbf{x}_{i})^{\mathsf{T}}\mathbf{\phi}(\mathbf{x}_{j})$ and $K_{\mathbf{S}} (\mathbf{s}_{i}, \mathbf{s}_{j}) = \langle  \mathbf{\psi}(\mathbf{s}_{i}) , \mathbf{\psi}(\mathbf{s}_{j}) \rangle_{K_S} = \mathbf{\psi}(\mathbf{s}_{i})^{\mathsf{T}}\mathbf{\psi}(\mathbf{s}_{j})$, which correspond to the Euclidean dot product in their respective Hilbert spaces.
% 
Applying the so-called ``kernel trick" (i.e. $\mathbf{\Phi} \mathbf{\Phi}^{\mathsf{T}} \in \mathbb{R}^{n \times n}$ represents the kernel Gram matrix $\mathbf{K}_{\mathbf{X}}$ of the cross dot products between all mapped input data points), we rewrite \eqref{eq:4} in terms of the kernel matrix $\mathbf{K}_{\mathbf{X}}$ and $\mathbf{K}_{\mathbf{S}}$.
% 
Recall that in Fair PLS approach (Section \ref{sec:FPLS}), we measured the independence with respect to the sensitive attribute through the cross covariance operator between the components and the protected feature.
% 
When kernelising this method, to measure independence we will use the Hilbert Schmidt Independence Criterion (HSIC) introduced in \cite{NIPS2007_d5cfead9}.
% 
To this end, let us provide the functional analytic background necessary to describe cross-covariance operators between RKHSs and introduce the HSIC.
% 

\begin{definition}
    \emph{Cross covariance operator and Hilbert-Schmidt Independence Criterion}
    
    We assume that $(X, \Gamma)$ and $(S, \Lambda)$ are settled up with probability measures $p_{x}$ and $p_{s}$ respectively ($\Gamma$ being the Borel sets on $X$, and $\Lambda$ the Borel sets on $S$).
    % 
    Following \cite{10.7551/mitpress/4057.003.0014} and \cite{10.1007/11564089_7} the cross-covariance operator associated with the joint measure $P_{xy}$ on $(\mathcal{X} \times \mathcal{Y}, \Gamma \times \Lambda)$ is a linear operator $\mathcal{C}_{X S} : \mathcal{H}_{K_X} \rightarrow \mathcal{H}_{K_S}$ defined as:
    % 
    \begin{equation}
        \label{eq:5}
        \mathcal{C}_{X S} := \mathbb{E}_{X S} [(\phi(X) - \mu_{X}) \otimes (\psi(S) - \mu_{S})] = \mathbb{E}_{X S} [\phi(X) \otimes \psi(S)] - \mu_{X} \otimes \mu_{S}.
    \end{equation}
    % 
    Given a sample $\{ (\boldsymbol{x}_{1}, \boldsymbol{s}_{1}), \cdots, (\boldsymbol{x}_{n}, \boldsymbol{s}_{n}) \}$ the empirical cross-covariance operator $\mathbf{C}_{X, S} : \mathcal{H}_{K} \rightarrow \mathcal{H}_{K_S}$ is defined as:
    \begin{equation}
        \label{eq:6}
        \mathbf{C}_{X, S} := \frac{1}{n} \sum_{i = 1}^{n} [\phi(\boldsymbol{x}_{i}) \otimes \psi(\boldsymbol{s}_{i})] - \Hat{\mu}_{\boldsymbol{x}} \otimes \Hat{\mu}_{\boldsymbol{s}},
    \end{equation}
    where $\Hat{\mu}_{\boldsymbol{x}} = \frac{1}{n} \sum_{i = 1}^{n} \phi(\boldsymbol{x}_{i})$ and $\Hat{\mu}_{\boldsymbol{s}} = \frac{1}{n} \sum_{i = 1}^{n} \psi(\boldsymbol{s}_{i})$.
    % 
    
    The Hilbert-Schmidt Independence Criterion (HSIC) is defined as the squared HS-norm of the cross-covariance operator $\mathcal{C}_{X S}$. Then $ HSIC(P_{X S}, \mathcal{H}_{K}, \mathcal{H}_{K_S}) := \Vert \mathcal{C}_{X S} \Vert^{2}_{HS} $.
\end{definition}
% 

To sum up, we formulate Kernel Fair PLS as an optimization problem rewritten in terms of the Kernel matrices, where fairness is incorporated as a regularization term detecting statistical independence through the $HSIC(P_{\mathbf{\phi}(\mathbf{X})\mathbf{w}, \mathbf{\psi}(\mathbf{S})}, \mathcal{H}_{K}, \mathcal{H}_{K_S})$ operator. 
% 
By the Representer's Theorem, the weight can be expressed as $\mathbf{w} = \mathbf{\Phi}^{\mathsf{T}} \mathbf{\alpha}$ \citep{10.5555/944790.944806}.
% 
Hence, the Kernel Fair PLS (KFPLS) is:
% 
\begin{equation}
\begin{split}
\label{eq:7} 
    & \forall h \in [k] \quad \mathbf{w}_{h} = \argmax_{\mathbf{w} \in \mathcal{W}_{h}^{Kernel}} \quad  \mathbf{C}_{\mathbf{\phi}(\mathbf{X})\mathbf{w}, \mathbf{Y}}^{2} - \eta \text{ } \Vert \mathbf{C}_{\mathbf{\phi}(\mathbf{X})\mathbf{w}, \mathbf{\psi}(\mathbf{S})} \Vert^{2}_{HS}
    \equiv \\    
    % 
    & \forall h \in [k] \quad \mathbf{\alpha}_{h} = \argmax_{\mathbf{\alpha} \in \mathcal{\aleph}_{h}} \quad \left( \frac{1}{n^{2}} \text{ } \text{Tr}(\mathbf{\alpha}^{\mathsf{T}} \widetilde{\mathbf{K}}_{\mathbf{X}} \mathbf{Y} \mathbf{Y}^{\mathsf{T}} \widetilde{\mathbf{K}}_{\mathbf{X}} \mathbf{\alpha}) - \eta \text{ } \frac{1}{n^{2}} \text{ } \text{Tr}(\mathbf{\alpha}^{\mathsf{T}} \widetilde{\mathbf{K}}_{\mathbf{X}}  \widetilde{\mathbf{K}}_{\mathbf{S}} \widetilde{\mathbf{K}}_{\mathbf{X}} \mathbf{\alpha})\right),
\end{split}
\end{equation}
% 
where $\eta > 0 $ is the regularization parameter and $ \mathcal{W}_{h}^{Kernel} = \{ \mathbf{w} \in \mathbb{R}^{d} \quad | \quad \mathbf{w}^{\mathsf{T}} \mathbf{w} = 1, \quad \mathbf{w}^{\mathsf{T}} \mathbf{\Phi}^{\mathsf{T}} \mathbf{\Phi} \mathbf{w}_{l} = 0 \quad \forall l \in [h-1]  \}$, $ \mathcal{\aleph}_{h} = \{ \mathbf{a} \in \mathbb{R}^{n} \quad | \quad \mathbf{a}^{\mathsf{T}} \mathbf{K}_{\mathbf{X}} \mathbf{a} = 1, \quad \mathbf{a}^{\mathsf{T}} \mathbf{K}_{\mathbf{X}} \mathbf{K}_{\mathbf{X}} \mathbf{\alpha}_{l} = 0 \quad \forall l \in [h-1]  \}$.
% 
The Gram matrices for the variables centred in their respective feature spaces are shown by \cite{Scholkopf1998} to be: $\widetilde{\mathbf{K}}_{\mathbf{X}} = \mathbf{H} \mathbf{K}_{\mathbf{X}} \mathbf{H}$ and $\widetilde{\mathbf{K}}_{\mathbf{S}} = \mathbf{H} \mathbf{K}_{\mathbf{S}} \mathbf{H}$, where $\mathbf{H} = \mathbf{I}_{n} - \frac{1}{n} \mathbf{1}_{n} \mathbf{1}_{n}^{\mathsf{T}}$, and $\mathbf{1}_{n}$ is an $n \times 1$ vector of ones.
% 
Then, the matrices $\widetilde{\mathbf{\Phi}}$ and $\widetilde{\mathbf{\Psi}}$ contain the centered data in Hilbert space.
% 
In the case where the kernel is $\mathbf{K}(\boldsymbol{a}, \boldsymbol{b}) = \langle \boldsymbol{a} , \boldsymbol{b} \rangle$ we recover the Fair PLS approach.
% 


\subsection{Imposing Equality of Odds constraint}
\label{sec:FPLS_odds}
% 
The aim of Fair PLS, as formulated in Section \ref{sec:fairPLS}, is to represent the data such that it is independent of the demographic attribute. 
% 
This approach guarantees that any classifier trained with the new data achieves demographic parity fairness.
% 
Yet the methodology we develop can be extended to other notions of global fairness, for instance to Equality of Odds (EO) or its relaxed version Equality of Opportunity. 
% 
As mentioned before, EO can be mathematically expressed as the conditional independence $\hat{Y} \perp S \mid Y$, in the sense that the forecast error should not depend on the sensitive attribute.
% 
If we aim to attain EO, we could apply the methodology of Fair PLS to the input data where we replace $\hat{Y}$ by $\mathbf{X}\mathbf{w}$. 
% 
In this case, the EO condition will hold for any function which is built with the PLS directions $\mathbf{X}\mathbf{w}$'s.
% 
In this case we can define the EO Fair PLS estimator as:
\begin{equation}
\label{eq:8}
    \forall h \in [k] \quad \mathbf{w}_{h} = \argmax_{\mathbf{w} \in \mathcal{W}_{h}} \quad ( \mathbf{C}_{\mathbf{X}\mathbf{w}, \mathbf{Y}}^{2} - \eta \text{ } \mathbf{C}_{\mathbf{X}\mathbf{w}, S | Y}^{2}), 
\end{equation}
where $\mathbf{C}_{\mathbf{X}\mathbf{w}, S |Y}$ is the conditional cross covariance which is defined as $$ \mathbf{C}_{\mathbf{X}\mathbf{w}, S \mid Y}=\mathbf{C}_{\mathbf{X}\mathbf{w} S}-\mathbf{C}_{\mathbf{X}\mathbf{w} Y} \mathbf{C}_{Y Y}^{-1} \mathbf{C}_{Y S}.$$ 
% 
This idea has been already used in \citet{perez-suay2023fair} to impose fairness as equality of odds for regression. Our method for PLS representation can thus be extended to this setting. 
% 
The RKHS framework still holds by replacing the constraint on the covariance by the HSIC criterion for the conditional cross covariance operator, following the guidelines in \citet{perez-suay2023fair}.



\subsection{Application to Large Language Models}
% 
In the context of supervised learning, a decision rule to perform a specific classification task is obtained from a set of labeled samples $\mathcal{X}$. 
% 
However, in the setting of Large Language Models (LLM), such a decision rule is considered to be $f: \mathcal{Z} \rightarrow \mathcal{Y}$, where $\mathbf{z} \in \mathcal{Z}$ represents an input text and $y \in \mathcal{Y}$ denotes their corresponding label. 
% 
Therefore, the decision rule $f$ can be viewed as a composition of two functions $f = c \circ h$. The first one $h: \mathcal{Z} \rightarrow \mathcal{A}$ encompasses all the layers to transform the input data $\mathbf{z} \in \mathcal{Z}$ to a vector $\mathbf{a}$ belonging to the latent space $\mathcal{A}$.
% 
The second function $c: \mathcal{A} \rightarrow \mathcal{Y}$ involves all the layers to classify the transformed data $h(\mathbf{z}) \in \mathbb{R}^{d}$.
% 
We represent the dataset of $n$ points $\mathbf{a}_{1}, \mathbf{a}_{2}, ..., \mathbf{a}_{n} \in \mathbb{R}^{d}$ as a matrix $\mathbf{A} = h(\mathbf{Z}) \in \mathbb{R}^{n \times d}$, where the $i$-th row is equals $\mathbf{a}_{i}$. 
% 
This matrix is known as CLS-embedding matrix in the encoder transformer model. 
% 
SVD decomposition is a successful way to understand how the embedding matrix can be factorized into concepts that enable to understand the behavior of the language model. This framework has been recently presented in \cite{jourdan2023cockatiel} and bias analysis in this context is discussed in \cite{jourdan2023taco} for instance.
% 
Hence, for $\operatorname{SVD}$, the matrix $\boldsymbol{A}$ is decomposed into $\boldsymbol{A}= U_0 \Sigma_0 V_0^{\boldsymbol{\top}}$, where $U_0 \in \mathbb{R}^{n \times n}$ and $V_0 \in \mathbb{R}^{d \times d}$ are orthonormal matrices, and $\Sigma_0 \in \mathbb{R}^{n \times d}$ is diagonal. 
% 
This decomposition reveals the main variability in $\boldsymbol{A}$. 
% 
By retaining the $r \ll d$ largest singular values in $\Sigma$, we approximate $\boldsymbol{A}$ as: $\boldsymbol{A} \approx \boldsymbol{U} \boldsymbol{W}$, where $\boldsymbol{U} \in \mathbb{R}^{n \times r}$ contains the leading $r$ columns of $U_0$, and $\boldsymbol{W}=\Sigma V^{\top} \in \mathbb{R}^{r \times d}$ combines the singular values and right singular vectors (see \citet{eckart1936approximation}). 
% 
SVD is used to capture the most significant patterns in the data by reducing dimensionality while preserving maximum variance.
% 
For our purposes, $\boldsymbol{U}$ is the concept matrix that is needed to be fair. The PLS-based approach is relevant to not lose explainability of the new components due to the linear transformation.
% 

% 
An application of the Fair PLS methodology proposed is to reduce the influence of demographic factors that contribute to the model predictions using an algebraic decomposition of the latent representations of the model into orthogonal dimensions.
% 
In other words, we aim to intervene in the latent representations to generate a new fair representation with respect to dimensions that convey bias.
% 
This approach is justified due to the orthogonality of the dimensions of the new representation obtained with Fair PLS. 
% 











\section{Experimental Results}
\label{sec:experiments}
% 

In this section, we present a number of experiments \footnote{Code available on \href{https://github.com/emartindedi/fair_pls.git}{GitHub repository}} conducted on six public datasets to demonstrate the effectiveness of our approach in achieving both fair representation (experiment $\mathcal{A}$) and fair predictions for classification and regression tasks (experiment $\mathcal{B}$). 
% 
Therefore, in a first phase we checked with these real datasets that the proposed method achieves a good and fair representation of the data. 
% 
Then, in a second phase, we used such representation for prediction purposes and look at its efficiency in achieving a fairness-accuracy trade-off. 
% 

% 
Fairness as DP is usually measured through the so-called Disparate Impact (DI) index, namely $DI(\hat{Y}, S) = P(\hat{Y} = 1 | S = 0) / P(\hat{Y} = 1 | S = 1)$, which can be empirically estimated as: $\frac{n_{1,0}}{n_{0,0} + n_{1,0}} / \frac{n_{1,1}}{n_{0,1} + n_{1,1}},$ where $n_{i,j}$ is the number of observations such that $Y = i, S = j$. Additionally, Confidence intervals (with $95\%$ confidence) were computed using the method described in \cite{besse2018confidence}.
%
All tables and figures presented in this section, as well as in the supplementary appendix to this section, contain average results (together with standard deviations) over 3 random splits into train and test data for ($\mathcal{A}$) and 7 random splits for ($\mathcal{B}$), respectively.
%
Further information regarding the datasets and implementation details analyzing runtime performance and comparisons with state-of-the-art methods, can be found in Appendix B.

\paragraph{(Experiment $\mathcal{A}$) Fair Representation. }
% 
The primary goal of our approach is to learn from the original data $\mathbf{X} \in \mathbb{R}^{n \times d} $ a new representation $r_{\eta}(\mathbf{X}) \in \mathbb{R}^{n \times k}$, in a way that, at the same time, it
\begin{itemize}
    \item[]\textbf{(Result $\mathcal{A}-1$)} is useful for target prediction;
    \item[]\textbf{(Result $\mathcal{A}-2$)} is approximately independent of the sensitive variable; and
    \item[]\textbf{(Result $\mathcal{A}-3$)} preserves information about the input space.
\end{itemize}
% 
In order to check that we effectively achieve all three results, the analyses carried out consist, based on our Fair PLS formulation, of evaluating the behaviour of the covariance between the projections and the target, and between the projections and the sensitive attribute, as the parameter $\eta$ increases for six different datasets.
% 
% 
\begin{table}[!htbp] 
    \caption{The table summarizes the three results: \textbf{($\mathcal{A}-1$)} how well the fair representation explains the target variable ($Cov^{2}(r_{\eta}(\mathbf{X}),Y)$); \textbf{($\mathcal{A}-2$)} how strongly the fair representation is associated with a sensitive variable ($Cov^{2}(r_{\eta}(\mathbf{X}),S)$); and \textbf{($\mathcal{A}-3$)} how the new representation is close to the original one ($Error(X, r_{\eta}(\mathbf{X}))$) for different datasets and values of the parameter $\eta$.} 
    \label{table:frl} 
    \begin{center} 
    \begin{tabular}{lllll} \bf Dataset & $\eta$ & $Cov^{2}(r_{\eta}(\mathbf{X}),Y)$ & $Cov^{2}(r_{\eta}(\mathbf{X}),S)$ & $ Error(X, r_{\eta}(\mathbf{X})) $ \\ \hline \\ \multirow{4}{8em}{Adult Income}  & $0.0$ &   $ 0.3227 \pm 0.1593 $ & $ 0.1351 \pm 0.0654 $ & $ 0.7891 \pm 0.0045$ \\  & $1.0$ &   $ 0.29 \pm 0.1488 $ & $ 0.0474 \pm 0.0238 $ & $ 0.794 \pm 0.0034$ \\  & $2.0$ &   $ 0.2625 \pm 0.1302 $ & $ 0.021 \pm 0.0107 $ & $ 0.8012 \pm 0.0052$ \\  & $10.0$ &   $ 0.2031 \pm 0.1013 $ & $ 0.0016 \pm 0.0005 $ & $ 0.8027 \pm 0.0068$ \\  \cdashline{1-5}  \multirow{4}{8em}{German Credit}  & $0.0$ &   $ 0.0954 \pm 0.0464 $ & $ 0.0455 \pm 0.0234 $ & $ 0.5949 \pm 0.0054$ \\  & $1.0$ &   $ 0.0956 \pm 0.0426 $ & $ 0.0055 \pm 0.0032 $ & $ 0.6094 \pm 0.014$ \\  & $2.0$ &   $ 0.0811 \pm 0.0497 $ & $ 0.0078 \pm 0.0046 $ & $ 0.6072 \pm 0.0193$ \\  & $10.0$ &   $ 0.0874 \pm 0.0436 $ & $ 0.0062 \pm 0.0021 $ & $ 0.6056 \pm 0.0144$ \\  \cdashline{1-5} \multirow{4}{8em}{Law School}  & $0.0$ &   $ 0.0385 \pm 0.0189 $ & $ 0.032 \pm 0.0157 $ & $ 0.2398 \pm 0.0051$ \\  & $1.0$ &   $ 0.0129 \pm 0.007 $ & $ 0.0054 \pm 0.0035 $ & $ 0.3915 \pm 0.0125$ \\  & $2.0$ &   $ 0.0037 \pm 0.0021 $ & $ 0.0004 \pm 0.0004 $ & $ 0.4603 \pm 0.004$ \\  & $10.0$ &   $ 0.0018 \pm 0.001 $ & $ 0.0 \pm 0.0 $ & $ 0.4735 \pm 0.0031$ \\  \cdashline{1-5} \multirow{4}{8em}{Diabetes}  & $0.0$ &   $ 0.0338 \pm 0.0176 $ & $ 0.0073 \pm 0.0038 $ & $ 0.7436 \pm 0.0052$ \\  & $1.0$ &   $ 0.0326 \pm 0.0174 $ & $ 0.0014 \pm 0.001 $ & $ 0.7399 \pm 0.0025$ \\  & $2.0$ &   $ 0.031 \pm 0.0163 $ & $ 0.0006 \pm 0.0005 $ & $ 0.7428 \pm 0.002$ \\  & $10.0$ &   $ 0.0289 \pm 0.0147 $ & $ 0.0002 \pm 0.0001 $ & $ 0.7439 \pm 0.0103$ \\  \cdashline{1-5} \multirow{4}{8em}{COMPAS}  & $0.0$ &   $ 0.472 \pm 0.2397 $ & $ 0.0647 \pm 0.0314 $ & $ 0.0953 \pm 0.028$ \\  & $1.0$ &   $ 0.4518 \pm 0.227 $ & $ 0.0424 \pm 0.0202 $ & $ 0.1521 \pm 0.0109$ \\  & $2.0$ &   $ 0.4225 \pm 0.2125 $ & $ 0.028 \pm 0.0145 $ & $ 0.1681 \pm 0.0061$ \\  & $10.0$ &   $ 0.3081 \pm 0.1509 $ & $ 0.0061 \pm 0.0029 $ & $ 0.242 \pm 0.0173$ \\  \cdashline{1-5} \multirow{4}{8em}{Communities and Crimes}  & $0.0$ &   $ 0.5563 \pm 0.2898 $ & $ 0.5965 \pm 0.2882 $ & $ 0.1954 \pm 0.011$ \\  & $1.0$ &   $ 0.4948 \pm 0.2653 $ & $ 0.3098 \pm 0.1696 $ & $ 0.209 \pm 0.013$ \\  & $2.0$ &   $ 0.3715 \pm 0.1874 $ & $ 0.153 \pm 0.0922 $ & $ 0.2177 \pm 0.0052$ \\  & $10.0$ &   $ 0.1358 \pm 0.0704 $ & $ 0.0098 \pm 0.007 $ & $ 0.2576 \pm 0.0077$ \\ 
    \end{tabular} 
    \end{center} 
\end{table}
% 
% 
Evidence is shown in Table \ref{table:frl} and Figure \ref{im:fair_representation_A1}, where for all datasets, we make the parameter $\eta$ vary in $[0,10]$ (see first column).
% 
The second column of Table \ref{table:frl} compares in terms of $Cov^{2}(r_{\eta}(\mathbf{X}),Y)$ how important the choice of $\eta$ is for building a representation that is balanced \textbf{($\mathcal{A}-1$)}. Moreover, the third column shows the dependence between the new fair representation and the sensitive variable through $Cov^{2}(r_{\eta}(\mathbf{X}),S)$ \textbf{($\mathcal{A}-2$)}. Both results can also be seen in the blue and orange lines, respectively, in Figure \ref{im:fair_representation_A1}. 
% 
First, notice that in order to achieve a good representation in terms of balance between predictive performance (PLS objective function) and fairness (constraint added to PLS), the parameter $\eta$ should not be much higher than the value $1$, which is in fact the fixed parameter for the $Cov^{2}(r_{\eta}(\mathbf{X}),Y)$ term in \eqref{eq:4}. 
% 
Furthermore, if $\eta >> 2$, the new representation lacks of achieving the supervised learning purpose, since the values of $Cov^{2}(r_{\eta}(\mathbf{X}),Y)$ decrease considerably.
% 
The second experiment related to the representation itself consists of studying the amount of information preserved from the original data, which can be quantified through the reconstruction error $ Error(X, r_{\eta}(\mathbf{X})) =Tr((\mathbf{X}-r_{\eta}(\mathbf{X}))^{\mathsf{T}} (\mathbf{X}-r_{\eta}(\mathbf{X})))$ . 
% 
The results are displayed in the last column of Table \ref{table:frl} together with Figure \ref{im:fair_representation_A2}.
% 
The reconstruction error could be interpreted as the variability of the data which we are not able to capture in the lower dimensional space.
% 
As the ignored subspace is the orthogonal complement of the principal subspace, then the reconstruction error can be seen as the average squared distance between the original data points and their respective projections onto the principal subspace.
% 
For our purposes, an optimal representation is one for which the reconstruction error is small, as is the case with the COMPAS and Communities and Crimes Datasets.
%

% 
\paragraph{(Experiment $\mathcal{B}$) Fair Predictions.}
%
The final aim of the Fair PLS formulation is to achieve an optimal fair representation so that any ML model trained on $r_{\eta}(X)$ is fair and has a good predictive performance.
% 
For this evaluation, we consider two different settings, classification and regression. 
% 
Therefore, we used binary target values from five real datasets for the first task (Adult Income, German Credit, Law School, Diabetes and COMPAS datasets) and a positive variable for the second one (Communities and Crimes dataset).
%
The classification results for the Adult and Diabetes datasets are shown and discussed below, while for the rest of datasets, as well as the regression problem, results can be found in Table \ref{table:pf_classification} and Table \ref{table:pf_regression} in Appendix \ref{sec:app_experiments}.
% 

% 
In order to study the trade-off between fairness (DI) and accuracy in Figure \ref{im:fair_predictions_B1}, several ML models were used. Precisely, logistic regression (LR, in the first column), decision trees (DT, in the second column), and extreme gradient boosting (XGB, in the third column) were trained considering two protected attributes in both cases. 
% 
Specifically, we applied our method as a pre-processing bias mitigation technique and plot the average values of DI and accuracy obtained from a 7-fold cross-validation, for different values of $\eta \in [0,2]$. 
% 
Recall from the previous experiment that these are desirable values for this parameter. 
% 
In particular, it can be seen that the best trade-off is achieved for $\eta=1$ in all cases.


\begin{figure}[!htbp]
\caption{($\mathcal{B}$) Prediction accuracy vs. disparate impact (DI) using various ML models with the new fair representation as input data.
% 
Each point represents the average value from a 7-fold cross-validation and the different colors are for the wide range of $\eta$ used to compute the components. 
}    
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/adult_LR_Pareto.pdf}
    \label{im:b11}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/adult_DT_Pareto.pdf}
    \label{im:b12}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/adult_XGB_Pareto.pdf}
    \label{im:b13}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/diabetes_LR_Pareto.pdf}
        \label{im:b14}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/diabetes_DT_Pareto.pdf}
        \label{im:b15}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/diabetes_XGB_Pareto.pdf}
        \label{im:b16}
    \end{subfigure}
\label{im:fair_predictions_B1}
\end{figure}


\section{Conclusions}
\label{sec:conclusions}
% 
We define a Fair Partial Least Squares approach that allows to balance between utility (predictive performance) and fairness (independence of the demographic information) and can be kernelized.
% 
Our formulation have the same complexity (algorithmically) as standard Partial Least Squares, or Kernel Partial Least Squares, and have applications on different domains and with different data structures as tabular, image or text embeddings. 
% 
Furthermore, it can be adapted to the equality of odds paradigm through the use of the conditional cross covariance operator. 
% 
This poses a robust methodology able to solve different fair scenarios. 
% 
The experiments demonstrates empirical guarantees of fairness of any model trained on top of the Fair PLS representation and better predictive performance for the same level of fairness when is compared to existing methods for FRL as Fair PCA.


% \subsubsection*{Acknowledgments}
 % 
% The first author was supported by the Gobierno de Navarra through the MRR Investigo 2023 program under the grant 0011-4001-2023-000071.
% 
% The third co-author was supported by the Spanish Ministry of Science and Innovation under the grant PID2021-128314NB-I00.
% 
% The authors would like to thank Alvaro Cía-Mina and Jesús López-Fidalgo for their helpful discussions.

\begin{thebibliography}{62}
%\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Morik(2010)]{Morik2010}
Katharina Morik.
\newblock \emph{Medicine: Applications of Machine Learning}, pages 654--661.
\newblock Springer US, Boston, MA, 2010.
\newblock ISBN 978-0-387-30164-8.
\newblock \doi{10.1007/978-0-387-30164-8_530}.
\newblock URL \url{https://doi.org/10.1007/978-0-387-30164-8_530}.

\bibitem[Trippi and Turban(1992)]{10.5555/573193}
Robert~R. Trippi and Efraim Turban.
\newblock \emph{Neural Networks in Finance and Investing: Using Artificial
  Intelligence to Improve Real World Performance}.
\newblock McGraw-Hill, Inc., USA, 1992.
\newblock ISBN 1557384525.

\bibitem[Angwin et~al.(2016)Angwin, Larson, Mattu, and
  Kirchner]{angwin2016machine}
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner.
\newblock {Machine Bias}.
\newblock \emph{Propublica}, 2016.
\newblock URL
  \url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}.

\bibitem[Dastin(2018)]{Amazon}
Jeffrey Dastin.
\newblock Amazon scraps secret ai recruiting tool that showed bias against
  women, 2018.
\newblock URL
  \url{https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secretai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G%7D}.

\bibitem[Zemel et~al.(2013)Zemel, Wu, Swersky, Pitassi, and
  Dwork]{pmlr-v28-zemel13}
Rich Zemel, Yu~Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork.
\newblock Learning fair representations.
\newblock In Sanjoy Dasgupta and David McAllester, editors, \emph{Proceedings
  of the 30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pages 325--333, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v28/zemel13.html}.

\bibitem[Kleindessner et~al.(2023)Kleindessner, Donini, Russell, and
  Zafar]{pmlr-v206-kleindessner23a}
Matth\"aus Kleindessner, Michele Donini, Chris Russell, and Muhammad~Bilal
  Zafar.
\newblock Efficient fair pca for fair representation learning.
\newblock In Francisco Ruiz, Jennifer Dy, and Jan-Willem van~de Meent, editors,
  \emph{Proceedings of The 26th International Conference on Artificial
  Intelligence and Statistics}, volume 206 of \emph{Proceedings of Machine
  Learning Research}, pages 5250--5270. PMLR, 25--27 Apr 2023.
\newblock URL \url{https://proceedings.mlr.press/v206/kleindessner23a.html}.

\bibitem[Olfat and Aswani(2019)]{Olfat_Aswani_2019}
Matt Olfat and Anil Aswani.
\newblock Convex formulations for fair principal component analysis.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  33\penalty0 (01):\penalty0 663--670, Jul. 2019.
\newblock \doi{10.1609/aaai.v33i01.3301663}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/3843}.

\bibitem[Lee et~al.(2022{\natexlab{a}})Lee, Kim, Olfat, Hasegawa-Johnson, and
  Yoo]{Lee_Kim_Olfat_Hasegawa-Johnson_Yoo_2022}
Junghyun Lee, Gwangsu Kim, Mahbod Olfat, Mark Hasegawa-Johnson, and Chang~D.
  Yoo.
\newblock Fast and efficient mmd-based fair pca via optimization over stiefel
  manifold.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  36\penalty0 (7):\penalty0 7363--7371, Jun. 2022{\natexlab{a}}.
\newblock \doi{10.1609/aaai.v36i7.20699}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/20699}.

\bibitem[Bair et~al.(2006)Bair, Hastie, Paul, and
  Tibshirani]{5bf28666-1390-327d-b81f-93157976a3f4}
Eric Bair, Trevor Hastie, Debashis Paul, and Robert Tibshirani.
\newblock Prediction by supervised principal components.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 119--137, 2006.
\newblock ISSN 01621459.
\newblock URL \url{http://www.jstor.org/stable/30047444}.

\bibitem[Barocas et~al.(2018)Barocas, Hardt, and
  Narayanan]{barocas2018fairness}
Solon Barocas, Moritz Hardt, and Arvind Narayanan.
\newblock \emph{Fairness and Machine Learning}.
\newblock fairmlbook.org, 2018.
\newblock URL \url{http://www.fairmlbook.org}.

\bibitem[Wang et~al.(2022)Wang, Zhang, and Zhu]{wang2022brief}
Xiaomeng Wang, Yishi Zhang, and Ruilin Zhu.
\newblock A brief review on algorithmic fairness.
\newblock \emph{Management System Engineering}, 1\penalty0 (1):\penalty0 7,
  2022.
\newblock ISSN 2731-5843.
\newblock \doi{10.1007/s44176-022-00006-z}.
\newblock URL \url{https://doi.org/10.1007/s44176-022-00006-z}.

\bibitem[Kamiran and Calders(2012)]{kamiran2012data}
Faisal Kamiran and Toon Calders.
\newblock Data preprocessing techniques for classification without
  discrimination.
\newblock \emph{Knowledge and Information Systems}, 33\penalty0 (1):\penalty0
  1--33, 2012.
\newblock ISSN 0219-3116.
\newblock \doi{10.1007/s10115-011-0463-8}.
\newblock URL \url{https://doi.org/10.1007/s10115-011-0463-8}.

\bibitem[Hardt et~al.(2016)Hardt, Price, Price, and Srebro]{NIPS2016_9d268236}
Moritz Hardt, Eric Price, Eric Price, and Nati Srebro.
\newblock Equality of opportunity in supervised learning.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf}.

\bibitem[Dwork et~al.(2012)Dwork, Hardt, Pitassi, Reingold, and
  Zemel]{10.1145/2090236.2090255}
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.
\newblock Fairness through awareness.
\newblock In \emph{Proceedings of the 3rd Innovations in Theoretical Computer
  Science Conference}, ITCS '12, page 214–226, New York, NY, USA, 2012.
  Association for Computing Machinery.
\newblock ISBN 9781450311151.
\newblock \doi{10.1145/2090236.2090255}.
\newblock URL \url{https://doi.org/10.1145/2090236.2090255}.

\bibitem[Kusner et~al.(2017)Kusner, Loftus, Russell, and
  Silva]{NIPS2017_a486cd07}
Matt~J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva.
\newblock Counterfactual fairness.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf}.

\bibitem[Lara et~al.(2024)Lara, Gonz{{\'a}}lez-Sanz, Asher, Risser, and
  Loubes]{JMLR:v25:21-1440}
Lucas~De Lara, Alberto Gonz{{\'a}}lez-Sanz, Nicholas Asher, Laurent Risser, and
  Jean-Michel Loubes.
\newblock Transport-based counterfactual models.
\newblock \emph{Journal of Machine Learning Research}, 25\penalty0
  (136):\penalty0 1--59, 2024.
\newblock URL \url{http://jmlr.org/papers/v25/21-1440.html}.

\bibitem[Calmon et~al.(2017)Calmon, Wei, Vinzamuri, Natesan~Ramamurthy, and
  Varshney]{NIPS2017_9a49a25d}
Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan
  Natesan~Ramamurthy, and Kush~R Varshney.
\newblock Optimized pre-processing for discrimination prevention.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf}.

\bibitem[Gordaliza et~al.(2019)Gordaliza, Del~Barrio, Fabrice, and
  Loubes]{gordaliza2019obtaining}
Paula Gordaliza, Eustasio Del~Barrio, Gamboa Fabrice, and Jean-Michel Loubes.
\newblock Obtaining fairness using optimal transport theory.
\newblock In \emph{International conference on machine learning}, pages
  2357--2365. PMLR, 2019.

\bibitem[Zafar et~al.(2017)Zafar, Valera, Gomez~Rodriguez, and
  Gummadi]{10.1145/3038912.3052660}
Muhammad~Bilal Zafar, Isabel Valera, Manuel Gomez~Rodriguez, and Krishna~P.
  Gummadi.
\newblock Fairness beyond disparate treatment \& disparate impact: Learning
  classification without disparate mistreatment.
\newblock In \emph{Proceedings of the 26th International Conference on World
  Wide Web}, WWW '17, page 1171–1180, Republic and Canton of Geneva, CHE,
  2017. International World Wide Web Conferences Steering Committee.
\newblock ISBN 9781450349130.
\newblock \doi{10.1145/3038912.3052660}.
\newblock URL \url{https://doi.org/10.1145/3038912.3052660}.

\bibitem[Donini et~al.(2018)Donini, Oneto, Ben-David, Shawe-Taylor, and
  Pontil]{NEURIPS2018_83cdcec0}
Michele Donini, Luca Oneto, Shai Ben-David, John~S Shawe-Taylor, and
  Massimiliano Pontil.
\newblock Empirical risk minimization under fairness constraints.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf}.

\bibitem[Risser et~al.(2022)Risser, Sanz, Vincenot, et~al.]{risser2022tackling}
L.~Risser, A.G. Sanz, Q.~Vincenot, et~al.
\newblock Tackling algorithmic bias in neural-network classifiers using
  wasserstein-2 regularization.
\newblock \emph{Journal of Mathematical Imaging and Vision}, 64:\penalty0
  672--689, 2022.
\newblock \doi{10.1007/s10851-022-01090-2}.

\bibitem[Wei et~al.(2021)Wei, Ramamurthy, and Calmon]{JMLR:v22:20-1143}
Dennis Wei, Karthikeyan~Natesan Ramamurthy, and Flavio~P. Calmon.
\newblock Optimized score transformation for consistent fair classification.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (258):\penalty0 1--78, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-1143.html}.

\bibitem[Beaudry and Renner(2012)]{10.5555/2230996.2231000}
Normand~J. Beaudry and Renato Renner.
\newblock An intuitive proof of the data processing inequality.
\newblock \emph{Quantum Info. Comput.}, 12\penalty0 (5–6):\penalty0
  432–441, May 2012.
\newblock ISSN 1533-7146.

\bibitem[Pearl(2009)]{Pearl_2009}
Judea Pearl.
\newblock \emph{Causality}.
\newblock Cambridge University Press, 2 edition, 2009.

\bibitem[Gupta et~al.(2021)Gupta, Ferber, Dilkina, and
  Ver~Steeg]{Gupta_Ferber_Dilkina_Ver_Steeg_2021}
Umang Gupta, Aaron~M Ferber, Bistra Dilkina, and Greg Ver~Steeg.
\newblock Controllable guarantees for fair outcomes via contrastive information
  estimation.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  35\penalty0 (9):\penalty0 7610--7619, May 2021.
\newblock \doi{10.1609/aaai.v35i9.16931}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/16931}.

\bibitem[Louizos et~al.(2015)Louizos, Swersky, Li, Welling, and
  Zemel]{louizos2015variational}
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel.
\newblock The variational fair autoencoder.
\newblock \emph{arXiv preprint arXiv:1511.00830}, 2015.

\bibitem[Edwards and Storkey(2016)]{edwards2016censoring}
Harrison Edwards and Amos Storkey.
\newblock Censoring representations with an adversary.
\newblock In \emph{4th International Conference on Learning Representations},
  pages 1--14, 2016.

\bibitem[Madras et~al.(2018)Madras, Creager, Pitassi, and
  Zemel]{pmlr-v80-madras18a}
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel.
\newblock Learning adversarially fair and transferable representations.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 3384--3393. PMLR,
  10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/madras18a.html}.

\bibitem[Xie et~al.(2017)Xie, Dai, Du, Hovy, and Neubig]{NIPS2017_8cb22bdd}
Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig.
\newblock Controllable invariance through adversarial feature learning.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf}.

\bibitem[Liao et~al.(2019)Liao, Huang, Kairouz, and Sankar]{liao2019gap}
Jian Liao, Chang Huang, Peter Kairouz, and Lalitha Sankar.
\newblock Learning generative adversarial representations (gap) under fairness
  and censoring constraints.
\newblock \emph{CoRR}, 2019.
\newblock URL \url{http://arxiv.org/abs/2019}.

\bibitem[Samadi et~al.(2018)Samadi, Tantipongpipat, Morgenstern, Singh, and
  Vempala]{NEURIPS2018_cc4af25f}
Samira Samadi, Uthaipon Tantipongpipat, Jamie~H Morgenstern, Mohit Singh, and
  Santosh Vempala.
\newblock The price of fair pca: One extra dimension.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/cc4af25fa9d2d5c953496579b75f6f6c-Paper.pdf}.

\bibitem[Pelegrina et~al.(2021)Pelegrina, Brotto, Duarte, Attux, and
  Romano]{pelegrina2021novel}
G.~Pelegrina, R.~Brotto, L.~Duarte, R.~Attux, and J.~Romano.
\newblock A novel multi-objective-based approach to analyze trade-offs in fair
  principal component analysis.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Kamani et~al.(2022)Kamani, Haddadpour, Forsati, and
  Mahdavi]{Kamani2022}
Mohammad~Mahdi Kamani, Farzin Haddadpour, Rana Forsati, and Mehrdad Mahdavi.
\newblock Efficient fair principal component analysis.
\newblock \emph{Machine Learning}, 111\penalty0 (10):\penalty0 3671--3702, Oct
  2022.
\newblock ISSN 1573-0565.
\newblock \doi{10.1007/s10994-021-06100-9}.
\newblock URL \url{https://doi.org/10.1007/s10994-021-06100-9}.

\bibitem[Champion et~al.(2023)Champion, Neagoe, Effernberger, Sala, Servant,
  Christensen, Arnoriaga-Rodriguez, Amar, Lelouvier, Loubieres,
  et~al.]{champion2023human}
Camille Champion, Radu~M Neagoe, Maria Effernberger, Daniela~T Sala, Florence
  Servant, Jeffrey~E Christensen, Maria Arnoriaga-Rodriguez, Jacques Amar,
  Benjamin Lelouvier, Pascale Loubieres, et~al.
\newblock Human liver microbiota modeling strategy at the early onset of
  fibrosis.
\newblock \emph{BMC microbiology}, 23\penalty0 (1):\penalty0 34, 2023.

\bibitem[Höskuldsson(1988)]{https://doi.org/10.1002/cem.1180020306}
Agnar Höskuldsson.
\newblock Pls regression methods.
\newblock \emph{Journal of Chemometrics}, 2\penalty0 (3):\penalty0 211--228,
  1988.
\newblock \doi{https://doi.org/10.1002/cem.1180020306}.
\newblock URL
  \url{https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.1180020306}.

\bibitem[Rosipal and Kr{\"a}mer(2006)]{10.1007/11752790_2}
Roman Rosipal and Nicole Kr{\"a}mer.
\newblock Overview and recent advances in partial least squares.
\newblock In Craig Saunders, Marko Grobelnik, Steve Gunn, and John
  Shawe-Taylor, editors, \emph{Subspace, Latent Structure and Feature
  Selection}, pages 34--51, Berlin, Heidelberg, 2006. Springer Berlin
  Heidelberg.
\newblock ISBN 978-3-540-34138-3.

\bibitem[Abdi(2010)]{https://doi.org/10.1002/wics.51}
Hervé Abdi.
\newblock Partial least squares regression and projection on latent structure
  regression (pls regression).
\newblock \emph{WIREs Computational Statistics}, 2\penalty0 (1):\penalty0
  97--106, 2010.
\newblock \doi{https://doi.org/10.1002/wics.51}.
\newblock URL
  \url{https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.51}.

\bibitem[Wold(1975)]{wold1975path}
Herman Wold.
\newblock Path models with latent variables: The nipals approach.
\newblock In Hubert~M. Blalock, editor, \emph{Quantitative Sociology}, pages
  307--357. Seminar Press, New York, 1975.

\bibitem[Abdi(2006)]{Abdi2006TheE}
Herv{\'e} Abdi.
\newblock The eigen-decomposition : Eigenvalues and eigenvectors.
\newblock In \emph{Mathematics}, 2006.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:14829978}.

\bibitem[H{\"o}skuldsson(1988)]{Hskuldsson1988PLSRM}
Agnar H{\"o}skuldsson.
\newblock Pls regression methods.
\newblock \emph{Journal of Chemometrics}, 2, 1988.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:120052390}.

\bibitem[Belrose et~al.(2023)Belrose, Schneider-Joseph, Ravfogel, Cotterell,
  Raff, and Biderman]{NEURIPS2023_d066d21c}
Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward
  Raff, and Stella Biderman.
\newblock Leace: Perfect linear concept erasure in closed form.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and
  S.~Levine, editors, \emph{Advances in Neural Information Processing Systems},
  volume~36, pages 66044--66063. Curran Associates, Inc., 2023.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/d066d21c619d0a78c5b557fa3291a8f4-Paper-Conference.pdf}.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{understandingML}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock \emph{Understanding Machine Learning - From Theory to Algorithms.}
\newblock Cambridge University Press, 2014.
\newblock ISBN 978-1-10-705713-5.

\bibitem[Blazere et~al.(2015)Blazere, Gamboa, and Loubes]{blazere2015partial}
M{\'e}lanie Blazere, Fabrice Gamboa, and Jean-Michel Loubes.
\newblock Partial least squares a new statistical insight through orthogonal
  polynomials.
\newblock In \emph{19th European Young Statisticians Meeting}, volume~13,
  page~12, 2015.

\bibitem[L{\"o}fstedt(2024)]{lofstedt2024using}
Tommy L{\"o}fstedt.
\newblock Using the krylov subspace formulation to improve regularisation and
  interpretation in partial least squares regression.
\newblock \emph{Computational Statistics}, pages 1--22, 2024.

\bibitem[Rosipal and Trejo(2002)]{10.5555/944790.944806}
Roman Rosipal and Leonard~J. Trejo.
\newblock Kernel partial least squares regression in reproducing kernel hilbert
  space.
\newblock \emph{J. Mach. Learn. Res.}, 2:\penalty0 97–123, March 2002.
\newblock ISSN 1532-4435.

\bibitem[Tan et~al.(2020)Tan, Yeom, Fredrikson, and
  Talwalkar]{pmlr-v108-tan20a}
Zilong Tan, Samuel Yeom, Matt Fredrikson, and Ameet Talwalkar.
\newblock Learning fair representations for kernel models.
\newblock In Silvia Chiappa and Roberto Calandra, editors, \emph{Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pages 155--166. PMLR, 26--28 Aug 2020.
\newblock URL \url{https://proceedings.mlr.press/v108/tan20a.html}.

\bibitem[Fukumizu et~al.(2007)Fukumizu, Gretton, Sun, and
  Sch\"{o}lkopf]{NIPS2007_3a077244}
Kenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Sch\"{o}lkopf.
\newblock Kernel measures of conditional dependence.
\newblock In J.~Platt, D.~Koller, Y.~Singer, and S.~Roweis, editors,
  \emph{Advances in Neural Information Processing Systems}, volume~20. Curran
  Associates, Inc., 2007.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2007/file/3a0772443a0739141292a5429b952fe6-Paper.pdf}.

\bibitem[Gretton et~al.(2007)Gretton, Fukumizu, Teo, Song, Sch\"{o}lkopf, and
  Smola]{NIPS2007_d5cfead9}
Arthur Gretton, Kenji Fukumizu, Choon Teo, Le~Song, Bernhard Sch\"{o}lkopf, and
  Alex Smola.
\newblock A kernel statistical test of independence.
\newblock In J.~Platt, D.~Koller, Y.~Singer, and S.~Roweis, editors,
  \emph{Advances in Neural Information Processing Systems}, volume~20. Curran
  Associates, Inc., 2007.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf}.

\bibitem[Yamanishi et~al.(2004)Yamanishi, Vert, and
  Kanehisa]{10.7551/mitpress/4057.003.0014}
Yoshihiro Yamanishi, Jean-Philippe Vert, and Minoru Kanehisa.
\newblock {Heterogeneous Data Comparison and Gene Selection with Kernel
  Canonical Correlation Analysis}.
\newblock In \emph{{Kernel Methods in Computational Biology}}. The MIT Press,
  07 2004.
\newblock ISBN 9780262256926.
\newblock \doi{10.7551/mitpress/4057.003.0014}.
\newblock URL \url{https://doi.org/10.7551/mitpress/4057.003.0014}.

\bibitem[Gretton et~al.(2005)Gretton, Bousquet, Smola, and
  Sch{\"o}lkopf]{10.1007/11564089_7}
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch{\"o}lkopf.
\newblock Measuring statistical dependence with hilbert-schmidt norms.
\newblock In Sanjay Jain, Hans~Ulrich Simon, and Etsuji Tomita, editors,
  \emph{Algorithmic Learning Theory}, pages 63--77, Berlin, Heidelberg, 2005.
  Springer Berlin Heidelberg.
\newblock ISBN 978-3-540-31696-1.

\bibitem[Sch{\"o}lkopf et~al.(1998)Sch{\"o}lkopf, Smola, and
  M{\"u}ller]{Scholkopf1998}
B.~Sch{\"o}lkopf, A.~J. Smola, and K.-R. M{\"u}ller.
\newblock Nonlinear component analysis as a kernel eigenvalue problem.
\newblock \emph{Neural Computation}, 10:\penalty0 1299--1319, 1998.

\bibitem[Perez-Suay et~al.(2023)Perez-Suay, Gordaliza, Loubes, Sejdinovic, and
  Camps-Valls]{perez-suay2023fair}
Adrian Perez-Suay, Paula Gordaliza, Jean-Michel Loubes, Dino Sejdinovic, and
  Gustau Camps-Valls.
\newblock Fair kernel regression through cross-covariance operators.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=MyQ1e1VQQ3}.

\bibitem[Jourdan et~al.(2023{\natexlab{a}})Jourdan, Picard, Fel, Risser,
  Loubes, and Asher]{jourdan2023cockatiel}
Fanny Jourdan, Agustin Picard, Thomas Fel, Laurent Risser, Jean-Michel Loubes,
  and Nicholas Asher.
\newblock Cockatiel: Continuous concept ranked attribution with interpretable
  elements for explaining neural net classifiers on nlp tasks.
\newblock In \emph{61st Annual Meeting of the Association for Computational
  Linguistics (ACL 2023)}, pages 5120--5136, 2023{\natexlab{a}}.

\bibitem[Jourdan et~al.(2023{\natexlab{b}})Jourdan, B{\'e}thune, Picard,
  Risser, and Asher]{jourdan2023taco}
Fanny Jourdan, Louis B{\'e}thune, Agustin Picard, Laurent Risser, and Nicholas
  Asher.
\newblock Taco: Targeted concept removal in output embeddings for nlp via
  information theory and explainability.
\newblock \emph{arXiv e-prints}, pages arXiv--2312, 2023{\natexlab{b}}.

\bibitem[Eckart and Young(1936)]{eckart1936approximation}
C.~Eckart and G.~Young.
\newblock The approximation of one matrix by another of lower rank.
\newblock \emph{Psychometrika}, 1\penalty0 (3):\penalty0 211--218, 1936.
\newblock \doi{10.1007/BF02288367}.

\bibitem[Besse et~al.(2022)Besse, del Barrio, Gordaliza, Loubes, and
  Risser]{besse2018confidence}
Philippe Besse, Eustasio del Barrio, Paula Gordaliza, Jean-Michel Loubes, and
  Laurent Risser.
\newblock A survey of bias in machine learning through the prism of statistical
  parity.
\newblock \emph{The American Statistician}, 76\penalty0 (2):\penalty0 188--198,
  2022.

\bibitem[Dua and Graff(2017)]{Dua_2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Hofmann(1994)]{misc_statlog_(german_credit_data)_144}
Hans Hofmann.
\newblock {Statlog (German Credit Data)}.
\newblock UCI Machine Learning Repository, 1994.
\newblock {DOI}: https://doi.org/10.24432/C5NC77.

\bibitem[Clore et~al.(2014)Clore, Cios, DeShazo, and Strack]{diabetes_data}
J.~Clore, K.~Cios, J.~DeShazo, and B.~Strack.
\newblock Diabetes 130-us hospitals for years 1999-2008 [dataset], 2014.
\newblock URL \url{https://doi.org/10.24432/C5230J}.

\bibitem[Redmond(2002)]{communities_and_crime_183}
Michael Redmond.
\newblock {Communities and Crime}.
\newblock UCI Machine Learning Repository, 2002.
\newblock {DOI}: https://doi.org/10.24432/C53W3X.

\bibitem[Lee et~al.(2022{\natexlab{b}})Lee, Kim, Olfat, Hasegawa-Johnson, and
  Yoo]{lee2022fairpca}
Junghyun Lee, Gwangsu Kim, Mahbod Olfat, Mark Hasegawa-Johnson, and Chang~D.
  Yoo.
\newblock {Fast and Efficient MMD-Based Fair PCA via Optimization over Stiefel
  Manifold}.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 7363--7371, Jun. 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2109.11196.pdf}.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\end{thebibliography}



%\appendix

\section{Appendix to Sections \ref{sec:fairPLS} and \ref{sec:KFPLS}}
\label{sec:app_fairPLS}
We provide here the algorithms  for NIPALS. 

\begin{algorithm}[!htbp]
\nextfloat \caption{Nonlinear Iterative Partial Least Squares (NIPALS): A PLS algorithm}
\label{alg:NIPALS}
\KwInput{$d$ independent variables stored in a matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ and $m$ dependent variables stored in a matrix $\mathbf{Y} \in \mathbb{R}^{n \times m}$.}
\KwOutput{$\mathbf{W}$, $\mathbf{T}$, $\mathbf{C}$, $\mathbf{U}$ and $\mathbf{P}$.}
Create two matrices $\mathbf{E} = \mathbf{X}$ and $\mathbf{F} = \mathbf{Y}$\;
The matrices $\mathbf{E}$ and $\mathbf{F}$ are column centred and normalized\;
Set $\mathbf{u}$ to the first column of $\mathbf{F}$ (could be also initialized with random values)\;

\While{$\mathbf{E}$ is not the null matrix}{
    \While{$\mathbf{t}$ not converged}{
    $\mathbf{w} = \mathbf{E}^{\mathsf{T}} \mathbf{u} / ({\mathbf{u}^{\mathsf{T}} \mathbf{u}} )$\;
    Scale $\mathbf{w}$ to be of length one\;
    $\mathbf{t} = \mathbf{E} \mathbf{w}$\;
    $\mathbf{c} = \mathbf{F}^{\mathsf{T}} \mathbf{t} / ({\mathbf{t}^{\mathsf{T}} \mathbf{t}} )$\;
    Scale $\mathbf{c}$ to be of length one\;
    $\mathbf{u} = \mathbf{F}^{\mathsf{T}} \mathbf{c} $\;
    }
    
$\mathbf{p} = \mathbf{E}^{\mathsf{T}} \mathbf{t} / ({\mathbf{t}^{\mathsf{T}} \mathbf{t}} )$\;
$\mathbf{q} = \mathbf{F}^{\mathsf{T}} \mathbf{u} / ({\mathbf{u}^{\mathsf{T}} \mathbf{u}} )$\;
$b = \mathbf{u}^{\mathsf{T}} \mathbf{t} / ({\mathbf{t}^{\mathsf{T}} \mathbf{t}} )$\;

Compute the residual matrices: $\mathbf{E} = \mathbf{E} - \mathbf{t} \mathbf{p}^{\mathsf{T}}$ and $\mathbf{F} = \mathbf{F} - b \mathbf{t} \mathbf{c}^{\mathsf{T}}$\;

Store the vectors $\mathbf{w}, \mathbf{t}, \mathbf{c}, \mathbf{u}, \mathbf{p}$ in the corresponding matrices\;

}
\end{algorithm}


\begin{algorithm}[!htbp]
\caption{Naive Fair PLS}\label{alg:Vanilla}
\KwInput{$d$ independent variables stored in a centred matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$; $m$ dependent variables stored in a centred matrix $\mathbf{Y} \in \mathbb{R}^{n \times m}$; sensitive variable $S$; threshold $\tau$.}
\KwOutput{$\mathbf{T}$ composed of each latent variable $\mathbf{t}_{h}$ selected.}

\For{$h=1$ \textbf{to} $k$}{
Solve $\mathbf{w}_{h} = \underset{\Vert \mathbf{w} \Vert=1}{\argmax} \ Cov(\mathbf{X}\mathbf{w}, \mathbf{Y})$ \;
Extract $\mathbf{t}_{h}= \mathbf{X} \mathbf{w}_{h} $ \;
Calculate the correlation ratio $\text{Corr}_{h}=\eta^2(\mathbf{t}_{h}, S)$ \;
\If{$\text{Corr}_{h}<\tau$}{$\mathbf{t}_{h}$ is added as a column of $\mathbf{T}$\;}
}
\end{algorithm}

\begin{algorithm}[!htbp]
\caption{Kernel Fair PLS algorithm}\label{alg:FairPLS_kenrel}
\KwInput{$\mathbf{\Phi} \in \mathbb{R}^{n \times d}$ matrix of mapped input data and $m$ dependent variables stored in a centred matrix $\mathbf{Y} \in \mathbb{R}^{n \times m}$; sensitive mapped data $\mathbf{\Psi}$;  $\eta$ parameter; $k$ number of components.}
\KwOutput{$\mathbf{T}$.}
% 
Set $\mathbf{K}_{\mathbf{X}, 1} = \mathbf{K}_{\mathbf{X}}$ and $\mathbf{Y}_{1} = \mathbf{Y}$\;
Center the matrices $\widetilde{\mathbf{K}}_{\mathbf{X},1} = \mathbf{H} \mathbf{K}_{\mathbf{X},1} \mathbf{H}$ and $\widetilde{\mathbf{K}}_{\mathbf{S},1} = \mathbf{H} \mathbf{K}_{\mathbf{S},1} \mathbf{H}$, where $\mathbf{H} = \mathbf{I}_{n} - \frac{1}{n} \mathbf{1}_{n} \mathbf{1}_{n}^{\mathsf{T}}$ \;
% 
\For{$h \in [k]$}{
% 
Compute the vector $\mathbf{\alpha}_{h} \in \mathbb{R}^{n}$ the maximum of the function $f_{FKPLS} (\mathbf{\alpha}) = \frac{1}{n^{2}} \text{ } \text{Tr}(\mathbf{\alpha}^{\mathsf{T}} \widetilde{\mathbf{K}}_{\mathbf{X},h} \mathbf{Y} \mathbf{Y}^{\mathsf{T}} \widetilde{\mathbf{K}}_{\mathbf{X},h} \mathbf{\alpha}) - \eta \text{ } \frac{1}{n^{2}} \text{ } \text{Tr}(\mathbf{\alpha}^{\mathsf{T}} \widetilde{\mathbf{K}}_{\mathbf{X},h}  \widetilde{\mathbf{K}}_{\mathbf{S},h} \widetilde{\mathbf{K}}_{\mathbf{X},h} \mathbf{\alpha}) $\;
% 
Scale them to be of length one\;
% 
Obtain the scores $\mathbf{t}_{h} = \widetilde{\mathbf{K}}_{\mathbf{X},h} \mathbf{\alpha}_{h}$ \;
% 
Compute residual matrices: $\widetilde{\mathbf{K}}_{\mathbf{X},h+1} = \widetilde{\mathbf{K}}_{\mathbf{X},h} - \mathbf{t}_{h} \mathbf{t}_{h}^{\mathsf{T}} \widetilde{\mathbf{K}}_{\mathbf{X},h} - \widetilde{\mathbf{K}}_{\mathbf{X},h} \mathbf{t}_{h} \mathbf{t}_{h}^{\mathsf{T}} + \mathbf{t}_{h} \mathbf{t}_{h}^{\mathsf{T}} \widetilde{\mathbf{K}}_{\mathbf{X},h} \mathbf{t}_{h} \mathbf{t}_{h}^{\mathsf{T}}$\;
}
Store the vectors $ \mathbf{t}$ in the corresponding matrices\;
\end{algorithm}

\newpage
\section{Appendix to section \ref{sec:experiments}}
\label{sec:app_experiments}

\subsection{Details about Datasets}

\paragraph{Adult Income dataset}
% 
The Adult Income dataset, available through the UCI repository \citep{Dua_2019} provides the results of a census made in 1994 in the United States. 
% 
Specifically, it contains information about 48842 of individuals, described as values of 14 features: 8 categorical and 6 numeric. 
% 
The objective of this dataset is to accurately predict whether an individual's annual income is above or below $50,000\$$, taking into account factors such its occupation, marital status, and education. 

\paragraph{German Credit dataset}
% 
The German credit dataset \citep{misc_statlog_(german_credit_data)_144}, comprises records of individuals who hold bank accounts. 
% 
This dataset serves the purpose of forecasting risk, specifically to assess whether it's advisable to extend credit to an individual. 
% 
Specifically, it contains information about 1000 individuals, described as values of 21 features: 14 categorical and 7 numerical.
% 
The objective of this dataset is to accurately predict the customer's level of risk when granting a credit, taking into account factors such as the status of the existing checking account, credit amount or marital status.


\paragraph{Law School dataset}
% 
The Law School Admission Council dataset, gathers statistics from 163 US law schools and more than 20,000 students, obtained through a survey across 163 law schools in the United States.
% 
This dataset serves the purpose of forecasting the first -year grade from the profile. 
% 
Specifically, it contains information about 21,791 individuals, described as values of 7 features: 2 categorical and 7 numerical.
% 
The objective of this dataset is to accurately predict if an applicant will have a high FYA, taking into account factors such as students entrance exam scores (LSAT), their grade-point average (GPA) collected prior to law school, and their first year average grade (FYA).


\paragraph{Diabetes dataset}

The Diabetes dataset \citep{diabetes_data}, represents ten years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. Each row concerns hospital records of patients diagnosed with diabetes, who underwent laboratory, medications, and stayed up to 14 days.
% 
This dataset serves the purpose of forecasting if a patient will be readmitted within 30 days of discharge.
% 
Specifically, it contains information about 101766 individuals, described as values of 49 features: 36 categorical and 13 numerical.
% 
The objective of this dataset is to accurately predict the readmitted $\{<30, >30\}$ indicating whether a patient will readmit within 30 days (the positive class is $<30$), taking into account factors such as weight, gender or the number of lab tests performed during the encounter.


\paragraph{COMPAS dataset} 
% 
The COMPAS dataset, \citep{angwin2016machine}, which was released by ProPublica in 2016 is based on the Broward County data (collected from January 2013 to December 2014).
% 
This dataset serves the purpose of forecasting recidivism risk scores, specifically to predict if an individual is rearrested within 2 years after the first arrest.
% 
Specifically, it contains information about 7214 individuals, described as values of 52 features: 33 categorical and 19 numerical.
% 
The objective of this dataset is to accurately predict the COM-PAS recid, taking into account factors such as the risk of recidivism in general, sex or age.


\paragraph{Communities and Crimes dataset}
% 
The Communities and Crimes dataset \citep{communities_and_crime_183}, is a small dataset containing the socioeconomic data from 46 states of the United States in 1990 (the US Census).
% 
This dataset serves the purpose of forecasting the total number of violent crimes per 100 thousand population.
% 
Specifically, it contains information about 1994 individuals, described as values of 127 features: 4 categorical and 123 numerical.
% 
The objective of this dataset is to accurately predict the number of violent crimes per 100,000 population (normalized to [0,1]) taking into account factors such as median household income, per capita income or number of kids born to never married.

\begin{table}[!htbp]
    \label{im:appendix_datasets}
    \caption{Bias measured in the original datasets.  For the datasets whose task is regression (Communities and Crimes) the column of DI is actually the KS value.}
    \begin{center}
    \begin{tabular}{lllll}
    \bf Dataset & \bf Sensitive & \bf Privileged group & \bf Disparate impact & \bf Conf. Interval
    \\ \hline \\
    Adult Income & Gender & Male & 0.3597 & [0.3428 , 0.3765] \\ 
    German Credit & Age & $>25$ & 0.7948 & [0.6928 , 0.8968] \\ 
    Law School & Race & White & 0.6713 & [0.6423 , 0.7004] \\ 
    Diabetes & Race & Caucasian & 0.8952 & [0.8758 , 0.9146] \\ 
    COMPAS & Race & Caucasian & 0.8009 & [0.7641 , 0.8378] \\ 
    Communities and Crimes & Race & Not black & $0.129^{\star}$ & - \\ 
    \end{tabular}
    \end{center}
\end{table}

\paragraph{Synthetic dataset}
% 
The synthetic dataset contains two groups ($S = 0$ and $S = 1$) with distinct statistical properties. 
% 
The data includes four quantitative variables (0-3) and three binary variables (4-6), all correlated within each group. 
% 
We generate $500$ samples for each group from two multivariate normal distributions on $\mathbb{R}^{7}$; with means $(9, 8, 10, 10, 0, 0, 0)$ and $(10, 10, 10, 10, 0, 0, 0)$, respectively, and different covariance matrix.
% 
For females, binary variables 4 and 5 strongly impact variable 0, while variable 6 influences variable 1. 
% 
In contrast, for males, binary variables have little to no impact on these quantitative variables. 
% 
The target variable, \(Y\), is generated using a weighted combination of these features, with different coefficients for each group. 
% 
The data is then shuffled, and a binary indicator \(S\) is added to distinguish between genders.
% 
This setup provides a useful framework for testing biases and statistical analysis.

%  ORIGINAL POSITION
% \begin{table}[h!]
%     \label{im:appendix_datasets}
%     \caption{Bias measured in the original datasets.  For the datasets whose task is regression (Communities and Crimes) the column of DI is actually the KS value.}
%     \begin{center}
%     \begin{tabular}{lllll}
%     \bf Dataset & \bf Sensitive & \bf Privileged group & \bf Disparate impact & \bf Conf. Interval
%     \\ \hline \\
%     Adult Income & Gender & Male & 0.3597 & [0.3428 , 0.3765] \\ 
%     German Credit & Age & $>25$ & 0.7948 & [0.6928 , 0.8968] \\ 
%     Law School & Race & White & 0.6713 & [0.6423 , 0.7004] \\ 
%     Diabetes & Race & Caucasian & 0.8952 & [0.8758 , 0.9146] \\ 
%     COMPAS & Race & Caucasian & 0.8009 & [0.7641 , 0.8378] \\ 
%     Communities and Crimes & Race & Not black & $0.129^{\star}$ & - \\ 
%     \end{tabular}
%     \end{center}
% \end{table}


\subsection{Details about Implementation setup}

\paragraph{General details.}

\begin{itemize}
    \item Data pre-processing: the details about how each dataset has been processed can be find in the \href{https://github.com/emartindedi/fair_pls.git}{GitHub repository}.

    \item Data normalization: We normalized the input data to have zero mean and unit variance.

    \item Dimension of the fair representation: As target dimension we chose $k \in [d]$ with the classical cross validation procedure, where the objective is to find the best trade off of $Cov^{2}(r(\mathbf{X}), Y) - \eta Cov^{2} (r(\mathbf{X}), S)$. Notice that the $k$ selected could it be selected differently for each $\eta$.
\end{itemize}
\begin{table}[!htbp]
    \label{im:target_selection}
    \caption{Best number of components $k$ for each dataset in terms of the objective function of the maximization problem \eqref{eq:4}. The value $d$ is the number of features after the preprocessing of the datasets.}
    \begin{center}
    \begin{tabular}{lllll}
    % \hline
    \bf Dataset & $d$ & $k (\eta = 0.0)$ & $k (\eta = 1.0)$ & $k (\eta = 2.0)$
    \\ \hline \\
    Adult Income & 36 & 5 & 5 & 3 \\ 
    German Credit & 21 & 20 & 7 & 1 \\
    Law School & 3 & 3 & 2 & 2 \\ 
    Diabetes & 27 & 20 & 5 & 2 \\ 
    COMPAS & 6 & 6 & 5 & 2 \\ 
    Communities and Crimes & 33 & 7 & 5 & 3\\
    \end{tabular}
    \end{center}
\end{table}

\paragraph{Details about the prediction models.}
% 
We propose three different state-of-the-art supervised learning models: Logistic Regression (LR) / Linear Regression (LR), Decision Tree (DT) and Extreme Gradient Boosting (XGB).
% 
The selection of these algorithms stems from their ability to encompass distinct modeling approaches, each of them representing different learning paradigms.
% 
It is important to note that the aforementioned modeling approaches do not incorporate any algorithmic fairness constraints throughout their modeling process. 
Consequently, they serve as reference solutions against which bias mitigation techniques can be evaluated and compared.
% 
We trained the three prediction models using Scikit-learn and the default specifications for each of them.


\subsection{Experiments of Fair Partial Least Squares}
% 
Figures \ref{im:fair_representation_A1} and \ref{im:fair_representation_A2} provide the results of the experiment ($\mathcal{A}$) Fair Representations where the aim is to verify that the new representation satisfies the conditions imposed. 
% 
\begin{figure}[!htbp]
\caption{(Fair Representation). Comparing the objective functions of the Fair PLS formulation for the representation $r_{\eta} (\mathbf{X})$. The blue line shows result ($\mathcal{A} - 1$) while result ($\mathcal{A} - 2$) is represented by the orange one.}
\label{im:fair_representation_A1}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/adult_criterio_2_fill.pdf}
    \label{im:a11}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/german_criterio_2_fill.pdf}
    \label{im:a112}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/law_criterio_2_fill.pdf}
    \label{im:a113}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/diabetes_criterio_2_fill.pdf}
        \label{im:a124}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/compas_criterio_2_fill.pdf}
        \label{im:a135}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/crime_criterio_2_fill.pdf}
        \label{im:a136}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{img/lengend_criterio.pdf}
        \label{im:a147}
    \end{subfigure}
\end{figure}
% 

% 
\begin{figure}[!htbp]
\caption{(Fair Representation). The reconstruction error for the new representation $r_{\eta} (\mathbf{X})$ with respect to the original variables $\mathbf{X}$, showing result $(\mathcal{A} - 3$).}
\label{im:fair_representation_A2}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/adult_rec_error.pdf}
    \label{im:a211}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/german_rec_error.pdf}
        \label{im:a2311}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/law_rec_error.pdf}
        \label{im:a221}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/diabetes_rec_error.pdf}
        \label{im:a2312}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/compas_rec_error.pdf}
        \label{im:a2313}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/crime_rec_error.pdf}
        \label{im:a2314}
    \end{subfigure}
\end{figure}
% 

% 
Table \ref{table:pf_classification} and Table \ref{table:pf_regression} provide the results of the experiments for the classification and regression setup respectively.
% 
This is, for different values of $\eta$ and diverse datasets, we predict with three ML models and show the accuracy (mean square error) and disparate impact (KS) values for classification (and regression). 

% 
\begin{table}[!htbp] 
\caption{($\mathcal{B}$ - Fair Predictions: classification) Results of accuracy and fairness (quantified by the Disparate Impact), for different ML models trained using the Fair-PLS learned representation.} 
\label{table:pf_classification} 
\begin{center}
    \begin{tabular}{lllllll} 
    $\eta$ & \bf Dataset & \bf Model & \bf Disparate impact & \bf Accuracy & $Cov^{2}(r(X),\Hat{Y})$ & $Cov^{2}(r(X),S)$ \\ \hline \\ \multirow{12}{1em}{$0.0$}  & \multirow{3}{4em}{Adult Income}  & LR & $ 0.2433 \pm 0.0255 $ & $ 0.8403 \pm 0.0036 $ & $ 0.2236 \pm 0.0127 $ & $ 0.068 \pm 0.0075 $\\ &  & DT & $ 0.395 \pm 0.026 $ & $ 0.7794 \pm 0.0067 $ & $ 0.1714 \pm 0.013 $ & $ 0.068 \pm 0.0075 $\\ &  & XGB & $ 0.2618 \pm 0.0299 $ & $ 0.8375 \pm 0.0047 $ & $ 0.2231 \pm 0.0129 $ & $ 0.068 \pm 0.0075 $\\  \cdashline{3-7}   & \multirow{3}{4em}{German Credit}  & LR & $ 0.8532 \pm 0.1133 $ & $ 0.716 \pm 0.0315 $ & $ 0.1613 \pm 0.0449 $ & $ 0.0804 \pm 0.0497 $\\ &  & DT & $ 0.8505 \pm 0.1735 $ & $ 0.64 \pm 0.0563 $ & $ 0.1062 \pm 0.0335 $ & $ 0.0804 \pm 0.0497 $\\ &  & XGB & $ 0.8888 \pm 0.1453 $ & $ 0.701 \pm 0.0339 $ & $ 0.1458 \pm 0.0539 $ & $ 0.0804 \pm 0.0497 $\\  \cdashline{3-7}   & \multirow{3}{4em}{Law School}  & LR & $ 0.6626 \pm 0.0548 $ & $ 0.9049 \pm 0.0021 $ & $ 0.0099 \pm 0.0053 $ & $ 0.0166 \pm 0.006 $\\ &  & DT & $ 0.6901 \pm 0.0308 $ & $ 0.8475 \pm 0.0056 $ & $ 0.0219 \pm 0.0069 $ & $ 0.0166 \pm 0.006 $\\ &  & XGB & $ 0.719 \pm 0.0487 $ & $ 0.9003 \pm 0.0013 $ & $ 0.0089 \pm 0.0056 $ & $ 0.0166 \pm 0.006 $\\  \cdashline{3-7}   & \multirow{3}{4em}{Diabetes}  & LR & $ 0.6647 \pm 0.0391 $ & $ 0.6235 \pm 0.0024 $ & $ 0.1149 \pm 0.008 $ & $ 0.0047 \pm 0.002 $\\ &  & DT & $ 0.9392 \pm 0.0366 $ & $ 0.5391 \pm 0.006 $ & $ 0.0149 \pm 0.0037 $ & $ 0.0047 \pm 0.002 $\\ &  & XGB & $ 0.7743 \pm 0.0278 $ & $ 0.6087 \pm 0.0047 $ & $ 0.096 \pm 0.0068 $ & $ 0.0047 \pm 0.002 $\\  \cdashline{3-7}   & \multirow{3}{4em}{COMPAS}  & LR & $ 0.8083 \pm 0.0282 $ & $ 0.8922 \pm 0.0087 $ & $ 0.3287 \pm 0.013 $ & $ 0.0342 \pm 0.0066 $\\ &  & DT & $ 0.7938 \pm 0.0574 $ & $ 0.7956 \pm 0.0163 $ & $ 0.191 \pm 0.0321 $ & $ 0.0342 \pm 0.0066 $\\ &  & XGB & $ 0.7847 \pm 0.0338 $ & $ 0.8842 \pm 0.0081 $ & $ 0.3092 \pm 0.0118 $ & $ 0.0342 \pm 0.0066 $\\  \cdashline{3-7}   \cdashline{2-7} \multirow{12}{1em}{$1.0$}  & \multirow{3}{4em}{Adult Income}  & LR & $ 0.2288 \pm 0.0289 $ & $ 0.8325 \pm 0.0044 $ & $ 0.1903 \pm 0.0143 $ & $ 0.0623 \pm 0.0084 $\\ &  & DT & $ 0.4163 \pm 0.0473 $ & $ 0.7588 \pm 0.0068 $ & $ 0.1185 \pm 0.018 $ & $ 0.0623 \pm 0.0084 $\\ &  & XGB & $ 0.2736 \pm 0.0786 $ & $ 0.8187 \pm 0.007 $ & $ 0.143 \pm 0.0301 $ & $ 0.0623 \pm 0.0084 $\\  \cdashline{3-7}   & \multirow{3}{4em}{German Credit}  & LR & $ 0.9593 \pm 0.1347 $ & $ 0.705 \pm 0.0302 $ & $ 0.107 \pm 0.06 $ & $ 0.0454 \pm 0.0322 $\\ &  & DT & $ 0.9508 \pm 0.1584 $ & $ 0.613 \pm 0.0294 $ & $ 0.0543 \pm 0.0356 $ & $ 0.0454 \pm 0.0322 $\\ &  & XGB & $ 0.9028 \pm 0.2125 $ & $ 0.672 \pm 0.0318 $ & $ 0.0879 \pm 0.0589 $ & $ 0.0454 \pm 0.0322 $\\  \cdashline{3-7}   & \multirow{3}{4em}{Law School}  & LR & $ 1.0 \pm 0.0 $ & $ 0.9004 \pm 0.0002 $ & $ 0.0005 \pm 0.0006 $ & $ 0.001 \pm 0.001 $\\ &  & DT & $ 0.9743 \pm 0.1191 $ & $ 0.7268 \pm 0.0917 $ & $ 0.0112 \pm 0.0197 $ & $ 0.001 \pm 0.001 $\\ &  & XGB & $ 0.974 \pm 0.1044 $ & $ 0.8075 \pm 0.1055 $ & $ 0.0105 \pm 0.0177 $ & $ 0.001 \pm 0.001 $\\  \cdashline{3-7}   & \multirow{3}{4em}{Diabetes}  & LR & $ 0.7826 \pm 0.1003 $ & $ 0.6176 \pm 0.0075 $ & $ 0.0714 \pm 0.0323 $ & $ 0.0032 \pm 0.0022 $\\ &  & DT & $ 0.979 \pm 0.0341 $ & $ 0.5263 \pm 0.0071 $ & $ 0.0076 \pm 0.0051 $ & $ 0.0032 \pm 0.0022 $\\ &  & XGB & $ 0.8584 \pm 0.0499 $ & $ 0.5904 \pm 0.0121 $ & $ 0.0481 \pm 0.0249 $ & $ 0.0032 \pm 0.0022 $\\  \cdashline{3-7}   & \multirow{3}{4em}{COMPAS}  & LR & $ 0.8046 \pm 0.0274 $ & $ 0.8907 \pm 0.0077 $ & $ 0.3229 \pm 0.0137 $ & $ 0.0294 \pm 0.007 $\\ &  & DT & $ 0.867 \pm 0.1809 $ & $ 0.7369 \pm 0.0751 $ & $ 0.1364 \pm 0.0681 $ & $ 0.0294 \pm 0.007 $\\ &  & XGB & $ 0.7476 \pm 0.0509 $ & $ 0.8652 \pm 0.0306 $ & $ 0.2796 \pm 0.0526 $ & $ 0.0294 \pm 0.007 $\\  \cdashline{3-7}   \cdashline{2-7} \multirow{12}{1em}{$2.0$}  & \multirow{3}{4em}{Adult Income}  & LR & $ 1.0 \pm 0.0 $ & $ 0.7607 \pm 0.0001 $ & $ 0.0 \pm 0.0 $ & $ 0.0046 \pm 0.0016 $\\ &  & DT & $ 0.8519 \pm 0.1714 $ & $ 0.6537 \pm 0.0195 $ & $ 0.0013 \pm 0.0009 $ & $ 0.0046 \pm 0.0016 $\\ &  & XGB & $ 0.6111 \pm 0.167 $ & $ 0.7467 \pm 0.0185 $ & $ 0.0018 \pm 0.0014 $ & $ 0.0046 \pm 0.0016 $\\  \cdashline{3-7}   & \multirow{3}{4em}{German Credit}  & LR & $ 0.969 \pm 0.0821 $ & $ 0.705 \pm 0.013 $ & $ 0.0411 \pm 0.0585 $ & $ 0.0385 \pm 0.0281 $\\ &  & DT & $ 0.9004 \pm 0.0703 $ & $ 0.617 \pm 0.0479 $ & $ 0.0315 \pm 0.0402 $ & $ 0.0385 \pm 0.0281 $\\ &  & XGB & $ 0.9514 \pm 0.0972 $ & $ 0.638 \pm 0.0429 $ & $ 0.0435 \pm 0.0566 $ & $ 0.0385 \pm 0.0281 $\\  \cdashline{3-7}   & \multirow{3}{4em}{Law School}  & LR & $ 1.0 \pm 0.0 $ & $ 0.9004 \pm 0.0002 $ & $ 0.0006 \pm 0.0005 $ & $ 0.0012 \pm 0.0011 $\\ &  & DT & $ 0.9792 \pm 0.0431 $ & $ 0.7613 \pm 0.1223 $ & $ 0.0012 \pm 0.0014 $ & $ 0.0012 \pm 0.0011 $\\ &  & XGB & $ 0.9651 \pm 0.0844 $ & $ 0.814 \pm 0.1163 $ & $ 0.0043 \pm 0.0071 $ & $ 0.0012 \pm 0.0011 $\\  \cdashline{3-7}   & \multirow{3}{4em}{Diabetes}  & LR & $ 1.0 \pm 0.0 $ & $ 0.601 \pm 0.0 $ & $ 0.0 \pm 0.0 $ & $ 0.0013 \pm 0.0011 $\\ &  & DT & $ 0.9924 \pm 0.0309 $ & $ 0.5161 \pm 0.011 $ & $ 0.0006 \pm 0.0005 $ & $ 0.0013 \pm 0.0011 $\\ &  & XGB & $ 0.9212 \pm 0.0581 $ & $ 0.5748 \pm 0.0057 $ & $ 0.0036 \pm 0.0028 $ & $ 0.0013 \pm 0.0011 $\\  \cdashline{3-7}   & \multirow{3}{4em}{COMPAS}  & LR & $ 0.8047 \pm 0.0275 $ & $ 0.892 \pm 0.0094 $ & $ 0.323 \pm 0.0116 $ & $ 0.0296 \pm 0.006 $\\ &  & DT & $ 0.9212 \pm 0.1419 $ & $ 0.6431 \pm 0.0979 $ & $ 0.0738 \pm 0.0634 $ & $ 0.0296 \pm 0.006 $\\ &  & XGB & $ 0.7556 \pm 0.0383 $ & $ 0.8724 \pm 0.0184 $ & $ 0.2899 \pm 0.0268 $ & $ 0.0296 \pm 0.006 $\\ 
     % \hline 
    \end{tabular} 
\end{center}
\end{table}

\begin{table}[!htbp] 
\caption{($\mathcal{B}$ - Fair Predictions: regression) Similar table as Table \ref{table:pf_classification} is provided for the regression task on the Communities and Crimes Dataset.} \label{table:pf_regression}
\begin{center} 
\begin{tabular}{llllll} $\eta$ & \bf Model & \bf KS & \bf MSE & $Cov^{2}(r(X),\Hat{Y})$ & $Cov^{2}(r(X),S)$ \\ \hline \\
\multirow{3}{1em}{$0.0$}  & LR & $ 0.8223 \pm 0.0349 $ & $ 0.0341 \pm 0.0046 $ & $ 0.658 \pm 0.2337 $ & $ 0.3462 \pm 0.1179 $\\  & DT & $ 0.6653 \pm 0.0695 $ & $ 0.0401 \pm 0.0068 $ & $ 0.4062 \pm 0.0985 $ & $ 0.3462 \pm 0.1179 $\\  & XGB & $ 0.7568 \pm 0.0259 $ & $ 0.0254 \pm 0.003 $ & $ 0.4376 \pm 0.1382 $ & $ 0.3462 \pm 0.1179 $\\  \cdashline{2-6}  \multirow{3}{1em}{$1.0$}  & LR & $ 0.7047 \pm 0.0594 $ & $ 0.0278 \pm 0.0032 $ & $ 0.2645 \pm 0.0711 $ & $ 0.2342 \pm 0.1327 $\\  & DT & $ 0.5395 \pm 0.0549 $ & $ 0.0509 \pm 0.0062 $ & $ 0.2534 \pm 0.0624 $ & $ 0.2342 \pm 0.1327 $\\  & XGB & $ 0.6668 \pm 0.0538 $ & $ 0.032 \pm 0.0037 $ & $ 0.2337 \pm 0.0571 $ & $ 0.2342 \pm 0.1327 $\\  \cdashline{2-6} \multirow{3}{1em}{$2.0$}  & LR & $ 0.7059 \pm 0.0595 $ & $ 0.0277 \pm 0.0032 $ & $ 0.2652 \pm 0.0706 $ & $ 0.2345 \pm 0.1267 $\\  & DT & $ 0.5934 \pm 0.0813 $ & $ 0.057 \pm 0.0133 $ & $ 0.2442 \pm 0.0973 $ & $ 0.2345 \pm 0.1267 $\\  & XGB & $ 0.6666 \pm 0.0348 $ & $ 0.0325 \pm 0.0025 $ & $ 0.2435 \pm 0.0558 $ & $ 0.2345 \pm 0.1267 $\\  
% \cdashline{2-6} 
% \hline 
\end{tabular} 
\end{center}
\end{table}


\begin{figure}[!htbp]
\caption{($\mathcal{B}$ - Fair Predictions) Similar Figure as \ref{im:fair_predictions_B1} for Law School and COMPAS datasets. In this case, we measured the fairness of the predictions made  with the new representation in terms of the Equality of Opportunity (EOpp), which is represented versus the Accuracy. EOpp is estimated as the ratio $\hat{P}(c(X) = 1 | S = 0, Y = 1) / \hat{P}(c(X) = 1 | S = 1, Y = 1)$ }

\label{im:fair_predictions_B2}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/law_EO.pdf}
    \label{im:ab21}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/law_DT_Pareto_EO.pdf}
    \label{im:ab22}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/law_XGB_Pareto_EO.pdf}
    \label{im:ab23}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/compas_EO.pdf}
        \label{im:ab24}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/compas_DT_Pareto_EO.pdf}
        \label{im:ab25}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/compas_XGB_Pareto_EO.pdf}
        \label{im:an26}
    \end{subfigure}
\end{figure}


\newpage



\subsection{Comparison with Vanilla Fair PLS}
% 
We compared our proposed algorithm to the state-of-the-art method Vanilla Fair PLS \cite{champion2023human}. 
% 
The Vanilla PLS method consists in selecting the features that are related to the target (PLS) such that the correlation with the sensitive ones is below a predefined threshold $\tau$.
% 
In other words, it is a naive strategy that directly make use of the latent variables $(\mathbf{t}_{1}, ..., \mathbf{t}_{k})$ generated with the standard PLS technique which are highly correlated with the outcome $\mathbf{Y}$ to impose fairness. 
% 
This methodology is based on the conditional marginal distribution of those components to the sensitive variable $S$. 
% 
In contrast, our formulation aims to obtain components that seek a balance between being target-related ($\eta$ small) and being independent with respect to the sensitive attribute ($\eta$ large enough). 
% 
The left column of Figure \ref{im:comparison_vanilla} shows the behaviour of $\mathbf{C}_{r(\mathbf{X}), \mathbf{Y}}^{2}$ for new representations $r(\mathbf{X})$ obtained with the Vanilla PLS procedures. It is clear that, on the one hand, for $\tau$ close to 0 the representation has no features ($k=0$). Moreover, as $\tau$ increases, the number of features in the representation also increases, while there is no trade-off between covariance with respect to the target $Y$, nor with respect to the sensitive feature.
% 
This is, the $\mathbf{C}_{r(\mathbf{X}), \mathbf{Y}}^{2}$ increases but the $\mathbf{C}_{r(\mathbf{X}), S}^{2}$ also increases, therefore fairness goal is not achieved. In contrast, this is not the case with Fair PLS as shown in the right column of Figure \ref{im:comparison_vanilla}
% 
\begin{figure}[!htbp]
\caption{Comparison of the covariance with respect to the target $Y$ and the sensitive attribute $S$ between the new representation $r(\mathbf{X})$ obtained via the Vanilla Fair PLS and our proposed formulation. The plots display the mean and standard deviation resulting from a 5-fold cross-validation procedure. For $\tau < 0.2$, $r(\mathbf{X}) \in \mathbb{R}^{1000 \times 0}$; for $0.2 \leq \tau < 0.6$, $r(\mathbf{X}) \in \mathbb{R}^{1000 \times 6}$ and for $0.6 \leq \tau < 1.0$, $r(\mathbf{X}) \in \mathbb{R}^{1000 \times 7}$. The data used for this analysis is described in Appendix B. 1. - Synthetic dataset.}
\label{im:comparison_vanilla}
\centering
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/naive_fairness_3.pdf}
    \label{im:cv1}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/fairness_3.pdf}
        \label{im:cv2}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/naive_utility_3.pdf}
        \label{im:cv3}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\textwidth]{img/utility_3.pdf}
        \label{im:cv4}
    \end{subfigure}
\end{figure}


\subsection{Comparison with existing methods for Fair PCA}
% 
We compared our proposed algorithm by means of bias mitigation to the state-of-the-art Fair PCA method introduced by \citet{pmlr-v206-kleindessner23a}. 
% 
First, we demonstrated that our method, like the aforementioned, manage to equalise the conditional means of the groups (see Figure \ref{im:illustration_fair_PLS}).
% 
This is because the condition on the weights that states $Cov(\mathbf{w}^{\mathsf{T}} \mathbf{x}_{i}, s_{i}) = 0, \ \forall i \in [n]$, is equivalent to finding the optimal projection such that the group-conditional means of the projected data align.

\begin{figure}[!htbp]
    \centering
    \caption{Results of applying the Fair PLS method to the synthetic dataset from \citet{pmlr-v206-kleindessner23a}. Points in red color red are from group $S=1$ and in blue color from group $S=0$.}
    \label{im:illustration_fair_PLS}
    \includegraphics[width=.3\textwidth]{img/illustration_fair_PLS.pdf}%\hfill
\end{figure}
% 

% 
Secondly, we compared the performance of the two dimensional reduction techniques for Fair Representation Learning, Fair PCA \citep{pmlr-v206-kleindessner23a} and Fair PLS (ours), using the Adult Income dataset. 
% 
This comparison consists on learning a fair classifier (Logistic Regression or Decision Tree Classifier) on top of the representation and measuring the classifier's predictive performance.
% 
While both methods achieve full fairness in terms of Demographic Parity ($DI = 1$) when being applied as preprocessing methods, the predictive performance of Fair PCA is much lower than with Fair PLS (see Figure \ref{im:comparison_fpca}). Precisely, the mean accuracy for 7-fold CV is $0.7649$ for the Logistic Regression and $0.7480$ for the Decision Tree Classifier, while for Fair-PLS is higher than $0.8300$ and $0.7800$, respetively.
% 
\begin{figure}[!htbp]
\centering
\caption{Comparison between the performance of Fair PCA and Fair PLS using Adult Income dataset. The points are test values of a 7-fold CV procedure. We have fixed $k = 2$ for both methodologies.}
\label{im:comparison_fpca}
    \begin{subfigure}[b]{0.35\textwidth}
        \includegraphics[width=\textwidth]{img/adult_LR_Pareto_comparison_fpca.pdf}
    \label{im:comfpca1}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \includegraphics[width=\textwidth]{img/adult_DT_Pareto_comparison_fpca.pdf}
        \label{im:comfpca2}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/lengend_comparison_fpca.pdf}
        \label{im:comfpca3}
    \end{subfigure}
\end{figure}

% 
Notice that PCA works well because the orthogonality of the singular vectors eliminates the multicolinearity problem.
% 
But the optimum subset of components were originally chosen to explain $\mathbf{X}$ rather than $Y$, and so, nothing guarantees that the principal components, which ‘explain’ $\mathbf{X}$ optimally, will be relevant for the prediction of $Y$.
% 
The PCA unsupervised dimensionality reduction technique is based on the covariance matrix $\mathbf{X}^{\mathsf{T}} \mathbf{X}$. 
% 
Nevertheless, in many applications it is important to weight the covariance matrix, this is, to replace $\mathbf{X}^{\mathsf{T}} \mathbf{X}$ with $\mathbf{X}^{\mathsf{T}} \mathbf{V} \mathbf{X}$, being $\mathbf{V}$ a positive definite matrix.
% 
PLS algorithm choose as $\mathbf{V}$ the representative matrix for the "size" of the data in the $\mathbf{Y}$ matrix, which is $\mathbf{V} = \mathbf{Y} \mathbf{Y}^{\mathsf{T}}$.



\subsection{Runtime comparison}
% 
We tested our method in terms of running time of training with different data dimension and compared it with the standard PLS implementation of Scikit-learn. 
% 
We used the data for this study provided by  \citet{lee2022fairpca} as Synthetic data \#2
% 
In detail, the dataset is composed of two groups, each of half of the size $n$ and sampled from two different $3$-variate normal distributions.
% 
\begin{figure}[!htbp]
    \caption{
    The runtime of the standard method, as implemented in \citep{scikit-learn}, is shown as a function of the data dimension in blue, while the corresponding runtime of the Fair PLS method is depicted in orange. The target dimension is fixed to 1. }
    \label{im:appendix_running_time}
    \centering
    \includegraphics[width=.5\textwidth]{img/running_time.pdf}%\hfill
\end{figure}















\end{document}
