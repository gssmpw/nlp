\section{Papers List - Delete after Rewrite}

\iffalse
\begin{table*}
    \centering
    \begin{tabular}{|l|l|lllll|l|}
        \hline
         \textbf{Paper} & \textbf{Model} & \textbf{XAI} & \textbf{Type} &  \textbf{Dependency} & \textbf{Scope} & \textbf{Output} & \textbf{Main HCE} \\
        \hline        \cite{sivaraman2023ignore} & XGBoost & SHAP & Post Hoc  & Agnostic  &  Local  &  Numerical & Think Aloud \\
        
        \cite{barda2020qualitative} & Random Forest & SHAP & Post Hoc & Agnostic & Local & Numerical  & Focus Group  \\
        
        \cite{brennan2019comparing} & Many & & Intrinsic & Specific & Local & Numerical & Think Aloud  \\
        
        \cite{matthiesen2021clinician} & Random Forest & LIME & Post Hoc & Agnostic & Local & Visual & Interviews  \\
        
        \cite{ellenrieder2023promoting} & DenseNet & Heatmap & Post hoc & Agnostic & Local & Visual & Think Aloud  \\
        
        %\cite{diprose2020physician} & Post Hoc & Agnostic & Mixed & Numerical & Survey & Positive \\
        \cite{hwang2022clinical} & TRIER (CNN) & Saliency & Intrinsic & Specific & Local & Visual & Mixed  \\
        
        \cite{calisto2022breastscreening} & DenseNet & Heatmap &   Intrinsic & Specific & Local & Visual & Interviews  \\
        
        \cite{kumarakulasinghe2020evaluating} & Many & LIME & Post Hoc & Agnostic & Local & Numerical & Survey  \\
        
        \cite{kovalchuk2022three} & Many & SHAP & Post Hoc & Agnostic & Mixed & Numerical & Mixed  \\ 
        
        \cite{kyrimi2020incremental} & Bayesian Network &  & Intrinsic & Specific & Local & Numerical & Survey  \\ 
        
        \cite{sabol2020explainable} & CFCMC &  & Intrinsic & Specific & Local & Visual & Survey  \\ 
        
        \cite{rajashekar2024human} & Random Forest & ICE, ALE & Post Hoc & Agnostic & Global & Numerical & Mixed  \\
        
        \cite{zhang2024rethinking} & LSTM & Attn Weights & Intrinsic & Agnostic & Local & Numerical & Interviews  \\ 
        
        \cite{kim2022prediction}  & XGBoost & SHAP & Post Hoc & Agnostic & Local & Numerical & Interviews  \\
        
        \cite{pumplun2023bringing} & DenseNet & Heatmap & Post Hoc & Agnostic & Local & Visual & Survey  \\ 

        \cite{singh2021evaluation} & Inception-v-3 & Many & Post Hoc & Agnostic & Local & Visual & Survey \\

        \cite{neves2021interpretable} & Many & Many & Post Hoc & Agnostic & Local & Visual & Survey \\
        
        \hline
    \end{tabular}
    \caption{Summary table of papers}
    \label{tab:plain}
\end{table*}
\fi









% % XAI: Type: Post Hoc, Dependency: Model-Agnostic, Scope: Local 
% HCI: Evaluation: AP, Method: Think Aloud
% HC: Field: Clinical Medicine, Internal Medicine 
\cite{sivaraman2023ignore} examined clinicians' interactions with an AI-based interpretable treatment recommender in the intensive care unit (ICU) focusing on treating sepsis. The system was built using XGBoost, and SHAP as feature explainer. 
Through a think-aloud study involving 24 ICU clinicians, the analysis revealed that while explanations improved clinicians' perceptions of the AI's usefulness in their decisions, explanations did not significantly adjust their overall rates of concordance with the AI recommendations. In particular, the study identified four distinct behavior patterns among clinicians: Ignore, Negotiate, Consider, and Rely. Specifically, in the Negotiate group, clinicians displayed a tendency to weigh and prioritize aspects of the AI recommendations, suggesting that treatment decisions in the ICU may involve partial reliance on AI recommendations, with potential implications for the effectiveness of chosen treatments. Overall, clinicians who found the AI recommendations most useful saw it as additional evidence alongside their assessments.


% XAI: Type: Post Hoc, Dependency: Model Agnostic, Scope: Local 
% HCI: Evaluation: AP, Method: Interviews - Focus Groups  
% HC: Field: Field: Clinical Medicine, Internal Medicine & Allied Health (nurses)
% WHERE: ICU (pediatric), PERCEPTION: positive, EXPLANATIONS TYPE: feature-based (SHAP), METHODS: standard ML models, with features explanations on the best model, focus groups later, DATA: PICU admission at the Children’s Hospital of Pittsburgh (CHP) between January 1, 2015 and December 31, 2016 - 422 features, STAKEHOLDERS: focus group: pediatric critical care providers of differing clinical expertise (e.g., nurses, residents, fellows, attending physicians) was recruited through professional connections of one of the authors (CMH), children hospitalized 
\cite{barda2020qualitative} discussed the development of a framework for designing user-centered displays of explanations for ML models such as logistic regression and random forest used in healthcare. 
%The solution was informed by previously published theoretical frameworks and refined through qualitative inquiries and design review sessions with critical care nurses and physicians. 
They focused on creating explanation displays for predictions from a pediatric intensive care unit (PICU) in-hospital mortality risk model using a model-agnostic approach based on SHAP feature values. 
Feedback from focus group sessions indicated that the proposed displays were perceived as useful tools for assessing model predictions. However, preferences for specific display features varied among clinical roles and levels of predictive modeling knowledge. 
Providers preferred displays that required less cognitive effort and could accommodate diverse user information needs. Incorporating supporting information to aid interpretation was deemed crucial for fostering provider understanding and acceptance of predictions and explanations. 


% XAI: Type: Intrinsic, Dependency: Model Specific, Scope: Local 
% HCI: Evaluation: AP, Method: Think-Aloud  
% HC: Field: Clinical Medicine, Internal Medicine 
% WHERE: surgery, PERCEPTION: positive, EXPLANATIONS TYPE: feature-based: most important features for a patient risk derived based on difference from “average”-patient risk. METHODS: model: 2 steps algo: (1) patient-specific risk scores -> patient mortality prediction: logistic model, (2) random forest. HCI-part: think aloud study,  DATA: EHR from Bihorac et al., STAKEHOLDERS: physicians, patients 
~\cite{brennan2019comparing} validated the usability of \textit{MyRiskSurgery}, an analytical framework based on generalized linear models and random forests to estimate postoperative risk complications, as a pilot study for a real application. Given the  box nature of random forests, MyRiskSurgery shows the top 3 features contribution in making predicitons as explainability component. 
Through think-aloud studies, a set of physicians evaluated risk complications for 8-10 individual cases, with the system displaying the top 3 contributing features for each prediction. As a result, physicians experienced notable improvements in their risk assessment abilities for acute kidney injury and intensive care unit admissions. This led to a substantial net improvement in reclassification rates, with increases of 12\% and 16\% respectively. In terms of adoption, physicians found the algorithm user-friendly and beneficial.



% XAI: Type: , Dependency: , Scope:
% HCI: Evaluation: , Method:  
% HC: Field: 
% WHERE: cardiology, PERCEPTION: positive, EXPLANATIONS TYPE: feature-based (LIME), box RF model, METHOD: random forest, qualitative interviews DATA: self-data, STAKEHOLDERS: patients, cardiologists: electrophysiologic, nurses, technicians 
~\cite{matthiesen2021clinician} conducted a human study in the application of a system being able to output a risk prediction for ventricular fibrillation. They deliberately organized the study into three phases: (1) observations were conducted at the clinic to comprehend the clinical workflow, (2) development of the AI algorithm (random forest), and (3) feasibility and and qualitative interview study. In phase 2, they adopted a random forest model to predict a risk score of disease, showing the top/bottom 5 most important features through LIME. They found that, although many clinicians did not change their decision, they claimed that having a prediction tool could increase confidence in their actions. Also, the visualization of key features was useful, as the presentation of key features facilitates explainability regarding factors contributing to risk of ventricular fibrillation. 




% WHERE: radiology, PERCEPTION: positive, EXPLANATIONS TYPE: visual(heatmaps), METHOD: ???, DATA: BraTS20, STAKEHOLDERS: patients, radiologists  
\cite{ellenrieder2023promoting} framed the impact of CDSSs in decision-making as a human learning problem, extending the findings from a prior trained DenseNet model developed with ~\cite{pumplun2023bringing}, who effectively implemented the integrated gradients approach to produce heatmaps from a DenseNet showing image regions that increase cancer malignancy probabilities~\cite{sundararajan2017axiomatic}. 
Working with radiologists in scanning brain tumors, the authors conducted a between-subjects study in which radiologists were provided with differently performing CDSSs as follows: non-explainable high-performing (NEHP), explainable high-performing(EHP), non-explainable low-performing (NELP), and explainable low-performing (ELP). The authors checked whether radiologist could improve their detection accuracy after being "trained" with the CDSSs. Explainability was in the form of confidence heatmaps on mRI scans.  They found that the EHP model led to better-informed decision making, but also that false learning is advanced by interacting with NELP. 






% % WHERE: sleeping, PERCEPTION: positive, DATA: ISRUC-Sleep Dataset, EXPLANATIONS TYPE: visual from CNNs, METHOD: CNNs TRIER, STAKEHOLDERS: sleep technicians 
\cite{hwang2022clinical} presented an AI-driven CDSS to aid polysomnographic technicians in reviewing AI-generated sleep staging results. The authors developed the study in three phases. Firstly, the authors interviewed sleep technicians asking them why they would need explanations in the CDSS system. Secondly, the authors performed a user observation study to understand the sleep staging conventions of clinical practitioners. Thirdly, as a result of technicians' positive feedback, and monitored by a single sleep technician, a CDSS based on TRIER was developed, with convolutional filters, saliency values, and intermediate activation, as sources of information for generating explanations. 


% WHERE: oncology (breast cancer), PERCEPTION: positive, DATA: , EXPLANATIONS TYPE: visual (heatmaps), METHOD: CNNs DenseNet, within-subject, DATA:  STAKEHOLDERS: oncologists 
%\cite{calisto2022breastscreening} introduced \textit{BreastScreening-AI}, a system for classifying breast images using deep learning. The study developed on two scenarios: a no-AI assistant and an AI assistant based on a DenseNet, adopting visual heatmaps as the explainability factor to highlight severity regions. Using a within-subjects design to compare the performance of the two scenarios, the study aims to understand how 45 clinicians interact with the tool, their receptiveness to AI assistance, and the impact of AI on their work. The evaluation involves selecting patients with varying severities and analyzing results qualitatively and quantitatively. Results showed that the Clinician-AI scenario outperforms the Clinician-Only scenario, with a decrease in false positives by 27\% and false negatives by 4\%. Also, the study suggests that the design techniques, including the explainability factor, positively impact clinicians' expectations and satisfaction.


% WHERE: sepsis, PERCEPTION: mainly positive, EXPLANATIONS TYPE: feature-based LIME, METHOD: random forest, survey, DATA: Physionet dataset, STAKEHOLDERS: physicians, patients (not mentioned) 
\cite{kumarakulasinghe2020evaluating} conducted a study analyzing the rate of agreement between physicians and LIME in terms of feature importance in a sepsis/non-sepsis prediction experiment. Firstly, the authors trained several models such as random forest, Adaboost and support vector machines, and applied the LIME framework on the best performing one (random forest). Secondly, physicians were asked to (1) agree with the model whether the patient presented sepsis based on individual features, and (2) blindly rank the top 3 features the model used to make predictions without accessing LIME explanations. Next, LIME explanations were revealed and physicians were asked to rate their satisfaction with those on a 5-stars Likert scale. Results indicated that physicians agreed with model predictions (87\%), and that LIME explanations are equal to the physicians' explanations in at least two features for all patients (69\%). Also, physicians were satisfied with LIME predictions (78\%), and 68\% presented a positive attitude towards LIME in terms of trust. 


% WHERE: diabetes, PERCEPTION: mainly positive, EXPLANATIONS TYPE: feature-based SHAP, METHOD: many white box ML models, surveys- user questionnaires, DATA: internal, STAKEHOLDERS: physicians, patients (not mentioned) 
\cite{kovalchuk2022three} proposed a three-staged clinical framework for CDSS implementation as follows: (1) \textit{Basic Reasoning}, \textit{i.e.}, implementing healthcare practices by referencing existing policies to guide decision-making, (2) \textit{Data-Driven Predictive Modeling}, \textit{i.e.}, applying machine learning models in the context of the previous stage, and (3) \textit{Explanation}, in which XAI techniques are applied in support of the earlier stages to enhance HCI to provide better recommendations. 
The authors tested the framework in a diabetes mellitus case study. In the first stage, expert endocrinologists guided the selection of insulin resistance predictors. In the second stage, a predictive model was optimized to estimate the risk. Finally, in the third stage, XAI SHAP values were employed to interpret the CDSS results, which were then validated using physician questionnaires. Perceptions from physicians were generally positive, with an agreement rate with the system of 87\%. 




% WHERE: coagulopathy, PERCEPTION: mixed, lack of trust, EXPLANATIONS TYPE: textual: from BN output, METHOD: Bayesian Networks, surveys- user questionnaires, DATA: internal, STAKEHOLDERS: physicians, patients (not mentioned) 
\cite{kyrimi2020incremental} examined how explanations from Bayesian Networks could be used to understand how predictions were generated. They tested how their developed method could be used to predict coagulopathy in the first 10 minutes of hospital care. The tool outputted textual explanations ranking the predictors that supported a high risk of coagulopathy. In a before-after questionnaire, a group of clinicians rated their perception of the system: the method produced explanations that generally aligned with clinicians' identified evidence, though clinicians found them wordy and not significantly impactful on trust or decision-making.



% WHERE: oncology, PERCEPTION: positive ((indicated higher acceptability and trust in the X-CFCMC system compared to the plain CNN), EXPLANATIONS TYPE: mixed (Semantic explanations Visualizations of training images responsible for predictionsVisualizations of other tissue types), METHOD: Cumulative Fuzzy Class Membership Criterion CFCMC, CNN, within-subject experiment, DATA: kather et al. 2016, STAKEHOLDERS: doctors (pathologists)
\cite{sabol2020explainable} enhanced the explainability and performance of the Cumulative Fuzzy Class Membership Criterion (CFCMC) classifier for classifying eight tissue types from histopathological cancer images. By utilizing a fine-tuned CNN as a feature extractor, they improved the CFCMC classifier's accuracy on colorectal image data. They introduced the factor of misclassification (FoM) to estimate misclassification probabilities and defined a certainty threshold to determine prediction certainty. The proposed uncertainty measure, distinct from other models, aids in identifying possible misclassification classes. Two segmentation systems for whole-slide histopathological images were developed: one using a standalone CNN and the other using the X-CFCMC classifier using CNN extracted features, which provides explanations, visualizations of training images, and other tissue types. 
Acceptability tests developed as within-subject experimental designs with 14 pathologists showed higher acceptability and trust in the X-CFCMC system compared to a plain CNN. They conclude that explainable classifiers improve AI usability and reliability in medical settings, with future plans to expand clinical trials and test the classifier on imperfect data.

\cite{rajashekar2024human} developed GutGPT, a system tailored to provide a natural language guidance interface for gastrointestinal bleeding prediction, including hospital-based intervention. This system integrates LLMs interface, with a standard random forest model to predict the risk score. Then, interpretability plots such as individual conditional expectations and accumulated local effects are used to explain feature-wise effects on predicted scores. By interacting with human text, GutGPT leverages in-context learning to present easy-to-understand natural language explanations alongside XAI explanations for predictions. 
System usability was measured by means of interviews and surveys post hoc, finding that perceptions were mostly positive. 


\cite{zhang2024rethinking} showed that existing AI-driven sepsis risk predictors do not support decision-making, as they predict a final outcome without really focusing on intermediate steps and on easy accessible explanations. To address this, they developed SepsisLab, a system that predicts ahead-of-time future sepsis diagnosis, embedding explanations. SepsisLab is based on a LSTM model and is capable of predicting sepsis risk 4 hours ahead. Before, an attention module was deployed to generate a fixed-length vector that was fed as input to the LSTM to select important variables, with attention weights to be used as the feature's importance score. Besides, a counterfactual module is added to provide further transparency. Next, researchers validated the system using semi-structured interviews, with practitioners expressing a positive interest seeing the tool as a collaborator rather than a competitor. 


\cite{kim2022prediction} developed MindScope, a stress level tracker based on a personalized ML system trained on an XGBoost, using smartphone usage data including GPS information, app usage, screen status etc. Researchers measured feature importance using SHAP values with three different explanation settings: (1) no explanations, (2) broad explanations, and (3) detailed explanations. Next, interviews were conducted to gather user feedback on the use experience, and algorithmic perception, particularly focusing on the type of preferred explanations. Here, it was found that the explanation-based settings were more useful than no explanations. However, detailed explanations were perceived as equally useful as broad explanations. 

\cite{singh2021evaluation} evaluated 13 types of different visual explanation methods for an Inception-v-3 model trained to diagnose eye retinal diseases, and surveyed clinical physicians about which kind of explanation method would be the optimal one. They found that an attribution method based on Taylor expansions (DeepTaylor) was rated highest (3.85/5), followed by Guided Backprogagation (3.29/5), and SHAP (3.26/5). Perceptions of the usability were mixed.  

\cite{neves2021interpretable} analyzed how ECG (time series data) could be interpreted by machine learning algorithms such as CNN and KNN. In particular, they introduced a high-level conceptual framework for visually explainable time series data using tools such as LIME. With a user study, clinicians were surveyed about the perceived usefulness of the system, finding that explanations could be detrimental. 