%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems}


% Single author syntax
\iffalse
\author{
    Author Name
    \affiliations
    Affiliation
    \emails
    email@example.com
}
\fi

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)

\author{
Alessandro Gambetti $^{1,2}$
\and
Qiwei Han$^2$\and
Hong Shen$^3$\And
Cl\'audia Soares$^1$\\
\affiliations
$^1$Nova School of Science and Technology\\
$^2$Nova School of Business and Economics \\
$^3$Carnegie Mellon University \\
%$^4$Fourth Affiliation\\
\emails
\{gambetti.alessandro, qiwei.han\}@novasbe.pt,
hongs@andrew.cmu.edu,
claudia.soares@fct.unl.pt
}


\begin{document}

\maketitle

\begin{abstract}
Explainable AI (XAI) has become a crucial component of Clinical Decision Support Systems (CDSS) to enhance transparency, trust, and clinical adoption. However, while many XAI methods have been proposed, their effectiveness in real-world medical settings remains underexplored.
This paper provides a survey of human-centered evaluations of Explainable AI methods in Clinical Decision Support Systems. By categorizing existing works based on XAI methodologies, evaluation frameworks, and clinical adoption challenges, we offer a structured understanding of the landscape. Our findings reveal key challenges in the integration of XAI into healthcare workflows and propose a structured framework to align the evaluation methods of XAI with the clinical needs of stakeholders.
\end{abstract}

\section{Introduction}

% 1. general AI introduction
% 2. AI the medical field, include sentence on public funding 
% 3. introduce the XAI concept. refer to literature review from loh et al. highlight the missing of HCI component gap 
% 4. say how this paper closes this gap 
% 5, present a short overview of this paper and how it is structured 

Artificial Intelligence (AI) is increasingly being integrated into Clinical Decision Support Systems (CDSS) to improve timely diagnosis, optimize treatment plans, and reduce healthcare costs \cite{sendak2020human}. Estimates suggest that AI adoption in healthcare could reduce U.S. healthcare expenditures by 5-10\%, translating to \$200–\$360 billion in annual savings while maintaining quality and access \cite{sahni2023potential}. The rapid growth of AI-driven CDSS has also attracted significant investment, with venture capital funding in AI in health exceeding \$11 billion in 2024 \cite{goldsack2024healthcareai}. These trends indicate that the role of AI in medical decision making will continue to expand.

However, a key challenge accompanying this expansion is the necessity for trust and transparency in AI-driven medical decisions. Despite advancements in deep learning and other black-box AI models, clinicians often hesitate to adopt AI-generated recommendations due to their lack of interpretability \cite{hou2024physician}. Trust in CDSS is critical because errors in automated decision-making can have life-threatening consequences \cite{zhang2023ethics}. In response, Explainable AI (XAI) has emerged as a solution to improve model transparency, inderectly allowing medical professionals to better understand why an AI system makes certain predictions \cite{loh2022application}. Additionally, regulatory frameworks, such as the General Data Protection Regulation (GDPR), emphasize the need for explainability in automated decision-making, thereby necessitating interpretable AI solutions in healthcare. %\cite{GDPR2016a}.

% Alessandro: I changed bharati2023 to loh2023 as they basically do the same
Despite extensive research in XAI for healthcare \cite{loh2022application}, a critical gap remains: there is limited systematic analysis of how XAI explanations impact real-world clinical adoption and decision-making \cite{ghassemi2021false}. While prior work has primarily focused on developing new XAI techniques, fewer studies have systematically examined whether these methods effectively support clinicians in practice \cite{amann2022explain}. Furthermore, the evaluation of XAI in healthcare is highly fragmented, relying on inconsistent methodologies that range from proxy simulations to real-world clinical trials \cite{doshi2017towards}. This raises a fundamental question: How should XAI techniques be evaluated to ensure their usability, trustworthiness, and effectiveness in real clinical settings?

% original 
%To address this gap, this paper presents a systematic survey of human-centered evaluations of XAI methods in CDSS, focusing on the intersection of explainability, usability, and clinical decision-making. Specifically, we: (1) categorize existing works based on XAI methodologies, evaluation frameworks, and clinical adoption challenges, offering a structured synthesis of the field; (2) identify key gaps in current evaluation practices, highlighting inconsistencies in how human-centered XAI is assessed in CDSS; (3) introduce a conceptual framework to bridge the socio-technical gap in AI-driven CDSS by aligning XAI evaluation methods with the needs of diverse stakeholders (\textit{e.g.}, clinicians, hospital administrators, AI developers); (4) analyze clinician perceptions of XAI-based CDSS, revealing that while XAI is generally well-received, significant challenges remain—such as high cognitive load, misalignment with clinical knowledge, and barriers to adoption; (5) propose a medical field-agnostic, stakeholder-centric approach for CDSS development, advocating that CDSS should be designed as human-augmentation tools rather than replacements.

% modified 
To address this gap, this paper presents a systematic survey of human-centered evaluations of XAI methods in CDSS, focusing on the intersection of explainability, usability, and clinical decision-making. Specifically, we: 
(1) categorize existing works based on XAI methodologies, evaluation frameworks, and clinical adoption challenges, offering a structured synthesis of the field; 
(2) identify key gaps in current evaluation practices from clinicians perceptions of XAI-based CDSS, revealing that while XAI is generally well-received, significant challenges remain—such as high cognitive load, misalignment with clinical knowledge, and lack of early involvement with stakeholders; 
(3) introduce a conceptual framework to bridge the socio-technical gap in AI-driven CDSS by aligning XAI evaluation methods with the needs of diverse stakeholders (\textit{e.g.}, clinicians, hospital administrators, AI developers); 
%(4) analyze clinician perceptions of XAI-based CDSS, revealing that while XAI is generally well-received, significant challenges remain—such as high cognitive load, misalignment with clinical knowledge, and barriers to adoption; 
(4) propose a medical field-agnostic, stakeholder-centric approach for CDSS development, advocating that CDSS should be designed as human-augmentation tools rather than replacements.






To theoretically ground this misalignment between human expectations and technological capabilities, we leverage Ackerman’s socio-technical gap theory, which highlights the challenges arising from such discrepancies \cite{ackerman2000intellectual}. This analysis informs our proposed framework, emphasizing stakeholder-driven evaluations to ensure CDSS solutions are clinically relevant, interpretable, and effectively integrated into healthcare workflows. 
%By aligning technical advancements in XAI with rigorous human-centered evaluations, we aim to develop trustworthy, deployable CDSS solutions.
By aligning technical advancements in XAI with rigorous human-centered evaluations, we aim to steer research toward developing trustworthy, deployable CDSS solutions.

%This paper provides a systematic survey of human-centered evaluations of XAI methods in CDSS. By categorizing existing works based on XAI methodologies, evaluation frameworks, and clinical adoption challenges, we offer a structured understanding of the landscape. Furthermore, we introduce a \textit{conceptual taxonomy framework} to bridge the socio-technical gap in AI-driven CDSS by aligning XAI evaluation methods with the needs of diverse stakeholders   \cite{speith2022review}. As a result, we found that XAI-based CDSS clinicians' perceptions were generally favorable. However, incorporating XAI still poses significant challenges as, for example, XAI methods may demand significant cognitive load to be mentally processed, and that such methods may not be informative if not aligned with clinical knowledge, thus preventing clinical adoption. We then theoretically evaluate this adoption gap by leveraging the Ackerman's socio-technical gap theory, which highlights the gap between human requirements, including expectations, and technological capabilities of any system \cite{ackerman2000intellectual}. Bridging from this theory, we then propose a medical field-agnostic, stakeholder-centric approach for CDSS development, underscoring that it is possible to reduce that gap by on-boarding all stakeholders from the early phases to align their requirements, eventually suggesting that CDSS should serve as human-augmentation tools rather than replacement tools. This survey not only synthesizes existing research, but also provides actionable insights for developing stakeholder-driven, clinically relevant, and practically deployable XAI solutions.

%and its potential to be successfully implemented in actual healthcare environments is limited, high. 
 


We organize the paper as follows. Section \ref{sec:background} provides background on CDSS, XAI methods, and human-centered evaluations. Section \ref{sec:papers_list} presents a structured review of existing works in this field, categorizing them based on XAI methodologies, evaluation frameworks, and clinical adoption challenges. Section \ref{sec:challenges} discusses the socio-technical challenges and proposes a conceptual framework to bridge the gap between XAI methodologies and stakeholder needs, concluding pointing out future research directions. 

%Through mapping the domain of research studies that unite explainability and human-centered evaluation in healthcare, this survey aims to take a snapshot of ongoing contributions, identify prevailing trends, and outline areas requiring additional research.

%Although current research has extensively examined the use of XAI in Clinical Decision Support Systems (CDSS), 

%\cite{loh2022application} provided a literature review on XAI in healthcare, but the HCI component is missing. 



\section{Background} \label{sec:background}


\subsection{Clinical Decision Support Systems (CDSS)}
CDSS have long been integral to medical decision-making, assisting clinicians in enhancing patient outcomes. Initially emerging in the late 1950s with rule-based systems, CDSS evolved significantly with advancements in AI. Modern CDSS now leverage sophisticated machine learning algorithms to process vast datasets efficiently. Concurrently, the rise of XAI has enabled the integration of interpretable solutions, essential for fostering trust and promoting the adoption of CDSS in clinical practice \cite{ghassemi2021false}.

%~\cite{poncette2020improvements}.
%in the healthcare domain. 
%In light of this, regulations such as the European GDPR enforce the right of individuals to receive explanations for decisions made by automated systems, including AI CDSS applications in healthcare ~\cite{GDPR2016a}. 
%Therefore, besides respecting regulations, XAI solutions in CDSS emphasize accountability, and transparency, 

Operationally, CDSS utilize patient data to generate specific treatment recommendations~\cite{musen2021clinical}. The back-end employs machine learning algorithms trained for specific tasks, processing data from Electronic Health Records (EHRs). EHRs include both structured data (e.g., tabular data) and/or unstructured data (e.g., text, images), representing comprehensive digital versions of patients’ medical histories, including diagnoses, treatments, and test results. System performance is evaluated using machine learning metrics pertinent to the specific task. % discuss accuracy for those: depends on the task 
Synchronously, the front-end runs a user interface (UI) connected to the back-end that clinical practitioners are supposed to interact with. Such UI displays patient data, alerts, recommendations (sometimes with explanations), and decision-support reports in an accessible manner to facilitate clinical decision-making.
To this end, human-centered evaluations are conducted to assess the overall effectiveness and efficiency of the system in the specific task. However, researchers have identified that the absence of such evaluations is a primary factor in the lack of adoption of AI-based CDSS solutions ~\cite{musen2021clinical}.
%particularly due to insufficient consideration of clinicians’ workflows and the collaborative nature of clinical practice
%~\cite{kohli2018cad,musen2021clinical,wears2005computer,horsky2012interface}.
%Therefore, the use of AI urges CDSS developers to perform human-centered evaluations with stakeholders to assess the validity of the developed CDSS. 
%This collaboration not only boosts performance, subsequently implying successful clinical adoption, but also assures that all the ethical considerations are respected, \textit{i.e.}, that patients with the same conditions receive equal treatment. 
%Therefore, the integration of AI necessitates that CDSS developers conduct human-centered evaluations with stakeholders to validate the system during the whole development process. This collaboration not only boosts performance, facilitating clinical adoption, but also ensures adherence to ethical principles —for example, that patients with similar conditions receive equitable treatment.
%Therefore, to successfully integrate AI, CDSS developers should involve stakeholders in human-centered evaluations throughout the development process. This collaboration not only enhances performance and supports clinical adoption but also ensures ethical principles are upheld, such as providing equitable treatment to patients with similar conditions.


In summary, the integration of XAI together with human-centered evaluation assures that the system is usable within clinical workflows and more transparently meets the needs of stakeholders to foster acceptance of CDSS.

%forms a symbiotic relationship that fosters transparency and trust. The integration 
%assures that the system is usable within clinical workflows and meets the needs of stakeholders to foster acceptance of CDSS.




%The development pipeline can be divided into four iterative phases: (1) Problem identification, which implies the early definition and interaction of/with stakeholders (\textit{e.g.}, doctors, patients, developers, hospital, etc.) to set up the common goals, needs, and challenges, \textit{e.g.}, formative studies; (2) Data collection, to gather relevant high quality data, usually from Electronic Health Records (EHR), which are systematized collections of patients data in digital form - both structured (tabular data) or unstructured (images and text); (3) System development, which includes from development of (X)AI technology to user interface; and (4) System evaluation, to test the tool with stakeholders, usually by means of human-centered evaluations, in which stakeholders (doctors, nurses, technicians, etc.) are active participatory subjects that express their opinions and beliefs on the developed system. 





%%% ADD A  PARAGEAPH ON COSTS AND BENEFITS 



%Although CDSS may theoretically be autonomous in decision-making ~\cite{brynjolfsson2017can}, as they may reduce diagnosis costs and time ~\cite{garg2005effects}, such tools should augment, but not replace human practitioners. 

\subsection{Explainability and XAI Methods: a Taxonomy} \label{sec:XAI_methods}
From a psychological standpoint, humans seek explanations to predict future events, as explanations facilitate generalization~\cite{vasil2022explanations}. Unlike descriptions, explanations provide understanding by identifying ``difference-makers" in causal relationships. In medicine, where transparency and accountability are crucial, explanations may offer insights into the rationale behind AI-based CDSS recommendations, enabling practitioners to align the model's reasoning with their medical expertise.

%Under a psychological perspective, humans look for explanations to better predict similar events in the future, or more formally, explanations support generalization~\cite{vasil2022explanations}.
%Such explanations are often selective, meaning that we ask for explanations only for a set of observations, and heterogeneous, meaning that there are a variety of possibilities such as ``\textit{why?}", ``\textit{how?}", etc., to ask for an explanation ~\cite{aronowitz2020experiential,lombrozo2012explanation}. 
%Conceptually, explanations differ from mere descriptions because they convey a sense of understanding, identifying a ``difference-maker" in the causal chain. 
%that is often a cause that led to an effect, and they are underspecified, as they focus on a small subset of factors that do not rely on the whole causal chain.

% deleted this
%As the medical domain requires a high level of transparency and accountability, explanations may give better insights into the rationale behind an AI-based CDSS recommendation for a specific diagnosis or treatment, increasing trust and allowing practitioners to compare the model's reasoning with their own medical knowledge. 

%On the other hand, interpretations explain how a model interprets clinical data, allowing practitioners to judge its reliability and identify potential biases or limitations. 
%In practice, CDSS explainability should ideally not only ensure that clinicians can justify their decisions to patients, but also enhance shared decision-making and compliance with regulatory requirements (\textit{e.g.,} GDPR). 
%Also, the ability of clinicians to generalize and focus selectively should be improved by explainability, anticipating similar outcomes in future cases.
In AI, explainability and interpretability improve people's understanding of such models, driving from suboptimal to ``better" outcomes~\cite{boyd2022human}. 
More in general, the main principle of XAI is to make the decision-making process of AI systems transparent and understandable to humans.
%%% ALERT - decide if keeping this sentence %%%
%In particular, it has been proven that explanations are effective when presented to domain experts, \textit{e.g.}, medical practitioners~\cite{wang2021explanations}.
%Explanations describe \textit{why} a model made a prediction; while, interpretations tend to describe more \textit{how} a model made a prediction. It follows that interpretability and explainability are distinct concepts. However, they are closely related, as they both augment human-AI interaction. 
Many taxonomies have been proposed to categorize XAI methodologies (\textit{e.g.}, see~\cite{speith2022review}).
In this paper, we adopt the \textit{conceptual} approach based on the following macro criteria (for a more comprehensive review, please refer to~\cite{molnar2022}):
\begin{itemize}
    \item \underline{\textbf{Type}}: \textbf{Intrinsic} (or Ante Hoc) or \textbf{Post Hoc};
    \item \underline{\textbf{Dependency}}: \textbf{Model Specific} or \textbf{Model Agnostic};
    \item \underline{\textbf{Scope}}: \textbf{Local} or \textbf{Global};
    \item \underline{\textbf{Output}}: \textbf{Numerical}, or \textbf{Visual}.
\end{itemize}

\subsubsection{Type: Intrinsic or Post Hoc}
\textbf{Intrinsic} interpretability is associated with machine learning models such that humans can easily trace their decision-making process. This is typically achieved through common white-box models such as linear models, in which coefficients can be considered the marginal effects on the output in regression problems, and tree methods as decision trees, in which their hierarchical structure allows for direct observation and understanding of the decision-making process. In addition, ensemble models such as random forests or boosting methods, although considered as black box, contain an intrinsic interpretable component in terms of feature contribution. 
Finally, some deep learning models may have intrinsic interpretability components in their structure, \textit{e.g.}, attention weights in Transformers, highlighting the importance of different input features.
\underline{\textit{Methods}}: Linear Models, Tree-models, Ensemble Models, Attention Weights in Transformers, Bayesian Networks, etc. 




%Technically, the overall importance of a feature in a tree is computed by summing the reduction in impurity (\textit{e.g.}, variance for regression or Gini index for classification) across all splits where the feature is used. For each split, the feature’s contribution is measured by the difference in impurity between the parent node and the resulting child nodes. Finally, individual contributions are aggregated and normalized to sum to 100, allowing for a proportional feature contribution to the model’s predictive power. 
%Simply put, the three can be visually decomposed by showing the decision path. Decision rules at each node can be framed as IF-THEN statements such that IF $x_i > x^{*}$, THEN $path_a$, ELSE $path_b$, resembling the way of natural language in how we think. Therefore, intrinsic interpretability methodologies are strictly inherent to the adopted learning algorithm.

\textbf{Post Hoc} interpretability refers to methodologies used after model training, providing insights into how predictions are made without altering the model itself. These methodologies are not intrinsic to the model, requiring further optimization to extract interpretation-oriented representations such as feature importance scores, partial dependence plots, surrogate models, among others.  
Notwithstanding, these methods can be applied on top of intrinsically interpretable models to provide a further layer of explainability as well. 
\underline{\textit{Methods}}:
 SHAP \cite{lundberg2017shap}, LIME \cite{ribeiro2016should}, GradCAM \cite{selvaraju2017grad}, DeepTaylor \cite{montavon2017explaining}, Integrated Gradients \cite{sundararajan2017axiomatic}, Surrogate Models, Partial Dependence Plots, Counterfactual Explanations, etc. 


%In the medical domain, when regulatory requirements demand simplicity and transparency to justify, for example,  treatmetents recommendations, intrinsic interpretable models can be advantageous. Besides, such models are less prone to misunderstanding, as their interpretation structure is closer to human understanding, thus requiring less cognitive load and expertise in the AI domain. However, such models may underperform block-box models, and, in situations when clinical accuracy is critical, they may not be the best option. Thus, a high-performing model with post hoc interpretability may be the best solution. Notwithstanding, post hoc method may not generalize well across various models, leading to different explanations and inconsistencies. Here, a recommendation could be to apply post hoc methods to several models and manually verify whether feature interpretations patterns emerge.  

\subsubsection{Dependency: Model Specific or Model Agnostic}

\textbf{Model Specific} methods are limited to certain families of model architectures, and their use is coupled to the inner workings of those models. For example, neural networks require specific explainability methodologies, \textit{e.g.}, saliency maps such as  GradCAM for CNNs. 
Importantly, note that intrinsic interpretable models are by definition model specific, \textit{e.g.}, interpreting linear regression coefficients is specific to that kind of model only. 
\underline{\textit{Methods}}: Architecture-specific Saliency Maps, GradCAM, DeepTaylor, Linear Models, Tree Paths, etc.


\textbf{Model Agnostic} methodologies are suitable for multiple families of machine-learning models, regardless of their internal structure. These methods are called ``agnostic" because they do not rely on the internal workings of the model, which they treat as black box, but focus on the inputs and outputs only.
These methodologies are applied post-hoc. \textit{\underline{Methods}}: SHAP, LIME, Partial Dependence Plots, Accumulated Local Effects, Counterfactual Explanations, Scoped Rules, Integrated Gradients, etc. 


%\underline{\textit{Methods}}: GradCAM \cite{selvaraju2017grad}, DeepTaylor \cite{montavon2017explaining}, Integrated Gradients \cite{sundararajan2017axiomatic}, SHAP \cite{lundberg2017shap}, LIME \cite{ribeiro2016should}, etc. 




\subsubsection{Scope: Local or Global}
\textbf{Local} interpretability methods explain an \textit{individual} prediction, \textit{e.g.}, how each feature contributes to a given output, thus approximating model's behavior locally. Also, local methods are particularly suitable for models leveraging unstructured data, as, for instance, they can pinpoint the specific pixels in an image or tokens in a text that predominantly drive predictions. 
\underline{\textit{Methods}}: Individual Conditional Expectations, LIME, SHAP, Counterfactual Explanations, Integrated Gradients, Attention Weights, etc. 

\textbf{Global} interpretability methods provide a wider perspective on how a model is explainable across the whole dataset (usually the test set). These methods are particularly suitable for models leveraging structured data. 
While local interpretability is essential for justifying individual choices, global interpretability promotes global model transparency.
\underline{\textit{Methods}}: SHAP, Partial Dependence Plots, Accumulated Local Effects, Global Surrogates, etc.


 
%In the medical domain, local interpretability methods are useful in CDSS designed for clinical medicine (internal medicine, cardiology, neurology, etc.), \textit{e.g.}, when a predicted treatment is recommended for a specific patient, and the practitioner wants to understand which features determined the specific outcome. On the contrary, in the public health domain (\textit{e.g.}, epidemiology), global methods may be more useful, as the focus is targeted toward a population such as diseases prevention. 


\subsubsection{Output: Numerical or Visual}

%The output of the explanation refers to how explanation results are presented to the relevant stakeholders. 

\textbf{Numerical} explanations are suitable for structured data, especially for feature-based approaches. Examples include feature summary statistics, which is particularly indicated for intrinsic interpretable models such as linear regression, where coefficients, standard errors, and t-statistics provide information on how a particular feature is contributing to the model output, or tree-based methods such as decision trees, where feature importances can be traced back from the top of the tree. Besides, post hoc and model-agnostic methods such as SHAP  or LIME can provide a feature-wise numerical value for pattern interpretations. 
\underline{\textit{Methods}}: all feature summary statistics methodologies developed for models leveraging structured data.

%Obviously, what can be expressed in a numerical fashion, can be presented in a \textbf{Visual} modality. This includes for example feature summary visualisations in case of structured data. In particular, careful visualizations can enhance the understanding of CSDT systems even from non-AI-experienced practitioners. 

\textbf{Visual} output methods are particularly suitable for deep learning models operating with unstructured data such as images. Visualization techniques, such as saliency maps, class activation maps (CAM), and Grad-CAM, enable the interpretation of feature importance in CNNs, highlighting regions of interest in input images. Parallely, this also extends to token importance visualisation for language models. 
Beyond deep learning, structured data can also benefit from visual representations, including partial dependence plots, feature importance bar charts, and decision boundary plots. However, we treat those methodologies as numerical. %, as technically each numerical output can be represented visually. 
\textit{\underline{Methods}}: all methodologies suitable for unstructured data. 

%Examples in this field are model hidden layer visualisations, which are particularly suitable for convolutional neural networks ~\cite{olah2017feature}, or pixel attribution methods such as saliency maps, \textit{e.g.}, GradCAM ~\cite{selvaraju2017grad}, including attention maps in vision transformer models that highlight the portion of the image that is relevant for prediction. 











\subsection{Human-Centered XAI Evaluations} \label{sec:HCE}
% change title here, focusing on evaluations of XAI,bc this is a big word 

In practical medical applications, socio-technical gaps may arise between the CDSS explainability factors provided by XAI techniques and end-users' perceptions of their utility~\cite{ackerman2000intellectual}. Human-centered evaluations offer methodologies to bridge this gap, aiming to align explanations with user expectations. 
%Human-centered evaluations may offer a methodology to bridge that gap. 
Such evaluations should hopefully reflect human desired properties of explanations, serving some practical end goal~\cite{liao2023rethinking}.

%HCI literature extensively defines metrics such as usability metrics, satisfaction metrics, and engagement metrics, among others (for a more complete picture, refer to~\cite{lazar2017research}). 


%Such a framework can benefit a wide range of stakeholders including doctors (both current and future), nurses, qualified technicians (e.g., radiologists, paramedics, etc.) for decision assistance, patients for appealing and/or improvement, regulatory bodies for auditing, model developers for debugging, and researchers for better validating their scientific hypotheses.

Human-centered evaluations of XAI systems can be categorized into three levels: (1) application-grounded, (2) human-grounded, and (3) proxy evaluation ~\cite{doshi2017towards}.
Application-grounded evaluations involve testing real tasks with real experts and should be employed whenever possible because of their fidelity to real-world scenarios. For example, in the medical domain, doctors may be evaluated on specific application tasks. Here, domain experts are required for an effective evaluation. For this reason, application-grounded evaluation may be costly to implement.
On the other hand, human-grounded evaluations, use proxy tasks with human evaluations, conducting simplified human-subject experiments. This approach is particularly suitable for assessing general aspects of a system. As such evaluations can be performed with layman participants instead of domain experts, they are not advisable in the medical domain.
Finally, proxy evaluations assess proxy tasks without human participation, relying on simulated tasks to provide feedback about a system. 

The main difference between application-grounded and human-grounded evaluation is the non-reliance on human subjects. Similar to human-grounded evaluations, proxy evaluations are not advisable for the full evaluation of CDSS ready for production. However, they may be useful for an early-stage prototype, provided a careful application-grounded evaluation is eventually performed.
These three levels of XAI evaluations imply a trade-off between fidelity and cost, where application-grounded evaluations provide the most realistic insights but are expensive, human-grounded evaluations offer a balance of generalizability and affordability but lack domain specificity, and proxy evaluations are the most scalable and cost-efficient but least reflective of real-world performance.
In the medical domain, application-grounded evaluation is the preferred choice because it ensures high-fidelity testing with domain experts. This is crucial for patient safety and clinical reliability, despite its higher cost and complexity.

Human-centered evaluation methodologies leverage various techniques, either qualitative or quantitative. In a nutshell, the most widely known are: (1) think-aloud studies, (2) interviews, and (3) surveys. 
First, in think-aloud studies participants verbalize their thoughts while performing tasks (concurrent) or afterward (retrospective), which can be very useful in qualitatively explaining cognitive processes and identifying usability issues. 
Second, interviews are a qualitative way to gather detailed information from users about their experiences with a system. They are versatile tools in all stages of design, ranging from open-ended unstructured interviews to tightly controlled structured interviews, with semi-structured interviews offering a valuable compromise. A variant of those is focus groups, in which a small number of participants are gathered to evaluate their opinions about a product or service. %, and retrospective interviews, in which participants articulate their interactions with the system post-hoc. 
Finally, survey methodologies mainly rely on quantitative questionnaires that help researchers gather data on users’ satisfaction, usability issues, and interaction patterns. 




%Finally, experimental designs include within-subject designs (same participants in all conditions) and between-subject designs (different participants in separate conditions).






%Human-centered evaluation can be categorized into three levels—application-grounded, human-grounded, and proxy evaluation ~\cite{doshi2017towards}.Application-grounded evaluations involve testing real tasks with real expert, and should be employed whenever possible because of their fidelity to real-world scenarios. For example, in the medical domain, doctors may be evaluated on the specific application task. Here, domain experts are required for an effective evaluation. For this reason, application-grounded evaluation may be costly to implement, but suitable for the medical domain. 
%An evaluation is successful on the quality of its explanation towards its end task, \textit{e.g.}, whether it provides better error identifications, new outcomes, or fewer biases. 
%A key baseline is how effectively explanations aid task completion. Real-world impact requires valuing the effort of such evaluations and ensuring rigorous experimental standards, though the HCI community noted that this is a challenging operation~\cite{antunes2008structuring}.
%Instead, human-grounded evaluations use proxy tasks with human evaluations, conducting simplified human-subject experiments that capture the essence of the target application. This approach is particularly suitable for assessing general aspects of the explanation quality. As such evaluations can be performed with layman participants instead of domain experts, they are not advisable in the medical domain. 
%These evaluations are advantageous because they can be performed with layman participants instead of domain experts. As a consequence, costs are reduced, but the lack of human expertise may negatively impact the results. 
%In fact, a central challenge is evaluating the explanation quality without anchoring it to a specific end-goal. For these reasons, the evaluation framework should ideally and exclusively focus on the intrinsic quality of the explanation itself.
%Finally, proxy evaluations assess proxy tasks without human participation, relying on simulated tasks instead of human users to achieve feedback about a system. Therefore, the main difference from application-grounded and human-grounded evaluation is the non reliance on human subjects' evaluation. Similar to the latter, they are not advisable for the full evaluation of a CDSS ready for production. However, they may be useful for an early stage prototype, provided a careful application grounded evaluation is performed before production. 
%This class of methods is useful when a method is in its early stage, or human testing is unethical. As they often use benchmark datasets/models to achieve an evaluation, they are fast, cost-effective, and easy to implement. However, the main challenge is setting the right proxy up for the right experiment.  
%Human-centered evaluation methodologies leverage many techniques for understanding user experience for better product design. In think-aloud studies participants verbalize their thoughts while performing tasks or afterwards, and can be very useful in explaining cognitive processes and identifying usability issues. However, they may interrupt natural behavior or rely on recall. Interviews are a versatile tool in all stages of design, ranging from open-ended unstructured interviews to tightly controlled structured interviews, with semi-structured interviews offering a valuable compromise. Other methods are focus groups, in which a small number of participants are gathered to evaluate their opinion about a product or service, and surveys, in which participants fill their perceptions about a product or service. Finally, experimental designs include within-subject (same participants in all conditions) and between-subject (different participants in separate conditions). 

% what can you say about evaluating a CDSS ?
%For evaluating CDSS, 






\section{Dissecting extant Human-Centered XAI in CDSS} \label{sec:papers_list}

In this section, we present research works that leveraged human-centered XAI to build production-ready CDSS in the medical field. Leveraging the XAI taxonomy outlined before, we discuss papers, highlighting how human-centered evaluations - all relying on application-grounded evaluations - were useful in testing clinical perceptions.
%, and in turn medical adoption - if applicable. 
In table \ref{tab:sorted_tab}, we provide a summary of the papers discussed. 
We start discussing CDSS adopting intrinsic interpretable models in Section \ref{sec:cdss_intrinsic_only}. Next, we examine CDSS using post hoc and model agnostic methodologies on top of intrinsic interpretable models in Section \ref{sec:cdss_post_hoc_intrinsic}. Finally, we discuss CDSS adopting XAI methodologies in deep learning in Section \ref{sec:cdss_deep_learning}, starting with model-agnostic methods, and concluding with model-specific ones.



\begin{table*}
    \centering
    \begin{tabular}{|l|l|llll|l|}
        \hline
         \textbf{Paper} & \textbf{Model} & \textbf{XAI} & \textbf{Type} &  \textbf{Dependency} & \textbf{Output} & \textbf{Main HCE} \\
        \hline        
        \cite{barda2020qualitative} & Random Forest & SHAP & Post Hoc & Agnostic & Numerical & Focus Group  \\

        \cite{brennan2019comparing} & Random Forest & - & Intrinsic & Specific  & Numerical & Think Aloud  \\

        %\cite{calisto2022breastscreening} & DenseNet & Saliency &   Intrinsic & Specific & Local & Visual & Interviews  \\

        \cite{ellenrieder2023promoting} & DenseNet & IGrads & Post hoc & Agnostic & Visual & Think Aloud  \\

        \cite{hwang2022clinical} & TRIER (CNN) & SG & Post Hoc & Specific & Visual & Mixed  \\

        \cite{kim2022prediction}  & XGBoost & SHAP & Post Hoc & Agnostic & Numerical & Interviews  \\

        \cite{kumarakulasinghe2020evaluating} & Random Forest & LIME & Post Hoc & Agnostic & Numerical & Survey  \\

        \cite{kovalchuk2022three} & Many & SHAP & Post Hoc & Agnostic & Numerical & Mixed  \\ 

        \cite{kyrimi2020incremental} & Bayesian Network & - & Intrinsic & Specific & Numerical & Survey  \\ 

        \cite{matthiesen2021clinician} & Random Forest & LIME & Post Hoc & Agnostic & Visual & Interviews  \\

        \cite{neves2021interpretable} & Many & Many & Post Hoc & Agnostic & Visual & Survey \\

        \cite{pumplun2023bringing} & DenseNet & IGrads & Post Hoc & Agnostic & Visual & Survey  \\ 

        \cite{rajashekar2024human} & Random Forest & ICE, ALE & Post Hoc & Agnostic & Numerical & Mixed  \\

        \cite{sabol2020explainable} & CNN & X-CFCMC & Post Hoc & Agnostic & Visual & Survey  \\ 

        \cite{singh2021evaluation} & Inception-v-3 & DeepTaylor & Post Hoc & Specific & Visual & Survey \\

        \cite{sivaraman2023ignore} & XGBoost & SHAP & Post Hoc  & Agnostic   &  Numerical & Think Aloud \\

        \cite{zhang2024rethinking} & LSTM & Attention & Intrinsic & Specific & Numerical & Interviews  \\ 

        \hline
    \end{tabular}
    \caption{Summary of papers in alphabetical order adopting XAI methods and Human-Centered Evaluations in medical CDSS. Categorization is executed by defining ML models, XAI methods and their taxonomy (type, dependency, output), and HCE methodologies.}
    \label{tab:sorted_tab}
\end{table*}


\subsection{CDSS adopting Intrinsic Interpretable Models} \label{sec:cdss_intrinsic_only}

Intrinsic interpretable models are easily testable in the medical field for CDSS due to their transparency, as no post-hoc evaluations are necessarily required to perform model evaluations. Examples in this scope are the works from  \cite{brennan2019comparing} and \cite{kyrimi2020incremental}.
%that leveraged the feature importance ranking of ensemble methods and the probabilistic reasoning of Bayesian networks, respectively. 
%More in depth, 
\cite{brennan2019comparing} validated the usability of \textit{MyRiskSurgery}, an analytical CDSS based on generalized linear models and random forests to estimate postoperative risk complications. Through think-aloud studies, a set of physicians evaluated risk complications for a set of 8-10 individual cases in two case studies such as diagnosing acute kidney injury and intensive care unit admission, with the system displaying the top 3 contributing features for each prediction. As a result, physicians experienced notable improvements in their risk assessment, which led to a substantial net improvement in reclassification rates from a no XAI evaluation (+12\% and +16\%, respectively), also finding the system user-friendly and beneficial.

Then, \cite{kyrimi2020incremental} examined how explanations from Bayesian Networks could be used to predict coagulopathy in the first 10 minutes of hospital care. %understand how predictions were generated. 
Bayesian networks generate explanations by using probabilistic reasoning to identify the most likely causes based on the relationships between variables.
%They tested how their developed method could be used to predict coagulopathy in the first 10 minutes of hospital care. 
In their framework, the tool outputted explanations in three levels of complexity ranking the predictors that supported a high risk of coagulopathy. In a before-after survey questionnaire, a group of clinicians rated the perception of explanations of the system, resulting in
clinicians overall trusting the CDSS prediction, yet their level of trust remained unchanged when an explanation was provided, with the third level of explanations deemed too complex. 


\subsection{CDSS adopting Post Hoc XAI Methodologies on top of Intrinsic Interpretable Models} \label{sec:cdss_post_hoc_intrinsic}
Post-hoc methodologies can be applied on top of intrinsic interpretable models to provide a further layer of explainability. %, as well as test if explainability patterns are congruent between the model and the post-hoc methodology. 
In this subfield, we surveyed the works of \cite{barda2020qualitative}, \cite{kim2022prediction}, \cite{kumarakulasinghe2020evaluating}, \cite{kovalchuk2022three}, \cite{matthiesen2021clinician}, \cite{rajashekar2024human}, and \cite{sivaraman2023ignore}. 
%% KOVAL: diabetes prediction
%% RAJA: gastrointestinal bleeding
All these works employed model-agnostic methodologies. 

\cite{barda2020qualitative} developed a CDSS to prevent in-hospital mortality in pediatric intensive care units (PICU). The CDSS leveraged a random forest, and SHAP values on top, to predict PICU admission. Overall, feedback from focus group sessions with clinicians indicated that the proposed solution was perceived as useful. However, the capacity to digest the information varied across clinical roles and levels of predictive modeling knowledge. In general, they preferred solutions that required less cognitive effort and could accommodate diverse user information needs.
Still adopting ensemble methods,
\cite{kumarakulasinghe2020evaluating} analyzed the agreement rate between physicians and LIME feature importances for sepsis prediction. Firstly, the authors trained several models such as random forest, adaboost, among others, and applied the LIME framework on top of the best-performing one (random forest). Secondly, physicians were asked to: (1) agree with the model whether the patient presented sepsis, based on individual features, and (2) blindly rank the top 3 features the model used to make predictions without accessing LIME explanations. Finally, LIME explanations were revealed and physicians were asked to rate their satisfaction on a 5-stars Likert scale. Results indicated that physicians agreed with model predictions (87\%), and that LIME explanations were equal to the physicians' explanations in at least two features for all patients (69\%). Also, physicians were satisfied with LIME predictions (78\%), and 68\% presented a positive attitude towards explainability in terms of trust. However, such positive attitudes should not be translated into increased trust, as they found inconsistency in LIME explanations.
Similarly in the scope of sepsis prediction, 
\cite{sivaraman2023ignore} examined clinicians' interactions with an AI-based interpretable treatment recommender. The system was modeled using XGBoost, and SHAP as feature explainer. 
Through a think-aloud study involving 24 intensive care clinicians, the analysis revealed that explanations improved clinicians' perceptions of the AI's usefulness. In particular, the study identified four distinct behavior patterns among clinicians summarised as: \textit{Ignore}, \textit{Negotiate}, \textit{Consider}, and \textit{Rely}. %Specifically, in the Negotiate group, clinicians displayed a tendency to weigh and prioritize aspects of the AI recommendations, suggesting that treatment decisions in the ICU may involve partial reliance on AI recommendations, with potential implications for the effectiveness of chosen treatments. 
Overall, clinicians who found the AI recommendations most useful saw it as additional evidence alongside their assessments.

In the field of medical wearable technologies, \cite{kim2022prediction} developed MindScope, a system based on a personalized ML system trained on an XGBoost to predict patients' stress levels.
Interviews were conducted to gather feedback on the relevancy of feature importance (SHAP values) in three different explanation settings: (1) no explanations, (2) broad explanations, and (3) detailed explanations. Overall, it was found that the explanation-based settings were more useful than no explanations. However, detailed explanations were perceived as equally useful as broad explanations, thus questioning how detailed explanations should be. 

Next, \cite{matthiesen2021clinician} conducted a human study in the application of a CDSS able to predict ventricular fibrillation.
The study was organized into three phases: (1) observations were conducted at the clinic to comprehend the clinical workflow, (2) development of the CDSS adopting a random forest model to predict the risk score, showing the top/bottom 5 most important features through LIME, and (3) interview study to assess feasibility. Although many clinicians did not change their decisions, they claimed that having a prediction tool could increase confidence in their actions. Also, the visualization of key features was useful, as the presentation of key features facilitates explainability regarding factors contributing to the risk of disease. However, the explainability component did not alter clinical actions.


In endocrinology, \cite{kovalchuk2022three} proposed a three-staged CDSS to for diabetic diagnosis as follows: (1) \textit{Basic Reasoning}, \textit{i.e.}, implementing healthcare practices by referencing existing policies to guide decision-making, (2) \textit{Data-Driven Predictive Modeling}, \textit{i.e.}, applying machine learning models in the context of the previous stage, and (3) \textit{Explanation}, in which XAI techniques are applied to provide better recommendations. 
The authors tested the framework in a diabetes mellitus case study. In the first stage, expert endocrinologists guided the selection of insulin resistance predictors. In the second stage, a predictive model was optimized to estimate the risk. Finally, SHAP values were employed to interpret the CDSS predictions, which were then validated using questionnaires. %Perceptions from physicians were generally positive, with an agreement rate with the system of 87\%. 
Unexpectedly, though not extensively, the basic reasoning case showed higher understandability, followed by the explanation case, and the data-driven case. These findings suggest that adding explanations is useful, but well-grounded, known conventional clinical practices may still outperform AI-driven solutions.

Finally, integrating large language models (LLMs), \cite{rajashekar2024human} developed GutGPT, a system tailored to provide a natural language interface for gastrointestinal bleeding prediction, including hospital-based intervention. This system integrates LLMs interface, with a standard random forest model to predict a risk score. Then, interpretability plots such as individual conditional expectations (ICE) and accumulated local effects (ALE) were used to explain feature-wise effects on predicted scores. By interacting with human text, GutGPT leverages in-context learning to present easy-to-understand natural language explanations alongside XAI explanations for predictions. 
System usability was measured by means of interviews and surveys post hoc, finding that perceptions were mostly positive. 








\subsection{CDSS adopting XAI Methodologies in Deep Learning} \label{sec:cdss_deep_learning}

CDSS using deep learning mainly rely on computer vision architectures to locally explain predictions, usually in form of heatmaps. 
Based on the explainability methodology, dependency may be either agnostic or specific. For example, agnostic methodologies such as integrated gradients could be applied to wider families of deep learning models; while, specific methodologies may be reserved for only a family of models (\textit{e.g.}, GradCAM for CNNs only).
%Notably, all explanations are assessed post hoc. 

In the field of model-agnostic XAI methodologies, \cite{ellenrieder2023promoting} extended the work of \cite{pumplun2023bringing} that leveraged a DenseNet and the integrated gradients framework to produce heatmaps of image regions to highlight cancer malignancy probabilities. Working with radiologists, a think-aloud study was conducted to assess how much they would learn
with four differently developed UIs: non-explainable high-performing (NEHP), explainable high-performing (EHP), non-explainable low-performing (NELP), and explainable low-performing (ELP). 
Four hypotheses were confirmed: 
H1 - small learning gains occur with NEHP;
H2 - larger learning gains occur with EHP;
H3 - false learning happens with NELP;
H4 - false learning is reduced with ELP, suggesting that explanations can significantly enhance clinical decision-making through learning. 
%, but also that false learning was advanced by interacting with NELP. 

Next, model-specific XAI methodologies were presented in the works from \cite{hwang2022clinical}, \cite{sabol2020explainable}, \cite{singh2021evaluation}, and \cite{neves2021interpretable}. 
Here, \cite{hwang2022clinical} developed a  CDSS to aid sleep-staging technicians in reviewing AI-generated sleep staging results. The authors developed the study in three phases. Firstly, sleep technicians were asked why they would need explanations. Secondly, a user observation study was performed to understand the sleep staging conventions of clinical practitioners. Thirdly, a CDSS based on CNNs was developed, with convolutional filters, saliency values, and intermediate activation, as sources of information for generating explanations. 
Human evaluations suggested that
explanations from the system effectively served as convincing elements for model predictions; however, they failed to reveal ambiguous predictions.
As a general principle, technicians sought explanations to confirm the accuracy of the AI predictions, while adhering to their clinical knowledge. 
%that creating clinical explanations for AI-driven predictions through a user-centered design process is an effective approach to developing CDSS.


Next, in the field of oncology, \cite{sabol2020explainable} adopted a Cumulative Fuzzy Class Membership Criterion (CFCMC) to predict colorectal cancer. They developed two segmentation systems for whole-slide histopathological images: one using a standalone CNN and the other using the CFCMC classifier on CNN-extracted features to provide explanations.
Through surveys with 14 pathologists, the CFMC showed higher acceptability and trust compared to a plain CNN, pointing out that explainable classifiers improve AI usability and reliability in medical settings.

%With a similar comparative approach surveying more than one XAI methodology, 

Next,
\cite{singh2021evaluation} evaluated multiple types of different visual explanation methods for an Inception-v-3 model trained to diagnose eye retinal diseases, while \cite{neves2021interpretable} similarly adopted various explainability methods to
analyze how a CDSS based on electrocardiogram (ECG) time series data could be used to predict heart arrhythmia, pointing out which explanatory method would perform optimally. 
In both studies, practitioners were carefully surveyed, resulting in an attribution method based on Taylor series expansions (DeepTaylor) and LIME, respectively, to be considered useful, %respectively. %highlighting the importance of seeing the tool as a collaborator rather than a competitor.  
in particular by less experienced clinicians.
Finally, achieving comparable usability outcomes through semi-structured interviews, \cite{zhang2024rethinking} uniquely leveraged an attention module to generate a fixed-length vector fed as input to an LSTM to select important variables, with attention weights to be used as the feature's importance score, to develop \textit{SepsisLab}, a CDSS able to predict ahead-of-time sepsis. 
%Besides, a counterfactual module was added to provide further transparency.
Clinicians suggested that providing more explanations could improve their decision-making, but shifting the AI focus away from final decision predictions could improve their overall experience.





%\section{Lessons, Challenges, and Future Directions}
%%% OR %%%
\section{Challenges and Future Directions} \label{sec:challenges}

\subsection{How XAI Impacts CDSS: a Socio-Technical Perspective}

We have reviewed the diversity of CDSS that adopt XAI human-centered evaluations in the medical field. Given the high-risk environment, application-grounded evaluations with domain experts were the main evaluation methodology. 
On average, clinical perceptions of XAI-based CDSS were mostly positive. % , with a favorable attitude toward adoption. %For example, 
For example, \cite{ellenrieder2023promoting} demonstrated that providing explanations not only enhances clinicians' CDSS learning but also reduces false learning. In particular, \cite{neves2021interpretable} demonstrated that less experienced practitioners could benefit from XAI explanations, while \cite{brennan2019comparing} revealed that adding XAI aids in significant improvements in physicians' risk assessments. 

However, incorporating XAI still poses significant challenges. For instance, \cite{barda2020qualitative} discovered that the capacity to absorb explanations varied across clinical roles and levels of AI knowledge, thus advocating solutions that demand less cognitive effort. For the same reason, \cite{kim2022prediction} found that detailed explanations were perceived as equally useful as broad explanations, thus questioning how deep explanations should extend. Again, \cite{kyrimi2020incremental} documented that too-engineered explanations led to brain clutter, preventing their full internalization. 

Notwithstanding, \cite{sivaraman2023ignore} more granularly identified that 
%four different patterns to categorize clinicians' perceptions of XAI explanations: (1) \textit{Ignore}, (2) \textit{Negotiate}, (3) \textit{Consider}, and (4) \textit{Trust}. Here, 
clinicians who ignored explanations were already confident in their own settled decisions, 
%but the majority were 
but clinicians who ``negotiated" accepting at least one aspect of the recommendation were able to reach a balanced intermediate solution that combined 
%the recommendation's key elements 
with their own intuition. %, compared to those who completely trusted the system. 
Similarly, \cite{matthiesen2021clinician} argued that ML and explainability did not surpass specialized and experienced clinicians' capabilities, but at best support them during the clinical workflow, as well as \cite{kyrimi2020incremental}, who found that, although XAI was useful, current conventional clinical practices are still perceived as more understandable.
Finally, we highlight that only \cite{hwang2022clinical,matthiesen2021clinician} early gathered clinicians' feedback before CDSS development, which may be crucial to ensure that the CDSS aligns with real-world clinical expectations, thus purportedly reducing frictions in subsequent stages. 
%These findings highlight two key lessons: explanations should align with prior clinical knowledge, incorporating early stakeholders' feedback to ensure uniformity expectation alignment ex-ante, and explanations should not impose an excessive cognitive burden on clinicians using CDSS.
These findings emphasize the need for explanations to align with prior clinical knowledge through early stakeholder feedback and to minimize the cognitive burden on clinicians using CDSS.


%Finally, although explanations favored AI understanding, shifting AI away from final clinical decisions enhanced human-AI collaboration \cite{zhang2024rethinking}.
%These aggregated findings raise questions about how explanations favor human understanding of AI medical systems, 
%including the appropriate level of detail, 
%which circularly affects the perception of the explanations themselves. 

More in general, despite explainability methods face intrinsical issues such as lack of robustness \cite{slack2020fooling} and intra-method disagreement \cite{krishna2022disagreement}, 
%and human understandability \cite{zhang2019dissonance},
issues that may be framed as independent research topics per sé, the friction from clinicians in perceiving explainability may be attributed to 
socio-technical gaps between human requirements and the effective utility of technological solutions, because human activity is ``flexible and nuanced", while computational mechanisms are ``rigid and brittle" \cite{ackerman2000intellectual}.
In other words, a socio-technical gap refers to the discrepancy between human needs, including expectations, and the technological capabilities designed to support them, highlighting the limitations of technological systems in fully accommodating the nuances of human interactions. 
This general theoretical construct can be extended to the purpose of XAI explanations in CDSS, as explanation capabilities might not meet rigorous human requirements from precise clinical knowledge, given that CDSS are dramatically at high stake, as output recommendations may have a significant impact on patient lives.
In other words, implementing XAI solutions in CDSS might not always bridge the socio-technical gap when compared to systems that do not adopt XAI, or systems that use basic reasoning in current clinical knowledge. 
%For this reason, unlike conventional system evaluations, CDSS must be rigorously evaluated to narrow down such socio-technical gap to the minimum.
% say this later 
%, even from early in development to ensure alignment with stakeholder perspectives.

 
Therefore, we suggest a reference protocol for effective CDSS development. This is an \textbf{iterative} process that requires exploring and connecting both the XAI and HCI fields, leveraging their theoretical constructs to eventually devise a practical methodology. 
%By doing so, we aim to foster the adoption and integration of CDSS in clinical practice.
We begin by identifying two theoretical dimensions: \textbf{T1}: determining stakeholders and their requirements for use case models (\textit{human requirement realism}), and \textbf{T2}: developing XAI evaluation methods to reliably assess whether and to what extent these requirements are met in downstream use cases (\textit{context realism}) \cite{liao2021human}.
%% discuss about tradeoffs
Consistent with \cite{doshi2017towards}, who stated that in high-stake domains, application-grounded XAI evaluations should be preferred because of their high fidelity, \textbf{T2} particularly entails trade-offs with higher pragmatic and monetary costs to bear for effective evaluation. % when performing the evaluation. 
%This view is consistent with \cite{doshi2017towards}, and given that the medical domain is at high stake, application-grounded XAI evaluations should be preferred because of their high fidelity to real-world case scenarios, despite their higher cost (see Section \ref{sec:HCE}).

In practical terms,
designing a human-centered CDSS that conforms to the above-mentioned principles involves creating a system that is \textbf{accurate} in treatment recommendations, \textbf{explainable} in how and why such recommendations are conveyed, easily \textbf{usable} by clinicians, thus not imposing a significant cognitive burden, and \textbf{seamlessly integrable} into extant technological workflows. In other words, all stakeholders' requirements (\textit{human requirement realism}) should be addressed thoroughly by the CDSS use case (\textit{context realism}). % to specify a medical field-agnostic development pipeline. 

%The pipeline can be broken into the following steps.  %% rewrite 
%% say that we want to be medical field agnostic

\subsection{A Medical Field-Agnostic, Stakeholder-Centric Iterative Approach to CDSS Development}
The iterative process can be decomposed into the following phases: (1) stakeholder identification, (2) stakeholders' requirements identification, and (3) system development and evaluation. 

First, it is fundamental to identify all relevant stakeholders in each specific case. 
As a rule of thumb, identifiable stakeholders in the healthcare domain may be: (1) patients, (2) clinicians, (3) clinics and hospitals - including their corporate governance and management, and (4) CDSS developers - including researchers and engineers. 
Patients may be considered as passive stakeholders, as they do not directly interact, but are ``targeted" by the system. 
On the contrary, clinicians are actively supported by CDSS, corporate management may decide whether the clinic/hospital should adopt AI-based solutions, evaluating the costs and benefits, and developers are responsible for the creation and maintenance of the technological workflows. 

Second, we argue that a participatory AI methodology in which all stakeholders are early on involved is an effective starting point, where diverse requirements are early considered, potentially reducing frictions afterward, \textit{e.g.}, \cite{hwang2022clinical,matthiesen2021clinician}.
This entails gathering all stakeholders' needs, setting goals, and clearly framing an evaluation strategy specific to the CDSS and to the medical field in which the CDSS would operate. 
To this end, we suggest developers inform and instruct non-technical stakeholders of how CDSS operate ex-ante, including carefully describing their inner workings based on machine learning model(s) and XAI explainability component(s), and possible evaluation frameworks. Here, presenting them with relevant taxonomies can help them become familiar with the material more effectively (see Section \ref{sec:XAI_methods} for XAI principles and methods, and \ref{sec:HCE} for human-centered evaluations). %Importantly, stakeholders should be aware of the inner workings of XAI, and how XAI evaluations could be executed. 
%Once stakeholders are informed, it is crucial to gather early feedback. 
%Methodologies can take the form of a pilot study if a simple, existing prototype is available, or a formative study if no prototype exists. Additionally, this phase should capture stakeholders’ expectations and determine whether a specialized evaluation method is needed for the specific medical field in which the CDSS will operate. Here, focus groups may be a relevant methodology, as all stakeholders are simultaneously convened. 
Methodologies may include a pilot study for existing prototypes or a formative study if none exist. This phase should capture stakeholders’ expectations and assess the need for specialized evaluation methods in the CDSS’s medical field. Also, focus groups can be particularly useful, as they convene all stakeholders simultaneously. 

%% application grounded studies - you are forced to do that as this field is at high stake. drawback high costs  
%% participatory AI gathering all stakeholders' feedback
%% pilot study - if simpler and extant technology exists
%% formative study if no design is there 
%% discuss all stakeholders about XAI taxonomy, and ask them their expectations.
%% ask whether a new method of evaluation may be suitable for the specific field. this bridges the gap 


Third, CDSS developers would optimally internalize stakeholders' requirements gathered from the previous phase to develop a prototype to be quickly tested with synthetic patients' data. The prototype should adhere to both technical backend requirements and usability requirements and be designed with a focus on future adoption, thus seamlessly integrable into medical workflows.  
We argue that concurrent think-aloud studies can be effective in this stage, as clinicians (the end users) can articulate their thoughts on the perception of the accuracy and explainability of the models. 


%% system creation 
%% study with early feedback about usability: think aloud study - concurrent or retrospective. focus on explanation 
%% if not satisfactory, iterate; else, end. 
In general, steps 2 and 3 should be conceived as iterative, adhering to the concept that continuous feedback stimulates CDSS improvement. 
This iterative process guarantees that system performance is continuously and rigorously improved. % according to real-world data, user interaction, and new clinical insights. 
Through the incorporation of ongoing feedback, the CDSS can be fine-tuned with new information, thus refining both its predictive value and clinical usefulness. As such, this refinement process facilitates evidence-based decision-making and optimizes overall system effectiveness in clinical practice.


\subsection{Conclusion}

In this work, we have examined the role of explainability in CDSS and the socio-technical challenges it faces.
While explainability enhances clinicians’ trust, its effectiveness varies based on user expertise, cognitive load, and the depth of explanation required. The socio-technical gap remains a central challenge, as human decision-making is inherently flexible, whereas (X)AI models operate within rigid constraints. 
Given the high stakes in healthcare, rigorous evaluation strategies - particularly application-grounded methodologies - are necessary to ensure that CDSS align with stakeholders' requirements.
To address these challenges, we proposed an iterative framework for CDSS development to integrate XAI and HCI principles. By identifying key stakeholders, gathering their requirements through participatory methodologies, and continuously refining CDSS through iterative evaluations, we advocate for a development approach that prioritizes usability, trust, and clinical integration. Overall, rather than replacing human expertise, CDSS should serve as augmentative tools that enhance clinicians’ decision-making, ultimately fostering more effective and informed patient care.



%\iffalse
\section*{Acknowledgments}
This work was funded by Fundação para a Ciência e a Tecnologia (UIDB/00124/2020, UIDP/00124/2020 and Social Sciences DataLab - PINFRA/22209/2016), POR Lisboa and POR Norte (Social Sciences DataLab, PINFRA/22209/2016).
%\fi


\newpage

%%% TODO:
% review again each paper, and find gaps in XAI or HCI
% frame better the XAI suggestions 













\appendix




%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

