You are about to be provided with summaries of multiple papers, one by one. Your task is to write a critical literature review section for a paper discussing the intersection between explainable artificial intelligence (XAI), human-computer interaction (HCI) with human-centered evaluation, and medicine.

The literature review should not simply list the approaches, but rather critically describe them in relation to the three key aspects: XAI methods with taxonomies, human-centered evaluations, and their application in the medical field. Focus should be placed on critically analyzing the contribution, strengths, and weaknesses of each approach.

Here is a table summary containing XAI methods with taxonomies and human-centered evaluations:

Paper	Model	XAI	Type	Dependency	Scope	Output	Main HCE
Barda et al., 2020	Random Forest	SHAP	Post Hoc	Agnostic	Local	Numerical	Focus Group
Brennan et al., 2019	Random Forest	-	Intrinsic	Specific	Local	Numerical	Think Aloud
Ellenrieder et al., 2023	DenseNet	IG	Post Hoc	Agnostic	Local	Visual	Think Aloud
Hwang et al., 2022	TRIER (CNN)	IG	Intrinsic	Specific	Local	Visual	Mixed
Kim et al., 2022	XGBoost	SHAP	Post Hoc	Agnostic	Local	Numerical	Interviews
Kumarakulasignhe et al., 2020	Many	LIME	Post Hoc	Agnostic	Local	Numerical	Survey
Kovalchuk et al., 2022	Many	SHAP	Post Hoc	Agnostic	Mixed	Numerical	Mixed
Kyrimi et al., 2020	Bayesian Network	-	Intrinsic	Specific	Local	Numerical	Survey
Matthiesen et al., 2021	Random Forest	LIME	Post Hoc	Agnostic	Local	Visual	Interviews
Neves et al., 2021	Many	Many	Post Hoc	Agnostic	Local	Visual	Survey
Pumplun et al., 2023	DenseNet	IG	Post Hoc	Agnostic	Local	Visual	Survey
Rajashekar et al., 2024	Random Forest	ICE, ALE	Post Hoc	Agnostic	Mixed	Numerical	Mixed
Sabol et al., 2020	CNN	X-CFCMC	Intrinsic	Agnostic	Local	Visual	Survey
Singh et al., 2021	Inception-v3	DeepTaylor	Post Hoc	Specific	Local	Visual	Survey
Sivaraman et al., 2023	XGBoost	SHAP	Post Hoc	Agnostic	Local	Numerical	Think Aloud
Zhang et al., 2024	LSTM	Attention	Intrinsic	Agnostic	Local	Numerical	Interviews



The summary of the papers is as follows:


\cite{sivaraman2023ignore} examined clinicians' interactions with an AI-based interpretable treatment recommender in the intensive care unit (ICU) focusing on treating sepsis. The system was built using XGBoost, and SHAP as feature explainer. 
Through a think-aloud study involving 24 ICU clinicians, the analysis revealed that while explanations improved clinicians' perceptions of the AI's usefulness in their decisions, explanations did not significantly adjust their overall rates of concordance with the AI recommendations. In particular, the study identified four distinct behavior patterns among clinicians: Ignore, Negotiate, Consider, and Rely. Specifically, in the Negotiate group, clinicians displayed a tendency to weigh and prioritize aspects of the AI recommendations, suggesting that treatment decisions in the ICU may involve partial reliance on AI recommendations, with potential implications for the effectiveness of chosen treatments. Overall, clinicians who found the AI recommendations most useful saw it as additional evidence alongside their assessments.



\cite{barda2020qualitative} discussed the development of a framework for designing user-centered displays of explanations for ML models such as logistic regression and random forest used in healthcare. 
They focused on creating explanation displays for predictions from a pediatric intensive care unit (PICU) in-hospital mortality risk model using a model-agnostic approach based on SHAP feature values. 
Feedback from focus group sessions indicated that the proposed displays were perceived as useful tools for assessing model predictions. However, preferences for specific display features varied among clinical roles and levels of predictive modeling knowledge. 
Providers preferred displays that required less cognitive effort and could accommodate diverse user information needs. Incorporating supporting information to aid interpretation was deemed crucial for fostering provider understanding and acceptance of predictions and explanations. 



~\cite{brennan2019comparing} validated the usability of \textit{MyRiskSurgery}, an analytical framework based on generalized linear models and random forests to estimate postoperative risk complications, as a pilot study for a real application. Given the white box nature of random forests, MyRiskSurgery shows the top 3 features contribution in making predicitons as explainability component. 
Through think-aloud studies, a set of physicians evaluated risk complications for 8-10 individual cases, with the system displaying the top 3 contributing features for each prediction. As a result, physicians experienced notable improvements in their risk assessment abilities for acute kidney injury and intensive care unit admissions. This led to a substantial net improvement in reclassification rates, with increases of 12\% and 16\% respectively. In terms of adoption, physicians found the algorithm user-friendly and beneficial.



~\cite{matthiesen2021clinician} conducted a human study in the application of a system being able to output a risk prediction for ventricular fibrillation. They deliberately organized the study into three phases: (1) observations were conducted at the clinic to comprehend the clinical workflow, (2) development of the AI algorithm (random forest), and (3) feasibility and and qualitative interview study. In phase 2, they adopted a random forest model to predict a risk score of disease, showing the top/bottom 5 most important features through LIME. They found that, although many clinicians did not change their decision, they claimed that having a prediction tool could increase confidence in their actions. Also, the visualization of key features was useful, as the presentation of key features facilitates explainability regarding factors contributing to risk of ventricular fibrillation. 



 
\cite{ellenrieder2023promoting} framed the impact of CDSSs in decision-making as a human learning problem, extending the findings from a prior trained DenseNet model developed with ~\cite{pumplun2023bringing}, who effectively implemented the integrated gradients approach to produce heatmaps from a DenseNet showing image regions that increase cancer malignancy probabilities~\cite{sundararajan2017axiomatic}. 
Working with radiologists in scanning brain tumors, the authors conducted a between-subjects study in which radiologists were provided with differently performing CDSSs as follows: non-explainable high-performing (NEHP), explainable high-performing(EHP), non-explainable low-performing (NELP), and explainable low-performing (ELP). The authors checked whether radiologist could improve their detection accuracy after being "trained" with the CDSSs. Explainability was in the form of confidence heatmaps on mRI scans.  They found that the EHP model led to better-informed decision making, but also that false learning is advanced by interacting with NELP. 






\cite{hwang2022clinical} presented an AI-driven CDSS to aid polysomnographic technicians in reviewing AI-generated sleep staging results. The authors developed the study in three phases. Firstly, the authors interviewed sleep technicians asking them why they would need explanations in the CDSS system. Secondly, the authors performed a user observation study to understand the sleep staging conventions of clinical practitioners. Thirdly, as a result of technicians' positive feedback, and monitored by a single sleep technician, a CDSS based on TRIER was developed, with convolutional filters, saliency values, and intermediate activation, as sources of information for generating explanations. 



\cite{kumarakulasinghe2020evaluating} conducted a study analyzing the rate of agreement between physicians and LIME in terms of feature importance in a sepsis/non-sepsis prediction experiment. Firstly, the authors trained several models such as random forest, Adaboost and support vector machines, and applied the LIME framework on the best performing one (random forest). Secondly, physicians were asked to (1) agree with the model whether the patient presented sepsis based on individual features, and (2) blindly rank the top 3 features the model used to make predictions without accessing LIME explanations. Next, LIME explanations were revealed and physicians were asked to rate their satisfaction with those on a 5-stars Likert scale. Results indicated that physicians agreed with model predictions (87\%), and that LIME explanations are equal to the physicians' explanations in at least two features for all patients (69\%). Also, physicians were satisfied with LIME predictions (78\%), and 68\% presented a positive attitude towards LIME in terms of trust. 


\cite{kovalchuk2022three} proposed a three-staged clinical framework for CDSS implementation as follows: (1) \textit{Basic Reasoning}, \textit{i.e.}, implementing healthcare practices by referencing existing policies to guide decision-making, (2) \textit{Data-Driven Predictive Modeling}, \textit{i.e.}, applying machine learning models in the context of the previous stage, and (3) \textit{Explanation}, in which XAI techniques are applied in support of the earlier stages to enhance HCI to provide better recommendations. 
The authors tested the framework in a diabetes mellitus case study. In the first stage, expert endocrinologists guided the selection of insulin resistance predictors. In the second stage, a predictive model was optimized to estimate the risk. Finally, in the third stage, XAI SHAP values were employed to interpret the CDSS results, which were then validated using physician questionnaires. Perceptions from physicians were generally positive, with an agreement rate with the system of 87\%. 



\cite{kyrimi2020incremental} examined how explanations from Bayesian Networks could be used to understand how predictions were generated. They tested how their developed method could be used to predict coagulopathy in the first 10 minutes of hospital care. The tool outputted textual explanations ranking the predictors that supported a high risk of coagulopathy. In a before-after questionnaire, a group of clinicians rated their perception of the system: the method produced explanations that generally aligned with clinicians' identified evidence, though clinicians found them wordy and not significantly impactful on trust or decision-making.



\cite{sabol2020explainable} enhanced the explainability and performance of the Cumulative Fuzzy Class Membership Criterion (CFCMC) classifier for classifying eight tissue types from histopathological cancer images. By utilizing a fine-tuned CNN as a feature extractor, they improved the CFCMC classifier's accuracy on colorectal image data. They introduced the factor of misclassification (FoM) to estimate misclassification probabilities and defined a certainty threshold to determine prediction certainty. The proposed uncertainty measure, distinct from other models, aids in identifying possible misclassification classes. Two segmentation systems for whole-slide histopathological images were developed: one using a standalone CNN and the other using the X-CFCMC classifier using CNN extracted features, which provides explanations, visualizations of training images, and other tissue types. 
Acceptability tests developed as within-subject experimental designs with 14 pathologists showed higher acceptability and trust in the X-CFCMC system compared to a plain CNN. They conclude that explainable classifiers improve AI usability and reliability in medical settings, with future plans to expand clinical trials and test the classifier on imperfect data.

\cite{rajashekar2024human} developed GutGPT, a system tailored to provide a natural language guidance interface for gastrointestinal bleeding prediction, including hospital-based intervention. This system integrates LLMs interface, with a standard random forest model to predict the risk score. Then, interpretability plots such as individual conditional expectations and accumulated local effects are used to explain feature-wise effects on predicted scores. By interacting with human text, GutGPT leverages in-context learning to present easy-to-understand natural language explanations alongside XAI explanations for predictions. 
System usability was measured by means of interviews and surveys post hoc, finding that perceptions were mostly positive. 


\cite{zhang2024rethinking} showed that existing AI-driven sepsis risk predictors do not support decision-making, as they predict a final outcome without really focusing on intermediate steps and on easy accessible explanations. To address this, they developed SepsisLab, a system that predicts ahead-of-time future sepsis diagnosis, embedding explanations. SepsisLab is based on a LSTM model and is capable of predicting sepsis risk 4 hours ahead. Before, an attention module was deployed to generate a fixed-length vector that was fed as input to the LSTM to select important variables, with attention weights to be used as the feature's importance score. Besides, a counterfactual module is added to provide further transparency. Next, researchers validated the system using semi-structured interviews, with practitioners expressing a positive interest seeing the tool as a collaborator rather than a competitor. 


\cite{kim2022prediction} developed MindScope, a stress level tracker based on a personalized ML system trained on an XGBoost, using smartphone usage data including GPS information, app usage, screen status etc. Researchers measured feature importance using SHAP values with three different explanation settings: (1) no explanations, (2) broad explanations, and (3) detailed explanations. Next, interviews were conducted to gather user feedback on the use experience, and algorithmic perception, particularly focusing on the type of preferred explanations. Here, it was found that the explanation-based settings were more useful than no explanations. However, detailed explanations were perceived as equally useful as broad explanations. 

\cite{singh2021evaluation} evaluated 13 types of different visual explanation methods for an Inception-v-3 model trained to diagnose eye retinal diseases, and surveyed clinical physicians about which kind of explanation method would be the optimal one. They found that an attribution method based on Taylor expansions (DeepTaylor) was rated highest (3.85/5), followed by Guided Backprogagation (3.29/5), and SHAP (3.26/5). Perceptions of the usability were mixed.  

\cite{neves2021interpretable} analyzed how ECG (time series data) could be interpreted by machine learning algorithms such as CNN and KNN. In particular, they introduced a high-level conceptual framework for visually explainable time series data using tools such as LIME. With a user study, clinicians were surveyed about the perceived usefulness of the system, finding that explanations could be detrimental. 



