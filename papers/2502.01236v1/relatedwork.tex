\section{Related Works}
Automated red teaming of language models has been a popular research direction \cite{perez2022redteaming,zou2024gcg,liu2023autodan}. This problem is often cast as an inference-time optimization, where the objective is to identify prompts that elicit a specific harmful response. For example, GCG \cite{zou2024gcg} and AutoDAN \cite{liu2023autodan} optimize the prompt for each response instance individually, requiring significant computational resources for each search. In contrast, our approach moves this expensive inference cost to training, obtaining a model that can learn a general approach that applies to each elicitation task.

Similar to our approach, many prior works amortize the cost of search by training a model to perform red teaming. For example, \citet{perez2022redteaming} and \citet{hong2024curiosity} use reinforcement learning to elicit generally harmful behaviors. Our investigators elicit finer-grained behaviors and can condition on rubrics. This rubric-conditioning as well as the diversity-seeking algorithm inspired by Frank-Wolfe lead to improved precision and coverage.