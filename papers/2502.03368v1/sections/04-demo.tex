\section{Demonstration}
\label{sec:demoscenario}

In this demonstration scenario, we will present a real-world use case leveraging \chat{} to build an end-to-end pipeline for scientific discovery.

\textbf{Use case:} Consider medical researchers interested in performing a literature survey on \emph{colorectal cancer}. 
They are interested in finding studies that report on the correlation between gene mutation and tumor cells, providing or using publicly available datasets of genomic data, and fetching these datasets to run their own analysis.
Assume they have access digital library of scientific papers, but this collection is potentially large, containing unrelated papers, and it is not annotated with metadata about the data sources.

To automate this task, they can build an AI data-processing pipeline using \sys{} through the \chat{} interface.
At a high level, the medical researcher has to upload a collection of papers, filter for those about colorectal cancer, and then extract any reference of publicly available datasets.

% \subsection{Dataset Operations}
% \textbf{Dataset operations:}
At the core of \sys{}, there are \emph{datasets}: collections of input records.
The first step when building a pipeline is to define an input dataset - this could either be a local folder, for which every file will constitute an individual record; or an iterable object in memory, for which every item will be a record.
Additionally, more experienced users can define any custom logic to marshal arbitrary objects or paths into input datasets.

% offers several interfaces for managing datasets, including listing available datasets, registering new ones, and removing unused entries. It also supports retrieving and displaying specific datasets. 

For our demonstration, the user instructs \chat{} to load an input dataset from PDFs of scientific papers contained in a local folder (See Figure~\ref{fig:dataset_ops}).

%\subsection{Data transformations}
The core \chat{} system includes a native \texttt{PDFFile} schema, which is automatically chosen to parse the files in this dataset given their extension. 
However, this schema only represents the filename and the raw textual content extracted for a given paper.

Next, the user informs \chat{} that they are interested in papers that are about colorectal cancer, and for these papers, that they would like to extract whatever public dataset is been used by the study in the paper.
From these requests, the reasoning agent creates a pipeline of \sys{} operations: it first uses the \texttt{filter()} function to select papers that satisfy the desired condition.
Then, it dynamically generates an extraction schema with the fields \texttt{dataset\_name}, \texttt{description}, and \texttt{URL}.
Finally, it applies the extraction schema to the filtered dataset with a \texttt{convert()} operation.
Figure~\ref{fig:chat-extract} highlights these steps. 
In the background, \chat{} decomposes the user’s request into sub-steps for data loading, filtering, conversion, and execution.
Before running the full pipeline to process the inputs, the users can specify their optimization goal: minimum cost, maximum quality, or minimum runtime.
\sys{} will automatically determine the appropriate physical plan that implements the logical plan built so far.

The final code generated can be seen in Figure~\ref{fig:finalcode}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/demo-pipeline.png}
    \caption{Building a pipeline through natural language. As seen in the last call, the agent reasons and may decide to decompose a user question into several tasks required  before execution.}
    \label{fig:chat-extract}
\end{figure}

When the users prompt the chat to run the pipeline, they will visualize the extracted dataset names and URLs.
Moreover, users can gain insights into the workload execution by asking the system to provide statistics such as how much runtime was needed to produce the output, and how much the LLM invocations costed.
A sample of this output can be seen in Figure~\ref{fig:demo-results}.
For the scientific discovery use case, out of an input dataset of 11 papers, the pipeline managed to extract 6 publicly available datasets related to colorectal cancers, together with the associated URLs.
We manually verified the validity of these URLs.

The execution statistics of the pipeline show that the workload was executed in about 240s and with a cost of about 0.35 USD.

Finally, after building a pipeline, users may continue to iterate on the code produced either through the chat interface or by downloading a Jupyter notebook that contains all inputs and generated snippets of code.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/filter_result.png}
%     \caption{\chat{} reasons and decomposes the complex task into compatible steps before execution. 
%     % The outputs of running a \sys{} pipeline. 
%     Users may specify whether they want to optimize for cost, quality, or runtime.
%     }
%     \label{fig:chat-extract}
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/demo-results.png}
    \caption{The output of the execution of the scientific discovery use case. Users can visualize both output records, as well as summary information about the plan execution such as the operators chosen and the total pipeline cost and runtime. 
    }
    \label{fig:demo-results}
\end{figure}

% to create or select a schema that precisely defines the attributes to be extracted or transformed. During this step, users provide a broad schema description—optionally clarifying field requirements, providing attribute descriptions, and indicating which fields are mandatory. If the schema definition is ambiguous or lacks detail, \chat{} prompts the user to supply additional information. This iterative process ensures that \sys{} has a precise understanding of the dataset's structure and context for downstream processing tasks.

% \subsection{Task Execution}
% Finally, we demonstrate how \chat{} orchestrates the user-defined tasks over the dataset. The system interprets the steps needed—such as converting documents to a target schema or filtering records based on user-defined criteria—and executes the pipeline. 

\input{code/final_code}
