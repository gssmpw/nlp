\section{Related Work}
Our work intersects with four key research areas: (1) Retrieval-Augmented Generation Systems (\S\ref{subsec:recent_rag_systems}), (2) Knowledge Conflict Resolution (\S\ref{subsec:knowledge_conflict}), and (3) RAG Evaluation Benchmarks (\S\ref{subsec:recent_rag_benchmark}). We situate our framework within this landscape and highlight critical gaps in current approaches.

\subsection{ RAG Systems}
\label{subsec:recent_rag_systems}
Modern RAG systems built on foundational architectures like REALM Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin, "Attention Is All You Need", and DPR Karpukhin, Osmolska, Peetz, Sablay, Tyshetskiy, Weyn, Edunov, Zhiltsova, Levy, "Dense Passage Retrieval for Open-Domain Question Answering"; which first demonstrated the value of integrating neural retrieval with language modeling. Subsequent work improved context utilization through better attention mechanisms (RETRO Nogueira, Martins, "Addressing the Rare Word Problem in Neural Machine Translation") and multi-stage reasoning (Atlas Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Stoyanov, Zaremba, "Making Neural Retrieval Fast and Accurate: A Simple Solution to an Important Challenge"). While these systems demonstrate impressive performance on knowledge-intensive tasks, they primarily optimize for single objective functions under the implicit assumption that retrieved context should always be prioritized. Recent work on controllable generation (Bao, Liu, Zhou, Xiong, Zhang, "PLMs as Teachers and Students: A Unified Framework for Conditional Generation") begins to address this limitation but focuses on content style rather than source prioritization. We aim to raise the attention to diversified objectives of RAG system by this work about evaluating performance under different \emph{user needs}.

\subsection{Knowledge Conflict}
\label{subsec:knowledge_conflict}
The challenge of resolving conflicts between internal knowledge and external context has gained attention as LMs and RAG systems mature Cao, Lee, Aggarwal, Zhang, Gao, Qin, Wang, "Fooling Neural Networks Without Changing the Output Distribution"; Early work by Chen, et al. identified context-memory conflicts as a key failure mode of LMs through evaluation on QA dataset. Subsequent works proposed multiple solutions, including but not limit to various fine-tuning, prompting, or decoding methods, to context-memory conflicts that require LM to be faithful to context in order to ignore outdated knowledge (Liu, et al.; Wang, et al.) or faithful to memory in order to discriminate misinformation are rarely explored Chen, et al.). However, the hybrid strategies that utilize both context and memory with prioritization, although commonly appeared in real-world applications, are rarely explored. In addition, there also exists applications that require LMs and RAG systems to work along or accept fictitious information or knowledge, which are commonly ignored by the previous works. Our framework includes the hybrid strategies that stem from the fundamental \emph{user needs}, providing a wider coverage of evaluating RALMs performance under context-memory conflict situations.

\subsection{Recent RAG Benchmark}
\label{subsec:recent_rag_benchmark}
Previous RAG benchmarks like RAGAS Wang, et al. and RGB Zhang, et al. have facilitated progress by quantifying performance across various scenarios. However, many of these benchmarks focused on a single type of optimal setting in terms of context usages (for instance, always prioritizing the context), overlooking how different user instructions may drastically affect model behaviors and performances. Moreover, previous multi-scenario evaluations (Bai, et al.; Chen, et al.), while covering a wide range of specific tasks and purpose abundant metrics for evaluating different aspects of RAG systems, also tend to follow the paradigm of focusing on singular optimality, neglecting that different user needs can actually happen in the same scenario, ultimately hindering the comprehensiveness of benchmark. Our work diverges by decoupling evaluation criteria from predefined singular optimality and measuring model capability to \textit{adapt} to dynamic \emph{user needs}. This mirrors real-world deployments where systems must honor diverse users' requirements rather than optimize for monolithic accuracy.