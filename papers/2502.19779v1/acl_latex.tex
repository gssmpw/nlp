% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\newcommand{\zxl}[1]{\textcolor{blue}{[Xinlu: #1]}}
\newcommand{\ywh}[1]{\textcolor{red}{wenhao: #1}}
\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}       % enable \mathbb, \mathcal
\usepackage{bold-extra}    % enable \texttt{\textbf{}}
\usepackage{bm}            % enable bold font in math mode
\renewcommand{\UrlFont}{\ttfamily\small}  % define fonts for URLs
\usepackage{multirow}      % merge rows in tables
\usepackage{booktabs}      % borders in tabular
\usepackage{enumitem}      % list margins
\usepackage{subcaption}    % create sub-tables and sub-figures
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[hang,flushmargin]{footmisc}  % minimize footnote indentation
\newcommand{\LN}{\linebreak\noindent}    % manage inline spacing
\usepackage{listings}
\usepackage{xcolor}
\renewcommand{\thesubfigure}{(\arabic{subfigure})}
\captionsetup[subfigure]{labelformat=simple}

\definecolor{codegray}{gray}{0.9}
\lstdefinestyle{jsonstyle}{
    backgroundcolor=\color{codegray},   
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=none,                    
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}



\newcommand{\zhiyu}[1]{\textcolor{red}{[zhiyu: #1]}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Do Retrieval-Augmented Language Models Adapt to Varying User Needs?}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Peilin Wu \\
%   Department of Computer Science \\
%   University of Texas at Dallas \\
%   % Affiliation / Address line 3 \\
%   \texttt{peilin.wu@utdallas.edu} \\\And
%   Zhiyu Zoey Chen \\
%   Department of Computer Science \\
%   University of Texas at Dallas \\
%   % Affiliation / Address line 3 \\
%   \texttt{zhiyu.chen2@utdallas.edu} \\}
{\author{Peilin Wu\textsuperscript{1}\thanks{Equal contribution}, Xinlu Zhang\textsuperscript{2}\footnotemark[1], Wenhao Yu\textsuperscript{3}, Xingyu Liu\textsuperscript{4}, Xinya Du\textsuperscript{1}, Zhiyu Zoey Chen\textsuperscript{1} \\
\textsuperscript{1}Department of Computer Science, The University of Texas at Dallas,\\
\textsuperscript{2}Department of Computer Science, University of California, Santa Barbara,\\
\textsuperscript{3}Tencent AI Seattle Lab,\textsuperscript{4} 
Harvard University\\
\texttt{\{peilin.wu, zhiyu.chen2\}@utdallas.edu}
}}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Recent advancements in Retrieval-Augmented Language Models (RALMs) have demonstrated their efficacy in knowledge-intensive tasks. However, existing evaluation benchmarks often assume a single optimal approach to leveraging retrieved information, failing to account for varying user needs. This paper introduces a novel evaluation framework that systematically assesses RALMs under three user need cases—Context-Exclusive, Context-First, and Memory-First—across three distinct context settings: Context Matching, Knowledge Conflict, and Information Irrelevant. By varying both user instructions and the nature of retrieved information, our approach captures the complexities of real-world applications where models must adapt to diverse user requirements. Through extensive experiments on multiple QA datasets, including HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find that restricting memory usage improves robustness in adversarial retrieval conditions but decreases peak performance with ideal retrieval results and model family dominates behavioral differences. Our findings highlight the necessity of user-centric evaluations in the development of retrieval-augmented systems and provide insights into optimizing model performance across varied retrieval contexts. We will release our code and URAQ dataset upon acceptance of the paper.

\end{abstract}
\section{Introduction}
Recent advances in Language Models (LMs) have yielded impressive performance in knowledge‐intensive tasks through Retrieval Augmented Generation (RAG) \cite{10.5555/3495724.3496517}, including Real‐time Question Answering \cite{wang-etal-2024-rear}, Educational Tutoring \cite{han2024improvingassessmenttutoringpractices}, and Personal Assistants \cite{wang-etal-2024-crafting}. While these applications showcase RAG’s versatility, they also demand LMs that can adapt to diverse user needs—expressed via instructions on whether to prioritize external evidence or internal knowledge. For instance, Real‐time QA may rely heavily on updated external facts, whereas tutoring may draw more on the model’s conceptual understanding. Despite this potential, current RAG methods still struggle with identifying relevant references \cite{laban2024summaryhaystackchallengelongcontext}, resolving knowledge conflicts \cite{wang2024astuteragovercomingimperfect}, and reasoning effectively \cite{islam-etal-2024-open}. These challenges underscore the need for robust evaluation strategies capturing how well Retrieval Augmented Language Models (RALMs) adapt to evolving user requirements.
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Figures/Motivation.pdf}
\caption{User needs may have different directions on how to use retrieved context and internal memory as knowledge sources and most of the previous work only focused on a small portion of them.}
\vspace{-0.2in}
\label{fig:motivation}
\end{figure}

Even though existing RAG/RALM benchmarks \citep{yu2024evaluationretrievalaugmentedgenerationsurvey, es2023ragasautomatedevaluationretrieval, 10.1609/aaai.v38i16.29728}—including those that focus on multi‐scenario evaluations \citep{friel2024ragbenchexplainablebenchmarkretrievalaugmented, zhu2024ragevalscenariospecificrag}—have advanced retrieval‐augmented evaluation, they typically assume a single “optimal” approach to external information (e.g., always relying on retrieved context). This narrow perspective overlooks how diverse user instructions can dramatically alter model behavior and performance within the same scenario. In medical fact‐checking, for instance, one user might demand answers derived only from peer‐reviewed studies, while another relies on the model’s internal knowledge—even if these sources conflict \citep{Miao2024IntegratingRG}. Such constraints underscore an urgent question: \textit{how can we systematically evaluate LMs under varying context usage requirements to reflect different user needs?}

In this paper, we present a simple yet effective  \emph{evaluation framework} that rigorously examines how Retrieval‐Augmented Language Models (RALMs) respond to varying user instructions and context conditions. We consider three generic \textbf{user cases}—\textbf{(1)~Context-Exclusive }, \textbf{(2)~Context-First}, and \textbf{(3)~Memory-First }—to capture different degrees of reliance on external information versus internal knowledge. Alongside these cases, we vary the \textbf{context settings}—\textbf{(a)~Context Matching}, \textbf{(b)~Knowledge Conflict}, and \textbf{(c)~Information Irrelevant}—to represent scenarios where retrieved materials may align with, contradict, or fail to address the query. By intersecting user cases with distinct context conditions, we more closely mirror the complexities of real‐world applications, where both the user’s priorities and the reliability of retrieved information can shift dramatically. This approach reveals how each scenario might alter the correct response—especially when context and memory conflict—an aspect often overlooked in previous work.

% Existing RAG/RALM benchmarks (\citealt{yu2024evaluationretrievalaugmentedgenerationsurvey}; \citealt{es2023ragasautomatedevaluationretrieval}; \citealt{10.1609/aaai.v38i16.29728}), including benchmarks specifically on multi-scenario evaluations (\citealt{friel2024ragbenchexplainablebenchmarkretrievalaugmented}; \citealt{zhu2024ragevalscenariospecificrag}) have largely focused on a single type of optimal setting in terms of context usages (for instance, always prioritizing the context), overlooking how different user instructions may drastically affect model behaviors and performances in the same scenario, ultimately hindering the comprehensiveness of benchmark. For example, in medical fact-checking, users may demand answers derived solely from peer-reviewed studies (retrieved context) or standard parametric medical knowledge, even when these sources conflict \cite{Miao2024IntegratingRG}.


% In this paper, we propose a novel evaluation framework that rigorously assesses the behavior and performance of Retrieval-Augmented Language Models (RALMs) within various context settings, from high-to-low dependency on external context, across diverse user needs. Specifically, this framework varies the user needs by three generic \textbf{user cases}—\textbf{1. Context-only}: RALMs rely solely on context; \textbf{2. Context-priority}: RALMs prioritize context as the primary source and use memory as backup; \textbf{3. Memory-priority}: RALMs prioritize memory as the primary source and use context as backup. In addition, the retrieval contents that fed into the LLMs can be varied and not always optimal in terms of whether the context is relevant or agree to the memory. To further explore how the variation of context affects the final result in combination with different \textbf{user cases}, this framework also varies the context situation by three \textbf{context settings}—\textbf{a. Match}: Context's information is relevant \zhiyu{to the question} and matches with RALMs' internal knowledge; \textbf{b. Conflict}: Context's information is relevant but conflicts with RALMs' internal knowledge; \textbf{c. Irrelevant}: Context's information is irrelevant with RALMs' internal knowledge \zhiyu{to the question?}. The combinations of \textbf{user cases} and \textbf{context settings} enable us to correctly evaluate RALMs with multiple optimality, especially because the shift on \textbf{user cases} may change the ground truth response of LMs when dealing with context-memory conflicts \cite{xu-etal-2024-knowledge-conflicts}, which the previous works have neglected. \zhiyu{Give more language to emphasize these combinations better mimic comprehensive real world scenarios.}


We conduct extensive experiments on our curated dataset, URAQ, along with two public datasets, DisentQA \cite{neeman-etal-2023-disentqa} and HotpotQA \cite{yang-etal-2018-hotpotqa}, evaluating two model families, Llama3.1 \citealt{grattafiori2024llama3herdmodels} and Qwen2.5 \citealt{qwen2025qwen25technicalreport}, across various model sizes and numbers of retrieved contexts. Our findings reveal that:  
1) \textbf{Current LMs struggle to satisfy diverse user needs}, achieving below 50\% accuracy across all datasets, with Llama-3.1-8B-Instruct occasionally nearing 0\%. 
2)  \textbf{Contextual restriction alters performance}: Restricting models to rely solely on retrieved context improves LMs performance when external context content is different from internal memory  by up to 23\% accuracy difference on the same model but decreases the performance under ideal retrieval by up to 17\%. 3) \textbf{Model family dominate behavioral differences}: Model family contributes the majority of behavioral differences, which further emphasize the importance of choosing the correct model for different user needs through proper evaluations. For instance, under retrieval with knowledge conflict, Llama3.1 models exhibit a performance decline of up to 10.2\% in accuracy when transitioning from Context-First and Memory-First to the Context-Exclusive case, whereas Qwen2.5 models show the opposite pattern, with an improvement of nearly 20\%.




% 3) \textbf{Instruction adherence vs. reasoning trade-off}: \zhiyu{I'm confused about this point. Is `following instructions on formatting' relevant to our main topic?} Under hybrid user cases, models may refuse to follow instructions on formatting as the input length increases, which allows more room for explicit Chain-of-Through reasoning during inference and increases performance.
% 3) Scaling Model Size Improves Consistency Across Retrieval Conditions: Our findings reveal that larger models demonstrate greater stability in performance across varying retrieval contexts and user needs. For instance, Qwen2.5-72B-Instruct achieves up to 37.7\% higher accuracy compared to its smaller counterpart, Qwen2.5-7B-Instruct. Similarly, Llama-3.1-70B-Instruct shows a 36.3\% improvement over Llama-3.1-8B-Instruct.


\section{Related Work}
Our work intersects with four key research areas: (1) Retrieval-Augmented Generation Systems (\S\ref{subsec:recent_rag_systems}), (2) Knowledge Conflict Resolution (\S\ref{subsec:knowledge_conflict}), and (3) RAG Evaluation Benchmarks (\S\ref{subsec:recent_rag_benchmark}). We situate our framework within this landscape and highlight critical gaps in current approaches.

\subsection{ RAG Systems}
\label{subsec:recent_rag_systems}
Modern RAG systems built on foundational architectures like REALM \cite{10.5555/3524938.3525306} and DPR \cite{karpukhin-etal-2020-dense}, which first demonstrated the value of integrating neural retrieval with language modeling. Subsequent work improved context utilization through better attention mechanisms (RETRO \cite{Borgeaud2021ImprovingLM}) and multi-stage reasoning (Atlas \cite{10.5555/3648699.3648950}). While these systems demonstrate impressive performance on knowledge-intensive tasks, they primarily optimize for single objective functions under the implicit assumption that retrieved context should always be prioritized. Recent work on controllable generation (\citealt{li-etal-2023-large}; \citealt{ashok2024controllabletextgenerationinstructiontuning}; \citealt{wei2024instructraginstructingretrievalaugmentedgeneration}) begins to address this limitation but focuses on content style rather than source prioritization. We aim to raise the attention to diversified objectives of RAG system by this work about evaluating performance under different \emph{user needs}.

\subsection{Knowledge Conflict}
\label{subsec:knowledge_conflict}
The challenge of resolving conflicts between internal knowledge and external context has gained attention as LMs and RAG systems mature \cite{xu-etal-2024-knowledge-conflicts}. Early work by \citet{longpre-etal-2021-entity} identified context-memory conflicts as a key failure mode of LMs through evaluation on QA dataset. Subsequent works proposed multiple solutions, including but not limit to various fine-tuning, prompting, or decoding methods, to context-memory conflicts that require LM to be faithful to context in order to ignore outdated knowledge (\citealp{shi-etal-2024-trusting}; \citealp{zhou-etal-2023-context}) or faithful to memory in order to discriminate misinformation are rarely explored \cite{xu-etal-2024-earth}. However, the hybrid strategies that utilize both context and memory with prioritization, although commonly appeared in real-world applications, are rarely explored. In addition, there also exists applications that require LMs and RAG systems to work along or accept fictitious information or knowledge, which are commonly ignored by the previous works. Our framework includes the hybrid strategies that stem from the fundamental \emph{user needs}, providing a wider coverage of evaluating RALMs performance under context-memory conflict situations.

\subsection{Recent RAG Benchmark}
\label{subsec:recent_rag_benchmark}
Previous RAG benchmarks like RAGAS \cite{es2023ragasautomatedevaluationretrieval} and RGB \cite{10.1609/aaai.v38i16.29728} have facilitated progress by quantifying performance across various scenarios. However, many of these benchmarks focused on a single type of optimal setting in terms of context usages (for instance, always prioritizing the context), overlooking how different user instructions may drastically affect model behaviors and performances. Moreover, previous multi-scenario evaluations (\citealt{friel2024ragbenchexplainablebenchmarkretrievalaugmented}; \citealt{zhu2024ragevalscenariospecificrag}), while covering a wide range of specific tasks and purpose abundant metrics for evaluating different aspects of RAG systems, also tend to follow the paradigm of focusing on singular optimality, neglecting that different user needs can actually happen in the same scenario, ultimately hindering the comprehensiveness of benchmark. Our work diverges by decoupling evaluation criteria from predefined singular optimality and measuring model capability to \textit{adapt} to dynamic \emph{user needs}. This mirrors real-world deployments where systems must honor diverse users' requirements rather than optimize for monolithic accuracy.

\section{Evaluation Framework}
\label{sec:framework}

% \ywh{Maybe two subsections: (1) user cases (2) context settings. Measuring Performance Via Combination should be some experimental analysis}

In this section, we present our evaluation framework to measure Language Models' (LMs') performance. Specifically, we first describe the design of three abstract \textbf{user need cases} (\S\ref{subsec:user_cases}) representing different typical \textit{user needs} expressed by context usages. Then, we describe the three \textbf{context settings} (\S\ref{subsec:context_settings}) motivated by practical usage conditions in which the relevancy of the context varies and may conflict with the LMs' memory.
% \zxl{I think you should highlight the user case, instead of " that involve both the model's own knowledge (memory) and the externally retrieved information (context)." }

\subsection{User Need Cases}
\label{subsec:user_cases}
% By examining the variety of user instructions on context usages, we find that they all depend on how much LMs should rely on retrieved information versus its own knowledge. In essence, these instructions vary along a spectrum from high or total dependency on context to high dependency on memory (total dependency on memory does not require RAG). Critically, each of these scenarios can be understood as specifying which source has priority. Compared to simply instructing using both context and memory,\zxl{I am confused about this part, why this may case hallucination? } which may cause confusion for LMs and prone to hallucination, this prioritization is important because it allows for better clarity in generation, especially for questions with definite answers. Recognizing this hierarchy of information sources clarifies that all user needs naturally fall into one of three categories as follows. 
% \zxl{I think you can concise this part. For example: To evaulate ...., we conduct four cases:}
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Figures/Framework.pdf}
\vspace{-0.2in}
\caption{An illustration of the framework with an example question with its possible retrieved context and the ground truth answer under each situation. According to different user needs and context settings, the ground truth answer can be different.}
\vspace{-0.2in}
\label{fig:framework}
\end{figure}



To evaluate RALMs under varying \textit{user needs}, we define a spectrum based on reliance on contextual information versus internal memory. This spectrum, illustrated in Figure~\ref{fig:framework}, consists of three distinct \textbf{user needs}, determined by how LMs are instructed. Example prompts are in Appendix~\ref{sec:example_user_need}.

 \paragraph{Context-Exclusive:} LMs must strictly base answers on retrieved context, responding ``I don’t know'' if context is unhelpful. Prompts enforce unconditional adherence to external evidence, eliminating reliance on internal knowledge.
 \paragraph{Context-First:} LMs prioritize retrieved context but fall back on memory when no relevant context exists. Prompts establish context as primary, with memory as a secondary source.
 \paragraph{Memory-First:} LMs rely on internal memory unless uncertain, in which case they defer to retrieved context. Prompts invert the hierarchy, making memory the default unless confidence is low.





% To evaluate RALMs under varying \textit{user needs}, we establish a spectrum ranging from reliance on contextual information to sole dependence on the language model's own memory. Based on the degree of dependency on different information sources, we define three distinct \textbf{user needs} by instructing LM differently as the horizontal axis in Figure \ref{fig:framework}. Example prompts can be found in Appendix \ref{sec:example_user_need}.

% \paragraph{Context-Exclusive}
% LMs must answer \textit{strictly} based on the retrieved context. If the retrieved context is not helpful for the question, the model should respond with “I don’t know.” This scenario is designed to measure the model’s ability to rely solely on external evidence when instructed. To enforce this, prompts explicitly mandate unconditional adherence to the provided context (e.g., “You MUST utilize the RELEVANT knowledge even if INCORRECT.”), thereby eliminating reliance on internal knowledge.

% \paragraph{Context-First}
% LMs should answer the question based on the retrieved context if relevant information is present. Only when no relevant context exists should the model rely on its memory. This case measures the model’s prioritization of external evidence over its own knowledge. Prompts for this case (e.g., “If the provided information is RELEVANT, you MUST use it. Otherwise, you MUST utilize your own knowledge.”) establish a fallback mechanism: context is primary, and memory serves as a secondary source only when context is irrelevant or absent.

% \paragraph{Memory-First}
% LMs should answer the question primarily using its memory. If the model is confident it has relevant knowledge, it should rely on it. However, if the model is \emph{unconfident} about its own memory, it should then use the retrieved context. This scenario tests how well the model balances its internal knowledge and external evidence, especially under uncertainty.
% Corresponding prompts (e.g., ``You MUST use your own knowledge if CONFIDENT'') invert the hierarchy: memory dominates unless confidence is low, in which case context becomes the back-up solution.


% \zxl{ Why you said there are 4 cases but only introduce 3?  In addition, you can help a plot to show the spectrum with the user cases if needed. Otherwise, considering refer figure 2. }
% \paragraph{Case 0: Autonomous}
% LMs should decide which knowledge source should be prioritized without any guidance in the prompt. This represents the situations that users do not specify their \textit{user needs} and can be seen as a default user case.

% \ywh{Maybe we missed one setting. The user does not mention anything, and LLM should decide which knowledge to use, like a general RAG setting.}

\subsection{Context Settings}
\label{subsec:context_settings}
To better analyze RALMs under real-world situations with sub-optimal retrieval results, it is beneficial to also consider the spectrum of context quality on top of each user case. For any context retrieved in an RAG system, we can assess its quality based on two primary dimensions: 1) \textbf{Relevance to the Task or Question}: Whether the retrieved context contains information that is semantically or factually related to the question. 2) \textbf{Alignment with LM’s Internal Knowledge}: Whether the retrieved context supports or contradicts the knowledge that the model already possesses. These two dimensions create a 2 × 2 space (relevant/irrelevant × match/conflict), but due to the nature of irrelevant context (which neither supports nor contradicts), the space reduces to three distinct context settings.

\paragraph{Conext Matching.}
There is at least one retrieved context \emph{relevant} to the question and \emph{matches} with the LM’s memory. This is an ideal situation for RALMs as correct knowledge is presented in both the external context and the internal memory.  

\paragraph{Knowledge Conflict.}
There is at least one retrieved context \emph{relevant} to the question but \emph{conflicts} with the LM’s memory. This setting simulates context-memory knowledge conflicts \cite{xu-etal-2024-knowledge-conflicts} and tests the model’s ability on generation with strictly following instructions regarding context usages.

\paragraph{Information Irrelevant.}
All retrieved contexts are unrelated to the question. This setting simulates the Needle-In-a-Haystack \cite{laban2024summaryhaystackchallengelongcontext} situation and tests the model’s ability on knowledge selection. Models are expected to avoid hallucinating and admit knowledge gaps by responding with ``I don’t know'' under Case 1 instructions or relying on its memory in Case 2 and 3.

\section{ Experimental Setup}
% In this section, we first introduce the setups of our experiments, including datasets, model configuration, and prompt formatting, targeting RALMs' open-domain performance, which can represent most of the real-world scenarios. Then, we introduce the result and analysis of experiments, \zhiyu{This paragraph is not necessary, may cut to reduce length if needed.}

\label{subsec:datasets}

\subsection{Datasets}
\paragraph{Overview of QA Datasets.}  This experiment employs three QA datasets: HotpotQA \cite{yang-etal-2018-hotpotqa}, DisentQA \cite{neeman-etal-2023-disentqa}, and our synthetic User-focused Retrieval-Augmented QA (URAQ). To assess RALMs' real-world performance, we use HotpotQA and DisentQA versions augmented with conflicted knowledge by \citet{shaier-etal-2024-adaptive} for the retrieval-content knowledge conflict setting. While valuable, these benchmarks lack controlled knowledge boundaries and have varying question difficulty, limiting evaluation. They also rely on long-document contexts, restricting retrieval diversity. URAQ addresses these issues with uniformly difficult questions and concise factual contexts, enabling evaluation under extensive retrieval without exceeding LMs' context windows.

\begin{table}[htbp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|ccc} 
\toprule
\bf Dataset & \bf Num. of Context Sequence & \bf Size & \bf Max. Token \\
\midrule
Synthetic & 1, 10, 25, 50, 100, 250, 500, 1000 & 231 & 25k \\
DisentQA & 1, 2, 4, 8, 16, 32, 64 & 1415 & 59k \\
HotpotQA & 1, 2, 4, 8, 16, 32 & 1274 & 35k \\
\bottomrule
\end{tabular}}

\caption{Basic information of the three datasets used in the experiment. For the sequence of the number of retrieved context,the number of retrieved context is increased in a exponential way until the average number of tokens at the highest number of each sequence reaches around 20k in order to balance the effectiveness of the experiment on long context and the consumption of computational resources. The Max. Token, which refers to the number of maximum tokens among all samples for a dataset, may vary based on context retrieved.}
\label{tab:dataset_info}
\end{table}

\paragraph{URAQ Construction.}  We construct URAQ by first generating simple, distinct knowledge statements via GPT-4o-mini \cite{openai2024gpt4ocard} and removing near-duplicates using SentenceBERT \cite{reimers-gurevych-2019-sentence}, then creating both original and “manipulated” versions by substituting key information or adding negations. For each knowledge pair, we produce a question requiring 1–5 reasoning steps and two separate answers (one from the original knowledge, one from the manipulated), ultimately selecting the 4-hop subset for the final dataset. A detailed description of this procedure is provided in Appendix \ref{sec:dataset_construction}, ensuring the pipeline’s applicability across various domains.


\subsection{Context Setting and Prompt Formatting}
\paragraph{Retrieval Context Setting.}  To examine how performance changes with varying amounts of retrieved context, rather than using a fixed retrieval count as in previous work \cite{zhu2024ragevalscenariospecificrag}, we evaluate LM performance by exponentially increasing the retrieval count across different datasets, shown in Table \ref{tab:dataset_info}. To assess the models’ tolerance to distracting or irrelevant contexts, we ensure that only one relevant context is present for both the context-matching and conflicting settings, randomly positioned within the prompt. All other contexts are selected from a pool of \emph{original} and \emph{manipulated} knowledge that excludes any information directly related to the current question.


\paragraph{Prompt Formatting}\label{subsec:prompt_formatting} The input prompt is organized as $(I, C, Q)$ or $(I_f, I_u, C, Q)$, where $I$ is the instruction and can be separated into formatting instruction $I_f$ and user needs instructions $I_u$, $C=\{c_1, c_2, ..., c_n\}$ is a series of retrieved context with retrieval number of $n$, and $Q$ is the question. Given an input $(I_f, I_u, C, Q)$, we have the following prompting template: 
\begin{equation}
    \text{<sys>}I_f\oplus I_u\text{</sys>}\text{<user>}C\oplus Q\text{</user>}
\end{equation}
where <sys></sys> and <user></user> denote the system prompt and the user prompt. Among all data samples, the $I_u$ and $C$ may change according to the \textbf{user case} and \textbf{context setting}, while the $I_f$ remaining the same by instructing models to directly output a simple answer that is either a numeric value, a boolean ("yes" or "no"), or an entity, as described in Section \ref{subsec:datasets}. A complete example prompt template is in the Appendix \ref{sec:example_input_prompt}.

% Following the format of DisentQA and HotpotQA, we obtain testing samples in QA-pair format with different answers associated with different knowledge source for URAQ by: 1) Generating a list of simple, short common knowledge with gpt-4o-mini \cite{openai2024gpt4ocard}. 2) Remove each possible pair of knowledge that is similar to each other by measuring their cosine similarity through SentenceBERT \cite{reimers-gurevych-2019-sentence} with a threshold of 0.5. 3) Generate one \emph{manipulated} knowledge for each \emph{original} knowledge by substituting key information or negation through prompting gpt-4o-mini \cite{openai2024gpt4ocard}. 4) Generate question-answer pairs (QA) that contain one question and two separate answers based on both the \emph{original} and \emph{manipulated} knowledge, with 1 to 5 \emph{reasoning steps} as the difficulty metric to answer the question. The QA pairs is designed that the answer can only be a numeric value, a boolean ("yes" or "no"), or an entity. We select the subset of QA pairs with 4-hop questions as the final synthetic dataset due to its better quality compared with other subsets. 5) For each question, select the correct ground truth answer that complies the framework's requirement as the example shown in Figure \ref{fig:framework}. A detailed procedure for curating URAQ is described in the Appendix \ref{sec:dataset_construction}. In such a way, the synthetic dataset curation pipeline is widely applicable for any domain, enhancing the generalizability of this evaluation framework.


% To evaluate how performance evolves with increasing amounts of retrieved context, for all datasets, instead of selecting a fixed count of retrieval like the previous works, we take a list of the number of retrieved context increased exponentially and evaluate all the models on each of the number of retrieved context, shown in Table \ref{tab:dataset_info}. To examine the models' tolerance of distracting/irrelevant contexts, we strictly allow only one relevant context to exist in the contexts for \textbf{Setting a and b} with randomized positions in the prompt. The rest of the context are guaranteed to be irrelevant from the question by randomly selecting knowledge from a set of all \emph{original} and \emph{manipulated} knowledge excluding the knowledge associated with the current question.


\subsection{Evaluation  Metrics}
 To rigorously assess user-need awareness across across different user needs with different retrieval content, we test each user need with identical questions but varying the guidance on context usage, spanning three levels:

\paragraph {1. Overall User Need Accuracy}: The model must satisfy \textit{all user needs} simultaneously. Specifically, each test sample can be counted as correct if and only if the model can answer the same question under \textit{all user cases and all context settings}. In this way, we can evaluate the LMs in a generic setting.

\paragraph {2. Case-Level Accuracy}
For each individual user need, we assess the model’s performance across multiple context settings. A test sample is considered correct only if the model consistently provides the correct answer \textit{across all variations of context under that specific user need}. This evaluation method ensures that the model demonstrates reliability in addressing a given requirement, independent of the context variations presented.

\paragraph {3. Setting-Specific Accuracy} In each context setting, test sample is considered correct if the model obtain the answer is same as the ground truth in the corresponding setting. 
By evaluating models at these three levels, we obtain a comprehensive view of how consistently and robustly they meet each user need across different contextual requirements.



% \subsection{Evaluation  Metrics}


% Language models often exhibit a preference for either memory-based knowledge or contextual cues, sometimes performing well in one setting but failing in another \cite{longpre-etal-2021-entity, jin2024tugofwarknowledgeexploringresolving}. To rigorously assess user-need awareness across diverse conditions, we propose testing each user need with identical questions but varying the guidance on context usage—thereby ensuring that performance differences reflect how well models adhere to each user’s needs rather than task- or question-specific factors. Our evaluation spans three levels:

% \paragraph {1. Overall User Need Accuracy}: The model must satisfy \textit{all user needs} simultaneously. Specifically, each test sample can be counted as correct if and only if the model can answer the same question under \textit{all user cases and all context settings}. In this way, we can evaluate the LMs in a generic setting.


% \paragraph {2. Case-Level Accuracy}
% For each individual user need, we assess the model’s performance across multiple context settings. A test sample is considered correct only if the model consistently provides the correct answer \textit{across all variations of context under that specific user need}. This evaluation method ensures that the model demonstrates reliability in addressing a given requirement, independent of the context variations presented.

% \paragraph {3. Setting-Specific Accuracy}  A test sample is considered correct if the model obtain the answer is same as the ground thruth.


% By evaluating models at these three levels, we obtain a comprehensive view of how consistently and robustly they meet each user need across different contextual requirements.

\subsection{Evaluation model }
To evaluate user‐need awareness, we conduct comprehensive experiments on 4 Instruct LMs using two distinct open-source LLM families—Llama 3.1 \cite{grattafiori2024llama3herdmodels}, and Qwen 2.5 \cite{qwen2025qwen25technicalreport}—which vary in model size. We set the maximum context length to 128k, the temperature to 0, and Top‐p to 1, while leaving all other configurations at their default values which defers to the Appendix D.
\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{Figures/overall.pdf}
\caption{Overall user need performance curve of all models on each dataset.}
\label{fig:overall_performance}
\end{figure*}

\section{Result \& Analysis}


\subsection{Overall Performance}


We start our analysis on the overall performance across all three user cases by using the overall user need accuracy to access the capacity of  user need awareness on different LMs. The results are shown in Figure \ref{fig:overall_performance}.


% \paragraph{LMs perform poorly under all datasets.} Overall, we observe that all models achieve an accuracy under 50\% for all datasets,  with the smaller size models on URAQ and DisentQA getting as low as close to 0 accuracy. This demonstrates the necessity of this evaluation allowing us to judge the performance of current LLMs under different \emph{user needs} and improve accordingly.

\paragraph{LMs struggle across all datasets, and URAQ is more challenging than existing benchmarks} 
No model surpasses 50\% accuracy across different user needs, with Llama-3.1-8B-Instruct performing particularly poorly, nearing 0\%. While performance is low across all datasets, URAQ proves significantly more challenging than DisentQA and HotpotQA. The best-performing model, Qwen2.5-72B-Instruct, scores up to 44.4\% lower on URAQ. URAQ’s diverse external information, multi-step reasoning, and conflicting knowledge make retrieval and synthesis more challenging for LLMs, emphasizing the need for stronger reasoning capabilities to handle complex real-world user needs.




% \paragraph{LMs Struggle Across All Datasets, and URAQ is More Challenging Than Existing Benchmarks}  
% No model exceeds 50\% accuracy across context settings, with Llama-3.1-8B-Instruct performing particularly poorly, nearing 0\%.  Though performance varies across datasets, with URAQ proving significantly harder than DisentQA and HotpotQA. The best-performing model, Qwen2.5-72B-Instruct, scores up to 44.4\% lower on URAQ, likely due to its multi-step reasoning demands and misleading knowledge. These findings emphasize the need for more robust reasoning capabilities in LLMs to handle real-world, multi-faceted user needs effectively.  
% .

% This highlights the difficulty instruction-tuned LLMs face in handling diverse user needs that require complex reasoning. Our strict evaluation reveals generalization gaps overlooked by prior benchmarks \cite{friel2024ragbenchexplainablebenchmarkretrievalaugmented}.  

% \paragraph{LMs perform poorly across all datasets.} Across three datasets, none of the models exceed 50\% accuracy under various context settings. Llama-3.1-8B-Instruct performs particularly poorly, nearing 0\% on all three datasets. This stark underperformance highlights how even today’s instruction-tuned LLMs struggle with diverse user needs, requiring complex, multi-step reasoning. The strict user-need evaluation exposes weaknesses missed by previous benchmarks \cite{friel2024ragbenchexplainablebenchmarkretrievalaugmented}, revealing gaps in LLMs' generalization. Our findings underscore the need for more robust reasoning in LLMs to effectively handle real-world, multi-faceted user needs.

% Across different datasets, we observe a notable performance disparity, with models achieving significantly lower accuracy on the URAQ dataset compared to DisentQA and HotpotQA. For instance, the best-performing model (Qwen2.5-72B-Instruct) exhibits up to 44.4\% lower accuracy on URAQ relative to DisentQA and 37.5\% lower accuracy relative to HotpotQA when using the same number of retrieved contexts. This trend likely emerges because the URAQ dataset presents a greater challenge, as it is designed to require multi-step reasoning and explicitly includes manipulated knowledge that may mislead models. 

\paragraph{LMs behave differently at the model-family level but similarly within the same family.} Overall, we observe distinct patterns in LMs across different model families on two out of three datasets. Specifically, there is a clear divergence in behavior between the Qwen2.5 and Llama-3.1 model families on DisentQA and HotpotQA. The Qwen2.5-7B-Instruct and its larger 72B variant exhibit an increasing trend in accuracy as the number of retrieved contexts grows, whereas the Llama-3.1-8B-Instruct and 70B-Instruct models follow a decreasing trend. This difference likely stems from model-specific behavioral tendencies and a potential trade-off between instruction-following capability and multi-hop reasoning ability, which we further discuss in Section \ref{subsec:general_performance_nonideal}. On URAQ, although both model families exhibit declining trends, the Llama-3.1 models experience a steeper drop in performance compared to the Qwen2.5 models. For example, the performance gap from 1 to 10 retrieved contexts in the Qwen family is around relative accuracy 1.5\%, whereas for the Llama-3.1 family, it is 9.1\%, indicating a more pronounced decline.

\paragraph{Larger models exhibit better user needs awareness.} Within the same model family, larger models (70B+/72B) consistently outperform their smaller counterparts (7B/8B), demonstrating improved user needs awareness. Notably, Qwen models exhibit up to a 37.7\% accuracy improvement, while Llama models achieve a 36.3\% gain on DisentQA, highlighting the substantial benefits of scaling model size. However, it is also important to note that the magnitude of performance improvement diminishes as the number of retrieved contexts increases, suggesting potential saturation effects or increased difficulty in effectively leveraging larger context windows.

\subsection{General Performance for Each User Need}
\label{subsec:general_performance_nonideal}

\begin{figure}[t!]
\centering

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/abc_hotpotqa_qwen.pdf}
\caption{Case-Level Accuracy curve of Qwen2.5 on HotpotQA dataset.}
\label{fig:abc_hotpotqa_qwen}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/abc_hotpotqa_llama.pdf}
\caption{Case-Level Accuracy curve of Llama-3.1 on HotpotQA dataset.}
\label{fig:abc_hotpotqa_llama}
\end{subfigure}
\caption{Case-Level Accuracy curve of Qwen2.5 and Llama-3.1-70B on HotpotQA }
% \caption{Case-Level Accuracy curve of Qwen2.5 and Llama-3.1-70B on HotpotQA dataset. These two model as the representative show the performance drop from \emph{Context-Exclusive} user need to \emph{Context or Memory-First}.}
\label{fig:setting_abc}
\end{figure}

To further analyze the behavior of LMs on each user need, we measure the curve of \emph{Case-Level Accuracy} versus number of retrieved context on HotpotQA, as shown in Figure \ref{fig:setting_abc}. We defer other two datasets to Figure \ref{fig:setting_abc_total} in the Appendix \ref{sec:accuracy_curve_of_all_datasets}.

\paragraph{Restricting memory usage improves real-world performance.} We find that the model's accuracy increased from \emph{Context or Memory-First} to \emph{Context-Exclusive} case, meaning that limiting the usage of internal memory improves the lower limit of general performance, possibly because \emph{Context-Exclusive} strategy forces strict reliance on retrieved evidence and prevents hallucinations. This trend is particularly evident in Qwen2.5 models on HotpotQA dataset that maintain at least 7.7\% increase in accuracy. However, as the number of context increases, the  performance gap gradually shrinks and may even be inverted on Llama-3.1 models where \emph{Context-Exclusive} accuracy drops by up to 12.5\% when the number of retrieved context increases to 32. 

% \paragraph{Trade-off exists between Instruction Following and Reasoning.} 
\paragraph{Models Tend to Be Lazy with More Context.} To investigate the counterintuitive pattern in which the accuracy of \emph{Context or Memory-First} cases increases as the number of retrieved contexts grows across all models, we analyze the impact of different context settings in both cases, as shown in Figure \ref{fig:case23_hotpotqa_qwen72b}.
Interestingly, the \emph{Information Irrelevant} setting appears to contribute to this upward trend. By randomly sampling 100 cases across different retrieval context lengths, we observe that models are easily influenced by irrelevant information, often generating responses such as “no,” “none,” or “0.” However, as more context is retrieved, models exhibit emergent Chain-of-Thought reasoning capabilities. This phenomenon may stem from a form of "lazy" behavior, where models, instead of actively identifying the correct context, increasingly rely on their own memory as the context length grows. We defer the case study example into Appendix \ref{sec:case_study_of_model_lazy}.  


% This is due to the model 


% To understand why this occurs, we analyze 100 randomly sampled questions from three datasets. Specifically, we focus on cases where the model answered incorrectly with a small number of retrieved contexts but correctly when provided with the maximum number. Under the \emph{Context or Memory-First} case with the \emph{Information Irrelevant} setting, we observe a distinct pattern: with fewer retrieved contexts, models frequently output isolated negative words such as “no,” “none,” or “0.” However, as the number of retrieved contexts increases, models shift towards explicit Chain-of-Thought reasoning, often producing responses like, “There is no information about... According to my knowledge, the answer is... (correct answer).”


\begin{figure}[htbp!]
\centering
\includegraphics[width=\columnwidth]{Figures/case23_hotpotqa_qwen72b.pdf}
\caption{Accuracy curve of Qwen2.5-72B-Instruct on HotpotQA dataset under all context settings with \emph{Context-First} and \emph{Memory-First}.}
\label{fig:case23_hotpotqa_qwen72b}
\end{figure}


% To investigate this trend, we first examine the \emph{Setting-Specific Accuracy} ($Acc_c$) curves for different context settings under the \emph{Context or Memory-First} case. 



% \zxl{need further revision}

% The trend of \emph{Context or Memory-First} cases accuracy counterintuitively increases as the number of retrieved context increases. To further explore on this trend, we first examine the \emph{Setting-Specific Accuracy} $Acc_c$ curve of each context setting under \emph{Context or Memory-First} case to see which one actually contributes the most to the shape of curve in Figure \ref{fig:setting_abc} and find that only \emph{Information Irrelevant} matches with the increasing trend as shown in Figure \ref{fig:case23_hotpotqa_qwen72b}. Then, we explore why this trend happened by randomly sampled and analyze 100 questions from all three datasets that answered incorrectly with smallest number of retrieved context but correctly with largest number of retrieved context for \emph{Context or Memory-First} case with \emph{Information Irrelevant} context setting. We find that models tend to output single negative words like “no”, “none” or “0” when number of retrieved context is small. When number of retrieved context get bigger, for the same question, models tend to ignore instructions for formatting by outputting explicit Chain-of-Thought processes like “There is no information about… According to my knowledge, the answer is … (correct answer)”. This can be seen as, with small number of retrieved context, models tend to follow instructions with the cost of thinking and reasoning ability. With big number of retrieved context, because of the introduced noise from long context, models tend to not follow the instructions, which actually delimit its ability to reasoning as it allows more space for explicit Chain-of-Thought output and thus improves performance.

\begin{figure}[t!]
\centering

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/a_hotpotqa_qwen.pdf}
\caption{Setting-Specific Accuracy curve of Qwen2.5 models on HotpotQA dataset with context matching setting.}
\label{fig:a_disentqa_qwen}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/a_hotpotqa_llama.pdf}
\caption{Setting-Specific Accuracy curve of Llama-3.1 models on HotpotQA dataset with context matching setting.}
\label{fig:a_disentqa_llama}
\end{subfigure}

\caption{Setting-Specific Accuracy curve of Qwen2.5 and Llama-3.1 models on HotpotQA dataset with context matching setting. These two model as the representative demonstrate the large and small performance drop from \emph{Context or Memory-First} user need to \emph{Context-Exclusive}.}
\label{fig:setting_a}
\end{figure}


\subsection{Individual Setting Performance}
To provide more detailed analysis on models' behavior on the context setting-level, we measure the \emph{Setting-Specific Accuracy} $Acc_c$ curve for each user need case, categorizing them into two groups: \textbf{Optimal Context}, where the provided context aligns with the model's memory, and \textbf{Challenging Context}, where the context is conflicting or irrelevant.




\subsubsection{Performance on Optimal Context}
\label{sssec:general_performance_ideal}
Under the \emph{Context Matching} setting, where the model receives fully relevant and correct context, we assess its maximum potential performance. This defines an \textbf{optimal performance}, isolating the model’s ability to utilize ideal context without retrieval constraints.

\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cc|cc} 
\toprule
\multirow{2}{*}{\bf Dataset} & \multicolumn{2}{c|}{\bf Llama-3.1-Instruct} & \multicolumn{2}{c}{\bf Qwen2.5-Instruct} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& \bf 8B (\%) & \bf 70B (\%) & \bf 7B (\%) & \bf 72B (\%) \\
\midrule
Synthetic & 52 & 74 & 85 & 97 \\
DisentQA & 70 & 84 & 92 & 98 \\
HotpotQA & 63 & 76 & 84 & 95 \\
\bottomrule
\end{tabular}}
\caption{Percentage of errors that is "I don't know" among the shortest 100 randomly selected samples that under \emph{Context Matching} setting that is \textbf{incorrect} for \emph{Context-Exclusive} user need and \textbf{correct} for \emph{Context or Memory-First}. A number exceeding 50 hints that the model is leaning towards reject answering when it has trouble locating the source or deducing the answers.}
\label{tab:a_error_type}
\end{table}

\paragraph{Restricting memory usage limits optimal performance.} Based on the results in Figure \ref{fig:setting_a}, we observe that models’ accuracy declines when internal memory is restricted under the \emph{Context-Exclusive} strategy. This effect is more pronounced in the Qwen2.5 family, where Qwen2.5-7B-Instruct experiences up to a 12.1\% accuracy drop from \emph{Context or Memory-First} to \emph{Context-Exclusive}, whereas the Llama-3.1 family shows only a slight decrease, with Llama-3.1-8B-Instruct losing up to 4.1\%.


\paragraph{LLMs exhibit self-protective conservatism. }To examine the accuracy drop under the \emph{Context-Exclusive} setting, we analyze 100 randomly selected cases with up to four retrieved context segments, where the model provides an incorrect answer under \emph{Context-Exclusive} but a correct one under \emph{Context or Memory-First}. Errors are categorized into two types: (1) the model refuses to answer by stating, "I don't know," and (2) the model generates an incorrect hallucinated response. Table \ref{tab:a_error_type} reports the percentage of refusals.



We observe that models overwhelmingly prefer rejection over hallucination when they struggle to locate relevant context, with refusal rates exceeding 50\% across all models and datasets.  This tendency is particularly strong in the Qwen2.5 family, where the 7B and 72B models reject answers in over 85\% of cases, with Qwen2.5-72B-Instruct reaching a 98\% rejection rate on DisentQA. Similarly, the Llama-3.1 models exhibit high rejection rates, ranging from 70\% to 84\% on DisentQA. This conservative behavior may stem from its training objectives or alignment strategies prioritizing answer correctness over speculative responses.





% To explore the reason for such drop, we randomly select 100 samples that, for the same question, the \emph{Context-Exclusive} answer is \textbf{incorrect} and \emph{Context or Memory-First} answer is \textbf{correct}, from the shortest 3 level of the number of retrieved contexts (in our case, retrieved context equals to 1, 2, 4), and examine the error types. With ideal context retrieval, a model can either reject answering the question by outputting "I don't know" or answer with hallucinations not equal to the correct answer. As shown in Table \ref{tab:a_error_type}, we find that across all datasets, models overwhelmingly prefer rejecting the question rather than hallucinating when they struggle to locate the relevant context. This behavior is especially dominant in the Qwen2.5 family, where the 7B and 72B models reject answers in over 85\% of cases, reaching 98\% rejection for Qwen2.5-72B-Instruct on DisentQA. The Llama-3.1 models exhibit a lower but still significant rejection rate, with 70–84\% rejection for DisentQA and lower values for other datasets. Given that models of similar size typically exhibit comparable accuracy, we can conclude that the primary cause of accuracy drop is excessive conservatism in rejecting answers when context information is ambiguous or sparse. The Qwen2.5 family, in particular, appears overly cautious, favoring rejection over potential hallucination. This conservative behavior may stem from its training objectives or alignment strategies prioritizing answer correctness over speculative responses.



\begin{figure}[h!]
\centering

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ab_hotpotqa_qwen.pdf}
\caption{Setting-Specific Accuracy curve of Qwen2.5 model family on HotpotQA dataset with knowledge conflict.}
\label{fig:ab_synthetic_qwen}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ab_hotpotqa_llama.pdf}
\caption{Setting-Specific Accuracy curve of Llama-3.1 model family on HotpotQA dataset with knowledge conflict.}
\label{fig:ab_synthetic_llama}
\end{subfigure}

\caption{Setting-Specific Accuracy curve of Qwen2.5 and Llama-3.1 model family on HotpotQA dataset with knowledge conflict. While two models have similar accuracy on \emph{Context or Memory-First} case, Llama models has lower accuracy on \emph{Memory-Exclusive} compared with \emph{Context or Memory-First} and Qwen models has higher accuracy.}
\label{fig:setting_ab}
\end{figure}

\subsubsection{Performance with Challenging Context}
% Evaluating Knowledge Conflict or Information Irrelevant in isolation can bias results, as models may favor memory over retrieved context (\citealp{longpre-etal-2021-entity}; \citealp{jin2024tugofwarknowledgeexploringresolving}). A model excelling in Context Matching but failing in Knowledge Conflict suggests over-reliance on memory. To ensure robustness, we measure Setting-Specific Accuracy, requiring correctness in both Context Matching and Challenging Context settings.
For performance under \emph{Knowledge Conflict} or \emph{Irrelevant Context}, we realize that evaluating only the performance of single context setting in isolation can introduce bias and skewed interpretations due to LMs preference on using memory than context or vise versa (\citealp{longpre-etal-2021-entity}; \citealp{jin2024tugofwarknowledgeexploringresolving}), resulting performing perfectly in one setting but failed in other. For example, succeeding in \emph{Irrelevant Context} but failing in \emph{Matching Context} may suggest that the model is prone always relying on memory without actually complying with the instructions to use retrieved context. Therefore, we measure the \emph{Setting-Specific Accuracy} $Acc_c$ for Challenging Context in a way that the same question need to be also answered correctly in \emph{Context Matching} settings, ensuring the robustness of evaluation. Such measuring method is applied to all experiments in this section shown in Figure \ref{fig:setting_ab} and \ref{fig:setting_ac}.

% \paragraph{Knowledge source prioritization does not alter performance.} Across all models and datasets, we observe minimal divergence between accuracy curve of \emph{Context-First} and \emph{Memory-First} cases, suggesting that instruction prioritization has limited impact on performance when both knowledge sources are permitted with knowledge conflict's existence or no relevant context existence as shown in Figure \ref{fig:setting_ab} and \ref{fig:setting_ac}. 

\begin{figure}[htbp!]
\centering

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ac_hotpotqa_qwen.pdf}
\caption{Setting-Specific Accuracy curve of Qwen2.5 model family on HotpotQA dataset with irrelevant context.}
\label{fig:ac_hotpotqa_qwen}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ac_hotpotqa_llama.pdf}
\caption{Setting-Specific Accuracy curve of Llama-3.1 model family on HotpotQA dataset with irrelevant context.}
\label{fig:ac_hotpotqa_llama}
\end{subfigure}

\caption{Setting-Specific Accuracy curve of Qwen2.5 and Llama-3.1 model family on HotpotQA dataset with irrelevant context.}
\label{fig:setting_ac}
\end{figure}

\paragraph{Model family dominates behavioral difference.} Model families still exhibit distinct behavioral patterns: When knowledge conflict exists as Figure \ref{fig:setting_ab}, Llama3.1 models show degradation of performance from \emph{Context-First} and \emph{Memory-First} to \emph{Context-Exclusive} case for up to 10.2\% accuracy, while Qwen2.5 models demonstrate the opposite trend with an increase close to 20\%. This behavior suggests fundamental differences in knowledge reliance—Llama3.1 appears more context-dependent, struggling to effectively integrate memory, whereas Qwen2.5 leverages its parametric knowledge more effectively when permitted. Such difference also appears in the as Figure \ref{fig:setting_ac} with \emph{Information Irrelevant} setting, Llama models exhibit significant decreasing accuracy on \emph{Context-Exclusive} strategy with increasing context length for up to 60.1\%, whereas Qwen exhibit almost no loss in performance, for the same reason as discussed in Section \ref{subsec:general_performance_nonideal}.
% This observation is different than previous works that p
% For all models and all datasets, case 2 and 3 have almost the same performance, although 2b and 3b requires different answers. Priority of context and memory does not change the performance much as long as memory is allowed in instructions. 

% For Llama performance decreases if the internal memory is allowed to use because accuracy of case 1 is slightly better than 2 and 3. For Qwen, performance increases if the internal memory is allowed to use because accuracy of case 1 is worse than 2 and 3. This may also related to the question whether the model is more rely on memory or context. Different model families may have different tendencies on whether more rely on memory or context.

% The knowledge selection evaluation uncovers critical trade-offs between conservatism and reasoning flexibility.  By analyzing error types based on Table \ref{tab:a_error_type}, This divergence stems from fundamental response strategies: Qwen2.5’s conservative "I don’t know" responses enhances \textbf{Case 1} reliability but hinders \textbf{Cases 2/3} by prematurely abandoning valid memory-based reasoning. This also agrees with Section \ref{subsec:generation_performance} that model family-level behavior may determine their behavior under different user cases.

% 1. For this evaluation, all models under all datasets follows the same trend, decreasing for case 1 and increasing for case 2 and 3. Larger model has better performance and smaller slope for case 2 and 3. For case 1 larger model has small improvement. 

% 2. Reject answering vs. hallucination affects behavior on this. Qwen2.5 family has excellent result for case 1 but worse performance than Llama for case 2 and 3. This is probably due to the behavior that Qwen is more incline to say “I don’t know” where as Llama will try to give out an answer, which is also shown in the setting a Upper Limit section.
 

% \section{Discussion}

% \subsection{Impact on Context-Only Senarios}

% \subsection{Context-Primacy vs. Knowledge-Primacy}

% \subsection{Influence of Increasing Context Length}

% \subsection{Model Family and Size Difference}

\section{Conclusion}

We introduce an evaluation framework for RALMs that systematically assesses performance across diverse user needs and context settings. By decomposing user instructions into three generic user need cases (Context-only, Context-priority, Memory-priority) and three context settings (Match, Conflict, Irrelevant), our framework provides comprehensive insights into model capabilities and limitations. Our analysis covers overall user requirements, case-level evaluations, and the impact of varying context contents across different context lengths. The findings highlight the need for user-centric evaluations and architectural innovations to enhance RAG system reliability and real-world applicability.

% We present a novel evaluation framework for Retrieval-Augmented Language Models (RALMs) that systematically assesses model performance under diverse user needs and context settings. By decomposing user instructions into three generic user cases (Context-only, Context-priority, Memory-priority) and three context settings (Match, Conflict, Irrelevant), our framework provides comprehensive insights into model capabilities and limitations. Through extensive experiments across synthetic and real-world datasets with varying context lengths, we reveal three key findings: (1) Restricting models to context-only usage increases robustness but sacrifices peak performance, while prioritization between context and memory shows minimal impact on overall accuracy; (2) Model family fundamentally determines knowledge reliance patterns, with Llama3.1 favoring context and Qwen2.5 leveraging parametric knowledge more effectively as examples. In addition, in the experiment, we found that increasing context length introduces critical trade-offs between instruction adherence and flexible reasoning, challenging conventional assumptions about long-context utilization, which is worth to be explored in the future. These findings emphasize the need for user-centric evaluation protocols and architectural innovations that balance instruction compliance with adaptive reasoning. Our framework and findings provide actionable insights for developing more reliable RAG systems and designing next-generation benchmarks that better reflect real-world usage scenarios.

\section{Limitations}
While our study provides a structured evaluation framework for Retrieval-Augmented Language Models (RALMs) under diverse user needs and retrieval conditions, several limitations remain. Our experiments rely on three datasets: HotpotQA, DisentQA, and the synthetic URAQ dataset. While these datasets cover various knowledge retrieval challenges, they may not fully capture the diversity of real-world retrieval scenarios, particularly in highly specialized domains such as medical or legal applications. Additionally, the synthetic URAQ dataset, although designed to control retrieval complexity, may not generalize perfectly to naturally occurring retrieval conflicts found in real-world settings. In addition, our results are based on evaluations of two model families, Llama-3.1 and Qwen-2.5, across different sizes. While these models are representative of current state-of-the-art retrieval-augmented systems, our conclusions may not generalize to other architectures, such as retrieval-heavy fine-tuned transformers or proprietary models with distinct retrieval and reasoning mechanisms. Future work should extend this analysis to a broader range of models.

\section{Ethics Statement}
Our framework is designed to assess how well RALMs adhere to different user instructions, reflecting real-world applications where users may have distinct expectations regarding knowledge usage. However, models may still exhibit disparities in their ability to satisfy certain user needs, especially in adversarial retrieval settings. We recommend further research on mitigating disparities and enhancing fairness in retrieval-augmented systems. The datasets used in our experiments include HotpotQA, DisentQA, and the newly introduced synthetic URAQ dataset. While these datasets contain diverse question-answer pairs, we acknowledge that biases may be present in both retrieved and internally generated content. We have taken measures to minimize biases by curating synthetic data with balanced question difficulty and by evaluating model performance under varying retrieval conditions. However, residual biases in training corpora or retrieval mechanisms may influence the observed model behavior. One of our primary motivations is to analyze how models handle conflicting or irrelevant retrieved information. While our evaluation reveals scenarios where models fail to distinguish misinformation or exhibit hallucination tendencies, our work does not actively promote the generation or dissemination of false information. Instead, we highlight the need for more robust mechanisms to ensure factual consistency, particularly in knowledge-conflict scenarios. By conducting this study, we aim to advance the ethical design of retrieval-augmented models while encouraging further research on mitigating biases, improving factual robustness, and ensuring alignment with diverse user needs.

\section*{Acknowledgments}
We acknowledge the use of the GPT-4o language model provided by OpenAI in the final stages of manuscript preparation. This tool was employed exclusively for identifying and correcting typographical and grammatical errors, ensuring clarity and precision in the written presentation. Its use was strictly limited to linguistic refinement and did not impact the study’s conceptual framework, research methodology, data analysis, or conclusions. All intellectual contributions and substantive content remain those of the authors.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix


% \section{Perilin's setting}

% To holistically evaluate RALMs, each user case and context setting should be tested with identical questions, varying only the guidance on contextual reliance based on user needs and the provided context. This ensures that performance differences stem from the LM’s adherence to user needs rather than question or task-specific factors. However, evaluating only the performance of single settings in isolation can introduce bias and skewed interpretations. This is especially important because LM may prefer use memory than context and vise versa (\citealp{longpre-etal-2021-entity}; \citealp{jin2024tugofwarknowledgeexploringresolving}), performing perfectly in one setting but failed in other. Only considering considering each setting independently can not show how the model's real ability across various user needs. 
% % For example, if a model answers correctly in a (match) but fails in b (conflict), it indicates that the model cannot override its memory when instructed to prioritize context. Conversely, succeeding in Setting \textbf{b} but failing in \textbf{a} suggests that the model is prone to hallucination without actually complying with the instructions. 
% To address this issue, we propose that for each specific task or question in evaluation, it can be counted as correct only if LMs generate correct output under both the ideal and non-ideal context settings. This approach allows us to maintain the original goal of evaluating LMs' adherence to instructions under different \textbf{user cases} and \textbf{context settings}, while also consider the robustness of LMs in real-world situations.

% We focus on three key ways of combining the above user cases and settings to quantify performance and one singular context setting as the upper limit of performance and control group:

% \zxl{ Should we write each setting into the analysis instead of separate ?}
% \paragraph{Setting (a): Performance with Ideal Context}
% Evaluating only \textbf{Setting a} for each case measures the general performance in an ideal retrieval scenario. By giving the correct, relevant, and matched context, we measure the maximum potential accuracy for each user case. This serves as an \textbf{upper limit} because it isolates the model’s ability to utilize context when it is both relevant and consistent with its memory. A high accuracy here indicates the model’s theoretical potential under optimal conditions. Formally, given the language model as $LM()$, the accuracy for \textbf{Setting a} $Acc_a$ can be written as:
% \begin{equation}
% Acc_a = \frac{\sum_{i=1}^{N} \mathbb{I}(\text{LM}_i(C_a) = A^a_i)}{N}
% \end{equation}
% where \( N \) is the total number of test samples; \( \mathbb{I}(\cdot) \) is the indicator function (1 if the model generates the correct answer, 0 otherwise); \( C_a \) is the series of external context for question \( i \) under \textbf{Setting a}; \( A^a_i \) is the ground-truth answer for question \( i \) under \textbf{Setting a}.

% \paragraph{Setting (a \& b \& c): Performance with Non-Ideal Context}
% This combination assesses the model’s \textit{robustness across diverse context qualities} and thus serves as a \textbf{lower limit}. It is calculated by requiring the model to answer correctly in all three settings (\textbf{a}, \textbf{b}, and \textbf{c}) for the same question. Success here demands adherence to instructions regardless of context relevance or alignment: the model must use a matched context (\textbf{a}), override conflicting memory with context (\textbf{b} for Cases 1/2) or vice versa (\textbf{Setting b} for \textbf{Case 3}), and handle irrelevant context (\textbf{Setting c}) appropriately. This stringent evaluation identifies the lower bound of real-world reliability, as failure in any setting exposes vulnerabilities. For example, hallucinating when context is absent (\textbf{Setting c}) or being influenced by conflicting evidence (\textbf{Setting b}). The accuracy for this combination $Acc_{abc}$ can be written as:
% \begin{align}
% Acc_{abc} &= \frac{1}{N} \sum\limits_{i=1}^{N} 
% \Big( \mathbb{I}(\text{LM}_i(C_{a}) = A^a_i) \land  \notag \\
%      & \quad \mathbb{I}(\text{LM}_i(C_{b}) = A^b_i) \land \mathbb{I}(\text{LM}_i(C_{c}) = A^c_i) \Big)
% \end{align}
% where \( C_a \), $C_b$, $C_c$ are the series of external context for question \( i \) under \textbf{Setting a, b, c} respectively; \( A^a_i \), $A^b_i$, $A^c_i$ is the ground-truth answer for question \( i \) under \textbf{Setting a, b, c} respectively.

% \paragraph{Setting (a \& b) Generation-with-conflict Performance}
% This combination tests the model’s ability of \textbf{generation} under context-memory conflicts. It is calculated by requiring the model to answer correctly under \textbf{Setting a} and \textbf{Setting b} for the same question. Models are expected to matched context (\textbf{a}) and prioritizing context over conflicting memory (\textbf{b} for \textbf{Cases 1/2}) or memory over conflicting context (\textbf{Setting b} for \textbf{Case 3}). Failing on \textbf{Setting b} while succeeding on \textbf{Setting a} implies the model cannot follow the instruction to trust the conflicting retrieved context over its own memory (\textbf{Case 1/2}) or to trust its memory over conflicting context (\textbf{Case 3}). The accuracy for this combination $Acc_{abc}$ can be written as:
% \begin{align}
% Acc_{ab} &= \frac{1}{N} \sum\limits_{i=1}^{N} 
% \Big( \mathbb{I}(\text{LM}_i(C_{a}) = A^a_i) \land  \notag \\
%      & \quad \mathbb{I}(\text{LM}_i(C_{b}) = A^b_i)\Big)
% \end{align}

% \paragraph{Setting (a \& c) Knowledge Selection Performance}
% This combination evaluates the model’s \textit{ability of selecting relevant context}. It is calculated by testing performance in \textbf{Setting a} (relevant context) and \textbf{Setting c} (irrelevant context). For \textbf{Case 1}, the model must answer from context in \textbf{a} and respond with “I don’t know” in \textbf{c} to show that it actually understand which context is relevant and reject answering if no context is relevant; for \textbf{Cases 2/3}, it should use knowledge from context in \textbf{a} and from memory in \textbf{c} according to their instructions, which are easier settings compared with \textbf{Case 1}. Failure in either \textbf{a} or \textbf{c} shows that the model is unable to detect and select the related context, suggesting a high possibility of hallucination.
% \begin{align}
% Acc_{ac} &= \frac{1}{N} \sum\limits_{i=1}^{N} 
% \Big( \mathbb{I}(\text{LM}_i(C_{a}) = A^a_i) \land  \notag \\
%      & \quad \mathbb{I}(\text{LM}_i(C_{c}) = A^c_i) \Big)
% \end{align}


\section{Detailed Dataset Curation Procedure}
\label{sec:dataset_construction}

Below, we provide a step-by-step description of how we constructed the URAQ dataset:

\subsection{Knowledge Generation}
We used \emph{gpt-4o-mini}~\cite{openai2024gpt4ocard} to produce an initial list of short, simple knowledge statements. These statements are general facts (e.g., ``A hummingbird can hover in mid-air'' or ``Blue whales are the largest animals on Earth'') rather than domain-specific or specialized knowledge. The generated statements were deliberately kept concise and straightforward to facilitate subsequent manipulation and question generation.

\subsection{Redundancy Filtering}
Since GPT-based generators can produce highly similar or paraphrased statements, we employed \emph{SentenceBERT}~\cite{reimers-gurevych-2019-sentence} to measure the semantic similarity between all knowledge statements. Any pair of statements with a cosine similarity above 0.5 was considered near-duplicate and therefore removed to ensure diversity in the final knowledge set.

\subsection{Manipulated Knowledge Creation}
For every remaining ``original'' knowledge statement, we prompted \emph{gpt-4o-mini} to generate a \emph{manipulated} variant. The manipulation involved either substituting key elements (e.g., entities, numerical values, or critical details) or adding a negation that changes the statement's truth value (e.g., ``A hummingbird cannot hover in mid-air''). Each pair of statements (original vs.\ manipulated) thus serves as a pairwise contrast for subsequent question-answer (QA) creation.

\subsection{Question-Answer (QA) Generation}
From each pair of original and manipulated knowledge statements, we prompted \emph{gpt-4o-mini} to generate a question that requires between 1 to 5 \emph{reasoning steps} to arrive at an answer. The reasoning steps typically involve either numerical computation, logical inference, or entity comparison. Each question was tied to both the original and the manipulated knowledge. The resulting QA format consists of one question and two different answers: one correct answer derived from the original statement, and a second answer derived from the manipulated statement.

\subsection{Answer Format and Difficulty Selection}
We constrained valid answers to be either (i) a numeric value, (ii) a boolean (``yes'' or ``no''), or (iii) a single entity. Among the generated questions, those requiring 4-hop reasoning were chosen for the final dataset, as manual inspection suggested these exhibited higher quality and clearer multi-step logic compared to simpler or more complex variants.

\subsection{Final Ground Truth Assignment}
For each question, we designated the correct ground truth answer to be the one aligned with the \emph{original} knowledge statement. An example illustrating how this ground truth is integrated into the evaluation framework is provided in Figure~\ref{fig:framework} of the main paper.

By following these steps, we ensure that the URAQ dataset offers well-defined pairs of knowledge (original vs.\ manipulated) and corresponding multi-step questions designed to differentiate between factual and altered information. This framework supports a diverse range of potential use cases, from fact-checking systems to more elaborate multi-step reasoning models.

% \section{Dataset Construction Pipeline}
% In this section, we introduce our pipeline to construct the dataset. This dataset is designed to test retrieval-augmented question answering with various levels of reasoning depth, including scenarios where the model has conflicting or insufficient knowledge in context. Through this process, users can build a dataset tailored for their specific domain or task, or can evaluate the generalized performance of RALMs on open-domain dataset. We first elaborate on the data collection and filtering process, then discuss our question-answer pairs generation strategy.
% \zhiyu{You can claim that this data synthesize framework is widely applicable for any domains. And you still need to give the details of the one we used in this work. Put all the prompts used in appendix and link to them. }

% % A figure about data generation pipeline should be put here.

% % \subsection{Necessity of Synthetic Dataset}
% % \label{subsec:necessity_of_synthetic_dataset}
% % Observations that LLMs' memorization of real-world text suggest that data contamination can become a major problem because the model might appear to produce correct answers even when the retrieval source is conflict or irrelevant. This complicates the evaluation of whether the model is genuinely complying with our instructions rather than memorizing its stored knowledge directly. \zhiyu{Why this matters? We have these user needs here; even if they are indeed contaminated, the model still needs to follow instructions to rely on the context? I guess the contamination only affects reasoning evaluation?} At the same time, controlling data quality and difficulty matters when overly simple or loosely structured questions fail to reveal a model’s potential weaknesses in deeper reasoning or instruction following, especially when conflicting or irrelevant contexts are introduced. Creating a synthetic dataset directly addresses these issues by establishing clear knowledge boundaries, ensuring minimal overlap with data that might have been seen during training, and systematically varying question complexity. This approach makes it easier to pinpoint whether the model truly depends on the retrieved context, how it handles conflicts between its memory and the provided text, and the extent to which it can follow specific instructions across a wide range of scenarios.

% \paragraph{Knowledge Collection}
% \zhiyu{Not very clear about the distribution of knowledge in the synthesized dataset. } The pipeline begins by collecting a list of knowledge that agrees with the model’s internal memory. This list can be curated by either prompting the model to generate factual statements that it \emph{believes} to be true, or a list of knowledge coming from external source with a subsequent consensus check by directly asking the model about these knowledge. For this work's experiment, we utilize gpt-4o-mini \cite{openai2024gpt4ocard} to curate such knowledge list. If multiple LLMs are involved, any knowledge item that is \emph{not} confirmed by all models is removed. This is done to make sure that all models can be tested under the same set of knowledge.

% Next, the pipeline ensures that each pair of remaining knowledge items is sufficiently \emph{independent} by measuring semantic similarity between each possible pair of knowledge, which is done by SentenceBERT\cite{reimers-gurevych-2019-sentence} in this experiment. Any pair exceeding a similarity threshold that is set to 0.5 is discarded so that each piece of knowledge constitutes a distinct factual statement.

% To introduce \emph{conflicts} into the dataset as required by \textbf{Setting b}, the pipeline generates \emph{manipulated versions} of each knowledge item that contradicts the original statement with gpt-4o-mini \cite{openai2024gpt4ocard}. Each piece of original knowledge thus has one \emph{conflict} counterparts. \zhiyu{How specifically do you generate manipulated versions?}

% Finally, the pipeline verifies that the model indeed retains correct answers for each knowledge item in the dataset: single-hop QA pairs are generated from each statement, and any statement that the model fails to answer correctly in a closed-book scenario is removed. This ensures that the final dataset only includes knowledge that all participating models can confidently handle.

% \paragraph{Question-Answer Generation}
% Once the filtered knowledge base is ready, the pipeline generate question-answer (QA) pairs with different level of difficulties. Specifically, the pipeline prompt the LLM to 1 to 5 \emph{reasoning steps} as the metric of difficulty to answer the question based on both the \emph{original} and the \emph{manipulated} knowledge. For each QA pair, we require an explicit list of \emph{reasoning steps} (Chain-of-Thought), ensuring that the final answer can be derived from the knowledge. In this work, we prompt gpt-o1 \cite{openai2024openaio1card} for generating questions for different levels of difficulty. This process yields multi-hop QA data that tests the model’s ability to reconcile or detect conflicts between memory and retrieved context. 

% Below is an illustrative snippet of the prompt used:
% \begin{lstlisting}[language=Python, style=jsonstyle]
% Please generate a series of questions with answers based on a factual knowledge and a manipulated knowledge, with varying levels of difficulty from 1 to 5. The difficulty level corresponds to the number of reasoning steps required to reach the factual knowledge or the manipulated knowledge as the answer using Chain-of-Thought reasoning. For example:
% Difficulty 1: Requires one direct reasoning step.
% Difficulty 2: Requires two reasoning steps.
% Difficulty 3: Requires three reasoning steps.
% Difficulty 4: Requires four reasoning steps.
% Difficulty 5: Requires five reasoning steps.
% For each difficulty level, generate one unique question. Ensure that each question is solvable using the specified number of reasoning steps and the difficulty of the question is purely on the reasoning. The given knowledge cannot appear in the question itself. For each question, provide the steps for reasoning.
% \end{lstlisting}

% Finally, the generated QA pairs need to be fitted with the framework described in Section \ref{sec:framework} with correct ground truth answer. While all context settings under each user case use the same question, the answer using the \emph{original} knowledge as the basis should be adopted as the ground truth answer for \textbf{Case 1 Setting a}, \textbf{Case 2 Setting (a/c)}, and \textbf{Case 3 Setting (a/b/c)}; the answer using the \emph{manipulated} knowledge should be adopted as the ground truth answer for \textbf{Case 2 Setting (a/b)}; the ground truth answer of \textbf{Case 1 Setting c} should be "I don't know", as the example shown in Figure \ref{fig:framework}. 

\section{Example User Need Instructions}
\label{sec:example_user_need}
\subsection{Context-Exclusive}
\begin{lstlisting}[language=Python, style=jsonstyle]
You are a helpful AI assistant tasked with answering the given question ONLY based on the provided information. Here are the requirements to answer the question:

1. The answer should be a numeric value, a boolean ("yes" or "no"), or an entity.

2. You MUST directly provide the final answer within an <output> XML tag, without including any units if the answer is numeric.

3. You MUST utilize the RELEVANT knowledge contained in the provided information to answer the question, even if the knowledge is INCORRECT. If NONE of the provided information is RELEVANT to the question, you MUST output "I don't know".
\end{lstlisting}

\subsection{Context-First}
\begin{lstlisting}[language=Python, style=jsonstyle]
You are a helpful AI assistant tasked with answering the given question by referring to the provided information. Here are the requirements to answer the question:

1. The answer should be a numeric value, a boolean ("yes" or "no"), or an entity.

2. You MUST directly provide the final answer within an <output> XML tag, without including any units if the answer is numeric.

3. If the provided information contains RELEVANT knowledge that can be used to answer the question, you MUST utilize the provided information, even if the knowledge is INCORRECT.

4. If NONE of the provided information is RELEVANT to the question, you MUST utilize your own knowledge to answer the question.
\end{lstlisting}

\subsection{Memory-First}
\begin{lstlisting}[language=Python, style=jsonstyle]
You are a helpful AI assistant tasked with answering the given question by referring to the provided information. Here are the requirements to answer the question:

1. The answer should be a numeric value, a boolean ("yes" or "no"), or an entity.

2. You MUST directly provide the final answer within an <output> XML tag, without including any units if the answer is numeric.

3. You MUST utilize your own knowledge to answer the question if you are certain of the accuracy (e.g., factual information you are sure about). If you are UNSURE about your knowledge, you MUST use the relevant knowledge from the given information instead.
\end{lstlisting}

\section{Example Input Prompt}
\label{sec:example_input_prompt}
In this section, we introduce an example input prompt that is designed for \textbf{Case 1 Setting a} with 2 total retrieved context following the abstract input $(I_f, I_u, C, Q)$ in Section \ref{subsec:prompt_formatting}. The prompt is formatted with XML for both input and output. Specifically, the formatting instructions $I_f$ are separated into two parts: 1) The first and second instructions in the system prompt describing that the answer should be as simple as possible with XML format. 2) The instruction in the user prompt about format of context with an reinforcement of output format. The \emph{user need} instruction $I_u$ is at the third instruction in the system prompt. The retrieved context $C$ is all the sentences in the user prompt within <info> xml tag. Finally, the question $Q$ is at the last of user prompt within <question> xml tag. 
\begin{lstlisting}[language=Python, style=jsonstyle]
System Prompt:
You are a helpful AI assistant tasked with answering the given question ONLY based on the provided information. Here are the requirements to answer the question:
1. The answer should be a numeric value, a boolean ("yes" or "no"), or an entity.
2. You MUST directly provide the final answer within an <output> XML tag, without including any units if the answer is numeric.
3. You MUST utilize the RELEVANT knowledge contained in the provided information to answer the question, even if the knowledge is INCORRECT. If NONE of the provided information is RELEVANT to the question, you MUST output 'I don't know'.

User Prompt:
Given the following information enclosed in xml tag <info></info>, answer the question enclosed in xml tag <question></question>.
**
<info>2 + 2 equals 4.</info>
<info>The sun is the only star in our solar system.</info>
**
<question>If you have two apples and you get two more, how many apples do you have?</question>
\end{lstlisting}

\section{Case Study of Model Laziness}
\label{sec:case_study_of_model_lazy}
\begin{lstlisting}[language=Python, style=jsonstyle]
Question: How do plants create their food, photosynthesis or moonthesis?

Answer with 1 context: <output>No</output>

Answer with 1000 context: I think there might be a small mistake in your question. The correct term is "photosynthesis" not "moonthesis". Photosynthesis is the process by which plants, algae, and some bacteria make their own food using sunlight, water, and carbon dioxide. So, to answer your question: Plants create their food through photosynthesis.
\end{lstlisting}
Above question-answer pair from our synthetic URAQ dataset is an example of model relying on their own memory on long context and acquire better result than the result from shorter input. For answer with only 1 context, the model outputs a single word "No" with correct output format. For answer with 1000 context, the model provides clear thinking path with correct, affirmative answer without the desinated XML format for output. We also calculate the percentage of 100 randomly selected testing samples that has similar behaviors using Qwen2.5-72B-Instruct and Llama-3.1-70B-Instruct as shown in Table \ref{tab:model_lazy}. 

\begin{table}[htbp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cc} 
\toprule
 & \bf Context-First (\%) & \bf Memory-First (\%) \\
\midrule
Qwen2.5-72B-Instruct & 84 & 77 \\
Llama-3.1-70B-Instruct & 56 & 65 \\
\bottomrule
\end{tabular}}
\caption{Percentage of testing samples that answered with single negative output for short input but correct output with explicit reasoning, among 100 randomly selected samples that the question answered incorrectly with 1 retrieved context and correctly with 1000 retrieved context.}
\label{tab:model_lazy}
\end{table}

\section{Accuracy Curves of URAQ and DisentQA}
\label{sec:accuracy_curve_of_all_datasets}

% \subsection{Context Matching-Specific Accuracy}
\begin{figure}[htbp!]
\centering

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/a_synthetic_llama.pdf}
\caption{Accuracy curve of Llama-3.1 on URAQ dataset under \emph{Context Matching} setting.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/a_synthetic_qwen.pdf}
\caption{Accuracy curve of Qwen2.5 on URAQ dataset under \emph{Context Matching} setting.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/a_disentqa_llama.pdf}
\caption{Accuracy curve of Llama-3.1 on DisentQA dataset under \emph{Context Matching} setting.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/a_disentqa_qwen.pdf}
\caption{Accuracy curve of Qwen2.5 on DisentQA dataset under \emph{Context Matching} setting.}
\end{subfigure}

\caption{Accuracy curve of all models under \emph{Context Matching} setting.}
\label{fig:setting_a_total}
\end{figure}

% \subsection{Case-Level Accuracy}

\begin{figure}[htbp!]
\centering

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/abc_synthetic_llama.pdf}
\caption{Case-Level Accuracy curve of Llama-3.1 on URAQ dataset.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/abc_synthetic_qwen.pdf}
\caption{Case-Level Accuracy curve of Qwen2.5 on URAQ dataset.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/abc_disentqa_llama.pdf}
\caption{Case-Level Accuracy curve of Llama-3.1 on DisentQA dataset.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/abc_disentqa_qwen.pdf}
\caption{Case-Level Accuracy curve of Qwen2.5 on DisentQA dataset.}
\end{subfigure}

\caption{Case-Level Accuracy of all models.}
\label{fig:setting_abc_total}
\end{figure}

% \subsection{Context Matching \& Knowledge Conflict Accuracy}

\begin{figure}[htbp!]
\centering

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ab_synthetic_llama.pdf}
\caption{Accuracy curve of Llama-3.1 on URAQ dataset under \emph{Context Matching \& Knowledge Conflict} setting.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ab_synthetic_qwen.pdf}
\caption{Accuracy curve of Qwen2.5 on URAQ dataset under \emph{Context Matching \& Knowledge Conflict} setting.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ab_disentqa_llama.pdf}
\caption{Accuracy curve of Llama-3.1 on DisentQA dataset under \emph{Context Matching \& Knowledge Conflict} setting.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ab_disentqa_qwen.pdf}
\caption{Accuracy curve of Qwen2.5 on DisentQA dataset under \emph{Context Matching \& Knowledge Conflict} setting.}
\end{subfigure}

\caption{Accuracy curve of all models under \emph{Context Matching \& Knowledge Conflict} setting.}
\label{fig:setting_ab_total}
\end{figure}

% \subsection{Context Matching \& Information Irrelevant Accuracy}

\begin{figure}[htbp!]
\centering

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ac_synthetic_llama.pdf}
\caption{Accuracy curve of Llama-3.1 on URAQ dataset under \emph{Context Matching \& Information Irrelevant} setting.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ac_synthetic_qwen.pdf}
\caption{Accuracy curve of Qwen2.5 on URAQ dataset under \emph{Context Matching \& Information Irrelevant} setting.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ac_disentqa_llama.pdf}
\caption{Accuracy curve of Llama-3.1 on DisentQA dataset under \emph{Context Matching \& Information Irrelevant} setting.}
\end{subfigure}

\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{Figures/ac_disentqa_qwen.pdf}
\caption{Accuracy curve of Qwen2.5 on DisentQA dataset under \emph{Context Matching \& Information Irrelevant} setting.}
\end{subfigure}

\caption{Accuracy curve of all models under \emph{Context Matching \& Information Irrelevant} setting.}
\label{fig:setting_ac_total}
\end{figure}

\end{document}
