To evaluate representations for downstream MIR tasks, we follow the procedure as outlined in \cite{JukeMIR}: shallow supervised models (linear models and one-layer MLPs) are trained on each task using the representations as input features. A grid search over the following 216 hyperparameter configurations is conducted:

\begin{itemize}
    \item Feature standardization: \{off, on\}
    \item Model type: \{Linear, one-layer MLP with 512 hidden units\}
    \item Batch size: \{64, 256\}
    \item Learning rate: \{1e-5, 1e-4, 1e-3\}
    \item Dropout probability: \{0.25, 0.5, 0.75\}
    \item L2 regularization: \{0, 1e-4, 1e-3\}
\end{itemize}

Early stopping is applied based on task-specific metrics computed on validation sets, with the optimal model from each grid search evaluated on the task-specific test set. Loss functions are tailored to each task: cross-entropy for genre classification and key detection, independent binary cross-entropy for tagging, and mean squared error for emotion recognition.
