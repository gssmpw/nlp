@inproceedings{BaumannKey,
  author       = {Stefan A Baumann},
  title        = {{Deeper Convolutional Neural Networks and Broad 
                   Augmentation Policies Improve Performance in
                   Musical Key Estimation}},
  booktitle    = {{Proceedings of the 22nd International Society for 
                   Music Information Retrieval Conference}},
  year         = 2021,
  pages        = {42-49},
  publisher    = {ISMIR},
  address      = {Online},
  month        = nov,
  venue        = {Online},
  doi          = {10.5281/zenodo.5624477},
  url          = {https://doi.org/10.5281/zenodo.5624477}
}

@article{SimCLR,
  author       = {Ting Chen and
                  Simon Kornblith and
                  Mohammad Norouzi and
                  Geoffrey E. Hinton},
  title        = {A Simple Framework for Contrastive Learning of Visual Representations},
  journal      = {CoRR},
  volume       = {abs/2002.05709},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.05709},
  eprinttype    = {arXiv},
  eprint       = {2002.05709},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05709.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{CPC,
  author       = {A{\"{a}}ron van den Oord and
                  Yazhe Li and
                  Oriol Vinyals},
  title        = {Representation Learning with Contrastive Predictive Coding},
  journal      = {CoRR},
  volume       = {abs/1807.03748},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.03748},
  eprinttype    = {arXiv},
  eprint       = {1807.03748},
  timestamp    = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{JukeMIR,
  title={Codified audio language modeling learns useful representations for music information retrieval},
  author={Castellon, Rodrigo and Donahue, Chris and Liang, Percy},
  booktitle={ISMIR},
  year={2021}
}

@misc{Jukebox,
      title={Jukebox: A Generative Model for Music}, 
      author={Prafulla Dhariwal and Heewoo Jun and Christine Payne and Jong Wook Kim and Alec Radford and Ilya Sutskever},
      year={2020},
      eprint={2005.00341},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@misc{MERT,
      title={MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training}, 
      author={Yizhi Li and Ruibin Yuan and Ge Zhang and Yinghao Ma and Xingran Chen and Hanzhi Yin and Chenghao Xiao and Chenghua Lin and Anton Ragni and Emmanouil Benetos and Norbert Gyenge and Roger Dannenberg and Ruibo Liu and Wenhu Chen and Gus Xia and Yemin Shi and Wenhao Huang and Zili Wang and Yike Guo and Jie Fu},
      year={2024},
      eprint={2306.00107},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@article{MusicFM,
    title={A Foundation Model for Music Informatics},
    author = {Won, Minz and Hung, Yun-Ning and Le, Duc},
    journal={arXiv preprint arXiv:2311.03318},
    year={2023}
}

@misc{COLA,
      title={Contrastive Learning of General-Purpose Audio Representations}, 
      author={Aaqib Saeed and David Grangier and Neil Zeghidour},
      year={2020},
      eprint={2010.10915},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}



@article{HARES,
  author       = {Luyu Wang and
                  Pauline Luc and
                  Yan Wu and
                  Adri{\`{a}} Recasens and
                  Lucas Smaira and
                  Andrew Brock and
                  Andrew Jaegle and
                  Jean{-}Baptiste Alayrac and
                  Sander Dieleman and
                  Jo{\~{a}}o Carreira and
                  A{\"{a}}ron van den Oord},
  title        = {Towards Learning Universal Audio Representations},
  journal      = {CoRR},
  volume       = {abs/2111.12124},
  year         = {2021},
  url          = {https://arxiv.org/abs/2111.12124},
  eprinttype    = {arXiv},
  eprint       = {2111.12124},
  timestamp    = {Fri, 26 Nov 2021 13:48:43 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2111-12124.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{SpliceOut,
      title={SpliceOut: A Simple and Efficient Audio Augmentation Method}, 
      author={Arjit Jain and Pranay Reddy Samala and Deepak Mittal and Preethi Jyoti and Maneesh Singh},
      year={2021},
      eprint={2110.00046},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{slowfast,
      title={SlowFast Networks for Video Recognition}, 
      author={Christoph Feichtenhofer and Haoqi Fan and Jitendra Malik and Kaiming He},
      year={2019},
      eprint={1812.03982},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{nfnets,
      title={High-Performance Large-Scale Image Recognition Without Normalization}, 
      author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},
      year={2021},
      eprint={2102.06171},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{CLMR,
  author       = {Janne Spijkervet and
                  John Ashley Burgoyne},
  title        = {Contrastive Learning of Musical Representations},
  journal      = {CoRR},
  volume       = {abs/2103.09410},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.09410},
  eprinttype    = {arXiv},
  eprint       = {2103.09410},
  timestamp    = {Tue, 23 Mar 2021 16:29:47 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-09410.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{torchaudiomentations,
  author = {Asteroid Team},
  title = {torch-audiomentations},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/asteroid-team/torch-audiomentations}},
}

@Article{SampleCNN,
AUTHOR = {Lee, Jongpil and Park, Jiyoung and Kim, Keunhyoung Luke and Nam, Juhan},
TITLE = {SampleCNN: End-to-End Deep Convolutional Neural Networks Using Very Small Filters for Music Classification},
JOURNAL = {Applied Sciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {150},
URL = {https://www.mdpi.com/2076-3417/8/1/150},
ISSN = {2076-3417},
ABSTRACT = {Convolutional Neural Networks (CNN) have been applied to diverse machine learning tasks for different modalities of raw data in an end-to-end fashion. In the audio domain, a raw waveform-based approach has been explored to directly learn hierarchical characteristics of audio. However, the majority of previous studies have limited their model capacity by taking a frame-level structure similar to short-time Fourier transforms. We previously proposed a CNN architecture which learns representations using sample-level filters beyond typical frame-level input representations. The architecture showed comparable performance to the spectrogram-based CNN model in music auto-tagging. In this paper, we extend the previous work in three ways. First, considering the sample-level model requires much longer training time, we progressively downsample the input signals and examine how it affects the performance. Second, we extend the model using multi-level and multi-scale feature aggregation technique and subsequently conduct transfer learning for several music classification tasks. Finally, we visualize filters learned by the sample-level CNN in each layer to identify hierarchically learned features and show that they are sensitive to log-scaled frequency.},
DOI = {10.3390/app8010150}
}

@misc{CLAR,
      title={CLAR: Contrastive Learning of Auditory Representations}, 
      author={Haider Al-Tahan and Yalda Mohsenzadeh},
      year={2020},
      eprint={2010.09542},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@article{MusiCNN,
  author       = {Jordi Pons and
                  Xavier Serra},
  title        = {musicnn: Pre-trained convolutional neural networks for music audio
                  tagging},
  journal      = {CoRR},
  volume       = {abs/1909.06654},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.06654},
  eprinttype    = {arXiv},
  eprint       = {1909.06654},
  timestamp    = {Mon, 23 Sep 2019 18:07:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-06654.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{PonsCNN,
      title={Timbre Analysis of Music Audio Signals with Convolutional Neural Networks}, 
      author={Jordi Pons and Olga Slizovskaia and Rong Gong and Emilia GÃ³mez and Xavier Serra},
      year={2017},
      eprint={1703.06697},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{MULE,
      title={Supervised and Unsupervised Learning of Audio Representations for Music Understanding}, 
      author={Matthew C. McCallum and Filip Korzeniowski and Sergio Oramas and Fabien Gouyon and Andreas F. Ehmann},
      year={2022},
      eprint={2210.03799},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@inproceedings{MinzSA,
  title={Evaluation of CNN-based automatic music tagging models},
  author={Won, Minz and Ferraro, Andres and Bogdanov, Dmitry and Serra, Xavier},
  booktitle={Proc. of 17th Sound and Music Computing},
  year={2020}
}

@ARTICLE{nnAudio, author={K. W. {Cheuk} and H. {Anderson} and K. {Agres} and D. {Herremans}}, journal={IEEE Access}, title={nnAudio: An on-the-Fly GPU Audio to Spectrogram Conversion Toolbox Using 1D Convolutional Neural Networks}, year={2020}, volume={8}, number={}, pages={161981-162003}, doi={10.1109/ACCESS.2020.3019084}}

@misc{BestRQ,
      title={Self-supervised Learning with Random-projection Quantizer for Speech Recognition}, 
      author={Chung-Cheng Chiu and James Qin and Yu Zhang and Jiahui Yu and Yonghui Wu},
      year={2022},
      eprint={2202.01855},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{swin,
  author       = {Ze Liu and
                  Yutong Lin and
                  Yue Cao and
                  Han Hu and
                  Yixuan Wei and
                  Zheng Zhang and
                  Stephen Lin and
                  Baining Guo},
  title        = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  journal      = {CoRR},
  volume       = {abs/2103.14030},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.14030},
  eprinttype    = {arXiv},
  eprint       = {2103.14030},
  timestamp    = {Mon, 05 Jun 2023 16:18:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-14030.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{s3t,
  author={Zhao, Hang and Zhang, Chen and Zhu, Bilei and Ma, Zejun and Zhang, Kejun},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={S3T: Self-Supervised Pre-Training with Swin Transformer For Music Classification}, 
  year={2022},
  volume={},
  number={},
  pages={606-610},
  keywords={Representation learning;Time-frequency analysis;Pipelines;Tagging;Signal processing;Transformers;Feature extraction;Self-supervised learning;Swin Transformer;music genre classification;music tagging},
  doi={10.1109/ICASSP43922.2022.9746056}}


@INPROCEEDINGS{augmentation-embedding,
  author={McCallum, Matthew C. and Davies, Matthew E. P. and Henkel, Florian and Kim, Jaehun and Sandberg, Samuel E.},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={On The Effect Of Data-Augmentation On Local Embedding Properties In The Contrastive Learning Of Music Audio Representations}, 
  year={2024},
  volume={},
  number={},
  pages={671-675},
  keywords={Training data;Self-supervised learning;Signal processing;Data augmentation;Labeling;Task analysis;Speech processing;Music audio embeddings;data augmentation},
  doi={10.1109/ICASSP48485.2024.10446274}}


@misc{CLIP,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{iSogCLR,
  title={Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization},
  author={Zi-Hao Qiu, Quanqi Hu, Zhuoning Yuan, Denny Zhou, Lijun Zhang, and Tianbao Yang},
  booktitle={International Conference on Machine Learning},
  pages={TBD},
  year={2023},
  organization={PMLR}
}

@inproceedings{FMA,
  title = {{FMA}: A Dataset for Music Analysis},
  author = {Defferrard, Micha\"el and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},
  booktitle = {18th International Society for Music Information Retrieval Conference (ISMIR)},
  year = {2017},
  archiveprefix = {arXiv},
  eprint = {1612.01840},
  url = {https://arxiv.org/abs/1612.01840},
}

@article{TSNE,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@misc{LARS,
      title={Large Batch Training of Convolutional Networks}, 
      author={Yang You and Igor Gitman and Boris Ginsburg},
      year={2017},
      eprint={1708.03888},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{magnatagatune,
author = {Law, Edith and West, Kris and Mandel, Michael and Bay, Mert and Downie, J.},
year = {2009},
month = {01},
pages = {387-392},
title = {Evaluation of Algorithms Using Games: The Case of Music Tagging.}
}

@article{m2d,
    title   = {{Masked Modeling Duo: Towards a Universal Audio Pre-training Framework}},
    author  = {Daisuke Niizumi and Daiki Takeuchi and Yasunori Ohishi and Noboru Harada and Kunio Kashino},
    journal = {IEEE/ACM Trans. Audio, Speech, Language Process.},
    year    = {2024},
    volume  = {32},
    pages   = {2391-2406},
    url     = {https://ieeexplore.ieee.org/document/10502167},
    doi     = {10.1109/TASLP.2024.3389636}}

@article{gtzanfaults,
  author       = {Bob L. Sturm},
  title        = {The {GTZAN} dataset: Its contents, its faults, their effects on evaluation,
                  and its future use},
  journal      = {CoRR},
  volume       = {abs/1306.1461},
  year         = {2013},
  url          = {http://arxiv.org/abs/1306.1461},
  eprinttype    = {arXiv},
  eprint       = {1306.1461},
  timestamp    = {Mon, 13 Aug 2018 16:48:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/Sturm13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{gtzan,
  author={Tzanetakis, G. and Cook, P.},
  journal={IEEE Transactions on Speech and Audio Processing}, 
  title={Musical genre classification of audio signals}, 
  year={2002},
  volume={10},
  number={5},
  pages={293-302},
  keywords={Humans;Music information retrieval;Instruments;Computer science;Multiple signal classification;Signal analysis;Pattern recognition;Feature extraction;Wavelet analysis;Cultural differences},
  doi={10.1109/TSA.2002.800560}}

  @inproceedings{giantsteps,
  title={Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections},
  author={Peter Knees and {\'A}ngel Faraldo and Perfecto Herrera and Richard Vogl and Sebastian B{\"o}ck and Florian H{\"o}rschl{\"a}ger and Mickael Le Goff},
  booktitle={International Society for Music Information Retrieval Conference},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:15836728}
}

@inproceedings{oord2014,
  title={Transfer Learning by Supervised Pre-training for Audio-based Music Classification},
  author={A{\"a}ron van den Oord and Sander Dieleman and Benjamin Schrauwen},
  booktitle={International Society for Music Information Retrieval Conference},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:6159614}
}

@inproceedings{randaugment,
 author = {Cubuk, Ekin Dogus and Zoph, Barret and Shlens, Jon and Le, Quoc},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18613--18624},
 publisher = {Curran Associates, Inc.},
 title = {RandAugment: Practical Automated Data Augmentation with a Reduced Search Space},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{CHOI,
  title={Transfer learning for music classification and regression tasks},
  author={Choi, Keunwoo and Fazekas, George and Sandler, Mark and Cho, Kyunghyun},
  booktitle={The 18th International Society of Music Information Retrieval (ISMIR) Conference 2017, Suzhou, China},
  year={2017},
  organization={International Society of Music Information Retrieval}
}

@misc{SimCSE,
      title={SimCSE: Simple Contrastive Learning of Sentence Embeddings}, 
      author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
      year={2022},
      eprint={2104.08821},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{SogCLR,
  title={Provable stochastic optimization for global contrastive learning: Small batch does not harm performance},
  author={Yuan, Zhuoning and Wu, Yuexin and Qiu, Zi-Hao and Du, Xianzhi and Zhang, Lijun and Zhou, Denny and Yang, Tianbao},
  booktitle={International Conference on Machine Learning},
  pages={25760--25782},
  year={2022},
  organization={PMLR}
}

@article{ViT,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  journal      = {CoRR},
  volume       = {abs/2010.11929},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.11929},
  eprinttype    = {arXiv},
  eprint       = {2010.11929},
  timestamp    = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{glip,
      title={Centered Masking for Language-Image Pre-Training}, 
      author={Mingliang Liang and Martha Larson},
      year={2024},
      eprint={2403.15837},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      doi={https://doi.org/10.1007/978-3-031-70371-3_6},
      url={https://arxiv.org/abs/2403.15837}, 
}

@INPROCEEDINGS{Audioset,
  author={Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Audio Set: An ontology and human-labeled dataset for audio events}, 
  year={2017},
  volume={},
  number={},
  pages={776-780},
  keywords={Ontologies;Birds;Music;Taxonomy;Labeling;Audio event detection;sound ontology;audio databases;data collection},
  doi={10.1109/ICASSP.2017.7952261}}


@misc{Adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@misc{SimpleViT,
      title={Better plain ViT baselines for ImageNet-1k}, 
      author={Lucas Beyer and Xiaohua Zhai and Alexander Kolesnikov},
      year={2022},
      eprint={2205.01580},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2205.01580}, 
}

@article{MAE,
  author       = {Kaiming He and
                  Xinlei Chen and
                  Saining Xie and
                  Yanghao Li and
                  Piotr Doll{\'{a}}r and
                  Ross B. Girshick},
  title        = {Masked Autoencoders Are Scalable Vision Learners},
  journal      = {CoRR},
  volume       = {abs/2111.06377},
  year         = {2021},
  url          = {https://arxiv.org/abs/2111.06377},
  eprinttype    = {arXiv},
  eprint       = {2111.06377},
  timestamp    = {Tue, 16 Nov 2021 12:12:31 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2111-06377.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{FLIP,
      title={Scaling Language-Image Pre-training via Masking}, 
      author={Yanghao Li and Haoqi Fan and Ronghang Hu and Christoph Feichtenhofer and Kaiming He},
      year={2023},
      eprint={2212.00794},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.00794}, 
}

@article{MixUp,
  author       = {Hongyi Zhang and
                  Moustapha Ciss{\'{e}} and
                  Yann N. Dauphin and
                  David Lopez{-}Paz},
  title        = {mixup: Beyond Empirical Risk Minimization},
  journal      = {CoRR},
  volume       = {abs/1710.09412},
  year         = {2017},
  url          = {http://arxiv.org/abs/1710.09412},
  eprinttype    = {arXiv},
  eprint       = {1710.09412},
  timestamp    = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1710-09412.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{emomusic,
author = {Soleymani, Mohammad and Caro, Micheal and Schmidt, Erik and Sha, Cheng-Ya and Yang, yi-hsuan},
year = {2013},
month = {10},
pages = {1-6},
title = {1000 songs for emotional analysis of music},
journal = {CrowdMM 2013 - Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
doi = {10.1145/2506364.2506365}
}

@article{gtzan-ff,
author = {Kereliuk, Corey and Sturm, Bob L. and Larsen, Jan},
title = {Deep Learning and Music Adversaries},
year = {2015},
issue_date = {Nov. 2015},
publisher = {IEEE Press},
volume = {17},
number = {11},
issn = {1520-9210},
url = {https://doi.org/10.1109/TMM.2015.2478068},
doi = {10.1109/TMM.2015.2478068},
abstract = {An adversary is an agent designed to make a classification system perform in some particular way, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, exploiting the parameters of the system to find the minimal perturbation of the input image such that the system misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the system inputs are magnitude spectral frames, which require special care in order to produce valid input audio signals from network- derived perturbations . For two different train-test partitionings of two benchmark datasets, and two different architectures , we find that this adversary is very effective. We find that convolutional architectures are more robust compared to systems based on a majority vote over individually classified audio frames. Furthermore , we experiment with a new system that integrates an adversary into the training loop, but do not find that this improves the resilience of the system to new adversaries.},
journal = {Trans. Multi.},
month = nov,
pages = {2059â2071},
numpages = {13}
}

@inproceedings{transformer,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000â6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{patchout,
  author       = {Khaled Koutini and
                  Jan Schl{\"{u}}ter and
                  Hamid Eghbal{-}zadeh and
                  Gerhard Widmer},
  title        = {Efficient Training of Audio Transformers with Patchout},
  journal      = {CoRR},
  volume       = {abs/2110.05069},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.05069},
  eprinttype    = {arXiv},
  eprint       = {2110.05069},
  timestamp    = {Thu, 21 Oct 2021 16:20:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-05069.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@InProceedings{msemae,
    title     = {Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation},
    author    = {Niizumi, Daisuke and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio},
    booktitle = {HEAR: Holistic Evaluation of Audio Representations (NeurIPS 2021 Competition)},
    pages     = {1--24},
    year      = {2022},
    editor    = {Turian, Joseph and Schuller, BjÃ¶rn W. and Herremans, Dorien and Kirchoff, Katrin and Perera, Paola Garcia and Esling, Philippe},
    volume    = {166},
    series    = {Proceedings of Machine Learning Research},
    month     = {13--14 Dec},
    publisher = {PMLR},
    pdf       = {https://proceedings.mlr.press/v166/niizumi22a/niizumi22a.pdf},
    url       = {https://proceedings.mlr.press/v166/niizumi22a.html}
}



@article{infonce,
  author       = {A{\"{a}}ron van den Oord and
                  Yazhe Li and
                  Oriol Vinyals},
  title        = {Representation Learning with Contrastive Predictive Coding},
  journal      = {CoRR},
  volume       = {abs/1807.03748},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.03748},
  eprinttype    = {arXiv},
  eprint       = {1807.03748},
  timestamp    = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@misc{nsynth,
    Author = {Jesse Engel and Cinjon Resnick and Adam Roberts and
              Sander Dieleman and Douglas Eck and Karen Simonyan and
              Mohammad Norouzi},
    Title = {Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders},
    Year = {2017},
    Eprint = {arXiv:1704.01279},
}