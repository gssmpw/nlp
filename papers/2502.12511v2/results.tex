\begin{table*}[h]
\centering
\begin{tabular}{lccccccccc}
\toprule
\textbf{Approach} & \textbf{Size} & \multicolumn{2}{c}{\textbf{Tags}} & \textbf{Genre} & \textbf{Key} & \multicolumn{2}{c}{\textbf{Emotion}} & \textbf{Average} \\
                  &                     & \textbf{MTT\textsubscript{AUC}} & \textbf{MTT\textsubscript{AP}} & \textbf{GTZAN} & \textbf{GS} & \textbf{Emo\textsubscript{A}} & \textbf{Emo\textsubscript{V}} &  \\
\midrule
\midrule
MULE$^{\dagger}$             & 62M                 & 91.2 & 40.1 & 75.5 & 64.9 & 73.1 & 60.7 & 68.2 \\
MERT-95M$^{\dagger}$         & 95M                 & 91.0 & 39.3 & 78.6 & 63.5 & \textbf{76.4} & 60.0 & 68.9 \\
MERT-330M$^{\dagger}$        & 330M                & 91.3 & 40.2 & 79.3 & 65.6 & 74.7 & 61.2 & 69.7 \\
Jukebox$^{\dagger}$          & 5B                  & \textbf{91.5} & \textbf{41.4} & \textbf{79.7} & 66.7 & 72.1 & \textbf{61.7} & \textbf{69.9} \\
MusiCNN$^{\ast}$          & 7M                  & 90.6 & 38.3 & 79.0 & 12.8 & 70.3 & 46.6 & 53.7 \\
CLMR$^{\ast}$             & 3M                  & 89.4 & 36.1 & 68.6 & 14.9 & 67.8 & 45.8 & 50.8 \\
MERT-95M-public$^{\ast}$  & 95M                 & 90.7 & 38.4 & 72.8 & 67.3 & 72.5 & 59.7 & 67.7 \\
MAE$^{\ast}$  & 32M                  & 88.9 & 35.6 & 75.5 & 53.6 & 69.7 & 50.2 & 62.8 \\
\midrule
Myna-Base$^{\ast}$        & 22M                 & 90.8 & 39.5 & 78.3 & 63.5 & 73.5 & 55.8 & 67.9 \\
Myna-Vertical$^{\ast}$    & 22M                 & 90.1 & 37.4 & 75.9 & \textbf{68.6} & 66.5 & 45.9 & 66.1 \\
Myna-Hybrid$^{\ast}$      & 22M                 & 91.0 & 39.8 & 77.9 & \textbf{68.0} & 70.8 & 55.2 & 68.6 \\
\bottomrule
\end{tabular}
\caption{Comparison of Different Approaches on Various MIR Tasks. All results except ours are as reported in \cite{JukeMIR, MERT} as our evaluation procedure is identical. As in \cite{JukeMIR}, tasks with multiple evaluation metrics have their metrics averaged first, and then the averages across all tasks are computed. Models labeled with $^{\ast}$ are trained on publicly-available data, while models labeled with $^{\dagger}$ were trained on private datasets. All data splits are identical. The max score for all metrics is 100 and higher is better. Note that CLMR was pre-trained on MTT, so its evaluation on MTT is not a demonstration of out-of-distribution generalization.}
\end{table*}
