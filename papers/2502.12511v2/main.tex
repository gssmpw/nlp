\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{subcaption}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Myna: Masking-Based Contrastive Learning of Musical Representations}

\begin{document}

\twocolumn[
\icmltitle{Myna: Masking-Based Contrastive Learning of Musical Representations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ori Yonay}{tamu}
\icmlauthor{Tracy Hammond}{tamu}
\icmlauthor{Tianbao Yang}{tamu}
\end{icmlauthorlist}

\icmlaffiliation{tamu}{Department of Computer Science, Texas A\&M University, College Station, TX, USA}

\icmlcorrespondingauthor{Ori Yonay}{oyonay12@tamu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Contrastive Learning, Music Representations, Masked Contrastive Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% UNCOMMENT THIS IN FINAL PAPER
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In this paper, we present Myna, a simple yet effective approach for self-supervised musical representation learning. Built on a contrastive learning framework, Myna introduces two key innovations: (1) the use of a Vision Transformer (ViT) on mel-spectrograms as the backbone, replacing SampleCNN on raw audio; and (2) a simple yet novel data augmentation strategy—token masking—that masks 90\% of spectrogram tokens (e.g., 16x16 patches). These innovations deliver both effectiveness and efficiency: (i) Token masking enables a significant increase in per-GPU batch size, from 48 or 120 in traditional contrastive methods (e.g., CLMR, MULE) to 4096. % —an 85x improvement.
(ii) By avoiding traditional augmentations (e.g., pitch shifts), Myna retains pitch sensitivity, enhancing performance in tasks like key detection. (iii) The use of vertical patches (128x2 instead of 16x16) allows the model to better capture critical features for key detection. Our hybrid model, Myna-22M-Hybrid, processes both 16x16 and 128x2 patches, achieving state-of-the-art results. Trained on a single GPU, it outperforms MULE (62M) on average and rivals MERT-95M, which was trained on 16 and 64 GPUs, respectively. Additionally, it surpasses MERT-95M-public, establishing itself as the best-performing model trained on publicly available data. We release our code and models to promote reproducibility and facilitate future research: \href{https://github.com/ghost-signal/myna}{https://github.com/ghost-signal/myna}
\end{abstract}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.27]{main_visual.png}
    \caption{Myna is efficient: we achieve competitive downstream task performance while requiring significantly fewer computational resources compared to other models. Models trained on public datasets are represented in blue, while models trained on private datasets are shown in green. Myna is trained on a publicly-available dataset and is marked in red.}
    \label{fig:visual}
\end{figure}

\begin{figure*}[h]
    \centering
    \includegraphics[scale=0.3]{framework.png}
    \caption{The Myna pre-training framework. Tokens from spectrogram patches are randomly masked before being processed by a transformer encoder. The resulting embeddings are contrasted to maximize similarity between masked views of the same data while minimizing similarity with all other samples (negatives). Tokenizers, encoders and projector modules refer to the same sets of shared weights. For downstream tasks, the projector is discarded and replaced with a task-specific head (labeled ``Probe'' above) to leverage the learned embeddings.}
    \label{fig:visual}
\end{figure*}

\section{Introduction}
The field of Music Information Retrieval (MIR) has been revolutionized by deep learning. Traditionally, tasks such as genre classification, music auto-tagging, chord recognition, and key detection were approached using supervised learning on labeled datasets \cite{PonsCNN, MusiCNN, CHOI, MinzSA, BaumannKey}. However, the creation of these datasets is time-consuming and costly, while raw, unlabeled musical data is abundant. This disparity has fueled interest in un- and self-supervised learning, with self-supervised contrastive learning becoming a prominent approach. Recent research has applied frameworks like SimCLR and masked language modeling to extract meaningful musical representations from raw audio or spectrograms \cite{CLMR, MULE, MERT, JukeMIR}.

Self-supervised representation learning minimizes reliance on labeled data by learning a rich latent space that can generalize well to downstream tasks. In contrastive learning, the objective is to maximize agreement between different augmented views of the same data while pushing away other pairs of data (negatives). The use of data augmentations is key to contrastive learning; however, traditional data augmentations for musical data do not necessarily give good performance. For example, augmentations such as pitch shifting alter critical musical properties that are essential for tasks like key detection \cite{augmentation-embedding}. Our approach instead relies entirely on token masking to sample different subsets of spectrograms as ``views'' of the data, which preserves the meaningful relationships between views. We argue that it is more beneficial to teach a model that the relationship between two masked subsets of the input is the same than that two noisy versions of the input are the same; the former keeps the model sensitive to augmentations while the latter makes representations biased to the choice of transformations used. This ensures that we retain musically relevant features while significantly reducing the number of hyperparameters for augmentations (for example, the augmentation chain in CLMR contains 21 hyperparameters \cite{CLMR}, not including chain ordering; we reduce this to 1).

% One primary challenge in contrastive learning is its reliance on large batch sizes, making it inaccessible to researchers with limited computational resources. Our framework addresses this explicitly and implicitly: first, we employ a more memory-efficient model backbone and use token masking to further increase per-GPU batch size by 42x (from 48 to 2048). Additionally, we optimize the SogCLR objective \cite{SogCLR}, a global contrastive objective with a provable convergence guarantee as a replacement to the traditional local InfoNCE loss used in methods like SimCLR upon which CLMR is based. This optimization reduces the dependency on large batch sizes, enabling more efficient training with meaningful gradients. To the best of our knowledge, this is the first work in which the SogCLR framework is applied to the audio domain.

Building on these insights, our work presents Myna \footnote{The name Myna is inspired by the bird native to southern Asia.}, the first contrastive framework free of domain-specific augmentations that advances the efficiency of musical representation learning. Myna refines the Contrastive Learning of Musical Representations (CLMR) framework by introducing several key ideas to overcome its limitations.

Our primary contributions are as follows:
\begin{itemize}
    \item We introduce a simple domain-agnostic contrastive learning framework and demonstrate that masking spectrogram tokens alone can replace traditional data augmentations while maintaining musically relevant features.
    \item We leverage the ViT architecture to increase memory efficiency and allow for large batch sizes (85x increase in efficiency over CLMR), making training on a single GPU feasible.\footnote{In the contrastive setting, larger batch sizes yield better performance. See Appendix A for batch size ablations.}
    % \item We train a "hybrid" model that supports both square (16x16) and vertical (128x2) patch sizes and find that this model outperforms MULE 
    \item Our model, Myna-Hybrid (22M), trained on a single GPU, outperforms MULE and MERT-95M-public on average and is competitive with MERT-95M, making it the best model trained only on publicly available data.
\end{itemize}

While we restrict our experiments to a single GPU as a proof of concept, we believe that scaling up Myna could yield state-of-the-art results and present key insights to accelerate research in unsupervised musical representation learning.

\section{Related Work}
% This section explores the advancements in self-supervised learning frameworks for music information retrieval, with a focus on the development of general-purpose audio and musical representations. We highlight the transition from traditional supervised methods to unsupervised techniques that leverage the vast amounts of unlabeled musical data. 

\subsection{Self-Supervised Learning Frameworks}

SimCLR \cite{SimCLR} is a simple contrastive approach for learning discriminative representations and has found success in areas ranging from computer vision to language \cite{CLMR, SimCSE}. A similar notable framework is Contrastive Predictive Coding \cite{CPC}, a universal approach to contrastive learning, which has been successful for MIR- and audio-related tasks such as speaker and phoneme classification using raw audio. Additionally, this work introduced the InfoNCE loss, which is used in SimCLR, CLMR, and Myna.

Recently, due to the widespread success of transformer-based models on various tasks and modalities, MIR researchers have borrowed unsupervised learning paradigms from natural language processing. In \cite{JukeMIR}, the authors probe the hidden layers of OpenAI's Jukebox model \cite{Jukebox} and achieve state-of-the-art results, suggesting that CALM (codified audio language modeling) is an effective pre-training approach for MIR tasks. The authors of this work also suggested that transformer-encoder based models are likely to outperform JukeMIR's performance in music audio representation. Building on this, \cite{MERT} and \cite{MusicFM} have emerged as pioneering efforts that harness masked language modeling for musical applications. Masked auto-encoding (MAE) has found success as another non-contrastive pre-training task in images and was recently shown to be effective in environmental sound and genre classification \cite{MAE, msemae, m2d}. 

\subsection{General-purpose Audio Representations}

The COLA framework \cite{COLA} employs a simple contrastive learning framework built on SimCLR and utilizes Mel-spectrogram representations and bilinear comparisons to achieve better results than supervised counterparts. HARES \cite{HARES} further demonstrated that normalizer-free Slowfast networks (trained on the SimCLR objective) lead to effective generalization of audio representations \cite{slowfast, nfnets}; this finding was later used by \cite{MULE} for music-specific tasks.

\subsection{Patch Masking}
While effective in sequence modeling, transformers \cite{transformer} suffer from quadratic memory and time complexity with respect to the number of tokens. To address this issue, prior work has explored various token masking strategies to reduce computational overhead. In the self-supervised domain, MAE and FLIP \cite{MAE, FLIP} used masking on image tokens to increase pre-training efficiency. In the supervised setting, PaSST \cite{patchout} introduced Patchout (spectrogram masking) to speed up transformer training and achieved state-of-the-art results in audio tagging. Our work is the first to show that spectrogram masking works in the contrastive setting. 

\subsection{Musical Representations}

MusiCNN \cite{MusiCNN}, a CNN designed for log-mel spectrograms, draws on the discussion in \cite{PonsCNN} for its efficient design and is pre-trained on a supervised music auto-tagging task. CLMR \cite{CLMR} adapted the SimCLR framework for music using SampleCNN \cite{SampleCNN} on raw waveforms and achieved competitive results with supervised counterparts; S3T \cite{s3t} improved on this by using a swin transformer \cite{swin} on spectrograms with simplified augmentations and achieved notable gains in tagging and classification. MULE \cite{MULE} provides a broad analysis of supervised and unsupervised (contrastive) pre-training methodologies on MIR downstream tasks and are the only existing work to not use pitch shifting as an augmentation in a contrastive setting, instead favoring MixUp \cite{MixUp} as their sole augmentation. We believe this is a step in the right direction and this work aims to further refine this approach. Their follow-up work studies the effect of various augmentations on model performance \cite{augmentation-embedding}. Recent work has adopted NLP techniques for MIR: JukeMIR \cite{JukeMIR} successfully probed representations from Jukebox \cite{Jukebox}, a music generation model based on the GPT architecture. Following this, MERT \cite{MERT} and MusicFM \cite{MusicFM} achieve state-of-the-art results via masked language modeling on music audio tokens.

\section{Method}

\subsection{Preliminaries}
Our work builds upon CLMR, which is the music audio adaptation of SimCLR's contrastive learning framework for visual representations. In SimCLR, for every sample \(x_i\) in a batch, two augmentations \(A(x_i)\) and \(A'(x_i)\) are applied, generating two correlated views. These views are passed through the same encoder, and the objective is to maximize agreement between their latent representations using a contrastive loss while minimizing agreement between all other samples in the batch.

SimCLR consists of:
\begin{itemize}
    \item An encoder \(enc(\cdot)\), which maps the augmented views to a latent space \(\mathbb{R}^{\text{data}} \mapsto \mathbb{R}^{\text{latent}}\).
    \item A projector network \(proj(\cdot)\), mapping latent representations to a projection space \(\mathbb{R}^{\text{latent}} \mapsto \mathbb{R}^{\text{proj}}\).
    \item Stochastic augmentations \(A(x)\), producing two correlated views \(A(x_i), A'(x_i)\) for each sample. % Our work demonstrates that simple subset selection of spectrogram patches suffices.
    \item A contrastive loss to maximize the similarity between \(A(x_i)\) and \(A'(x_i)\) and minimize it between views of all other samples.
\end{itemize}
% We denote \(net(\cdot) = proj(enc(\cdot))\) for simplicity.

The contrastive loss used in SimCLR, CLMR, and our work, is the InfoNCE loss \cite{CPC}, defined for a positive pair of examples $(i, j)$ as:

\[
\ell_i = 
- \log \Biggl(
\frac{\exp\bigl(\text{sim}(\mathbf{z}_i^{(1)}, \mathbf{z}_i^{(2)}) / \tau\bigr)}
{\sum\limits_{j=1}^N \sum_{v=1}^2 
\,\mathbbm{1}_{[j \neq i]}\,
\exp\bigl(\text{sim}(\mathbf{z}_i^{(1)}, \mathbf{z}_j^{(v)}) / \tau\bigr)}
\Biggr)
\]

where \(\text{sim}(\mathbf{z}_p^{(u)}, \mathbf{z}_q^{(v)})\) denotes the cosine similarity between the normalized representations \(\mathbf{z}_p^{(u)}\) and \(\mathbf{z}_q^{(v)}\), and \(\tau > 0\) is a temperature parameter. Minimizing \(\ell_i\) encourages the positive pair 
\(\bigl(\mathbf{z}_i^{(1)}, \mathbf{z}_i^{(2)}\bigr)\) 
to have a higher similarity than all negative pairs 
\(\bigl(\mathbf{z}_i^{(1)}, \mathbf{z}_j^{(v)}\bigr)\) 
for \(j \neq i\) and \(v \in \{1,2\}\).


\input{algorithm.tex}

\subsection{Creation of Positive Pairs}
To generate positive pairs, we first select two three-second segments from the same audio. We generate Mel spectrograms for each segment and then patchify them into $16 \times 16$ or $128 \times 2$ sections. Each spectrogram patch undergoes a linear projection combined with 2D sinusoidal positional encodings \cite{SimpleViT} to create token representations. % \footnote{Early ablations showed that $16 \times 16$ patches lead to the best results; we additionally show that using $128 \times 2$ patches leads to decreased performance in all tasks except key detection, where we surpass the self-supervised SOTA.}

Following this, we randomly mask 90\% of the tokens from each spectrogram, inspired by the methods in \cite{FLIP} and \cite{MAE}. Positive pairs are constructed using the strategy described in Algorithm \ref{alg:myna} and illustrated in Figure \ref{fig:visual}. This masking enables the model to learn meaningful relationships between the remaining tokens, effectively treating the masked spectrograms as augmented views of the same underlying data. The resulting masked pairs serve as positive samples for our contrastive learning framework.

Intuitively, masking a high percentage of tokens encourages the model to focus on global patterns and relationships between the unmasked tokens. By treating masked spectrograms as augmented views, the model is trained to reconstruct meaningful relationships between the unmasked tokens and their masked counterparts. This forces the model to infer higher-level, context-aware features rather than overfitting to specific low-level details that might only be locally relevant. Since the masking process only hides information without altering it (unlike traditional augmentations such as pitch shifting or time stretching), the underlying properties of the music, like pitch/key and BPM, remain intact in both views. This ensures that the model learns representations that are robust to missing information and invariant to the masking operation, allowing it to generalize better to downstream tasks that depend on recognizing the overall structure and relationships in the data.

\subsection{Why Not Masked Auto-Encoding?}
Previous work has demonstrated that masked auto-encoding is an effective pre-training task for learning representations in various domains \cite{msemae, MAE}. Below, we outline three reasons against using masked auto-encoding for musical representation learning and instead favor a contrastive learning framework.

\subsubsection{Efficiency}  
MAE frameworks require training both an encoder and a decoder. While the decoder is necessary for reconstruction during pre-training, it is discarded when transitioning to downstream tasks. This means a substantial portion of computational resources during training is devoted to learning and optimizing a decoder that is ultimately unused. By contrast, our masking-based contrastive learning framework eliminates the need for a decoder entirely and thus reduces computational overhead. 

\subsubsection{Task Difficulty}  
In masked auto-encoding, the model is tasked with reconstructing the original input from masked portions, which can be a challenging and sometimes counterproductive objective for music. While MAE has shown success in environmental sound classification, where sounds often exhibit simpler and more repetitive patterns, music exhibits high variability and structural complexity. Musical patterns often span longer temporal contexts, and the relationships between different components (e.g., melody, harmony, rhythm) can be intricate. This makes the reconstruction task disproportionately difficult. Contrastive learning, on the other hand, focuses on learning high-level relationships and invariances rather than predicting low-level details, making it better suited for music (see Appendix B).

\subsubsection{Preserving Musically Relevant Features}  
MAE forces the model to focus on reconstructing fine-grained details, which may not always align with the musically meaningful features needed for tasks like music tagging, key detection, or emotion recognition. For example, reconstructing the exact values of masked spectrogram tokens could encourage the model to focus on local energy patterns rather than higher-level tonal or rhythmic structures. Contrastive learning emphasizes capturing meaningful global representations, ensuring that the learned features are aligned with the downstream tasks.

% \subsection{Contrastive Framework}

% A major challenge in contrastive learning is the reliance on large batch sizes, which limits efficiency in limited resource setups. Methods like SimCLR, based on the InfoNCE objective, contrast positive pairs against negatives within the current batch, necessitating large batch sizes. To mitigate this, we employ the SogCLR objective \cite{SogCLR}, which contrasts each positive pair against \emph{all} negative pairs via an unbiased approximation of the global contrastive objective. SogCLR introduces a memory-efficient stochastic optimization algorithm that reduces the dependence on large batch sizes.

% While work exists on learning \(\tau\) via gradient descent \cite{CLIP} or individualized temperature values \cite{iSogCLR}, we keep it constant in this work.

\subsection{Model Architecture}

We use a simplified version of the Vision Transformer (ViT) \cite{ViT}, SimpleViT \cite{SimpleViT}, which replaces the CLS token with global average pooling and employs 2D sinusoidal positional encodings. For all experiments in this paper, we use the ViT-S/16 architecture (22M parameters), with the exception of using $16 \times 16$ or $128 \times 2$ non-overlapping patches.

\subsection{Hybrid Models}
\input{hybrid_visual}
Our experiments show that using square ($16 \times 16$) patches yields competitive performance. Conversely, using vertical ($128 \times 2$) patches reduces performance across all metrics, except for key detection, where it achieves state-of-the-art (SOTA) performance among self-supervised methods. To combine the strengths of both approaches, we propose a novel hybrid model compatible with both patch configurations.

The hybrid model retains a shared encoder and projector but employs two separate tokenizers (linear projections) and positional embeddings tailored for the two patch sizes. During training, we alternate between patch configurations. Specifically, at each iteration, we calculate the contrastive loss for each patch configuration independently and then optimize the average of the two losses. The overall objective is:

\[
\mathcal{L}_{\text{hybrid}} = \frac{1}{2} (\mathcal{L}_{\text{square}} + \mathcal{L}_{\text{vertical}})
\]

where \(\mathcal{L}_{\text{square}}\) and \(\mathcal{L}_{\text{vertical}}\) are the contrastive losses computed using two separate forward passes with $16 \times 16$ and $128 \times 2$ patches using Algorithm \ref{alg:myna}, respectively. Figure \ref{fig:hybrid} illustrates the computation process of the hybrid model's loss.

By incorporating this dual-patch training strategy, the hybrid model benefits from the general-purpose performance of square patches while leveraging the superior key detection capabilities of vertical patches. This results in a model capable of excelling across a broader range of musical representation tasks.

\subsection{Hyperparameters}
We extract Mel spectrograms with 128 bins with a sample rate of 16 kHz using the nnAudio library \cite{nnAudio}, a batch size of 4096, and a 90\% masking ratio. We use the Adam optimizer \cite{Adam} with a learning rate of 3e-4 and weight decay of 1e-5 for 500 epochs (total of 411 million examples seen). For the contrastive loss, we set $\tau = 0.1$. We use a single NVIDIA A100 GPU for training and four NVIDIA A100 GPUs for masking ablations, as lower masking ratios are less efficient and require multiple GPUs.

While work exists on learning \(\tau\) via gradient descent \cite{CLIP} or individualized temperature values \cite{iSogCLR}, we keep it constant in this work.

\section{Experiments}
\input{results}
\input{results_visuals}
\subsection{Pre-Training Dataset}
We pre-train our models on the music subset of the Audioset dataset \cite{Audioset}, containing roughly 822k 10-second music audio segments. Notably, unlike CLMR, we did not train our model on any of the datasets used for downstream tasks (CLMR was pre-trained on the MagnaTagATune dataset).

\subsection{Downstream Datasets}
\textbf{MagnaTagATune (MTT)}: The MTT dataset comprises 25,863 clips, each 29 seconds long, annotated with a set of 188 tags that cover genres, moods, instruments, and other sonic characteristics \cite{magnatagatune}. Similarly to previous work, we use the standard (12:1:3) train, validation, and test split \cite{oord2014, PonsCNN} and do not discard any examples (see \cite{MinzSA}). We report both common metrics for this task: area under the receiver operating characteristic curve and average precision. Following previous work, we only use the top 50 tags for evaluation. 

\textbf{GTZAN}: The GTZAN dataset \cite{gtzan}, a cornerstone dataset for genre classification in MIR, comprises 1,000 audio tracks, each 30 seconds long, spanning 10 diverse genres. For a fair comparison with previous work, we use the fault-filtered set as described in \cite{gtzan-ff, gtzanfaults}.

\textbf{GiantSteps}: The GiantSteps Key dataset \cite{giantsteps} features electronic dance music annotated with key information. It includes roughly 1,000 2-minute song clips covering all 24 major and minor keys (though the data is imbalanced). This dataset challenges models to accurately predict musical keys, which requires sensitivity to harmonic and tonal content.  

\textbf{EmoMusic}: The EmoMusic dataset \cite{emomusic} consists of 744 clips, each 45 seconds long, annotated with valence and arousal scores derived from human listeners' emotional responses. The dataset tests the model's capacity to capture and interpret the emotional cues encoded in music, a sophisticated challenge that probes the depth of the learned musical representations.  

\subsection{Models}
We train and evaluate three Myna models with various patch size configurations. Myna-Base, our base model, operates on $16 \times 16$ patches. Myna-Vertical operates on $128 \times 2$ patches. Myna-Hybrid is a hybrid model trained to support both patch sizes simultaneously. During evaluation of Myna-Hybrid, we evaluate a single linear model on the square, vertical, and concatenated (both square and vertical) representations for each task and use the representation that yields the best performance.

\subsection{Results}
For a direct and fair comparison with other approaches, we use the exact data splits, metrics, and evaluation procedure as in \cite{JukeMIR}. We briefly summarize the evaluation procedure below for completeness.

To extract relevant information from representations, we employ simple models—linear probes and shallow MLPs—trained on fixed representation vectors to predict task-specific labels. We conduct a grid search over architectures and hyperparameters for each task, varying the model type, hidden dimension, learning rate, and regularization (see Appendix C for more). The model achieving the best performance on a validation set is then evaluated on the test set. This protocol allows for an apples-to-apples comparison of the quality of representations produced by different pre-training strategies.

Based on the results presented in Table 1, Myna demonstrates competitive or superior performance across multiple MIR tasks. Myna-Hybrid achieves an average score of 68.6, surpassing MERT-95M-public (67.7) and MULE (68.2) while rivaling MERT-95M (68.9). Furthermore, the hybrid model improves performance on music tagging tasks due to its ability to integrate features from both square and vertical patches. Notably, Myna-Vertical and Myna-Hybrid excel in the key detection task with scores of 68.6 and 68.0, surpassing the previous self-supervised SOTA of 67.3. In comparison, Myna-Base and Myna-Vertical exhibit slight trade-offs in performance. Myna-Base delivers robust general-purpose capabilities (67.9 average score), while Myna-Vertical's specialization in key detection (68.6) comes at the cost of lower scores in other areas.

% \subsection{Ablation Studies}

\subsection{Masking Ratios}
We investigate the impact of varying the masking ratio on model performance. As shown in Figure \ref{fig:masking-all}, we find that higher masking percentages consistently lead to better results. Additionally, we note a clear correlation between the masking ratio and the model's average performance, and suspect that low masking ratios make the contrastive task too easy, which leads to less discriminative (and thus useful) representations. This is particularly advantageous since increasing the masking ratio also improves computational efficiency by reducing the number of tokens that the model needs to attend to.

We qualitatively test Myna's discriminative capacity on the GTZAN dataset against MAE and CLMR in Figure \ref{fig:tsne-comparison}. Myna demonstrates clearer separation between classes, with noticeably reduced overlap between class clusters. This indicates that Myna's embeddings capture more meaningful and discriminative features and explains its improved generalization.

\subsection{Comparing with Masked Auto-Encoder}
MAE, with its focus on reconstructing masked spectrogram tokens, performs well in tasks requiring detailed local information (such as local harmonics that aid with genre classification and many of the tags in MTT, as shown in Table 1) but struggles with tasks that rely on understanding broader musical contexts, such as key detection. Our approach, which instead emphasizes learning global relationships through token masking, consistently achieves stronger generalization across these tasks. For example, in key detection, our model benefits from its ability to capture harmonic relationships without being constrained by the need to reconstruct low-level spectrogram details. This suggests that while MAE excels at learning fine-grained patterns, its objectives might not align with the structural and contextual complexities of music, whereas our contrastive framework effectively bridges this gap by focusing on meaningful, high-level representations.

\section{Future Work}

Although Myna has shown promising results in the domain of musical representation learning, there are several potential avenues to extend this research to broader applications and further enhance its capabilities. In the following, we outline three key directions for future work.

\subsection{Other Modalities} The Myna framework’s reliance solely on token masking as an augmentation strategy is inherently domain-agnostic. We do not see any limitations to the adaptation of this masking-only approach to other modalities, such as images or text. In the text domain, token masking has already shown success with transformer models like BERT, but we believe applying a masking-only contrastive framework could offer new insights.

\subsection{Scaling} Our proposed method significantly reduces the computational burden necessary to achieve competitive results, making large-scale training more accessible. Future work should explore the effects of scaling Myna with larger models and more extensive datasets. We anticipate further improvements in representation quality and downstream task performance from this avenue of future work. % Such scaling could provide valuable insights into the upper bounds of performance achievable by our masking-only contrastive approach.

\subsection{Masking Policies} In this work, we sampled token subsets for positive pairs from a uniform distribution. We believe that more sophisticated sampling policies (learned or fixed) could lead to better results or faster convergence. This has recently been shown to work in language/image pretraining \cite{glip}.

\section{Conclusion}
In this work, we introduced Myna, a domain-agnostic contrastive learning framework that uses token masking as the sole augmentation strategy. Our approach has shown that this method is effective in learning meaningful representations in the music audio domain while offering significant computational benefits. By leveraging a ViT-based architecture and using token masking as our augmentation, we achieved competitive results with significantly reduced computational requirements. We hope that Myna inspires future research to further explore masking-based contrastive learning.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Batch Size Ablations}
\input{batch_ablations}

\section{Masked Auto-Encoder Visualizations}
\input{mae_appendix}

\section{Evaluation Procedure}
\input{eval_appendix}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
