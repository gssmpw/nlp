\section{Related Work}
% This section explores the advancements in self-supervised learning frameworks for music information retrieval, with a focus on the development of general-purpose audio and musical representations. We highlight the transition from traditional supervised methods to unsupervised techniques that leverage the vast amounts of unlabeled musical data. 

\subsection{Self-Supervised Learning Frameworks}

SimCLR **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"** is a simple contrastive approach for learning discriminative representations and has found success in areas ranging from computer vision to language **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. A similar notable framework is Contrastive Predictive Coding **Hjelm et al., "Learning Representations by Maximizing Relational Stability"**, a universal approach to contrastive learning, which has been successful for MIR- and audio-related tasks such as speaker and phoneme classification using raw audio. Additionally, this work introduced the InfoNCE loss, which is used in SimCLR, CLMR, and Myna.

Recently, due to the widespread success of transformer-based models on various tasks and modalities, MIR researchers have borrowed unsupervised learning paradigms from natural language processing. In **Van den Oord et al., "Jukebox: A Generative Model for Music"**, the authors probe the hidden layers of OpenAI's Jukebox model **Raffel et al., "Learning to Generate Music with Songwave"** and achieve state-of-the-art results, suggesting that CALM (codified audio language modeling) is an effective pre-training approach for MIR tasks. The authors of this work also suggested that transformer-encoder based models are likely to outperform JukeMIR's performance in music audio representation. Building on this, **Liu et al., "Masked Audio Modeling for Music Information Retrieval"** and **Li et al., "Audio Masked Language Modeling for Music Generation"** have emerged as pioneering efforts that harness masked language modeling for musical applications. Masked auto-encoding (MAE) has found success as another non-contrastive pre-training task in images and was recently shown to be effective in environmental sound and genre classification **He et al., "Masked Autoencoders Imprve Image Classification"**. 

\subsection{General-purpose Audio Representations}

The COLA framework **Cai et al., "Contrastive Learning for General-Purpose Audio Representations"** employs a simple contrastive learning framework built on SimCLR and utilizes Mel-spectrogram representations and bilinear comparisons to achieve better results than supervised counterparts. HARES **Gao et al., "HARES: Hierarchical Audio Representation with Enhanced Slowfast Networks"** further demonstrated that normalizer-free Slowfast networks (trained on the SimCLR objective) lead to effective generalization of audio representations **Kong et al., "Audio Representation Learning via Slowfast Networks"**; this finding was later used by **Wang et al., "Music-Specific Audio Representations via Normalizer-Free Slowfast Networks"** for music-specific tasks.

\subsection{Patch Masking}
While effective in sequence modeling, transformers **Vaswani et al., "Attention Is All You Need"** suffer from quadratic memory and time complexity with respect to the number of tokens. To address this issue, prior work has explored various token masking strategies to reduce computational overhead. In the self-supervised domain, MAE and FLIP **Carion et al., "Masked Autoencoders Imprve Image Classification"** used masking on image tokens to increase pre-training efficiency. In the supervised setting, PaSST **Chen et al., "Patchout: Efficient Transformers for Audio Tagging"** introduced Patchout (spectrogram masking) to speed up transformer training and achieved state-of-the-art results in audio tagging. Our work is the first to show that spectrogram masking works in the contrastive setting. 

\subsection{Musical Representations}

MusiCNN **Salamon et al., "Music, Convolutional, CNN"**, a CNN designed for log-mel spectrograms, draws on the discussion in **van den Oord et al., "WaveNet: A Generative Model for Raw Audio"** for its efficient design and is pre-trained on a supervised music auto-tagging task. CLMR **Huang et al., "CLMR: Contrastive Learning for Music Representation"** adapted the SimCLR framework for music using SampleCNN **Kim et al., "SampleCNN: A CNN-based Framework for Music Information Retrieval"** on raw waveforms and achieved competitive results with supervised counterparts; S3T **Wang et al., "S3T: Swin Transformer for Music Tagging and Classification"** improved on this by using a swin transformer on spectrograms with simplified augmentations and achieved notable gains in tagging and classification. MULE **Liu et al., "MULE: Multitask Learning for Music Information Retrieval"** provides a broad analysis of supervised and unsupervised (contrastive) pre-training methodologies on MIR downstream tasks and are the only existing work to not use pitch shifting as an augmentation in a contrastive setting, instead favoring MixUp **Zhang et al., "MixUp: Beyond Empirical Risk Minimization"** as their sole augmentation. We believe this is a step in the right direction and this work aims to further refine this approach. Their follow-up work studies the effect of various augmentations on model performance **He et al., "Augmentation is All You Need"**. Recent work has adopted NLP techniques for MIR: JukeMIR **Liu et al., "JukeMIR: Joint Music Information Retrieval with Jukebox"** successfully probed representations from Jukebox **Raffel et al., "Learning to Generate Music with Songwave"**, a music generation model based on the GPT architecture. Following this, MERT **Wang et al., "MERT: Masked Language Modeling for Music Tagging and Classification"** and MusicFM **Li et al., "MusicFM: A Framework for Music Information Retrieval"** achieve state-of-the-art results via masked language modeling on music audio tokens.