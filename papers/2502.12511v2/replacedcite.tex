\section{Related Work}
% This section explores the advancements in self-supervised learning frameworks for music information retrieval, with a focus on the development of general-purpose audio and musical representations. We highlight the transition from traditional supervised methods to unsupervised techniques that leverage the vast amounts of unlabeled musical data. 

\subsection{Self-Supervised Learning Frameworks}

SimCLR ____ is a simple contrastive approach for learning discriminative representations and has found success in areas ranging from computer vision to language ____. A similar notable framework is Contrastive Predictive Coding ____, a universal approach to contrastive learning, which has been successful for MIR- and audio-related tasks such as speaker and phoneme classification using raw audio. Additionally, this work introduced the InfoNCE loss, which is used in SimCLR, CLMR, and Myna.

Recently, due to the widespread success of transformer-based models on various tasks and modalities, MIR researchers have borrowed unsupervised learning paradigms from natural language processing. In ____, the authors probe the hidden layers of OpenAI's Jukebox model ____ and achieve state-of-the-art results, suggesting that CALM (codified audio language modeling) is an effective pre-training approach for MIR tasks. The authors of this work also suggested that transformer-encoder based models are likely to outperform JukeMIR's performance in music audio representation. Building on this, ____ and ____ have emerged as pioneering efforts that harness masked language modeling for musical applications. Masked auto-encoding (MAE) has found success as another non-contrastive pre-training task in images and was recently shown to be effective in environmental sound and genre classification ____. 

\subsection{General-purpose Audio Representations}

The COLA framework ____ employs a simple contrastive learning framework built on SimCLR and utilizes Mel-spectrogram representations and bilinear comparisons to achieve better results than supervised counterparts. HARES ____ further demonstrated that normalizer-free Slowfast networks (trained on the SimCLR objective) lead to effective generalization of audio representations ____; this finding was later used by ____ for music-specific tasks.

\subsection{Patch Masking}
While effective in sequence modeling, transformers ____ suffer from quadratic memory and time complexity with respect to the number of tokens. To address this issue, prior work has explored various token masking strategies to reduce computational overhead. In the self-supervised domain, MAE and FLIP ____ used masking on image tokens to increase pre-training efficiency. In the supervised setting, PaSST ____ introduced Patchout (spectrogram masking) to speed up transformer training and achieved state-of-the-art results in audio tagging. Our work is the first to show that spectrogram masking works in the contrastive setting. 

\subsection{Musical Representations}

MusiCNN ____, a CNN designed for log-mel spectrograms, draws on the discussion in ____ for its efficient design and is pre-trained on a supervised music auto-tagging task. CLMR ____ adapted the SimCLR framework for music using SampleCNN ____ on raw waveforms and achieved competitive results with supervised counterparts; S3T ____ improved on this by using a swin transformer ____ on spectrograms with simplified augmentations and achieved notable gains in tagging and classification. MULE ____ provides a broad analysis of supervised and unsupervised (contrastive) pre-training methodologies on MIR downstream tasks and are the only existing work to not use pitch shifting as an augmentation in a contrastive setting, instead favoring MixUp ____ as their sole augmentation. We believe this is a step in the right direction and this work aims to further refine this approach. Their follow-up work studies the effect of various augmentations on model performance ____. Recent work has adopted NLP techniques for MIR: JukeMIR ____ successfully probed representations from Jukebox ____, a music generation model based on the GPT architecture. Following this, MERT ____ and MusicFM ____ achieve state-of-the-art results via masked language modeling on music audio tokens.