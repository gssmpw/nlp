\section{Related Work}
% This section explores the advancements in self-supervised learning frameworks for music information retrieval, with a focus on the development of general-purpose audio and musical representations. We highlight the transition from traditional supervised methods to unsupervised techniques that leverage the vast amounts of unlabeled musical data. 

\subsection{Self-Supervised Learning Frameworks}

SimCLR \cite{SimCLR} is a simple contrastive approach for learning discriminative representations and has found success in areas ranging from computer vision to language \cite{CLMR, SimCSE}. A similar notable framework is Contrastive Predictive Coding \cite{CPC}, a universal approach to contrastive learning, which has been successful for MIR- and audio-related tasks such as speaker and phoneme classification using raw audio. Additionally, this work introduced the InfoNCE loss, which is used in SimCLR, CLMR, and Myna.

Recently, due to the widespread success of transformer-based models on various tasks and modalities, MIR researchers have borrowed unsupervised learning paradigms from natural language processing. In \cite{JukeMIR}, the authors probe the hidden layers of OpenAI's Jukebox model \cite{Jukebox} and achieve state-of-the-art results, suggesting that CALM (codified audio language modeling) is an effective pre-training approach for MIR tasks. The authors of this work also suggested that transformer-encoder based models are likely to outperform JukeMIR's performance in music audio representation. Building on this, \cite{MERT} and \cite{MusicFM} have emerged as pioneering efforts that harness masked language modeling for musical applications. Masked auto-encoding (MAE) has found success as another non-contrastive pre-training task in images and was recently shown to be effective in environmental sound and genre classification \cite{MAE, msemae, m2d}. 

\subsection{General-purpose Audio Representations}

The COLA framework \cite{COLA} employs a simple contrastive learning framework built on SimCLR and utilizes Mel-spectrogram representations and bilinear comparisons to achieve better results than supervised counterparts. HARES \cite{HARES} further demonstrated that normalizer-free Slowfast networks (trained on the SimCLR objective) lead to effective generalization of audio representations \cite{slowfast, nfnets}; this finding was later used by \cite{MULE} for music-specific tasks.

\subsection{Patch Masking}
While effective in sequence modeling, transformers \cite{transformer} suffer from quadratic memory and time complexity with respect to the number of tokens. To address this issue, prior work has explored various token masking strategies to reduce computational overhead. In the self-supervised domain, MAE and FLIP \cite{MAE, FLIP} used masking on image tokens to increase pre-training efficiency. In the supervised setting, PaSST \cite{patchout} introduced Patchout (spectrogram masking) to speed up transformer training and achieved state-of-the-art results in audio tagging. Our work is the first to show that spectrogram masking works in the contrastive setting. 

\subsection{Musical Representations}

MusiCNN \cite{MusiCNN}, a CNN designed for log-mel spectrograms, draws on the discussion in \cite{PonsCNN} for its efficient design and is pre-trained on a supervised music auto-tagging task. CLMR \cite{CLMR} adapted the SimCLR framework for music using SampleCNN \cite{SampleCNN} on raw waveforms and achieved competitive results with supervised counterparts; S3T \cite{s3t} improved on this by using a swin transformer \cite{swin} on spectrograms with simplified augmentations and achieved notable gains in tagging and classification. MULE \cite{MULE} provides a broad analysis of supervised and unsupervised (contrastive) pre-training methodologies on MIR downstream tasks and are the only existing work to not use pitch shifting as an augmentation in a contrastive setting, instead favoring MixUp \cite{MixUp} as their sole augmentation. We believe this is a step in the right direction and this work aims to further refine this approach. Their follow-up work studies the effect of various augmentations on model performance \cite{augmentation-embedding}. Recent work has adopted NLP techniques for MIR: JukeMIR \cite{JukeMIR} successfully probed representations from Jukebox \cite{Jukebox}, a music generation model based on the GPT architecture. Following this, MERT \cite{MERT} and MusicFM \cite{MusicFM} achieve state-of-the-art results via masked language modeling on music audio tokens.