@article{CLMR,
  author       = {Janne Spijkervet and
                  John Ashley Burgoyne},
  title        = {Contrastive Learning of Musical Representations},
  journal      = {CoRR},
  volume       = {abs/2103.09410},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.09410},
  eprinttype    = {arXiv},
  eprint       = {2103.09410},
  timestamp    = {Tue, 23 Mar 2021 16:29:47 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-09410.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{COLA,
      title={Contrastive Learning of General-Purpose Audio Representations}, 
      author={Aaqib Saeed and David Grangier and Neil Zeghidour},
      year={2020},
      eprint={2010.10915},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@article{CPC,
  author       = {A{\"{a}}ron van den Oord and
                  Yazhe Li and
                  Oriol Vinyals},
  title        = {Representation Learning with Contrastive Predictive Coding},
  journal      = {CoRR},
  volume       = {abs/1807.03748},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.03748},
  eprinttype    = {arXiv},
  eprint       = {1807.03748},
  timestamp    = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{FLIP,
      title={Scaling Language-Image Pre-training via Masking}, 
      author={Yanghao Li and Haoqi Fan and Ronghang Hu and Christoph Feichtenhofer and Kaiming He},
      year={2023},
      eprint={2212.00794},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.00794}, 
}

@article{HARES,
  author       = {Luyu Wang and
                  Pauline Luc and
                  Yan Wu and
                  Adri{\`{a}} Recasens and
                  Lucas Smaira and
                  Andrew Brock and
                  Andrew Jaegle and
                  Jean{-}Baptiste Alayrac and
                  Sander Dieleman and
                  Jo{\~{a}}o Carreira and
                  A{\"{a}}ron van den Oord},
  title        = {Towards Learning Universal Audio Representations},
  journal      = {CoRR},
  volume       = {abs/2111.12124},
  year         = {2021},
  url          = {https://arxiv.org/abs/2111.12124},
  eprinttype    = {arXiv},
  eprint       = {2111.12124},
  timestamp    = {Fri, 26 Nov 2021 13:48:43 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2111-12124.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{JukeMIR,
  title={Codified audio language modeling learns useful representations for music information retrieval},
  author={Castellon, Rodrigo and Donahue, Chris and Liang, Percy},
  booktitle={ISMIR},
  year={2021}
}

@misc{Jukebox,
      title={Jukebox: A Generative Model for Music}, 
      author={Prafulla Dhariwal and Heewoo Jun and Christine Payne and Jong Wook Kim and Alec Radford and Ilya Sutskever},
      year={2020},
      eprint={2005.00341},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@article{MAE,
  author       = {Kaiming He and
                  Xinlei Chen and
                  Saining Xie and
                  Yanghao Li and
                  Piotr Doll{\'{a}}r and
                  Ross B. Girshick},
  title        = {Masked Autoencoders Are Scalable Vision Learners},
  journal      = {CoRR},
  volume       = {abs/2111.06377},
  year         = {2021},
  url          = {https://arxiv.org/abs/2111.06377},
  eprinttype    = {arXiv},
  eprint       = {2111.06377},
  timestamp    = {Tue, 16 Nov 2021 12:12:31 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2111-06377.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{MERT,
      title={MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training}, 
      author={Yizhi Li and Ruibin Yuan and Ge Zhang and Yinghao Ma and Xingran Chen and Hanzhi Yin and Chenghao Xiao and Chenghua Lin and Anton Ragni and Emmanouil Benetos and Norbert Gyenge and Roger Dannenberg and Ruibo Liu and Wenhu Chen and Gus Xia and Yemin Shi and Wenhao Huang and Zili Wang and Yike Guo and Jie Fu},
      year={2024},
      eprint={2306.00107},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{MULE,
      title={Supervised and Unsupervised Learning of Audio Representations for Music Understanding}, 
      author={Matthew C. McCallum and Filip Korzeniowski and Sergio Oramas and Fabien Gouyon and Andreas F. Ehmann},
      year={2022},
      eprint={2210.03799},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@article{MixUp,
  author       = {Hongyi Zhang and
                  Moustapha Ciss{\'{e}} and
                  Yann N. Dauphin and
                  David Lopez{-}Paz},
  title        = {mixup: Beyond Empirical Risk Minimization},
  journal      = {CoRR},
  volume       = {abs/1710.09412},
  year         = {2017},
  url          = {http://arxiv.org/abs/1710.09412},
  eprinttype    = {arXiv},
  eprint       = {1710.09412},
  timestamp    = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1710-09412.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{MusiCNN,
  author       = {Jordi Pons and
                  Xavier Serra},
  title        = {musicnn: Pre-trained convolutional neural networks for music audio
                  tagging},
  journal      = {CoRR},
  volume       = {abs/1909.06654},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.06654},
  eprinttype    = {arXiv},
  eprint       = {1909.06654},
  timestamp    = {Mon, 23 Sep 2019 18:07:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-06654.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{MusicFM,
    title={A Foundation Model for Music Informatics},
    author = {Won, Minz and Hung, Yun-Ning and Le, Duc},
    journal={arXiv preprint arXiv:2311.03318},
    year={2023}
}

@misc{PonsCNN,
      title={Timbre Analysis of Music Audio Signals with Convolutional Neural Networks}, 
      author={Jordi Pons and Olga Slizovskaia and Rong Gong and Emilia Gómez and Xavier Serra},
      year={2017},
      eprint={1703.06697},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@Article{SampleCNN,
AUTHOR = {Lee, Jongpil and Park, Jiyoung and Kim, Keunhyoung Luke and Nam, Juhan},
TITLE = {SampleCNN: End-to-End Deep Convolutional Neural Networks Using Very Small Filters for Music Classification},
JOURNAL = {Applied Sciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {150},
URL = {https://www.mdpi.com/2076-3417/8/1/150},
ISSN = {2076-3417},
ABSTRACT = {Convolutional Neural Networks (CNN) have been applied to diverse machine learning tasks for different modalities of raw data in an end-to-end fashion. In the audio domain, a raw waveform-based approach has been explored to directly learn hierarchical characteristics of audio. However, the majority of previous studies have limited their model capacity by taking a frame-level structure similar to short-time Fourier transforms. We previously proposed a CNN architecture which learns representations using sample-level filters beyond typical frame-level input representations. The architecture showed comparable performance to the spectrogram-based CNN model in music auto-tagging. In this paper, we extend the previous work in three ways. First, considering the sample-level model requires much longer training time, we progressively downsample the input signals and examine how it affects the performance. Second, we extend the model using multi-level and multi-scale feature aggregation technique and subsequently conduct transfer learning for several music classification tasks. Finally, we visualize filters learned by the sample-level CNN in each layer to identify hierarchically learned features and show that they are sensitive to log-scaled frequency.},
DOI = {10.3390/app8010150}
}

@article{SimCLR,
  author       = {Ting Chen and
                  Simon Kornblith and
                  Mohammad Norouzi and
                  Geoffrey E. Hinton},
  title        = {A Simple Framework for Contrastive Learning of Visual Representations},
  journal      = {CoRR},
  volume       = {abs/2002.05709},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.05709},
  eprinttype    = {arXiv},
  eprint       = {2002.05709},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05709.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{SimCSE,
      title={SimCSE: Simple Contrastive Learning of Sentence Embeddings}, 
      author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
      year={2022},
      eprint={2104.08821},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{augmentation-embedding,
  author={McCallum, Matthew C. and Davies, Matthew E. P. and Henkel, Florian and Kim, Jaehun and Sandberg, Samuel E.},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={On The Effect Of Data-Augmentation On Local Embedding Properties In The Contrastive Learning Of Music Audio Representations}, 
  year={2024},
  volume={},
  number={},
  pages={671-675},
  keywords={Training data;Self-supervised learning;Signal processing;Data augmentation;Labeling;Task analysis;Speech processing;Music audio embeddings;data augmentation},
  doi={10.1109/ICASSP48485.2024.10446274}}

@article{m2d,
    title   = {{Masked Modeling Duo: Towards a Universal Audio Pre-training Framework}},
    author  = {Daisuke Niizumi and Daiki Takeuchi and Yasunori Ohishi and Noboru Harada and Kunio Kashino},
    journal = {IEEE/ACM Trans. Audio, Speech, Language Process.},
    year    = {2024},
    volume  = {32},
    pages   = {2391-2406},
    url     = {https://ieeexplore.ieee.org/document/10502167},
    doi     = {10.1109/TASLP.2024.3389636}}

@InProceedings{msemae,
    title     = {Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation},
    author    = {Niizumi, Daisuke and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio},
    booktitle = {HEAR: Holistic Evaluation of Audio Representations (NeurIPS 2021 Competition)},
    pages     = {1--24},
    year      = {2022},
    editor    = {Turian, Joseph and Schuller, Björn W. and Herremans, Dorien and Kirchoff, Katrin and Perera, Paola Garcia and Esling, Philippe},
    volume    = {166},
    series    = {Proceedings of Machine Learning Research},
    month     = {13--14 Dec},
    publisher = {PMLR},
    pdf       = {https://proceedings.mlr.press/v166/niizumi22a/niizumi22a.pdf},
    url       = {https://proceedings.mlr.press/v166/niizumi22a.html}
}

@misc{nfnets,
      title={High-Performance Large-Scale Image Recognition Without Normalization}, 
      author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},
      year={2021},
      eprint={2102.06171},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{patchout,
  author       = {Khaled Koutini and
                  Jan Schl{\"{u}}ter and
                  Hamid Eghbal{-}zadeh and
                  Gerhard Widmer},
  title        = {Efficient Training of Audio Transformers with Patchout},
  journal      = {CoRR},
  volume       = {abs/2110.05069},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.05069},
  eprinttype    = {arXiv},
  eprint       = {2110.05069},
  timestamp    = {Thu, 21 Oct 2021 16:20:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-05069.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{s3t,
  author={Zhao, Hang and Zhang, Chen and Zhu, Bilei and Ma, Zejun and Zhang, Kejun},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={S3T: Self-Supervised Pre-Training with Swin Transformer For Music Classification}, 
  year={2022},
  volume={},
  number={},
  pages={606-610},
  keywords={Representation learning;Time-frequency analysis;Pipelines;Tagging;Signal processing;Transformers;Feature extraction;Self-supervised learning;Swin Transformer;music genre classification;music tagging},
  doi={10.1109/ICASSP43922.2022.9746056}}

@misc{slowfast,
      title={SlowFast Networks for Video Recognition}, 
      author={Christoph Feichtenhofer and Haoqi Fan and Jitendra Malik and Kaiming He},
      year={2019},
      eprint={1812.03982},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{swin,
  author       = {Ze Liu and
                  Yutong Lin and
                  Yue Cao and
                  Han Hu and
                  Yixuan Wei and
                  Zheng Zhang and
                  Stephen Lin and
                  Baining Guo},
  title        = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  journal      = {CoRR},
  volume       = {abs/2103.14030},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.14030},
  eprinttype    = {arXiv},
  eprint       = {2103.14030},
  timestamp    = {Mon, 05 Jun 2023 16:18:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-14030.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{transformer,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

