As established, identifying the critical atom(s) of a \dnli~example allows us to pinpoint the knowledge or fact that the example is designed to evaluate (\S\ref{subsec:quds}).
%
The two different examples in Figure~\ref{fig:critical-atom} share the same critical atom, representing two different contexts under which the model must directly evaluate the fact \textit{``The people are friends.''}
%
A model correctly predicting whether or not people are friends under some contexts but not others may indicate that it has not fully understood the factors that influence the inference.
%
Because they contain independent examples, few datasets accommodate measuring a model's \textit{inferential consistency} ($I_C$), or the likelihood that its prediction for a particular inference will remain consistently correct or incorrect under different contexts (here, contexts refers to different $(P, U)$ pairs). 
%
Correctly drawing an inference under a single context does not guarantee that the model will make a correct prediction for the same inference under a different context.
%

To quantify this, we group examples in \dsnlitest~by their critical atom and report the inferential consistency of different models.
%


\begin{figure}[t!]
\centering
\includegraphics[scale=0.6]{figures/critical-atom.pdf}
\caption{Grouping examples by their critical atom(s) allows us to understand under which \textit{contexts} ($P + U$) a model has understood a piece of knowledge. Here, we show two \dnli~examples that evaluate the same atom (top): one that \textit{strengthens} it (left), and one that \textit{weakens} it (right). 
A model that truly understands a fact and the factors that influence it (or, conversely does not) should yield consistently correct or incorrect predictions. However, some models have mixed accuracy among examples targeting the same atom, indicating that they only understand the inference under \textit{some} contexts.}
\label{fig:critical-atom}
\end{figure}

%
\paragraph{How many unique critical atoms does \dsnlitest~evaluate?} While \dsnli~contains around 2K test examples and 88K training examples, how many distinct critical atoms underlie those examples? 
%
We quantify this by identifying semantically equivalent critical atoms in \dsnlitest~by embedding each atom with \texttt{NV-Embed-7B}~\cite{lee2024nv} and then computing pairwise cosine similarity between all critical atoms.
%
We construct a graph $G$ where nodes correspond to critical atoms and edges are drawn between nodes if their similarity is above a threshold ($\theta=0.75$) and they are bidirectionally entailed as determined by the NLI model in \S\ref{subsec:validate-atoms}.
%
Finding semantically equivalent groupings then reduces to finding all maximal cliques~\cite{tomita2006worst} in $G$.

The 1,761 \dsnlitest~examples that contain at least one valid atom (from 193 unique $P$-$H$ pairs) contain 429 unique atoms.\footnote{Note that the number of unique atoms is upper-bounded by the \snli~hypotheses that \dsnli~builds upon.}
%
Following the procedure above yields 349 unique cliques, or \textbf{349 unique \textit{critical} atoms}.\footnote{We also experiment with generating the QUDs (Appendix~\ref{appendix:qud-diversity}) for each critical atom and find that \dsnlitest~evaluates 223 unique QUDs, around an 81\% coverage of the available atoms.}
%
We bucket examples by these cliques, resulting in groups of examples that share a critical atom (which we call \textit{critical atom buckets}, or just \textit{buckets}, represented as \bucket).
%
We find that certain critical atoms arise frequently: top critical atoms include \textit{``The others are friends of the person''} (21 examples) or \textit{``The person is a man''} (20 examples).
%

%
\paragraph{Measuring Inferential Consistency ($I_C$).} \citet{srikanth2024often} introduce \textit{paraphrastic consistency}, a metric capturing the probability that a model's prediction for two paraphrases of the same NLI problem remain consistent (both incorrect or both correct).
%
We adapt this metric to compute a model's \textit{inferential consistency}, or the probability that its predictions for two defeasible NLI examples $e_i$ and $e_j$ \textbf{that share the same critical atom} are either both correct or incorrect.
%
As in \citet{srikanth2024often}, we define two terms: 
\begin{packed_itemize}
  \item $R_{\bucketsmall}$: a discrete random binary variable representing whether a model's prediction for a full \dnli~example is correct (1) or incorrect (0). 
  \item $\theta_{\bucketsmall}$: $\mathds{E}[R_{\bucketsmall}]$, or the average correctness (i.e., accuracy) of the \dnli~examples in a particular critical atom bucket.
\end{packed_itemize}

\noindent For a binary classification task ($y \in \{0, 1\}$) and a model $M$, we define $I_C$ as
\vspace{-0.5em}
\begin{multline}
    I_C = \underbrace{P(M(e_i)= y, M(e_j) = y)}_{\text{prob. of both predictions correct}} + \\
    \underbrace{P(M(e_i) \neq y, M(e_j) \neq y)}_{\text{prob. of both predictions incorrect}}
\end{multline}

\noindent We estimate $I_C$ directly from the accuracies of critical atom buckets as: 
\vspace{-0.5em}
\begin{equation}
    I_C = \mathds{E}[\theta_{\bucketsmall}^2] + \mathds{E}[(1-\theta_{\bucketsmall})^2]
\label{eq:pstay}
\end{equation}

\noindent Note that \dnli~examples can have multiple critical atoms (Figure~\ref{fig:num-atoms}).
%
In these cases, we divide the weight of the example $e$ across all critical example buckets that share $e$ when computing $\theta_{\bucketsmall}$.
%
\input{tables/inferential_consistency}


\paragraph{Results.} All models exhibit room for improvement in inferential consistency (Table~\ref{tab:inferential-consistency}), giving us a sense of how well the LLMs we analyze have internalized the critical atomic facts evaluated in \dsnlitest.
%
One source of inconsistency we observe arises when certain contexts (premise-update pairs) demand more implicit reasoning or background knowledge.
%
For example, consider two different contexts under which the model must evaluate the same critical atom \textit{``The people are tall''}:

\begingroup
\addtolength\leftmargini{-0.2in}
\begin{quote}
\small

\noindent \colorbox{lightpurple}{\textcolor{darkpurple}{\textbf{Context 1:}}} $P$: Four people standing on a hiking trail in a forest with big tree logs on the ground, $U$: Their long legs step across several logs at once. \\ 
            \colorbox{lightteal}{\textcolor{teal}{\textbf{Context 2}}} $P$: Two men in orange uniforms stand before a train and do some work, $U$: They can easily touch the top of the train.
\end{quote}
\endgroup

\noindent Several models struggle to draw the strengthening inference under the first context, but all models that we analyze successfully draw the strengthening inference under the second.
%

These analyses help us understand whether models understand pieces of knowledge and the factors that influence them, raising interesting questions about how best to collect updates to increase the coverage of a fact or situation.
%
Defeasible reasoning systems must be able to deftly modulate inferences in a manner that is sensitive to diverse contexts. 
%
Future work may leverage the identification of critical atomic sub-problems to nudge annotators toward underrepresented critical atoms or contexts.
