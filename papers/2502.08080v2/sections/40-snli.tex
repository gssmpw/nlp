\input{tables/benchmarking_snli}

While performing strongly on various benchmarks, LLMs still struggle with many types of consistency including paraphrastic consistency~\cite{srikanth2024often, verma-etal-2023-evaluating}, hypothetical consistency~\cite{chen2023two}, or even preferential consistency~\cite{zhao2024measuring}.
%
When LLMs make entailment judgments, another desirable property is \textit{logical consistency}.
%
Namely, when an LLM itself deems a set of atoms $a_1$...$a_n$ entailed by $H$, we can hold it accountable to maintain consistency between its judgments on each ($P$, $a_i$) sub-problem and its overall ($P$, $H$) judgment in a logical way.
%
This gives us necessary, but not sufficient, evidence to help signal that it has ``understood'' the situation.

\subsection{Atomic and Overall Label Consistency}
\label{sec:snli-atoms:rules}
We construct a set of rules to establish the relationship between atomic sub-problems and overall problem labels.
%
\vspace{-0.5em}
\begin{tight_enumerate}
    \item If $H$ is entailed by $P$: Each valid $a_i$ must be entailed by $P$.
    \item If $H$ contradicts $P$: At least one valid $a_i$ must contradict $P$.
    \item If $H$ is neutral with respect to $P$: At least one valid $a_i$ must be neutral with respect to $P$, all others may be either neutral or entailed.
\end{tight_enumerate}

\subsection{Experimental Setup}
We experiment with six LLMs: \texttt{gpt-4o}~\cite{openai2024gpt4o}, \texttt{gpt-4o-mini}~\cite{openai2024gpt4o}, \texttt{gpt-3.5-turbo-0125}~\cite{ouyang2022training}, \texttt{llama-3-8b-instruct}~\cite{dubey2024llama}, \texttt{llama-3-70b-instruct}~\cite{dubey2024llama}, and \texttt{gemma-2-9b-instruct}~\cite{team2024gemma}.
%

First, we benchmark each models's performance on \snlitest~original examples (Table~\ref{tab:benchmarking-snli}) using Prompt~\ref{prompt:nli} adapted from~\citet{liu2023evaluating}.
%
We use 12 in-context \textit{original} \snli~examples from the dev split evenly distributed over the three NLI labels.
%
Then, for each \snlitest~example, we have each model predict the relation between $H$ and each generated atom $a_i$ from~\S\ref{subsec:generate-atoms} using the same prompt and exemplar set.
%
For each atom that the LLM predicts as entailed, we have it predict the relation between $P$ and $a_i$ using the same prompt and exemplar set.
%

We report overall logical consistency as the percent of examples where the full prediction was logically consistent with the predicted labels for sub-problems as dictated by the rules in \S\ref{sec:snli-atoms:rules}.

\paragraph{Results.} Despite higher \textit{accuracy} numbers, LLMs seem to struggle with \textit{logical consistency} (Table~\ref{tab:benchmarking-snli}).
%
Interestingly, a model's accuracy is not fully indicative of its logical consistency. 
%
When models incorrectly predict the full example's label, they are more prone to logical inconsistencies between atomic sub-problems and the full problem (Table~\ref{tab:benchmarking-snli}, Columns 3 and 4).
%
Though not the top performing model, \texttt{gpt-4o} outperformed other models on logical consistency \textbf{even on examples where its full prediction was incorrect}.
%
Such logical consistency indirectly captures the \textit{reliability} of a model's full prediction: when two LLMs achieve similar accuracies, logical consistency serves as another point of comparison.

We also stratify our results by the predicted overall label and report logical consistency within each class (Table~\ref{tab:benchmarking-snli}, Columns 5---7). 
%
All models exhibit consistency gaps \textit{within the 3 labels}, and different models struggle with different example classes.

Lastly, we experiment with \textit{atomic inference}~\cite{stacey-etal-2024-atomic}, or inducing an overall label via logical rules over the predicted atomic sub-problem labels, to understand whether a setting in which models only provide granular inferences can be more effective.
%
We induce an overall label with similar logical rules to those in \S\ref{sec:snli-atoms:rules}: (1) if all $a_i$ are predicted as entailed by $P$, we predict entailment, (2) if at least one $a_i$ is predicted as contradicting $P$, we predict a contradiction, (3) otherwise, predict neutral.
%
While this strategy does not yield competitive performance with full example accuracy (Table~\ref{tab:benchmarking-snli}, Column 1 versus Column 8), it does offer a more \textit{interpretable} framework for LLMs that otherwise seem to struggle with logical consistency.
%
Atom judgments may more difficult than overall judgments for a variety of reasons (see Appendix~\ref{appendix:inconsistency-analysis}), and in turn, inducing a label from individual atomic predictions may be less reliable.
