Decomposition of text into atomic propositions is a flexible framework allowing for the closer inspection of input and output text.
%
We use atomic decomposition of hypotheses in two natural language reasoning tasks, traditional NLI and defeasible NLI, to form \textit{atomic sub-problems}, or granular inferences that models must weigh when solving the overall problem.
%
These atomic sub-problems serve as a tool to further understand the structure of both NLI and defeasible reasoning, probe a model's consistency and understanding of different inferences, and measure the diversity of examples in benchmark datasets. 
%
Our results indicate that LLMs still struggle with logical consistency on atomic NLI and defeasible NLI sub-problems.
%
Lastly, we identify \textit{critical atomic sub-problems} of defeasible NLI examples, or those that most contribute to the overall label, and propose a method to measure the \textit{inferential consistency} of a model, a metric designed to capture the degree to which a model makes consistently correct or incorrect predictions about the same fact under different contexts.