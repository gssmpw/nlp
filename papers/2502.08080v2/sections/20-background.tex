\subsection{Natural Language Inference}
Natural language inference~\cite{giampiccolo2007third, maccartney2009natural, bowman-etal-2015-large} is the task of determining whether a premise $P$ entails, contradicts, or is neutral in relation to a hypothesis, $H$.
%
For example, the premise \textit{``A little girl in a lush green field''} contradicts the hypothesis \textit{``A little girl rides her ox in a desert''}, as ``desert'' directly contradicts the ``lush green field''.

\paragraph{\snli.}
The first large-scale NLI dataset, \snli, uses image captions as premises paired with hypotheses elicited from crowdworkers.
%
Though this benchmark has been well-saturated by LLMs over the past few years, it serves as a useful resource for studies in model robustness~\cite{srikanth-rudinger-2022-partial, kaushik2019learning} and annotation artifacts~\cite{gururangan-etal-2018-annotation}.

\subsection{Defeasible Inference}

Defeasible reasoning is a form of non-monotonic reasoning in which inferences may be altered in light of new evidence~\cite{reiter1980logic}.
%
For example, given the premise \textit{``A group of people sitting around a rectangular table''}, the inference \textit{``they have a work meeting''} is weakened upon learning that \textit{``the people are toddlers.''}

\paragraph{\dnli.}
\citet{rudinger-etal-2020-thinking} introduce the task of defeasible natural language inference and an accompanying benchmark, \dnli. 
%
Given a $P$ and $H$ pair with a \textit{neutral} relation, a third \textbf{update} sentence $U$ \textit{strengthens} $H$ if, upon reading $U$, $H$ is more likely to be true, and \textit{weakens} $H$ if $H$ is less likely to be true upon reading $U$.
%
%
Defeasible NLI is then a binary classification task of predicting a strengthener or weakener label for a $(P, H, U)$ set.
% 
\dnli~is built on top of three popular commonsense reasoning datasets: \snli~\cite{bowman-etal-2015-large}, \abr{atomic}~\cite{sap2019atomic}, and \abr{social-chem-101}~\cite{forbes2020social}.
%
For a $P$ and $H$ pair (or, in the case of \abr{social-chem-101}, just $H$), crowdworkers write an update sentence for a target label, but are not instructed to target a particular part of $H$ when doing so.
%
The authors ensure that the train, development, and test splits of the data are split at the $P$-$H$ level to avoid leakage.
%
Crowdworkers may not write updates that directly contradict information in the premise.
%
For simplicity, we focus on the \snli-derived split of \dnli, or \dsnli, which selects neutral $P$-$H$ \snli~pairs.


\subsection{Related Work}
Atomic decomposition has been used in fact checking~\cite{min-etal-2023-factscore, glover-etal-2022-revisiting, yuan-vlachos-2024-zero}, claim verification~\cite{chen-etal-2024-complex}, summarization~\cite{nenkova-passonneau-2004-evaluating}, and text-to-image generation~\cite{cho2023davidsonian} among others.
%
\citet{kamoi-etal-2023-wice} construct a dataset of claims and sub-claims for claim checking where sub-claims, analogous to our atoms, are labeled with respect to evidence.
%
Most relevant to our work, \citet{stacey-etal-2022-logical} train a span-based NLI model to make \textit{span-level} decisions on \snli~and SICK examples that are composed to produce an overall label. 
%
In contrast, our work measures the logical consistency of LLMs that may have seen NLI data during pretraining, but that are not explicitly trained to weigh atomic inferences.
%
Their followup~\cite{stacey-etal-2024-atomic} \textit{trains} another NLI system using LLM-generated atoms, however their study focuses primarily on \textit{premise} decomposition.
%
To the best of our knowledge, our study is the first to explore atoms in defeasible inference.