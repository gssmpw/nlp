\input{tables/annotated_atom_stats}
\input{tables/proportion-completeness}

\section{Atom Generation}
\label{appendix:atom-generation}
\vspace{-3em}
\begin{prompt}[title={Prompt \thetcbcounter: Atom Generation}, label=prompt:atom-generation]
\texttt{\colorbox{lightblue}{Prompt:} You are an expert linguist. You are given a \textbf{sentence}. Generate a list of atomic facts that are strictly logically entailed from the given \textbf{sentence}. Keep each fact independent and self-contained. Each fact should make sense when read on its own. Only write facts that are directly described or supported by the \textbf{sentence}. End your response with [END].\\\\SENTENCE: \{\textbf{sentence}\}\\\\FACTS:}
\end{prompt}

\subsection{Coverage of Generated Atoms}
\label{appendix:coverage-atoms}
Generated atoms must cover the information presented in the hypothesis. 
%
For example, for the hypothesis $H$ in Figure~\ref{fig:teaser-example}, an atom generation model should produce atoms covering all three pieces of information: at least one mentioning \textit{professional}, \textit{actors}, and \textit{summer}.
%
Pruning should not reduce the coverage of the atom set.
%
Here, we estimate the completeness, or coverage, of the generated atoms with respect to the hypothesis of NLI examples. 
%

\paragraph{Completeness in the case of SNLI.} We note that completeness of atoms only matters in particular cases. 
%
When the overall $P$-$H$ pair is predicted by the model to be entailed, the logical consistency check does not rely on completeness, as no single atom, if predicted by the model to be entailed, should be predicted as anything other than entailment.
%
However, there are two possible scenarios of inconsistency when $P-H$ is predicted to be neutral: (a) All atoms were predicted to be entailed by P (this does necessitate ensuring completeness), or (b) one or more atoms is predicted to be a contradiction (this does not require ensuring completeness).
%
Consistency when $P$-$H$ is predicted to be a contradiction does require checking that all atoms cover the hypothesis.
%
Table~\ref{tab:proportion-completeness} shows the proportion of examples in \snlitest~that are dependent upon a completeness assumption as per the description for each label above.
These help contexualize our results in Table~\ref{tab:benchmarking-snli} by estimating an upper bound of logical consistency.
%
%

To measure completeness in SNLI, we randomly sample 50 SNLI examples and annotate for completeness, and find that in 49/50 examples, all pieces of information from the original hypothesis project into at least one atom. The only example in our random sample that was missing an atom was for the hypothesis shown in Table~\ref{tab:completeness-snli}.

We also sample another set of 50 random examples from \snlitest~where one of our models (\texttt{gpt-4o-2024-08-06}) predicted either contradiction or neutral \textit{and} where consistency failed in order to understand of what percentage of errors are due to actual failures in consistency or are simply due to lack of completeness.
%
Here, we study the atoms that the model deemed entailed from the hypothesis for each example, and annotate whether those cover all pieces of information in the hypothesis.
%
In 6/50 examples, \texttt{gpt-4o-2024-08-06} incorrectly judged that at least one of the generated atoms was not entailed by the hypothesis, hence omitting it and causing a completeness issue for the set of atoms over which we measure its logical consistency. 
%
However, we find that of these six cases, only two examples were missing an atom in the set of generated atoms, again indicating that our \textit{atom generation} process reliably projects all information from the hypothesis into the generated set of atoms.


\paragraph{Completeness in the case of \dnli.} We manually validated all generated 4,079 atomic subproblems in the entire test set of \dsnlitest. 
%
Lack of completeness is most likely when none of the atoms have a gold label in the direction of the gold label of the overall example, indicating that an atom may be missing.
%
This happens in only 3\% of examples (70/1761) in \dsnlitest.
%
We annotate these 70 examples to understand whether or not an atom was indeed missing after our automatic pruning step. 
%
We find that in only 28 of the 70 examples (representing only ~1\% of \textit{all} examples), at least one atom was missing.
%
Most of the cases where none of the atoms have a gold label in the direction of the gold label of the overall example are cases where the original example is flawed in some way (hinges on some stereotype, ambiguous language, faulty reasoning, or the original crowdworker who wrote the update did not understand the instructions) and our annotations do not propagate the flawed assumptions or reasoning from the original example. Table~\ref{tab:completeness} shows an example.
The original gold label propagates a stereotype or attitude towards janitors (i.e it is less likely they have an important meeting if they are a janitor). We choose not to propagate such attitudes or stereotypes in our annotation, hence, all atoms have the ``no effect'' label.


\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ll}
        \toprule
        \rotatebox[origin=c]{90}{\textbf{Example}} &
        \makecell*[{{p{10.7cm}}}]{
            \colorbox{lightpurple}{\textcolor{darkpurple}{\textbf{Premise:}}} A female within the foreground is heading towards a large white colored pillar that is apart of a large building with people are loitering or waiting on the steps of said building. \\ 
            \colorbox{lightteal}{\textcolor{teal}{\textbf{Hypothesis}}} The woman has an important meeting today in the building. \\
            \colorbox{lightorange}{\textcolor{burntorange}{\textbf{Update}}} The woman is wearing a janitor's uniform (\textit{weakener})
        }\\
        
        \midrule
        \rotatebox[origin=c]{90}{\textbf{\textcolor{coolgreen}{Atoms}}}
        &\makecell*[{{p{10cm}}}]{
        $a_1$: The person has something (0, no effect).\\
        $a_2$: The thing the person has is a meeting. (0, no effect) \\
        $a_3$: The meeting is important (0, no effect) \\
        $a_4$: The meeting is today (0, no effect) \\
        $a_5$: The meeting is in a building (0, no effect) \\
}\\

        
        \bottomrule
    \end{tabular}}
    \caption{An example of a lack of completeness in generated atoms.}
    \label{tab:completeness}
\end{table}


\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ll}
        \toprule
        \rotatebox[origin=c]{90}{\textbf{Hypothesis}} &
        \makecell*[{{p{10.7cm}}}]{
            The man and woman are going to a movie in the city.
        }\\
        
        \midrule
        \rotatebox[origin=c]{90}{\textbf{\textcolor{coolgreen}{Atoms}}}
        &\makecell*[{{p{10cm}}}]{
        $a_1$: There are two people. \\
        $a_2$: There are two people who are a man and a woman. \\
        $a_3$: There are people going. \\
        $a_4$: There are people going to a movie. \\
        $a_5$: There is a destination for the people going. \\
        $a_6$: The destination is a movie. \\
        $a_7$: There is a city. \\
        $a_8$: The people are going to the city.
}\\

        
        \bottomrule
    \end{tabular}}
    \caption{An example of a lack of completeness in generated atoms for SNLI. Here the missing atom is \textit{``The movie is in the city.''}}
    \label{tab:completeness-snli}
\end{table}



\section{SNLI}
\begin{prompt}[title={Prompt \thetcbcounter: Traditional NLI}, label=prompt:nli]
\texttt{\colorbox{lightblue}{Prompt:} You will be given a \textbf{premise} and a \textbf{hypothesis} about that \textbf{premise}. You need to decide whether the \textbf{hypothesis} is entailed by the \textbf{premise} by choosing one of the following answers: 'e': The \textbf{hypothesis} follows logically from the information contained in the \textbf{premise}. 'c': The \textbf{hypothesis} is logically false from the information contained in the \textbf{premise}. 'n': It is not possible to determine whether the \textbf{hypothesis} is true or false without further information. Read the \textbf{premise} and \textbf{hypothesis} and select the correct answer from the three answer labels (e, n, c). Also provide a single line of explanation in a new line. End your response with [END] and output nothing after.\\\\Premise: \{premise\}\\Hypothesis:\{hypothesis\}\\\\Is the hypothesis entailed by, contradicted by, or neutral with respect to the premise?}
\end{prompt}

\subsection{Inconsistency in SNLI}
\label{appendix:inconsistency-analysis}
We analyze the set of randomly selected sample of 50 examples from Appendix~\ref{appendix:coverage-atoms} where one of our models (\texttt{gpt-4o-2024-08-06}) was logically inconsistent.
%
We find that inconsistencies arise for a number of reasons, some of which we discuss here.

\paragraph{Misjudgment of atom entailment from the hypothesis.} Measuring logical consistency in SNLI examples happens in two phases (1) the model determines whether the atom entails from the hypothesis, and (2) the model predicts the relationship between the premise and an atom. Depending on its strength, we find that there are inconsistencies that arise from the modelâ€™s misjudgment of atom entailment.
%
For example: 

\begingroup
\addtolength\leftmargini{-0.2in}
\begin{quote}
\small

\noindent \colorbox{lightpurple}{\textcolor{darkpurple}{\textbf{Premise:}}} \textit{A man on a bicycle, wearing cycle gear, riding at a fast past down paved trail surrounded by tree's and grass.} \\ 
            \colorbox{lightteal}{\textcolor{teal}{\textbf{Hypothesis}}} \textit{The man is riding on the sidewalk.}\\
            \colorbox{lightorange}{\textcolor{burntorange}{\textbf{Generated Atoms}}} \textit{($a_1$) There is a person. ($a_2$) There is a person who is a man.
 ($a_3$) There is a person riding. ($a_4$) There is a person riding on something. ($a_5$) The thing the person is riding on is a sidewalk.}
\end{quote}
\endgroup

 \noindent Based on the full premise and hypothesis, \texttt{gpt-4o-2024-08-06} predicted that the hypothesis was \textit{neutral} in relation to the premise. 
 %
 However, in its atomic predictions between the premise and each atom, the model incorrectly determined that the last atom was not entailed by the hypothesis, and hence it was not included in the set of atoms that were used to measure consistency. It judged all other atoms as entailed from the premise, hence leading to the model behaving inconsistently in this example.

\paragraph{Use of hypernyms in atoms.} We make sure to include hypernyms of entities in the set of atomic facts. 
%
For example, \textit{``the man dances''} is decomposed into \textit{``there is a person'', ``the person is a man'', ``the person dances''}. 
%
In some cases, models have difficulty on premise-atom judgments where the premise uses the hyponym (``man'') and the hypothesis uses the hypernym (``person'').

\paragraph{Out of domain syntactic constructions.} The syntactic structures of many of our atoms differ, in some cases significantly, from the constructions in the original dataset. For example \textit{``the thing the person is eating is a sandwich''} is a pseudo-cleft construction that is very rare in the SNLI dataset. 
%
As such, some sentences are out of domain for encoder models that were trained on SNLI or prompt-based models that have inadvertently seen SNLI training data in pretraining corpora.

\paragraph{Weaker effects based on annotation elicitation.} Both SNLI and defeasible NLI are datasets created by crowdworkers writing hypotheses and updates conditioned on a label. 
%
One of the consequences of this process is that hypotheses and updates tend to strongly express the desired label.
%
In contrast, atoms tend to express labels in a softer way, and their effects often compound when taken together.
%
As such, these atoms may be out of distribution as compared to hypotheses that express the label with a higher magnitude.
%
Since the effect of each atom is lighter than when they are taken together in a full sentence, atomic judgments are sometimes much more difficult than overall judgments.

\paragraph{Co-reference Effects.} In some cases, atomic generations remove some of the implicit co-reference between the hypothesis and premise. 
%
We observe that changing the co-reference in atoms can result in inconsistencies between the overall example and each premise-atom judgment.



\section{Defeasible NLI}

\begin{prompt}[title={Prompt \thetcbcounter: Defeasible Inference}, label=prompt:defeasible]
\texttt{\colorbox{lightblue}{Prompt:} You are a reasoning system. You are given a description of a \textbf{situation} and a \textbf{hypothesis} about that \textbf{situation} that may or may not be true. Given some more \textbf{evidence} about the \textbf{situation}, output 'more' if the \textbf{hypothesis} seems more likely to be true after learning the \textbf{evidence}, or output 'less' if the \textbf{hypothesis} seems less likely to be true after learning the \textbf{evidence}. Also provide a single line of explanation in a new line. End your response with [END] and output nothing after.\\ \\Situation: \{context\}\\\\Hypothesis: \{hypothesis\}\\Evidence: \{evidence\}\\\\Does the evidence make the hypothesis about the situation more or less likely to be true?}
\end{prompt}

\begin{prompt}[title={Prompt \thetcbcounter: Defeasible Inference Atoms}, label=prompt:defeasible-atom]
\texttt{\colorbox{lightblue}{Prompt:} You are a reasoning system. You are given a description of a \textbf{situation} and a \textbf{hypothesis} about that \textbf{situation} that may or may not be true. Given some more \textbf{evidence} about the \textbf{situation}, output 'more' if the \textbf{hypothesis} seems more likely to be true after learning the \textbf{evidence}, output 'less' if the \textbf{hypothesis} seems less likely to be true after learning the \textbf{evidence}, or output 'none' if the likelihood of the \textbf{hypothesis} remains unchanged after learning the \textbf{evidence}. Also provide a single line of explanation in a new line. End your response with [END] and output nothing after.\\ \\Situation: \{context\}\\\\Hypothesis: \{hypothesis\}\\Evidence: \{evidence\}\\\\Does the evidence make the hypothesis about the situation more or less likely to be true?}
\end{prompt}


\begin{figure}[t!]
\centering
\includegraphics[scale=0.5]{figures/atom_label_histogram.pdf}
\caption{Distribution of fine-grained labels across all atoms in \dsnlitest.}
\label{fig:atom-label-hist}
\end{figure}


\begin{figure}
\centering
\includegraphics[scale=0.6]{figures/number_of_atoms.pdf}
\caption{Number of atoms (top) and number of critical atoms (bottom) per example in \dsnlitest.}
\label{fig:num-atoms}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.5]{figures/critical_atom_usage_for_single_update.pdf}
\caption{Proportion of valid atoms used as critical atoms per update in \dsnlitest. 52\% of updates target all valid possible atoms at once in a single update sentence.}

\label{fig:critical-atom-usage}
\end{figure}

\section{QUD Generation for Understanding Diversity}
\label{appendix:qud-diversity}

\input{tables/qud_generation}

In order to generate QUDs from critical atom propositions, we use \texttt{gpt-4o-2024-08-06} with a temperature of 0.01 and 15 exemplars of critical atom to QUD mappings along with Prompt~\ref{prompt:qud-gen}.
%
For example, the critical atom \textit{The dog is brown.} translates to the QUD \textit{What color is the dog?} Exemplars include critical atoms that do not use generics (e.g.``girl'') mapped to QUDs that use generics (``person''). This is to make sure that similar inferences are correctly mapped to the same QUD. Table~\ref{tab:quds-generated} shows examples of QUDs generated by the model.
%
We validate all generated QUDs by asking an external annotator if the QUD can be answered by the critical atom sentence, and find that in 92\% of cases, the QUD generated by \texttt{gpt4o} correctly captures the critical atom.

\begin{prompt}[title={Prompt \thetcbcounter: QUD Generation}, label=prompt:qud-gen]
\texttt{\colorbox{lightblue}{Prompt:} You are an expert linguist. Given a short \textbf{sentence}, generate a \textbf{question} that is answered by the sentence. Read the whole sentence carefully before generating the question.\\\textbf{Sentence}: {critical 
atom}\\\textbf{Question}:}
\end{prompt}


\section{Validation Instructions}
\label{appendix:instructions}
Since atomic sub-problems mirror the original validation task for defeasible inference, we use the instructions provided to annotators from \citet{rudinger-etal-2020-thinking} to ensure alignment.


