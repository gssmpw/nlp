\documentclass[11pt,letterpaper]{article}
\pdfoutput=1 % for arXiv
\usepackage[height=8in]{geometry}
\usepackage{amsmath, amssymb, amsthm, mathabx, mathrsfs, mathtools, bbm, dsfont}
\usepackage{graphicx, color}
\usepackage{array, multirow}
\usepackage[font=small,labelfont=bf]{caption} 
\usepackage{lipsum}
%\usepackage{amsfonts}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\usepackage{comment, todonotes}


\usepackage{tikz}

%i  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
%\else
%  \DeclareGraphicsExtensions{.eps}
%\fi

% FIX MARGINS FOR arXiv
\AtBeginDocument{%
	\setlength{\oddsidemargin}{\dimexpr(\paperwidth-\textwidth)/2-1in}%
	\setlength{\evensidemargin}{\oddsidemargin}%
	\setlength{\topmargin}{%
		\dimexpr(\paperheight-\textheight)/2-\headheight-\headsep-1in}%
}
\DeclareMathOperator*{\sinc}{sinc}
\DeclareMathOperator*{\argmin}{\arg\!\min}

% Lists
\usepackage[shortlabels]{enumitem}

% No orphans/widows
\clubpenalty10000
\widowpenalty10000

% Add a serial/Oxford comma by default.
\newcommand{\creflastconjunction}{, and~}

% Bibliography
\usepackage[sort]{natbib}
\def\bibfont{\small}
\setcitestyle{numbers,square,comma}
\setlength{\bibsep}{4pt plus 8pt}

% hyperlinks
\usepackage{hyperref}
\hypersetup{colorlinks=true,
			urlcolor=blue,
			linkcolor=blue,
			citecolor=blue,
			bookmarksdepth=paragraph}
\usepackage[nameinlink]{cleveref}

\usepackage[nameinlink]{cleveref}
\crefname{equation}{}{}
\crefname{section}{section}{sections}
\crefname{figure}{figure}{figures}
\crefname{table}{table}{tables}
\crefname{example}{example}{examples}
\crefname{proposition}{proposition}{propositions}
\Crefname{section}{Section}{Sections}
\Crefname{figure}{Figure}{Figures}
\Crefname{table}{Table}{Tables}
\Crefname{definition}{Definition}{Definitions}
\Crefname{theorem}{Theorem}{Theorems}
\Crefname{remark}{Remark}{Remarks}
\Crefname{example}{Example}{Examples}
\Crefname{proposition}{Proposition}{Propositions}
\numberwithin{equation}{section}

% Theorems
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{claim}[definition]{Claim}
%\theoremstyle{definition}

\newtheorem{remark}[definition]{Remark}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{exmp}[definition]{Example}



% Commands
\newcommand{\alert}[1]{{\color{red}[#1]}}

\newcommand{\C}{\mathbb{C}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}


% fix for the QED in equation    
%\newenvironment{equation}{\[}{\]\ignorespacesafterend}

%\title{Reconstruction of nearly space-frequency-limited functions from random samples with least squares and deep learning}

%Possible titles:

\title{Reconstruction of frequency-localized functions from pointwise samples via least squares and deep learning}
%Approximating bandlimited functions from random samples with least squares and deep learning}

\author{A. Martina Neuman$^1$, Andr\'es Felipe Lerma Pineda$^1$, \\
Jason J.\ Bramburger$^2$, Simone Brugiapaglia$^2$}
\date{\small $^1$Faculty of Mathematics, University of Vienna, Vienna, Austria\\
$^2$Department of Mathematics and Statistics, Concordia University, Montr\'eal, QC, Canada}


\begin{document}

\maketitle



\begin{abstract}
Recovering frequency-localized functions from pointwise data is a fundamental task in signal processing. We examine this problem from an approximation-theoretic perspective, focusing on least squares and deep learning-based methods. First, we establish a novel recovery theorem for least squares approximations using the Slepian basis from uniform random samples in low dimensions, explicitly tracking the dependence of the bandwidth on the sampling complexity. Building on these results, we then present a recovery guarantee for approximating bandlimited functions via deep learning from pointwise data. This result, framed as a practical existence theorem, provides conditions on the network architecture, training procedure, and data acquisition sufficient for accurate approximation. To complement our theoretical findings, we perform numerical comparisons between least squares and deep learning for approximating one- and two-dimensional functions. We conclude with a discussion of the theoretical limitations and the practical gaps between theory and implementation.
\end{abstract}

%\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The study of frequency-localized functions is fundamental to signal processing and sampling theory. For example, reconstructing bandlimited functions from pointwise samples is the object of the classical Nyquist-Shannon sampling theorem, which establishes that a bandlimited signal can be reconstructed if the sampling rate is twice its bandwidth \citep{jerri1977shannon}. Frequency-localized and bandlimited signals are ubiquitous in engineering and signal processing, with far-reaching applications from geophysical signal estimation \citep{simons2010slepian}, radar imaging \citep{chen2002highly}, radio transmission \citep{cabric2004implementation}, and compressed sensing \citep{tropp2009beyond}.

Motivated by these applications, we study the approximation of multi-dimensional frequency-localized functions from uniform random samples using both least squares regression and deep learning. We start by considering least squares reconstruction methods based on the \emph{Slepian basis}, or the \emph{prolate spheroidal wave functions (PSWFs)} \citep{osipov2013prolate}. 
%The Slepian basis, introduced in \citep{slepian1964prolate, slepian1965some, slepian1961prolate}, provides an orthogonal basis for frequency-localized and bandlimited functions and has attracted attention for its unique mathematical characteristics, including spectral and energy maximization properties. 
The Slepian basis, introduced by Slepian et al. at Bell Laboratories \citep{slepian1964prolate, slepian1965some, slepian1961prolate}, provides an orthogonal basis for frequency-localized, bandlimited functions and has attracted attention for its unique mathematical properties.
Its optimal concentration in frequency and space (or time, for univariate signals) \citep{moore2004prolate, simons2010slepian, walter2003sampling, xiao2001prolate} makes it particularly effective for approximating such functions. %It is a key asset for the effective approximation of these types of functions due to its optimal concentration in frequency and space (or time, for univariate signals) \citep{moore2004prolate, simons2010slepian, walter2003sampling, xiao2001prolate}. 
In contrast to Chebyshev- or Lagrange-based interpolation methods, the Slepian basis has a more uniform spatial resolution and supports longer stable time steps, thus offering a distinct advantage in applications such as weather prediction \citep{boyd2004prolate}. Research on the Slepian basis has continued to see important developments, as highlighted in the review \citep{wang2017review}. 

%Motivated by the utility of the Slepian basis, 
%In this work we use the Slepian basis to study the approximation of multi-dimensional smooth functions, with a targeted application to nearly space-frequency-limited functions from uniform random samples using least squares regression and deep learning. Specifically, we start by considering least squares reconstruction methods within a subspace spanned by an orthonormal system of Slepian functions.
%Our least squares regression analysis assumes known anisotropy \citep[Chapter~5]{adcock2022sparse}, where the multi-index set for expansion corresponds to a hyperbolic cross index set.
%Then, central to this analysis is the quantification of the Christoffel function of the resulting approximation subspace. This involves the challenge of determining the $L^{\infty}$-norm of multi-dimensional Slepian functions with low integer indices, whose linear combinations in turn provide a classical example of nearly space-frequency-limited functions.

Building on recent advances in deep learning, we further consider the approximation of smooth functions via \emph{neural networks (NNs)}. Here, linear expansions in the Slepian basis are replaced by nonlinear artificial NNs. This approach is motivated by the demonstrated capability of NNs to learn piecewise smooth functions \citep{eckle2019comparison,petersen2018optimal}, with established dimension-independent approximation rates in the Fourier \citep{barron1993universal} and Radon domains \citep{parhi2022near}. These results strongly suggest that deep learning can provide an efficient means of modeling frequency-localized functions in applications requiring high spatial resolution.

Our contribution to recovering frequency-localized functions via deep learning falls within the class of approximation theorems known as \emph{practical existence theorems (PETs)}, as conceived in \citep{adcock2021gap} and further developed in \citep{adcock2022deep}; see also \citep{adcock2024learning} for a review. 
Traditional universal approximation theorems establish the existence of NNs capable of approximating functions from a given target class with arbitrary accuracy, provided certain conditions on network architecture are met. By leveraging results from sampling and approximation theories, PETs deliver recovery guarantees for trained NNs, contingent on sufficient conditions related to the training data and optimization strategy. Recent developments in PETs include using NNs to approximate orthogonal polynomials for basis expansions in Hilbert spaces \citep{daws2019analysis, de2021approximation, opschoor2022exponential}, realizing Fourier basis functions through physics-informed NNs \citep{brugiapaglia2024physics}, and applying convolutional auto-encoders for reduced-order modeling \citep{franco2025practical}.

%Traditional universal approximation theorems establish the existence of NNs able to approximate functions from a given target class with arbitrary accuracy and with sufficient conditions on the NN architecture. Leveraging results from sampling and approximation theory, PETs provide recovery guarantees for trained NNs under sufficient conditions on the training data and optimization strategy. Recent developments in PETs involve using NNs to approximate orthogonal polynomials for basis expansions in Hilbert spaces \citep{daws2019analysis, de2021approximation, opschoor2022exponential}, the realization of Fourier basis functions via physics-informed NNs \citep{brugiapaglia2024physics}, and applications to convolutional autoencoders for reduced-order modeling \citep{franco2025practical}. 

In this work, we prove a PET for approximating frequency-localized, bandlimited functions, using an intermediate step that represents Slepian basis functions through Legendre polynomials, which can in turn be modeled by NNs \citep{opschoor2022exponential}.
We build on results from \citep{adcock2022deep}, leveraging previously established findings on least squares approximation sample complexities (see, e.g., \citep[Chapter~5]{adcock2022sparse} and references therein). This means that theoretically our NNs are assumed to be partially pre-trained. A closely related field of study to these ideas is \emph{transfer learning} \citep{pan2009survey}, where some layers of a previously trained model are fine-tuned on new data to solve a similar task. Specifically, this method of fine-tuning (or retraining) the last NN layer has been shown to improve the accuracy of neural networks when dealing with spurious features in the data \citep{kirichenko2022last,strombergenhancing}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main contributions}

Our main theoretical contributions in this paper can be found in Section~\ref{sec:main_results} and are summarized as follows. 

First, Theorem~\ref{thm:leastsquaresampling} provides a sufficient condition on the number of random samples required for least squares approximation methods to accurately recover frequency-localized functions defined over the hypercube $[-1,1]^d$, where $d = 1, 2, 3$, using an expansion in Slepian functions of bandwidth $\mathsf{w} \geq 1$. The linear expansion is supported on a \emph{hyperbolic cross}, a popular multi-index set used in multi-variate function approximation \citep{dung2018hyperbolic}. Specifically, we demonstrate that to recover the best approximation error in a hyperbolic cross of order $n$, it is sufficient to collect $n^{\gamma(\mathsf{w})}$ random samples (up to constants and a log factor), where $\gamma(\mathsf{w}) = \lceil\log_2(30\mathsf{w})\rceil$. 

%Second, Theorem~\ref{thm:ApproxSpan}, presented as a PET, establishes that, under the same sample complexity bound as in Theorem~\ref{thm:leastsquaresampling}, nearly space-frequency-limited functions, including Slepian functions, can be accurately approximated using deep neural networks. In particular, this finding demonstrates the existence of a class of trainable neural networks that can be fitted to the given data by solving a least squares-type optimization problem. A key auxiliary result in our proof is Proposition~\ref{prop:ApproxSlepian}, which provides a deep neural network construction of Slepian functions.

Second, Theorem~\ref{thm:ApproxSpan} shows that assuming the same sample complexity bound as in Theorem~\ref{thm:leastsquaresampling} it is possible to accurately approximate frequency-localized functions using deep neural networks (DNNs). In particular, this finding demonstrates the existence of a class of trainable neural networks that can be fitted to the given data by solving a least squares-type optimization problem. A key auxiliary result in our proof is Proposition~\ref{prop:ApproxSlepian}, which provides a DNN construction of Slepian functions.

Third, in Section~\ref{sec:numerics}, we run a numerical comparison between least squares and deep learning for approximating frequency-localized functions in dimensions one and two. %Our experiments show that both techniques are convergent as a function of the number of samples. 
%Moreover, they highlight the important role played by the dimension of the function's domain in the comparison. 
Our experiments show that both techniques exhibit convergence in approximation error as the number of samples increases while also underscoring the influence of the function's domain on their performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Paper outline}

After providing some essential background notions and illustrating the problem setting in Section~\ref{sec:background}, we state our theoretical guarantees for least squares and deep learning in Section~\ref{sec:main_results}. Our theory is complemented by several numerical illustrations, carried out in Section~\ref{sec:numerics}. Section~\ref{sec:proofs} contains the proofs of the theoretical results stated in Section~\ref{sec:main_results} and constitutes the main technical core of the paper. We conclude by discussing the limitations of our results and directions of future work in Section~\ref{s:conclusions}. Appendix~\ref{appx:basicnetworks} contains auxiliary results on neural networks needed for some of the proofs in Section~\ref{sec:proofs}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Setup} \label{sec:background}

In this section we provide the appropriate background material to present our results. Precisely, in Subsection~\ref{sec:bandlimited} we provide an overview of the relevant literature on bandlimited functions, in turn leading up to the definition of Slepian functions. Then, in Subsection~\ref{sec:problem} we describe the framework for the least squares approximation and formally define the class of NNs considered in this paper. Before proceeding, we note that throughout this paper we denote $\N_0:=\N\cup\{0\}$ as the set of non-negative integers. We will also make use of different vector norms. Specifically, for a vector $\vec{v} = (v_j)_{j=1}^n\in\mathbb{R}^n$, we define the $\ell^0$-norm and (Euclidean) $\ell^2$-norm of $\vec{v}$ as $\|\vec{v}\|_0 := \texttt{\#} \{ j: v_j \neq 0  \}$ and $\|\vec{v}\|_2 := (\sum_{j=1}^n v_j^2)^{1/2}$, respectively.
Moreover, by treating a matrix $A\in\mathbb{R}^{m\times n}$ as a vector, we write $\|A\|_2$ to denote its Frobenius norm.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bandlimited and Slepian functions} \label{sec:bandlimited}

Given a function $f:\R^{d}\to\mathbb{C}$, its Fourier transform is defined to be
\begin{equation*} 
    \hat{f}(\vec{v}):=\int_{\R^{d}} f(\vec{x})e^{-i\vec{v}\cdot\vec{x}}\,{\rm d}\vec{x}, \qquad \forall \vec{v}\in\R^d, 
\end{equation*}
with inverse transform 
\begin{equation} \label{inverseF}
    \widecheck{f}(\vec{x}):= \frac{1}{(2\pi)^d}\int_{\R^{d}} f(\vec{v})e^{i\vec{v}\cdot\vec{x}}\,{\rm d}\vec{v}, \qquad \forall \vec{x}\in\R^d,
\end{equation}
whenever either exists. 
The Fourier transform is known to be a unitary map on $L^2(\R^d)$.
Within this space lies a linear subspace of bandlimited functions, distinguished by their compactly supported Fourier transforms, as formalized in the following definition.

\begin{definition} Let $\mathsf{w}>0$. A function  $f\in L^2(\R^d)$ is said to be \emph{$\mathsf{w}$-bandlimited} if $\hat{f}=0$ for almost every $x\in\R^d\setminus [-\mathsf{w},\mathsf{w}]^d$. Equivalently,
\begin{equation} \label{eq:bandlimitedequiv}
    f(\vec{x}) = \frac{1}{(2\pi)^d} \int_{[-\mathsf{w},\mathsf{w}]^d} \hat{f}(\vec{v})e^{i\vec{v}\cdot\vec{x}}\,{\rm d}\vec{v}.
\end{equation}
\end{definition}

When $d = 1$ the space of $\mathsf{w}$-bandlimited functions is referred to as the \emph{Paley-Wiener space} \citep[Subsection~2.2]{zayed2018advances}, denoted by
\begin{equation*}
    {\rm PW}_\mathsf{w} := \{f\in L^2(\R): \mathrm{supp}(\hat{f})\subset [-\mathsf{w},\mathsf{w}]\}.
\end{equation*}
The uncertainty principle asserts that no nontrivial $f\in L^2(\R)$ can have both the sets $\{x: f(x)\neq 0\}$ and $\{v: \hat{f}(v)\neq 0\}$ confined to finite measures.  
%Consequently, a bandlimited function cannot be simultaneously \emph{timelimited}. i.e. it cannot be compactly supported in the physical space. 
%However, a natural resolution to this time-frequency problem has been proposed through the identification of \emph{essentially timelimited} functions, as outlined in the so-called ``Bell Labs theory''. 
Nevertheless, the ``Bell Labs theory'' presents a framework for obtaining functions that achieve optimal concentration in both the time and frequency domains, as we now describe in greater detail. 

\paragraph{The Slepian basis in one dimension.} 
For $T,\mathsf{w}>0$, we define the following two restriction operators on $L^2(\mathbb{R})$, using the notation in \eqref{inverseF}, as
\begin{equation*}
    P_\mathsf{w} f := (\hat{f}\mathbbm{1}_{[-\mathsf{w},\mathsf{w}]})\, \widecheck{} \quad \text{ and } \quad Q_{T} f := f\mathbbm{1}_{[-T,T]},
\end{equation*}
where $\mathbbm{1}_A$ denotes the indicator function for a set $A\subset\R$. It is easily verified that both $P_\mathsf{w}$ and $Q_T$ are compact self-adjoint operators on $L^2(\R)$, and moreover, the range of $P_\mathsf{w}$ is exactly the Payley-Wiener space, i.e. $P_\mathsf{w}L^2(\R) = {\rm PW}_{\mathsf{w}}$. 
Since no nontrivial $f \in L^2(\R)$ remains invariant under both $P_\mathsf{w}$ and $Q_T$, an alternative approach is to assess the extent to which the energy of a function in ${\rm PW}_{\mathsf{w}}$ is concentrated on $[-T,T]$.
Particularly, we consider the operator $P_\mathsf{w}Q_T$, that is,
\begin{equation*}
    P_\mathsf{w}Q_Tf(x) = \frac{1}{2\pi}\int_{-\mathsf{w}}^{\mathsf{w}} \int_{-T}^T f(t)e^{-itv}e^{ivx}\,{\rm d}t{\rm d}v, \qquad \forall x\in\R.
\end{equation*}
A quick calculation shows that $P_\mathsf{w}Q_Tf = P_{T\mathsf{w}}Q_1 f(T\cdot)$ for all $f \in L^2(\R)$, and hence without loss of generality we may set $T=1$ and simply define $\mathcal{Q}_{\mathsf{w}} := P_\mathsf{w}Q_1$. From a straightforward calculation, we can equivalently write
\begin{equation} \label{eqdef:Qw}
     \mathcal{Q}_{\mathsf{w}}f(x) = P_\mathsf{w}Q_1f(x)= \int_{-1}^1 f(t)\,\frac{\sin\mathsf{w}(x-t)}{\pi(x-t)}\,{\rm d}t, \qquad \forall x\in\R.
\end{equation}
We are particularly interested in the eigenfunctions of $\mathcal{Q}_{\mathsf{w}}$, i.e. the solutions to the homogeneous Fredholm integral equation of the second kind, $\mathcal{Q}_{\mathsf{w}} f = \mu f$, on ${\rm PW}_{\mathsf{w}}$.
It follows that the corresponding eigenvalue roughly indicates ``how much'' of an eigenfunction is fixed by $\mathcal{Q}_{\mathsf{w}}$.
%Since no functions are fixed by $\mathcal{Q}_{\mathsf{w}}$, alternatively one can seek to identify eigenfunctions, for which the associated eigenvalues roughly represent ``how much'' of the function is retained by the operator $\mathcal{Q}_{\mathsf{w}}$. 
The compactness of $P_{\mathsf{w}}$ and $Q_1$ ensures that the spectrum of $\mathcal{Q}_{\mathsf{w}}$ is countable and accumulates at $0$. We denote these eigenvalues to be
\begin{equation} \label{eq:muj}
    1> \mu_{\mathsf{w},0} >\mu_{\mathsf{w},1}>\dots> 0,
\end{equation}
where $\lim_{j\to\infty} \mu_{ \mathsf{w}, j} = 0$ \citep[Subsection~3.2.1]{osipov2013prolate}. 
An orthogonal basis for the space $L^2_{\bf u}([-1,1])$, which is the $L^2$-Lebesgue space on $[-1,1]$ with norm calibrated to the uniform measure ${\bf u}$ on $[-1,1]$, can be generated using the eigenfunctions of $\mathcal{Q}_{\mathsf{w}}$.
Specifically, for $j \geq 0$, let $\varphi_{\mathsf{w},j}\in{\rm PW}_{\mathsf{w}}\cap L_{{\bf u}}^2([-1,1])$ to be the \emph{normalized} eigenfunction of $\mu_{\mathsf{w},j}$, i.e. $\mathcal{Q}_{\mathsf{w}}\varphi_{\mathsf{w},j} = \mu_{\mathsf{w},j}\varphi_{\mathsf{w},j}$ and
\begin{equation*} 
    \|\varphi_{\mathsf{w},j}\|^2_{L^2_{{\bf u}}[-1,1]} = \int_{-1}^1 |\varphi_{\mathsf{w},j}(x)|^2 \, {\rm d}{\bf u}(x) = 1.
\end{equation*}
We refer to the set $\{\varphi_{\mathsf{w},j}\}_{j\in\N_0}$ of normalized eigenfunctions, the Slepian basis functions (the prolate spheroidal wave functions), illustrated in Figure~\ref{fig:PSAF} for $\mathsf{w}=16$.
From \eqref{eq:muj}, it is apparent that the function in ${\rm PW}_{\mathsf{w}}$ that minimizes the energy loss most effectively when first time-limited to $[-1,1]$ and subsequently bandlimited to $[-\mathsf{w},\mathsf{w}]$, is $\varphi_{\mathsf{w},0}$. 
Thus, heuristically, a function is interpreted as being $(1,\mathsf{w})$-time-frequency localized if it is a finite linear combination of eigenfunctions $\varphi_{\mathsf{w},j}$ of $\mathcal{Q}_{\mathsf{w}}$ corresponding to eigenvalues $\mu_{\mathsf{w},j}$ close to $1$, or equivalently, those with low indices $j$.

\begin{figure}[t]
    \centering
    \includegraphics[width = 0.49\textwidth]{psaf_plot2.pdf} 
    \includegraphics[width = 0.49\textwidth]{psaf_plot.pdf}
    \caption{Plot of Slepian functions (PSWFs) $\varphi_{16,j}$ with $\mathsf{w} = 16$ for odd (left) and even (right) indices $j = 0,1,2,3,4,5,6,7$.} 
    \label{fig:PSAF} 
\end{figure}

An alternative way to arrive at the Slepian basis comes in the form of solutions to a Sturm-Louville equation. Precisely, consider the second-order boundary value problem
\begin{equation} \label{eqdef:Lw}
    \mathcal{L}_{\mathsf{w}}f(x) := -\frac{d}{dx}\bigg((1-x^2)\frac{df}{dx}(x)\bigg) + \mathsf{w}^2x^2f(x), \qquad \forall x\in (-1,1).
\end{equation}
The normalized eigenfunctions of this problem are exactly the PSWFs, $\mathcal{L}_{\mathsf{w}}\varphi_{\mathsf{w},j} = \chi_{\mathsf{w},j}\varphi_{\mathsf{w},j}$, but now with the eigenvalues satisfy the ordering
\begin{equation} \label{eq:spectrumchi}
    0< \chi_{\mathsf{w},0} < \chi_{\mathsf{w},1} <\cdots
\end{equation}
with $\lim_{j\to\infty} \chi_{\mathsf{w},j} = \infty$. The progression of the spectrum \eqref{eq:spectrumchi} will play a key role in our subsequent analysis of the $L^{\infty}$-norm of a Slepian basis function on $[-1,1]$, a prerequisite for the proofs of the main results.



\paragraph{The Slepian basis in $d$ dimensions.}
To produce a multi-dimensional Slepian basis, we will simply tensorize the one-dimensional basis. That is, for $d \geq 2$ we construct a $d$-dimensional Slepian basis function as the $d$-tensorization of its one-dimensional counterparts
\begin{equation} \label{tensor}
    \varphi_{\mathsf{w},\vec{\nu}} := \varphi_{\mathsf{w},\nu_1} \otimes \cdots \otimes \varphi_{\mathsf{w},\nu_d},
\end{equation}
for a multi-index $\vec{\nu}=(\nu_1,\dots,\nu_d)\in\N^d_0$. Continuing to denote ${\bf u}$ to be the uniform measure on $[-1,1]^d$, it follows from the preceding discussion that $\{\varphi_{\mathsf{w},\vec{\nu}}\}_{\vec{\nu}\in\N_0^d}$ forms an orthonormal basis of $L^2_{{\bf u}}([-1,1]^d)$, which we simply refer to as the Slepian basis in $d$ dimensions.

%\footnote{In this paper, a $d$-dimensional function is normalized if its $L^2_{\bf u}([-1,1]^d)$-norm equals to one.} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem setting} \label{sec:problem}

To further initialize the work in this paper, in this subsection we present the main elements of our problem setting. To remain consistent with the relevant literature, the notation adopted here is analogous to that of \citep[Section~5]{adcock2022sparse}. %We start by letting $m\in\mathbb{N}$ and $\mathcal{C}([-1,1]^d)$ denote the space of continuous functions over $[-1,1]^d$. 
Letting $m\in\mathbb{N}$, we seek to approximate a continuous function $f\in \mathcal{C}([-1,1]^d)$ %$f\in L_{{\bf u}}^2([-1,1]^d)\cap\mathcal{C}([-1,1]^d)$ 
from noisy samples
\begin{equation*}
    f(\vec{y}_1) + \eta_1,\  \dots,\ f(\vec{y}_m) + \eta_m,
\end{equation*}
where $\vec{y}_1,\dots, \vec{y}_m \in [-1,1]^d$ are random sample points drawn independently from the uniform measure ${\bf u}$. Note that the assumption $f \in \mathcal{C}([-1,1]^d)$ is only made for pointwise evaluations of $f$ to be well-defined, but it can be weakened to, for example, having $f$ be piecewise continuous. Defining the noise vector as $\vec{e}:=\frac{1}{\sqrt{m}}(\eta_j)_{j=1}^m$, we assume throughout a %bounded and 
deterministic noise model, meaning that the $\eta_j$'s are arbitrary complex numbers.

First, we approximate $f$ using a least squares fit estimator from a hypothesis set composed of linear combinations of Slepian basis functions. %frequency-localized functions formed by a linear span of Slepian basis functions. 
Let $\Lambda\subset\N_0^d$ be a finite index set, whose cardinality is denoted as $\texttt{\#}\Lambda$. Then, for a fixed $\mathsf{w}>0$, we consider the orthonormal Slepian system $\{\varphi_{\mathsf{w},\vec{\nu}}\}_{\vec{\nu}\in\Lambda}$ whose linear span $\mathcal{S}_{\Lambda}$ constitutes a $(\texttt{\#}\Lambda)$-dimensional subspace in $L_{{\bf u}}^2([-1,1]^d)$.
Supposing $m\geq\texttt{\#}\Lambda$, we approximate $f$ through $f^{\natural}$, the least squares fit among $\mathcal{S}_{\Lambda}$, given by
\begin{equation} \label{leastsquaresproblem}
    f^{\natural} \in \argmin_{g\in\mathcal{S}_{\Lambda}} 
    \frac{1}{m} \sum_{j=1}^m |g(\vec{y}_j) - f(\vec{y}_j) - \eta_j|^2.
\end{equation}
The index set $\Lambda$ will be chosen strategically for our analysis, specifically utilizing the hyperbolic cross, as defined below.

\begin{definition} \label{def:HC}
Let $d,n\in\N$. The $d$-dimensional hyperbolic cross index set of order $n$ is defined to be
\begin{equation*}
    \Lambda^{\rm HC}_{n-1} := \bigg\{\vec{\nu}\in\N_0^{d}: \prod_{k=1}^{d}(\nu_k+1)\leq n\bigg\}.        
\end{equation*}
\end{definition}

To maintain a compact presentation, we omit the dimension $d$ in the notation $\Lambda^{\rm HC}_{n-1}$, allowing it to be inferred from the context. 
Continuing, we highlight two key properties of a hyperbolic cross index set. 
From \citep[Lemma~5.15]{adcock2022sparse}, we have the following \emph{slicing property}:
\begin{equation} \label{slicing}
    \Lambda^{\rm HC}_{n-1} = \bigsqcup_{k=0}^{n-1} \{k\}\times \Lambda_k, 
\end{equation}
where $\bigsqcup$ denotes the disjoint union, and $\Lambda_k := \{(\nu_2,\dots,\nu_d): (k,\nu_2,\dots,\nu_d) \in\Lambda\}$. Moreover, for $0\leq k\leq j\leq n-1$, we further have the \emph{monotonicity property}:
\begin{equation} \label{nesting}
    \texttt{\#} \Lambda_j \leq \texttt{\#}\Lambda_k.
\end{equation}
%Together the slicing and monotonicity property make the hyperbolic cross index set optimal for our coming analysis.

Second, we establish the existence of a class of NNs capable of Slepian basis functions. %approximating bandlimited functions $f\in L_{{\bf u}}^2([-1,1]^d)\cap\mathcal{C}([-1,1]^d)$. 
For this purpose, we introduce the formal definition of an NN, as follows.

\begin{definition}[{\citep[Definition~2.1]{PetV2018OptApproxReLU}}] \label{def:NeuralNetworks}
Let $d, L \in \N$, and $d_1, \dots, d_{L} \in \N$, and $\varrho: \R \to \R$ be given, referred to as an \emph{activation function}. A \emph{neural network (NN)} $\Phi$ with input dimension $d$, output dimension $d_L$, and comprised of $L$ layers is a sequence of matrix-vector pairs
\begin{equation*}
    \Phi = \big((A_1,b_1), \dots, (A_L, b_L)\big), 
\end{equation*}
where $A_l \in \R^{d_l\times d_{l-1}}$ and $b_l \in \R^{d_l}$, for $l =1,\dots,L$, with $d_0 := d$.

Moreover, the \emph{realization} of $\Phi$ is a function ${\rm R}(\Phi): \R^d \to \R^{d_L}$, mapping an input $x\in\R^d$ to an output ${\rm R}(\Phi)(x)\in\R^{d_L}$, defined by the  sequence
\begin{align} \label{eq:NetworkScheme}
    \nonumber x_0 &\equiv x, \\
    x_{l} &:= \varrho(A_{l} \, x_{l-1} + b_l) \quad \text{ for }\quad l = 1, \dots, L-1,\\
    \nonumber x_L &:= A_{L} \, x_{L-1} + b_{L} \equiv {\rm R}(\Phi)(x).
\end{align}
In \eqref{eq:NetworkScheme}, $\varrho$ acts component-wise on vector inputs. We refer to $L(\Phi):= L$ as the \emph{depth} %or the number of \emph{layers} 
and $W(\Phi) := \sum_{j=1}^L \| A_j\|_{0} + \| b_j \|_{0}$ as the \emph{size} of $\Phi$. 
\end{definition}

In this paper, we fix the activation function to be the ReLU function $\varrho(x) = \max \{x,0\}$. The resulting neural networks are referred to as ReLU NNs.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Main results}\label{sec:main_results}

% We present the two main results of our paper: Theorem~\ref{thm:leastsquaresampling}, a least squares approximation result, in Subsection~\ref{subsec:LSresult}, and Theorem~\ref{thm:ApproxSpan}, a PET, in Subsection~\ref{subsec:PETresult}. 
% Finally, we conclude this section with a discussion in Subsection~\ref{ss:more_LS_results}, where we introduce two complementary least squares results.

In this section we provide the two main results of our paper: the least squares approximation result (Theorem~\ref{thm:leastsquaresampling}) and a PET (Theorem~\ref{thm:ApproxSpan}). In what follows, a universal constant %$C>0$ or $c>0$ 
refers to a constant that is independent of any other parameters or hyperparameters of the problem. Additionally, to streamline the presentation, for a bandwidth $\mathsf{w}\geq 1$ we define
\begin{equation} \label{eqdef:gammaw}
    \gamma(\mathsf{w}):= \lceil \log_2(30\mathsf{w}) \rceil,
\end{equation}
to be used in the ensuing statements. We begin in Subsection~\ref{subsec:LSresult} with our least squares approximation result. Then, our PET is presented in Subsection~\ref{subsec:LSresult}. We conclude in Subsection~\ref{ss:more_LS_results} with two additional least squares corollaries.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Least squares approximation} \label{subsec:LSresult}

Our first result provides an error bound for the least squares problem \eqref{leastsquaresproblem} of approximating a continuous function %in $\mathcal{C}([-1,1]^d)$ 
using Slepian basis functions %of bandwidth $\mathsf{w}$ 
indexed over a hyperbolic cross.
A proof can be found in Subsection~\ref{sec:leastsquaresproof}.

\begin{theorem}[Recovery guarantee for least squares]  \label{thm:leastsquaresampling}
Let $\beta,\delta\in (0,1)$, $d=1,2,3$, and $\Lambda=\Lambda^{\rm HC}_{n-1}$ be the $d$-dimensional hyperbolic cross of order $n$, where $n\in\mathbb{N}$ if $d=1,2$, and $n\geq 26$ if $d=3$.
For $\mathsf{w}\geq 1$, let $\mathcal{S}_{\Lambda}$ be the linear span of $\{\varphi_{\mathsf{w},\vec{\nu}}\}_{\vec{\nu}\in\Lambda}$ and let $\vec{y}_1, \dots, \vec{y}_m$ be random sample points drawn independently from the uniform measure $\bf u$ on $[-1,1]^d$. 
If $m \geq c_{\delta}\kappa^{\star}  \log(\texttt{\#}\Lambda/\beta)$, where 
\begin{equation} \label{eq:choices}
    \kappa^{\star} := (2\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})} \quad\text{ and }\quad
    c_{\delta} := ((1-\delta)\log(1-\delta)+\delta)^{-1},
\end{equation}
then, with probability at least $1-\beta$, for every $f \in \mathcal{C}([-1,1]^d)$ %$f \in L_{ {\bf u}}^2([-1,1]^d) \cap \mathcal{C}([-1,1]^d)$ 
and every noise vector $\vec{e} = \frac{1}{\sqrt{m}} (\eta_j)_{j=1}^m \in \C^m$, the least squares problem \eqref{leastsquaresproblem} has a unique solution $f^{\natural}\in\mathcal{S}_{\Lambda}$ satisfying 
\begin{equation}\label{eq:AcFinal}
    \| f - f^{\natural} \|_{L_{ {\bf u}}^2([-1,1]^d)} 
    \leq 
    \left(1+ \dfrac{1}{\sqrt{1-\delta}} \right)\inf_{g \in \mathcal{S}_{\Lambda}} \| f - g \|_{L^{\infty}([-1,1]^d)} 
    + \dfrac{1}{\sqrt{1-\delta}} \| \vec{e} \|_2.
\end{equation}
\end{theorem}

%Theorem~\ref{thm:leastsquaresampling} shows that it is possible to accurately recover functions $f$ that are well-approximated by the Slepian basis $\{\varphi_{\mathsf{w},\vec{\nu}}\}_{\vec{\nu}\in\Lambda}$ (hence, approximately $\mathsf{w}$-bandlimited) using a number of pointwise samples that scales proportionally to $\kappa^\star = (2\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})}$, up to a constant and a log factor. 

Theorem~\ref{thm:leastsquaresampling} concludes that frequency-localized functions that are well-approximated by the $\mathsf{w}$-bandlimited Slepian basis can be accurately recovered from pointwise samples. The number of samples required scales as $\kappa^\star = (2\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})}$, up to constants and a logarithmic factor.
The corresponding $L^2_{\bf u}$-error is dominated by the $L^\infty_{\bf u}$ distance of $f$ from the given Slepian basis and the $\ell^2$-norm of the noise vector, up to multiplicative constants. 
In particular, $\mathsf{w}$-bandlimited functions that are exactly linear combinations of elements in $\{\varphi_{\mathsf{w},\vec{\nu}}\}_{\vec{\nu}\in\Lambda}$ can be recovered with accuracy proportional to the noise level $\|\vec{e}\|_2$. 
The error estimate in Theorem~\ref{thm:leastsquaresampling} provides an example of a \emph{uniform recovery guarantee}, meaning that a single draw of sample points is sufficient to recover any function $f \in \mathcal{C}([-1,1]^d)$ %$f \in L_{ {\bf u}}^2([-1,1]^d) \cap \mathcal{C}([-1,1]^d)$ 
with high probability up to the bound given in  \eqref{eq:AcFinal}. Further recovery guarantees for least squares are presented in Subsection~\ref{ss:more_LS_results}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Practical existence theorem} \label{subsec:PETresult}

Using the definition of $\gamma(\mathsf{w})$ in \eqref{eqdef:gammaw} for $\mathsf{w}\geq 1$, for $n\in\mathbb{N}$ we further define
\begin{equation} \label{eq:B}
    B(d,n) := 
    \begin{cases}
        n^{\gamma(\mathsf{w})} + 1 &\text{ if } d=1, \\
        3n^{2\gamma(\mathsf{w})} + 4n^{\gamma(\mathsf{w})} +2 &\text{ if } d=2, \\
        7n^{3\gamma(\mathsf{w})} + 12n^{2\gamma(\mathsf{w})} + 8n^{\gamma(\mathsf{w})} +3 &\text{ if } d=3,
    \end{cases}
\end{equation}
and 
\begin{equation} \label{eq:M}
    M(d,n) := 
    \begin{cases}
        1 &\text{ if } d=1, \\
        2n^{\gamma(\mathsf{w})} + 1 &\text{ if } d=2, \\
        4n^{2\gamma(\mathsf{w})} + 4n^{\gamma(\mathsf{w})} +2 &\text{ if } d=3.
    \end{cases}
\end{equation}

Our second result presents a PET for the least squares approximation of continuous functions, %in $L_{\bf u}^2([-1,1]^d) \cap \mathcal{C}([-1,1]^d)$, 
with a complexity rate derived from Theorem~\ref{thm:leastsquaresampling}.
A proof is given in Subsection~\ref{sec:ProofTheo3}. 


\begin{theorem}[Practical existence theorem]\label{thm:ApproxSpan} 
Let $\varepsilon,\delta,\beta \in (0,1)$, $d=1,2,3$, and $\Lambda=\Lambda^{\rm HC}_{n-1}$ be the $d$-dimensional hyperbolic cross of order $n$, where $n\in\mathbb{N}$ if $d=1,2$, and $n\geq 26$ if $d=3$. Let $\mathsf{w}\geq 1$ and $B(d,n)$, $M(d,n)$ be as in \eqref{eq:B}, \eqref{eq:M}, respectively.
Let $\vec{y}_1, \dots, \vec{y}_m$ be independent sample points drawn from the uniform measure $\bf u$ on $[-1,1]^d$, with $m \geq c_{\delta}\kappa^{\star} \log(\texttt{\#}\Lambda/\beta)$, where $\kappa^{\star}$, $c_{\delta}$ are given in \eqref{eq:choices}.
Then for every $\varepsilon>0$ sufficiently small satisfying
\begin{equation} \label{epscondition}
    \varepsilon \leq \frac{\sqrt{1-\delta}}{2\sqrt{\texttt{\#}\Lambda} B(d,n)},
\end{equation}
there exists a class of NNs, denoted $\mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}$, such that the following holds with probability at least $1-\beta$. For every $f \in \mathcal{C}([-1,1]^d)$ %$f \in L_{ {\bf u}}^2([-1,1]^d) \cap \mathcal{C}([-1,1]^d)$ 
and every noise vector $\vec{e}=\frac{1}{\sqrt{m}}(\eta_j)_{j=1}^m \in \C^m$, the problem 
\begin{equation} \label{leastsquaresproblem2}
    \Psi^{\natural} \in \argmin_{\Psi \in \mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}} 
    \frac{1}{m} \sum_{j=1}^m |f(\vec{y}_j) - {\rm R}(\Psi)(\vec{y}_j) - \eta_j|^2
\end{equation}
has a unique solution $\Psi^{\natural}\in\mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}$ satisfying
\begin{align} \label{eq:PETFinal}
    \nonumber &\| f - {\rm R}(\Psi^{\natural}) \|_{L_{ {\bf u}}^2([-1,1]^d)} \\
    &\leq 
    \Big(1+ \dfrac{2}{\sqrt{1-\delta}} \Big) \Big(\inf_{g \in \mathcal{S}_{\Lambda}} \|f-g\|_{L^{\infty}([-1,1]^d)} + \sqrt{\texttt{\#}\Lambda} B(d,n)\|g\|_{L^2_{\bf u}([-1,1]^d)}\varepsilon \Big) + \dfrac{2\|\vec{e}\|_2}{\sqrt{1-\delta}}.
\end{align}
Moreover, it is guaranteed that
\begin{align*}   
    L(\Psi^{\natural}) 
    &
    \leq C \big((1+\log_2N_{\star})(N_{\star}+\log_2(N_{\star}/\varepsilon)) + (1+ \log_2 (M(d,n)/\varepsilon)\big),\\
    W(\Psi^{\natural}) 
    & 
    \leq C \texttt{\#}\Lambda\big((N_{\star}^2 + N_{\star})(N_{\star} + \log_2(N_{\star}/\varepsilon)) + (1+ \log_2 (M(d,n)/\varepsilon)\big),
\end{align*}
for a universal constant $C>0$ and $N_{\star}=N_{\star}(\Lambda,\mathsf{w},\varepsilon)\in\mathbb{N}$. 
\end{theorem}

Similar to Theorem~\ref{thm:leastsquaresampling}, Theorem~\ref{thm:ApproxSpan} states that functions well-approximated by the Slepian system $\{\varphi_{\mathsf{w},\vec{\nu}}\}_{\vec{\nu}\in\Lambda}$ can be accurately recovered via deep learning using a training set of pointwise samples with size proportional to $\kappa^\star = (2\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})}$ (up a constant and a log factor).  Theorem~\ref{thm:ApproxSpan} assumes training to be performed by solving a least squares-type optimization problem \eqref{leastsquaresproblem2} that closely resembles \eqref{leastsquaresproblem}, operating over a class of NNs $\mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}$ with specific architectures, determined by $\Lambda$, $\mathsf{w}$, $\varepsilon$. The class $\mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}$ is defined in \eqref{eqdef:NLambda} and is composed by NNs whose first-to-second-last layers are explicitly constructed to emulate the Slepian basis and whose last layer is made of trainable weights, optimized through \eqref{leastsquaresproblem2}. Theorem~\ref{thm:ApproxSpan} provides explicit bounds on the depth $L(\Psi^{\natural})$ and width $W(\Psi^{\natural})$ of NNs in the class $\mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}$, while the dependence of $N_{\star}$ on $\Lambda$, $\mathsf{w}$, $\varepsilon$ is detailed later in the proof of Theorem~\ref{thm:ApproxSpan}, particularly in \eqref{eqdef:Nstar}. The error bound \eqref{eq:PETFinal} of Theorem~\ref{thm:ApproxSpan} is similar to \eqref{eq:AcFinal} in Theorem~\ref{thm:leastsquaresampling}, so that besides minor changes in the constant factors, the main difference between \eqref{eq:PETFinal} and \eqref{eq:AcFinal} is the presence of an extra term in the best approximation error. 
The impact of this extra term is mild, though, as it scales linearly with an auxiliary parameter $\varepsilon \in (0,1)$ that can be made arbitrarily close to $0$ at the price of increasing the architecture bounds proportionally to $\log(1/\varepsilon)$. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion on further recovery guarantees for least squares} \label{ss:more_LS_results}

We now turn to two additional recovery guarantees for least squares approximation. The reason we present these results is that the recovery guarantee in Theorem~\ref{thm:leastsquaresampling} comes with the drawback of bounding the error in terms of the best approximation of $f$ in the $L^{\infty}$-norm, which is a stronger and more restrictive norm compared to the $L_{ {\bf u}}^{2}$-norm used to measure the recovery error. Thus, here we present two results aimed at addressing this shortcoming. They can be seen as direct corollaries of Theorem~\ref{thm:LS} and Proposition~\ref{prop:Theta}, while their proofs are omitted as they are identical to other published results, which we will provide precise pointers to as we proceed through this subsection. 

The first result is an $L_{ {\bf u}}^{2}$-$L_{ {\bf u}}^{2}$ uniform recovery guarantee in probability. It is stated as follows.

\begin{corollary}[$L_{ {\bf u}}^{2}$-$L_{ {\bf u}}^{2}$ recovery guarantee in probability for least squares]\label{cor:least_squares_L2L2_prob}
    In the same setting of Theorem~\ref{thm:leastsquaresampling}, let $\beta, \delta \in (0,1)$ and $f \in \mathcal{C}([-1,1]^d)$. %$f \in L_{ {\bf u}}^2([-1,1]^d) \cap \mathcal{C}([-1,1]^d)$. 
    Assume that the number of samples $m$ satisfies the condition 
    $m \geq c\kappa^{\star}  \log(2\texttt{\#}\Lambda/\beta),$
    where $c>0$ is a universal constant, and $\kappa^{\star}$ is defined as in Theorem~\ref{thm:leastsquaresampling}. Then for every noise vector $e\in \mathbb{C}^m$, with probability at least $1-\beta$, the least squares solution $f^{\natural}$ obtained in \eqref{leastsquaresproblem} satisfies
    \begin{equation*}
        \| f - f^{\natural} \|_{L_{ {\bf u}}^2([-1,1]^d)} 
        \leq 
        \left(1+ \sqrt{\dfrac{2}{\beta}}\dfrac{1}{\sqrt{1-\delta}} \right)\inf_{g \in \mathcal{S}_{\Lambda}} \| f - g \|_{L_{\bf u}^{2}([-1,1]^d)} 
        + \dfrac{1}{\sqrt{1-\delta}} \| \vec{e} \|_2.
    \end{equation*}
\end{corollary}

This result can be obtained from Theorem~\ref{thm:LS} and Proposition~\ref{prop:Theta} by applying the same argument as in \citep[Corollary 5.10]{adcock2022sparse}. The error bound now contains the best approximation error of $f$ with respect to the $L^2_{\bf u}$-norm as desired. However, as opposed to Theorem~\ref{thm:leastsquaresampling}, Corollary~\ref{cor:least_squares_L2L2_prob} is a \emph{nonuniform recovery guarantee}, i.e., the error bound holds with probability $1-\beta$ for a fixed $f$. %, and not for all $f$. 
In other words, one single draw of sample points is not sufficient for the validity of the recovery guarantee for all $f \in \mathcal{C}([-1,1]^d)$ simultaneously. Another limitation of Corollary~\ref{cor:least_squares_L2L2_prob} is the poor scaling of the error bound with respect to the failure probability $\beta$. The following result addresses these limitations by providing a recovery guarantee in expectation.

\begin{corollary}[$L_{ {\bf u}}^{2}$-$L_{ {\bf u}}^{2}$ recovery guarantee in expectation for least squares]
\label{cor:least_squares_L2L2_exp}
    Let $\delta, \beta \in (0,1)$ and $f \in \mathcal{C}([-1,1]^d)$ %$f \in L_{ {\bf u}}^2([-1,1]^d) \cap \mathcal{C}([-1,1]^d)$ 
    be such that $\| f \|_{L_{ {\bf u}}^2([-1,1]^d)} \leq L$ for some $L>0$. Define the operator $\tau_L : L_{ {\bf u}}^2([-1,1]^d) \to L_{ {\bf u}}^2([-1,1]^d)$ as
    \begin{equation*}
        \tau_L(g) := \min \bigg \{1, \dfrac{L}{ \| g \|_{L_{ {\bf u}}^2([-1,1]^d)}} \bigg \}g.
    \end{equation*}
    Then, under the same assumptions as Theorem~\ref{thm:leastsquaresampling}, if $f^{\natural}$ is the least squares solution of \eqref{leastsquaresproblem}, for every noise vector $e \in \mathbb{C}^m$ we have 
    \begin{equation*}
        \mathbb{E}
        \| f - \tau_L(f^{\natural}) \|^2_{L_{ {\bf u}}^2([-1,1]^d)}
        \leq 
        \left(\dfrac{3- \delta}{1-\delta} \right)\inf_{g \in \mathcal{S}_{\Lambda}} \| f - g \|_{L^2_{\bf u}([-1,1]^d)}  
        + \dfrac{2}{1-\delta} \| \vec{e} \|_2^2 + 4 L^2 \beta.
    \end{equation*} 
\end{corollary}

This result can obtained from Theorem~\ref{thm:LS} and Proposition~\ref{prop:Theta} by applying the same argument as in \citep[Corollary 5.11]{adcock2022sparse} (originally proposed in \citep[Theorem 2]{cohen2013stability}). Notably, the poor scaling with respect to $\beta$ in the error bound of Corollary~\ref{cor:least_squares_L2L2_prob} is not there anymore. However, this comes at the price of having a recovery guarantee in expectation, as opposed to high probability, while also requiring a priori knowledge of a constant $L$ such that $\| f \|_{L_{ {\bf u}}^2([-1,1]^d)} \leq L$. 

We conclude with a brief remark that the proof of Theorem~\ref{thm:ApproxSpan} can also be modified to incorporate $L^2_{\bf u}$-$L^2_{\bf u}$ recovery guarantees by employing Corollary~\ref{cor:least_squares_L2L2_prob} or \ref{cor:least_squares_L2L2_exp} in place of Theorem~\ref{thm:leastsquaresampling}. 
For the sake of conciseness, we omit the statements of the resulting corollaries.

\begin{comment}
\begin{remark} %[Alternatives to Theorem~\ref{thm:ApproxSpan}] 
The proof of Theorem~\ref{thm:ApproxSpan} can be modified to incorporate $L^2_{\bf u}$-$L^2_{\bf u}$ recovery guarantees by employing Corollary~\ref{cor:least_squares_L2L2_prob} or \ref{cor:least_squares_L2L2_exp} in place of Theorem~\ref{thm:leastsquaresampling}. This would lead to analogous error bounds, up to a minor change in the constants and the presence of an additional term $\sqrt{\texttt{\#}\Lambda} B(d,n)\|g\|_{L^2_{\bf u}([-1,1])}\varepsilon$ in the best approximation error, like in \eqref{eq:PETFinal}. For the sake of conciseness, we omit the statement of the resulting two corollaries.
\end{remark}
\end{comment}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical experiments}\label{sec:numerics}

In this section, we provide numerical examples illustrating the performance of least squares approximation and deep learning in reconstructing frequency-localized functions from pointwise samples. 
For simplicity, we refer to these methods as ``Least Squares'' and ``Deep Learning'' throughout. 
We describe our numerical setup and discuss the results obtained in dimensions one and two.
The Python code necessary to reproduce our experiments can be found in the GitHub repository: \url{https://github.com/andreslerma01/PSWF-}.

\paragraph{Benchmark functions.}
We assess the performance of Least Squares and Deep Learning for the reconstruction of the one-dimensional function
\[
f_1(x) := \cos(10x)e^{-\pi x^2}, \qquad \forall x\in [-1,1],
\]
and the two-dimensional function 
\[
f_2(x,y) := \cos(0.2x)\cos(0.2y)e^{-\pi (x^2 + y^2)}, \qquad \forall (x,y)\in [-1,1]^2.
\]
%The one-dimensional function $f_1$ is composed of the product of a trigonometric and a Gaussian-type function. This test function was considered in \cite{moore2004prolate} and provides a simple example in which the Slepian basis can achieve better accuracy than the standard Fourier basis. 
The one-dimensional function $f_1$, defined as the product of a trigonometric function and a Gaussian, was considered in \cite{moore2004prolate} as a simple example in which the Slepian basis achieves better approximation accuracy than the standard Fourier basis.
The two-dimensional function $f_2$ generalizes the one-dimensional function $f_1$ %to two dimensions in a separable way via the tensor product 
while reducing the cosine frequency from 10 to 0.2. Both functions are frequency-localized since most of their energy is concentrated within a compact interval in the frequency domain. 

\paragraph{Training set, test set, and error metric.}
For a given target function $f$ that we seek to approximate using either Least Squares or Deep Learning, we generate data sets of the form $\{ (x_j, f(x_j) \}_{j=1}^m$ with $x_j$ drawn independently from the uniform distribution on $[-1,1]^d$ and the sample size $m$ taking values up to $10000$. 
For a set of $m$ training points, we also consider a test set $\{x_j^{\text{test}}\}_{j=1}^{m_{\text{test}}}$ of size $m_{\text{test}} = 0.2m $, sampled independently according to the uniform distribution on $[-1,1]^d$. The corresponding test error is given by the \emph{Root Mean Square Error (RMSE)}, defined as 
\begin{equation*} %\label{eq:TestError}
    \mathcal{E}_{{\text{test}}} := 
    \sqrt{\dfrac{1}{m_{\text{test}}} \sum_{j=1}^{m_{\text{test}}} |f(x_j^{{\text{test}}}) - \tilde{f}(x_j^{{\text{test}}}) |^2},
\end{equation*} 
where $f$ is the target function, and $\tilde{f}$ is the approximation obtained from either method.
Specifically, adopting the notation in Section~\ref{sec:main_results}, $\tilde{f} = f^\natural$ for Least Squares, and $\tilde{f} = {\rm R}(\Psi^\natural)$ for Deep Learning, where $\Psi^\natural$ is the trained NN.  


\paragraph{Slepian basis functions.}
To construct one-dimensional Slepian basis functions $\varphi_{\mathsf{w}, j}$, we employ the Python package \texttt{scipy.special} \cite{2020SciPy-NMeth}, which is based on the angular solutions of the Helmholtz wave equation of the first kind. 
In fact, each function $\varphi_{\mathsf{w},j}$ is a scalar multiple of a corresponding angular solution \citep{moore2004prolate}. 
%Finally, in dimension one we consider Slepian bases of the form $\{\varphi_{ \mathsf{w}, j}\}_{j=1}^n$ whereas in dimension two we utilize tensor-product bases supported on a hyperbolic cross, namely $\{\varphi_{ \mathsf{w}, \nu_1 } \otimes \varphi_{ \mathsf{w}, \nu_2 }: \vec{\nu}=(\nu_1, \nu_2) \in \Lambda^{\text{HC}}_{n}\}$ (recall Definition~\ref{def:HC}).
To construct two-dimensional Slepian basis functions, we utilize the tensor-product definition \eqref{tensor} $\varphi_{\mathsf{w},\vec{\nu}} = \varphi_{\mathsf{w},\nu_1} \otimes \varphi_{\mathsf{w},\nu_2}$. 
Finally, in both cases, we take the index set to be the hyperbolic cross $\Lambda^{\text{HC}}_{n-1}$ (Definition~\ref{def:HC}) for various orders $n\in\N$.


\paragraph{Neural network architecture and training.} Following standard deep learning practice, we consider fully trained NNs. Here, we deviate from the theoretical setting of Theorem~\ref{thm:ApproxSpan}, where only the last layer is trained. NN architectures are built according to the ratio $r = L/N = 0.1$, where $L$ represents the number of hidden layers and $N$ the number of neurons per layer. Specifically, we conduct experiments for $L = 1,\dots, 10$, with $r = 0.1$, a choice empirically supported by \cite{adcock2021gap}. Weights and biases are initialized from a normal distribution with mean $0$ and standard deviation $0.1$. %, optimized with Adam \cite{diederik2015adam}, with an exponential decay rate of $0.85$. Additionally, each NN was trained for $150$ epochs, 
Training is performed using Adam \cite{diederik2015adam} with an exponential decay rate of $0.85$ for $150$ epochs. To ensure the robustness and reliability of the results, each experiment is repeated $20$ times.  


\paragraph{One-dimensional results.} Figure~\ref{fig:1D} illustrates the results of our first experiment on approximating the one-dimensional function $f_1$.
In accordance with our theoretical guarantees, both methods successfully approximate this frequency-localized function, with the RMSE converging as the number of sampling points increases.
%Moreover, we observe RMSE convergence as a function of the number of sampling points in both methods. 
Moreover, the number of parameters in each reconstruction model significantly affects the RMSE. For Least Squares, increasing the number of Slepian basis functions (corresponding to increasing the order $n$) generally lowers test errors, with the only recorded exception being $n=36$. %for small values of $m$
Similarly, for Deep Learning, increasing the number of network layers $L$ %and parameters 
results in a noticeable error reduction. %especially for larger values of $m$. 
However, note that Least Squares typically requires fewer samples as well as significantly fewer parameters to achieve high accuracy. %In other words, NNs are much less parameter and sample efficient than least squares in this case. 
The best accuracy achieved by Least Squares (RMSE $\approx 10^{-12}$) is several orders of magnitude higher than that by Deep Learning (RMSE $\approx 10^{-4}$). In summary, in one dimension, Least Squares is much more effective than Deep Learning for the approximation of $f_1$.

\begin{figure}[t!]
    \centering
    \includegraphics[width = 0.49\textwidth]{errorNN1D.pdf} 
    \includegraphics[width = 0.49\textwidth]{errorvsparams.pdf}
  \caption{Comparison of reconstruction methods for $f_1(x) = \cos(10x)e^{-\pi x^2}$
  from samples. The left panel shows results using Deep Learning, while the right panel shows results using Least Squares. Here, $L$ denotes the number of layers in each NN, and $n$ denotes the order of the hyperbolic cross $\Lambda^{\rm HC}_{n-1}$.}
  \label{fig:1D} 
\end{figure}




\paragraph{Two-dimensional results.} %We now turn our attention to the approximation of the two-dimensional function $f_2$. The results are summarized in Figure~\ref{fig:2D}.
Figure~\ref{fig:2D} illustrates the results of our second experiment on approximating the two-dimensional function $f_2$.
Both Least Squares and Deep Learning successfully approximate this frequency-localized function, in line with our theory. 
%We observe the convergence of the RMSE as a function of the number of samples $m$ for both methods, as in the one-dimensional case. 
As in the one-dimensional case, we observe RMSE convergence as a function of the number of samples $m$.
However, there are notable differences between the one- and two-dimensional cases. 
First, the best accuracy achieved by both methods is now much more similar. Both Least Squares and Deep Learning achieve a best mean RMSE of around $10^{-4}$. 
%Interestingly, Deep Learning can do better than that in some experiments due to the size of the variance regions. 
Interestingly, in some experiments, Deep Learning can attain a lower RMSE, as indicated by the size of the variance regions (the shaded regions in Figure~\ref{fig:2D}, left).
Hence, the accuracy gap between Least Squares and Deep Learning is much less pronounced in dimension two than in dimension one. Second, although the advantage of adding parameters to the model (for both Least Squares and Deep Learning) is still there, we observe some intriguing numerical phenomena. For Least Squares, considering a larger Slepian basis (i.e., larger order $n$) might not lead to a better accuracy (see the curve corresponding to $n=26$). Meanwhile, for Deep Learning, the benefit of increasing $L$ for accuracy becomes limited after $L\geq 6$.
In summary, dimensionality appears to play a crucial role when comparing Least Squares and Deep Learning. While a comprehensive numerical study is beyond the scope of this paper, our preliminary results suggest that %it is reasonable to believe that the benefits of Deep Learning versus Least Squares could increase in dimension three and higher.
the performance difference between the two reconstruction methods could increase in dimension three and higher.

\begin{figure}[t]
    \centering
    \includegraphics[width = 0.49\textwidth]{errorNN2D.pdf}
    \includegraphics[width = 0.49\textwidth]{errorvsparams2D.pdf} 
    \caption{Comparison of reconstruction methods for $f_2(x) = \cos(0.2x)\cos(0.2y)e^{-\pi (x^2+y^2)}$
    from samples. The left panel shows results using Deep Learning, while the right panel shows results using Least Squares. Here, $L$ denotes the number of layers in each NN, and $n$ denotes the order of the hyperbolic cross $\Lambda^{\rm HC}_{n-1}$.}
  \label{fig:2D} 
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proofs}
\label{sec:proofs}

To facilitate the proofs of Theorem~\ref{thm:leastsquaresampling} and Theorem~\ref{thm:ApproxSpan}, detailed in Subsection~\ref{sec:leastsquaresproof} and Subsection~\ref{sec:ProofTheo3} respectively, we first establish several key preliminary results in Subsection~\ref{sec:prelimformainthms} and Subsection~\ref{sec:prelimformainthm2}. 
Both theorems necessitate an understanding of the uniform norm of a normalized Slepian basis function within the centered $d$-dimensional unit cube. 
Additionally, Theorem~\ref{thm:ApproxSpan} relies on the polynomial approximability of neural networks and fundamental network operations, which are outlined in Appendix~\ref{appx:basicnetworks} for clarity and reference.

\subsection{A preliminary result for the main theorems} \label{sec:prelimformainthms}

%Below, we establish a preliminary result on the uniform norm of a normalized Slepian basis function in one dimension, which is pivotal to our forthcoming analysis and the proofs of the two main theorems.
In what follows, we recall from \eqref{eqdef:gammaw} the simplified notation $\gamma(\mathsf{w}) = \lceil \log_2(30\mathsf{w}) \rceil$.


\begin{proposition} \label{prop:gammaw} For any $\mathsf{w}\geq 1$, let $\{\varphi_{\mathsf{w},j}\}_{j\in\N_0}$ be the Slepian basis in $L_{ {\bf u}}^2([-1,1])$. 
Then we have
\begin{equation} \label{eq:varphijall}
    \|\varphi_{\mathsf{w},j}\|^2_{L^{\infty}([-1,1])}\leq \mathbbm{1}_{\{j=0\}} \Big(\frac{4\mathsf{w}}{\pi}\Big) + \mathbbm{1}_{\{j\in\mathbb{N}\}} (j+1)^{\gamma(\mathsf{w})}.
\end{equation}
\end{proposition}

In preparation, we present a key a priori result, with its proof deferred to the end of this subsection.

\begin{proposition} \label{prop:Phi_j} Let $\mathsf{w}\geq 1$. 
Let $j^{\star}(\mathsf{w}):= \lfloor 4\mathsf{w} \rfloor -1$. 
Then it holds for all $j\in\N_0$ that
\begin{equation*} 
    \|\varphi_{\mathsf{w},j}\|_{L^{\infty}([-1,1])} \leq 
    \begin{cases} 
        2\sqrt{\frac{\mathsf{w}}{\pi}} &\text{ if } j\leq j^{\star}(\mathsf{w}),\\
        {}\\
        \frac{12}{5} \sqrt{j+1} &\text{ if } j> j^{\star}(\mathsf{w}).
    \end{cases}
\end{equation*}
\end{proposition}

\begin{remark} \label{rem:firsteigen}
The distinction between Proposition~\ref{prop:Phi_j} and Proposition~\ref{prop:gammaw} lies in the latter providing a more encompassing upper bound for $\|\varphi_{\mathsf{w},j}\|_{L^{\infty}([-1,1])}$ as a function of the index $j$, without relying on a critical division $j^{\star}(\mathsf{w})$ tied to the bandwidth $\mathsf{w}$.
Such a broader bound is needed in the proof of Theorem~\ref{thm:leastsquaresampling}. 
A frequently cited emblematic estimate indicates that, for all $j\in\mathbb{N}_0$ 
\begin{equation} \label{eq:misinfo}
    \|\varphi_{\mathsf{w},j}\|_{L^{\infty}([-1,1])}\leq C\sqrt{j};
\end{equation}
see for example \citep[Remark~2.4]{shkolnisky2006approximation}.
However, \eqref{eq:misinfo} holds only asymptotically (and a rectified version of \eqref{eq:misinfo} is given in \citep[Theorem~2.7]{xiao2001prolate}). Specifically, for sufficiently large values of $j$, beyond the critical index $j^{\star}(\mathsf{w})$, the global maximum of $|\varphi_{\mathsf{w},j}|$ on $[-1,1]$ shifts to the boundary of the interval, becoming equal to $|\varphi_{\mathsf{w},j}(1)|$, which is of order $\mathcal{O}(\sqrt{j})$, as numerically demonstrated in \citep[Theorem~8.4]{xiao2001prolate}.
This shifting behavior, as discussed in both \citep[Theorem~3.38]{xiao2001prolate}, \citep[Theorem~3.1]{bonami2014uniform}, occurs when $\mathsf{w}^2/\chi_{\mathsf{w},j} \leq 1$. The second theorem will be later utilized in our proof of Proposition~\ref{prop:Phi_j}.
For low indices $j$, \citep[Remark~3.1]{bonami2014uniform} implies that $\|\varphi_{\mathsf{w},j}\|_{L^{\infty}([-1,1])}\leq C\sqrt{\mathsf{w}}$.
Notably, $\varphi_{\mathsf{w},0}$ is even, positive on the interval $(-1,1)$ and attains its global maximum at the origin. Yet, for low indices $j$, including $j=0$, the behavior of $\varphi_{\mathsf{w},j}(0)$ is poorly understood.
%A rectified version of \eqref{eq:misinfo} is also given in \citep[Theorem~2.7]{xiao2001prolate}, and Proposition~\ref{prop:Phi_j} can be viewed as an almost immediate consequence of this result.
\end{remark} 



\begin{proof}[Proof of Proposition~\ref{prop:gammaw}]
Leveraging Proposition~\ref{prop:Phi_j}, the claim $\|\varphi_{\mathsf{w},0}\|_{L^{\infty}([-1,1])}^2 \leq 4\mathsf{w}/\pi$ becomes clear, leaving only the proof of \eqref{eq:varphijall} to be addressed for $j\in\N$.
Toward this end, we write
\begin{equation} \label{eq:varphLinftybd}
    \|\varphi_{\mathsf{w},j}\|^2_{L^{\infty}([-1,1])} \leq \Big(\frac{4\mathsf{w}}{\pi}\Big) \mathbbm{1}_{\{1\leq j\leq j^{\star}(\mathsf{w})\}} + \Big(\frac{12}{5}\Big)^2 (j+1) \mathbbm{1}_{\{j>j^{\star}(\mathsf{w})\}}.
\end{equation}
Let $\tilde{c} = (\frac{12}{5})^2$, and let
\begin{equation} \label{eqdef:h}
    h(x) := \frac{\log\big(30\mathsf{w}\mathbbm{1}_{\{x\leq j^{\star}(\mathsf{w})\}} + \tilde{c}(x+1) \mathbbm{1}_{\{x>j^{\star}(\mathsf{w})\}} \big)}{\log(x+1)}.
\end{equation}
We claim the following.
\begin{claim} \label{claim}
    \qquad $h(x)$ \text{ is nonincreasing on } $[1,\infty)$.
\end{claim}
Observe that the claim is evident for either the case $1\leq x\leq j^{\star}(\mathsf{w})$ or for the case $x > j^{\star}(\mathsf{w})$. Indeed, by definition \eqref{eqdef:h}, on $[1,j^{\star}(\mathsf{w})]$,
\begin{equation*}
    h(x)=\frac{\log(30\mathsf{w})}{\log(x+1)}
\end{equation*}
is decreasing, and on $(j^{\star}(\mathsf{w}),\infty)$, 
\begin{equation*}
    h(x) = \frac{\log(\tilde{c}\, (x+1))}{\log(x+1)} = \frac{\log(\tilde{c})}{\log(x+1)} + 1
\end{equation*}
is also decreasing. 
Hence it suffices to show, if $1\leq x_1\leq j^{\star}(\mathsf{w})< x_2\leq j^{\star}(\mathsf{w})+1$, then $h(x_2)\leq h(x_1)$. 
Write $x_2 = j^{\star}(\mathsf{w})+\varepsilon = \lfloor 4\mathsf{w} \rfloor - 1 + \varepsilon$ for some $\varepsilon\in (0,1]$. 
Given that $\mathsf{w}\geq 1$, and so $30\mathsf{w}\geq 6(4\mathsf{w}+1)\geq \tilde{c} (j^{\star}(\mathsf{w})+\varepsilon+1)$, we gather
\begin{equation*}
    h(x_1) = \frac{\log (30\mathsf{w})}{\log(x_1+1)} \geq \frac{\log(\tilde{c} (j^{\star}(\mathsf{w})+\varepsilon+1))}{\log(j^{\star}(\mathsf{w})+\varepsilon+1)} = h(x_2),
\end{equation*}
thereby affirming the validity of Claim~\ref{claim}. 
As a consequence, for all $x\geq 1$,
\begin{equation*}
    \frac{\log\big(30\mathsf{w}\mathbbm{1}_{\{x\leq j^{\star}(\mathsf{w})\}} + \tilde{c} (x+1) \mathbbm{1}_{\{x>j^{\star}(\mathsf{w})\}} \big)}{\log(x+1)} \leq \frac{\log (30\mathsf{w})}{\log(2)} =\gamma(\mathsf{w})
\end{equation*}
or equivalently,
\begin{equation} \label{eq:claimconsequence}
    30\mathsf{w}\mathbbm{1}_{\{1\leq x\leq j^{\star}(\mathsf{w})\}} + \tilde{c}(x+1)\mathbbm{1}_{\{x>j^{\star}(\mathsf{w})\}}
    \leq (x+1)^{\gamma(\mathsf{w})}.
\end{equation}
Combining \eqref{eq:claimconsequence}, \eqref{eq:varphLinftybd}, we can infer that
\begin{equation*} \label{eq:caseN}
    \|\varphi_{\mathsf{w},j}\|^2_{L^{\infty}([-1,1])} \leq 30\mathsf{w}\mathbbm{1}_{\{1\leq j\leq j^{\star}(\mathsf{w})\}} + \tilde{c}(j+1)\mathbbm{1}_{\{j>j^{\star}(\mathsf{w})\}} \leq (j+1)^{\gamma(\mathsf{w})}, 
\end{equation*}
as wanted.
\end{proof}

To prove Proposition~\ref{prop:Phi_j}, we rely on the following two well-known lemmas. The first, Lemma~\ref{lem:spliteigenrecall} (see, for instance, \citep{landau1965eigenvalue, landau1993density}) concerns the spectral gap in the eigenvalues $\mu_{\mathsf{w},j}$ \eqref{eq:muj}, while the second, Lemma~\ref{lem:higheigncase}, pertains to the magnitude of $\varphi_{\mathsf{w},j}$ for sufficiently large index $j$.
The proof of Proposition~\ref{prop:Phi_j} will come after the presentation of these lemmas.

\begin{lemma} \label{lem:spliteigenrecall} Let $\mathcal{Q}_{\mathsf{w}}$ be defined in \eqref{eqdef:Qw}. Then the eigenvalues $\mu_{\mathsf{w},j}$ of $\mathcal{Q}_{\mathsf{w}}$ satisfy
\begin{equation} \label{crucialupperbound} 
    \mu_{\mathsf{w}, \lfloor 4\mathsf{w}\rfloor-1}\geq \frac{1}{2} \geq \mu_{\mathsf{w}, \lceil 4\mathsf{w}\rceil}. 
\end{equation}
\end{lemma}


Lemma~\ref{lem:higheigncase} below is a version of an estimate commonly referenced in the literature but seldom proved. 
For the convenience of the reader, we offer a brief proof sketch, supplemented with references for more rigorous details.

\begin{lemma} \label{lem:higheigncase} Let $j\in\N_0$. Then for every $j\geq \frac{2\mathsf{w}}{\pi}$, the following holds%$\chi_{\mathsf{w},j}>\mathsf{w}^2$. Moreover, for 
\begin{equation} \label{eq:psijlarge}
    \|\varphi_{\mathsf{w},j}\|_{L^{\infty}([-1,1])} \leq \frac{12}{5} \sqrt{j+1}.
\end{equation}
\end{lemma}

\begin{proof}[Sketch of proof]
Recall that the Slepian basis functions $\varphi_{\mathsf{w},j}$ serve as eigenfunctions for the operator $\mathcal{L}_{\mathsf{w}}$ \eqref{eqdef:Lw}, with the corresponding eigenvalues denoted by $\chi_{\mathsf{w},j}$ \eqref{eq:spectrumchi}.
Let $j\geq \frac{2\mathsf{w}}{\pi}$. Then it follows from \citep[Theorem 4.4]{osipov2013prolate} that
\begin{equation} \label{eq:leg1}
    \chi_{\mathsf{w},j} > \mathsf{w}^2.
\end{equation}
On the one hand, when \eqref{eq:leg1} is satisfied, we can employ \citep[Theorem 3.39]{osipov2013prolate} to obtain
\begin{equation} \label{eq:leg2}
    \|\varphi_{\mathsf{w},j}\|_{L^{\infty}([-1,1])} = |\varphi_{\mathsf{w},j}(1)|.
\end{equation}
On the other hand, according to \citep[Theorem~3.1]{bonami2014uniform}, \eqref{eq:leg1} also yields 
\begin{equation} \label{eq:leg3}
    |\varphi_{\mathsf{w},j}(1)| \leq \frac{12}{5}\sqrt{j+1}.
\end{equation}
Combining \eqref{eq:leg1}, \eqref{eq:leg2}, \eqref{eq:leg3}, we acquire \eqref{eq:psijlarge}, as desired.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:Phi_j}]
Extend the uniform measure ${\bf u}$ on $[-1,1]$ to a measure on $\R$ by setting ${\rm d}{\bf u}(t) = \frac{1}{2} {\rm d}t$. 
We show that $\{\varphi_{\mathsf{w},j}\}_{j\in\N_0}$ forms an orthogonal system in $L_{\bf u}^2(\R)$. 
Since $\varphi_{\mathsf{w},j}$ is an eigenfunction of the operator $\mathcal{Q}_{\mathsf{w}}$ \eqref{eqdef:Qw} corresponding to the eigenvalue $\mu_{\mathsf{w},j}$,
\begin{equation} \label{eq:eigenfQw}
    \mathcal{Q}_{\mathsf{w}}\varphi_{\mathsf{w},j}(x) = \int_{-1}^1 \varphi_{\mathsf{w},j}(t)\,\frac{\sin\mathsf{w}(x-t)}{\pi(x-t)}\,{\rm d}t = \mu_{\mathsf{w},j}\varphi_{\mathsf{w},j}(x), \qquad \forall x\in\R. 
\end{equation}
Hence, we deduce that
\begin{align} \label{eq:Slepianinnerproduct}
    \nonumber &\int_{\R} \varphi_{\mathsf{w},j}(x)\varphi_{\mathsf{w},k}(x)\,{\rm d}{\bf u}(x) \\
    \nonumber &= \frac{1}{2\mu_{\mathsf{w},j}\mu_{\mathsf{w},k}} \int_{[-1,1]\times [-1,1]} \varphi_{\mathsf{w},j}(t)\varphi_{\mathsf{w},k}(u) \bigg( \int_{\R} \frac{\sin\mathsf{w}(x-t)}{\pi(x-t)} \,\frac{\sin\mathsf{w}(x-u)}{\pi(x-u)}\,{\rm d}x \bigg) \, {\rm d}t{\rm d}u\\
    \nonumber &= \frac{1}{2\mu_{\mathsf{w},j}\mu_{\mathsf{w},k}} \int_{[-1,1]\times [-1,1]} \varphi_{\mathsf{w},j}(t)\varphi_{\mathsf{w},k}(u) \frac{\sin\mathsf{w}(t-u)}{\pi(t-u)} \, {\rm d}t{\rm d}u\\
    &= \frac{1}{\mu_{\mathsf{w},j}} \int_{[-1,1]} \varphi_{\mathsf{w},j}(t)\varphi_{\mathsf{w},k}(t) \,{\rm d}{\bf u}(t).
\end{align}
Above, we have applied a \emph{reproducing kernel trick} \citep[Subsection~3.1.2]{benedetto2012modern} at the second inequality and \eqref{eq:eigenfQw} at the first and last. 
Due to the orthonormality of $\{\varphi_{\mathsf{w},j}\}_{j\in\N_0}$ in $L_{\bf u}^2([-1,1])$, \eqref{eq:Slepianinnerproduct} evaluates to zero if $j \neq k$ and to $\frac{1}{\mu_{\mathsf{w},j}}$ if $j=k$, we conclude that $\{\varphi_{\mathsf{w},j}\}_{j\in\N_0}$ indeed constitutes an orthogonal system in $L_{\bf u}^2(\R)$\footnote{In other words, the Slepian functions are bi-orthogonal in the sense that they are orthogonal over both a given finite interval and $\mathbb{R}$.}. 
In particular,
\begin{equation} \label{eq:decayvarphi}
    \|\varphi_{\mathsf{w},j} \|_{L_{\bf u}^2(\R)} = \sqrt{\frac{1}{\mu_{\mathsf{w},j}}}.
\end{equation}
Hence, by applying Lemma~\ref{lem:spliteigenrecall} to \eqref{eq:decayvarphi}, we obtain for $j\leq \lfloor 4\mathsf{w}\rfloor-1$ that 
\begin{equation} \label{eq:varphibigj}
    \|\varphi_{\mathsf{w},j} \|_{L_{\bf u}^2(\R)} \leq \sqrt{2}.
\end{equation}
Since $\varphi_{\mathsf{w},j}\in{\rm PW}_{\mathsf{w}}$, \eqref{eq:bandlimitedequiv} holds.
It follows that
\begin{equation*}
    |\varphi_{\mathsf{w},j}(x)| = \bigg|\frac{1}{2\pi}\int_{-\mathsf{w}}^{\mathsf{w}} \widehat{\varphi_{\mathsf{w},j}}(t)e^{ixt}\,dt\bigg|
    \leq \frac{\sqrt{\mathsf{w}}}{\sqrt{2}\pi}\|\widehat{\varphi_{\mathsf{w},j}}\|_{L^2(\R)} = \sqrt{\frac{\mathsf{w}}{\pi}} \|\varphi_{\mathsf{w},j}\|_{L^2(\R)}, \qquad \forall x\in\R,
\end{equation*}
and subsequently, for $j\in\N_0$,
\begin{equation} \label{eq:key}
    \|\varphi_{\mathsf{w},j}\|_{L^{\infty}(\R)} %= \|\varphi_{\mathsf{w},j}\|_{L_{\bf u}^{\infty}(\R)} 
    \leq \sqrt{\frac{2\mathsf{w}}{\pi}}\|\varphi_{\mathsf{w},j}\|_{L_{\bf u}^2(\R)}.
\end{equation}
Then combining \eqref{eq:varphibigj} with \eqref{eq:key} for $j\leq \lfloor 4\mathsf{w}\rfloor-1$ yields
\begin{equation*} \label{eq:varphibigjLinfty}
    \|\varphi_{\mathsf{w},j}\|_{L^{\infty}([-1,1])} 
    \leq \|\varphi_{\mathsf{w},j}\|_{L^{\infty}(\R)} 
    \leq \sqrt{\frac{2\mathsf{w}}{\pi}}\|\varphi_{\mathsf{w},j}\|_{L_{\bf u}^2(\R)}
    \leq 2\sqrt{\frac{\mathsf{w}}{\pi}}.  
\end{equation*}

For the remaining case of $j\geq \lfloor 4\mathsf{w}\rfloor$, we recall from Lemma~\ref{lem:higheigncase} that \eqref{eq:psijlarge} holds whenever $j\geq  \frac{2\mathsf{w}}{\pi}$. 
The proof is now completed. 
\end{proof}

\begin{comment}
\begin{remark} 
Crucial to our proof of Proposition~\ref{prop:Phi_j} is Lemma~\ref{lem:spliteigenrecall}. In this context, we are compelled to present another asymptotic result, derived from \citep[Theorem~1]{landau1980eigenvalue}, that has guided our investigation.
%
\begin{lemma} \label{lem:Ncalpha} 
Let $\mathcal{Q}_{\mathsf{w}}$ be defined in \eqref{eqdef:Qw}. Let $\alpha>0$, and let $N(\mathsf{w},\alpha)$ denote the number of first eigenvalues $\mu_{\mathsf{w},j}$ of $\mathcal{Q}_{\mathsf{w}}$ that are greater than $\alpha$. Then\footnote{The version of Lemma~\ref{lem:Ncalpha} stated in \citep{landau1980eigenvalue} differs slightly, since our time interval $[-1,1]$ has length $2$; see also \citep[Theorem~2]{landau1980eigenvalue}. A comprehensive summary of the arguments presented in \citep{landau1980eigenvalue} can be found in \citep{izu2009time}.}
\begin{equation} \label{eq:Nalpha}
    N(\mathsf{w},\alpha) = \frac{2\mathsf{w}}{\pi} + \Big(\frac{1}{\pi^2}\log\frac{1-\alpha}{\alpha}\Big)\log\mathsf{w}+ o(\log \mathsf{w}).
\end{equation}
\end{lemma}

Note that Lemma~\ref{lem:Ncalpha} is also a qualitative result. Specifically, \eqref{eq:Nalpha} becomes significant when $\mathsf{w}$ becomes large; that is,
\begin{equation*} 
    \lim_{\mathsf{w}\to\infty} \frac{N(\mathsf{w},\alpha) - \frac{2\mathsf{w}}{\pi}}{\log\mathsf{w}} = \frac{1}{\pi^2}\log\frac{1-\alpha}{\alpha}.
\end{equation*}
\end{remark}
\end{comment}

\subsection{Proof of Theorem~\ref{thm:leastsquaresampling}} \label{sec:leastsquaresproof}


The existence of a unique solution to \eqref{leastsquaresproblem}, for any finite index set $\Lambda\subset\mathbb{N}_0^d$ can be ensured using the \emph{discrete stability constant} associated with $\mathcal{S}_{\Lambda}$ and the sample size $m$; that is
\begin{equation*}
    \alpha 
    = \alpha(\mathcal{S}_{\Lambda}, \{\vec{y}_j\}_{j=1}^m) \\
    := \bigg\{\frac{1}{m}\sum_{j=1}^m |g(x_j)|^2 : g\in\mathcal{S}_{\Lambda} \text{ and } \|g\|_{L_{{\bf u}}^2([-1,1]^d)}=1  \bigg\}.
\end{equation*}
To simplify the notation, we let $K:=\texttt{\#}\Lambda$ and assign a fixed order to the elements of $\Lambda$ via a bijection $\Pi: \{1,\dots,K\}\to\Lambda$. 
We rewrite $\varphi_{\mathsf{w},\vec{\nu}}$ as $\varphi_{\mathsf{w},\Pi(k)}$ temporarily, using $k=1, 2, \dots, K$ as indices. 
Then, since $\mathcal{S}_{\Lambda}$ is a finite-dimensional space, with a basis formed by the vectors $\{\varphi_{\mathsf{w},\Pi(k)}\}_{k=1}^K$, we apply the Courant-Fisher min-max theorem \citep[Theorem 4.2.6]{horn2012matrix} to get 
\begin{equation*} 
    \alpha = \sigma_{\rm min}(A) = \lambda_{\rm min}(A^*A),
\end{equation*}
where $A\in\mathbb{R}^{K\times m}$ is given by 
\begin{equation} \label{eqdef:A}
    A_{jk} := \dfrac{1}{\sqrt{m}} \varphi_{\mathsf{w},\Pi(k)}(\vec{y}_j).
\end{equation}
Here, $\sigma_{\rm min}(A)$ denotes the minimum singular value of $A$ and $\lambda_{\rm min}(A^*A)$ the minimum eigenvalue of $A^*A$. A positive lower bound on $\alpha$, crucial for the existence of a unique solution to \eqref{leastsquaresproblem}, can be further guaranteed via Monte Carlo sampling if $m$ is sufficiently large. 
This is formalized in the following theorem, adapted from \citep[Theorem 5.7]{adcock2022sparse}.
%We recall that $c_{\delta} = ((1-\delta)\log(1-\delta)+\delta)^{-1}$.

\begin{theorem} \label{thm:LS}
Let $\beta,\delta\in (0,1)$ and set\footnote{The quantity $\kappa(\mathcal{S}_{\Lambda})$ in \eqref{coherence} is known as the $L^{\infty}$-norm of the \emph{Christoffel function} of $\mathcal{S}_{\Lambda}$ \citep[Definition~5.4]{adcock2022sparse}.}
\begin{equation} \label{coherence}
    \kappa(\mathcal{S}_{\Lambda}) := \max_{\vec{y}\in [-1,1]^d} \sum_{k=1,\dots,K} |\varphi_{\mathsf{w},\Pi(k)}(\vec{y})|^2.
\end{equation}
If $m\geq c_{\delta}\kappa(\mathcal{S}_{\Lambda})\log(K/\beta)$, then with probability at least $1-\beta$, the discrete stability constant $\alpha=\alpha(\mathcal{S}_{\Lambda}, \{\vec{y}_j\}_{j=1}^m)$ satisfies
\begin{equation} \label{lowbdalpha}
    \alpha > \sqrt{1-\delta}.
\end{equation}
\end{theorem}


Assuming the validity of \eqref{lowbdalpha}, the theory outlined in \citep[Section~5]{adcock2022sparse}, particularly \citep[Corollary~5.9]{adcock2022sparse}, demonstrates that every $f \in \mathcal{C}([-1,1]^d)$ %$f \in L^2_{\bf u}([-1,1]^d) \cap \mathcal{C}([-1,1]^d)$ 
has a unique least squares approximation $f^{\natural}$ in \eqref{leastsquaresproblem} that satisfies 
\begin{align} \label{punchline}
    \nonumber 
    \|f - f^{\natural}\|_{L_{ {\bf u}}^2([-1,1]^d)} 
    &\leq (1+ \alpha^{-1}) \inf_{g \in \mathcal{S}_{\Lambda}} 
    \|f-g \|_{L^{\infty}([-1,1]^d)} + \dfrac{1}{\alpha} \|\vec{e}\|_2 \\
    &< \Big(1+ \frac{1}{\sqrt{1-\delta}} \Big) \inf_{g \in \mathcal{S}_{\Lambda}} 
    \|f-g \|_{L^{\infty}([-1,1]^d)} + \frac{1}{\sqrt{1-\delta}} \|\vec{e}\|_2.
\end{align}
It then follows from Theorem~\ref{thm:LS} that the estimation of $\kappa(\mathcal{S}_{\Lambda})$ is the key piece to guarantee \eqref{punchline} with probability at least $1-\beta$. 
Evidently from \eqref{coherence}, the size of $\kappa(\mathcal{S}_{\Lambda})$ is impacted by the choice of $\Lambda$. 
When $\Lambda$ is a hyperbolic cross in low dimensions, an upper bound for $\kappa(\mathcal{S}_{\Lambda})$ can be provided by the next proposition. 

\begin{proposition} \label{prop:Theta} 
Let $n\in\N$, and let $\Lambda = \Lambda^{\rm HC}_{n-1}$ be of dimension $d = 1,2,3$. 
If $\mathsf{w}\geq 1$, and $\mathcal{S}_{\Lambda}$ denotes the linear span of $\{\varphi_{\mathsf{w},\vec{\nu}}\}_{\vec{\nu}\in\Lambda}$, then it holds that 
\begin{equation} \label{Theta}
    %\kappa(\mathcal{S}_{\Lambda})\leq  (2\,\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})},
    \kappa(\mathcal{S}_{\Lambda})\leq  (2\,\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})},
\end{equation}
for the following cases: 
\begin{enumerate}
    \item for $d=1,2$ and all $n\in\N$,
    \item for $d=3$ and all $n\geq 26$. 
\end{enumerate}
\end{proposition}

Since the index set $\Lambda=\Lambda^{\rm HC}_{n-1}$ contains $\vec{0}$, the function $\varphi_{\mathsf{w},\vec{0}}$ will always be included in the span $\mathcal{S}_\Lambda$. 
Proposition~\ref{prop:gammaw} and Remark~\ref{rem:firsteigen} suggest that estimating $\|\varphi_{\mathsf{w},\vec{0}}\|_{L^{\infty}([-1,1]^d)}$ may pose complications, as an upper bound for this value could reach $\mathsf{w}^d$, leading to an exponential growth of $\kappa(\mathcal{S}_{\Lambda})$ in high dimensions. 
Consequently, we restrict the selection of $\Lambda=\Lambda^{\rm HC}_{n-1}$ to dimensions up to $d=3$.
%This choice allows us to derive the following crucial result regarding the size of $\kappa(\mathcal{S}_{\Lambda})$.

If we accept Proposition~\ref{prop:Theta}, then the rationale behind the choice of $\kappa^{\star}=(2\,\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})}$ in \eqref{eq:choices} can be seen from \eqref{Theta}.
Specifically, by ensuring $m\geq c_{\delta} \kappa^{\star}\log(\texttt{\#}\Lambda/\beta)$, the proof of Theorem~\ref{thm:leastsquaresampling} can now be completed through a combination of Theorem~\ref{thm:LS} and \eqref{punchline}. \qed

It remains for us to establish Proposition~\ref{prop:Theta} next.

\begin{proof}[Proof of Proposition~\ref{prop:Theta}]
We begin with an outline and a preliminary result.
To derive an estimate for $\kappa(\mathcal{S}_{\Lambda})$, where $\Lambda=\Lambda^{\rm HC}_{n-1}=\{\vec{\nu}\in\N_0^{d}: \prod_{k=1}^{d}(\nu_k+1)\leq n\}$ and $d=1,2,3$, we utilize an induction strategy on dimension, as employed in the proof of \citep[Proposition~5.13]{adcock2022sparse} (also \citep[Lemma 3.3]{chkifa2015discrete}). Our induction step will end at $d=3$.
For the base case $d=1$, we make use of Proposition~\ref{prop:gammaw} to provide an upper bound for $\kappa(\mathcal{S}_{\Lambda})$.
For the induction step, we observe from the slicing property \eqref{slicing} that each $\Lambda_k$ is a $(d-1)$-dimensional hyperbolic cross of order $\big\lfloor \frac{n}{k+1}\big\rfloor$, i.e.
\begin{equation} \label{eq:LambdakHC}
    \Lambda_{k} = \bigg\{\nu'=(\nu_2,\dots,\nu_d) \in\N_0^{d-1}: \prod_{k=2}^{d}(\nu_k+1)\leq \big\lfloor \frac{n}{k+1}\big\rfloor\bigg\},
\end{equation}
and particularly, 
\begin{equation} \label{eq:Lambda0HC}
    \Lambda_0 = \bigg\{\nu'=(\nu_2,\dots,\nu_{d})\in\N^{d-1}_0: \prod_{k=2}^{d}(\nu_k+1)\leq n\bigg\}
\end{equation}
is a $(d-1)$-dimensional hyperbolic cross of exactly order $n$.
From there, we rely on the auxiliary yet crucial result stated below, which compares the sizes of horizontal slices in two- and three-dimensional hyperbolic cross index sets. 
Its proof is given at the conclusion of this subsection.

\begin{proposition} \label{prop:relating}
Let $n\in\N$ and $d=2,3$. Let $\Lambda=\Lambda^{\rm HC}_{n-1}$ be the $d$-dimensional hyperbolic cross of order $n$ where $\Lambda = \bigsqcup_{k=0}^{n-1} \, \{k\}\times\Lambda_k$ as in \eqref{slicing}, \eqref{eq:LambdakHC}.
Then for $d=2$ and all $n\geq 2$, or for $d=3$ and all $n\geq 26$, 
\begin{equation} \label{crucialinequality}
    \sum_{k=1}^{n-1} (2k+1)\,(\texttt{\#}\Lambda_k)^2 \geq \frac{(\texttt{\#}\Lambda_0)^2}{n}. 
\end{equation}
\end{proposition}

Returning to the proof of Proposition~\ref{prop:Theta}, we note that $\gamma(\mathsf{w})>1$ for $\mathsf{w}\geq 1$. Moreover
\begin{equation} \label{eq:powerscompared}
    %4^{\gamma(\mathsf{w})-1} \geq 
    2^{\gamma(\mathsf{w})-1}\geq \frac{4\mathsf{w}}{\pi}.
\end{equation}
When $d=1$, $\Lambda = \Lambda^{\rm HC}_{n-1} = \{0,\dots,n-1\}$. 
On the one hand, if $n=1$, then it follows from Proposition~\ref{prop:gammaw} and \eqref{eq:powerscompared} that
\begin{equation} \label{eq:s1}
    \kappa(\mathcal{S}_{\Lambda}) = \|\varphi_{\mathsf{w},0}\|^2_{L^{\infty}([-1,1])} \leq \frac{4\mathsf{w}}{\pi} \leq 2^{\gamma(\mathsf{w})} = (\sqrt{2}\,\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})},
\end{equation}
which satisfies \eqref{Theta}.
On the other hand, if $n\geq 2$, then we can again apply Proposition~\ref{prop:gammaw} to obtain
\begin{equation} \label{eq:d1s2}
    \kappa(\mathcal{S}_{\Lambda}) = \sum_{k=0}^{n-1} \|\varphi_{\mathsf{w},k}\|^2_{L^{\infty}([-1,1])} \leq \frac{4\mathsf{w}}{\pi} + \sum_{k=1}^{n-1} (k+1)^{\gamma(\mathsf{w})} \leq \frac{4\mathsf{w}}{\pi} + \sum_{k=1}^{n-1} (2k+1)^{\gamma(\mathsf{w})}. %\leq \big(\sum_{k=0}^{n-1} 2k+1 \big)^{\gamma(\mathsf{w})} = n^{2\gamma(\mathsf{w})} = (2\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})},
\end{equation}
We claim that 
\begin{equation} \label{eq:claim}
    \frac{4\mathsf{w}}{\pi} + \sum_{k=1}^{n-1} (2k+1)^{\gamma(\mathsf{w})} \leq \Big(\sum_{k=0}^{n-1} 2k+1 \Big)^{\gamma(\mathsf{w})} = (n^2-1)^{\gamma(\mathsf{w})} \leq (2n^2)^{\gamma(\mathsf{w})} = (\sqrt{2}\,\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})}.
\end{equation}
It is enough to validate the first inequality in \eqref{eq:claim}. 
Then on account of Pascal's identity \citep[Chapter~6.4, Theorem~2]{rosen1999discrete} and \eqref{eq:powerscompared}, we have
\begin{align*}
    \Big(\sum_{k=0}^{n-1} 2k+1 \Big)^{\gamma(\mathsf{w})} &= 
    \Big( \Big(\sum_{k=1}^{n-1} 2k+1 \Big) + 1\Big)^{\gamma(\mathsf{w})} \\
    &= \sum_{l=0}^{\gamma(\mathsf{w})-1} {\gamma(\mathsf{w}) \choose l} \Big(\sum_{k=1}^{n-1} 2k+1\Big)^{l} + \Big(\sum_{k=1}^{n-1} 2k+1 \Big)^{\gamma(\mathsf{w})} \\
    &\geq \sum_{l=0}^{\gamma(\mathsf{w})-1} {\gamma(\mathsf{w}) \choose l} \Big(\sum_{k=1}^{n-1} 2k+1\Big)^{l} + \sum_{k=1}^{n-1} (2k+1)^{\gamma(\mathsf{w})} \\
    &\geq \sum_{l=0}^{\gamma(\mathsf{w})-1} {\gamma(\mathsf{w}) -1 \choose l} 3^l +  \sum_{k=1}^{n-1} (2k+1)^{\gamma(\mathsf{w})} \\
    &= 4^{\gamma(\mathsf{w})-1} +  \sum_{k=1}^{n-1} (2k+1)^{\gamma(\mathsf{w})} \\
    &\geq \frac{4\mathsf{w}}{\pi} + \sum_{k=1}^{n-1} (2k+1)^{\gamma(\mathsf{w})}.
\end{align*}
Therefore, \eqref{eq:claim} holds. By combining \eqref{eq:claim} and \eqref{eq:d1s2}, we deduce \eqref{Theta} for $d=1$ and $n\geq 2$. 

Turning to the case $d=2$ or $d=3$, we have that $\Lambda=\Lambda^{\rm HC}_{n-1}$ is a two-dimensional or three-dimensional hyperbolic cross, where each $\Lambda_k$ is a one-dimensional or two-dimensional hyperbolic cross, respectively. 
In what follows, we will employ an argument applicable for both the case $d=2$ when $n\geq 2$ and for $d=3$ when $n\geq 26$.
For the latter case, we additionally assume that \eqref{Theta} already holds for $d=2$ for all $n\in\N$. The separate case of $d=2$ and $n=1$ will be addressed at the end.

Let $d=2$ and $n\geq 2$. 
By applying \eqref{Theta} to each one-dimensional hyperbolic cross $\Lambda_k$ and invoking Proposition~\ref{prop:gammaw}, we obtain
\begin{align} \label{eq:induction1}
    \nonumber \kappa(\mathcal{S}_{\Lambda}) &= \sum_{k=0}^{n-1} \|\varphi_{\mathsf{w},k}\|^2_{L^{\infty}([-1,1])} \sum_{j\in\Lambda_k} \|\varphi_{\mathsf{w},j}\|^2_{L^{\infty}([-1,1])} \\
    \nonumber &= \|\varphi_{\mathsf{w},0}\|^2_{L^{\infty}([-1,1])} \sum_{j\in\Lambda_0} \|\varphi_{\mathsf{w},j}\|^2_{L^{\infty}([-1,1])} + \sum_{k=1}^{n-1} \|\varphi_{\mathsf{w},k}\|^2_{L^{\infty}([-1,1])} \sum_{j\in\Lambda_k} \|\varphi_{\mathsf{w},j}\|^2_{L^{\infty}([-1,1])} \\
    &\leq \frac{4\mathsf{w}}{\pi}(2\,\texttt{\#}\Lambda_0)^{2\gamma(\mathsf{w})} + \sum_{k=1}^{n-1} (k+1)^{\gamma(\mathsf{w})}(2\,\texttt{\#}\Lambda_k)^{2\gamma(\mathsf{w})}. 
\end{align}
Similarly, let $d=3$ and $n\geq 26$. Assuming that \eqref{Theta} holds for all two-dimensional hyperbolic cross index sets $\Lambda_k$, we can write
\begin{align} \label{eq:induction1d3}
    \nonumber \kappa(\mathcal{S}_{\Lambda}) &= \sum_{k=0}^{n-1} \|\varphi_{\mathsf{w},k}\|^2_{L^{\infty}([-1,1])} \sum_{\vec{\nu}\in\Lambda_k} \|\varphi_{\mathsf{w},\vec{\nu}}\|^2_{L^{\infty}([-1,1]^2)} \\
    &\leq \frac{4\mathsf{w}}{\pi}(2\,\texttt{\#}\Lambda_0)^{2\gamma(\mathsf{w})} + \sum_{k=1}^{n-1} (k+1)^{\gamma(\mathsf{w})}(2\,\texttt{\#}\Lambda_k)^{2\gamma(\mathsf{w})}. 
\end{align}
As with the case $d=1$, we aim to dominate the right-hand sides of \eqref{eq:induction1}, \eqref{eq:induction1d3} using the quantity $(\sum_{k=0}^{n-1} (2k+1)\,(2\,\texttt{\#}\Lambda_k)^2)^{\gamma(\mathsf{w})}$. For brevity, we denote $A_k := 2\,\texttt{\#}\Lambda_k$. Then
\begin{align}
    \nonumber &\Big(\sum_{k=0}^{n-1} (2k+1)\,A_k^2\Big)^{\gamma(\mathsf{w})} \\
    \nonumber &= \Big(\sum_{k=1}^{n-1} (2k+1)\,A_k^2\Big)^{\gamma(\mathsf{w})} + \sum_{l=1}^{\gamma(\mathsf{w})} {\gamma(\mathsf{w}) \choose l} \Big(\sum_{k=1}^{n-1} (2k+1)\,A_k^2\Big)^{\gamma(\mathsf{w})-l} A_0^{2l} \\
    \label{eq:induction2} &\geq \sum_{k=1}^{n-1} (2k+1)^{\gamma(\mathsf{w})} A_k^{2\gamma(\mathsf{w})} + \sum_{l=1}^{\gamma(\mathsf{w})} {\gamma(\mathsf{w}) \choose l} \Big(\sum_{k=1}^{n-1} (2k+1)\,A_k^2\Big)^{\gamma(\mathsf{w})-l} A_0^{2l}.
\end{align}
An application of Proposition~\ref{prop:relating} gives
\begin{equation*}
    \Big(\sum_{k=1}^{n-1} (2k+1)\,A_k^2\Big)^{\gamma(\mathsf{w})-l} \geq \frac{A_0^{2(\gamma(\mathsf{w})-l)}}{n^{\gamma(\mathsf{w})-l}}.
\end{equation*}
Therefore
\begin{align}
    \nonumber \sum_{l=1}^{\gamma(\mathsf{w})} {\gamma(\mathsf{w}) \choose l} \Big(\sum_{k=1}^{n-1} (2k+1)\,A_k^2\Big)^{\gamma(\mathsf{w})-l} A_0^{2l} &\geq \frac{A_0^{2\gamma(\mathsf{w})}}{n^{\gamma(\mathsf{w})}} \sum_{l=1}^{\gamma(\mathsf{w})} {\gamma(\mathsf{w}) \choose l} n^{l} \\
    %\nonumber &= A_0^{2\gamma(\mathsf{w})} \frac{(n+1)^{\gamma(\mathsf{w})}-1}{n^{\gamma(\mathsf{w})}} \\
    \nonumber &= A_0^{2\gamma(\mathsf{w})}\big(1+ n + n^2 + \dots + n^{\gamma(\mathsf{w})-1}\big)\\
    \nonumber &\geq A_0^{2\gamma(\mathsf{w})}2^{\gamma(\mathsf{w})-1} \\
    \label{eq:induction3} &\geq A_0^{2\gamma(\mathsf{w})} \frac{4\mathsf{w}}{\pi},
\end{align}
where the last inequality is due to \eqref{eq:powerscompared}.
Putting back $A_k = 2\,\texttt{\#}\Lambda_k$, and combining \eqref{eq:induction2}, \eqref{eq:induction3} together with \eqref{eq:induction1} or \eqref{eq:induction1d3} yields
\begin{align} \label{eq:induction4}
    \nonumber \kappa(\mathcal{S}_{\Lambda}) &\leq \frac{4\mathsf{w}}{\pi}(2\,\texttt{\#}\Lambda_0)^{2\gamma(\mathsf{w})} + \sum_{k=1}^{n-1} (k+1)^{\gamma(\mathsf{w})}(2\,\texttt{\#}\Lambda_k)^{2\gamma(\mathsf{w})} \\
    &\leq 4^{\gamma(\mathsf{w})} \Big(\sum_{k=0}^{n-1} (2k+1)\,(\texttt{\#}\Lambda_k)^2\Big)^{\gamma(\mathsf{w})}.
\end{align}
Next, by utilizing the monotonicity property \eqref{nesting}
\begin{equation*}
    k(\texttt{\#}\Lambda_k)^2 = \sum_{j=1}^{k} (\texttt{\#}\Lambda_k)^2 \leq \sum_{j=0}^{k-1} (\texttt{\#}\Lambda_k) (\texttt{\#}\Lambda_j),
\end{equation*}
we can conclude from \eqref{eq:induction4} that
\begin{align*}
    \kappa(\mathcal{S}_{\Lambda}) 
    &\leq 4^{\gamma(\mathsf{w})} \Big(\sum_{k=0}^{n-1} (2k+1)\,(\texttt{\#}\Lambda_k)^2\Big)^{\gamma(\mathsf{w})} \\
    &\leq 4^{\gamma(\mathsf{w})} \Big(\sum_{k=0}^{n-1}(\texttt{\#}\Lambda_k)^2 + 2\sum_{j<k} (\texttt{\#}\Lambda_k)(\texttt{\#}\Lambda_j) \Big)^{\gamma(\mathsf{w})} \\
    &= 4^{\gamma(\mathsf{w})} \Big(\sum_{k=0}^{n-1} \texttt{\#}\Lambda_k \Big)^{2\gamma(\mathsf{w})}
    = (2\,\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})},
\end{align*}
which is \eqref{Theta} for $d=2$ when $n\geq 2$ and for $d=3$ when $n\geq 26$. 

It remains to check the validity of \eqref{Theta} when $d=2$ and $n=1$. In this case, $\Lambda = \{(0,0)\}$. Then performing a similar calculation as in \eqref{eq:s1} yields
\begin{equation*}
    \kappa(\mathcal{S}_{\Lambda}) = \|\varphi_{\mathsf{w},(0,0)}\|^2_{L^{\infty}([-1,1]^2)} = \|\varphi_{\mathsf{w},0}\|^4_{L^{\infty}([-1,1])} \leq \frac{16\mathsf{w}^2}{\pi^2} \leq 2^{2\gamma(\mathsf{w})} = (2\,\texttt{\#}\Lambda)^{2\gamma(\mathsf{w})}.
\end{equation*}
With this, we close the induction loop and conclude \eqref{Theta} for $d=1,2$ for all $n\in\N$ and for $d=3$ for all $n\geq 26$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:relating}] 
We rely on the following cardinality estimate of the hyperbolic cross. 


\begin{lemma}[{See \citep[Theorem 3.5]{chernov2016new} and \citep[Lemma~B.3]{adcock2022sparse}}]
\label{lem:HCsize}
Let $d,n\in\N$, and suppose $\Lambda^{\rm HC}_{n-1}$ is the $d$-dimensional hyperbolic cross of order $n$. Then there exists $n_{\star}(d)\in\mathbb{N}$, depending only on $d$, such that if $n\geq n_{\star}(d)$, 
\begin{equation*}
    \texttt{\#} \Lambda^{\rm HC}_{n-1} < \frac{n(\log n + d\log 2)^{d-1}}{(d-1)!}.
\end{equation*}
\end{lemma}

An estimate on $n_{\star}(d)$ can be derived, for example, from the calculations presented in the proof of \citep[Theorem~2.4]{chernov2016new}. In particular, we can take $n_{\star}(2)=1$.    
We proceed to prove Proposition~\ref{prop:relating} for $d=2$. 
In this case, $\Lambda=\Lambda^{\rm HC}_{n-1}$ is a two-dimensional hyperbolic cross where each slice $\Lambda_k$, for $k=0,\dots,n-1$, is a one-dimensional hyperbolic cross.
It also follows from \eqref{eq:Lambda0HC} that $\texttt{\#}\Lambda_0 = n$.
Therefore, proving the proposition amounts to proving
\begin{equation*} 
    \sum_{k=1}^{n-1} (2k+1)\,(\texttt{\#}\Lambda_k)^2 \geq n. 
\end{equation*}
However, it can be seen that the right-hand side term above is at least
\begin{equation*}
    \sum_{k=1}^{n-1} (2k+1)\,(\texttt{\#}\Lambda_k)^2 \geq \sum_{k=1}^{n-1} (2k+1) = n(n-1) + n-1 = n^2-1\geq n, 
\end{equation*}
when $n\geq 2$, and we are done with $d=2$. 

Turning to $d=3$, we obtain from the Cauchy-Schwarz inequality that
\begin{eqnarray} \label{L2embed}
    \sum_{k=1}^{n-1} (\texttt{\#}\Lambda_k)^2 (2k+1) \geq \frac{1}{n} \big(\sum_{k=1}^{n-1} (\texttt{\#}\Lambda_k) \sqrt{2k+1} \big)^2.
\end{eqnarray}
Note that in this case, each $\Lambda_k$ is a two-dimensional hyperbolic cross.
Further, we claim:
\begin{claim} \label{baseclaim}
    $\sum_{k=1}^{n-1} (\texttt{\#}\Lambda_k) \sqrt{2k+1} \geq \texttt{\#}\Lambda_0$, whenever $n\geq 26$.
\end{claim}
We demonstrate this. 
First, since $\texttt{\#}\Lambda_{k} \geq 1$ for all $k \leq n-1$, we have
\begin{equation} \label{baseclaim1}
    \sum_{k=1}^{n-1} (\texttt{\#}\Lambda_{k}) \sqrt{2k+1} \geq \sum_{k=1}^{n-1} \sqrt{2k+1} \geq   \int_0^{n-1} (2t+1)^{1/2}\,{\rm d}t \geq  \frac{(2n-1)^{3/2}-1}{3}.
\end{equation}
Second, from Lemma~\ref{lem:HCsize} and the fact that $n_{\star}(2)=1$, we have $\texttt{\#}\Lambda_0 \leq n(\log n + 2\log 2)$, whenever $n\geq 1$.
Third,
\begin{equation} \label{baseclaim2}
    \frac{(2n-1)^{3/2}-1}{3} \geq n(\log n + 2\log 2),
\end{equation}
as soon as $n\geq 26$. 
It is now apparent that Claim~\ref{baseclaim} follows from \eqref{baseclaim1}, \eqref{baseclaim2}. 

Combining \eqref{L2embed} and Claim~\ref{baseclaim}, we deduce for $d=3$ and $n\geq 26$,
\begin{equation*}
    \sum_{k=1}^{n-1} (\texttt{\#}\Lambda_k)^2 (2k+1) \geq \frac{1}{n} \big(\sum_{k=1}^{n-1} (\texttt{\#}\Lambda_k) \sqrt{2k+1} \big)^2 \geq \frac{(\texttt{\#}\Lambda_0)^2}{n},
\end{equation*}
which is the wanted conclusion. 
\end{proof}

\subsection{Preliminary results for Theorem~\ref{thm:ApproxSpan}} \label{sec:prelimformainthm2}



The proof of Theorem~\ref{thm:ApproxSpan}, is based on a three-step approximation approach using NNs: 
\begin{itemize}
    \item {\bf Step 1:} approximate Legendre polynomials using NNs;
    \item {\bf Step 2:} construct Slepian basis functions through linear combinations of Legendre polynomials and subsequently of the NN approximations derived in {\bf Step 1};
    \item {\bf Step 3:} approximate frequency-localized functions by linearly combining the appropriate Slepian basis functions, subsequently expressed in terms of the NN approximations obtained in {\bf Step 2}.
\end{itemize}
The culmination of {\bf Step 1} and {\bf Step 2} is Proposition~\ref{prop:ApproxSlepian}, the main preliminary result of this subsection, which will be presented shortly. 
The outcome of {\bf Step 3} will be our second main theorem, Theorem~\ref{thm:ApproxSpan}, with its proof provided in Subsection~\ref{sec:ProofTheo3}.
The initial steps rely on the fact that Legendre polynomials can effectively approximate Slepian basis functions.
Thus, we begin with an overview of Legendre polynomials.

For each $k \in \N_0$, the Legendre polynomial $\widetilde{P}_k$ in one dimension is defined as the solution to the Sturm-Liouville eigenvalue problem
\begin{eqnarray*}
    (1-x^2)\dfrac{d^2}{dx^2}\widetilde{P}_k - 2x \dfrac{d}{dx}\widetilde{P}_k+k(k+1)\widetilde{P}_k = 0, \qquad \forall x\in [-1,1], 
\end{eqnarray*}
such that $\max_{x\in [-1,1]} |\widetilde{P}_k(x)|=1$.
The family $\{\widetilde{P}_k\}_{k\in\mathbb{N}_0}$ forms an orthogonal basis in $L^2_{\bf u}([-1,1])$. Moreover, as shown in, e.g., \citep[Subsection~2.2.2]{adcock2022sparse}, 
\begin{equation*} 
    \|\widetilde{P}_k \|_{L^{2}_{ {\bf u}}([-1,1])} = 
    \dfrac{1}{\sqrt{2k+1}}.
\end{equation*} 
From this, we obtain a normalized\footnote{Recall here that ``normalized'' means having the $L^2_{\bf u}([-1,1])$-norm of $1$.} Legendre polynomial $P_k$ of degree $k$ by setting 
\begin{equation} \label{eq:NormLegendr}
    P_{k} := \sqrt{2k+1}  \widetilde{P}_k.
\end{equation} 
Similar to the approach used to build the $d$-dimensional Slepian functions in Section~\ref{sec:background}, for $d \geq 2$ and for each index $\vec{\nu}=(\nu_1,\dots,\nu_d)\in\N^d_0$ we define 
\begin{equation} \label{eq:highLeg}
    P_{\vec{\nu}} := P_{\nu_1} \otimes \cdots \otimes P_{\nu_d},
\end{equation}
where each $P_{\nu_j}$ is a normalized Legendre polynomial in one dimension. 
Evidently, $\{P_{\vec{\nu}}\}_{\vec{\nu}\in\N_0^d}$ forms an orthonormal basis of $L^2_{{\bf u}}([-1,1]^d)$. 

It is known that the normalized Legendre polynomials $\{P_k\}_{k \in \mathbb{N}_0}$ in one dimension can be effectively approximated by ReLU neural networks. 
A concrete example is provided in \citep[Proposition 2.11]{opschoor2022exponential} (see also Proposition~\ref{prop:AppLeg1D}). 
In the case of $d$ dimensions, we frequently rely on the tensorization definition \eqref{eq:highLeg} and the fundamental network calculus operations outlined in Appendix~\ref{appx:basicnetworks}, such as concatenation, parallelization, and product approximation, to develop approximating NNs for normalized Legendre polynomials. 
Although these constructions are well-established in the literature, we include them for completeness and convenient reference as the reader progresses through the material.

We are now ready to present Proposition~\ref{prop:ApproxSlepian}. 

%In this subsection, we provide a foundational result for Theorem~\ref{thm:ApproxSpan}, which addresses the approximability of normalized Slepian basis functions via neural networks.

\begin{proposition} \label{prop:ApproxSlepian} 
Let $\varepsilon\in (0,1)$, and $\mathsf{w}\geq 1$.
Let $\Lambda=\Lambda_{n-1}^{\rm HC}$ be the $d$-dimensional hyperbolic cross of order $n\geq 2$, with $d=1,2,3$. 
Let $B(d,n)$, $M(d,n)$ be given \eqref{eq:B}, \eqref{eq:M}, respectively.
Then there exist a universal constant $C>0$ and $N_{\star}=N_{\star}(\Lambda,\mathsf{w},\varepsilon)\in\N$ for which the following holds.
For every Slepian basis function $\varphi_{\mathsf{w}, \vec{\nu}}$ where $\vec{\nu}\in\Lambda_{n-1}^{\rm HC}$, there exists an NN $\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}$ such that 
\begin{equation*}
     \| \varphi_{\mathsf{w},\vec{\nu}} - {\rm R}(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) \|_{L^{\infty}([-1, 1]^d)}
    \leq B(d,n)\varepsilon,
\end{equation*}
with 
\begin{align*}   
    L(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) 
    &
    \leq C \big((1+\log_2N_{\star})(N_{\star}+\log_2(N_{\star}/\varepsilon)) + (1+ \log_2 (M(d,n)/\varepsilon)\big),\\
    W(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) 
    & 
    \leq C\big((N_{\star}^2 + N_{\star})(N_{\star} + \log_2(N_{\star}/\varepsilon)) + (1+ \log_2 (M(d,n)/\varepsilon)\big).
\end{align*}
\end{proposition}

To continue with the proof, we need the following two supporting results. The first, Lemma~\ref{lem:ApproxOfSlep1D}, establishes that every $\varphi_{\mathsf{w}, k}$ can be well-approximated by a linear combination of normalized Legendre polynomials, with a controllable number of terms.
The second, Lemma~\ref{lem:AppSumOfLeg}, establishes that any such linear combination can be efficiently modeled by an NN of manageable length and complexity.

\begin{lemma}\label{lem:ApproxOfSlep1D}
Let $\mathsf{w}\geq 1$. 
Let $\Lambda\subset\mathbb{N}$ be a finite set. 
For $k\in\Lambda$, let $\varphi_{\mathsf{w},k}$ be the $k$th Slepian basis function associated with the eigenvalue $\mu_{\mathsf{w},k}$ \eqref{eq:muj}. 
For every $\varepsilon\in (0,1)$, let
\begin{equation} \label{eqdef:Nstar}
    N_{\star} := \Big \lceil  \max \Big\{2\lfloor e  \mathsf{w} \rfloor +1, \frac{\log(3/(c_{\star}\varepsilon))}{\log (3/2)} \Big\} \Big\rceil,
\end{equation}
where $c_{\star} := \min_{k\in\Lambda} \mu_{\mathsf{w},k}$.
For every normalized Legendre polynomial $P_j$ of degree $j\in\mathbb{N}$, let
\begin{equation} \label{eqdef:betacoef}
    \beta_j^k := \frac{1}{2} \int_{-1}^1 \varphi_{\mathsf{w},k}(x) \overline{P_j}(x) \, {\rm d}x.
\end{equation}
Then for every $N\geq N_{\star}$ and for every $k \in\Lambda$, it holds that
\begin{equation}\label{eq:ApprSlepianUnif}
    \Big\| \varphi_{\mathsf{w}, k} - \sum_{j=0}^{N} \beta_j^k P_j \Big\|_{L^{\infty}([-1,1])} \leq \varepsilon.
\end{equation}
\end{lemma}

\begin{lemma} \label{lem:AppSumOfLeg}
Let $\varepsilon \in (0,1)$, and $N \in \N$. Let $\{\beta_j\}_{j=0}^N $ be a set of real numbers such that $|\beta_j| \leq c_0$ for some $c_0>0$.
Let $P_j$ be the normalized Legendre polynomial of degree $j$. 
Then there exists an NN $\widetilde{\Phi}_{\varepsilon, N}$ such that
\begin{equation*}
    \Big\| \sum_{j=0}^N \beta_j P_j -  {\rm R}( \widetilde{\Phi}_{\varepsilon, N}) \Big\|_{L^{\infty}([-1,1])} \leq c_0 \varepsilon.
\end{equation*}
Moreover, %there exists a universal constant $C>0$ such that
\begin{align*}
    L (\widetilde{\Phi}_{\varepsilon, N}) &\leq  C(1+ \log_2 N)(N + \log_2(N/\varepsilon) ), \\
    W ( \widetilde{\Phi}_{\varepsilon, N} ) &\leq C (N^2+N)(N+\log_2(N/\varepsilon)),
\end{align*}
for some universal constant $C>0$.
\end{lemma}

The proofs of Lemmas~\ref{lem:ApproxOfSlep1D},~\ref{lem:AppSumOfLeg} are given at the end of this subsection, in the order in which they are presented.


\begin{proof}[Proof of Proposition~\ref{prop:ApproxSlepian}]
We begin with the case $\Lambda=\Lambda^{\rm HC}_{n-1}\subset\mathbb{N}$, i.e. when $\{\varphi_{\mathsf{w}, k}\}_{k \in \Lambda}$ is a finite subset of the Slepian orthonormal basis of $L^2_{\bf u}([-1,1])$. 
Let $N_{\star}$ be as given in \eqref{eqdef:Nstar}, with $\varepsilon\in (0,1)$.
Note, when $\Lambda=\Lambda^{\rm HC}_{n-1}=\{0,\dots,n-1\}$,
\begin{equation} \label{eq:cstarreq}
    c_{\star} = \min_{k\in\Lambda} \mu_{\mathsf{w},k} = \mu_{\mathsf{w},n-1}.
\end{equation}
By Lemma~\ref{lem:ApproxOfSlep1D}, \eqref{eq:ApprSlepianUnif} holds with $N=N_{\star}$. 
To apply Lemma~\ref{lem:AppSumOfLeg} to our context, we need an estimate for $\max_{k \in \Lambda} \max_{j = 0, \dots, N_{\star}} |\beta_j^k|$, which will act as the value $c_0$.
Since $n\geq 2$, we obtain from Proposition~\ref{prop:gammaw} that 
\begin{equation} \label{eq:ngammaw}
    \|\varphi_{\mathsf{w},k}\|_{L^{\infty}([-1,1])} \leq \max\Big\{\frac{4\mathsf{w}}{\pi}, (k+1)^{\gamma(\mathsf{w})}\Big\} \leq n^{\gamma(\mathsf{w})},
\end{equation}
and subsequently, from \eqref{eqdef:betacoef}, 
\begin{multline*} 
    |\beta_j^k| 
    = \frac{1}{2} \Big|\int_{-1}^1 \varphi_{\mathsf{w},k}(x) \overline{P_j}(x) \, {\rm d}x \Big|
    \leq \|P_j\|_{L^2_{\bf u}([-1,1])} \|\varphi_{\mathsf{w},k}\|_{L^{\infty}_{\bf u}([-1,1])} \\
    = \|P_j\|_{L^2_{\bf u}([-1,1])} \|\varphi_{\mathsf{w},k}\|_{L^{\infty}([-1,1])}
    \leq n^{\gamma(\mathsf{w})}.
\end{multline*}
Thus, Lemma \ref{lem:AppSumOfLeg} ensures the existence of NN $\widetilde{\Phi}_{\varepsilon,N_{\star}}^k$ satisfying 
\begin{equation} \label{eq:NNuniformLambda}
    \Big\| \sum_{j=0}^{N_{\star}} \beta_j^k P_j -  {\rm R} (\widetilde{\Phi}_{\varepsilon,N_{\star}}^k) \Big\|_{L^{\infty}([-1,1])} \leq n^{\gamma(\mathsf{w})}\varepsilon,
\end{equation} 
with the following respective numbers of layers and weights
\begin{align}
    \label{eq:layers1d}
    L (\widetilde{\Phi}_{\varepsilon,N_{\star}}^k) &\leq  C(1+\log_2 N_{\star})(N_{\star} + \log_2(N_{\star}/\varepsilon)), \\
    \label{eq:weight1d}
    W (\widetilde{\Phi}_{\varepsilon,N_{\star}}^k) &\leq C(N_{\star}^2+N_{\star})(N_{\star}+\log_2(N_{\star}/\varepsilon)).
\end{align}
By setting $N = N_{\star}$ in \eqref{eq:ApprSlepianUnif} and combining it with \eqref{eq:NNuniformLambda}, we derive
\begin{equation} \label{eq:NNfor1d}
    \Big \| \varphi_{\mathsf{w}, k } - {\rm R} (\widetilde{\Phi}_{\varepsilon,N_{\star}}^k) \Big \|_{L^{\infty}([-1,1])} 
    \leq \big(1+ n^{\gamma(\mathsf{w})}\big)\varepsilon, 
\end{equation} 
and we conclude for the case $\Lambda^{\rm HC}_{n-1}\subset\mathbb{N}$ by letting $\Psi_{\varepsilon,N_{\star}}^k=\widetilde{\Phi}_{\varepsilon,N_{\star}}^k$.

For the case $\Lambda=\Lambda^{\rm HC}_{n-1}\subset\mathbb{N}^2$, we write, $\varphi_{\mathsf{w}, \vec{\nu}} = \varphi_{\mathsf{w}, \nu_1} \otimes \varphi_{\mathsf{w}, \nu_2}$, for each $\vec{\nu}=(\nu_1, \nu_2)\in\Lambda^{\rm HC}_{n-1}$. 
Note, $\nu_j\in \{0,\dots,n-1\}$.
Let $N_{\star}$ remain as previously defined with the choice $c_{\star}$ in \eqref{eq:cstarreq}.
Then by \eqref{eq:NNfor1d}, for $j=1,2$, there exists an NN $\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_j}$, such that 
\begin{equation} \label{eq:diffdimone}
    \| \varphi_{\mathsf{w}, \nu_j} - {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_j}) \|_{L^{\infty}([-1,1])} \leq (1+n^{\gamma(\mathsf{w})})\varepsilon.
\end{equation}
This, together with \eqref{eq:ngammaw}, implies
\begin{equation} \label{eq:NNinftydimone}
    \| {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_j}) \|_{L^{\infty}([-1,1])} 
    \leq 
    \|\varphi_{\mathsf{w}, \nu_j}\|_{L^{\infty}([-1,1])} + (1+n^{\gamma(\mathsf{w})})\varepsilon \leq 2n^{\gamma(\mathsf{w})} + 1.
\end{equation}
Combining \eqref{eq:ngammaw}, \eqref{eq:diffdimone}, \eqref{eq:NNinftydimone}, we derive for $\vec{\nu}=(\nu_1, \nu_2)\in\Lambda^{\rm HC}_{n-1}$ that
\begin{equation} \label{eq:splittingsum}
    \| \varphi_{\mathsf{w}, \nu_1}\otimes\varphi_{\mathsf{w},\nu_2}  - {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1}) \otimes {\rm R} (\widetilde{\Phi}_{\varepsilon, N_{\star}}^{\nu_2}) \|_{L^{\infty}([-1,1]^2)} 
    \leq a + b,
\end{equation}
where
\begin{align*}
    a &= \| \varphi_{\mathsf{w}, \nu_1} \otimes \varphi_{\mathsf{w}, \nu_2} - \varphi_{\mathsf{w}, \nu_1}\otimes {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_2}) \|_{L^{\infty}([-1,1]^2)} \\
    & \leq \| \varphi_{\mathsf{w}, \nu_1} \|_{L^{\infty}([-1,1])} \| \varphi_{\mathsf{w}, \nu_2}  - {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_2}) \|_{L^{\infty}([-1,1])} 
    \leq n^{\gamma(\mathsf{w})}(1+n^{\gamma(\mathsf{w})})\varepsilon,
\end{align*}
and 
\begin{align*}
    b &= \| \varphi_{\mathsf{w}, \nu_1}\otimes {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_2}) - {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1}) \otimes {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_2}) 
    \|_{L^{\infty}([-1,1]^2)} \\
    &\leq \| {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_2}) \|_{L^{\infty}([-1,1])}  
    \| \varphi_{\mathsf{w}, \nu_1} - {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1})
    \|_{L^{\infty}([-1,1])} 
    \leq (2n^{\gamma(\mathsf{w})} + 1)(1+n^{\gamma(\mathsf{w})})\varepsilon.
\end{align*}
Therefore, \eqref{eq:splittingsum} yields
\begin{equation}\label{eq:Esti1}
    \| \varphi_{\mathsf{w}, \nu_1} \otimes \varphi_{\mathsf{w}, \nu_2} - {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1}) \otimes {\rm R} (\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_2}) \|_{L^{\infty}([-1,1])} 
    \leq (3n^{2\gamma(\mathsf{w})} + 4n^{\gamma(\mathsf{w})} +1) \varepsilon.
\end{equation}
%for all $\vec{\nu}=(\nu_1,\nu_2)\in\Lambda^{\rm HC}_{n-1}$. 
Next, we utilize the network construction $\tilde{\times}_{\varepsilon, B}$ in  Proposition \ref{prop:Multiplication}, with $B=2n^{\gamma(\mathsf{w})}+1$ (obtained from \eqref{eq:NNinftydimone}), as well as the concatenation and parallelization operations, denoted as $\odot$ and ${\rm P}$, from Definitions~\ref{def:concat} and \ref{def:Parallelization} respectively, to construct
\begin{equation*}
    \Psi_{\varepsilon,N_{\star}}^{\vec{\nu}} :=
    \tilde{\times}_{\varepsilon, B} \odot {\rm P}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1},\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_2}).
\end{equation*}
It follows the construction that
\begin{equation} \label{eq:ProdMat}
    \| {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1}) \otimes {\rm R}(\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_2}) - {\rm R}(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) \|_{L^{\infty}([-1,1]^2)} 
    \leq \varepsilon. 
\end{equation}
Combining \eqref{eq:Esti1}, \eqref{eq:ProdMat}, we obtain
\begin{equation} \label{eq:Approx2D}
    \| \varphi_{\mathsf{w}, \nu_1} \otimes \varphi_{\mathsf{w}, \nu_2}  - {\rm R}(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) \|_{L^{\infty}([-1,1]^2)}
    \leq (3n^{2\gamma(\mathsf{w})} + 4n^{\gamma(\mathsf{w})} +2)\varepsilon.
\end{equation}
Furthermore, by applying \eqref{eq:layers1d}, \eqref{eq:weight1d}, Proposition~\ref{prop:Multiplication}, and Remark~\ref{rem:NNdefsprops}, we conclude 
\begin{align*}
    L(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) &\leq C\big((1+\log_2N_{\star})(N_{\star}+\log_2(N_{\star}/\varepsilon)) + (1+ \log_2 ((2n^{\gamma(\mathsf{w})}+1)/\varepsilon)\big) \\
    W(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) &\leq C\big((N_{\star}^2 + N_{\star})(N_{\star} + \log_2(N_{\star}/\varepsilon)) + (1+ \log_2 ((2n^{\gamma(\mathsf{w})}+1)/\varepsilon)\big),
\end{align*}
and thus, we are done with the case $\Lambda^{\rm HC}_{n-1}\subset\mathbb{N}^2$.

For the case $d=3$, let $\vec{\nu} = (\nu_1, \nu_2, \nu_3) \in \Lambda_{n-1}^{\rm HC}$ and denote $\varphi_{\mathsf{w}, \vec{\nu}} = \varphi_{\mathsf{w}, \nu_1}\otimes \varphi_{\mathsf{w}, \nu_2}\otimes \varphi_{\mathsf{w}, \nu_3}$.
By \eqref{eq:NNfor1d}, there exists an NN  $\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1}$ such that
\begin{equation} \label{eq:NNfor1drecall}
    \| \varphi_{\mathsf{w}, \nu_1} - {\rm R} (\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1}) \|_{L^{\infty}([-1,1])} \leq (1+n^{\gamma(\mathsf{w})})\varepsilon.
\end{equation}
Denote $\vec{\nu}^*= (\nu_2, \nu_3)$.
Then by \eqref{eq:Approx2D}, there also exists an NN $\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}^*}$ satisfying
\begin{equation} \label{eq:diffdimtwo}
    \| \varphi_{\mathsf{w}, \nu_2} \otimes \varphi_{\mathsf{w}, \nu_3}  - {\rm R}(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}^*}) \|_{L^{\infty}([-1,1]^2)} \leq (3n^{2\gamma(\mathsf{w})} + 4n^{\gamma(\mathsf{w})} +2)\varepsilon.
 \end{equation}
Altogether, using \eqref{eq:ngammaw}, \eqref{eq:NNinftydimone}, \eqref{eq:NNfor1drecall}, \eqref{eq:diffdimtwo}, via similar arguments to those leading to \eqref{eq:Esti1}, we conclude 
\begin{multline} \label{eq:NNdiffdimtwo}
    \| \varphi_{\mathsf{w}, \nu_1} \otimes \varphi_{\mathsf{w}, \nu_2}\otimes \varphi_{\mathsf{w}, \nu_3}  - {\rm R} (\widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1}) \otimes {\rm R}(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}^*}) \|_{L^{\infty}([-1,1]^3)}
    \\
    \leq (7n^{3\gamma(\mathsf{w})} + 12n^{2\gamma(\mathsf{w})} + 8n^{\gamma(\mathsf{w})} +2)\varepsilon.
\end{multline}
Let $\Phi_I$ be the NN formulation in Proposition \ref{prop:ApproxIdent} (with $d=1$) such that $\Psi_{\varepsilon,N_{\star}}^{\nu_1}:= \Phi_I \odot \widetilde{\Phi}_{\varepsilon,N_{\star}}^{\nu_1}$ and $\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}^*}$ have the same number of layers. 
We construct
\begin{equation*}
    \Psi_{\varepsilon,N_{\star}}^{\vec{\nu}} := \tilde{\times}_{\varepsilon, B'}\odot {\rm P}(\Psi_{\varepsilon,N_{\star}}^{\nu_1}, \Psi_{\varepsilon,N_{\star}}^{\vec{\nu}^*})
\end{equation*}
where $B'=4n^{2\gamma(\mathsf{w})} + 4n^{\gamma(\mathsf{w})} +2$.
Then from Proposition~\ref{prop:Multiplication} again and \eqref{eq:NNdiffdimtwo}, 
\begin{equation*}\label{eq:Approx3D}
    \| \varphi_{\mathsf{w},\nu_1} \otimes \varphi_{\mathsf{w},\nu_2} \otimes \varphi_{\mathsf{w},\nu_3}  - {\rm R} (\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) \|_{L^{\infty}([-1,1]^3)} \leq (7n^{3\gamma(\mathsf{w})} + 12n^{2\gamma(\mathsf{w})} + 8n^{\gamma(\mathsf{w})} +3)\varepsilon.
\end{equation*}
Since it is readily verified from the construction of $\Psi_{\varepsilon,N_{\star}}^{\nu_1}$ and \eqref{eq:ngammaw}, \eqref{eq:NNfor1drecall}, \eqref{eq:diffdimtwo} that
\begin{equation*}
    \max\big\{|{\rm R}(\Psi_{\varepsilon,N_{\star}}^{\nu_1})|, |{\rm R}(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}^*})|\} \leq 4n^{2\gamma(\mathsf{w})} + 4n^{\gamma(\mathsf{w})} +2,
\end{equation*}
employing similar reasoning and computations analogous to those before, delivers
\begin{align*}
    L(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) &\leq C\big((1+\log_2N_{\star})(N_{\star}+\log_2(N_{\star}/\varepsilon)) + (1+ \log_2 ((4n^{2\gamma(\mathsf{w})} + 4n^{\gamma(\mathsf{w})} +2)/\varepsilon)\big) \\
    W(\Psi_{\varepsilon,N_{\star}}^{\vec{\nu}}) &\leq C\big((N_{\star}^2 + N_{\star})(N_{\star} + \log_2(N_{\star}/\varepsilon)) + (1+ \log_2 ((4n^{2\gamma(\mathsf{w})} + 4n^{\gamma(\mathsf{w})} +2)/\varepsilon)\big).
\end{align*}
The proof is now complete.
\end{proof}



\begin{proof}[Proof of Lemma \ref{lem:ApproxOfSlep1D}]
The set of all normalized Legendre polynomials is an orthonormal basis for $L_{\bf u}^2([-1,1])$. 
Therefore, for every Slepian basis function $\varphi_{\mathsf{w}, k}$, there exist $\{\beta_j^k\}_{j\in\mathbb{N}_0} \in \ell^2(\mathbb{N}_0)$ satisfying \eqref{eqdef:betacoef} such that
\begin{equation}\label{eq:L2ConvSlep}
    \lim_{N\to\infty} \Big\| \sum_{j=0}^N \beta_j^k P_j - \varphi_{\mathsf{w}, k} \Big\|_{L^2_{\bf u}([-1,1])} = 0;
\end{equation} 
see also \citep[Equation (2.47)]{osipov2013prolate}.
Since $L^{\infty}([-1,1])$ is a Banach space, the absolute convergence of the series $\sum_{j=0}^{\infty} \beta_j^k P_j$, i.e.
\begin{equation} \label{eq:tailconvergence}
    \lim_{N\to\infty} \sum_{j=N+1}^{\infty} |\beta_j^k| \|P_j\|_{L^{\infty}([-1,1])} = 0,
\end{equation}
will guarantee the existence of the limit $\sum_{j=0}^{\infty} \beta_j^k P_j$ as a function in $L^{\infty}([-1,1])$ \citep[Theorem~5.1]{folland1999real}.
Once held, \eqref{eq:tailconvergence} together with \eqref{eq:L2ConvSlep}, and the continuity of $\sum_{j=0}^N \beta_j^k P_j$ and $\varphi_{\mathsf{w},k}$, yields
\begin{equation} \label{eq:Linftylimit}
    \sum_{j=0}^{\infty} \beta_j^k P_j = \varphi_{\mathsf{w}, k},
\end{equation}
on $[-1,1]$.
We proceed to demonstrate \eqref{eq:tailconvergence} for all $k\in\Lambda$. 
By \citep[Theorem 7.2]{osipov2013prolate}, if $j \geq 2(\lfloor e \mathsf{w} \rfloor +1)$, then 
\begin{equation}\label{eq:beta}
    | \beta_j^k | 
    = \frac{1}{2} \Big| \int_{-1}^1 \varphi_{\mathsf{w},k}(x) \overline{P_j}(x) \, {\rm d}x \Big| \leq \dfrac{1}{\mu_{\mathsf{w}, k}} \Big( \dfrac{1}{2}\Big)^j 
    \leq \frac{1}{c_{\star}} \Big( \dfrac{1}{2}\Big)^j.
\end{equation} 
Using \eqref{eq:NormLegendr}, \eqref{eq:beta}, we find for $N\geq 2\lfloor e \mathsf{w} \rfloor +1$
\begin{align} \label{eq:TailBound1}
    \nonumber \sum_{j=N+1}^{\infty} |\beta_j^k| \| P_j \|_{L^{\infty}([-1,1])} 
    &\leq \dfrac{1}{c_{\star}} \sum_{j=N+1}^{\infty} \dfrac{\sqrt{2j+1}}{2^j}   \\
    \nonumber &\leq \dfrac{1}{c_{\star}} \sum_{j=N}^{\infty} \dfrac{\sqrt{j}}{2^j}\\
    \nonumber &\leq \dfrac{1}{c_{\star}} \sum_{j=N}^{\infty}  \Big(\dfrac{2}{3} \Big)^j \\
    &= \frac{3}{c_{\star}} \Big(\dfrac{2}{3} \Big)^N,
\end{align}
which is \eqref{eq:tailconvergence}, valid for all $k\in\Lambda$.
Hence, \eqref{eq:Linftylimit} holds, and we conclude from \eqref{eq:TailBound1} that, 
\begin{equation} \label{eq:TailBound2}
    \Big\| \sum_{j=0}^N \beta_j^k P_j - \varphi_{\mathsf{w}, k} \Big\|_{L^{\infty}([-1,1])} 
    = \Big \| \sum_{j=N+1}^{\infty}\beta_j^k P_j \Big \|_{L^{\infty}([-1,1])} \leq \frac{3}{c_{\star}} \Big(\dfrac{2}{3} \Big)^N,
\end{equation}
whenever $N\geq 2\lfloor e \mathsf{w} \rfloor -1$. 
For the final step, we require in addition that 
\begin{equation*}
    N\geq \max\Big\{1, \frac{\log(3/(c_{\star}\varepsilon))}{\log (3/2)}\Big\}
\end{equation*}
in \eqref{eq:TailBound2}, to get \eqref{eq:ApprSlepianUnif}, as desired. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{proof}[Proof of Lemma~\ref{lem:AppSumOfLeg}]
Let $j=0,\dots,N$. 
A result derived from \citep[Proposition 2.11]{opschoor2022exponential}, and stated as Proposition~\ref{prop:AppLeg1D} in Appendix~\ref{appx:basicnetworks}, guarantees for $j=1,\dots,N$, the existence of an NN $\Phi^j_{\varepsilon}$ with both input dimension and output dimension $d^j_{L_j}$ equal to $1$, such that
\begin{equation} \label{eq:indLeg}
    \| P_j - {\rm R}(\Phi^j_{\varepsilon})\|_{L^{\infty}([-1,1])} \leq \varepsilon,
\end{equation}
and for $j=0$, an NN $\Phi^0_{\varepsilon}$ satisfying ${\rm R}(\Phi^0_{\varepsilon})= P_0=1$ on $[-1,1]$.
Moreover,
\begin{equation} \label{eq:lwpoly}
    L(\Phi^j_{\varepsilon}) \leq C(1+ \log_2 j)(j+ \log_2 (1/\varepsilon)) \quad\text{ and }\quad
    W(\Phi^j_{\varepsilon}) \leq Cj(j+\log_2(1/\varepsilon)),
\end{equation}
and $L(\Phi^0_{\varepsilon}) = 2$, $W(\Phi^0_{\varepsilon}) = 1$.
Therefore, by Proposition~\ref{prop:NNLinComb}, there exists an NN $\widetilde{\Phi}_{\varepsilon, N}$ that fulfills
\begin{equation} \label{eq:lincombrealized}
    {\rm R}(\widetilde{\Phi}_{\varepsilon, N}) = \sum_{j=0}^N \beta_j {\rm R}(\Phi^j_{\varepsilon}),
\end{equation}
with, from \eqref{eq:lwpoly}, the number of layers being at most
\begin{eqnarray} \label{eq:giantlayer}
    L(\widetilde{\Phi}_{\varepsilon, N}) = \max_{0\leq j \leq N} L (\Phi^j_{\varepsilon}) \leq C(1+ \log_2 N)(N+ \log_2 (1/\varepsilon)),
\end{eqnarray}
and the number of weights being at most, 
\begin{align} \label{eq:giantweight}
    \nonumber W (\widetilde{\Phi}_{\varepsilon, N}) 
    &\leq 2 \sum_{j=0}^N W( \Phi^j_{\varepsilon}) + 2\sum_{j=0}^N d^j_{L_j} \Big(\max_{0\leq j\leq N} L (\Phi^j_{\varepsilon})\Big)\\
    \nonumber &\leq C\sum_{j=1}^N j(j+\log_2(1/\varepsilon)) + CN(1+ \log_2 N)(N+ \log_2 (1/\varepsilon))\\
    \nonumber &\leq C (N^2+N)(N+\log_2(1/\varepsilon)) + CN(1+ \log_2 N)(N+ \log_2 (1/\varepsilon))\\
    &\leq C (N^2+N)(N+\log_2(1/\varepsilon)).
\end{align}
Now, from \eqref{eq:indLeg}, \eqref{eq:lincombrealized}, we obtain
\begin{align} \label{eq:sumLeg}
    \nonumber \Big\| \sum_{j=0}^N \beta_j P_j - {\rm R}(\widetilde{\Phi}_{\varepsilon, N}) \Big\|_{L^{\infty}([-1,1])} &= 
    \Big\| \sum_{j=0}^N \beta_j P_j - \sum_{j=0}^N \beta_j {\rm R}(\Phi^j_{\varepsilon}) \Big\|_{L^{\infty}([-1,1])} \\
    \nonumber &\leq \sum_{j=0}^N |\beta_j| \Big\| P_j - {\rm R}(\Phi^j_{\varepsilon})\Big\|_{L^{\infty}([-1,1])} \\
    &\leq c_0N\varepsilon.
\end{align}
By replacing $N\varepsilon$ with $\varepsilon$ in \eqref{eq:giantlayer}, \eqref{eq:giantweight}, \eqref{eq:sumLeg}, we arrive at the desired conclusion. 
\end{proof}


\subsection{Proof of Theorem \ref{thm:ApproxSpan}} \label{sec:ProofTheo3}


To simplify the presentation, we adopt the convention introduced at the beginning of Subsection~\ref{sec:leastsquaresproof}.
Namely, we use $\Lambda$ to denote $\Lambda^{\rm HC}_{n-1}$, and let $K=\texttt{\#}\Lambda=\texttt{\#}\Lambda^{\rm HC}_{n-1}$.
For $\vec{\nu}\in\Lambda$, we write $\varphi_{\mathsf{w},\vec{\nu}} = \varphi_{\mathsf{w},\Pi(k)}$, where $\Pi: \{1,\dots,K\}\to\Lambda$ is a bijection.
Now let $g \in \mathcal{S}_{\Lambda}$, i.e.
\begin{equation} \label{eq:lincombSlep}
    g = \sum_{k=1}^{K} b_k \varphi_{\mathsf{w},\Pi(k)},
\end{equation}
for $b_k := \frac{1}{2}\int_{-1}^1 g(x)\varphi_{\mathsf{w},\Pi(k)}(x) \, {\rm d}x$. 
%We first prove that for each such $g$, an NN exists that approximates $g$ with arbitrary accuracy.
According to Proposition~\ref{prop:ApproxSlepian}, for a given $\varepsilon>0$ and each Slepian basis function $\varphi_{\mathsf{w},\Pi(k)}$, there exists an NN $\Psi_{\varepsilon}^{k}$ such that\footnote{Given that $N_{\star}$, in \eqref{eqdef:Nstar}, depends on $\varepsilon$, we will omit its reference in the subscript and simply write $\Psi^{k}_{\varepsilon}$ going forward.} 
\begin{equation} \label{eq:indSlepapp}
     \|\varphi_{\mathsf{w}, k} - {\rm R}(\Psi_{\varepsilon}^{k}) \|_{L^{\infty}([-1, 1]^d)}
    \leq B(d,n)\varepsilon. 
\end{equation}
In turn, by Proposition~\ref{prop:NNLinComb}, there exists an NN $\Psi_{g;\varepsilon}$ satisfying 
\begin{equation} \label{eq:lincombreal}
    {\rm R}(\Psi_{g;\varepsilon}) = \sum_{k=1}^K b_k {\rm R}(\Psi_{\varepsilon}^{k}).
\end{equation}
Thus we obtain
\begin{align} \label{eq:approxg}
    \nonumber \|g - {\rm R}(\Psi_{g;\varepsilon})
    \|_{L^{\infty}([-1, 1]^d)}
    &\leq \sum_{k=1}^K |b_k| \| \varphi_{\mathsf{w}, k} - {\rm R}(\Psi_{\varepsilon}^{k}) \|_{L^{\infty}_{\bf u}([-1, 1]^d)} \\
    &\leq \sqrt{K}B(d,n)\|g\|_{L^2_{\bf u}([-1,1]^d)}\varepsilon,
\end{align}
from \eqref{eq:indSlepapp} and the fact that $\|g\|_{L^2_{\bf u}([-1,1]^d)} = (\sum_{k=1}^K |b_k|^2)^{1/2}$.
It also follows from Proposition~\ref{prop:ApproxSlepian} and another application of Proposition~\ref{prop:NNLinComb} that
\begin{equation} \label{eq:LWSLambda}
    \begin{split}
        L(\Psi_{g;\varepsilon}) 
        &
        \leq C \big((1+\log_2N_{\star})(N_{\star}+\log_2(N_{\star}/\varepsilon)) + (1+ \log_2 (M(d,n)/\varepsilon)\big),\\
        W(\Psi_{g;\varepsilon}) 
        & 
        \leq C K\big((N_{\star}^2 + N_{\star})(N_{\star} + \log_2(N_{\star}/\varepsilon)) + (1+ \log_2 (M(d,n)/\varepsilon)\big).
    \end{split}
\end{equation}
Now, let 
\begin{equation} \label{eqdef:NLambda}
    \mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}:=\{\Psi_{g;\varepsilon}: \Psi_{g;\varepsilon} \text{ satisfies } \eqref{eq:lincombreal} \text{ where } g \text{ takes the form of } \eqref{eq:lincombSlep}\}.
\end{equation}
We will show in what follows that, given $\mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}$ in \eqref{eqdef:NLambda}, the optimization problem \eqref{leastsquaresproblem2} admits a unique solution fulfilling \eqref{eq:PETFinal}.

Recall the matrix $A\in\mathbb{R}^{m\times K}$, introduced in \eqref{eqdef:A}, where $A_{jk} = \dfrac{1}{\sqrt{m}} \varphi_{\mathsf{w},\Pi(k)}(\vec{y}_j)$.
Since the Slepian basis is an orthonormal set, the columns of $A$ are linearly independent.
Consider $\tilde{A}\in\mathbb{R}^{m\times K}$, such that $\tilde{A}_{jk} = \dfrac{1}{\sqrt{m}} {\rm R}(\Psi_{\varepsilon}^{k})(\vec{y}_j)$.
Then it follows from \eqref{eq:indSlepapp} that the columns of $\tilde{A}$ are also linearly independent for sufficiently small $\varepsilon$.
Indeed, let $\varepsilon\in (0,1/(\sqrt{K} B(d,n)))$.
If $0 = \sum_{k=1}^K b_k {\rm R}(\Psi^k_{\varepsilon})$ for some not-all-zero scalars $\{b_k\}_{k=1}^K$, then
\begin{equation*}
    \sum_{k=1}^K b_k (\varphi^k_{\varepsilon} - {\rm R}(\Psi^k_{\varepsilon})) = \sum_{k=1}^K b_k \varphi^k_{\varepsilon},
\end{equation*}
which leads to 
\begin{align*}
    \Big(\sum_{k=1}^K |b_k|^2\Big)^{1/2} 
    = \Big\| \sum_{k=1}^K b_k \varphi^k_{\varepsilon} \|_{L^2_{\bf u}([-1,1]^d)} 
    &= \Big\| \sum_{k=1}^K b_k (\varphi^k_{\varepsilon}-{\rm R}(\Psi^k_{\varepsilon})) \Big \|_{L^2_{\bf u}([-1,1]^d)} \\
    &\leq \sum_{k=1}^K |b_k| \|\varphi^k_{\varepsilon}-{\rm R}(\Psi^k_{\varepsilon})\|_{L^2_{\bf u}([-1,1]^d)} \\
    &\leq \varepsilon\sqrt{K} B(d,n) \Big(\sum_{k=1}^K |b_k|^2\Big)^{1/2} 
    < \Big(\sum_{k=1}^K |b_k|^2\Big)^{1/2},
\end{align*}
a contradiction.
Since, due to \eqref{eq:lincombreal}, every element in $\mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}$ is realized as a linear combination of ${\rm R}(\Psi_{\varepsilon}^{k})$, the problem \eqref{leastsquaresproblem2} is equivalent to finding $(\vec{z})^{\natural} \in \C^{K}$ such that
\begin{equation*}
    (\vec{z})^{\natural} \in \argmin_{\vec{z} \in \C^{K}} 
    \|\tilde{A}\vec{z} - \vec{b}\|_2^2
\end{equation*}
where $\vec{b} = \frac{1}{\sqrt{m}}(f(\vec{y}_j) + \eta_j)_{j=1}^m$. 
We look at the smallest singular value of $\tilde{A}$ next.
By Weyl's Lemma \citep[Theorem 4.3.1]{horn2012matrix}, we have
\begin{equation}\label{eq:Weyls}
    \max_{j =1,\dots,m} | \sigma_j(A) - \sigma_j( \tilde{A}) | \leq \| A - \tilde{A}\|_2
\end{equation}
where $\sigma_j$ denotes the $j$th singular value of $A$. 
Hence, for all unit vector $\vec{z}\in\C^K$, 
\begin{align*}
    \| (A-\tilde{A})\vec{z} \|_2^2 
    &= \sum_{j=1}^m | \sum_{k=1}^K (A_{jk} - \tilde{A}_{jk}) z_k |^2 \\
    &\leq \sum_{j=1}^m \bigg(\sum_{k=1}^K |A_{jk} - \tilde{A}_{jk}| |z_k|\bigg)^2 \\
    &\leq \sum_{j=1}^m \bigg(\sum_{k=1}^K \frac{1}{\sqrt{m}} \| \varphi_{\mathsf{w}, k} - {\rm R}(\Psi^k_{\varepsilon}) \|_{L^{\infty}([-1,1]^d)} |z_k|\bigg)^2\\
    &= \bigg(\sum_{k=1}^K \| \varphi_{\mathsf{w}, k} - {\rm R}(\Psi^k_{\varepsilon}) \|_{L^{\infty}([-1,1]^d)} |z_k|\bigg)^2\\
    &\leq \sum_{k=1}^K\| \varphi_{\mathsf{w}, k} - {\rm R}(\Psi^k_{\varepsilon}) \|_{L^{\infty}([-1,1]^d)}^2 \|\vec{z} \|_2^2 \\
    &\leq K B(d,n)^2 \varepsilon^2, 
\end{align*}
where we have used the Cauchy-Schwarz inequality in the third inequality and \eqref{eq:indSlepapp} in the fourth.
Therefore,
\begin{equation}\label{eq:MatricesDif}
    \| A - \tilde{A} \|_2 \leq \sqrt{K} B(d,n) \varepsilon.
\end{equation}
We have demonstrated in the proof of Theorem~\ref{thm:leastsquaresampling} that $\sigma_{\rm min}(A)> \sqrt{1-\delta}$ with probability at least $1-\beta$, as long as $m \geq c_{\delta}\kappa^{\star}  \log(\texttt{\#}\Lambda/\beta)$.
Hence, from \eqref{eq:Weyls}, \eqref{eq:MatricesDif}, we get 
\begin{equation} \label{eq:lowbdtildesigma}
    \sigma_{\rm min}(\tilde{A}) \geq \sigma_{\rm min}(A) - \|A - \tilde{A}\|_2
    \geq \sigma_{\rm min}(A) -\sqrt{K} B(d,n) \varepsilon
    > \sqrt{1- \delta} - \sqrt{K} B(d,n) \varepsilon,
\end{equation}
with the same probability.
Thus, by choosing $\varepsilon\in (0,1/(\sqrt{K}B(d,n)))$ additionally sufficiently small such that
\begin{equation*}
    \sqrt{K} B(d,n) \varepsilon \leq \frac{1}{2} \sqrt{1 - \delta},
\end{equation*}
which is \eqref{epscondition}, we further obtain from \eqref{eq:lowbdtildesigma}
\begin{equation} \label{eq:minsing}
   \sigma_{\rm min}(\tilde{A}) > \frac{1}{2} \sqrt{1 - \delta}.
\end{equation}
By applying the least squares approximation theory from Subsection~\ref{sec:problem}, specifically leveraging \eqref{punchline} and the lower bound given in \eqref{eq:minsing}, we conclude that \eqref{leastsquaresproblem2} has, with probability at least $1-\beta$, a unique solution $\Psi^{\natural}$ such that
\begin{equation*}    
    \| f - {\rm R}(\Psi^{\natural}) \|_{L_{ {\bf u}}^2([-1,1]^d)} 
    \leq 
    \Big(1+ \dfrac{2}{\sqrt{1-\delta}} \Big)\inf_{\Psi\in \mathcal{N}_{\Lambda,\mathsf{w},\varepsilon}} \| f - {\rm R}(\Psi) \|_{L^{\infty}([-1,1]^d)} 
    + \dfrac{2}{\sqrt{1-\delta}} \| \vec{e} \|_2.
\end{equation*}
Additionally, by \eqref{eq:approxg}, for any $\Psi\in\mathcal{N}_{\varepsilon}$ and any $g \in\mathcal{S}_{\Lambda}$,
\begin{align*}
    \| f - {\rm R}(\Psi) \|_{L^{\infty}([-1,1]^d)} 
    &\leq \| f - g\|_{L^{\infty}([-1,1]^d)} + \| g - {\rm R}(\Psi) \|_{L^{\infty}([-1,1]^d)} \\
    &\leq \| f - g\|_{L^{\infty}([-1,1]^d)} + \sqrt{K}B(d,n)\|g\|_{L^2_{\bf u}([-1,1]^d)}\varepsilon,
\end{align*}
and we arrive at \eqref{eq:PETFinal}. 
Finally, we note that the necessary upper bounds on the layers and weights for $\Psi^{\natural}$ have already been specified in \eqref{eq:LWSLambda}. \qed


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{s:conclusions}



%Motivated by the success of deep learning, Theorem~\ref{thm:ApproxSpan} provides a Practical Existence Theorem (PET) for the reconstruction of (approximately) bandlimited functions from pointwise samples. Our approach builds upon new recovery guarantees for least squares approximation in the Slepian basis; see Theorem~\ref{thm:leastsquaresampling}. Specifically, we focus on approximating these functions in finite-dimensional spaces that are associated with hyperbolic cross-index sets. Each Slepian basis function is approximated using Legendre polynomials, which are then further approximated using Neural Networks (NNs). \\

We conclude by discussing some limitations of our work and indicating possible directions for future research. First, our theoretical results hold in dimensions $d=1,2, 3$. In principle, our arguments can be applied to higher dimensions, but the results would be affected by the curse of dimensionality. This arises due to the following reasons.
The sample complexity bound in $m$ of Theorem~\ref{thm:leastsquaresampling} relies on the estimation of an upper bound on $\kappa(S_{\Lambda})$, which is given in Proposition~\ref{prop:Theta}. This upper bound estimate stems from Proposition~\ref{prop:relating} and Lemma~\ref{lem:HCsize}, both providing dimension-dependent bounds for the cardinality of the hyperbolic cross index set and its slices.
Specifically, \eqref{crucialupperbound} holds only beyond a certain threshold $n_{\star}(d)$. For $d \geq 3$, $n_{\star}(d)$ is no longer guaranteed to be one. Additionally, \eqref{crucialinequality} applies after another distinct threshold in $n$ for each $d$.
Since both \eqref{crucialupperbound} and \eqref{crucialinequality} do not necessarily hold for all $n\in\mathbb{N}$ for each $d$, this disrupts our inductive reasoning across dimensions, which is intricately tied to the geometry of hyperbolic crosses.

While Proposition~\ref{prop:relating} and Lemma~\ref{lem:HCsize} are integral to our proof strategy for circumventing the dimensional impact in the upper bound of $\kappa(\mathcal{S}_{\Lambda})$, the potential for the curse of dimensionality to emerge remains evident, as $\kappa(\mathcal{S}_{\Lambda}) \geq \|\varphi_{\mathsf{w},\vec{0}}\|_{L^{\infty}([-1,1]^d)}^2$, and the latter quantity is of order $\mathcal{O}(\mathsf{w}^d)$, by Proposition~\ref{prop:Phi_j}. 
A potential fix to mitigate this problem is to consider a \emph{preconditioning scheme} (see \citep[Chapter 5]{adcock2022sparse}) where samples are drawn from an orthogonal measure on $[-1,1]^d$, alongside leveraging hyperbolic crosses of anisotropic type. 
Alternatively, recovery strategies based on compressed sensing (see \citep[Chapter 7]{adcock2022sparse} and the references therein) could be explored. These considerations fall outside the scope of this paper, but they present promising directions for future research.

The error bounds in Theorems~\ref{thm:leastsquaresampling} and \ref{thm:ApproxSpan} depend on the best approximation error of the target function $f$ with respect to the Slepian basis $\{\varphi_{\mathsf{w},\vec{\nu}}\}_{\vec{\nu}\in\Lambda}$. It is possible to obtain approximation rates that depend on the number of samples $m$ for specific function classes, similar to the approach in \citep{adcock2022deep} for analytic functions. 
To address this approximation-theoretical challenge, one would need to define a suitable class of (approximately) bandlimited functions for which the best approximation error $\inf_{g \in \mathcal{S}_\Lambda} \| f - g \|_{L^{\infty}([-1,1]^d)}$ can be explicitly estimated in terms of $\texttt{\#}\Lambda$. This type of bound is not readily available in the literature and so would need to be derived.

Additionally, as noted in Section~\ref{sec:main_results}, the NNs in the class $\mathcal{N}_{\Lambda, \mathsf{w}, \varepsilon}$ of Theorem~\ref{thm:ApproxSpan} are not fully trained like those considered in our numerical experiments. In fact, their first-to-second-last layers are explicitly constructed to emulate the Slepian basis, and the last layer is trained via least squares. Addressing this gap between theory and practice is an important direction for future work. 
%A possible way to address it 
One possible approach is given by the proof strategy of \cite[Theorem 3.2]{adcock2024optimal}, which shows that training problems based on any family of fully connected NNs possess uncountably many minimizers that achieve accurate approximations. 

Finally, our numerical experiments in dimensions one and two demonstrate that both reconstruction methods using least squares and deep learning %converge 
improve as the number of training samples increases. %Although the performance of least squares is much better than deep learning in dimension one, deep learning can reach and slightly outperform least squares in the two-dimensional case. 
Although least squares approximation is far more effective in dimension one, deep learning can match and slightly surpass it in dimension two.
%However, identifying an NN architecture that consistently outperforms least squares approximation across different benchmark functions is an open question that deserves a more comprehensive numerical investigation.  
However, identifying an NN architecture that consistently achieves superior performance across different benchmark functions is an open question that deserves a more comprehensive numerical investigation.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}

AMN is supported by the Austrian Science Fund (FWF) Project P-37010. SB and JJB were partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) through grants RGPIN-2020-06766 and RGPIN-2023-04244, respectively, and the Fonds de Recherche
du Qu\'ebec Nature et Technologies (FRQNT) through grants 313276 and 340894, respectively. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
\begin{thebibliography}{10}

\bibitem{adcock2024learning}
B.~Adcock, S.~Brugiapaglia, N.~Dexter, and S.~Moraga.
\newblock Learning smooth functions in high dimensions: From sparse polynomials
  to deep neural networks.
\newblock In S.~Mishra and A.~Townsend, editors, {\em Numerical Analysis Meets
  Machine Learning}, volume~25 of {\em Handbook of Numerical Analysis}, pages
  1--52. Elsevier, 2024.

\bibitem{adcock2022deep}
B.~Adcock, S.~Brugiapaglia, N.~Dexter, and S.~Morage.
\newblock Deep neural networks are effective at learning high-dimensional
  {H}ilbert-valued functions from limited data.
\newblock In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, {\em
  Proceedings of the 2nd Mathematical and Scientific Machine Learning
  Conference}, volume 145 of {\em Proceedings of Machine Learning Research},
  pages 1--36. PMLR, 16--19 Aug 2022.

\bibitem{adcock2022sparse}
B.~Adcock, S.~Brugiapaglia, and C.~G. Webster.
\newblock {\em Sparse Polynomial Approximation of High-Dimensional Functions}.
\newblock SIAM, 2022.

\bibitem{adcock2021gap}
B.~Adcock and N.~Dexter.
\newblock The gap between theory and practice in function approximation with
  deep neural networks.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 3(2):624--655,
  2021.

\bibitem{adcock2024optimal}
B.~Adcock, N.~Dexter, and S.~Moraga.
\newblock Optimal deep learning of holomorphic operators between {B}anach
  spaces.
\newblock {\em arXiv preprint arXiv:2406.13928}, 2024.

\bibitem{barron1993universal}
A.~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Transactions on Information theory}, 39(3):930--945, 1993.

\bibitem{benedetto2012modern}
J.~J. Benedetto and P.~J. S.~G. Ferreira.
\newblock {\em Modern Sampling Theory: Mathematics and Applications}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{bonami2014uniform}
A.~Bonami and A.~Karoui.
\newblock Uniform bounds of prolate spheroidal wave functions and eigenvalues
  decay.
\newblock {\em Comptes Rendus Math{\'e}matique}, 352(3):229--234, 2014.

\bibitem{boyd2004prolate}
J.~P. Boyd.
\newblock Prolate spheroidal wavefunctions as an alternative to chebyshev and
  legendre polynomials for spectral element and pseudospectral algorithms.
\newblock {\em Journal of Computational Physics}, 199(2):688--716, 2004.

\bibitem{brugiapaglia2024physics}
S.~Brugiapaglia, N.~Dexter, S.~Karam, and W.~Wang.
\newblock Physics-informed deep learning and compressive collocation for
  high-dimensional diffusion-reaction equations: practical existence theory and
  numerics.
\newblock {\em arXiv preprint arXiv:2406.01539}, 2024.

\bibitem{cabric2004implementation}
D.~Cabric, S.~M. Mishra, and R.~W. Brodersen.
\newblock Implementation issues in spectrum sensing for cognitive radios.
\newblock In {\em Conference Record of the Thirty-Eighth Asilomar Conference on
  Signals, Systems and Computers, 2004.}, volume~1, pages 772--776. Ieee, 2004.

\bibitem{chen2002highly}
R.~Chen and B.~Cantrell.
\newblock Highly bandlimited radar signals.
\newblock In {\em Proceedings of the 2002 IEEE Radar Conference (IEEE Cat. No.
  02CH37322)}, pages 220--226. IEEE, 2002.

\bibitem{chernov2016new}
A.~Chernov and D.~D{\~u}ng.
\newblock New explicit-in-dimension estimates for the cardinality of
  high-dimensional hyperbolic crosses and approximation of functions having
  mixed smoothness.
\newblock {\em Journal of Complexity}, 32(1):92--121, 2016.

\bibitem{chkifa2015discrete}
A.~Chkifa, A.~Cohen, G.~Migliorati, F.~Nobile, and R.~Tempone.
\newblock Discrete least squares polynomial approximation with random
  evaluations- application to parametric and stochastic elliptic {PDEs}.
\newblock {\em ESAIM: Mathematical Modelling and Numerical
  Analysis-Mod{\'e}lisation Math{\'e}matique et Analyse Num{\'e}rique},
  49(3):815--837, 2015.

\bibitem{cohen2013stability}
A.~Cohen, M.~A. Davenport, and D.~Leviatan.
\newblock On the stability and accuracy of least squares approximations.
\newblock {\em Foundations of Computational Mathematics}, 13:819--834, 2013.

\bibitem{daws2019analysis}
J.~Daws and C.~G. Webster.
\newblock Analysis of deep neural networks with quasi-optimal polynomial
  approximation rates.
\newblock {\em arXiv preprint arXiv:1912.02302}, 2019.

\bibitem{de2021approximation}
T.~De~Ryck, S.~Lanthaler, and S.~Mishra.
\newblock On the approximation of functions by tanh neural networks.
\newblock {\em Neural Networks}, 143:732--750, 2021.

\bibitem{dung2018hyperbolic}
D.~D{\~u}ng, V.~Temlyakov, and T.~Ullrich.
\newblock {\em Hyperbolic Cross Approximation}.
\newblock Springer, 2018.

\bibitem{eckle2019comparison}
Konstantin Eckle and Johannes Schmidt-Hieber.
\newblock A comparison of deep networks with relu activation function and
  linear spline-type methods.
\newblock {\em Neural Networks}, 110:232--242, 2019.

\bibitem{folland1999real}
G.~B. Folland.
\newblock {\em Real Analysis: Modern Techniques and their Applications},
  volume~40.
\newblock John Wiley \& Sons, 1999.

\bibitem{franco2025practical}
N.~R. Franco and S.~Brugiapaglia.
\newblock A practical existence theorem for reduced order models based on
  convolutional autoencoders.
\newblock {\em Foundations of Data Science}, 7(1):72--98, 2025.

\bibitem{horn2012matrix}
R.~A. Horn and C.~R. Johnson.
\newblock {\em Matrix Analysis}.
\newblock Cambridge University Press, 2012.

\bibitem{jerri1977shannon}
A.~J. Jerri.
\newblock The {S}hannon sampling theorem—{I}ts various extensions and
  applications: A tutorial review.
\newblock {\em Proceedings of the IEEE}, 65(11):1565--1596, 1977.

\bibitem{diederik2015adam}
D.P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Y.~Bengio and Y.~LeCun, editors, {\em 3rd International Conference
  on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015,
  Conference Track Proceedings}, 2015.

\bibitem{kirichenko2022last}
P.~Kirichenko, P.~Izmailov, and A.~G. Wilson.
\newblock Last layer re-training is sufficient for robustness to spurious
  correlations.
\newblock {\em arXiv preprint arXiv:2204.02937}, 2022.

\bibitem{landau1965eigenvalue}
H.~J. Landau.
\newblock The eigenvalue behavior of certain convolution equations.
\newblock {\em Transactions of the American Mathematical Society},
  115:242--256, 1965.

\bibitem{landau1993density}
H.~J. Landau.
\newblock On the density of phase-space expansions.
\newblock {\em IEEE Transactions on Information Theory}, 39(4):1152--1156,
  1993.

\bibitem{moore2004prolate}
I.~C. Moore and M.~Cada.
\newblock Prolate spheroidal wave functions, an introduction to the {S}lepian
  series and its properties.
\newblock {\em Applied and Computational Harmonic Analysis}, 16(3):208--230,
  2004.

\bibitem{FEMNNsPetersenSchwab}
J.~A.~A. Opschoor, P.~C. Petersen, and C.~Schwab.
\newblock Deep {ReLU} networks and high-order finite element methods.
\newblock {\em Analysis and Applications}, 18(05):715--770, 2020.

\bibitem{opschoor2022exponential}
J.~A.~A. Opschoor, C.~Schwab, and J.~Zech.
\newblock Exponential {ReLU DNN} expression of holomorphic maps in high
  dimension.
\newblock {\em Constructive Approximation}, 55(1):537--582, 2022.

\bibitem{osipov2013prolate}
A.~Osipov, V.~Rokhlin, and H.~Xiao.
\newblock {\em Prolate Spheroidal Wave Functions of Order Zero}.
\newblock Springer, 2013.

\bibitem{pan2009survey}
S.~J. Pan and Q.~Yang.
\newblock A survey on transfer learning.
\newblock {\em IEEE Transactions on Knowledge and Data Engineering},
  22(10):1345--1359, 2009.

\bibitem{parhi2022near}
R.~Parhi and R.~D. Nowak.
\newblock Near-minimax optimal estimation with shallow {ReLU} neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 2022.

\bibitem{PetV2018OptApproxReLU}
P.~C. Petersen and F.~Voigtlaender.
\newblock Optimal approximation of piecewise smooth functions using deep {ReLU}
  neural networks.
\newblock {\em Neural Networks}, 180:296--330, 2018.

\bibitem{petersen2024mathematical}
Philipp Petersen and Jakob Zech.
\newblock Mathematical theory of deep learning.
\newblock {\em arXiv preprint arXiv:2407.18384}, 2024.

\bibitem{petersen2018optimal}
R.~Petersen and F.~Voigtlaender.
\newblock Optimal approximation of piecewise smooth functions using deep {ReLU}
  neural networks.
\newblock {\em Neural Networks}, 108:296--330, 2018.

\bibitem{rosen1999discrete}
K.~H. Rosen and K.~Krithivasan.
\newblock {\em Discrete Mathematics and its Applications}.
\newblock McGraw-Hill New York, 1999.

\bibitem{shkolnisky2006approximation}
Y.~Shkolnisky, M.~Tygert, and V.~Rokhlin.
\newblock Approximation of bandlimited functions.
\newblock {\em Applied and Computational Harmonic Analysis}, 21(3):413--420,
  2006.

\bibitem{simons2010slepian}
F.~J. Simons.
\newblock {\em Slepian Functions and Their Use in Signal Estimation and
  Spectral Analysis}, pages 891--923.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2010.

\bibitem{slepian1964prolate}
D.~Slepian.
\newblock Prolate spheroidal wave functions, {F}ourier analysis and
  uncertainty—{IV}: extensions to many dimensions; generalized prolate
  spheroidal functions.
\newblock {\em Bell System Technical Journal}, 43(6):3009--3057, 1964.

\bibitem{slepian1965some}
D.~Slepian.
\newblock Some asymptotic expansions for prolate spheroidal wave functions.
\newblock {\em Journal of Mathematics and Physics}, 44(1-4):99--140, 1965.

\bibitem{slepian1961prolate}
D.~Slepian and H.~O. Pollak.
\newblock Prolate spheroidal wave functions, {F}ourier analysis and
  uncertainty—{I}.
\newblock {\em Bell System Technical Journal}, 40(1):43--63, 1961.

\bibitem{strombergenhancing}
N.~Stromberg, R.~Ayyagari, S.~Koyejo, R.~Nock, and L.~Sankar.
\newblock Enhancing robustness of last layer two-stage fair model corrections.
\newblock In {\em The Thirty-eighth Annual Conference on Neural Information
  Processing Systems}.

\bibitem{tropp2009beyond}
J.~A. Tropp, J.~N. Laska, M.~F. Duarte, J.~K. Romberg, and R.~G. Baraniuk.
\newblock Beyond {N}yquist: {E}fficient sampling of sparse bandlimited signals.
\newblock {\em IEEE Transactions on Information Theory}, 56(1):520--544, 2009.

\bibitem{2020SciPy-NMeth}
P.~Virtanen, R.~Gommers, T.~E. Oliphant, M.~Haberland, T.~Reddy, D.~Cournapeau,
  E.~Burovski, P.~Peterson, W.~Weckesser, J.~Bright, S.~J. {van der Walt},
  M.~Brett, J.~Wilson, K.~J. Millman, N.~Mayorov, A.~R.~J. Nelson, E.~Jones,
  R.~Kern, E.~Larson, C.~J. Carey, {\.I}.~Polat, Y.~Feng, E.~W. Moore,
  J.~{VanderPlas}, D.~Laxalde, J.~Perktold, R.~Cimrman, I.~Henriksen, E.~A.
  Quintero, C.~R. Harris, A.~M. Archibald, A.~H. Ribeiro, F.~Pedregosa, P.~{van
  Mulbregt}, and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock {\em Nature Methods}, 17:261--272, 2020.

\bibitem{walter2003sampling}
G.~G. Walter and X.~A. Shen.
\newblock Sampling with prolate spheroidal wave functions.
\newblock {\em Sampling Theory in Signal and Image Processing}, 2:25--52, 2003.

\bibitem{wang2017review}
L.-L. Wang.
\newblock A review of prolate spheroidal wave functions from the perspective of
  spectral methods.
\newblock {\em Journal of Mathematical Study}, 50(2):101--143, 2017.

\bibitem{xiao2001prolate}
H.~Xiao, V.~Rokhlin, and N.~Yarvin.
\newblock Prolate spheroidal wavefunctions, quadrature and interpolation.
\newblock {\em Inverse Problems}, 17(4):805, 2001.

\bibitem{zayed2018advances}
A.~I. Zayed.
\newblock {\em Advances in {Shannon’s} Sampling Theory}.
\newblock Routledge, 2018.

\end{thebibliography}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Basic neural network calculus} \label{appx:basicnetworks}

We first introduce a classical result that constructively demonstrates the existence of an NN for approximating products within a compact region, as detailed below.

\begin{proposition}[{\citep[Proposition 2.5]{opschoor2022exponential}}] \label{prop:Multiplication}
Let $\varepsilon \in (0,1)$, $B>0$, and $\times: \mathbb{R}^2 \to \R$ be defined by $\times(x,y)=xy$. 
Then, there exists a ReLU NN $\widetilde{\times}_{\varepsilon,B}$ such that ${\rm R}(\widetilde{\times}_{\varepsilon,B}): [-B,B]^2\to \mathbb{R}$ and that
\begin{equation*}
   \| {\rm R}(\widetilde{\times}_{\varepsilon,B})- \times \|_{L_{\bf u}^{\infty}([-B,B]^2)} \leq \varepsilon.
\end{equation*}
Moreover, there is a universal constant $C>0$ such that 
\begin{equation*}
   L (\widetilde{\times}_{\varepsilon,B}) \leq C(1+ \log_2(B/\varepsilon)) 
\quad\text{ and }\quad
    W (\widetilde{\times}_{\varepsilon,B}) \leq C(1+ \log_2(B/\varepsilon)).
\end{equation*}
\end{proposition}

Continuing, we present two foundational network operations; they are, concatenation, as described in Definition~\ref{def:concat}, and parallelization, as described in Definition~\ref{def:Parallelization}.


\begin{definition}[{\citep[Definition~2.2]{PetV2018OptApproxReLU}}]\label{def:concat}
Let $L_1, L_2 \in \N$ and suppose 
\begin{equation*}
    \Phi^1 = ((A_1^1,b_1^1), \dots, (A_{L_1}^1,b_{L_1}^1)) 
    \quad\text{ and }\quad
    \Phi^2 = ((A_1^2,b_1^2), \dots, (A_{L_2}^2,b_{L_2}^2))
\end{equation*}
are two NNs with the number of layers $L_1$ and $L_2$, respectively, such that the input layer of $\Phi^1$ has the same dimension as that of the output layer of $\Phi^2$. Then, $\Phi^1 \odot \Phi^2$ is an NN with $L_1+L_2-1$ layers such that
\begin{equation*}
    \Phi^1 \odot \Phi^2 := ((A_1^2,b_1^2), \dots, (A_{L_2-1}^2,b_{L_2-1}^2), (A_1^1A_{L_2}^2, A_1^1b_{L_2}^2+b_1^1) , (A_2^1,b_2^1), \dots, (A_{L_1}^1,b_{L_1}^1)),
\end{equation*}
called the concatenation of $\Phi^1$ and $\Phi^2$.  Moreover, ${\rm R}(\Phi^1 \odot \Phi^2) = {\rm R}(\Phi^1) \circ {\rm R}(\Phi^2)$. 
\end{definition}


\begin{definition}[{\citep[Definition~2.7]{PetV2018OptApproxReLU}}]\label{def:Parallelization} 
Let $d, L \in \N$ and suppose 
\begin{equation*}
    \Phi^1 = ((A_1^1,b_1^1), \dots, (A_{L}^1,b_{L}^1))
    \quad\text{ and }\quad
    \Phi^2 = ((A_1^2,b_1^2), \dots, (A_{L}^2,b_{L}^2))
\end{equation*}
are NNs both with $L$ layers and $d$-dimensional input.
Then, ${\rm P}(\Phi^1,\Phi^2)$ is an NN with $L$ layers such that
\begin{equation*}
    {\rm P}(\Phi^1,\Phi^2):= ((\tilde{A}_1,\tilde{b}_1), \dots, (\tilde{A}_L,\tilde{b}_L) )
\end{equation*}
where 
\begin{equation*}
\widetilde{A}_{1}:=\begin{pmatrix}
	A_{1}^{1} \\
	A_{1}^{2}  
\end{pmatrix},\quad
\widetilde{b}_{1}:=\begin{pmatrix}
	b_{1}^{1} \\
	b_{1}^{2}  
\end{pmatrix},
\,\text{ and }\,
\widetilde{A}_{l}:=\begin{pmatrix}
	A_{l}^{1} & 0 \\
	0 & A_{l}^{2}  
\end{pmatrix},\quad
\widetilde{b}_{l}:=\begin{pmatrix}
	b_{l}^{1} \\
	b_{l}^{2}  
\end{pmatrix},
\, \text{ for } \,
l= 2, \dots, L,
\end{equation*}
called the parallelization of $\Phi^1$ and $\Phi^2$. Moreover, ${\rm R}({\rm P}(\Phi^1,\Phi^2))= ({\rm R}(\Phi^1), {\rm R}(\Phi^2))$.
\end{definition}

The following observation is immediate from the two preceding definitions. 

\begin{remark} \label{rem:NNdefsprops}
It can be easily seen that
\begin{equation*} 
    L(\Phi^1 \odot \Phi^2) \leq L(\Phi^1) + L(\Phi^2), \quad\text{ and }\quad
    W(\Phi^1 \odot \Phi^2) \leq W(\Phi^1) + W(\Phi^2),
\end{equation*}
and that 
\begin{equation*} 
    L({\rm P}(\Phi^1, \Phi^2)) = L(\Phi^1) = L(\Phi^2),
    \quad\text{ and }\quad
    W({\rm P}(\Phi^1, \Phi^2)) = W(\Phi^1) + W(\Phi^2).
\end{equation*} 
It is also readily confirmed that concatenation and parallelization can be extended naturally to involve multiple NNs, such that
\begin{align*}
    \Phi^1\odot \Phi^2 \odot \Phi^3 &= (\Phi^1\odot \Phi^2)\odot \Phi^3 = \Phi^1\odot (\Phi^2 \odot \Phi^3), \\
    {\rm P}(\Phi^1,\Phi^2,\Phi^3) &= {\rm P}({\rm P}(\Phi^1,\Phi^2),\Phi^3) = {\rm P}(\Phi^1,{\rm P}(\Phi^2,\Phi^3)).
\end{align*}
\end{remark}

A key result, shown below, establishes that the identity function can be effectively approximated by an NN with any chosen number of layers.

\begin{proposition}[{\citep[Proposition~2.4]{FEMNNsPetersenSchwab}}] \label{prop:ApproxIdent} 
For every $d,L \in \N$, if $I: \R^d \to \R^d$ denotes the identity map $I(x) = x$ for all $x \in \R^d$, then there exists an NN $\Phi_I$ with input dimension $d$ and $L$ layers, such that ${\rm R}(\Phi_I)(x) = I(x) = x$ for all $x \in \R^d$ and $W(\Phi_I) \leq 2dL$.
\end{proposition}

We provide another key result, which, as a culmination of the previous definitions and proposition, demonstrates that NNs can be linearly combined.


\begin{proposition}[{\citep[Lemma 5.4]{petersen2024mathematical}}]
\label{prop:NNLinComb}
For $d, K \in \N$, let $\{\Phi_j\}_{j = 1}^K$ be a collection of NNs where
\begin{equation*}
    \Phi^j = ((A_1^j,b_1^j), \dots, (A_{L_j}^j,b_{L_j}^j))
\end{equation*}
where $A^j_i \in \R^{d^j_i \times d^j_{i-1}}$, for $i=1,\dots, L_j$ and assume $d^1_{L_1} = d^2_{L_2} = \dots = d^K_{L_K}$. For a set of scalars $\{\alpha_j\}_{j=1}^K$, there is an NN $\Phi_{\Sigma}$ such that 
\begin{equation*}
     {\rm R} (\Phi_{\Sigma}) 
     = \sum_{j=1}^K \alpha_j {\rm R} (\Phi_{j}), 
\end{equation*}
and that
\begin{equation*}
    W  (\Phi_{\Sigma}) \leq 2 \sum_{j=1}^K W (\Phi_{j}) + 2 \sum_{j=1}^K (L_{\rm max} - L_j)d_{L_j}^j
    \quad\text{ and }\quad
    L  (\Phi_{\Sigma}) = \max_{1\leq j \leq K} L (\Phi_{j}). 
\end{equation*}
where $L_{\rm max} := \max_{1\leq j \leq K} L (\Phi_{j}) = \max_{1\leq j \leq K} L_j$.
\end{proposition}

Our final result addresses the polynomial approximability of NNs and is derived from \citep[Proposition 2.11]{opschoor2022exponential}.

\begin{proposition} %\citep[Proposition 2.11]{opschoor2022exponential} 
\label{prop:AppLeg1D} 
Let $\varepsilon\in (0,1)$ and $P_k$ be a normalized Legendre polynomial \eqref{eq:NormLegendr} of degree $k \in \N_0$. For each $k\in\N$ there exists an NN $\Phi^k_{\varepsilon}$ with input and output of dimension one satisfying
\begin{equation*}
    \| P_k  - {\rm R}(\Phi^k_{\varepsilon}) \|_{L^{\infty}([-1,1])} \leq \varepsilon,
\end{equation*}
and for $k=0$, an NN $\Phi^0_{\varepsilon}$ satisfying $P_0 = {\rm R}(\Phi^0_{\varepsilon}) = 1$ on $[-1,1]$. 
Moreover, there exists a universal constant $C>0$ 
such that for $k\in\N$
\begin{equation*}
    L(\Phi^k_{\varepsilon}) \leq C(1+ \log_2 k)(k+ \log_2 (1/\varepsilon))
    \quad\text{ and }\quad
    W(\Phi^k_{\varepsilon}) \leq Ck(k+\log_2(1/\varepsilon)),
\end{equation*}
and in the case $k=0$, $L(\Phi^0_{\varepsilon}) = 2$ and $W(\Phi^0_{\varepsilon}) = 1$.
\end{proposition}










\end{document}