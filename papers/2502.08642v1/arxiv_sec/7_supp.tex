



\vspace{-0.2cm}









\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data_sample_cat.pdf}
    \caption{An example from the ControlSketch dataset, which includes the input image, object mask, attention map, and the corresponding sketch generated using ControlSketch.}
    \label{fig:data_sample}
\end{figure}

\section{ControlSketch Dataset}
The data generation process begins with generating images, followed by creating corresponding sketches using the ControlSketch framework, as described in section 4.2 in the main paper.
We generate images using the SDXL model \cite{podell2023sdxlimprovinglatentdiffusion} with the following prompt:
\textit{``A highly detailed wide-shot image of one $<c>$, set against a plain mesmerizing background. Center.''}, where $c$ is the class label.
Additionally, a negative prompt, \textit{``close up, few, multiple,''} is applied to ensure images depict a single object in a clear and high-quality pose. The generated images are of size $1024 \times 1024$. An example output image for the class ``cat'' is shown in \Cref{fig:data_sample}.

During the image generation process, we retain cross attention maps of the class label token extracted from internal layers of the model for future use. To isolate the object, we employ the BRIA Background Removal v1.4 Model  \cite{briaRMBG} to extract an object mask. After generating the image, we use BLIP2 \cite{li2023blip2bootstrappinglanguageimagepretraining} to extract the image caption that provides context beyond the object’s class. For example, for the image in \cref{fig:data_sample}, the caption  describe the cat as sitting, offering richer semantic information.
The sketches are generated using the ControlSketch method with 32 strokes. These strokes are subsequently arranged according to our stroke-sorting schema. The final SVG files contains the sorted strokes.
We use the Hugging Face implementation of SDXL version 1.0 \cite{podell2023sdxlimprovinglatentdiffusion} with its default parameters. Generating a single image with SDXL takes approximately 10 seconds, while sketch generation using the ControlSketch method on an NVIDIA RTX3090 GPU requires about 10 minutes.

Our dataset comprises 35,000 pairs of images and their corresponding sketches in SVG format, spanning 100 object categories. These categories are derived by combining common ones from existing sketch datasets {\cite{Mukherjee2023SEVALS,SketchRNN,SketchyCOCO2020,Eitz2012HowDH} with additional, unique categories such as astronaut, robot and sculpture. These unique categories are not present in prior datasets, highlighting the advantages of a synthetic data approach. The full list of categories is available in Table~\ref{tab:ControlSketch_dataset}.
All the sketches in our data were manually verified, we filtered very few generated images with artifacts that caused artifacts in the generated sketches.
The 15 categories used in training are: angel, bear, car, chair, crab, fish, rabbit, sculpture, astronaut, bicycle, cat, dog, horse, robot, woman. For each of these categories we generated 1200 image-sketch pairs, where 1000 samples are used for training and the rest for testing.
For the rest of 85 categories we created 200 samples per class. 
We show 78 random samples from each class of the training data in \Cref{fig:dog,fig:angle,fig:astronaut,fig:bicycle,fig:chair,fig:bear,fig:car,fig:cat,fig:horse,fig:chair,fig:crab,fig:fish,fig:rabbit,fig:Sculpture,fig:robot,fig:woman}, and 100 random samples from the entire dataset (one of each class) in \Cref{fig:100ControlSketch}.
Since the entire data creation pipeline is fully automated, we continuously extend the dataset and plan to release the code to enable future work in this area.

\begin{table}[h!]
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{|c|c|c|c|c|}
\midrule
\small
airplane & alarm clock & angel & astronaut & backpack \\ \midrule
bear & bed & bee & beer & bicycle \\ \midrule
boat & broccoli & burger & bus & butterfly \\ \midrule
cabin & cake & camel & camera & candle \\ \midrule
car & carrot & castle & cat & cell phone \\ \midrule
chair & chicken & child & cow & crab \\ \midrule
cup & deer & doctor & dog & dolphin \\ \midrule
dragon & drill & duck & elephant & fish \\ \midrule
flamingo & floor lamp & flower & fork & giraffe \\ \midrule
goat & hammer & hat & helicopter & horse \\ \midrule
house & ice cream & jacket & kangaroo & kimono \\ \midrule
laptop & lion & lobster & margarita & mermaid \\ \midrule
motorcycle & mountain & octopus & parrot & pen \\ \midrule
\begin{tabular}[c]{@{}c@{}}pickup \\ truck \end{tabular}  & pig & purse & quiche & rabbit \\ \midrule
robot & sandwich & scissors & sculpture & shark \\ \midrule
sheep & spider & squirrel & strawberry & sword \\ \midrule
t-shirt & table & teapot & television & tiger \\ \midrule
tomato & train & tree & truck & umbrella \\ \midrule
vase & waffle & watch & whale & wine bottle \\ \midrule
woman & yoga & zebra & \begin{tabular}[c]{@{}c@{}}The Eiffel \\ Tower \end{tabular} & book \\ \midrule
\end{tabular}
\caption{The 100 categories of the ControlSketch dataset.}
\label{tab:ControlSketch_dataset}
\end{table}



\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/all1.pdf}
    \caption{100 random samples of sketches generated with ControlSketch.}
    \label{fig:100ControlSketch}
\end{figure*}




\section{ControlSketch Method}
\paragraph{Technical details}
In the ControlSketch optimization, we leverage the pretrained depth ControlNet model \cite{controlnet2023} to compute the SDS loss. The Adam optimizer is employed with a learning rate of 0.8. The optimization process runs for 2000 iterations, taking approximately 10 minutes to generate a single sketch on an RTX 3090 GPU.  However, after 700 iterations most images already yield a clearly identifiable sketch.

\paragraph{Strokes initialization}
The number of areas, $k$, is defined as the rounded square root of the total number of strokes $n$ (for our default number of strokes, 32, $k$ is set to 6). 
Our initialization technique combines between saliency and full coverage of the sketch, which we find to be important when the SDS loss is applied with our spatial control. In \Cref{fig:initalization_vis} we demonstrate how the final sketches will look like when applied with and without our enhances initialization, where the default case is defined based on the attention map as was proposed in CLIPasso \cite{vinker2022clipasso}. As seen, our approach ensures comprehensive object coverage while emphasizing critical areas, resulting in visually effective and recognizable sketches without omitting essential elements. For example, in the lion image, initializing strokes based solely on saliency results in almost all strokes focusing on the lion's head. Consequently, the final sketch omits significant portions of the lion's body.


\paragraph{Spatial control}
The ControlNet model receives two inputs as conditions: the text prompt and the depth condition. The balance between these conditions which is determined  by the conditioning scale parameter influences the final sketch attributes. We found that a conditioning scale of 1.5 provides the best results, effectively maintaining both semantic and geometric attributes of the subject. 

The depth ControlNet model used in the control SDS loss can be replaced with any other ControlNet model, along with the extraction of the appropriate condition from the input image. Different ControlNet models influence the style and attributes of the final sketch. Examples of different sketches generated with different ControlNet models and conditions are shown in Figure~\ref{fig:deifferent_conditions}.


 


\input{tables_supp/different_conditions}



\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figs_sup/initalization_vis.pdf}
    \caption{ Strokes initialization in the ControlSketch method. The "Saliency" column demonstrates the result when strokes are initialized based solely on the attention map (following common practive \cite{vinker2022clipasso}), often leading to an overemphasis on critical regions, such as the lion's head, at the expense of other important parts like the body. The "Saliency + coverage" column showcases our enhanced initialization method, which combines saliency with full object coverage, ensuring both essential details and global object representation are maintained, resulting in complete and recognizable sketches.}
    \label{fig:initalization_vis}
\end{figure}

\section{SwiftSketch}
Our implementation is built on the MDM codebase \cite{tevet2023human}.
Our model consists of 8 self- and cross-attention layers. It was trained with a batch size of 32, a learning rate of $5 \times 10^{-5}$, for 400,000 steps. The refinement network shares the same architecture as our diffusion model and is initialized with its final weights. The timestep condition is fixed at 0. We train the refinement network on the diffusion output sketches from the training dataset, using only the LPIPS loss between the network’s rendered output sketch and the target rendered sketch, as we found it resulting in more polished and visually improved final sketches. The refinement network was trained for 30,000 steps with a learning rate of $5 \times 10^{-6}$.

For training, We scaled the ground truth $(x, y)$ coordinates to the range [-2, 2]. Our experiments revealed that a scaling factor of 2 outperformed the standard value of 1.0 which is used in image generation tasks.
To extract input image features for our model, the image is processed using a pretrained CLIP ResNet model \cite{Radfordclip}, with features extracted from its fourth layer. These features are subsequently refined through three convolutional layers to capture additional spatial details. Each patch embedding is further refined using three linear layers, enhancing feature learning and aligning dimensions for compatibility with the model. The resulting feature representation is seamlessly integrated into the generation process via a cross-attention mechanism.

To encourage the diffusion model to focus on fine details, we adjust the noise scheduler to perturb the signal more subtly for small timesteps, by reducing the exponent in the standard cosine noise schedule proposed in \cite{Nichol2021ImprovedDD} from 2 to 0.4.
Our model $M_{\theta}$ was trained using classifier-free guidance so during inference, we enhance fidelity to the input image by extrapolating the following variants using s= 2.5:
\begin{equation}
    M_{\theta_s}(s^t, t, I) = M_{\theta}(s^t, t, \emptyset) + s \cdot \big( M_{\theta}(s^t, t, I) - M_{\theta}(s^t, t, \emptyset) \big)
\end{equation}.



Figure~\ref{fig:swiftsketch_random} showcases 100 random SwiftSketch samples across all categories in the ControlSketch dataset. The last three rows correspond to classes our model was trained on, while the remaining rows are unseen classes. Each sketch is generated in under a second. The results demonstrate that our model generalizes well to unseen categories, producing sketches with high fidelity to the input images. However, in some cases, high-level details are absent, and the sketch's category label can be difficult to identify. More examples for unseen classes are shown in Figure~\ref{fig:swiftskwtch_unseen1}, Figure~\ref{fig:swiftskwtch_unseen2} and Figure~\ref{fig:swiftskwtch_unseen3}


\section{Qualitative Comparison}
Figure~\ref{fig:comparison_train} and Figure~\ref{fig:comparison_test}  show more examples of qualitative comparison of seen and unseen categories. Input images are shown on the left. From left to right, the sketches are generated using  PhotoSketching \cite{Li2019PhotoSketchingIC}, Chan et al. \cite{Chan2022LearningTG} (in anime style), InstantStyle \cite{Wang2024InstantStyleFL}, and CLIPasso \cite{vinker2022clipasso}. On the right are the resulting sketches from our proposed methods, ControlSketch and SwiftSketch.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/strokes_number_sort.jpeg}
    \caption{Stroke Order Visualization. SwiftSketch generated sketches are visualized progressively, with the stroke count shown on top. The first row for each example is with our sorting technique (w), while the second row omits it (w/o)}
    \label{fig:strokes_number_sort}
\end{figure}



\section{Quantitative Evaluation}
In this section, we present the details of the user study conducted to compare our new optimization method, ControlSketch, with the state-of-the-art optimization method for object sketching, CLIPasso.
We selected 24 distinct categories for the user study: 16 categories from our ControlSketch dataset, and 8 categories from the SketchyCOCO dataset. For each category, we randomly sampled one image. Participants were presented with the input image alongside two sketches—one generated by CLIPasso and the other by ControlSketch—displayed in random order. We asked participants two questions for each pair of sketches: 1. Which sketch more effectively depicts the input image? 2. Which sketch is of higher quality? Participants were required to choose one sketch for each question. A total of 40 individuals participated in the survey. The results are as follows: For the ControlSketch dataset, 87\% of participants chose ControlSketch for the first question, and 88\% for the second question. For the SketchyCOCO dataset—which is more challenging due to its low-resolution images and difficult lighting conditions—90\% chose ControlSketch for the first question, and 93\% for the second question.
These results highlight the significant advantages of ControlSketch over CLIPasso across diverse categories and datasets.


\section{Ablation}
Figure~\ref{fig:comparison_refine} presents a comparison of results with and without the refinement step in the SwiftSketch pipeline. As can be seen, the final output sketches generated by the denoising process of our diffusion model may still retain slight noise. Incorporating the refinement stage significantly enhances the quality and cleanliness of the sketches
Figure~\ref{fig:strokes_number_sort} illustrates the impact of the stroke sorting technique used for training. Early strokes effectively capture the object’s contour and key features, while later strokes refine the details. With sorting, the object is significantly more recognizable with fewer strokes compared to the case without sorting.



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/limitation_supp_new.jpg}
    \caption{Limitations of SwiftSketch. (a) When trained solely on masked object images, SwiftSketch struggles to generate accurate sketches for complex scenes. As shown, it incorrectly assigns strokes to the image frame instead of capturing the scene's key elements. (b) During the refinement stage, fine details particularly facial features are often lost, resulting in oversimplified representations. (c) Sketches may appear unrecognizable. }
    \label{fig:limitations1}
\end{figure}


\section{Limitations}
SwiftSketch, which was trained only on masked object images, faces challenges in handling complex scenes. When provided with a scene image, as illustrated in Figure~\ref{fig:limitations1}(a), SwiftSketch struggles to generate accurate sketches, often misplacing strokes onto the image frame instead of capturing key elements of the scene. Another significant limitation is its tendency to omit fine details, particularly facial features, leading to oversimplified representations, as shown in Figure~\ref{fig:limitations1}(b). In some cases, sketches may appear unrecognizable, as shown in Figure~\ref{fig:limitations1}(c).



\input{tables_supp/quali_refine}


\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figs_sup/ramdom_refine/test.pdf}
    \caption{100 random sapmels of SwiftSketch sketches. The last three rows are seen classes, while the remaining rows are unseen classes}
    \label{fig:swiftsketch_random}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/swiftsketch_unseen_6A.pdf}
    \caption{ Sketches generated by SwiftSketch for unseen categories.}
    \label{fig:swiftskwtch_unseen1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/swiftsketch_unseen_6b.pdf}
    \caption{ Sketches generated by SwiftSketch for unseen categories.}
    \label{fig:swiftskwtch_unseen2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/swiftsketch_unseen_6c.pdf}
    \caption{ Sketches generated by SwiftSketch for unseen categories.}
    \label{fig:swiftskwtch_unseen3}
\end{figure*}




\begin{figure*}
    \centering
    \includegraphics[trim=0cm 0.2cm 0 0cm,clip,width=0.8\linewidth]{figs_sup/comparison_images/titles.pdf}
    \includegraphics[trim=0cm 0cm 0 3.2cm,clip,width=0.8\linewidth]{figs_sup/comparison_images/train2.pdf}
    \caption{Qualitative comparison, seen categories}
    \label{fig:comparison_train}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[trim=0cm 0.2cm 0 0cm,clip,width=0.8\linewidth]{figs_sup/comparison_images/titles.pdf}
    \includegraphics[trim=0cm 0cm 0 3.2cm,clip,width=0.8\linewidth]{figs_sup/comparison_images/test.pdf}
    \caption{Qualitative comparison, unseen categories}
    \label{fig:comparison_test}
\end{figure*}



 
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/dog.pdf}
    \caption{Dog - SwiftSketch training data examples }
    \label{fig:dog}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/horse.pdf}
    \caption{Horse - SwiftSketch training data examples }
    \label{fig:horse}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/cat.pdf}
    \caption{Cat - SwiftSketch training data examples}
    \label{fig:cat}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/angel.pdf}
    \caption{Angel - SwiftSketch training data examples}
    \label{fig:angle}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/astronaut.pdf}
    \caption{Astronaut - SwiftSketch training data examples}
    \label{fig:astronaut}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/bear.pdf}
    \caption{Bear - SwiftSketch training data examples}
    \label{fig:bear}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/bicycle.pdf}
    \caption{Bicycle - SwiftSketch training data examples}
    \label{fig:bicycle}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/car.pdf}
    \caption{Car - SwiftSketch training data examples}
    \label{fig:car}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/chair.pdf}
    \caption{Chair - SwiftSketch training data examples }
    \label{fig:chair}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/crab.pdf}
    \caption{Crab - SwiftSketch training data examples}
    \label{fig:crab}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/fish.pdf}
    \caption{Fish - SwiftSketch training data examples}
    \label{fig:fish}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/rabbit.pdf}
    \caption{Rabbit - SwiftSketch training data examples}
    \label{fig:rabbit}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/sculpture.pdf}
    \caption{:Sculpture - SwiftSketch training data examples }
    \label{fig:Sculpture}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/robot.pdf}
    \caption{Robot - SwiftSketch training data examples }
    \label{fig:robot}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs_sup/data/woman.pdf}
    \caption{Woman - SwiftSketch training data examples }
    \label{fig:woman}
\end{figure*}


