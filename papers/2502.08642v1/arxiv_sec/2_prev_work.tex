\section{Related Work}
\paragraph{\textbf{Sketch Datasets}}
Existing sketch datasets are primarily composed of human-drawn sketches, and are designed to accomplish different sketching tasks. Class-conditioned datasets \cite{Eitz2012HowDH,SketchRNN} are particularly common, with the largest being the QuickDraw dataset \cite{SketchRNN}, containing 50 million sketches spanning 345 categories. Datasets of image-referenced sketches cover a spectrum of styles, including image trace and contours \cite{Wang2021Tracing,Li2019PhotoSketchingIC,ArbelaezBSDS500,Eitz2012HowDH}, or more abstract but still fine-grained depictions \cite{Sangkloy2016TheSD,SketchyCOCO2020}, and very abstract sketches \cite{Mukherjee2023SEVALS}. These large-scale datasets are often created by non-artists.
Efforts have been made to collect sketches from professionals \cite{Berger2013StyleAA, Gryaditskaya2019OpenSketch, Han2023AGF, Xiao2022DifferSketching}, but these datasets are often smaller in scale, and are limited to specific domains like portraits \cite{Berger2013StyleAA} or household items \cite{Gryaditskaya2019OpenSketch}. These constraints make them unsuitable for training generative models that can generalize broadly to diverse concepts.

\paragraph{\textbf{Data-Driven Sketch Generation}}
These datasets have facilitated data-driven approaches for various sketch-related tasks \cite{SurveySketchxu2020deep}. Multiple generative frameworks and architectures have been explored for vector sketch generation, including RNNs \cite{SketchRNN}, BERT \cite{Lin2020SketchBERTLS}, Transformers \cite{Bhunia2020EdinburghRE, Ribeiro2020SketchformerTR}, CNNs \cite{Kampelmhler2020SynthesizingHS,Chen2017Sketchpix2seqAM,Song2018LearningTS}, LSTMs \cite{Qi2021SketchLatticeLR, Song2018LearningTS}, GANs \cite{V2019TeachingGT}, reinforcement learning \cite{Zhou2018LearningTD,Muhammad2018LearningDS}, and diffusion models \cite{wang2023sketchknitter}. However, these methods are fundamentally designed to operate in a class-conditioned manner, restricting their ability to generate sketches to only the classes included in the training data. Additionally, they rely on crowd-sourced datasets which contain non-professional sketches, restricting their ability to handle more complex or artistic styles.
On the other hand, existing works for generating more professionally looking sketches are either restricted to specific domains \cite{Liu_2021_ICCV} or can only generate sketches in pixel space \cite{Li2019PhotoSketchingIC,Chan2022LearningTG}.
Note that image-to-sketch generating can be formulated as a style transfer task, with recent works that employ the text-to-image diffusion priors achieving highly artistic results with high fidelity \cite{Wang2024InstantStyleFL,frenkel2024implicit,hertz2023StyleAligned}, however, all of these works also operate only in pixel space.
In contrast, we focus on vector sketches due to their resolution independence, smooth and clean appearance, control over abstraction, and editable nature.



\paragraph{\textbf{VLMs for Vector Sketches}}
To reduce reliance on existing vector datasets, recent research leverages the rich priors of large pre-trained vision-language models (VLMs) in a zero-shot manner. Early methods \cite{vinker2022clipasso,Frans2021CLIPDrawET,Vinker2022CLIPasceneSS} utilize CLIP \cite{Radfordclip} as the backbone for image- and text-conditioned generation. These approaches iteratively optimize a randomly initialized set of strokes using a differentiable renderer \cite{Li2020DifferentiableVG} to bridge the gap between vector and pixel representations. More recently, text-to-image diffusion models \cite{rombach2022highresolution} have been employed as backbones, with the SDS loss \cite{Poole2022DreamFusionTU} used to guide the optimization process, achieving superior results \cite{jain2022vectorfusion,svgdreamer_xing_2023,Xing2023DiffSketcherTG}. However, the use of the SDS loss has so far been limited to text-conditioned generation.
While these approaches yield highly artistic results across diverse concepts, they are computationally expensive, relying on iterative backpropagation.


\paragraph{\textbf{Diffusion Models for Non-Pixel Data}}
Diffusion models have emerged as a powerful generative framework, extending their impact beyond traditional pixel-based data. Recent research demonstrates their versatility across diverse domains, including tasks such as human motion synthesis \cite{tevet2023human}, 3D point cloud generation \cite{Luo2021DiffusionPM, Huang2025SPAR3DSP}, and object detection reframed as a generative process \cite{Chen2022DiffusionDetDM}.
Some prior works have explored diffusion models for vector graphics synthesis. 
VecFusion \cite{Thamizharasan_2024_CVPR} uses a two-stage diffusion process for vector font generation but its architecture and vector representation are highly complex and specialized for fonts, limiting adaptability to other vector tasks. SketchKnitter \cite{wang2023sketchknitter} and Ashcroft~\etal~\cite{ashcroft2024modelling} generate vector sketches using a diffusion-based model trained on the QuickDraw and Anime-Vec10k dataset, but without conditioning on images or text inputs.



















\section{Preliminaries}
\label{sec:preliminaries}
\paragraph{\textbf{Diffusion Models}}
\label{sec:diffusion}
Diffusion models \cite{ddpm2020Ho,song2021ddim} are a class of generative models that learn a distribution by gradually denoising a Gaussian. 
Diffusion models consist of a forward process $q(x_t|x_{t-1})$ that progressively noises data samples $x_0 \sim p_{data}$ at different timesteps $t\in [1,T]$, and a backward or reverse process $p(x_{t-1}|x_t)$ that progressively cleans the noised signal. 
The reverse process is the generative process and is approximate with a neural network $\epsilon_{\theta}(x_t,t)$.
During training, a noised signal at differnet timesteps is derived from a sample $x_0$ as follows:
\begin{equation}
\label{eq:xt}
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon,
\end{equation}
where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$, and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$ is called the noise scheduler.
The common approach for training the model is with the following simplified objective:
\begin{equation}
\label{eq:lsimple}
L_\text{simple} = \mathbb{E}_{x_0\sim q(x_0), \epsilon \sim \mathcal{N}(0, \mathbf{I}), t \sim \mathcal{U}(1, T)} \big\| \epsilon - \epsilon_\theta(x_t, t) \big\|^2.
\end{equation}
At inference, to generate a new sample, the process starts with a Gaussian noise $x_T \sim \mathcal{N}(0, \mathbf{I})$ and the denoising network is applied iteratively for $T$ steps, yielding a final sample $x_0$.













\paragraph{\textbf{SDS Loss}}
The Score Distillation Sampling (SDS) loss \cite{Poole2022DreamFusionTU} is used to extract signals from a pretrained text-to-image diffusion model to optimize a parametric representation. For vector graphics, the parameters $\phi$ defining an SVG can be optimized using the SDS loss to represent a desired textual concept. A differentiable rasterizer \cite{Li2020DifferentiableVG} rasterize $\phi$ into a pixel image $x$, which is then noised to produce $x_t$ at a sampled timestep $t$. This noised image, conditioned on a text prompt $c$, is passed through the pretrained diffusion model, $\epsilon_\theta(x_t, t, c)$.
The deviation of the diffusion loss in \cref{eq:lsimple} is used to approximate the gradients of the initial image synthesis model's parameters, $\phi$, to better align its outputs with the conditioning prompt. Specifically, the gradient of the SDS loss is defined as:
\begin{equation}\label{eq:sds_loss}
    \nabla_\phi \mathcal{L}_{SDS} = \left[ w(t)(\epsilon_\theta(x_t,t,y) - \epsilon) \frac{\partial x}{\partial \phi} \right] ,
\end{equation}
where $w(t)$ is a constant that depends on $\alpha_t$. This optimization process iteratively adjusts the parametric model.



