









\section{Method}
Our method consists of three key components: (1) ControlSketch, an optimization-based technique for generating high-quality vector sketches of input objects; (2) a synthetic paired image-sketch dataset, created using ControlSketch; and (3) SwiftSketch, a diffusion model trained on our dataset for efficient sketch generation.



\subsection{ControlSketch}
\label{sec:controlsketch}
Given an input image $I$ depicting an object, our goal is to generate a corresponding sketch $S$ that maintains high fidelity to the input while preserving a natural sketch-like appearance. Following common practice in the field, we define $S$ as a set of $n$ strokes $\{s_i\}_{i=1}^n$, where each stroke is a two-dimensional cubic BÃ©zier curve: $s_i = \{p_j^i\}_{j=1}^4 = \{(x_j, y_j)^i\}_{j=1}^4$. 
We optimize the set of strokes using the standard SDS-based optimization pipeline, as described in \Cref{sec:preliminaries}, with two key enhancements: an improved stroke initialization process and the introduction of spatial control. Our process rely on the image's attention map $I_{attn}$, depth map $I_{depth}$, and caption $y$, extracted using DDIM inversion \cite{song2021ddim}, MiDaS \cite{birkl2023midasv31model}, and BLIP2 \cite{li2023blip2bootstrappinglanguageimagepretraining} respectively.
While previous approaches \cite{vinker2022clipasso,Xing2023DiffSketcherTG} sample initial stroke locations based on the image's attention map, we observe that this method often results in missing areas in the output sketch, especially when spatial control is applied. To address this, we propose an enhanced initialization method (see \cref{fig:ControlSketch_pipeline}, left) that ensures better coverage. We divide the object area into $k=6$ equal-area regions (\cref{fig:ControlSketch_pipeline}c), using a weighted K-Means method that accounts for both attention weights and pixel locations. We distribute $\frac{n}{2}$ points equally across the regions, while the remaining $\frac{n}{2}$ points are allocated proportionally to the average attention value in each region. This means that more points are assigned to regions with higher attention. Within each region, the points are evenly spaced to further ensure good coverage.
This process determines the location of the initial set of strokes' control points to be optimized, as demonstrated in \Cref{fig:ControlSketch_pipeline}d.

The stroke optimization process is depicted in \Cref{fig:ControlSketch_pipeline}, right.
At each optimization step, the rasterized sketch $\mathcal{R}(\{s_i\}_{i=1}^n)$ is noised based on $t$ and $\epsilon$, then fed into a depth ControlNet text-to-image diffusion model \cite{controlnet2023}. The model predicts the noise $\hat{\epsilon}$ conditioned on the caption $y$ and the depth map $I_{depth}$.
We balance the weighting between the spatial and textual conditions to achieve an optimal trade-off between ``semantic'' fidelity, derived from $y$ (ensuring the sketch is recognizable), and ``geometric'' fidelity, derived from $I_{depth}$, which governs the accuracy of the spatial structure. 





\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/controlsketch_pipe.pdf}
    \caption{ControlSketch Pipeline. Left: The object area is divided into $k$ regions (c), with $n$ points distributed based on attention values from (b) while ensuring a minimum allocation per region. (d) The initial strokes are derived from these points. Right: The initial strokes are iteratively optimized to form the sketch. At each iteration, the rasterized sketch is noised based on $t$ and $\epsilon$ and fed into a diffusion model with a depth ControlNet conditioned on the image's depth and caption $y$. The predicted noise $\hat{\epsilon}$ is used for the SDS loss.}
    \label{fig:ControlSketch_pipeline}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/strokes_sorting.pdf}
    \caption{(a) Input image. (b) Object mask. (c) The object's contour is extracted from the mask using morphological operations, and sketch pixels that intersect with the contour are given higher weight. (d) Attention map. (e) We sort the strokes based on a combination of contour intersection count and attention score. (f) A visualization of the first 16 strokes in the ordered sketch, demonstrating the effectiveness of our sorting scheme.}
    \label{fig:sketchsort}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/diffusion_pipeline.pdf}
    \caption{SwiftSketch Training Pipeline. At each training iteration, an image $I$ is passed through a frozen CLIP image encoder, followed by a lightweight CNN, to produce the image embedding $I_e$. The corresponding vector sketch $S^0$ is noised based on the sampled timestep $t$ and noise $\epsilon$, forming $S^t$ (with $\mathcal{R}(S^t)$ illustrating the rasterized noised sketch, which is not used in training). The network $M_\theta$, a transformer decoder, receives the noised signal  $S^t$ and is tasked with predicting the clean signal $\hat{S^0}$, conditioned on the image embedding $I_e$ and the timestep $t$ (fed through the cross-attention mechanism). The network is trained with two loss functions: one based on the distance between the control points and the other on the similarity of the rasterized sketches.}\vspace{-0.2cm}
    \label{fig:diffusion_pipe}
\end{figure*}

\subsection{The ControlSketch Dataset}
\label{sec:data}
We utilize ControlSketch to generate a paired image-vector sketch dataset. Each data sample comprises the set $\{I$, $I_{attn}$, $I_{depth}$, $I_{mask}$, $S$, $c$, $y\}$, which includes, respectively, the image, its attention map, depth map, and object mask, along with the corresponding vector sketch of the object, class label, and caption. 
To generate the images, we utilize SDXL \cite{podell2023sdxlimprovinglatentdiffusion}, along with a prompt template designed to produce images for each desired class $c$ (an example of a generated image for the class ``lion'' is shown in \Cref{fig:ControlSketch_pipeline}a).
We then apply ControlSketch on the masked images to generate the corresponding vector sketches. Additional details are provided in the supplementary.
Optimization-based methods, such as ControlSketch, do not impose an inherent stroke ordering. Learning an internal stroke order enables the generation of sketches with varying levels of abstraction by controlling the number of strokes generated.
Thus, we propose a heuristic stroke-sorting scheme that prioritizes contour strokes and those depicting salient regions (illustrated in \Cref{fig:sketchsort}). Consequently, each vector sketch $S$ is represented as an ordered sequence of strokes $(s_1, \dots, s_n)$.



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/inference_refine.pdf}
    \vspace{-0.4cm}
    \caption{Inference Process. Starting with randomly sampled Gaussian noise $S^T \sim \mathcal{N}(0, \mathbf{I})$, the model $M_{\theta}$ predicts the clean sketch $\hat{S}^0 = M_{\theta}(S^t, t, I_e)$ at each step $t$, which is then re-noised to $S^{t-1}$. This iterative process is repeated for $T$ steps and is followed by a final feed-forward pass through a refinement network, $M_{\theta^*}$, which is a trainable copy of $M_{\theta}$, specifically trained to correct very small residual noise.}
    \label{fig:Inference}
\end{figure}

\input{tables/denoising}


\subsection{SwiftSketch}
\label{sec:swiftsketch}
We utilize the ControlSketch dataset to train a generative model $M_{\theta}$ that learns to efficiently produce a vector sketch from an input image $I$.
We define $M_{\theta}$ as a transformer decoder to account for the discrete and long-range dependencies inherent in vector sketches.
The training of $M_{\theta}$ follows the standard conditional diffusion framework, as outlined in \Cref{sec:diffusion}, with task-specific modifications to address the characteristics of vector data and the image-to-sketch task. In our case, the model learns to denoise the set of $(x,y)$ coordinates that define the strokes in the sketch. 

The training process is depicted in \Cref{fig:diffusion_pipe}. 
At each iteration, a pair $(I, S^0)$ is sampled from the dataset, where $S^0\in R^{2\times 4 \times n}$ is the clean sketch in vector representation, and $\mathcal{R}(S^0)$ denotes the corresponding rasterized sketch in pixel space, with $\mathcal{R}$ being a differentiable rasterizer \cite{Li2020DifferentiableVG}. 
The image $I$ is processed using a pretrained CLIP ResNet model \cite{Radfordclip}, where features are extracted from its fourth layer, recognized for effectively capturing both geometric and semantic information \cite{vinker2022clipasso}. These features are then refined through a lightweight CNN to enhance learning and align dimensions for compatibility with $M_\theta$. This process yields the image embedding $I_e$.
At each iteration, we sample a timestep $t \sim \mathcal{U}(1,T)$ and noise $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ to define $S^t$:
\begin{equation}
S^t = \sqrt{\bar{\alpha}_t} S^0 + \sqrt{1 - \bar{\alpha}_t} \epsilon,
\label{eq:St}
\end{equation} 

where $\bar{\alpha}_t$ is the noise scheduler as a function of $t$. As illustrated in \Cref{fig:diffusion_pipe}, $S^t$ represents a noised version of $S^0$ in vector space, with the level of noise determined by the timestep $t$. 
The control points $\{s_1^t,\dots s_n^t\}$ are fed into the network $M_{\theta}$, where they are first encoded via a linear layer (depicted in green), and combined with a standard positional embedding before being passed through the transformer decoder (in pink), which consists of 8 layers of cross-attention and self-attention. The encoded timestep $t$ and image features $I_e$ are fed into the transformer through the cross-attention mechanism.
The decoder output is projected back to the original points dimension through a linear layer, yielding the prediction $M_{\theta}(S_t,t,I_e) = \hat{S^0}$. 

We train $M_{\theta}$ with two training objectives $\mathcal{L}_{\text{points}}$ and $\mathcal{L}_{\text{raster}}$, applied on both the vector and raster representation of the sketch:
\vspace{-0.1cm}
\begin{equation}
\label{eq:1}
\begin{aligned}
\mathcal{L}_{\text{points}} = \| S^0 - \hat{S}^0 \|_1 = \sum_{i=1}^{n} \| s_i^0-\hat{s_i^0}\|_1,  \\ 
\mathcal{L}_{\text{raster}} = LPIPS \left( \mathcal{R}(S^0), \mathcal{R}(\hat{S}^0)\right), 
\end{aligned}
\end{equation}

where $\mathcal{L}_{\text{points}}$ is defined by the $L_1$ distance between the sorted control points of the ground truth sketch $S^0$ and the predicted sketch $\hat{S}^0$, and $\mathcal{L}_{\text{raster}}$ is the LPIPS distance \cite{zhang2018perceptual} between the rasterized sketches.
$\mathcal{L}_{\text{points}}$ encourages per-stroke precision, while $\mathcal{L}_{\text{raster}}$ encourages the generated sketch to align well with the overall structure of the ground truth sketch. 
Together, our training loss is: $\mathcal{L} = \mathcal{L}_{\text{points}} + \lambda \mathcal{L}_{\text{raster}}$, with $\lambda=0.2$.

As is often common, to apply classifier-free guidance \cite{Ho2022ClassifierFreeDG} at inference, we train $M_{\theta}$ to learn both the conditioned and the unconditioned distributions by randomly setting \( I = \emptyset \) for 10\% of the training steps.




The inference process is illustrated in \Cref{fig:Inference}. The model, $M_{\theta}$, generates a new sketch by progressively denoising randomly sampled Gaussian noise, $S^T \sim \mathcal{N}(0, \mathbf{I})$. At each step $t$, $M_{\theta}$ predicts the clean sketch $\hat{S^0} = M_{\theta}(S^t, t, I_e)$, conditioned on the image embedding $I_e$ and time step $t$. The next intermediate sketch, $S^{t-1}$, is derived from $\hat{S^0}$ using \Cref{eq:St}. This process is repeated for $T$ steps.
We observe that the final output sketches from the denoising process may retain slight noise. This is likely because the network prioritizes learning to clean heavily noised signals during training, while small inaccuracies in control point locations have a smaller impact on the loss function, leading to reduced precision at finer timesteps.
To address this, we introduce a refinement stage, where a learned copy of our network, $M_{\theta^*}$, is fine-tuned to perform an additional cleaning step. This refinement network is trained in a manner similar to the original model, with the objective of denoising a slightly noised sketch, conditioned on the same input image features, while the timestep condition is fixed at 0. More details are provided in the supplementary. This refinement stage is inspired by similar strategies employed in the pixel domain \cite{podell2023sdxlimprovinglatentdiffusion,saharia2022photorealistic}, where additional processing steps are used to improve the quality and resolution of generated images. As illustrated in \Cref{fig:Inference}, after the final denoising step of $M_\theta$ is applied, $\hat{S^0}$ is passed through $M_{\theta^*}$ to perform the additional refinement.










\begin{figure}
    \centering
    \setlength{\tabcolsep}{0pt}
    {\small
    \begin{tabular}{c c c c c c }
    Input & 12s & 17s & 22s& 27s & 32s\\

         \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/dog_910/dog_910_input.jpg} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/dog_910/dog_910_12_strokes.png} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/dog_910/dog_910_17_strokes.png} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/dog_910/dog_910_22_strokes.png} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/dog_910/dog_910_27_strokes.png} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/dog_910/dog_910_32_strokes.png} \\

        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/car_112/car_112_input.jpg} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/car_112/car_112_12_strokes.png} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/car_112/car_112_17_strokes.png} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/car_112/car_112_22_strokes.png} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/car_112/car_112_27_strokes.png} &
        \includegraphics[width=0.17\linewidth]{figs/best_sketches_strokes_final/car_112/car_112_32_strokes.png}\\

    \end{tabular}
    }
    \caption{Stroke Order Visualization. Generated sketches are visualized progressively, with the stroke count shown on top. Early strokes capture the object's contour and key features, while later strokes add finer details.}
    \label{fig:order}
\end{figure}

\subsection{Implementation Details}
ControlSketch requires approximately 2,000 steps to converge, taking around $10$ minutes on a standard RTX3090 GPU.
SwiftSketch is trained with $T = 50$ noising steps, to support fast generation. 
To encourage the model to focus on fine details, we adjust the noise scheduler to perturb the signal more subtly for small timesteps compared to the cosine noise schedule proposed in \cite{Nichol2021ImprovedDD}.
The model is trained on images from 15 classes, with 1,000 samples per class. The training process spans 400K steps, requiring approximately six days on a single A100 GPU. At inference, we use a guidance scale of 2.5. Our synthetic dataset includes an additional 200 test samples for the 15 training classes, as well as 85 additional object categories, each with 200 samples. Additional implementation details, as well as detailed class labels and dataset visualizations are provided in the supplementary material.












    



    
