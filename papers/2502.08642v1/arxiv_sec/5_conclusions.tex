

\section{Ablation}
We evaluate the contribution of SwiftSketch's main components by systematically removing each one and retraining the network. Specifically, we examine the impact of excluding the LPIPS loss, the L1 loss, and the sorting technique, as well as the effect of incorporating the refinement network.
The results are summarized in \Cref{tb:ablation_metrics}, where ``Full'' represents our complete diffusion pipeline prior to refinement, and ``+Refine'' denotes the inclusion of the refinement stage.
Notably, removing the L1 loss results in a significant drop in performance, highlighting its essential role in the training process. Excluding the LPIPS loss negatively impacts performance, particularly in unseen classes. 
The metrics indicate comparable performance in the absence of the sorting stage. While the resulting sketches may appear visually similar, the sorting stage is crucial for supporting varying levels of abstraction. Although the network can be trained without this stage and still achieve reasonable results, learning an internal stroke order provides a foundation for training across abstraction levels, where sketches implicitly encode the importance of strokes.
The refinement stage enhances recognizability, especially in unseen categories where the output sketches from the diffusion process are noisier. We further illustrate the impact of the refinement network in \Cref{fig:refine_ablation}, with additional results provided in the supplementary. 


\input{arxiv_tables/metrics_ablation}
\section{Limitations and Future Work}
While SwiftSketch can generate vector sketches from images efficiently, it comes with limitations.
First, although SwiftSketch performs well on seen categories, as evidenced by our evaluation, its performance decreases for unseen categories. This is particularly apparent in categories that differ significantly from those seen during training (e.g., non-human or non-animal objects). Failure cases often exhibit a noisy appearance or are entirely unrecognizable, such as the carrot in \Cref{fig:limitations}. Expanding the number of training categories in future work could enhance the model's generalization.
Second, our refinement stage, which is meant to fix the noisy appearance, might over-simplify the sketches, resulting in lost details such as the nose and eyes of the cow in \cref{fig:limitations}.
Lastly, in the scope of this paper, we trained SwiftSketch on sketches with a fixed number of strokes (32). Extending the training to sketches with varying numbers of strokes, spanning multiple levels of abstraction, presents an exciting direction for future research. Our transformer-decoder architecture is inherently suited for such an extension, and our results show that the network can capture essential features from sorted sketches, highlighting its potential to effectively handle more challenging levels of abstraction.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figs/sketch_limitations2.pdf}\vspace{-0.45cm}
    \caption{Limitations. (a) Sketches may appear unrecognizable (e.g., carrot) or noisy (e.g., Eiffel Tower). (b) The refinement stage can lead to the loss of fine details, such as the cow's nose and eye.}
    \label{fig:limitations}
\end{figure}





\section{Conclusions}
We introduced SwiftSketch, a method for object sketching capable of generating plausible \emph{vector} sketches in under a second. SwiftSketch employs a diffusion model with a transformer-decoder architecture, generating sketches by progressively denoising a Gaussian distribution in the space of stroke control points.
To address the scarcity of professional-quality paired vector sketch datasets, we constructed a synthetic dataset spanning 100 classes and over 35,000 sketches. This dataset was generated using ControlSketch, an improved SDS-based sketch generation method enhanced with a depth ControlNet for better spatial control. We demonstrated both visually and numerically that ControlSketch produces high-quality, high-fidelity sketches and that SwiftSketch effectively learns the data distribution of ControlSketch, achieving high-quality sketch generation while reducing generation time from approximately 10 minutes to 0.5 seconds.
We believe this work represents a meaningful step toward real-time, high-quality vector sketch generation with the potential to enable more interactive processes. Additionally, our extensible dataset construction process will be made publicly available to support future research in this field.


