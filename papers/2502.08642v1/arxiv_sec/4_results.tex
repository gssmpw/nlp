



\input{arxiv_tables/quali_all}

\section{Results}
We begin by showcasing \methodname's ability to generate high-quality vector sketches for a diverse set of input images.
SwiftSketch successfully generalizes to unseen images within the training categories (\Cref{fig:qualitative-swift-seen}), creating sketches that depict the input images well while demonstrating a plausible and detailed appearance. On images of unseen categories that pose greater challenges, SwiftSketch effectively captures the essential features of the input images, producing abstract yet faithful representations (\Cref{fig:qualitative-swift-unseen}). Notably, all sketches are provided in vector format, and are generated in just 50 diffusion steps, followed by a single refinement step, with the entire process taking less than one second.
In \Cref{fig:denoising}, we illustrate the denoising steps of the generation process, starting from a Gaussian distribution and progressively refining towards the data distribution. 
In \Cref{fig:order}, we demonstrate the ability of our method to create level-of-abstraction using our ordered stroke technique. We visualize the progressive addition of strokes in the sequence they appear in the output SVG file. Note how the first strokes already convey the intended concept effectively. Additional results of both SwiftSketch and ControlSketch are available in the supplementary.





\subsection{Comparisons}
We evaluate the performance of SwiftSketch and ControlSketch with respect to state-of-the-art methods for image-to-sketch generation, including Photo-Sketching \cite{Li2019PhotoSketchingIC}, Chan \etal \cite{Chan2022LearningTG}, InstantStyle \cite{Wang2024InstantStyleFL}, and CLIPasso \cite{vinker2022clipasso}. InstantStyle is applied with a sketch image as the style reference.
\Cref{fig:comparison} shows representative results from each method, with XDoG \cite{Winnemller2011XDoGAI}, a classic edge detection technique, shown on the left as a baseline.
The sketches of Chan \etal and InstantStyle are detailed and align well with the overall structure of the input images. However note that they closely follow the edge maps shown on the left. The sketches of Photo-Sketching (fifth column) are more abstract, but can fail to effectively capture the images' content in a natural way. While these approaches are efficient, producing sketches in less than a minute, they focus on generating \emph{raster} sketches. In contrast, our method produces vector sketches, which are resolution-independent, easily editable, and exhibit a smooth, clean style.
CLIPasso (sixth column) generates vector sketches that achieve a good balance between fidelity and semantics. However, it is significantly slower, requiring 5 minutes to produce a single sketch, and it may introduce artifacts, such as the noisy overlapping strokes observed in the robot example.
ControlSketch (seventh column) produces high-fidelity sketches that remain abstract, smooth, and natural, effectively depicting the input images while avoiding artifacts. However, it is even slower than CLIPasso, as SDS-based methods generally require more time to converge, making it impractical for interactive applications.
SwiftSketch, shown in the rightmost column, successfully learns the data distribution from ControlSketch samples, enabling it to produce sketches that approach the quality of optimization-based techniques but in real time.
Additional results are available in the supplamentary material. 

\input{arxiv_tables/metrics}

\paragraph{Quantitative Evaluation} We sample 4,000 images from our dataset (2,000 from our test set of categories seen during training and 2,000 from unseen categories) and additional 2,000 images from the SketchyCOCO \cite{SketchyCOCO2020} dataset to assess generalization on external data. Each set consists of 10 randomly selected categories with 200 images per category. 
Following common practice in the field, we use the CLIP zero-shot classifier \cite{Radfordclip} to assess class-level recognition, MS-SSIM \cite{wang2003multiscale} for image-sketch fidelity following the settings proposed in CLIPascene \cite{Vinker2022CLIPasceneSS}, and DreamSim \cite{fu2023dreamsim}. 
The results are presented in \Cref{tb:clip_metrics}, where scores for each data type are reported separately, with human sketches from the SketchyCOCO dataset included as a baseline.
Chan \etal and InstantStyle achieve the highest scores across most metrics due to their highly detailed sketches, which closely resemble the image's edge map. This level of detail ensures that their sketches are both easily recognizable as depicting the correct class (as indicated by the CLIP score) and exhibit high fidelity (as reflected in other measurements).
The results show that SwiftSketch generalizes well to test set images from seen categories, as evidenced by its similar scores to ControlSketch (which serves as the ground truth in our case). However, its performances decrease for unseen categories, particularly in class-based recognition. This is especially apparent on the SketchyCOCO dataset, which is highly challenging due to its low-resolution images and difficult lighting conditions. It is important to note that SwiftSketch is trained on only 15 image categories due to limited resources, suggesting that more extensive training could improve its generalization capabilities.

The results demonstrate that ControlSketch produces sketches that are both highly recognizable and of high fidelity, outperforming alternative methods, particularly on the SketchyCOCO dataset. To further highlight the advantages of ControlSketch over CLIPasso, we conduct a two-alternative forced-choice (2AFC) perceptual study with 40 participants. Each participant was shown pairs of sketches generated by the two methods (presented in random order) alongside the input image and asked to choose the sketch they perceived to be of higher quality. The study included 24 randomly selected sketches from both our dataset and SketchyCOCO, spanning 24 object classes. Participants rated sketches generated by ControlSketch as higher quality in 89\% of cases. Examples of sketches presented in the user study are shown in \Cref{fig:comparison_opt}.
 











 



   
    
