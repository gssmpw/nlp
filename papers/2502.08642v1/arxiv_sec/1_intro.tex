\section{Introduction}
In recent years, several works have explored the task of generating sketches from images, tackling both scene-level and object-level sketching \cite{Chan2022LearningTG, vinker2022clipasso, Vinker2022CLIPasceneSS, svgdreamer_xing_2023}. This task involves transforming an input image into a line drawing that captures its key features, such as structure, contours, and overall visual essence.
Sketches can be represented as pixels or vector graphics, with the latter often preferred for their resolution independence, enhanced editability, and ability to capture sketches' sequential and abstract nature.
Existing vector sketch generation methods often involve training a network to learn the distribution of human-drawn sketches~\cite{SurveySketchxu2020deep}.
However, collecting human-drawn sketch datasets is labor-intensive, and crowd-sourced contributors often lack artistic expertise, resulting in datasets that primarily feature amateur-style sketches (\cref{fig:sketch_data_example}, left). 
On the other hand, sketch datasets created by professional designers or artists are typically limited in scale, comprising only a few hundred samples, and are often restricted to specific domains, such as portraits or product design (\cref{fig:sketch_data_example}, right). Therefore, existing data-driven sketch generation methods are often restricted to specific domains or reflect a non-professional style present in the training data.

With recent advancements in Vision-Language Models (VLMs) \cite{zhang2024visionlanguagemodelsvisiontasks}, new approaches have emerged in the sketch domain, shifting sketch generation from reliance on human-drawn datasets to leveraging the priors of pretrained models \cite{Frans2021CLIPDrawET,vinker2022clipasso,Vinker2022CLIPasceneSS,Xing2023DiffSketcherTG}.
These methods generate professional-looking sketches by optimizing parametric curves to represent an input concept, guided by the pretrained VLM.
However, they have a significant drawback: The generation process depends on repeated feedback (backpropagation) from the pretrained model, which is inherently time-consuming -- often requiring from several minutes to over an hour to produce a single sketch.
This makes these approaches impractical for interactive applications or for tasks that require large-scale sketch data generation.

In this work, we introduce \textit{\methodname}, a diffusion-based object sketching method capable of generating high-quality vector sketches in under a second per sketch. SwiftSketch can generalize across a wide range of concepts and produce sketches with high fidelity to the input image (see Figure~\ref{fig:teaser}).

Inspired by recent advancements in diffusion models for non-pixel data \cite{tevet2023human,Luo2021DiffusionPM,Thamizharasan_2024_CVPR}, we train a diffusion model that learns to map a Gaussian distribution in the space of stroke coordinates to the data distribution (see Figure~\ref{fig:teaser}, top).
To address the discrete nature of vector graphics and the complex global topological relationships between shapes, we employ a transformer-decoder architecture with self- and cross-attention layers, trained to reconstruct ground truth sketches in both vector and pixel spaces.
The image condition is integrated into the generation process through the cross-attention mechanism, where meaningful features are first extracted from the input image using a pretrained CLIP image encoder \cite{Radfordclip}.


With the lack of available professional-quality paired vector sketch datasets, we construct a \emph{synthetic} dataset to train our network.
The input images are generated with SDXL \cite{podell2023sdxlimprovinglatentdiffusion}, and their corresponding vector sketches are produced with a novel optimization-based technique we introduce called \emph{ControlSketch}. 
ControlSketch enhances the SDS loss \cite{Poole2022DreamFusionTU}, commonly used for text-conditioned generation, by integrating a depth ControlNet \cite{controlnet2023} into the loss, enabling object sketch generation with spatial control.
Our dataset comprises over 35,000 high-quality vector sketches across 100 classes and is designed for scalability. 
We demonstrate SwiftSketch's capability to generate high-quality vector sketches of diverse concepts, balancing fidelity to input images and the abstract appearance of natural sketches.























\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/sketch_examples.pdf}
    \vspace{-0.5cm}
    \caption{Amateur vs. Professional Sketches. (a) QuickDraw \cite{SketchRNN} and (b) Sketchy \cite{Sangkloy2016TheSD} are large-scale datasets, with Sketchy offering more fine-grained sketches, though both exhibit non-professional style. (c) OpenSketch \cite{Gryaditskaya2019OpenSketch} and (d) Berger \etal \cite{Berger2013StyleAA} contain professional sketches but are limited in scale and focus on specific domains.}
    \vspace{-0.5cm}
    \label{fig:sketch_data_example}
\end{figure}
