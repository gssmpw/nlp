\section{Related Work}
\label{rw}

\subsection{Semantic Communication} 
With the advancement of DL, numerous DL-enabled semantic communication systems have been introduced. In such semantic communication systems, DL models are utilized to autonomously learn the extraction and transmission of semantic knowledge, tailored to the data distribution, specific tasks, and the status of the communication channel \cite{lan2021semantic,qin2021semantic,zhang2022deep,zhang2023predictive}. 
There are two main kinds of semantic communication systems: task-oriented systems and full data reconstruction systems. The task-oriented semantic communications train the decoder for specified tasks, such as face identification and speech recognition \cite{xie2022task,xie2021deep,xie2021task,zhang2023toward}. In contrast, the full data reconstruction systems train the decoder to recover the source data \cite{hu2023robust,zhang2022deep,weng2021semantic,zhang2023predictive}, which can be used in downstream tasks efficiently.
Therefore, task-oriented semantic communications usually employ supervised methods to train the employed DL models, while full data reconstruction systems train models in an unsupervised way. 
Most existing unlearning methods focus on supervised scenarios \cite{nguyen2020variational,sekhari2021remember}, which provide possible solutions in task-oriented semantic communications. Full data reconstruction semantic communications \cite{hu2023robust,huang2022toward,nan2023physical} are also an important component, but there is a lack of sufficient studies about unsupervised unlearning to support full data reconstruction semantic communications. 

%For example, the semantic decoder in \cite{hu2023robust,huang2022toward,nan2023physical} is used to recover the transmitted image and in \cite{weng2021semantic} is used to recover the transmitted speech.

\subsection{Machine Unlearning}
Existing machine unlearning methods give insightful inspiration to design unlearning methods for DL-enabled semantic communications, as most of them employ DL models to extract and recover the semantic information \cite{zhang2022deep,zhang2023predictive,weng2021semantic}. There are two rough categories of machine unlearning methods: fast retraining and approximate unlearning, according to the methodology and target problems.



%zhang2023toward,hu2023robust,ma2023task,li2022cross,zhang2023predictive,   huang2022toward,zhang2022deep,weng2021semantic

%Although most existing machine unlearning methods focused on unlearning supervised ML models, we still first review the related studies as they give insightful inspiration to implement unlearning for DL-enabled semantic communications.
%According to the methodology and target problems, existing studies can be classified into two categories, fast retraining and approximate unlearning.

%\textit{fast retraining} \cite{cao2015towards,bourtoule2021machine,yanarcane2022unlearning,wu2020deltagrad} and \textit{approximate unlearning} \cite{guo2019certified,sekhari2021remember,nguyen2020variational,fu2022knowledge}. 

%\vspace{2mm}
%\noindent
%\textbf{Fast Retraining.}

The fast retraining methods are extended based on the naive retraining method to reduce the computational costs \cite{cao2015towards,wu2020deltagrad}. They split the whole training dataset into multi-subsets and divided the learning process by training multiple sub-models based on the split sub-datasets. So that when the unlearning requests come, they only need to retrain the corresponding sub-model that is trained based on the sub-dataset with the erased samples, which significantly reduces the unlearning computation cost \cite{cao2015towards,bourtoule2021machine,yanarcane2022unlearning}. However, these methods are impractical for frequent unlearning requests or when the data to be erased is distributed across multiple subsets. 


%\vspace{2mm}
%\noindent
%\textbf{Approximate Unlearning.}

Approximate unlearning seeks to create a model that closely resembles one retrained solely on the remaining dataset \cite{guo2019certified,nguyen2020variational,kim2022efficient}. Two mainstream approximate unlearning methods are designed based on the Hessian matrix \cite{guo2019certified,sekhari2021remember} and variation Bayes \cite{nguyen2020variational,fu2022knowledge,nguyen2022markov}. They learn a differential similar model and variational approximate posterior, respectively. These methods are easy to cause huge model utility degradation, named catastrophic unlearning. To minimize the decrease in utility, a fixed threshold is typically employed to regulate the extent of unlearning \cite{nguyen2020variational,guo2019certified}. {Moreover, \citeauthor{fan2023salun} and \citeauthor{liu2024model} \cite{fan2023salun,liu2024model} find that considering model sparsity based on existing unlearning methods can effectively enhance the unlearning effect while not significantly decreasing model utility. Other methods, such as \cite{kurmanji2024towards,chundawat2023can,wang2023machine}, considered both the unlearning effect and model utility during the unlearning process and proposed corresponding methods to achieve the optimal balance.} 

However, all these methods are mainly designed for solely trained ML models in supervised scenarios. 
While in DL-enabled semantic communications, they usually jointly train two DL models, and there are huge amounts of unsupervised recovering tasks \cite{hu2023robust,zhang2022deep}. Unlearning for these kinds of jointly-trained DL-enabled semantic communication and other unsupervised models still requires further exploration.