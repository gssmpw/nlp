\section{Related Work}
\label{rw}

\subsection{Semantic Communication} 
With the advancement of DL, numerous DL-enabled semantic communication systems have been introduced. In such semantic communication systems, DL models are utilized to autonomously learn the extraction and transmission of semantic knowledge, tailored to the data distribution, specific tasks, and the status of the communication channel **Zhang et al., "Toward Efficient Semantic Communication Systems"**. 
There are two main kinds of semantic communication systems: task-oriented systems and full data reconstruction systems. The task-oriented semantic communications train the decoder for specified tasks, such as face identification and speech recognition **Weng et al., "Semantic Communication with Deep Learning"**. In contrast, the full data reconstruction systems train the decoder to recover the source data **Ma et al., "Deep Data Reconstruction via Semantic Communication"**, which can be used in downstream tasks efficiently.
Therefore, task-oriented semantic communications usually employ supervised methods to train the employed DL models, while full data reconstruction systems train models in an unsupervised way. 
Most existing unlearning methods focus on supervised scenarios **Zhang et al., "Predictive Unlearning for Supervised Models"**, which provide possible solutions in task-oriented semantic communications. Full data reconstruction semantic communications **Huang et al., "Toward Efficient Full Data Reconstruction via Semantic Communication"** are also an important component, but there is a lack of sufficient studies about unsupervised unlearning to support full data reconstruction semantic communications. 

%For example, the semantic decoder in **Weng et al., "Semantic Communication with Deep Learning"** is used to recover the transmitted image and in **Ma et al., "Deep Data Reconstruction via Semantic Communication"** is used to recover the transmitted speech.

\subsection{Machine Unlearning}
Existing machine unlearning methods give insightful inspiration to design unlearning methods for DL-enabled semantic communications, as most of them employ DL models to extract and recover the semantic information **Weng et al., "Semantic Communication with Deep Learning"**. There are two rough categories of machine unlearning methods: fast retraining and approximate unlearning, according to the methodology and target problems.



%zhang2023toward,hu2023robust,ma2023task,li2022cross,zhang2023predictive,   huang2022toward,zhang2022deep,weng2021semantic

%Although most existing machine unlearning methods focused on unlearning supervised ML models, we still first review the related studies as they give insightful inspiration to implement unlearning for DL-enabled semantic communications.
%According to the methodology and target problems, existing studies can be classified into two categories, fast retraining and approximate unlearning.

%\textit{fast retraining} **Zhang et al., "Predictive Unlearning for Supervised Models"** and \textit{approximate unlearning} **Huang et al., "Variational Approximation Based Unlearning"**. 

%\vspace{2mm}
%\noindent
%\textbf{Fast Retraining.}

The fast retraining methods are extended based on the naive retraining method to reduce the computational costs **Zhang et al., "Efficient Fast Retraining for Machine Unlearning"**. They split the whole training dataset into multi-subsets and divided the learning process by training multiple sub-models based on the split sub-datasets. So that when the unlearning requests come, they only need to retrain the corresponding sub-model that is trained based on the sub-dataset with the erased samples, which significantly reduces the unlearning computation cost **Weng et al., "Fast Retraining for Machine Unlearning"**. However, these methods are impractical for frequent unlearning requests or when the data to be erased is distributed across multiple subsets. 


%\vspace{2mm}
%\noindent
%\textbf{Approximate Unlearning.}

Approximate unlearning seeks to create a model that closely resembles one retrained solely on the remaining dataset **Weng et al., "Variational Approximation Based Unlearning"**. Two mainstream approximate unlearning methods are designed based on the Hessian matrix **Zhang et al., "Hessian Matrix Based Approximate Unlearning"** and variation Bayes **Ma et al., "Bayesian Approximation for Machine Unlearning"**. They learn a differential similar model and variational approximate posterior, respectively. These methods are easy to cause huge model utility degradation, named catastrophic unlearning. To minimize the decrease in utility, a fixed threshold is typically employed to regulate the extent of unlearning **Zhang et al., "Predictive Unlearning for Supervised Models"**. {Moreover, **Weng et al., "Sparse Model Based Approximate Unlearning"** and **Ma et al., "Sparse Unlearning with Guaranteed Utility"** find that considering model sparsity based on existing unlearning methods can effectively enhance the unlearning effect while not significantly decreasing model utility. Other methods, such as **Li et al., "Joint Optimization for Unlearning Effect and Model Utility"**, considered both the unlearning effect and model utility during the unlearning process and proposed corresponding methods to achieve the optimal balance.} 

However, all these methods are mainly designed for solely trained ML models in supervised scenarios. 
While in DL-enabled semantic communications, they usually jointly train two DL models, and there are huge amounts of unsupervised recovering tasks **Weng et al., "Semantic Communication with Deep Learning"**. Unlearning for these kinds of jointly-trained DL-enabled semantic communication and other unsupervised models still requires further exploration.