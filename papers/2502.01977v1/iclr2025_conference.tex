
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\PassOptionsToPackage{table,xcdraw}{xcolor}\usepackage{xcolor}

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{multirow}
\usepackage{amsmath}
% user imported packages
% \usepackage{color}
% \usepackage[symbol]{footmisc}
% \usepackage{authblk}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{cleveref}
\usepackage{scalefnt}
\usepackage{chngcntr}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{titletoc} % For custom ToC
\usepackage{wrapfig}
% \usepackage{footmisc}
\usepackage{caption}

\newcommand{\yuntao}[1]{\textcolor{red}{[Yuntao:#1]}}
\newcommand{\lhx}[1]{\textcolor{brown}{[LHX:#1]}}

\newcommand{\xmark}{\ding{55}}
\newcommand{\cross}{\textcolor{red}{\xmark}}
\colorlet{Mycolor1}{green!100}
\newcommand{\cmark}{\textcolor{Mycolor1}{\ding{51}}}

\newcommand{\methodname}{AutoGUI}
\newcommand{\ourvlm}{UI-VLM}

\title{AutoGUI: Scaling GUI Grounding with Automatic Functionality Annotations from LLMs}


  
\author{Hongxin Li
 \\
University of Chinese Academy of Sciences (UCAS)\\
New Laboratory of Pattern Recognition (NLPR), CASIA\\
\And
Jingfan Chen \& Jingran Su \\
The Hong Kong Polytechnic University \\
CAIR, HKISI, CAS \\
\AND
Yuntao Chen \\
CAIR, HKISI, CAS \\
\AND
Qing Li \\
The Hong Kong Polytechnic University \\
\AND
Zhaoxiang Zhang \\
University of Chinese Academy of Sciences (UCAS) \\
New Laboratory of Pattern Recognition (NLPR), CASIA \\
CAIR, HKISI, CAS \\
Shanghai Artificial Intelligence Laboratory \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
User interface understanding with vision-language models has received much attention due to its potential for enabling next-generation software automation.
However, existing UI datasets either only provide large-scale context-free element annotations or contextualized functional descriptions for elements at a much smaller scale.
In this work, we propose the \methodname{} pipeline for automatically annotating UI elements with detailed functionality descriptions at scale.
Specifically, we leverage large language models (LLMs) to infer element functionality by comparing the UI content changes before and after simulated interactions with specific UI elements. To improve annotation quality, we propose LLM-aided rejection and verification, eliminating invalid and incorrect annotations without human labor.
We construct an \methodname{}-704k dataset using the proposed pipeline, featuring multi-resolution, multi-device screenshots, diverse data domains, and detailed functionality annotations that have never been provided by previous datasets.
Human evaluation shows that the AutoGUI pipeline achieves annotation correctness comparable to trained human annotators. Extensive experimental results show that our \methodname{}-704k dataset remarkably enhances VLM's UI grounding capabilities, exhibits significant scaling effects, and outperforms existing web pre-training data types. We envision AutoGUI as a scalable pipeline for generating massive data to build GUI-oriented VLMs. AutoGUI dataset can be viewed at this anonymous URL: \url{https://autogui-project.github.io/}.
\end{abstract}

\input{sec/1_Intro}
\input{sec/2_Related_Work}
\input{sec/3_Method}
\input{sec/4_Experiments}
\input{sec/5_conclusion}

% \subsubsection*{Reproducibility Statement}
% The AutoGUI annotation pipeline is fully reproducible. The prompts used for annotating, LLM-aided rejection, and verification are listed in Tab.~\ref{tab:supp:funcpred manip prompt}, Tab.~\ref{tab:supp:rejection prompt}, and Tab.\ref{tab:supp:verif prompt}, respectively. The fine-tuning experiments are also reproducible, as we employ the training code repositories of open-source VLMs, i.e., SeeClick and SliME. Readers can download our data and use these training code repos to reproduce our models.


\bibliography{iclr2025_conference}

% \begin{thebibliography}{60}
% \providecommand{\natexlab}[1]{#1}
% \providecommand{\url}[1]{\texttt{#1}}
% \expandafter\ifx\csname urlstyle\endcsname\relax
%   \providecommand{\doi}[1]{doi: #1}\else
%   \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

% \bibitem[AI@Meta(2024)]{llama3modelcard}
% AI@Meta.
% \newblock Llama 3 model card.
% \newblock 2024.
% \newblock URL \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

% \bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
% Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
% \newblock Flamingo: a visual language model for few-shot learning.
% \newblock \emph{Advances in neural information processing systems}, 35:\penalty0 23716--23736, 2022.

% \bibitem[Baechler et~al.(2024)Baechler, Sunkara, Wang, Zubach, Mansoor, Etter, C{\u{a}}rbune, Lin, Chen, and Sharma]{baechler2024screenai}
% Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor C{\u{a}}rbune, Jason Lin, Jindong Chen, and Abhanshu Sharma.
% \newblock Screenai: A vision-language model for ui and infographics understanding.
% \newblock \emph{arXiv preprint arXiv:2402.04615}, 2024.

% \bibitem[Bai et~al.(2021)Bai, Zang, Xu, Sunkara, Rastogi, Chen, and y~Arcas]{Bai2021UIBertLG}
% Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, and Blaise~Ag{\"u}era y~Arcas.
% \newblock Uibert: Learning generic multimodal representations for ui understanding.
% \newblock In \emph{International Joint Conference on Artificial Intelligence}, 2021.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:236493482}.

% \bibitem[Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou]{bai2023qwen}
% Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
% \newblock Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.
% \newblock 2023.

% \bibitem[Bai et~al.(2024)Bai, Ying, Cao, Lv, He, Wang, Yu, Zeng, Xiao, Lyu, et~al.]{bai2024benchmarking}
% Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et~al.
% \newblock Benchmarking foundation models with language-model-as-an-examiner.
% \newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

% \bibitem[Burns et~al.(2022)Burns, Arsan, Agrawal, Kumar, Saenko, and Plummer]{Burns2022ADF}
% Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan~A. Plummer.
% \newblock A dataset for interactive vision-language navigation with unknown command feasibility.
% \newblock In \emph{European Conference on Computer Vision}, 2022.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:251040563}.

% \bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Wang, Changpinyo, Piergiovanni, Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer, Kolesnikov, Puigcerver, Ding, Rong, Akbari, Mishra, Xue, Thapliyal, Bradbury, Kuo, Seyedhosseini, Jia, Ayan, Ruiz, Steiner, Angelova, Zhai, Houlsby, and Soricut]{chen2023pali}
% Xi~Chen, Xiao Wang, Soravit Changpinyo, AJ~Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish~V Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu~Karagol Ayan, Carlos~Riquelme Ruiz, Andreas~Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut.
% \newblock Pa{LI}: A jointly-scaled multilingual language-image model.
% \newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{a}}.
% \newblock URL \url{https://openreview.net/forum?id=mWVoBz4W0u}.

% \bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Wu, Wang, Su, Chen, Xing, Zhong, Zhang, Zhu, Lu, Li, Luo, Lu, Qiao, and Dai]{chen2023internvl}
% Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu~Qiao, and Jifeng Dai.
% \newblock Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
% \newblock \emph{arXiv preprint arXiv:2312.14238}, 2023{\natexlab{b}}.

% \bibitem[Cheng et~al.(2024)Cheng, Sun, Chu, Xu, Li, Zhang, and Wu]{cheng2024seeclick}
% Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu.
% \newblock Seeclick: Harnessing gui grounding for advanced visual gui agents.
% \newblock \emph{arXiv preprint arXiv:2401.10935}, 2024.

% \bibitem[Deka et~al.(2017{\natexlab{a}})Deka, Huang, Franzen, Hibschman, Afergan, Li, Nichols, and Kumar]{Deka2017RicoAM}
% Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Y.~Li, Jeffrey Nichols, and Ranjitha Kumar.
% \newblock Rico: A mobile app dataset for building data-driven design applications.
% \newblock \emph{Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, 2017{\natexlab{a}}.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:6623010}.

% \bibitem[Deka et~al.(2017{\natexlab{b}})Deka, Huang, Franzen, Hibschman, Afergan, Li, Nichols, and Kumar]{deka2017rico}
% Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar.
% \newblock Rico: A mobile app dataset for building data-driven design applications.
% \newblock In \emph{Proceedings of the 30th annual ACM symposium on user interface software and technology}, pp.\  845--854, 2017{\natexlab{b}}.

% \bibitem[Deng et~al.(2024)Deng, Gu, Zheng, Chen, Stevens, Wang, Sun, and Su]{deng2024mind2web}
% Xiang Deng, Yu~Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu~Su.
% \newblock Mind2web: Towards a generalist agent for the web.
% \newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

% \bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021vit}
% Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
% \newblock An image is worth 16x16 words: Transformers for image recognition at scale.
% \newblock In \emph{International Conference on Learning Representations}, 2021.
% \newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

% \bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu, Huang, Chebotar, Sermanet, Duckworth, Levine, Vanhoucke, Hausman, Toussaint, Greff, Zeng, Mordatch, and Florence]{driess2023palme}
% Danny Driess, Fei Xia, Mehdi S.~M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence.
% \newblock Palm-e: An embodied multimodal language model.
% \newblock In \emph{arXiv preprint arXiv:2303.03378}, 2023.

% \bibitem[Gugger et~al.(2022)Gugger, Debut, Wolf, Schmid, Mueller, Mangrulkar, Sun, and Bossan]{accelerate}
% Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan.
% \newblock Accelerate: Training and inference at scale made simple, efficient and adaptable.
% \newblock \url{https://github.com/huggingface/accelerate}, 2022.

% \bibitem[Hong et~al.(2023)Hong, Wang, Lv, Xu, Yu, Ji, Wang, Wang, Dong, Ding, et~al.]{hong2023cogagent}
% Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et~al.
% \newblock Cogagent: A visual language model for gui agents.
% \newblock \emph{arXiv preprint arXiv:2312.08914}, 2023.

% \bibitem[Hu et~al.(2022)Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
% Edward~J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
% \newblock Lo{RA}: Low-rank adaptation of large language models.
% \newblock In \emph{International Conference on Learning Representations}, 2022.
% \newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

% \bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{mistral}
% Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
% \newblock Mistral 7b, 2023.
% \newblock URL \url{https://arxiv.org/abs/2310.06825}.

% \bibitem[Kapoor et~al.(2024)Kapoor, Butala, Russak, Koh, Kamble, Alshikh, and Salakhutdinov]{kapoor2024omniact}
% Raghav Kapoor, Yash~Parag Butala, Melisa Russak, Jing~Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov.
% \newblock Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web.
% \newblock \emph{arXiv preprint arXiv:2402.17553}, 2024.

% \bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{vllm}
% Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu, Joseph~E. Gonzalez, Hao Zhang, and Ion Stoica.
% \newblock Efficient memory management for large language model serving with pagedattention.
% \newblock In \emph{Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.

% \bibitem[Laurençon et~al.(2024)Laurençon, Tronchon, Cord, and Sanh]{laurençon2024idefics}
% Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh.
% \newblock What matters when building vision-language models?, 2024.

% \bibitem[Lee et~al.(2023)Lee, Joshi, Turc, Hu, Liu, Eisenschlos, Khandelwal, Shaw, Chang, and Toutanova]{lee2023pix2struct}
% Kenton Lee, Mandar Joshi, Iulia~Raluca Turc, Hexiang Hu, Fangyu Liu, Julian~Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova.
% \newblock Pix2struct: Screenshot parsing as pretraining for visual language understanding.
% \newblock In \emph{International Conference on Machine Learning}, pp.\  18893--18912. PMLR, 2023.

% \bibitem[Li et~al.(2023)Li, Zhang, Chen, Wang, Yang, and Liu]{li2023otter}
% Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.
% \newblock Otter: A multi-modal model with in-context instruction tuning, 2023.

% \bibitem[Li et~al.(2020{\natexlab{a}})Li, Li, He, Zheng, Li, and Guan]{Li2020WidgetCG}
% Y.~Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan.
% \newblock Widget captioning: Generating natural language description for mobile user interface elements.
% \newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, 2020{\natexlab{a}}.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:222272319}.

% \bibitem[Li et~al.(2020{\natexlab{b}})Li, He, Zhou, Zhang, and Baldridge]{Li2020MappingNL}
% Yang Li, Jiacong He, Xiaoxia Zhou, Yuan Zhang, and Jason Baldridge.
% \newblock Mapping natural language instructions to mobile ui action sequences.
% \newblock \emph{ArXiv}, abs/2005.03776, 2020{\natexlab{b}}.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:218571167}.

% \bibitem[Li et~al.(2024)Li, Yang, Liu, Ma, Zhang, Yang, Sun, Liu, and Bai]{li2023monkey}
% Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.
% \newblock Monkey: Image resolution and text label are important things for large multi-modal models.
% \newblock In \emph{proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2024.

% \bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{Lightman2023LetsVS}
% Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
% \newblock Let's verify step by step.
% \newblock \emph{ArXiv}, abs/2305.20050, 2023.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:258987659}.

% \bibitem[Lin et~al.(2023{\natexlab{a}})Lin, Yin, Ping, Lu, Molchanov, Tao, Mao, Kautz, Shoeybi, and Han]{Lin2023VILAOP}
% Ji~Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.
% \newblock Vila: On pre-training for visual language models.
% \newblock \emph{ArXiv}, abs/2312.07533, 2023{\natexlab{a}}.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:266174746}.

% \bibitem[Lin et~al.(2023{\natexlab{b}})Lin, Liu, Zhang, Gao, Qiu, Xiao, Qiu, Lin, Shao, Chen, Han, Huang, Zhang, He, Li, and Qiao]{lin2023sphinx}
% Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu~Qiao.
% \newblock Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models, 2023{\natexlab{b}}.

% \bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Eisenschlos, Piccinno, Krichene, Pang, Lee, Joshi, Chen, Collier, and Altun]{liu2023deplot}
% Fangyu Liu, Julian~Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun.
% \newblock Deplot: One-shot visual language reasoning by plot-to-table translation.
% \newblock In \emph{The 61st Annual Meeting Of The Association For Computational Linguistics}, 2023{\natexlab{a}}.

% \bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee]{liu2023llava}
% Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
% \newblock Visual instruction tuning, 2023{\natexlab{b}}.

% \bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Li, Li, Li, Zhang, Shen, and Lee]{liu2024llavanext}
% Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
% \newblock Llava-next: Improved reasoning, ocr, and world knowledge, January 2024{\natexlab{a}}.
% \newblock URL \url{https://llava-vl.github.io/blog/2024-01-30-llava-next/}.

% \bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Song, Lin, Lam, Neubig, Li, and Yue]{liu2024visualwebbench}
% Junpeng Liu, Yifan Song, Bill~Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue.
% \newblock Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding?
% \newblock \emph{arXiv preprint arXiv:2404.05955}, 2024{\natexlab{b}}.

% \bibitem[Liu et~al.(2024{\natexlab{c}})Liu, Yang, Liu, Li, Ma, Zhang, and Bai]{liu2024textmonkey}
% Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai.
% \newblock Textmonkey: An ocr-free large multimodal model for understanding document.
% \newblock \emph{arXiv preprint arXiv:2403.04473}, 2024{\natexlab{c}}.

% \bibitem[Lu et~al.(2024)Lu, Liu, Zhang, Wang, Dong, Liu, Sun, Ren, Li, Yang, Sun, Deng, Xu, Xie, and Ruan]{lu2024deepseekvl}
% Haoyu Lu, Wen Liu, Bo~Zhang, Bingxuan Wang, Kai Dong, Bo~Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan.
% \newblock Deepseek-vl: Towards real-world vision-language understanding, 2024.

% \bibitem[Mu et~al.(2023)Mu, Zhang, Hu, Wang, Ding, Jin, Wang, Dai, Qiao, and Luo]{mu2023embodiedgpt}
% Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu~Qiao, and Ping Luo.
% \newblock Embodied{GPT}: Vision-language pre-training via embodied chain of thought.
% \newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
% \newblock URL \url{https://openreview.net/forum?id=IL5zJqfxAa}.

% \bibitem[Panickssery et~al.(2024)Panickssery, Bowman, and Feng]{panickssery2024llm}
% Arjun Panickssery, Samuel~R Bowman, and Shi Feng.
% \newblock Llm evaluators recognize and favor their own generations.
% \newblock \emph{arXiv preprint arXiv:2404.13076}, 2024.

% \bibitem[Peng et~al.(2024)Peng, Wang, Dong, Hao, Huang, Ma, Ye, and Wei]{peng2024kosmos}
% Zhiliang Peng, Wenhui Wang, Li~Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei.
% \newblock Grounding multimodal large language models to the world.
% \newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
% \newblock URL \url{https://openreview.net/forum?id=lLmqxkfSIw}.

% \bibitem[Rawles et~al.(2023)Rawles, Li, Rodriguez, Riva, and Lillicrap]{rawles2023android}
% Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap.
% \newblock Android in the wild: A large-scale dataset for android device control.
% \newblock \emph{arXiv preprint arXiv:2307.10088}, 2023.

% \bibitem[Russakovsky et~al.(2014)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{Russakovsky2014ImageNetLS}
% Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael~S. Bernstein, Alexander~C. Berg, and Li~Fei-Fei.
% \newblock Imagenet large scale visual recognition challenge.
% \newblock \emph{International Journal of Computer Vision}, 115:\penalty0 211 -- 252, 2014.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:2930547}.

% \bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, Schramowski, Kundurthy, Crowson, Schmidt, Kaczmarczyk, and Jitsev]{LAION5B}
% Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade~W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa~R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.
% \newblock {LAION}-5b: An open large-scale dataset for training next generation image-text models.
% \newblock In \emph{Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2022.
% \newblock URL \url{https://openreview.net/forum?id=M3Y74vmsMcY}.

% \bibitem[Tang et~al.(2022)Tang, Yang, Wang, Fang, Liu, Zhu, Zeng, Zhang, and Bansal]{Tang2022UnifyingVT}
% Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Chao-Yue Zhang, and Mohit Bansal.
% \newblock Unifying vision, text, and layout for universal document processing.
% \newblock \emph{2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  19254--19264, 2022.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:254275326}.

% \bibitem[Team(2024)]{openai2024gpt4}
% OpenAI Team.
% \newblock Gpt-4 technical report, 2024.

% \bibitem[Wang et~al.(2021)Wang, Li, Zhou, Chen, Grossman, and Li]{Wang2021Screen2WordsAM}
% Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li.
% \newblock Screen2words: Automatic mobile ui summarization with multimodal learning.
% \newblock \emph{The 34th Annual ACM Symposium on User Interface Software and Technology}, 2021.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:236957064}.

% \bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Bai, Tan, Wang, Fan, Bai, Chen, Liu, Wang, Ge, Fan, Dang, Du, Ren, Men, Liu, Zhou, Zhou, and Lin]{qwen2vl}
% Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin.
% \newblock Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution, 2024{\natexlab{a}}.
% \newblock URL \url{https://arxiv.org/abs/2409.12191}.

% \bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Chen, Chen, Wu, Zhu, Zeng, Luo, Lu, Zhou, Qiao, et~al.]{wang2024visionllm}
% Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu~Qiao, et~al.
% \newblock Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.
% \newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

% \bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, brian ichter, Xia, Chi, Le, and Zhou]{wei2022chain}
% Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed~H. Chi, Quoc~V Le, and Denny Zhou.
% \newblock Chain of thought prompting elicits reasoning in large language models.
% \newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
% \newblock URL \url{https://openreview.net/forum?id=_VjQlMeSB_J}.

% \bibitem[Weng et~al.(2022)Weng, Zhu, Xia, Li, He, Liu, and Zhao]{Weng2022LargeLM}
% Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao.
% \newblock Large language models are better reasoners with self-verification.
% \newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, 2022.
% \newblock URL \url{https://api.semanticscholar.org/CorpusID:258840837}.

% \bibitem[Xia et~al.(2024)Xia, Zhang, Ye, Yan, Liu, Zhou, Chen, Dou, Shi, Yan, et~al.]{xia2024chartx}
% Renqiu Xia, Bo~Zhang, Hancheng Ye, Xiangchao Yan, Qi~Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et~al.
% \newblock Chartx \& chartvlm: A versatile benchmark and foundation model for complicated chart reasoning.
% \newblock \emph{arXiv preprint arXiv:2402.12185}, 2024.

% \bibitem[Yao et~al.(2022)Yao, Chen, Yang, and Narasimhan]{yao2022webshop}
% Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
% \newblock Webshop: Towards scalable real-world web interaction with grounded language agents.
% \newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 20744--20757, 2022.

% \bibitem[Ye et~al.(2023{\natexlab{a}})Ye, Hu, Xu, Ye, Yan, Dan, Zhao, Xu, Li, Tian, Qi, Zhang, and Huang]{ye2023mplugdocowl}
% Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji~Zhang, and Fei Huang.
% \newblock mplug-docowl: Modularized multimodal large language model for document understanding, 2023{\natexlab{a}}.

% \bibitem[Ye et~al.(2023{\natexlab{b}})Ye, Hu, Xu, Ye, Yan, Xu, Li, Tian, Qian, Zhang, Jin, He, Lin, and Huang]{2023-ureader}
% Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi~Qian, Ji~Zhang, Qin Jin, Liang He, Xin Lin, and Fei Huang.
% \newblock {UR}eader: Universal {OCR}-free visually-situated language understanding with multimodal large language model.
% \newblock In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pp.\  2841--2858, Singapore, December 2023{\natexlab{b}}. Association for Computational Linguistics.
% \newblock \doi{10.18653/v1/2023.findings-emnlp.187}.
% \newblock URL \url{https://aclanthology.org/2023.findings-emnlp.187}.

% \bibitem[You et~al.(2024{\natexlab{a}})You, Zhang, Gan, Du, Zhang, Wang, Cao, Chang, and Yang]{you2024ferret}
% Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang.
% \newblock Ferret: Refer and ground anything anywhere at any granularity.
% \newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{a}}.
% \newblock URL \url{https://openreview.net/forum?id=2msbbX3ydD}.

% \bibitem[You et~al.(2024{\natexlab{b}})You, Zhang, Schoop, Weers, Swearngin, Nichols, Yang, and Gan]{you2024ferretui}
% Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan.
% \newblock Ferret-ui: Grounded mobile ui understanding with multimodal llms.
% \newblock \emph{arXiv preprint arXiv:2404.05719}, 2024{\natexlab{b}}.

% \bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Han, Liu, Zhou, Lu, Qiao, Li, and Gao]{zhang2024llamaadapter}
% Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu~Qiao, Hongsheng Li, and Peng Gao.
% \newblock {LL}a{MA}-adapter: Efficient fine-tuning of large language models with zero-initialized attention.
% \newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{a}}.
% \newblock URL \url{https://openreview.net/forum?id=d4UiXAHN2W}.

% \bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Wen, Fu, Wang, Zhang, Wang, and Jin]{slime}
% Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin.
% \newblock Beyond llava-hd: Diving into high-resolution large multimodal models, 2024{\natexlab{b}}.
% \newblock URL \url{https://arxiv.org/abs/2406.08487}.

% \bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging}
% Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
% \newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
% \newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 46595--46623, 2023.

% \bibitem[Zhou et~al.(2023)Zhou, Xu, Zhu, Zhou, Lo, Sridhar, Cheng, Ou, Bisk, Fried, et~al.]{zhou2023webarena}
% Shuyan Zhou, Frank~F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et~al.
% \newblock Webarena: A realistic web environment for building autonomous agents.
% \newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

% \bibitem[Zhu et~al.(2024)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2024minigpt}
% Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
% \newblock Mini{GPT}-4: Enhancing vision-language understanding with advanced large language models.
% \newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
% \newblock URL \url{https://openreview.net/forum?id=1tZbq88f27}.

% \end{thebibliography}


\bibliographystyle{iclr2025_conference}

\appendix

\counterwithin{figure}{section}
\counterwithin{table}{section}
\renewcommand\thefigure{\Alph{figure}}
\renewcommand\thetable{\Alph{table}}
\renewcommand{\thesubsection}{\Alph{subsection}}
\input{sec/Appendix}



\end{document}
