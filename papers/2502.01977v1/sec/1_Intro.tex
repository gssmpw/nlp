\section{Introduction}

User interface understanding with visual language models(VLMs)~\citep{hong2023cogagent,cheng2024seeclick,you2024ferret,lee2023pix2struct,baechler2024screenai} has received wide attention due to its potential in fundamentally transforming how we interact with software as well as unleashing unseen flexibility for existing apps (Fig.~\ref{fig: navigation apps}). 
\emph{Functionality prediction}, which aims to understand the semantic purpose and interactive affordance of individual UI elements, is a crucial task that goes beyond previous UI understanding tasks focusing on structural mapping between UI code and visual layout, such as UI REG/REC~\citep{hong2023cogagent,Li2020WidgetCG} and diagram to code~\citep{xia2024chartx, liu2023deplot}.

To enhance the UI understanding capability of VLMs, large-scale high-quality training data is indispensable.
However, the scale of existing open-source datasets~\citep{Li2020WidgetCG,Deka2017RicoAM,Li2020MappingNL,kapoor2024omniact,Wang2021Screen2WordsAM} for UI understanding remains on the order of millions, significantly fewer than natural image datasets such as LAION-5B~\citep{LAION5B}.
Additionally, the prevailing methods~\citep{Deka2017RicoAM,Li2020WidgetCG} for collecting UI annotation are labor-intensive, leading to prohibitive costs that hinder scalability.
Moreover, existing UI understanding datasets predominantly focus on describing either the visual appearance~\citep{Li2020WidgetCG,Li2020MappingNL} (e.g., a button beside the navigation bar), element categories~\citep{cheng2024seeclick} (e.g., ``menu button''), or brief functions weakly related to the UI context~\citep{Bai2021UIBertLG} (e.g., ``show more information'') shown in Fig.~\ref{fig: functionality vs others}.
These datasets lack contextual functional descriptions of UI elements, which poses a challenge for VLMs in comprehending the roles these elements serve within specific UI contexts, such as distinguishing between two visually similar magnifying glass icons that may represent distinct functionalities like searching and zooming.

\input{figure/navigation_apps}

To address the challenge. we propose \methodname{}, a scalable and automatic UI data annotation pipeline that provides unlimited UI element functionality annotations.
Our annotation pipeline automatically collects UI interaction trajectories and leverages large language models (LLMs) to infer element functionalities based on UI content changes, eliminating the need for manual annotation by human experts.
Initially, the proposed pipeline crawls a multitude of interaction trajectories on either a web browser or an Android emulator and captures screenshots at various aspect ratios. Subsequently, we use open-source LLMs~\citep{llama3modelcard} to annotate the functionalities of elements on collected GUIs based on changes to UI contents when interacting with these elements. To ensure data quality, LLM-aided rejection is utilized to eliminate invalid samples, such as incompletely rendered UIs. Additionally, inspired by recent works on LLM verification~\citep{Weng2022LargeLM,Lightman2023LetsVS}, multiple LLMs are prompted as verifiers to identify false functionality predictions. With both the rejection and verification processes, our pipeline removes unclear and invalid samples.

We curate the \methodname{}-704k dataset with the proposed pipeline. 
\methodname{}-704k contains 704k high-quality functionality grounding and referring tasks used to finetune and evaluate open-source VLMs.
With the vast knowledge embedded within LLMs (e.g., Llama-3-70B~\citep{llama3modelcard}) and fast inference infrastructure~\citep{vllm,accelerate}, our pipeline can efficiently annotate high-quality samples at a large scale and substantially reduced cost compared to traditional methods. Moreover, pioneer experiments find that our pipeline achieves annotation accuracy of \textbf{96.7\%} comparable to a trained human annotator.

Based on the collected \methodname{}-704k dataset, we finetune open-source VLMs that own little UI grounding capabilities. Experimental results demonstrate that data collected through our \methodname{} pipeline significantly enhances the VLMs' UI grounding accuracy and exhibits remarkable scaling effects. The results also show that our functionality annotation type is superior to the data type directly derived from web HTML code~\citep{hong2023cogagent,cheng2024seeclick}, serving as a promising data source for building VLMs capable of UI grounding.