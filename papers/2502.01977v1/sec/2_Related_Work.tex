\section{Related Works}
\input{figure/functionality_vs_others}
\subsection{Recent Advancement of VLMs}
Recently, a new wave of research has started to enhance LLMs with the capability of processing both visual and textual information~\citep{alayrac2022flamingo, chen2023pali,li2023otter,Lin2023VILAOP,liu2023llava,lin2023sphinx,chen2023internvl,lu2024deepseekvl,bai2023qwen,qwen2vl,zhu2024minigpt,wang2024visionllm,li2023monkey,zhang2024llamaadapter,you2024ferret,lauren√ßon2024idefics,peng2024kosmos,driess2023palme}, opening the new field of Vision Language Model (VLM). Pioneering efforts Flamingo~\citep{alayrac2022flamingo} uses interleaved visual and language inputs as prompts and shows remarkable few-shot visual question-answering capability. Fueled by GPT-4~\citep{openai2024gpt4}, both academia and industry have endeavored to democratize its amazing multimodal reasoning capability. LLaVA~\citep{liu2023llava} and LLaMA-Adapter~\citep{zhang2024llamaadapter} have attempted to align vision encoders~\citep{dosovitskiy2021vit} with LLMs to enable visual instruction following. Models such as VisionLLM~\citep{wang2024visionllm}, Ferret~\citep{you2024ferret}, and Qwen-VL~\citep{bai2023qwen} further enhance these capabilities with robust visual grounding. Additionally, Research is also expanding into VLM applications in scenarios rich in textual imagery~\citep{Tang2022UnifyingVT,2023-ureader,ye2023mplugdocowl,liu2024textmonkey} and embodied interactions~\citep{driess2023palme, mu2023embodiedgpt}, offering new possibilities in multimodal reasoning. Despite these advancements, the domain of UI understanding remains under-explored due to data scarcity. This paper proposes an autonomous UI annotation pipeline to tackle this challenge, aiming to expand the data available for training VLMs in this crucial area.

\subsection{Existing UI Datasets and Benchmarks}

Unlike mature natural image datasets~\citep{Russakovsky2014ImageNetLS,LAION5B}, UI understanding datasets have received less attention in computer vision. Several efforts have been made to develop mobile UI modeling datasets~\citep{Wang2021Screen2WordsAM,Li2020WidgetCG,Li2020MappingNL,Bai2021UIBertLG,Burns2022ADF}, primarily annotating the RICO dataset~\citep{deka2017rico}, which includes 72K screenshots from Android apps. Examples include Widget Captioning~\citep{Li2020WidgetCG}, which analyzes captions and linguistic features of UI elements, and RICOSCA~\citep{Li2020MappingNL}, which maps single-step instructions to UI locations. Recently, MoTIF~\citep{Burns2022ADF} and AITW~\citep{rawles2023android} have been proposed to focus on interpreting high-level instructions in Android environments. However, these manually curated and crowd-annotated datasets are limited in size and costly to update, presenting challenges in adapting to new UI types.


The web scenario has also gained much attention. WebShop~\citep{yao2022webshop}, as an early attempt, introduces a simplified simulator for web navigation tasks. More recent projects, such as Mind2Web~\citep{deng2024mind2web} and WebArena~\citep{zhou2023webarena}, have developed realistic and reproducible web environments to improve web agent capabilities. VisualWebBench~\citep{liu2024visualwebbench} has established a comprehensive evaluation framework for VLMs, focusing on UI grounding. To tackle data insufficiency issues, recent studies like SeeClick~\citep{cheng2024seeclick} and CogAgent~\citep{hong2023cogagent} have utilized the latest Common Crawl data to create large-scale datasets. However, these data are derived from HTML code snippets which contain plenty of noise.


This paper aims to address the aforementioned limitations of existing UI datasets by introducing an automatic LLM-based annotation pipeline. By focusing on contextual functional descriptions of elements, our pipeline aims to enhance VLM's capability of understanding users' functional intents. The advantages of our AutoGUI dataset over existing datasets are summarized in Tab.~\ref{tab:data comparison}.
