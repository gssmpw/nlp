\newpage
\section{Appendix}

\startcontents

{
\hypersetup{linkcolor=black}
\printcontents{}{1}{}
}
\clearpage


The appendix comprises the following sections:

Section A: Details for implementation details for the autonomous annotation pipeline, including dataset statistics, visualized annotation pipeline, and LLM prompts.

Section B: Details for model implementation and training.

Section C: Additional experimental analysis including analysis of successful and failure cases on two benchmarks.

Section D and E: Limitations and Potential Societal Impact.

\subsection{Details of the AutoGUI Pipeline}
\subsubsection{Extra Statistics of the AutoGUI Dataset}
\label{sec:supp:data stats}

\input{figure/verbnoun_sunburst}
\input{figure/domain_ratio}

Fig.~\ref{fig: verbnoun} visualizes the verb-noun statistics of the AutoGUI dataset, highlighting its extensive coverage of diverse UI functionalities. Fig.~\ref{fig: domain ratio} lists the top 50 most frequent top-level domains in the AutoGUI dataset, showing that the AutoGUI dataset involves a broad spectrum of real-world scenarios, including technology (e.g., apple.com), entertainment (e.g., tiktok.com), office (e.g., outlook.com), news (e.g., medium.org), and finance (e.g., paypal.com).


\subsubsection{Recording Interaction Trajectories on Web}
\label{sec:supp:record traj detail}
\noindent\textbf{Interactive Crawler for Common Crawl} We design an in-house web crawler that interacts with most elements rendered on the web page.
In contrast with existing methods which contain information for elements on the initial static web page for a given URL, our crawler randomly interacts with a rendered web page for \textbf{multiple steps} within a given action horizon $T_\text{act}$ to collect UI data with abundant functional semantics. Fig.~\ref{fig: pipeline comparison} compares the proposed AutoGUI and the existing annotation methods.
We empirically set $T_\text{act} = 10$ in all our recordings.
Therefore, our interactive crawler could collect functionality of elements that are not visible to static pages, including nested drop-down menus, date and location selectors, and secondary menus.

\input{figure/pipeline_comparison}

\noindent\textbf{Data Source and Data Format}
To incorporate a wide basis of web pages, we first obtain a list of the top-200 most visited domains~\footnote{\url{https://tranco-list.eu/}} and manually remove content delivery network (CDN) and not safe for work (NSFW) sites.
We use URLs in this curated list as seeds to query the Common Crawl index~\footnote{\url{https://index.commoncrawl.org/}} to find additional URLs with maximum sub-domain and path diversity.
Querying URLs from the Common Crawl index ensures that our crawler respects each site's robots.txt file, making the dataset collection process legally safe. 
By obeying the directives in robots.txt, we avoid potential legal issues associated with unauthorized web scraping.
For each web page, we collect the following data:
\begin{itemize}
    \item Screenshot image of the rendered page
    \item Accessible Tree (AXTree) text representing the page's accessibility structure
    \item HTML source code of the page
    \item Accessible Node (AXNode) text for the specific element our crawler interacted with at each step
\end{itemize}


\subsubsection{Recording Interaction Trajectories on Android Devices}
We also implement an in-house crawler that interacts with multiple emulated Google Pixel phones. The phones are reset to different starting UIs before a script randomly interacts with these phones to record trajectories. To improve data diversity, the starting UIs include the home page, drop-down panel, settings page, and Apps drawer.

Similar to webpage HTML, mobile phone UIs are rendered with XML code, which is cleaned and converted to AXTree-like content before being used to annotate functionalities.

\subsubsection{Functionality Annotation Details}
\label{sec:supp:anno details}
The AutoGUI pipeline utilizes UI content changes to predict the functionalities of the interacted elements. For interactions that manipulate the existing UI, the pipeline analyzes differences in the AXTrees to annotate functionalities. Conversely, when interactions result in navigation to a new UI, the pipeline examines changes in UI descriptions to guide the annotation process. Details on these methodologies are outlined below:

\noindent\textbf{UI manipulation case} We use a file-comparison library, DiffLib, to generate line-by-line differences of the AXtrees before and after interactions. To balance efficiency with annotation integrity, we limit the differences to 250 lines. In addition to the standard markings by DiffLib—addition, deletion, and unchanged status—we incorporate two additional change markers: `Repositioning' and `Attribute Update'. These markers provide detailed information about UI content changes, essential for representing realistic structural variations. For example, without the attribute update marker, a clicked menu icon would erroneously appear as both deleted and added in the difference output, despite the menu element remaining in place. An example of this case is shown in Fig.~\ref{fig: funcpred diff case}. The used prompt is shown in Tab.~\ref{tab:supp:funcpred manip prompt}.

\input{figure/funcpred_diff_case}
\input{table/appendix/funcpred_prompt}


\noindent\textbf{UI navigation case} When an interacted element causes navigation to a new UI, the resultant changes are often extensive, potentially exceeding the context limit of an LLM and complicating the analysis of these changes. To handle this situation, UI description changes are used to predict functionalities. Concretely, the LLM is initially prompted to describe the UIs before and after interaction given UI AXTrees as input. Subsequently, the LLM then uses these descriptions to analyze content changes and predict the functionality of the interacted element. The description length of the AXTree is limited to 150 lines. An illustration of this process is shown in Fig.~\ref{fig: funcpred desc case}. The corresponding prompt is detailed in Tab.~\ref{tab:supp:funcpred nav prompt}.

\input{figure/funcpred_desc_case}
\input{table/appendix/funcpred_desc_prompt}

\subsubsection{Details of Rejecting Invalid Samples via Hand-Written Rules}
To clarify the hand-written rules used in the process of removing invalid samples: (1) \textbf{Removing blank GUIs}. We remove blank GUIs by verifying whether the accessibility tree contains more than one node. If no nodes are present, the GUI is considered blank. (2) \textbf{Removing UIs containing elements indicating content loading}. GUIs containing elements indicative of content loading (e.g., keywords such as "loading", "please wait", or "refreshing") are excluded. These keywords typically suggest that the content has not fully loaded and may affect the validity of the sample. (3) \textbf{Removing interaction targets outside of screens}. Occasionally, part of the UI, including the interacted element, may fail to be captured. We filter out GUIs where interaction targets appear outside of the visible screen area. This is determined by checking whether the interacted element exists within the bounds of the recorded accessibility tree. Note that these rules are designed mainly for the domains from which we collected GUI metadata. Nevertheless, one can extend the rules flexibly according to the noise characteristics of new domains.

\subsubsection{Details of Rejecting Invalid Samples via LLMs}
\label{suc:supp:reject details}
To eliminate invalid samples before functionality annotation, the AutoGUI pipeline prompts the annotating LLM to also determine the validity of samples by analyzing the predictability of the UI content changes. The LLM evaluates each sample against three criteria: 1) Explicitness of Changes: This measures how clearly the changes indicate the element's functionality. Changes that directly suggest functionality receive higher scores, while vague or irrelevant changes are not scored. 2) Relevance of Changes: This criterion assesses the significance of the modifications in relation to the element's intended function. Highly related modifications obtain a high score. No scores for irrelevant or unrelated content changes. 3) Predictability of Outcome: This involves determining how anticipated the interaction outcome is based on the changes, considering common web conventions and user experience principles. Highly predictable changes obtain a high score, whereas moderate, unexpected, or counter-intuitive outcomes receive no score.


\subsubsection{Details of LLM-Based Verification}
\label{suc:supp:verif details}
To improve the quality of functionality annotations, the AutoGUI pipeline prompts two LLMs (i.e.g, Llama-3-70B and Mistral-7B-Instruct-v0.2) as verifiers to assign scores to samples based on how well the target elements adhere to their functionality annotations. The LLMs receive as the input a) the target element along with its surrounding UI content (up to 20 lines), b) the functionality annotation of this element, and c) the outcome of interacting with the element, either being the UI line-by-line differences (at most 250 lines) in manipulation cases or the UI description after the interaction in navigation cases. Given these inputs, the two LLMs generate two responses containing a score. Samples that do not achieve two full scores are discarded for higher quality of the AutoGUI dataset. The used prompt is shown in Tab.~\ref{tab:supp:verif prompt} and an example is illustrated in Fig.~\ref{fig: verif diff case}.


\input{table/appendix/rejection_prompt}
\input{figure/rejection_desc_case}
\input{figure/rejection_examples}
\input{table/appendix/verif_prompt}
\input{figure/verif_diff_case}

\subsubsection{Details of Grounding/Captioning Task Generation}
\input{table/appendix/task_templates}
After collecting the element-functionality pairs, the AutoGUI pipeline converts these pairs into functionality grounding and captioning tasks by formatting a multitude of task templates (several examples are shown in Tab.~\ref{tab:task templates}). A functionality grounding task requires a VLM to output point coordinates of the element fulfilling the given functionality, while a captioning task demands that the VLM articulate a functionality description for an element, given its coordinates. It is important to note that each element-functionality pair is utilized to generate both a grounding task and a captioning task.

To optimize training efficiency and minimize token expenditure, all point coordinates are normalized within the range $[0,100)$. For tokenization, we employ the tokenizer from Qwen-VL-Chat without incorporating special tokens for the numerical range 0-99.


\subsection{Implementation Details}
\subsubsection{Human Evaluation Details}
\label{sec:supp:humaneval details}
To justify the efficacy of the AutoGUI pipeline, we conducted a comparative evaluation of annotation correctness between a trained human annotator and the AutoGUI system. The human annotator was a student proficient in using digital devices, ensuring familiarity with diverse user interfaces.

We selected a set of 30 invalid samples, each showcasing a variety of element functionalities, to prepare the annotator for the annotation process. These functionalities included drop-down menu expansions, menu item selections, date-pickers, filtering options, pop-up modals, webpage navigation, and zooming in/out buttons. The purpose of this selection was to expose the annotator to a broad spectrum of potential UI interactions, enhancing their ability to accurately assess element functionality based on UI content changes.

During the training phase, we provided the annotator with detailed guidelines, including three specific criteria outlined in Fig~\ref{fig: check criteria}, to ensure the clarity and correctness of their annotations. Additionally, we incorporated 15 invalid samples to instruct the human annotator on how to identify and exclude these cases during the evaluation process. These invalid samples encompassed scenarios such as incompletely loaded UIs, network failure incidents, login restrictions, and UIs displaying inappropriate content.

Following the training stage, the human annotator evaluated a total of 146 samples. Remarkably, the annotator successfully identified all invalid samples, achieving an overall annotation correctness rate of 95.5\%. The few incorrect annotations were categorized as such due to vagueness or instances of hallucination, where the descriptions did not accurately reflect the UI elements.

\subsubsection{Fine-Tuning Details}
\label{sec:supp:impl details}
Qwen-VL-Chat~\cite{bai2023qwen} and SliME~\cite{slime} are selected as the base models in the experiments. To investigate the scaling effects of our dataset, 25k, 125k, and the entirety of the 702k samples in the training split are used as training data in the three scaling experiments. For the first two smaller-scale experiments, a subset of the 702k data is randomly sampled.

Pilot experiments find that the non-UI training data (i.e., LLaVA-instruct-150k and the Cauldron) significantly outnumber the 25k and 125k UI training data, resulting in data imbalance that biases the trained UI-VLM towards the general Q\&A tasks in the non-UI data and leads to inferior UI grounding performance. To tackle this issue, the 25k/125k samples are resampled to the same number of the non-UI training data to enable the UI-VLM to acquire more supervising signals from the UI data. This resampling approach is not employed in the 702k experiment as this experiment does not encounter the imbalance issue.

\input{table/appendix/training_config_qwen}
\input{table/appendix/training_config_slime}

We train our UI-VLM based on the HuggingFace Transformers\footnote{https://huggingface.co/docs/transformers/index} and the PEFT library\footnote{https://huggingface.co/docs/peft/index}. The training configuration is shown in Tab.~\ref{tab:training cfg qwen} and Tab.~\ref{tab:training cfg slime}.

\subsection{Additional Experimental Analysis}
\subsubsection{Growing Grounding Performance Brought by Scaling Data Size}
\input{figure/point2boxcenter_dist_distrib}
To further investigate the benefit of scaling the AutoGUI functionality data, the histogram of distance from a predicted point to the ground truth box center is plotted for the 25k, 125k, and 702k experiments. The results in Fig.~\ref{fig: dist histogram} demonstrate that the distance distributions become denser at lower ranges, suggesting that increasing the AutoGUI training data leads to consistently improved grounding performances.


\subsubsection{Case Analysis on FuncPred Test Split}
\label{sec:supp:case analysis}
\noindent\textbf{Successful cases} Fig.~\ref{fig:funcpred scaling success} demonstrates several examples of the grounding results from Qwen-VL trained with the 25k, 125k, and 702k AutoGUI data. The model trained with the 702k data (ours-702k) exhibits more accurate functionality grounding performance. For instance, Fig.~\ref{fig:funcpred scaling success} (a) shows that ours-702k predicts the point right on the target (The `Get an account' button) while the other two models slightly miss the target. Case (c) shows that ours-702k correctly understands the functional intent to locate the WordPress logo, in contrast to the other models, which incorrectly focus on the text `Get WordPress'. Additionally, case (f) illustrates that ours-702k successfully locates the three-dot menu icon, aligning with the intent to expand a dropdown menu. These results suggest that increasing the AutoGUI training data enhances the model's ability to understand complex functional intents and to recognize diverse iconic elements accurately.

\noindent\textbf{Failure cases} To explore the limitations of our model, we analyze several failure cases across the scaling experiments, as shown in Fig.~\ref{fig:funcpred scaling failure}. The primary failure cases comprise (1) Difficulty in accurately locating very small target elements, as illustrated by the tiny ‘Policy’ button in case (a); (2) Misunderstanding functional intents, as shown in case (b) where the three models fail to locate the element for account creation and case (g) where ours-702k mistakenly focuses on navigating to previous content instead of subsequent content; (3) Challenges in recognizing abstract iconic elements, as seen with the map style icon in case (d) and the compass icon in case (f).

Despite these challenges, the enhanced performance observed with ours-702k supports the potential of the AutoGUI pipeline to further improve functionality grounding. The successful cases underscore that increasing the size of the training dataset not only boosts the model’s ability to interpret functional intents but also its capability to process a variety of textual and iconic elements effectively.

\input{figure/funcpred_scaling_success}
\input{figure/funcpred_scaling_failure}


\subsubsection{Case Analysis on MoTIF Test Split}
\input{figure/motif_case_analysis}
\input{figure/motif_bad_case}
We evaluate the instruction following ability on MoTIF dataset. Our analysis focuses on two aspects: (1) what improvements our model can achieve with the scaling of our functionality dataset (Fig.~\ref{fig: motif case}); and (2)  in which scenarios our model still fails to achieve correct grounding (Fig.~\ref{fig: motif bad case}).

Fig.~\ref{fig: motif case} shows that the model can more accurately understand the action instruction and make meaningful localization as scaling improves from \textcolor{red}{125k} to \textcolor{blue}{702k}. For instance, when the objective is to \textit{click sleep noise recording and click enable}, the model can comprehend the semantics of this global objective and identify \textit{turn on}. Additionally, the model can mitigate localization errors, such as the \textcolor{blue}{702k} being more accurately positioned on the target element (e.g., the icon of \textit{reservation}) than the \textcolor{red}{125k}.
However, MoTIF still struggles with certain tasks. For example, as shown Fig.~\ref{fig: motif bad case}, it has difficulty with localization in fine-grained steps for the instruction \textit{search for Kingston Drive and show me the route to it}. It can be seen that the model does not effectively understand situations involving widget pop-ups (e.g., protocol and advertisement). This may be attributed to the weak semantic connection between pop-ups and the instruction. Furthermore, the model still falls short in precise localization. Enriching the dataset further could alleviate this issue.

\input{figure/autogui_vs_human_anno}


\subsection{Limitations}
AutoGUI is dedicated to providing an autonomous way to collect scalable UI grounding/captioning data for training capable UI-VLMs. However, AutoGUI still encounters several limitations:

\noindent\textbf{Lack of Diverse Mobile App Data.} As many Apps implement anti-emulator code, it is extremely difficult to navigate through popular Apps, such as TikTok and WeChat, on Android emulators. To circumvent this issue, AutoGUI renders webpages at various resolutions, including smartphone resolution, to mimic diverse device types. Although mainstream websites, such as YouTube and Reddit, provide delicately designed webpage responsiveness for various resolutions, a number of less common websites do not possess such flexible responsiveness and distort severely when rendered at smartphone resolutions. Therefore, collecting UI data at a smartphone resolution probably leads to domain gaps between the collected data and real smartphone Apps that are not rendered with HTML.

\noindent\textbf{AutoGUI is Not Indented to Record Task-Oriented Interaction Trajectories.} AutoGUI randomly interacts with UIs to record transition trajectories and utilize the UI content changes to predict the functionalities of the interacted elements. Hence, the collected trajectories do not provide high-level task semantics. In other words, the AutoGUI dataset does not contain tasks that combine multiple low-level steps, such as selecting a check-in date and then a check-out date. These long-horizon tasks are usually generated by human annotators in the existing works~\cite{deng2024mind2web,rawles2023android}. In future work, we can also utilize capable LLMs to generate high-level tasks and then prompt the LLMs to interact with UIs according to the tasks.

\noindent\textbf{AutoGUI Cannot Annotate UI Elements That Modify Content on the Internet} To avoid causing potential contamination on the Internet and bearing unexpected responsibilities, we try our best to eliminate interaction samples that manipulate sensitive elements that probably modify contents on the Internet. For example, elements used to post comments, make purchases, and enter account information are discarded. Consequently, the AutoGUI pipeline mainly annotates elements that only support read-only functionalities.

\subsection{Potential Societal Impact}
The potential societal impacts of the proposed AutoGUI can be considered across various dimensions:

\noindent\textbf{Accessibility Enhancements} VLMs trained with the AutoGUI data obtain stronger UI grounding capabilities, thereby possessing the potential to act as UI agents. By enabling context-aware understanding of UI functionalities, the VLMs can help users locate elements on complex UIs, significantly improving accessibility features in software. This could lead to the development of applications that are more intuitive for users with disabilities, such as those requiring screen readers or other assistive technologies.


\noindent\textbf{Research Impact}: By reducing the labor and time required for annotating UI data via the AutoGUI, the industry and academia could lower costs to easily build UI agents. This could also shift labor demands towards more creative and strategic roles rather than repetitive annotation tasks.

\noindent\textbf{Privacy and Security Concerns}: Although we employ precautions of eliminating samples related to sensitive UI elements (e.g., avoid interacting with elements modifying the Internet and use only popular public websites without exposing privacy), corner cases still exist on the vast Internet. UI data involving either content modification or personal information are hard to discern as UI designs are distinct and no universal detection rules exist. Therefore, it is essential for cyber-security research to consider the potential leakage of personal information in the collected data and devise preemptive protective approaches.

\noindent\textbf{Potential for Bias and Fairness}: The bias of the LLMs used in the AutoGUI annotation pipeline is probably reflected in the collected data, leading to a trained UI-VLM that inherits the bias. Therefore, mitigating bias in the LLM's annotations will be important for developing fair VLM agents that align with the values of users from diverse cultures.