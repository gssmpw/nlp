\section{\methodname{}: Automatic Functionality Annotation Pipeline}
\label{sec: annotation pipeline}
This section introduces \methodname{}, an annotation pipeline (Fig.~\ref{fig: anno pipeline}) that automatically produces contextual element functionality annotations used to enhance VLMs' GUI grounding capabilities.

\input{table/main_paper/data_comparison}


\input{figure/AnnoPipeline}

\subsection{Collecting UI Interaction Trajectories}
Our pipeline initiates by collecting interaction trajectories, which are sequences of UI contents captured by interacting with UI elements. Each trajectory step captures all interactable elements and the accessibility tree (AXTree) that briefly outlines the UI structure, which will be used to generate functionality annotations. To amass these trajectories, we utilize the latest Common Crawl repository as the data source for web UIs and Android Emulator for mobile UIs. Note that illegal websites and Apps are excluded manually from the sources to ensure no pornographic or violent content is included in our dataset. Please refer to Sec.~\ref{sec:supp:record traj detail} for collecting details and data license.

\subsection{Functionality Annotation Based on UI Dynamics}
Subsequently, the pipeline generates functionality annotations for elements in the collected trajectories. Interacting with an element $e$, by clicking or hovering over it, triggers content changes in the UI. In turn, these changes can be used to predict the functionality $f$ of the interacted element. For instance, if clicking an element causes new buttons to appear in a column, we can predict that the element likely functions as a dropdown menu activator (an example in Fig.~\ref{fig: funcpred diff case}).
With this observation, we utilize a capable LLM (i.e., Llama-3-70B~\citep{llama3modelcard}) as a surrogate for humans to summarize an element's functionality based on the UI content changes resulting from interaction. Concretely, we generate compact content differences for AXTrees before ($s_t$) and after ($s_{t+1}$) the interaction using a file-comparing library\footnote{https://docs.python.org/3/library/difflib.html}. Then, we prompt the LLM to thoroughly analyze the UI content changes (addition, deletion, and unchanged lines), present a detailed Chain-of-Thoughts~\citep{wei2022chain} reasoning process explaining how the element affects the UI, and finally summarize the element's functionality.

In cases where element interactions significantly transform the UI and cause lengthy differences—such as navigating to a new screen—we adjust our approach by using UI description changes instead of the AXTree differences. Specifically, we prompt the same LLM to discern the UI hierarchy, describe UI regions, and finally describe the entire UI functionality. After describing the UIs before and after the interaction, the LLM analyzes the description differences, presents reasoning, and summarizes the element's functionality. This annotation process is formulated as:
\begin{equation}
    f = \text{LLM}(p_{\text{anno}}, s_t, s_{t+1})
\end{equation}

where $f$ is the predicted functionality, $p_{\text{anno}}$ is the annotation prompt (Tab.~\ref{tab:supp:funcpred manip prompt} and Tab.~\ref{tab:supp:funcpred nav prompt}). Examples of annotated elements are depicted in Fig.~\ref{fig: our dataset} and more annotation details are explained in Sec.~\ref{sec:supp:anno details}.

\subsection{Removing Invalid Samples via LLM-Aided Rejection}
The collected trajectories may contain invalid samples due to broken UIs, such as incomplete UI loading. These samples are meaningless as they contain corrupted UI content and can mislead the models trained with them.

To filter out these invalid samples, we introduce an LLM-aided rejection approach. Initially, hand-written rules are used to detect obvious broken cases, such as blank UI contents, UIs containing elements indicating content loading, and interaction targets outside of UIs. While these obvious cases constitute a large portion of the invalid samples, there are a few types that are difficult to detect with hand-written rules. For instance, interacting with a “view more” button might unexpectedly redirect the user to a login page instead of the desired information page due to website login restrictions. To identify these challenging samples, we prompt the annotating LLM to also act as a rejector. Specifically, the LLM takes the UI content changes, generated using a file-comparing library, as input, provides detailed reasoning on whether the changes are meaningful for predicting the element's functionality, and finally outputs predictability scores ranging from 0 to 3. This process is formulated as follows:
\begin{equation}
 score = \text{LLM}(p_{\text{reject}}, e, s_t, s_{t+1})
\end{equation}
where $p_{\text{reject}}$ is the rejection prompt (Tab.~\ref{tab:supp:rejection prompt}).

This approach ensures that clear and predictable samples receive higher scores, while those that are ambiguous or unpredictable receive lower scores. For instance, if a button labeled "Show More", upon interaction, clearly adds new content, this sample will considered to provide sufficient changes that can anticipate the content expansion functionality and will get a score of 3. Conversely, if clicking on a "View Profile" link fails to display the profile possibly due to web browser issues, this unpredictable sample will get a score less than 3.

After implementing empirical experiments, we deploy this LLM-based rejector to discard the bottom 30\% of samples based on their scores to strike a balance between the elimination of invalid samples and the preservation of valid ones (More details in Sec.~\ref{suc:supp:reject details}). The samples that pass the hand-written rules and the LLM rejector are subsequently submitted for functionality annotation. Please see representative rejection examples in Fig.~\ref{fig: rejection examples}.

\subsection{Improving Annotation Quality via LLM-Based Verification}
The functionality annotations produced by the LLM probably contain incorrect, ambiguous, and hallucinated samples (See a case in Fig.~\ref{fig: anno pipeline}), which probably misleads the trained VLMs and compromises evaluation accuracy. To improve dataset quality, we prompt LLMs to verify the annotations by checking whether the targeted element $e$ fulfills the intent of the annotated functionality $f$. This process presents the LLMs with the interacted element, its UI context, the UI changes induced by this element, and the functionality generated in the previous annotation process. The LLMs are then tasked with analyzing the UI content changes before predicting whether the interacted element aligns with the given functionality. If the LLMs determine that the interacted element fulfills the functionality given its UI context, the LLMs will grant a full score (An example in Fig.~\ref{fig: verif diff case}). If the interacted element is considered to mismatch the functionality, this functionality can be seen as incorrect as this mismatch indicates that it may not accurately reflect the element's actual role within the UI context.

To mitigate the potential biases in LLMs~\citep{panickssery2024llm, zheng2023judging, bai2024benchmarking}, two different LLMs (i.e., Llama-3-70B~\citep{llama3modelcard} and Mistral-7B-Instruct-v0.2~\citep{mistral}) are employed as verifiers and prompted to output 0-3 scores. The scoring process is formulated as follows:
\begin{equation}
 score = \text{LLM}(p_{\text{verify}}, e, f, s_t, s_{t+1})
\end{equation}
where $p_{\text{verify}}$ denotes the verification prompt (Tab.~\ref{tab:supp:verif prompt}). Only if the two scores are both 3s do we consider the functionality label correct (More details in Sec.~\ref{suc:supp:verif details}). Although this filtering approach seems stringent, we can make up the number of annotations through scaling. 

\input{figure/our_dataset}

\subsection{Functionality Grounding and Referring Task Generation}
\vspace{-2mm}
After rejecting, annotating, and verifying, we obtain a high-quality UI functionality dataset containing triplets of \{UI screenshot, Interacted element, Functionality\}. To convert this dataset into an instruction-following dataset for training and evaluation, we generate functionality grounding and referring tasks using diverse prompt templates (see Tab.~\ref{tab:task templates}). To mitigate the difficulty of predicting absolute values for various resolutions, the coordinates of element bounding boxes are all normalized within the range $[0,99]$ (see Fig.~\ref{fig: our dataset} for examples).

\subsection{Explore the \methodname{} Dataset}
\input{table/main_paper/simple_data_stats}

\input{figure/wordcloud_tokendistrib}

The \methodname{} pipeline finally collects 22.4k trajectories, from which we select 2k grounding samples (evenly divided between web and smartphone views) as the test set and remove the trajectories to which these samples belong. Subsequently, 702k samples are randomly selected from the remaining instances to constitute the training set. The statistics of our dataset in Tab.~\ref{tab:simple data stats} and Sec.~\ref{sec:supp:data stats} show that our dataset covers diverse UIs and exhibits variety in lengths and functional semantics of the annotations. Moreover, our dataset presents a unique ensemble of research challenges for developing generalist web agents in real-world settings. As shown in Tab.~\ref{tab:data comparison} and Fig.~\ref{fig: functionality vs others}, our dataset distinguishes itself from existing literature by providing functionality-rich data as well as tasks that require VLMs to discern the contextual functionalities of elements to achieve high grounding accuracy.

\section{Analysis of Data Quality}
This section analyzes the reliability of the proposed annotation pipeline and data quality.

\noindent{\textbf{Comparison with Human Annotation}} To demonstrate the superiority of the proposed automatic annotation pipeline based on open-source LLMs, $N=145$ samples (99 valid and 46 invalid) are randomly selected as a testbed for comparing the annotation correctness of a trained human annotator and the pipeline. Here, correctness is defined as $Correctness = C / (N - R)$, where $C$ and $R$ denote the numbers of correctly annotated and rejected samples, respectively. The denominator subtracts the number of rejected samples as we are more interested in the percentage of correct samples after rejecting the samples considered invalid by the annotator. The authors thoroughly check the annotation results according to the three criteria in Fig.~\ref{fig: check criteria}: 1. Context-specificity. The functionality annotations must include context-specific descriptions to ensure one-to-one mapping between the element and its annotation. 2. Appropriate details. Avoid detailing unnecessary aspects of the UIs to keep the description focused on functionality. 3. No hallucination. The annotations must not include information not grounded in the visual context of the UIs. See more details in Sec.~\ref{sec:supp:humaneval details}.

After experimenting with three runs, Tab.~\ref{tab:ablate autogui} shows that the proposed AutoGUI pipeline achieves high correctness comparable to the trained human annotator (r6 vs. r1). Without rejection and verification (r2), AutoGUI is inferior as it cannot recognize invalid samples. Notably, simply using the rules written by the authors can improve the correctness, which is further enhanced with the LLM-aided rejector (r4 vs. r3). Moreover, utilizing the annotating LLM itself to self-verify its annotations helps AutoGUI surpass the trained annotator (r5 vs. r1). Introducing another LLM verifier (i.e., Mistral-7B-Instruct-v0.2) brings a slight increase which results from Mistral recognizing Llama-3-70B’s incorrect descriptions of how dropdown menu options work. Overall, these results justify the efficacy of the AutoGUI annotation pipeline.

Qualitatively comparing the annotation patterns of the human and AutoGUI (Fig.~\ref{fig: autogui vs human}), we find that AutoGUI employs the strong LLM to generate more detailed and clear annotations which would take significantly more time for the human annotator. This result suggests that the AutoGUI pipeline can lessen the burden of collecting data for training UI-VLMs.

\noindent{\textbf{Impact of LLM Output Uncertainty}} The uncertainty of LLM outputs manifests in annotation, rejection, and verification, possibly impacting the quality of the AutoGUI dataset. To evaluate this impact, we first sample 100 valid samples to test the AutoGUI pipeline for three runs. The consistency rate is 94.5\%, indicating that 94.5\% of the samples possess consistent annotation outcomes (i.e. correct or incorrect) across the runs. We also test the LLM-aided rejector with 46 invalid samples and find that the rejection consistency over three runs is 79.3\%. This indicates that LLM uncertainty impacts this rejection process. Nevertheless, this impact is minor due to the low prevalence of invalid samples (4\% of all samples) that fail the hand-written rules.

In summary, AutoGUI exhibits annotation correctness comparable to that of human annotators and LLM output uncertainty poses a minor impact on the AutoGUI annotation process.



\input{figure/check_criteria}
\input{table/experiments/ablate_autogui}

