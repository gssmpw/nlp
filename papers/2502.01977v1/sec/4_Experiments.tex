

\section{Fine-Tuning Experiments}
This section validates that our dataset can enhance the GUI grounding capabilities of VLMs and that the proposed functionality grounding and referring are effective fine-tuning tasks.
\subsection{Experimental Settings}
\noindent\textbf{Evaluation Benchmarks} We base our evaluation on the UI grounding benchmarks for various scenarios: \textbf{FuncPred} is the test split from our collected functionality dataset. This benchmark requires a model to locate the element specified by its functionality description. \textbf{ScreenSpot}~\citep{cheng2024seeclick} is a benchmark comprising test samples on mobile, desktop, and web platforms. It requires the model to locate elements based on short instructions. \textbf{RefExp}~\citep{Bai2021UIBertLG} is to locate elements given crowd-sourced referring expressions. \textbf{VisualWebBench (VWB)}~\citep{liu2024visualwebbench} is a comprehensive multi-modal benchmark assessing the understanding capabilities of VLMs in web scenarios. We select the element and action grounding tasks from this benchmark. To better align with high-level semantic instructions for potential agent requirements and avoid redundancy evaluation with ScreenSpot, we use ChatGPT to expand the OCR text descriptions in the original task instructions, such as \textit{Abu Garcia College Fishing} into functionality descriptions like \textit{This element is used to register for the Abu Garcia College Fishing event}.
\textbf{MOTIF}~\citep{Burns2022ADF} requires an agent to complete a natural language command in mobile Apps.
For all of these benchmarks, we report the grounding accuracy (\%): $\text { Acc }= \sum_{i=1}^N \mathbf{1}\left(\text {pred}_i \text { inside GT } \text {bbox}_i\right) / N \times 100 $ where $\mathbf{1}$ is an indicator function and $N$ is the number of test samples. This formula denotes the percentage of samples with the predicted points lying within the bounding boxes of the target elements.

\noindent\textbf{Training Details}
We select Qwen-VL-10B~\citep{bai2023qwen} and SliME-8B~\citep{slime} as the base models and fine-tune them on 25k, 125k, and 702k samples of the AutoGUI training data to investigate how the AutoGUI data enhances the UI grounding capabilities of the VLMs. The models are fine-tuned on 8 A100 GPUs for one epoch. We follow SeeClick~\citep{cheng2024seeclick} to fine-tune Qwen-VL with LoRA~\citep{hu2022lora} and follow the recipe of SliME~\citep{slime} to fine-tune it with only the visual encoder frozen (More details in Sec.~\ref{sec:supp:impl details}).

\noindent\textbf{Compared VLMs}
We compare with both general-purpose VLMs (i.e., LLaVA series~\citep{liu2023llava,liu2024llavanext}, SliME~\citep{slime}, and Qwen-VL~\citep{bai2023qwen}) and UI-oriented ones (i.e., Qwen2-VL~\citep{qwen2vl}, SeeClick~\citep{cheng2024seeclick}, CogAgent~\citep{hong2023cogagent}). SeeClick finetunes Qwen-VL with around 1 million data combining various data sources, including a large proportion of human-annotated UI grounding/referring samples. CogAgent is trained with a huge amount of text recognition, visual grounding, UI understanding, and publicly available text-image datasets, such as LAION-2B~\citep{LAION5B}. During the evaluation, we manually craft grounding prompts suitable for these VLMs.
\subsection{Experimental Results and Analysis}
\input{table/experiments/five_eval_results}

\noindent\textbf{A) AutoGUI functionality annotations effectively enhance VLMs' UI grounding capabilities and achieve scaling effects.} We endeavor to show that the element functionality data autonomously collected by AutoGUI contributes to high grounding accuracy. The results in Tab.~\ref{tab:eval results} demonstrate that on all benchmarks the two base models achieve progressively rising grounding accuracy as the functionality data size scales from 25k to 702k, with SliME-8B's accuracy increasing from merely \textbf{3.2} and \textbf{13.0} to \textbf{62.6} and \textbf{44.0} on FuncPred and ScreenSpot, respectively. This increase is visualized in Fig.~\ref{fig:funcpred scaling success} showing that increasing AutoGUI data amount leads to more precise localization performance.

After fine-tuning with AutoGUI 702k data, the two base models surpass SeeClick, the strong UI-oriented VLM on FuncPred and MOTIF. We notice that the base models lag behind SeeClick and CogAgent on ScreenSpot and RefExp, as the two benchmarks contain test samples whose UIs cannot be easily recorded (e.g., Apple devices and Desktop software) as training data, causing a domain gap. Nevertheless, SliME-8B still exhibits noticeable performance improvements on ScreenSpot and RefExp when scaling up the AutoGUI data, suggesting that the AutoGUI data helps to enhance grounding accuracy on the out-of-domain tasks.

To further unleash the potential of the AutoGUI data, the base model, Qwen-VL, is finetuned with the combination of the AutoGUI and SeeClick UI-grounding data. This model becomes the new state-of-the-art on FuncPred, ScreenSpot, and VWB EG, surpassing SeeClick and CogAgent. This result suggests that our AutoGUI data can be mixed with existing UI grounding training data to foster better UI grounding capabilities.

In summary, our functionality data can endow a general VLM with stronger UI grounding ability and exhibit clear scaling effects as the data size increases.

\input{table/experiments/ablate_ground_feat}


\noindent\textbf{B) Our functionality annotations are effective for enhancing UI grounding capabilities.} To assess the effectiveness of functionality annotations, we compare this annotation type with two existing types: 1) \textbf{Naive element-HTML pairs}, which are directly obtained from the UI source code~\citep{hong2023cogagent} and associate HTML code with elements in specified areas of a screenshot. Examples are shown in Fig.~\ref{fig: functionality vs others}. To create these pairs, we replace the functionality annotations with the corresponding HTML code snippets recorded during trajectory collection. 2) \textbf{Brief functionality descriptions} that are generated by prompting GPT-4o-mini\footnote{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/} to condense the AutoGUI functionality annotations. For example, a full description such as \textit{`This element provides access to a documentation category, allowing users to explore relevant information and guides'} is shortened to \textit{`Documentation category access'}.

After experimenting with Qwen-VL~\citep{bai2023qwen} at the 25k and 125k scales, the results in Tab.~\ref{tab:ablation} show that fine-tuning with the complete functionality annotations is superior to the other two types. Notably, our functionality annotation type yields the largest gain on the challenging FuncPred benchmark that emphasizes contextual functionality grounding. In contrast, the Elem-HTML type performs poorly due to the noise inherent in HTML code (e.g., numerous redundant tags), which reduces fine-tuning efficiency. The condensed functionality annotations are inferior, as the consensing loses details necessary for fine-grained UI understanding. In summary, the AutoGUI functionality annotations provide a clear advantage in enhancing UI grounding capabilities.


\subsection{Failure Case Analysis}
After analyzing the grounding failure cases, we identified several failure patterns in the fine-tuned models: a) difficulty in accurately locating small elements; b) challenges in distinguishing between similar but incorrect elements; and c) issues with recognizing icons that have uncommon shapes. Please refer to Sec.~\ref{sec:supp:case analysis} for details.


