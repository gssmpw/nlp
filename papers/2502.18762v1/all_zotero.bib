@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}
@inproceedings{aljundi2017expert,
  title={Expert gate: Lifelong learning with a network of experts},
  author={Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3366--3375},
  year={2017}
}
@inproceedings{zenke2017continual,
  title={Continual learning through synaptic intelligence},
  author={Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={3987--3995},
  year={2017},
  organization={PMLR}
}
@article{riemer2018learning,
  title={Learning to learn without forgetting by maximizing transfer and minimizing interference},
  author={Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  journal={arXiv preprint arXiv:1810.11910},
  year={2018}
}
@inproceedings{mallya2018piggyback,
  title={Piggyback: Adapting a single network to multiple tasks by learning to mask weights},
  author={Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={67--82},
  year={2018}
}
@article{li2017learning,
  title={Learning without forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017},
  publisher={IEEE}
}
@inproceedings{serra2018overcoming,
  title={Overcoming catastrophic forgetting with hard attention to the task},
  author={Serra, Joan and Suris, Didac and Miron, Marius and Karatzoglou, Alexandros},
  booktitle={International conference on machine learning},
  pages={4548--4557},
  year={2018},
  organization={PMLR}
}
@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={779--788},
  year={2016}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{wang2022foster,
  title={Foster: Feature boosting and compression for class-incremental learning},
  author={Wang, Fu-Yun and Zhou, Da-Wei and Ye, Han-Jia and Zhan, De-Chuan},
  booktitle={European Conference on Computer Vision},
  pages={398--414},
  year={2022},
}
@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9650--9660},
  year={2021}
}
@inproceedings{simon2021learning,
  title={On learning the geodesic path for incremental learning},
  author={Simon, Christian and Koniusz, Piotr and Harandi, Mehrtash},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1591--1600},
  year={2021}
}
@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={702--703},
  year={2020}
}

@article{tian2019contrastive,
  title={Contrastive representation distillation},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  journal={arXiv preprint arXiv:1910.10699},
  year={2019}
}
@article{tamaazousti2019learning,
  title={Learning more universal representations for transfer-learning},
  author={Tamaazousti, Youssef and Le Borgne, Herv{\'e} and Hudelot, C{\'e}line and Tamaazousti, Mohamed and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={9},
  pages={2212--2224},
  year={2019},
  publisher={IEEE}
}
@inproceedings{douillard2020podnet,
  title={Podnet: Pooled outputs distillation for small-tasks incremental learning},
  author={Douillard, Arthur and Cord, Matthieu and Ollion, Charles and Robert, Thomas and Valle, Eduardo},
  booktitle={16th European Conference on Conputer Vision (ECCV)},
  pages={86--102},
  year={2020},
}
@inproceedings{aguilar2020knowledge,
  title={Knowledge distillation from internal representations},
  author={Aguilar, Gustavo and Ling, Yuan and Zhang, Yu and Yao, Benjamin and Fan, Xing and Guo, Chenlei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={7350--7357},
  year={2020}
}

@inproceedings{hou2018lifelong,
  title={Lifelong learning via progressive distillation and retrospection},
  author={Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={437--452},
  year={2018}
}

@inproceedings{hou2019learning,
  title={Learning a unified classifier incrementally via rebalancing},
  author={Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={831--839},
  year={2019}
}

@article{yu_semantic_2020,
	title = {Semantic {Drift} {Compensation} for {Class}-{Incremental} {Learning}},
	url = {http://arxiv.org/abs/2004.00440},
	abstract = {Class-incremental learning of deep networks sequentially increases the number of classes to be classified. During training, the network has only access to data of one task at a time, where each task contains several classes. In this setting, networks suffer from catastrophic forgetting which refers to the drastic drop in performance on previous tasks. The vast majority of methods have studied this scenario for classification networks, where for each new task the classification layer of the network must be augmented with additional weights to make room for the newly added classes. Embedding networks have the advantage that new classes can be naturally included into the network without adding new weights. Therefore, we study incremental learning for embedding networks. In addition, we propose a new method to estimate the drift, called semantic drift, of features and compensate for it without the need of any exemplars. We approximate the drift of previous tasks based on the drift that is experienced by current task data. We perform experiments on fine-grained datasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks suffer significantly less from catastrophic forgetting. We outperform existing methods which do not require exemplars and obtain competitive results compared to methods which store exemplars. Furthermore, we show that our proposed SDC when combined with existing methods to prevent forgetting consistently improves results.},
	urldate = {2021-06-06},
	journal = {arXiv:2004.00440 [cs]},
	author = {Yu, Lu and Twardowski, Bart≈Çomiej and Liu, Xialei and Herranz, Luis and Wang, Kai and Cheng, Yongmei and Jui, Shangling and van de Weijer, Joost},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.00440},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, drift compensation},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/GJBJJTF2/2004.html:text/html;Yu et al. - 2020 - Semantic Drift Compensation for Class-Incremental .pdf:/Users/nicolas/Documents/Zotero/arXiv2004.00440 [cs]2020/Yu et al. - 2020 - Semantic Drift Compensation for Class-Incremental .pdf:application/pdf},
}

@inproceedings{lee_overcoming_2019,
  title={Overcoming catastrophic forgetting with unlabeled data in the wild},
  author={Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={312--321},
  year={2019}
}


@article{mai_online_2021,
  title={Online continual learning in image classification: An empirical survey},
  author={Mai, Zheda and Li, Ruiwen and Jeong, Jihwan and Quispe, David and Kim, Hyunwoo and Sanner, Scott},
  journal={Neurocomputing},
  volume={469},
  pages={28--51},
  year={2022},
  publisher={Elsevier}
}

@article{masana_class-incremental_2021,
	title = {Class-incremental learning: survey and performance evaluation on image classification},
	shorttitle = {Class-incremental learning},
	url = {http://arxiv.org/abs/2010.15277},
	abstract = {For future learning systems incremental learning is desirable, because it allows for: efficient resource usage by eliminating the need to retrain from scratch at the arrival of new data; reduced memory usage by preventing or limiting the amount of data required to be stored -- also important when privacy limitations are imposed; and learning that more closely resembles human learning. The main challenge for incremental learning is catastrophic forgetting, which refers to the precipitous drop in performance on previously learned tasks after learning a new one. Incremental learning of deep neural networks has seen explosive growth in recent years. Initial work focused on task-incremental learning, where a task-ID is provided at inference time. Recently, we have seen a shift towards class-incremental learning where the learner must discriminate at inference time between all classes seen in previous tasks without recourse to a task-ID. In this paper, we provide a complete survey of existing class-incremental learning methods for image classification, and in particular we perform an extensive experimental evaluation on thirteen class-incremental methods. We consider several new experimental scenarios, including a comparison of class-incremental methods on multiple large-scale image classification datasets, investigation into small and large domain shifts, and comparison of various network architectures.},
	urldate = {2021-06-04},
	journal = {arXiv:2010.15277 [cs]},
	author = {Masana, Marc and Liu, Xialei and Twardowski, Bartlomiej and Menta, Mikel and Bagdanov, Andrew D. and van de Weijer, Joost},
	month = may,
	year = {2021},
	note = {arXiv: 2010.15277},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, survey},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/JZT3YMJN/2010.html:text/html;Masana et al. - 2021 - Class-incremental learning survey and performance.pdf:/Users/nicolas/Documents/Zotero/arXiv2010.15277 [cs]2021/Masana et al. - 2021 - Class-incremental learning survey and performance.pdf:application/pdf},
}

@inproceedings{mai_supervised_2021,
  title={Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning},
  author={Mai, Zheda and Li, Ruiwen and Kim, Hyunwoo and Sanner, Scott},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3589--3599},
  year={2021}
}

@article{serra_overcoming_2018,
	title = {Overcoming catastrophic forgetting with hard attention to the task},
	url = {http://arxiv.org/abs/1801.01423},
	abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80\%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
	urldate = {2021-06-03},
	journal = {arXiv:1801.01423 [cs, stat]},
	author = {Serr√†, Joan and Sur√≠s, D√≠dac and Miron, Marius and Karatzoglou, Alexandros},
	month = may,
	year = {2018},
	note = {arXiv: 1801.01423},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, HAT},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/QG8778JN/1801.html:text/html;Serr√† et al. - 2018 - Overcoming catastrophic forgetting with hard atten.pdf:/Users/nicolas/Documents/Zotero/arXiv1801.01423 [cs, stat]2018/Serr√† et al. - 2018 - Overcoming catastrophic forgetting with hard atten.pdf:application/pdf},
}

@article{li_learning_2017,
	title = {Learning without {Forgetting}},
	url = {http://arxiv.org/abs/1606.09282},
	abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
	urldate = {2021-05-31},
	journal = {arXiv:1606.09282 [cs, stat]},
	author = {Li, Zhizhong and Hoiem, Derek},
	month = feb,
	year = {2017},
	note = {arXiv: 1606.09282},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, CL data-focused},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/BUBNICC8/Li and Hoiem - 2017 - Learning without Forgetting.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/5HBFLTKJ/1606.html:text/html},
}

@article{van_de_ven_generative_2019,
	title = {Generative replay with feedback connections as a general strategy for continual learning},
	url = {http://arxiv.org/abs/1809.10635},
	abstract = {A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as "soft targets") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.},
	urldate = {2021-05-20},
	journal = {arXiv:1809.10635 [cs, stat]},
	author = {van de Ven, Gido M. and Tolias, Andreas S.},
	month = apr,
	year = {2019},
	note = {arXiv: 1809.10635},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, RtF},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/6HJPCRYF/van de Ven and Tolias - 2019 - Generative replay with feedback connections as a g.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/HJPKQXPN/1809.html:text/html},
}

@article{shin_continual_2017,
	title = {Continual {Learning} with {Deep} {Generative} {Replay}},
	url = {http://arxiv.org/abs/1705.08690},
	urldate = {2021-05-19},
	journal = {arXiv preprint arXiv:1705.08690},
	author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
	month = dec,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/I9J76NS4/1705.html:text/html;Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf:/Users/nicolas/Documents/Zotero/arXiv1705.08690 [cs]2017/Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf:application/pdf},
}

@article{farquhar_unifying_2019,
	title = {A {Unifying} {Bayesian} {View} of {Continual} {Learning}},
	url = {http://arxiv.org/abs/1902.06494},
	abstract = {Some machine learning applications require continual learning - where data comes in a sequence of datasets, each is used for training and then permanently discarded. From a Bayesian perspective, continual learning seems straightforward: Given the model posterior one would simply use this as the prior for the next task. However, exact posterior evaluation is intractable with many models, especially with Bayesian neural networks (BNNs). Instead, posterior approximations are often sought. Unfortunately, when posterior approximations are used, prior-focused approaches do not succeed in evaluations designed to capture properties of realistic continual learning use cases. As an alternative to prior-focused methods, we introduce a new approximate Bayesian derivation of the continual learning loss. Our loss does not rely on the posterior from earlier tasks, and instead adapts the model itself by changing the likelihood term. We call these approaches likelihood-focused. We then combine prior- and likelihood-focused methods into one objective, tying the two views together under a single unifying framework of approximate Bayesian continual learning.},
	urldate = {2021-05-19},
	journal = {arXiv:1902.06494 [cs, stat]},
	author = {Farquhar, Sebastian and Gal, Yarin},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06494
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/YSSG36Y5/Farquhar and Gal - 2019 - A Unifying Bayesian View of Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/SLJGK4V6/1902.html:text/html},
}

@article{farquhar_towards_2019,
	title = {Towards {Robust} {Evaluations} of {Continual} {Learning}},
	url = {http://arxiv.org/abs/1805.09733},
	abstract = {Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.},
	urldate = {2021-05-19},
	journal = {arXiv:1805.09733 [cs, stat]},
	author = {Farquhar, Sebastian and Gal, Yarin},
	month = jun,
	year = {2019},
	note = {arXiv: 1805.09733},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/NI84R2GY/Farquhar and Gal - 2019 - Towards Robust Evaluations of Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/HXZPQYLT/1805.html:text/html},
}

@inproceedings{rebuffi2017icarl,
  title={icarl: Incremental classifier and representation learning},
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2001--2010},
  year={2017}
}

@article{rao_continual_2019,
  title={Continual unsupervised representation learning},
  author={Rao, Dushyant and Visin, Francesco and Rusu, Andrei and Pascanu, Razvan and Teh, Yee Whye and Hadsell, Raia},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{de_lange_continual_2021,
	title = {A continual learning survey: {Defying} forgetting in classification tasks},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {A continual learning survey},
	url = {http://arxiv.org/abs/1909.08383},
	doi = {10.1109/TPAMI.2021.3057446},
	abstract = {Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.},
	urldate = {2021-04-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ales and Slabaugh, Gregory and Tuytelaars, Tinne},
	year = {2021},
	note = {arXiv: 1909.08383
version: 3},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--1},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/U9TC9FW2/1909.html:text/html;De Lange et al. - 2021 - A continual learning survey Defying forgetting in.pdf:/Users/nicolas/Documents/Zotero/IEEE Transactions on Pattern Analysis and Machine Intelligence2021/De Lange et al. - 2021 - A continual learning survey Defying forgetting in.pdf:application/pdf},
}

@article{kirkpatrick_overcoming_2017,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}


@article{lomonaco_cvpr_2020,
	title = {{CVPR} 2020 {Continual} {Learning} in {Computer} {Vision} {Competition}: {Approaches}, {Results}, {Current} {Challenges} and {Future} {Directions}},
	shorttitle = {{CVPR} 2020 {Continual} {Learning} in {Computer} {Vision} {Competition}},
	url = {http://arxiv.org/abs/2009.09929},
	abstract = {In the last few years, we have witnessed a renewed and fast-growing interest in continual learning with deep neural networks with the shared objective of making current AI systems more adaptive, efficient and autonomous. However, despite the significant and undoubted progress of the field in addressing the issue of catastrophic forgetting, benchmarking different continual learning approaches is a difficult task by itself. In fact, given the proliferation of different settings, training and evaluation protocols, metrics and nomenclature, it is often tricky to properly characterize a continual learning algorithm, relate it to other solutions and gauge its real-world applicability. The first Continual Learning in Computer Vision challenge held at CVPR in 2020 has been one of the first opportunities to evaluate different continual learning algorithms on a common hardware with a large set of shared evaluation metrics and 3 different settings based on the realistic CORe50 video benchmark. In this paper, we report the main results of the competition, which counted more than 79 teams registered, 11 finalists and 2300\$ in prizes. We also summarize the winning approaches, current challenges and future research directions.},
	urldate = {2021-04-15},
	journal = {arXiv:2009.09929 [cs, stat]},
	author = {Lomonaco, Vincenzo and Pellegrini, Lorenzo and Rodriguez, Pau and Caccia, Massimo and She, Qi and Chen, Yu and Jodelet, Quentin and Wang, Ruiping and Mai, Zheda and Vazquez, David and Parisi, German I. and Churamani, Nikhil and Pickett, Marc and Laradji, Issam and Maltoni, Davide},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.09929},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Y4U5I3CK/2009.html:text/html;Lomonaco et al. - 2020 - CVPR 2020 Continual Learning in Computer Vision Co.pdf:/Users/nicolas/Documents/Zotero/arXiv2009.09929 [cs, stat]2020/Lomonaco et al. - 2020 - CVPR 2020 Continual Learning in Computer Vision Co.pdf:application/pdf},
}

@misc{noauthor_facebookresearchswav_2021,
	title = {facebookresearch/swav},
	copyright = {View license         ,                 View license},
	url = {https://github.com/facebookresearch/swav},
	abstract = {PyTorch implementation of SwAV https//arxiv.org/abs/2006.09882},
	urldate = {2021-04-15},
	publisher = {Facebook Research},
	month = apr,
	year = {2021},
	note = {original-date: 2020-07-16T21:17:58Z},
}

@article{caron_unsupervised_2021,
  title={Unsupervised learning of visual features by contrasting cluster assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9912--9924},
  year={2020}
}

@misc{noauthor_catastrophic_2021,
	title = {Catastrophic interference},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Catastrophic_interference&oldid=1017898147},
	abstract = {Catastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks are an important part of the network approach and connectionist approach to cognitive science. With these networks, human capabilities such as memory and learning can be modeled using computer simulations. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ratcliff (1990). It is a radical manifestation of the 'sensitivity-stability' dilemma or the 'stability-plasticity' dilemma. Specifically, these problems refer to the challenge of making an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network can generalize to unseen inputs, but they are very sensitive to new information. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is an issue when modelling human memory, because unlike these networks, humans typically do not show catastrophic forgetting.},
	language = {en},
	urldate = {2021-04-15},
	journal = {Wikipedia},
	month = apr,
	year = {2021},
	note = {Page Version ID: 1017898147},
	file = {Snapshot:/Users/nicolas/Zotero/storage/7E34IVWL/index.html:text/html},
}

@article{seff_continual_2017,
	title = {Continual {Learning} in {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1705.08395},
	abstract = {Developments in deep generative models have allowed for tractable learning of high-dimensional data distributions. While the employed learning procedures typically assume that training data is drawn i.i.d. from the distribution of interest, it may be desirable to model distinct distributions which are observed sequentially, such as when different classes are encountered over time. Although conditional variations of deep generative models permit multiple distributions to be modeled by a single network in a disentangled fashion, they are susceptible to catastrophic forgetting when the distributions are encountered sequentially. In this paper, we adapt recent work in reducing catastrophic forgetting to the task of training generative adversarial networks on a sequence of distinct distributions, enabling continual generative modeling.},
	urldate = {2021-04-15},
	journal = {arXiv:1705.08395 [cs, stat]},
	author = {Seff, Ari and Beatson, Alex and Suo, Daniel and Liu, Han},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08395},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/H59KVJYH/1705.html:text/html;Seff et al. - 2017 - Continual Learning in Generative Adversarial Nets.pdf:/Users/nicolas/Documents/Zotero/arXiv1705.08395 [cs, stat]2017/Seff et al. - 2017 - Continual Learning in Generative Adversarial Nets.pdf:application/pdf},
}

@inproceedings{zenke_continual_2017-1,
	title = {Continual {Learning} {Through} {Synaptic} {Intelligence}},
	url = {http://proceedings.mlr.press/v70/zenke17a.html},
	abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biologica...},
	language = {en},
	urldate = {2021-04-15},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {3987--3995},
	file = {Snapshot:/Users/nicolas/Zotero/storage/N2C6PYKM/zenke17a.html:text/html;Zenke et al. - 2017 - Continual Learning Through Synaptic Intelligence.pdf:/Users/nicolas/Documents/Zotero/PMLR2017/Zenke et al. - 2017 - Continual Learning Through Synaptic Intelligence.pdf:application/pdf},
}

@article{lomonaco_continual_nodate,
	title = {Continual {Learning} with {Deep} {Architectures}},
	language = {en},
	author = {Lomonaco, Vincenzo},
	pages = {150},
	file = {Lomonaco - Continual Learning with Deep Architectures.pdf:/Users/nicolas/Zotero/storage/RMRAX28S/Lomonaco - Continual Learning with Deep Architectures.pdf:application/pdf},
}

@incollection{schwenker_comparing_2016,
	address = {Cham},
	title = {Comparing {Incremental} {Learning} {Strategies} for {Convolutional} {Neural} {Networks}},
	volume = {9896},
	isbn = {978-3-319-46181-6 978-3-319-46182-3},
	url = {http://link.springer.com/10.1007/978-3-319-46182-3_15},
	abstract = {In the last decade, Convolutional Neural Networks (CNNs) have shown to perform incredibly well in many computer vision tasks such as object recognition and object detection, being able to extract meaningful high-level invariant features. However, partly because of their complex training and tricky hyper-parameters tuning, CNNs have been scarcely studied in the context of incremental learning where data are available in consecutive batches and retraining the model from scratch is unfeasible. In this work we compare different incremental learning strategies for CNN based architectures, targeting real-word applications.},
	language = {en},
	urldate = {2021-04-15},
	booktitle = {Artificial {Neural} {Networks} in {Pattern} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Lomonaco, Vincenzo and Maltoni, Davide},
	editor = {Schwenker, Friedhelm and Abbas, Hazem M. and El Gayar, Neamat and Trentin, Edmondo},
	year = {2016},
	doi = {10.1007/978-3-319-46182-3_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {175--184},
	file = {Lomonaco and Maltoni - 2016 - Comparing Incremental Learning Strategies for Conv.pdf:/Users/nicolas/Zotero/storage/6RP63QPU/Lomonaco and Maltoni - 2016 - Comparing Incremental Learning Strategies for Conv.pdf:application/pdf},
}

@article{pomponi_efficient_2020,
	title = {Efficient {Continual} {Learning} in {Neural} {Networks} with {Embedding} {Regularization}},
	volume = {397},
	issn = {09252312},
	url = {http://arxiv.org/abs/1909.03742},
	doi = {10.1016/j.neucom.2020.01.093},
	abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
	urldate = {2021-04-15},
	journal = {Neurocomputing},
	author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
	month = jul,
	year = {2020},
	note = {arXiv: 1909.03742},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {139--148},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/AZ6TMM7Q/1909.html:text/html;Pomponi et al. - 2020 - Efficient Continual Learning in Neural Networks wi.pdf:/Users/nicolas/Documents/Zotero/Neurocomputing2020/Pomponi et al. - 2020 - Efficient Continual Learning in Neural Networks wi.pdf:application/pdf},
}

@article{caron_deep_2019,
  title={Deep clustering for unsupervised learning of visual features},
  author={Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={132--149},
  year={2018}
}

@inproceedings{chen_exploring_2020,
  title={Exploring simple siamese representation learning},
  author={Chen, Xinlei and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15750--15758},
  year={2021}
}


@article{chen_improved_2020,
	title = {Improved {Baselines} with {Momentum} {Contrastive} {Learning}},
	abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
	urldate = {2021-04-13},
	journal = {arXiv:2003.04297},
	author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
	year = {2020},
	note = {arXiv: 2003.04297},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{yan_clusterfit_2019,
	title = {{ClusterFit}: {Improving} {Generalization} of {Visual} {Representations}},
	shorttitle = {{ClusterFit}},
	url = {http://arxiv.org/abs/1912.03330},
	abstract = {Pre-training convolutional neural networks with weakly-supervised and self-supervised strategies is becoming increasingly popular for several computer vision tasks. However, due to the lack of strong discriminative signals, these learned representations may overfit to the pre-training objective (e.g., hashtag prediction) and not generalize well to downstream tasks. In this work, we present a simple strategy - ClusterFit (CF) to improve the robustness of the visual representations learned during pre-training. Given a dataset, we (a) cluster its features extracted from a pre-trained network using k-means and (b) re-train a new network from scratch on this dataset using cluster assignments as pseudo-labels. We empirically show that clustering helps reduce the pre-training task-specific information from the extracted features thereby minimizing overfitting to the same. Our approach is extensible to different pre-training frameworks -- weak- and self-supervised, modalities -- images and videos, and pre-training tasks -- object and action classification. Through extensive transfer learning experiments on 11 different target datasets of varied vocabularies and granularities, we show that ClusterFit significantly improves the representation quality compared to the state-of-the-art large-scale (millions / billions) weakly-supervised image and video models and self-supervised image models.},
	urldate = {2021-04-13},
	journal = {arXiv:1912.03330 [cs]},
	author = {Yan, Xueting and Misra, Ishan and Gupta, Abhinav and Ghadiyaram, Deepti and Mahajan, Dhruv},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.03330},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/ZLUNWWTI/1912.html:text/html;Yan et al_2019_ClusterFit.pdf:/Users/nicolas/Documents/Zotero/arXiv1912.03330 [cs]2019/Yan et al_2019_ClusterFit.pdf:application/pdf},
}

@inproceedings{caron_unsupervised_2019,
	title = {Unsupervised {Pre}-{Training} of {Image} {Features} on {Non}-{Curated} {Data}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Caron_Unsupervised_Pre-Training_of_Image_Features_on_Non-Curated_Data_ICCV_2019_paper.html},
	urldate = {2021-04-13},
	author = {Caron, Mathilde and Bojanowski, Piotr and Mairal, Julien and Joulin, Armand},
	year = {2019},
	pages = {2959--2968},
	file = {Caron et al_2019_Unsupervised Pre-Training of Image Features on Non-Curated Data.pdf:/Users/nicolas/Documents/Zotero/2019/Caron et al_2019_Unsupervised Pre-Training of Image Features on Non-Curated Data.pdf:application/pdf;Snapshot:/Users/nicolas/Zotero/storage/E6LLFK9A/Caron_Unsupervised_Pre-Training_of_Image_Features_on_Non-Curated_Data_ICCV_2019_paper.html:text/html},
}

@article{saito_scanimate_2021,
	title = {{SCANimate}: {Weakly} {Supervised} {Learning} of {Skinned} {Clothed} {Avatar} {Networks}},
	shorttitle = {{SCANimate}},
	url = {http://arxiv.org/abs/2104.03313},
	abstract = {We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de.},
	urldate = {2021-04-09},
	journal = {arXiv:2104.03313 [cs]},
	author = {Saito, Shunsuke and Yang, Jinlong and Ma, Qianli and Black, Michael J.},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.03313
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/EDCZKQY3/2104.html:text/html;Saito et al. - 2021 - SCANimate Weakly Supervised Learning of Skinned C.pdf:/Users/nicolas/Documents/Zotero/arXiv2104.03313 [cs]2021/Saito et al. - 2021 - SCANimate Weakly Supervised Learning of Skinned C.pdf:application/pdf},
}

@article{lin_streaming_2021,
	title = {Streaming {Self}-{Training} via {Domain}-{Agnostic} {Unlabeled} {Images}},
	url = {http://arxiv.org/abs/2104.03309},
	abstract = {We present streaming self-training (SST) that aims to democratize the process of learning visual recognition models such that a non-expert user can define a new task depending on their needs via a few labeled examples and minimal domain knowledge. Key to SST are two crucial observations: (1) domain-agnostic unlabeled images enable us to learn better models with a few labeled examples without any additional knowledge or supervision; and (2) learning is a continuous process and can be done by constructing a schedule of learning updates that iterates between pre-training on novel segments of the streams of unlabeled data, and fine-tuning on the small and fixed labeled dataset. This allows SST to overcome the need for a large number of domain-specific labeled and unlabeled examples, exorbitant computational resources, and domain/task-specific knowledge. In this setting, classical semi-supervised approaches require a large amount of domain-specific labeled and unlabeled examples, immense resources to process data, and expert knowledge of a particular task. Due to these reasons, semi-supervised learning has been restricted to a few places that can house required computational and human resources. In this work, we overcome these challenges and demonstrate our findings for a wide range of visual recognition tasks including fine-grained image classification, surface normal estimation, and semantic segmentation. We also demonstrate our findings for diverse domains including medical, satellite, and agricultural imagery, where there does not exist a large amount of labeled or unlabeled data.},
	urldate = {2021-04-09},
	journal = {arXiv:2104.03309 [cs]},
	author = {Lin, Zhiqiu and Ramanan, Deva and Bansal, Aayush},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.03309
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/DYM7TQWF/2104.html:text/html;Lin et al. - 2021 - Streaming Self-Training via Domain-Agnostic Unlabe.pdf:/Users/nicolas/Documents/Zotero/arXiv2104.03309 [cs]2021/Lin et al. - 2021 - Streaming Self-Training via Domain-Agnostic Unlabe.pdf:application/pdf},
}

@article{grill_bootstrap_2020,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}


@article{chen_improved_2020-1,
	title = {Improved {Baselines} with {Momentum} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2003.04297},
	abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
	urldate = {2021-04-06},
	journal = {arXiv:2003.04297 [cs]},
	author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.04297},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/9B85MNZ5/2003.html:text/html;Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf:/Users/nicolas/Documents/Zotero/arXiv2003.04297 [cs]2020/Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf:application/pdf},
}

@misc{noauthor_unsupervised_nodate,
	title = {Unsupervised {Machine} {Learning} {For} {Data} {Clustering}},
	url = {https://ehikioya.com/forums/topic/unsupervised-machine-learning-for-data-clustering/},
	abstract = {Unsupervised Machine Learning helps us understand the relationships that exist within a dataset. It is a branch of Machine Learning that learns from test data that has not been labeled, classified or categorized. In a Multiple Linear Regression problem (which is a Supervised Machine Learning Algorithm), you would typically have independent variables and a response [‚Ä¶]},
	language = {en-US},
	urldate = {2021-03-17},
	journal = {Ehi Kioya},
	file = {Snapshot:/Users/nicolas/Zotero/storage/6HRA5F3F/unsupervised-machine-learning-for-data-clustering.html:text/html},
}

@misc{noauthor_vgg16_2018,
	title = {{VGG16} - {Convolutional} {Network} for {Classification} and {Detection}},
	url = {https://neurohive.io/en/popular-networks/vgg16/},
	abstract = {How does VGG16 neural network achieves 92.7\% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes.},
	language = {en-US},
	urldate = {2021-03-16},
	month = nov,
	year = {2018},
	file = {Snapshot:/Users/nicolas/Zotero/storage/XI3B2HC6/vgg16.html:text/html},
}

@misc{says_neural_2017,
	title = {Neural {Network} {Tutorial} - {Artificial} {Intelligence} {\textbar} {Deep} {Learning}},
	url = {https://www.edureka.co/blog/neural-network-tutorial/},
	abstract = {This blog on Neural Network tutorial, talks about what is Multi Layer Perceptron and how it works. It also includes a use-case in the end.},
	language = {en-US},
	urldate = {2021-03-16},
	journal = {Edureka},
	author = {says, Yves},
	month = dec,
	year = {2017},
	note = {Section: Uncategorized},
	file = {Snapshot:/Users/nicolas/Zotero/storage/K9QJ7V8V/neural-network-tutorial.html:text/html},
}

@misc{noauthor_what_2020,
	title = {What {Is} {Artificial} {Neural} {Network}? {All} {You} {Need} {To} {Know}},
	shorttitle = {What {Is} {Artificial} {Neural} {Network}?},
	url = {https://k21academy.com/datascience/deep-learning/artificial-neural-network/},
	abstract = {Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks.},
	language = {en-US},
	urldate = {2021-03-16},
	journal = {Cloud Training Program},
	month = dec,
	year = {2020},
	file = {Snapshot:/Users/nicolas/Zotero/storage/EYUCTQDI/artificial-neural-network.html:text/html},
}

@article{zemouri_intelligence_2019,
	title = {Intelligence artificielle : quel avenir en anatomie pathologique ?},
	volume = {39},
	shorttitle = {Intelligence artificielle},
	doi = {10.1016/j.annpat.2019.01.004},
	abstract = {R√©sum√©
Les techniques d‚Äôintelligence artificielle et en particulier les r√©seaux de neurones profonds (Deep Learning) sont en pleine √©mergence dans le domaine biom√©dical. Les r√©seaux de neurones s‚Äôinspirent du mod√®le biologique, ils sont interconnect√©s entre eux et suivent des mod√®les math√©matiques. Lors de l‚Äôutilisation des r√©seaux de neurones artificiels, deux phases sont n√©cessaires : une phase d‚Äôapprentissage et une phase d‚Äôexploitation. Les deux principales applications sont la classification et la r√©gression. Des outils informatiques comme les processeurs graphiques acc√©l√©rateurs de calcul ou des biblioth√®ques de d√©veloppement sp√©cifiques ont donn√© un nouveau souffle √† ces techniques. Leur champ d‚Äôapplication est vaste et permet la gestion de donn√©es de masse (Big data) en g√©nomique et biologie mol√©culaire ainsi que l‚Äôanalyse automatis√©e de lames histologiques gr√¢ce aux techniques de num√©risation r√©alis√©es √† l‚Äôaide de scanners de lames de type Whole Slide Image. Le Whole Slide Image scanner peut acqu√©rir et stocker des lames de microscopie sous forme d‚Äôimage num√©riques. Cette num√©risation associ√©e aux algorithmes de deep learning permet une reconnaissance automatique des l√©sions gr√¢ce √† l‚Äôidentification de r√©gions d‚Äôint√©r√™t, valid√©es au pr√©alable par le pathologiste. Ces techniques d‚Äôaide assist√©e par ordinateur sont test√©es en particulier en pathologie mammaire et dermatologique. Elles permettront, associ√©es aux donn√©es cliniques, radiologiques et de biologie mol√©culaire, une vision plus globale et performante, et r√©aliseront une aide au diagnostic en pathologie.},
	journal = {Annales de Pathologie},
	author = {Zemouri, Ryad and devalland, Christine and Valmary-Degano, S√©verine and Zerhouni, Noureddine},
	month = feb,
	year = {2019},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/SYUEKPXE/Zemouri et al. - 2019 - Intelligence artificielle  quel avenir en anatomi.pdf:application/pdf},
}

@article{dave_tclr_2021,
	title = {{TCLR}: {Temporal} {Contrastive} {Learning} for {Video} {Representation}},
	shorttitle = {{TCLR}},
	url = {http://arxiv.org/abs/2101.07974},
	abstract = {Contrastive learning has nearly closed the gap between supervised and self-supervised learning of image representations. Existing extensions of contrastive learning to the domain of video data however do not explicitly attempt to represent the internal distinctiveness across the temporal dimension of video clips. We develop a new temporal contrastive learning framework consisting of two novel losses to improve upon existing contrastive self-supervised video representation learning methods. The first loss adds the task of discriminating between non-overlapping clips from the same video, whereas the second loss aims to discriminate between timesteps of the feature map of an input clip in order to increase the temporal diversity of the features. Temporal contrastive learning achieves significant improvement over the state-of-the-art results in downstream video understanding tasks such as action recognition, limited-label action classification, and nearest-neighbor video retrieval on video datasets across multiple 3D CNN architectures. With the commonly used 3D-ResNet-18 architecture, we achieve 82.4\% (+5.1\% increase over the previous best) top-1 accuracy on UCF101 and 52.9\% (+5.4\% increase) on HMDB51 action classification, and 56.2\% (+11.7\% increase) Top-1 Recall on UCF101 nearest neighbor video retrieval.},
	urldate = {2021-03-02},
	journal = {arXiv:2101.07974 [cs]},
	author = {Dave, Ishan and Gupta, Rohit and Rizve, Mamshad Nayeem and Shah, Mubarak},
	month = feb,
	year = {2021},
	note = {arXiv: 2101.07974},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/VV3WSARN/2101.html:text/html;Dave et al_2021_TCLR.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.07974 [cs]2021/Dave et al_2021_TCLR.pdf:application/pdf},
}

@article{wu_conditional_2020,
	title = {Conditional {Mutual} information-based {Contrastive} {Loss} for {Financial} {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/2002.07638},
	abstract = {We present a representation learning framework for financial time series forecasting. One challenge of using deep learning models for finance forecasting is the shortage of available training data when using small datasets. Direct trend classification using deep neural networks trained on small datasets is susceptible to the overfitting problem. In this paper, we propose to first learn compact representations from time series data, then use the learned representations to train a simpler model for predicting time series movements. We consider a class-conditioned latent variable model. We train an encoder network to maximize the mutual information between the latent variables and the trend information conditioned on the encoded observed variables. We show that conditional mutual information maximization can be approximated by a contrastive loss. Then, the problem is transformed into a classification task of determining whether two encoded representations are sampled from the same class or not. This is equivalent to performing pairwise comparisons of the training datapoints, and thus, improves the generalization ability of the encoder network. We use deep autoregressive models as our encoder to capture long-term dependencies of the sequence data. Empirical experiments indicate that our proposed method has the potential to advance state-of-the-art performance.},
	urldate = {2021-03-02},
	journal = {arXiv:2002.07638 [cs, stat]},
	author = {Wu, Hanwei and Gattami, Ather and Flierl, Markus},
	month = may,
	year = {2020},
	note = {arXiv: 2002.07638},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/294MC5WG/2002.html:text/html;Wu et al_2020_Conditional Mutual information-based Contrastive Loss for Financial Time Series.pdf:/Users/nicolas/Documents/Zotero/arXiv2002.07638 [cs, stat]2020/Wu et al_2020_Conditional Mutual information-based Contrastive Loss for Financial Time Series.pdf:application/pdf},
}

@article{kamnitsas_semi-supervised_2018,
	title = {Semi-{Supervised} {Learning} via {Compact} {Latent} {Space} {Clustering}},
	url = {http://arxiv.org/abs/1806.02679},
	abstract = {We present a novel cost function for semi-supervised learning of neural networks that encourages compact clustering of the latent space to facilitate separation. The key idea is to dynamically create a graph over embeddings of labeled and unlabeled samples of a training batch to capture underlying structure in feature space, and use label propagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the graph that regularizes the latent space to form a single compact cluster per class, while avoiding to disturb existing clusters during optimization. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our approach combines the benefits of graph-based regularization with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily applied to existing networks to enable an effective use of unlabeled data.},
	urldate = {2021-03-02},
	journal = {arXiv:1806.02679 [cs, stat]},
	author = {Kamnitsas, Konstantinos and Castro, Daniel C. and Folgoc, Loic Le and Walker, Ian and Tanno, Ryutaro and Rueckert, Daniel and Glocker, Ben and Criminisi, Antonio and Nori, Aditya},
	month = jul,
	year = {2018},
	note = {arXiv: 1806.02679},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/7BFUBHPQ/1806.html:text/html;Kamnitsas et al_2018_Semi-Supervised Learning via Compact Latent Space Clustering.pdf:/Users/nicolas/Documents/Zotero/arXiv1806.02679 [cs, stat]2018/Kamnitsas et al_2018_Semi-Supervised Learning via Compact Latent Space Clustering.pdf:application/pdf},
}

@article{chen_simple_2020,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}


@article{sermanet_time-contrastive_2018,
	title = {Time-{Contrastive} {Networks}: {Self}-{Supervised} {Learning} from {Video}},
	shorttitle = {Time-{Contrastive} {Networks}},
	url = {http://arxiv.org/abs/1704.06888},
	abstract = {We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate},
	language = {en},
	urldate = {2021-02-24},
	journal = {arXiv:1704.06888 [cs]},
	author = {Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey},
	month = mar,
	year = {2018},
	note = {arXiv: 1704.06888},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Sermanet et al_2018_Time-Contrastive Networks.pdf:/Users/nicolas/Documents/Zotero/arXiv1704.06888 [cs]2018/Sermanet et al_2018_Time-Contrastive Networks.pdf:application/pdf},
}

@inproceedings{zimmermann_contrastive_2021,
  title={Contrastive learning inverts the data generating process},
  author={Zimmermann, Roland S and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
  booktitle={International Conference on Machine Learning},
  pages={12979--12990},
  year={2021},
  organization={PMLR}
}


@article{hinton_distilling_2015,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}


@article{wang_normface_2017,
	title = {{NormFace}: {L2} {Hypersphere} {Embedding} for {Face} {Verification}},
	shorttitle = {{NormFace}},
	url = {http://arxiv.org/abs/1704.06369},
	doi = {10.1145/3123266.3123359},
	abstract = {Thanks to the recent developments of Convolutional Neural Networks, the performance of face verification methods has increased rapidly. In a typical face verification method, feature normalization is a critical step for boosting performance. This motivates us to introduce and study the effect of normalization during training. But we find this is non-trivial, despite normalization being differentiable. We identify and study four issues related to normalization through mathematical analysis, which yields understanding and helps with parameter settings. Based on this analysis we propose two strategies for training using normalized features. The first is a modification of softmax loss, which optimizes cosine similarity instead of inner-product. The second is a reformulation of metric learning by introducing an agent vector for each class. We show that both strategies, and small variants, consistently improve performance by between 0.2\% to 0.4\% on the LFW dataset based on two models. This is significant because the performance of the two models on LFW dataset is close to saturation at over 98\%. Codes and models are released on https://github.com/happynear/NormFace},
	urldate = {2021-02-19},
	journal = {Proceedings of the 25th ACM international conference on Multimedia},
	author = {Wang, Feng and Xiang, Xiang and Cheng, Jian and Yuille, Alan L.},
	month = oct,
	year = {2017},
	note = {arXiv: 1704.06369},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1041--1049},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/MA7LFS46/1704.html:text/html;Wang et al_2017_NormFace.pdf:/Users/nicolas/Documents/Zotero/Proceedings of the 25th ACM international conference on Multimedia2017/Wang et al_2017_NormFace.pdf:application/pdf},
}

@article{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artiÔ¨Åcial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	journal = {arXiv:1807.03748},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mutual Information},
	file = {Oord et al_2019_Representation Learning with Contrastive Predictive Coding.pdf:/Users/nicolas/Documents/Zotero/arXiv1807.03748 [cs, stat]2019/Oord et al_2019_Representation Learning with Contrastive Predictive Coding.pdf:application/pdf},
}

@article{hjelm_learning_2019,
	title = {Learning deep representations by mutual information estimation and maximization},
	url = {http://arxiv.org/abs/1808.06670},
	abstract = {This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can signiÔ¨Åcantly improve a representation‚Äôs suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classiÔ¨Åcation tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards Ô¨Çexible formulations of representation learning objectives for speciÔ¨Åc end-goals.},
	language = {en},
	urldate = {2021-02-05},
	journal = {arXiv:1808.06670 [cs, stat]},
	author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
	month = feb,
	year = {2019},
	note = {arXiv: 1808.06670},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mutual Information},
}

@article{gordon_watching_2020,
	title = {Watching the {World} {Go} {By}: {Representation} {Learning} from {Unlabeled} {Videos}},
	shorttitle = {Watching the {World} {Go} {By}},
	url = {http://arxiv.org/abs/2003.07990},
	abstract = {Recent single image unsupervised representation learning techniques show remarkable success on a variety of tasks. The basic principle in these works is instance discrimination: learning to differentiate between two augmented versions of the same image and a large batch of unrelated images. Networks learn to ignore the augmentation noise and extract semantically meaningful representations. Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial ways and are not aligned with how objects actually change e.g. occlusion, deformation, viewpoint change. In this paper, we argue that videos offer this natural augmentation for free. Videos can provide entirely new views of objects, show deformation, and even connect semantically similar but visually distinct concepts. We propose Video Noise Contrastive Estimation, a method for using unlabeled video to learn strong, transferable single image representations. We demonstrate improvements over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks. Code and the Random Related Video Views dataset are available at https://www.github.com/danielgordon10/vince},
	language = {en},
	urldate = {2021-02-19},
	journal = {arXiv:2003.07990 [cs]},
	author = {Gordon, Daniel and Ehsani, Kiana and Fox, Dieter and Farhadi, Ali},
	month = may,
	year = {2020},
	note = {arXiv: 2003.07990},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Gordon et al. - 2020 - Watching the World Go By Representation Learning .pdf:/Users/nicolas/Zotero/storage/TUYINXAB/Gordon et al. - 2020 - Watching the World Go By Representation Learning .pdf:application/pdf},
}

@article{bourrier_echantillonnage_nodate,
	title = {√âchantillonnage compress√© et r√©duction de dimension pour l'apprentissage non supervis√©},
	language = {fr},
	author = {Bourrier, Anthony},
	pages = {129},
	file = {Bourrier - √âchantillonnage compress√© et r√©duction de dimensio.pdf:/Users/nicolas/Zotero/storage/HHKEYQQH/Bourrier - √âchantillonnage compress√© et r√©duction de dimensio.pdf:application/pdf},
}

@inproceedings{chopra_learning_2005,
	address = {San Diego, CA, USA},
	title = {Learning a {Similarity} {Metric} {Discriminatively}, with {Application} to {Face} {Verification}},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467314/},
	doi = {10.1109/CVPR.2005.202},
	abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or veriÔ¨Åcation applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the ¬¢¬§¬£ norm in the target space approximates the ‚Äúsemantic‚Äù distance in the input space. The method is applied to a face veriÔ¨Åcation task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artiÔ¨Åcial occlusions such as dark glasses and obscuring scarves.},
	language = {en},
	urldate = {2021-02-18},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	publisher = {IEEE},
	author = {Chopra, S. and Hadsell, R. and LeCun, Y.},
	year = {2005},
	pages = {539--546},
	file = {Chopra et al. - 2005 - Learning a Similarity Metric Discriminatively, wit.pdf:/Users/nicolas/Zotero/storage/BGRMK3N5/Chopra et al. - 2005 - Learning a Similarity Metric Discriminatively, wit.pdf:application/pdf},
}

@article{ma_noise_2018,
	title = {Noise {Contrastive} {Estimation} and {Negative} {Sampling} for {Conditional} {Models}: {Consistency} and {Statistical} {Efficiency}},
	shorttitle = {Noise {Contrastive} {Estimation} and {Negative} {Sampling} for {Conditional} {Models}},
	url = {http://arxiv.org/abs/1809.01812},
	abstract = {Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and trade-offs of both methods.},
	urldate = {2021-02-16},
	journal = {arXiv:1809.01812 [cs, stat]},
	author = {Ma, Zhuang and Collins, Michael},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.01812},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/P24M95IV/1809.html:text/html;Ma_Collins_2018_Noise Contrastive Estimation and Negative Sampling for Conditional Models.pdf:/Users/nicolas/Documents/Zotero/arXiv1809.01812 [cs, stat]2018/Ma_Collins_2018_Noise Contrastive Estimation and Negative Sampling for Conditional Models.pdf:application/pdf},
}

@article{sermanet_time-contrastive_2018-1,
	title = {Time-{Contrastive} {Networks}: {Self}-{Supervised} {Learning} from {Video}},
	shorttitle = {Time-{Contrastive} {Networks}},
	url = {http://arxiv.org/abs/1704.06888},
	abstract = {We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate},
	language = {en},
	urldate = {2021-02-15},
	journal = {arXiv:1704.06888 [cs]},
	author = {Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey},
	month = mar,
	year = {2018},
	note = {arXiv: 1704.06888},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Sermanet et al. - 2018 - Time-Contrastive Networks Self-Supervised Learnin.pdf:/Users/nicolas/Zotero/storage/WXHTMU5C/Sermanet et al. - 2018 - Time-Contrastive Networks Self-Supervised Learnin.pdf:application/pdf},
}

@article{tian_weakly-supervised_2021,
	title = {Weakly-supervised {Video} {Anomaly} {Detection} with {Contrastive} {Learning} of {Long} and {Short}-range {Temporal} {Features}},
	url = {http://arxiv.org/abs/2101.10030},
	abstract = {In this paper, we address the problem of weaklysupervised video anomaly detection, in which given videolevel labels for training, we aim to identify in test videos, the snippets containing abnormal events. Although current methods based on multiple instance learning (MIL) show effective detection performance, they ignore important video temporal dependencies. Also, the number of abnormal snippets can vary per anomaly video, which complicates the training process of MIL-based methods because they tend to focus on the most abnormal snippet ‚Äì this can cause it to mistakenly select a normal snippet instead of an abnormal snippet, and also to fail to select all abnormal snippets available. We propose a novel method, named Multiscale Temporal Network trained with top-K Contrastive Multiple Instance Learning (MTN-KMIL), to address the issues above. The main contributions of MTN-KMIL are: 1) a novel synthesis of a pyramid of dilated convolutions and a self-attention mechanism, with the former capturing the multi-scale short-range temporal dependencies between snippets and the latter capturing long-range temporal dependencies; and 2) a novel contrastive MIL learning method that enforces large margins between the top-K normal and abnormal video snippets at the feature representation level and anomaly score level, resulting in accurate anomaly discrimination. Extensive experiments show that our method outperforms several state-of-the-art methods by a large margin on three benchmark data sets (ShanghaiTech, UCF-Crime and XD-Violence). Code is available at https://github.com/tianyu0207/MTNKMIL.},
	language = {en},
	urldate = {2021-02-15},
	journal = {arXiv:2101.10030 [cs]},
	author = {Tian, Yu and Pang, Guansong and Chen, Yuanhong and Singh, Rajvinder and Verjans, Johan W. and Carneiro, Gustavo},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.10030},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Tian et al_2021_Weakly-supervised Video Anomaly Detection with Contrastive Learning of Long and.pdf:/Users/nicolas/Documents/Zotero/test_group_sync/Contrastive Learning/Tian et al_2021_Weakly-supervised Video Anomaly Detection with Contrastive Learning of Long and.pdf:application/pdf},
}

@article{mnih_fast_nodate,
	title = {A fast and simple algorithm for training neural probabilistic language models},
	abstract = {In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.},
	language = {en},
	author = {Mnih, Andriy and Teh, Yee Whye},
	pages = {8},
	file = {Mnih_Teh_A fast and simple algorithm for training neural probabilistic language models.pdf:/Users/nicolas/Documents/Zotero/_/Mnih_Teh_A fast and simple algorithm for training neural probabilistic language models.pdf:application/pdf},
}

@inproceedings{dosovitskiy2020vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2021-02-10},
	journal = {arXiv:1905.11946 [cs, stat]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = may,
	year = {2019},
	note = {arXiv: 1905.11946
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/GB83UQPI/1905.html:text/html;Tan_Le_2019_EfficientNet.pdf:/Users/nicolas/Documents/Zotero/arXiv1905.11946 [cs, stat]2019/Tan_Le_2019_EfficientNet.pdf:application/pdf},
}

@inproceedings{hadsell_dimensionality_2006,
	address = {New York, NY, USA},
	title = {Dimensionality {Reduction} by {Learning} an {Invariant} {Mapping}},
	volume = {2},
	isbn = {978-0-7695-2597-6},
	url = {http://ieeexplore.ieee.org/document/1640964/},
	doi = {10.1109/CVPR.2006.100},
	abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that ‚Äúsimilar‚Äù points in input space are mapped to nearby points on the manifold. Most existing techniques for solving the problem suffer from two drawbacks. First, most of them depend on a meaningful and computable distance metric in input space. Second, they do not compute a ‚Äúfunction‚Äù that can accurately map new input samples whose relationship to the training data is unknown. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent non-linear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distance measure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.},
	language = {en},
	urldate = {2021-02-08},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Volume} 2 ({CVPR}'06)},
	publisher = {IEEE},
	author = {Hadsell, R. and Chopra, S. and LeCun, Y.},
	year = {2006},
	pages = {1735--1742},
	file = {Hadsell et al_2006_Dimensionality Reduction by Learning an Invariant Mapping.pdf:/Users/nicolas/Documents/Zotero/IEEE2006/Hadsell et al_2006_Dimensionality Reduction by Learning an Invariant Mapping.pdf:application/pdf},
}

@article{khosla_supervised_2020,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18661--18673},
  year={2020}
}


@misc{tian_hobbitlongsupcontrast_2021,
	title = {{HobbitLong}/{SupContrast}},
	copyright = {BSD-2-Clause License         ,                 BSD-2-Clause License},
	url = {https://github.com/HobbitLong/SupContrast},
	abstract = {PyTorch implementation of "Supervised Contrastive Learning"  (and SimCLR incidentally)},
	urldate = {2021-02-05},
	author = {Tian, Yonglong},
	month = feb,
	year = {2021},
	note = {original-date: 2020-05-08T09:58:30Z},
}

@misc{zhirongw_zhirongwlemniscatepytorch_2021,
	title = {zhirongw/lemniscate.pytorch},
	url = {https://github.com/zhirongw/lemniscate.pytorch},
	abstract = {Unsupervised Feature Learning via Non-parametric Instance Discrimination},
	urldate = {2021-02-05},
	author = {zhirongw},
	month = feb,
	year = {2021},
	note = {original-date: 2018-05-04T08:59:24Z},
	keywords = {computer-vision, cvpr2018, deep-learning, imagenet, nce, pytorch, representation-learning, self-supervised-learning, unsupervised-learning},
}

@article{henaff_data-efficient_2020,
	title = {Data-{Efficient} {Image} {Recognition} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1905.09272},
	abstract = {Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artiÔ¨Åcial ones remains an open challenge. We hypothesize that data-efÔ¨Åcient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-theart linear classiÔ¨Åcation accuracy on the ImageNet dataset. When used as input for non-linear classiÔ¨Åcation with deep neural networks, this representation allows us to use 2‚Äì5√ó less labels than classiÔ¨Åers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classiÔ¨Åers.},
	language = {en},
	urldate = {2021-02-05},
	journal = {arXiv:1905.09272 [cs]},
	author = {H√©naff, Olivier J. and Srinivas, Aravind and De Fauw, Jeffrey and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and Oord, Aaron van den},
	month = jul,
	year = {2020},
	note = {arXiv: 1905.09272},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {H√©naff et al_2020_Data-Efficient Image Recognition with Contrastive Predictive Coding.pdf:/Users/nicolas/Documents/Zotero/arXiv1905.09272 [cs]2020/H√©naff et al_2020_Data-Efficient Image Recognition with Contrastive Predictive Coding.pdf:application/pdf},
}

@article{mnih_learning_nodate,
	title = {Learning word embeddings efficiently with noise-contrastive estimation},
	abstract = {Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor.},
	language = {en},
	author = {Mnih, Andriy and Kavukcuoglu, Koray},
	pages = {9},
	file = {Mnih_Kavukcuoglu_Learning word embeddings efficiently with noise-contrastive estimation.pdf:/Users/nicolas/Documents/Zotero/_/Mnih_Kavukcuoglu_Learning word embeddings efficiently with noise-contrastive estimation.pdf:application/pdf},
}

@article{frosst_analyzing_2019,
	title = {Analyzing and {Improving} {Representations} with the {Soft} {Nearest} {Neighbor} {Loss}},
	url = {http://arxiv.org/abs/1902.01889},
	abstract = {We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we Ô¨Ånd that maximizing the entanglement of representations of different classes in the hidden layers is beneÔ¨Åcial for discrimination in the Ô¨Ånal layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.},
	language = {en},
	urldate = {2021-02-05},
	journal = {arXiv:1902.01889 [cs, stat]},
	author = {Frosst, Nicholas and Papernot, Nicolas and Hinton, Geoffrey},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.01889},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Frosst et al_2019_Analyzing and Improving Representations with the Soft Nearest Neighbor Loss.pdf:/Users/nicolas/Documents/Zotero/arXiv1902.01889 [cs, stat]2019/Frosst et al_2019_Analyzing and Improving Representations with the Soft Nearest Neighbor Loss.pdf:application/pdf},
}

@article{monnier_deep_2020,
	title = {Deep {Transformation}-{Invariant} {Clustering}},
	url = {http://arxiv.org/abs/2006.11132},
	abstract = {Recent advances in image clustering typically focus on learning better deep representations. In contrast, we present an orthogonal approach that does not rely on abstract features but instead learns to predict transformations and performs clustering directly in pixel space. This learning process naturally Ô¨Åts in the gradient-based training of K-means and Gaussian mixture model, without requiring any additional loss or hyper-parameters. It leads us to two new deep transformation-invariant clustering frameworks, which jointly learn prototypes and transformations. More speciÔ¨Åcally, we use deep learning modules that enable us to resolve invariance to spatial, color and morphological transformations. Our approach is conceptually simple and comes with several advantages, including the possibility to easily adapt the desired invariance to the task and a strong interpretability of both cluster centers and assignments to clusters. We demonstrate that our novel approach yields competitive and highly promising results on standard image clustering benchmarks. Finally, we showcase its robustness and the advantages of its improved interpretability by visualizing clustering results over real photograph collections.},
	language = {en},
	urldate = {2021-02-04},
	journal = {arXiv:2006.11132 [cs, stat]},
	author = {Monnier, Tom and Groueix, Thibault and Aubry, Mathieu},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.11132},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Monnier et al_2020_Deep Transformation-Invariant Clustering.pdf:/Users/nicolas/Documents/Zotero/arXiv2006.11132 [cs, stat]2020/Monnier et al_2020_Deep Transformation-Invariant Clustering.pdf:application/pdf},
}

@article{zhong_deep_2020,
	title = {Deep {Robust} {Clustering} by {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2008.03030},
	abstract = {Recently, many unsupervised deep learning methods have been proposed to learn clustering with unlabelled data. By introducing data augmentation, most of the latest methods look into deep clustering from the perspective that the original image and its transformation should share similar semantic clustering assignment. However, the representation features could be quite different even they are assigned to the same cluster since softmax function is only sensitive to the maximum value. This may result in high intra-class diversities in the representation feature space, which will lead to unstable local optimal and thus harm the clustering performance. To address this drawback, we proposed Deep Robust Clustering (DRC). Different from existing methods, DRC looks into deep clustering from two perspectives of both semantic clustering assignment and representation feature, which can increase inter-class diversities and decrease intra-class diversities simultaneously. Furthermore, we summarized a general framework that can turn any maximizing mutual information into minimizing contrastive loss by investigating the internal relationship between mutual information and contrastive learning. And we successfully applied it in DRC to learn invariant features and robust clusters. Extensive experiments on six widely-adopted deep clustering benchmarks demonstrate the superiority of DRC in both stability and accuracy. e.g., attaining 71.6\% mean accuracy on CIFAR-10, which is 7.1\% higher than state-of-the-art results.},
	language = {en},
	urldate = {2021-02-04},
	journal = {arXiv:2008.03030 [cs]},
	author = {Zhong, Huasong and Chen, Chong and Jin, Zhongming and Hua, Xian-Sheng},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.03030},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhong et al_2020_Deep Robust Clustering by Contrastive Learning.pdf:/Users/nicolas/Documents/Zotero/arXiv2008.03030 [cs]2020/Zhong et al_2020_Deep Robust Clustering by Contrastive Learning.pdf:application/pdf},
}

@article{bucher_hard_2016,
	title = {Hard {Negative} {Mining} for {Metric} {Learning} {Based} {Zero}-{Shot} {Classification}},
	url = {http://arxiv.org/abs/1608.07441},
	abstract = {Zero-Shot learning has been shown to be an eÔ¨Écient strategy for domain adaptation. In this context, this paper builds on the recent work of Bucher et al. [1], which proposed an approach to solve ZeroShot classiÔ¨Åcation problems (ZSC) by introducing a novel metric learning based objective function. This objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and attributes. This paper extends their approach by proposing several schemes to control the generation of the negative pairs, resulting in a signiÔ¨Åcant improvement of the performance and giving above state-of-the-art results on three challenging ZSC datasets.},
	language = {en},
	urldate = {2021-02-04},
	journal = {arXiv:1608.07441 [cs, stat]},
	author = {Bucher, Maxime and Herbin, St√©phane and Jurie, Fr√©d√©ric},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.07441},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Bucher et al_2016_Hard Negative Mining for Metric Learning Based Zero-Shot Classification.pdf:/Users/nicolas/Documents/Zotero/arXiv1608.07441 [cs, stat]2016/Bucher et al_2016_Hard Negative Mining for Metric Learning Based Zero-Shot Classification.pdf:application/pdf},
}

@inproceedings{he_momentum_2020,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition},
  pages={9729--9738},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}


@article{chen_imclr_2020,
	title = {{ImCLR}: {Implicit} {Contrastive} {Learning} for {Image} {Classification}},
	shorttitle = {{ImCLR}},
	url = {http://arxiv.org/abs/2011.12618},
	abstract = {Contrastive learning is an effective method for learning visual representations. In most cases, this involves adding an explicit loss function to encourage similar images to have similar representations, and different images to have different representations. Inspired by contrastive learning, we introduce a clever input construction for Implicit Contrastive Learning (ImCLR), primarily in the supervised setting: there, the network can implicitly learn to differentiate between similar and dissimilar images. Each input is presented as a concatenation of two images, and the label is the mean of the two one-hot labels. Furthermore, this requires almost no change to existing pipelines, which allows for easy integration and for fair demonstration of effectiveness on a wide range of well-accepted benchmarks. Namely, there is no change to loss, no change to hyperparameters, and no change to general network architecture. We show that ImCLR improves the test error in the supervised setting across a variety of settings, including 3.24\% on Tiny ImageNet, 1.30\% on CIFAR-100, 0.14\% on CIFAR-10, and 2.28\% on STL-10. We show that this holds across different number of labeled samples, maintaining approximately a 2\% gap in test accuracy down to using only 5\% of the whole dataset. We further show that gains hold for robustness to common input corruptions and perturbations at varying severities with a 0.72\% improvement on CIFAR-100-C, and in the semi-supervised setting with a 2.16\% improvement with the standard benchmark Œ†-model. We demonstrate that ImCLR is complementary to existing data augmentation techniques, achieving over 1\% improvement on CIFAR-100 and 2\% improvement on Tiny ImageNet by combining ImCLR with CutMix over either baseline, and 2\% by combining ImCLR with AutoAugment over either baseline.},
	language = {en},
	urldate = {2021-02-03},
	journal = {arXiv:2011.12618 [cs]},
	author = {Chen, John and Sinha, Samarth and Kyrillidis, Anastasios},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.12618},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al_2020_ImCLR.pdf:/Users/nicolas/Documents/Zotero/arXiv2011.12618 [cs]2020/Chen et al_2020_ImCLR.pdf:application/pdf},
}

@inproceedings{gutmann_noise-contrastive_2010,
  title={Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={297--304},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{trivedi2022revisiting,
  title={Revisiting lp-constrained Softmax Loss: A Comprehensive Study},
  author={Trivedi, Chintan and Makantasis, Konstantinos and Liapis, Antonios and Yannakakis, Georgios N},
  journal={arXiv preprint arXiv:2206.09616},
  year={2022}
}

@article{sohn_improved_2016,
	title = {Improved {Deep} {Metric} {Learning} with {Multi}-class {N}-pair {Loss} {Objective}},
	abstract = {Deep metric learning has gained much popularity in recent years, following the success of deep learning. However, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update. In this paper, we propose to address this problem with a new metric learning objective called multi-class N -pair loss. The proposed objective function Ô¨Årstly generalizes triplet loss by allowing joint comparison among more than one negative examples ‚Äì more speciÔ¨Åcally, N -1 negative examples ‚Äì and secondly reduces the computational burden of evaluating deep embedding vectors via an efÔ¨Åcient batch construction strategy using only N pairs of examples, instead of (N +1)√óN . We demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including Ô¨Åne-grained object recognition and veriÔ¨Åcation, image clustering and retrieval, and face veriÔ¨Åcation and identiÔ¨Åcation.},
	language = {en},
	author = {Sohn, Kihyuk},
	year = {2016},
	pages = {9},
	file = {Sohn_2016_Improved Deep Metric Learning with Multi-class N-pair Loss Objective.pdf:/Users/nicolas/Documents/Zotero/2016/Sohn_2016_Improved Deep Metric Learning with Multi-class N-pair Loss Objective.pdf:application/pdf},
}
@inproceedings{legate2023re,
  title={Re-weighted softmax cross-entropy to control forgetting in federated learning},
  author={Legate, Gwen and Caccia, Lucas and Belilovsky, Eugene},
  booktitle={Conference on Lifelong Learning Agents},
  pages={764--780},
  year={2023},
  organization={PMLR}
}
@article{ren2020balanced,
  title={Balanced meta-softmax for long-tailed visual recognition},
  author={Ren, Jiawei and Yu, Cunjun and Ma, Xiao and Zhao, Haiyu and Yi, Shuai and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={4175--4186},
  year={2020}
}
@article{beck2024xlstm,
  title={xLSTM: Extended Long Short-Term Memory},
  author={Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.04517},
  year={2024}
}
@inproceedings{wang2023yolov7,
  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023}
}

@article{tian_contrastive_2019,
  title={Contrastive representation distillation},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  journal={arXiv preprint arXiv:1910.10699},
  year={2019}
}
@inproceedings{jodelet2021balanced,
  title={Balanced softmax cross-entropy for incremental learning},
  author={Jodelet, Quentin and Liu, Xin and Murata, Tsuyoshi},
  booktitle={International Conference on Artificial Neural Networks},
  pages={385--396},
  year={2021},
  organization={Springer}
}
@article{yeung2001details,
  title={Details of the adjusted rand index and clustering algorithms, supplement to the paper an empirical study on principal component analysis for clustering gene expression data},
  author={Yeung, Ka Yee and Ruzzo, Walter L},
  journal={Bioinformatics},
  volume={17},
  number={9},
  pages={763--774},
  year={2001},
  publisher={Citeseer}
}

@InProceedings{Simonyan15,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}


@article{ru2024maintaining,
  title={Maintaining Adversarial Robustness in Continuous Learning},
  author={Ru, Xiaolei and Cao, Xiaowei and Liu, Zijia and Moore, Jack Murdoch and Zhang, Xin-Ya and Zhu, Xia and Wei, Wenjia and Yan, Gang},
  journal={arXiv preprint arXiv:2402.11196},
  year={2024}
}

@article{li2019large,
  title={Large-margin regularized softmax cross-entropy loss},
  author={Li, Xiaoxu and Chang, Dongliang and Tian, Tao and Cao, Jie},
  journal={IEEE access},
  volume={7},
  pages={19572--19578},
  year={2019},
  publisher={IEEE}
}

@article{menezes2023continual,
  title={Continual object detection: a review of definitions, strategies, and challenges},
  author={Menezes, Angelo G and de Moura, Gustavo and Alves, C{\'e}zanne and de Carvalho, Andr{\'e} CPLF},
  journal={Neural networks},
  volume={161},
  pages={476--493},
  year={2023},
  publisher={Elsevier}
}

@article{huang2021continual,
  title={Continual learning for text classification with information disentanglement based regularization},
  author={Huang, Yufan and Zhang, Yanzhe and Chen, Jiaao and Wang, Xuezhi and Yang, Diyi},
  journal={arXiv preprint arXiv:2104.05489},
  year={2021}
}

@inproceedings{zheng2021continual,
  title={A continual learning framework for uncertainty-aware interactive image segmentation},
  author={Zheng, Ervine and Yu, Qi and Li, Rui and Shi, Pengcheng and Haake, Anne},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={7},
  pages={6030--6038},
  year={2021}
}

 @article{zhou2017places,
   title={Places: A 10 million Image Database for Scene Recognition},
   author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
   year={2017},
   publisher={IEEE}
 }

@article{bertasius_cobe_2020,
	title = {{COBE}: {Contextualized} {Object} {Embeddings} from {Narrated} {Instructional} {Video}},
	shorttitle = {{COBE}},
	url = {http://arxiv.org/abs/2007.07306},
	abstract = {Many objects in the real world undergo dramatic variations in visual appearance. For example, a tomato may be red or green, sliced or chopped, fresh or fried, liquid or solid. Training a single detector to accurately recognize tomatoes in all these different states is challenging. On the other hand, contextual cues (e.g., the presence of a knife, a cutting board, a strainer or a pan) are often strongly indicative of how the object appears in the scene. Recognizing such contextual cues is useful not only to improve the accuracy of object detection or to determine the state of the object, but also to understand its functional properties and to infer ongoing or upcoming human-object interactions. A fully-supervised approach to recognizing object states and their contexts in the real-world is unfortunately marred by the long-tailed, open-ended distribution of the data, which would effectively require massive amounts of annotations to capture the appearance of objects in all their different forms. Instead of relying on manually-labeled data for this task, we propose a new framework for learning Contextualized OBject Embeddings (COBE) from automatically-transcribed narrations of instructional videos. We leverage the semantic and compositional structure of language by training a visual detector to predict a contextualized word embedding of the object and its associated narration. This enables the learning of an object representation where concepts relate according to a semantic language metric. Our experiments show that our detector learns to predict a rich variety of contextual object information, and that it is highly effective in the settings of few-shot and zero-shot learning.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2007.07306 [cs]},
	author = {Bertasius, Gedas and Torresani, Lorenzo},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.07306},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Bertasius_Torresani_2020_COBE.pdf:/Users/nicolas/Documents/Zotero/arXiv2007.07306 [cs]2020/Bertasius_Torresani_2020_COBE.pdf:application/pdf},
}

@article{anand_contrastive_2020,
	title = {Contrastive {Self}-{Supervised} {Learning}},
	url = {http://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html},
	abstract = {Contrastive self-supervised learning techniques are a promising class of methods that build representations by learning to encode what makes two things similar or different.},
	language = {en},
	journal = {ankeshanand.com},
	author = {Anand, Ankesh},
	year = {2020},
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{chattopadhay2018grad,
  title={Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks},
  author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  booktitle={2018 IEEE winter conference on applications of computer vision (WACV)},
  pages={839--847},
  year={2018},
  organization={IEEE}
}


@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={11},
  year={2008}
}

@article{soviany2022curriculum,
  title={Curriculum learning: A survey},
  author={Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  journal={International Journal of Computer Vision},
  volume={130},
  number={6},
  pages={1526--1565},
  year={2022},
  publisher={Springer}
}

@inproceedings{yuan2020revisiting,
  title={Revisiting knowledge distillation via label smoothing regularization},
  author={Yuan, Li and Tay, Francis EH and Li, Guilin and Wang, Tao and Feng, Jiashi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2020}
}

@article{pereyra2017regularizing,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}

@article{chen_sampling_2017,
	title = {On {Sampling} {Strategies} for {Neural} {Network}-based {Collaborative} {Filtering}},
	url = {http://arxiv.org/abs/1706.07881},
	abstract = {Recent advances in neural networks have inspired people to design hybrid recommendation algorithms that can incorporate both (1) user-item interaction information and (2) content information including image, audio, and text. Despite their promising results, neural network-based recommendation algorithms pose extensive computational costs, making it challenging to scale and improve upon. In this paper, we propose a general neural network-based recommendation framework, which subsumes several existing stateof-the-art recommendation algorithms, and address the efficiency issue by investigating sampling strategies in the stochastic gradient descent training for the framework. We tackle this issue by first establishing a connection between the loss functions and the useritem interaction bipartite graph, where the loss function terms are defined on links while major computation burdens are located at nodes. We call this type of loss functions ‚Äúgraph-based‚Äù loss functions, for which varied mini-batch sampling strategies can have different computational costs. Based on the insight, three novel sampling strategies are proposed, which can significantly improve the training efficiency of the proposed framework (up to √ó30 times speedup in our experiments), as well as improving the recommendation performance. Theoretical analysis is also provided for both the computational cost and the convergence. We believe the study of sampling strategies have further implications on general graphbased loss functions, and would also enable more research under the neural network-based recommendation framework. ACM Reference format: Ting Chen, Yizhou Sun, Yue Shi, and Liangjie Hong. 2017. On Sampling Strategies for Neural Network-based Collaborative Filtering. In Proceedings of KDD ‚Äô17, Halifax, NS, Canada, August 13-17, 2017, 14 pages.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:1706.07881 [cs, stat]},
	author = {Chen, Ting and Sun, Yizhou and Shi, Yue and Hong, Liangjie},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.07881},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	file = {Chen et al_2017_On Sampling Strategies for Neural Network-based Collaborative Filtering.pdf:/Users/nicolas/Documents/Zotero/arXiv1706.07881 [cs, stat]2017/Chen et al_2017_On Sampling Strategies for Neural Network-based Collaborative Filtering.pdf:application/pdf},
}

@article{eddine2023weighted,
  title={Weighted Ensemble Models Are Strong Continual Learners},
  author={Eddine Marouf, Imad and Roy, Subhankar and Tartaglione, Enzo and Lathuili{\`e}re, St{\'e}phane},
  journal={arXiv e-prints},
  pages={arXiv--2312},
  year={2023}
}

@inproceedings{covington_deep_2016,
	address = {Boston Massachusetts USA},
	title = {Deep {Neural} {Networks} for {YouTube} {Recommendations}},
	isbn = {978-1-4503-4035-9},
	url = {https://dl.acm.org/doi/10.1145/2959100.2959190},
	doi = {10.1145/2959100.2959190},
	abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: Ô¨Årst, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.},
	language = {en},
	urldate = {2021-01-26},
	booktitle = {Proceedings of the 10th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Covington, Paul and Adams, Jay and Sargin, Emre},
	month = sep,
	year = {2016},
	pages = {191--198},
	file = {Covington et al_2016_Deep Neural Networks for YouTube Recommendations.pdf:/Users/nicolas/Documents/Zotero/ACM2016/Covington et al_2016_Deep Neural Networks for YouTube Recommendations.pdf:application/pdf},
}

@inproceedings{wu_unsupervised_2018,
  title={Unsupervised feature learning via non-parametric instance discrimination},
  author={Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X and Lin, Dahua},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3733--3742},
  year={2018}
}


@article{zhou_generating_2020,
	title = {Generating {Adjacency} {Matrix} for {Video}-{Query} based {Video} {Moment} {Retrieval}},
	url = {http://arxiv.org/abs/2008.08977},
	abstract = {In this paper, we continue our work on Video-Query based Video Moment retrieval task. Based on using graph convolution to extract intra-video and inter-video frame features, we improve the method by using similarity-metric based graph convolution, whose weighted adjacency matrix is achieved by calculating similarity metric between features of any two different timesteps in the graph. Experiments on ActivityNet v1.2 and Thumos14 dataset shows the effectiveness of this improvement, and it outperforms the state-of-the-art methods.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:2008.08977 [cs, eess]},
	author = {Zhou, Yuan and Wang, Mingfei and Wang, Ruolin and Huo, Shuwei},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.08977},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Zhou et al_2020_Generating Adjacency Matrix for Video-Query based Video Moment Retrieval.pdf:/Users/nicolas/Documents/Zotero/arXiv2008.08977 [cs, eess]2020/Zhou et al_2020_Generating Adjacency Matrix for Video-Query based Video Moment Retrieval.pdf:application/pdf},
}

@article{zhou_graph_2020,
	title = {Graph {Neural} {Network} for {Video}-{Query} based {Video} {Moment} {Retrieval}},
	url = {http://arxiv.org/abs/2007.09877},
	abstract = {In this paper, we focus on Video Query based Video Moment Retrieval (VQ-VMR) task, which uses a query video clip as input to retrieve a semantic relative video clip in another untrimmed long video. we find that in VQ-VMR datasets, there exists a phenomenon showing that there does not exist consistent relationship between feature similarity by frame and feature similarity by video, which affects the feature fusion among frames. However, existing VQ-VMR methods do not fully consider it. Taking this phenomenon into account, in this article, we treat video features as a graph by concatenating the query video feature and proposal video feature along time dimension, where each timestep is treated as a node, each row of the feature matrix is treated as feature of each node. Then, with the power of graph neural networks, we propose a Multi-Graph Feature Fusion Module to fuse the relation feature of this graph. After evaluating our method on ActivityNet v1.2 dataset and Thumos14 dataset, we find that our proposed method outperforms the state of art methods.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:2007.09877 [cs, eess]},
	author = {Zhou, Yuan and Wang, Mingfei and Wang, Ruolin and Huo, Shuwei},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.09877},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Zhou et al_2020_Graph Neural Network for Video-Query based Video Moment Retrieval.pdf:/Users/nicolas/Documents/Zotero/arXiv2007.09877 [cs, eess]2020/Zhou et al_2020_Graph Neural Network for Video-Query based Video Moment Retrieval.pdf:application/pdf},
}

@article{zhang_noise_nodate,
	title = {Noise {Learning} for {Weakly} {Supervised} {Segment} {ClassiÔ¨Åcation} in {Video}},
	abstract = {This paper describes our solution for the 3rd YouTube8M video understanding challenge. The challenge of this year is different from the previous challenge. Given a large scale video dataset with video-level labels and a small scale video dataset with segment-level labels, we are asked to recognize segments in videos this year. It can be regarded as a weakly supervised learning problem. To answer the challenge, we propose a solution consists of three different models, i.e., segment-level classiÔ¨Åer, self-attention mechanism, noise learning classiÔ¨Åer. Among them, the noise learning classiÔ¨Åer performs the best. By noise learning, it can reduce the noise of label and sample for training, and improve the performance. Moreover, we achieve the MAP of 0.78878 in the private leaderboard by model ensemble based on introduced models, ranking the 8th place on the challenge.},
	language = {en},
	author = {Zhang, Zhaoyu and Wu, Xiang and Dong, Jianfeng and He, Yuan and Xue, Hui and Mao, Feng},
	pages = {5},
	file = {Zhang et al_Noise Learning for Weakly Supervised Segment ClassiÔ¨Åcation in Video.pdf:/Users/nicolas/Documents/Zotero/_/Zhang et al_Noise Learning for Weakly Supervised Segment ClassiÔ¨Åcation in Video.pdf:application/pdf},
}

@article{le_hierarchical_2021,
	title = {Hierarchical {Conditional} {Relation} {Networks} for {Multimodal} {Video} {Question} {Answering}},
	url = {http://arxiv.org/abs/2010.10019},
	abstract = {Video QA challenges modelers in multiple fronts. Modeling video necessitates building not only spatio-temporal models for the dynamic visual channel but also multimodal structures for associated information channels such as subtitles or audio. Video QA adds at least two more layers of complexity - selecting relevant content for each channel in the context of the linguistic query, and composing spatio-temporal concepts and relations in response to the query. To address these requirements, we start with two insights: (a) content selection and relation construction can be jointly encapsulated into a conditional computational structure, and (b) video-length structures can be composed hierarchically. For (a) this paper introduces a general-reusable neural unit dubbed Conditional Relation Network (CRN) taking as input a set of tensorial objects and translating into a new set of objects that encode relations of the inputs. The generic design of CRN helps ease the common complex model building process of Video QA by simple block stacking with flexibility in accommodating input modalities and conditioning features across both different domains. As a result, we realize insight (b) by introducing Hierarchical Conditional Relation Networks (HCRN) for Video QA. The HCRN primarily aims at exploiting intrinsic properties of the visual content of a video and its accompanying channels in terms of compositionality, hierarchy, and near and far-term relation. HCRN is then applied for Video QA in two forms, short-form where answers are reasoned solely from the visual content, and long-form where associated information, such as subtitles, presented. Our rigorous evaluations show consistent improvements over SOTAs on well-studied benchmarks including large-scale real-world datasets such as TGIF-QA and TVQA, demonstrating the strong capabilities of our CRN unit and the HCRN for complex domains such as Video QA.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:2010.10019 [cs]},
	author = {Le, Thao Minh and Le, Vuong and Venkatesh, Svetha and Tran, Truyen},
	month = jan,
	year = {2021},
	note = {arXiv: 2010.10019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Le et al_2021_Hierarchical Conditional Relation Networks for Multimodal Video Question.pdf:/Users/nicolas/Documents/Zotero/arXiv2010.10019 [cs]2021/Le et al_2021_Hierarchical Conditional Relation Networks for Multimodal Video Question.pdf:application/pdf},
}

@article{mao_hierarchical_2019,
	title = {Hierarchical {Video} {Frame} {Sequence} {Representation} with {Deep} {Convolutional} {Graph} {Network}},
	volume = {11132},
	url = {http://arxiv.org/abs/1906.00377},
	doi = {10.1007/978-3-030-11018-5_24},
	abstract = {High accuracy video label prediction (classiÔ¨Åcation) models are attributed to large scale data. These data could be frame feature sequences extracted by a pre-trained convolutional-neural-network, which promote the eÔ¨Éciency for creating models. Unsupervised solutions such as feature average pooling, as a simple label-independent parameter-free based method, has limited ability to represent the video. While the supervised methods, like RNN, can greatly improve the recognition accuracy. However, the video length is usually long, and there are hierarchical relationships between frames across events in the video, the performance of RNN based models are decreased. In this paper, we proposes a novel video classiÔ¨Åcation method based on a deep convolutional graph neural network(DCGN). The proposed method utilize the characteristics of the hierarchical structure of the video, and performed multi-level feature extraction on the video frame sequence through the graph network, obtained a video representation reÔ¨Çecting the event semantics hierarchically. We test our model on YouTube-8M Large-Scale Video Understanding dataset, and the result outperforms RNN based benchmarks.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:1906.00377 [cs]},
	author = {Mao, Feng and Wu, Xiang and Xue, Hui and Zhang, Rong},
	year = {2019},
	note = {arXiv: 1906.00377},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {262--270},
	file = {Mao et al_2019_Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph.pdf:/Users/nicolas/Documents/Zotero/arXiv1906.00377 [cs]2019/Mao et al_2019_Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph.pdf:application/pdf},
}

@article{chen_negative_2020,
	title = {Negative sampling in semi-supervised learning},
	url = {http://arxiv.org/abs/1911.05166},
	abstract = {We introduce Negative Sampling in SemiSupervised Learning (NS3L), a simple, fast, easy to tune algorithm for semi-supervised learning (SSL). NS3L is motivated by the success of negative sampling/contrastive estimation. We demonstrate that adding the NS3L loss to state-of-theart SSL algorithms, such as the Virtual Adversarial Training (VAT), signiÔ¨Åcantly improves upon vanilla VAT and its variant, VAT with Entropy Minimization. By adding the NS3L loss to MixMatch, the current state-of-the-art approach on semi-supervised tasks, we observe signiÔ¨Åcant improvements over vanilla MixMatch. We conduct extensive experiments on the CIFAR10, CIFAR100, SVHN and STL10 benchmark datasets. Finally, we perform an ablation study for NS3L regarding its hyperparameter tuning.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:1911.05166 [cs, stat]},
	author = {Chen, John and Shah, Vatsal and Kyrillidis, Anastasios},
	month = jun,
	year = {2020},
	note = {arXiv: 1911.05166},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Chen et al_2020_Negative sampling in semi-supervised learning.pdf:/Users/nicolas/Documents/Zotero/arXiv1911.05166 [cs, stat]2020/Chen et al_2020_Negative sampling in semi-supervised learning.pdf:application/pdf},
}

@misc{liu_automix_2022,
	title = {{AutoMix}: {Unveiling} the {Power} of {Mixup} for {Stronger} {Classifiers}},
	shorttitle = {{AutoMix}},
	url = {http://arxiv.org/abs/2103.13027},
	abstract = {Data mixing augmentation have proved to be effective in improving the generalization ability of deep neural networks. While early methods mix samples by hand-crafted policies (e.g., linear interpolation), recent methods utilize saliency information to match the mixed samples and labels via complex offline optimization. However, there arises a trade-off between precise mixing policies and optimization complexity. To address this challenge, we propose a novel automatic mixup (AutoMix) framework, where the mixup policy is parameterized and serves the ultimate classification goal directly. Specifically, AutoMix reformulates the mixup classification into two sub-tasks (i.e., mixed sample generation and mixup classification) with corresponding sub-networks and solves them in a bi-level optimization framework. For the generation, a learnable lightweight mixup generator, Mix Block, is designed to generate mixed samples by modeling patch-wise relationships under the direct supervision of the corresponding mixed labels. To prevent the degradation and instability of bi-level optimization, we further introduce a momentum pipeline to train AutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks prove the superiority of AutoMix compared with state-of-the-art in various classification scenarios and downstream tasks.},
	urldate = {2022-10-28},
	publisher = {arXiv},
	author = {Liu, Zicheng and Li, Siyuan and Wu, Di and Liu, Zihan and Chen, Zhiyuan and Wu, Lirong and Li, Stan Z.},
	month = sep,
	year = {2022},
	note = {arXiv:2103.13027 [cs]
version: 6},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/JKAFVXIZ/2103.html:text/html;Liu et al. - 2022 - AutoMix Unveiling the Power of Mixup for Stronger.pdf:/Users/nicolas/Documents/Zotero/arXiv2022/Liu et al. - 2022 - AutoMix Unveiling the Power of Mixup for Stronger.pdf:application/pdf},
}

@inproceedings{variani_gaussian_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {A {Gaussian} {Mixture} {Model} layer jointly optimized with discriminative features within a {Deep} {Neural} {Network} architecture},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178776/},
	doi = {10.1109/ICASSP.2015.7178776},
	abstract = {This article proposes and evaluates a Gaussian Mixture Model (GMM) represented as the last layer of a Deep Neural Network (DNN) architecture and jointly optimized with all previous layers using Asynchronous Stochastic Gradient Descent (ASGD). The resulting ‚ÄúDeep GMM‚Äù architecture was investigated with special attention to the following issues: (1) The extent to which joint optimization improves over separate optimization of the DNN-based feature extraction layers and the GMM layer; (2) The extent to which depth (measured in number of layers, for a matched total number of parameters) helps a deep generative model based on the GMM layer, compared to a vanilla DNN model; (3) Head-to-head performance of Deep GMM architectures vs. equivalent DNN architectures of comparable depth, using the same optimization criterion (frame-level Cross Entropy (CE)) and optimization method (ASGD); (4) Expanded possibilities for modeling offered by the Deep GMM generative model. The proposed Deep GMMs were found to yield Word Error Rates (WERs) competitive with state-of-the-art DNN systems, at the cost of pre-training using standard DNNs to initialize the Deep GMM feature extraction layers. An extension to Deep Subspace GMMs is described, resulting in additional gains.},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Variani, Ehsan and McDermott, Erik and Heigold, Georg},
	month = apr,
	year = {2015},
	pages = {4270--4274},
	file = {Variani et al. - 2015 - A Gaussian Mixture Model layer jointly optimized w.pdf:/Users/nicolas/Zotero/storage/6JK9KBES/Variani et al. - 2015 - A Gaussian Mixture Model layer jointly optimized w.pdf:application/pdf},
}

@misc{inoue_cross-domain_2018,
	title = {Cross-{Domain} {Weakly}-{Supervised} {Object} {Detection} through {Progressive} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1803.11365},
	doi = {10.48550/arXiv.1803.11365},
	abstract = {Can we detect common objects in a variety of image domains without instance-level annotations? In this paper, we present a framework for a novel task, cross-domain weakly supervised object detection, which addresses this question. For this paper, we have access to images with instance-level annotations in a source domain (e.g., natural image) and images with image-level annotations in a target domain (e.g., watercolor). In addition, the classes to be detected in the target domain are all or a subset of those in the source domain. Starting from a fully supervised object detector, which is pre-trained on the source domain, we propose a two-step progressive domain adaptation technique by fine-tuning the detector on two types of artificially and automatically generated samples. We test our methods on our newly collected datasets containing three image domains, and achieve an improvement of approximately 5 to 20 percentage points in terms of mean average precision (mAP) compared to the best-performing baselines.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Inoue, Naoto and Furuta, Ryosuke and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
	month = mar,
	year = {2018},
	note = {arXiv:1803.11365 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/SUBPDNAA/1803.html:text/html;Inoue et al. - 2018 - Cross-Domain Weakly-Supervised Object Detection th.pdf:/Users/nicolas/Documents/Zotero/arXiv2018/Inoue et al. - 2018 - Cross-Domain Weakly-Supervised Object Detection th.pdf:application/pdf},
}

@article{tao_improved_2022,
	title = {An {Improved} {Inter}-{Intra} {Contrastive} {Learning} {Framework} on {Self}-{Supervised} {Video} {Representation}},
	volume = {32},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2022.3141051},
	abstract = {In this paper, we propose a self-supervised contrastive learning method to learn video feature representations. In traditional self-supervised contrastive learning methods, constraints from anchor, positive, and negative data pairs are used to train the model. In such a case, different samplings of the same video are treated as positives, and video clips from different videos are treated as negatives. Because the spatio-temporal information is important for video representation, we set the temporal constraints more strictly by introducing intra-negative samples. In addition to samples from different videos, negative samples are extended by breaking temporal relations in video clips from the same anchor video. With the proposed Inter-Intra Contrastive (IIC) framework, we can train spatio-temporal convolutional networks to learn feature representations from videos. Strong data augmentations, residual clips, as well as head projector are utilized to construct an improved version. Three kinds of intra-negative generation functions are proposed and extensive experiments using different network backbones are conducted on benchmark datasets. Without using pre-computed optical flow data, our improved version can outperform previous IIC by a large margin, such as 19.4\% (from 36.8\% to 56.2\%) and 5.2\% (from 15.5\% to 20.7\%) points improvements in top-1 accuracy on UCF101 and HMDB51 datasets for video retrieval, respectively. For video recognition, over 3\% points improvements can also be obtained on these two benchmark datasets. Discussions and visualizations validate that our IICv2 can capture better temporal clues and indicate the potential mechanism.},
	number = {8},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Tao, Li and Wang, Xueting and Yamasaki, Toshihiko},
	month = aug,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Data models, Feature extraction, Learning systems, Optical imaging, Optical sensors, Representation learning, Self-supervised learning, spatio-temporal convolution, Task analysis, video recognition, video representation, video retrieval},
	pages = {5266--5280},
	file = {IEEE Xplore Abstract Record:/Users/nicolas/Zotero/storage/CDKTBBLM/9674754.html:text/html;Tao et al. - 2022 - An Improved Inter-Intra Contrastive Learning Frame.pdf:/Users/nicolas/Documents/Zotero/IEEE Transactions on Circuits and Systems for Video Technology2022/Tao et al. - 2022 - An Improved Inter-Intra Contrastive Learning Frame.pdf:application/pdf},
}

@article{wightman2021resnet,
  title={Resnet strikes back: An improved training procedure in timm},
  author={Wightman, Ross and Touvron, Hugo and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2110.00476},
  year={2021}
}
@article{bai2021transformers,
  title={Are transformers more robust than cnns?},
  author={Bai, Yutong and Mei, Jieru and Yuille, Alan L and Xie, Cihang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={26831--26843},
  year={2021}
}


@article{picard2021torch,
  title={Torch. manual\_seed (3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision},
  author={Picard, David},
  journal={arXiv preprint arXiv:2109.08203},
  year={2021}
}

@inproceedings{yu2023scale,
  title={Scale: Online self-supervised lifelong learning without prior knowledge},
  author={Yu, Xiaofan and Guo, Yunhui and Gao, Sicun and Rosing, Tajana},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  year={2023}
}

@article{wang_continual_2022,
	title = {Continual {Learning} through {Retrieval} and {Imagination}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20837},
	doi = {10.1609/aaai.v36i8.20837},
	abstract = {Continual learning is an intellectual ability of artificial agents to learn new concepts from sequential data. The main impediment to continual learning is catastrophic forgetting, a severe performance degradation on previously learned tasks. Although simply replaying all previous data or continuously adding the model parameters could alleviate the issue, it is impractical in real-world applications due to the limited available resources. Inspired by the mechanism of the human brain to deepen its past impression, we propose a novel framework, Deep Retrieval and Imagination (DRI), which consists of two components: 1) an embedding network that constructs a unified embedding space without adding model parameters on the arrival of new tasks; and 2) a generative model to produce additional (imaginary) data based on the limited memory. By retrieving the past experiences and corresponding imaginary data, DRI distills knowledge and rebalances the embedding space to further mitigate forgetting. Theoretical analysis demonstrates that DRI can reduce the loss approximation error and improve the robustness through retrieval and imagination, bringing better generalizability to the network. Extensive experiments show that DRI performs significantly better than the existing state-of-the-art continual learning methods and effectively alleviates catastrophic forgetting.},
	language = {en},
	number = {8},
	urldate = {2022-07-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Zhen and Liu, Liu and Duan, Yiqun and Tao, Dacheng},
	month = jun,
	year = {2022},
	pages = {8594--8602},
	file = {Wang et al. - 2022 - Continual Learning through Retrieval and Imaginati.pdf:/Users/nicolas/Zotero/storage/PWPTATFL/Wang et al. - 2022 - Continual Learning through Retrieval and Imaginati.pdf:application/pdf},
}

@inproceedings{zhao2022decoupled,
  title={Decoupled knowledge distillation},
  author={Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11953--11962},
  year={2022}
}


@inproceedings{negrel_boosted_2015,
	address = {Swansea},
	title = {Boosted {Metric} {Learning} for {Efficient} {Identity}-{Based} {Face} {Retrieval}},
	isbn = {978-1-901725-53-7},
	url = {http://www.bmva.org/bmvc/2015/papers/paper139/index.html},
	doi = {10.5244/C.29.139},
	abstract = {This paper presents MLBoost, an efÔ¨Åcient method for learning to compare face signatures, and shows its application to the hierarchical organization of large face databases. More precisely, the proposed metric learning (ML) algorithm is based on boosting so that the metric is learned iteratively by combining several weak metrics. Boosting allows our method to be free of any hyper-parameters (no cross-validation required) and to be robust with respect to overÔ¨Åtting. This MLBoost algorithm can be trained from constraints involving two pairs of vectors (quadruplets) with a quadratic complexity. The paper also shows how it can be included in a semi-supervised hierarchical clustering framework adapted to identity based face search. Our approach is validated on a benchmark relying on the Labelled Faces in the Wild (LFW) dataset supplemented with 1M face distractors.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2015},
	publisher = {British Machine Vision Association},
	author = {Negrel, Romain and Lechervy, Alexis and Jurie, Frederic},
	year = {2015},
	keywords = {boosting},
	pages = {139.1--139.12},
	file = {Negrel et al. - 2015 - Boosted Metric Learning for Efficient Identity-Bas.pdf:/Users/nicolas/Zotero/storage/EHHP3J8H/Negrel et al. - 2015 - Boosted Metric Learning for Efficient Identity-Bas.pdf:application/pdf},
}

@techreport{zhang_feature_2022,
	title = {Feature {Forgetting} in {Continual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2205.13359},
	abstract = {In continual and lifelong learning, good representation learning can help increase performance and reduce sample complexity when learning new tasks. There is evidence that representations do not suffer from "catastrophic forgetting" even in plain continual learning, but little further fact is known about its characteristics. In this paper, we aim to gain more understanding about representation learning in continual learning, especially on the feature forgetting problem. We devise a protocol for evaluating representation in continual learning, and then use it to present an overview of the basic trends of continual representation learning, showing its consistent deficiency and potential issues. To study the feature forgetting problem, we create a synthetic dataset to identify and visualize the prevalence of feature forgetting in neural networks. Finally, we propose a simple technique using gating adapters to mitigate feature forgetting. We conclude by discussing that improving representation learning benefits both old and new tasks in continual learning.},
	number = {arXiv:2205.13359},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Zhang, Xiao and Dou, Dejing and Wu, Ji},
	month = may,
	year = {2022},
	note = {arXiv:2205.13359 [cs]
type: article},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/SZXHPICT/2205.html:text/html;Zhang et al. - 2022 - Feature Forgetting in Continual Representation Lea.pdf:/Users/nicolas/Documents/Zotero/arXiv2022/Zhang et al. - 2022 - Feature Forgetting in Continual Representation Lea.pdf:application/pdf},
}

@article{ho_prototypes-guided_2021,
  title={Prototype-guided memory replay for continual learning},
  author={Ho, Stella and Liu, Ming and Du, Lan and Gao, Longxiang and Xiang, Yong},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}


@inproceedings{davari_probing_2022,
  title={Probing representation forgetting in supervised and unsupervised continual learning},
  author={Davari, MohammadReza and Asadi, Nader and Mudur, Sudhir and Aljundi, Rahaf and Belilovsky, Eugene},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16712--16721},
  year={2022}
}


@article{pham_continual_2022,
	title = {Continual {Normalization}: {Rethinking} {Batch} {Normalization} for {Online} {Continual} {Learning}},
	shorttitle = {Continual {Normalization}},
	url = {http://arxiv.org/abs/2203.16102},
	abstract = {Existing continual learning methods use Batch Normalization (BN) to facilitate training and improve generalization across tasks. However, the non-i.i.d and non-stationary nature of continual learning data, especially in the online setting, amplify the discrepancy between training and testing in BN and hinder the performance of older tasks. In this work, we study the cross-task normalization effect of BN in online continual learning where BN normalizes the testing data using moments biased towards the current task, resulting in higher catastrophic forgetting. This limitation motivates us to propose a simple yet effective method that we call Continual Normalization (CN) to facilitate training similar to BN while mitigating its negative effect. Extensive experiments on different continual learning algorithms and online scenarios show that CN is a direct replacement for BN and can provide substantial performance improvements. Our implementation is available at {\textbackslash}url\{https://github.com/phquang/Continual-Normalization\}.},
	urldate = {2022-04-04},
	journal = {arXiv:2203.16102 [cs]},
	author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.16102},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/93PJYMNY/2203.html:text/html;Pham et al_2022_Continual Normalization.pdf:/Users/nicolas/Documents/Zotero/arXiv2203.16102 [cs]2022/Pham et al_2022_Continual Normalization.pdf:application/pdf},
}

@inproceedings{gopalakrishnan_knowledge_2022,
	address = {Waikoloa, HI, USA},
	title = {Knowledge {Capture} and {Replay} for {Continual} {Learning}},
	isbn = {978-1-66540-915-5},
	url = {https://ieeexplore.ieee.org/document/9706856/},
	doi = {10.1109/WACV51458.2022.00041},
	abstract = {Deep neural networks model data for a task or a sequence of tasks, where the knowledge extracted from the data is encoded in the parameters and representations of the network. Extraction and utilization of these representations is vital when data is no longer available in the future, especially in a continual learning scenario. We introduce flashcards, which are visual representations that capture the encoded knowledge of a network as a recursive function of some predefined random image patterns. In a continual learning scenario, flashcards help to prevent catastrophic forgetting by consolidating the knowledge of all the previous tasks. Flashcards are required to be constructed only before learning the subsequent task, hence, they are independent of the number of tasks trained before, making them task agnostic. We demonstrate the efficacy of flashcards in capturing learned knowledge representation (as an alternative to the original data), and empirically validate on a variety of continual learning tasks: reconstruction, denoising, and task-incremental classification, using several heterogeneous (varying background and complexity) benchmark datasets. Experimental evidence indicates that: (i) flashcards as a replay strategy is task agnostic, (ii) performs better than generative replay, and (iii) is on par with episodic replay without additional memory overhead.},
	language = {en},
	urldate = {2022-03-15},
	booktitle = {2022 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Gopalakrishnan, Saisubramaniam and Singh, Pranshu Ranjan and Fayek, Haytham and Ramasamy, Savitha and Ambikapathi, ArulMurugan},
	month = jan,
	year = {2022},
	pages = {337--345},
	file = {Gopalakrishnan et al. - 2022 - Knowledge Capture and Replay for Continual Learnin.pdf:/Users/nicolas/Zotero/storage/9QDU9UK7/Gopalakrishnan et al. - 2022 - Knowledge Capture and Replay for Continual Learnin.pdf:application/pdf},
}

@article{thai_does_2021,
	title = {Does {Continual} {Learning} = {Catastrophic} {Forgetting}?},
	url = {http://arxiv.org/abs/2101.07295},
	abstract = {Continual learning is known for suffering from catastrophic forgetting, a phenomenon where earlier learned concepts are forgotten at the expense of more recent samples. In this work, we challenge the assumption that continual learning is inevitably associated with catastrophic forgetting by presenting a set of tasks that surprisingly do not suffer from catastrophic forgetting when learned continually. We provide evidence that these reconstruction-type tasks exhibit positive forward transfer and that single-view 3D shape reconstruction improves the performance on learned and novel categories over time. We provide the novel analysis of knowledge transfer ability by looking at the output distribution shift across sequential learning tasks. Finally, we show that the robustness of these tasks leads to the potential of having a proxy representation learning task for continual classification. The codebase, dataset, and pre-trained models released with this article can be found at https://github.com/rehg-lab/CLRec.},
	urldate = {2022-03-15},
	journal = {arXiv:2101.07295 [cs]},
	author = {Thai, Anh and Stojanov, Stefan and Huang, Zixuan and Rehg, Isaac and Rehg, James M.},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.07295
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/VL7MRNGQ/2101.html:text/html;Thai et al. - 2021 - Does Continual Learning = Catastrophic Forgetting.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.07295 [cs]2021/Thai et al. - 2021 - Does Continual Learning = Catastrophic Forgetting.pdf:application/pdf},
}

@article{wang_ordisco_2021,
	title = {{ORDisCo}: {Effective} and {Efficient} {Usage} of {Incremental} {Unlabeled} {Data} for {Semi}-supervised {Continual} {Learning}},
	shorttitle = {{ORDisCo}},
	url = {http://arxiv.org/abs/2101.00407},
	abstract = {Continual learning usually assumes the incoming data are fully labeled, which might not be applicable in real applications. In this work, we consider semi-supervised continual learning (SSCL) that incrementally learns from partially labeled data. Observing that existing continual learning methods lack the ability to continually exploit the unlabeled data, we propose deep Online Replay with Discriminator Consistency (ORDisCo) to interdependently learn a classifier with a conditional generative adversarial network (GAN), which continually passes the learned data distribution to the classifier. In particular, ORDisCo replays data sampled from the conditional generator to the classifier in an online manner, exploiting unlabeled data in a time- and storage-efficient way. Further, to explicitly overcome the catastrophic forgetting of unlabeled data, we selectively stabilize parameters of the discriminator that are important for discriminating the pairs of old unlabeled data and their pseudo-labels predicted by the classifier. We extensively evaluate ORDisCo on various semi-supervised learning benchmark datasets for SSCL, and show that ORDisCo achieves significant performance improvement on SVHN, CIFAR10 and Tiny-ImageNet, compared to strong baselines.},
	urldate = {2022-03-15},
	journal = {arXiv:2101.00407 [cs, stat]},
	author = {Wang, Liyuan and Yang, Kuo and Li, Chongxuan and Hong, Lanqing and Li, Zhenguo and Zhu, Jun},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.00407
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/4HVJE9JT/2101.html:text/html;Wang et al. - 2021 - ORDisCo Effective and Efficient Usage of Incremen.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.00407 [cs, stat]2021/Wang et al. - 2021 - ORDisCo Effective and Efficient Usage of Incremen.pdf:application/pdf},
}

@misc{noauthor_representation_nodate,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {https://deepmind.com/research/publications/2019/representation-learning-contrastive-predictive-coding},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding.},
	language = {ALL},
	urldate = {2022-02-16},
	journal = {Deepmind},
	file = {Snapshot:/Users/nicolas/Zotero/storage/2DRUC47X/representation-learning-contrastive-predictive-coding.html:text/html},
}

@inproceedings{singh_task-agnostic_2021,
	title = {Task-{Agnostic} {Continual} {Learning} {Using} {Base}-{Child} {Classifiers}},
	doi = {10.1109/ICIP42928.2021.9506504},
	abstract = {Continual learning (CL) aims to learn new tasks by forward transfer of information learnt from previous tasks and without forgetting them. In task incremental CL, task information is vital during both strategy development and inference. Providing such partial knowledge about the test sample demands additional complexity and may become intractable, especially when the sample source is ambiguous. In this work, we design a task-agnostic approach that uses base-child hybrid setup to incrementally learn tasks while mitigating forgetting. Multiple base classifiers guided by reference points learn new tasks and this information is distilled via feature space induced sampling strategy. A central child classifier consolidates information across tasks and infers the task identifier automatically. Experimental results on standard datasets show that the proposed approach outperforms the various state-of-the-art regularization and replay CL algorithms in terms of accuracy, by 50\% and 7\% with homogeneous and heterogeneous tasks, respectively, in task-agnostic scenarios.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Singh, Pranshu Ranjan and Gopalakrishnan, Saisubramaniam and ZhongZheng, Qiao and Suganthan, Ponnuthurai N. and Ramasamy, Savitha and Ambikapathi, ArulMurugan},
	month = sep,
	year = {2021},
	note = {ISSN: 2381-8549},
	keywords = {Task analysis, Classification algorithms, Complexity theory, Conferences, Continual Learning, Hybrid Networks, Image processing, Knowledge Distillation, Standards, Task-Incremental Classification},
	pages = {794--798},
	file = {IEEE Xplore Abstract Record:/Users/nicolas/Zotero/storage/TB7A2GZB/9506504.html:text/html;Singh et al. - 2021 - Task-Agnostic Continual Learning Using Base-Child .pdf:/Users/nicolas/Documents/Zotero/2021/Singh et al. - 2021 - Task-Agnostic Continual Learning Using Base-Child .pdf:application/pdf},
}

@article{brahma_hypernetworks_2021,
	title = {Hypernetworks for {Continual} {Semi}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2110.01856},
	abstract = {Learning from data sequentially arriving, possibly in a non i.i.d. way, with changing task distribution over time is called continual learning. Much of the work thus far in continual learning focuses on supervised learning and some recent works on unsupervised learning. In many domains, each task contains a mix of labelled (typically very few) and unlabelled (typically plenty) training examples, which necessitates a semi-supervised learning approach. To address this in a continual learning setting, we propose a framework for semi-supervised continual learning called Meta-Consolidation for Continual Semi-Supervised Learning (MCSSL). Our framework has a hypernetwork that learns the metadistribution that generates the weights of a semisupervised auxiliary classiÔ¨Åer generative adversarial network (Semi-ACGAN) as the base network. We consolidate the knowledge of sequential tasks in the hypernetwork, and the base network learns the semi-supervised learning task. Further, we present Semi-Split CIFAR-10, a new benchmark for continual semi-supervised learning, obtained by modifying the Split CIFAR-10 dataset, in which the tasks with labelled and unlabelled data arrive sequentially. Our proposed model yields signiÔ¨Åcant improvements in the continual semi-supervised learning setting. We compare the performance of several existing continual learning approaches on the proposed continual semi-supervised learning benchmark of the Semi-Split CIFAR-10 dataset.},
	language = {en},
	urldate = {2022-02-08},
	journal = {arXiv:2110.01856 [cs, stat]},
	author = {Brahma, Dhanajit and Verma, Vinay Kumar and Rai, Piyush},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.01856},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Brahma et al. - 2021 - Hypernetworks for Continual Semi-Supervised Learni.pdf:/Users/nicolas/Zotero/storage/GJ4QM8SN/Brahma et al. - 2021 - Hypernetworks for Continual Semi-Supervised Learni.pdf:application/pdf},
}

@incollection{mccloskey_catastrophic_1989,
	title = {Catastrophic {Interference} in {Connectionist} {Networks}: {The} {Sequential} {Learning} {Problem}},
	volume = {24},
	shorttitle = {Catastrophic {Interference} in {Connectionist} {Networks}},
	url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
	abstract = {Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
	language = {en},
	urldate = {2022-02-08},
	booktitle = {Psychology of {Learning} and {Motivation}},
	publisher = {Academic Press},
	author = {McCloskey, Michael and Cohen, Neal J.},
	editor = {Bower, Gordon H.},
	month = jan,
	year = {1989},
	doi = {10.1016/S0079-7421(08)60536-8},
	pages = {109--165},
	file = {ScienceDirect Snapshot:/Users/nicolas/Zotero/storage/GZFTS7NC/S0079742108605368.html:text/html},
}

@article{he_deep_2015,
 title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@article{boschini_continual_2021,
	title = {Continual {Semi}-{Supervised} {Learning} through {Contrastive} {Interpolation} {Consistency}},
	url = {http://arxiv.org/abs/2108.06552},
	abstract = {Continual Learning (CL) investigates how to train Deep Networks on a stream of tasks without incurring forgetting. CL settings proposed in literature assume that every incoming example is paired with ground-truth annotations. However, this clashes with many real-world applications: gathering labeled data, which is in itself tedious and expensive, becomes infeasible when data flow as a stream. This work explores Continual Semi-Supervised Learning (CSSL): here, only a small fraction of labeled input examples are shown to the learner. We assess how current CL methods (e.g.: EWC, LwF, iCaRL, ER, GDumb, DER) perform in this novel and challenging scenario, where overfitting entangles forgetting. Subsequently, we design a novel CSSL method that exploits metric learning and consistency regularization to leverage unlabeled examples while learning. We show that our proposal exhibits higher resilience to diminishing supervision and, even more surprisingly, relying only on 25\% supervision suffices to outperform SOTA methods trained under full supervision.},
	language = {en},
	urldate = {2022-01-31},
	journal = {arXiv:2108.06552 [cs, stat]},
	author = {Boschini, Matteo and Buzzega, Pietro and Bonicelli, Lorenzo and Porrello, Angelo and Calderara, Simone},
	month = dec,
	year = {2021},
	note = {arXiv: 2108.06552},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Boschini et al. - 2021 - Continual Semi-Supervised Learning through Contras.pdf:/Users/nicolas/Zotero/storage/VWZJ6XSU/Boschini et al. - 2021 - Continual Semi-Supervised Learning through Contras.pdf:application/pdf},
}

@article{chaudhry_using_2021,
	title = {Using {Hindsight} to {Anchor} {Past} {Knowledge} in {Continual} {Learning}},
	url = {http://arxiv.org/abs/2002.08165},
	abstract = {In continual learning, the learner faces a stream of data whose distribution changes over time. Modern neural networks are known to suffer under this setting, as they quickly forget previously acquired knowledge. To address such catastrophic forgetting, many continual learning methods implement different types of experience replay, re-learning on past data stored in a small buffer known as episodic memory. In this work, we complement experience replay with a new objective that we call ‚Äúanchoring‚Äù, where the learner uses bilevel optimization to update its knowledge on the current task, while keeping intact predictions on some anchor points of past tasks. These anchor points are learned using gradientbased optimization to maximize forgetting, which is approximated by Ô¨Åne-tuning the currently trained model on the episodic memory of past tasks. Experiments on several supervised learning benchmarks for continual learning demonstrate that our approach improves the standard experience replay in terms of both accuracy and forgetting metrics and for various sizes of episodic memory.},
	language = {en},
	urldate = {2022-01-31},
	journal = {arXiv:2002.08165 [cs, stat]},
	author = {Chaudhry, Arslan and Gordo, Albert and Dokania, Puneet K. and Torr, Philip and Lopez-Paz, David},
	month = mar,
	year = {2021},
	note = {arXiv: 2002.08165},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Chaudhry et al. - 2021 - Using Hindsight to Anchor Past Knowledge in Contin.pdf:/Users/nicolas/Zotero/storage/37MSUPCN/Chaudhry et al. - 2021 - Using Hindsight to Anchor Past Knowledge in Contin.pdf:application/pdf},
}

@article{shahbaz_international_2021,
	title = {International {Workshop} on {Continual} {Semi}-{Supervised} {Learning}: {Introduction}, {Benchmarks} and {Baselines}},
	shorttitle = {International {Workshop} on {Continual} {Semi}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2110.14613},
	abstract = {The aim of this paper is to formalise a new continual semi-supervised learning (CSSL) paradigm, proposed to the attention of the machine learning community via the IJCAI 2021 International Workshop on Continual Semi-Supervised Learning (CSSL@IJCAI)1, with the aim of raising the Ô¨Åeld‚Äôs awareness about this problem and mobilising its effort in this direction. After a formal deÔ¨Ånition of continual semi-supervised learning and the appropriate training and testing protocols, the paper introduces two new benchmarks speciÔ¨Åcally designed to assess CSSL on two important computer vision tasks: activity recognition and crowd counting. We describe the Continual Activity Recognition (CAR) and Continual Crowd Counting (CCC) challenges built upon those benchmarks, the baseline models proposed for the challenges, and describe a simple CSSL baseline which consists in applying batch self-training in temporal sessions, for a limited number of rounds. The results show that learning from unlabelled data streams is extremely challenging, and stimulate the search for methods that can encode the dynamics of the data stream.},
	language = {en},
	urldate = {2022-01-31},
	journal = {arXiv:2110.14613 [cs]},
	author = {Shahbaz, Ajmal and Khan, Salman and Hossain, Mohammad Asiful and Lomonaco, Vincenzo and Cannons, Kevin and Xu, Zhan and Cuzzolin, Fabio},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.14613},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Shahbaz et al. - 2021 - International Workshop on Continual Semi-Supervise.pdf:/Users/nicolas/Zotero/storage/NZFSVLBM/Shahbaz et al. - 2021 - International Workshop on Continual Semi-Supervise.pdf:application/pdf},
}

@article{lee_neural_2020,
	title = {A {Neural} {Dirichlet} {Process} {Mixture} {Model} for {Task}-{Free} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2001.00689},
	abstract = {Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.},
	urldate = {2022-01-18},
	journal = {arXiv:2001.00689 [cs, stat]},
	author = {Lee, Soochan and Ha, Junsoo and Zhang, Dongsu and Kim, Gunhee},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.00689},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Lee et al. - 2020 - A Neural Dirichlet Process Mixture Model for Task-.pdf:/Users/nicolas/Documents/Zotero/arXiv2001.00689 [cs, stat]2020/Lee et al. - 2020 - A Neural Dirichlet Process Mixture Model for Task-.pdf:application/pdf},
}

@article{vitter_random_1985,
  title={Random sampling with a reservoir},
  author={Vitter, Jeffrey S},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={11},
  number={1},
  pages={37--57},
  year={1985},
  publisher={ACM New York, NY, USA}
}

@article{qu_recent_2021,
	title = {Recent {Advances} of {Continual} {Learning} in {Computer} {Vision}: {An} {Overview}},
	shorttitle = {Recent {Advances} of {Continual} {Learning} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/2109.11369},
	abstract = {In contrast to batch learning where all training data is available at once, continual learning represents a family of methods that accumulate knowledge and learn continuously with data available in sequential order. Similar to the human learning process with the ability of learning, fusing, and accumulating new knowledge coming at different time steps, continual learning is considered to have high practical significance. Hence, continual learning has been studied in various artificial intelligence tasks. In this paper, we present a comprehensive review of the recent progress of continual learning in computer vision. In particular, the works are grouped by their representative techniques, including regularization, knowledge distillation, memory, generative replay, parameter isolation, and a combination of the above techniques. For each category of these techniques, both its characteristics and applications in computer vision are presented. At the end of this overview, several subareas, where continuous knowledge accumulation is potentially helpful while continual learning has not been well studied, are discussed.},
	urldate = {2022-01-10},
	journal = {arXiv:2109.11369 [cs]},
	author = {Qu, Haoxuan and Rahmani, Hossein and Xu, Li and Williams, Bryan and Liu, Jun},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.11369},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/JQI7G559/2109.html:text/html;Qu et al. - 2021 - Recent Advances of Continual Learning in Computer .pdf:/Users/nicolas/Documents/Zotero/arXiv2109.11369 [cs]2021/Qu et al. - 2021 - Recent Advances of Continual Learning in Computer 2.pdf:application/pdf},
}

@inproceedings{zhao_maintaining_2020,
	address = {Seattle, WA, USA},
	title = {Maintaining {Discrimination} and {Fairness} in {Class} {Incremental} {Learning}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156766/},
	doi = {10.1109/CVPR42600.2020.01322},
	abstract = {Deep neural networks (DNNs) have been applied in class incremental learning, which aims to solve common realworld problems of learning new classes continually. One drawback of standard DNNs is that they are prone to catastrophic forgetting. Knowledge distillation (KD) is a commonly used technique to alleviate this problem. In this paper, we demonstrate it can indeed help the model to output more discriminative results within old classes. However, it cannot alleviate the problem that the model tends to classify objects into new classes, causing the positive effect of KD to be hidden and limited. We observed that an important factor causing catastrophic forgetting is that the weights in the last fully connected (FC) layer are highly biased in class incremental learning. In this paper, we propose a simple and effective solution motivated by the aforementioned observations to address catastrophic forgetting. Firstly, we utilize KD to maintain the discrimination within old classes. Then, to further maintain the fairness between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal training process. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings. Experimental results show that the proposed method can effectively alleviate catastrophic forgetting and signiÔ¨Åcantly outperform state-of-the-art methods.},
	language = {en},
	urldate = {2022-01-04},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhao, Bowen and Xiao, Xi and Gan, Guojun and Zhang, Bin and Xia, Shu-Tao},
	month = jun,
	year = {2020},
	pages = {13205--13214},
	file = {Zhao et al. - 2020 - Maintaining Discrimination and Fairness in Class I.pdf:/Users/nicolas/Zotero/storage/XPF3XAQ8/Zhao et al. - 2020 - Maintaining Discrimination and Fairness in Class I.pdf:application/pdf},
}

@article{jaiswal_survey_2021,
  title={A survey on contrastive self-supervised learning},
  author={Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
  journal={Technologies},
  volume={9},
  number={1},
  pages={2},
  year={2020},
  publisher={MDPI}
}

@article{french1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in cognitive sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier}
}


@article{hinton_distilling_2015-1,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2021-12-13},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/FKQMPSLU/1503.html:text/html;Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:/Users/nicolas/Documents/Zotero/arXiv1503.02531 [cs, stat]2015/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf},
}

@inproceedings{rolnick_experience_2019,
	title = {Experience {Replay} for {Continual} {Learning}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html},
	urldate = {2021-12-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Gregory},
	year = {2019},
	file = {Rolnick et al. - 2019 - Experience Replay for Continual Learning.pdf:/Users/nicolas/Documents/Zotero/Curran Associates, Inc.2019/Rolnick et al. - 2019 - Experience Replay for Continual Learning.pdf:application/pdf},
}

@inproceedings{chaudhry_tiny_2019,
  title={Continual learning with tiny episodic memories},
  author={Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, P and Torr, P and Ranzato, M},
  booktitle={Workshop on Multi-Task and Lifelong Reinforcement Learning},
  year={2019}
}


@inproceedings{vedaldi_gdumb_2020,
  title={Gdumb: A simple approach that questions our progress in continual learning},
  author={Prabhu, Ameya and Torr, Philip HS and Dokania, Puneet K},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Proceedings, Part II 16},
  pages={524--540},
  year={2020},
}


@incollection{ferrari_riemannian_2018,
	address = {Cham},
	title = {Riemannian {Walk} for {Incremental} {Learning}: {Understanding} {Forgetting} and {Intransigence}},
	volume = {11215},
	isbn = {978-3-030-01251-9 978-3-030-01252-6},
	shorttitle = {Riemannian {Walk} for {Incremental} {Learning}},
	url = {http://link.springer.com/10.1007/978-3-030-01252-6_33},
	abstract = {Incremental learning (IL) has received a lot of attention recently, however, the literature lacks a precise problem deÔ¨Ånition, proper evaluation settings, and metrics tailored speciÔ¨Åcally for the IL problem. One of the main objectives of this work is to Ô¨Åll these gaps so as to provide a common ground for better understanding of IL. The main challenge for an IL algorithm is to update the classiÔ¨Åer whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, IL also suffers from a problem we call intransigence, its inability to update knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of IL algorithms. Furthermore, we present RWalk, a generalization of EWC++ (our efÔ¨Åcient version of EWC [6]) and Path Integral [25] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various IL algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off for forgetting and intransigence.},
	language = {en},
	urldate = {2021-12-06},
	booktitle = {Computer {Vision} ‚Äì {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Chaudhry, Arslan and Dokania, Puneet K. and Ajanthan, Thalaiyasingam and Torr, Philip H. S.},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01252-6_33},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {556--572},
	file = {Chaudhry et al. - 2018 - Riemannian Walk for Incremental Learning Understa.pdf:/Users/nicolas/Zotero/storage/S4SUDVGZ/Chaudhry et al. - 2018 - Riemannian Walk for Incremental Learning Understa.pdf:application/pdf},
}

@inproceedings{aljundi_online_2019,
	title = {Online {Continual} {Learning} with {Maximal} {Interfered} {Retrieval}},
	volume = {32},
	urldate = {2021-12-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and Page-Caccia, Lucas},
	year = {2019},
}

@article{french_catastrophic_nodate-1,
	title = {Catastrophic {Forgetting} in {Connectionist} {Networks}: {Causes}, {Consequences} and {Solutions}},
	abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Consequently, plausible models of human cognition should exhibit similar patterns of gradual forgetting old information as new information is acquired. Only rarely (see Box 3) does new learning in natural cognitive systems completely disrupt or erase previously learned information. In other words, natural cognitive systems do not, in general, forget catastrophically. Unfortunately, however, this is precisely what occurs under certain circumstances in distributed connectionist networks. It turns out that the very features that give these networks their much-touted abilities to generalize, to function in the presence of degraded input, etc., are the root cause of catastrophic forgetting. The challenge is how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article, we examine the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks. We consider how the brain might have overcome this problem and explore the consequences of this solution.},
	language = {en},
	author = {French, Robert M},
	pages = {18},
	file = {French - Catastrophic Forgetting in Connectionist Networks.pdf:/Users/nicolas/Zotero/storage/NA95YRD9/French - Catastrophic Forgetting in Connectionist Networks.pdf:application/pdf},
}

@article{abu-el-haija_youtube-8m_2016,
	title = {{YouTube}-{8M}: {A} {Large}-{Scale} {Video} {Classification} {Benchmark}},
	shorttitle = {{YouTube}-{8M}},
	url = {http://arxiv.org/abs/1609.08675},
	abstract = {Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of {\textasciitilde}8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics.},
	urldate = {2021-12-02},
	journal = {arXiv:1609.08675 [cs]},
	author = {Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.08675},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 8M},
	file = {Abu-El-Haija et al. - 2016 - YouTube-8M A Large-Scale Video Classification Ben.pdf:/Users/nicolas/Documents/Zotero/arXiv1609.08675 [cs]2016/Abu-El-Haija et al. - 2016 - YouTube-8M A Large-Scale Video Classification Ben.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Z2SUWNGM/1609.html:text/html},
}

@article{liu_rotate_2018,
	title = {Rotate your {Networks}: {Better} {Weight} {Consolidation} and {Less} {Catastrophic} {Forgetting}},
	shorttitle = {Rotate your {Networks}},
	url = {http://arxiv.org/abs/1802.02950},
	abstract = {In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to other state-of-the-art in lifelong learning without forgetting.},
	urldate = {2021-11-16},
	journal = {arXiv:1802.02950 [cs]},
	author = {Liu, Xialei and Masana, Marc and Herranz, Luis and Van de Weijer, Joost and Lopez, Antonio M. and Bagdanov, Andrew D.},
	month = dec,
	year = {2018},
	note = {arXiv: 1802.02950},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/N2CCFBCY/1802.html:text/html;Liu et al. - 2018 - Rotate your Networks Better Weight Consolidation .pdf:/Users/nicolas/Documents/Zotero/arXiv1802.02950 [cs]2018/Liu et al. - 2018 - Rotate your Networks Better Weight Consolidation .pdf:application/pdf},
}

@article{smith_memory-efficient_2021,
	title = {Memory-{Efficient} {Semi}-{Supervised} {Continual} {Learning}: {The} {World} is its {Own} {Replay} {Buffer}},
	shorttitle = {Memory-{Efficient} {Semi}-{Supervised} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2101.09536},
	abstract = {Rehearsal is a critical component for class-incremental continual learning, yet it requires a substantial memory budget. Our work investigates whether we can significantly reduce this memory budget by leveraging unlabeled data from an agent's environment in a realistic and challenging continual learning paradigm. Specifically, we explore and formalize a novel semi-supervised continual learning (SSCL) setting, where labeled data is scarce yet non-i.i.d. unlabeled data from the agent's environment is plentiful. Importantly, data distributions in the SSCL setting are realistic and therefore reflect object class correlations between, and among, the labeled and unlabeled data distributions. We show that a strategy built on pseudo-labeling, consistency regularization, Out-of-Distribution (OoD) detection, and knowledge distillation reduces forgetting in this setting. Our approach, DistillMatch, increases performance over the state-of-the-art by no less than 8.7\% average task accuracy and up to 54.5\% average task accuracy in SSCL CIFAR-100 experiments. Moreover, we demonstrate that DistillMatch can save up to 0.23 stored images per processed unlabeled image compared to the next best method which only saves 0.08. Our results suggest that focusing on realistic correlated distributions is a significantly new perspective, which accentuates the importance of leveraging the world's structure as a continual learning strategy.},
	urldate = {2021-11-15},
	journal = {arXiv:2101.09536 [cs]},
	author = {Smith, James and Balloch, Jonathan and Hsu, Yen-Chang and Kira, Zsolt},
	month = may,
	year = {2021},
	note = {arXiv: 2101.09536},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/YUDU6QGY/2101.html:text/html;Smith et al. - 2021 - Memory-Efficient Semi-Supervised Continual Learnin.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.09536 [cs]2021/Smith et al. - 2021 - Memory-Efficient Semi-Supervised Continual Learnin.pdf:application/pdf},
}

@article{lee_overcoming_2019-1,
	title = {Overcoming {Catastrophic} {Forgetting} with {Unlabeled} {Data} in the {Wild}},
	url = {http://arxiv.org/abs/1903.12648},
	abstract = {Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: the performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: our method shows up to 15.8\% higher accuracy and 46.5\% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.},
	urldate = {2021-11-09},
	journal = {arXiv:1903.12648 [cs, stat]},
	author = {Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
	month = oct,
	year = {2019},
	note = {arXiv: 1903.12648},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/3T29S9N5/1903.html:text/html;Lee et al. - 2019 - Overcoming Catastrophic Forgetting with Unlabeled .pdf:/Users/nicolas/Documents/Zotero/arXiv1903.12648 [cs, stat]2019/Lee et al. - 2019 - Overcoming Catastrophic Forgetting with Unlabeled 2.pdf:application/pdf},
}

@inproceedings{he_online_2021,
  title={Online continual learning via candidates voting},
  author={He, Jiangpeng and Zhu, Fengqing},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3154--3163},
  year={2022}
}


@inproceedings{dhar_learning_2019,
	address = {Long Beach, CA, USA},
	title = {Learning {Without} {Memorizing}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953962/},
	doi = {10.1109/CVPR.2019.00528},
	abstract = {Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classiÔ¨Åer to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called ‚ÄòLearning without Memorizing (LwM)‚Äô, to preserve the information about existing (base) classes, without storing any of their data, while making the classiÔ¨Åer progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss (LAD), and demonstrate that penalizing the changes in classiÔ¨Åers‚Äô attention maps helps to retain information of the base classes, as new classes are added. We show that adding LAD to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Dhar, Prithviraj and Singh, Rajat Vikram and Peng, Kuan-Chuan and Wu, Ziyan and Chellappa, Rama},
	month = jun,
	year = {2019},
	pages = {5133--5141},
	file = {Dhar et al. - 2019 - Learning Without Memorizing.pdf:/Users/nicolas/Zotero/storage/2AK499D8/Dhar et al. - 2019 - Learning Without Memorizing.pdf:application/pdf},
}

@article{chaudhry_efficient_2019,
  title={Efficient lifelong learning with a-gem},
  author={Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:1812.00420},
  year={2018}
}

@article{cha_co2l_2021,
  title={Co2l: Contrastive continual learning},
  author={Cha, Hyuntak and Lee, Jaeho and Shin, Jinwoo},
  journal={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9516--9525},
  year={2021}
}

@article{He_2020_CVPR,
author = {He, Jiangpeng and Mao, Runyu and Shao, Zeman and Zhu, Fengqing},
title = {Incremental Learning in Online Scenario},
journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
month = {June},
year = {2020}
}


@article{he_incremental_2021,
	title = {Incremental {Learning} {In} {Online} {Scenario}},
	url = {http://arxiv.org/abs/2003.13191},
	abstract = {Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-specific data. However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental learning methods require a long time to train the model whenever new classes are added and none of them takes into consideration the new observations of old classes. In this paper, we propose an incremental learning framework that can work in the challenging online learning scenario and handle both new classes data and new observations of old classes. We address problem (1) in online mode by introducing a modified cross-distillation loss together with a two-step learning technique. Our method outperforms the results obtained from current state-of-the-art offline incremental learning methods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the same experiment protocol but in online scenario. We also provide a simple yet effective method to mitigate problem (2) by updating exemplar set using the feature of each new observation of old classes and demonstrate a real life application of online food image classification based on our complete framework using the Food-101 dataset.},
	urldate = {2021-09-17},
	journal = {arXiv:2003.13191 [cs]},
	author = {He, Jiangpeng and Mao, Runyu and Shao, Zeman and Zhu, Fengqing},
	month = apr,
	year = {2021},
	note = {arXiv: 2003.13191},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/LJ85ZUDW/2003.html:text/html;He et al. - 2021 - Incremental Learning In Online Scenario.pdf:/Users/nicolas/Documents/Zotero/arXiv2003.13191 [cs]2021/He et al. - 2021 - Incremental Learning In Online Scenario.pdf:application/pdf},
}

@article{ji_complementary_2021,
	title = {Complementary {Calibration}: {Boosting} {General} {Continual} {Learning} with {Collaborative} {Distillation} and {Self}-{Supervision}},
	shorttitle = {Complementary {Calibration}},
	url = {http://arxiv.org/abs/2109.02426},
	abstract = {General Continual Learning (GCL) aims at learning from non independent and identically distributed stream data without catastrophic forgetting of the old tasks that don't rely on task boundaries during both training and testing stages. We reveal that the relation and feature deviations are crucial problems for catastrophic forgetting, in which relation deviation refers to the deficiency of the relationship among all classes in knowledge distillation, and feature deviation refers to indiscriminative feature representations. To this end, we propose a Complementary Calibration (CoCa) framework by mining the complementary model's outputs and features to alleviate the two deviations in the process of GCL. Specifically, we propose a new collaborative distillation approach for addressing the relation deviation. It distills model's outputs by utilizing ensemble dark knowledge of new model's outputs and reserved outputs, which maintains the performance of old tasks as well as balancing the relationship among all classes. Furthermore, we explore a collaborative self-supervision idea to leverage pretext tasks and supervised contrastive learning for addressing the feature deviation problem by learning complete and discriminative features for all classes. Extensive experiments on four popular datasets show that our CoCa framework achieves superior performance against state-of-the-art methods.},
	urldate = {2021-09-13},
	journal = {arXiv:2109.02426 [cs]},
	author = {Ji, Zhong and Li, Jin and Wang, Qiang and Zhang, Zhongfei},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.02426},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Coca},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/VKRK7I6D/2109.html:text/html;Ji et al. - 2021 - Complementary Calibration Boosting General Contin.pdf:/Users/nicolas/Documents/Zotero/arXiv2109.02426 [cs]2021/Ji et al. - 2021 - Complementary Calibration Boosting General Contin.pdf:application/pdf},
}

@article{rosasco_distilled_2021,
	title = {Distilled {Replay}: {Overcoming} {Forgetting} through {Synthetic} {Samples}},
	shorttitle = {Distilled {Replay}},
	url = {http://arxiv.org/abs/2103.15851},
	abstract = {Replay strategies are Continual Learning techniques which mitigate catastrophic forgetting by keeping a buffer of patterns from previous experiences, which are interleaved with new data during training. The amount of patterns stored in the buffer is a critical parameter which largely influences the final performance and the memory footprint of the approach. This work introduces Distilled Replay, a novel replay strategy for Continual Learning which is able to mitigate forgetting by keeping a very small buffer (1 pattern per class) of highly informative samples. Distilled Replay builds the buffer through a distillation process which compresses a large dataset into a tiny set of informative examples. We show the effectiveness of our Distilled Replay against popular replay-based strategies on four Continual Learning benchmarks.},
	urldate = {2021-09-10},
	journal = {arXiv:2103.15851 [cs]},
	author = {Rosasco, Andrea and Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.15851
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/NIQ5JTJC/2103.html:text/html;Rosasco et al. - 2021 - Distilled Replay Overcoming Forgetting through Sy.pdf:/Users/nicolas/Documents/Zotero/arXiv2103.15851 [cs]2021/Rosasco et al. - 2021 - Distilled Replay Overcoming Forgetting through Sy.pdf:application/pdf},
}

@article{de_lange_continual_2021-1,
  title={Continual prototype evolution: Learning online from non-stationary data streams},
  author={De Lange, Matthias and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8250--8259},
  year={2021}
}

@article{orabona_training_2017,
	title = {Training {Deep} {Networks} without {Learning} {Rates} {Through} {Coin} {Betting}},
	url = {http://arxiv.org/abs/1705.07795},
	abstract = {Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a significant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning-rate-free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.},
	urldate = {2021-09-01},
	journal = {arXiv:1705.07795 [cs, math, stat]},
	author = {Orabona, Francesco and Tommasi, Tatiana},
	month = nov,
	year = {2017},
	note = {arXiv: 1705.07795},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, COCOB, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/98JWCVVY/Orabona and Tommasi - 2017 - Training Deep Networks without Learning Rates Thro.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/LKSU3V2Y/1705.html:text/html},
}

@article{masuyama_multi-label_2021,
	title = {Multi-label {Classification} via {Adaptive} {Resonance} {Theory}-based {Clustering}},
	url = {http://arxiv.org/abs/2103.01511},
	abstract = {This paper proposes a multi-label classification algorithm capable of continual learning by applying an Adaptive Resonance Theory (ART)-based clustering algorithm and the Bayesian approach for label probability computation. The ART-based clustering algorithm adaptively and continually generates prototype nodes corresponding to given data, and the generated nodes are used as classifiers. The label probability computation independently counts the number of label appearances for each class and calculates the Bayesian probabilities. Thus, the label probability computation can cope with an increase in the number of labels. Experimental results with synthetic and real-world multi-label datasets show that the proposed algorithm has competitive classification performance to other well-known algorithms while realizing continual learning.},
	urldate = {2021-08-25},
	journal = {arXiv:2103.01511 [cs]},
	author = {Masuyama, Naoki and Nojima, Yusuke and Loo, Chu Kiong and Ishibuchi, Hisao},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.01511},
	keywords = {Computer Science - Machine Learning, ART},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/VZEZD7LH/2103.html:text/html;Masuyama et al. - 2021 - Multi-label Classification via Adaptive Resonance .pdf:/Users/nicolas/Documents/Zotero/arXiv2103.01511 [cs]2021/Masuyama et al. - 2021 - Multi-label Classification via Adaptive Resonance 2.pdf:application/pdf},
}

@inproceedings{he_unsupervised_2021,
  title={Unsupervised continual learning via pseudo labels},
  author={He, Jiangpeng and Zhu, Fengqing},
  booktitle={International Workshop on Continual Semi-Supervised Learning},
  pages={15--32},
  year={2021},
  organization={Springer}
}


@article{khare_unsupervised_2021,
	title = {Unsupervised {Class}-{Incremental} {Learning} {Through} {Confusion}},
	url = {http://arxiv.org/abs/2104.04450},
	abstract = {While many works on Continual Learning have shown promising results for mitigating catastrophic forgetting, they have relied on supervised training. To successfully learn in a label-agnostic incremental setting, a model must distinguish between learned and novel classes to properly include samples for training. We introduce a novelty detection method that leverages network confusion caused by training incoming data as a new class. We found that incorporating a class-imbalance during this detection method substantially enhances performance. The effectiveness of our approach is demonstrated across a set of image classification benchmarks: MNIST, SVHN, CIFAR-10, CIFAR-100, and CRIB.},
	urldate = {2021-08-25},
	journal = {arXiv:2104.04450 [cs]},
	author = {Khare, Shivam and Cao, Kun and Rehg, James},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.04450
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, UCIL},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/RSUCNL8T/2104.html:text/html;Khare et al. - 2021 - Unsupervised Class-Incremental Learning Through Co.pdf:/Users/nicolas/Documents/Zotero/arXiv2104.04450 [cs]2021/Khare et al. - 2021 - Unsupervised Class-Incremental Learning Through Co.pdf:application/pdf},
}

@inproceedings{stojanov_incremental_2019,
	title = {Incremental {Object} {Learning} {From} {Contiguous} {Views}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Stojanov_Incremental_Object_Learning_From_Contiguous_Views_CVPR_2019_paper.html},
	urldate = {2021-08-25},
	author = {Stojanov, Stefan and Mishra, Samarth and Thai, Ngoc Anh and Dhanda, Nikhil and Humayun, Ahmad and Yu, Chen and Smith, Linda B. and Rehg, James M.},
	year = {2019},
	keywords = {CRIB},
	pages = {8777--8786},
	file = {Snapshot:/Users/nicolas/Zotero/storage/XZ2ILDBY/Stojanov_Incremental_Object_Learning_From_Contiguous_Views_CVPR_2019_paper.html:text/html;Stojanov et al. - 2019 - Incremental Object Learning From Contiguous Views.pdf:/Users/nicolas/Documents/Zotero/2019/Stojanov et al. - 2019 - Incremental Object Learning From Contiguous Views.pdf:application/pdf},
}

@article{aljundi_gradient_2019,  
title={Gradient based sample selection for online continual learning},
  author={Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{pratama_unsupervised_2021,
	title = {Unsupervised {Continual} {Learning} via {Self}-{Adaptive} {Deep} {Clustering} {Approach}},
	url = {http://arxiv.org/abs/2106.14563},
	abstract = {Unsupervised continual learning remains a relatively uncharted territory in the existing literature because the vast majority of existing works call for unlimited access of ground truth incurring expensive labelling cost. Another issue lies in the problem of task boundaries and task IDs which must be known for model's updates or model's predictions hindering feasibility for real-time deployment. Knowledge Retention in Self-Adaptive Deep Continual Learner, (KIERA), is proposed in this paper. KIERA is developed from the notion of flexible deep clustering approach possessing an elastic network structure to cope with changing environments in the timely manner. The centroid-based experience replay is put forward to overcome the catastrophic forgetting problem. KIERA does not exploit any labelled samples for model updates while featuring a task-agnostic merit. The advantage of KIERA has been numerically validated in popular continual learning problems where it shows highly competitive performance compared to state-of-the art approaches. Our implementation is available in {\textbackslash}textit\{{\textbackslash}url\{https://github.com/ContinualAL/KIERA\}\}.},
	urldate = {2021-08-25},
	journal = {arXiv:2106.14563 [cs]},
	author = {Pratama, Mahardhika and Ashfahani, Andri and Lughofer, Edwin},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.14563},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, KIERA},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/KVNK6AEU/2106.html:text/html;Pratama et al. - 2021 - Unsupervised Continual Learning via Self-Adaptive .pdf:/Users/nicolas/Documents/Zotero/arXiv2106.14563 [cs]2021/Pratama et al. - 2021 - Unsupervised Continual Learning via Self-Adaptive 2.pdf:application/pdf},
}

@article{caccia_online_2020,
	title = {Online {Learned} {Continual} {Compression} with {Adaptive} {Quantization} {Modules}},
	url = {http://arxiv.org/abs/1911.08019},
	abstract = {We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning environments.},
	urldate = {2021-08-25},
	journal = {arXiv:1911.08019 [cs, stat]},
	author = {Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Pineau, Joelle},
	month = aug,
	year = {2020},
	note = {arXiv: 1911.08019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, AQM, VAE},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/IJBC33K8/1911.html:text/html;Caccia et al. - 2020 - Online Learned Continual Compression with Adaptive.pdf:/Users/nicolas/Documents/Zotero/arXiv1911.08019 [cs, stat]2020/Caccia et al. - 2020 - Online Learned Continual Compression with Adaptive2.pdf:application/pdf},
}

@article{achille_life-long_2018,
	title = {Life-{Long} {Disentangled} {Representation} {Learning} with {Cross}-{Domain} {Latent} {Homologies}},
	url = {http://arxiv.org/abs/1808.06508},
	abstract = {Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.},
	urldate = {2021-08-25},
	journal = {arXiv:1808.06508 [cs, stat]},
	author = {Achille, Alessandro and Eccles, Tom and Matthey, Loic and Burgess, Christopher P. and Watters, Nick and Lerchner, Alexander and Higgins, Irina},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.06508},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, representation learning, unsupervised},
	file = {Achille et al. - 2018 - Life-Long Disentangled Representation Learning wit.pdf:/Users/nicolas/Documents/Zotero/arXiv1808.06508 [cs, stat]2018/Achille et al. - 2018 - Life-Long Disentangled Representation Learning wit.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/YEXKFU4L/1808.html:text/html},
}

@article{smith_always_2021,
	title = {Always {Be} {Dreaming}: {A} {New} {Approach} for {Data}-{Free} {Class}-{Incremental} {Learning}},
	shorttitle = {Always {Be} {Dreaming}},
	url = {http://arxiv.org/abs/2106.09701},
	abstract = {Modern computer vision applications suffer from catastrophic forgetting when incrementally learning new concepts over time. The most successful approaches to alleviate this forgetting require extensive replay of previously seen data, which is problematic when memory constraints or data legality concerns exist. In this work, we consider the high-impact problem of Data-Free Class-Incremental Learning (DFCIL), where an incremental learning agent must learn new concepts over time without storing generators or training data from past tasks. One approach for DFCIL is to replay synthetic images produced by inverting a frozen copy of the learner's classification model, but we show this approach fails for common class-incremental benchmarks when using standard distillation strategies. We diagnose the cause of this failure and propose a novel incremental distillation strategy for DFCIL, contributing a modified cross-entropy training and importance-weighted feature distillation, and show that our method results in up to a 25.1\% increase in final task accuracy (absolute difference) compared to SOTA DFCIL methods for common class-incremental benchmarks. Our method even outperforms several standard replay based methods which store a coreset of images.},
	urldate = {2021-08-25},
	journal = {arXiv:2106.09701 [cs]},
	author = {Smith, James and Hsu, Yen-Chang and Balloch, Jonathan and Shen, Yilin and Jin, Hongxia and Kira, Zsolt},
	month = aug,
	year = {2021},
	note = {arXiv: 2106.09701},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, data-free},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/N6YAXSZP/2106.html:text/html;Smith et al. - 2021 - Always Be Dreaming A New Approach for Data-Free C.pdf:/Users/nicolas/Documents/Zotero/arXiv2106.09701 [cs]2021/Smith et al. - 2021 - Always Be Dreaming A New Approach for Data-Free C.pdf:application/pdf},
}

@article{caccia_special_2021,
	title = {{SPeCiaL}: {Self}-{Supervised} {Pretraining} for {Continual} {Learning}},
	shorttitle = {{SPeCiaL}},
	url = {http://arxiv.org/abs/2106.09065},
	abstract = {This paper presents SPeCiaL: a method for unsupervised pretraining of representations tailored for continual learning. Our approach devises a meta-learning objective that differentiates through a sequential learning process. Specifically, we train a linear model over the representations to match different augmented views of the same image together, each view presented sequentially. The linear model is then evaluated on both its ability to classify images it just saw, and also on images from previous iterations. This gives rise to representations that favor quick knowledge retention with minimal forgetting. We evaluate SPeCiaL in the Continual Few-Shot Learning setting, and show that it can match or outperform other supervised pretraining approaches.},
	urldate = {2021-08-25},
	journal = {arXiv:2106.09065 [cs]},
	author = {Caccia, Lucas and Pineau, Joelle},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.09065},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, SPECIAL},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/RLH7FRTC/Caccia and Pineau - 2021 - SPeCiaL Self-Supervised Pretraining for Continual.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/YM46Q82D/2106.html:text/html},
}

@inproceedings{stojanov_incremental_2019-1,
	address = {Long Beach, CA, USA},
	title = {Incremental {Object} {Learning} {From} {Contiguous} {Views}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953997/},
	doi = {10.1109/CVPR.2019.00898},
	abstract = {In this work, we present CRIB (Continual Recognition Inspired by Babies), a synthetic incremental object learning environment that can produce data that models visual imagery produced by object exploration in early infancy. CRIB is coupled with a new 3D object dataset, Toys-200, that contains 200 unique toy-like object instances, and is also compatible with existing 3D datasets. Through extensive empirical evaluation of state-of-the-art incremental learning algorithms, we Ô¨Ånd the novel empirical result that repetition can signiÔ¨Åcantly ameliorate the effects of catastrophic forgetting. Furthermore, we Ô¨Ånd that in certain cases repetition allows for performance approaching that of batch learning algorithms. Finally, we propose an unsupervised incremental learning task with intriguing baseline results.},
	language = {en},
	urldate = {2021-08-18},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Stojanov, Stefan and Mishra, Samarth and Thai, Ngoc Anh and Dhanda, Nikhil and Humayun, Ahmad and Yu, Chen and Smith, Linda B. and Rehg, James M.},
	month = jun,
	year = {2019},
	pages = {8769--8778},
	file = {Stojanov et al. - 2019 - Incremental Object Learning From Contiguous Views.pdf:/Users/nicolas/Zotero/storage/VBC7KYY5/Stojanov et al. - 2019 - Incremental Object Learning From Contiguous Views.pdf:application/pdf},
}

@article{lin_continual_2021,
	title = {Continual {Contrastive} {Self}-supervised {Learning} for {Image} {Classification}},
	url = {http://arxiv.org/abs/2107.01776},
	abstract = {For artificial learning systems, continual learning over time from a stream of data is essential. The burgeoning studies on supervised continual learning have achieved great progress, while the study of catastrophic forgetting in unsupervised learning is still blank. Among unsupervised learning methods, self-supervise learning method shows tremendous potential on visual representation without any labeled data at scale. To improve the visual representation of self-supervised learning, larger and more varied data is needed. In the real world, unlabeled data is generated at all times. This circumstance provides a huge advantage for the learning of the self-supervised method. However, in the current paradigm, packing previous data and current data together and training it again is a waste of time and resources. Thus, a continual self-supervised learning method is badly needed. In this paper, we make the first attempt to implement the continual contrastive self-supervised learning by proposing a rehearsal method, which keeps a few exemplars from the previous data. Instead of directly combining saved exemplars with the current data set for training, we leverage self-supervised knowledge distillation to transfer contrastive information among previous data to the current network by mimicking similarity score distribution inferred by the old network over a set of saved exemplars. Moreover, we build an extra sample queue to assist the network to distinguish between previous and current data and prevent mutual interference while learning their own feature representation. Experimental results show that our method performs well on CIFAR100 and ImageNet-Sub. Compared with the baselines, which learning tasks without taking any technique, we improve the image classification top-1 accuracy by 1.60\% on CIFAR100, 2.86\% on ImageNet-Sub and 1.29\% on ImageNet-Full under 10 incremental steps setting.},
	urldate = {2021-08-17},
	journal = {arXiv:2107.01776 [cs]},
	author = {Lin, Zhiwei and Wang, Yongtao and Lin, Hongxiang},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.01776},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/7UKJ5R3E/2107.html:text/html;Lin et al. - 2021 - Continual Contrastive Self-supervised Learning for.pdf:/Users/nicolas/Documents/Zotero/arXiv2107.01776 [cs]2021/Lin et al. - 2021 - Continual Contrastive Self-supervised Learning for.pdf:application/pdf},
}

@article{moons_representation_nodate,
	title = {Representation {Learning} for {Automated} {Document} {Classification}},
	language = {en},
	author = {Moons, Elias},
	pages = {132},
	file = {Moons - Representation Learning for Automated Document Cla.pdf:/Users/nicolas/Zotero/storage/KUVSRVX4/Moons - Representation Learning for Automated Document Cla.pdf:application/pdf},
}

@article{caccia_online_2020-1,
	title = {Online {Learned} {Continual} {Compression} with {Adaptive} {Quantization} {Modules}},
	url = {http://arxiv.org/abs/1911.08019},
	abstract = {We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning environments.},
	urldate = {2021-08-17},
	journal = {arXiv:1911.08019 [cs, stat]},
	author = {Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Pineau, Joelle},
	month = aug,
	year = {2020},
	note = {arXiv: 1911.08019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, AQM},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/A65XS5WZ/1911.html:text/html;Caccia et al. - 2020 - Online Learned Continual Compression with Adaptive.pdf:/Users/nicolas/Documents/Zotero/arXiv1911.08019 [cs, stat]2020/Caccia et al. - 2020 - Online Learned Continual Compression with Adaptive.pdf:application/pdf},
}

@inproceedings{sun_ilcoc_2021,
	title = {{ILCOC}: {An} {Incremental} {Learning} {Framework} {Based} on {Contrastive} {One}-{Class} {Classifiers}},
	shorttitle = {{ILCOC}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Sun_ILCOC_An_Incremental_Learning_Framework_Based_on_Contrastive_One-Class_Classifiers_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-08-12},
	author = {Sun, Wenju and Zhang, Jing and Wang, Danyu and Geng, Yangli-ao and Li, Qingyong},
	year = {2021},
	pages = {3580--3588},
	file = {Snapshot:/Users/nicolas/Zotero/storage/NFKJY3SW/Sun_ILCOC_An_Incremental_Learning_Framework_Based_on_Contrastive_One-Class_Classifiers_CVPRW_20.html:text/html;Sun et al. - 2021 - ILCOC An Incremental Learning Framework Based on .pdf:/Users/nicolas/Documents/Zotero/2021/Sun et al. - 2021 - ILCOC An Incremental Learning Framework Based on .pdf:application/pdf},
}

@article{song_multi-label_2020,
	title = {Multi-label {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/2007.09852},
	abstract = {Variational mutual information (MI) estimators are widely used in unsupervised representation learning methods such as contrastive predictive coding (CPC). A lower bound on MI can be obtained from a multi-class classification problem, where a critic attempts to distinguish a positive sample drawn from the underlying joint distribution from \$(m-1)\$ negative samples drawn from a suitable proposal distribution. Using this approach, MI estimates are bounded above by \${\textbackslash}log m\$, and could thus severely underestimate unless \$m\$ is very large. To overcome this limitation, we introduce a novel estimator based on a multi-label classification problem, where the critic needs to jointly identify multiple positive samples at the same time. We show that using the same amount of negative samples, multi-label CPC is able to exceed the \${\textbackslash}log m\$ bound, while still being a valid lower bound of mutual information. We demonstrate that the proposed approach is able to lead to better mutual information estimation, gain empirical improvements in unsupervised representation learning, and beat a current state-of-the-art knowledge distillation method over 10 out of 13 tasks.},
	urldate = {2021-08-09},
	journal = {arXiv:2007.09852 [cs, stat]},
	author = {Song, Jiaming and Ermon, Stefano},
	month = dec,
	year = {2020},
	note = {arXiv: 2007.09852},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/NUSJ6ZX3/Song and Ermon - 2020 - Multi-label Contrastive Predictive Coding.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/F4ZXXJFG/2007.html:text/html},
}

@article{mai_online_2021-1,
	title = {Online {Continual} {Learning} in {Image} {Classification}: {An} {Empirical} {Survey}},
	shorttitle = {Online {Continual} {Learning} in {Image} {Classification}},
	url = {http://arxiv.org/abs/2101.10423},
	abstract = {Online continual learning for image classification studies the problem of learning to classify images from an online stream of data and tasks, where tasks may include new classes (class incremental) or data nonstationarity (domain incremental). One of the key challenges of continual learning is to avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence of more recent tasks. Over the past few years, many methods and tricks have been introduced to address this problem, but many have not been fairly and systematically compared under a variety of realistic and practical settings. To better understand the relative advantages of various approaches and the settings where they work best, this survey aims to (1) compare state-of-the-art methods such as MIR, iCARL, and GDumb and determine which works best at different experimental settings; (2) determine if the best class incremental methods are also competitive in domain incremental setting; (3) evaluate the performance of 7 simple but effective trick such as "review" trick and nearest class mean (NCM) classifier to assess their relative impact. Regarding (1), we observe iCaRL remains competitive when the memory buffer is small; GDumb outperforms many recently proposed methods in medium-size datasets and MIR performs the best in larger-scale datasets. For (2), we note that GDumb performs quite poorly while MIR -- already competitive for (1) -- is also strongly competitive in this very different but important setting. Overall, this allows us to conclude that MIR is overall a strong and versatile method across a wide variety of settings. For (3), we find that all 7 tricks are beneficial, and when augmented with the "review" trick and NCM classifier, MIR produces performance levels that bring online continual learning much closer to its ultimate goal of matching offline training.},
	urldate = {2021-08-09},
	journal = {arXiv:2101.10423 [cs]},
	author = {Mai, Zheda and Li, Ruiwen and Jeong, Jihwan and Quispe, David and Kim, Hyunwoo and Sanner, Scott},
	month = jun,
	year = {2021},
	note = {arXiv: 2101.10423},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/ZA96YRTW/2101.html:text/html;Mai et al. - 2021 - Online Continual Learning in Image Classification.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.10423 [cs]2021/Mai et al. - 2021 - Online Continual Learning in Image Classification2.pdf:application/pdf},
}

@article{masuyama_multi-label_2021-1,
	title = {Multi-label {Classification} via {Adaptive} {Resonance} {Theory}-based {Clustering}},
	url = {http://arxiv.org/abs/2103.01511},
	abstract = {This paper proposes a multi-label classification algorithm capable of continual learning by applying an Adaptive Resonance Theory (ART)-based clustering algorithm and the Bayesian approach for label probability computation. The ART-based clustering algorithm adaptively and continually generates prototype nodes corresponding to given data, and the generated nodes are used as classifiers. The label probability computation independently counts the number of label appearances for each class and calculates the Bayesian probabilities. Thus, the label probability computation can cope with an increase in the number of labels. Experimental results with synthetic and real-world multi-label datasets show that the proposed algorithm has competitive classification performance to other well-known algorithms while realizing continual learning.},
	urldate = {2021-08-09},
	journal = {arXiv:2103.01511 [cs]},
	author = {Masuyama, Naoki and Nojima, Yusuke and Loo, Chu Kiong and Ishibuchi, Hisao},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.01511},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/9EM69QSE/2103.html:text/html;Masuyama et al. - 2021 - Multi-label Classification via Adaptive Resonance .pdf:/Users/nicolas/Documents/Zotero/arXiv2103.01511 [cs]2021/Masuyama et al. - 2021 - Multi-label Classification via Adaptive Resonance .pdf:application/pdf},
}

@inproceedings{wang_cifdm_2021,
	address = {New York, NY, USA},
	series = {{SIGIR} '21},
	title = {{CIFDM}: {Continual} and {Interactive} {Feature} {Distillation} for {Multi}-{Label} {Stream} {Learning}},
	isbn = {978-1-4503-8037-9},
	shorttitle = {{CIFDM}},
	url = {https://doi.org/10.1145/3404835.3463096},
	doi = {10.1145/3404835.3463096},
	abstract = {Multi-label learning algorithms have attracted more and more attention as of recent. This is mainly because real-world data is generally associated with multiple and non-exclusive labels, which could correspond to different objects, scenes, actions, and attributes. In this paper, we consider the following challenging multi-label stream scenario: the new labels emerge continuously in the changing environments, and are assigned to the previous data. In this setting, data mining solutions must be able to learn the new concepts and avoid catastrophic forgetting simultaneously. We propose a novel continual and interactive feature distillation-based learning framework (CIFDM), to effectively classify instances with novel labels. We utilize the knowledge from the previous tasks to learn new knowledge to solve the current task. Then, the system compresses historical and novel knowledge and preserves it while waiting for new emerging tasks. CIFDM consists of three components: 1) a knowledge bank that stores the existing feature-level compressed knowledge, and predicts the observed labels so far; 2) a pioneer module that aims to learn and predict new emerged labels based on knowledge bank.; 3) an interactive knowledge compression function which is used to compress and transfer the new knowledge to the bank, and then apply the current compressed knowledge to initialize the label embedding of the pioneer for the next task.},
	urldate = {2021-08-09},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Yigong and Wang, Zhuoyi and Lin, Yu and Khan, Latifur and Li, Dingcheng},
	month = jul,
	year = {2021},
	keywords = {incremental learning, multi-label, neural network, stream mining},
	pages = {2121--2125},
	file = {Wang et al. - 2021 - CIFDM Continual and Interactive Feature Distillat.pdf:/Users/nicolas/Documents/Zotero/Association for Computing Machinery2021/Wang et al. - 2021 - CIFDM Continual and Interactive Feature Distillat.pdf:application/pdf},
}

@article{roseberry_self-adjusting_2021,
	title = {Self-{Adjusting} k {Nearest} {Neighbors} for {Continual} {Learning} from {Multi}-{Label} {Drifting} {Data} {Streams}},
	volume = {442},
	doi = {10.1016/j.neucom.2021.02.032},
	abstract = {Drifting data streams and multi-label data are both challenging problems. Multi-label instances may simultaneously be associated with many labels and classifiers must predict the complete set of labels. Learning from data streams requires algorithms able to learn from potentially unbounded data that is constantly changing. When multi-label data arrives as a stream, the challenges of both problems must be addressed, but additional challenges unique to the combined problem also arise. Each label may experience different concept drifts, simultaneously or distinctly, and parameter optimizations may be different for each label. In this paper we present a self-adapting algorithm for drifting, multi-label data streams, that can adapt to a variety of concepts drifts, is robust to data-level difficulties, and mitigates the necessity to tune multiple parameters. The window of retained instances self-adjusts in size to retain only the current concept, enabling efficient response to abrupt concept drift. The value k is self-adapting for each label, relieving the necessity to tune and allowing it to change, over time, for each label individually. A novel, label-based mechanism disables individual labels that contribute to error, while another punitive measure removes erroneous instances entirely, increasing robustness to noise, concept drift and label differences. Extensive experiments on 35 multi-label streams and generators demonstrate the superiority and advantages of the self-adapting mechanisms proposed compared to existing state-of-the-art methods.},
	journal = {Neurocomputing},
	author = {Roseberry, Martha and Krawczyk, Bartosz and Djenouri, Youcef and Cano, Alberto},
	month = jun,
	year = {2021},
	pages = {10--25},
	file = {Roseberry et al. - 2021 - Self-Adjusting k Nearest Neighbors for Continual L.pdf:/Users/nicolas/Documents/Zotero/Neurocomputing2021/Roseberry et al. - 2021 - Self-Adjusting k Nearest Neighbors for Continual L.pdf:application/pdf},
}

@article{boschini2023apprendimento,
  title={Apprendimento Continuo mediante Metodi Rehearsal},
  author={Boschini, Matteo and others},
  year={2023},
  publisher={Universit{\`a} degli studi di Modena e Reggio Emilia}
}

@article{alberghini_continual_2021,
  title={Continual Learning for Multi-label Drifting Data Streams Using Homogeneous Ensemble of Self-adjusting Nearest Neighbors},
  author={Alberghini, Gavin},
  year={2021}
}

@article{he_unsupervised_2021-1,
	title = {Unsupervised {Continual} {Learning} {Via} {Pseudo} {Labels}},
	url = {http://arxiv.org/abs/2104.07164},
	abstract = {Continual learning aims to learn new tasks incrementally using less computation and memory resources instead of retraining the model from scratch whenever new task arrives. However, existing approaches are designed in supervised fashion assuming all data from new tasks have been manually annotated, which are not practical for many real-life applications. In this work, we propose to use pseudo label instead of the ground truth to make continual learning feasible in unsupervised mode. The pseudo labels of new data are obtained by applying global clustering algorithm and we propose to use the model updated from last incremental step as the feature extractor. Due to the scarcity of existing work, we introduce a new benchmark experimental protocol for unsupervised continual learning of image classification task under class-incremental setting where no class label is provided for each incremental learning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC) datasets by incorporating the pseudo label with various existing supervised approaches and show promising results in unsupervised scenario.},
	urldate = {2021-08-06},
	journal = {arXiv:2104.07164 [cs]},
	author = {He, Jiangpeng and Zhu, Fengqing},
	month = jul,
	year = {2021},
	note = {arXiv: 2104.07164
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/4W63EKHI/He and Zhu - 2021 - Unsupervised Continual Learning Via Pseudo Labels.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Q74Z6RT3/2104.html:text/html},
}

@inproceedings{chaudhry_continual_2020,
	title = {Continual {Learning} in {Low}-rank {Orthogonal} {Subspaces}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/70d85f35a1fdc0ab701ff78779306407-Abstract.html},
	urldate = {2021-08-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chaudhry, Arslan and Khan, Naeemullah and Dokania, Puneet and Torr, Philip},
	year = {2020},
	keywords = {CL},
	pages = {9900--9911},
	file = {Chaudhry et al. - 2020 - Continual Learning in Low-rank Orthogonal Subspace.pdf:/Users/nicolas/Documents/Zotero/Curran Associates, Inc.2020/Chaudhry et al. - 2020 - Continual Learning in Low-rank Orthogonal Subspace.pdf:application/pdf},
}

@inproceedings{yang_deep_2019,
	address = {Long Beach, CA, USA},
	title = {Deep {Spectral} {Clustering} {Using} {Dual} {Autoencoder} {Network}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953592/},
	doi = {10.1109/CVPR.2019.00419},
	abstract = {The clustering methods have recently absorbed evenincreasing attention in learning and vision. Deep clustering combines embedding and clustering together to obtain optimal embedding subspace for clustering, which can be more effective compared with conventional clustering methods. In this paper, we propose a joint learning framework for discriminative embedding and spectral clustering. We Ô¨Årst devise a dual autoencoder network, which enforces the reconstruction constraint for the latent representations and their noisy versions, to embed the inputs into a latent space for clustering. As such the learned latent representations can be more robust to noise. Then the mutual information estimation is utilized to provide more discriminative information from the inputs. Furthermore, a deep spectral clustering method is applied to embed the latent representations into the eigenspace and subsequently clusters them, which can fully exploit the relationship between inputs to achieve optimal clustering results. Experimental results on benchmark datasets show that our method can signiÔ¨Åcantly outperform state-of-the-art clustering approaches.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Xu and Deng, Cheng and Zheng, Feng and Yan, Junchi and Liu, Wei},
	month = jun,
	year = {2019},
	pages = {4061--4070},
	file = {Yang et al. - 2019 - Deep Spectral Clustering Using Dual Autoencoder Ne.pdf:/Users/nicolas/Zotero/storage/SB32QYNC/Yang et al. - 2019 - Deep Spectral Clustering Using Dual Autoencoder Ne.pdf:application/pdf},
}

@article{kumar_unified_2020,
	title = {A {Unified} {Bayesian} {Framework} for {Discriminative} and {Generative} {Continual} {Learning}},
	url = {https://openreview.net/forum?id=98fWAc-sFkv},
	abstract = {Continual Learning is a learning paradigm where learning systems are trained on a sequence of tasks. The goal here is to perform well on the current task without suffering from a performance drop...},
	language = {en},
	urldate = {2021-08-03},
	author = {Kumar, Abhishek and Chatterjee, Sunabha and Rai, Piyush},
	month = sep,
	year = {2020},
	file = {Kumar et al. - 2020 - A Unified Bayesian Framework for Discriminative an.pdf:/Users/nicolas/Documents/Zotero/2020/Kumar et al. - 2020 - A Unified Bayesian Framework for Discriminative an.pdf:application/pdf;Snapshot:/Users/nicolas/Zotero/storage/YHPCAUMI/forum.html:text/html},
}

@article{pratama_unsupervised_2021-1,
	title = {Unsupervised {Continual} {Learning} via {Self}-{Adaptive} {Deep} {Clustering} {Approach}},
	url = {http://arxiv.org/abs/2106.14563},
	abstract = {Unsupervised continual learning remains a relatively uncharted territory in the existing literature because the vast majority of existing works call for unlimited access of ground truth incurring expensive labelling cost. Another issue lies in the problem of task boundaries and task IDs which must be known for model's updates or model's predictions hindering feasibility for real-time deployment. Knowledge Retention in Self-Adaptive Deep Continual Learner, (KIERA), is proposed in this paper. KIERA is developed from the notion of flexible deep clustering approach possessing an elastic network structure to cope with changing environments in the timely manner. The centroid-based experience replay is put forward to overcome the catastrophic forgetting problem. KIERA does not exploit any labelled samples for model updates while featuring a task-agnostic merit. The advantage of KIERA has been numerically validated in popular continual learning problems where it shows highly competitive performance compared to state-of-the art approaches. Our implementation is available in {\textbackslash}textit\{{\textbackslash}url\{https://github.com/ContinualAL/KIERA\}\}.},
	urldate = {2021-08-03},
	journal = {arXiv:2106.14563 [cs]},
	author = {Pratama, Mahardhika and Ashfahani, Andri and Lughofer, Edwin},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.14563},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/5TC93IC2/2106.html:text/html;Pratama et al. - 2021 - Unsupervised Continual Learning via Self-Adaptive .pdf:/Users/nicolas/Documents/Zotero/arXiv2106.14563 [cs]2021/Pratama et al. - 2021 - Unsupervised Continual Learning via Self-Adaptive .pdf:application/pdf},
}

@article{loshchilov_sgdr_2017,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {http://arxiv.org/abs/1608.03983},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
	urldate = {2021-07-23},
	journal = {arXiv:1608.03983 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = may,
	year = {2017},
	note = {arXiv: 1608.03983},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, cosine, lr, scheduler},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/MULW49WZ/Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/CVG4S5F8/1608.html:text/html},
}

@article{chuang_debiased_2020,
	title = {Debiased {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2007.00224},
	abstract = {A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classification task.},
	urldate = {2021-07-07},
	journal = {arXiv:2007.00224 [cs, stat]},
	author = {Chuang, Ching-Yao and Robinson, Joshua and Yen-Chen, Lin and Torralba, Antonio and Jegelka, Stefanie},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.00224},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, supervised contrastive learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/84IW33QV/2007.html:text/html;Chuang et al. - 2020 - Debiased Contrastive Learning.pdf:/Users/nicolas/Documents/Zotero/arXiv2007.00224 [cs, stat]2020/Chuang et al. - 2020 - Debiased Contrastive Learning.pdf:application/pdf},
}

@misc{noauthor_gt-riplcontinual-learning-benchmark_2021,
	title = {{GT}-{RIPL}/{Continual}-{Learning}-{Benchmark}},
	copyright = {MIT},
	url = {https://github.com/GT-RIPL/Continual-Learning-Benchmark},
	abstract = {Evaluate three types of task shifting with popular continual learning algorithms.},
	urldate = {2021-06-21},
	publisher = {GT-RIPL},
	month = jun,
	year = {2021},
	note = {original-date: 2018-11-30T16:40:23Z},
	keywords = {survey, incremental-learning, deep-learning, artificial-neural-networks, continual-learning, continuous-learning, lifelong-learning},
}

@misc{mai_raptormaionline-continual-learning_2021,
	title = {{RaptorMai}/online-continual-learning},
	url = {https://github.com/RaptorMai/online-continual-learning},
	abstract = {A collection of online continual learning paper implementations for computer vision in PyTorch, including ASER(AAAI-21), SCR(CVPR21-W) and a survey under review.},
	urldate = {2021-06-14},
	author = {Mai, Zheda (Marco)},
	month = jun,
	year = {2021},
	note = {original-date: 2021-01-29T20:54:21Z},
	keywords = {incremental-learning, computer-vision, deep-learning, continual-learning, lifelong-learning, catastrophic-forgetting, class-incremental-learning, convolutional-neural-networks, incremental-continual-learning, online-continual-learning, SCR},
}

@misc{mai_raptormaicontinual_learning_papers_2021,
	title = {{RaptorMai}/continual\_learning\_papers},
	url = {https://github.com/RaptorMai/continual_learning_papers},
	abstract = {Relevant papers in Continual Learning},
	urldate = {2021-06-14},
	author = {Mai, Zheda (Marco)},
	month = feb,
	year = {2021},
	note = {original-date: 2021-02-14T04:45:17Z},
	keywords = {CL},
}

@article{noauthor_theory_nodate,
	title = {Theory of {Deep} {Learning}},
	language = {en},
	pages = {118},
	file = {Theory of Deep Learning.pdf:/Users/nicolas/Zotero/storage/JKUVJ7MY/Theory of Deep Learning.pdf:application/pdf},
}

@article{aljundi_memory_2018,
	title = {Memory {Aware} {Synapses}: {Learning} what (not) to forget},
	shorttitle = {Memory {Aware} {Synapses}},
	url = {http://arxiv.org/abs/1711.09601},
	abstract = {Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb's rule,which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting \${\textless}\$subject, predicate, object\${\textgreater}\$ triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions.},
	urldate = {2021-06-09},
	journal = {arXiv:1711.09601 [cs, stat]},
	author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
	month = oct,
	year = {2018},
	note = {arXiv: 1711.09601},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, MAS},
	file = {Aljundi et al. - 2018 - Memory Aware Synapses Learning what (not) to forg.pdf:/Users/nicolas/Documents/Zotero/arXiv1711.09601 [cs, stat]2018/Aljundi et al. - 2018 - Memory Aware Synapses Learning what (not) to forg.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/IYJYL5DB/1711.html:text/html},
}

@inproceedings{aljundi_task-free_2019,
  title={Task-free continual learning},
  author={Aljundi, Rahaf and Kelchtermans, Klaas and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11254--11263},
  year={2019}
}

@misc{noauthor_deepminddeepmind-research_2021,
	title = {deepmind/deepmind-research},
	copyright = {Apache-2.0},
	url = {https://github.com/deepmind/deepmind-research},
	abstract = {This repository contains implementations and illustrative code to accompany DeepMind publications},
	urldate = {2021-06-08},
	publisher = {DeepMind},
	month = jun,
	year = {2021},
	note = {original-date: 2019-01-15T09:54:13Z},
	keywords = {CURL},
}

@misc{taylor_camerontaylorflstam_2021,
	title = {{CameronTaylorFL}/stam},
	url = {https://github.com/CameronTaylorFL/stam},
	abstract = {STAM Code},
	urldate = {2021-06-08},
	author = {Taylor, Cameron},
	month = may,
	year = {2021},
	note = {original-date: 2021-05-10T16:45:59Z},
}

@article{von_luxburg_tutorial_2007,
	title = {A tutorial on spectral clustering},
	volume = {17},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-007-9033-z},
	doi = {10.1007/s11222-007-9033-z},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved eÔ¨Éciently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the Ô¨Årst glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe diÔ¨Äerent graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several diÔ¨Äerent approaches. Advantages and disadvantages of the diÔ¨Äerent spectral clustering algorithms are discussed.},
	language = {en},
	number = {4},
	urldate = {2021-06-07},
	journal = {Statistics and Computing},
	author = {von Luxburg, Ulrike},
	month = dec,
	year = {2007},
	keywords = {tutorial},
	pages = {395--416},
	file = {von Luxburg - 2007 - A tutorial on spectral clustering.pdf:/Users/nicolas/Zotero/storage/CYYXLREJ/von Luxburg - 2007 - A tutorial on spectral clustering.pdf:application/pdf},
}

@article{smith_unsupervised_2021,
	title = {Unsupervised {Progressive} {Learning} and the {STAM} {Architecture}},
	url = {http://arxiv.org/abs/1904.02021},
	abstract = {We first pose the Unsupervised Progressive Learning (UPL) problem: an online representation learning problem in which the learner observes a non-stationary and unlabeled data stream, learning a growing number of features that persist over time even though the data is not stored or replayed. To solve the UPL problem we propose the Self-Taught Associative Memory (STAM) architecture. Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical features rather than specific examples. We evaluate STAM representations using clustering and classification tasks. While there are no existing learning scenarios that are directly comparable to UPL, we compare the STAM architecture with two recent continual learning models, Memory Aware Synapses (MAS) and Gradient Episodic Memories (GEM), after adapting them in the UPL setting.},
	urldate = {2021-06-07},
	journal = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)},
	author = {Smith, James and Taylor, Cameron and Baer, Seth and Dovrolis, Constantine},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition, I.2.6, UPL},
}

@misc{kim_puzzle_2020,
	title = {Puzzle {Mix}: {Exploiting} {Saliency} and {Local} {Statistics} for {Optimal} {Mixup}},
	shorttitle = {Puzzle {Mix}},
	url = {http://arxiv.org/abs/2009.06962},
	doi = {10.48550/arXiv.2009.06962},
	abstract = {While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets. The source code is available at https://github.com/snu-mllab/PuzzleMix.},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Kim, Jang-Hyun and Choo, Wonho and Song, Hyun Oh},
	month = dec,
	year = {2020},
	note = {arXiv:2009.06962 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/F3ZEKEHL/2009.html:text/html;Kim et al_2020_Puzzle Mix.pdf:/Users/nicolas/Documents/Zotero/Kim et al_2020_Puzzle Mix.pdf:application/pdf},
}

@misc{walawalkar_attentive_2020,
	title = {Attentive {CutMix}: {An} {Enhanced} {Data} {Augmentation} {Approach} for {Deep} {Learning} {Based} {Image} {Classification}},
	shorttitle = {Attentive {CutMix}},
	url = {http://arxiv.org/abs/2003.13048},
	abstract = {Convolutional neural networks (CNN) are capable of learning robust representation with different regularization methods and activations as convolutional layers are spatially correlated. Based on this property, a large variety of regional dropout strategies have been proposed, such as Cutout [1], DropBlock [2], CutMix [3], etc. These methods aim to promote the network to generalize better by partially occluding the discriminative parts of objects. However, all of them perform this operation randomly, without capturing the most important region(s) within an object. In this paper, we propose Attentive CutMix, a naturally enhanced augmentation strategy based on CutMix [3]. In each training iteration, we choose the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image. Our proposed method is simple yet effective, easy to implement and can boost the baseline signiÔ¨Åcantly. Extensive experiments on CIFAR-10/100 datasets with various CNN architectures (in a uniÔ¨Åed setting) demonstrate the effectiveness of our proposed method, which consistently outperforms the baseline CutMix and other methods by a signiÔ¨Åcant margin.},
	language = {en},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Walawalkar, Devesh and Shen, Zhiqiang and Liu, Zechun and Savvides, Marios},
	month = apr,
	year = {2020},
	note = {arXiv:2003.13048 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Walawalkar et al. - 2020 - Attentive CutMix An Enhanced Data Augmentation Ap.pdf:/Users/nicolas/Zotero/storage/9VXB6I7F/Walawalkar et al. - 2020 - Attentive CutMix An Enhanced Data Augmentation Ap.pdf:application/pdf},
}

@misc{walawalkar_attentive_2020-1,
	title = {Attentive {CutMix}: {An} {Enhanced} {Data} {Augmentation} {Approach} for {Deep} {Learning} {Based} {Image} {Classification}},
	shorttitle = {Attentive {CutMix}},
	url = {http://arxiv.org/abs/2003.13048},
	doi = {10.48550/arXiv.2003.13048},
	abstract = {Convolutional neural networks (CNN) are capable of learning robust representation with different regularization methods and activations as convolutional layers are spatially correlated. Based on this property, a large variety of regional dropout strategies have been proposed, such as Cutout, DropBlock, CutMix, etc. These methods aim to promote the network to generalize better by partially occluding the discriminative parts of objects. However, all of them perform this operation randomly, without capturing the most important region(s) within an object. In this paper, we propose Attentive CutMix, a naturally enhanced augmentation strategy based on CutMix. In each training iteration, we choose the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image. Our proposed method is simple yet effective, easy to implement and can boost the baseline significantly. Extensive experiments on CIFAR-10/100, ImageNet datasets with various CNN architectures (in a unified setting) demonstrate the effectiveness of our proposed method, which consistently outperforms the baseline CutMix and other methods by a significant margin.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Walawalkar, Devesh and Shen, Zhiqiang and Liu, Zechun and Savvides, Marios},
	month = apr,
	year = {2020},
	note = {arXiv:2003.13048 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/G8MNEK5Z/Walawalkar et al. - 2020 - Attentive CutMix An Enhanced Data Augmentation Ap.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/2F5RRJUU/2003.html:text/html},
}

@misc{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	url = {http://arxiv.org/abs/1710.09412},
	doi = {10.48550/arXiv.1710.09412},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	year = {2018},
	note = {arXiv:1710.09412},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/XNE96JX3/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/UHAR9RV8/1710.html:text/html},
}

@misc{yu_scale_2022-1,
	title = {{SCALE}: {Online} {Self}-{Supervised} {Lifelong} {Learning} without {Prior} {Knowledge}},
	shorttitle = {{SCALE}},
	url = {http://arxiv.org/abs/2208.11266},
	doi = {10.48550/arXiv.2208.11266},
	abstract = {Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries) which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios and the current studies, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. We conduct preliminary analyses and show that existing approaches fail to learn useful information in this setup. To address the challenges, we propose Self-Supervised ContrAstive Lifelong LEarning without Prior Knowledge (SCALE) which can extract and memorize representations on-the-fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgetting loss, and an online memory update for uniform subset selection. All three components are designed to work collaboratively to maximize learning performance. We perform comprehensive experiments of SCALE under iid and four non-iid data streams. The results show that SCALE outperforms the best state-of-the-art algorithm in all settings with improvements up to 3.83\%, 2.77\% and 5.86\% in terms of kNN accuracy on CIFAR-10, CIFAR-100, and SubImageNet datasets.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Yu, Xiaofan and Guo, Yunhui and Gao, Sicun and Rosing, Tajana},
	month = nov,
	year = {2022},
	note = {arXiv:2208.11266 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/XGZ4IP3T/Yu et al. - 2022 - SCALE Online Self-Supervised Lifelong Learning wi.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/V98SEAS4/2208.html:text/html},
}

@inproceedings{douillard_dytox_2022,
  title={Dytox: Transformers for continual learning with dynamic token expansion},
  author={Douillard, Arthur and Ram{\'e}, Alexandre and Couairon, Guillaume and Cord, Matthieu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9285--9295},
  year={2022}
}

@inproceedings{fini_self-supervised_2022,
	title = {Self-{Supervised} {Models} are {Continual} {Learners}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878593/},
	doi = {10.1109/CVPR52688.2022.00940},
	abstract = {Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings. Code: github.com/DonkeyShot21/cassle.},
	urldate = {2022-12-19},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Fini, Enrico and Da Costa, Victor G. Turrisi and Alameda-Pineda, Xavier and Ricci, Elisa and Alahari, Karteek and Mairal, Julien},
	year = {2022},
	pages = {9611--9620},
}

@article{madaan_representational_2022,
  title={Representational continuity for unsupervised continual learning},
  author={Madaan, Divyam and Yoon, Jaehong and Li, Yuanchun and Liu, Yunxin and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2110.06976},
  year={2021}
}

@article{chawla_smote_2002,
	title = {{SMOTE}: synthetic minority over-sampling technique},
	volume = {16},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy.},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
	year = {2002},
	pages = {321--357},
}

@inproceedings{yun_cutmix_2019,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}

@misc{davari_probing_2022-1,
	title = {Probing {Representation} {Forgetting} in {Supervised} and {Unsupervised} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2203.13381},
	doi = {10.48550/arXiv.2203.13381},
	abstract = {Continual Learning research typically focuses on tackling the phenomenon of catastrophic forgetting in neural networks. Catastrophic forgetting is associated with an abrupt loss of knowledge previously learned by a model when the task, or more broadly the data distribution, being trained on changes. In supervised learning problems this forgetting, resulting from a change in the model's representation, is typically measured or observed by evaluating the decrease in old task performance. However, a model's representation can change without losing knowledge about prior tasks. In this work we consider the concept of representation forgetting, observed by using the difference in performance of an optimal linear classifier before and after a new task is introduced. Using this tool we revisit a number of standard continual learning benchmarks and observe that, through this lens, model representations trained without any explicit control for forgetting often experience small representation forgetting and can sometimes be comparable to methods which explicitly control for forgetting, especially in longer task sequences. We also show that representation forgetting can lead to new insights on the effect of model capacity and loss function used in continual learning. Based on our results, we show that a simple yet competitive approach is to learn representations continually with standard supervised contrastive learning while constructing prototypes of class samples when queried on old samples.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Davari, MohammadReza and Asadi, Nader and Mudur, Sudhir and Aljundi, Rahaf and Belilovsky, Eugene},
	month = apr,
	year = {2022},
	note = {arXiv:2203.13381 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/A7NG4F7K/2203.html:text/html;Davari et al_2022_Probing Representation Forgetting in Supervised and Unsupervised Continual.pdf:/Users/nicolas/Documents/Zotero/Davari et al_2022_Probing Representation Forgetting in Supervised and Unsupervised Continual.pdf:application/pdf},
}

@misc{xie_general_2022,
	title = {General {Incremental} {Learning} with {Domain}-aware {Categorical} {Representations}},
	url = {http://arxiv.org/abs/2204.04078},
	doi = {10.48550/arXiv.2204.04078},
	abstract = {Continual learning is an important problem for achieving human-level intelligence in real-world applications as an agent must continuously accumulate knowledge in response to streaming data/tasks. In this work, we consider a general and yet under-explored incremental learning problem in which both the class distribution and class-specific domain distribution change over time. In addition to the typical challenges in class incremental learning, this setting also faces the intra-class stability-plasticity dilemma and intra-class domain imbalance problems. To address above issues, we develop a novel domain-aware continual learning method based on the EM framework. Specifically, we introduce a flexible class representation based on the von Mises-Fisher mixture model to capture the intra-class structure, using an expansion-and-reduction strategy to dynamically increase the number of components according to the class complexity. Moreover, we design a bi-level balanced memory to cope with data imbalances within and across classes, which combines with a distillation loss to achieve better inter- and intra-class stability-plasticity trade-off. We conduct exhaustive experiments on three benchmarks: iDigits, iDomainNet and iCIFAR-20. The results show that our approach consistently outperforms previous methods by a significant margin, demonstrating its superiority.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Xie, Jiangwei and Yan, Shipeng and He, Xuming},
	month = oct,
	year = {2022},
	note = {arXiv:2204.04078 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/WV85LCJE/2204.html:text/html;Xie et al_2022_General Incremental Learning with Domain-aware Categorical Representations.pdf:/Users/nicolas/Documents/Zotero/Xie et al_2022_General Incremental Learning with Domain-aware Categorical Representations.pdf:application/pdf},
}

@article{wang_continual_2022-1,
	title = {Continual {Learning} through {Retrieval} and {Imagination}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20837},
	doi = {10.1609/aaai.v36i8.20837},
	abstract = {Continual learning is an intellectual ability of artificial agents to learn new streaming labels from sequential data. The main impediment to continual learning is catastrophic forgetting, a severe performance degradation on previously learned tasks. Although simply replaying all previous data or continuously adding the model parameters could alleviate the issue, it is impractical in real-world applications due to the limited available resources. Inspired by the mechanism of the human brain to deepen its past impression, we propose a novel framework, Deep Retrieval and Imagination (DRI), which consists of two components: 1) an embedding network that constructs a unified embedding space without adding model parameters on the arrival of new tasks; and 2) a generative model to produce additional (imaginary) data based on the limited memory. By retrieving the past experiences and corresponding imaginary data, DRI distills knowledge and rebalances the embedding space to further mitigate forgetting. Theoretical analysis demonstrates that DRI can reduce the loss approximation error and improve the robustness through retrieval and imagination, bringing better generalizability to the network. Extensive experiments show that DRI performs significantly better than the existing state-of-the-art continual learning methods and effectively alleviates catastrophic forgetting.},
	language = {en},
	number = {8},
	urldate = {2022-12-20},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Zhen and Liu, Liu and Duan, Yiqun and Tao, Dacheng},
	month = jun,
	year = {2022},
	note = {Number: 8},
	keywords = {Machine Learning (ML)},
	pages = {8594--8602},
	file = {Wang et al_2022_Continual Learning through Retrieval and Imagination.pdf:/Users/nicolas/Documents/Zotero/Wang et al_2022_Continual Learning through Retrieval and Imagination.pdf:application/pdf},
}

@inproceedings{gatys_image_2016,
  title={Image style transfer using convolutional neural networks},
  author={Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2414--2423},
  year={2016}
}


@article{jackson_style_nodate,
	title = {Style {Augmentation}: {Data} {Augmentation} via {Style} {Randomization}},
	abstract = {We introduce style augmentation, a new form of data augmentation based on random style transfer, for improving the robustness of Convolutional Neural Networks (CNN) over both classiÔ¨Åcation and regression based tasks. During training, style augmentation randomizes texture, contrast and color, while preserving shape and semantic content. This is accomplished by adapting an arbitrary style transfer network to perform style randomization, by sampling target style embeddings from a multivariate normal distribution instead of computing them from a style image. In addition to standard classiÔ¨Åcation experiments, we investigate the effect of style augmentation (and data augmentation generally) on domain transfer tasks. We Ô¨Ånd that data augmentation signiÔ¨Åcantly improves robustness to domain shift, and can be used as a simple, domain agnostic alternative to domain adaptation. Comparing style augmentation against a mix of seven traditional augmentation techniques, we Ô¨Ånd that it can be readily combined with them to improve network performance. We validate the efÔ¨Åcacy of our technique with domain transfer experiments in classiÔ¨Åcation and monocular depth estimation illustrating superior performance over benchmark tasks.},
	language = {en},
	author = {Jackson, Philip T and Atapour-Abarghouei, Amir and Bonner, Stephen and Breckon, Toby P and Obara, Boguslaw},
	file = {Jackson et al. - Style Augmentation Data Augmentation via Style Ra.pdf:/Users/nicolas/Zotero/storage/RHKU9V3N/Jackson et al. - Style Augmentation Data Augmentation via Style Ra.pdf:application/pdf},
}

@inproceedings{huang_arbitrary_2017,
  title={Arbitrary style transfer in real-time with adaptive instance normalization},
  author={Huang, Xun and Belongie, Serge},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1501--1510},
  year={2017}
}


@article{krizhevsky_learning_2009,
author = {Krizhevsky, Alex},
year = {2012},
month = {05},
pages = {},
title = {Learning Multiple Layers of Features from Tiny Images},
journal = {University of Toronto}
}
@inproceedings{le_tiny_2015,
	title = {Tiny {ImageNet} {Visual} {Recognition} {Challenge}},
	url = {https://www.semanticscholar.org/paper/Tiny-ImageNet-Visual-Recognition-Challenge-Le-Yang/384ce792cf2b2afbe001f2168bfe7d5e7804c736},
	abstract = {In this work, we investigate the effect of convolutional network depth, receptive field size, dropout layers, rectified activation unit type and dataset noise on its accuracy in Tiny-ImageNet Challenge settings. In order to make a thorough evaluation of the cause of the peformance improvement, we start with a basic 5 layer model with 5√ó5 convolutional receptive fields. We keep increasing network depth or reducing receptive field size, and continue applying modern techniques, such as PReLu and dropout, to the model. Our model achieves excellent performance even compared to state-of-the-art results, with 0.444 final error rate on the test set.},
	urldate = {2022-12-20},
	author = {Le, Ya and Yang, Xuan S.},
	year = {2015},
	file = {Le_Yang_2015_Tiny ImageNet Visual Recognition Challenge.pdf:/Users/nicolas/Documents/Zotero/Le_Yang_2015_Tiny ImageNet Visual Recognition Challenge.pdf:application/pdf},
}

@article{hsu_re-evaluating_2019,
      title={Re-evaluating continual learning scenarios: A categorization and case for strong baselines},
      author={Hsu, Yen-Chang and Liu, Yen-Cheng and Ramasamy, Anita and Kira, Zsolt},
      journal={arXiv preprint arXiv:1810.12488},
      year={2018}
}


@inproceedings{he_online_2022,
	title = {Online {Continual} {Learning} via {Candidates} {Voting}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/He_Online_Continual_Learning_via_Candidates_Voting_WACV_2022_paper.html},
	language = {en},
	urldate = {2022-12-21},
	author = {He, Jiangpeng and Zhu, Fengqing},
	year = {2022},
	pages = {3154--3163},
	file = {He_Zhu_2022_Online Continual Learning via Candidates Voting.pdf:/Users/nicolas/Documents/Zotero/He_Zhu_2022_Online Continual Learning via Candidates Voting.pdf:application/pdf},
}

@misc{agarap_deep_2019,
	title = {Deep {Learning} using {Rectified} {Linear} {Units} ({ReLU})},
	url = {http://arxiv.org/abs/1803.08375},
	doi = {10.48550/arXiv.1803.08375},
	abstract = {We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer \$h\_\{n - 1\}\$ in a neural network, then multiply it by weight parameters \${\textbackslash}theta\$ to get the raw scores \$o\_\{i\}\$. Afterwards, we threshold the raw scores \$o\_\{i\}\$ by \$0\$, i.e. \$f(o) = {\textbackslash}max(0, o\_\{i\})\$, where \$f(o)\$ is the ReLU function. We provide class predictions \${\textbackslash}hat\{y\}\$ through argmax function, i.e. argmax \$f(x)\$.},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Agarap, Abien Fred},
	month = feb,
	year = {2019},
	note = {arXiv:1803.08375 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/4GUCVC2P/Agarap - 2019 - Deep Learning using Rectified Linear Units (ReLU).pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/KH5TCVDA/1803.html:text/html},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll√°r, Piotr},
	year = {2015},
	note = {arXiv:1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{nikoloutsopoulos_online_nodate,
	title = {Online {Continual} {Learning} from {Imbalanced} {Data} with {Kullback}-{Leibler}-loss based replay buffer updates},
	abstract = {We propose an online replay-based Continual Learning policy, in which the learner stores data points to a local buffer and replays it during training. The core of our contribution is a new replay buffer contents update policy that combines a Kullback-Leibler (K-L) loss and an appropriate modification of the celebrated Reservoir Sampling algorithm. The decisions at each time are, whether the newly arriving training data points will be inserted in the buffer, and which existing data points from the buffer will be substituted. We update the buffer content so that the proportion of stored data points from different classes in the buffer approximates a target distribution that depends on the empirical distribution of classes seen in the training data stream. We parameterize the target distribution with a single parameter that allows us to model different target class distributions in the buffer, such as the class distribution that is present in the training data stream, the uniform class distribution, and a distribution with class percentages that are inversely proportional to those in the training data stream. We evaluate our method on MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100, and we show that our method is superior to the state-of-the-art Reservoir Sampling algorithm. Our main finding is that the best (in terms of accuracy and forgetting) value of the parameter that determines the distribution of classes in the buffer versus that of the stream depends on statistics of the training data and on the dataset itself. Our work paves the way for further work to learn this parameter in the realistic scenario that it is unknown, thus contributing to the objective of an optimal replay-based continual learning approach that adapts to the specifics of each scenario.},
	language = {en},
	author = {Nikoloutsopoulos, Sotirios and Koutsopoulos, Iordanis and Titsias, Michalis K},
	file = {Nikoloutsopoulos et al. - Online Continual Learning from Imbalanced Data wit.pdf:/Users/nicolas/Zotero/storage/H6MYRG8W/Nikoloutsopoulos et al. - Online Continual Learning from Imbalanced Data wit.pdf:application/pdf},
}

@inproceedings{guo_online_2022,
	title = {Online {Continual} {Learning} through {Mutual} {Information} {Maximization}},
	url = {https://proceedings.mlr.press/v162/guo22g.html},
	abstract = {This paper proposed a new online continual learning approach called OCM based on mutual information (MI) maximization. It achieves two objectives that are critical in dealing with catastrophic forgetting (CF). (1) It reduces feature bias caused by cross entropy (CE) as CE learns only discriminative features for each task, but these features may not be discriminative for another task. To learn a new task well, the network parameters learned before have to be modified, which causes CF. The new approach encourages the learning of each task to make use of the full features of the task training data. (2) It encourages preservation of the previously learned knowledge when training a new batch of incrementally arriving data. Empirical evaluation shows that OCM substantially outperforms the latest online CL baselines. For example, for CIFAR10, OCM improves the accuracy of the best baseline by 13.1\% from 64.1\% (baseline) to 77.2\% (OCM).The code is publicly available at https://github.com/gydpku/OCM.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	author = {Guo, Yiduo and Liu, Bing and Zhao, Dongyan},
	year = {2022},
	pages = {8109--8126},
}

@article{zhu_class-incremental_nodate,
	title = {Class-{Incremental} {Learning} via {Dual} {Augmentation}},
	abstract = {Deep learning systems typically suffer from catastrophic forgetting of past knowledge when acquiring new skills continually. In this paper, we emphasize two dilemmas, representation bias and classiÔ¨Åer bias in class-incremental learning, and present a simple and novel approach that employs explicit class augmentation (classAug) and implicit semantic augmentation (semanAug) to address the two biases, respectively. On the one hand, we propose to address the representation bias by learning transferable and diverse representations. SpeciÔ¨Åcally, we investigate the feature representations in incremental learning based on spectral analysis and present a simple technique called classAug, to let the model see more classes during training for learning representations transferable across classes. On the other hand, to overcome the classiÔ¨Åer bias, semanAug implicitly involves the simultaneous generating of an inÔ¨Ånite number of instances of old classes in the deep feature space, which poses tighter constraints to maintain the decision boundary of previously learned classes. Without storing any old samples, our method can perform comparably with representative data replay based approaches.},
	language = {en},
	author = {Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-Lin},
	file = {Zhu et al. - Class-Incremental Learning via Dual Augmentation.pdf:/Users/nicolas/Zotero/storage/D5RZNHLL/Zhu et al. - Class-Incremental Learning via Dual Augmentation.pdf:application/pdf},
}

@inproceedings{wang_hybrid_2022,
	title = {Hybrid {Contrastive} {Quantization} for {Efficient} {Cross}-{View} {Video} {Retrieval}},
	url = {http://arxiv.org/abs/2202.03384},
	doi = {10.1145/3485447.3512022},
	abstract = {With the recent boom of video-based social platforms (e.g., YouTube and TikTok), video retrieval using sentence queries has become an important demand and attracts increasing research attention. Despite the decent performance, existing text-video retrieval models in vision and language communities are impractical for large-scale Web search because they adopt brute-force search based on highdimensional embeddings. To improve efficiency, Web search engines widely apply vector compression libraries (e.g., FAISS [26]) to post-process the learned embeddings. Unfortunately, separate compression from feature encoding degrades the robustness of representations and incurs performance decay. To pursue a better balance between performance and efficiency, we propose the first quantized representation learning method for cross-view video retrieval, namely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both coarse-grained and fine-grained quantizations with transformers, which provide complementary understandings for texts and videos and preserve comprehensive semantic information. By performing Asymmetric-Quantized Contrastive Learning (AQCL) across views, HCQ aligns texts and videos at coarse-grained and multiple fine-grained levels. This hybrid-grained learning strategy serves as strong supervision on the cross-view video quantization model, where contrastive learning at different levels can be mutually promoted. Extensive experiments on three Web video benchmark datasets demonstrate that HCQ achieves competitive performance with state-of-the-art non-compressed retrieval methods while showing high efficiency in storage and computation.},
	language = {en},
	urldate = {2023-01-29},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	author = {Wang, Jinpeng and Chen, Bin and Liao, Dongliang and Zeng, Ziyun and Li, Gongfu and Xia, Shu-Tao and Xu, Jin},
	month = apr,
	year = {2022},
	note = {arXiv:2202.03384 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Social and Information Networks, Computer Science - Multimedia},
	pages = {3020--3030},
	file = {Wang et al. - 2022 - Hybrid Contrastive Quantization for Efficient Cros.pdf:/Users/nicolas/Zotero/storage/PD9C5AZK/Wang et al. - 2022 - Hybrid Contrastive Quantization for Efficient Cros.pdf:application/pdf},
}

@article{hu_weighted_2002,
	title = {The weighted likelihood},
	volume = {30},
	issn = {03195724, 1708945X},
	url = {http://doi.wiley.com/10.2307/3316141},
	doi = {10.2307/3316141},
	language = {en},
	number = {3},
	urldate = {2023-02-11},
	journal = {Canadian Journal of Statistics},
	author = {Hu, Feifang and Zidek, James V.},
	month = sep,
	year = {2002},
	pages = {347--371},
	file = {Hu and Zidek - 2002 - The weighted likelihood.pdf:/Users/nicolas/Zotero/storage/BAH3D9PY/Hu and Zidek - 2002 - The weighted likelihood.pdf:application/pdf},
}

@article{lin_anchor_2022,
	title = {Anchor {Assisted} {Experience} {Replay} for {Online} {Class}-{Incremental} {Learning}},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2022.3219605},
	abstract = {Online class-incremental learning (OCIL) studies the problem of mitigating the phenomenon of catastrophic forgetting while learning new classes from a continuously non-stationary data stream. Existing approaches mainly constrain the updating of parameters to prevent the drift of previous classes that reflects the movement of samples in the embedding space. Although this kind of drift can be relieved to some extent by existing approaches, it is usually inevitable. Therefore, only prevention of drift is not enough, and we also need to further compensate for it. To this end, for each previous class, we exploit the sample with the smallest loss value as its anchor, which can representatively characterize the corresponding class. Based on the assistance of anchors, we present a novel Anchor Assisted Experience Replay (AAER) method that not only prevents the drift but also compensates for the inevitable drift to overcome the catastrophic forgetting. Specifically, we design a Drift-Prevention with Anchor (DPA) operation, which plays a preventive role by reducing the drift implicitly as well as encouraging the samples with the same label cluster tightly. Moreover, we propose a Drift-Compensation with Anchor (DCA) operation that contains two remedy mechanisms: one is Forward-offset which keeps embedding of previous data but estimates new classification centers; the other is just the opposite named Backward-offset, which keeps the old classification centers unchanged but updates the embedding of previous data. We conduct extensive experiments on three real-world datasets, and empirical results consistently demonstrate the superior performance of AAER over various state-of-the-art methods.},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Lin, Huiwei and Feng, Shanshan and Li, Xutao and Li, Wentao and Ye, Yunming},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Task analysis, Airplanes, Atmospheric modeling, Automobiles, Deep Learning, Image Recognition, Memory management, Neural Networks, Online Class-Incremental Learning, Reservoirs, Training},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/nicolas/Zotero/storage/FQ6K6HFS/stamp.html:text/html;Lin et al_2022_Anchor Assisted Experience Replay for Online Class-Incremental Learning.pdf:/Users/nicolas/Documents/Zotero/Lin et al_2022_Anchor Assisted Experience Replay for Online Class-Incremental Learning.pdf:application/pdf},
}

@article{liang_new_2023,
  title={New insights on relieving task-recency bias for online class incremental learning},
  author={Liang, Guoqiang and Chen, Zhaojie and Chen, Zhaoqiang and Ji, Shiyu and Zhang, Yanning},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2023},
  publisher={IEEE}
}


@incollection{chaudhry_riemannian_2018,
	title = {Riemannian {Walk} for {Incremental} {Learning}: {Understanding} {Forgetting} and {Intransigence}},
	volume = {11215},
	shorttitle = {Riemannian {Walk} for {Incremental} {Learning}},
	url = {http://arxiv.org/abs/1801.10112},
	abstract = {Incremental learning (IL) has received a lot of attention recently, however, the literature lacks a precise problem deÔ¨Ånition, proper evaluation settings, and metrics tailored speciÔ¨Åcally for the IL problem. One of the main objectives of this work is to Ô¨Åll these gaps so as to provide a common ground for better understanding of IL. The main challenge for an IL algorithm is to update the classiÔ¨Åer whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, IL also suffers from a problem we call intransigence, inability of a model to update its knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of IL algorithms. We present RWalk, a generalization of EWC++ (our efÔ¨Åcient version of EWC [7]) and Path Integral [26] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various IL algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off between forgetting and intransigence.},
	language = {en},
	urldate = {2023-03-03},
	author = {Chaudhry, Arslan and Dokania, Puneet K. and Ajanthan, Thalaiyasingam and Torr, Philip H. S.},
	year = {2018},
	doi = {10.1007/978-3-030-01252-6_33},
	note = {arXiv:1801.10112 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {556--572},
	file = {Chaudhry et al. - 2018 - Riemannian Walk for Incremental Learning Understa.pdf:/Users/nicolas/Zotero/storage/FN8CW9HL/Chaudhry et al. - 2018 - Riemannian Walk for Incremental Learning Understa.pdf:application/pdf},
}

@inproceedings{gu_not_2022,
	title = {Not {Just} {Selection}, but {Exploration}: {Online} {Class}-{Incremental} {Continual} {Learning} via {Dual} {View} {Consistency}},
	shorttitle = {Not {Just} {Selection}, but {Exploration}},
	doi = {10.1109/CVPR52688.2022.00729},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Gu, Yanan and Yang, Xu and Wei, Kun and Deng, Cheng},
	year = {2022},
	pages = {7432--7441},
}

@inproceedings{buzzega_dark_2020,
  title={Dark experience for general continual learning: a strong, simple baseline},
  author={Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and Calderara, Simone},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15920--15930},
  year={2020}
}

@inproceedings{caccia_new_2022,
  title={New insights on reducing abrupt representation change in online continual learning},
  author={Caccia, Lucas and Aljundi, Rahaf and Asadi, Nader and Tuytelaars, Tinne and Pineau, Joelle and Belilovsky, Eugene},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{zauner2010implementation,
  title={Implementation and benchmarking of perceptual image hash functions},
  author={Zauner, Christoph},
  year={2010},
  publisher={na}
}


@software{youtube-catalogue_2022,
	title = {Vitrox77/{YouTube}-Catalogue},
	url = {https://github.com/Vitrox77/YouTube-Catalogue},
	author = {J, Alexandre},
	urldate = {2024-04-03},
	date = {2022-08-31},
	note = {original-date: 2021-10-15T08:59:13Z},
}


@software{ytdlp_2024,
	title = {YT-DLP: A feature-rich command-line audio/video downloader.},
	rights = {Unlicense},
	url = {https://github.com/yt-dlp/yt-dlp},
	abstract = {A feature-rich command-line audio/video downloader},
	publisher = {yt-dlp},
	keywords = {python, sponskrub, sponsorblock, video-downloader, youtube-dl, youtube-dlc, youtube-downloader, yt-dlp},
}



@inproceedings{bang_online_2022,
  title={Online continual learning on a contaminated data stream with blurry task boundaries},
  author={Bang, Jihwan and Koh, Hyunseo and Park, Seulki and Song, Hwanjun and Ha, Jung-Woo and Choi, Jonghyun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9275--9284},
  year={2022}
}

@article{ahmed1974discrete,
  title={Discrete cosine transform},
  author={Ahmed, Nasir and Natarajan, T\_ and Rao, Kamisetty R},
  journal={IEEE transactions on Computers},
  volume={100},
  number={1},
  pages={90--93},
  year={1974},
  publisher={IEEE}
}


@inproceedings{lin2023pcr,
  title={PCR: Proxy-based contrastive replay for online class-incremental continual learning},
  author={Lin, Huiwei and Zhang, Baoquan and Feng, Shanshan and Li, Xutao and Ye, Yunming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24246--24255},
  year={2023}
}


@misc{yoon_scalable_2020,
	title = {Scalable and {Order}-robust {Continual} {Learning} with {Additive} {Parameter} {Decomposition}},
	url = {http://arxiv.org/abs/1902.09432},
	abstract = {While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, some issues remain to be tackled to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efÔ¨Åcient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a critical role (e.g. medical diagnosis). To tackle these practical challenges, we propose a novel continual learning method that is scalable as well as order-robust, which instead of learning a completely shared set of weights, represents the parameters for each task as a sum of task-shared and sparse task-adaptive parameters. With our Additive Parameter Decomposition (APD), the task-adaptive parameters for earlier tasks remain mostly unaffected, where we update them only to reÔ¨Çect the changes made to the task-shared parameters. This decomposition of parameters effectively prevents catastrophic forgetting and order-sensitivity, while being computation- and memory-efÔ¨Åcient. Further, we can achieve even better scalability with APD using hierarchical knowledge consolidation, which clusters the task-adaptive parameters to obtain hierarchically shared parameters. We validate our network with APD, APD-Net, on multiple benchmark datasets against state-of-the-art continual learning methods, which it largely outperforms in accuracy, scalability, and order-robustness.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Yoon, Jaehong and Kim, Saehoon and Yang, Eunho and Hwang, Sung Ju},
	year = {2020},
	note = {arXiv:1902.09432},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, I.2.10},
}

@article{wang_comprehensive_2023,
  title={A comprehensive survey of continual learning: Theory, method and application},
  author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}


@inproceedings{buzzega_rethinking_2020,
  title={Rethinking experience replay: a bag of tricks for continual learning},
  author={Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Calderara, Simone},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={2180--2187},
  year={2021},
  organization={IEEE}
}


@misc{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	shorttitle = {Barlow {Twins}},
	url = {http://arxiv.org/abs/2103.03230},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St√©phane},
	month = jun,
	year = {2021},
	note = {arXiv:2103.03230 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	file = {Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf:/Users/nicolas/Zotero/storage/VV6F5EZD/Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf:application/pdf},
}

@article{fisher_dispersion_1997,
	title = {Dispersion on a sphere},
	volume = {217},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1953.0064},
	doi = {10.1098/rspa.1953.0064},
	abstract = {Any topological framework requires the development of a theory of errors of characteristic and appropriate mathematical form. The paper develops a form of theory which appears to be appropriate to measurements of position on a sphere. The primary problems of estimation as applied to the true direction, and the precision of observations, are discussed in the subcases which arise. The simultaneous distribution of the amplitude and direction of the vector sum of a number of random unit vectors of given precision, is demonstrated. From this is derived the test of significance appropriate to a worker whose knowledge of precision lies entirely in the internal evidence of the sample. This is the analogue of ‚ÄòStudent‚Äôs‚Äô test in the Gaussian theory of errors. The general formulae obtained are illustrated using measurements of the direction of remanent magnetization in the directly and inversely magnetized lava flows obtained in Iceland by Mr J. Hospers.},
	number = {1130},
	urldate = {2023-04-28},
	journal = {Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
	author = {Fisher, Ronald Aylmer},
	month = jan,
	year = {1997},
	note = {Publisher: Royal Society},
	pages = {295--305},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/XYH3K4ZS/Fisher - 1997 - Dispersion on a sphere.pdf:application/pdf},
}

@misc{hasnat_von_2017,
	title = {von {Mises}-{Fisher} {Mixture} {Model}-based {Deep} learning: {Application} to {Face} {Verification}},
	shorttitle = {von {Mises}-{Fisher} {Mixture} {Model}-based {Deep} learning},
	url = {http://arxiv.org/abs/1706.04264},
	abstract = {A number of pattern recognition tasks, e.g., face veriÔ¨Åcation, can be boiled down to classiÔ¨Åcation or clustering of unit length directional feature vectors whose distance can be simply computed by their angle. In this paper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical foundation for an effective deep-learning of such directional features and derive a novel vMF Mixture Loss and its corresponding vMF deep features. The proposed vMF feature learning achieves the characteristics of discriminative learning, i.e., compacting the instances of the same class while increasing the distance of instances from different classes. Moreover, it subsumes a number of popular loss functions as well as an effective method in deep learning, namely normalization. We conduct extensive experiments on face veriÔ¨Åcation using 4 different challenging face datasets, i.e., LFW, YouTube faces, CACD and IJB-A. Results show the effectiveness and excellent generalization ability of the proposed approach as it achieves stateof-the-art results on the LFW, YouTube faces and CACD datasets and competitive results on the IJB-A dataset.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Hasnat, Md Abul and Bohn√©, Julien and Milgram, Jonathan and Gentric, St√©phane and Chen, Liming},
	year = {2017},
	note = {arXiv:1706.04264},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hasnat et al. - 2017 - von Mises-Fisher Mixture Model-based Deep learning.pdf:/Users/nicolas/Zotero/storage/29VZP44E/Hasnat et al. - 2017 - von Mises-Fisher Mixture Model-based Deep learning.pdf:application/pdf},
}

@article{wang_continual_2022-2,
	title = {Continual {Learning} through {Retrieval} and {Imagination}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20837},
	doi = {10.1609/aaai.v36i8.20837},
	abstract = {Continual learning is an intellectual ability of artificial agents to learn new streaming labels from sequential data. The main impediment to continual learning is catastrophic forgetting, a severe performance degradation on previously learned tasks. Although simply replaying all previous data or continuously adding the model parameters could alleviate the issue, it is impractical in real-world applications due to the limited available resources. Inspired by the mechanism of the human brain to deepen its past impression, we propose a novel framework, Deep Retrieval and Imagination (DRI), which consists of two components: 1) an embedding network that constructs a unified embedding space without adding model parameters on the arrival of new tasks; and 2) a generative model to produce additional (imaginary) data based on the limited memory. By retrieving the past experiences and corresponding imaginary data, DRI distills knowledge and rebalances the embedding space to further mitigate forgetting. Theoretical analysis demonstrates that DRI can reduce the loss approximation error and improve the robustness through retrieval and imagination, bringing better generalizability to the network. Extensive experiments show that DRI performs significantly better than the existing state-of-the-art continual learning methods and effectively alleviates catastrophic forgetting.},
	language = {en},
	number = {8},
	urldate = {2023-04-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Zhen and Liu, Liu and Duan, Yiqun and Tao, Dacheng},
	month = jun,
	year = {2022},
	note = {Number: 8},
	keywords = {Machine Learning (ML)},
	pages = {8594--8602},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/QVHKITZQ/Wang et al. - 2022 - Continual Learning through Retrieval and Imaginati.pdf:application/pdf},
}

@inproceedings{yu_scale_2023,
  title={Scale: Online self-supervised lifelong learning without prior knowledge},
  author={Yu, Xiaofan and Guo, Yunhui and Gao, Sicun and Rosing, Tajana},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={2484--2495},
  year={2023}
}

@misc{asao_convergence_2022,
	title = {Convergence of neural networks to {Gaussian} mixture distribution},
	url = {http://arxiv.org/abs/2204.12100},
	abstract = {We give a proof that, under relatively mild conditions, fully-connected feed-forward deep random neural networks converge to a Gaussian mixture distribution as only the width of the last hidden layer goes to inÔ¨Ånity. We conducted experiments for a simple model which supports our result. Moreover, it gives a detailed description of the convergence, namely, the growth of the last hidden layer gets the distribution closer to the Gaussian mixture, and the other layer successively get the Gaussian mixture closer to the normal distribution.},
	language = {en},
	urldate = {2023-05-04},
	publisher = {arXiv},
	author = {Asao, Yasuhiko and Sakamoto, Ryotaro and Takagi, Shiro},
	month = apr,
	year = {2022},
	note = {arXiv:2204.12100 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Probability},
	file = {Asao et al. - 2022 - Convergence of neural networks to Gaussian mixture.pdf:/Users/nicolas/Zotero/storage/5ZUWETDX/Asao et al. - 2022 - Convergence of neural networks to Gaussian mixture.pdf:application/pdf},
}

@inproceedings{zhang_tricl_2023,
	title = {{TRICL}: {Triplet} {Continual} {Learning}},
	shorttitle = {{TRICL}},
	doi = {10.1109/ICASSP49357.2023.10095851},
	abstract = {A class-incremental learning agent learns online with a neverending stream of data in one training epoch. In this setting, the agent suffers from severe catastrophic forgetting due to the absence of data from the observed classes after learning data from new classes. Besides, the prototypes rapidly become outdated as the agent adapts to new data sequentially, and the previous example embeddings spread out in an unforeseen way, which exacerbates forgetting (i.e., concept drift). Based on this observation, we propose a replay-based method, called TriCL, which gathers the embeddings near the prototype from the same class and separates the embeddings from the different class prototypes. TriCL leverages an improved triplet loss without extra arranged input data triplets. To facilitate rapid convergence between the same class samples, we design a memory update algorithm for decreasing the variance among the buffered samples from the same class. Furthermore, we make a prototype compensation strategy for preventing drift. Compared to the state-of-the-art benchmarks, the experiments demonstrate that our proposed method presents improved performance in the online class-incremental learning setting.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zhang, Xianchao and Wang, Guanglu and Zhang, Xiaotong and Liu, Han and Yin, Zhengxi and Yang, Wentao},
	month = jun,
	year = {2023},
	keywords = {Task analysis, Training, Acoustics, Benchmark testing, Online Continual Learning, Prototypes, Signal processing, Signal processing algorithms, Triplet Loss},
	pages = {1--5},
}

@inproceedings{zhang_tricl_2023-1,
	title = {{TRICL}: {Triplet} {Continual} {Learning}},
	shorttitle = {{TRICL}},
	doi = {10.1109/ICASSP49357.2023.10095851},
	abstract = {A class-incremental learning agent learns online with a neverending stream of data in one training epoch. In this setting, the agent suffers from severe catastrophic forgetting due to the absence of data from the observed classes after learning data from new classes. Besides, the prototypes rapidly become outdated as the agent adapts to new data sequentially, and the previous example embeddings spread out in an unforeseen way, which exacerbates forgetting (i.e., concept drift). Based on this observation, we propose a replay-based method, called TriCL, which gathers the embeddings near the prototype from the same class and separates the embeddings from the different class prototypes. TriCL leverages an improved triplet loss without extra arranged input data triplets. To facilitate rapid convergence between the same class samples, we design a memory update algorithm for decreasing the variance among the buffered samples from the same class. Furthermore, we make a prototype compensation strategy for preventing drift. Compared to the state-of-the-art benchmarks, the experiments demonstrate that our proposed method presents improved performance in the online class-incremental learning setting.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zhang, Xianchao and Wang, Guanglu and Zhang, Xiaotong and Liu, Han and Yin, Zhengxi and Yang, Wentao},
	month = jun,
	year = {2023},
	keywords = {Task analysis, Training, Acoustics, Benchmark testing, Online Continual Learning, Prototypes, Signal processing, Signal processing algorithms, Triplet Loss},
	pages = {1--5},
}

@article{kong_trust-region_2023,
	title = {Trust-{Region} {Adaptive} {Frequency} for {Online} {Continual} {Learning}},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-023-01775-0},
	doi = {10.1007/s11263-023-01775-0},
	abstract = {In the paradigm of online continual learning, one neural network is exposed to a sequence of tasks, where the data arrive in an online fashion and previously seen data are not accessible. Such online fashion causes insufficient learning and severe forgetting on past tasks issues, preventing a good stability-plasticity trade-off, where ideally the network is expected to have high plasticity to adapt to new tasks well and have the stability to prevent forgetting on old tasks simultaneously. To solve these issues, we propose a trust-region adaptive frequency approach, which alternates between standard-process and intra-process updates. Specifically, the standard-process replays data stored in a coreset and interleaves the data with current data, and the intra-process updates the network parameters based on the coreset. Furthermore, to improve the unsatisfactory performance stemming from online fashion, the frequency of the intra-process is adjusted based on a trust region, which is measured by the confidence score of current data. During the intra-process, we distill the dark knowledge to retain useful learned knowledge. Moreover, to store more representative data in the coreset, a confidence-based coreset selection is presented in an online manner. The experimental results on standard benchmarks show that the proposed method significantly outperforms state-of-art continual learning algorithms.},
	language = {en},
	urldate = {2023-05-10},
	journal = {International Journal of Computer Vision},
	author = {Kong, Yajing and Liu, Liu and Qiao, Maoying and Wang, Zhen and Tao, Dacheng},
	month = apr,
	year = {2023},
	keywords = {Catastrophic forgetting, Deep learning, Online continual learning, Trust-region},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/5CQ44HV4/Kong et al. - 2023 - Trust-Region Adaptive Frequency for Online Continu.pdf:application/pdf},
}

@inproceedings{wang2020understanding,
  title={Understanding contrastive representation learning through alignment and uniformity on the hypersphere},
  author={Wang, Tongzhou and Isola, Phillip},
  booktitle={International Conference on Machine Learning},
  pages={9929--9939},
  year={2020},
  organization={PMLR}
}

@inproceedings{grill_bootstrap_2020-1,
	title = {Bootstrap {Your} {Own} {Latent} - {A} {New} {Approach} to {Self}-{Supervised} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Grill, Jean-Bastien and Strub, Florian and Altch√©, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {21271--21284},
}

@inproceedings{koh_online_2023,
  title={Online Boundary-Free Continual Learning by Scheduled Data Prior},
  author={Koh, Hyunseo and Seo, Minhyuk and Bang, Jihwan and Song, Hwanjun and Hong, Deokki and Park, Seulki and Ha, Jung-Woo and Choi, Jonghyun},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{kim2020imbalanced,
  title={Imbalanced continual learning with partitioning reservoir sampling},
  author={Kim, Chris Dongjoo and Jeong, Jinseo and Kim, Gunhee},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XIII 16},
  pages={411--428},
  year={2020},
  organization={Springer}
}


@article{han_online_2022,
	title = {Online {Continual} {Learning} via the {Meta}-learning update with {Multi}-scale {Knowledge} {Distillation} and {Data} {Augmentation}},
	volume = {113},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197622001725},
	doi = {10.1016/j.engappai.2022.104966},
	abstract = {Continual learning aims to rapidly and continually learn the current task from a sequence of tasks, using the knowledge obtained in the past, while performing well on prior tasks. A key challenge in this setting is the stability‚Äìplasticity dilemma existing in current and previous tasks, i.e., a high-stability network is weak to learn new knowledge in an effort to maintain previous knowledge. Correspondingly, a high-plasticity network can easily forget old tasks while dealing with well on the new task. Compared to other kinds of methods, the methods based on experience replay have shown great advantages to overcome catastrophic forgetting. One common limitation of this method is the data imbalance between the previous and current tasks, which would further aggravate forgetting. Moreover, how to effectively address the stability‚Äìplasticity dilemma in this setting is also an urgent problem to be solved. In this paper, we overcome these challenges by proposing a novel framework called Meta-learning update via Multi-scale Knowledge Distillation and Data Augmentation (MMKDDA). Specifically, we apply multi-scale knowledge distillation to grasp the evolution of long-range and short-range spatial relationships at different feature levels to alleviate the problem of data imbalance. Besides, our method mixes the samples from the episodic memory and current task in the online continual training procedure, thus alleviating the side influence due to the change of probability distribution. Moreover, we optimize our model via the meta-learning update by resorting to the number of tasks seen previously, which is helpful to keep a better balance between stability and plasticity. Finally, our extensive experiments on four benchmark datasets show the effectiveness of the proposed MMKDDA framework against other popular baselines, and ablation studies are also conducted to further analyze the role of each component in our framework.},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Han, Ya-nan and Liu, Jian-wei},
	year = {2022},
	keywords = {Continual learning, Data augmentation, Knowledge distillation, Meta-learning, The stability‚Äìplasticity dilemma},
	pages = {104966},
}

@misc{zhou_deep_2023,
	title = {Deep {Class}-{Incremental} {Learning}: {A} {Survey}},
	shorttitle = {Deep {Class}-{Incremental} {Learning}},
	url = {http://arxiv.org/abs/2302.03648},
	doi = {10.48550/arXiv.2302.03648},
	abstract = {Deep models, e.g., CNNs and Vision Transformers, have achieved impressive achievements in many vision tasks in the closed world. However, novel classes emerge from time to time in our ever-changing world, requiring a learning system to acquire new knowledge continually. For example, a robot needs to understand new instructions, and an opinion monitoring system should analyze emerging topics every day. Class-Incremental Learning (CIL) enables the learner to incorporate the knowledge of new classes incrementally and build a universal classifier among all seen classes. Correspondingly, when directly training the model with new class instances, a fatal problem occurs -- the model tends to catastrophically forget the characteristics of former ones, and its performance drastically degrades. There have been numerous efforts to tackle catastrophic forgetting in the machine learning community. In this paper, we survey comprehensively recent advances in deep class-incremental learning and summarize these methods from three aspects, i.e., data-centric, model-centric, and algorithm-centric. We also provide a rigorous and unified evaluation of 16 methods in benchmark image classification tasks to find out the characteristics of different algorithms empirically. Furthermore, we notice that the current comparison protocol ignores the influence of memory budget in model storage, which may result in unfair comparison and biased results. Hence, we advocate fair comparison by aligning the memory budget in evaluation, as well as several memory-agnostic performance measures. The source code to reproduce these evaluations is available at https://github.com/zhoudw-zdw/CIL\_Survey/},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhou, Da-Wei and Wang, Qi-Wei and Qi, Zhi-Hong and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
	year = {2023},
	note = {arXiv:2302.03648},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/CSTZTI3M/Zhou et al. - 2023 - Deep Class-Incremental Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/GHQTY5KH/2302.html:text/html},
}

@misc{zhou_online_2022,
	title = {Online {Continual} {Adaptation} with {Active} {Self}-{Training}},
	url = {http://arxiv.org/abs/2106.06526},
	doi = {10.48550/arXiv.2106.06526},
	abstract = {Models trained with offline data often suffer from continual distribution shifts and expensive labeling in changing environments. This calls for a new online learning paradigm where the learner can continually adapt to changing environments with limited labels. In this paper, we propose a new online setting -- Online Active Continual Adaptation, where the learner aims to continually adapt to changing distributions using both unlabeled samples and active queries of limited labels. To this end, we propose Online Self-Adaptive Mirror Descent (OSAMD), which adopts an online teacher-student structure to enable online self-training from unlabeled data, and a margin-based criterion that decides whether to query the labels to track changing distributions. Theoretically, we show that, in the separable case, OSAMD has an \$O(\{T\}{\textasciicircum}\{2/3\})\$ dynamic regret bound under mild assumptions, which is aligned with the \${\textbackslash}Omega(T{\textasciicircum}\{2/3\})\$ lower bound of online learning algorithms with full labels. In the general case, we show a regret bound of \$O(\{T\}{\textasciicircum}\{2/3\} + {\textbackslash}alpha{\textasciicircum}* T)\$, where \${\textbackslash}alpha{\textasciicircum}*\$ denotes the separability of domains and is usually small. Our theoretical results show that OSAMD can fast adapt to changing environments with active queries. Empirically, we demonstrate that OSAMD achieves favorable regrets under changing environments with limited labels on both simulated and real-world data, which corroborates our theoretical findings.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhou, Shiji and Zhao, Han and Zhang, Shanghang and Wang, Lianzhe and Chang, Heng and Wang, Zhi and Zhu, Wenwu},
	month = mar,
	year = {2022},
	note = {arXiv:2106.06526 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/TRTS23HB/Zhou et al. - 2022 - Online Continual Adaptation with Active Self-Train.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/V2AG27KT/2106.html:text/html},
}

@misc{zhao_maintaining_2019,
	title = {Maintaining {Discrimination} and {Fairness} in {Class} {Incremental} {Learning}},
	url = {http://arxiv.org/abs/1911.07053},
	abstract = {Deep neural networks (DNNs) have been applied in class incremental learning, which aims to solve common realworld problems of learning new classes continually. One drawback of standard DNNs is that they are prone to catastrophic forgetting. Knowledge distillation (KD) is a commonly used technique to alleviate this problem. In this paper, we demonstrate it can indeed help the model to output more discriminative results within old classes. However, it cannot alleviate the problem that the model tends to classify objects into new classes, causing the positive effect of KD to be hidden and limited. We observed that an important factor causing catastrophic forgetting is that the weights in the last fully connected (FC) layer are highly biased in class incremental learning. In this paper, we propose a simple and effective solution motivated by the aforementioned observations to address catastrophic forgetting. Firstly, we utilize KD to maintain the discrimination within old classes. Then, to further maintain the fairness between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal training process. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings. Experimental results show that the proposed method can effectively alleviate catastrophic forgetting and signiÔ¨Åcantly outperform state-of-the-art methods.},
	language = {en},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhao, Bowen and Xiao, Xi and Gan, Guojun and Zhang, Bin and Xia, Shutao},
	month = nov,
	year = {2019},
	note = {arXiv:1911.07053 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhao et al. - 2019 - Maintaining Discrimination and Fairness in Class I.pdf:/Users/nicolas/Zotero/storage/UFMSZX4G/Zhao et al. - 2019 - Maintaining Discrimination and Fairness in Class I.pdf:application/pdf},
}

@misc{zhou_model_2023,
	title = {A {Model} or 603 {Exemplars}: {Towards} {Memory}-{Efficient} {Class}-{Incremental} {Learning}},
	shorttitle = {A {Model} or 603 {Exemplars}},
	url = {http://arxiv.org/abs/2205.13218},
	abstract = {Real-world applications require the classification model to adapt to new classes without forgetting old ones. Correspondingly, Class-Incremental Learning (CIL) aims to train a model with limited memory size to meet this requirement. Typical CIL methods tend to save representative exemplars from former classes to resist forgetting, while recent works find that storing models from history can substantially boost the performance. However, the stored models are not counted into the memory budget, which implicitly results in unfair comparisons. We find that when counting the model size into the total budget and comparing methods with aligned memory size, saving models do not consistently work, especially for the case with limited memory budgets. As a result, we need to holistically evaluate different CIL methods at different memory scales and simultaneously consider accuracy and memory size for measurement. On the other hand, we dive deeply into the construction of the memory buffer for memory efficiency. By analyzing the effect of different layers in the network, we find that shallow and deep layers have different characteristics in CIL. Motivated by this, we propose a simple yet effective baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends specialized layers based on the shared generalized representations, efficiently extracting diverse representations with modest cost and maintaining representative exemplars. Extensive experiments on benchmark datasets validate MEMO's competitive performance. Code is available at: https://github.com/wangkiw/ICLR23-MEMO},
	language = {en},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhou, Da-Wei and Wang, Qi-Wei and Ye, Han-Jia and Zhan, De-Chuan},
	month = feb,
	year = {2023},
	note = {arXiv:2205.13218 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhou et al. - 2023 - A Model or 603 Exemplars Towards Memory-Efficient.pdf:/Users/nicolas/Zotero/storage/3SSE8E55/Zhou et al. - 2023 - A Model or 603 Exemplars Towards Memory-Efficient.pdf:application/pdf},
}

@inproceedings{wu_large_2019,
  title={Large scale incremental learning},
  author={Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={374--382},
  year={2019}
}


@misc{zhou_model_2023-1,
	title = {A {Model} or 603 {Exemplars}: {Towards} {Memory}-{Efficient} {Class}-{Incremental} {Learning}},
	shorttitle = {A {Model} or 603 {Exemplars}},
	url = {http://arxiv.org/abs/2205.13218},
	abstract = {Real-world applications require the classification model to adapt to new classes without forgetting old ones. Correspondingly, Class-Incremental Learning (CIL) aims to train a model with limited memory size to meet this requirement. Typical CIL methods tend to save representative exemplars from former classes to resist forgetting, while recent works find that storing models from history can substantially boost the performance. However, the stored models are not counted into the memory budget, which implicitly results in unfair comparisons. We find that when counting the model size into the total budget and comparing methods with aligned memory size, saving models do not consistently work, especially for the case with limited memory budgets. As a result, we need to holistically evaluate different CIL methods at different memory scales and simultaneously consider accuracy and memory size for measurement. On the other hand, we dive deeply into the construction of the memory buffer for memory efficiency. By analyzing the effect of different layers in the network, we find that shallow and deep layers have different characteristics in CIL. Motivated by this, we propose a simple yet effective baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends specialized layers based on the shared generalized representations, efficiently extracting diverse representations with modest cost and maintaining representative exemplars. Extensive experiments on benchmark datasets validate MEMO's competitive performance. Code is available at: https://github.com/wangkiw/ICLR23-MEMO},
	language = {en},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhou, Da-Wei and Wang, Qi-Wei and Ye, Han-Jia and Zhan, De-Chuan},
	month = feb,
	year = {2023},
	note = {arXiv:2205.13218 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhou et al. - 2023 - A Model or 603 Exemplars Towards Memory-Efficient.pdf:/Users/nicolas/Zotero/storage/3AZ2SX92/Zhou et al. - 2023 - A Model or 603 Exemplars Towards Memory-Efficient.pdf:application/pdf},
}

@article{he_incremental_nodate,
	title = {Incremental {Learning} in {Online} {Scenario}},
	abstract = {Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-speciÔ¨Åc data. However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental learning methods require a long time to train the model whenever new classes are added and none of them takes into consideration the new observations of old classes. In this paper, we propose an incremental learning framework that can work in the challenging online learning scenario and handle both new classes data and new observations of old classes. We address problem (1) in online mode by introducing a modiÔ¨Åed cross-distillation loss together with a two-step learning technique. Our method outperforms the results obtained from current state-of-the-art ofÔ¨Çine incremental learning methods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the same experiment protocol but in online scenario. We also provide a simple yet effective method to mitigate problem (2) by updating exemplar set using the feature of each new observation of old classes and demonstrate a real life application of online food image classiÔ¨Åcation based on our complete framework using the Food-101 dataset.},
	language = {en},
	author = {He, Jiangpeng and Mao, Runyu and Shao, Zeman and Zhu, Fengqing},
	file = {He et al. - Incremental Learning in Online Scenario.pdf:/Users/nicolas/Zotero/storage/FUPT5X28/He et al. - Incremental Learning in Online Scenario.pdf:application/pdf},
}

@article{he_incremental_nodate-1,
	title = {Incremental {Learning} in {Online} {Scenario}},
	abstract = {Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-speciÔ¨Åc data. However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental learning methods require a long time to train the model whenever new classes are added and none of them takes into consideration the new observations of old classes. In this paper, we propose an incremental learning framework that can work in the challenging online learning scenario and handle both new classes data and new observations of old classes. We address problem (1) in online mode by introducing a modiÔ¨Åed cross-distillation loss together with a two-step learning technique. Our method outperforms the results obtained from current state-of-the-art ofÔ¨Çine incremental learning methods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the same experiment protocol but in online scenario. We also provide a simple yet effective method to mitigate problem (2) by updating exemplar set using the feature of each new observation of old classes and demonstrate a real life application of online food image classiÔ¨Åcation based on our complete framework using the Food-101 dataset.},
	language = {en},
	author = {He, Jiangpeng and Mao, Runyu and Shao, Zeman and Zhu, Fengqing},
	file = {He et al. - Incremental Learning in Online Scenario.pdf:/Users/nicolas/Zotero/storage/SHYJH3P4/He et al. - Incremental Learning in Online Scenario.pdf:application/pdf},
}

@misc{jung_new_2023,
	title = {New {Insights} for the {Stability}-{Plasticity} {Dilemma} in {Online} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2302.08741},
	doi = {10.48550/arXiv.2302.08741},
	abstract = {The aim of continual learning is to learn new tasks continuously (i.e., plasticity) without forgetting previously learned knowledge from old tasks (i.e., stability). In the scenario of online continual learning, wherein data comes strictly in a streaming manner, the plasticity of online continual learning is more vulnerable than offline continual learning because the training signal that can be obtained from a single data point is limited. To overcome the stability-plasticity dilemma in online continual learning, we propose an online continual learning framework named multi-scale feature adaptation network (MuFAN) that utilizes a richer context encoding extracted from different levels of a pre-trained network. Additionally, we introduce a novel structure-wise distillation loss and replace the commonly used batch normalization layer with a newly proposed stability-plasticity normalization module to train MuFAN that simultaneously maintains high plasticity and stability. MuFAN outperforms other state-of-the-art continual learning methods on the SVHN, CIFAR100, miniImageNet, and CORe50 datasets. Extensive experiments and ablation studies validate the significance and scalability of each proposed component: 1) multi-scale feature maps from a pre-trained encoder, 2) the structure-wise distillation loss, and 3) the stability-plasticity normalization module in MuFAN. Code is publicly available at https://github.com/whitesnowdrop/MuFAN.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Jung, Dahuin and Lee, Dongjin and Hong, Sunwon and Jang, Hyemi and Bae, Ho and Yoon, Sungroh},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08741 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/2XD7SH22/Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Q8Y3AM5Q/2302.html:text/html},
}

@article{zhang_simple_2022,
  title={A simple but strong baseline for online continual learning: Repeated augmented rehearsal},
  author={Zhang, Yaqian and Pfahringer, Bernhard and Frank, Eibe and Bifet, Albert and Lim, Nick Jin Sean and Jia, Yunzhe},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14771--14783},
  year={2022}
}

@misc{zhang_simple_2022-1,
	title = {A simple but strong baseline for online continual learning: {Repeated} {Augmented} {Rehearsal}},
	shorttitle = {A simple but strong baseline for online continual learning},
	url = {http://arxiv.org/abs/2209.13917},
	abstract = {Online continual learning (OCL) aims to train neural networks incrementally from a non-stationary data stream with a single pass through data. Rehearsal-based methods attempt to approximate the observed input distributions over time with a small memory and revisit them later to avoid forgetting. Despite their strong empirical performance, rehearsal methods still suffer from a poor approximation of past data‚Äôs loss landscape with memory samples. This paper revisits the rehearsal dynamics in online settings. We provide theoretical insights on the inherent memory overÔ¨Åtting risk from the viewpoint of biased and dynamic empirical risk minimization, and examine the merits and limits of repeated rehearsal. Inspired by our analysis, a simple and intuitive baseline, repeated augmented rehearsal (RAR), is designed to address the underÔ¨Åtting-overÔ¨Åtting dilemma of online rehearsal. Surprisingly, across four rather different OCL benchmarks, this simple baseline outperforms vanilla rehearsal by 9\%-17\% and also signiÔ¨Åcantly improves the state-of-the-art rehearsal-based methods MIR, ASER, and SCR. We also demonstrate that RAR successfully achieves an accurate approximation of the loss landscape of past data and high-loss ridge aversion in its learning trajectory. Extensive ablation studies are conducted to study the interplay between repeated and augmented rehearsal, and reinforcement learning (RL) is applied to dynamically adjust the hyperparameters of RAR to balance the stability-plasticity trade-off online. Code is available at https://github.com/YaqianZhang/RepeatedAugmentedRehearsal.},
	language = {en},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Zhang, Yaqian and Pfahringer, Bernhard and Frank, Eibe and Bifet, Albert and Lim, Nick Jin Sean and Jia, Yunzhe},
	month = nov,
	year = {2022},
	note = {arXiv:2209.13917 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Zhang et al. - 2022 - A simple but strong baseline for online continual .pdf:/Users/nicolas/Zotero/storage/4R6GKD4E/Zhang et al. - 2022 - A simple but strong baseline for online continual .pdf:application/pdf},
}

@article{mathieu2015deep,
  title={Deep multi-scale video prediction beyond mean square error},
  author={Mathieu, Michael and Couprie, Camille and LeCun, Yann},
  journal={arXiv preprint arXiv:1511.05440},
  year={2015}
}

@article{ouali2020overview,
  title={An overview of deep semi-supervised learning},
  author={Ouali, Yassine and Hudelot, C{\'e}line and Tami, Myriam},
  journal={arXiv preprint arXiv:2006.05278},
  year={2020}
}


@article{belouadah2021comprehensive,
  title={A comprehensive study of class incremental learning algorithms for visual tasks},
  author={Belouadah, Eden and Popescu, Adrian and Kanellos, Ioannis},
  journal={Neural Networks},
  volume={135},
  pages={38--54},
  year={2021},
  publisher={Elsevier}
}


@article{verwimp2023continual,
  title={Continual learning: Applications and the road forward},
  author={Verwimp, Eli and Ben-David, Shai and Bethge, Matthias and Cossu, Andrea and Gepperth, Alexander and Hayes, Tyler L and H{\"u}llermeier, Eyke and Kanan, Christopher and Kudithipudi, Dhireesha and Lampert, Christoph H and others},
  journal={arXiv preprint arXiv:2311.11908},
  year={2023}
}
@inproceedings{aggarwal2020active,
  title={Active learning for imbalanced datasets},
  author={Aggarwal, Umang and Popescu, Adrian and Hudelot, C{\'e}line},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1428--1437},
  year={2020}
}

@misc{zhao_decoupled_2022,
	title = {Decoupled {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2203.08679},
	doi = {10.48550/arXiv.2203.08679},
	abstract = {State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we reformulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the "difficulty" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megvii-research/mdistiller.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
	month = jul,
	year = {2022},
	note = {arXiv:2203.08679 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/GTLMD248/Zhao et al. - 2022 - Decoupled Knowledge Distillation.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/B2XQ5PGC/2203.html:text/html},
}

@misc{sachdeva_data_2023,
	title = {Data {Distillation}: {A} {Survey}},
	shorttitle = {Data {Distillation}},
	url = {http://arxiv.org/abs/2301.04272},
	abstract = {The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as eÔ¨Äective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for diÔ¨Äerent data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.},
	language = {en},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Sachdeva, Noveen and McAuley, Julian},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04272 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
	file = {Sachdeva and McAuley - 2023 - Data Distillation A Survey.pdf:/Users/nicolas/Zotero/storage/DC2D2E7Y/Sachdeva and McAuley - 2023 - Data Distillation A Survey.pdf:application/pdf},
}

@misc{zhao_decoupled_2022-1,
	title = {Decoupled {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2203.08679},
	doi = {10.48550/arXiv.2203.08679},
	abstract = {State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we reformulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the "difficulty" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megvii-research/mdistiller.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
	month = jul,
	year = {2022},
	note = {arXiv:2203.08679 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/SAACCV9B/Zhao et al. - 2022 - Decoupled Knowledge Distillation.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/WJ83EVFK/2203.html:text/html},
}

@article{dong_class-incremental_2023,
	title = {Class-incremental object detection},
	volume = {139},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320323001887},
	doi = {10.1016/j.patcog.2023.109488},
	abstract = {Deep learning architectures have shown remarkable results in the object detection task. However, they experience a critical performance drop when they are required to learn new classes incrementally without forgetting old ones. This catastrophic forgetting phenomenon impedes the deployment of artificial intelligence in real word scenarios where systems need to learn new and different representations over time. Recently, many incremental learning methods have been proposed to avoid the catastrophic forgetting problem. However, current state-of-the-art class-incremental learning strategies aim at preserving the knowledge of old classes while learning new ones sequentially, which would encounter other problems as follows: (1) In the process of preserving information of old classes, only a small portion of data in the previous tasks are kept and replayed during training, which inevitably incurs bias that is favorable for the new classes but malicious to the old classes. (2) With the knowledge of previous classes distilled into the new model, a sub-optimal solution for the new task is obtained since the preserving process of previous classes sabotages the training of new classes. To address these issues, termed as Information Asymmetry (IA), we propose a double-head framework which preserves the knowledge of old classes and learns the knowledge of new classes separately. Specifically, we transfer the knowledge of the previous model to the current learned one for overcoming the catastrophic forgetting problem. Furthermore, considering that IA would introduce impacts on the training of the new model, we propose a Non-Affection mask to distill the knowledge of the interested regions at the feature level. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods on PASCAL VOC and MS COCO datasets.},
	language = {en},
	urldate = {2023-05-19},
	journal = {Pattern Recognition},
	author = {Dong, Na and Zhang, Yongqiang and Ding, Mingli and Bai, Yancheng},
	month = jul,
	year = {2023},
	keywords = {Deep learning, Class-incremental learning, Information asymmetry, Non-affection distillation, Object detection},
	pages = {109488},
	file = {ScienceDirect Snapshot:/Users/nicolas/Zotero/storage/UEQY6FSY/S0031320323001887.html:text/html},
}

@misc{wang_positive_2022,
	title = {Positive {Pair} {Distillation} {Considered} {Harmful}: {Continual} {Meta} {Metric} {Learning} for {Lifelong} {Object} {Re}-{Identification}},
	shorttitle = {Positive {Pair} {Distillation} {Considered} {Harmful}},
	url = {http://arxiv.org/abs/2210.01600},
	doi = {10.48550/arXiv.2210.01600},
	abstract = {Lifelong object re-identification incrementally learns from a stream of re-identification tasks. The objective is to learn a representation that can be applied to all tasks and that generalizes to previously unseen re-identification tasks. The main challenge is that at inference time the representation must generalize to previously unseen identities. To address this problem, we apply continual meta metric learning to lifelong object re-identification. To prevent forgetting of previous tasks, we use knowledge distillation and explore the roles of positive and negative pairs. Based on our observation that the distillation and metric losses are antagonistic, we propose to remove positive pairs from distillation to robustify model updates. Our method, called Distillation without Positive Pairs (DwoPP), is evaluated on extensive intra-domain experiments on person and vehicle re-identification datasets, as well as inter-domain experiments on the LReID benchmark. Our experiments demonstrate that DwoPP significantly outperforms the state-of-the-art. The code is here: https://github.com/wangkai930418/DwoPP\_code},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Wang, Kai and Wu, Chenshen and Bagdanov, Andy and Liu, Xialei and Yang, Shiqi and Jui, Shangling and van de Weijer, Joost},
	month = oct,
	year = {2022},
	note = {arXiv:2210.01600 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/7WVHHUEQ/Wang et al. - 2022 - Positive Pair Distillation Considered Harmful Con.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/IIDAW46X/2210.html:text/html},
}

@article{gou_multi-target_2023,
  title={Multi-target knowledge distillation via student self-reflection},
  author={Gou, Jianping and Xiong, Xiangshuo and Yu, Baosheng and Du, Lan and Zhan, Yibing and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={131},
  number={7},
  pages={1857--1874},
  year={2023},
  publisher={Springer}
}


@misc{verwimp_rehearsal_2021,
	title = {Rehearsal revealed: {The} limits and merits of revisiting samples in continual learning},
	shorttitle = {Rehearsal revealed},
	url = {http://arxiv.org/abs/2104.07446},
	doi = {10.48550/arXiv.2104.07446},
	abstract = {Learning from non-stationary data streams and overcoming catastrophic forgetting still poses a serious challenge for machine learning research. Rather than aiming to improve state-of-the-art, in this work we provide insight into the limits and merits of rehearsal, one of continual learning's most established methods. We hypothesize that models trained sequentially with rehearsal tend to stay in the same low-loss region after a task has finished, but are at risk of overfitting on its sample memory, hence harming generalization. We provide both conceptual and strong empirical evidence on three benchmarks for both behaviors, bringing novel insights into the dynamics of rehearsal and continual learning in general. Finally, we interpret important continual learning works in the light of our findings, allowing for a deeper understanding of their successes.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Verwimp, Eli and De Lange, Matthias and Tuytelaars, Tinne},
	month = apr,
	year = {2021},
	note = {arXiv:2104.07446 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/SC95QJ2R/2104.html:text/html;Verwimp et al_2021_Rehearsal revealed.pdf:/Users/nicolas/Documents/Zotero/Verwimp et al_2021_Rehearsal revealed.pdf:application/pdf},
}

@inproceedings{ahn_ss-il_2021,
	title = {{SS}-{IL}: {Separated} {Softmax} for {Incremental} {Learning}},
	shorttitle = {{SS}-{IL}},
	url = {https://ieeexplore.ieee.org/document/9710553/},
	doi = {10.1109/ICCV48922.2021.00088},
	language = {en},
	urldate = {2023-05-31},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Ahn, Hongjoon and Kwak, Jihwan and Lim, Subin and Bang, Hyeonsu and Kim, Hyojun and Moon, Taesup},
	month = oct,
	year = {2021},
	pages = {824--833},
}

@misc{pernici_class-incremental_2020,
	title = {Class-incremental {Learning} with {Pre}-allocated {Fixed} {Classifiers}},
	url = {http://arxiv.org/abs/2010.08657},
	abstract = {In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the Ô¨Ånal classiÔ¨Åer nodes to accommodate the new classes.},
	language = {en},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Pernici, Federico and Bruni, Matteo and Baecchi, Claudio and Turchini, Francesco and Del Bimbo, Alberto},
	month = oct,
	year = {2020},
	note = {arXiv:2010.08657 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Pernici et al. - 2020 - Class-incremental Learning with Pre-allocated Fixe.pdf:/Users/nicolas/Zotero/storage/RJ5QUHV3/Pernici et al. - 2020 - Class-incremental Learning with Pre-allocated Fixe.pdf:application/pdf},
}

@misc{pernici_class-incremental_2020-1,
	title = {Class-incremental {Learning} with {Pre}-allocated {Fixed} {Classifiers}},
	url = {http://arxiv.org/abs/2010.08657},
	abstract = {In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the Ô¨Ånal classiÔ¨Åer nodes to accommodate the new classes.},
	language = {en},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Pernici, Federico and Bruni, Matteo and Baecchi, Claudio and Turchini, Francesco and Del Bimbo, Alberto},
	month = oct,
	year = {2020},
	note = {arXiv:2010.08657 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Pernici et al. - 2020 - Class-incremental Learning with Pre-allocated Fixe.pdf:/Users/nicolas/Zotero/storage/IY7Y4TR4/Pernici et al. - 2020 - Class-incremental Learning with Pre-allocated Fixe.pdf:application/pdf},
}

@article{zhao_rethinking_nodate,
	title = {Rethinking {Gradient} {Projection} {Continual} {Learning}: {Stability} / {Plasticity} {Feature} {Space} {Decoupling}},
	abstract = {Continual learning aims to incrementally learn novel classes over time, while not forgetting the learned knowledge. Recent studies have found that learning would not forget if the updated gradient is orthogonal to the feature space. However, previous approaches require the gradient to be fully orthogonal to the whole feature space, leading to poor plasticity, as the feasible gradient direction becomes narrow when the tasks continually come, i.e., feature space is unlimitedly expanded. In this paper, we propose a space decoupling (SD) algorithm to decouple the feature space into a pair of complementary subspaces, i.e., the stability space I, and the plasticity space R. I is established by conducting space intersection between the historic and current feature space, and thus I contains more task-shared bases. R is constructed by seeking the orthogonal complementary subspace of I, and thus R mainly contains taskspecific bases. By putting distinguishing constraints on R and I, our method achieves a better balance between stability and plasticity. Extensive experiments are conducted by applying SD to gradient projection baselines, and show SD is model-agnostic and achieves SOTA results on publicly available datasets.},
	language = {en},
	author = {Zhao, Zhen and Zhang, Zhizhong and Tan, Xin and Liu, Jun and Qu, Yanyun and Xie, Yuan and Ma, Lizhuang},
	file = {Zhao et al. - Rethinking Gradient Projection Continual Learning.pdf:/Users/nicolas/Zotero/storage/X43ERQWE/Zhao et al. - Rethinking Gradient Projection Continual Learning.pdf:application/pdf},
}

@article{zhao_rethinking_nodate-1,
	title = {Rethinking {Gradient} {Projection} {Continual} {Learning}: {Stability} / {Plasticity} {Feature} {Space} {Decoupling}},
	abstract = {Continual learning aims to incrementally learn novel classes over time, while not forgetting the learned knowledge. Recent studies have found that learning would not forget if the updated gradient is orthogonal to the feature space. However, previous approaches require the gradient to be fully orthogonal to the whole feature space, leading to poor plasticity, as the feasible gradient direction becomes narrow when the tasks continually come, i.e., feature space is unlimitedly expanded. In this paper, we propose a space decoupling (SD) algorithm to decouple the feature space into a pair of complementary subspaces, i.e., the stability space I, and the plasticity space R. I is established by conducting space intersection between the historic and current feature space, and thus I contains more task-shared bases. R is constructed by seeking the orthogonal complementary subspace of I, and thus R mainly contains taskspecific bases. By putting distinguishing constraints on R and I, our method achieves a better balance between stability and plasticity. Extensive experiments are conducted by applying SD to gradient projection baselines, and show SD is model-agnostic and achieves SOTA results on publicly available datasets.},
	language = {en},
	author = {Zhao, Zhen and Zhang, Zhizhong and Tan, Xin and Liu, Jun and Qu, Yanyun and Xie, Yuan and Ma, Lizhuang},
	file = {Zhao et al. - Rethinking Gradient Projection Continual Learning.pdf:/Users/nicolas/Zotero/storage/CV9U3TFK/Zhao et al. - Rethinking Gradient Projection Continual Learning.pdf:application/pdf},
}

@inproceedings{das_weakly-supervised_2023,
	title = {Weakly-{Supervised} {Domain} {Adaptive} {Semantic} {Segmentation} {With} {Prototypical} {Contrastive} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Das_Weakly-Supervised_Domain_Adaptive_Semantic_Segmentation_With_Prototypical_Contrastive_Learning_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	author = {Das, Anurag and Xian, Yongqin and Dai, Dengxin and Schiele, Bernt},
	year = {2023},
	pages = {15434--15443},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/CGHZLVCC/Das et al. - 2023 - Weakly-Supervised Domain Adaptive Semantic Segment.pdf:application/pdf},
}

@misc{jung_new_2023-1,
	title = {New {Insights} for the {Stability}-{Plasticity} {Dilemma} in {Online} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2302.08741},
	abstract = {The aim of continual learning is to learn new tasks continuously (i.e., plasticity) without forgetting previously learned knowledge from old tasks (i.e., stability). In the scenario of online continual learning, wherein data comes strictly in a streaming manner, the plasticity of online continual learning is more vulnerable than offline continual learning because the training signal that can be obtained from a single data point is limited. To overcome the stability-plasticity dilemma in online continual learning, we propose an online continual learning framework named multi-scale feature adaptation network (MuFAN) that utilizes a richer context encoding extracted from different levels of a pre-trained network. Additionally, we introduce a novel structure-wise distillation loss and replace the commonly used batch normalization layer with a newly proposed stability-plasticity normalization module to train MuFAN that simultaneously maintains high plasticity and stability. MuFAN outperforms other state-of-the-art continual learning methods on the SVHN, CIFAR100, miniImageNet, and CORe50 datasets. Extensive experiments and ablation studies validate the significance and scalability of each proposed component: 1) multi-scale feature maps from a pre-trained encoder, 2) the structure-wise distillation loss, and 3) the stability-plasticity normalization module in MuFAN. Code is publicly available at https://github.com/whitesnowdrop/MuFAN.},
	language = {en},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Jung, Dahuin and Lee, Dongjin and Hong, Sunwon and Jang, Hyemi and Bae, Ho and Yoon, Sungroh},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08741 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:/Users/nicolas/Zotero/storage/9YIKJSMR/Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:application/pdf},
}

@misc{jung_new_2023-2,
	title = {New {Insights} for the {Stability}-{Plasticity} {Dilemma} in {Online} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2302.08741},
	abstract = {The aim of continual learning is to learn new tasks continuously (i.e., plasticity) without forgetting previously learned knowledge from old tasks (i.e., stability). In the scenario of online continual learning, wherein data comes strictly in a streaming manner, the plasticity of online continual learning is more vulnerable than offline continual learning because the training signal that can be obtained from a single data point is limited. To overcome the stability-plasticity dilemma in online continual learning, we propose an online continual learning framework named multi-scale feature adaptation network (MuFAN) that utilizes a richer context encoding extracted from different levels of a pre-trained network. Additionally, we introduce a novel structure-wise distillation loss and replace the commonly used batch normalization layer with a newly proposed stability-plasticity normalization module to train MuFAN that simultaneously maintains high plasticity and stability. MuFAN outperforms other state-of-the-art continual learning methods on the SVHN, CIFAR100, miniImageNet, and CORe50 datasets. Extensive experiments and ablation studies validate the significance and scalability of each proposed component: 1) multi-scale feature maps from a pre-trained encoder, 2) the structure-wise distillation loss, and 3) the stability-plasticity normalization module in MuFAN. Code is publicly available at https://github.com/whitesnowdrop/MuFAN.},
	language = {en},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Jung, Dahuin and Lee, Dongjin and Hong, Sunwon and Jang, Hyemi and Bae, Ho and Yoon, Sungroh},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08741 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:/Users/nicolas/Zotero/storage/KN9FX88A/Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:application/pdf},
}

@misc{gu_summarizing_2023,
	title = {Summarizing {Stream} {Data} for {Memory}-{Restricted} {Online} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2305.16645},
	doi = {10.48550/arXiv.2305.16645},
	abstract = {Replay-based methods have proved their effectiveness on online continual learning by rehearsing past samples from an auxiliary memory. With many efforts made on improving training schemes based on the memory, however, the information carried by each sample in the memory remains under-investigated. Under circumstances with restricted storage space, the informativeness of the memory becomes critical for effective replay. Although some works design specific strategies to select representative samples, by only employing original images, the storage space is still not well utilized. To this end, we propose to Summarize the knowledge from the Stream Data (SSD) into more informative samples by distilling the training characteristics of real images. Through maintaining the consistency of training gradients and relationship to the past tasks, the summarized samples are more representative for the stream data compared to the original images. Extensive experiments are conducted on multiple online continual learning benchmarks to support that the proposed SSD method significantly enhances the replay effects. We demonstrate that with limited extra computational overhead, SSD provides more than 3\% accuracy boost for sequential CIFAR-100 under extremely restricted memory buffer. The code is available in https://github.com/vimar-gu/SSD.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Gu, Jianyang and Wang, Kai and Jiang, Wei and You, Yang},
	month = may,
	year = {2023},
	note = {arXiv:2305.16645 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/LRL94QU4/Gu et al. - 2023 - Summarizing Stream Data for Memory-Restricted Onli.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/65SQYQBP/2305.html:text/html},
}

@misc{lee_class_2023,
	title = {Class {Conditional} {Gaussians} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/2305.19076},
	doi = {10.48550/arXiv.2305.19076},
	abstract = {Dealing with representation shift is one of the main problems in online continual learning. Current methods mainly solve this by reducing representation shift, but leave the classifier on top of the representation to slowly adapt, in many update steps, to the remaining representation shift, increasing forgetting. We propose DeepCCG, an empirical Bayesian approach to solve this problem. DeepCCG works by updating the posterior of a class conditional Gaussian classifier such that the classifier adapts instantly to representation shift. The use of a class conditional Gaussian classifier also enables DeepCCG to use a log conditional marginal likelihood loss to update the representation, which can be seen as a new type of replay. To perform the update to the classifier and representation, DeepCCG maintains a fixed number of examples in memory and so a key part of DeepCCG is selecting what examples to store, choosing the subset that minimises the KL divergence between the true posterior and the posterior induced by the subset. We demonstrate the performance of DeepCCG on a range of settings, including those with overlapping tasks which thus far have been under-explored. In the experiments, DeepCCG outperforms all other methods, evidencing its potential.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Lee, Thomas L. and Storkey, Amos},
	month = may,
	year = {2023},
	note = {arXiv:2305.19076 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/QMVVG88N/Lee and Storkey - 2023 - Class Conditional Gaussians for Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/TVSCWIJS/2305.html:text/html},
}

@misc{mirzadeh_improved_2019,
	title = {Improved {Knowledge} {Distillation} via {Teacher} {Assistant}},
	url = {http://arxiv.org/abs/1902.03393},
	doi = {10.48550/arXiv.1902.03393},
	abstract = {Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Mirzadeh, Seyed-Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
	month = dec,
	year = {2019},
	note = {arXiv:1902.03393 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/IQRCCTVB/1902.html:text/html;Mirzadeh et al_2019_Improved Knowledge Distillation via Teacher Assistant.pdf:/Users/nicolas/Documents/Zotero/Mirzadeh et al_2019_Improved Knowledge Distillation via Teacher Assistant.pdf:application/pdf},
}

@article{phuong_towards_nodate,
	title = {Towards {Understanding} {Knowledge} {Distillation}},
	abstract = {Knowledge distillation, i.e. one classiÔ¨Åer being trained on the outputs of another classiÔ¨Åer, is an empirically very successful technique for knowledge transfer between classiÔ¨Åers. It has even been observed that classiÔ¨Åers learn much faster and more reliably if trained with the outputs of another classiÔ¨Åer as soft labels, instead of from ground truth data. So far, however, there is no satisfactory theoretical explanation of this phenomenon. In this work, we provide the Ô¨Årst insights into the working mechanisms of distillation by studying the special case of linear and deep linear classiÔ¨Åers. SpeciÔ¨Åcally, we prove a generalization bound that establishes fast convergence of the expected risk of a distillation-trained linear classiÔ¨Åer. From the bound and its proof we extract three key factors that determine the success of distillation: data geometry ‚Äì geometric properties of the data distribution, in particular class separation, has an immediate inÔ¨Çuence on the convergence speed of the risk; optimization bias ‚Äì gradient descent optimization Ô¨Ånds a very favorable minimum of the distillation objective; and strong monotonicity ‚Äì the expected risk of the student classiÔ¨Åer always decreases when the size of the training set grows.},
	language = {en},
	author = {Phuong, Mary and Lampert, Christoph H},
	file = {Phuong and Lampert - Towards Understanding Knowledge Distillation.pdf:/Users/nicolas/Zotero/storage/7XNXZNTB/Phuong and Lampert - Towards Understanding Knowledge Distillation.pdf:application/pdf},
}

@misc{abhishek_multi-sample_2022,
	title = {Multi-{Sample} \${\textbackslash}zeta\$-mixup: {Richer}, {More} {Realistic} {Synthetic} {Samples} from a \$p\$-{Series} {Interpolant}},
	shorttitle = {Multi-{Sample} \${\textbackslash}zeta\$-mixup},
	url = {http://arxiv.org/abs/2204.03323},
	abstract = {Modern deep learning training procedures rely on model regularization techniques such as data augmentation methods, which generate training samples that increase the diversity of data and richness of label information. A popular recent method, mixup, uses convex combinations of pairs of original samples to generate new samples. However, as we show in our experiments, mixup can produce undesirable synthetic samples, where the data is sampled off the manifold and can contain incorrect labels. We propose Œ∂-mixup, a generalization of mixup with provably and demonstrably desirable properties that allows convex combinations of N ‚â• 2 samples, leading to more realistic and diverse outputs that incorporate information from N original samples by using a p-series interpolant. We show that, compared to mixup, Œ∂-mixup better preserves the intrinsic dimensionality of the original datasets, which is a desirable property for training generalizable models. Furthermore, we show that our implementation of Œ∂-mixup is faster than mixup, and extensive evaluation on controlled synthetic and 24 real-world natural and medical image classiÔ¨Åcation datasets shows that Œ∂-mixup outperforms mixup and traditional data augmentation techniques.},
	language = {en},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Abhishek, Kumar and Brown, Colin J. and Hamarneh, Ghassan},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03323 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Abhishek et al. - 2022 - Multi-Sample \$zeta\$-mixup Richer, More Realistic.pdf:/Users/nicolas/Zotero/storage/YAUAYLWY/Abhishek et al. - 2022 - Multi-Sample \$zeta\$-mixup Richer, More Realistic.pdf:application/pdf},
}

@article{nie_bilateral_nodate,
	title = {Bilateral {Memory} {Consolidation} for {Continual} {Learning}},
	abstract = {Humans are proficient at continuously acquiring and integrating new knowledge. By contrast, deep models forget catastrophically, especially when tackling highly long task sequences. Inspired by the way our brains constantly rewrite and consolidate past recollections, we propose a novel Bilateral Memory Consolidation (BiMeCo) framework that focuses on enhancing memory interaction capabilities. Specifically, BiMeCo explicitly decouples model parameters into short-term memory module and long-term memory module, responsible for representation ability of the model and generalization over all learned tasks, respectively. BiMeCo encourages dynamic interactions between two memory modules by knowledge distillation and momentum-based updating for forming generic knowledge to prevent forgetting. The proposed BiMeCo is parameterefficient and can be integrated into existing methods seamlessly. Extensive experiments on challenging benchmarks show that BiMeCo significantly improves the performance of existing continual learning methods. For example, combined with the state-of-the-art method CwD [55], BiMeCo brings in significant gains of around 2\% to 6\% while using 2x fewer parameters on CIFAR-100 under ResNet-18.},
	language = {en},
	author = {Nie, Xing and Xu, Shixiong and Liu, Xiyan and Meng, Gaofeng and Huo, Chunlei and Xiang, Shiming},
	file = {Nie et al. - Bilateral Memory Consolidation for Continual Learn.pdf:/Users/nicolas/Zotero/storage/UDYXPW6Q/Nie et al. - Bilateral Memory Consolidation for Continual Learn.pdf:application/pdf},
}

@article{zhao_few-shot_nodate,
	title = {Few-{Shot} {Class}-{Incremental} {Learning} via {Class}-{Aware} {Bilateral} {Distillation}},
	abstract = {Few-Shot Class-Incremental Learning (FSCIL) aims to continually learn novel classes based on only few training samples, which poses a more challenging task than the well-studied Class-Incremental Learning (CIL) due to data scarcity. While knowledge distillation, a prevailing technique in CIL, can alleviate the catastrophic forgetting of older classes by regularizing outputs between current and previous model, it fails to consider the overÔ¨Åtting risk of novel classes in FSCIL. To adapt the powerful distillation technique for FSCIL, we propose a novel distillation structure, by taking the unique challenge of overÔ¨Åtting into account. Concretely, we draw knowledge from two complementary teachers. One is the model trained on abundant data from base classes that carries rich general knowledge, which can be leveraged for easing the overÔ¨Åtting of current novel classes. The other is the updated model from last incremental session that contains the adapted knowledge of previous novel classes, which is used for alleviating their forgetting. To combine the guidances, an adaptive strategy conditioned on the class-wise semantic similarities is introduced. Besides, for better preserving base class knowledge when accommodating novel concepts, we adopt a two-branch network with an attention-based aggregation module to dynamically merge predictions from two complementary branches. Extensive experiments on 3 popular FSCIL datasets: mini-ImageNet, CIFAR100 and CUB200 validate the effectiveness of our method by surpassing existing works by a signiÔ¨Åcant margin. Code is available at https://github.com/LinglanZhao/BiDistFSCIL.},
	language = {en},
	author = {Zhao, Linglan and Lu, Jing and Xu, Yunlu and Cheng, Zhanzhan and Guo, Dashan and Niu, Yi and Fang, Xiangzhong},
	file = {Zhao et al. - Few-Shot Class-Incremental Learning via Class-Awar.pdf:/Users/nicolas/Zotero/storage/BHNQLAFF/Zhao et al. - Few-Shot Class-Incremental Learning via Class-Awar.pdf:application/pdf},
}

@misc{noauthor_sciencedirectcom_nodate,
	title = {{ScienceDirect}.com {\textbar} {Science}, health and medical journals, full text articles and books.},
	url = {https://www-sciencedirect-com.revproxy.esiee.fr/science/article/pii/S0020025522005576/pdfft?crasolve=1&r=7dc9d68e1af67729&ts=1687660745961&rtype=https&vrr=UKN&redir=UKN&redir_fr=UKN&redir_arc=UKN&vhash=UKN&host=d3d3LXNjaWVuY2VkaXJlY3QtY29tLnJldnByb3h5LmVzaWVlLmZy&tsoh=d3d3LXNjaWVuY2VkaXJlY3QtY29tLnJldnByb3h5LmVzaWVlLmZy&rh=d3d3LXNjaWVuY2VkaXJlY3QtY29tLnJldnByb3h5LmVzaWVlLmZy&re=X2JsYW5rXw%3D%3D&ns_h=d3d3LXNjaWVuY2VkaXJlY3QtY29tLnJldnByb3h5LmVzaWVlLmZy&ns_e=X2JsYW5rXw%3D%3D&rh_fd=rrr(n%5Ed%60i%5E%60_dm%60%5Eo(%5Ejh)m%60qkmjst)%60nd%60%60)am&tsoh_fd=rrr(n%5Ed%60i%5E%60_dm%60%5Eo(%5Ejh)m%60qkmjst)%60nd%60%60)am&iv=82794e0460cc8a04fc082d710e61711b&token=37323838356362633032393064393765663233363263343464373862653839323763623234346662366530653765333063336637396132663931373637336362666135663966316264623630333439396563346261333330356664633a333332353935353333376463343738323165363238623062&text=7ba111d972a69950622e14f6da2eab7e34cc3ca1dc57892a4f4a3c54acb78ef996fcb83024c8b51cc3a96dade25d393b49a16b70dad37ec445e0d5277d4d18f59830755a389e66f2148deb478c94aa1c98cb1e51b0798a920916f220aab350613fb364a50eaf313713373300bde4e091ebc4d20487acec160f866bb1d8d450343a641fdfb60e41eb2b4d9a36389954993378ae5cee97e01ee68bcfa1f59cd77c4c140c6cf42f59e270808dee8a7c906ad432c2a495a2b320ac52cce7cc4604a822d497dd8aeb115a8a48d5b9dbcb374f6440410110ac71e7d3c6bf69e859e54d9dec0b7d06796a6cb1c46e36b59ff325884ad2ea81ddf8702b50bb8f0e254c89a92aa524147ed43ea65a9fa58c3d9649e7d0d3d0a4e5e4079988634dfead7eedadb76c59b3a04d7c3de8e112c76d4899&original=3f6d64353d6133303664653563333732356138353564353335383861303434666632363539267069643d312d73322e302d53303032303032353532323030353537362d6d61696e2e706466},
	urldate = {2023-06-25},
}

@inproceedings{guo_dealing_2023,
  title={Dealing with Cross-Task Class Discrimination in Online Continual Learning},
  author={Guo, Yiduo and Liu, Bing and Zhao, Dongyan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11878--11887},
  year={2023}
}


@inproceedings{zhao_rethinking_2023,
	title = {Rethinking {Gradient} {Projection} {Continual} {Learning}: {Stability} / {Plasticity} {Feature} {Space} {Decoupling}},
	shorttitle = {Rethinking {Gradient} {Projection} {Continual} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Rethinking_Gradient_Projection_Continual_Learning_Stability__Plasticity_Feature_Space_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-29},
	author = {Zhao, Zhen and Zhang, Zhizhong and Tan, Xin and Liu, Jun and Qu, Yanyun and Xie, Yuan and Ma, Lizhuang},
	year = {2023},
	pages = {3718--3727},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/XTRNM7R2/Zhao et al. - 2023 - Rethinking Gradient Projection Continual Learning.pdf:application/pdf},
}

@article{huang_knowledge_2022,
  title={Knowledge distillation from a stronger teacher},
  author={Huang, Tao and You, Shan and Wang, Fei and Qian, Chen and Xu, Chang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33716--33727},
  year={2022}
}


@inproceedings{soutif--cormerais_improving_2023,
  title={Improving Online Continual Learning Performance and Stability with Temporal Ensembles},
  author={Soutif--Cormerais, Albin and Carta, Antonio and Van de Weijer, Joost},
  booktitle={Conference on Lifelong Learning Agents},
  pages={828--845},
  year={2023},
  organization={PMLR}
}


@misc{bai_recent_2021,
	title = {Recent {Advances} in {Adversarial} {Training} for {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2102.01356},
	abstract = {Adversarial training is one of the most effective approaches to defending deep learning models against adversarial examples. Unlike other defense strategies, adversarial training aims to enhance the robustness of models intrinsically. During the last few years, adversarial training has been studied and discussed from various aspects. A variety of improvements and developments of adversarial training are proposed, which were, however, neglected in existing surveys. For the Ô¨Årst time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy. Then we discuss the generalization problems in adversarial training from three perspectives and highlight the challenges which are not fully tackled. Finally, we present potential future directions.},
	language = {en},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
	month = apr,
	year = {2021},
	note = {arXiv:2102.01356 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Bai et al. - 2021 - Recent Advances in Adversarial Training for Advers.pdf:/Users/nicolas/Zotero/storage/7LNAHKTN/Bai et al. - 2021 - Recent Advances in Adversarial Training for Advers.pdf:application/pdf},
}

@misc{kwon_enhancing_2023,
	title = {Enhancing {Accuracy} and {Robustness} through {Adversarial} {Training} in {Class} {Incremental} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2305.13678},
	doi = {10.48550/arXiv.2305.13678},
	abstract = {In real life, adversarial attack to deep learning models is a fatal security issue. However, the issue has been rarely discussed in a widely used class-incremental continual learning (CICL). In this paper, we address problems of applying adversarial training to CICL, which is well-known defense method against adversarial attack. A well-known problem of CICL is class-imbalance that biases a model to the current task by a few samples of previous tasks. Meeting with the adversarial training, the imbalance causes another imbalance of attack trials over tasks. Lacking clean data of a minority class by the class-imbalance and increasing of attack trials from a majority class by the secondary imbalance, adversarial training distorts optimal decision boundaries. The distortion eventually decreases both accuracy and robustness than adversarial training. To exclude the effects, we propose a straightforward but significantly effective method, External Adversarial Training (EAT) which can be applied to methods using experience replay. This method conduct adversarial training to an auxiliary external model for the current task data at each time step, and applies generated adversarial examples to train the target model. We verify the effects on a toy problem and show significance on CICL benchmarks of image classification. We expect that the results will be used as the first baseline for robustness research of CICL.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Kwon, Minchan and Kim, Kangil},
	month = may,
	year = {2023},
	note = {arXiv:2305.13678 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/AF795TI4/2305.html:text/html;Kwon_Kim_2023_Enhancing Accuracy and Robustness through Adversarial Training in Class.pdf:/Users/nicolas/Documents/Zotero/Kwon_Kim_2023_Enhancing Accuracy and Robustness through Adversarial Training in Class.pdf:application/pdf},
}

@inproceedings{chrysakis2023online,
  title={Online Bias Correction for Task-Free Continual Learning},
  author={Chrysakis, Aristotelis and Moens, Marie-Francine},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{chrysakis2020online,
  title={Online continual learning from imbalanced data},
  author={Chrysakis, Aristotelis and Moens, Marie-Francine},
  booktitle={International Conference on Machine Learning},
  pages={1952--1961},
  year={2020},
  organization={PMLR}
}

@inproceedings{jian2022adaptive,
  title={Adaptive Feature Generation for Online Continual Learning from Imbalanced Data},
  author={Jian, Yingchun and Yi, Jinfeng and Zhang, Lijun},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  pages={276--289},
  year={2022},
  organization={Springer}
}

@article{cano2022rose,
  title={ROSE: robust online self-adjusting ensemble for continual learning on imbalanced drifting data streams},
  author={Cano, Alberto and Krawczyk, Bartosz},
  journal={Machine Learning},
  volume={111},
  number={7},
  pages={2561--2599},
  year={2022},
  publisher={Springer}
}

@article{jang2021sequential,
  title={Sequential targeting: A continual learning approach for data imbalance in text classification},
  author={Jang, Joel and Kim, Yoonjeon and Choi, Kyoungho and Suh, Sungho},
  journal={Expert Systems with Applications},
  volume={179},
  pages={115067},
  year={2021},
  publisher={Elsevier}
}


@inproceedings{rannen2017encoder,
  title={Encoder based lifelong learning},
  author={Rannen, Amal and Aljundi, Rahaf and Blaschko, Matthew B and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1320--1328},
  year={2017}
}

@article{ramachandram2017deep,
  title={Deep multimodal learning: A survey on recent advances and trends},
  author={Ramachandram, Dhanesh and Taylor, Graham W},
  journal={IEEE signal processing magazine},
  volume={34},
  number={6},
  pages={96--108},
  year={2017},
  publisher={IEEE}
}

@inproceedings{tao2020self,
  title={Self-supervised video representation learning using inter-intra contrastive framework},
  author={Tao, Li and Wang, Xueting and Yamasaki, Toshihiko},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={2193--2201},
  year={2020}
}

@article{zhang2023multi,
  title={Multi-modal contrastive mutual learning and pseudo-label re-learning for semi-supervised medical image segmentation},
  author={Zhang, Shuo and Zhang, Jiaojiao and Tian, Biao and Lukasiewicz, Thomas and Xu, Zhenghua},
  journal={Medical Image Analysis},
  volume={83},
  pages={102656},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{wei2023online,
  title={Online Prototype Learning for Online Continual Learning},
  author={Wei, Yujie and Ye, Jiaxin and Huang, Zhizhong and Zhang, Junping and Shan, Hongming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={18764--18774},
  year={2023}
}

@inproceedings{lomonaco2017core50,
  title={Core50: a new dataset and benchmark for continuous object recognition},
  author={Lomonaco, Vincenzo and Maltoni, Davide},
  booktitle={Conference on robot learning},
  pages={17--26},
  year={2017},
  organization={PMLR}
}

@article{van2022three,
  title={Three types of incremental learning},
  author={van de Ven, Gido M and Tuytelaars, Tinne and Tolias, Andreas S},
  journal={Nature Machine Intelligence},
  volume={4},
  number={12},
  pages={1185--1197},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{shannon2001mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={ACM SIGMOBILE mobile computing and communications review},
  volume={5},
  number={1},
  pages={3--55},
  year={2001},
  publisher={ACM New York, NY, USA}
}

@article{janocha2017loss,
  title={On loss functions for deep neural networks in classification},
  author={Janocha, Katarzyna and Czarnecki, Wojciech Marian},
  journal={arXiv preprint arXiv:1702.05659},
  year={2017}
}

@article{liu2017deep_softmax,
  title={Deep hyperspherical learning},
  author={Liu, Weiyang and Zhang, Yan-Ming and Li, Xingguo and Yu, Zhiding and Dai, Bo and Zhao, Tuo and Song, Le},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{bouchard2007efficient_softmax,
  title={Efficient bounds for the softmax function and applications to approximate inference in hybrid models},
  author={Bouchard, Guillaume},
  booktitle={NIPS 2007 workshop for approximate Bayesian inference in continuous/hybrid systems},
  volume={6},
  year={2007}
}

@article{de2015exploration_softmax,
  title={An exploration of softmax alternatives belonging to the spherical loss family},
  author={De Brebisson, Alexandre and Vincent, Pascal},
  journal={arXiv preprint arXiv:1511.05042},
  year={2015}
}
@article{banerjee2020exploring_softmax,
  title={Exploring alternatives to softmax function},
  author={Banerjee, Kunal and Gupta, Rishi Raj and Vyas, Karthik and Mishra, Biswajit and others},
  journal={arXiv preprint arXiv:2011.11538},
  year={2020}
}

@inproceedings{martins2016softmax,
  title={From softmax to sparsemax: A sparse model of attention and multi-label classification},
  author={Martins, Andre and Astudillo, Ramon},
  booktitle={International conference on machine learning},
  pages={1614--1623},
  year={2016},
  organization={PMLR}
}

@article{laha2018controllable,
  title={On controllable sparse alternatives to softmax},
  author={Laha, Anirban and Chemmengath, Saneem Ahmed and Agrawal, Priyanka and Khapra, Mitesh and Sankaranarayanan, Karthik and Ramaswamy, Harish G},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{agarwala2020temperature,
  title={Temperature check: theory and practice for training models with softmax-cross-entropy losses},
  author={Agarwala, Atish and Pennington, Jeffrey and Dauphin, Yann and Schoenholz, Sam},
  journal={arXiv preprint arXiv:2010.07344},
  year={2020}
}

@inproceedings{wan2018rethinking,
  title={Rethinking feature distribution for loss functions in image classification},
  author={Wan, Weitao and Zhong, Yuanyi and Li, Tianpeng and Chen, Jiansheng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9117--9126},
  year={2018}
}

@inproceedings{yan2020unknown,
  title={Unknown intent detection using Gaussian mixture model with an application to zero-shot intent classification},
  author={Yan, Guangfeng and Fan, Lu and Li, Qimai and Liu, Han and Zhang, Xiaotong and Wu, Xiao-Ming and Lam, Albert YS},
  booktitle={Proceedings of the 58th annual meeting of the association for computational linguistics},
  pages={1050--1060},
  year={2020}
}

 @book{pml1Book,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: An introduction",
 publisher = "MIT Press",
 year = 2022,
 url = "probml.ai"
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016},
	pages={178-183}
}

@InProceedings{Cho_2019_ICCV,
author = {Cho, Jang Hyun and Hariharan, Bharath},
title = {On the Efficacy of Knowledge Distillation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
year = {2019}
}

@article{platt1999probabilistic,
  title={Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods},
  author={Platt, John and others},
  journal={Advances in large margin classifiers},
  volume={10},
  number={3},
  pages={61--74},
  year={1999},
  publisher={Cambridge, MA}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International Conference on Machine Learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@inproceedings{wang2017normface,
  title={Normface: L2 hypersphere embedding for face verification},
  author={Wang, Feng and Xiang, Xiang and Cheng, Jian and Yuille, Alan Loddon},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={1041--1049},
  year={2017}
}

@inproceedings{zhang2020rbf,
  title={RBF-Softmax: Learning deep representative prototypes with radial basis function softmax},
  author={Zhang, Xiao and Zhao, Rui and Qiao, Yu and Li, Hongsheng},
  booktitle={European Conference on Computer Vision},
  pages={296--311},
  year={2020},
  organization={Springer}
}

@InProceedings{Yang_2018_CVPR,
author = {Yang, Hong-Ming and Zhang, Xu-Yao and Yin, Fei and Liu, Cheng-Lin},
title = {Robust Classification With Convolutional Prototype Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2018}
} 

@article{mettes2019hyperspherical,
  title={Hyperspherical prototype networks},
  author={Mettes, Pascal and Van der Pol, Elise and Snoek, Cees},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{Saw1973,
author = { John G.   Saw },
title = {Jacobians of singular transformations with applications to statistical distribution theory},
journal = {Communications in Statistics},
volume = {1},
number = {1},
pages = {81-91},
year  = {1973},
}

@article{Saw78,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2335278},
 abstract = {A general class of distributions on the m-sphere is introduced and some properties of the class are noted. A subclass of this general class is shown to have interesting additional properties. Simple hypothesis tests (a) for randomness on the sphere, and (b) for the direction of the modal vector in the nonuniform case are suggested. The significance level of each test statistic and the power of the test are simple to compute.},
 author = {John G. Saw},
 journal = {Biometrika},
 number = {1},
 pages = {69--73},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {A Family of Distributions on the m-Sphere and Some Hypothesis Tests},
 urldate = {2023-05-04},
 volume = {65},
 year = {1978}
}

@article{asao2022convergence,
      title={Convergence of neural networks to Gaussian mixture distribution}, 
      author={Yasuhiko Asao and Ryotaro Sakamoto and Shiro Takagi},
      year={2022},
      journal={{arXiv}:2204.12100},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{bordes_guillotine_2022,
	title = {Guillotine Regularization: Improving Deep Networks Generalization by Removing their Head},
	url = {http://arxiv.org/abs/2206.13378},
	doi = {10.48550/arXiv.2206.13378},
	shorttitle = {Guillotine Regularization},
	abstract = {One unexpected technique that emerged in recent years consists in training a Deep Network ({DN}) with a Self-Supervised Learning ({SSL}) method, and using this network on downstream tasks but with its last few layers entirely removed. This usually skimmed-over trick is actually critical for {SSL} methods to display competitive performances. For example, on {ImageNet} classification, more than 30 points of percentage can be gained that way. This is a little vexing, as one would hope that the network layer at which invariance is explicitly enforced by the {SSL} criterion during training (the last layer) should be the one to use for best generalization performance downstream. But it seems not to be, and this study sheds some light on why. This trick, which we name Guillotine Regularization ({GR}), is in fact a generically applicable form of regularization that has also been used to improve generalization performance in transfer learning scenarios. In this work, through theory and experiments, we formalize {GR} and identify the underlying reasons behind its success in {SSL} methods. Our study shows that the use of this trick is essential to {SSL} performance for two main reasons: (i) improper data-augmentations to define the positive pairs used during training, and/or (ii) suboptimal selection of the hyper-parameters of the {SSL} loss.},
	journal = {{arXiv}:2206.13378},
	publisher = {{arXiv}},
	author = {Bordes, Florian and Balestriero, Randall and Garrido, Quentin and Bardes, Adrien and Vincent, Pascal},
	urldate = {2023-04-26},
	date = {2022-06-27},
	eprinttype = {arxiv},
	eprint = {2206.13378},
	keywords = {Computer Science - Machine Learning},
    year = {2022}
}

@inproceedings{gu_dvc_2022,
	title = {Not {Just} {Selection}, but {Exploration}: {Online} {Class}-{Incremental} {Continual} {Learning} via {Dual} {View} {Consistency}},
	language = {en},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	author = {Gu, Yanan and Yang, Xu and Wei, Kun and Deng, Cheng},
	year = {2022},
	pages = {7432--7441},
}

@inproceedings{pernici_incremental_2021,
  title={Class-incremental learning with pre-allocated fixed classifiers},
  author={Pernici, Federico and Bruni, Matteo and Baecchi, Claudio and Turchini, Francesco and Del Bimbo, Alberto},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={6259--6266},
  year={2021},
  organization={IEEE}
}

@inproceedings{shim2021online,
  title={Online class-incremental continual learning with adversarial shapley value},
  author={Shim, Dongsub and Mai, Zheda and Jeong, Jihwan and Sanner, Scott and Kim, Hyunwoo and Jang, Jongseong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={9630--9638},
  year={2021}
}

@inproceedings{verwimp2021rehearsal,
  title={Rehearsal revealed: The limits and merits of revisiting samples in continual learning},
  author={Verwimp, Eli and De Lange, Matthias and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9385--9394},
  year={2021}
}

@article{anil2018large,
  title={Large scale distributed neural network training through online distillation},
  author={Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre and Ormandi, Robert and Dahl, George E and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1804.03235},
  year={2018}
}

@inproceedings{zhang2018deep,
  title={Deep mutual learning},
  author={Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2018}
}

@inproceedings{guo2020online,
  title={Online knowledge distillation via collaborative learning},
  author={Guo, Qiushan and Wang, Xinjiang and Wu, Yichao and Yu, Zhipeng and Liang, Ding and Hu, Xiaolin and Luo, Ping},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2020}
}

@inproceedings{zhu2018knowledge,
  title={Knowledge distillation by on-the-fly native ensemble},
  author={Zhu, Xiatian and Gong, Shaogang and others},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{song2018collaborative,
  title={Collaborative learning for deep neural networks},
  author={Song, Guocong and Chai, Wei},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{chaudhry2018riemannian,
  title={Riemannian walk for incremental learning: Understanding forgetting and intransigence},
  author={Chaudhry, Arslan and Dokania, Puneet K and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={532--547},
  year={2018}
}


@inproceedings{lopez2017gradient,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  booktitle={NeurIPS},
  year={2017}
}
@article{bensimhoun2009n,
  title={N-dimensional cumulative function, and other useful facts about gaussians and normal densities},
  author={Bensimhoun, Michael},
  journal={Jerusalem, Israel, Tech. Rep},
  pages={1--8},
  year={2009}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@book{mardia2009,
  title={Directional Statistics},
  author={Jupp, P.E. and Mardia, K.V.},
  isbn={9780470317815},
  series={Wiley Series in Probability and Statistics},
  url={https://books.google.fr/books?id=PTNiCm4Q-M0C},
  year={2009},
  publisher={Wiley}
}

@article{PUKKILA1988379,
title = {Pattern recognition based on scale invariant discriminant functions},
journal = {Information Sciences},
volume = {45},
number = {3},
pages = {379-389},
year = {1988},
issn = {0020-0255},
doi = {https://doi.org/10.1016/0020-0255(88)90012-6},
url = {https://www.sciencedirect.com/science/article/pii/0020025588900126},
author = {Tarmo M. Pukkila and C. {Radhakrishna Rao}},
abstract = {Some probability models for classifying individuals as belonging to one of two or more populations using scale invariant discriminant functions are considered. The investigation is motivated by practical situations where the observed data on an individual are in the form of ratios of some basic measurements or measurements scaled by an unknown nonnegative number. The probability models are obtained by considering a p-vector random variable X with a known distribution and deriving the distribution of the random vector Y = [G(X)]‚àí1 X, where G(X) is a nonnegative measure of size such that G(Œª X) = ŒªG(X) for Œª > 0. Explicit expressions are obtained for the densities of what are called angular Gaussian; compositional Gaussian, type 1; and compositional Gaussian, type 2 distributions.}
}

@article{paine2018,
    title={An elliptically symmetric angular Gaussian distribution},
    author={Paine, P. J. and Preston, S. P. and Tsagris, M. and Wood, Andrew T. A.},
    journal={Statistics and Computing},
    year={2018},
    month={05},
    pages={689--697},
    volume={28},
    number={3},
    doi={10.1007/s11222-017-9756-4}
}

@book{Gradshteyn2014,
authors = {I.S. Gradshteyn and I.M. Ryzhik},
editor = {Daniel Zwillinger and Victor Moll and I.S. Gradshteyn and I.M. Ryzhik},
title = {Table of Integrals, Series, and Products (Eighth Edition)},
publisher = {Academic Press},
edition = {Eighth Edition},
address = {Boston},
pages = {i-ii},
year = {2014},
isbn = {978-0-12-384933-5},
doi = {https://doi.org/10.1016/B978-0-12-384933-5.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012384933500014X}
}

@article{hinton2002stochastic,
  title={Stochastic neighbor embedding},
  author={Hinton, Geoffrey E and Roweis, Sam},
  journal={Advances in neural information processing systems},
  volume={15},
  year={2002}
}
@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@book{courant2011differential,
  title={Differential and Integral Calculus, Volume 2},
  author={Courant, R.},
  isbn={9781118031483},
  series={Wiley Classics Library},
  url={https://books.google.fr/books?id=F0AhmkhyBYUC},
  year={2011},
  publisher={Wiley}
}

@article{banerjee2005clustering,
  title={Clustering on the Unit Hypersphere using von Mises-Fisher Distributions.},
  author={Banerjee, Arindam and Dhillon, Inderjit S and Ghosh, Joydeep and Sra, Suvrit and Ridgeway, Greg},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={9},
  year={2005}
}

@inproceedings{zhou2024ptmsurvey,
    title={Continual learning with pre-trained models: A survey},
    author={Zhou, Da-Wei and Sun, Hai-Long and Ning, Jingyi and Ye, Han-Jia and Zhan, De-Chuan},
    booktitle={International Joint Conference on Artificial Intelligence},
    pages={8363-8371},
    year={2024}
}

@article{zeman2023superformer,
  title={SuperFormer: Continual learning superposition method for text classification},
  author={Zeman, Marko and Pucer, Jana Faganeli and Kononenko, Igor and Bosni{\'c}, Zoran},
  journal={Neural Networks},
  volume={161},
  pages={418--436},
  year={2023},
}

@article{smith2023clora,
  title={Continual diffusion: Continual customization of text-to-image diffusion with c-lora},
  author={Smith, James Seale and Hsu, Yen-Chang and Zhang, Lingyu and Hua, Ting and Kira, Zsolt and Shen, Yilin and Jin, Hongxia},
  journal={arXiv preprint arXiv:2304.06027},
  year={2023}
}

@InProceedings{Tiwari_2022_CVPR,
    author    = {Tiwari, Rishabh and Killamsetty, Krishnateja and Iyer, Rishabh and Shenoy, Pradeep},
    title     = {GCR: Gradient Coreset Based Replay Buffer Selection for Continual Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year      = {2022},
    pages     = {99-108}
}

@inproceedings{wang2024dealing,
  title={Dealing with Synthetic Data Contamination in Online Continual Learning},
  author={Wang, Maorong and Michel, Nicolas and Mao, Jiafeng and Yamasaki, Toshihiko},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}


@inproceedings{michel2024rethinking,
  title={Rethinking Momentum Knowledge Distillation in Online Continual Learning},
  author={Michel, Nicolas and Wang, Maorong and Xiao, Ling and Yamasaki, Toshihiko},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{michel2024learning,
  title={Learning Representations on the Unit Sphere: Investigating Angular Gaussian and Von Mises-Fisher Distributions for Online Continual Learning},
  author={Michel, Nicolas and Chierchia, Giovanni and Negrel, Romain and Bercher, Jean-Fran{\c{c}}ois},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={13},
  pages={14350--14358}, 
  year={2024}
}

@inproceedings{moon2023mvp,
  title={Online class incremental learning on stochastic blurry task boundary via mask and visual prompt tuning},
  author={Moon, Jun-Yeong and Park, Keon-Hee and Kim, Jung Uk and Park, Gyeong-Moon},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11731--11741},
  year={2023}
}

@article{mcdonnell2024ranpac,
  title={Ranpac: Random projections and pre-trained models for continual learning},
  author={McDonnell, Mark D and Gong, Dong and Parvaneh, Amin and Abbasnejad, Ehsan and van den Hengel, Anton},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{zhou2024ease,
  title={Expandable subspace ensemble for pre-trained model-based class-incremental learning},
  author={Zhou, Da-Wei and Sun, Hai-Long and Ye, Han-Jia and Zhan, De-Chuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23554--23564},
  year={2024}
}

@inproceedings{baydin2018hypergradient,
  title = {Online Learning Rate Adaptation with Hypergradient Descent},
  author = {Baydin, Atƒ±lƒ±m G√ºne≈ü and Cornish, Robert and Rubio, David Mart√≠nez and Schmidt, Mark and Wood, Frank},
  booktitle = {Sixth International Conference on Learning Representations},
  year = {2018}
}

@inproceedings{he2024gradient,
  title={Gradient reweighting: Towards imbalanced class-incremental learning},
  author={He, Jiangpeng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16668--16677},
  year={2024}
}

@article{mirzadeh2020understanding,
  title={Understanding the role of training regimes in continual learning},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Pascanu, Razvan and Ghasemzadeh, Hassan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7308--7320},
  year={2020}
}

@inproceedings{almeida1999parameter,
  title={Parameter adaptation in stochastic optimization},
  author={Almeida, Lu{\'\i}s B and Langlois, Thibault and Amaral, Jos{\'e} D and Plakhov, Alexander},
  booktitle={On-line learning in neural networks},
  pages={111--134},
  year={1999}
}

@inproceedings{smith2023coda,
  title={Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning},
  author={Smith, James Seale and Karlinsky, Leonid and Gutta, Vyshnavi and Cascante-Bonilla, Paola and Kim, Donghyun and Arbelle, Assaf and Panda, Rameswar and Feris, Rogerio and Kira, Zsolt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11909--11919},
  year={2023}
}

@inproceedings{wang2022l2p,
  title={Learning to prompt for continual learning},
  author={Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={139--149},
  year={2022}
}

@inproceedings{wang2022dualprompt,
  title={Dualprompt: Complementary prompting for rehearsal-free continual learning},
  author={Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and others},
  booktitle={European Conference on Computer Vision},
  pages={631--648},
  year={2022},
  organization={Springer}
}

@inproceedings{roy2024convprompt,
  title={Convolutional Prompting meets Language Models for Continual Learning},
  author={Roy, Anurag and Moulick, Riddhiman and Verma, Vinay K and Ghosh, Saptarshi and Das, Abir},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23616--23626},
  year={2024}
}

@techreport{wah2011cub,
	Title = {The caltech-ucsd birds-200-2011 dataset},
	Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	Year = {2011},
	Institution = {California Institute of Technology},
	Number = {CNS-TR-2011-001}
}

@inproceedings{hendrycks2021imagenetr,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8340--8349},
  year={2021}
}

@article{keskar2017improving,
  title={Improving generalization performance by switching from adam to sgd},
  author={Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07628},
  year={2017}
}

@inproceedings{wang2024improving,
  title={Improving Plasticity in Online Continual Learning via Collaborative Learning},
  author={Wang, Maorong and Michel, Nicolas and Xiao, Ling and Yamasaki, Toshihiko},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23460--23469},
  year={2024}
}

@inproceedings{guo2023dealing,
  title={Dealing with cross-task class discrimination in online continual learning},
  author={Guo, Yiduo and Liu, Bing and Zhao, Dongyan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11878--11887},
  year={2023}
}

@article{zhou2024revisiting,
    author = {Zhou, Da-Wei and Cai, Zi-Wen and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
    title = {Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need},
    journal = {International Journal of Computer Vision},
    year = {2024}
}