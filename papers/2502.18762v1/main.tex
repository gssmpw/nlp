%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xcolor,colortbl}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage[finalizecache]{minted}
\usepackage[frozencache, cachedir=mintedcache]{minted}
% \usepackage[]{minted}

\definecolor{Gray}{gray}{0.90}
\newcolumntype{a}{>{\columncolor{Gray}}l}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{authblk}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Online Prototypes and Class-Wise Hypergradients for Online Continual Learning with Pre-Trained Models}

\author[1,3]{Nicolas Michel}
\author[1]{Maorong Wang}
\author[2]{Jiangpeng He}
\author[1]{Toshihiko Yamasaki}

\affil[1]{Department of Information and Communication Engineering, The University of Tokyo, Tokyo, Japan}
\affil[2]{Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, USA}
\affil[3]{Japanese French Laboratory for Informatics, CNRS, Tokyo, Japan}

\date{}                     %% if you don't need date to appear
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}

\begin{document}

\twocolumn[
\title{Online Prototypes and Class-Wise Hypergradients for Online Continual Learning with Pre-Trained Models}
\maketitle
% \icmltitle{Online Continual Learning with Pre-Trained Models leveraging Prototypes and Class-Wise Hypergradients}
% \icmltitle{Continual Learning with Pre-Trained Models:\\ from Offline to Online with Prototypes and Class-Wise Hypergradients}
% \icmltitle{Prototypes and Class-Wise Hypergradients for Online Continual Learning with Pre-trained}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Nicolas Michel}{todai,jfli}
% \icmlauthor{Maorong Wang}{todai}
% \icmlauthor{Jiangpeng He}{mit}
% \icmlauthor{Toshihiko Yamasaki}{todai}
% \end{icmlauthorlist}

% \icmlaffiliation{todai}{Department of Information and Communication Engineering, University of Tokyo, Tokyo, Japan}
% \icmlaffiliation{jfli}{Japanese French Laboratory for Informatics, CNRS, Tokyo, Japan}
% \icmlaffiliation{mit}{Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA}

\icmlcorrespondingauthor{Nicolas Michel}{nicolas@cvm.t.u-tokyo.ac.jp}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Online Continual Learning, Hypergradients}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Continual Learning (CL) addresses the problem of learning from a data sequence where the distribution changes over time. Recently, efficient solutions leveraging Pre-Trained Models (PTM) have been widely explored in the offline CL (offCL) scenario, where the data corresponding to each incremental task is known beforehand and can be seen multiple times. However, such solutions often rely on 1) prior knowledge regarding task changes and 2) hyper-parameter search, particularly regarding the learning rate. Both assumptions remain unavailable in online CL (onCL) scenarios, where incoming data distribution is unknown and the model can observe each datum only once. Therefore, existing offCL strategies fall largely behind performance-wise in onCL, with some proving difficult or impossible to adapt to the online scenario. In this paper, we tackle both problems by leveraging Online Prototypes (OP) and Class-Wise Hypergradients (CWH). OP leverages stable output representations of PTM by updating its value on the fly to act as replay samples without requiring task boundaries or storing past data. CWH learns class-dependent gradient coefficients during training to improve over sub-optimal learning rates. We show through experiments that both introduced strategies allow for a consistent gain in accuracy when integrated with existing approaches. We will make the code fully available upon acceptance.
\let\thefootnote\relax\footnote{Correspondance to Nicolas Michel, nicolas@cvm.t.u-tokyo.ac.jp}
\addtocounter{footnote}{-1}\let\thefootnote\svthefootnote
\end{abstract}

\section{Introduction}
\label{sec:intro}

Continual Learning (CL) has gained significant popularity in the past decade~\cite{kirkpatrick_overcoming_2017,rao_continual_2019,zhou2024ptmsurvey}. The main idea relies on learning from a sequence of data rather than a fixed dataset. Consequently, the data distribution can change, and new classes can appear, which often leads to the well-known problem of Catastrophic Forgetting~\cite{french1999catastrophic}. In this paper, we focus on the Class Incremental Learning problem~\cite{hsu_re-evaluating_2019}.

Fundamentally, Continual Learning scenarios can be divided into two categories: \textit{offline} Continual Learning (offCL)~\cite{Tiwari_2022_CVPR} and \textit{online} Continual Learning (onCL)~\cite{mai_online_2021}. The former and most popular scenario assumes that this sequence of data is clearly defined in various tasks and that the learning process on a given task is completely analogous to traditional learning. Namely, data in each task are i.i.d. and the model can be trained for several epochs on a given task before visiting the next one. On the contrary, onCL assumes that the incoming data is analogous to a stream and therefore cannot be seen more than once by the model to enforce fast adaptation. Moreover, in the objective of moving toward more realistic scenarios, various studies now consider \textit{unclear} or \textit{blurry} task boundaries~\cite{moon2023mvp,koh_online_2023,bang_online_2022} which completely disables task information usage. Such differences make methods designed for offCL seldom transferable to onCL as the vast majority relies on leveraging multiple epochs, as well as task boundary information. Recent representation-based methods such as RANPAC~\cite{mcdonnell2024ranpac} or EASE~\cite{zhou2024ease} are good examples, which require clear task boundaries to compute the task-specific representations, making them incompatible with onCL.
While the difficulty of not having access to task information in onCL has been addressed in various studies~\cite{aljundi_task-free_2019,koh_online_2023,moon2023mvp,michel2024rethinking}, it remains under-explored when considering Pre-Trained Models (PTM).

Another significant problem when considering the online continuous adaption of a model is the inability to estimate the optimal hyper-parameters. As the dataset of interest cannot be stored, typical hyper-parameters such as the Learning Rate (LR) should not be optimized to fully reflect the problem at hand. However, such consideration has been widely omitted in current onCL literature where LR selection strategies are often unclear. A common practice is to choose the \textit{same} fixed Learning Rate (LR) and optimizer for all considered methods~\cite{gu_dvc_2022,mai_supervised_2021,moon2023mvp,lin2023pcr}, typically Stochastic Gradient Descent (SGD) with LR value of $0.1$. This design choice is extremely limited as various methods and datasets have different optimal LR. It is also well-known that a suboptimal LR has a critical impact on the final performances. Another strategy is to estimate the best hyper-parameters on a given dataset and to transfer them onto other configurations~\cite{michel2024rethinking}. While more realistic, there is no guarantee that such hyper-parameters can realistically be transferred from one dataset to another.

In this paper, we propose to improve the usage of PTM in onCL by addressing both previously introduced problems. Firstly, we solve the lack of task boundaries by simply computing Online Prototypes (OP) to recalibrate the model's final layer towards previous classes similar to the replay strategy at the representation level. Secondly, we adapt Hypergradients~\cite{baydin2018hypergradient} to onCL by considering class-wise gradients~\cite{he2024gradient}; which we call Class-Wise Hypergradients (CWH). In summary, our contributions are as follows:
\begin{itemize}
    % \item we leverage PTM in onCL by adapting flagship offCL methods;
    \item we introduce the problem of LR selection in onCL, which to the best of our knowledge remained largely unexplored;
    \item we tackle onCL difficulties by leveraging Online Prototypes and introducing Class-Wise Hypergradients;
    \item we demonstrate superior performances when combining such elements with state-of-the-art offCL methods, leveraging PTM in OCL across several datasets and initial LR.
\end{itemize}
\section{Related Work}
\label{sec:related}

\subsection{PTM based Continual Learning}
In recent years, pre-trained models (PTMs) have been widely utilized in offCL~\cite{mcdonnell2024ranpac,lin2023pcr,zhou2024ease,smith2023coda,wang2022l2p}. However, their application in onCL remains largely unexplored, partly because most existing methods heavily depend on task boundaries, i.e., explicit knowledge of when the task changes. This is often assumed in pair with \textit{clear} boundaries, meaning that all classes from the previous task are suddenly unavailable while all new classes encountered belong to the new task. With real-world streaming data, such a situation is equally unlikely.

\subsection{Online Continual Learning}
In onCL, incoming data can be seen only once, analogous to a continuous data stream~\cite{He_2020_CVPR}. Therefore, \textit{clear} boundaries are unlikely to be available and several studies suggest working in boundary-free scenarios~\cite{buzzega_dark_2020} where task change is unknown. However, if the change is \textit{clear}, it can easily be inferred. In that sense, \textit{blurry} boundary setting have been proposed~\cite{moon2023mvp,koh_online_2023,bang_online_2022,michel2024rethinking} in previous work. In particular, we are interested in the \textit{Si-Blurry} setting~\cite{moon2023mvp} where not only task change is \textit{blurry}, but some classes can appear or disappear during multiple tasks, which brings the experimental setup one step closer to real-world scenarios while being more challenging. While numerous studies rely on prototypes for Continual Learning~\cite{mcdonnell2024ranpac,lin2023pcr,zhou2024ease}, such representation-based methods must generally be combined with task boundary knowledge as prototypes are updated at the end of each task. In onCL, prototypes are harder to capitalize on when training a model from scratch due to the shift of representations hindering prototype computation~\cite{caccia_new_2022}. However, when working with PTM, such a shift is drastically reduced as representations are already of high quality, making the usage of prototypes more efficient.

\subsection{Hypergradients and Gradient Re-Weighting}
Hypergradient~\cite{baydin2018hypergradient, almeida1999parameter} addresses the problem of finding the optimal learning rate in conventional training scenarios. In that sense, the authors proposed to derive a gradient descent algorithm to learn the LR. Notably, they demonstrate that computing the dot product between gradients from previous steps $\nabla\mathcal{L}(\theta_t) \cdot \nabla\mathcal{L}(\theta_{t-1})$ is sufficient to complete one step of the learning rate update rule, with $t$ the index of the current step, $\theta$ the parameters, and $\mathcal{L}$ the loss function. However, such techniques have been, to the best of our knowledge, developed solely for offline scenarios at a global level. In Continual Learning, gradient re-weighting strategies have been designed for replay-based CL methods. Notably, previous work proposed to re-weight the gradient at the loss level to mitigate its accumulation during training in CL context, also called gradient imbalance~\cite{guo_dealing_2023}. Recently, to compensate for the class imbalance, class-wise manually defined weights in the last Fully Connected (FC) layer have been leveraged~\cite{he2024gradient}. Our work on Class-Wise Hypergradients lies at the cross-road between Hypergradients and Gradient Re-Weighting.

% \subsection{Prototypes in Continual Learning}
% The usage of prototypes in Continual Learning is rather straightforward and can be found in numerous studies~\cite{mcdonnell2024ranpac,lin2023pcr,zhou2024ease}. In offCL, such representation-based methods~\cite{zhou2024ptmsurvey} must be combined with task boundary knowledge as prototypes are updated at the end of each task. Prototypes in onCL are harder to capitalize on when training a model from scratch due to the shift of representations~\cite{caccia_new_2022}.

% \begin{itemize}
%     \item Usage of Prototypes in CL is not new, especially offline. However, such strategies, often called representation-based strategies, heavily rely on task boundary information and the ability to recompute all representations between tasks. Such a procedure remains unsuited for online continual learning as task change cannot necessarily be inferred and fast adaptation is a priority.
% \end{itemize}

\section{Learning Rate Selection in Online Continual Learning}
\label{sec:lr_in_CL}
% In this section we analyse the impact of LR selection in onCL.

\subsection{Preliminary}
Generally, the problem of CL is defined as training a model $f_{\theta}(\cdot)$ parameterized by $\theta$ on a sequence of $T$ tasks where each task of index $k \in \{1,\cdots, K\}$ is defined by its corresponding dataset $\mathcal{D}_k$, each potentially being drawn from a different distribution. In the case Class Incremental Learning~\cite{hsu_re-evaluating_2019}, we have $\mathcal{D}_k=(\mathcal{X}_k, \mathcal{Y}_k)$, the data-label pairs. In offCL, the model is trained sequentially on each task while other task data is unavailable. For onCL, while the model is similarly trained sequentially on each task, only the data of the \textit{current batch} is available and can be seen \textit{only once} during training. The final objective in offCL and onCL is the same: to maximize performance across all tasks. Equivalently, to minimize the average loss ${\mathcal{L}}$ across all tasks:
\begin{equation}
    \min_{\theta} \frac{1}{T}\sum_{k=1}^T \mathcal{L}(\theta, \mathcal{D}_k).
\end{equation}
% While offCL has practical applications, such setup cannot yet encompass the complexity of a continuous and potentially infinite ever-evolving data stream.

\subsection{Finding the Optimal Learning Rate}
\label{sec:finding_optimal_lr}
\paragraph{Traditional Learning.} Finding the optimal LR is a common challenge inherent to training any deep neural network with gradient-descent-based optimization techniques. Mainstream methods heavily rely on LR schedulers~\cite{he_deep_2015,vaswani2017attention}, which would typically decrease the LR value over time after each epoch. Of course, the starting value as well as the speed of the learning rate decrease must still be found. To this end, grid search remains a popular and powerful technique in traditional learning scenarios.

\paragraph{LR in offCL.} Similarly, in offCL, state-of-the-art methods have adopted LR schedulers~\cite{smith2023coda, wang2022l2p,roy2024convprompt}, however their strategy regarding finding hyperparameters is often unclear. While grid search is feasible in offCL at the task level, clear limitations can be identified. Firstly, finding the best initial LR at a given task $k$ on dataset $\mathcal{D}_k$ does not give any guarantee regarding the LR to use on subsequent datasets $\mathcal{D}_{k+1}, \cdots, \mathcal{D}_{T}$. Secondly, a suboptimal learning rate with regard to $\mathcal{D}_k$ can lead to overall higher performances when evaluating on $\{\mathcal{D}_{0}, \cdots, \mathcal{D}_{k-1}\}$. Indeed, as discussed in the work of Mirzadeh et al.~\cite{mirzadeh2020understanding}, large learning rates can disturb weights that are important for previous tasks. The loss minimized at specific timestep conflicts with the overall objective since the distribution of incoming data from $\mathcal{D}_k$ and $\bigcup_k \mathcal{D}_k$ differ.

\paragraph{LR in onCL.} Determining the optimal learning rate in onCL is even more challenging. In addition to the offCL challenges mentioned earlier, even grid search at the task level is unavailable. The online scenario constrains the model to see the incoming data only once, usually in small batches, making any hyper-parameter search completely impossible.
A long-lasting strategy to train in this context is to use the \textit{same} pre-determined fixed LR and optimizer for all considered methods~\cite{gu_dvc_2022,mai_supervised_2021,moon2023mvp,lin2023pcr}, typically Stochastic Gradient Descent (SGD) with an LR value of $0.1$. While the choice of SGD has been found to lead to better generalization in some cases~\cite{keskar2017improving}, the reasoning behind this choice remains unclear in the literature and presents significant limitations. Firstly, there is no justification for different methods, with different losses, to benefit equally from using the same learning and optimizer.
Secondly, as discussed in Section~\ref{sec:results}, it is apparent that some methods are more sensitive to LR change than others. To overcome this, some studies look for the best hyper-parameters offline for all considered methods before transferring such parameters to other setups~\cite{michel2024rethinking}. 
However, this remains insufficient as the optimal learning rate most likely varies from one dataset to the other. Despite the well-known critical impact of a suboptimal LR on final performance, the problem of selecting the best LR remains largely overlooked by the Online Continual Learning community.

\subsection{LR and Learning Behavior in onCL}
\label{sec:impact_of_lr}
\paragraph{Impact on Stability-Plasticity.} It is clear that selecting an appropriate learning rate is essential for optimal performance. In standard scenarios, the impact of its choice on loss minimization and convergence speed has been extensively studied~\cite{ruder2016overview}. For offCL, previous studies have considered to impact of the LR on forgetting~\cite{mirzadeh2020understanding}. Notably, a higher LR would increase forgetting, and vice-versa. Intuitively, the learning rate gives a direct control on the plasticity-stability tradeoff~\cite{wang2024improving}. To confirm such behavior in onCL, we experiment with larger and smaller LR values. As it can be seen in Figure~\ref{fig:lr_stab_plas}, when trained with a higher learning rate ($5\times 10^{-2}$), the model tends to obtain higher performances on the latest tasks while exhibiting especially low performances on earlier tasks. When trained with a lower LR ($5\times 10^{-5}$), the model tends to achieve better performance on earlier tasks compared to training with a higher LR. In other words, a high LR value induces more plasticity and less stability, and vice-versa.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.90\linewidth]{figures/lr_stab_plas.png}
    \caption{Task-wise Accuracy (\%) of DualPrompt at the end of training on CIFAR100, split in 10 tasks, for LR values in $\{5\times 10^{-5}, 5\times 10^{-2}\}$}
    \label{fig:lr_stab_plas}
\end{figure}

\paragraph{FC Layer Behavior.} For PTM in offCL, previous work considers different LR values for different layers~\cite{mcdonnell2024ranpac}. Notably, leveraging a higher LR in the last FC layer seems beneficial to the training procedure. In onCL, we observe similar behavior. To do so, we experiment with CODA~\cite{smith2023coda} where we multiply the gradient values with regard to the final layer by a coefficient $\kappa \in \{1, \cdots, 1000\}$, with $\kappa=1$ falling back to the regular CODA training in onCL. Such results are presented in Table~\ref{tab:manual} on various onCL datasets. It can be seen that increasing the LR of the final FC layer can often lead to an improvement in terms of average performances, even though in cases of lower LR such an effect is barely noticeable. While this naive approach can lead to improvements in certain scenarios, it induces more hyper-parameter tuning and considers the same LR strategy for each class. 
\input{tables/manual_coef}


\section{Proposed Method}

\subsection{Motivations}
As discussed in Section~\ref{sec:lr_in_CL}, LR is critical in onCL as it not only has a direct impact on the plasticity-stability trade-off, it is near impossible to use the optimal value. While previous studies highlight the need for different LR at the layer level, the case of different learning rates at the class level is still in its infancy. Following the analysis described in Section~\ref{sec:impact_of_lr}, we make the hypothesis that a class-wise LR strategy focusing on the last FC layer is crucial for onCL. The intuition is that different classes require different learning rates so that the model can adapt its stability-plasticity tradeoff not only over time, but also \textit{over classes}. In that sense, we reckon hypergradients~\cite{baydin2018hypergradient} to be adequate for improving over non-optimal initial LR values. However, hypergradients are not designed for the onCL scenario and a naive implementation leads to a severe performance drop. Indeed, hypergradient computation is identical regardless of the network layer or the classes. Inspired by the work of He et al.~\cite{he2024gradient}, we propose learning how to rescale the gradient coefficient of the last FC layer, class-wise, using hypergradient learning theory.

Additionally, to adapt PTM-based methods to the onCL scenario further, we leverage the stability of the output representation of PTM by computing Online Prototypes.

\subsection{Class-Wise Hypergradients}
\label{sec:cwh}
Let us consider a model $f_{\theta}$ parameterized by $\theta$ such that for an input $x \in \mathbb{R}^d$, with $d$ the dimension of the input space, we have $f_{\theta}(x)=h_{w}(x)^T \cdot W$, with $W \in \mathbb{R}^{l,c}$, $c$ the number of classes, $l$ the dimension of the output of $h_{w}$ and $\theta = \{w, W\}$. In this context, $h_w$ would typically be a PTM and $W$ the weight of the final FC layer (including the bias). Looking at the final FC layer $W$, for a learning rate $\eta$, we can write the gradient-based weight update:
\begin{equation}
    W_{t+1} = W_t - \eta\nabla\mathcal{L}(W_t),
\end{equation}
with $t$ the iteration index. Here, we omit the input data for simplicity.
Then, for any class index $j \in \{1, c\}$, we can write the update rule of the weights corresponding to $j$:
\begin{equation}
    W_{t+1}^j = W_t^j - \eta\nabla\mathcal{L}(W_t^j),
\end{equation}
with $W^j$ the $j^{th}$ column of $W$. Building upon previous studies~\cite{he2024gradient}, we introduce \textit{step-dependent} class-wise weighting coefficients, leading to the following update rule:
\begin{equation}
    W_{t+1}^j = W_t^j - \alpha_{t+1}^j \eta\nabla\mathcal{L}(W_t^j),
\end{equation}
with $\alpha_t^j \in \mathbb{R}^{+\star}$, class dependent gradient weighting coefficient at index $t$. While those coefficients were traditionally introduced to compensate for class interference, and computed with hand-crafted rules, we propose to learn them through an online adaptive rule based on hypergradients theory. In particular, we want to construct a higher level update for $\{\alpha_t^j\}_j$ such that, in the case:
\begin{equation}
    \alpha_{t+1}^j = \alpha_{t}^j - \beta\frac{\partial \mathcal{L}(W_{t}^j)}{\partial \alpha^j},
\end{equation}
with $\beta \in \mathbb{R}^+$ the hypergradient learning rate. To compute the partial derivative we apply the chain rule and make use of the fact that $W^j_{t} = W^j_{t-1} - \alpha^j \eta \nabla \mathcal{L}(W_{t-1}^j)$, such that:
\begin{align}
\frac{\partial \mathcal{L}(W_{t}^j)}{\partial \alpha^j} &=\nabla \mathcal{L}(W_{t}^j)\cdot \frac{\partial W_{t}^j}{\partial \alpha^j}\\ &= - \eta \nabla \mathcal{L}(W_{t}^j) \cdot \nabla \mathcal{L}(W_{t-1}^j).
\end{align}
The resulting Class-Wise Hypergradient update becomes:
\begin{equation}
\label{eq:cwh_sgd_final}
    \alpha_{t+1}^j = \alpha_{t}^j + \gamma \cdot \nabla \mathcal{L}(W_{t}^j) \cdot \nabla \mathcal{L}(W_{t-1}^j).
\end{equation}
With $\gamma=\beta\eta$. Naturally, this introduces an undesired extra-hyperparameter. We discuss this limitation in Section~\ref{sec:discussion}.
For clarity, the relation presented in Eq.~\eqref{eq:cwh_sgd_final} relies on an SGD update. In practice, we favor a momentum-based update, notably Adam~\cite{kingma2014adam}. We follow the guidelines of Baydin et al.~\cite{baydin2018hypergradient} regarding its implementation and give more detail in Appendix~\ref{sec:app_algo_adam}.

\subsection{Online Prototypes}
\label{sec:online_proto}
In order to reduce forgetting in the last layer, we compute Online Prototypes (OP) $\mathcal{P}=\{p_{k_1}^1, p_{k_2}^2, \cdots, p_{k_c}^c \}$ of each class during training. For a given class $j$, the class prototype $p_{k_j}^j$ computed over $k_j$ samples is updated when encountering a new sample $x_{k_j+1}^j$. For simplicity, we omit the $j$ index in $k_j$ going forward. Therefore, the prototype update rule is: 
\begin{equation}
    p^j_{k+1} = \frac{k\cdot p_k^{j} + h_{w}(x_{k+1}^j)}{k+1},
\end{equation}
with $x_{k+1}^j$ the ${k+1}^{th}$ encountered sample of class $j$. For all classes, prototypes are initialized such that $p_0^j=0$. Prototypes are then used to recalibrate the final FC layer, analogous to replaying the average of past data representation during training. In that sense, we define the prototype-based loss term as:
\begin{equation}
    \mathcal{L}_{OP} = \frac{\text{-}1}{c}\sum_{j\in \mathcal{C}_{old}} \log\left((p^j)^T \cdot W^j\right),
\end{equation}
with $\mathcal{C}_{old}=\{j\in \{1,c\}\ |\ p^j_{k_j} \neq 0 \}$. $\mathcal{L}_{OP}$ is the cross-entropy with regard to prototypes of encountered classes. OP acts as simple and low-budget memory data.

\subsection{Overall Training Procedure}
\label{sec:overall_training}
The approach relies on two components that are orthogonal to most existing methods, as long as the model optimizes a final FC layer for classification. In that sense, we propose to integrate it into various state-of-the-art baselines relying on PTM, notably prompt-based approaches. To do so, if we consider $\mathcal{L}_{base}$ to be the loss of the baseline method, to which we attach both components OP and CWH, we have the overall loss to minimize:
\begin{equation}
    \label{eq:overall_loss}
    \mathcal{L} = \mathcal{L}_{base} + \mathcal{L}_{OP}.
\end{equation}
A Pytorch-like~\cite{paszke2019pytorch} pseudo code is given in Algorithm~\ref{code:pseudo_code}. Additionally, we leverage batch-wise masking to consider the logits of classes that are only present in the current batch. More details can be found in the Appendix~\ref{sec:app_algo_adam}. Similarly, note that the bias has been omitted for simplicity.

\input{algorithms/main_sgd}

\section{Experiments}
In the following sections experiment with combining our proposed approach with several flagship CL approaches.
\input{tables/all_acc_clear.tex}
\input{tables/all_acc_blurry.tex}

\subsection{Metrics}
\label{sec:metric}
\paragraph{Average Performances (AP).} We follow previous work and define the Average Performance (AP) as the average of the accuracies computed after each task during training~\cite{zhou2024ptmsurvey}. 
Formally, when training on $\{\mathcal{D}_1, \cdots, \mathcal{D}_T\}$, we define $\mathcal{A}_k$, the Average Accuracy (AA) as:
\begin{equation}
    \mathcal{A}_k = \frac{1}{k}\sum_{l=1}^k a_{l,k}
\end{equation}
with $a_{l,k}$ the accuracy on task $l$ after training on $\mathcal{D}_k$. Building on this, we define the Average Performance (AP) as:
\begin{equation}
    \mathcal{P} = \frac{1}{T}\sum_{k=1}^{T} \mathcal{A}_k
\end{equation}

\paragraph{Performance Across LR.} To show the improvement in the case of unknown optimal LR, we propose to experiment with various LR values and report individual and averaged performances across these values. Specifically, we experiment for LR values in $\{5\times 10^{-5}, 5\times 10^{-4}, 5\times 10^{-3}\}$. The main motivation is that we reckon that the optimal LR is likely to fall into that range, therefore we wanna take into account the effect of experimenting with a learning rate that is either above or below the optimal value. Such a metric should emphasize the validity of the approach when the optimal LR is unknown and result in a fairer comparison than using the same LR blindly for every approach.

\subsection{Experimental Setting}
\paragraph{Baselines and Datasets.}
In order to demonstrate the efficiency of our approach as presented in Algorithm~\ref{code:pseudo_code}, we integrate it with several state-of-the-art methods in offCL. Notably, \textbf{L2P}~\cite{wang2022l2p}, \textbf{DualPrompt}~\cite{wang2022dualprompt}, \textbf{CODA}~\cite{smith2023coda}, \textbf{ConvPrompt}~\cite{roy2024convprompt}. These methods are not naturally suited for the online case, so they had to be adapted. More details on the adaptation of such methods are in Appendix~\ref{sec:app_adaptation}. Additionally, we include one state-of-the-art onCL method that leverages PTM, MVP~\cite{moon2023mvp}. We evaluate our method on \textbf{CUB}~\cite{wah2011cub}, \textbf{ImageNet-R}~\cite{hendrycks2021imagenetr} and \textbf{CIFAR100}~\cite{krizhevsky_learning_2009}. More details in Appendix~\ref{sec:app_baselines}.

\paragraph{\textit{Clear} Boundaries.} We experiment in \textit{clear} boundaries settings, for continuity with previous work, despite its lack of realism for onCL. In that sense, we consider an initial count of $10$ classes for the first task, with an increment of $10$ classes per task. This results in $10$ tasks with $10$ classes per task for CIFAR100, as well as $20$ tasks with $10$ classes per task for CUB and ImageNet-R.

\paragraph{\textit{Blurry} Boundaries.} To evaluate our method in more realistic scenarios, we reckon the \textit{Si-Blurry}~\cite{moon2023mvp} setting to be the most relevant to our study case. Specifically, we use their implementation of Stochastic incremental Blurry boundaries (\textit{Si-Blurry}). We use the same number of tasks as for the \textit{clear} setting. In this case, some classes can appear or disappear during training and the transitions are not necessarily clear. More details on this setting can be found in Appendix~\ref{sec:app_blurry}. To adequately adapt the proposed methods to the online problem, we use batch-wise masking. Specifically, we consider only the logits of the class that are observed in the current batch, for every method.

\paragraph{Implementation Details.}
Every method is evaluated in the onCL context. Namely, only one pass over the data is allowed. The batch size is fixed at $10$ to simulate small data increments with a low storage budget in the context of fast adaptation. The PTM used is a ViT-base, pre-trained on ImageNet 21k. Every experiment was conducted over 3 runs and the average and standard deviation are displayed. Each run was conducted with a different seed, which equally impacted the task generation process. For all experiments, we use $\gamma=1 \times 10^{-3}$. More details on $\gamma$ selection can be found in Section~\ref{sec:selecting_gamma}


\subsection{Experimental Results}
\label{sec:results}
We combined our method with four offline approaches and one online state-of-the-art approach, all using PTM. As mentioned in Section~\ref{sec:overall_training}, our method must be applied on the classification head, therefore prompt-based approaches are especially suited as they all leverage a final FC layer on top of the PTM representation for classification.

\paragraph{Average Performances.} We experiment in both \textit{clear} and \textit{blurry} settings and present the results in terms of Average Performance in Table~\ref{tab:clear_all} and Table~\ref{tab:blurry_all}. On top of datasets and boundary scenarios, we propose a novel evaluation procedure specific to the problem at hand. Namely, we present results for LR values in $\{5\times 10^{-5}, 5\times 10^{-4}, 5\times 10^{-3}\}$, as well as the AP across all these values. The objective is to observe how would the method perform on average when the optimal learning rate is unknown and might be far from optimal, by being either too high or too low. Therefore, in all cases, combining both proposed components leads to a significant improvement in AP over the baselines when looking at the average across learning rates, with up to $30\%$ improvement on CUB. Such improvements are observed in both \textit{blurry} and \textit{clear} scenarios, confirming the ability of our approach to perform in realistic and traditional contexts. Individual contributions of \textit{OP} and \textit{CWH} are detailed below.

\paragraph{\textit{Blurry} Boundaries.} Even though performance improvement can be observed as well in the \textit{blurry} scenario, all methods suffer from a significant drop when transitioning from one scenario to the other. Such behavior highlights the importance of focusing on more realistic setups in future research. In that sense, our method remains completely applicable regardless of the presence of boundaries.

\paragraph{Ablation Study.} To make apparent the contribution of each component of our method, we included the performances of the original baselines, followed by the performance of such baselines combined with OP (\textit{+ OP}) and eventually the performance of the same baseline combined with prototypes and CWH (\textit{+ CWH}). Such results are included in Tables~\ref{tab:clear_all} and~\ref{tab:blurry_all}. While it is clear that the usage of prototypes is largely beneficial, in some situations the addition of \textit{CWH} can lead to a slight drop in performances. However, in those rare scenarios, the performance drop remains minimal with the maximum drop value being $3.79\%$ in the case of CODA on CUB with an LR of $5\times 10^{-3}$. In other cases, performance loss is around $1\%$. Despite this limitation, it is important to note that the gain of including \textit{CWH} is especially important for longer task sequences such as CUB, which is a more realistic scenario. Moreover, when the initial LR value is particularly low ($5\times 10^{-5}$), the gain of including \textit{CWH} is generally more important. When fine-tuning PTM, it is common to start with low LR values. This property further confirms the ability of our approach to perform in realistic contexts.

\input{tables/rep_stab}

\section{Discussions}
\label{sec:discussion}
% In the following, we discuss and analyse the behavior of our approach and give additional insights.


\subsection{Stability of OP}
Despite its apparent simplicity, OP gives the largest performance gain.
% The potential of simple prototypes is known in offCL~\cite{zhou2024revisiting} and we argue that their successful application to onCL is mainly due to the stability of the output representation.
This is partially due to the fact that only the FC layer and the input prompts are trained, so the output prototypes are stable over time. In Table~\ref{tab:OP_no_freeze} we show that unfreezing intermediate weight not only drastically decreases overall performances but similarly negates the effect of OP significantly. Additionally, in Figure~\ref{fig:OP_stab}, we show the average Euclidean distance of Online Prototypes between two training step iterations. As we can see, the learned prototypes tend to stabilize rapidly over time, even more so when the intermediate weights remain frozen. This stability of the prototype indicates that even when computed online, they can be used as a reasonable proxy for class average representation.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/WB_proto_shift.png}
    \vspace{-0.3cm}
    \caption{Prototype shift of L2P with LR of $5 \times 10^{-5}$ with and without freezing the network weights.}
    \label{fig:OP_stab}
    \vspace{-0.5cm}
\end{figure}


\subsection{Selecting $\gamma$}
\label{sec:selecting_gamma}
The main drawback of leveraging \textit{CWH} is the addition of an extra hyper-parameter $\gamma$, whereas our objective is to reduce the dependency on hyper-parameters for onCL. However, we argue that selecting $\gamma$ remains particularly simple. To demonstrate this, we experiment with $\gamma \in \{0, 1\times 10^{-5}, 1\times 10^{-3}, 1\}$, for an initial LR $\eta \in \{5\times 10^{-5}, 5\times 10^{-3}\}$. In that case, $\gamma=0$ is equivalent to disabling \textit{CWH}. Such results are presented in Table~\ref{tab:disc_gamma}. When the initial LR is lower, including \textit{CWH} leads to an improvement in performance, especially when $\gamma$ is large. Experimentally, this can be seen as a consequence of $\{\alpha^j\}_j$ values increasing during training in all cases, as discussed in Section~\ref{sec:alpha_values}. Therefore, it is natural that lower initial LR values would benefit more from such a strategy. When the initial LR is large, as discussed in Section~\ref{sec:results}, a marginal drop in performance can be observed in some cases. However, the final performances remain stable for all values of $\gamma$, minimizing the need for hyperparameter search. Additionally, the lower the value of $\gamma$, the closer it is to the original method.

In conclusion, for higher LR we recommend lower values of $\gamma$, as their effect on the training is reduced. For lower LR, any value of $\gamma$ leads to an improvement, however, higher values are recommended for optimal accuracy gain. Default values such as $0.001$ should lead to an increase in performances in most cases or a slight decrease in worst cases. Overall, selecting $\gamma$ remains a simpler and more realistic task than doing a grid search in onCL.

\input{tables/discussion_gamma.tex}

\subsection{Values of learned $\alpha^j$}
\label{sec:alpha_values}
\paragraph{Trend and Distribution Across Classes.}
The class-wise coefficients $\alpha^j$ are learned in each training step as described in Eq.~\eqref{eq:cwh_sgd_final}. We report coefficients' evolution in Figure~\ref{fig:alpha_values}. Firstly, learned coefficient values are superior to $1$ and increase during training. Such a trend is aligned with findings from previous studies suggesting adopting a higher LR value for the final FC layer~\cite{mcdonnell2024ranpac}. Secondly, it can be seen that coefficient values tend to be larger for later tasks than for earlier tasks. Intuitively, following the analysis from Section~\ref{sec:lr_in_CL}, this corresponds to giving more plasticity to newly encountered classes than older classes.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/WB_coef_evo_clear.png}
    \vspace{-0.6cm}
    \caption{Average of $\alpha^j$ values per tasks when training CODA + CWH on CIFAR-100 with clear boundaries, $\gamma=0.001$, and LR of $0.005$. \textit{0-10} corresponds to the average of $\alpha^j,\ j\in\{1, 10\}$ and so on. Here, classes are given in order, so \textit{0-10} is the average for all classes corresponding to the first task.}
    \label{fig:alpha_values}
\end{figure}

\paragraph{Relation with Learning Rate.} 
We investigate the behavior of the learned coefficient with regard to the initial LR. As we can see in Figure~\ref{fig:alpha_lr_relation}, the smaller the initial LR, the larger the values of learned coefficients. Intuitively, a smaller initial LR requires a larger compensation in the FC layer to reach a similar learning regime. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/WB_coef_lr_rel2.png}
    \vspace{-0.4cm}
    \caption{Average of $\alpha^j$ values on the first task when training CODA+CWH on CIFAR-100 with clear boundaries, $\gamma=0.001$ for various initial learning rates.}
    \label{fig:alpha_lr_relation}
    \vspace{-0.5cm}
\end{figure}

\section{Conclusion}
In this paper, we studied the problem of leveraging Pre-Trained Models in the context of Online Continual Learning. In that sense, we focused on two central problems. Namely, the unavailability of task boundaries and the unfeasibility of hyper-parameter search. We tackled the former by leveraging Online Prototypes, a simple yet powerful strategy that takes advantage of the stable representation output of PTM to use class prototypes as replay samples. For the latter, we introduced Class-Wise Hypergradients which can help mitigate the drop in performances due to inadequate LR value. To reflect the efficiency of our approach, we evaluate every model with various initial LR values and show that on average both OP and CWH can be beneficial when combined with baseline models. Nonetheless, the problem of using optimal hyper-parameters in an online context remains unsolved and we hope that this work can shed light on its importance and pave the way to additional research in this direction.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{all_zotero}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Algorithm of our Adam Implementation}
\label{sec:app_algo_adam}
As explained in Section~\ref{sec:cwh}, the actual implementation that we used for our experiments is based on an Adam update. For the sake of clarity, we presented our method with SGD. Similarly, we omitted the bias from the pseudo-code. Therefore, we give the full details of the procedure in Algorithm~\ref{code:pseudo_code_adam}, in a pseudo-code Pytorch-like implementation.
\input{algorithms/main_adam}


\section{Datasets and Baselines}
\subsection{Datasets}
PTMs are often pre-trained on ImageNet-21k, making it unfair to experiment on such datasets. To showcase the performances of our approach we experiment with the following:
\begin{itemize}
    \item \textbf{CUB}~\cite{wah2011cub}: The CUB dataset (Caltech-UCSD Birds-200) contains 200 bird species with 11,788 images, annotated with attributes and part locations for fine-grained classification. 
    \item \textbf{ImageNet-R}~\cite{hendrycks2021imagenetr}: ImageNet-R is a set of images labeled with ImageNet label renditions. It contains 30,000 images spanning 200 classes, focusing on robustness with images in various artistic styles. 
    \item \textbf{CIFAR100}~\cite{krizhevsky_learning_2009}: CIFAR-100 consists of 60,000 32x32 color images across 100 classes, with 500 images per class, split into 500 training and 100 test samples per class.
\end{itemize}

\subsection{Baselines}
\label{sec:app_baselines}
Prompt learning-based methods~\cite{zhou2024ptmsurvey} are particularly suited for being combined with our approach in onCL as they all capitalize on a final FC layer for classification. Therefore, we consider the following.
\begin{itemize}
    \item \textbf{L2P}~\cite{wang2022l2p}: Learning to Prompt (L2P) is the foundation of prompt learning methods in Continual Learning. The main idea is to learn how to append a fixed-sized prompt to the input of the ViT~\cite{dosovitskiy2020vit}. The ViT stays frozen, only the appended prompt as well as the FC layer are trained. 
    \item \textbf{DualPrompt}~\cite{wang2022dualprompt}: DualPrompt follows closely the work of L2P by addressing forgetting in the prompt level with task-specific prompts as well as higher lever long-term prompts. 
    \item \textbf{CODA}~\cite{smith2023coda}: CODA-prompt improves prompt learning by computing prompt on the fly leveraging a component pool and an attention mechanism. Therefore, CODA benefits from a single gradient flow and achieves state-of-the-art performances. 
    \item \textbf{ConvPrompt}~\cite{roy2024convprompt}: ConvPrompt leverages convolutional prompts and dynamic task-specific embeddings while incorporating text information from large language models. 
    \item \textbf{MVP}~\cite{moon2023mvp}: MVP uses instance-wise logit masking, contrastive visual prompt tuning for Continual Learning in the \textit{Si-Blurry} context.
\end{itemize}


\section{Additionnal Evaluation Metrics}
Here we report additional metrics in the clear and blurry contexts for all methods for additional insights into the performances

\subsection{Final Average Accuracy}
We report the final Average Accuracy $\mathcal{A}_T$ as per the definition of Section~\ref{sec:metric}. Such results are presented in Table~\ref{tab:clear_total} and Table~\ref{tab:blurry_total}.


\subsection{Average Performances on Old Classes}
We report the Average Accuracy on old classes only at the end of training, $\mathcal{A}_{T-1}$ as per the definition of Section~\ref{sec:metric}. Such results are presented in Table~\ref{tab:clear_total} and Table~\ref{tab:blurry_total}. A higher value of performance in old classes indicates a better ability to retain knowledge, also known as stability.

\subsection{Additional Ablation}
We include additional experiments with CODA on CUB to evaluate the impact of combining CWH only. Such results are presented in Table~\ref{tab:suppl_ablation}. As we can see, CODA still benefits the most from \textit{OP}, however, it reaches the best performances in average across learning rate when being combined with \textit{OP} and \textit{CWH}.
\input{tables/suppl_ablation}

\subsection{Time Complexity}
Experiments were run on various machines including Quadro RTX 8000 GPU, Tesla V100 16Go GPU, A100 40Go. In this section, we report the times of execution of each method, as well as the overhead induced by leveraging our components. To do so we run all methods on a single V100-16Go. The results are presented in Table~\ref{tab:time_consumption}. As expected, the time consumption overhead of including \textit{OP} and \textit{CWH} is minimal.
\input{tables/time_consumption}


\subsection{Spatial Complexity}
\paragraph{Class-Wise Hypergradients.} The usage of CWH solely requires storing one float per class (with $c$ classes total) as well as previous gradient values in the last FC layer in the case of SGD. This amounts to a total of $c \times c \times (l+1)$ additional floats to store. We multiply by $l+1$ to account for the bias. For CIFAR100 a Vit-base, we have $c=100$ and $l=768$.  In the case of Adam update, Adam parameters must equally be included.

\paragraph{Online Prototypes.} Storing OP only requires one vector of dimension $l$ per class, with $l=768$ in the case of ViT base. Additionally, an extra integer per class must be stored to keep track of the index of the update of each OP. If the index is stored as a float, the additional amount of floating points to store is $c\times (l+1)$, with $c$ the number of classes, and $l$ the output dimension of the PTM.

\section{Adaptation of Methods to our setup}
\label{sec:app_adaptation}
Since most methods compared here were originally designed for offCL, they had to be specifically adapted to the onCL scenario. In that sense, some parameters have been chosen arbitrarily, based on their offCL values, without additional hyper-parameter search. Such a situation is similar to one that would be observed in real-world cases where an offCL model tries to be adapted to an onCL problem. For all methods, we use a fixed initial learning rate, no scheduler, and Adam optimizer. Of course, we disabled an operation that was operated at task change. Additionally, even though MVP was indeed designed for online cases, we found several differences between their training procedure and ours, which we discuss below.

\paragraph{Adaptation of CODA.}
In their original paper and implementation, the authors require freezing components after each task, therefore having task-specific components. Typically, they show that performances tend to plateau for more than $100$ components, and for a $10$-tasks sequence, they would reserve $10$ components per task. In our implementation, we decided to similarly use $100$ components for the entire training. However, we train all components together at all times during training since we cannot know when would be the correct time to freeze or unfreeze them. For other parameters, we followed the original implementation.

\paragraph{Adaptation of ConvPrompt.}
ConvPrompt is a method that heavily relies on task boundaries in its original implementation, notably by incorporating $5$ new prompts per task. Contrarily to CODA, allocating the maximum number of prompt generators at all times, without a freeze, would induce an important training time constraint. Therefore, we only use $5$ prompt generators at all times. Despite this reduction in overall parameters, ConvPrompt still achieves competitive results in the \textit{clear} setting. However, its performances drastically fall off in the \textit{Si-Blurry} case. Further, an in-depth adaptation of ConvPrompt in the online context could potentially improve its performance, however, such a study is not covered in this work.

Another interesting aspect of ConvPrompt adaptation into our setup is the high variance observed when the LR is high ($5\times 10^{-3}$). The reason for such results is that ConvPrompt is indeed very sensitive to the choice of the initial LR, therefore, when is it too large the training is unstable and some runs result in extremely poor performances. This observation once again highlights the importance of properly selecting the initial learning rate in onCL.

\paragraph{Adaptation of DualPrompt.}
Similar to CODA, but on a prompt level, DualPrompt requires freezing prompts at task change. For adapting it to onCL, we chose to use all prompts at all times without freezing the prompt pool. The E-Prompt pool size is set to $10$ and the G-Prompt pool size is set to $5$.

\paragraph{Adaptation of L2P.}
The same logic as the one described for CODA and DualPrompt applies to L2P. In that sense, we use the entire prompt pool at all times without freezing. The prompt pool size is set to $10$.

\paragraph{Adaptation and Performances of MVP}
Even though MVP is designed for the online case, their original training setup differs slightly. Firstly, the batch size is set to $32$ (we use $10$), and they similarly consider that each batch can be used for 3 separate gradient steps. In that sense, the performances reported in the original paper might be higher as they trained on a slightly more constrained setup. Secondly, the authors use the same learning rate and optimizer for each compared method, which as we argued in this work, might lead to different results relatively speaking compared to other methods. Such experimental differences might lead to the performances obtained in our experiments which are in most cases significantly lower than other methods.

\section{Details on the Si-Blurry Setting}
\label{sec:app_blurry}
We followed the procedure and code made available by the authors of MVP~\cite{moon2023mvp} in order to generate the \textit{Si-Blurry} versions of the datasets. Notably, we use $M=10$ and $N=50$, following the original work. The number of tasks is the same as in the \textit{clear} setting. We show the number of images per class apparition during training for a subset of classes to give a better overview of this training environment in Figure~\ref{fig:blurry_example}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/WB_blurry_example.png}
    \vspace{-0.3cm}
    \caption{Example of class apparition during training in the \textit{Si-Blurry} setting.  The y-axis represents the average number of images of a given class present in the current batch of size $10$.}
    \label{fig:blurry_example}
\end{figure}

\input{tables/total_clear}
\input{tables/total_blurry}
\input{tables/old_clear}
\input{tables/old_blurry}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
