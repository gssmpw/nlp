\begin{algorithm}[ht]
    \tiny
    \begin{minted}{python}
gamma = 1e-3
grad_weight = torch.ones(n_classes)
prev_grad = None
for x, y in dataloader:
  # Baseline loss
  h, y_hat = network(x) # features and logits
  loss_baseline = criterion_baseline(y_hat, y)
  # FC recalibration
  proto, labels = get_prototypes()
  logits = network.fc(proto)
  loss_op = cross_entropy(logits, labels) # Eq. 10
  
  loss = loss_baseline + loss_op # Eq. 11
  
  optim.zero_grad()
  loss.backward()
  
  # Class-Wise Hypergradient update
  curr_grad = network.fc.weight.grad
  if prev_grad is not None:
    grad_weight += gamma * (curr_grad @ prev_grad.T).diag() #Eq. 8
    for i in range(n_classes):
        network.fc.weight.grad[i, :] *= grad_weight[i]
  prev_grad = curr_grad
  
  optim.step()
  update_proto(h, y) # Eq. 9
    \end{minted}
    \vspace{-0.3cm}
    \caption{PyTorch-like pseudo-code of our method integration with other baselines}
    \label{code:pseudo_code}
\end{algorithm}