\begin{algorithm*}[ht]
    \tiny
    \begin{minted}{python}
# Adam parameters
m = 0
v = 0
beta1 = 0.9
beta2 = 0.999
step = 0

# Hypergrad parameters
gamma = 1e-3
grad_weight = torch.ones(n_classes)
prev_grad = None
for x, y in dataloader:
  # Baseline loss
  h, logits_base = network(x) # features and logits
  # Batch-wise masking
  logits_base[:, [i for i in range(logits_base.shape[-1]) if i not in y.unique()]] = float('-inf')
  loss_baseline = criterion_baseline(logits_base, y)
  
  # FC recalibration
  proto, labels = get_prototypes()
  logits = network.fc(proto)
  # Batch-wise masking
  logits[:, [i for i in range(logits.shape[-1]) if i not in label.unique()]] = float('-inf')
  loss_op = cross_entropy(logits, labels) # Eq. 10

  loss = loss_baseline + loss_op # Eq. 11
  
  optim.zero_grad()
  loss.backward()

  # Class-Wise Hypergradient update
  curr_W = network.fc.weight.grad
  curr_B = network.fc.bias.grad
  curr_grad = torch.cat([curr_W, curr_B.unsqueeze(1)], dim=1)
  if prev_grad is not None:
    # Adam update
    m = beta1 * m + (1 - beta1) * curr_grad
    v = beta2 * v + (1 - beta2) * (curr_grad ** 2)
    m_hat = m / (1 - beta1 ** step)
    v_hat = v / (1 - beta2 ** step)
    curr_grad = m_hat / (torch.sqrt(v_hat) + 1e-8)
    
    grad_weight += gamma * (curr_grad @ prev_grad.T).diag() #Eq. 8
    for i in range(n_classes):
        network.fc.weight.grad[i, :] = network.fc.weight.grad[i, :] * grad_weight[i]
        network.fc.bias.grad[i] = network.fc.bias.grad[i] * grad_weight[i]
  prev_grad = curr_grad
  optim.step()

  update_proto(h, y) # Eq. 9
    \end{minted}
    \caption{PyTorch-like pseudo-code of our Adam-based method integration with other baselines. Extra details are given in this version regarding bias consideration and batch-wise masking.}
    \label{code:pseudo_code_adam}
\end{algorithm*}