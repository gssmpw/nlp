%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
\RequirePackage{fix-cm}
%
\documentclass[twocolumn]{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
% \documentclass[smallextended]{svjour3}       % onecolumn (second format)
% \documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
\usepackage{amsmath}
\usepackage[mathlines]{lineno}% Enable numbering of text and display math
\usepackage{physics}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage[misc]{ifsym}
\usepackage{csquotes}
\usepackage{listings}
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
\setlength{\parindent}{0pt}
\setlength{\emergencystretch}{1em}
%
% Insert the name of "your journal" with
\journalname{Multibody System Dynamics}
%
\begin{document}

\title{Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software}
\titlerunning{Experiments with LLMs on RAG for Closed-Source Simulation Software}
% \subtitle{Do you have a subtitle?\\ If so, write it here}


\author{
  Andreas Baumann (0000-0001-5831-4510) \and
  Peter Eberhard (0000-0003-1809-4407)
}
\authorrunning{A. Baumann \and P. Eberhard}

\institute{
    \Letter\ Peter Eberhard\\
    \email{peter.eberhard@itm.uni-stuttgart.de}\\
    A. Baumann \and P. Eberhard
    \at Institute of Engineering and Computational Mechanics, University of Stuttgart, Pfaffenwaldring 9, 70569 Stuttgart, Germany
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
Large Language Models (LLMs) are increasingly helpful in text generation, even writing code in programming languages based on user prompts written in natural language. They are even applied to generate simulation models for multibody systems from natural language. Research results suggest that LLMs surpass the mere replication of existing code examples, where some LLMs have been trained on an open-source multibody simulation code. However, for closed-source simulation software, such results are not to be expected as their ideas and concepts might differ from other publicly available ones. LLMs can hallucinate for knowledge-intensive tasks, such as model creation, which can lead to wrong responses. This is especially the case for the LLM unknown closed-source simulation software. The same applies to other internal knowledge kept private to protect intellectual property or data privacy.
The Retrieval-Augmented Generation (RAG) approach might yield a solution for these knowledge-intensive tasks. This paper explores the application of RAG to closed-source simulation software and presents first experiments. After a brief introduction to LLMs, the RAG approach, and the simulation method applied by the close-source simulation software, several examples are provided to test LLMs' knowledge of the simulation software and the creation of simulation models using two RAG systems.
The examples show promising results indicating the benefits of applying RAG systems to closed-source simulation software, helping to access their knowledge. Nevertheless, they also reveal gaps in the applied information and open questions for further research.


\keywords{Large Language Models \and Retrieval-Augmented Generation \and Closed-Source Simulation Software}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

Large Language Models (LLMs) are becoming increasingly important and have become an integral part of everyday use. Writing texts or developing programming codes are just two examples that are generated using LLMs in the shortest time using questions and instructions written in natural human language. Among the most commonly used tools are ChatGPT or GitHub Copilot.

Overly simplified, however, it must be stressed that classic LLMs do not understand the posed tasks but rather predict the response on the prompt based on learned word and sentence sequence relationships. To obtain these relationships of natural language, large training sets are required. This allows LLMs to answer also prompts which were not included in its training set.

However, for knowledge-intensive tasks this often leads to wrong answers, so-called \emph{hallucinations} \cite{Lewis20,Ji2023}. One example of such knowledge-intensive task is generating simulation models for, e.g., multibody simulations. Research shows that LLMs are well able in writing basic simulation codes for dynamic systems \cite{Gerstmayr2024}, especially when LLMs include this knowledge in their training. For example, in \cite{Gerstmayr2024} the investigated multibody simulation library Exudyn \cite{Gerstmayr2023} is available as open-source on GitHub, which is reported to be part of the training data for LLMs like ChatGPT.

For closed-source software packages no such abilities are to be expected, simply because the ideas and concepts of these software packages are not known to the LLM. Therefore, any LLM response will be guessed based only on other publicly available software packages. This assumption will later be tested.

Nevertheless, the abilities of LLMs to develop simulation models and support developers of these closed-source simulation packages would be of tremendous help. The protection of the intellectual property, data privacy, or independence from a few of companies, which are able to develop such LLMs, are just some reasons why these software packages or other knowledge remain internally. The same applies for other internal knowledge, ranging from software projects to internal project reports.

In principle, LLMs can also be developed in-house or fine-tuned on the internal data on the basis of open-source LLMs. Examples for open-source LLMs are Teuken 7B from OpenGPT-X \cite{Ali2024} or the Llama models from Meta \cite{llama_herd}. Nevertheless, the training or fine-tuning of such LLMs requires massive computational resources. For example, the training of Llama 3.2 3B required up to 460k GPU hours on Nvidia H100-80GB GPUs \cite{Llama3.2}, which is not feasible for many researchers or companies. On the other hand, pretrained LLMs can be evaluated on a single gaming GPU. This is also referred to as LLM inferencing, where the LLM is used to generate a reply, respective a response, on an input prompt.

Retrieval-Augmented Generation (RAG) \cite{Lewis20} provides an approach to combine additional knowledge with the application of LLMs. This additional information is searched based on the user prompt and relevant documents are supplied to the LLM for response generation. As knowledge bases, internal documents or closed-source simulation packages can be used. RAG also reduces the risk of hallucinations \cite{Shuster2021,Wu2024}, as relevant information for knowledge-intensive tasks can be provided. Additionally, as the evaluation of an LLM can be done locally, the supplied knowledge not needs to be disclosed.

This work provides first experiments of RAG for closed-source simulation software in order to asses its further application. Several small examples are provided and issues in the LLM responses are discussed before open questions for future research are pointed out. The tests are performed using the closed-source simulation software Pasimodo \cite{Fleissner2010}. Pasimodo is a program package for particle-based simulation methods, such as Smoothed Particle Hydrodynamics (SPH) and the Discrete Element Method (DEM). In order to understand the challenges involved in carrying out the experiments, a brief introduction to the SPH simulation method is first provided. Afterwards, the knowledge of common LLMs regarding Pasimodo is tested before additional internal knowledge about it is supplied to two available RAG systems. Further tests are carried out to assess the performance of the RAG systems.


\subsection{Smoothed Particle Hydrodynamics}

Smoothed particle hydrodynamics (SPH) is a mesh-free Lagrangian method that discretizes a continuum medium through discrete integration points, called particles. SPH can be used to approximate any partial differential equations and is frequently applied in fluid dynamics, such as modeling the complex chip evacuation process during deep-hole drilling, where fluid-structure interaction plays a key role \cite{Baumann2024a}. The meshless nature of SPH makes it particularly suitable for handling dynamic surfaces and interfaces, which can be computationally expensive to track with traditional mesh-based methods.

Some basic theoretical aspects are shortly summarized to provide some background for the following tests. However, this work is not intended to be an introduction into SPH, the interested reader is referred to additional literature \cite{Violeau2016} about SPH.

Basically, SPH applies a summation over neighboring particles $b$ to determine the properties of the current particle $a$. Consequently, applying SPH on the Navier-Stokes questions leads to the discretization of the momentum equation as
\begin{eqnarray}
  \label{eqn:SPHMomentumEqn}
  \frac{d\mathbf{v}}{dt}\eval_{a} = &&\mathbf{g} - \sum_{b}m_{b}\left[ \left (\frac{p_{a}}{\rho_{a}^{2}}+ \frac{p_{b}}{\rho_{b}^{2}} \right ) \right . \nonumber \\
  &&\left .-\frac{1}{\rho_{a}\rho_{b}}\frac{(\mu_{a}+\mu_{b}) \mathbf{v}_{ab} \cdot \mathbf{r}_{ab}}{||\mathbf{r}_{ab}||^{2} + 0.01h^{2}} \right] \nabla_a W_{ab},
\end{eqnarray}
and the continuity equation reads
\begin{equation}
  \label{eqn:SPHContinuityEqn}
  \frac{d\rho}{dt}\eval_{a} = \rho_{a}\sum_{b}\frac{m_{b}}{\rho_{b}}(\mathbf{v}_{a} - \mathbf{v}_{b}) \cdot \nabla_a W_{ab}.
\end{equation}
Here, $\mathbf{v}$ is the velocity, $t$ is time, $\mathbf{g}$ is gravity, $m$ is the mass, $p$ is the pressure, $\rho$, is the density, and $\mu$ is the dynamic viscosity of particle $b$ respective particle $a$. The smoothing kernel function $W(\mathbf{x}-\mathbf{x}',h)$ has compact support, which is defined by the smoothing length $h$. Therefore, it allows the summation to be restricted to the effective neighboring particles $b$ for any given particle $a$ in the fluid domain. Instead of considering every particle combination in the complete domain, a neighborhood search is carried out reducing the computational effort significantly.

An equation of state relating density and pressure is required ensuring that all equations for the unknowns $\rho, \mathbf{v}, p, \mathbf{r}$ are available. Commonly, the \emph{Tait} equation is therefore used for SPH \cite{Cole1948}, which is given by
\begin{equation}
\label{eqn:EquationOfState}
    p = \frac{c_{0}^2\rho_{o}}{\gamma}\left [ \left (\frac{\rho}{\rho_{o}}\right )^{\gamma} - 1 \right ].
\end{equation}

Here, $c_{0}$ denotes the numerical speed of sound, $\rho_{o}$ is a reference density, and the polytropic index is represented by $\gamma$. In general, this equation of state is quite stiff, requiring small step sizes during the numerical integration. Instead of the fluid's speed of sound, a numerical speed of sound $c_{0}$ is chosen as ten times the maximum velocity expected in the fluid flow. This provides a reasonable time step size during numerical integration while the density fluctuations remain less than $1\%$ \cite{Monaghan1994}.
However, the maximum velocity in the flow is usually not known before the simulation is completed. For the example of a collapsing water column with height $H$, also called a breaking dam example, which is a common example for SPH, an upper bound for the speed of the water is given by
\begin{equation}
\label{eqn:BreakingDamVelocity}
    v = \sqrt{2gH}.
\end{equation}

Due to its particle nature, the incomplete summation over the neighbors near free surfaces leads to inconsistencies in the SPH approximations. A simple and efficient approach to treat these errors is called kernel gradient correction \cite{Bonet1999}. Therefore, a correction matrix is calculated for each particle and stored in an additional particle state variable.

\section{Application of Large Language Models}

Large Language Models (LLMs) are a type of machine learning model designed to generate text based on user input. The user input, also called prompt, can be natural language, which significantly simplifies the application. LLMs are trained on large sets of data, hence the name \emph{large}, to learn word and sentence sequence relationships. In oversimplified terms, a LLM learns to predict the most likely text sequence following the previous sequences. Although, the LLM does not actually comprehend the problem behind the user input, it can create based on likelihoods responses on prompt which have not been part of its training set.

In the following, we give a brief introduction in the concept behind LLMs and the computational resources needed for them. Afterwards, we test their knowledge about a closed-source simulation software which has not been part of their training data.

\subsection{Brief Introduction into Large Language Models}

The latest LLMs are based on the transformer \cite{AttentionIsAllYouNeed} approach, which consists primarily of an encoder and a decoder. The encoder and the decoder are both made of multiple layers of self-attention and feedforward neural networks. It is beyond the scope of this work to explain the details on these mechanisms, therefore only a brief introduction is provided. For more details, the interested reader is referred to further literature, e.g. \cite{AttentionIsAllYouNeed,Liu24,Alammar18,Phi20}.

The encoder starts with the embedding. Neural networks are not based on the words the input consists of, but on numbers respective a numerical vector representation of each word. Therefore, the encoder converts the input into a vector representation, which is referred to as embedding. Any position information is lost by this at first. However, the position and order of the words in the input are crucial for its understanding. Therefore, the positional information of the input words is added into the embeddings.
The following layers of self-attention allow the LLM to learn which words of the input are connected. This allows the LLM to detect the important words while ignoring others. This also allows the LLM to learn that words ordered in a certain pattern, e.g., typically compose a question to respond accordingly.

The decoder creates the actual response for the input. Its structure is similar to the encoder, but it has a different job. The decoder takes as input the information from the encoder and the list of the previous outputs. Based on these, the decoder predicts the next word creating the output word by word.

Depending on the number of words known to the LLM and the number of layers, the size of current LLMs reaches into the billions of parameters. Usually the name indicates the model size, where the word billion is shortened to \emph{B}. Table~\ref{tab:llm_overview} lists three common LLM, the numbers of parameters of their model, and the maximum input length. Gemini 1.5 Flash is an LLM offered by Google, whereas the Llama models are provided by Meta as Open-Source which can be evaluated locally. Both are used later as foundation of the compared retrieval-augmented generation systems. ChatGPT is probably the most commonly known LLM and included in the first test.

The training of an LLM requires massive computational resources and sufficient amount of memory. Llama 3.2 3B required up to 460k GPU hours of training on data center Nvidia H100-80GB GPUs \cite{Llama3.2}. Such training expenses are not feasible for individual research groups.
After the LLM has been trained, the required amount of memory to operate the LLM reduces to storing the model parameters, either in RAM or in the VRAM on a GPU and performing its evaluation. The required amount of memory can be further reduced by applying a method called quantization. Therefore, the parameters are converted from high precision 32-bit or 16-bit floating point numbers to lower-precision format \cite{Liu24}, like 8-bit or 4-bit integers. The 3-billion-parameter model Llama 3.2 3B for example, takes about 1.8 GB when the parameters are in stored in 4-byte integer format. However, this comes with the price of less accuracy.
Small models, like Llama 3.2 3B, can be fitted on current gaming GPUs and evaluated in reasonable times. Larger models with 70B parameters or more require more specialized GPUs, though.

\begin{table}[ht!]
  \center
  \caption{Overview of the LLMs used in the first example.}
  \label{tab:llm_overview}
  \begin{tabular}{p{2.35cm}p{2.1cm}p{2.7cm}}
    \toprule
    LLM & parameters & max. input/context length \\
    \midrule
      ChatGPT 4o & 1 - 1.8 trillion\footnotemark & 128,000 tokens \cite{OpenAi4o} \\
      Gemini 1.5 Flash & ?\footnotemark & 1,048,576 token \cite{GeminiAPIDocs} \\
      Llama 3.2 3B (local) & 3.21 billion \cite{Llama3.2} & 128,000 tokens \cite{Llama3.2} \\
    \bottomrule
  \end{tabular}
\end{table}
\footnotetext[1]{Officially OpenAI does not disclose the number of parameters of ChatGPT, thus only estimations are available \cite{perplexity,ExploringTopics}.}
\footnotetext[2]{Google does not disclose the parameter size \cite{Beebom}.}

\subsection{Example 1: Knowledge about Pasimodo}

Depending on their training data, LLMs are able to answer questions reliable without further data. Therefore, the LLMs listed in Table~\ref{tab:llm_overview} are asked about Pasimodo and their responses are listed in Table~\ref{tab:example_pasi}.

Whereas the local evaluation of the Llama 3.2 3B model diverted and offered a summery on the fictional character Quasimodo, the other two LLMs performed a web search about Pasimodo. Both compiled their answers from the websites available on Pasimodo, which they listed as sources too. Apparently, all three LLMs did not include any information about Pasimodo in their training. The web searches of ChatGPT and Gemini indicate that both are not pure LLMs but rather have already capabilities to retrieve additional information before generating their answers. However, as the public information on Pasimodo is limited, more detailed questions were not asked. The next section will present an approach to leverage more internal knowledge about Pasimodo for answering questions and task using LLMs.

\begin{table*}[ht!]
  \center
  \caption{Answers on the question: \enquote{What do you know about Pasimodo?}.}
  \label{tab:example_pasi}
  \begin{tabular}{p{3cm}p{13cm}}
    \toprule
    LLM & output (shortened)\\
    \midrule
    Llama 3.2 3B & \enquote{I think you meant "Quasimodo". Quasimodo is the fictional protagonist of Victor Hugo's novel "The Hunchback of Notre-Dame", published in 1831. He is a deformed and lonely bell-ringer who lives in the famous Notre-Dame Cathedral in Paris. [\dots]} \\
    ChatGPT 4o & \enquote{Searched 3 sites: Pasimodo is a software package designed for particle-based simulation methods, particularly suited for the study of granular media and Smoothed Particle Hydrodynamics (SPH) simulations. Developed by the Institute of Engineering and Computational Mechanics (ITM) at the University of Stuttgart, [\dots]} \\
    Gemini 1.5 Flash & \enquote{Pasimodo is a program package for particle-based simulation methods. It is primarily used for simulating granular media, such as sand, gravel, and chemical industry granulates. However, it can also be used for other Lagrange-based methods. [\dots]}

    under the answer the ITM Pasimodo website was listed as source\\
    \bottomrule
  \end{tabular}
\end{table*}

\section{Application of Retrieval-Augmented Generation}

Retrieval-Augmented Generation (RAG) \cite{Lewis20} refers to an approach where additional information is retrieved and provided additionally to the LLM. In consequence, more knowledge can be used than the internal memory of the LLM. Next follows a brief introduction into RAG, which is followed by examples of the out-of-the-box performance of two available solutions. Therefore, NotebookLM \cite{notebooklm}, a RAG tool from Google using Gemini 1.5, and AnythingLLM \cite{AnythingLLM}, an open-source RAG application, which can be run locally using a local LLM instance, are tested.


\subsection{Brief Introduction into Retrieval-Augmented Generation}

A RAG system combines the abilities of an LLM with an additional knowledge database to provide up-to-date and context-specific information. It combines the information retrieval with natural language processing. The additional information can be internal documents, wiki or internet pages, or existing databases. The user prompt is then used to perform a semantic search on the document, in some cases also a fuzzy search or a search engine in the case of webpages can be used.

Before the documents can be retrieved, they need to be added to a vector database. Therefore, they are embedded into a vector representation, similar to the embedding of the user prompt done by the LLM. Depending on the data format of the documents, they are converted into plain text files first before embedding. LLMs are limited in the input length they can handle, thus the input documents are also split into smaller parts, so-called chunks \cite{Gao2023}.

For new user prompts, the retriever searches based on the embedded user prompt for close matches within the vector database. The retrieved documents with the highest similarity to the query are combined with the original prompt, which is called augmentation, before they are handed for the response generation \cite{Gao2023}. For the response generation, any existing LLM can be used. As with the classic use of the LLM, previous dialog can be added too.

\subsection{System Setup}

NotebookLM does not provide much setup options allowing only the selection of the uploaded sources which should be used for answering the posted questions. AnythingLLM allows for more modifications such as choosing the used LLM, the vector database storing the provided data, and more. However, for AnythingLLM the standard settings were retained, only a local LLM was used. Therefore, an existing local computer was used for the local system whose properties are listed in Table~\ref{tab:system_properties}. Especially, the GPU is important allowing the fast evaluation of the LLM. However, the limited GPU memory prevents the execution of larger LLMs with 7 billion parameters or more.

\begin{table}[ht!]
  \center
  \caption{Local setup for executing AnythingLLM.}
  \label{tab:system_properties}
  \begin{tabular}{ll}
    \toprule
    Component & Setup \\
    \midrule
      CPU & AMD Ryzen Threadripper 3970X\\
      RAM & 256 GB\\
      GPU & NVIDIA GeForce RTX 4070 (12 GB)\\
      LLM Provider & Ollama \cite{ollama} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Data Preprocessing}

The experiments are conducted using a collection of input examples of Pasimodo and a documentation on the most important points written as wiki including multiple tutorials for creating models with Pasimodo. The same data is supplied to new students as introduction and first steps to get familiar with Pasimodo.

The data needs to be pre-processed before adding it to the RAG systems. NotebookLM has a limit of 50 sources, e.g. pdf, text, or markdown-files, which each can contain up to 500,000 words. First, the repositories containing the input examples and the wiki were converted into a single text file. These text files were split into multiple files each containing 400,000 words, which were identified by being separated by spaces, equals signs, and slashes. In a first test, NotebookLM removed all XML markup from the uploaded plain text files. This was problematic as the input files for Pasimodo are written in XML, therefore, the output of NotebookLM was utter garbage. To prevent the XML markup removal, the plain text files were printed as pdf files before uploading them to NotebookLM. The same pdf files were added to AnythingLLM without further processing.

\subsection{Example 2: Knowledge about Pasimodo using RAG}

The previous test is repeated with the RAG systems questioning both about their knowledge regarding Pasimodo. The answers are given in Table~\ref{tab:example_pasi_rag}. This time, the systems answer the question with more details about Pasimodo. In particular, both systems replicate the information provided in the Pasimodo wiki which was provided to them as input. Both tools provide also citations, what parts of the sources were considered for generating the input. By default, AnythingLLM is limited to retrieving 4 sources. A limit for NotebookLM cannot be determined.

\begin{table*}[ht!]
  \center
  \caption{Answers using RAG on the question \enquote{What do you know about Pasimodo?} in comparison with a text excerpt from the Pasimodo documentation.}
  \label{tab:example_pasi_rag}
  \begin{tabular}{p{3cm}p{13cm}}
    \toprule
    RAG & output (shortened)\\
    \midrule
    NotebookLM\footnotemark & Pasimodo is a software package for the simulation of particle-based systems considering different types of interactions. It can be used as a simple command line based tool controlled by command line arguments and input files, or in combination with a GUI frontend that allows convenient post-processing with color coding of particle states. As a command line tool, Pasimodo can also be used in an MPI-based parallel environment with distributed memory parallelism. Pasimodo is written entirely in C++ and utilizes the advantages of object-oriented programming. [\dots] \\
    \raggedright AnythingLLM with Llama 3.2 3B & \enquote{Pasimodo is a program package for simulating particle-based systems, considering different types of interactions. It can be used as a command-line based tool controlled by command line arguments and input files, or in combination with a GUI frontend for comfortable post-processing with color coding of particle states. Pasimodo is written in C++ and takes advantage of parallel processing capabilities to simulate distributed memory parallelism.} \\
    Pasimodo Wiki & \enquote{Pasimodo is a program package for the simulation of particle based systems considering different types of interactions. It can be used as a simple command line based tool that is controlled by command line arguments and input files or in combination with a GUI frontend that allows for comfortable post processing with color coding of particle states. As a command line tool, pasimodo can be also used in a MPI based parallel environment featuring distributed memory parallelism. Pasimodo is fully written in C++, taking advantage of the intuitive decomposability of the applied particle simulation algorithm into a class hierarchy.} \\
    \bottomrule
  \end{tabular}
\end{table*}
\footnotetext[3]{NotebookLM answered consistently in German despite the questions were in English. This is probably since the author's Google account is set up in German. NotebookLM does not provide an independent language setting. The answers were therefore translated into English.}

\subsection{Example 3: How to define a liquid with the properties of oil?}

After showing that both systems are able to access the provided data, we test their abilities on unknown questions. In general, the Wiki and the input examples are using water for the fluid simulations. Therefore, the systems are asked how a liquid with the properties of oil is defined. The answers are listed in Table~\ref{tab:example_oil_example}.

\begin{table*}[ht!]
  \center
  \caption{Answers for \enquote{How do I define a liquid with the properties of oil?}.}
  \label{tab:example_oil_example}
  \begin{tabular}{p{3cm}p{13cm}}
    \toprule
    RAG & output (shortened)\\
    \midrule
    NotebookLM & To define a fluid with the properties of oil, you must adjust the parameters within the \verb|<SPH_Fluid>|  tag. Here are the most important parameters you should pay attention to:
    \begin{itemize}
      \item \verb|dynamic_viscosity|: This parameter controls the viscosity of the fluid. Oil has a higher viscosity than water. You should therefore increase this value to simulate the behavior of oil. Typical values for the dynamic viscosity of oil are in the range of 0.03 to 10 Pa-s. [\dots]
      \item \verb|reference_density|: This parameter defines the reference density of the fluid. The density of oil is lower than that of water.  You should therefore reduce this value to represent oil. The density of oil is typically in the range of 800 to 950 $kg/m^3$ [\dots].
      \item \verb|sound_velocity|: This parameter influences the compressibility of the fluid. The sound velocity in oil is lower than in water. You can adjust this value to more accurately reflect the real behavior of oil. The speed of sound in oil is in the range of 1300 to 1700 m/s. [\dots]
    \end{itemize} \\
    AnythingLLM & To define a liquid (in this case, an “oil”) with the properties of water using \verb|SPH_Fluid|, you would need to modify the existing parameters in the knowledge base.

    Specifically, you would set \verb|reference_density| to a lower value than 1000.0 (which is typically used for water), and adjust other relevant parameters as needed to match the properties of oil.
    For example:
    \begin{lstlisting}[language={XML}]
    <SPH_Fluid>
      reference_density: 800.0
      dynamic_viscosity: 0.8
      sound_velocity: '10*sqrt(2*9.81*height)'
    </SPH_Fluid>
    \end{lstlisting}
    
    Note that these values are only examples and may need to be adjusted further to accurately represent the properties of oil.

    Also, keep in mind that this is a simplified representation and actual behavior of oil will depend on many factors such as temperature, pressure, and other environmental conditions.\\
    \bottomrule
  \end{tabular}
\end{table*}

Both systems are able to provide the basics parameters of the \verb|SPH_Fluid| component, which are important for defining a liquid within Pasimodo. NotebookLM provides a more detailed answer explaining the individual parameters and providing common ranges for the physical parameters of oil. The answer of AnythingLLM is not that detailed, but even contains an example. However, the syntax of the provided example is incorrect. A deeper look in the citations provided show that the selected text chunks do not include an actual example defining a \verb|SPH_Fluid| component. Instead, a text excerpt containing a list of the \verb|SPH_Fluid| component properties from one of the tutorials is ranked with the highest score.

Based on the query, AnythingLLM provides to the LLM four text chunks from the provided documents that are \enquote{relevant} to the prompt \cite{AnythingLLM_docs}. The relevance is determined by finding close neighbors in the vector database storing the provided sources. Furthermore, these chunks have by default only a length of 1,000 characters. In consequence, the LLM did not see a correct syntax example from which it would be able to create a correct answer. Furthermore, the values provided for the properties in the example are just copied from this text excerpt.

\subsection{Example 4: Simulation Model with an SPH Particle and the Properties of Oil}

Next, both systems are asked to create a complete input file for a simulation with an SPH particle and the properties of oil. Table~\ref{tab:example_oil_model} lists excerpts from the answers of both system. The output is shortened, and the whitespace is manually adjusted. Both input files from the answers are syntactically correct, but some shortcomings are present. AnythingLLM is able to correct the definition of the SPH fluid with the new citations found for the new prompt but includes variables such as \emph{height} or \emph{density\_in} which are not defined as constants beforehand. NotebookLM, on the other hand, provides all variables used through the input file in the \emph{Constants} section while taking over values for oil in the ranges earlier provided. AnythingLLM already includes a particle state used for the kernel gradient correction scheme as it mentioned in one of its citations, whereas NotebookLM does not include any additional particle state.

Both systems include the needed neighborhood search in the response, but AnythingLLM adds the wrong attributes. Apparently the retrieved text chunk ends immediately after the first line of the neighborhood search definition, therefore, lacking any information about its attributes. Funnily enough, it copies a typo in the documentation (double \emph{oo} in \emph{Neighboorhood Search}).
NotebookLM, furthermore, includes the basic SPH fluid interaction between the individual liquid particles whereas AnythingLLM only adds an empty scenarios section. However, NotebookLM uses an outdated formulation for the fluid interaction, which is listed in the documentation next to the current implementation.

\begin{table*}[htbp!]
  \center
  \caption{Excerpts for the prompt to create an input file simulation with an SPH particle and the properties of oil. Whitespace manually adjusted and output shortened.}
  \label{tab:example_oil_model}
  \begin{tabular}{p{8cm}p{8cm}}
    \toprule
    AnythingLLM & NotebookLM\\
    \midrule
\begin{lstlisting}[language={XML},linewidth=8cm]
<Constants>
    <!-- Simulation duration-->
    <P n = "duration" v = "15"/>
    <!-- SPH particle radius -->
    <P n = "radius" v = "0.0375"/>
    <!-- Simulation time step -->
    <P n = "time_step" v = "1e-6"/>
</Constants>
\end{lstlisting}
    &
\begin{lstlisting}[language={XML},linewidth=8cm]
<Constants>
    <!-- Reference density of oil -->
    <P n="density" v="900.0" />
    <!-- Dynamic viscosity of oil -->
    <P n="dyn_visc" v="0.1" />
    <!-- Speed of sound in oil -->
    <P n="sound_vel" v="1400.0" />
    <!-- Gravity vector  -->
    <P n="gravity" v="(0,0,-9.81)" />
</Constants>
\end{lstlisting}
    \\
\begin{lstlisting}[language={XML},linewidth=8cm]
<SPH_Particle
    mass               = "'single_mass'"
    density            = "'density_in'"
    smoothing_length   = "'1.5*init_dx'"
    fixed_translation  = "0"
>
    <SPH_Fluid
        reference_density     = "800.0"
        dynamic_viscosity     = "0.8"
        sound_velocity        = "'abs(10*sqrt(2*9.81*height))'"
    />
    <SPH_KernelGradCorrection_State/>
</SPH_Particle>
\end{lstlisting}
    &
\begin{lstlisting}[language={XML},linewidth=8cm]
<SPH_Particle
    mass="0.001"
    density="'density'"
    smoothing_length="0.01"
>
    <SPH_Fluid
        sound_velocity="'sound_vel'"
        dynamic_viscosity="'dyn_visc'"
        reference_density="'density'"
    />
</SPH_Particle>
\end{lstlisting}
  \\
\begin{lstlisting}[language={XML},linewidth=8cm]
<Interactions>
    <!-- Neighboorhood Search for SPH
         particles to fill interaction
         cache -->
    <SPH_Interaction_Neighborhood_Search
        n = "num_neighbors"
        v = "10"
    />
</Interactions>
<Scenarios>
    <!-- Add scenarios here if needed -->
</Scenarios>
\end{lstlisting}
  &
\begin{lstlisting}[language={XML},linewidth=8cm]
<Interactions>
    <!-- Neighborhood search for SPH particles -->
    <SPH_Interaction_Neighborhood_Search 
        force_for_types="SPH_Particle,SPH_Particle"
        only_for_forced_pairs="true"
    />

    <!-- Definition of the fluid interaction -->
    <SPH_Scenario_Interaction_Fluid>
        <Nested name="interaction">
            <SPH_Interaction_Fluid
                initial_particle_spacing="0.01"
            >
                <Nested name="kernel">
                    <SPH_Wendland_Kernel
                        dim="3"
                    />
                </Nested>
            </SPH_Interaction_Fluid>
        </Nested>
    </SPH_Scenario_Interaction_Fluid>
</Interactions>
\end{lstlisting}
  \\
    \bottomrule
  \end{tabular}
\end{table*}

% /home/itm/abaumann/30_Simulationen/10_Minimal_Beispiele/310_AI_Exmaples/AnythingLLM/20241216_oil_example.pre
% /home/itm/abaumann/30_Simulationen/10_Minimal_Beispiele/310_AI_Exmaples/NotebookLM_V1/20241119_oil_example.pre

\subsection{Example 5: Replying to the RAG Systems Response}

As both simulation models created in the previous section are either incomplete or have several issues, issues are pointed out to the RAG systems and they are asked to correct them. NotebookLM was told it uses an older version of the SPH scenario and to use the newer version, which it correctly included. The choice of the sound velocity as the physical value of 1400 m/s was not corrected by us as detailed knowledge about its choice is not part of the Pasimodo documentation. In the past, students are referred to the SPH literature \cite{Monaghan1994} for deeper information about this simulation parameter, so the tests reveal some gaps in the documentation.

The same applies for the output of AnythingLLM, where an approximation for the artificial sound velocity ($10*\textrm{sqrt}(2*9.81*height)$) is used, which is given in \cite{Monaghan1994} for the collapse of a dam. Apparently, one of the used text chunks originates from a bursting dam example.
Instead, the RAG system is instructed to add the definitions of the missing variables (\emph{single\_mass, density\_in, init\_dx, height}), to fix the neighborhood search attributes, and to include the missing SPH fluid scenario. However, the updated response does not fix the mentioned problems, but the LLM adds new variables (e.g.\ \emph{fluid\_mass\_in}) and expands the attributes of the neighborhood search leading to a worse output. Listing~\ref{lst:reply_to_fix} shows a small excerpt on the updated input file. Even providing an example of the correct neighborhood interaction definition does not help the RAG system to improve its answer.

In conclusion, providing additional information, e.g. about issues in previous responses, can help the LLM and the RAG system to improve their response in some cases. This is described in literature as few-shot learning \cite{Brown2020}. However, especially for knowledge-intensive problems, this is rather difficult, and can lead to further issues in the response as seen.

\begin{lstlisting}[label=lst:reply_to_fix,caption={Excerpt on the output of AnythingLLM after pointing out the previous issues with the missing variables and the neighborhood search attributes},language={XML}]
<!-- Define the particle density
     and single mass -->
<P
    n = "fluid_density_in"
    v = "fluid_mass_in/fluid_volume"
/> 
<P
    n = "single_mass"
    v = "fluid_mass_in"
/>

<!-- Neighboorhood Search for SPH
     particles to fill interaction
     cache -->
<SPH_Interaction_Neighborhood_Search
    num_neighbors_per_particle  = "10"
    volume_factor             = "1.0"
/>
\end{lstlisting}

% The variables single_mass, density_in, init_dx, and height are not defined. The neighborhood search contains the wrong arguments and the fluid SPH scenario is missing. Add the definitions of the missing variables, fix the neighborhood search arguments, and include the missing scenario


% NotebookLM folgstaendigeres beispiel, inkludiert sogar variable und alle deren definitionen, allerdings auch hier probleme die auf unvollstaendiges wissen hindeuten:

% beide beispiele keine definition der partikel size, smoothing length als festser wert definiert
% und veraltetes scenario verwendet.


\section{Conclusions and Outlook}

Large Language Models have become very useful tools in creating text or developing scripts and software. Information that has not been part of their training sets is not available for them. Especially for closed-source software, LLMs cannot be applied as they lack the information for solving tasks about them.

Retrieval-Augmented Generation presents an approach to overcome this issue by providing additional information to the LLM along the user prompt. A commercial implementation, NotebookLM, and an open-source version, AnythingLLM, of such a RAG system have been tested in this work. Whereas NotebookLM is powered by the LLM Gemini from Google, AnythingLLM can be set up using a local LLM allowing to keep all information locally. As local LLM, the open-source Llama 3.2 3B model from Meta was used. The test of the RAG systems was carried out using the closed-source simulation software Pasimodo. Both RAG systems were supplied with a collection of input examples and the short documentation for Pasimodo.

The two systems were tested using several examples. In addition to defining a fluid with the properties of oil, the systems were instructed to create a complete input example. Both systems are able to answer basic questions to previously unknown information and about simulation aspects such as the definition of liquids for Pasimodo.

However, for more detailed inquiries, the RAG systems have their limits. In general, NotebookLM provided better results, but it was not immune to contradictory information such as double definition of the fluid scenario including an older version of it. Especially, the choice of retrieved additional knowledge which was handed to the LLM before answering the user input proved to be critical.

Further improvement of the user experience of the tested RAG systems would be desirable. Both systems are designed as chat interfaces with a linear history or order. In general, the chat history is also provided as context for later prompts. As the LLMs deviated on complex problems, the ability to revert the history or to branch of a response in multiple separate follow-up prompts would be beneficial. This would increase the control on the information provided to the LLM and hopefully result in better responses.

AnythingLLM retrieved only four text chunks of 1,000 character length from the provided sources by default. This did not offer sufficient information for the RAG system to answer complicate questions like creating a complete simulation input file. NotebookLM does not disclose the number of retrieved sources, but the citations offered along the answer indicate that more and longer text chunks are used than in AnythingLLM. When lacking context, AnythingLLM respective the underlying Llama 3.2 3B LLM stuck closely to the provided information, leading often to merely copying text from its citations. As we have seen, the quality of the answer depends largely on the provided text excerpts to the LLM. Therefore, the choice and amount of sources provided to the LLM are challenges for the further improvement of the systems. Depending on the selected LLM, the maximum input length the LLM can handle larger inputs, thus larger and more text chunks could be provided.

Besides the potential of improvement the actual retriever provides, other options need to be tested in the future. The capabilities of LLMs are increasing, allowing them to execute generated source code or calling tools to provide additional input for generating their responses. Larger version of the applied Llama LLM, such as the 7B and 70B, are promised to better be able to use these tools while maintaining the conversation with the user \cite{LlamaToolCalling}. Allowing the LLM to retrieve information as needed or additional lookups for the components used in input examples offer possible improvements for the system.

Additional potential for improving the responses offers the fine-tuning the LLM by training it on new data. Especially for the specific format of Pasimodo input files, this fine-tuning could be carried out on the set of Pasimodo input examples. However, the before-mentioned challenges of the required computational effort remain.

The tests provided insights for the further application of LLMs and RAG systems. The examples of applications range from details about single model parts to creating complete simulation models. In general, the problem must be clearly described regarding dimension and resolution. The results for the examples also reveal that the documentation is lacking information in some points. An example for this is the choice of the artificial sound velocity, a simulation parameter for which the relevant literature is usually referred. Therefore, future work must also include the improvement of the used information sources. The RAG system also provides reference points for this.

It should also be mentioned, that the results of LLMs are hardly reproducible due to the inherent nature of LLMs. Furthermore, as already discussed, especially retrieving different sources leads to significantly different outcomes. This can be seen especially for NotebookLM, which received an update after the tests were conducted and now is powered by latest version of the Gemini 2.0 Flash LLM. In general, NotebookLM showed the better results compared to AnythingLLM, however, responses with syntax error could also be seen in later tests. The outcome depends significantly on the provided sources and their retrieval.

In summary, the out-of-the-box results for both RAG systems tested are impressive. Asking questions and generating simulation models in natural language offers immense support and time reduction for the user, especially for new users of a simulation software like Pasimodo. However, several challenges remain as issues in the LLM responses remain. The RAG system revealed gaps in the sources provided to it, as well as in its internal source retrieval. Furthermore, the potential of fine-adjustments like adapting the retriever to provide larger text chunks or complete documents, applying larger LLMs, or adding other information sources like the API reference of Pasimodo, are questions of further research.



% grosse abhaengigkeit von den uebergebenen quellen in retriever, hier sind die groesten arbeiten notwendig. insb. der starke kontext abheagiges wissen benoetigt.
% bei weiteren versuchen auch beispiele die syntaktisch nicht korrekt sind

\begin{acknowledgements}
This research was supported by the Deutsche Forschungsgemeinschaft (DFG) under grant numbers 405605200 (EB 195/30-1 and EB 195/30-3).

\end{acknowledgements}

\section*{Usage of LLMs}
Apart from the experiments performed with LLMs, as clearly stated in the paper, we used LLMs
for spell checking, text improvement, and translation of text snippets from our mother tongue to English.

% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
\section*{Conflict of interest}
%
The authors declare that they have no conflict of interest.


% BibTeX users please use one of
% \bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}      % mathematics and physical sciences
% \bibliographystyle{spphys}       % APS-like style for physics
\bibliography{bibliography.bib}   % name your BibTeX data base

% Non-BibTeX users please use
% \begin{thebibliography}{}
% %
% % and use \bibitem to create references. Consult the Instructions
% % for authors for reference list style.
% %
% \bibitem{RefJ}
% % Format for Journal Reference
% Author, Article title, Journal, Volume, page numbers (year)
% % Format for books
% \bibitem{RefB}
% Author, Book title, page numbers. Publisher, place (year)
% % etc
% \end{thebibliography}

\end{document}
% end of file template.tex
