\section{Related Works}
\paragraph{Computational Optimal Transport.} 
Since the Sinkhorn algorithm was introduced by \citet{cuturi2013sinkhorn}, many improvements have been proposed for large-scale OT problems. Key advancements include the Newton accelerated algorithm \citep{tangaccelerating}, constrained OT \citep{tang2024sinkhorn}, and robust sparsification methods \citep{tang2024safe}. Other techniques, such as stochastic optimization \citep{altschuler2017near}, low-rank factorization \citep{scetbon2021low}, and kernel approximation \citep{solomon2015convolutional,altschuler2019massively,scetbon2020linear,huguet2023geodesic}, have also been developed to enhance efficiency. Some studies, like \citet{benamou2015iterative}, explore alternative entropy terms but often neglect computational speed or hyperparameter tuning. Despite these advances, existing methods still struggle with accuracy, efficiency, and robustness. Acceleration techniques often sacrifice precision or stability, while sparsification and approximation methods can fail in certain scenarios. These limitations underscore the need for more balanced solutions, which we will address in this work.

\paragraph{Proximal Methods.} Proximal methods are widely used in optimization and machine learning, with Bregman-distance-based proximal point algorithms showing strong performance for structured linear programming problems \citep{censor1992proximal,chen1993convergence,eckstein1993nonlinear,eckstein1998approximate,xie2020fast,chu2023efficient}. 

\paragraph{Applications of OT.}
OT has broad applications in machine learning. For example, solving entropic OT problems on path spaces enhances the efficiency of diffusion models \citep{de2021diffusion,wang2021deep} and flow matching \citep{shi2024diffusion}. OT distances are also widely used in large-scale multimodal pre-trained models \citep{wang2023large,wang2024recent} and for identifying distributional shifts in transfer learning and domain adaptation \citep{fatras2021unbalanced,chang2022unified}. In reinforcement learning, OT helps interpret offline rewards, analyze distribution shifts, measure uncertainty, and introduce a distributional perspective \citep{luo2023optimal,wu2024neural,kulinski2023towards,bellemare2017distributional,dabney2018distributional}. Further applications in machine learning are detailed in \citet{torres2021survey,montesuma2024recent}.