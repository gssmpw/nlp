\section{Related Works}
\paragraph{Computational Optimal Transport.} 
Since the Sinkhorn algorithm was introduced by ____, many improvements have been proposed for large-scale OT problems. Key advancements include the Newton accelerated algorithm ____, constrained OT ____, and robust sparsification methods ____. Other techniques, such as stochastic optimization ____, low-rank factorization ____, and kernel approximation ____, have also been developed to enhance efficiency. Some studies, like ____, explore alternative entropy terms but often neglect computational speed or hyperparameter tuning. Despite these advances, existing methods still struggle with accuracy, efficiency, and robustness. Acceleration techniques often sacrifice precision or stability, while sparsification and approximation methods can fail in certain scenarios. These limitations underscore the need for more balanced solutions, which we will address in this work.

\paragraph{Proximal Methods.} Proximal methods are widely used in optimization and machine learning, with Bregman-distance-based proximal point algorithms showing strong performance for structured linear programming problems ____. 

\paragraph{Applications of OT.}
OT has broad applications in machine learning. For example, solving entropic OT problems on path spaces enhances the efficiency of diffusion models ____ and flow matching ____. OT distances are also widely used in large-scale multimodal pre-trained models ____ and for identifying distributional shifts in transfer learning and domain adaptation ____. In reinforcement learning, OT helps interpret offline rewards, analyze distribution shifts, measure uncertainty, and introduce a distributional perspective ____. Further applications in machine learning are detailed in ____.