%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
\usepackage[preprint]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\input{defs}
\newcommand{\inprod}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\usepackage{tcolorbox}
\newtcolorbox{thmbox}{colback=cyan!5,colframe=white,top=2pt,bottom=2pt,left=0pt,right=2pt}
\newtcolorbox{questionbox}{colback=black!5!white,colframe=white}
\usepackage{enumitem}
\newcommand{\ls}[1]
{
    {\color{red}{\bf\sf [LS: #1]}}
}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{PINS: Proximal Iterations with Sparse Newton and Sinkhorn \\ for Optimal Transport}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Di Wu}{equal,umd}
\icmlauthor{Ling Liang}{equal,umd}
\icmlauthor{Haizhao Yang}{umd}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{umd}{University of Maryland, College Park, MD, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Haizhao Yang}{hzyang@umd.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Optimal transport (OT) is a critical problem in optimization and machine learning, where accuracy and efficiency are paramount. Although entropic regularization and the Sinkhorn algorithm improve scalability, they frequently encounter numerical instability and slow convergence, especially when the regularization parameter is small. In this work, we introduce \textbf{P}roximal \textbf{I}terations with Sparse \textbf{N}ewton and \textbf{S}inkhorn methods (\textbf{PINS}) to efficiently compute highly accurate solutions for large-scale OT problems. A reduced computational complexity through overall sparsity and global convergence are guaranteed by rigorous theoretical analysis. Our approach offers three key advantages: it achieves accuracy comparable to exact solutions, progressively accelerates each iteration for greater efficiency, and enhances robustness by reducing sensitivity to regularization parameters. Extensive experiments confirm these advantages, demonstrating superior performance compared to related methods.
\end{abstract}



\section{Introduction}
\label{intro}

Optimal transport (OT) \citep{villani2009optimal} is a foundational mathematical framework with broad applications across diverse fields such as economics \citep{galichon2018optimal}, physics \citep{de2015power}, and machine learning \citep{peyre2019computational}. At its core, OT seeks to find the most efficient way to transport mass from one probability distribution to another while minimizing the total transportation cost. This problem has attracted significant attention in recent years, particularly due to its relevance in machine learning, where it has been employed for tasks such as generative modeling, domain adaptation, and representation learning. Consequently, there has been a surge of interest in developing scalable and efficient algorithms to tackle large-scale OT problems \citep{sandler2011nonnegative,jitkrittum2016interpretable,arjovsky2017wasserstein,salimans2018improving,genevay2018learning,gao2019deep,chen2020graph,wang2021deep,fatras2021unbalanced,yang2024corrected,hou2024sparse,liang2024accelerating,zhu2024ripalm}. These advancements have not only improved computational efficiency but also expanded the applicability of OT to more complex and high-dimensional settings.

In this work, we consider the classical discrete optimal transport problem that admits a linear programming formulation \citep{villani2009optimal} of the following form:
\begin{equation}
    \label{ot}
    \begin{aligned}
        \min_{X\in\sR^{m\times n}}&\quad \inprod{C}{X} \\ \mathrm{s.t.}&\quad Xe_n = a,\; X^{\top}e_m = b,\; X\geq 0,
    \end{aligned}
\end{equation}
where $C\in\sR^{m\times n}$ is the cost matrix, $e_m$ and $e_n$ are the vectors of ones in $\sR^m$ and $\sR^n$, respectively, and $a\in\sR^m_{++}$ and $b\in\sR^n_{++}$ are given vectors satisfying $e_m^{\top}a = e_n^{\top}b = 1$. Here, the decision matrix $X$ can be interpreted as a joint distribution with marginals $a$ and $b$. Solving the above linear programming problem requires significant computational resources. Specifically, for two distributions with components $n$ each, the decision variable has a dimensionality of $ \mathcal{O}(n^2) $. Standard algorithms, such as the simplex method \citep{burkard2012assignment} or interior point methods \citep{nesterov1994interior}, incur a computational cost per iteration of approximately $\mathcal{O}(n^6)$, rendering them impractical for large-scale datasets.

To address the aforementioned challenges, an entropic regularization term was introduced to the objective of the original linear program, leading to a formulation also known as Schr{\"o}dinger’s problem \citep{leonard2013survey}. This regularization modifies the OT problem by adding a controlled level of smoothness, making the problem more numerically stable and computationally tractable. As a result, the Sinkhorn algorithm \citep{cuturi2013sinkhorn} can be employed to efficiently approximate the optimal transport solution through iterative matrix scaling, significantly improving scalability for large datasets. 

However, the entropy-regularized OT solution remains an approximation of the original problem. While reducing the regularization parameter improves approximation accuracy, excessively small regularization values lead to severe numerical issues, including overflow and underflow, due to the exponential computations involved in Sinkhorn updates. This trade-off between approximation accuracy and numerical stability presents a key challenge in applying entropy-regularized OT to real-world large-scale problems. Moreover, the Sinkhorn algorithm often converges slowly. This raises the following question:
\begin{center}
\begin{tcolorbox}[colframe=black, boxrule=1pt, colback=white, width=0.95\linewidth]
    How to design an efficient, robust, and scalable algorithm that can solve the original optimal transport problems with very high accuracy?
\end{tcolorbox}
\end{center}

We address this question by proposing a novel algorithmic framework, called \textbf{PINS}, which stands for \textbf{P}roximal \textbf{I}terations utilizing the sparse \textbf{N}ewton's method with the \textbf{S}inkhorn algorithm. The core idea of PINS is to solve the original OT problem by iteratively solving a sequence of entropy-regularized OT problems with changing cost matrices through a two-phase approach.  

In the first phase, the Sinkhorn algorithm is applied to efficiently compute an approximate solution to the entropy-regularized OT problem. This serves as a well-conditioned starting point, mitigating numerical instability and ensuring rapid convergence in the subsequent phase.  

In the second phase, we employ Newton's method to refine the solution, leveraging second-order information for accelerated convergence. To enhance efficiency, a sparsification technique is applied at each iteration of the Newton method, significantly reducing computational overhead while preserving solution accuracy. By exploiting these two complementary approaches, PINS achieves both scalability and high precision in solving large-scale OT problems.  

Our contributions can be summarized as follows:  
\begin{itemize}
    \item We propose PINS, a proximal iterative scheme that exploits the Sinkhorn algorithm with Newton’s method and sparsification techniques. Unlike entropy-regularized approaches, PINS is capable of computing highly accurate solutions to the original OT problems while maintaining remarkable efficiency and robustness. By leveraging a two-phase framework, PINS first employs the Sinkhorn's algorithm to obtain a well-conditioned initialization, followed by a sparse Newton refinement to accelerate convergence.  

    \item We conduct extensive numerical experiments to validate the practical performance of PINS, demonstrating its three key advantages:  
    \begin{enumerate}
        \item PINS computes the optimal solution with \textbf{high accuracy}, overcoming the approximation limitations of entropy-regularized methods. 
        
        \item The iterative sparsification in the Newton phase leads to cumulative computational savings, significantly enhancing \textbf{overall efficiency}.  
        \item PINS exhibits greater \textbf{robustness} to hyperparameter choices, reducing sensitivity to regularization parameters that often pose challenges in existing methods.  
    \end{enumerate}

    \item We establish \textbf{theoretical guarantees} for the sparsification process in PINS and rigorously prove the \textbf{global convergence} of the algorithm. Our analysis provides insights into the trade-off between computational complexity and solution accuracy, offering a principled foundation for further improvements.
\end{itemize}

\section{Related Works}
\paragraph{Computational Optimal Transport.} 
Since the Sinkhorn algorithm was introduced by \citet{cuturi2013sinkhorn}, many improvements have been proposed for large-scale OT problems. Key advancements include the Newton accelerated algorithm \citep{tangaccelerating}, constrained OT \citep{tang2024sinkhorn}, and robust sparsification methods \citep{tang2024safe}. Other techniques, such as stochastic optimization \citep{altschuler2017near}, low-rank factorization \citep{scetbon2021low}, and kernel approximation \citep{solomon2015convolutional,altschuler2019massively,scetbon2020linear,huguet2023geodesic}, have also been developed to enhance efficiency. Some studies, like \citet{benamou2015iterative}, explore alternative entropy terms but often neglect computational speed or hyperparameter tuning. Despite these advances, existing methods still struggle with accuracy, efficiency, and robustness. Acceleration techniques often sacrifice precision or stability, while sparsification and approximation methods can fail in certain scenarios. These limitations underscore the need for more balanced solutions, which we will address in this work.

\paragraph{Proximal Methods.} Proximal methods are widely used in optimization and machine learning, with Bregman-distance-based proximal point algorithms showing strong performance for structured linear programming problems \citep{censor1992proximal,chen1993convergence,eckstein1993nonlinear,eckstein1998approximate,xie2020fast,chu2023efficient}. 

\paragraph{Applications of OT.}
OT has broad applications in machine learning. For example, solving entropic OT problems on path spaces enhances the efficiency of diffusion models \citep{de2021diffusion,wang2021deep} and flow matching \citep{shi2024diffusion}. OT distances are also widely used in large-scale multimodal pre-trained models \citep{wang2023large,wang2024recent} and for identifying distributional shifts in transfer learning and domain adaptation \citep{fatras2021unbalanced,chang2022unified}. In reinforcement learning, OT helps interpret offline rewards, analyze distribution shifts, measure uncertainty, and introduce a distributional perspective \citep{luo2023optimal,wu2024neural,kulinski2023towards,bellemare2017distributional,dabney2018distributional}. Further applications in machine learning are detailed in \citet{torres2021survey,montesuma2024recent}.

\section{Methodology} \label{sec:method}
In this section, we first review the Sinkhorn algorithm and its accelerated variant. We then highlight significant numerical issues inherent to Sinkhorn-type algorithms. To address these challenges, we propose the PINS algorithm.

\subsection{The Sinkhorn Algorithm}
\label{sinkhorn}
\begin{algorithm}[htb!]
    \begin{algorithmic}[1]
        \STATE \textbf{Inputs:} Cost matrix $C\in\sR^{m\times n}$, marginal distributions $a\in\sR^m$ and $b\in\sR^n$, initial point $(f^0, g^0)\in\sR^{m+n}$, $X^0\in\sR^{m\times n}$, entropy regularization parameter $\eta > 0$.
        \FOR{$k\geq 0$}
            \STATE \begin{small}\(
                 f^{k+1} \leftarrow  f^k + \eta \left(\log(a) - \log \left(X(f^k, g^k)e_n\right)\right).
            \)\end{small}
            \STATE \begin{small}\(
                 g^{k+1} \leftarrow g^k + \eta \left(\log(b) - \log \left(X(f^{k+1}, g^k)^{\top}e_m\right)\right).
            \)\end{small}
        \ENDFOR 
        \STATE \textbf{Output:} $X(f^{k+1}, g^{k+1})$.
    \end{algorithmic}
    \caption{The Sinkhorn Algorithm}
    \label{alg-sinkhorn}
\end{algorithm}
The Sinkhorn algorithm is used to solve the following entropic regularized OT problem \cite{cuturi2013sinkhorn}:
\begin{align}
\begin{split}
    \label{eot}
    \min_{X\in\sR^{m\times n}}\; \inprod{C}{X} + \eta \sum_{i,j}X_{ij}\log (X_{ij}) \\ \mathrm{s.t.}\quad Xe_n = a,\; X^{\top}e_m = b,
\end{split}
\end{align}
where $\eta > 0$ is the entropy regularization parameter. The Lagrangian function for problem (\ref{eot}) is defined as 
\begin{align*}
    L(X, f, g) =& \inprod{C}{X} + \eta \sum_{i,j}X_{ij}\log (X_{ij})
    \\&\quad + \inprod{f}{a - Xe_n} + \inprod{g}{b - X^{\top}e_m},
\end{align*}
where $f\in \sR^m$ and $g\in \sR^n$ are dual variables (i.e., Lagrange multipliers). At optimality, let $X(f,g)$ satisfy $\partial_XL(X(f,g),f,g)  = 0$, then:
\begin{align*}
    &\partial_XL(X(f,g),f,g) \\
    =& C + \eta E + \eta \log (X(f,g)) - fe_n^{\top} - e_mg^{\top} = 0,
\end{align*}
where $E\in\sR^{m\times n}$ is the matrix of all ones. This condition implies that:
\begin{align}
\label{eq:sink_x}
    X(f,g) = \exp\left(\frac{1}{\eta}\left(fe_n^{\top} + e_mg^{\top} - C - \eta E\right)\right).
\end{align}
From Eq. (\ref{eq:sink_x}), the dual problem of problem (\ref{eot}) is derived as:
\begin{align}
\begin{split}
    \label{deot}
    &\max_{f\in\sR^m,\; g\in\sR^n}\; P(f,g) = \inprod{a}{f} + \inprod{b}{g} \\
     & \quad  - \eta \inprod{E}{\exp\left(\frac{1}{\eta}\left(fe_n^{\top} + e_mg^{\top} - C - \eta E\right)\right)}.
\end{split}
\end{align}
Clearly, the partial gradients with respect to $f$ and $g$, and the Hessian of $P(f,g)$ are given by:
\begin{small}\begin{align*}
         \nabla_f P(f,g)  = & a - X(f,g)e_n, \;
         \nabla_g P(f,g) = b - X(f,g)^{\top}e_m,\\
         \nabla^2P(f,g)
         = & -\frac{1}{\eta}
         \begin{bmatrix}
         \diag\left(X(f,g)e_n\right) & X(f,g) \\
             X(f,g)^{\top} & \diag\left(X(f,g)^{\top}e_m\right)
         \end{bmatrix}.
\end{align*}\end{small}

From these expressions, the log-domain version of the Sinkhorn algorithm \citep{schmitzer2019stabilized} can be interpreted as an alternating maximization process, as outlined in Algorithm \ref{alg-sinkhorn}. Compared to the original Sinkhorn algorithm \citep{cuturi2013sinkhorn}, Algorithm \ref{alg-sinkhorn} operates in the log-domain, effectively mitigating numerical instability and improving computational robustness, particularly for relatively small regularization parameters.

\paragraph{Issues in Sinkhorn-Type Algorithms} Despite its utility, the Sinkhorn algorithm suffers from several limitations:
\begin{itemize}[topsep=0cm, parsep=0cm, itemsep=0cm]
    \item \textit{Numerical instability:} The entropy regularization parameter $\eta$ must not be too small, as small values can lead to numerical issues such as overflow or underflow.
    \item \textit{Approximation error:} The algorithm solves the entropic regularized OT problem (\ref{eot}), providing only an approximate solution to the original OT problem (\ref{ot}). Specifically, let $X^*(\eta)$ and $X^*$ be the optimal solutions to problems (\ref{eot}) and (\ref{ot}), respectively. While $\lim_{\eta\downarrow 0^+}X^*(\eta) = X^*$, $\eta$ cannot be too small in practice due to computational constraints, leading to $X^*(\eta)$ being only a rough approximation of $X^*$.
    \item \textit{Slow convergence:} As described in Algorithm \ref{alg-sinkhorn}, the Sinkhorn algorithm uses a first-order alternating maximization approach, which can converge slowly, especially for small regularization parameters \citep{lin2022efficiency}. This slow convergence stems from its iterative updates, which involve repeated matrix scaling operations. To address this, \citep{tangaccelerating} proposed the Newton accelerated algorithm, which combines a second-order Newton method with a sparsification strategy to reduce computational costs. The Newton accelerated method focuses on updating only the most important entries of the transport plan, achieving a balance between accuracy and efficiency. This makes it particularly suitable for large-scale optimal transport problems. Empirical studies show its significant speedups over standard Sinkhorn iterations while maintaining numerical stability.
\end{itemize}

\begin{algorithm}[htb!]
    \begin{algorithmic}[1]
        \STATE \textbf{Inputs:} Cost matrix $C\in\sR^{m\times n}$, marginal distributions $a\in\sR^m$ and $b\in\sR^n$, initial point $(f^0, g^0)\in\sR^{m+n}, X^0\in\sR^{m\times n}$, entropy regularization parameter $\eta > 0$, sparsity threshold $\rho>0$, maximum iterations $T_1$ (Sinkhorn), $T_2$ (Newton), K (EPPA).
        \FOR{$k = 0,\ldots, K-1$}
        \STATE \(C^k \leftarrow C - \eta \log(X^k)\)
        \FOR{$t = 0,\dots, T_1-1$}
            \STATE \begin{small}\(
                 f^{t+1} \leftarrow   f^t + \eta \left(\log(a) - \log \left(\widetilde{X}^{k*}(f^t,g^t)e_n\right)\right)\)\end{small}
            \STATE \begin{small}\(
                 g^{t+1} \leftarrow  g^t + \eta \left(\log(b) - \log \left(\widetilde{X}^{k*}(f^{t+1}, g^t)^{\top}e_m\right)\right)\)\end{small}
        \ENDFOR
        \STATE \textbf{return} $(f^{T_1}, g^{T_1})$
        \FOR{$t = T_1,\dots, T_1+T_2-1$}
            \STATE Compute Hessian $\nabla^2P(f^t, g^t)$ based on Eq. (\ref{eq:eppa-hess}).
            \STATE $H^t \leftarrow \mathrm{Sparsify}(\nabla^2P(f^t, g^t), \rho)$
            \STATE \(
                (\Delta f^t, \Delta g^t) \leftarrow \mathrm{CG}(H^t, -\nabla P(f^t, g^t))
            \)
            \STATE \(
                \alpha^t \leftarrow \mathrm{LineSearch}(P, f^t, g^t, \Delta f^t, \Delta g^t)\)
            \STATE \((f^{t+1}, g^{t+1}) \leftarrow (f^t, g^t) + \alpha^t (\Delta f^t, \Delta g^t)\)
        \ENDFOR
        \STATE \textbf{return} $X^{k+1} \leftarrow \widetilde{X}^{k*}(f^{T_1+T_2}, g^{T_1+T_2})$
        \ENDFOR
        \STATE \textbf{Output:} $X^{K}$
    \end{algorithmic}
    \caption{PINS Algorithm}
    \label{alg-PINS}
\end{algorithm}

\subsection{PINS }
As discussed, Sinkhorn-type algorithms struggle with high accuracy due to their sensitivity to regularization parameters. To tackle these issues while boosting robustness and maintaining efficiency, we propose the PINS algorithm (see Algorithm \ref{alg-PINS}). PINS is an iterative method that innovatively integrates the Sinkhorn algorithm with Newton’s method and sparsification different from \citep{tangaccelerating}. By this new combination, PINS addresses numerical instabilities, speeds up convergence, and improves stability in optimal transport computations. This results in a more reliable and efficient solution for large-scale OT problems.

We first introduce the Boltzmann-Shannon entropy function:
\[
    \phi(X) := \sum_{i,j}X_{ij}\left(\log(X_{ij}) - 1\right),\quad \forall\; X\in\sR^{m\times n}_{+},
\]
with the convention $0\log(0) := 0$. Using this  $\phi$ as the kernel function, the Bregman distance between two distributions $X\in \sR^{m\times n}_{+}, Y\in \sR^{m\times n}_{+}$ is defined as:
\begin{align*}
    D_\phi(X,Y) := \phi(X) - \phi(Y) - \inprod{\nabla\phi(Y)}{X-Y}.
\end{align*}
It is evident that $D(X,Y)\geq 0$ with equality holding if and only if $X = Y$. This allows us to apply the entropic proximal point algorithm (EPPA) \citep{xie2020fast, chu2023efficient}:
\begin{align}
    \begin{split}
    \label{eq:x_inner}
    X^{k+1} \in \mathrm{argmin}\;\{\inprod{C}{X} + \eta D_\phi(X, X^k)  : \\ Xe_n = a, X^{\top}e_m = b\},
    \end{split}
\end{align}
Our focus is on solving the EPPA subproblem efficiently. Rearranging terms and using the fact that for any $X\in \sR^{m\times n}_{+} $ satisfying $Xe_n = a, \; X^{\top}e_m = b$, it holds that $\sum_{ij}X_{ij} = 1$, the minimization problem in (\ref{eq:x_inner}) is equivalent to the following problem:
\begin{align}
\label{eppa-sub}
\begin{split}
    \min_{X\in\sR^{m\times n}}& \; \inprod{C - \eta \log(X^k)}{X} + \eta \sum_{i,j}X_{ij}\log(X_{ij}) \\ \mathrm{s.t.}&\quad Xe_n = a, \; X^{\top}e_m = b,
\end{split}
\end{align}
which resembles the entropic regularized OT problem (\ref{eot}). The primary difference is that the cost matrix is now changed to $C - \eta \log (X^k)$, which is denoted as $C^k$ for notational simplicity.

Similarly, we propose to solve problem (\ref{eppa-sub}) by solving its dual problem. To this end, we need to derive the Lagrangian function for problem (\ref{eppa-sub}) as follows:
\begin{align*}
    L^k(X, f, g):= \inprod{C^k}{X} + \eta \sum_{i,j}X_{ij}\log(X_{ij}) \\ + \inprod{f}{a - Xe_n} + \inprod{g}{b - X^{\top}e_m},
\end{align*}
where $f\in \sR^m$ and $g\in \sR^n$ are dual variables. At optimality, the solution matrix $\widetilde{X}^{k*}(f,g)\in\sR^{m\times n}$ satisfies $\nabla_XL^k(\widetilde{X}^{k*}(f,g), f, g) = 0$. This condition indicates that 
\[
    C^k - fe_n^{\top} - e_mg^{\top} + \eta E + \eta \log(\widetilde{X}^{k*}(f,g)) = 0,
\]
which further implies that 
\begin{align}
\label{eq:eppa_x}
    \widetilde{X}^{k*}(f,g) = \exp\left(\frac{1}{\eta}\left(fe_n^{\top} + e_mg^{\top} - C^k - \eta E\right)\right).
\end{align}
Consequently, the corresponding dual problem of problem (\ref{eppa-sub}) is:
\begin{align}
\label{eppa-sub-dual}
\begin{split}
    &\;\max_{f\in\sR^m,\; g\in\sR^n} P^k(f,g) = \inprod{a}{f} + \inprod{b}{g} \\ 
    &\;- \eta \inprod{E}{\exp\left(\frac{1}{\eta}\left(fe_n^{\top} + e_mg^{\top} - C^k - \eta E\right)\right)}.
\end{split}
\end{align}
And the partial gradients with respect to $f$ and $g$, and the Hessian of $P^k(f,g)$ are given as:
\begin{align}
\begin{split}
    \label{eq:eppa-hess}
         \nabla_f P^k &=  a - \widetilde{X}^k(f,g)e_n, \;
         \nabla_g P^K =  b - \widetilde{X}^k(f,g)^{\top}e_m,\\
         \nabla^2P^k
         &=  -\frac{1}{\eta}\begin{bmatrix}
             \diag\left(\widetilde{X}^ke_n\right) & \widetilde{X}^k(f,g) \\
             \widetilde{X}^k(f,g)^{\top} & \diag\left((\widetilde{X}^k)^{\top}e_m\right)
         \end{bmatrix}.
\end{split}
\end{align}

PINS solves the dual problem (\ref{eppa-sub-dual}) in two stages: (1) the Sinkhorn algorithm provides a coarse solution sufficient to reach Newton’s convergence region, and (2) Newton's method accelerates convergence. To further improve efficiency, a sparsification technique is applied during each Newton iteration. Details are provided in Algorithm \ref{alg-PINS}.

In the remainder of this section, we provide further discussions on several methods utilized in the algorithm.
\paragraph{Sparsification}
The sparsification procedure is employed to reduce the computational cost of solving the Hessian system via a certain iterative linear system solver. Specifically, given a threshold $\rho$, we truncate the $(1-\rho) n^2$-smallest entries in an $n\times n$ matrix.

\paragraph{Conjugate Gradient Method}
The conjugate gradient (CG) method \citep{fletcher1964function} is a Krylov subspace method \citep{nocedal1999numerical} that efficiently solves sparse symmetric linear systems by performing matrix-vector products. In the PINS algorithm, the CG method is used to solve the linear system formed after sparsifying the Hessian matrix in each Newton iteration.
\paragraph{Line Search}
The line search procedure determines the step size $\alpha^t$ in each iteration of the Newton method. Specifically, we employ the backtracking line search approach. To minimize a function $f(x)$, given the current point $x_k$ and the search direction $\Delta_k$, we select the largest step length $\alpha$ such that $f(x_k+\alpha\Delta_k)\leq f(x_k)+\alpha\nabla f(x_k)^{\top}\Delta_k$.

\section{Theoretical Guarantee}
In this section, we first establish the global convergence of PINS, followed by a theoretical analysis of the sparsification procedure employed in each outer loop.

\subsection{Global Convergence}
\begin{theorem}
\label{thm:convergence}
Let $X^k$ be the sequence generated by (\ref{eq:x_inner}). Then $\{X^k\}$ converges to the optimal solution of the original OT problem (\ref{ot}).
\end{theorem}
\begin{remark}
Theorem \ref{thm:convergence} establishes the global convergence of PINS under the assumption that each subproblem is solved exactly, thereby yielding asymptotic convergence. However, even if each subproblem is solved only approximately, convergence is still ensured under suitable inexactness conditions. This can be proved in a similar way with the help of numerical sequence lemmas (see Section 2.2 of \citet{polyak1987introduction}). 
\end{remark}

\begin{proof}
Let $P$ be any matrix satisfying $Pe_n=a,P^{\top}e_m=b$. From the definition of $X^{k+1}$ in (\ref{eq:x_inner}), the following optimality condition holds:
\[\langle C+\eta(\nabla \phi(X^{k+1})-\nabla\phi(X^k)), X^{k+1}-P \rangle \leq 0.\]
Since
\begin{align*}
    \langle &\nabla \phi(X^{k+1})-\nabla \phi(X^k),P-X^{k+1}\rangle \\&=D_{\phi}(P,X^k)-D_{\phi}(P,X^{k+1})-D_{\phi}(X^{k+1},X^k), 
\end{align*}
we obtain
\begin{align*}
    &\quad\langle C,X^{k+1}\rangle \leq \langle C,P\rangle \\
    &+\eta\left(D_{\phi}(P,X^k)-D_{\phi}(P,X^{k+1})-D_{\phi}(X^{k+1},X^k)\right).
\end{align*}
By choosing $P=X^{k}$, it follows that
\begin{align*}
    \langle C,X^{k+1}\rangle \leq \langle C,X^{k}\rangle.
\end{align*}
Since $\{X^k\}$ is bounded from below, the sequence $\{\langle C,X^{k}\rangle\}$ is convergent. Next, let $P=X^*$, where $X^*$ is an optimal solution to the original problem (\ref{ot}). Then
\begin{align*}
    \eta D_{\phi}(X^*,X^{k+1})\leq \eta D_{\phi}(X^*,X^{k}).
\end{align*}
Because $D_{\phi}(X^*,X^{k})$ is bounded from below, $\{D_{\phi}(X^*,X^{k})\}$ is also convergent. Considering $P=X^*$ again, we have
\begin{align*}
     \langle C,X^{k+1}\rangle \leq&\; \langle C,X^*\rangle \\  + &\;\eta D_{\phi}(X^*,X^k)-\eta D_{\phi}(X^*,X^{k+1}).
\end{align*}
Denote the limit of $\{X^{k}\}$ by $X^{\infty}$. Since $\{D_{\phi}(X^*,X^{k})\}$ is convergent and the feasible set is closed, we conclude $X^{\infty}$ must be an optimal solution to (\ref{ot}). This completes the proof.
\end{proof}

\subsection{Sparsification Analysis}
We use the following notations:
\begin{align*}
    \gX^{k}:&=\argmin_{X\in\sR^{m\times n}}\; \inprod{C^k}{X} \quad \mathrm{s.t.}\quad Xe_n = a, \; X^{\top}e_m = b. \\
    \widetilde{X}^{k*}&\in\argmin_{X\in\sR^{m\times n}}\;\inprod{C^k}{X} + \eta \sum_{i,j}X_{ij}\log(X_{ij}) \\ &\quad\quad\quad\mathrm{s.t.}\quad Xe_n = a, \; X^{\top}e_m = b. \\
    \widetilde{X}^{k}&\text{ is the output of the Sinkhorn phase. } \\
    X_{\gX}^{k*}&\in\argmin_{X\in\gX^{k}}\|\widetilde{X}^{k*}-X\|_1. \\
    \tau(A) :&= \text{\#nonzero entries }/\text{ \#entries of $A$}. \\
    \mathcal{P}:&=\{X:Xe_n = a,X^{\top}e_m = b\}. \\
    \Delta &\text{ is the gap in the original objective value between} \\
    &\quad\text{an optimal vertex and a suboptimal vertex in $\mathcal{P}$.}
\end{align*}



\begin{lemma} \label{lem:d1bound} Let $d_1(A,\gB):=\max_{B\in\gB}\|A-B\|_1$. For Problem (\ref{eppa-sub}), if $\eta\leq\frac{\Delta}{R_1+R_H}$, then:
\[d_1(\widetilde{X}^{k*},\gX^{k})\leq2R_1\exp\left(-\frac{\Delta}{\eta R_1}+\frac{R_1+R_H}{R_1}\right).\]
Here $R_1=\max_{X\in\gP}\|X\|_1$, $R_{H}=\max_{X,Y\in\gP}H(X)-H(Y)$. Specially, we have
$R_1 = 1$, $R_H=\log(mn)$.
\end{lemma}
The proof of this lemma follows from Corollary 9 in \cite{weed2018explicit}. Moreover, since $X$ is a doubly stochastic matrix, it is evident that $\|X\|_1=1$. Furthermore, the maximum value of $H(X)$ is achieved when $a$ and $b$ are uniformly distributed, and the minimum value is $0$.


\begin{theorem} \label{thm:sparse} For each subproblem in (\ref{eppa-sub}), there exist constants $\kappa$ and $l$, such that for Sinkhorn steps $T_1>\kappa$ and the regularization coefficient $\eta\leq\frac{\Delta}{1+\log(mn)}$, then one can construct a matrix $H^k$ such that $\tau(H^k)\leq\frac{2mn\tau(X_{\gX}^{k*})+m+n}{(m+n)^2}$ and 
\[\|H^k-\nabla^2P(\widetilde{X}^k)\|_1\leq \eta^{-1}\left(6mn\exp(-\frac{\Delta}{\eta})+\frac{\sqrt{l}}{\sqrt{T_1}}\right).\]
In particular, if $m=n$, then $\tau(H^k)\leq \frac{3n-1}{2n^2}$ almost surely for any subproblem.

\end{theorem}
\begin{remark}
    We observe from Theorem \ref{thm:sparse} that tuning $\eta$ remains a central challenge for both numerical considerations and the accuracy of the sparsification procedure.  On the one hand, if $\eta$ is too small, $\eta^{-1}$ becomes large. On the other hand, if $\eta$ is too large, the constraint condition $\eta\leq\frac{\Delta}{1+\log(mn)}$ might not be satisfied. Moreover, balancing accuracy with numerical stability also complicates this choice. Comprehensive analysis in \citet{tangaccelerating,tang2024safe} confirms that the approximation error scales on the order of $\eta^{-1}$. Notably, PINS offers greater flexibility in selecting $\eta$ than the Sinkhorn algorithm, as it does not rely on a specific value of $\eta$ for convergence. This flexibility underlines the advantages of PINS: it is more user-friendly and less sensitive to hyperparameter tuning.
\end{remark}
\begin{remark}
    This theorem also demonstrates that the solution computed in the first (Sinkhorn) phase of each subproblem is approximately sparse, independent of $k$. Consequently, the sparsification technique can be applied to accelerate the computation of each subproblem.
\end{remark}
\begin{proof}
The proof follows from~\citep{tangaccelerating}. We define an approximate Hessian matrix $H^k\in\mathbb{R}^{(m+n)\times(m+n)}$ by:
\begin{align*}
    H^k=-\frac{1}{\eta}\begin{pmatrix}
        \diag(\widetilde{X}^{k} e_n) & X_{\gX}^{k*} \\
        X_{\gX}^{k*}{}^{\top} & \diag(\widetilde{X}^{k}{}^{\top}e_m)
    \end{pmatrix}.
\end{align*}
The number of nonzero entries in $H^k$ is given by $2mn\tau(X_{\gX}^{k*})+m+n$. The difference between $H^k$ and $\nabla^2P(\widetilde{X}^k)$ is bounded as:
\begin{align*}
    \|H^k-\nabla^2P(\widetilde{X}^k)\|_1=\frac{1}{\eta}\|\widetilde{X}^k-X_{\gX}^{k*}\|_1.
\end{align*}
Using the triangle inequality, the term on the right-hand side can be further bounded:
\begin{align}
    \|\widetilde{X}^{k}-X_{\gX}^{k*}\|_1 &\leq \|\widetilde{X}^{k}-\widetilde{X}^{k*}\|_1+\|X_{\gX}^{k*}-\widetilde{X}^{k*}\|_1 \nonumber \\
    &\leq \|\widetilde{X}^{k}-\widetilde{X}^{k*}\|_1+d_1(\widetilde{X}^{k*},\gX^{k}). \label{eq:bounddcp}
\end{align}
The first term in (\ref{eq:bounddcp}) can be bounded by the Pinsker inequality and Theorem 4.4 in \cite{ghosal2022convergence}. There exists some constants $\kappa,l$, such that for any Sinkhorn steps exceeding $\kappa$, 
$\|\widetilde{X}^{k}-\widetilde{X}^{k*}\|_1\leq\frac{\sqrt{l}}{\sqrt{T_1}}.$ The second term in (\ref{eq:bounddcp}) could be bounded by Lemma \ref{lem:d1bound} when $\eta\leq\frac{\Delta}{1+\log(mn)}$:
\begin{align*}
    d_1(\widetilde{X}^{k*},\gX^{k})&\leq 2\exp(-\frac{\Delta}{\eta}+1+\log(mn)) \\
    &\leq 6mn\exp(-\frac{\Delta}{\eta}).
\end{align*}

Thus, combining the bounds, the Hessian matrix sparsity follows directly.

Finally, based on Theorem 3.7 in \citet{dieci2019boundary}, there is a unique solution to the original unregularized problem almost surely (with probability one). If $m=n$, the feasible set defines a Birkhoff-von Neumann polytope, spanning an $(n-1)^2$-dimensional affine subspace of $\sR^{n^2}$, whose vertices are precisely the permutation matrices \citep{stanley2011enumerative}. According to Theorem 2.7 in \citet{bertsimas1997introduction}, the linear objective's minimum over a nonempty polyhedron is achieved at an extreme point. Thus $\tau(X_{\gX}^{k*})\leq\frac{n^2-(n-1)^2}{n^2}=\frac{2n-1}{n^2}$, which then implies $\tau(H^k)\leq\frac{2n+2(2n-1)}{(2n)^2}=\frac{3n-1}{2n^2}$.
\end{proof}

\section{Numerical Experiments}
To evaluate the properties and performance of the proposed algorithm, PINS, we conducted experiments on datasets of varying sizes. These experiments highlight the advantages of PINS: (a) its ability to find highly accurate solutions; (b) its improved computational efficiency; and (c) its robustness to hyperparameter choices. Detailed descriptions of the experiments can be found in the Appendix. As demonstrated in the following results, PINS outperforms other methods in terms of reduced computation time, robustness, and lower error relative to the exact solution.


\subsection{Datasets} \label{sec:data} Our experiments were conducted on three types of datasets. Synthetic, MNIST, and augmented MNIST.

\noindent\textbf{Synthetic Data.} The first dataset consists of randomly generated data. The random assignment problem with additional constraints is widely studied in the literature \cite{mezard1987solution,steele1997probability,aldous2001zeta}. Specifically, we generate an $n\times n$ cost matrix $C$ with entries $c_{ij}\sim \text{Unif}([0,1])$. The source and target vectors are defined as $a=b=\frac{1}{n}\ones$. The values of $n$ are set to $50,100,200,400$.

\noindent\textbf{MNIST.} We sample two images from the MNIST dataset, a collection of 28$\times$28 grayscale images of handwritten digits. Each image is converted into a probability distribution by normalizing pixel intensities. The optimal transport problem is then solved between the two images using the Euclidean distance of the pixel positions as the cost matrix.

\noindent\textbf{Augmented MNIST.} To increase the problem size, we combine $N^2$ MNIST images into a grid of size $N \times N$, resulting in larger images of size $28N \times 28N$ pixels. This setup introduces two significant challenges. First, the cost matrix becomes extremely large. Second, the augmented images, formed by stacking several MNIST images, contain numerous gaps between sub-images, creating sparse graphs. Consequently, it leads to disconnected regions and restricts the mass-moving path, making the problem more complicated to solve \citep{cipolla2024regularized}.

\begin{remark}
Images in the MNIST dataset contain significant regions of background (black) pixels, which are represented as zeros. This results in many zeros in the vectors $a$ and $b$ in (\ref{ot}). To avoid numerical issues in the algorithm, we preprocess $a$ and $b$ by squeezing their values before inputting them into the algorithm. It is noteworthy that this preprocessing does not diminish the relevance of the augmented MNIST dataset. The primary difficulty lies in the large gaps between sub-images, which make the corresponding optimal transport problem intrinsically ill-posed.
\end{remark}

\subsection{Exact Optimality} \label{sec:optimality}
We conducted experiments to evaluate the performance of PINS. In the figures, the errors are compared with the exact solution of the original optimal transport problem. 

The results presented in Figures \ref{fig:eppa_synthetic} and \ref{fig:eppa_mnist} demonstrate the advantages of PINS over Newton accelerated methods without the entropic proximal point algorithm (EPPA). PINS achieves significantly higher accuracy for the original problem without a substantial loss in time efficiency. Specifically, PINS attains an accuracy of $10^{-10}$, while the Newton accelerated method without EPPA achieves only around $10^{-5}$ and exhibits occasional instability. 

\begin{figure}[htb!]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{figures/synthetic_comparison.pdf}}
    \caption{Comparison of PINS and Newton accelerated methods without EPPA loops on synthetic datasets. The vertical axis represents the logarithmic error between the computed cost and the exact cost. The gray dashed line indicates the converging solution of the Newton accelerated methods for clarity.}
    \label{fig:eppa_synthetic}
    \end{center}
    \vskip -0.2in
\end{figure}

\begin{figure}[htb!]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{figures/mnist_comparison.pdf}}
    \caption{Comparison of PINS and Newton accelerated methods without EPPA loops on (augmented) MNIST datasets.}
    \label{fig:eppa_mnist}
    \end{center}
    \vskip -0.2in
    \end{figure}

\begin{remark}
    It is important to note that the initial points for the two methods shown in the figures are not identical. This discrepancy arises because, in PINS, the Lagrange multipliers $f,g$ are initialized as zero vectors. However, the error displayed in the figures represents the difference in computed cost. Since the cost matrices $C$ and $C^k$ differ, the solutions derived from (\ref{eq:sink_x}) and (\ref{eq:eppa_x}) are also different. This difference leads to the apparent inconsistency in initialization between the methods shown in the figures.
\end{remark}

\subsection{Time Efficiency} \label{sec:efficiency}
In this part, we compare the time efficiency of PINS with the Sinkhorn algorithm using EPPA. The results, shown in Figures \ref{fig:sinkhorn_synthetic} and \ref{fig:sinkhorn_mnist}, indicate that PINS runs significantly faster than the Sinkhorn algorithm with EPPA, particularly on large-scale datasets. For instance, on the augmented MNIST 16 dataset, PINS is over 100 times faster (58910 seconds vs. 425 seconds).

\begin{figure}[tb!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/efficiency_comparison_synthetic.pdf}}
\caption{The comparison between PINS and Sinkhorn with EPPA on synthetic dataset.}
\label{fig:sinkhorn_synthetic}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[tb!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/efficiency_comparison_mnist.pdf}}
\caption{The comparison between PINS and Sinkhorn with EPPA on MNIST dataset.}
\label{fig:sinkhorn_mnist}
\end{center}
\vskip -0.2in
\end{figure}

These findings align well with our theoretical conclusion in Theorem (\ref{thm:sparse}). The approximate Hessian matrix's sparsity, bounded by $\tau(H^k)\leq \frac{3n-1}{2n^2}$, is independent of the cost matrix and holds at every iteration. This property enables efficient solutions to the sparse Hessian system via the conjugate gradient method. In contrast, the Sinkhorn algorithm with EPPA must solve a Kantorovich linear program problem using first-order methods at each iteration, resulting in higher computational costs that are clearly reflected in the experimental results.



\subsection{Regularization Parameter} \label{sec:eta}
We conducted several experiments to show the performance of PINS with different regularization parameters. The results are shown in Figure \ref{fig:eta}. We can observe that a small regularization parameter $\eta$ might bring a more accurate solution to the Sinkhorn-type method, but it also brings some numerical issues (the curve with a smaller $\eta$ is more unstable than the curve with a larger $\eta$. ). For example, the entries in the Hessian matrix might be too large, causing an overflow problem. However, as we can see, even with a larger $\eta$, PINS can still converge to a much more accurate solution than the Sinkhorn-type method without EPPA.

\begin{figure}[tb!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/eta_comparison.pdf}}
\caption{The comparison among different regularization parameters of experiments on two datasets.}
\label{fig:eta}
\end{center}
\vskip -0.2in
\end{figure}

Therefore, our experimental results show that PINS achieves a more robust descent and higher accuracy than the Sinkhorn-type method without EPPA, regardless of whether the regularization parameter $\eta$ is small or large. This finding highlights the robustness and efficiency of PINS for solving large-scale optimal transport problems.

\section{Conclusion}
In this work, we have reviewed Sinkhorn-type methods for computational optimal transport, highlighting key practical challenges such as insufficient accuracy, low efficiency, and numerical instability. To overcome these limitations, we have introduced the PINS algorithm, which utilizes a proximal iterative scheme with the Sinkhorn's algorithm and a sparse Newton acceleration strategy to enhance both accuracy and computational efficiency. We have established rigorous theoretical guarantees for the proposed approach and validated its effectiveness through extensive numerical experiments, demonstrating its superior performance compared to related methods.

\section*{Acknowledgments}
Di Wu, Ling Liang, and Haizhao Yang were partially supported by the US National Science Foundation under awards DMS-2244988, DMS-2206333, the Office of Naval Research Award N00014-23-1-2007, and the DARPA D24AP00325-00.

\section*{Impact Statement}
The goal of this paper is to advance the field of machine learning and optimization. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Implementation Details}

Here we provide detailed implementation details for the experiments conducted in our paper.

\subsection{Dataset}
As discussed in Section \ref{sec:data}, we generate the cost matrix for the random synthetic dataset using a uniform distribution. For the augmented MNIST dataset, images are chosen randomly with a seed of 42, and the cost matrix is determined by pairwise Euclidean distances.

\subsection{Traning Parameters}
\paragraph{Regularization} We set the regularization parameter $\eta$ to $10^{-2}$ for the random synthetic dataset and $10^{-1}$ for the augmented MNIST dataset. We use this setting because we expect a relatively robust performance of the Sinkhorn-type method (see Section \ref{sec:eta}).

\paragraph{PINS} We use a relative error threshold of $10^{-4}$ and the maximum number of EPPA iterations is set to $500$. For each inner loop of PINS, the maximum number of Sinkhorn iterations is set to $50000$ with an absolute error threshold of approximately $6.3\times 10^{-4}$. The maximum number of Newton iterations is set to $20$ and with an absolute error threshold of $10^{-8}$. The sparsification parameter is set to $0.1$.

\paragraph{Sinkhorn with EPPA} Here, the relative error threshold is $10^{-4}$. For each inner loop, the maximum number of Sinkhorn iterations is set to 5000 and and maintain an absolute error threshold of $10^{-8}$.

\paragraph{Sinkhorn-type method without EPPA} The threshold of the absolute error is set to $10^{-8}$. The maximum number of Sinkhorn iterations is set to $50000$, with an absolute error threshold around $6.3\times 10^{-4}$ in the Sinkhorn phase. The second Newton phase allows up to 50 iterations.

\end{document}