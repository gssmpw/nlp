\section{Related Work}

\subsection{Retrieval-Augmented Generation (RAG)}

RAG has emerged as a powerful paradigm for enhancing language models in knowledge-intensive tasks. By integrating external knowledge sources into generative models, RAG enables the production of more accurate and informative responses. This approach has seen success across various domains, such as open-domain question answering \citep{karpukhin2020dense} and conversational agents \citep{shuster2021retrieval}. Researchers have aimed to improve RAG by optimizing retrieval components \citep{zhang2023retrieve,nguyen2024reward,xu2024bmretriever} or enhancing the generative modelâ€™s ability to leverage retrieved content \citep{izacard2023atlas,zhang2024raft,fang2024enhancing,wei2024instructrag,jin2024rag,wang2024speculative,zhang2025rag}. Applications of RAG extend to specialized fields, like biomedical question answering \citep{xiong2024benchmarking,xu2024simrag,kim2024mdagents}, showcasing its versatility. Despite these advancements, vanilla RAG architectures typically rely on a single round of retrieval, limiting their effectiveness in scenarios requiring iterative reasoning and complex information integration.

\subsection{Multi-hop Question Answering (QA)}

Multi-hop QA tasks require systems to perform reasoning across multiple pieces of information, often from disparate sources, to derive a correct answer \citep{yang2018hotpotqa,ho2020constructing}. These tasks are inherently challenging for traditional RAG architectures due to their single retrieval pass, which restricts the ability to gather all necessary information for complex questions. Early attempts to address multi-hop QA in RAG systems include agent designs such as ReAct \citep{yao2023react}, which integrate reasoning processes into retrieval steps \citep{trivedi2023interleaving,asai2023self,jiang2023active,press2023measuring,ram2023context,wang2024rat,khot2022decomposed,shao2023enhancing,wang2023knowledge,shi2024generate,yu2024autorag}. Nevertheless, these strategies commonly rely on finely tuned heuristics or handcrafted prompts, lacking robustness when facing diverse question formulations.

\subsection{Process Reward Modeling (PRM)}

PRM provides a structured framework for guiding LLM reasoning by incorporating intermediate feedback during the reasoning process \citep{lightman2023let,ma2023let}. This approach has been shown to enhance the reasoning capabilities and accuracy of models in complex tasks \citep{setlur2024rewarding,wu2024inference}. Researchers have explored various methods to implement process supervision, including constructing intermediate supervision signals through rollouts, human feedback, or LLM feedback \citep{zhang2024rest,cobbe2021training,wang2024math,sun2024large}. By shifting the focus from outcome-based rewards to reasoning trajectories, this paradigm fosters more nuanced understanding and improved problem-solving abilities in language models.