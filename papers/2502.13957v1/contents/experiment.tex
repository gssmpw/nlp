\section{Results} \label{sec:results}

\subsection{Experimental Settings}

To evaluate how different agents perform on knowledge-intensive QA tasks and how they can benefit from various process supervision methods implemented in RAG-Gym, we select different datasets that are both knowledge-intensive and reasoning-intensive, covering questions from both general and medical domains. Specifically, we choose HotpotQA \citep{yang2018hotpotqa}, 2WikiMultihopQA \citep{ho2020constructing}, and Bamboogle \citep{press2023measuring}, which are commonly used multi-hop QA datasets constructed from Wikipedia. We also include the MedQA dataset \citep{jin2021disease}, which consists of medical exam questions requiring specialized domain knowledge and complex reasoning. Following prior work \citep{shao2023enhancing,xiong2024benchmarking}, we use Exact Match (EM) and F1 scores as the evaluation metrics of HotpotQA, 2WikiMultihopQA, and Bamboogle. The multi-choice MedQA dataset is evaluated with accuracy (Acc) as the metric. We also computed the average EM and F1 scores across different tasks, with accuracy being considered as both EM and F1 scores in the multi-choice evaluation setting.


We tested the RAG-Gym process supervision methods on six different agent designs, including the direct prompting (Direct), Chain-of-thought (CoT) \citep{wei2022chain}, RAG \citep{lewis2020retrieval}, ReAct \citep{yao2023react}, Search-o1 \citep{li2025search}, and the newly proposed ReSearch. More descriptions about the baselines can be found in Appendix \ref{sec:baseline_details}.
We selected Llama-3.1-8B-Instruct \cite{dubey2024llama} as the base LLM for the implementations of various reasoning and search agents, due to its context length of 128k tokens and its availability of open-source parameters. We also involved GPT-4o-mini as the representative of proprietary LLMs to examine if the trained reward models can be generalized to guide the action selection of other LLMs.
For the implementation of process supervision algorithms in RAG-Gym, we sampled 1k instances from the training sets of both HotpotQA and MedQA to generate process reward data for LLM tuning. 2WikiMultihopQA and Bamboogle were tested using LLMs trained on HotpotQA to see if the tuned agents are generalizable to other datasets with similar formats. More implementation details can be found in Appendix \ref{sec:implementation_details}.

\subsection{Comparison of Process Supervision Methods}

Table \ref{tab:llama-3-1-8b} shows the performance of various agents and their tuned versions using different process supervision methods in RAG-Gym. Process supervision consistently improves performance across all agents compared to the zero-shot learning (ZSL) baseline, demonstrating its effectiveness in enhancing intermediate reasoning and query generation. Among the three process supervision algorithms, PRM achieves the best results overall, outperforming ZSL baselines by up to 25.6\% (ReAct; Average F1).

While PRM outperforms the other methods, both DPO and SFT show significant improvements over the ZSL baseline. Interestingly, SFT slightly outperforms DPO on the Direct, CoT, and RAG agents, where the tuning focuses exclusively on the answer generation step. In contrast, DPO significantly surpasses SFT on ReAct, Search-o1, and ReSearch, where the tuning process also involves learning to generate high-quality queries by contrasting positive and negative samples. This result highlights the importance and effectiveness of leveraging contrastive learning in tasks that require the generation of search queries, especially for agents that tightly integrate reasoning and query generation.

\subsection{Comparison of ReSearch and other Agents}

The results also show that ReSearch consistently outperforms other agents, both in the ZSL setting and in settings with process supervision. Without tuning, ReSearch achieves strong zero-shot performance, demonstrating the effectiveness of explicitly aligning answer reasoning with query generation. Using process reward models, ReSearch achieves state-of-the-art performance, with an average EM score of 54.31\% and an average F1 score of 62.41\% across different datasets. Furthermore, ReSearch exhibits superior generalization, achieving top scores on 2WikiMultihopQA and Bamboogle without task-specific fine-tuning. These results validate the ability of ReSearch's unified reasoning and search framework to effectively leverage process supervision for both task-specific improvements and broader generalization across knowledge-intensive QA tasks.

\subsection{Reward Model Transferability}



Figure \ref{fig:gpt-4o-mini} highlights the performance improvements of the ReSearch agent with GPT-4o-mini using Llama-3.1-8B-based process reward models. The action selection with reward models leads to consistent gains across all tasks, demonstrating the transferability of PRM to effectively select high-quality actions in different LLMs. This result also highlights the potential of using process reward models as a plug-and-play module to enhance the reasoning and search capabilities of proprietary LLMs, where direct fine-tuning is not feasible due to restrictions on model access. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs/gpt-4o-mini.pdf}
    \caption{Performance improvement of GPT-4o-mini on different tasks with the reward model (PRM) trained on Llama-3.1 actions.}
    \label{fig:gpt-4o-mini}
\end{figure}


\begin{figure*}[h!] 
    \centering
    \includegraphics[width=1.0\linewidth]{figs/train_scale.pdf}
    \caption{Performance of ReSearch agents with process reward models tuned on different numbers of training samples.}
    \label{fig:train_scale}
\end{figure*}


\section{Analysis and Discussion} \label{sec:discussions}

\subsection{Comparison of Different Reward Sources}
To evaluate the effectiveness of different process reward sources in training reward models, we conducted experiments on MedQA and compared their alignments with domain expert preferences as well as their impact on downstream accuracy. Specifically, we examined whether using LLMs like GPT-4o for process data annotation is an effective approach and how its preferences align with human annotations. Four domain experts annotated 200 MedQA questions. A reward model was trained on the remaining 800 training questions annotated with GPT-4o, and its preferences were compared to those of domain experts.

\input{tables/reward_comparison}

The results are shown in Table \ref{tab:human_eval}. The reward model trained with GPT-4o annotations achieved the highest agreement with human preferences (85.85\%), significantly outperforming the rollout-based method (71.03\%) introduced in Math-Shepherd \cite{wang2024math}. This demonstrates that GPT-4o annotations closely align with human reasoning and decision-making in this context. Furthermore, the model trained with GPT-4o annotations achieved the highest accuracy (71.96\%), highlighting its effectiveness in knowledge-intensive tasks. In contrast, the rollout-based process reward model achieved an accuracy of 68.34\%, comparable to random action selection (68.26\%). We also evaluated the outcome reward model (ORM), which uses the final evaluation metric (accuracy) for trajectory assessment, and found it achieved an accuracy of only 66.77\%. This result shows the limitations of ORM in multi-step reasoning and search scenarios, where process-level supervision is critical. These findings suggest that leveraging advanced LLMs like GPT-4o for reward annotation is both practical and effective.

\subsection{Training Time Scaling}

\begin{figure*}[h!] 
    \centering
    \includegraphics[width=1.0\linewidth]{figs/test_scale.pdf}
    \caption{Performance of ReSearch agents with different numbers of actions sampled for reward scoring and selection per step.}
    \label{fig:test_scale}
\end{figure*}

For the evaluation of training sample size and its impacts on the performance of ReSearch agents, we conducted experiments using process reward models trained on varying numbers of instances, ranging from 250 to 1000 questions. The results, presented in Figure \ref{fig:train_scale}, show how the agent's performance scales with the availability of more training data across four datasets.
In general, the performance of ReSearch improves with an increasing number of training samples, but the gains tend to converge as the sample size grows. Notably, there is a sharp improvement in F1 scores on HotpotQA, 2WikiMultihopQA, and Bamboogle when comparing the ZSL baseline to process reward models trained on 250 samples, showing that even a small amount of process reward data can yield significant performance gains. However, the improvements become less pronounced on HotpotQA and 2WikiMultihopQA when increasing the training samples from 500 to 1000, indicating diminishing returns as the model approaches a saturation point in its learning from additional data. This convergence may also be attributed to the candidate actions being sampled from the untuned agent during inference, which constrains the space of actions that process reward models can evaluate.

For MedQA, which involves complex reasoning and information-seeking tasks requiring domain-specific knowledge, a different trend is observed. With only 250 training samples, the performance slightly drops below the ZSL baseline, highlighting the challenges of capturing intricate domain-specific processes with limited training data. As the sample size increases, however, the performance gradually recovers and eventually surpasses the ZSL baseline, achieving the highest accuracy of 71.72 with 1000 samples. This underscores the importance of sufficient training data in capturing the nuanced reasoning and query-generation processes required for specialized tasks.

\subsection{Inference Time Scaling}

Since process reward models optimize action-taking by identifying high-quality actions from the generated candidates during inference, we explored how the agent performance changes with the increasing number of sampled actions at each time step. Figure \ref{fig:test_scale} displays the results of our inference time scaling study, with ReSearch as the tested agent.
We observe a consistent trend across multiple benchmarks, where increasing the number of sampled actions generally improves performance. Specifically, for HotpotQA and Bamboogle, the F1 score continues to rise as more actions are sampled, demonstrating the benefits of expanding the candidate set to enable better action selection at each step. However, performance gains gradually diminish, indicating that the agent reaches a point where additional sampled actions contribute less to improvement. This suggests that while action sampling is beneficial, there is a limit to how much additional sampling enhances decision-making.
