\section{Comparison with Concurrent Work}

Table \ref{tab:related_work_comparison} provides a detailed comparison of RAG-Gym with concurrent methods. SmartRAG \cite{gao2024smartrag} shares a similar MDP formulation but relies solely on reinforcement learning-based tuning without process-level supervision. LeReT \cite{hsu2024grounding}, AR-MCTS \cite{dong2024progressive}, and CR-Planner \cite{li2024can} incorporate process supervision but focus exclusively on either agent tuning or verifier training, without a unified framework for both. Additionally, these methods, along with the recent ReARTeR \cite{sun2025rearter}, are tailored to specific agent architectures, limiting their generalizability. In contrast, RAG-Gym is agent-agnostic and supports both process reward modeling and direct agent tuning, enabling robust optimization of information-seeking agents. Furthermore, we systematically explore different process reward designs and highlight the effectiveness of advanced LLMs as scalable process reward judges, a key aspect largely unexplored in prior work.

\begin{table}[h!]
    \centering
    \caption{Comparison of RAG-Gym with concurrent work.}
    \begin{tabular}{l|ccccccccccc}
    \toprule
        Method & Process Supervision & Agent Tuning & Verifier Training & Agent-Agnostic \\
    \midrule
        SmartRAG \cite{gao2024smartrag} & \textcolor{red}{\xmark} & \greencheck & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\
        LeRet \cite{hsu2024grounding} & \greencheck  & \greencheck  & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\
        AR-MCTS \cite{dong2024progressive} & \greencheck & \textcolor{red}{\xmark}  & \greencheck & \textcolor{red}{\xmark} \\
        CR-Planner \cite{li2024can} & \greencheck & \textcolor{red}{\xmark} & \greencheck& \textcolor{red}{\xmark} \\
        ReARTeR  \cite{sun2025rearter}&  \greencheck & \greencheck & \greencheck & \textcolor{red}{\xmark}  \\
        RAG-Gym (ours) &  \greencheck & \greencheck & \greencheck & \greencheck  \\
    \bottomrule
    \end{tabular}
    \label{tab:related_work_comparison}
\end{table}

\section{Dataset Descriptions}

In this section, we provide detailed descriptions of the datasets used in our experiments, including HotpotQA \cite{yang2018hotpotqa}, 2WikiMultihopQA \cite{ho2020constructing}, Bamboogle \cite{press2023measuring}, and MedQA \cite{jin2021disease}.

\paragraph{HotpotQA.}
HotpotQA is a large-scale, multi-hop question-answering dataset that requires reasoning across multiple documents. It consists of questions that explicitly demand retrieving and synthesizing information from different sources. The dataset provides both distractor and supporting documents, allowing evaluation of models' ability to filter relevant information effectively. As the answers to the test questions in HotpotQA are not publicly available we took a subsample from its validation set (7,405 instances) as previous research did \cite{yao2023react,li2025search}. The last 1,000 validation questions were selected for the agent evaluation on HotpotQA. The first 1,000 questions were used as the training data for process supervision.

\paragraph{2WikiMultihopQA.}
2WikiMultihopQA is another multi-hop question-answering dataset constructed from Wikipedia.  2WikiMultihopQA focuses on high-quality reasoning paths by selecting supporting documents more systematically. The dataset contains questions that require reasoning across different Wikipedia pages, ensuring a diverse range of factual and inferential challenges. The last 1000 questions in the development set (12,576 question in total) were used for agent evaluation.

\paragraph{Bamboogle.}
Bamboogle is a manually constructed dataset designed to evaluate compositional reasoning and adversarial robustness. It consists of 2-hop questions written by researchers, where both supporting facts exist in Wikipedia but are structured to be challenging for retrieval-based systems. Unlike automatically generated datasets like 2WikiMultihopQA and Musique, Bamboogle questions do not follow fixed templates, increasing their variability. We used the whole test set with 125 questions for the evaluation of agents on Bamboogle.

\paragraph{MedQA.}
MedQA is a medical question-answering dataset sourced from professional medical exams such as the USMLE (United States Medical Licensing Examination). It requires domain-specific knowledge and reasoning to answer multiple-choice medical questions. We focused on the English split of MedQA with 1,273 USMLE-style test questions. A subset of 1,000 questions was sampled from the training set (10,178 questions) for the optimization of various agents.

\section{Baseline Descriptions} \label{sec:baseline_details}

Here are the detailed descriptions of various baseline agents that we implemented in the experiments.
\begin{itemize}
    \item Direct: The agent directly outputs the predicted answer without reasoning at the first iteration.
    \item CoT \citep{wei2022chain}: The agent outputs both the reasoning and the predicted answer at the first iteration.
    \item RAG \citep{lewis2020retrieval}: The agent outputs the original question as the search query at the first iteration. It reasons about the updated state and generates a predicted answer at the second iteration.
    \item ReAct \citep{yao2023react}: The agent reasons about the given state and generates either a search action or an answer action.
    \item Search-o1 \citep{li2025search}: Before the state reasoning, the agent summarizes the retrieved documents in the information-seeking history as a direct answer to the corresponding search query. The agent uses query-answer pairs instead of query-documents pairs for the construction of LLM input as our ReSearch does. It can be considered a special version of ReAct with RAG instead of information retrieval (IR) as the tool. 
\end{itemize}

\section{Implementation Details} \label{sec:implementation_details}
For the implementation of the IR environment, we select Wikipedia as the supporting corpus for the retrieval of relevant information for questions from HotpotQA, 2WikiMultihopQA, and Bamboogle. For the environment of solving MedQA questions, we use a combination of medical textbooks and StatPearls which were pre-processed by \citet{xiong2024benchmarking}. For all tasks, we used both lexical and semantic retrievers whose results were merged with Reciprocal Rank Fusion \citep{cormack2009reciprocal}. BM25 \citep{robertson2009probabilistic} and BGE-Base \citep{bge_embedding} were used for HotpotQA, 2WikiMultihopQA, and Bamboogle, while in MedQA, we selected BM25 and MedCPT \citep{jin2023medcpt}.

For all LLM tuning in our paper, we employed the LoRA fine-tuning \cite{hu2021lora} with \(r=256\) and \(alpha=512\) on all attention components in the transformers architecture \cite{vaswani2017attention}. The process supervision methods are implemented using the TRL package \cite{vonwerra2022trl}. We used the instruction-tuned version of Llama-3.1-8B as the base model for implementing various agents, which also served as the base model for process reward modeling (PRM). For the tuning of Search-o1 and ReSearch agents, only the LLM for action reasoning is trained while the one for history knowledge summarization remains unturned.

All results of zero-short learning (ZSL), supervised fine-tuning (SFT), and direct preference optimization (DPO) are generated with a temperature of 0.0. For the evaluation of PRM, we employed a temperature of 1.0 with 10 different actions sampled for each step in the information-seeking trajectory. Algorithm \ref{alg:prm_best_of_n} presents our algorithm of using the trained process reward model to guide the action selection during inference.


\begin{algorithm}
\caption{PRM-Guided Inference with Best-of-N Selection}
\label{alg:prm_best_of_n}
\begin{enumerate}
    \item \textbf{Input:} Original question $Q$, agent policy $\pi_{f(\theta)}$, process reward model $r_{\phi}$, number of candidate actions $N$, maximum steps $T$, information retrieval function IR.
    \item \textbf{Initialize} state $S \leftarrow (Q, H_1 = \emptyset)$.
    \item \textbf{For} $t = 1$ to $T$:
    \begin{enumerate}
        \item Generate $N$ candidate actions: $a_q,\cdots,a_N \sim \pi_{f(\theta)}(\cdot|S)$.
        \item Compute process rewards and select the best action: $a^* \leftarrow \arg\max_{a \in \{a_1,\cdots,a_N\}} r_{\phi}(S, a) $.
        
        \item \textbf{If} $a^*$ is a search query:
        \begin{enumerate}
            \item Retrieve documents: $D \leftarrow \text{IR}(a^*)$.
            \item Update state: $S \leftarrow (Q, H_{t+1} = H_{t} \cup \{(a^*, D)\})$.
        \end{enumerate}
        
        \item \textbf{If} $a^*$ is a final answer:
        \begin{enumerate}
            \item Return $a^*$ and terminate the process.
        \end{enumerate}
    \end{enumerate}
    \item \textbf{End For}
\end{enumerate}
\end{algorithm}

\section{Case Studies} \label{sec:case_study}

\subsection{Comparison of Agent Architectures on Bamboogle}
We analyze the reasoning and search behaviors of RAG, ReAct, Search-o1, and ReSearch using an example from the Bamboogle dataset. As shown in Figure \ref{fig:example}, given the question ``What was the father of the last surviving Canadian father of Confederation?", the three agents show distinct behaviors when generating the first action.

The RAG agent directly passes the question as a search query without decomposition, relying entirely on retrieval to infer the answer. This often leads to ineffective searches that fail to retrieve necessary intermediate facts. ReAct and Search-o1 improve upon this by engaging in stepwise query reasoning, first identifying the need to determine the last surviving Canadian father of Confederation before issuing a search query. However, the generated query, ``List of Canadian fathers of Confederation'', retrieves broad information rather than directly resolving the missing knowledge.

In contrast, ReSearch explicitly integrates answer reasoning with search. It first constructs a potential answer, identifying an unverified claim that William Lyon Mackenzie King is among the last surviving Canadian fathers of Confederation. Recognizing the missing evidence, it formulates a targeted query, ``Who is the last surviving Canadian father of Confederation?'', to resolve the uncertainty. This approach ensures that retrieval is aligned with answer construction, minimizing unnecessary queries and improving information efficiency. The case study illustrates how ReSearch effectively refines the search process by linking query generation to specific knowledge gaps.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/research_agent.pdf}
    \caption{Comparison of different agent architectures in handling a multi-hop question from Bamboogle. ReSearch explicitly aligns reasoning with query generation, leading to more targeted retrieval and improved answer quality.}
    \label{fig:example}
\end{figure*}

\subsection{PRM-Guided Action Selection in MedQA}

\input{tables/case_study_medqa}

To illustrate how the Process Reward Modeling (PRM) improves decision-making, we examine a case from the MedQA dataset (Table \ref{tab:case_study}). The model is tasked with identifying the mechanism of action of the most likely anti-diabetic medication for a 60-year-old patient presenting with symptoms indicative of fluid overload.

For the generation of the first action, the agent initially generates a broad and less actionable query about heart failure, which, while relevant, does not directly contribute to identifying the correct medication. Meanwhile, it also provides another candidate action with the query ``What are common classes of oral anti-diabetic medications?", which leads to retrieving a structured list of relevant drug categories. The process reward model estimates the quality of these two candidates and identifies the second candidate as the better action.

As the reasoning progresses, the process reward model continues to refine action selection, identifying the best queries to resolve missing information efficiently. The rewarded queries ultimately guide the agent toward correctly inferring that the medication most likely falls under the sulfonylureas class, which acts by closing ATP-sensitive K-channels in pancreatic beta cells.
This case demonstrates how process reward models enhance decision quality by selecting queries that effectively bridge knowledge gaps, leading to more precise reasoning and higher answer accuracy.

\section{Prompt Templates}

We provide structured prompt templates for history knowledge summarization and action generation in our proposed ReSearch agent. The template in Figure \ref{fig:prompt_summarize} ensures that retrieved documents are summarized concisely and factually for follow-up queries. Figure \ref{fig:prompt_research} shows the prompt template used by ReSearch to align the answer construction with query formulation. The prompt of using GPT-4o for process reward data annotation is presented in Figure \ref{fig:prompt_rank}.

\begin{figure*}[h]
\begin{AIbox}{Prompt template for history knowledge summarization in Search-o1 and ReSearch}
You are a helpful assistant tasked with answering a follow-up query using the relevant documents provided.\\

\#\#\# Relevant Documents\\
\verb|{{documents}}|\\

\#\#\# Context\\
Original question: \verb|{{question}}|\\

\#\#\# Follow-up Query\\
\verb|{{query}}|\\

Answer the follow-up query succinctly, using only the information from the documents. When the documents do not provide sufficient information, explicitly point this out instead of making up facts. Do not include unrelated or excessive details in the response.
\end{AIbox}
\caption{Template used for history knowledge summarization in Search-o1 and ReSearch.}
\label{fig:prompt_summarize}
\end{figure*}


\begin{figure*}[h]
\begin{AIbox}{Prompt template for generating actions using the ReSearch agent}
You are a helpful assistant. Your task is to answer a given question following user instructions.'\\

\#\#\# Information-seeking History\\
\verb|{{history}}|\\

\#\#\# Original Question\\
\verb|{{question}}|\\

Your output must include three sections:\\
1. **\#\#\# Step-by-step Reasoning**:\\
  - Think step-by-step and then answer the question.\\

2. **\#\#\# Unverified Claim Identification**:\\
  - Identify if there are claims in the reasoning chain that are not grounded in the user-provided information-seeking history and need to be verified.\\
  - Summarize the first piece of missing information as an atomic query that will be searched in an external knowledge base. If there is no unverified claim, clearly state that no further query is needed.\\

3. **\#\#\# Structured Output**:\\
  - Present your predicted answer and generated query (if applicable) in the following JSON format:\\
    ```json\\
    \{\\
        ``predicted\_answer": ``Provide a single letter (for multiple-choice questions), digit, word, or short phrase here.",\\
        ``generated\_query": ``Provide an entity, question, or statement to be searched in an external knowledge base. Output \textbackslash``None\textbackslash" if no query is generated.",\\
    \}\\
    ```
\end{AIbox}
\caption{Template used to generate actions for the ReSearch agent.}
\label{fig:prompt_research}
\end{figure*}


\begin{figure*}[h]
\begin{AIbox}{Prompt template for ranking candidate actions with GPT-4o}
You are a decision-evaluation assistant. Your task is to rank the proposed actions from the most appropriate to the least appropriate as the next step in a sequential decision-making process aimed at solving a given question.\\

\#\#\# Original Question:\\
\verb|{{question}}|\\

\#\#\# Information-Seeking History:\\
\verb|{{curr_history}}|\\

\#\#\# Proposed Next Actions:\\
\verb|{{actions_text}}|\\

\#\#\# Important Assumption\\
The agent has no prior knowledge about the subject matter. It must rely solely on the information-seeking history provided to evaluate and answer the original question. Assumptions not explicitly supported by the history must not influence the ranking of proposed actions.\\

\#\#\# Evaluation Criteria for Appropriateness\\
1. **Sufficiency Check**:\\
- Determine whether the available information is sufficient to directly answer the original question. If not, the proposed action to ``Answer'' is inappropriate.\\
- Prioritize queries that gather specific, missing information essential to solving the question.\\
- If the history already contains all necessary information, then ``Answer'' is the most appropriate action, and the correct answer should be ranked highest.\\

2. **Utility Check**:\\
- Queries must be precise, actionable, and directly relevant to solving the question.\\
- Prioritize foundational queries that establish critical context or general knowledge necessary for more specific follow-ups.\\
- Rank overly narrow or prematurely specific queries lower if they presume knowledge not yet available.\\
- Avoid irrelevant queries that do not contribute to solving the original question.\\

3. **Redundancy Check**:\\
- Queries that duplicate information already covered in the history or repeat previous queries should be ranked lower.\\
- Proposed actions must add new value to the decision-making process by seeking new or clarifying missing information.\\

\#\#\# Expected Output Format\\
- Output the indices of the ranked actions in JSON format: ```json\{``ranked\_indices'': [list of indices]\}'''.\\
- Rank actions from most appropriate to least appropriate based on the evaluation criteria above.\\
- Do not provide additional explanations or reasoning.'''
\end{AIbox}
\caption{Template used by GPT-4o to rank action candidates given the state.}
\label{fig:prompt_rank}
\end{figure*}

