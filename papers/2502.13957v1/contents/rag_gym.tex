\section{RAG-Gymnasium (RAG-Gym)}

RAG-Gym models knowledge-intensive QA as a nested Markov Decision Process (MDP), where the outer MDP governs high-level action generation through interactions with the information retrieval (IR) environment, while the inner MDP controls token generation within LLMs. This formulation is compatible with various agentic RAG architecture that leverages IR for augmented answer generation. By assigning process rewards from high-level actions to sequential token generation, RAG-Gym enables effective tuning of language agents, which can be further utilized for optimizing agent parameters or training process verifiers.

\subsection{Knowledge-intensive QA as Nested MDP}

Figure \ref{fig:rag_gym_mdp} presents the architecture of our RAG-Gym framework, which formulates a knowledge-intensive question answering task as a nested Markov decision process (MDP). With LLMs as the core engine of the reasoning and search agents, the sequential token generation is the inner MDP, where each action is to generate the next token given all existing ones. Below we explain in detail how the outer MDP is constructed to generalize various language agent designs on knowledge-intensive questions.

\noindent \textbf{State Space $\mathcal{S}$.} At each time step $t$, the state \(s_t \in \mathcal{S}\) comprises the original question \(\mathcal{Q}\) and the information-seeking history \(\mathcal{H}_t\). Formally, the state \(s_t\) can be expressed as
\begin{equation}
    s_t = (\mathcal{Q}, \mathcal{H}_t)
\end{equation}
where \(\mathcal{H}_t=\{(q_1, D_1),\cdots,(q_{t-1},D_{t-1})\}\) is history of information-seeking queries \(q_1, \cdots, q_{t-1}\) and their corresponding retrieved documents \(D_1, \cdots, D_{t-1}\) returned by the environment. 
Initially, \(\mathcal{H}_1=\emptyset\) since no queries have been made. 
The state space \(\mathcal{S}\) is the set of all possible states:
\begin{equation}\scriptsize
\begin{aligned}
    \mathcal{S} = \big\{(\mathcal{Q}, \mathcal{H})\big|\mathcal{Q} \in \mathcal{I}, \mathcal{H}\in \{q \in \mathcal{A}_q, D\subseteq \mathcal{D}\} \cup \{\emptyset\}\big\},
\end{aligned}
\end{equation}
where $\mathcal{I}$ is the question space, \(\mathcal{A}_q\) is the space of all possible search queries, \(\mathcal{D}\) is the space of all documents.

\noindent \textbf{Action Space $\mathcal{A}$.} While different agents may use various designs to reason about the given state and generate token sequences, we standardize these action sequences by defining a shared abstract action space \(\mathcal{A}\). At each time step \(t\), the action \(a_t\) can be either a search query or a predicted answer to the original question. Thus, we define the action space \(\mathcal{A} = \mathcal{A}_q \cup \mathcal{A}_p\) where  \(\mathcal{A}_q\) is the set of all possible queries,  and \(\mathcal{A}_p\) denotes the set of possible answers.


\noindent \textbf{IR Environment.} The environment of the outer MDP in RAG-Gym is driven by an IR system, which takes a search query \(q_t\) as input and returns a set of relevant documents \({D}_t\) as output. Formally, the IR system can be represented as a mapping from \(\mathcal{A}_q\) to \(\mathcal{P}(\mathcal{D})\)
where \(\mathcal{P}(\mathcal{D})\) is the power set of \(\mathcal{D}\). The retrieval process is determined by the underlying text retriever and specific retrieval settings (e.g., the number of returned documents). These settings are fixed to ensure stability of state transitions.


\noindent \textbf{MDP Workflow.} For a given question $\mathcal{Q}$, the MDP starts at the initial state $s_1 = (\mathcal{Q}, \emptyset)$. For every step $t$, $a_t$ is  sampled from the agent's policy $\pi_{f(\theta)}(\cdot\,|\, s_t)$ where $\pi_{f(\theta)}: \mathcal{S} \rightarrow \Delta(\mathcal{A})$ defines the action distribution given a state. The agent's policy consists of $\theta$, which denotes the base language model's parameters, and an agent-specific function $f$, which denotes how the base LLM policy is utilized (e.g., by applying different prompts to generate actions).  If $a_t \in \mathcal{A}_q$, we reinterpret it as $q_t$,  perform the query, and get document \({D}_t = \text{IR}(q_t)\). The history is then updated to $\mathcal{H}_{t+1}$ by appending $(q_t, {D}_t)$, and the state transitions to $s_{t+1} = (\mathcal{Q}, \mathcal{H}_{t+1})$. Otherwise, if $a_t \in \mathcal{A}_p$, the current episode is considered complete, and the MDP terminates.

\noindent \textbf{Rewards.} For the outer MDP, the reward of an episode is determined by the correctness of the final prediction. The immediate reward for a state-action pair \((s_t, a_t)\) is:
\begin{equation}\small
\mathcal{R}(s_t, a_t) = \begin{cases} 
0, & \text{if } a_t \in \mathcal{A}_q \\ 
\mathcal{F}(a_t, \mathtt{g}(\mathcal{Q})), & \text{if } a_t \in \mathcal{A}_p.
\end{cases}
\end{equation}
Here, \(\mathtt{g}(\mathcal{Q})\) is the ground-truth answer for the original question \(\mathcal{Q}\), and \(\mathcal{F}\) is the evaluation metric, such as accuracy.
The objective for the agent in the outer MDP is to maximize the expected cumulative reward over a trajectory:
\begin{equation}\small
\text{maximize}_{\theta} \quad \mathbb{E}_{\pi_{f(\theta)}}\left[\sum_{t=1}^{T} \gamma^{t-1}\mathcal{R}(s_t, a_t)\right]
\end{equation}
where \(T\) is the time step at termination, and \(\gamma \in [0, 1]\) is the discount factor that penalizes longer trajectories. 

\subsection{Improving Search Agents with Process Supervision}

While outcome-based rewards, such as answer correctness, provide a clear optimization objective, they offer limited insight into the quality of intermediate reasoning and search steps. In RAG-Gym, a poorly formulated query may still lead to a correct answer if the language agent compensates with internal knowledge, making it difficult to assess the effectiveness of the search query. To address this, process supervision is essential. Recent advancements in process reward modeling \citep{lightman2023let,wang2024math} show that guiding intermediate steps enhances both performance and robustness in language agents. Since outer MDP actions in RAG-Gym are discrete, optimizing agents solely through outcome rewards is challenging. By incorporating process rewards, RAG-Gym enables more effective LLM tuning, aligning token generation with high-quality search behavior. Section \ref{sec:process_collect} details our process reward data collection pipeline, and Section \ref{sec:process_tune} describes the algorithms for tuning language agents with process supervision.

\subsubsection{Collection of Process Reward Data} \label{sec:process_collect}

To evaluate intermediate reasoning and search steps in RAG-Gym, we design a process reward function that assesses queries based on three key criteria: 
\begin{itemize}
    \item Sufficiency: A query must first be necessary. If the retrieval history already contains sufficient information, answering should be the preferred action instead of searching further.
    \item Utility: Queries should also be precise, actionable, and foundational to solving the question while avoiding unnecessary details.
    \item Redundancy: Queries should introduce new, useful information rather than repeating past searches.
\end{itemize}
These criteria ensure that queries are efficient, targeted, and contribute meaningfully to constructing the final answer.

The data collection pipeline begins with trajectory sampling, where the language agent generates a sequence of actions based on its current policy. At each step in the trajectory, multiple candidate actions are proposed, and the best action is selected according to predefined evaluation criteria. To streamline the annotation process and ensure consistency, we employ a ranking-based evaluation framework rather than assigning numerical scores. The selected action is then executed, and the trajectory transitions to the next state. This process is repeated iteratively until the trajectory terminates.

To ensure quality, only trajectories that result in a correct final answer are retained, as determined by the outcome reward. This filtering guarantees that the selected actions not only align with the process reward criteria but also contribute to successful task completion. To address the challenges of slow and costly human annotation, we leverage LLMs such as GPT-4o to annotate the sampled trajectories. As demonstrated in our experiments (Table \ref{tab:human_eval}), annotations generated by GPT-4o exhibit high reliability, closely aligning with domain expert judgments. This approach enables scalable and efficient data collection, making it feasible to gather high-quality process reward data at scale.

\subsubsection{Tuning Agents with Process Supervision} \label{sec:process_tune}

The process reward data collected serves as a key resource for improving language agents in RAG-Gym through three distinct methods: supervised fine-tuning, direct preference optimization \citep{rafailov2024direct}, and process reward modeling. Each method leverages the data to address specific aspects of the training paradigm and task requirements, enabling robust optimization of reasoning and action generation.

In \textit{supervised fine-tuning (SFT)}, selected actions from the process rewards are used to train the language agent. Formally, the goal of SFT is to minimize the negative log-likelihood of the selected actions given their states:
\begin{equation}
\mathcal{L}_{\text{SFT}}(\theta) = -\mathbb{E}_{(s_t, a_t^+) \sim \mathfrak{D}} \left[ \log \pi_{f(\theta)}(a_t^+ | s_t) \right],
\end{equation}
where \(\mathfrak{D}\) is the dataset of process reward-labeled state-action pairs. This method provides a straightforward way to incorporate process supervision but does not explicitly account for unselected actions, potentially limiting its ability to distinguish between subtle preferences.

\textit{Direct preference optimization (DPO)} introduces a contrastive learning framework that incorporates both selected and unselected actions. The process reward data is reformulated into preference pairs \((a_t^+, a_t^-)\), where \(a_t^+\) is the preferred action and \(a_t^-\) is the less-preferred alternative for \(s_t\). The DPO objective minimizes the following loss:
\begin{equation}\scriptsize
\begin{aligned}
\mathcal{L}_{\text{DPO}}(\theta) = - \mathbb{E}_{(s_t, a_t^+, a_t^-) \sim \mathfrak{D}} 
\Bigg[ \log \sigma &\Big( \beta \log \frac{\pi_{f(\theta)}(a_t^+ | s_t)}{\pi_{\text{ref}}(a_t^+ | s_t)}\\ &  - \beta \log \frac{\pi_{f(\theta)}(a_t^- | s_t)}{\pi_{\text{ref}}(a_t^- | s_t)}  \Big) \Bigg],
\end{aligned}
\end{equation}
where \(\pi_{f(\theta)}\) is the policy being optimized, \(\pi_{\text{ref}}\) is the reference policy, \(\beta\) is a temperature parameter controlling the strength of the preference weighting, and \(\sigma(\cdot)\) is the sigmoid function. By explicitly comparing actions, DPO captures nuanced preferences and enables the agent to learn from both positive and negative feedback.

\textit{Process reward modeling (PRM)} takes a different approach by training a separate reward model \(r_\phi(s_t, a_t)\) to predict process rewards based on the collected data. The objective is to minimize a contrastive loss that evaluates the quality of preferred actions relative to less-preferred actions:
\begin{equation}\scriptsize
\begin{aligned}
\mathcal{L}_{\text{PRM}}(\phi) = - \mathbb{E}_{(s_t, a_t^+, a_t^-) \sim \mathfrak{D}} \Big[  \log \sigma \big( r_\phi(s_t, a_t^+) - r_\phi(s_t, a_t^-) \big) \Big].
\end{aligned}
\end{equation}
Unlike SFT and DPO, PRM does not directly tune the policy \(\pi_{f(\theta)}\) but instead trains the reward model \(r_\phi\) parameterized by \(\phi\) to estimate the quality of intermediate reasoning and actions. The reward model can then guide decision-making by selecting high-quality actions during inference, eliminating the need for agent fine-tuning. This makes PRM especially useful for large-scale or proprietary models, offering a flexible and scalable approach to improving reasoning and search.
Algorithm \ref{alg:prm_best_of_n} details how trained process reward models are applied during inference.


