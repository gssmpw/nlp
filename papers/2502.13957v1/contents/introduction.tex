\section{Introduction}

Large language models (LLMs) have revolutionized online information-seeking by providing direct responses to user needs. Such capabilities are often evaluated by knowledge-intensive tasks such as multi-hop question answering (QA), which require sequential reasoning over relevant knowledge \cite{lewis2020retrieval,welbl2018constructing,tang2024multihop}. Pre-trained on trillions of tokens from diverse sources, LLMs have demonstrated strong reasoning capabilities and acquired considerable general knowledge \cite{wei2022emergent}. As such, they are often used as standalone systems that process user queries and generate responses without relying on external utilities \cite{brown2020language,achiam2023gpt,touvron2023llama1,touvron2023llama2}. Despite these impressive capabilities, LLMs often struggle with questions where they may lack sufficient and up-to-date domain knowledge. This may lead to inaccurate responses or hallucinations, which can be particularly problematic in high-stakes fields \cite{zhang2023siren,sahoo2024large,ji2023survey}.

Retrieval-augmented generation (RAG) has emerged as a promising solution to address knowledge-intensive tasks \cite{lewis2020retrieval}. By grounding the LLM outputs in relevant information returned by information retrieval (IR) systems, RAG improves both accuracy and verifiability \cite{gao2023retrieval}. Expanding on this paradigm, information-seeking agents such as ReAct \cite{yao2023react} iteratively interact with IR systems to tackle complex questions more effectively \cite{yao2023react,asai2023self,shinn2024reflexion,li2025search}. However, most existing methods rely heavily on prompting techniques \cite{trivedi2023interleaving,asai2023self,jiang2023active,press2023measuring}, which require substantial efforts for the prompt design and might not generalize well across different tasks \cite{khot2022decomposed,shao2023enhancing,yu2024autorag}.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{figs/gym_overview.pdf}
    \caption{Overview of RAG-Gym. (a) RAG-Gym formulates the knowledge-intensive question-answering task as a nested Markov Decision Process (MDP). The process reward data is collected by randomly sampling action candidates at each time step and using an external annotator to select the best one. (b) Different process supervision methods implemented in RAG-Gym.}
    \label{fig:rag_gym_mdp}
\end{figure*}

To improve such agents in solving knowledge-intensive and multi-hop QA tasks, we propose \textbf{RAG-Gym}, a unified framework for optimizing agentic RAG through process supervision. RAG-Gym formulates the knowledge-intensive QA as a nested Markov Decision Process (MDP), where the outer MDP action quality is annotated with process rewards, which in turn guide the inner MDP token generation. By supporting diverse agent architectures and process supervision methods, RAG-Gym provides a comprehensive platform for improving information-seeking agents.
In addition to RAG-Gym, we propose \textbf{ReSearch}, a novel agent architecture that unifies answer reasoning and search query generation in a single action. This design explicitly aligns the generated query with missing information in answer construction. 
Through extensive experiments, we demonstrate that RAG-Gym effectively enhances agentic RAG performance on knowledge-intensive tasks, with ReSearch outperforming other baseline agents. Notably, our findings highlight the benefits of training process reward models as verifiers for agentic RAG actions and show their transferability across different LLMs. Further analysis reveals the effectiveness of advanced LLMs as process reward judges and uncovers the scaling laws of process supervision in RAG-Gym across both training and inference phases.

Our key contributions are four-fold: (1) We introduce RAG-Gym, a unified framework for optimizing agentic RAG with process supervision. (2) We propose ReSearch, a novel agent architecture that synergizes answer reasoning and search, achieving state-of-the-art performance over existing baselines. (3) We demonstrate that using trained process reward models as verifiers significantly improves search agent performance. (4) We provide a comprehensive analysis of process supervision sources, reward model transferability, and scaling laws in agentic RAG.