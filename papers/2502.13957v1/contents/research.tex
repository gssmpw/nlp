\section{Reasoning and Search (ReSearch) Agent}

Existing agent architectures, such as ReAct \citep{yao2023react}, exhibit inherent limitations when applied to knowledge-intensive tasks. They can only reason about either the answer generation or the query generation at each step. This may lead to queries that cannot directly contribute to answer construction, as these two tasks are not necessarily aligned during reasoning. As illustrated in Appendix \ref{sec:case_study}, existing designs may generate queries that fail to retrieve the most relevant evidence, leading to suboptimal information acquisition and degrading answer quality. This limitation underscores the necessity of an architecture that explicitly aligns search queries with answer construction.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figs/agent_architecture.pdf}
    \caption{Architecture of ReSearch, which synergizes \underline{Re}asoning and \underline{Search} by integrating history knowledge summarization, answer reasoning, and query generation to iteratively resolve missing information in constructing the final answer.}
    \label{fig:architecture}
\end{figure}

\input{tables/llama-3-1-8b}

To address this issue, we propose the Reasoning and Search (ReSearch, as shown in Fig~\ref{fig:architecture}) agent, which integrates reasoning and search in a unified, answer-driven framework. 
Given the state \(s_t\) with the original question \(\mathcal{Q}\) and the history \(\mathcal{H}_t = \{(q_1, D_1), \dots, (q_{t-1}, D_{t-1})\}\),
the agent will first summarize the retrieved documents into structured responses to their corresponding queries, forming a refined knowledge representation $\mathcal{H}_t'$:
\begin{equation}
    \mathcal{H}_t' = \{(q_1, m_1), \dots, (q_{t-1}, m_{t-1})\},
\end{equation}
where $ m_i $ represents the summarized answer to $q_i$ given $ D_i $. This summarization step filters out irrelevant information and mitigates the challenge of long-context processing, allowing the agent to focus on the most relevant facts when constructing an answer.

Using this refined knowledge $\mathcal{H}_t'$, the agent then engages in structured reasoning to infer a candidate answer to the question.
It will then examine the reasoning steps and judge if all claims are well grounded in the history.
If the agent determines that all claims in the answer reasoning are supported by retrieved evidence, it outputs the answer as its final action. Otherwise, it identifies unverified claims, which are statements that lack sufficient justification based on the available evidence. These unverified claims serve as the basis for generating the next search query, which is specifically designed to retrieve the missing information. The retrieved documents from this query are then added to $\mathcal{H}_t$, and the reasoning process is repeated iteratively until all claims are verified or the retrieval budget is exhausted.

A key advantage of the ReSearch agent lies in its answer-driven query formulation, which ensures that each search query is explicitly designed to search for missing information in the answer reasoning process. In contrast, conventional architectures such as ReAct rely on implicit LLM heuristics to determine when and how to issue search queries, often resulting in the retrieval of unhelpful information that cannot be effectively utilized in answer generation. By structuring query generation around ungrounded claims, our approach guarantees that retrieved evidence directly contributes to constructing the final answer, significantly improving retrieval efficiency and overall response quality. Empirical results, as presented in Section \ref{sec:results}, demonstrate that this structured approach to query generation leads to more precise and well-supported answers compared to conventional information-seeking agents.
