
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/pipe3.png}
    \vspace{-3ex}
    \caption{\textbf{GlossGau Pipeline.} Our framework initializes with surfel-based Gaussian primitives, where each primitive carries both geometric attributes (position, covariance, and opacity) and appearance properties. The appearance representation decomposes into view-dependent diffuse albedo, parameterized via Spherical Harmonics, and specular BRDF terms, modeled through Anisotropic Spherical Gaussians. The final rendering integrates these material maps with differentiable environment lighting to achieve high-fidelity inverse rendering results.
    }
    \label{fig:pipe}
    \vspace{-2ex}
\end{figure*}



\section{Related Works}
\label{sec:related}
\noindent{\bf Neural Radiance Field.}
Neural Radiance Fields (NeRF) \cite{mildenhall2020nerf} have achieved remarkable progress in photo-realistic novel view synthesis through implicit representations and volume rendering. Subsequent research has expanded NeRF to various applications. \cite{barron2021mip, barron2022mip} which enhance rendering fidelity through 3D conical frustum sampling, establishing new benchmarks in novel view synthesis. Several works \cite{oechsle2021unisurf, yariv2021volume, wang2021neus, yu2022monosdf} augment NeRF with implicit surface representations to achieve higher geometric accuracy. Some studies \cite{srinivasan2021nerv, zhang2021nerfactor, yao2022neilf, verbin2022ref} address the challenges of modeling specular surfaces and view-dependent reflections. A parallel direction of research \cite{muller2022instant, fridovich2022plenoxels, liu2020neural, chen2022tensorf, garbin2021fastnerf, sun2022direct} tackles NeRF's computational limitations through spatial acceleration structures such as voxel grids and hash encodings. However, despite these innovations, NeRF-based methods  continue to face constraints in rendering performance and memory efficiency during training.

\noindent{\bf 3D Gaussian Splatting.}
Point primitives as a fundamental rendering element were first proposed by~\cite{levoy1985use}. Following seminal developments in point-based rendering~\cite{aliev2020neural,xu2022point,zhang2022differentiable}, 3D Gaussian Splatting~\cite{kerbl20233d} represents a breakthrough that enables real-time photorealistic neural rendering through efficient tile-based rasterization of splatted 3D Gaussian primitives. This methodology bypasses the computational burden of dense ray sampling while surpassing implicit neural representations in both visual fidelity and computational efficiency. Subsequently, various enhancements have emerged to improve 3D-GS. \cite{Yu2023MipSplatting} utilizes a 3D smoothing filter and a 2D mip filter to reduce aliasing when zooming in and zooming out. \cite{huang2024erroranalysis3dgaussian} proposes an optimal projection strategy by minimizing projection errors, resulting in photo-realistic rendering under various focal lengths and various camera models. Scaffold-GS~\cite{lu2024scaffold} multi-scale hybrid architecture that combines explicit 3D Gaussians with implicit MLP networks, significantly reducing the size of 3D-GS and improving the rendering quality. \cite{Huang2DGS2024} extends it to a perspective-accurate 2D splatting process for detailed geometry. 



\noindent{\bf Material and Lighting Estimation.}
Reconstruction and inverse rendering of a glossy object from multview images has been a challenging task due to the complex light interactions and ambiguities. Some methods simplify the problem by assuming controllable lighting conditions~\cite{azinovic2019inverse,guo2019relightables,park2020seeing,bi2020DRV,bi2020deep3d,nam2018practical,schmitt2020joint,bi2020NRF}. Subsequent research has explored more complex lighting models to handle realistic scenarios. NeRV~\cite{srinivasan2021nerv} and PhySG~\cite{zhang2021physg} leverage environmental maps to manage arbitrary lighting conditions. NeRD~\cite{boss2021nerd} addresses the challenge of images captured under varying illumination by attributing Spherical Gaussians to each image.  Ref-NeRF~\cite{verbin2022ref} employs integrated direction encoding to improve the reconstruction fidelity of reflective objects. NeRO~\cite{liu2023nero} use Integrated Directional Encoding similar to Ref-NeRF~\cite{verbin2022ref} and employs a two-stage inverse rendering strategy. TensoIR~\cite{jin2023tensoir} employs TensoRF~\cite{chen2022tensorf} as its geometry representation and utilizes ray marching to compute indirect illumination and visibility. However, these methods that rely on implicit representations~\cite{zhang2021physg, verbin2022ref, liu2023nero, karis2013real, jin2023tensoir, srinivasan2021nerv, wu2023nerf, zeng2023relighting} involve densely sampled points along rays, leading to a significant decrease in the performance of inverse rendering.  


Recent works~\cite{jiang2024gaussianshader,liang2024gs, gao2023relightable} aim to employ 3D-GS~\cite{kerbl20233d} as an explicit representation of scenes in order to enhance the performance of inverse rendering. GaussianShader~\cite{jiang2024gaussianshader} decomposes the original color attributes into components including diffuse, specular and residual color for modeling the color of reflective surfaces. In GS-IR~\cite{liang2024gs}, a technique similar to light probes~\cite{debevec2008median} is employed to obtain the indirect light and ambient occlusion. These methods~\cite{jiang2024gaussianshader, liang2024gs} encounter challenges in representing accurate surfaces and materials using 3D Gaussians and struggle to disambiguate between macroscopic surface normals and microscopic roughness parameters, leading to reduced inverse rendering quality and extended optimization times. \cite{gao2023relightable} directly estimates BRDF parameters for each Gaussian and utilizes the normal-based densification to ensure the volumetric rendering quality. For complex surfaces, its result suffers from over-inflated size and significantly reduces rendering speed. In contrast, our method effectively preserves the efficiency and achieves comparable rendering quality with better geometry estimation. Our method effectively balances the inverse rendering quality and the space and time complexity of optimization. 


