\section{Method}

In the following section, we describe the GlossGau pipeline. Our method begins with reconstructing Gaussian Surfels and estimates BRDF parameters for specular surface representation. We employs Anisotropic Spherical Gaussian to represent normal distribution function, which enhances the representation ability without additional optimization overhead. Moreover, we compute the visibility based upon the modified point-based ray tracing~\cite{gao2023relightable}, which contributes to the rendering of realistic shadow effect. The overview of our method is illustrated in Figure \ref{fig:pipe}.


\subsection{Preliminaries}
\textbf{3D Gaussian Splatting.} 3DGS~\cite{kerbl20233d} is a point-based method that employs explicit 3D Gaussian points as its rendering primitives. A 3D Gaussian point is mathematically defined by a center position $\boldsymbol{x}$, opacity $\sigma$, and a 3D covariance matrix $\Sigma$. The appearance $\boldsymbol{c}$ of each 3D Gaussian is represented using the first three orders of spherical harmonics (SH) to achieve the view-dependent variation. The 3D Gaussian value at point $x$ is evaluated as
\begin{equation}
G(\boldsymbol{x}) = \exp(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^\top\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu}))
\label{eq:3d_gaussian}
\end{equation}
In practice, the 3D covariance matrix $\Sigma$ is decomposed into the quaternion $\boldsymbol{r}$, representing rotation, and the 3D vector $\boldsymbol{s}$, representing scaling such that $\Sigma = \boldsymbol{R}\boldsymbol{S}\boldsymbol{S^T}\boldsymbol{R^T}$ for better optimization. 

During the rendering process, 3D Gaussians are firstly ``splatted'' into 2D Gaussians on the image plane. The 2D means are determined by accurate projection of 3D means, while the 2D covariance matrices are approximated by:
$\Sigma'=\boldsymbol{J}\boldsymbol{W}\Sigma\boldsymbol{W}^\top\boldsymbol{J}^\top$,
where $\boldsymbol{W}$ and $\boldsymbol{J}$ represent the viewing transformation and the Jacobian of the affine approximation of perspective projection transformation~\cite{zwicker2001surface}. The pixel color is thus computed by alpha blending corresponding $N$ 2D Gaussians sorted by depth:
\begin{equation}
C(\textbf{p}) = \sum_{i\in{N}}T_{i}\alpha_{i}\boldsymbol{c}_{i}, \hspace{0.5em} T_i = \prod_{j=1}^{i-1}(1-\alpha_{j}) .
    \label{eq:alpha_blending}
\end{equation}
$\alpha$ is obtained by querying 2D splatted Gaussian associated with pixel $\textbf{p}$. 
\begin{equation}
    \alpha_i = \sigma_i e^{-\frac{1}{2} (\textbf{p} - \mu_i)^T \Sigma^{-1} (\textbf{p} - \mu_i) }
\end{equation}


\noindent\textbf{Surfel-based Gaussian Splatting.}
3D-GS is known to fall short in capturing intricate geometry because the volumetric 3D Gaussian, which models complete angular radiance, conflicts with the thin nature of surfaces. To achieve accurate geometry as well as high-quality novel view synthesis, methods \cite{Huang2DGS2024, dai2024high} simplify the 3-dimensional modeling by adopting Gaussian surfels embedded in 3D space. This 2D surfel approach allows each primitive to distribute densities across a planar disk, with the normal vector simultaneously defined as the direction exhibiting the steepest density gradient. In GlossGau, we follow the design of 2D-GS \cite{Huang2DGS2024} to reformulate the covariance matrix into two principal tangential vectors $\bt_u$ and $\bt_v$, and a scaling vector $\bS = (s_u,s_v)$. The scaling vector $s_z$ is enforced to be zero. 


A 2D Gaussian is therefore defined in a local tangent plane in world space, which is parameterized:
\begin{gather}
    P(u,v) = \bp + s_u \bt_u u + s_v \bt_v v = \bH(u,v,1,1)^{\mathrm{T}}\\
    \text{where} \, \bH = 
    \begin{bmatrix}
        s_u \bt_u & s_v \bt_v & \boldsymbol{0} & \bp \\
        0 & 0 & 0 & 1 \\
    \end{bmatrix} = \begin{bmatrix}
        \bR\bS & \bp_k \\ 
        \boldsymbol{0} & 1\\
    \end{bmatrix}
    \label{eq:plane-to-world}
\end{gather}
in which $\bH \in {4\times 4}$ is a homogeneous transformation matrix representing the geometry of the Gaussian surfel. For the point $\bu=(u,v)$ in the local $uv$ space of the primitive, its 2D Gaussian value can then be evaluated by standard Gaussian
\begin{equation}
G(\bu) = \exp\left(-\frac{u^2+v^2}{2}\right)
    \label{eq:gaussian-2d}
\end{equation} 

\subsection{Glossy Appearance modeling}
Both 3D-GS and 2D-GS model the view-dependent appearances with low-order spherical harmonic functions, which has limited ability to capture the high-frequency information, such as specular surfaces in the scene. We model the light-surface interactions in a way consistent with the rendering equation~\cite{kajiya1986rendering}: 

\begin{equation}
L_{o}(\boldsymbol{\omega_{o}}, \boldsymbol{x}) = \int_{\Omega}f(\boldsymbol{\omega_{o}}, \boldsymbol{\omega_{i}}, \boldsymbol{x})L_{i}(\boldsymbol{\omega_{i}}, \boldsymbol{x})(\boldsymbol{\omega_{i}}\cdot\boldsymbol{n})d\boldsymbol{\omega_{i}}
\label{eq:rendering_equation}
\end{equation}
where $\boldsymbol{x}$ and $\boldsymbol{n}$ are the surface point and its normal vector, $f$ is the Bidirectional Reflectance Distribution Function (BRDF), and $L_{i}$ and $L_{o}$ denote the incoming and outgoing radiance in directions $\boldsymbol{\omega_{i}}$ and $\boldsymbol{\omega_{o}}$. $\Omega$ signifies the hemispherical domain above the surface. However, accurately modeling light-surface interactions necessitates a precise evaluation of the Rendering Equation, which can be time-consuming for point-based rendering and thus undermines the efficiency benefits offered by Gaussian Splatting. In our approach, the incident light $L_i$ is decomposed into a global 16x32 direct environment map and a local per-Gaussian indirect illumination component, parameterized using three-level SH. For each 3D Gaussian, we sample $N_s$ incident directions over the hemisphere space through Fibonacci sampling~\cite{yao2022neilf, gao2023relightable}, we have found to effectively balance efficiency during both training and rendering phases. 


We further decompose the rendered color into diffuse and specular components: 
\begin{equation}
    \boldsymbol{c} = \boldsymbol{c}_d + \boldsymbol{c}_s 
\end{equation}
Since SH functions model low-frequency information in a low cost manner, only $c_s$, as the specular component, involves the computation of BRDF parameter for the best efficiency. We adopt the simplified Disney Model~\cite{burley2012physically,karis2013real} as in prior work~\cite{zhang2021physg,gao2023relightable}. The specular term of BRDF is defined as 
\begin{equation}
    f_{s}(\boldsymbol{\omega}_{o}, \boldsymbol{\omega}_{i})=\frac{D(\boldsymbol{h}; r) \cdot F(\boldsymbol{\omega}_{o},\boldsymbol{h}) \cdot G(\boldsymbol{\omega}_{i},\boldsymbol{\omega}_{o}, h; r)}{4(\boldsymbol{n} \cdot \boldsymbol{\omega}_{i}) \cdot (\boldsymbol{n} \cdot \boldsymbol{\omega}_{o})}
    \label{eq:specular_term}
\end{equation}
where $r\in[0, 1]$ is the surface roughness, $\boldsymbol{h}=({\boldsymbol{\omega}_{i}+\boldsymbol{\omega}_{o}}) / \|\viewdir + \lightdir\|_2$ is the half vector, \textit{D}, \textit{F} and \textit{G} denote the normal distribution function, Fresnel term and geometry term. We utilize isotropic Spherical Gaussians (SG)~\cite{wang2009all} to approximate \textit{D} under the microfacet-based model \cite{cook1981reflectance}. An $n$-dimensional SG is a spherical function that takes the form:
\begin{equation}\label{eq:sg} 
    G(\boldsymbol{\nu}; \lightsgdir, \lightsgsharp, \lightsgamp) = \lightsgamp \, e^{\lightsgsharp (\boldsymbol{\nu} \cdot \lightsgdir - 1)}
\end{equation}
where $\boldsymbol{\nu} \in \mathbb{S}^2$ is the function input, $\lightsgdir \in \mathbb{S}^2$ is the lobe axis, $\lightsgsharp \in \mathbb{R}_+$ is the lobe sharpness, and $\lightsgamp \in \mathbb{R}_+^n$ is the lobe amplitude. Under the isotropic assumption, we have the approximated \textit{D} after normalization as
\begin{equation}
    D(\boldsymbol{h}) = G(\boldsymbol{h}; \boldsymbol{n}, \frac{2}{r^2}, \frac{1}{\pi r^2})
\end{equation}

However, to faithfully represent real-world BRDFs, which typically exhibit some degree of anisotropy, \cite{wang2009all} relies on a mixture of $n$ distributed Spherical Gaussians. While effective, this mixture model introduces optimization overhead. Anisotropic spherical Gaussian (ASG) \cite{xu2013anisotropic} extends SG and has been demonstrated to effectively model anisotropic scenes with a small number of functions. The ASG function is defined as:
\begin{equation}
\label{equ: asg-function}
\begin{aligned}
    ASG(\mathbf{\nu} \: | \: [\mathbf{x}, \mathbf{y}, \mathbf{z}],[\lambda, \mu], c)= c \cdot \mathrm{S}(\mathbf{\nu} ; \mathbf{z}) \cdot e^{-\lambda(\mathbf{\nu} \cdot \mathbf{x})^{2}-\mu(\mathbf{\nu} \cdot \mathbf{y})^{2}}
\end{aligned}
\end{equation}
where $\nu$ is the unit direction serving as the function input; $\mathbf{x}$, $\mathbf{y}$, and $\mathbf{z}$ correspond to the tangent, bi-tangent, and lobe axis, respectively, and are mutually orthogonal; $\lambda$ and $\mu$ are the sharpness parameters for the $\mathbf{x}$- and $\mathbf{y}$-axis, satisfying $\lambda, \mu>0$; $c$ is the lobe amplitude; $\mathrm{S}$ is the smooth term defined as $\mathrm{S}(\nu ; \mathbf{z}) = \max (\nu \cdot \mathbf{z}, 0)$. 
At the rendering stage, we apply the warping operator presented in \cite{xu2013anisotropic} that takes a normal distribution function approximated as a SG and stretch it along the view direction to produce an ASG that better represents the actual BRDF. We thereby obtain a more accurate BRDF representation while eliminating the need for additional parameter estimation. More detailed are provided in the supplementary. 

In summary, the ASG parameters of 2D Gaussians remain determined exclusively by two factors: the surface normal vector $\boldsymbol{n}$ and roughness value $r$. The next critical step involves calculating view-dependent visibility, which we detail in the following section.




\noindent\textbf{Visibility Approximation.}
Computing visibility values through online ray tracing operations presents significant performance bottlenecks due to its intensive computational demands. Inspired by \cite{gao2023relightable,moenne20243d}, we implement the ray tracing for 2D-GS and precompute the visibility term \textit{V}. Since 2D-GS primitives are primarily thin surfaces, naively constructing a Bounding Volume Hierarchy (BVH) using axis-aligned bounds as AABBs significantly increases the false-positive intersections during the traversal, resulting needless computations. We thereby construct the bounding proxy as an anisotropically scaled icosahedron~\cite{moenne20243d} with a threshold response value $\alpha_{min}$. The scaling factor takes into account particle opacity, allowing for more efficient bounds. We formulate a precise mathematical framework for computing Gaussian particle responses in 3D ray tracing. Rather than a segment falls within a 3D volume, our ray-surfel intersection reduces to a point and the response can be directly evaluated using Eq. \ref{eq:gaussian-2d}. This formulation eliminates the need for approximate solutions previously employed to resolve intersection ambiguities. Furthermore, in contrast to prior approaches, the inverse of our covariance matrix $\bH$ is analytically formulated, which significantly enhances numerical stability of computing particle response on flat Gaussian primitives. We provide detailed information in the supplementary. 



\subsection{Regularizations}
\label{sec:regularization}

\noindent{\bf Surface Normal Estimation.}
Accurate geometric representation is essential for realistic physically-based rendering, as emphasized by the formulation of rendering equation. Predicting accurate normals on discrete Gaussian spheres has been a recognized challenge~\cite{kerbl20233d,Huang2DGS2024,jiang2024gaussianshader,liang2024gs,gao2023relightable}. Although adapting 2D Gaussian methodology demonstrates strong performance in geometric representation, relying exclusively on photometric loss functions can result in reconstruction artifacts. We incorporate a normal attribute $\bn$ into the set of optimizable attributes for each 3D Gaussian. Instead of selecting the shortest axis and predicting residuals for better alignment in 3D-GS, we are able to explicitly define the normal direction as the cross product of two tangential vectors that define the Gaussian surfel. This normal is initialized in association with the Gaussian and optimized through the differentiable rasterization process alongside the tangential vectors. $\bn$ is supervised by the consistency between the rendered normal map and depth gradient, enforcing geometric alignment between the surface normals and depth-derived geometry.

\begin{equation}
\mathcal{L}_{n} = \sum_{i} \omega_i (1-\bn_i^\mathrm{T}\bN)
\vspace{-1ex}
\end{equation}
where $i$ indexes over intersected splats along the ray, $\omega$ denotes the blending weight of the intersection point, $\bn_i$ represents the normal of the splat that is oriented towards the camera, and $\bN$ is the normal estimated by the gradient of the depth map. Specifically, $\bN$ is computed with finite differences from nearby depth points as follows:
\begin{equation}
\mathbf{N}(x,y) = \frac{\nabla_x \bp_s \times \nabla_y \bp_s}{|\nabla_x \bp_s \times \nabla_y \bp_s|}
\label{eq:normal_depth}
\end{equation} 
By orienting each Gaussian splat's normal vector parallel to the estimated surface normal, we ensure the 2D projections locally conform to the underlying geometric surface. 


\noindent{\bf Light Regularization.}
To tackle the inherent material-illumination ambiguity, we apply physics-based regularization terms~\cite{gao2023relightable} that guide the separation of surface properties from lighting effects. Following the assumption of approximately neutral incident illumination~\cite{liu2023nero}:
\begin{equation}
\label{eq:reg_light}
    \mathcal{L}_{light} = \sum\nolimits_{c}(L_{c} - \frac{1}{3}\sum\nolimits_{c}L_{c}), c\in\{R,G,B\}.
\end{equation}

\noindent{\bf Alpha Mask Constraint. }
We observe the optimization of 2D-GS on bounded scenes sometimes can be unstable under normal regularization, where the background Gaussians cannot be well eliminated. When the foreground mask is provided, we constrain the training by L1 loss. 
\begin{equation}
    \mathcal{L}_{alpha} = \| M - O\|
\end{equation}
where $M$ is the alpha channel of the input image and $O$ is the rendered opacity. 

