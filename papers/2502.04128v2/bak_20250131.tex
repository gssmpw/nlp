%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{subcaption}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{CJKutf8}

% ljh: 表格优化的包
% \usepackage[margin=2cm]{geometry}
\usepackage{array}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{graphicx}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{lineno}
\usepackage{graphicx} 
% \usepackage{ctex}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{LLaSA: Scaling Train-Time and Test-Time Compute for   \\
           LLaMA-based Speech Synthesis}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% Text-based large language models (LLMs) have achieved remarkable success, with the GPT series largely attributed to the scaling law of training and the O1 model primarily linked to the scaling law of inference. 
% Building on these insights, we aim to explore the scaling of train-time and test-time compute in speech synthesis. However, current state-of-the-art (SOTA) TTS systems are often multi-stage, complicating the decision of whether to scale a particular model during training or testing.  To simplify the problem, we developed a well-designed single vector quantizer (VQ) codec, enabling a single Transformer architecture for TTS. Since our single Transformer is initialized from LLaMA with an expanded vocabulary that includes speech tokens, we name our TTS system LLaSA.
% Our experiments find that 1) 从tts系统的角度看，single transformer + tokenizer 的方案的主要缺点在于single token的codec会损失相对多一点声学细节，但是generative model部分single transformer 并没有比 ar +nar 差.
% 2) 从scale train compute，我们发现 scale train compute  模型生成的音频 naturalness 稳定的提升，模型的incontext learning ability 稳定提升。
% 3) 从scale test compute 角度看，我们发现选择合适的verifier，tts系统可以在 verifier的方向上取得了越来越好的效果，随着scale test compute的增加。 比如 音色相似度，情感相似度，内容准确度等等
% Our experiments find that 1) from the perspective of whole TTS systems, the single Transformer with tokenizer approach's main drawback is that the single-VQ codec tends to lose more acoustic details. However, from the view of the generative model, a single Transformer is comparable to the current SOTA solution AR+NAR methods. 
% Recent advances in text-based large language models (LLMs), particularly in the GPT series and the O1 model, have demonstrated the effectiveness of scaling both training and inference. However, current state-of-the-art (SOTA) TTS systems based on LLM are often multi-stage, complicating the decision of whether to scale a particular model during training or testing.  In this work, we focus on the scaling of test-time and train-time compute and propose a simple framework for speech synthesis that utilizes a single vector quantizer (VQ) codec and a single Transformer architecture. Our TTS system, named LLaSA, is initialized from the LLaMA model with an expanded vocabulary that includes speech tokens and trained with the next token prediction paradigm.
% Our experiments find that
% 1) When scaling train-time compute for LLaSA, we observe a constant improvement in the naturalness of synthesized speech and the model's in-context learning ability. 2) from the perspective of scaling test-time compute, we find that by selecting an appropriate verifier model, the TTS system can achieve increasingly better performance with increased test compute. Combined with these together, LLaSA outperforms all previous tts systems on both the seed-tts-eval and Librispeech-test-clean benchmarks.
% Recent advances in text-based large language models (LLMs), particularly in the GPT series and the O1 model, have demonstrated the effectiveness of scaling both training-time and inference-time. However, current state-of-the-art (SOTA) TTS systems 用了 LLM 的 are often multi-stage, complicating the decision of whether to scale a particular model during training or testing. In this work, 为了学习text llm的成功， we focus on explore scaling of  train-time compute  and  test-time compute for speech synthesis.  ，We propose a simple framework for speech synthesis that utilizes a single vector quantizer (VQ) codec and a single Transformer architecture 使得tts的架构第一次完全对齐了text LLM.    Our TTS system, named LLaSA, is initialized from the LLaMA model with an expanded vocabulary that includes speech tokens and trained with the next token prediction paradigm. % Our experiments find that
% % 1) When scaling train-time compute for LLaSA, we observe a constant improvement in the naturalness of synthesized speech  core TTS aspects such as naturalness ， render more complex and accurate prosody patterns, taking cues from texts without explicit labels。

% 2) from the perspective of scaling test-time compute,  We  employ speech understanding model as  
% verifiers for scaling inference-time compute in search. 
% we find that scaling inference-time  shifts the sample modes more toward the bias of specific verifiers， 从而提升 情感，音色以及内容准确度等等.  
%\vspace{-10pt}
Recent advances in text-based large language models (LLMs), particularly in the GPT series and the O1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. The contribution of this work includes the follows. First, 
%In this work, inspired by the success of text-based LLMs, 
we explore the scaling of train-time and test-time compute for speech synthesis. 
Second, we propose a simple framework LLaSA for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard language transformers such as LLaMA.
%LLMs. Our TTS system, named LLaSA, is initialized from the LLaMA model with an expanded vocabulary that incorporates speech tokens and is trained using the next-token prediction paradigm. 
Our experiments reveal that scaling train-time compute for LLaSA consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. 
%, effectively capturing the meaning conveyed in text without explicit labels. 
Furthermore, from the perspective of scaling test-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the biases of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy.
In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model.  

\end{abstract}

\section{Introduction}
%\vspace{-10pt}
% establish the context, background and/or importance of the topic
% present an issue, problem, or controversy in the field of study
% define the topic and/or key terms used in the paper
% state the purpose of the essay or short paper
% provide an overview of the coverage and/or structure of the writing


%  llm 很成功 ， GPT series 是其中一个代表，demonstrated that larger models and more training data lead to better performance.但是由于数据的有限，已经接近瓶颈了 另一个方向，o1 model 
% has gained attention for its outstanding performance ，slow, deep, demonstrates a test-time computing scaling effect: the greater the computational effort in the inference, the better the model’s performance. 可以更 can produce higher-quality responses。解决hard 问题。 Inference-time scaling opens new avenues for improving model performance when additional resources become available after training. 

% 最近的 Speech synthesis很多效果都很好， 用了很多数据，更大的模型，更精心设计的结构，去从整个系统的角度展示自己是最好的模型，但是，这也导致了大家为了更好的效果，总是在探索更好的架构。然而
% 从text llm的角度来看，模型结构的问题算是解决了，就是一个简单的transformer+tokenizer，这也使得text llm社区的发展，将精力和视角放到在有一个标准的模型上，从而探索更多更进一步的事情，比如training scaling law，test scaling law。 又或者在base 模型上的 fine-tune or 为了部署开发剪枝量化。
% however，这些探索在tts领域是缺失的。 我们认为 ，tts仍然没有一个统一的架构，这导致了到了今天， 从而阻碍了很多规律探索或技术的发展。 从这一角度，我们首先第一步 完全对齐了text llm， 仅仅一个single transformer + tokenizer，which well design for speech。 这一系统，在某些层面上也许不是最优的，对于tts任务，我们会在后续的实验中探索。 但得益于架构已经统一，而且简单，灵活，可扩展的结构，我们可以做很多beyond架构的更进一步的探索。 

% 在从模型架构上对齐了text llm后，我们从 语音合成的角度在training-time 和 inference time 的scaling law开始探索，发现了 Our experiments reveal that scaling train-time compute for LLaSA consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns, effectively capturing the meaning conveyed in text without explicit labels. 


% Furthermore, from the perspective of scaling test-time compute, we employ speech understanding models as verifiers during search, finding that scaling inference-time compute shifts the sampling modes toward the biases of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. 在librispeech-test-clean 和seed tts eval上取得了sota，由于之前的模型都是依靠 in-context learning 。利用promt去control生成音频的属性 比如音色，情感等。 但我们的方法预示了新方向，tts合成可以在 in-context learning ability 的基础上进一步通过 scaling inference-time via search 去加强对这些属性的控制。

% 总的来说，我们的论文贡献3点。
% 1，借助精心设计的speech tokenizer，我们实现了一个和text llm架构完全对齐的tts模型，十分的简单灵活， 
% 2，scala train 发现 
% 3，scale test 发现
% 4，全部开源！


Recent years have witnessed the remarkable success of large language models (LLMs) in the text domain, represented by the GPT series\cite{brown2020language}. These advances have demonstrated that increasing model size and training data consistently yields better performance across a wide array of natural language understanding and generation tasks. However, as the text domain approaches data saturation, new directions are emerging, such as “O1” models \cite{jaech2024openai} that emphasize extensive computational effort at test time—thereby exhibiting a test-time scaling effect. By investing more resources during inference, these models are able to produce higher-quality outputs and handle more complex tasks, offering a flexible avenue for performance improvement after the training phase.

Meanwhile, text-to-speech (TTS) research has also made impressive strides. Many existing TTS systems focus on devising better model architectures—leveraging well-designed modules, larger datasets and model size —to push synthesis quality ever higher. While these efforts have propelled the field forward, they also tend to narrow the community’s perspective: the exploration for better architectures can overshadow investigations into broader, potentially transformative research questions. In contrast, the text LLM community has converged on a relatively standard framework—a simple Transformer model with a tokenizer—which allows researchers to concentrate on fundamental issues such as training-time scaling laws \cite{kaplan2020scaling}, test-time scaling behaviors \cite{snell2024scaling}, and downstream adaptations (e.g., fine-tuning \cite{hu2021lora}, pruning, and quantization\cite{zhu2024survey}). Such a common design philosophy has catalyzed rapid progress and deeper insights into the text domain.

Motivated by these observations, we propose aligning TTS with the minimalist yet powerful paradigm of text LLMs. We introduce a single Transformer-based TTS model that relies on a well-designed speech tokenizer. More specifically,  our TTS
system, named LLaSA, is initialized from the LLaMA \cite{touvron2023llama} model with an expanded vocabulary that incorporates speech tokens and is trained using
the next-token prediction paradigm.
Although this model may not always match the performance of highly customized TTS systems, its streamlined design creates a unified foundation for exploring a wider range of research questions—beyond architecture exploration.

In particular, we systematically investigate both training-time and inference-time scaling effects under this unified TTS framework. Our experiments show that scaling training-time compute (e.g., increasing model size or training data) not only improves naturalness but also enhances expressive prosody, effectively capturing the meaning conveyed in text
without explicit labels. Additionally, we examine the utility of inference-time scaling by incorporating speech understanding models as verifiers in the search process. We find that spending more computational effort during inference aligns generation outputs more closely with specific verifier biases, yielding better emotional expressiveness, timbre consistency, and content accuracy. Evaluations on LibriSpeech test sets and seed-tts-eval demonstrate state-of-the-art results, and further highlight how in-context learning capabilities can be combined with search-based refinements to control factors such as speaker identity or emotion.

To foster continued innovation in this emerging “unified TTS” direction, we have open-sourced our entire codebase and model implementations. We hope this work will inspire the TTS community to look beyond the pursuit of ever more elaborate architectures and instead investigate fundamental scaling laws and inference strategies—just as has been done so successfully for text LLMs.

 
In summary, our paper makes several key contributions. We design a TTS model named LLaSA that is fully aligned with standard LLM architectures by utilizing a single Transformer and a well-designed speech tokenizer, resulting in a simple, flexible, and scalable system. Additionally, we find that increasing training-time compute for LLaSA leads to significant improvements in speech naturalness and prosody accuracy, reflecting a deeper semantic understanding of the input text. We also demonstrate that scaling inference-time compute through the use of speech understanding verifiers enhances emotional expressiveness, timbre consistency, and content accuracy in synthesized speech. Furthermore, by providing open access to our models and frameworks, we foster further research and development in the TTS community.


\vspace{-15pt}
\section{Related work}
\vspace{-5pt}
\subsection{Scaling Law during training and testing}
\vspace{-5pt}
Scaling Law during Training and Testing
Over the past decades, neural networks, with their scaling law, have been the driving engine behind the AI revolution. Particularly in the text domain, large language models (LLMs) represented by the GPT series have demonstrated that scaling up data volumes, computational resources, and model sizes leads to better performance on various tasks, making it a promising path for advancing LLMs toward cognitive intelligence.

However, although existing models still perform below expectations in terms of robustness and handling complex tasks, further scaling in the training phase becomes difficult due to the scarcity of data and computational resources. To address this issue, a scaling law during testing is proposed: the greater the computational effort in the inference, the better the model’s performance \cite{ji2025test}. With technologies such as repeat sampling, self-correction and tree search, the reasoning depth and accuracy of LLMs in solving complex problems are greatly improved.

% Although its effects have been widely proven in text modality, scaling law during training and testing in speech modality has yet to be investigated. Parke et al. demonstrate that scaling a transformer architecture with large parameters and speech corpora effectively improves the performance of speech tokenizers. The scaling law for TTS models during training and testing is an open question.
\vspace{-10pt}
\subsection{LLM-based TTS}
\vspace{-5pt}
% audiolm
% valle 
% seed 
% cosy
% touchtts
% basetts
% clam-tts
% melle
% kelle
% speartts
% fishspeech
% LLM TTS
Significant achievements have been made by large language models (LLMs) in speech generative tasks, such as multiple-speaker speech syntheses \cite{speartts} and zero-shot voice cloning \cite{wang2023neural,chen2024vall}. VALL-E \cite{wang2023neural} first regards TTS tasks as a conditional language task, which utilizes neural codec to extract speech tokens and employs a two-stage modeling method to predict them, with an autoregressive (AR) model for generating coarse audio tokens, followed by a non-autoregressive (NAR) model to iteratively predict residual tokens. VALL-E X \cite{vallex} extends VALL-E into multi-lingual scenarios to support zero-shot cross-lingual speech synthesis. Spear-TTS \cite{speartts} integrates multi-stage AR language models and achieves a multi-speaker TTS system with minimal supervision.

Recently, many TTS systems have focused on achieving more natural and controllable speech synthesis by leveraging well-designed modules, larger datasets and model sizes. TorToise-TTS \cite{tortoisetts}, BASE TTS \cite{basetts}, Seed-TTS \cite{anastassiou2024seed}, CosyVoice \cite{du2024cosyvoice}, and FireRedTTS \cite{guo2024fireredtts} combine an AR language model and a diffusion model and are trained on hundreds of thousands of hours of speech data, which integrate advantages of the language model and diffusion model, realizing impressive performance in speech quality and speaker timbre similarity. On the other hand, some researchers push for a way to simplify the TTS paradigm. MELL-E \cite{melle} introduces a VAE-like latent sampling module into an AR language model to predict Mel spectrograms, while KALL-E \cite{kalle} leverages an AR language model to predict speech distributions extracted by a WaveVAE model.

% While these efforts have propelled the TTS field forward, there is no broad consensus on a relatively standard framework. A simple Transformer model with a tokenizer aligns the text LLM and allows researchers to concentrate on fundamental issues such as training-time scaling laws, test-time scaling behaviors, and downstream adaptations.

% \subsection{Speech Tokenizer}
% % Speech Tokenizer
% Speech tokenizers refer to a kind of neural network model that converts continuous speech signals into discrete tokens, which discretize high-rate audio signals into a finite set of tokens, hence enabling the application of LLM architectures to speech data. Current discrete speech tokens can be categorized into two types: semantic tokens and acoustic tokens. Semantic tokens are typically from self-supervised pre-trained models with masked language modeling as training objectives, such as HuBERT and WavLM. Derived through k-means clustering on representations from a specific intermediate layer, semantic tokens are depicted as a single-level sequence. Acoustic tokens can be extracted from neural audio codecs with reconstruction as a training objective. Mostly utilizing residual vector quantization (RVQ) with hierarchical quantizers for discretization, acoustic tokens are represented as multiple sequences.

% Semantic tokens exhibit a high alignment with text while losing some information in speech, such as timbre, while acoustic tokens excel in effectively preserving speech information but do not demonstrate a strong alignment with the text [SpeechTokenizer]. To this end, SpeechTokenizer builds specialized speech tokens designed for speech-language models by unifying semantic and acoustic tokens within RVQ. However, such an RVQ-based tokenizer still brings challenges to downstream tasks, which requires additional structures to predict multiple tokens at each time step. To address this issue, some researchers investigate single-codebook neural speech tokenizers [Single-Codec, Wavtokenizers]. For example,  Single-Codec employs a disentangled VQ-VAE on Mel Spectrograms to decouple speech into time-invariant global embedding and one phonetically-rich discrete sequence quantized by one codebook.

% Although they simplify discrete sequences into a single sequence, these speech tokenizers require additional information when decoding. How to ensure that all aspects of the speech signals — content, prosody, timbre — are captured by one discrete sequence still requires exploration.
% \newpage
\vspace{-15pt}
\section{Methods}
\vspace{-5pt}
% \subsection{Overview}

% % % Similar to text llm， 我们的tts  只有 speech tokenizer 和 一个 transformer。 given 一个waveform，我们speech_to
% % 我们的模型架构与初始话的text llm 完全一致，我们第一步

% Since our architecture aims to fully align text LLM, so we only have a   tokenizer and LLM.   our llm structure is initialized from an existing LLM such as llama. Our tokenizer is also 继承这个 text llm。 这样，剩下的只有 is to convert speech waveform to discrete token。我们设计了
% speech tokenizer X-codec2，can encoder waveform to token，然后decode 回waveform 高质量的，同时，用了single vq to satisfy 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. 同时，区别于之前的一些tts tokenizer，我们的speech tokenizer的decode仅从token，不需要额外信息。也就是说llm建模了全部的speech信息。%所以可以 scale llm

% 我们的模型训练遵循next token predication范式，我们把tts数据，构建了《text tokens》《speech tokens》这样的sequence。我们计算 speech token部分的ce loss。 

% %

\subsection{Overview}
\vspace{-5pt}
% Our TTS framework is designed to fully align with standard text LLM architectures, retaining only two core components: (1) a tokenizer and (2) a single Transformer-based LLM. We initialize the Transformer from an existing LLM (e.g., LLaMA) and inherit its text tokenizer. The primary task, then, is to convert speech waveforms into discrete tokens, expand the vocabulary to include  speech  token. To this end, we propose a novel speech tokenizer, X-codec2, which encodes waveforms into tokens and decodes them back to high-quality audio. Unlike some prior TTS tokenizers, X-codec2 decodes audio solely from the token sequence, obviating the need for extra features and ensuring that all speech information is modeled within a single LLM.

% To train this system, we follow the standard next-token prediction paradigm. Specifically, we convert the TTS dataset into sequences of  text tokens  +  speech tokens  and apply the cross-entropy loss on the predicted speech tokens.

% ### 2.1 Overview 
Our TTS framework is designed to fully align with the standard text LLM paradigm, keeping only two main components: (1) a tokenizer and (2) a single Transformer-based LLM. We initialize the Transformer parameters \(\phi\) from an existing LLM (e.g., LLaMA) and inherit its tokenizer for the text portion. Hence, the core new challenge is to convert raw speech waveforms into sequences of discrete tokens such that the Transformer can model them in an autoregressive manner. 

To achieve this, we introduce our speech tokenizer X-codec2, which encodes waveforms into speech tokens and can decode them back to high-quality audio. Unlike some prior tokenizers for TTS, ours requires no additional extra information when decoding, ensuring that all aspects of the speech signal—content, prosody, timbre—are captured by the LLM.

Formally, let:
1. \( \texttt{Tokenize}_\text{text}(X) = \{x_1, \dots, x_T\} \) be the text tokenizer, which converts input text \(X\) into \(T\) text tokens.
2. \( \texttt{Tokenize}_\text{speech}(Y) = \{y_1, \dots, y_S\} \) be the speech tokenizer, which converts a speech waveform \(Y\) into \(S\) discrete tokens.
3. \( \texttt{Detokenize}_\text{speech}(\{y_1, \dots, y_S\}) = \hat{Y} \) be the speech decoder, which reconstructs the waveform \(\hat{Y}\) from its token representation.

Given a dataset \(\mathcal{D} = \{(X_i, Y_i)\}_{i=1}^N\), where \(X_i\) is the text transcription and \(Y_i\) is the corresponding audio, we represent each pair \((X_i, Y_i)\) as a token sequence \((x_1, \dots, x_T, y_1, \dots, y_S)\). Our Transformer with parameters \(\phi\) learns the joint distribution of text and speech tokens via 
% \[
% P(x_1, \ldots, x_T, y_1, \ldots, y_S) = 
% P(x_1, \ldots, x_T) \cdot P(y_1, \ldots, y_S | x_1, \ldots, x_T)
% \]
\begin{align}
    &P(x_1, \ldots, x_T, y_1, \ldots, y_S)  \notag \\
  = & P(x_1, \ldots, x_T) \cdot  P(y_1, \ldots, y_S | x_1, \ldots, x_T) \notag
\end{align}

Since the text tokens \( \{x_1, \ldots, x_T\} \) are always given as input during training and inference, the model focuses on learning the conditional probability:
% \[
% P(y_1, \ldots, y_S | x_1, \ldots, x_T) = \prod_{s=1}^S P(y_s | x_1, \ldots, x_T, y_1, \ldots, y_{s-1})
% \]
\begin{align}
    &P(y_1, \ldots, y_S | x_1, \ldots, x_T) \notag \\
    =& \prod_{s=1}^S P(y_s | x_1, \ldots, x_T, y_1, \ldots, y_{s-1}) \notag
\end{align}

Therefore, the loss is calculated   over the speech tokens \( \{y_1, \ldots, y_S\} \). The objective is to minimize the negative log-likelihood:
\[
\mathcal{L} = - \sum_{s=1}^S \log P(y_s | x_1, \ldots, x_T, y_1, \ldots, y_{s-1})
\]
make the model learns to predict each speech token \( y_s \) conditioned on both the text tokens \( \{x_1, \ldots, x_T\} \) and the previously speech tokens \( \{y_1, \ldots, y_{s-1}\} \).
 
\vspace{-10pt}
\subsection{Speech Tokenizer}
\vspace{-5pt}
As highlighted by AudioLM \cite{borsos2023audiolm}, discrete speech representations can be categorized into both semantic token and acoustic token. 
language modeling on semantic tokens excels at capturing high-level information such as content and emotion, while modeling with acoustic tokens focuses on low-level details, including timbre and acoustic details. 

Our X-codec2 tokenizer builds on the concepts from prior work X-codec \cite{ye2024codec}, we fuse these semantic and acoustic features together into a unified codebook, but introduce a crucial modification: replacing residual vector quantization with a single vector quantizer to ensure  1D causal dependency. This design naturally aligns with the left-to-right autoregressive mechanism of LLMs and also reflects the inherently left-to-right temporal structure of audio signals.
 
Specifically, our X-codec2 consists of three main components: Encoder, Vector Quantizer and Decoder.
\vspace{-15pt}
\paragraph{Encoder}
Given a raw speech waveform \(\mathbf{Y}\), we employ two separate encoders to derive its semantic and acoustic representations:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Semantic Encoder} \(\mathrm{Enc}_s\): We adopt a pre-trained Wav2Vec2-BERT \cite{barrault2023seamless} to obtain multilingual speech semantic features capturing content and emotional cues.
    \item \textbf{Acoustic Encoder} \(\mathrm{Enc}_a\): Following the design in \citet{xin2024bigcodec} \citet{kumar2024high}, this module uses multiple residual convolutional blocks with Snake activation functions to encode low-level acoustic details.
\end{itemize}
We then concatenate the two outputs to form a fused feature embedding,
\[
\mathbf{H} = \bigl[\mathrm{Enc}_s(\mathbf{Y}), \; \mathrm{Enc}_a(\mathbf{Y})\bigr],
\]
which serves as the input to our vector quantizer.
\vspace{-15pt}
\paragraph{Vector Quantization.}
To obtain discrete tokens, we apply \(\mathrm{FSQ}(\cdot)\)\cite{mentzer2024finite} to \(\mathbf{H}\):
\[
\mathbf{H}_q \;=\; \mathrm{FSQ}(\mathbf{H}),
\]
where \(\mathbf{H}_q\) is the quantized feature. We adopt FSQ due to its stability in training and high codebook usage efficiency. Notably, FSQ does not require an explicit VQ objective term (e.g., codebook commitment loss), simplifying optimization during training.
\vspace{-15pt}
\paragraph{Decoder}
From the quantized representation \(\mathbf{H}_q\), we aim to reconstruct both \emph{semantic} and \emph{acoustic} information:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Semantic Reconstruction}: We follow \citet{ye2024codec} and employ a semantic decoder to predict semantic feature  and  applying  \(\ell_2\)   loss for reconstruction. It is worth noting that during inference, predicting semantic feature is unnecessary; this component is designed to provide a supervisory signal to ensure the codebook retains sufficient semantic information.
    \item \textbf{Acoustic Reconstruction}: Follow Vocos \cite{siuzdakvocos}, we replace the ConvNeXt backbone with a Transformer-based decoder that predicts short-time Fourier transform (STFT) magnitude and phase; an inverse STFT (iSTFT) head then converts predicted spectra back to time-domain waveforms.
\end{itemize}
\vspace{-15pt}
\paragraph{Codec Training}
The training process closely follows that of X-Codec, simultaneously optimizing both semantic and acoustic reconstruction. 
We incorporate a multi-period discriminator (MPD) \citep{kong2020hifi}, a multi-scale STFT (MS-STFT) discriminator, and a spectral discriminator with FFT sizes\cite{anonymous2025scaling} \(\{78, 126, 206, 334, 542, 876, 1418, 2296\}\) for  adversarial training. Additionally, following \cite{anonymous2025scaling}, we incorporate a perceptual loss during the final steps of the training process to further enhance the intelligibility.

\vspace{-10pt}
\subsection{Scaling Train-time Compute}
\vspace{-5pt}
Our primary goal in this section is not to locate a compute-optimal configuration for TTS, but rather to show that, akin to text-based LLMs, increasing train-time resources (either by enlarging the model or expanding the training dataset) consistently improves performance. Specifically, we investigate two   scaling strategies:

\begin{itemize}[leftmargin=1.5em]  

    \item \textbf{Fix Training Data, Vary Model Size.} We fix the training data at \(250\text{k}\) hours and scale the size of the Transformer \(\phi\). Concretely, we adopt LLaMA~3.2 with 1B and 3B parameters, as well as LLaMA~3.1 with 8B parameters, to observe how increased model capacity influences TTS quality.

    \item \textbf{Fix Model Size, Vary Training Data.} We choose LLM initialized from LLaMA~3.2 1B  and train on three progressively larger subsets of our dataset \(\mathcal{D}\), containing \(80\text{k}\), \(160\text{k}\), and \(250\text{k}\) hours of speech respectively. Notably, the 80k and 160k subsets are randomly sample as 1/3 and 2/3 partitions of the full 250k dataset,
    
\end{itemize}

we evaluate these on two aspects:
\vspace{-15pt}
\paragraph{1) Text Understanding Ability.}

A longstanding challenge in Text-to-Speech (TTS) technology is that TTS systems are unable to fully comprehend the meaning of the text like humans do, which leads to mechanical pronunciation, lack of emotion, unnatural pauses, and difficulties in distinguishing homographs.

% 对于english评价
% with 7 categories of texts: Questions, Emotions, Compound Nouns, Syntactic Complexities, Foreign Words, Paralinguistics, and Punctuations.

% 对于chinese评价，我们根据chinese的特性，也设计了新的七种评价，
%  Questions, Emotions,Paralinguistics，中国古诗词，生僻字，多音字，绕口令。

%  对于这些text而言，tts系统不能仅仅扮演渲染的角色，而是需要理解text，才能正确的发音like human。
% By examining synthesized speech for each category, we assess how increased data or model parameters help the Transformer grasp more nuanced textual cues.
English Evaluation: Following BASE TTS, We use seven categories of texts—Questions, Emotions, Compound Nouns, Complex Syntax, Foreign Words, Paralinguistics, and Punctuation.

Chinese Evaluation: We propose seven categories tailored for Chinese which are Questions, Emotions, Paralinguistics, Chinese Poetry, Rare Characters, Polyphonic Words, and Tongue Twisters.

In each case, the TTS system must exhibit a deeper textual understanding to produce natural and context-appropriate speech (e.g., correct pronunciation for polyphonic words and more expressive speech for emotional content). By examining the synthesized audio for each category, we measure how increased training data or parameter count benefits    understanding ability of the text.

\vspace{-15pt}
\paragraph{2) In-context Learning Ability.}
We next evaluate the model’s zero-shot TTS capabilities—whether it can produce intelligible, high-quality speech for speakers unseen during training, and whether it can capture the appropriate emotion. This aligns with prior zero-shot TTS protocols, which typically assess how well a model generalizes to new speaker identities, timbres, and emotional expressions with no additional fine-tuning.


\vspace{-10pt} 
\subsection{Scaling Test-time Compute}
\vspace{-5pt}

Recent research has begun exploring the  scaling behavior of  LLMs during inference, showing that additional computational resources—often in the form of sophisticated search strategies—can further boost performance. Concretely, such approaches adjust the model’s output distribution at test time by generating multiple candidate outputs from a baseline model and then applying post-hoc filtering and refinement via verifiers or scoring mechanisms, elevates the quality of the generated content.

When extending this concept to  text-to-speech (TTS), we hypothesize that generating multiple speech candidates and performing a targeted search among them can yield outputs more closely matching the task requirements. In line with prior work~\cite{snell2024scaling,ma2025inference}, our search framework centers on two fundamental design choices: 
 
\vspace{-15pt}
\paragraph{Verifier Selection.}
For TTS, many off-the-shelf speech understanding models can serve as verifiers (or reward model) to evaluate synthesized audio from multiple perspectives. These include speaker verification models for measuring timbre similarity, emotion representation models for gauging emotional content, prosodic analyzers (e.g., pitch and duration) to ensure natural intonation and rhythm, speech quality metrics (e.g., SpeechMos) to evaluate naturalness, and \textbf{automatic speech recognition (ASR)} models to assess transcription accuracy. By integrating feedback from these diverse verifiers, we can control and ensure that the generated speech meets our requirements across multiple aspects, all in a fully automated process.
% \paragraph{ Algorithms.}
% We categorize 两者不同的reward方法 into:
% \begin{itemize}[leftmargin=*]
%     \item \textbf{Process Reward Models (PRMs)}: 
%     模型在生成过程中就进行评估，
%     Optimize generation at each incremental step (e.g., every second in a 10-second clip). While this fine-grained approach can yield detailed control, it risks overfitting to intermediate rewards and falling into local optima. 基于prm的搜素算法最常见的是beam search。
%     \item \textbf{Output Reward Models (ORMs)}: 
%     模型生成完整段speech后进行评估，
%     Assess the \emph{entire} speech segment holistically. ORMs typically exhibit simpler designs but may be less efficient, as they lack intermediate guidance signals. 基于orm,最常见的就是best of N
% \end{itemize}

\begin{table*}[ht]
    \centering
    \caption{Comparison between different codec models. Bold values indicate the best for each token rate. We use token rate instead of bitrate because, from the perspective of LLMs, it is more intuitive: dividing the speech context window length by the token rate directly gives the generated audio duration in seconds.}
    \label{codec}
    \tiny
    \setlength{\tabcolsep}{5pt}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
    \toprule
    Model & 
    Token Rate & 
    \makecell{\scriptsize Codebook\\ \scriptsize Size} & 
    \makecell{\scriptsize Codebook\\ \scriptsize Layer} & 
    \makecell{\scriptsize Frame\\ \scriptsize Rate} & 
    \makecell{\scriptsize WER\\ $\downarrow$} &
    \makecell{\scriptsize STOI\\ $\uparrow$} &
    \makecell{\scriptsize PESQ-\\ \scriptsize WB$\uparrow$} &
    \makecell{\scriptsize PESQ-\\ \scriptsize NB$\uparrow$} &
    \makecell{\scriptsize SPK-\\ \scriptsize SIM$\uparrow$} &
    \makecell{\scriptsize UT\\ \scriptsize MOS$\uparrow$} \\
    \midrule
    Ground Truth        
        & -      & -     & -    & -    & 1.96   & 1.00  & 4.64    & 4.55 & 1.00 & 4.09 \\ 
    \midrule
    DAC 
        & 600   & 1024  & 12    & 50   & \textbf{2.00 }  & \textbf{0.95 } & \textbf{4.01  }  & \textbf{4.15 }&\textbf{ 0.95} & \textbf{4.00 }\\ 
    Encodec     
        & 600   & 1024  & 8    & 75   & 2.15   & 0.94  & 2.77    & 3.18 & 0.89 & 3.09 \\   \hline
    Encodec & 150  & 1024  & 2    & 75   & 4.90   & 0.85  & 1.56    & 1.94 & 0.60 & 1.58 \\
    DAC & 100   & 1024  & 2    & 50   & 13.27   & 0.73  & 1.13    & 1.40 & 0.32 & 1.29 \\
    SpeechTokenizer
        & 100   & 1024  & 2    & 50   & 3.92   & 0.77  & 1.25    & 1.59 & 0.36 & 2.28 \\
    Mimi     
        & 100   & 2048  & 8    & 12.5   & 2.96   & \textbf{0.91 } &  2.25   & 2.80 &\textbf{ 0.73 }& 3.56 \\
    X-codec       
        & 100   & 1024  & 2    & 50   &\textbf{ 2.49 }  & 0.86  &\textbf{ 2.33 }   &\textbf{ 2.88} & 0.72 & \textbf{4.21} \\  \hline
    BigCodec     
        & 80    & 8192  & 1    & 80   &\textbf{ 2.76}   & \textbf{0.93 } & \textbf{2.68}    & \textbf{3.27} & \textbf{0.84} &\textbf{ 4.11} \\
    WavTokenizer
        & 75    & 4096  & 1    & 75   & 3.98   & 0.90  & 2.13    & 2.63 & 0.65 & 3.79 \\
    Mimi       
        & 75    & 2048  & 6    & 12.5   & 3.61   & 0.89  & 1.99    & 2.51 & 0.65 & 3.38 \\
    Encodec   
        & 75    & 1024  & 1    & 75   & 28.92  & 0.77  & 1.23    & 1.48 & 0.25 & 1.25 \\ \hline
    DAC   
        & 50    & 1024  & 1    & 50   & 74.55  & 0.62  & 1.06    & 1.20 & 0.08 & 1.25 \\
    SpeechTokenizer
        & 50    & 1024  & 1    & 50   & 5.01   & 0.64  & 1.14    & 1.30 & 0.17 & 1.27 \\
    Mimi
        & 50    & 2048  & 4    & 12.5   & 4.89   & 0.85  & 1.64    & 2.09 & 0.50 & 3.03 \\
    StableCodec 
        & 50    & 15625 & 2    & 25   & 5.12   & 0.91  & 2.24    & 2.91 & 0.62 & \textbf{4.23 }\\
    SemantiCodec
        & 50    & 32768/8192 & 2    & 25   & 6.89   & 0.84  & 1.66    & 2.18 & 0.58 & 2.71 \\
     
    X-codec        
        & 50    & 1024  & 1    & 50   & 3.42   & 0.83  & 1.84    & 2.38 & 0.52 & 4.05 \\  
    WavTokenizer        
        & 40    & 4096  & 1    & 40   & 11.20   & 0.85  & 1.62    & 2.06 & 0.48 &  3.57  \\
        \hline
    X-codec2 (ours) 
        & 50    & 65536 
        & 1 
        & 50 
        & \textbf{2.47} 
        &\textbf{ 0.92} 
        & \textbf{2.43} &
        \textbf{3.04} 
        & \textbf{0.82 }
        & 4.13 \\
 
    \bottomrule
    \end{tabular}
    }
\end{table*}
% In our experiments, we explore both PRMs and ORMs to analyze how different search strategies affect final TTS quality. A more detailed discussion of these methods and their outcomes is provided in the next section.
\vspace{-15pt}
\paragraph{Algorithms.}  
We categorize the two different reward methods into:  
\begin{itemize}[leftmargin=*]  
    \item \textbf{Output Reward Models (ORMs)}:  
    These models assess the speech segment only after it has been fully generated, evaluating it holistically. ORMs typically follow a simpler design but may be less efficient due to the absence of intermediate guidance signals. A common search strategy based on ORMs is the Best-of-N approach, where multiple candidate outputs are generated, scored using a reward model, and the highest-scoring output is selected. This method prioritizes overall output quality rather than guiding the generation process itself.  
    \item \textbf{Process Reward Models (PRMs)}:  
    These models evaluate the generation process step by step, optimizing at each incremental stage (e.g., every second in a 10-second clip). Unlike conventional reward models that produce a single score for the final output, PRMs provide a sequence of scores, allowing for more fine-grained feedback throughout the generation process. While this approach enables detailed control, it also increases the risk of overfitting to intermediate rewards and getting trapped in local optima. The most common search algorithm leveraging PRMs is beam search, which systematically explores the solution space while optimizing both the sampling and evaluation of intermediate steps.  
\end{itemize}  
In our experiments, we explore both PRMs and ORMs to analyze how different search strategies impact final TTS quality. A more detailed discussion of these methods and their outcomes is provided in the next experiments section.





\vspace{-15pt}
\section{Experiments}
\vspace{-5pt}
In this section, we conduct a series of experiments to comprehensively evaluate our approach. First, we compare our proposed speech tokenizer with existing codecs to assess its effectiveness. Second, we evaluate the performance of our TTS systems. We explore the effect of scaling both train-time and test-time compute and compare our models against other baseline TTS systems. Lastly, we evaluate the extensibility of our framework, particularly its applicability to speech understanding tasks, highlighting its versatility and potential for broader applications
\vspace{-10pt}
\subsection{Codec Experiments}
\vspace{-5pt}
\subsubsection{Training Details}
\vspace{-5pt}
We train our codec model on a corpus of approximately 150k hours of multilingual speech, drawn from the Emilia dataset (En/Zh/De/Fr/Ja/Ko)\cite{he2024emilia} and MLS (En/Fr/De/Nl/Es/It/Pt/Pl)\cite{Pratap2020MLSAL}. All audio is sampled at 16\,kHz. We set the total downsampling ratio \(R\) to 320, use a codebook size of 65536, and employ a projection dimension of 8 in our VQ module. During training, we randomly crop 6-second segments from the audio. The learning rate is \(1\times10^{-4}\), preceded by a 3{,}000-step warmup. In total, we train for 1.4 million steps, we activate perceptual loss at the final 0.2 million steps.
\vspace{-10pt}
\subsubsection{Evaluation}
\vspace{-5pt}
For evaluation, we use the test-clean subset of LibriSpeech~\cite{panayotov2015librispeech}, which contains 2620 utterances at 16\,kHz.  

We evaluate our system using several metrics. a HuBERT-based ASR system for Word Error Rate (WER) \footnote{https://huggingface.co/facebook/hubert-large-ls960-ft}. Short-Time Objective Intelligibility (STOI). Perceptual Evaluation of Speech Quality (PESQ). A  WavLM-based speaker verification model for speaker Similarity (SPK SIM) \footnote{https://github.com/microsoft/UniSpeech/}, and UTMOS \footnote{https://github.com/tarepan/SpeechMOS}.
 
\vspace{-10pt}
\subsubsection{Main Results}
\vspace{-5pt}
We compare our codec against multiple baselines, including DAC \cite{kumar2024high}, SpeechTokenizer \cite{zhang2024speechtokenizer}, BigCodec \cite{xin2024bigcodec}, StableCodec \cite{parker2024scaling}, SemanticCodec \cite{liu2024semanticodec},   X-codec \cite{ye2024codec}, Mimi \cite{defossez2024moshi}, EnCodec \cite{defossez2022high}, and WavTokenizer \cite{ji2024wavtokenizer}. All baseline results are obtained using their official checkpoints. 

As shown in Table \ref{codec},  X-codec2 achieves the best performance at a token rate of 50 for most metrics. Moreover, its UTMOS score closely matches that of the ground truth, indicating that the reconstructed audio faithfully preserves the original speech quality. We also observe that certain models exceed the ground truth in UTMOS when operating at low token rates. We suspect this occurs because, under limited token constraints, the decoder behaves partly as a generative model—yielding plausible speech output but the alignment with the input was less precise. Additionally, we found that metrics such as WER at  low token rate can achieve good results by integrating speech semantic information, as demonstrated by models like Mini, X-codec, and SpeechTokenizer.  Another important observation is that the acoustic reconstruction capability of codecs at low token rates remains relatively limited. For instance, DAC operating at 600 tokens achieves a speaker similarity (SPK SIM) of 0.95 and a PESQ score exceeding 4. In contrast, current codecs at lower token rates attain SPK SIM values below 0.85 and PESQ scores around 3. However, compared to earlier models like DAC and Encodec, there has been significant improvement in performance at low token rates. We believe that there is substantial potential for further advancements in low token rate codecs.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.9\textwidth]{five_models_barplot_improved_zh.png} % 调整图片宽度
%     \caption{chinese mos}
%     \label{chinesemos} % 用于引用
% \end{figure*}
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.9\textwidth]{five_models_barplot_improved.png} % 调整图片宽度
%     \caption{chinese mos}
%     \label{chinesemos} % 用于引用
% \end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{five_models_barplot_improved_zh.png} \\ % 第一张图
    \includegraphics[width=0.85\textwidth]{five_models_barplot_improved.png} % 第二张图
    \caption{Comparison of mean expert score for Chinese and English}
    \label{fig:mos_comparison}
\end{figure*}
\vspace{-10pt}
\subsection{TTS experiments}
\vspace{-5pt}
\subsubsection{Experimental deatails}
\vspace{-5pt}
All TTS models are trained for 3 epochs with a batch size of 2 million tokens and a maximum learning rate of 5e-5. We employ a cosine learning rate schedule with a warmup phase covering 3\% of an epoch.  The final learning rate is equal to 10\% of the peak learning rate.  During training, text sequences are tokenized and placed at the left, followed by tokenized speech sequences at the right, forming a sequence that is then cropped to a maximum length of 2048 tokens.  

Our TTS systems are evaluated through a series of comprehensive experiments designed to assess various aspects of performance. 

\textbf{Evaluation of Text understanding ability }
% Initially, we examined the models' text understanding capabilities, focusing on their ability to accurately comprehend and synthesize speech based on complex and nuanced textual inputs. This evaluation aimed to determine whether scaling train compute can increase text understanding ability for TTS systems.  As shown in table , 我们让模型给每个句子生成5遍，因为没有给prompt，音色还有风格等会是随机的。
% we ask a linguistic expert rates the TTS output on a
% discrete 3-point scale as mentioned in sec. 
Similar to \cite{lajszczak2024base}, we evaluated the models' text understanding capabilities by focusing on their ability to accurately comprehend and synthesize speech from complex textual inputs. This assessment aimed to evaluate   the text understanding abilities of TTS systems. The full testset is in Appendix  \ref{testset} , each sentence was synthesized five times by each model. Due to the absence of speech prompts, timbre and style were generated randomly.The evaluation criteria is also in Appendix \ref{testset}, a linguistic expert rated the TTS outputs using a discrete 3-point scale, we calculated the average score for each category.


%  To evaluate in-context learning ability for the model, 
%  我们在3个测试集上做了实验，seed-tts-eval，librispeech-test-clean，esd dataset
% 前两个主要是为了 we show ablity clone an unseen speaker voice  give a short speech clip.  克隆timbre
% First is seed-tts-eval, consist of 3 subset test-zh,test-en,test-zh-hard.
% These exp 在cross sentence mainly focus on speaker similarity and generate intelligable speech, there SIM and WER are used to measure
%  We use two test set, first is LibriSpeech [ 49 ] test-clean, a widely used test
% set for English zero-shot TTS .
% 第三个，这一个主要是克隆emotion的能力
%  ,  through the ESD emotion speech dataset. This evaluation focused on the models' proficiency in clone various emotional tones accurately given a emotion speech prompt, For esd dataset, consists of 
% 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). For exp的 reproduce, we select 每个speaker 每个emotion  最长的当target ，第二长的当test，这样总共100条，中英各50条。我们用 emotion2vec_plus_large 去测emotion sim。
% Through these evaluations, we aimed to provide a holistic assessment of our TTS models, ensuring their effectiveness in understanding, synthesizing, and expressing speech across a wide range of scenarios and linguistic challenges. 
 
\textbf{Evaluation of In-Context Learning Ability}
To assess the in-context learning capability of our model, we conducted experiments on three test sets: Seed-TTS-Eval, LibriSpeech test-clean, and the ESD dataset.  

The first two datasets primarily evaluate the model's ability to clone the voice of an unseen speaker given a short speech clip, focusing on speaker similarity. For both, speaker similarity (SIM) and Word Error Rate (WER) are used as key evaluation metrics:  

Seed-TTS-Eval \cite{anastassiou2024seed} consists of three subsets: test-zh, test-en, and test-zh-hard. These experiments focus on cross-sentence speaker similarity and the generation of intelligible speech.  
LibriSpeech Test-Clean is a widely used benchmark for English zero-shot TTS evaluation \cite{wang2023neural}, providing a standardized setting to assess the model’s ability to generate natural and intelligible speech from unseen speakers.   

The third test set, ESD (Emotional Speech Dataset)\cite{zhou2021seen,zhou2021emotional}, evaluates the model's ability to clone emotions in speech. This dataset consists of 10 native English speakers and 10 native Chinese speakers, covering five emotion categories: neutral, happy, angry, sad and surprise.

For reproducibility, we selected the longest utterance from each speaker as the prompt and the second longest as the ground truth, resulting in a total of 100 evaluation samples (50 English, 50 Chinese). We used Emotion2Vec-Plus-Large \cite{ma2023emotion2vec} \footnote{https://huggingface.co/emotion2vec/emotion2vec\_plus\_large} to measure emotional similarity.  

Through these evaluations, we aimed to provide a comprehensive assessment of our TTS models, ensuring their effectiveness in speaker identity preservation, intelligibility, and emotional expressiveness across diverse linguistic and contextual challenges.
\vspace{-10pt}
\subsubsection{Scaling Train-time Compute}
\vspace{-5pt}
Our objective in this section is to validate that, similar to text-based LLMs, increasing train-time compute improves TTS performance.  

\textbf{Text Understanding Ability}
Figure \ref{fig:mos_comparison} presents expert scores for Chinese TTS tasks, including Emotion, Paralinguistics, Poetry, Polyphonic Characters, Tongue Twisters, Questions, and Rare Characters. Increasing both model size (1B → 3B → 8B) and training data (80k → 160k → 250k hours) generally improves performance. Specifically, simpler tasks like Questions already achieve strong results, with only marginal gains from further scaling. In contrast, larger models (e.g., 8B parameters) yield significant improvements in emotional speech, Chinese poetry, and tongue twisters, where deeper semantic comprehension is essential. Meanwhile, rare characters benefit most from broader data coverage, as increasing model size alone has little effect. We also evaluate our models on English TTS tasks—Compound Nouns, Emotions, Foreign Words, Paralinguistics, Punctuations, Questions, and Syntactic Complexity. As with Chinese, scaling up both the model size (1B → 3B → 8B) and training data (80k → 160k → 250k hours) generally yields higher expert scores.  We observe that both Compound Nouns and Foreign Words primarily benefit from increased training data rather than model scaling, suggesting that a wider variety of examples is necessary for correct pronunciation and lexical coverage.  

\textbf{In-context Learning Ability.  }
Based on Table \ref{seedttseval}, Table \ref{emosim}, and Table \ref{librispeecheval}, we observe that as the model size and training data increase, the metrics for speaker similarity, word error rate, and emotional similarity consistently improve, reflecting the enhancement of in-context learning ability.


% \subsubsection{Scaling Test-time Compute}
% 在这里，我们只是做了一些简单的探索，去观察随着test time compute的增加，不同策略如prm或者orm，会对性能有什么变化。我们用llasa 1b 250k模型做全部的实验，我们首先考虑zero-shot-tts的 sim-o和wer。 baseline是直接infer的结果。 我们从如何提升sim开始，
% 我们主要用了prm，beam search这种策略，和 orm best of N另一种策略。sim的verifier模型用wavlm sim 


% 对于beam search，我们的做法是，根据 speech prompt，生成Beam Width个候选beam， beam 的process长度为M个token，我们设为25个token，在我们的实验中，也就是0.5s。
% 。 每个 beam 被拓展为N个 新候选序列，我们设N为一个固定的数在我们所有的实验中，N=16。 
% 接下来，根据sim score的排序，从所有 B × N 个候选序列中，选择得分最高的 B 个作为下一步的 beam。重复拓展和筛选，直到生成结束标记或达到最大长度 2048。

% 对于 best of N，做法是Generate 
%  independent responses and select the one with the highest RM reward as the final answer. 在下面的图中为了 为了compute budget和beam search对齐，我们生成的candidate数量是B × N 个。

% As shown in fig \ref{scaletest}，
% 首先，最简单的方法我们采取 best of N去提升 sim，也就是说生成多个candidate，直接用spk sim模型去选分最高的那个，如图橙色线所示，我们发现随着test time 增加，sim显著增加。
% 其次，我们考虑prm beam search，如蓝色线所示，我们发现同样compute budget的sim结果明显好于orm的sim。
% 接下来，我们考虑能否同时提升wer，我们wer用whisper large v3当verifier，通过best of N的方式选wer。
% 我们在prm生成到最后一步的时候， B × N 个候选序列中 ，选取最好的wer，结果如红线所示，发现wer的效果依然很差，尤其是beam width大的情况下，wer差于baseline，这说明prm sim指标的时候，使得生成的speeh陷入了局部最优，没有了多样性，使得选不出好wer的speech。
% 所以，为了solve这个，我们提出了一直partial prm sim的策略，也就是说仅在前n秒做prm，然后剩余的时间做orm，我们n设了2秒在我们的实验中
% 直接上来讲，这相当于orm和prm间找了一个trade off，从而避免完全的prm陷入局部最优，结果如绿线所示，我们发现sim比best of N高，而且wer上页和gt差不多，这说明生成的speech有足够的多样性。
% 在此基础上，我们把后面的那一步orm换成了wer作为verifier，结果如紫线所示。我们可以看出随着test time 增加，both sim 和wer的效果都实现了提升。



\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comparison_plot.png}
        
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comparison_plot2.png}
  
    \end{subfigure}
    \caption{Illustration of test-time scaling}
    \label{scaletest}
\end{figure*}

\vspace{-10pt}
\subsubsection{Scaling Test-time Compute}
\vspace{-5pt}
In this section, we conduct a series of experiments to explore how increasing test-time compute affects the performance of different strategies, such as PRM and ORM. All experiments are conducted using the LLASA~1B~250K model and evaluated on seed-tts-eval zh-hard testset.  We begin by evaluating zero-shot TTS performance using \emph{SIM-O} and \emph{WER} metrics, with direct inference serving as the baseline. Our initial goal is to improve \emph{SIM}, utilizing two key strategies: \emph{PRM} with beam search and \emph{ORM} with best-of-\(N\). The similarity verifier is a WavLM-finetuned speaker verification model.

For beam search, we generate \(B\) candidate beams conditioned on the speech prompt, where each beam is expanded by \(M\) tokens per step. We set \(M=25\), corresponding to \(0.5\)~seconds in our experiments. Each beam then expands into \(N=16\) new candidate sequences. From the resulting pool of \(B\times N\) sequences, we select the top~\(B\) based on their similarity scores. This process repeats until an end-of-sequence (EOS) token is generated or the sequence length reaches~\(2048\).

For best-of-\(N\), we generate multiple independent responses and select the one with the highest similarity score as the final output. To match the compute budget of beam search, we also produce \(B\times N\) candidates.

As shown in Figure~\ref{scaletest}, our simplest method for improving SIM is best-of-\(N\). The \textcolor{orange}{orange line} indicates that as test-time compute grows, SIM improves markedly. Next, the \textcolor{blue}{blue line} shows that PRM beam search outperforms ORM under the same compute budget.

However, when we also aim to optimize WER ( using Whisper~Large~v3 as a verifier), we select the lowest-WER candidate at the final PRM step from the \(B\times N\) sequences. The \textcolor{red}{red line} reveals that WER remains poor, especially for larger beam widths, sometimes lagging behind the \textbf{baseline}. We suspect that maximizing SIM through PRM leads to locally optimal speech with inadequate diversity.

To address this, we propose a partial PRM strategy, applying PRM only during the first \(n\)~seconds, then switching to ORM. Here, \(n=2\) in our experiments. This hybrid approach (the \textcolor{green}{green line}) achieves higher SIM than best-of-\(N\) while maintaining WER near ground truth, indicating sufficient diversity. Finally, substituting the later ORM step with a WER-based verifier (the \textcolor{purple}{purple line}) simultaneously boosts both SIM and WER as test-time compute increases, demonstrating that this mixed strategy strikes an effective balance between speaker similarity and transcription accuracy.

% 更宏观的，我们还比较了
% 从另外一个角度，我们还比较不同模型scale下，不同testset下，test-time scaling对性能的影响。 
% 在这些表格中，我们固定了 beam width 是16. 首先，test time scaling 对不光spekaer sim work也对 emotion sim work。
% As shown in table \ref{librispeecheval}，\ref{emosim}
% and \ref{seedttseval}, 总的来说，大模型的test-time scaling通常有着更好的效果， 但是对于一些不难的任务的指标而言，大模型和小模型的test time scaling后的结果相差不大，这也似乎预示着一种可能，与text llm类似，This finding suggests that rather than focusing purely on scaling training, in some settings it is more efficient to train smaller models with less compute, and then apply test-time compute to improve outputs. 
From another perspective, we also compare the impact of test-time scaling across different model sizes and test sets.  

In these evaluations, we fix the beam width at 16. s shown in Tables \ref{librispeecheval}, \ref{emosim}, and \ref{seedttseval}, our results show that test-time scaling not only improves speaker similarity but also enhances emotion similarity.  
Additionally, we generally observe that larger models benefit more from test-time scaling. However, for certain relatively simple tasks and metrics, the performance gap between large and small models after test-time scaling remains minimal.  

This finding suggests a possible parallel with text-based LLMs: rather than solely focusing on scaling model training, in some settings, a more efficient approach  may be to train smaller models with less compute  and then leverage  test-time compute  to enhance outputs.  

\begin{table*}[ht]
\centering
\caption{Results of llasa and recent TTS models on the SEED test sets. † denotes close-sourced models.  For llasa series, sim-o values include sim-r in parentheses.}
\small
\label{tab:results}
\begin{tabular}{l|cc|cc|cc}
\hline
\textbf{Model} & \multicolumn{2}{c|}{\textbf{test-zh}} & \multicolumn{2}{c|}{\textbf{test-en}} & \multicolumn{2}{c}{\textbf{test-hard}} \\
                & \textbf{CER ↓} & \textbf{sim-o ↑} & \textbf{WER ↓} & \textbf{sim-o ↑} & \textbf{WER ↓} & \textbf{sim-o ↑} \\ 
\hline
Human               & 1.26 & 0.755 & 2.14 & 0.734 & -    & -       \\
Our Codec Resyn.    & 1.92 & 0.677 & 2.91 & 0.619 & -    & -       \\ \hline
Seed-TTS $^\dagger$            & 1.12 & 0.796 & 2.25 & 0.762 & 7.59 & 0.776  \\
FireRedTTS          & 1.51 & 0.635 & 3.82 & 0.460 & 17.45& 0.621  \\
MaskGCT             & 2.27 & 0.774 & 2.62 & 0.714 & 10.27& 0.748  \\
E2 TTS (32 NFE)$^\dagger$ & 1.97 & 0.730 & 2.19 & 0.710 & -    & -       \\
F5-TTS (32 NFE)     & 1.56 & 0.741 & 1.83 & 0.647 & 8.67 & 0.713  \\  
CosyVoice           & 3.63 & 0.723 & 4.29 & 0.609 & 11.75& 0.709  \\
CosyVoice 2         & 1.45 & 0.748 & 2.57 & 0.652 & 6.83 & 0.724  \\ \hline
% CosyVoice 2-S     & 1.45 & 0.753 & 2.38 & 0.654 & 8.08 & 0.732  \\ \hline

\multicolumn{7}{c}{\textbf{ Train-time Scaling}} \\ \hline
llasa 1b 80k        & 2.69 & 0.648 (0.779) & 3.71 & 0.541 (0.685) & 17.11 & 0.618 (0.765) \\
llasa 1b 160k       & 2.22 & 0.658 (0.783) & 3.60 & 0.563 (0.701) & 16.73 & 0.627 (0.770) \\
llasa 1b 250k       & 1.89 & 0.669 (0.794) & 3.22 & 0.572 (0.708) & 12.13 & 0.638 (0.779) \\
llasa 3b 250k       & 1.60 & 0.675 (0.792) & 3.14 & 0.579 (0.708) & 13.37 & 0.652 (0.782) \\
llasa 8b 250k       & 1.59 & 0.684 (0.798) & 2.97 & 0.574 (0.706) & 11.09 & 0.660 (0.787) \\ \hline
% \multicolumn{7}{c}{\textbf{Test-time Scaling}} \\ \hline
\multicolumn{7}{c}{\textbf{Partial PRM (spk sim)}} \\ \hline
llasa 1b 80k        & 1.52 & 0.811 (0.849) & 2.30 & 0.761 (0.798) & 16.09 & 0.759 (0.824) \\
llasa 1b 160k       & 1.29 & 0.815 (0.851) & 2.29 & 0.774 (0.804) & 14.10 & 0.768 (0.830) \\
llasa 1b 250k       & 1.11 & 0.818 (0.855) & 2.03 & 0.781 (0.809) & 11.30 & 0.773 (0.833) \\
llasa 3b 250k       & 1.06 & 0.824 (0.856) & 1.89 & \textbf{0.784} (0.812) & 11.22 & 0.780 (0.836) \\
llasa 8b 250k       & 1.04 & \textbf{0.827} (0.856) & 1.84 & 0.783 (0.806) & 10.59 & \textbf{0.785} (0.839) \\ \hline
\multicolumn{7}{c}{\textbf{Partial PRM (spk sim)+ORM (WER)}} \\ \hline
llasa 1b 80k        & 0.53 & 0.809 (0.840) & 1.43 & 0.761 (0.792) & 7.22 & 0.732 (0.789) \\
llasa 1b 160k       & 0.53 & 0.812 (0.841) & 1.49 & 0.775 (0.798) & 6.35 & 0.745 (0.799) \\
llasa 1b 250k       & \textbf{0.45} & 0.818 (0.845) & 1.46 & 0.782 (0.801) & 5.24 & 0.750 (0.803) \\
llasa 3b 250k       & 0.50 & 0.823 (0.848) & \textbf{1.31} & 0.783 (0.803) & 5.39 & 0.759 (0.808) \\
llasa 8b 250k       & 0.47 & 0.825 (0.848) & 1.39 & 0.783 (0.799) & \textbf{4.38} & 0.767 (0.812) \\ \hline

llasa 8b 250k       & \multicolumn{4}{c|}{{Chunking: if $\text{len(char)} > 100 \rightarrow 2$ chunks, $> 200 \rightarrow 3$ chunks, $\dots$}} &  \textbf{3.12} & 0.770 (0.791) \\ \hline

\end{tabular}
\label{seedttseval} 
\end{table*}


\vspace{-10pt}
\subsubsection{Compare with baseline model}
\vspace{-5pt}
% 前面的章节，我们比较了我们的codec，探索了scaling train-time test-time compute 对tts系统性能的影响，这一部分，我们直接和其他tts进行比较。
% 首先，在seed-tts-eval上选了这些baseline
% Seed-TTS \cite{anastassiou2024seed}
% MaskGCT \cite{wang2024maskgct}  
% E2 TTS (32 NFE)$^\dagger$ \cite{eskimez2024e2}
% F5-TTS (32 NFE) \cite{chen2024f5} 
% CosyVoice \cite{du2024cosyvoice}
% CosyVoice 2   \cite{du2024cosyvoice2} 
% FireRedTTS \cite{guo2024fireredtts}

% 比较结果，as shown in \ref{seedttseval}
% 我们发现，对于直接infer，我们的模型在wer上，与现在的这些sota差不多，但是，我们有一个短板，那就是sim-o，正如前面分析的，单token的codec的重建能力还是比较有限，相比于baseline的用mel和vocoder重建或者用rvq codec重建。
% 对于 librispeech test clean，我们选了这些baseline ELLA-V \cite{song2024ella}
% VALL-E R \cite{han2024vall}
% CLaM-TTS \cite{kimclam}
% VALL-E \cite{wang2023neural}
% VALL-E 2 \cite{chen2024vall}
% Voicebox \cite{le2024voicebox}
% MELLE  \cite{meng2024autoregressive}
% 相似的结果在
%  librispeech test clean 上continue的 \ref{librispeecheval} 也是一样的。但是我们发现我们的sim-r很高，尤其是在librispeech speech test clean 我们发现我们最好的sim r 0.626 已经和gt codec resyn 非常close，并且since the continue task is fully aligh with autoregressive training的模式，这说明从generative 模型的角度来看，single transformer的架构比精心设计的ar+nar架构并没有劣势。 唯一的劣势，可能是从整个系统层面上，最后一步的acoustic 重建回waveform from 中间表征这一步有缺点。
 

% 从test-time scaling 的角度，我们的效果超过了所有baseline，当然，这可能不够fair，因为我们inference用了更多的计算量，但换一个角度，如果我们只是为了追求极致的效果，不在乎投入更多的计算量去生成满足的内容， test-time scaling是一个可以做到这点的方法。尤其是我们看seed-tts-eval的hard的wer到了3.12，这表明投入更多的计算量在合成困难speech上优势明显，可以解决之前模型很难去解决的问题。
In the previous sections, we analyzed our codec and explored the impact of scaling train-time and test-time compute on the performance of TTS systems. In this section, we directly compare our model against other TTS baselines.

For Seed-TTS-Eval, we select the following baseline models: Seed-TTS \cite{anastassiou2024seed}, MaskGCT \cite{wang2024maskgct}, E2-TTS (32 NFE)$^\dagger$ \cite{eskimez2024e2}, F5-TTS (32 NFE) \cite{chen2024f5}, CosyVoice \cite{du2024cosyvoice}, CosyVoice 2 \cite{du2024cosyvoice2}, and FireRedTTS \cite{guo2024fireredtts}. 

The results, as shown in Table \ref{seedttseval}, indicate that for direct inference, our model achieves WER performance comparable to these state-of-the-art (SOTA) models. However, one notable limitation of our approach is SIM-O. As discussed earlier, the reconstruction capability of single-token codecs remains constrained compared to mel-based vocoder reconstructions or RVQ codec-based reconstructions used in the baselines.

For LibriSpeech test-clean, we compare against the following baselines: ELLA-V \cite{song2024ella}, VALL-E R \cite{han2024vall}, CLaM-TTS \cite{kimclam}, VALL-E \cite{wang2023neural}, VALL-E 2 \cite{chen2024vall}, Voicebox \cite{le2024voicebox}, and MELLE \cite{meng2024autoregressive}. 

As shown in Table \ref{librispeecheval}, we observe similar trends in continuous TTS for LibriSpeech test-clean. However, an interesting finding is that our model achieves a high SIM-R score, particularly on LibriSpeech test-clean, where our best SIM-R (0.626) is already very close to the ground truth (GT) codec resynthesis. Given that the continuous task is fully aligned with the autoregressive training paradigm, this suggests that, from a generative modeling perspective, a single Transformer-based architecture does not inherently suffer disadvantages compared to carefully designed AR+NAR hybrid architectures. The only potential drawback may arise at the system level, particularly in the final step of acoustic reconstruction, where converting the intermediate representation back into a waveform may introduce limitations.

From a test-time scaling perspective, our approach outperforms all baselines. While this comparison might not be entirely fair—as our inference process utilizes greater computational resources—it presents an alternative viewpoint: if the goal is to achieve the best possible quality, disregarding computational constraints, test-time scaling provides a viable solution. Notably, our model achieves a WER of 3.12 on the hard subset of Seed-TTS-Eval, demonstrating that allocating more compute at inference time is particularly beneficial for synthesizing challenging speech, effectively addressing cases where previous models have struggled.

% 这时候说 Meanwhile, since the continue task is fully aligh with autoregressive training， 我们发现我们最好的sim r 0.626 已经和gt codec resyn 非常close，这说明从
% \subsection{Emotion sim exp}

% For esd dataset, consists of 
% 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). For exp的 reproduce, we select 每个speaker 每个emotion  最长的当target ，第二长的当test，这样总共100条，中英各50条。



% \subsection{Extenting to speech understanding tasks}
% % \subsection{ASR Results}
% % 在前面，我们探索了这一框架对于tts的可行性，我们还希望探索一下 这一套 transformer+tokenizer对speech理解任务的效果，我们在asr 任务上做了测试，用的相同的tokenizer，训练过程的唯一区别就是交换了speech 和text的位置，也就是说把speech token放到了前面，text token，训练的时候只计算text token的ce loss。 我们用了libriheay MLS english gigaspeech 提取token去训练asr模型，学习率2e-5，batch size 1m token， warmup 0.03， 训了2 epoch，结果如下。我们在librispeech test clean 和 other上测试。发现效果还不错，尤其是test clean 上效果已经接近了whisper large v3，但是它用百万小时级别的数据，我们发现other效果比较差，我们猜测这是因为我们的训练数据比较有限，而且clean，我们也没做数据增强。但我们这个实验使得我们相信，即使完全离散的speech token的asr范式也可以取得不错的效果，相比于主流的whisper这种在连续特征mel上做的范式。
% Below is a concise, polished version that captures all key points from the original text:

% ---
\vspace{-10pt}
\subsection{Extending to Speech Understanding Tasks}
\vspace{-5pt}
While we have primarily demonstrated the viability of our single Transformer + tokenizer approach for TTS, we also explored its performance on speech understanding—in particular, ASR. The only modification is to swap the position of speech and text tokens: speech tokens come first, followed by text tokens, and during training we apply the cross-entropy loss solely to the text tokens. We use the same tokenizer and ASR models are trained on LibriLight, MLS English, and GigaSpeech under a learning rate of \(2\times10^{-5}\), a batch size of 1M tokens, a 0.03 warmup ratio, and 2 training epochs. Table~\ref{tab:asr_results} presents ASR results on LibriSpeech. Notably, on the test-clean set, our model is competitive with Whisper Large v3. Performance on test-other, however, remains weaker—likely due to our smaller, relatively clean training set and the lack of data augmentation. Despite these limitations, our experiments confirm that an entirely discrete ASR paradigm operating on quantized speech tokens can still achieve promising results, comparable in many respects to mainstream approaches like Whisper, which relies on continuous Mel features.

\begin{table}[ht]
    \centering
    \caption{ASR Performance on LibriSpeech Test Sets}
    \small
    
    \label{tab:asr_results}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Test Clean  } & \textbf{Test Other } \\
        \midrule
        whisper large v3 & 1.8 & 3.6 \\
        whisper large v2 & 2.7 & 5.2 \\ \hline
        llasa asr 1b & 2.3 & 7.2 \\
        llasa asr 3b & 1.9 & 5.9 \\
        \bottomrule
    \end{tabular}
\end{table}

\vspace{-15pt}
\section{Conclusion}
    

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}
\newpage
\newpage
\newpage
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{esd}

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Model & en & zh \\
        \midrule
         GT  & 0.94  & 0.94  \\
        \midrule
        \multicolumn{3}{c}{\textbf{Train-time scaling}} \\
        \midrule
        llasa 1b (80k)  & 0.753  & 0.815  \\
        llasa 1b (160k) & 0.762  & 0.822  \\
        llasa 1b (250k) & 0.768  & 0.836  \\
        llasa 3b (250k) & 0.769  & 0.852  \\
        llasa 8b (250k) & 0.778  & 0.861  \\
        \midrule
        \multicolumn{3}{c}{\textbf{Process Reward Models (emotion sim)}} \\
        \midrule
        llasa 1b (80k)  & 0.933  & 0.970 \\
        llasa 1b (160k) & 0.936  & 0.971 \\
        llasa 1b (250k) & 0.937  & 0.974 \\
        llasa 3b (250k) & 0.949  & 0.975 \\
        llasa 8b (250k) & 0.951  & 0.974 \\
        \bottomrule
    \end{tabular}
    \caption{ en, zh  Emotion Similarity}
    \label{emosim}
\end{table}
\section{librispeech test clean evaluation}
 

\begin{table}[t]
\centering
\caption{Objective performance comparison on \textit{continuation} zero-shot speech synthesis tasks. WER-H (\%) denotes evaluation with the HuBERT-Large ASR model. The \textbf{boldface} indicates the best result, and the \underline{underline} denotes the second best. *We quote Han et al. [2024]'s reproduction results, which demonstrate better performance. $^\dagger$We evaluate metrics not reported in the original paper, using the audios provided by the authors.}
\begin{tabular}{l|ccc}
\hline
\multirow{2}{*}{System} & \multicolumn{3}{c}{Continuation} \\
& WER-H & SIM-o & SIM-r \\
\hline
Ground Truth & 2.15 & 0.668 & - \\
Our Codec  Resyn. & 2.49 &  0.580 & 0.638 \\
\hline
ELLA-V  * & 2.91 & 0.303 & 0.340 \\
VALL-E R  $^\dagger$ & 2.32 & 0.363 & 0.397 \\
 
CLaM-TTS   & 2.36 & 0.477 & 0.513 \\
VALL-E   & 3.8 & - & 0.508 \\
VALL-E 2  $^\dagger$ & 2.32 & 0.504 & 0.529 \\
Voicebox   & 2.0 & 0.593 &  0.616 \\

MELLE & 1.98 & 0.508 & 0.539 \\
\hline
 
\multicolumn{3}{c}{\textbf{ Train-time Scaling}} \\ \hline

LLaSA-TTS 1b 80k  & 2.57 & 0.457  & 0.614\\
LLaSA-TTS 1b 160k & 2.48 & 0.475 & 0.625 \\
LLaSA-TTS 1b 250k  & 2.47 & 0.478  & 0.627 \\
LLaSA-TTS 3b 250k & 2.35  & 0.484 & 0.628 \\
LLaSA-TTS 8b 250k &  2.29 & 0.483 & 0.626 \\
\hline
\multicolumn{3}{c}{\textbf{PRM (spk sim)}} \\ \hline
LLaSA-TTS-80k 1b  & 2.43 & 0.699  & 0.738 \\
LLaSA-TTS-160k 1b  & 2.36 & 0.712  & 0.744 \\
LLaSA-TTS 1b 250k  & 2.37 & 0.712  & 0.743 \\
LLaSA-TTS 3b  250k& 2.26  & 0.715 & 0.745 \\
LLaSA-TTS 8b  250k& 2.24  & 0.714 & 0.741 \\
\hline
\multicolumn{3}{c}{\textbf{PRM (spk sim)+ORMs (WER)}} \\ \hline
LLaSA-TTS-80k 1b  & 1.76 & 0.700   & 0.738 \\
LLaSA-TTS-160k 1b  & 1.66 & 0.710  & 0.743 \\
LLaSA-TTS 1b 250k  &1.62 & 0.712 & 0.744\\
LLaSA-TTS 3b 250k & 1.57 & 0.714 & 0.742\\
LLaSA-TTS 8b 250k &  1.49 & 0.714 &  0.740 \\
\hline
\end{tabular}
 
\label{librispeecheval} 
\end{table}
\section{Test set}
\label{testset}
\subsection{}


\subsection{Evaluation criteria}
 
\begin{CJK*}{UTF8}{gbsn}
 
\begin{table}[ht]
\centering
\caption{Evaluation Criteria for Chinese Test Set}
\label{table:chinese-evaluation-criteria}
\small % 缩小字体
\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}m{2.5cm}|>{\arraybackslash}X|>{\arraybackslash}X|>{\arraybackslash}X|}
\hline
\textbf{Category} & \textbf{1} & \textbf{2} & \textbf{3} \\
\hline
\textbf{Emotion} & 
No detectable emotion / 无可检测的情感 & 
Emotion present but not convincingly rendered / 存在情感但表达不够令人信服 & 
Correct emotion recognition and appropriate rendering / 正确识别情感并恰当表达 \\
\hline

\textbf{Paralinguistic} & 
No recognition of paralinguistic cues like interjections / 未识别出语调学关键词，如“哎呀”或“嘘” & 
Attempts to render paralinguistic cues but unnatural / 明确意图表达关键词，但表达不自然 & 
Natural rendering of paralinguistic cues with appropriate emphasis / 自然表达语调学关键词，恰当强调 \\
\hline

\textbf{Chinese Poetry} & 
Fails to capture the poetic structure and imagery / 未能捕捉诗歌的结构和意象 & 
Captures some poetic elements but lacks depth / 捕捉了一些诗歌元素但缺乏深度 & 
Accurately captures the poetic structure, imagery, and emotional depth / 准确捕捉诗歌的结构、意象和情感深度 \\
\hline

\textbf{Polyphonic Characters} & 
Incorrect pronunciation and meaning of polyphonic characters / 多音字发音错误，意义不正确 & 
Attempts correct pronunciation but with minor errors / 尝试正确发音但有小错误 & 
Correct pronunciation and meaning of polyphonic characters / 多音字发音和意义正确 \\
\hline

\textbf{Questions} & 
Intonation pattern incorrect, failing to convey questioning tone / 语调模式不正确，未能传达问句的语气 & 
Intonation pattern largely correct but with minor flaws / 语调模式大体正确，但有细微瑕疵 & 
Correct intonation patterns that clearly convey the questioning nature / 语调模式正确，清晰传达问句的性质 \\
\hline

\textbf{Tongue Twisters} & 
Inability to articulate the tongue twister, resulting in errors / 无法清晰表达绕口令，导致错误 & 
Attempts articulation with some errors / 尝试表达绕口令但有部分错误 & 
Clear and accurate articulation of the tongue twister without errors / 清晰准确地表达绕口令，无错误 \\
\hline

\textbf{Rare Characters} & 
Mispronunciation or incorrect interpretation of rare characters / 生僻字发音错误或解释不正确 & 
Attempts correct pronunciation and interpretation with minor mistakes / 尝试正确发音和解释但有小错误 & 
Accurate pronunciation and insightful interpretation of rare characters / 生僻字发音和解释准确 \\
\hline
\end{tabularx}
\end{table}
 

\subsection{Emotion}
她激动地喊道：“我做到了！真的做到了！”

他愤怒地说：“你再这样，我就受不了了！”

她悲伤地低声哭泣：“为什么会这样？”

他欣喜地笑着说：“这真是太棒了！”

她紧张地结结巴巴：“我不知道该怎么办。”

他轻松地说道：“没关系，我们可以解决的。”

她失望地叹了口气：“我本以为会更好。”

他兴奋地跳了起来：“我们赢了！”

她害怕地颤抖：“不要靠近我！”

他感激地说：“谢谢你，你帮了我大忙。”

她羞涩地笑了笑：“我不敢相信。”

他绝望地喊道：“一切都结束了吗？”

她自豪地说：“这是我的成就。”

他焦虑地问：“我们还能挽回吗？”

她愉快地唱起歌来：“今天真是美好的一天！”

他疲惫地叹息：“我需要休息。”

她兴奋地说道：“快看，那里有烟花！”

他冷静地回答：“我们需要保持镇定。”

她惊讶地说：“这是真的吗？”

他满足地微笑：“这一切都很值得。”

\subsection{Paralinguistic }
哎呀，这雨下得噼里啪啦的，看来今天的郊游计划又泡汤喽！

哇塞，烟花在夜空中嗖地一声炸开，瞬间绽放出五彩斑斓的光芒，好美呀！

哼，他总是大大咧咧的，走路都咚咚咚地响，一点都不安静呢！

嘿哟，这箱子可真沉啊，我费了好大劲才吭哧吭哧地搬起来。

咦，这只小猫怎么一直喵喵喵地叫呀，是不是饿了呢？

哟呵，你看那只小狗，摇着尾巴汪汪汪地跑过来，好可爱呀！

唉，那我就有点好奇了。

哇，厨房里传来咕噜咕噜的声音，肯定是妈妈煮的汤快好了，好香啊！

哈哈，小朋友们在操场上嘻嘻哈哈地玩耍，笑声一阵接着一阵呢！

哇哦，海浪拍打着沙滩，发出哗哗的声音，好像在演奏一首美妙的乐章呢！

哎呦，这蚊子嗡嗡嗡地在耳边飞来飞去，烦死我了！

哎呀妈呀，这只大鹅伸长了脖子嘎嘎嘎地叫着，气势汹汹地朝我冲过来。 

“嗯，我觉得这主意不错，”她迟疑地说。

“嘘，小声点，我们不能被发现，”他低语道。

“额，我不知道该怎么说，”她尴尬地回应。

“哦，没关系，我可以处理，”她自信地说。

“啊，天哪！”她惊讶地喊道。

“呃，好吧，那我们开始吧，”她决定性地说。

“嘘，露西，嘘……别吵醒你弟弟。”汤姆小声叮嘱，两人轻手轻脚地走过婴儿房。

“啊，忘了带钥匙了，”她懊恼地说。


 
\subsection{Chinese Poetry}
床前明月光，疑是地上霜。 举头望明月，低头思故乡。

恰同学少年，风华正茂；书生意气，挥斥方遒。指点江山，激扬文字，粪土当年万户侯。曾记否，到中流击水，浪遏飞舟？

人生易老天难老，岁岁重阳。今又重阳，战地黄花分外香。

雄关漫道真如铁，而今迈步从头越。从头越，苍山如海，残阳如血。

红军不怕远征难，万水千山只等闲。

踏遍青山人未老，风景这边独好。

江山如此多娇，引无数英雄竞折腰。惜秦皇汉武，略输文采；唐宗宋祖，稍逊风骚。一代天骄，成吉思汗，只识弯弓射大雕。俱往矣，数风流人物，还看今朝。

天若有情天亦老，人间正道是沧桑。

才饮长沙水，又食武昌鱼。万里长江横渡，极目楚天舒。

风雨送春归，飞雪迎春到。已是悬崖百丈冰，犹有花枝俏。俏也不争春，只把春来报。待到山花烂漫时，她在丛中笑。

朱雀桥边野草花，乌衣巷口夕阳斜。旧时王谢堂前燕，飞入寻常百姓家。

劝君莫惜金缕衣，劝君惜取少年时。花开堪折直须折，莫待无花空折枝。

红豆生南国，春来发几枝。愿君多采撷，此物最相思。

绿蚁新醅酒，红泥小火炉。晚来天欲雪，能饮一杯无？

生当作人杰，死亦为鬼雄。至今思项羽，不肯过江东。

遥想公瑾当年，小乔初嫁了，雄姿英发。羽扇纶巾，谈笑间樯橹灰飞烟灭。故国神游，多情应笑我，早生华发。人生如梦，一尊还酹江月。

多情自古伤离别，更那堪，冷落清秋节。今宵酒醒何处？杨柳岸，晓风残月。此去经年，应是良辰好景虚设。便纵有千种风情，更与何人说？

红藕香残玉簟秋，轻解罗裳，独上兰舟。云中谁寄锦书来？雁字回时，月满西楼。 花自飘零水自流，一种相思，两处闲愁。此情无计可消除，才下眉头，却上心头。
昨夜雨疏风骤，浓睡不消残酒。试问卷帘人，却道海棠依旧。知否，知否？应是绿肥红瘦。

帘外雨潺潺，春意阑珊。罗衾不耐五更寒。梦里不知身是客，一晌贪欢。独自莫凭栏，无限江山。别时容易见时难。流水落花春去也，天上人间。

\subsection{Polyphonic Characters}
这种弹弓的弹力很强。

人参苗长得参差不齐，还能让人参观吗？

下午要召开工作会议，你去通知一下张会计。

这几张茶几，几乎都要散架了。

这里有很多畜牧场，养殖了很多牲畜。

人要是行，干一行行一行，一行行行行行，行行行干哪行都行，要是不行，干一行不行一行，一行不行行行不行，行行不行，干哪行都不行。

他在长跑比赛中行动迅速，长期的训练让他在行动中表现出色，重视每一次长跑的行动细节。

他在行业会议上行动自如，行走于各类行动之间，行使政策得以行动实施，令在场行业内人士纷纷称赞他的行动能力。

他得到了一份得力的行政支持，得亏了他，得以顺利行动。

她乐于助人，有着乐观的工作态度，在乐队中乐声悠扬，乐迷们对她的乐器演奏赞不绝口。

他着急地着手准备，着实需要更多时间完成任务，着迷于工作的细节。

在好莱坞的片场，她很好学，也演好了每一个角色，赢得了观众的好评。

她干劲十足地干了所有使库房干燥的工作。

穿上便服，就可以买到便宜的商品，也可也搭乘便车。

这几天天天天气不好。

来到杨过曾经生活过的地方，小龙女动情的说：“我也想过过过儿过过的生活。”

我有一个小本本本来很干净。

今天下雨，我骑车差点摔倒，好在我一把把把把住了。

校长说校服上除了校徽别别别的，让你们别别别的你非别别的。

你去班上数数数数数不好的有多少。
\subsection{Questions}
今天的会议通知你收到了吗？咱们几点在哪个会议室集合呀？

这部新上映的电影口碑据说很不错，你打算去看吗？看完觉得怎么样？

你最近工作那么忙，有时间好好休息吗？身体吃得消吗？

周末咱们一起去郊外野餐怎么样？你有空吗？

你知道明天的天气预报吗？会不会下雨呀？

你在大学学的是什么专业？毕业后从事的工作和专业对口吗？

这道数学题我怎么都解不出来，你会做吗？能教教我吗？

你喜欢什么类型的音乐？是流行、摇滚还是古典呢？

你去过国外旅游吗？最喜欢哪个国家？为什么？

你觉得我们这次的项目计划可行吗？还有哪些地方需要改进？

难道我们遇到一点困难就应该退缩吗？这可不是我们一贯的作风！

父母含辛茹苦把我们养大，我们难道不应该好好孝顺他们吗？

浪费粮食这种行为，难道不应该受到谴责吗？

保护环境是每个人的责任，难道我们可以置之不理吗？

他为了集体利益付出了那么多，我们难道不应该感激他吗？

老师每天辛苦备课、批改作业，我们难道不应该尊重他们的劳动成果吗？

机会摆在面前，我们难道要眼睁睁地看着它溜走吗？

努力学习才能有更好的未来，难道这一点还需要怀疑吗？

大家都在为了目标拼搏奋斗，我们难道能偷懒懈怠吗？

诚实守信是做人的基本准则，难道我们可以随意违背吗？


\subsection{Tongue Twisters}
老龙恼怒闹老农，老农恼怒闹老龙。农怒龙恼农更怒，龙恼农怒龙怕农。
四是四，十是十，十四是十四，四十是四十。莫把四字说成十，休将十字说成四。若要分清四十和十四，经常练说十和四。

石狮寺前有四十四个石狮子，寺前树上结了四十四个涩柿子，四十四个石狮子不吃四十四个涩柿子，四十四个涩柿子倒吃四十四个石狮子。

粉红墙上画凤凰，凤凰画在粉红墙。红凤凰、粉凤凰，红粉凤凰、花凤凰。

哥哥挎筐过宽沟，快过宽沟看怪狗，光看怪狗瓜筐扣，瓜滚筐空哥怪狗。

坡上立着一只鹅，坡下就是一条河。宽宽的河，肥肥的鹅，鹅要过河，河要渡鹅不知是鹅过河，还是河渡鹅？

三哥三嫂子，借给我三斗三升酸枣子。等我上山摘了三升三斗酸枣子，再奉还三哥三嫂子这三斗三升酸枣子。

墙上一个窗，窗上一支枪，窗下一箩糠。枪落进了糠，糠埋住了枪。窗要糠让枪，糠要枪上墙，墙要枪上窗。互相不退让，糠赶不走枪，枪也上不了窗和墙。

蓝教练是女教练，吕教练是男教练，蓝教练不是男教练，吕教练不是女教练。蓝南是男篮主力，吕楠是女篮主力，吕教练在男篮训练蓝南，蓝教练在女篮训练吕楠。

任命是任命，人名是人名，任命人名不能错，错了人名错任命。

小华和胖娃，一同种庄稼。小华种棉花，胖娃种西瓜。小华的棉花开了花，胖娃的西瓜结了瓜。小华摘棉花，胖娃摘西瓜。棉花、西瓜装一塌，小华、胖娃笑哈哈。

黑豆放在黑斗里，黑斗里边放黑豆，黑豆放黑斗，黑斗放黑豆，不知黑豆放黑斗，还是黑斗放黑豆。

石小四和史肖石，一同来到阅览室。石小四年十四，史肖石年四十。年十四的石小四爱看诗词，年四十的史肖石爱看报纸。年四十的史肖石发现了好诗词，忙递给年十四的石小四，年十四的石小四见了好报纸，忙递给年四十的史肖石。

天上七颗星，地上七块冰，台上七盏灯，树上七只莺，墙上七枚钉。吭唷吭唷拔脱七枚钉。喔嘘喔嘘赶走七只莺。乒乒乓乓踏坏七块冰。一阵风来吹来七盏灯。一片乌云遮掉七颗星。

白老八门前栽了八颗白果树，从北边飞来了八个白八哥儿不知在哪住。白老八拿了八个巴达棍儿要打八个白八哥儿，八个白八哥儿飞上了八颗白果树，不知道白老八拿这八个巴达棍儿打着了八个白八哥儿，还是打着了八颗白果树。

针蓝线蓝领子蓝，蓝针蓝线蓝领蓝。蓝针蓝线连蓝领，针蓝线蓝领子蓝。

白猫满白毛房里一白猫，白猫满白毛。毛白白猫白，白猫白毛毛。

炖冻冬瓜冬瓜冻，冻冬瓜，炖冻冬瓜是炖冻冬瓜，不炖冻冬瓜不是炖冻冬瓜。炖冻冬瓜吃炖冻冬瓜，不炖冻冬瓜不吃炖冻冬瓜。

补皮裤皮裤破，补皮裤，皮裤不破不补裤。

父母的父母扶父母，父母扶父母的父母。父母是父母的父母，父母的父母是父母。

\subsection{Rare Characters}
嵇康在刑场抚琴，那曲《广陵散》如黄钟大吕，余音袅袅，展现出他不向世俗低头的狷介（juàn jiè）风骨。

林黛玉常常顾影自怜，在潇湘馆里，她的情思如缱绻（qiǎn quǎn）的丝线，缠绕着无尽的哀愁。

尼采的思想犹如浩瀚星空中的璀璨流星，以其特立独行的哲思，打破了人们习以为常的谫陋（jiǎn lòu）认知。

妲己凭借着自己的妖娆妩媚，在商纣王的宫廷中弄权，她的行为可谓是牝鸡司晨（pìn jī sī chén），加速了商朝的灭亡。

李白在月下独酌，酒入愁肠，他的才情如奔涌的江水，挥洒出一篇篇脍炙人口的诗篇，尽显其倜傥不羁（tì tǎng bù jī）的风采。

杨绛先生一生笔耕不辍，她的文字温润如玉，蕴含着对生活的深刻洞察，她的智慧与豁达令人肃然起敬，堪称一代懿范（yì fàn）。

蒲松龄笔下的狐仙鬼怪，或狡黠灵动，或温婉善良，在他构建的奇幻世界里，演绎着人间的悲欢离合，充满了谲诡（jué guǐ）的色彩。

阿基米德在浴缸中顿悟浮力原理的那一刻，灵感如电光石火般闪现，他的智慧光芒照亮了科学发展的漫漫长路，成为了后世敬仰的圭臬（guī niè）。

李清照在国破家亡之际，写下了许多凄婉动人的词句，她的愁绪如氤氲（yīn yūn）的雾气，弥漫在字里行间，令人为之动容。

苏轼一生宦海沉浮，却始终保持着豁达乐观的心态，他在黄州的岁月里，寄情山水，写下了许多脍炙人口的佳作，尽显其旷达超逸（kuàng dá chāo yì）的情怀。
王尔德以其华丽而叛逆的文风著称，他的作品中充满了对传统道德的揶揄（yé yú）和对人性的深刻剖析。

阮籍常常在山野间纵酒放歌，他的行为举止看似荒诞不经，实则是对当时黑暗社会的一种隐晦的訾议（zǐ yì）。

居里夫人在简陋的实验室里，经过无数次的尝试和失败，终于发现了镭元素，她的坚韧和执着成为了科学界的楷模，是当之无愧的巾帼豪杰（jīn guó háo jié）。
卡夫卡的作品中充满了荒诞与迷茫，他笔下的人物常常在孤独和绝望中挣扎，展现出一种难以言喻的幽眇（yōu miǎo）情感。

屈原在汨罗江畔徘徊，他的心中充满了对国家和人民的忧虑，最终以投江的方式表达了自己的忠贞不渝，他的精神成为了中华民族的亢宗之子（kàng zōng zhī zǐ）。

张爱玲的文字犀利而又细腻，她以独特的视角描绘了旧上海的繁华与落寞，她的才情和孤傲令人侧目，是文坛上一颗璀璨的明珠，散发着独特的姱容修态（kuā róng xiū tài）。

王阳明在龙场悟道，创立了心学，他的思想如醍醐灌顶（tí hú guàn dǐng），对后世的哲学发展产生了深远的影响。

泰戈尔的诗歌充满了对自然和生命的热爱，他的文字如潺潺溪流，流淌着温暖和希望，他的作品具有一种独特的骀荡（dài dàng）之美。

杜甫在战乱年代，目睹了百姓的疾苦，他的诗歌如黄钟毁弃（huáng zhōng huǐ qì），发出了对社会不公的强烈控诉，成为了历史的真实写照。

列夫·托尔斯泰在他的作品中，深刻地揭示了人性的善恶美丑，他的思想如振聋发聩（zhèn lóng fā kuì）的钟声，唤醒了人们对生活的思考。 

\nolinenumbers
\end{CJK*}

% ljh
\section{English test set}
\label{appendix:emergent}

\begin{table}[htp]
    \centering
    \caption{Emergent abilities testset by category and evaluation criteria.}
    \vspace{1em}
    \label{table:emergent-abilities} 
    \begin{tabular}{p{1.8cm}p{5cm}p{5.5cm}} %
        \toprule
        \textbf{Categories} & \textbf{Example sentence} & \textbf{Evaluation criteria} \\
        \midrule
        \textbf{Compound Nouns} & The Beckhams decided to rent a charming stone-built quaint countryside holiday cottage. & 1 = fails to recognise compound nouns \newline 2 = fails to realise the phrasal stress naturally \newline 3 = natural phrasal stress \\ [2ex]
        \textbf{Emotions} & "Oh my gosh! Are we really going to the Maldives? That’s unbelievable!" Jennie squealed, bouncing on her toes with uncontained glee.  &   1 = no audible emotions \newline 2 = emotion present but insufficient \newline 3 = correct emotion recognition and appropriate rendering \\ [2ex]
        \textbf{Foreign Words} & Mr. Henry, renowned for his mise en place, orchestrated a seven-course meal, each dish a pièce de résistance. & 1 = pronounces foreign words with incorrect anglicized pronunciation \newline 2 = applies foreign accent but not entirely correctly \newline 3 = correct rendering in the intended language or accepted anglicized reading  \\ [2ex]
        \textbf{Paralinguistics} & "Shh, Lucy, shhh, we mustn't wake your baby brother," Tom whispered, as they tiptoed past the nursery. & 1 = no recognition of paralinguistic keywords such as "shhh" or "phew" \newline 2 = clear intention to render keywords distinctly, but rendering unnatural \newline 3 = natural rendering, e.g. making speech voiceless on "shhh" and other whispered speech   \\ [2ex]
        \textbf{Punctuations} & She received an odd text from her brother: 'Emergency @ home; call ASAP! Mom \& Dad are worried...\#familymatters.' & 1 = glitches on uncommon punctuations such as \# or \& \newline 2 = no glitch but incorrect rendering \newline 3 = no glitch and correct pausing and verbalization, e.g. @ as "at".  \\ [2ex]
        \textbf{Questions} & But the Brexit question remains: After all the trials and tribulations, will the ministers find the answers in time? & 1 = intonation pattern incorrect \newline 2 = intonation pattern largely correct but with minor flaws \newline 3 = correct intonation  \\ [2ex]
        \textbf{Syntactic Complexities} & The movie that De Moya who was recently awarded the lifetime achievement award starred in 2022 was a box-office hit, despite the mixed reviews. & 1 = failure to parse the syntax correctly \newline 2 = parses the syntax largely correctly but the rendering is not entirely natural \newline 3 = parsing correct and rendering natural \\
        \bottomrule
    \end{tabular}
    \vspace{1em}
\end{table}

\subsection{Questions}
1. You went to the party, even though I explicitly told you not to?

2. There is another aircraft still in the air???

3. Now, seriously, you're saying I am the one to blame?

4. But she clearly doesn't want to?

5. To Hungary and back?

6. You're a copper?

7. What is Data Informed Diplomacy, with all its various manifestations?

8. What's really happening, and is there more than meets the eye?

9. How on earth is this Financial Report organized?

10. Where has Jason Abhisheki moved all the flowers to? 

11. What do we do in this situation, and what are the implications for Jordan's water supply?

12. But the Brexit question remains: After all the trials and tribulations, will the ministers find the answers in time?

13. Sorry, can you restate your name and address please? 

14. Here's the full story for today, would you like to learn more? 

15. Mr. Chairman, your highly anticipated interview with Channel 4 has turned into a catastrophe, hasn't it? 

16. Johnny boy, don't go around acting tough if you can't back it up, right? 

17. Are you in favor of the Latex usage policy or you're just sucking up to leadership?

18. Is it a bird, or is it a plane?

19. Madam, have you tried turning it off and on again? 

20. Were you the one with the hand-held camera or the one with a weird-looking android phone?

\subsection{Emotions}

1. Her hands shaking with excitement, Alice Monroe stuttered, "oh..I-I can't believe it! Is this really my acceptance letter to Harvard?" Marco cannot believe it either: "God damn it! How did you pull this off?"

2. A surge of anger flashed across the face of Matthew, as he bellowed, "You have crossed the line this time, and I won't stand for it any longer! Get out!"

3. Gazing at the panoramic view from a mountain in Iceland, Jeff Farahmand sighed deeply, whispering, "This... this is truly the face of the Divine. What more can I ask for?"

4. "You mustn't underestimate how profoundly I've missed your presence," Ito murmured, his eyes glistening with tears as he embraced his long lost sister. "You're finally back, but where do I find our lost years?"

5. "Oh my gosh! Are we really going to the Maldives? That’s unbelievable!" Jennie squealed, bouncing on her toes with obvious glee.

6. "I can confidently declare that this is the most exquisite chocolate cake my taste buds have ever had the pleasure to encounter!" Mo proclaimed, savoring every bite. He could not stop eating! 

7. A proud smile spread across his face as he softly said, "Son, your accomplishments fill my heart with such joy and pride." But then the smile suddenly ceased. Mike’s hearts were pounding like door knocks. His dad’s face now looks like that of the devil himself. 

8. Choking back sobs, Mahmoud whimpered, "I simply can't fathom a life without you by my side. Don't go!"

9. His voice trembled with palpable fear as he stuttered, "There's... there's a stranger at the window. Where the hell are you all waiting for?!"

10. Tears of joy trickled down her cheeks as she yelled, "Graduating as valedictorian... this is a dream come true!"

11. Jane's eyes wide with terror, she screamed, "The brakes aren't working! What do we do now? We're completely trapped!"

12. A profound sense of realization washed over Beal as he whispered, "You've been there for me all along, haven't you? I never truly appreciated you until now."

13. Beth collapsed into his arms, sobbing uncontrollably, "I failed them, I failed them all. They’re all dead! Nothing we can do will ever bring them back. How can I ever live with myself again? How?"

14. His face lit up with pure delight as he exclaimed, "We did it! We won the championship! I knew we could do it together!"

15. Overcome with guilt, Martin hung his head and muttered, "I'm so sorry. I never meant to hurt you like this. Can you ever forgive me?" It was obvious what the answer would be. 

16. The queen danced around the room, eyes twinkling with mischief, "Guess what? I got the lead role in the play! Can you believe it? Well, I can’t."

17. Staring into the distance, the firefighter said with a melancholic smile, "She used to sit right there, you know. I can still hear her laugh if I close my eyes." Outside the window, the rain was pouring down and gushing through every cracks. 

18. The detective’s voice, full of determination and fire, was heard loud and clear in the room, "No one will tell me what I can or cannot do. I'll prove them all wrong! Get me my gun. What are you all looking at me for?"

19. Overwhelmed with confusion and despair, David Darlan cried out, "What do you want from me? Why can't you just tell me what's wrong? Leave me alone!"

20. With a gentle touch and a loving smile, she reassured, "Don't worry, my love. We'll get through this together, just like we always have. I love you."

\subsection{Compound Nouns}

1. In the heart of Lagos, there is a public park with a serene duck pond. Nearby, the children's outdoor play area is full of joyful laughter. Nobody knows the darkness descending soon. 

2. At the family reunion, my grandfather, or father-in-law for some, told many tongue-in-cheek jokes. 

3. The physics teacher asked the students to build a new model solar system. Students were told to bring a tape measure and a pair of scissors, to cut the scale-model planet rings.

4. On this fateful day in 1987, the students boarded the little yellow school bus, chattering excitedly about their field trip to the zoo.

5. Hello, we are representatives from Northern Airlines. Please look out from the big second-floor window seat.

6. After years of work, Heisenberg finally published a ground-breaking cutting-edge research paper on quantum physics.

7. Recipe for a delicious breakfast sandwich: avocado, egg, and cheese on a bagel, cooked over a stovetop frying pan.

8. There is nothing more peaceful than a blue water fountain with a wooden greenhouse. Near there, Joseph installed a hard stone birdbath.

9. Prague, Czechia: Good morning, Harari! Here come the big shopping carts and last-minute video game shoppers.

10. My dog knocked over the tea table and all the books scattered across the second living room floor. 

11. The hiking trail up Yahu Mountain provides a spectacular view of the sunrise. Along the path, the wooden signposts with triple-checked trail maps and green distance markers guided us.

12. The fish clock tower was striking again, reminding us of that profound changing of the guard. 

13. Dean Graham sat on the packed wooden park bench, feeding the pigeons while enjoying the pleasant weather. 

14. The Beckhams decided to rent a charming stone-built quaint countryside holiday cottage. 

15. The construction of the new Newtown-council town hall has made huge trouble; rush-hour traffic jam has never been worse. 

16. Owen Farrell has taken England to the Rugby World Cup glory, with a razor-thin-margin victory against New Zealand in France.

17. Scientists at AWS teams are making last-minute pre-launch model preparations.

18. Bad weather in Northern Europe has caused a god-awful flight check-in time of 6 AM, when even the airport food court isn't open. 

19. Jake Park boasts a beautiful hand-built wooden bird feeder.

20. We visited a quaint bed-and-breakfast establishment, complete with lighthouse lamp room.



\subsection{Syntactic Complexity}

1. The complex houses married and single soldiers and their families.

2. Time flies like an arrow; fruit flies like a banana.

3. The rat the cat the dog chased killed ate the malt.

4. After the writer the editor the publisher hired fired quit, the company found itself in quite a bind.

5. The old man the boats on the shore were manned by had a long history of seafaring.

6. Anyone who feels that if so many more students whom we haven't actually admitted are sitting in on the course than ones we have that the room had to be changed, then probably auditors will have to be excluded, is likely to agree that the curriculum needs revision.

7. While John, who had been working late every night for a month on his novel, finally took a break to enjoy the fresh air, his neighbor, a painter who often found inspiration in the midnight moon, was just beginning her creative process.

8. In the old village with its winding roads, colorful marketplaces, a sense of history that permeates every brick, and a single traffic light, you'll find peace and simplicity.

9. The chef seasoning the fish tossed it gently.

10. As the sun dipped below the horizon, casting a golden glow over the ocean, Emily, who had spent her life dreaming of distant shores, stood on the deck of the ship, feeling a mixture of anticipation and nostalgia as her adventure began.

11. During the meeting, where Coke executives debated the future of the company, Thomas, a young intern who had discovered a solution, mustered the courage to speak, shifting the direction of the conversation, that preceded his intervention.

12. The movie that De Moya who was recently awarded the lifetime achievement award starred in 2022 was a box-office hit, despite the mixed reviews.

13. In the garden, where the flowers that the gardener who retired last year still bloomed, the children who play there every afternoon find peace and joy.

14. The scientist, Mateusz Gorka, who proposed the theory, which many experts in the field, including those who had initially been skeptical bordering on disbelieving, now support, was nominated for a prestigious award.

15. Although the meal that the chef, who had just returned from a culinary tour of Italy, prepared was delicious, the Greek guests barely noticed.

16. The book that the woman who the man who the child spoke to this morning was reading became a topic of conversation among the friends who had all read it.

17. Despite the fact that the road that led to the Five Villages, which was known for its scenic beauty, was narrow and winding, tourists flocked there throughout the year.

18. CNN journalists tracking the stories behind the officials who served during the tumultuous period when the protests rocked the nation to its core noticed significant inconsistencies in the official reports provided.

19. The musicians who performed the symphony that the composer, whose work had often been overlooked in his lifetime, wrote in his early years received a standing ovation.

20. Cars displayed in these showrooms with ENERGY-EFFICIENT AND GREEN decals prominently featured across the windshield aren't announcing environmentalism; they're virtue signaling.

\subsection{Foreign Words}

1. With an ample supply of joie de vivre, Mary danced through the streets of Nice, stopping only to enjoy a nice cafe with a warm croissant.

2. The modern exhibit was a mélange of styles, from German Expressionism to French Impressionism, capturing the Zeitgeist of the time.

3. As a gesture of camaraderie, the Spanish torero invited his rival, Leo the Monster, to a tapas bar, where they enjoyed jamón ibérico and the noche.

4. During Anthony’s wanderlust-filled travels, he discovered the gemütlich atmosphere of many Austrian villages.

5. CloudCorp’s CEO believes in gesamtkunstwerk, like integrating a symphony into a harmonious ensemble.

6. Mr. Henry, renowned for his mise en place, orchestrated a seven-course meal, each dish a pièce de résistance.

7. The fiesta, filled with música, dance, and the warmth of amigos, continued until dawn, embodying the true spirit of a Catalan celebration.

8. At the G20 Summit, leaders discussed rapprochement, trying to step away from the Schadenfreude of political rivalries.

9. After a tiring day, Sarah treated herself to a spa experience, enjoying the sauna and the jacuzzi, and relaxing with a glass of Riesling. 

10. Lasso's novella, rich in allegory and imbued with a sense of ennui, drew from his experiences living in a French château up near the border.

11. The master from Osaka, Japan, dedicated himself to crafting the perfect "nigiri," with "umami" flavors dancing on the palate.

12. Mikhail Gorbachev's Reforms: Perestroika and Glasnost Define a New Era.

13. Lakshmi's yoga practice, centered around the Sanskrit concept of "ahimsa," influenced her approach to life, mirroring the teachings of Mahatma Gandhi.

14. As they strolled through the Grand Bazaar in Istanbul, they were drawn to the beautiful "kilims," the best of Turkic craftsmanship.

15. Inspired by the ancient Chinese philosophy of "Feng Shui," Li rearranged her house to create a "qi" flow throughout.

16. Embracing the Japanese aesthetic of "wabi-sabi," Hokusai's masterpieces were on full display here.

17. During Rio de Janeiro's famous Carnaval do Brasil, the streets pulsated with the rhythms of "samba".

18. The novel's protagonist, guided by the ancient Greek concept of "arete," seeks excellence and virtue, a journey reminiscent of warrior-philosopher-kings.

19. As an aficionado of Scandinavian design, Ole Gunnarsson appreciated the principle of "hygge," evident in his Danish home.

20. These soldiers - they're supposed to practice with a sense of "bushido", the samurai code of honor, but they're behaving like the imperial beasts they are.

\subsection{Punctuations}

1. After a moment of silence, Elena Ivanova finally spoke..., —— her words barely audible over the cracking thunder of a torrential downpour.

2. What!?! You're telling me you've never seen a single episode of 'Game of Thrones' before????! (This was not heard by Prof. Johnson, Dr. Lewis, etc.)

3. "Can anyone hear me over there??? Please, we need help!!! NOW!!!!"

4. “The Power of \& and \% in the Digital Age.” won the first prize in this conference.

5. His latest invention (a device meant to assist in everyday chores (something he never seemed to run out of)), was nothing short of brilliant.

6. She read the label and was surprised to find --- that the "natural" ingredients were actually ..... heavily processed.

7. He relayed his conversation with the bartender, saying, "I told him, 'Your 'signature' cocktail is simply a Margarita with a fancy garnish.'"

8. The presently announced laws were announced in 35°N, 80°W. Specific provisions are to be found in §12 and §17.

9. Please ensure you replace [username] and [password] with your actual credentials before logging in, like jA8!fR3\$mQ1.

10. When Maria asked, 'What's happening tonight?' I replied, 'Well, John — who'll be there at 8:00 p.m. — said, "Let's meet at Sarah's place; bring games, snacks, etc., and don't be late!"'

11. "In the case of Johnson v. Smith, the court found that the defendant's actions — e.g., his failure to fulfill the terms of the contract (see sections 4.1, 4.2, and 4.3), etc. — amounted to a breach of trust.“

12. When asked for his thoughts, he simply replied, «I'll be gone in 5 minutes», and left.

13. I saw Gordon listing the ingredients as follows: <tomatoes>, <fresh basil> (or dried, if unavailable - but it's essential), <olive oil>, <garlic>; salt and pepper.

14. She received an odd text from her brother: 'Emergency @ home; call ASAP! Mom \& Dad are worried...\#familymatters.'

15. The sign at the park's entrance stated, 'Please adhere to the following rules: no littering; no pets (except service animals); no loud music after 9 p.m.'

16. “The Art of /Slash/ and $\backslash$backslash$\backslash$” was the best received talk on modern internet lingo.

17. Jeb's email was brief, but to the point: 'Meeting rescheduled for 3 p.m. tomorrow – apologies for any inconvenience. Regards, J.'.

18. The Dead Sea poems contained several annotations, some of which were quite puzzling: [Section unclear]; [Translation disputed]; [Original wording lost].

19. Her travel blog post was filled with enthusiastic descriptions: 'Best trip ever!!!'; 'Amazing people \& culture!'; 'Can't wait to go back...\#wanderlust.'

20. He shouted, 'Everyone, please gather 'round! Here's the plan: 1) Set-up at 9:15 a.m.; 2) Lunch at 12:00 p.m. (please RSVP!); 3) Playing — e.g., games, music, etc. — from 1:15 to 4:45; and 4) Clean-up at 5 p.m.‘

\subsection{Paralinguistics}

1. Principal Dickson began, addressing the Parkside assembly: "Ahem, I'd like to talk to you about something real serious."

2. "Aha! Now I understand," said Staff Sgt. Miller, piecing together the evidence. "The culprit left this behind. Phew."

3. "Ouch! That stings," Lilly cried, as her mother carefully applied the antiseptic. "Not beyond salvation, eh?" She dryly asked. 

4. "Shh, Lucy, sshhh, we mustn't wake your baby brother," Tom whispered, as they tiptoed past the nursery.

5. "Hmm, what do you think, is it too high or two low, um... Dr. Carter?" Haim asked, handing over the instrument. 

6. "Uh, well, Lisa," Tarek stuttered, nervously extending the ring he bought for god-knows how much, "mmm..will you marry me?"

7. "Yawn," Robert said, stretching out on the park bench, "this sunshine makes me sleepy."

8. "Oops! I did it again!" little Katie exclaimed, spilling her milk. 

9. "Whoa, can you believe this, Mike?" Susan said, staring at the intruder. "Wow, you're right. These men ain't meanin' well." 

10. James leaned back in his chair, wiped his forehead, and sighed, "Phew, haha, that was a tough meeting. Thanks for being there, Karen."

11. psst. psst. look right here. 

12. "Aha! I've found it, Professor Green," exclaimed Muzi Han, holding up the rare manuscript. "This could change our entire understanding of history."

13. "Ouch, be careful, Henry!" warned his sister, as he climbed the rickety ladder.

14. David whispered to Emily as the lights dimmed in the theater, "Shh, it's starting." 

15. "Hmm, I don't know about this, Jim," Mary said, looking at the folder paper. "It doesn't seem right."

16. "Uh, are you sure about this?" Tim asked nervously, looking at the steep slope before them. "Whoa, it's higher than I thought," he continued, his voice filled with trepidation. "Aha, but look at the view," Emily responded with excitement, "it's worth the climb!"

17. Ta-da! well? What do you think? This is the best right?

18. "Oops, sorry, Dad!" Jack apologized. "Ugh! you again". Dad was impatient. 

19. "Whoa, what a game, Alex!" Chris exclaimed. "I've never seen anything like that final play."

20. "Phew, we made it, Martha," Tom said, collapsing into the seat after the completion of the Manhattan Project.


% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
