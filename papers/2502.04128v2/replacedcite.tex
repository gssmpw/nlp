\section{Related work}
\vspace{-5pt}
\subsection{Scaling Law during training and testing}
\vspace{-5pt}
Scaling Law during Training and Testing
Over the past decades, neural networks, with their scaling law, have been the driving engine behind the AI revolution. Particularly in the text domain, large language models (LLMs) represented by the GPT series have demonstrated that scaling up data volumes, computational resources, and model sizes leads to better performance on various tasks, making it a promising path for advancing LLMs toward cognitive intelligence.

However, although existing models still perform below expectations in terms of robustness and handling complex tasks, further scaling in the training phase becomes difficult due to the scarcity of data and computational resources. To address this issue, a scaling law during testing is proposed: the greater the computational effort in the inference, the better the model’s performance ____. With technologies such as repeat sampling, self-correction and tree search, the reasoning depth and accuracy of LLMs in solving complex problems are greatly improved.

% Although its effects have been widely proven in text modality, scaling law during training and testing in speech modality has yet to be investigated. Parke et al. demonstrate that scaling a transformer architecture with large parameters and speech corpora effectively improves the performance of speech tokenizers. The scaling law for TTS models during training and testing is an open question.
\vspace{-10pt}
\subsection{LLM-based TTS}
\vspace{-5pt}
% audiolm
% valle 
% seed 
% cosy
% touchtts
% basetts
% clam-tts
% melle
% kelle
% speartts
% fishspeech
% LLM TTS
Significant achievements have been made by large language models (LLMs) in speech generative tasks, such as multiple-speaker speech syntheses ____ and zero-shot voice cloning ____. VALL-E ____ first regards TTS tasks as a conditional language task, which utilizes neural codec to extract speech tokens and employs a two-stage modeling method to predict them, with an autoregressive (AR) model for generating coarse audio tokens, followed by a non-autoregressive (NAR) model to iteratively predict residual tokens. VALL-E X ____ extends VALL-E into multi-lingual scenarios to support zero-shot cross-lingual speech synthesis. Spear-TTS ____ integrates multi-stage AR language models and achieves a multi-speaker TTS system with minimal supervision.

Recently, many TTS systems have focused on achieving more natural and controllable speech synthesis by leveraging well-designed modules, larger datasets and model sizes. TorToise-TTS ____, BASE TTS ____, Seed-TTS ____, CosyVoice ____, and FireRedTTS ____ combine an AR language model and a diffusion model and are trained on hundreds of thousands of hours of speech data, which integrate advantages of the language model and diffusion model, realizing impressive performance in speech quality and speaker timbre similarity. On the other hand, some researchers push for a way to simplify the TTS paradigm. MELL-E ____ introduces a VAE-like latent sampling module into an AR language model to predict Mel spectrograms, while KALL-E ____ leverages an AR language model to predict speech distributions extracted by a WaveVAE model.

% While these efforts have propelled the TTS field forward, there is no broad consensus on a relatively standard framework. A simple Transformer model with a tokenizer aligns the text LLM and allows researchers to concentrate on fundamental issues such as training-time scaling laws, test-time scaling behaviors, and downstream adaptations.

% \subsection{Speech Tokenizer}
% % Speech Tokenizer
% Speech tokenizers refer to a kind of neural network model that converts continuous speech signals into discrete tokens, which discretize high-rate audio signals into a finite set of tokens, hence enabling the application of LLM architectures to speech data. Current discrete speech tokens can be categorized into two types: semantic tokens and acoustic tokens. Semantic tokens are typically from self-supervised pre-trained models with masked language modeling as training objectives, such as HuBERT and WavLM. Derived through k-means clustering on representations from a specific intermediate layer, semantic tokens are depicted as a single-level sequence. Acoustic tokens can be extracted from neural audio codecs with reconstruction as a training objective. Mostly utilizing residual vector quantization (RVQ) with hierarchical quantizers for discretization, acoustic tokens are represented as multiple sequences.

% Semantic tokens exhibit a high alignment with text while losing some information in speech, such as timbre, while acoustic tokens excel in effectively preserving speech information but do not demonstrate a strong alignment with the text [SpeechTokenizer]. To this end, SpeechTokenizer builds specialized speech tokens designed for speech-language models by unifying semantic and acoustic tokens within RVQ. However, such an RVQ-based tokenizer still brings challenges to downstream tasks, which requires additional structures to predict multiple tokens at each time step. To address this issue, some researchers investigate single-codebook neural speech tokenizers [Single-Codec, Wavtokenizers]. For example,  Single-Codec employs a disentangled VQ-VAE on Mel Spectrograms to decouple speech into time-invariant global embedding and one phonetically-rich discrete sequence quantized by one codebook.

% Although they simplify discrete sequences into a single sequence, these speech tokenizers require additional information when decoding. How to ensure that all aspects of the speech signals — content, prosody, timbre — are captured by one discrete sequence still requires exploration.
% \newpage
\vspace{-15pt}