@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI},
  year={2018}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI},
  year={2019}
}

@misc{openai2024learning,
  title={Learning to Reason with LLMs},
  author={OpenAI},
  year={2024}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{touvron2023llama1,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@article{nadeem2024narrativebridge,
  title={NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative},
  author={Nadeem, Asmar and Sardari, Faegheh and Dawes, Robert and Husain, Syed Sameed and Hilton, Adrian and Mustafa, Armin},
  journal={arXiv preprint arXiv:2406.06499},
  year={2024}
}

@inproceedings{kim2024show,
  title={Show Think and Tell: Thought-Augmented Fine-Tuning of Large Language Models for Video Captioning},
  author={Kim, Byoungjip and Hwang, Dasol and Cho, Sungjun and Jang, Youngsoo and Lee, Honglak and Lee, Moontae},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2024}
}

@inproceedings{he2024ma,
  title={Ma-lmm: Memory-augmented large multimodal model for long-term video understanding},
  author={He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2024}
}

@inproceedings{lin2024learning,
  title={Learning Video Context as Interleaved Multimodal Sequences},
  author={Lin, Kevin Qinghong and Zhang, Pengchuan and Gao, Difei and Xia, Xide and Chen, Joya and Gao, Ziteng and Xie, Jinheng and Xiao, Xuhong and Shou, Mike Zheng},
  booktitle={European Conference on Computer Vision},
  year={2024}
}

@InProceedings{Han_2024_CVPR,
    author    = {Han, Jiaming and Gong, Kaixiong and Zhang, Yiyuan and Wang, Jiaqi and Zhang, Kaipeng and Lin, Dahua and Qiao, Yu and Gao, Peng and Yue, Xiangyu},
    title     = {OneLLM: One Framework to Align All Modalities with Language},
    booktitle = {Conference on Computer Vision and Pattern Recognition},
    year      = {2024}
}


@inproceedings{tan2024koala,
  title={Koala: Key frame-conditioned long video-LLM},
  author={Tan, Reuben and Sun, Ximeng and Hu, Ping and Wang, Jui-hsien and Deilamsalehy, Hanieh and Plummer, Bryan A and Russell, Bryan and Saenko, Kate},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2024}
}

@InProceedings{Wang_2024_CVPR,
    author    = {Wang, Kai and Tian, Yapeng and Hatzinakos, Dimitrios},
    title     = {Towards Efficient Audio-Visual Learners via Empowering Pre-trained Vision Transformers with Cross-Modal Adaptation},
    booktitle = {Conference on Computer Vision and Pattern Recognition Workshops},
    year      = {2024}
}

@inproceedings{NEURIPS2023_af01716e,
 author = {Duan, Haoyi and Xia, Yan and Mingze, Zhou and Tang, Li and Zhu, Jieming and Zhao, Zhou},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks},
 year = {2023}
}


@InProceedings{pmlr-v202-li23q,
  title = 	 {{BLIP}-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author =       {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2023}
}

@inproceedings{Liu2023Vis,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Visual Instruction Tuning},
 year = {2023}
}

@inproceedings{chen2024pathformer,
  author       = {Peng Chen and Yingying Zhang and Yunyao Cheng and Yang Shu and Yihang Wang and Qingsong Wen and Bin Yang and Chenjuan Guo},
  title        = {Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting},
  booktitle    = {International Conference on Learning Representations},
  year         = {2024}
}

@inproceedings{liu2024tackling,
  title={Tackling data bias in music-avqa: Crafting a balanced dataset for unbiased question-answering},
  author={Liu, Xiulong and Dong, Zhikang and Zhang, Peng},
  booktitle={Winter Conference on Applications of Computer Vision},
  year={2024}
}

@inproceedings{xu2016msr,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2016}
}

@inproceedings{bain2020condensed,
    title={Condensed Movies: Story Based Retrieval with Contextual Embeddings},
    author={Max Bain and Arsha Nagrani and Andrew Brown and Andrew Zisserman},
    booktitle={Asian Conference on Computer Vision},
    year={2020}
}

@inproceedings{Li2022learning,
  title={Learning to answer questions in dynamic audio-visual scenarios},
  author={Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@InProceedings{Lin_2023_CVPR,
    author    = {Lin, Yan-Bo and Sung, Yi-Lin and Lei, Jie and Bansal, Mohit and Bertasius, Gedas},
    title     = {Vision Transformers Are Parameter-Efficient Audio-Visual Learners},
    booktitle = {Conference on Computer Vision and Pattern Recognition},
    year      = {2023}
}

@article{hendria2023action,
  title={Action knowledge for video captioning with graph neural networks},
  author={Hendria, Willy Fitra and Velda, Vania and Putra, Bahy Helmi Hartoyo and Adzaka, Fikriansyah and Jeong, Cheol},
  journal={Journal of King Saud University-Computer and Information Sciences},
  year={2023},
}

@article{wang2022git,
  title={Git: A generative image-to-text transformer for vision and language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={Transactions on Machine Learning Research},
  year={2022}
}


@inproceedings{ren2023testa,
    title = "{TESTA}: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding",
    author = "Ren, Shuhuai  and
      Chen, Sishuo  and
      Li, Shicheng  and
      Sun, Xu  and
      Hou, Lu",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP",
    year = "2023"
}

@inproceedings{cheng2023vindlu,
  title={Vindlu: A recipe for effective video-and-language pretraining},
  author={Cheng, Feng and Wang, Xizi and Lei, Jie and Crandall, David and Bansal, Mohit and Bertasius, Gedas},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@inproceedings{maaz2023video,
    title = "Video-{C}hat{GPT}: Towards Detailed Video Understanding via Large Vision and Language Models",
    author = "Maaz, Muhammad  and
      Rasheed, Hanoona  and
      Khan, Salman  and
      Khan, Fahad",
    booktitle = "Annual Meeting of the Association for Computational Linguistics",
    year = "2024"
}

@inproceedings{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@inproceedings{hussein2019timeception,
  title={Timeception for complex action recognition},
  author={Hussein, Noureldien and Gavves, Efstratios and Smeulders, Arnold WM},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2019}
}

@inproceedings{wu2021towards,
  title={Towards long-form video understanding},
  author={Wu, Chao-Yuan and Krahenbuhl, Philipp},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{islam2022long,
  title={Long movie clip classification with state-space video models},
  author={Islam, Md Mohaiminul and Bertasius, Gedas},
  booktitle={European Conference on Computer Vision},
  year={2022}
}

@inproceedings{wang2023selective,
  title={Selective structured state-spaces for long-form video understanding},
  author={Wang, Jue and Zhu, Wentao and Wang, Pichao and Yu, Xiang and Liu, Linda and Omar, Mohamed and Hamid, Raffay},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@inproceedings{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{nadeem2023sem,
  title={Sem-pos: Grammatically and semantically correct video captioning},
  author={Nadeem, Asmar and Hilton, Adrian and Dawes, Robert and Thomas, Graham and Mustafa, Armin},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}
----------------working memory----------------
@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  year={2024}
}

@article{zhang2023visual,
  title={Visual cropping improves zero-shot question answering of multimodal large language models},
  author={Zhang, Jiarui and Khayatkhoei, Mahyar and Chhikara, Prateek and Ilievski, Filip},
  journal={arXiv preprint arXiv:2310.16033},
  year={2023}
}

@inproceedings{wu2024v,
  title={V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs},
  author={Wu, Penghao and Xie, Saining},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2024}
}

@inproceedings{li-etal-2023-large,
    title = "Large Language Models with Controllable Working Memory",
    author = "Li, Daliang  and
      Rawat, Ankit Singh  and
      Zaheer, Manzil  and
      Wang, Xin  and
      Lukasik, Michal  and
      Veit, Andreas  and
      Yu, Felix  and
      Kumar, Sanjiv",
    booktitle = "Findings of the Association for Computational Linguistics: ACL",
    year = "2023"
}

@inproceedings{gong2024working,
  title={Working memory capacity of ChatGPT: An empirical study},
  author={Gong, Dongyu and Wan, Xingchen and Wang, Dingmin},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2024}
}

@inproceedings{zhangchunhui2024working,
  title={Scaling Cognitive Limits: Identifying Working Memory Limits in LLMs},
  author={Zhang, Chunhui and Jian, Yiren and Ouyang, Zhongyu and Vosoughi, Soroush},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2024}
}


@article{baddeley2000episodic,
  title={The episodic buffer: a new component of working memory?},
  author={Baddeley, Alan},
  journal={Trends in cognitive sciences},
  year={2000},
}

@inproceedings{zhang2025pretrained,
  title={Pretrained Image-Text Models are Secretly Video Captioners},
  author={Zhang, Chunhui and Jian, Yiren and Ouyang, Zhongyu and Vosoughi, Soroush},
  booktitle={Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2025},
}

@inproceedings{diao2024learning,
  title={Learning Musical Representations for Music Performance Question Answering},
  author={Diao, Xingjian and Zhang, Chunhui and Wu, Tingxuan and Cheng, Ming and Ouyang, Zhongyu and Wu, Weiyi and Gui, Jiang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  year={2024}
}

@inproceedings{zhang2022look,
  title={Look twice as much as you say: Scene graph contrastive learning for self-supervised image caption generation},
  author={Zhang, Chunhui and Huang, Chao and Li, Youhuan and Zhang, Xiangliang and Ye, Yanfang and Zhang, Chuxu},
  booktitle={International Conference on Information \& Knowledge Management},
  year={2022}
}

@inproceedings{jian2024expedited,
  title={Expedited training of visual conditioned language generation via redundancy reduction},
  author={Jian, Yiren and Liu, Tingkai and Tao, Yunzhe and Zhang, Chunhui and Vosoughi, Soroush and Yang, Hongxia},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
  year={2024}
}

@article{liu2024protecting,
  title={Protecting privacy in multimodal large language models with mllmu-bench},
  author={Liu, Zheyuan and Dou, Guangyao and Jia, Mengzhao and Tan, Zhaoxuan and Zeng, Qingkai and Yuan, Yongle and Jiang, Meng},
  journal={arXiv preprint arXiv:2410.22108},
  year={2024}
}

@inproceedings{liu2024breaking,
  title={Breaking the trilemma of privacy, utility, and efficiency via controllable machine unlearning},
  author={Liu, Zheyuan and Dou, Guangyao and Chien, Eli and Zhang, Chunhui and Tian, Yijun and Zhu, Ziwei},
  booktitle={International World Wide Web Conference},
  year={2024}
}

@article{xie2025multi,
  title={Multi-View Factorizing and Disentangling: A Novel Framework for Incomplete Multi-View Multi-Label Classification},
  author={Xie, Wulin and Zhao, Lian and Long, Jiang and Lu, Xiaohuan and Nie, Bingyan},
  journal={arXiv preprint arXiv:2501.06524},
  year={2025}
}

@inproceedings{xie2024uncertainty,
  title={Uncertainty-aware pseudo-labeling and dual graph driven network for incomplete multi-view multi-label classification},
  author={Xie, Wulin and Lu, Xiaohuan and Liu, Yadong and Long, Jiang and Zhang, Bob and Zhao, Shuping and Wen, Jie},
  booktitle={International Conference on Multimedia},
  year={2024}
}

@inproceedings{yao2024multi,
  title={Multi-modal proxy learning towards personalized visual multiple clustering},
  author={Yao, Jiawei and Qian, Qi and Hu, Juhua},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2024}
}


@inproceedings{yao2024customized,
  title={Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning},
  author={Yao, Jiawei and Qian, Qi and Hu, Juhua},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}
