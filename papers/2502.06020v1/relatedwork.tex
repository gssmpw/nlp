\section{Related Works}
\label{related_works}
\paragraph{Temporal Modeling in MLLMs}
Multimodal LLMs (MLLMs) for long-video understanding aim to capture long-range temporal patterns. A common strategy is temporal pooling, as used in VideoChatGPT \cite{maaz2023video}, but this can limit performance due to inadequate temporal modeling. More advanced methods, such as video-LLAMA \cite{zhang2023video}, incorporate video query transformers to enhance temporal dynamics, but this comes with increased model complexity. To reduce computational demands, some models rely on pre-extracted features, avoiding joint training of backbone architectures \cite{hussein2019timeception, liu2024breaking, wu2021towards}. Techniques like Vis4mer \cite{islam2022long} and S5 \cite{wang2023selective} utilize the S4 transformer architecture \cite{gu2021efficiently} for efficient long-range temporal modeling. Recent developments, such as online video processing \cite{he2024ma}, employ memory banks to track past content for long-term analysis. In contrast, we propose a TWM mechanism that retains only query-relevant multimodal inputs through search engines within a temporal context.

\paragraph{Video Understanding}
Video understanding tasks evaluate a model's ability to process multimodal content, focusing on both temporal and semantic aspects. Key tasks for long-video understanding include audio-visual question answering (AVQA), video captioning, and video-text retrieval, supported by extensive research and large-scale datasets \cite{liu2024tackling, xu2016msr, bain2020condensed}. Prior AVQA methods fine-tuned pretrained visual models with adapters \cite{liu2024tackling, Lin_2023_CVPR, NEURIPS2023_af01716e, diao2024learning}, while recent approaches use unified multimodal encoders with LLMs \cite{Han_2024_CVPR}. Video captioning models employ graph neural networks (GNNs) \cite{hendria2023action}, simplified image-text architectures \cite{wang2022git}, or causal effect networks (CEN) to enhance temporal coherence \cite{nadeem2024narrativebridge}. In video-text retrieval, adaptive frame aggregation reduces visual tokens to accelerate encoding \cite{ren2023testa}.
In contrast to previous work focusing on specific multimodal applications, this study emphasizes the role of TWM in enhancing fundamental temporal grounding across audio, video, and language.