[
  {
    "index": 0,
    "papers": [
      {
        "key": "maaz2023video",
        "author": "Maaz, Muhammad  and\nRasheed, Hanoona  and\nKhan, Salman  and\nKhan, Fahad",
        "title": "Video-{C}hat{GPT}: Towards Detailed Video Understanding via Large Vision and Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhang2023video",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hussein2019timeception",
        "author": "Hussein, Noureldien and Gavves, Efstratios and Smeulders, Arnold WM",
        "title": "Timeception for complex action recognition"
      },
      {
        "key": "liu2024breaking",
        "author": "Liu, Zheyuan and Dou, Guangyao and Chien, Eli and Zhang, Chunhui and Tian, Yijun and Zhu, Ziwei",
        "title": "Breaking the trilemma of privacy, utility, and efficiency via controllable machine unlearning"
      },
      {
        "key": "wu2021towards",
        "author": "Wu, Chao-Yuan and Krahenbuhl, Philipp",
        "title": "Towards long-form video understanding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "islam2022long",
        "author": "Islam, Md Mohaiminul and Bertasius, Gedas",
        "title": "Long movie clip classification with state-space video models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2023selective",
        "author": "Wang, Jue and Zhu, Wentao and Wang, Pichao and Yu, Xiang and Liu, Linda and Omar, Mohamed and Hamid, Raffay",
        "title": "Selective structured state-spaces for long-form video understanding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gu2021efficiently",
        "author": "Gu, Albert and Goel, Karan and R{\\'e}, Christopher",
        "title": "Efficiently modeling long sequences with structured state spaces"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "he2024ma",
        "author": "He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam",
        "title": "Ma-lmm: Memory-augmented large multimodal model for long-term video understanding"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2024tackling",
        "author": "Liu, Xiulong and Dong, Zhikang and Zhang, Peng",
        "title": "Tackling data bias in music-avqa: Crafting a balanced dataset for unbiased question-answering"
      },
      {
        "key": "xu2016msr",
        "author": "Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong",
        "title": "Msr-vtt: A large video description dataset for bridging video and language"
      },
      {
        "key": "bain2020condensed",
        "author": "Max Bain and Arsha Nagrani and Andrew Brown and Andrew Zisserman",
        "title": "Condensed Movies: Story Based Retrieval with Contextual Embeddings"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2024tackling",
        "author": "Liu, Xiulong and Dong, Zhikang and Zhang, Peng",
        "title": "Tackling data bias in music-avqa: Crafting a balanced dataset for unbiased question-answering"
      },
      {
        "key": "Lin_2023_CVPR",
        "author": "Lin, Yan-Bo and Sung, Yi-Lin and Lei, Jie and Bansal, Mohit and Bertasius, Gedas",
        "title": "Vision Transformers Are Parameter-Efficient Audio-Visual Learners"
      },
      {
        "key": "NEURIPS2023_af01716e",
        "author": "Duan, Haoyi and Xia, Yan and Mingze, Zhou and Tang, Li and Zhu, Jieming and Zhao, Zhou",
        "title": "Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks"
      },
      {
        "key": "diao2024learning",
        "author": "Diao, Xingjian and Zhang, Chunhui and Wu, Tingxuan and Cheng, Ming and Ouyang, Zhongyu and Wu, Weiyi and Gui, Jiang",
        "title": "Learning Musical Representations for Music Performance Question Answering"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Han_2024_CVPR",
        "author": "Han, Jiaming and Gong, Kaixiong and Zhang, Yiyuan and Wang, Jiaqi and Zhang, Kaipeng and Lin, Dahua and Qiao, Yu and Gao, Peng and Yue, Xiangyu",
        "title": "OneLLM: One Framework to Align All Modalities with Language"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hendria2023action",
        "author": "Hendria, Willy Fitra and Velda, Vania and Putra, Bahy Helmi Hartoyo and Adzaka, Fikriansyah and Jeong, Cheol",
        "title": "Action knowledge for video captioning with graph neural networks"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2022git",
        "author": "Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan",
        "title": "Git: A generative image-to-text transformer for vision and language"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "nadeem2024narrativebridge",
        "author": "Nadeem, Asmar and Sardari, Faegheh and Dawes, Robert and Husain, Syed Sameed and Hilton, Adrian and Mustafa, Armin",
        "title": "NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ren2023testa",
        "author": "Ren, Shuhuai  and\nChen, Sishuo  and\nLi, Shicheng  and\nSun, Xu  and\nHou, Lu",
        "title": "{TESTA}: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding"
      }
    ]
  }
]