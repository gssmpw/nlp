@InProceedings{Han_2024_CVPR,
    author    = {Han, Jiaming and Gong, Kaixiong and Zhang, Yiyuan and Wang, Jiaqi and Zhang, Kaipeng and Lin, Dahua and Qiao, Yu and Gao, Peng and Yue, Xiangyu},
    title     = {OneLLM: One Framework to Align All Modalities with Language},
    booktitle = {Conference on Computer Vision and Pattern Recognition},
    year      = {2024}
}

@InProceedings{Lin_2023_CVPR,
    author    = {Lin, Yan-Bo and Sung, Yi-Lin and Lei, Jie and Bansal, Mohit and Bertasius, Gedas},
    title     = {Vision Transformers Are Parameter-Efficient Audio-Visual Learners},
    booktitle = {Conference on Computer Vision and Pattern Recognition},
    year      = {2023}
}

@inproceedings{NEURIPS2023_af01716e,
 author = {Duan, Haoyi and Xia, Yan and Mingze, Zhou and Tang, Li and Zhu, Jieming and Zhao, Zhou},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks},
 year = {2023}
}

@inproceedings{bain2020condensed,
    title={Condensed Movies: Story Based Retrieval with Contextual Embeddings},
    author={Max Bain and Arsha Nagrani and Andrew Brown and Andrew Zisserman},
    booktitle={Asian Conference on Computer Vision},
    year={2020}
}

@inproceedings{diao2024learning,
  title={Learning Musical Representations for Music Performance Question Answering},
  author={Diao, Xingjian and Zhang, Chunhui and Wu, Tingxuan and Cheng, Ming and Ouyang, Zhongyu and Wu, Weiyi and Gui, Jiang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  year={2024}
}

@inproceedings{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{he2024ma,
  title={Ma-lmm: Memory-augmented large multimodal model for long-term video understanding},
  author={He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2024}
}

@article{hendria2023action,
  title={Action knowledge for video captioning with graph neural networks},
  author={Hendria, Willy Fitra and Velda, Vania and Putra, Bahy Helmi Hartoyo and Adzaka, Fikriansyah and Jeong, Cheol},
  journal={Journal of King Saud University-Computer and Information Sciences},
  year={2023},
}

@inproceedings{hussein2019timeception,
  title={Timeception for complex action recognition},
  author={Hussein, Noureldien and Gavves, Efstratios and Smeulders, Arnold WM},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2019}
}

@inproceedings{islam2022long,
  title={Long movie clip classification with state-space video models},
  author={Islam, Md Mohaiminul and Bertasius, Gedas},
  booktitle={European Conference on Computer Vision},
  year={2022}
}

@inproceedings{liu2024breaking,
  title={Breaking the trilemma of privacy, utility, and efficiency via controllable machine unlearning},
  author={Liu, Zheyuan and Dou, Guangyao and Chien, Eli and Zhang, Chunhui and Tian, Yijun and Zhu, Ziwei},
  booktitle={International World Wide Web Conference},
  year={2024}
}

@inproceedings{liu2024tackling,
  title={Tackling data bias in music-avqa: Crafting a balanced dataset for unbiased question-answering},
  author={Liu, Xiulong and Dong, Zhikang and Zhang, Peng},
  booktitle={Winter Conference on Applications of Computer Vision},
  year={2024}
}

@inproceedings{maaz2023video,
    title = "Video-{C}hat{GPT}: Towards Detailed Video Understanding via Large Vision and Language Models",
    author = "Maaz, Muhammad  and
      Rasheed, Hanoona  and
      Khan, Salman  and
      Khan, Fahad",
    booktitle = "Annual Meeting of the Association for Computational Linguistics",
    year = "2024"
}

@article{nadeem2024narrativebridge,
  title={NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative},
  author={Nadeem, Asmar and Sardari, Faegheh and Dawes, Robert and Husain, Syed Sameed and Hilton, Adrian and Mustafa, Armin},
  journal={arXiv preprint arXiv:2406.06499},
  year={2024}
}

@inproceedings{ren2023testa,
    title = "{TESTA}: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding",
    author = "Ren, Shuhuai  and
      Chen, Sishuo  and
      Li, Shicheng  and
      Sun, Xu  and
      Hou, Lu",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP",
    year = "2023"
}

@article{wang2022git,
  title={Git: A generative image-to-text transformer for vision and language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@inproceedings{wang2023selective,
  title={Selective structured state-spaces for long-form video understanding},
  author={Wang, Jue and Zhu, Wentao and Wang, Pichao and Yu, Xiang and Liu, Linda and Omar, Mohamed and Hamid, Raffay},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@inproceedings{wu2021towards,
  title={Towards long-form video understanding},
  author={Wu, Chao-Yuan and Krahenbuhl, Philipp},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{xu2016msr,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2016}
}

@inproceedings{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

