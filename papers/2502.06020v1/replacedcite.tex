\section{Related Works}
\label{related_works}
\paragraph{Temporal Modeling in MLLMs}
Multimodal LLMs (MLLMs) for long-video understanding aim to capture long-range temporal patterns. A common strategy is temporal pooling, as used in VideoChatGPT ____, but this can limit performance due to inadequate temporal modeling. More advanced methods, such as video-LLAMA ____, incorporate video query transformers to enhance temporal dynamics, but this comes with increased model complexity. To reduce computational demands, some models rely on pre-extracted features, avoiding joint training of backbone architectures ____. Techniques like Vis4mer ____ and S5 ____ utilize the S4 transformer architecture ____ for efficient long-range temporal modeling. Recent developments, such as online video processing ____, employ memory banks to track past content for long-term analysis. In contrast, we propose a TWM mechanism that retains only query-relevant multimodal inputs through search engines within a temporal context.

\paragraph{Video Understanding}
Video understanding tasks evaluate a model's ability to process multimodal content, focusing on both temporal and semantic aspects. Key tasks for long-video understanding include audio-visual question answering (AVQA), video captioning, and video-text retrieval, supported by extensive research and large-scale datasets ____. Prior AVQA methods fine-tuned pretrained visual models with adapters ____, while recent approaches use unified multimodal encoders with LLMs ____. Video captioning models employ graph neural networks (GNNs) ____, simplified image-text architectures ____, or causal effect networks (CEN) to enhance temporal coherence ____. In video-text retrieval, adaptive frame aggregation reduces visual tokens to accelerate encoding ____.
In contrast to previous work focusing on specific multimodal applications, this study emphasizes the role of TWM in enhancing fundamental temporal grounding across audio, video, and language.