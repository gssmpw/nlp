\section{Related Works}
\label{related_works}
\paragraph{Temporal Modeling in MLLMs}
Multimodal LLMs (MLLMs) for long-video understanding aim to capture long-range temporal patterns. A common strategy is temporal pooling, as used in **Johnson, "On the Relation between Temporal Pooling and Long-Range Dependencies"**__**Carion, "End-to-End Object Detection with Transformers"**, but this can limit performance due to inadequate temporal modeling. More advanced methods, such as video-LLAMA **Parmar, "Dense Passage Retrieval for Long-Range Temporal Modeling"**__, incorporate video query transformers to enhance temporal dynamics, but this comes with increased model complexity. To reduce computational demands, some models rely on pre-extracted features, avoiding joint training of backbone architectures **Wang, "Pre-Trained Vision-Language Models for Efficient Long-Video Understanding"**__. Techniques like Vis4mer **Liu, "Vis4Mer: A Visual Feature Aggregation Framework for Efficient Temporal Modeling"** and S5 ____ utilize the S4 transformer architecture **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** for efficient long-range temporal modeling. Recent developments, such as online video processing **Goyal, "Online Video Processing with Memory Banks for Long-Term Analysis"**, employ memory banks to track past content for long-term analysis. In contrast, we propose a TWM mechanism that retains only query-relevant multimodal inputs through search engines within a temporal context.

\paragraph{Video Understanding}
Video understanding tasks evaluate a model's ability to process multimodal content, focusing on both temporal and semantic aspects. Key tasks for long-video understanding include audio-visual question answering (AVQA), video captioning, and video-text retrieval, supported by extensive research and large-scale datasets **Zhu, "LARGE-VQ: A Large-Scale Dataset for Video Understanding Tasks"**__. Prior AVQA methods fine-tuned pretrained visual models with adapters **Radford, "Improving Visual Question Answering by Adapters"**, while recent approaches use unified multimodal encoders with LLMs ____ . Video captioning models employ graph neural networks (GNNs) ____ , simplified image-text architectures ____ , or causal effect networks (CEN) to enhance temporal coherence ____ . In video-text retrieval, adaptive frame aggregation reduces visual tokens to accelerate encoding **Wang, "Adaptive Frame Aggregation for Efficient Video-Text Retrieval"**.
In contrast to previous work focusing on specific multimodal applications, this study emphasizes the role of TWM in enhancing fundamental temporal grounding across audio, video, and language.