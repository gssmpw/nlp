\section{Related Works}
\subsection{Vision Transformer}

Dosovitskiy \textit{et al.} introduced the vision transformer or more popularly, the ViT**Dosovitskiy, "Image Transformers"**, which facilitated the use of transformer-based models for vision problems by breaking down the input image into several small patches, termed visual sequences, enabling the natural calculation of attention between any two image patches. Subsequently, researchers in**Carion, "End-to-End Object Detection with Transformers"** explored data-efficient training and distillation to enhance ViT's performance on the ImageNet benchmark, achieving a top-1 accuracy of 81.8\% through extensive experiments, which was comparable to state-of-the-art convolutional networks. Recent surveys indicate a growing adoption of transformer architectures in computer vision tasks over the last few years, including image recognition**Sun, "Meta TransFormer"**, object detection**Cao, "TransFOV: Transforming Feature Pyramid with Object Queries"**, and segmentation**Chen, "Perceptual Transformer Network for Image Segmentation"**, as well as other tasks. However, as execution speed increases, maintaining good accuracy becomes a growing concern. As a remedy to such issues, Bucilua \textit{et al.} developed a model compression approach that enables the transfer of knowledge from a large model or an ensemble of models to a smaller model, mitigating the accuracy drop typically associated with model compression**Bucilua, "Learning Compact Transformers for Image Classification"**.

%In another paper, the knowledge transfer between a fully-supervised teacher model and a student model using the unlabeled data was shown for semi-supervised learning, which was suited when the teacher wasn't around)**. The process of learning a small model from such a large model was dubbed knowledge distillation **Hinton, "Distilling Knowledge into Small Neural Networks"**.


\subsection{Attention Mechanism}
Attention mechanism has been widely used in several transformer-based models. It has been the core part to learn long term information in terms of feature extraction. Attention in transformer model has been used in many tasks including object recognition **Lin, "CSFA: Cross-Scale Feature Aggregation"**, image classification **Wu, "Transformers for Image Classification with Multi-Resolution Features"**, image super-resolution **Isola, "Image-to-Image Translation with Conditional Adversarial Networks"**, image translation etc. Transformers utilize scaled dot-product and multi-head attention mechanisms, enhancing computational efficiency and capturing more complex data relationships **Vaswani, "Attention Is All You Need"**. These mechanisms have been important in allowing transformers to outperform traditional convolutional networks in various visual learning tasks. Among all attention mechanisms, channel attention modules focus on enhancing significant feature channels while suppressing less relevant ones, primarily introduced by SENet **Hu, "Squeeze-and-Excitation Networks"**. They use a two-step process: squeezing global spatial information and exciting inter-channel dependencies. On the other hand, spatial attention mechanisms target specific regions within an image, highlighting important areas and diminishing background noise **Wang, "Non-local Neural Networks"**. They generate spatial attention maps that assign weights to different image locations, improving feature expression. Another approach, branch attention mechanisms dynamically select and emphasize different network branches based on input data, optimizing feature learning **Hu, "Dual Attention Networks"**. However, all of these methods are not lightweight and do not cover deeper and longer feature dependencies.


\subsection{Knowledge Distillation}

Most of the new ideas for distilling knowledge focus on compressing very large neural networks. The lightweight student networks can be used in applications such as visual recognition, speech recognition, and natural language processing, and they can be set up quickly. It can also be used for other things, like adversarial attacks**Papernot, "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples"**, adding data**Hendrycks, "Natural Adversarial Examples"**, protecting data privacy and security, and more. The idea of knowledge distillation for model compression has been used to compress the training data, which is called dataset distillation. This process moves the knowledge from a large dataset into a small dataset to make it easier for deep models to train**Cheng, "Dataset Distillation"**.
In a recent study,  measured how many visual concepts were extracted from the intermediate layers of a deep neural network in order to show how knowledge was boiled down**Tompson, "Deformable Part-Based Models"**. Risk bound, data efficiency, and imperfect teachers all played a part in how knowledge was distilled on a wide neural network**Bartlett, "Near-Optimal Finite-Dimensional Representations of Probability Measures"**. Knowledge distillation had also been used to make labels smoother, to check the accuracy of the teacher, and to figure out what the best shape for the output layer should be**Goodfellow, "On the Difficulty of Training Recurrent Neural Networks"**. However, a recent study by  performed extensive experiments to see if knowledge distillation worked but results from the experiments suggested otherwise **Cho, "Distillation-Based Knowledge Transfer for Deep Neural Networks"**. It was theorized that the poor performance of knowledge distillation was linked to the fact that a bigger model may not be a better teacher because it, albeit being a larger model, might not have enough space for performing all the intended tasks**Sutskever, "Training Recurrent Neural Networks with Non-Uniform Time-Series Data"**.