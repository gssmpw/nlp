\section{Related Works}
\subsection{Vision Transformer}

Dosovitskiy \textit{et al.} introduced the vision transformer or more popularly, the ViT~\cite{dosovitskiy2020image}, which facilitated the use of transformer-based models for vision problems by breaking down the input image into several small patches, termed visual sequences, enabling the natural calculation of attention between any two image patches. Subsequently, researchers in~\cite{touvron2021training} explored data-efficient training and distillation to enhance ViT's performance on the ImageNet benchmark, achieving a top-1 accuracy of 81.8\% through extensive experiments, which was comparable to state-of-the-art convolutional networks. Recent surveys indicate a growing adoption of transformer architectures in computer vision tasks over the last few years, including image recognition~\cite{han2022survey}, object detection~\cite{zhu2020deformable}, and segmentation~\cite{zheng2021rethinking}, as well as other tasks. However, as execution speed increases, maintaining good accuracy becomes a growing concern. As a remedy to such issues, Bucilua \textit{et al.} developed a model compression approach that enables the transfer of knowledge from a large model or an ensemble of models to a smaller model, mitigating the accuracy drop typically associated with model compression~\cite{bucilua2006model}.

%In another paper, the knowledge transfer between a fully-supervised teacher model and a student model using the unlabeled data was shown for semi-supervised learning, which was suited when the teacher wasn't around~\cite{urner2011access}). The process of learning a small model from such a large model was dubbed knowledge distillation ~\cite{hinton2015distilling}.


\subsection{Attention Mechanism}
Attention mechanism has been widely used in several transformer-based models. It has been the core part to learn long term information in terms of feature extraction. Attention in transformer model has been used in many tasks including object recognition \cite{heo2022occlusion}, image classification \cite{wang2021not}, image super-resolution \cite{lu2022transformer}, image translation etc. Transformers utilize scaled dot-product and multi-head attention mechanisms, enhancing computational efficiency and capturing more complex data relationships \cite{fateh2024advancing}. These mechanisms have been important in allowing transformers to outperform traditional convolutional networks in various visual learning tasks. Among all attention mechanisms, channel attention modules focus on enhancing significant feature channels while suppressing less relevant ones, primarily introduced by SENet \cite{hu2018squeeze}. They use a two-step process: squeezing global spatial information and exciting inter-channel dependencies. On the other hand, spatial attention mechanisms target specific regions within an image, highlighting important areas and diminishing background noise \cite{zhu2019empirical}. They generate spatial attention maps that assign weights to different image locations, improving feature expression. Another approach, branch attention mechanisms dynamically select and emphasize different network branches based on input data, optimizing feature learning \cite{fukui2019attention}. However, all of these methods are not lightweight and do not cover deeper and longer feature dependencies. 


\subsection{Knowledge Distillation}

Most of the new ideas for distilling knowledge focus on compressing very large neural networks. The lightweight student networks can be used in applications such as visual recognition, speech recognition, and natural language processing, and they can be set up quickly. It can also be used for other things, like adversarial attacks~\cite{papernot2016distillation}, adding data~\cite{Yuan2019RevisitKD}, protecting data privacy and security, and more. The idea of knowledge distillation for model compression has been used to compress the training data, which is called dataset distillation. This process moves the knowledge from a large dataset into a small dataset to make it easier for deep models to train~\cite{bohdal2020flexible}.
In a recent study, Cheng \textit{et al.} measured how many visual concepts were extracted from the intermediate layers of a deep neural network in order to show how knowledge was boiled down~\cite{cheng2020explaining}. Risk bound, data efficiency, and imperfect teachers all played a part in how knowledge was distilled on a wide neural network~\cite{ji2020knowledge}. Knowledge distillation had also been used to make labels smoother, to check the accuracy of the teacher, and to figure out what the best shape for the output layer should be~\cite{tang2020understanding}. However, a recent study by Cho \textit{et al.} performed extensive experiments to see if knowledge distillation worked but results from the experiments suggested otherwise ~\cite{yang2020knowledge}. It was theorized that the poor performance of knowledge distillation was linked to the fact that a bigger model may not be a better teacher because it, albeit being a larger model, might not have enough space for performing all the intended tasks~\cite{mirzadeh2020improved}. 


%%%%%%%%%%%%%%%%%%%%%