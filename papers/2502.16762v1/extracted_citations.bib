@article{Yuan2019RevisitKD,
  title={Revisit Knowledge Distillation: a Teacher-free Framework},
  author={Li Yuan and Francis E. H. Tay and Guilin Li and Tao Wang and Jiashi Feng},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11723},
  url={https://api.semanticscholar.org/CorpusID:202889259}
}

@article{bohdal2020flexible,
  title={Flexible dataset distillation: Learn labels instead of images},
  author={Bohdal, Ondrej and Yang, Yongxin and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2006.08572},
  year={2020}
}

@article{bucilua2006model,
  title={Model compression, in proceedings of the 12 th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  author={Bucilua, C and Caruana, R and Niculescu-Mizil, A},
  journal={New York, NY, USA},
  year={2006}
}

@inproceedings{cheng2020explaining,
  title={Explaining knowledge distillation by quantifying the knowledge},
  author={Cheng, Xu and Rao, Zhefan and Chen, Yilan and Zhang, Quanshi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12925--12935},
  year={2020}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{fateh2024advancing,
  title={Advancing Multilingual Handwritten Numeral Recognition With Attention-Driven Transfer Learning},
  author={Fateh, Amirreza and Birgani, Reza Tahmasbi and Fateh, Mansoor and Abolghasemi, Vahid},
  journal={IEEE Access},
  volume={12},
  pages={41381--41395},
  year={2024},
  publisher={IEEE}
}

@inproceedings{fukui2019attention,
  title={Attention branch network: Learning of attention mechanism for visual explanation},
  author={Fukui, Hiroshi and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10705--10714},
  year={2019}
}

@article{han2022survey,
  title={A survey on vision transformer},
  author={Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

@article{heo2022occlusion,
  title={Occlusion-aware spatial attention transformer for occluded object recognition},
  author={Heo, Jiseong and Wang, Yooseung and Park, Jihun},
  journal={Pattern Recognition Letters},
  volume={159},
  pages={70--76},
  year={2022},
  publisher={Elsevier}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@inproceedings{hu2018squeeze,
  title={Squeeze-and-excitation networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7132--7141},
  year={2018}
}

@article{ji2020knowledge,
  title={Knowledge distillation in wide neural networks: Risk bound, data efficiency and imperfect teacher},
  author={Ji, Guangda and Zhu, Zhanxing},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20823--20833},
  year={2020}
}

@inproceedings{lu2022transformer,
  title={Transformer for single image super-resolution},
  author={Lu, Zhisheng and Li, Juncheng and Liu, Hong and Huang, Chaoyan and Zhang, Linlin and Zeng, Tieyong},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={457--466},
  year={2022}
}

@inproceedings{mirzadeh2020improved,
  title={Improved knowledge distillation via teacher assistant},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5191--5198},
  year={2020}
}

@inproceedings{papernot2016distillation,
  title={Distillation as a defense to adversarial perturbations against deep neural networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  booktitle={2016 IEEE symposium on security and privacy (SP)},
  pages={582--597},
  year={2016},
  organization={IEEE}
}

@article{tang2020understanding,
  title={Understanding and improving knowledge distillation},
  author={Tang, Jiaxi and Shivanna, Rakesh and Zhao, Zhe and Lin, Dong and Singh, Anima and Chi, Ed H and Jain, Sagar},
  journal={arXiv preprint arXiv:2002.03532},
  year={2020}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{urner2011access,
  title={Access to unlabeled data can speed up prediction time},
  author={Urner, Ruth and Shalev-Shwartz, Shai and Ben-David, Shai},
  booktitle={ICML},
  year={2011}
}

@article{wang2021not,
  title={Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition},
  author={Wang, Yulin and Huang, Rui and Song, Shiji and Huang, Zeyi and Huang, Gao},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11960--11973},
  year={2021}
}

@article{yang2020knowledge,
  title={Knowledge distillation via adaptive instance normalization},
  author={Yang, Jing and Martinez, Brais and Bulat, Adrian and Tzimiropoulos, Georgios},
  journal={arXiv preprint arXiv:2003.04289},
  year={2020}
}

@inproceedings{zheng2021rethinking,
  title={Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
  author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6881--6890},
  year={2021}
}

@inproceedings{zhu2019empirical,
  title={An empirical study of spatial attention mechanisms in deep networks},
  author={Zhu, Xizhou and Cheng, Dazhi and Zhang, Zheng and Lin, Stephen and Dai, Jifeng},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6688--6697},
  year={2019}
}

@article{zhu2020deformable,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  journal={arXiv preprint arXiv:2010.04159},
  year={2020}
}

