\documentclass[11pt]{article} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Fonts and margins
% \usepackage{geometry}  % set the margins to 1in on all sides
% \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[margin=1in]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\renewcommand{\baselinestretch}{1.2}

%% Figures, tables and lists
\usepackage[dvipsnames]{xcolor}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{longtable} 
\usepackage{multirow}
\usepackage{listings}
\usepackage{makecell}
\usepackage{array}
\usepackage{float}
\usepackage{dsfont}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{enumitem}
\usepackage{lipsum} % for generating filler text
\usepackage{titlesec}

%% Math
\usepackage{amsmath,amssymb,amsfonts,amsthm,bbm}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{mathabx}

%% References and author info 
\mathtoolsset{showonlyrefs}
\usepackage{natbib}
\usepackage{authblk}
\usepackage{todonotes}
\usepackage{xr-hyper}
\usepackage[
  colorlinks,
  citecolor=blue,
  linkcolor=red,
  anchorcolor=red,
  urlcolor=blue
]{hyperref}

\input{macro}

 
\usepackage{pifont}
\usepackage{tikz}
\newcount\Comments
\Comments=1 
\newcommand{\kibitz}[2]{\ifnum\Comments=1{\textcolor{#1}{\textsf{\footnotesize #2}}}\fi}
\newcommand{\bahar}[1]{\kibitz{blue}{[BT: #1]}}
\definecolor{portlandorange}{rgb}{1.0, 0.35, 0.21}
\definecolor{aurometalsaurus}{rgb}{0.43, 0.5, 0.5}
\definecolor{blue(munsell)}{rgb}{0.0, 0.5, 0.69}
\newcommand{\yg}[1]{\kibitz{blue(munsell)}{[YG: #1]}}

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand\thealgorithm{\Alph{algorithm}}
\hypersetup{colorlinks, breaklinks,
linkcolor={[RGB]{174,13,104}},
citecolor={[rgb]{0.0, 0.5, 0.69}},
urlcolor={[rgb]{0.6, 0.13, 0.12}}}



\newcommand{\delbt}[1]{{\color{RawSienna} [{\sffamily\bfseries DELETED BT:} #1]}}
\newcommand{\notebt}[1]{{\color{RawSienna} [{\sffamily\bfseries NOTE BT  :} {\em #1}]}}
\newcommand{\newbt}[1]{{\color{RawSienna} #1}}

\newcommand{\delyg}[1]{{\color{blue(munsell)} [{\sffamily\bfseries DELETED YG:} #1]}}
\newcommand{\noteyg}[1]{{\color{blue(munsell)} [{\sffamily\bfseries NOTE YG  :} {\em #1}]}}
\newcommand{\newyg}[1]{{\color{blue(munsell)} #1}}

\setcitestyle{authoryear}

\title{Statistical Equilibrium of Optimistic Beliefs} 


\author[$\dagger$]{Yu Gui}
\author[$\ddagger$]{Bahar Ta{\c{s}}kesen}

{\affil[$\dagger$]{\small Department of Statistics, University of Chicago\authorcr \href{mailto:yugui@uchicago.edu}{\texttt{yugui@uchicago.edu}}}
\affil[$\ddagger$]{\small Booth School of Business, University of Chicago \authorcr \href{mailto:bahar.taskesen@chicagobooth.edu}{\texttt{bahar.taskesen@chicagobooth.edu}}}
}


\begin{document}

\maketitle

\begin{abstract}
We introduce the Statistical Equilibrium of Optimistic Beliefs (SE-OB) for the mixed extension of finite normal-form games, drawing insights from discrete choice theory. Departing from the conventional best responders of Nash equilibrium and the better responders of Quantal Response Equilibrium (QRE), we reconceptualize player behavior as that of \textit{optimistic} better responders. In this setting, the players assume that their
expected payoffs are subject to random perturbations, and they form optimistic beliefs by
selecting the distribution of these perturbations that maximizes their highest anticipated payoffs among their belief sets. In doing so, SE-OB subsumes and extends the existing equilibria concepts.
The player's view of the existence of perturbations in their payoffs reflects an inherent risk sensitivity, and thus, each player is equipped with a risk-preference function for every action.
We demonstrate
that every Nash equilibrium of a game, where expected payoffs are regularized with the risk-preference functions of the players, naturally corresponds
to an SE-OB in the original game, provided that the belief sets coincide with the feasible set of
a multi-marginal optimal transport problem, where the marginals are determined by these risk-preference functions.
Building on this connection, we propose an algorithm for repeated games to be played by risk-sensitive players operating under optimistic beliefs when only zeroth-order feedback is available. We prove that, under appropriate conditions, the algorithm converges to an SE-OB. Our convergence analysis offers key insights into the strategic behaviors for equilibrium attainment: specifically, a player’s risk sensitivity enhances equilibrium stability, while forming optimistic beliefs in the face of ambiguity helps to mitigate overly aggressive strategies over time.
As a byproduct, our approach delivers the first generic convergent algorithm for general-form structural QRE beyond the classical logit-QRE.

\end{abstract}


\section{Introduction}

Every interaction among entities unfolds as a game—where each move carries the potential to reshape the \textit{status quo} of others. When played among people, these games define social interactions, where individuals make decisions, receive payoffs based on their actions, and continuously refine their strategies by learning from past experiences.  
On a broader scale, when nations engage in such strategic behavior, the resulting moves shape political alliances and international policies, while in nature, similar interactions drive the evolutionary success of species. Game theory offers a rigorous mathematical framework for illuminating the strategic behaviors underlying these choices. Its applications span a wide array of domains—from economics and political science to modern developments in artificial intelligence and multi-agent reinforcement learning. The study and advancement of game theory deepens our understanding of the underlying mechanisms governing social dynamics, equips us with strategies for conflict resolution, and provides the computational tools necessary to model and achieve stable outcomes in complex, multi-agent systems.

One of the central concepts in game theory is the Nash equilibrium, defined as a strategy profile in which each player's strategy is the best response to the strategies of all other players (\textit{i.e.}, no player can unilaterally improve their payoff by deviating) \citep{nash1950equilibrium}. In finite normal-form games, Nash demonstrated that an equilibrium always exists in its mixed extension \citep{nash1950equilibrium, nash1951}—a result later generalized to concave games \citep{ref:rosen1965existence}.
Although the universal existence of Nash equilibria is theoretically reassuring, devising a computationally efficient procedure for identifying one in any finite game remains a significant challenge. 
Moreover, even if a Nash equilibrium were computed, empirical investigations have consistently documented systematic deviations from its predictions. Real-world behavior—particularly among human players—often departs from the rationality assumptions underlying Nash equilibrium analysis. These limitations have spurred a vibrant body of research aimed at analyzing and extending the Nash framework from multiple perspectives. In the remainder of this section, we review the extensive literature on game-theoretic approaches that are most pertinent to this study.



\vspace{0.5em}

\noindent\textbf{Computational Complexity of Nash equilibrium.}
Computing a Nash equilibrium in a normal-form game is inherently complex, primarily because of the combinatorial explosion of potential strategies and the intricate interdependencies among players’ choices. While Nash’s theorem assures us that an equilibrium exists, subsequent research has demonstrated that finding this equilibrium in finite normal-form games is PPAD-complete \citep{daskalakis2009complexity,chen2009settling}. 
This classification implies that, in the worst case, no polynomial-time algorithm is likely to exist for computing a Nash equilibrium.
As a result, researchers have shifted their focus toward the approximability of Nash equilibria, seeking to compute an equilibrium that meets a predefined tolerance level rather than an exact solution. However, even this relaxed goal has proven to be computationally demanding \citep{daskalakis2013complexity, rubinstein2015inapproximability}, that is, even approximating a Nash equilibrium remains PPAD-hard. This persistent intractability, along with the potential tradeoff between the relatively high payoffs associated with Nash equilibria and the substantial computational resources required to compute them, raises important questions about whether players will always seek to reach such equilibria in practice \citep{daskalakis2013complexity, hart2010long}.


The independent nature of players' action choices introduces additional degrees of freedom, significantly contributing to the computational complexity of strategic dynamics. Relaxing the independence assumption by permitting correlated actions leads to equilibrium concepts that are computationally tractable. Notably, coarse correlated equilibrium (CCE) \citep{moulin1978strategically} and correlated equilibrium (CE) \citep{aumann1987correlated} exemplify such relaxations. However, because these concepts depend on the existence of a public signal and the coordination of players' actions, their applicability is limited in decentralized settings or in scenarios where privacy concerns restrict such coordination.


Drawing inspiration from smoothed analysis \citep{spielman2009smoothed}, \citet{daskalakis2023smooth} introduced the concept of a smooth Nash equilibrium. In this framework, players' strategies are confined to smooth distributions, meaning that the probability assigned to any pure strategy is bounded away from one. This constraint effectively limits the use of pure strategies. Moreover, under a given smoothness parameter and assuming independent players, \citep{daskalakis2023smooth} demonstrate the existence of a polynomial-time algorithm\footnote{Polynomial in the number of players and the number of actions of each player} for approximating the smooth Nash equilibrium in normal-form games, thereby substantially alleviating the computational challenges associated with computing a Nash equilibrium.
Another strategy to address these computational challenges is to incorporate risk aversion into players' decision-making. Rather than merely maximizing expected payoffs, risk-averse players optimize a specific risk measure, resulting in risk-adjusted equilibria. When players are constrained to bounded strategies, these equilibria are computationally tractable in all $n$-player matrix games and finite-horizon Markov games \citep{mazumdar2024tractable}.



\vspace{0.5em}
\noindent \textbf{Learning Nash equilibrium in repeated games.}
In its most natural form, a game is not a one-shot, isolated event but a dynamic process of continuous interaction, where players iteratively adapt their strategies based on the observed behavior of their opponents. This adaptive behavior is captured by the concept of fictitious play \citep{brown1951iterative,robinson1951iterative,hofbauer2002global}, in which each player chooses an action that is a \textit{best response} to the empirical distribution of the opponents' play until that time. First, \citet{robinson1951iterative} demonstrated that, in two-player zero-sum games, the joint distribution of action frequencies converges to a Nash equilibrium. However, \citet{ref:shapley1963some} later showed that this convergence result does not extend beyond zero-sum games.

In repeated games, players experience regret, defined as the cumulative difference between the payoff of the strategy they followed and that of the best fixed action in hindsight. Notably, fictitious play is not Hannan consistent—that is, it does not guarantee no-regret performance. In contrast, when players adopt no-regret strategies in repeated games, \citet{hart2000simple} proved that the empirical frequency of their strategies converges to a CCE.
Although this result is promising, the resulting CCE may fail to satisfy even the most basic rationalizability axioms \citep{viossat2013no}. Moreover, typical no-regret convergence results address the empirical frequency over the entire horizon rather than the strategies played in the final iteration.
The study of last-iterate convergence in repeated games seeks to address this shortcoming. Specifically, in smooth concave games that admit a stable Nash equilibrium, \citet{mertikopoulos2019learning} and \citet{ref:hsieh2021adaptive} show that strategies generated by a variant of the dual averaging algorithm \citep{nesterov2009primal} converge to the Nash equilibrium. Similarly, in  monotone \citep{ref:rosen1965existence} and smooth games, \citet{golowich2020tight} demonstrate that an optimistic gradient-based algorithm, augmented with an additional term, converges to a Nash equilibrium. Unfortunately, normal-form games do not invariably exhibit the desirable properties of smoothness or monotonicity, so these convergence guarantees do not directly extend to mixed extensions of general-form games.


\vspace{0.5em}
\noindent\textbf{Behavioral game theory from economic perspectives.}
Extending the classical Nash equilibrium framework, a significant line of research integrates economic insights to account for systematic deviations from perfect rationality observed in actual player behavior.
The classical Nash paradigm assumes that individuals are perfectly rational, selecting actions that maximize their payoffs based on precise expectations of others' actions. However, empirical evidence from laboratory experiments often reveals systematic deviations from these theoretical assumptions. In two-player zero-sum games, for example, a series of studies
\citep{lieberman1960human,brayer1964experimental,o1987nonmetric,brown1990testing,rapoport1992mixed} has tested minimax theory.
As noted by \citep{o1987nonmetric}, although the average frequency of moves tends to align with minimax predictions, significant subject-to-subject variability suggests that individual players frequently deviate from minimax play—likely due to limitations in information processing. 
{Similarly, \citep{nagel1995unraveling} demonstrates that players’ mental processes can be influenced by the actions of others in earlier rounds, further contributing to deviations from the predictions of Nash equilibrium.} 
Moreover, \citep{goeree2001ten} provides laboratory data showing that changes in the payoff structure can lead to departures from theoretical predictions.
In response to these empirical observations, behavioral game theory has evolved by incorporating factors such as social cooperation, individual incentives, group decision-making, and subjective beliefs, leading to the development of various alternative models and equilibrium concepts \citep{camerer1997progress,gintis2005behavioral}. 

Among these, the quantal response equilibrium (QRE) proposed by \citep{mckelvey1995quantal} has emerged as a central concept for modeling bounded rationality. QRE is a statistical parametric model in which players’ stochastic choice probabilities are determined as a fixed point in an infinite-dimensional distribution space, and it has been successfully applied in a wide range of scenarios. Nevertheless, the effectiveness of QRE is contingent upon accurate statistical modeling of players' deviations from perfect rationality, rendering it sensitive to model misspecification in practice. Furthermore, because QRE typically assumes homogeneity in statistical uncertainty, alternative approaches \citep{stahl1994experimental,camerer2004cognitive,friedman2005random} have sought to incorporate heterogeneity in players’ choices and beliefs. Recognizing the restrictive assumptions of perfect payoff expectations and optimization in Nash games, \citep{goeree2004model} introduced the notion of noisy introspection, and \citep{rogers2009heterogeneous} further extended the QRE framework by incorporating individual payoff responsiveness of the players.   

To provide a more general foundation for behavioral game theory, \citep{goeree2021m} proposed a set of minimal behavioral assumptions. These include relaxing the strict correspondence between choice frequencies and expected payoffs to merely require a monotonic relationship, as well as assuming that observed choices are unbiased relative to the underlying choice distributions. Building on this framework, the notion of $M$-equilibrium \citep{goeree2021m} extends Nash equilibrium by allowing for set-valued predictions of players’ choices and beliefs. This approach offers a robust, parameter-free, and detail-free criterion that captures empirical regularities. Moreover, $M$-equilibrium serves as a meta-theory, unifying equilibria derived from both strategy perturbation \citep{selten1988reexamination} and payoff perturbation \citep{harsanyi1973games}, and it encompasses all quantal response equilibria. However, despite its generality, the $M$-equilibrium profile set may be overly permissive—encompassing too many actions to be realistic in practice \citep{goeree2023s}—thereby motivating the search for a more robust equilibrium concept that balances behavioral assumptions with the informativeness of predicted actions.

An alternative research avenue extends the Nash equilibrium concept from a Bayesian perspective to settings with incomplete information regarding payoffs. Harsanyi's seminal work \citep{harsanyi1967games} introduced Bayesian games, where the actual payoffs are not known to the players, but the distributional information is assumed to be well specified. This framework was later generalized through the concept of ex post equilibrium \citep{cremer1985optimal,holmstrom1983efficient}, a distribution-free solution concept whose existence is not always guaranteed. Additionally, the robust Nash equilibrium proposed by \citet{aghassi2006robust} considers the worst-case payoff over an uncertainty set. More recently, the framework of Bayesian games has been extended to a distributionally robust setting \citep{liu2024bayesian,liu2018distributionally,loizou2016distributionally,loizou2015distributionally}, wherein the analysis focuses on the worst-case distribution of payoffs over a prescribed ambiguity set.



\vspace{0.5em}
\noindent 
\textbf{Our Contributions.} In this paper, we study mixed extensions of {multi-player} finite normal-form games and make the following key contributions:
\begin{enumerate}[label=(\roman*)]

    \item We introduce a new equilibrium concept—the \emph{Statistical Equilibrium of Optimistic Beliefs (SE-OB)}—that relaxes the parametric constraints of traditional structural QRE through insights from discrete choice theory. In particular, we model players' choice behavior by positing that their expected payoffs are subject to perturbations and that they adopt an optimistic bias by selecting the perturbation distribution that maximizes their anticipated maximally perturbed payoffs.
    
    \item Players' assumption that their expected payoffs are subject to perturbations reflects an inherent risk sensitivity. Consequently, each player is endowed with a risk-preference function for every available action. We demonstrate that any strategy profile forming a Nash equilibrium in a game—where expected payoffs are regularized by these risk-preference functions—also constitutes an SE-OB in the original game, provided that the players' belief sets are defined as the feasible sets of multi-marginal optimal transport with marginals corresponding to these risk-preference functions. 
    
    \item Moreover, by leveraging techniques from online optimization, we establish sufficient conditions on these belief sets to ensure the stability of the Nash equilibrium in the smooth-payoff game. Notably, these conditions are required to be satisfied exclusively in terms of risk-preference functions and when the underlying payoff functions are bounded, are independent of their values.
    \item We propose an algorithm for repeated games in which players receive zeroth order feedback, make estimates adjusted by their risk-preference function, and consistently adopt optimistic beliefs. Finally, we prove that, under appropriate conditions, this algorithm converges to an SE-OB. Notably, our approach yields the first convergent algorithm for general structural QRE beyond the classical logit-QRE.

\end{enumerate}


\paragraph{Paper organization.}
The rest of this paper unfolds as follows. In Section~\ref{sec:problem}, we introduce the mixed extension of normal-form games. In Section~\ref{sec:equilibrium-of-fixed-beliefs}, we present equilibria based on fixed beliefs of the players. Next, Section~\ref{sec:SE-OB} formally introduces the statistical equilibrium of optimistic beliefs, along with an analysis of its properties and its connections to other equilibrium concepts. In Section~\ref{sec:learning}, we present our algorithm for repeated games to be played by risk-sensitive players with optimistic beliefs, and Section~\ref{sec:convergence} is devoted to its convergence analysis. Finally, Section~\ref{sec:discuss} discusses the limitations of our approach and outlines directions for future research.

\paragraph{Notation.}
For any $n \in \mathbb Z_+$, we set $[N] = \{1, \ldots, n\}$.
For $\bs y_i \in \R^N $ for all $i \in [M]$, if their tuple is denoted by $\bs Y = (\bs y_1, \ldots,\bs y_M)$, then 
$\bs Y_{-i}$ denotes the tuple $(\bs y_1, \ldots, \bs y_{i-1}, \bs y_{i+1}, \ldots, \bs y_M)$.
Furthermore, for an element $\bs y_i'$, $(\bs y'_i, \bs y_{-i})$ denotes the tuple $(\bs y_1, \ldots, \bs y_{i-1}, \bs y_i', \bs y_{i+1}, \ldots, \bs y_M)$.
For $N \in \mathbb N$, $\Delta^N = \{x_i \in \R_+^N : \sum_{i=1}^N x_i =1 \}$ denotes the set of probability distributions over $\R^N$. For $i \in [N]$, $\bs e_i \in \Delta^N$ denotes the $i$th basis vector. $\| \cdot\|$ denotes the operator norm.  
Given a finite-dimensional vector space~$\mc V$ with norm $\|\cdot \|$, and a given $x \in \mc V$, the tangent cone $\textrm{TC}(x)$ is defined as the closure of the set of all rays emanating from $x$ and intersecting $\mc V$ on at least one other point.


\section{Problem Definition}\label{sec:problem}
Throughout this paper, we focus on games with $M \in \mathbb Z_{++}$ players and $N \in \mathbb Z_{++}$ actions. During the play, each player~$j \in [M]$ plays an action~$a_j\in [N]$, and receives payoff $ u^a_j(a_1, \ldots, a_M)$, where $u_j^a: [N]^M\to [0,1]$ denotes the payoff functions taking actions in $[N]$ as inputs. Then, a \textit{normal} game with $M$ players and $N$ actions is specified by the tuple of payoff mappings~$(u_1^a, \ldots, u_M^a)$ \citep{nash1950equilibrium}. 
A mixed strategy~$\bs p_j \in \Delta^N$ of player~$j$ is a probability distribution over~$[N]$. 
Given mixed strategies~$\bs p_j$, ${j \in [M]}$, the corresponding strategy profile is the tuple~$\bs P = (\bs p_1, \ldots, \bs p_M) \in (\Delta^N)^M$. 
For a given strategy profile $\bs P$ $\in (\Delta^N)^M$, the expected payoff received by player~$j \in [M]$ is the expected value of the payoff function when the random actions of the players follow probability distributions~$\otimes_{j=1}^M\bs p_j$, and is defined as 
\[u_j(\bs P) = \EE_{a_1 \sim \bs p_1, \ldots, a_M \sim \bs p_M}[u_j^a( a_1, \ldots, a_M)]\]

For a given strategy profile $\bs P$, we write $u_{j}(\bs p'_j; \bs P_{-j})$ for the expected payoff of player $j$ when it plays $a_j \sim \bs p'_j$ and others play according to $\bs P_{-j}$. In particular, $u_j(\bs p_j' ;\bs P_{-j})$ is defined as
\[u_j(\bs p_j'; \bs P_{-j}) = \EE_{ a_1 \sim\bs p_1, \ldots,  a_j \sim \bs p_j', \ldots,  a_M \sim \bs p_M } [u_j^a( a_1, \ldots, a_{j-1},  a_j, a_{j+1}, \ldots,  a_M)]. \]
We denote by~$\mathcal{G}([M], \Delta^N, (u_j)_{j \in [M]})$ the mixed extension of a finite game with $M$ players, each having $N$ available actions and expected payoffs given by $(u_j)_{j \in [M]}$.

\section{Equilibria of Fixed Beliefs}\label{sec:qre}
\label{sec:equilibrium-of-fixed-beliefs}
The central concept of game theory is the Nash equilibrium~\eqref{eq:NE}, and we present its formal definition below for a $M$-player, $N$-action normal-form game with expected payoffs given by~$(u_j)_{j\in[M]}$.
\begin{definition}[Nash equilibrium]
A strategy profile $\bs P^* = (\bs p_1^*, \ldots, \bs p_M^*) \in (\Delta^N)^M$ is a Nash equilibrium of the game~$\mc G([M], \Delta^N, (u_j)_{j\in[M]})$ if, for each $j \in [M]$,
\begin{align}
\label{eq:NE}
    \max\limits_{\bs p \in \Delta^N }\; u_{j}(\bs p;\bs P^*_{-j}) \leq u_j(\bs p_j^*; \bs P^*_{-j}). % \leq \varepsilon.
    \tag{\text{NE}}
\end{align}
\end{definition}
The set of Nash equilibria for the game $\mathcal{G}([M], \Delta^N, (u_j)_{j \in [M]})$ consists of all strategy profiles that satisfy equation~\eqref{eq:NE}. A Nash equilibrium in non-cooperative games is grounded in three fundamental principles that collectively eliminate errors in the players' decisions and beliefs:

\begin{itemize}[leftmargin=5mm]
\item[\ding{101}] \textit{Beliefs}: Each player's behavior is shaped by their expectations about how other players will act. 
\item[\ding{101}] \textit{Optimal Responses}: Given these expectations, every player's choice is the best possible response.
\item[\ding{101}] \textit{Perfect Foresight}: Players' expectations of others' behaviors are accurate in a probabilistic sense.
\end{itemize}
When these three conditions are met, they create an internally consistent model of behavior. This consistency leads to a stable equilibrium distribution of action profiles, where no player has an incentive to deviate unilaterally. However, human decision-making is far from flawless. Unlike the precise predictions of traditional game theory, real-world behavior is often messy, unpredictable, and prone to error. Indeed, the foundational assumptions of standard game theory frequently fail to align with data from laboratory experiments, revealing systematic and significant deviations that challenge its core principles \citep{lieberman1960human,brayer1964experimental,o1987nonmetric,brown1990testing,rapoport1992mixed}.


In response to these empirical discrepancies, the Quantal Response Equilibrium (QRE) framework was developed~\citep{mckelvey1995quantal} by using standard statistical models for quantal choice in a game theoretic setting. 

QRE relaxes the core assumption of traditional Nash equilibrium that individual choice behavior is always optimal by allowing players to make mistakes in their decisions. In QRE, the expected payoffs derived from players' expectations may be influenced by ``noise'', which can result in suboptimal decisions. In this framework, even if the first and third principles of the game still hold, players must account for the fact that others do not always optimize perfectly. This introduction of noise fundamentally alters the dynamics of behavior within the game, and thus its equilibrium. 
In (structural) QRE inspired by the additive random payoff models~\citep{ref:mcfadden1976quantal}, each player's expected payoff for each action is perturbed with some random noise $\bs \xi \sim \mu$. Specifically, the disturbed expected payoff of player~$j$ for some distribution $\mu_j \in \mc P(\R^N)$, is then in the form of $u_j(\bs e_i; \bs P_{-j}) + \xi_i$.
The assumed choice behavior is that each player chooses action~$i$, when $u_j(\bs e_i; \bs P_{-j}) + \xi_i \geq u_j(\bs e_{i'}; \bs P_{-j}) + \xi_{i'}$ for all $i' \in [N]$. 
This behavior of choice induces a distribution over each player's possible actions. Specifically, the probability of player~$j$ selecting action~$i$ is denoted by
\[\tau_{i}\left(\mu_j; (u_j( \bs e_i; \bs P_{-j}))_{i \in [N]}\right) = \text{Pr}_{\bs \xi \sim \mu_j} \left[i \in \argmax\limits_{k \in [N]} u_j(\bs e_k; \bs P_{-j}) + \xi_k\right].\]
Given the choice behavior of the players, we define the quantal response function induced by distribution $\mu \in \mc P(\R^N)$ evaluated at the expected payoff vector $\bs u \in \R^N$ as 
{\[\bs \tau(\mu; \bs u) = \left(\tau_{ 1}(\mu; \bs u), \ldots, \tau_{N}(\mu; \bs u)\right).\]
}
QRE is a stable state where each player optimizes their decisions as best as possible, given their behavioral limitations, fully acknowledging that other players are doing the same within their own constraints. 
Consequently, QRE models players as \textit{better responders} rather than just best responders. While quantal responders are more likely to choose strategies with higher expected payoffs, they do not always select the strategy with the absolute highest expected payoff due to bounded rationality.
In what follows, we formally define the quantal response equilibrium.
\begin{definition}[Quantal Response Equilibrium]
\label{def:aqre}
For given $\mu_j \in \mc P(\R^N)$, $j\in [M]$, a strategy profile $\bs P^* = (\bs p^*_1, \ldots, \bs p^*_M)$ is a \textit{quantal response equilibrium} of the game $\mathcal G([M], \Delta^N, (u_j)_{j\in [M]})$ if
 \[\bs p_j^* = \bs\tau\left(\mu_j; (u_j( \bs e_i; \bs P^*_{-j}))_{i \in [N]} \right) \quad \forall j \in [M].                 \]
\end{definition}
 QRE is a fixed point of the quantal response functions, analogous to how the Nash Equilibrium is a fixed point of the best response functions. Specifically, each player's choice behavior incorporates an inherent random component.
Therefore, QRE serves as a statistical generalization of Nash equilibrium, with Nash equilibrium representing the limiting case where no decision errors occur.
Among the various formulations of QRE, the logit-QRE is the most widely used.

\begin{example}[Logit-QRE~\citep{mckelvey1995quantal}]
    If~$\mu_j = \nu$ for all $j \in [M]$ , $\nu=\otimes_{i=1}^N \nu_i$ and if~$\nu_i\in\mathcal{P}(\R)$ is a Gumbel distribution with zero mean and variance $\pi^2\eta^2 / 6$ for some $\eta>0$, then the quantal response functions are available in closed form and are equivalent to the choice probabilities in the celebrated multinomial logit model in discrete choice theory (see \cite[Theorem~5.2]{mcfadden1981econometric}), that is, 
    \begin{equation} \tau_{i}(\mu_j;\bs u)  ={\exp\left(\frac{u_i}{\eta}\right)}\left /{\displaystyle\sum\limits_{k=1}^N \exp \left(\frac{u_k}{\eta}\right)
    }\right..
    \label{eq:choice-prop-sigmoid}
    \end{equation}
    \label{ex:logit-qre}
\end{example}


\paragraph{Computing QRE in repeated games.}
Developing online algorithms that converge to a game's quantal response equilibrium has attracted significant attention in the literature; however, many existing algorithms are specifically tailored to logit-QREs. It has been shown in \citep{fudenberg1998theory, hofbauer2002global} that regularizing expected payoffs with an entropy term yields a best-response mapping whose fixed point corresponds to the quantal response function defining the logit-QRE of the original game. Building on this notable relationship, \citep{cen2021fast,cen2023faster} demonstrate a linear-time convergence rate\footnote{Linear time convergence rate refers to convergence to an approximate QRE with tolerance $\varepsilon$ where the number of iterations scales with $\log (1/\varepsilon)$.} for achieving the logit-QRE in two-player zero-sum games. As an extension, \citep{cen2022independent} established finite-time convergence for multi-player potential games, wherein payoff functions can be expressed by a common potential function. More generally, to encompass multi-player normal-form games, \citep{sun2024linear} demonstrates that the natural policy gradient method—when coupled with sufficiently large regularization—achieves a linear-time convergence rate to the logit-QRE.
However, the natural policy gradient method is dependent on the form of logit-QRE, or equivalently entropy regularization, thus convergence results with other types of regularization remain unclear. 
Consequently, developing an algorithm with convergence guarantees for QREs under a broader class of noise distributions remains an open problem.


\section{Statistical Equilibrium of Optimistic Beliefs}\label{sec:SE-OB}
We now introduce the Statistical Equilibrium of Optimistic Beliefs (SE-OB) as a generalization of both QRE and Nash equilibrium. Unlike standard QRE—where the quantal response functions are specified exogenously—our approach relaxes this assumption by allowing each player to select their belief from a prescribed belief set. 
More precisely, given an expected payoff vector~$\bs{u} \in \R^N$ and a prescribed belief set $\mc{B} \subseteq \mc{P}(\R^N)$, each player forms beliefs that they will receive the following expected payoff:
\begin{equation}
 \tilde u(\bs u ; \mc B) = \sup\limits_{\mu \in \mc B} \EE_{\bs \xi \sim \mu}\left[\max\limits_{i \in [N]} u_i+ \xi_i\right],    
 \label{eq:opt-exp-payoff}
\end{equation}
where $\bs \xi \in \R^N$ represents a random vector of perturbations governed by a Borel probability measure $\mu $ from within the belief set $\mc B$. Subsequently, each player responds stochastically in accordance with their optimally chosen belief. When all players adopt this strategy, their collective actions culminate in a statistical equilibrium state, which we refer to as SE-OB, formally defined as follows.


\begin{definition}[Statistical Equilibrium of Optimistic Beliefs]
    For given belief sets $\mc B_j \subseteq P(\R^N)$, $j \in [M]$, a strategy profile $\bs P^* = (\bs p_1^*, \ldots, \bs p_M^*)$ forms an SE-OB of the game~$\mc G([M], \Delta^N, (u_j)_{j \in [M]})$ if there exists $\mu_j^*\in \mathcal B_j$ for each $ j \in [M]$ such that $\bs p_j^* = \bs \tau(\mu_j^*; (u_j( \bs e_i; \bs P_{-j}^*))_{i \in [N]})$ satisfying
    \begin{equation}
      \EE_{\bs \xi \sim \mu}\left[ \max\limits_{i \in [N]} u_j(\bs e_i; \bs P_{-j}^*) + \xi_i \right] \leq  \EE_{\bs \xi \sim \mu^*_j}\left[ \max\limits_{i \in [N]} u_j(\bs e_i; \bs P_{-j}^*) + \xi_i \right] \quad \forall \mu \in \mc B_j.
        \label{eq:b-done}
        \tag{SE-OB}
    \end{equation}
\end{definition}


In QRE, the quantal response functions are specified exogenously, yielding a parametric formulation of statistical equilibrium. Drawing inspiration from the semi-parametric extension of random payoff models~\citep{natarajan2009persistency}, SE-OB emerges as a semi-parametric generalization of QRE. 
Remarkably, by appropriately selecting the players’ belief sets, SE-OB seamlessly reduces to either QRE or Nash equilibrium, thereby unifying and extending these established concepts. In essence, the structure of the belief sets not only drives the dynamics of the game but also determines the eventual equilibrium outcome. 
\subsection{Singleton Belief Sets}
When the belief set is a singleton, then SE-OB corresponds to the equilibrium of fixed beliefs discussed in Section~\ref{sec:equilibrium-of-fixed-beliefs}.
Note that if each $\mc B_j$ is a singleton that contains only the Dirac distribution $\delta_{b \cdot \bs 1}$ at $ b \cdot \bs 1$ for some $b \in \R$, then a strategy profile satisfies \eqref{eq:b-done} of a game if and only if it satisfies \eqref{eq:NE} of the same game. This perspective is formalized below.
\begin{proposition}
    For some $b\in \R$, if $\mc B_j = \{\otimes_{i=1}^N \delta_{b \cdot \bs 1}\}$ for all $j\in[M]$, then $\bs P= (\bs p_1, \ldots, \bs p_M)$, forms an SE-OB of the game~$\mc G([M], \Delta^N, (u_j)_{j \in [M]})$ if and only if it satisfies~\eqref{eq:NE} of the same game.
    \label{prop:se-ob-ne}
\end{proposition}
{Similarly, if each belief set is a singleton containing a single probability measure, then any strategy profile that is an SE-OB for a given game automatically forms a QRE induced by that probability measure for the same game. More precisely, we have the following result.}
\begin{proposition}
   For some $\mu_j \in \mc P(\R^N)$ for all $j\in [M]$, if $\mc B_j = \{\mu_j\}$, then $\bs P = (\bs p_1, \ldots, \bs p_M)$ forms an SE-OB of the game~$\mc G([M], \Delta^N, (u_j)_{j \in [M]})$ if and only if it forms a QRE induced by $(\mu_j)_{j \in [M]}$ of the same game. 
\end{proposition}

\subsection{Marginal Belief Sets}
\label{sec:marginal-belief-sets}
We now investigate the class of belief sets with fixed marginal distributions of the form
\begin{equation}
\mathcal B=\Big\{\mu \in \mathcal P( \mathbb{R}^N ) \, : \, \mu( z_{i} \leq s )=F_{i} ( s ) \quad \forall s \in\mathbb{R},~ \forall i \in[N] \Big\},
\label{eq:marginal-belief-set}
\tag{MBS}
\end{equation}
where $F_{i}$ stands for the cumulative distribution function of the uncertain disturbance $\xi_{i}, \, i \in[N]$. 
Marginal belief sets~\eqref{eq:marginal-belief-set} fully determine the marginal distributions of the components of the random vector $\bs \xi $ while leaving their interdependence (\textit{i.e.}, the copula) entirely unconstrained. Often referred to as Fréchet ambiguity sets, they also coincide with the feasible set of a multi-marginal optimal transport problem~\citep{ref:abraham2017tomographic}. 
Below we denote by $F_{i}^{-1} : [ 0, 1 ] \to\mathbb{R}$ the (left) quantile function corresponding to $F_{i}$ , which is defined through
\[
F_{i}^{-1} ( t )=\inf \{s : F_{i} ( s ) \geq t \} \quad\forall t \in\mathbb{R}.
\]
{For the belief sets of the form~\eqref{eq:marginal-belief-set}, the following result shows that the expected payoffs optimistic players believe to receive~$\tilde{\bs u}$, in fact, admit an equivalent representation in the form of a regularized linear program over the probability simplex. This equivalence is formalized in the following lemma, originally developed for discrete choice models by \citet{natarajan2009persistency}.
}
\begin{lemma}{{\cite[Theorem~1]{natarajan2009persistency}}}
\label{lem:distributional-regularization}
If $\mathcal{B} \subseteq \mc P(\R^N)$ is a marginal belief set of the form~\eqref{eq:marginal-belief-set} and if the underlying cumulative distribution functions $F_i,~i \in[N]$, are continuous, then we have

\begin{equation}
\label{eq:frechet-reg-utility}
   \tilde u(\bs u; \mc B)= \max _{\bs p \in \Delta^N} \bs p^\top \bs u + \sum_{i=1}^N \int_{1-p_i}^1 F_i^{-1}(t) \diff t \quad \forall j \in [M].
\end{equation}


Additionally, if $F_i(s)$ is strictly increasing in $s$ whenever $F_i(s) \in (0, 1)$ for all $i \in [N]$, then the unique maximizer of the convex program on the right-hand-side of~\eqref{eq:frechet-reg-utility}, $\bs p\opt \in\Delta^N$, satisfy $\bs p\opt= \bs \tau(\mu\opt;\bs u)$, 
where $\mu\opt $ represents an optimizer of the problem~\eqref{eq:opt-exp-payoff}.
\end{lemma}
Lemma~\ref{lem:distributional-regularization} enables us to connect two distinct equilibrium criteria of two closely related games. Specifically, a strategy profile satisfying~\eqref{eq:NE} of a game when the respective expected payoffs are regularized forms an SE-OB of the game with non-regularized expected payoffs. 
For that purpose, we first define the \textit{smooth} expected payoffs as in the following. 
\begin{definition}[Smooth expected payoffs]
    For given expected payoff functions~$u_j : (\Delta^N)^M \to \R^N $, $j\in[M]$, and continuous cumulative distribution functions~$(F_{j,i})_{i \in [N]}$, $j \in [M]$, the smooth expected payoff functions are defined as 
    \begin{equation}
        \bar u_j\left(u_j(\bs p ; \bs P_{-j}) ; (F_{j, i})_{i \in [N]}\right) = u_j(\bs p, \bs P_{-j}) + \sum\limits_{i=1}^N \int_{1-p_i}^1 F_{j,i}^{-1}(t)\diff t \quad \forall j \in [M].
        \label{eq:smooth-exp-payoffs}
    \end{equation}
    \label{def:smooth-exp-ut}
\end{definition}
% \delbt{With the definitions above, we have the following theorem.}
We refer to the game with smooth expected payoffs as the \emph{smooth game}. The following lemma demonstrates that if the cumulative distribution functions $(F_{j,i})_{i \in [N]}$ for each $j \in [M]$ are continuous, then the smooth game is a concave game.
\begin{lemma}
If the cumulative distribution functions $(F_{j,i})_{i \in [N]}$ are continuous, then the game $\mc G([M], \Delta^N, (\bar u_j)_{j \in [M]})$ is a concave game, that is, $\bar u_j(u_j(\bs p_j, \bs P_{-j}); (F_{j,i})_{i \in [N]})$ is concave in $\bs p_j$ for all $\bs P_{-j} \in (\Delta^N)^{M-1}$ and $j\in[M]$.
\label{lem:concave-game}
\end{lemma}
We are now equipped to link the Nash equilibrium of the smooth game to an SE-OB of the original game.
\begin{theorem}
Suppose that for each $j\in[M]$, the belief set~$\mc B_{j}$ is of the form~\eqref{eq:marginal-belief-set} with $F_{j,i}(s)\in (0,1)$ that are continuous and strictly increasing in $s$ whenever $F_{j,i}(s)\in (0,1)$ for all $i \in [N]$. Then, any Nash equilibrium ${\bs P}^* \in (\Delta^N)^M$ of the game $\mathcal G([M],\Delta^N, (\bar u_{j})_{j\in[M]})$ also forms an SE-OB for the game~$\mathcal G([M], \Delta^N, (u_j)_{j\in [M]})$.
\label{thm:b-done-approx-ne}
\end{theorem}
By Lemma~\ref{lem:concave-game}, the smooth game is concave. Consequently, by \cite[Theorem 1]{ref:rosen1965existence}, its Nash equilibrium exists. Theorem~\ref{thm:b-done-approx-ne} then implies the existence of a solution to \eqref{eq:b-done} for the game $\mc G([M], \Delta^N, (u_j)_{j\in [M]})$.



Belief sets of the form~\eqref{eq:marginal-belief-set} may initially appear restrictive because they include only distributions with fixed marginal cumulative distribution functions $(F_{j,i})_{i \in [N]}$. However, as the following examples illustrate, choices of these marginal distributions offer considerable flexibility in designing both established and new statistical equilibria. For brevity, the derivations are omitted; they can be reconstructed following the examples presented in \cite[Section 3.3]{tacskesen2023semi}.
\begin{example}[Exponential distribution model]
\label{ex:exp-marginal}
    Suppose that $\mc B_j$, $j\in[M]$, is a marginal belief set with (shifted) exponential marginals of the form \eqref{eq:marginal-belief-set} induced by the following cumulative distribution functions
    \begin{align}\label{eg:exp}
    F_{j,i}(s) = \max\{0,  1 - \eta_{j, i}\exp(-s/\gamma_j - 1) \} \quad \forall i \in [N]
    \end{align}
    with $\eta_{j,i} \geq 0 $ and $\gamma_j > 0$, for all $j \in [M]$ and $i \in [N]$.
    Then, quantal response functions evaluated at the optimistic distributions that solves~\eqref{eq:opt-exp-payoff}
    are available in closed form and equivalent to \begin{equation}\tau_i (\mu_j\opt; \bs u ) = \frac{\eta_{j,i} \exp(u_i/ \gamma_j)}{ \sum\limits_{k=1}^N \eta_{j,k} \exp(u_k/\gamma_j)}.
    \label{eq:sigmoid}
    \end{equation}
    This example shows that logit-QRE is not only induced by singleton belief sets containing a generalized extreme value distribution (see Example~\ref{ex:logit-qre}) but also by marginal belief sets with exponential marginals.
\end{example}
\noindent The quantal response functions of Example~\ref{ex:exp-marginal} are known as softmax functions, which project a vector of real numbers to the probability simplex. By construction the projected vector always has full support, that is, $\tau_i(\mu\opt_j, \bs u) \neq 0$ for all $i \in [N]$ when $\mu_j\opt$ solves~\eqref{eq:opt-exp-payoff} for all $j \in [M]$. However, in applications with large output spaces, sparse probability distributions are often preferable because they filter out less relevant alternatives, reducing computational complexity and enhancing interpretability. To address this need, \citet{ref:martins2016softmax} introduces the sparsemax function, an alternative projection that truncates small probability values to zero.
\begin{definition}[Sparsemax~\citep{ref:martins2016softmax}] 
\label{def:sparsemax}
Given $\boldsymbol{u} \in \mathbb{R}^N$, let $\rho$ be a permutation of $[N]$ with $u_{\rho(1)} \geq u_{\rho(2)} \geq \cdots \geq u_{\rho(N)}$, for all $i \in [N]$, set
\[
k=\max \left\{j \in[N]: 2 N +{j \cdot u_{\rho(j)}} >\sum_{i=1}^j  u_{\rho(i)}\right\} \quad \text { and } \quad \kappa^{\star}=\frac{\left(\sum_{i=1}^k  u_{\rho(i)}\right)-2N}{k} .
\]
Then, $\text{sparsemax}(\bs u)=([u_i-\kappa^{\star}]_{+} / (2N))_{i \in[N]}$, where $[\cdot]_{+}=\max \{0, \cdot\}$ stands for the ramp function.
\end{definition}
\begin{example}[Uniform distribution model]\label{ex:uniform}
Suppose that each $\mc B_j$, $j \in [M]$, is a marginal belief set with a uniform belief set of the form \eqref{eq:marginal-belief-set}, induced by the cumulative distribution functions 
\[F_{j,i}(s) = \min\{1, \max\{0, 1 - N^{-1} (-s/(2\gamma_j) + 1/2) \}\} \quad \forall i \in [N]\]
with $\gamma_j >0$ for all $ j \in [M]$. In this case, the quantal response functions evaluated at the optimistic distributions~$\mu\opt_j$ that solves \eqref{eq:opt-exp-payoff} admit a closed-form expression
\[\bs \tau(\mu_j\opt; \bs u) = \text{sparsemax}(\bs u /\gamma_j) \quad \forall \bs u \in \R^N, \]
where the sparsemax operator is as defined in Definition~\ref{def:sparsemax}.
In fact, the sparsemax of an \(N\)-dimensional vector is precisely its Euclidean projection onto the probability simplex, and this projection admits a closed-form expression (see Definition~\ref{def:sparsemax}). Since the strategy profile \(\bs P^* = (\bs p^*_1, \ldots, \bs p^*_M)\) that satisfies \eqref{eq:b-done} is induced by quantal response functions evaluated at the optimistic distributions, the resulting equilibrium strategy profile is sparse, \textit{i.e.}, each \(\bs p_j^*\) contains many zero entries. 
This sparsity is crucial for computational tractability (see \cite[Corollary 3.3]{daskalakis2023smooth}). 
In particular, for some $\sigma \in (0,1)$, if $\gamma_j \geq (2/\sigma - N)^{-1} $, then $\tau_{i}(\mu_j\opt; \bs u) \in [0,(N\sigma)^{-1}]$ for all $i \in [N]$ and $j \in [M]$ (see Lemma~\ref{lem:uniform-dists-smooth-ne}).
Hence, \(\bs P^*\) belongs to the admissible policies of $\sigma$-smooth Nash equilibrium introduced by \cite[Definition 3.2]{daskalakis2023smooth}, and by construction, it also inherits its desirable sparsity property, which enables the approximation of smooth Nash equilibrium in polynomial time.
\end{example}

\begin{example}[Pareto distribution model] Suppose that $\mc B_j$, $j \in [M]$, is a marginal belief set with (shifted) Pareto distributed marginals of the form \eqref{eq:marginal-belief-set} induced by the cumulative distribution functions
\[F_{j,i}(s) = \max\left\{0, 1- \eta_{j,i} \left(-\frac{s(q-1) }{\gamma_{j} q}+ \frac{1}{q}\right)^{1 /(q-1)}\right\}\]
with $\eta_{j,i} \geq0$ and $\gamma_j, q>0$ for all $j\in[M]$ and $i \in [N]$. 
Then, the quantile response functions evaluated under the optimistic distributions $\mu\opt_j$ solving \eqref{eq:opt-exp-payoff},
\[\bs \tau(\mu_j\opt; \bs u) \in \argmax\limits_{\bs p\in \Delta^N} \bs p^\top \bs u +\frac{\gamma_j}{q-1} \sum\limits_{i=1}^N\eta_{j,i}\left(\left(\frac{p_i}{\eta_{j,i}}\right)^{q} - \frac{p_i}{\eta_{j,i}}\right).\]
The Pareto distribution model encapsulates the exponential model (in the limit $q \rightarrow 1$) and the uniform distribution model (for $q=2$) as special cases. The quantile functions admit no simple closed-form representation under this model.
\end{example}



One compelling interpretation of the formation of SE-OB is that players exhibit bounded rationality—that is, they do not optimize perfectly. Yet, there is another angle: when players assume that their expected payoffs are subject to perturbations, they reveal an inherent risk sensitivity. In essence, because they are not entirely confident in the precision of these payoffs, players adopt a risk-sensitive perspective. Paradoxically, however, when selecting their beliefs about the maximal payoff they might attain—specifically, in choosing the distribution of these perturbations—they display an optimistic bias. This dynamic interplay between risk sensitivity and optimistic belief formation is reflected in the consistency of their play over repeated interactions, a phenomenon that we examine in detail in the following section.




\section{Learning Equilibrium in Repeated Games with Optimistic Players}\label{sec:learning}
We consider a multi-agent learning model in which players interact repeatedly over time. 
For the settings in which each player exhibits individualized risk-preferences and receives only zeroth-order feedback, we present a learning algorithm called \textit{Repeated Game of Risk-sensitive Players with Optimistic Beliefs} presented in Algorithm~\ref{alg:rs-obl}. 
Specifically, each player $j$ is endowed with a set of \emph{risk-preference} functions \((F_{j,i})_{i\in[N]}\) and a \emph{dynamic} belief set $\mc M_j \subseteq \mc P(\R^N)$, and observes only their expected payoff vectors evaluated at the current strategy profile. 
At each iteration, every player computes a one-shot payoff estimate adjusted according to their risk preferences, and updates their internal statistics.
Based on these updated statistics, each player selects an optimistic belief from~$\mc M_j$ that maximizes their expected maximal perturbed statistic, and finally chooses a strategy in accordance with that optimistic belief.
\begin{algorithm}
\caption{: Repeated Game of Risk-sensitive Players with Optimistic Beliefs}
\label{alg:rs-obl}
\begin{algorithmic}[1]
\State\textbf{Input} Step-size sequence~$(\lambda_t)_{t \in [T]} \subseteq \mathbb{R}^T_{++}$, initial estimates
$\hat{\bs u}^{(0)} = (\hat {\bs u}_1^{(0)}, \ldots, \hat {\bs u}_M^{(0)})$, risk-preference functions, continuous, non-decreasing, $F_{j,i} : \R \to [0,1]$ for all $j \in [M], i\in [N]$, belief sets $(\mc M_j)_{j \in [M]}$, $\mc M_j \subseteq \mc P(\R^N)$
\For{round $t = 1, \ldots, T$}
    \State Every player~$j \in [M]$ selects $\bs p_j^{(t)} =\bs \tau(\mu_j\opt; \hat{\bs u}_j^{(t-1)})$, where
    \begin{equation}\mu_j\opt \in \argmax\limits_{\mu \in \mc M_j} \EE_{\bs \xi \sim \mu}\left[\max\limits_{i \in [N]} \hat u_{j, i}^{(t-1)} + \xi_i \right]
    \label{eq:optimistic-bel-dyn}
    \end{equation}
    \State $\bs P^{(t)} = (\bs p_1^{(t)}, \ldots, \bs p_M^{(t)})$  
    \State Every player~$j \in [M]$ receives {$\bs r^{(t)}_j = (u_j(\bs e_i; \bs P_{-j}^{(t)}))_{i \in [N]}$}
    \State Every player~$j \in [M]$ updates {$\hat{ u}_{j,i}^{(t)} = \hat{u}_{j,i}^{(t-1)} + \lambda_t ( r^{(t)}_{j,i} + F^{-1}_{j,i}(1- p_{j,i}^{(t)}))$} for all $i \in [N]$ \label{line:statistic-update}
\EndFor
\State \textbf{Return} $\bs P^{(T)} $
\end{algorithmic}
\end{algorithm}


The risk sensitivity of the players is incorporated into Algorithm~\ref{alg:rs-obl} through the update rule in Line~\ref{line:statistic-update}. The risk-preference functions, \(F_{j,i}\) explicitly captures whether the player exhibits risk-averse or risk-seeking behavior. We assume that each \(F_{j,i}\) is non-decreasing, implying that its inverse \(F_{j,i}^{-1}\) is also non-decreasing. For simplicity, suppose that \(F_{j,i} = F_j\) for all \(i\in[N]\).
Then, at round $t$, if arm $i$ is assigned a probability that is no greater than that of arm $i'$, \textit{i.e.}, $p_{j,i}^{(t)} \leq p_{j,i'}^{(t)}$,
the monotonicity of $F_j^{-1}$ implies that $F_j^{-1}(1-p_{j,i}^{(t)}) \geq F_j^{-1}(1-p_{j,i'}^{(t)}).$
This inequality indicates that the reinforcement applied to action $i$ is stronger than that for action $i'$ when estimating $\hat{\bs u}_{j}^{(t)}$, thereby promoting increased exploration of action $i$ in the subsequent round. Thus, our update mechanism explicitly accounts for the risk-sensitive behavior of the players, ensuring that the algorithm adapts to varying degrees of risk aversion or risk seeking.


In the following section, we demonstrate that if Algorithm~\ref{alg:rs-obl} is run with risk-preference functions $(F_{j,i})_{i\in [N]}$, $j\in [M]$, it converges to a strategy profile satisfying condition~\eqref{eq:b-done} for the game $\mc G([M], \Delta^N, (u_j)_{j\in [M]})$, provided that the belief sets $\mc B_j$ are in the form of~\eqref{eq:marginal-belief-set} and induced by the cumulative distribution functions $(F_{j,i})_{i \in [N]}$.
Remarkably, this convergence result is established by leveraging Theorem~\ref{thm:b-done-approx-ne}, which asserts that any strategy profile constituting a Nash equilibrium of the game with expected payoffs regularized with the risk-preference functions forms an SE-OB for the original game. From a static perspective, under certain regularity conditions on these risk-preference functions, we show that the Nash equilibrium of the regularized game is stable, a highly desirable property from the perspective of algorithm design.
From a dynamic standpoint, the algorithm’s incorporation of optimistic belief selection enables players to avoid overly aggressive actions over time. Moreover, we show that, for certain dynamic belief sets $\mc M_j$, if the strategy sequence generated by Algorithm~\ref{alg:rs-obl} converges—even in cases where the regularized game exhibits inherent instability—the limit of this sequence is guaranteed to be a Nash equilibrium of the regularized game. This result is significant because it shows that, even when the underlying dynamics of the regularized game might be unstable, the players behavior of operating under optimistic beliefs, ensures that convergence implies equilibrium attainment.
By synthesizing these insights and imposing additional restrictions on the dynamic belief sets $\mc M_j$, we ultimately prove that Algorithm~\ref{alg:rs-obl} converges to the Nash equilibrium of the regularized game, thereby forming an~\eqref{eq:b-done} for the original game.



\section{Convergence Analysis of Repeated Games of Risk-sensitive Players with Optimistic Beliefs}\label{sec:convergence}



A fundamental desideratum in algorithm design for repeated games with multiple players is the convergence to an equilibrium—a stationary state at which no player has an incentive to deviate. In our setting, the equilibrium is not known a priori to the players; instead, at each round, they receive a point estimate of their own expected payoff evaluated at the current joint strategy. Consequently, players dynamically adapt to one another’s strategies over time, which gives rise to the notion of regret—that is, the cumulative difference in performance between the actual trajectory of play and the best fixed action in hindsight. One might then ask whether the dynamic concept of regret is compatible with the static concept of equilibrium. Clearly, any equilibrium strategy incurs no regret; however, it remains less obvious whether the converse holds—namely, whether no-regret dynamics necessarily lead to an equilibrium.


Bridging the gap between no-regret dynamics and equilibrium in repeated multi-player games necessitates inducing desirable properties in both the static equilibrium and the dynamic learning process. From a static perspective, equilibrium stability plays a decisive role in ensuring that repeated interactions ultimately settle at a stationary point where no player benefits from deviating. In our framework, this stability is ensured by imposing suitable properties on the risk-preference functions that shape each player's attitude toward risk. On the other hand, the dynamic dimension concerns how players update their strategies over time: in particular, the mechanism governing action selection must satisfy specific conditions, captured by constraints on the dynamic belief sets that each player maintains. In the sections that follow, we first examine the static considerations that secure equilibrium stability, and then turn to the dynamic aspects, outlining how belief formation and action selection jointly guarantee the convergence of a learning algorithm to an equilibrium.

\subsection{Static Principles}
{To analyze the convergence of the sequence generated by Algorithm~\ref{alg:rs-obl}, we examine the stability of the smooth game's equilibrium. This stability guarantees that the dynamics of the regularized game are globally attracting. In particular, when the players' risk-preference functions satisfy certain conditions, the smooth game with regularized expected payoffs admits a stable equilibrium. We 
by formally defining the global variational stability criteria, as introduced in \citep{mertikopoulos2019learning}.}
\begin{definition}
$ \bs P' \in (\Delta^N)^M$ is a globally variationally stable strategy profile for the game $\mc G([M], \Delta^N, (u_j)_{j \in [M]})$ if 
    \begin{align}
        \label{eq:vs}
        \langle (\nabla_{\bs p_j} u_j(\bs p_j; \bs P_{-j}))_{j \in [M]}, \bs P - \bs P'\rangle = \sum\limits_{j=1}^M \langle \nabla_{\bs p_j} u_j(\bs p_j; \bs P_{-j}), \bs p_j - \bs p_j' \rangle < 0 \quad \forall \bs P \in (\Delta^N)^M, \bs P \neq \bs P'.\tag{\text{VS}}
    \end{align}
    \label{def:vs}
\end{definition}

\citep{mertikopoulos2019learning} introduced a sufficient second-order condition that can be used to verify whether Nash equilibria of a game are variationally stable.
To be more specific, consider the Hessian of the game~$\mc G([M], \Delta^N, (u_j)_{j \in[M]})$ that is defined as the block matrix $\bs H(\bs P,(u_j)_{j \in [M]}) = (\bs H_{j,k}(\bs P,(u_j,u_k)))_{j,k \in [M]}$ with
\[\bs H_{j,k}(\bs P,(u_j,u_k)) = \frac{1}{2} \nabla_{\bs p_k}\nabla_{\bs p_j} u_j(\bs p_j; \bs P_{-j}) + \frac{1}{2} (\nabla_{\bs p_j}\nabla_{\bs p_k} u_k(\bs p_k; \bs P_{-k}))^\top.\]
Note that $\bs H(\bs P,(u_j)_{j \in[M]})$ is a symmetric block matrix of size $MN  \times MN$.
\begin{lemma}{\cite[Proposition 2.8]{mertikopoulos2019learning}}
\label{lem:vs-hessian}
Assume $\bs P^\star$ is a Nash equilibrium of the game~$\mc G([M], \Delta^N, (u_j)_{j\in [M]})$. If $\bs H(\bs P,(u_j)_{j \in [M]})\prec 0$ on $ {\rm TC}(\bs P)$, for all $j \in [M]$ and $\bs P \in \mathcal (\Delta^N)^M$, then $\bs P^\star$ is globally variationally stable, and is the unique equilibrium of the game.
\end{lemma}
{In what follows, we derive a sufficient condition that guarantees 
\(\bs H(\bs P, (\bar u_j)_{j \in [M0]}) \prec 0\). Under this condition, 
the game with smooth expected payoffs admits a unique, globally variationally stable equilibrium.}


\begin{assumption}\label{ass:vs-hessian}
The risk-preference functions $F_{j,i}$, $j \in [M]$, $i \in [N]$, are continuous, differentiable and strictly increasing whenever $F_{j, i}(s) \in (0,1)$, and for all $j \in [M]$
\begin{equation}
    \inf\limits_{p \in (0,1)} \frac{1}{\min\limits_{i \in [N]} F_{j,i}'(F_{j,i}^{-1}(1-p))} > \frac{1}{2} \sum\limits_{k=1,~k\neq j}^M  \left\|\left(u_j(\bs P)\Big|_{\substack{\bs p_j = \bs e_i\\ \bs p_k = \bs e_{i'}}} \right)_{i, i'\in [N]}+ \left(u_j(\bs P )\Big|_{\substack{\bs p_k = \bs e_{i}\\ \bs p_j = \bs e_{i'}}}\right)_{i, i' \in [N]}^\top \right\|.
    \label{eq:vs-hessian-cond-F_k}
\end{equation}

\end{assumption}


Assuming the risk-preference functions meet the conditions outlined above, any game with smooth expected payoffs admits a Nash equilibrium that is globally stable in the variational sense. This result is formalized in the following proposition.
\begin{proposition}\label{prop:vs-done}
If the risk-preference functions $F_{j,i}$, $j \in [M]$, $i\in [N]$, satisfy Assumption~\ref{ass:vs-hessian} and the smooth expected payoffs $(\bar u_j)_{j\in [M]}$ are constructed as in~\eqref{eq:smooth-exp-payoffs}, then the game $\mc G([M],\Delta^N, (\bar u_j)_{j \in [M]})$ admits unique Nash equilibrium that is variationally globally stable. 
\end{proposition}
To further understand Assumption~\ref{ass:vs-hessian}, we return to two aforementioned examples based on \eqref{eq:marginal-belief-set} with uniform and exponential marginal distributions, respectively.


\begin{remark}
Note that the operator norm is bounded above by the Frobenius norm and that \(u_j(\bs P)\in[0,1]\), the right-hand side of~\eqref{eq:vs-hessian-cond-F_k} is at most \(N(M-1)\). 
When we consider the exponential distribution model presented in Example~\ref{ex:exp-marginal}, a straightforward calculation shows that
the left-hand side of condition~\eqref{eq:vs-hessian-cond-F_k} evaluates to $\lambda$.

Hence, the exponential distributions with parameter \(\lambda > N(M-1)\) in Example~\ref{ex:exp-marginal} satisfy Assumption~\ref{ass:vs-hessian}.
If we consider the uniform distribution model presented in Example~\ref{ex:uniform}, we can show that the left-hand-side of condition~\eqref{eq:vs-hessian-cond-F_k} evaluates to $2\lambda N$. 
Consequently, the uniform distributions with parameter \(\lambda > (M-1)/2\) in Example~\ref{ex:uniform} satisfy Assumption~\ref{ass:vs-hessian}.

\end{remark}
%


\subsection{Dynamic Principles}
The structural properties of the belief sets $(\mathcal{M}_j)_{j \in [M]}$ play a crucial role in shaping the long-run behavior of the learning process. In particular, exploiting the certain inherent structure of the belief sets, we can conclude that if the sequence of play generated by Algorithm~\ref{alg:rs-obl} converges to a specific strategy profile, then this profile is guaranteed to be a Nash equilibrium of the game~$\mathcal{G}([M], \Delta^N, (\bar{u}_j)_{j \in [M]})$.

At round \(t\) of Algorithm~\ref{alg:rs-obl}, each optimistic player uses their risk-adjusted cumulative payoff estimate, \(\hat{\bs u}_j^{(t-1)}\), to select the belief from their belief set \(\mathcal{M}_j\) that maximizes the expected maximal perturbed cumulative payoff. 
Once this optimistic belief is identified, then the player adopts it by choosing an action via the corresponding quantile function. 
Throughout this section, we will assume that the belief sets~$\mc M_j$ are of the form~\eqref{eq:marginal-belief-set} induced by the cumulative distribution functions~$(G_{j,i})_{i \in [N]}$. By Lemma~\ref{lem:distributional-regularization}, if $\bs p\opt (\bs u)$ solves 
\begin{equation}
    \max\limits_{\bs p \in \Delta^N} g(\bs p; \bs u; (G_i)_{i \in [N]}) = \bs p^\top \bs u + \sum\limits_{i=1}^N \int_{1-p_i}^1 G_{i}^{-1} (s)\diff s, 
    \label{eq:regularized-map}
\end{equation}
then $\bs p\opt(\hat{\bs u}_j^{(t-1)})$ constitutes a valid strategy for player~$j$ in round $t$ as Lemma~\ref{lem:distributional-regularization} ensures that \[\bs p\opt(\hat{\bs u}_j^{(t-1)}) =\bs \tau(\mu_j\opt; \hat{\bs u}_j^{(t-1)}),\] where $\mu_j\opt$ satisfies~\eqref{eq:optimistic-bel-dyn}. Consequently, we assume that the optimistic player~$j$ selects their strategy $\bs p_j^{(t)}$ by solving the maximization problem in~\eqref{eq:regularized-map} for $\bs u = \hat{\bs u}_j^{(t-1)} $. Curiously, from an optimization perspective, choosing actions based on optimistic beliefs yields a smoothing effect on the strategy that will be implemented in the subsequent round.
While one might expect a greedy strategy-choosing actions via $\bs p_j^{(t)} \mapsto \argmax_{\bs p \in \Delta^N} \bs p^\top \hat{\bs u}_j^{(t-1)}$ (which corresponds to choosing actions with certain fixed beliefs by Proposition~\ref{prop:se-ob-ne})-to suffice, such methods have been criticized as being too aggressive in the presence of uncertainty~\cite[\S 3.2]{mertikopoulos2019learning}.


\begin{proposition} Suppose that for each $j\in[M]$, $\mc M_j$ is of the form~\eqref{eq:marginal-belief-set} with $(G_{j,i})_{i \in [N]}$ that are continuous for all $i \in[N]$. If Algorithm \ref{alg:rs-obl} is run with a step size sequence $(\lambda_t)_{t\in [T]}$ such that 
$\sum_{t=1}^\infty \lambda_t = \infty$, and if $\bs P^{(T)}$ converges to $\bs P^* \in (\Delta^N)^M$ as $T \to \infty$, then $\bs P^*$ is a Nash equilibrium of the game~$\mc G([M], \Delta^N, (\bar u_j)_{j\in[M]})$, where $\bar u_j$, $j\in [M]$, is as defined in~\eqref{eq:smooth-exp-payoffs} for some continuous cumulative distribution functions~$(F_{j,i})_{i \in [N]}$.
\label{prop:convergence-to-point-and-this-is-nash}
\end{proposition}


\subsection{Convergence Analysis}
Equipped with the discussions on static and dynamic principles of Algorithm~\ref{alg:rs-obl}, we are ready to focus on the convergence of Algorithm~\ref{alg:rs-obl} to the Nash equilibrium of the game~$\mc G([M], \Delta^N, (\bar u_j)_{j\in [M]})$. We first state the following assumption regarding the marginal distributions associated with the dynamic belief set $\mc M_j$.

\begin{assumption}\label{ass:G-fenchel}
    For each $j \in [M]$, $\mc M_j$ is in the form of~\eqref{eq:marginal-belief-set} with cumulative distribution functions~$(G_{j,i})_{i\in [N]}$ that are Lipschitz continuous with Lipschitz constant $ L > 0$ and satisfy
    \begin{equation}
        \max\limits_{\bs p' \in \Delta^N} g(\bs p'; \bs u_n; (G_{j,i})_{i \in [N]}) - g(\bs p; \bs u_n ; (G_{j,i})_{i \in [N]}) \to 0 \quad \text{whenever} \quad \bs p\opt(\bs u_n) \to \bs p.
    \end{equation}
    \label{ass:dynamic-belief-sets-fenchel-coupling}
\end{assumption}
Although Assumption~\ref{ass:dynamic-belief-sets-fenchel-coupling} may initially appear overly restrictive, it can be demonstrated that every example of the marginal belief sets presented in Section~\ref{sec:marginal-belief-sets} satisfies this assumption. 
\begin{lemma}{\cite[Proposition 4.8 (ii)]{tacskesen2023semi}}
If $G_i$, $i \in [N]$, is Lipschitz continuous with Lipschitz constant $L >0$, then $- g(\bs p; \bs u; (G_i)_{i \in [N]})$ is $1/L$-strongly convex in $\bs p$. 
\label{lem:strong-convex-reg}
\end{lemma}

If the risk-preference functions satisfy Assumption~\ref{ass:vs-hessian} and the dynamic belief sets $\mc M_j$ satisfy Assumption~\ref{ass:dynamic-belief-sets-fenchel-coupling}, then the following theorem demonstrates that Algorithm~\ref{alg:rs-obl} converges to a strategy profile that forms an SE-OB for the game $\mc G([M], \Delta^N, (u_j)_{j \in [M]})$ when the belief sets of the players are of the form \eqref{eq:marginal-belief-set} induced by their corresponding risk-preference functions.  

\begin{theorem}
Under Assumptions~\ref{ass:vs-hessian} and~\ref{ass:dynamic-belief-sets-fenchel-coupling}, suppose that Algorithm~\ref{alg:rs-obl} is run with the step-size sequence $(\lambda_t)_{t\in [T]}$ such that %$\lambda_t > 0$ and
$\sum_{s=1}^t \lambda^2_t / \sum_{s=1}^t \lambda_t \to 0$. Then, the trajectory of play $\bs P^{(T)}$ converges to an \eqref{eq:b-done} of the game~$\mathcal G([M], \Delta^N, (u_j)_{j\in [M]})$ induced by the belief sets $\mc B_j$, ${j \in [M]}$, of the form~\eqref{eq:marginal-belief-set} with cumulative functions~$(F_{j,i})_{i \in [N]}$. 
\label{theorem:asymp-convergence}
\end{theorem}



\section{Concluding Remarks}
\label{sec:discuss}
{In this paper, we introduce the concept of the \emph{Statistical Equilibrium of Optimistic Beliefs (SE-OB)} for the mixed extension of multiplayer finite normal-form games. Building on insights from discrete choice theory, our framework relaxes the traditional parametric constraints of structural QRE and captures the nuanced interplay between risk sensitivity and the strategic formation of optimistic beliefs in decision-making. Specifically, we model players' choice behavior by positing that their expected payoffs are subject to random perturbations, and that they form optimistic beliefs by selecting the distribution of these perturbations that maximizes their highest anticipated payoffs. This assumption of perturbed expected payoffs reflects an inherent risk sensitivity; accordingly, each player is endowed with a risk-preference function for every action.
We further demonstrate that every Nash equilibrium of a game with regularized expected payoffs naturally corresponds to an SE-OB in the original game, provided that the belief sets coincide with the feasible set of a multi-marginal optimal transport problem, where the marginals are given by the players’ risk-preference functions. Moreover, we show that this regularization induces a smoothing effect on the payoff functions, and we establish the stability of the Nash equilibrium in the smoothed-payoff game.
To complement our theoretical findings, we propose an algorithm for repeated games in which players consistently adopt optimistic beliefs. We prove that, under suitable conditions, this algorithm converges to an SE-OB. Notably, our approach represents the first convergent algorithm for general structural QRE beyond the classical logit-QRE.

The concept of SE-OB encapsulates the culmination of our efforts to provide a consistent description of strategic behavior for players seeking convergence to equilibrium, we also acknowledge the limitations of our approach and the associated results.
}

First, our convergence proof relies on the assumption that players receive accurate expected payoff information at each round—evaluated at the current strategy profile—through a black-box feedback mechanism. However, there are several reasons to believe that this feedback may be imperfect and susceptible to noise. For example, the payoff estimates could be affected by measurement errors, the transmission of information might be corrupted by noise, or privacy concerns may compromise the quality of the data provided to the players. Consequently, the algorithm's convergence should be robust to such deviations in the feedback mechanism. Building on the fruitful line of research on the convergence of learning algorithms to Nash equilibrium in the presence of noise \citep{mertikopoulos2019learning, ref:hsieh2021adaptive, ref:mertikopoulos2024unified}, it seems plausible to expect that if the noise in the feedback mechanism is additive, zero-mean, and has bounded variance, then Algorithm \ref{alg:rs-obl} will still converge to the corresponding equilibrium—albeit under stricter conditions on its hyperparameters. We leave the exploration of this direction for future work.

Second, we have derived sufficient conditions to ensure the global variational stability of a game whose payoffs are regularized by the players' risk-preference functions. Specifically, these conditions impose a lower bound on the variance of the perturbations available to the players. In effect, the risk-averse behavior of the players induces a degree of smoothness in the game, thereby stabilizing the Nash equilibrium of the regularized payoff structure. This connection is particularly intriguing as it aligns with smoothing techniques used to design convergent algorithms in the literature—including, but not limited to ~\citep{mazumdar2024tractable,abernethy2016perturbation, ref:honda23follow, ref:kim2019optimality, li2024optimism}. Although the conditions we present are sufficient rather than necessary, we are optimistic that a more relaxed condition exists, which would still guarantee convergence of algorithmic designs to the SE-OB equilibrium of the original game.

Third, while we have established the asymptotic convergence to an SE-OB, it is of practical interest to characterize the finite-time convergence rate. In particular, drawing on the analysis in \citep{cai2022finite}, it appears plausible that finite-time convergence results could be obtained by modifying the estimation step of Algorithm~\ref{alg:rs-obl} to incorporate an additional gradient step. Such an approach would likely require imposing further restrictions on the players' risk-preference functions and considering \(\mathcal{M}_j\) in the form presented in Example~\ref{ex:uniform}.


Fourth, our analysis is exclusively confined to the mixed extension of finite normal-form games. An exciting direction for future research is to generalize these results to more complex and practically significant classes of games—such as Kelly auctions, congestion games, and Bayesian games. Furthermore, we conjecture that the theoretical framework developed herein may yield valuable insights into other paradigms in game theory, notably Nash bargaining and Stackelberg games.




\appendix
\section{Proofs}
\begin{proof}[\textbf{Proof of~Theorem~\ref{thm:b-done-approx-ne}}]
If $\bs P^*$ is~\eqref{eq:NE} of the game~$\mc G([M], \Delta^N, (\bar u_{j})_{j \in [M]})$, then by Definition~\ref{def:smooth-exp-ut}, for each $j \in [M]$, $\bs P^*$ satisfies
 \begin{align}
 \max\limits_{\bs p_j \in \Delta^N} \bar u_{j}(u_j; (\bs p_j ; \bs P_{-j}^*); (F_{j,i})_{i \in [N]})
 &= \max\limits_{\bs p \in \Delta^N} u_j(\bs p; \bs P^*_{-j}) + \sum\limits_{i=1}^N \int_{1-p_i}^1 F_{j,i}^{-1}(t)\diff t.
\label{eq:reg-ne}
 \end{align}
By Lemma~\ref{lem:distributional-regularization}, the problem on the right hand side of~\eqref{eq:reg-ne} admits a unique minimizer and we denote it by~$\bs p_j\opt$. Hence, we may conclude that $\bs p^*_j= \bs p_j\opt$ for all $j \in [M]$. By Lemma~\ref{lem:distributional-regularization}, for each $j \in [M]$, we have
\begin{equation}\bar u_{j}(u_j; (\bs p_j^* ; \bs P_{-j}^*); (F_{j,i})_{i \in [N]})=\sup\limits_{\mu \in \mc B} \EE_{\xi \sim \mu}\left[ \max\limits_{i \in [N]} u_j(\bs e_i ; \bs P^*_{-j}) + \xi_i\right],
\label{eq:reg-rel-1}
\end{equation}
and 
\[
\bs p^*_j = \bs \tau(\mu_j\opt; u_j( \bs e_i; \bs P_{-j}^*)_{i \in [N]}),
\]
where $\mu_j^\star$ denotes an optimal solution of the problem on the right side of~\eqref{eq:reg-rel-1}.
Finally, we may conclude that
\[\sup\limits_{\mu\in\mc B} \EE_{\bs \xi \sim \mu} \left[ \max\limits_{i \in [N]} u_j(\bs e_i; \bs P^*_{-j}) + \xi_i \right] =  \EE_{\bs \xi \sim \mu_j\opt} \left[ \max\limits_{i \in [N]} u_j(\bs e_i; \bs P^*_{-j}) + \xi_i \right]\quad \forall j \in [M].\]
Hence, the mixed strategy profile~$\bs P^*$ satisfies~\eqref{eq:b-done} of the game~$\mc G([M], \Delta^N, (u_j)_{j\in [M]})$. This observation concludes our proof.
\end{proof}

\begin{proof}[\textbf{Proof of Proposition~\ref{prop:vs-done}}]
For each $j\in[M]$, the gradient of $\bar u_j$ with respect to $\bs p_j$ is in the form of
\begin{equation}
    (\nabla_{\bs p_j} \bar u_{j}(u_j; (\bs p_j; \bs P_{-j}); (F_{j,i})_{i \in [N]}))_i = 
    u_j(\bs e_i; \bs P_{-j}) + F_{j,i}^{-1}(1-p_{j,i}) \quad \forall i \in [N].
\end{equation}
Then, for $k \neq j$, we have
\begin{equation}
    (\nabla_{\bs p_k}\nabla_{\bs p_j} \bar u_{j}(u_j; (\bs p_j; \bs P_{-j}); (F_{j,i})_{i \in [N]}))_{ii'} = u_j(\bs P)\Big|_{\substack{\bs p_j = \bs e_i\\ \bs p_k = \bs e_{i'}}}.
    \label{eq:hessian-off-diagonal}
\end{equation}
implying that for $k \neq j$, $\bs H_{j,k}$ satisfies
\[
\bs H_{j,k}(\bs P, (\bar u_j, \bar u_k)) = \frac{1}{2}\left(u_j(\bs P)\Big|_{\substack{\bs p_j = \bs e_i\\ \bs p_k = \bs e_{i'}}} \right)_{i, i'\in [N]}+ \frac{1}{2} \left(u_j(\bs P )\Big|_{\substack{\bs p_k = \bs e_{i}\\ \bs p_j = \bs e_{i'}}}\right)_{i, i' \in [N]}^\top.
\]
Additionally, for each $j \in [M]$, we have
\begin{align}
 \bs H_{j,j}(\bs P, \bar u_{j}) = \nabla^2_{\bs p_j} \bar u_{j}(u_j; (\bs p_j; \bs P_{-j}); (F_{j,i})_{i \in [N]})= -\text{diag}\left\{\left(\frac{1}{F_{j,i}'(F_{j,i}^{-1}(1-p_{j,i})))}\right)_{i\in [N]}\right\},
 \label{eq:hessian-diags}
\end{align}
which is diagonal matrix and is negative definite because $F_{j,i}$'s are strictly increasing under Assumption~\ref{ass:vs-hessian} and thus $F'_{j,i} > 0$. Then, for all $j\in [M]$, we have
\begin{align}
\label{eq:cond-negdef}
\frac{1}{\|\bs H^{-1}_{j,j}(\bs P, \bar u_j)\|} & = \left\| \text{diag}\left(\frac{1}{F_{j,i}'(F_{j,i}^{-1}(1-p_{j,i}))} \right)_{i \in [N]}\right\| \\
&=\max\limits_{i \in [N]} \frac{1}{F'_{j,i}(F_{j,i}^{-1}(1- p_{j,i}))} \\
&=  \frac{1}{\min\limits_{i \in [N]}F'_{j,i}(F_{j,i}^{-1}(1-p_{j,i}))}\\
% &\geq \min\limits_{i \in [N]}\\
&\geq \inf\limits_{p' \in (0,1)}  \frac{1}{{\min\limits_{i \in [N]}}F_{j,i}'(F_{j,i}^{-1}(1-p'))}\\
& > \frac{1}{2}\sum\limits_{k=1, k\neq j}^M  \bigg\|\left(u_j(\bs P)\Big|_{\substack{\bs p_j = \bs e_i\\ \bs p_k = \bs e_{i'}}} \right)_{i, i'\in [N]}+ \left(u_j(\bs P )\Big|_{\substack{\bs p_k = \bs e_{i}\\ \bs p_j = \bs e_{i'}}}\right)_{i, i' \in [N]}^\top \bigg\|\\
&= \sum\limits_{k=1, k\neq j}^M \| \bs H_{j,k}(\bs P, (\bar u_j, \bar u_k)) \|,
\end{align}
where the first equality follows by \eqref{eq:hessian-diags} and the second equality by the definition of operator norm. The strict inequality holds as $F_{j,i}$'s satisfy Assumption~\ref{ass:vs-hessian} and the last inequality follows by~\eqref{eq:hessian-off-diagonal}.
The ultimate inequality above implies that $\bs  H(\bs P, \bar u_j)_{j \in [M]})$ is strictly diagonally dominant (see \cite[Definition~1]{feingold1962block}). 
Therefore, by~\cite[Theorem~9]{feingold1962block}, we have
\[\bs  H\left(\bs P, (\bar u_{j})_{j \in [M]} \right) \prec 0.\] 
The claim then follows directly from Lemma~\ref{lem:vs-hessian}.

\end{proof}
\begin{proof}[\textbf{Proof of Lemma~\ref{lem:concave-game}}]
   By Definition~\ref{eq:smooth-exp-payoffs}, the derivatives of $\bar u_j$ with respect to $ p_{j,i}$ are in the form of $u_j(\bs e_i; \bs P_{-j}) + F_i^{-1}(1- p_{j,i})$, which is non-decreasing as left-quantile functions are non-decreasing, and $1-p_{j,i}$ is strictly decreasing in~$p_{j,i}$. This observation completes the proof. 
\end{proof}


Recall that the strategies of each player are chosen by solving~\eqref{eq:regularized-map}. 
Then, sequence of play created by Algorithm~\ref{alg:rs-obl} is equivalent to that of dual averaging algorithm~\cite[\S~3]{mertikopoulos2019learning} with regularization functions of the form 
\[\bs p \mapsto \sum_{i=1}^N \int_{1-p_{i}}^1 G_{j,i}^{-1}(s)\diff s \quad  \forall  j\in[M].\]
By Lemma~\ref{lem:concave-game},~$\mc G([M], \Delta^N, (\bar u_j)_{j \in [M]})$ is a concave game. Hence, our proof can be adapted from~\cite[Theorem 4.1]{mertikopoulos2019learning}, which is for general cases when the feedback mechanism is noisy. 
For the sake of completeness, we now present a slightly revised version of their proof.

\begin{lemma}
    If $\bs P^* \in (\Delta^N)^M$ is Nash equilibrium of the game $\mc G([M], \Delta^N, (u_j)_{j\in [M]})$, then 
    \begin{equation}\langle \nabla_{\bs p} u_j(\bs p; \bs P^*_{-j})|_{\bs p = \bs p_j^*},  \bs z_j\rangle  \leq 0 \quad \forall \bs z_j \in \textrm{TC}_j(\bs p_j^*),~j\in [M],
    \label{eq:ne-first-order}
    \end{equation}
    where $\textrm{TC}_j(\bs p)$ denotes the tangent cone to $\Delta^N$ at $\bs p \in \Delta^N$. 
\end{lemma}


\begin{proof}[\textbf{Proof of Proposition~\ref{prop:convergence-to-point-and-this-is-nash}}]
For strategy profile $\bs P^* = (\bs p_1^*, \ldots, \bs p^*_M) \in (\Delta^N)^M$, denote by
\[
\bs v_j^* = \left(u_j(\bs e_i, \bs P^*_{-j}) + F^{-1}_{j,i}\left(1-\bs p^*_{j,i}\right)\right)_{i \in [N]} \quad \forall j \in [M].
\]
Note that $\bs v_j^* = \nabla_{\bs p} \bar u_j(\bs p; \bs P_{-j}^*) |_{\bs p = \bs p_j^*}$, where $\bar u_j$ is as defined in \eqref{eq:smooth-exp-payoffs}. If $\bs P^*$ is a Nash equilibrium of the game $\mc G([M], \Delta^N, (\bar u_j)_{j\in [M]})$, then $\bs v^*_j$ should satisfy \eqref{eq:ne-first-order} for all $j \in [M]$.
Now, for the sake of argument, suppose that $\bs P^*$ is not a Nash equilibrium of the game $\mc G([M], \Delta^N, (\bar u_j)_{j\in [M]})$. 
Then, there exists $j \in [M]$ and a deviation $\bs q_j \in \Delta^N$ such that
\[
\langle \bs v^*_j, \bs q_j - \bs p^*_j \rangle > 0.
\]
We denote $\bs V^* = (\bs v^*_1, \cdots, \bs v^*_M)$, and neighborhoods of $\bs P^*$ and $\bs V^*$ as $\mc P$
Hence, by continuity, there exists a positive constant $c > 0$ and neighborhoods $\mc Z$ and $\mc V$ of $ \bs P^*$ and $ \bs V^*$, respectively, such that,
\begin{equation}
\langle \bs v'_j, \bs q_j - \bs p'_j \rangle \geq c > 0,
\label{eq:v-q-p-ineq}
\end{equation}
for all $\bs P' \in \mc Z$ and $\bs V' \in \mc V$. 
Suppose that $\bs P^{(T)}$ converges to $\bs P^*$, then, for simplicity, we assume that $\bs P^{(T)} \in \mc Z$ and
\[\bs V^{(T)} = \left(\bs v_1^{(T)}, \cdots, \bs v_M^{(T)}\right) \in \mc V~\text{where}~\bs v^{(T)}_j = \left(u_j\left(\bs e_i, \bs P^{(T)}_{-j}\right) + F^{-1}_{j,i}\left(1-\bs p^{(T)}_{j,i}\right)\right)_{i \in [N]}.\] 
Denote by $ \hat{\bs U}^{(t)} = ((\hat{\bs u}_{1,i}^{(t)})_{i \in [N]}, \ldots, (\hat{\bs u}_{M,i}^{(t)})_{i \in [N]})$. Then, we have
\[
\hat{\bs U}^{(T)} = \hat{\bs U}^{(0)} + \sum_{t=1}^T \lambda_t \bs V^{(t)} = \hat{\bs U}^{(0)} + \left(\sum_{t=1}^T \lambda_t\right) \bar{\bs V}^{(T)},
\]
where $\bar{\bs V}^{(T)} = (\bar{\bs v}_1^{(T)}, \cdots, \bar{\bs v}_M^{(T)})$ and satisfies
\[
\bar{\bs V}^{(T)} = \frac{\sum_{t=1}^T \lambda_t \bs V^{(t)}}{\sum_{t=1}^T \lambda_t}.
\]
If we denote by $h_j(\bs p) = \sum_{i=1}^N \int_{1-p_i}^1 G^{-1}_{j,i }(s) \diff s$, then $\hat{\bs u}_j^{(T-1)} \in \partial h_j(\bs p^{(T)}_j)$, as $\bs p^{(T)}_j$ solves 
\[ \max_{\bs p \in \Delta^N} g\left(\bs p; \bs u_j^{(T-1)}; (G_{j,i})_{i \in [N]}\right).\]
By the first-order Taylor approximation, we have
\begin{align}\label{eq:conv-ne}
h_j(\bs q_j) - h_j(\bs p^{(T)}_j) &\geq \langle \hat{\bs u}_j^{(T-1)}, \bs q_j - \bs p^{(T)}_j  \rangle\\ &= \langle \hat{\bs u}_j^{(0)}, \bs q_j - \bs p^{(T)}_j  \rangle + \left(\sum_{t=1}^{T-1} \lambda_t\right) \langle \bar{\bs v}_j^{(T-1)}, \bs q_j - \bs p^{(T)}_j  \rangle.
\end{align}
Given that $\bs P^{(T)} \to \bs P^*$ as $T \to \infty$, we have $\bs V^{(T)} \rightarrow \bs V^*$, and thus $\bar{\bs V}^{(T-1)} \rightarrow \bs V^*$.
By Cauchy-Schwarz inequality, we have
\[
\big|\langle \hat{\bs u}_j^{(0)}, \bs q_j - \bs p^{(T)}_j  \rangle\big| \leq \|\hat{\bs u}_j^{(0)}\| \cdot \|\bs q_j - \bs p^{(T)}_j\| \leq 2\|\hat{\bs u}_j^{(0)}\| = O(1),
\]
where the second inequality follows because $\max_{\bs p, \bs q \in \Delta^N} \| \bs p -\bs q\| = 2$. Additionally, note that we have $\langle \bar{\bs v}_j^{(T-1)}, \bs q_j - \bs p^{(T)}_j  \rangle \geq c > 0$ followed by~\eqref{eq:v-q-p-ineq}.
Hence, \eqref{eq:conv-ne} can be written as 
\[\lim_{T \to \infty}h_j(\bs q_j) - h_j(\bs p^{(T)}_j) \gtrsim c \cdot \sum_{t=1}^{\infty} \lambda_t = \infty,\]
where the last equality follows because $(\lambda_t)_{t \in \mathbb N}$ is such that $\sum_{t=1}^\infty \lambda_t  = \infty$. The conclusion above draws a contradiction {because $\max_{\bs p, \bs q \in \Delta^N} h_j(\bs p) - h_j(\bs q) < \infty$ for all $j \in [M]$.} Therefore, we conclude that $\bs P^*$ is a Nash equilibrium of the game $\mc G([M], \Delta^N, (\bar u_j)_{j\in[M]})$.
\end{proof}
% }
\begin{proof}[\textbf{Proof of Theorem~\ref{theorem:asymp-convergence}}]
By Proposition~\ref{prop:vs-done}, the game $\mc G([M], \Delta^N, (\bar u_j)_{j\in[M]})$ admits a unique Nash equilibrium that is variationally globally stable. 
Recall that the strategies of each player are chosen by solving~\eqref{eq:regularized-map}. Then, the sequence of play created by Algorithm~\ref{alg:rs-obl} is equivalent to that of dual averaging algorithm~\cite[\S 3]{mertikopoulos2019learning} with regularization functions $\sum_{i=1}^N \int_{1-p_i}^1 G_i^{-1}(s) \diff s$, which are strongly concave thanks to Lemma~\ref{lem:strong-convex-reg}.
Finally, the claim follows by~\cite[Theorem~4.6]{mertikopoulos2019learning}. 
\end{proof}

\section{Additional Technical Results}

\begin{lemma}
    For some $\sigma \in (0,1)$, if $\lambda \geq (2/\sigma - N)^{-1}$, then $(\text{sparsemax}(\bs u /\lambda))_i\in [0, (N\sigma)^{-1}]$ for all $i \in [N]$.
    \label{lem:uniform-dists-smooth-ne}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:uniform-dists-smooth-ne}]
We first show for $\textrm{sparsemax}(\bs u / \lambda)$ the index $k$, as defined in Definition~\ref{def:sparsemax}, satisfies $k \geq 2$. To demonstrate this, recall the definition of $k$,
\[
k=\max \left\{j \in[N]: 2+\frac{j}{N} u_{\rho(j)}> \frac{1}{N} \sum_{i=1}^j u_{\rho(i)}\right\}.
\]
For $j = 1$, the inequality above evaluates as $2 + u_{\rho(1)}/N > u_{\rho(1)}/N$, which holds trivially and thus $k \geq 1$. 
For $j = 2$, we have $2+ 2u_{\rho(2)}/N> \frac{1}{N} (u_{\rho(1)} + u_{\rho(2)})$, \textit{i.e.}, $(u_{\rho(1)} - u_{\rho(2)})/N < 2$, which is holds as $u_i \in [0,1]$. Thus, by definition, we have $k \geq 2$.
Then, by Definition~\ref{def:sparsemax}, we have
\begin{align}
\max_{i \in [N]} (\textrm{sparsemax}(\bs u/ \lambda))_{i} & \leq \frac{1}{2N}\left(\frac{u_{\rho(1)}}{\lambda} - \kappa^*\right)\\
&= \frac{1}{2N} \left(\frac{u_{\rho(1)}}{\lambda} - \frac{N^{-1}\left(\sum_{i=1}^k u_{\rho(i)}\right)-2}{k/N}\right)\\
& = \frac{1}{2N} \left(\frac{u_{\rho(1)}}{\lambda} + \frac{2N}{k} - \frac{1}{k}\sum_{i=1}^k u_{\rho(i)} \right)\\
& \leq \frac{1}{2N} \left(\frac{1}{\lambda} + N \right) \leq \frac{1}{N\sigma},
\end{align}
where the first inequality follows because $u_i \in [0,1]$ and $k \geq 2$. The second inequality holds as $\lambda \geq (2/\sigma - N)^{-1}$. This observation completes our proof.
\end{proof}


\bibliographystyle{abbrvnat}
\bibliography{bib}



\end{document}
