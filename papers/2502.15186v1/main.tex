\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
%\usepackage{caption}
\usepackage{float}
\usepackage[square,numbers]{natbib}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{LUMINA-Net: Low-light Upgrade through Multi-stage Illumination and Noise Adaptation Network for Image Enhancement}

%\author{Anonymous}
\author{\IEEEauthorblockN{Siddiqua Namrah}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
\textit{Korea University}\\
Seoul, South Korea \\
namrah96@korea.ac.kr}
\and
\IEEEauthorblockN{Suneung Kim}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
\textit{Korea University}\\
Seoul, South Korea \\
se\_kim@korea.ac.kr}
%\and
%\IEEEauthorblockN{3\textsuperscript{rd} Seong-Whan Lee}
%\IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
%\textit{Korea University}\\
%Seoul, South Korea \\
%email address or ORCID}
%\and
}
\maketitle

\begin{abstract}
Low-light image enhancement (LLIE) is a crucial task in computer vision aimed to enhance the visual fidelity of images captured under low-illumination conditions. Conventional methods frequently struggle to mitigate pervasive shortcomings such as noise, over-exposure, and color distortion thereby precipitating a pronounced degradation in image quality. To address these challenges, we propose LUMINA-Net an advanced deep learning framework designed specifically by integrating multi-stage illumination and reflectance modules. First, the illumination module intelligently adjusts brightness and contrast levels while meticulously preserving intricate textural details. Second, the reflectance module incorporates a noise reduction mechanism that leverages spatial attention and channel-wise feature refinement to mitigate noise contamination. Through a comprehensive suite of experiments conducted on LOL and SICE datasets using PSNR, SSIM and LPIPS metrics, surpassing state-of-the-art methodologies and showcasing its efficacy in low-light image enhancement.
\end{abstract}

\begin{IEEEkeywords}
LUMINA-Net, low light image enhancement, deep learning, reflectance refinement, noise adaptation, over-exposure, PSNR, SSIM, LPIPS
\end{IEEEkeywords}

\section{Introduction}
Low-Light Image Enhancement (LLIE) has emerged as a vital component in various image-based applications, including surveillance \cite{qu2024double,liu2022attention}, autonomous vehicles \cite{li2024light,liu2024lane}, medical imaging \cite{ma2021structure,gomez2019low}, and consumer electronics \cite{fu2022efficient,zhou2024real,hwang2006full}, where high-fidelity images are paramount. Traditional approaches to LLIE have primarily relied on two well-established methods, histogram-based and retinex-based techniques. Histogram-based LLIE techniques analyze and modify pixel intensity distributions to adjust contrast and brightness, with Histogram Equalization (HE) being a widely used method \cite{banik2018contrast,park2022histogram,lee2001automatic}. Retinex-based LLIE techniques separate images into reflectance and illumination components, adjusting the latter to enhance contrast and visibility while preserving natural colors and textures \cite{wang2021seeing,hai2023r2rnet,yi2023diff,yang2007reconstruction}.

\begin{figure}[t]
\includegraphics[width=8.5cm]{Fig1.png}
\centering
\caption{A comparative analysis of reflectance images utilized in existing methods and their corresponding final results reveals significant limitations. These limitations are evident across four scenarios: (a) input images captured under low-light conditions, (b) reflectance maps that frequently amplify overexposure artifacts, (c) enhanced images prone to color distortions and texture loss, and (d) ground truth images showcasing the desired balance of color fidelity and structural detail. }
\label{figure1}
\end{figure}

However, conventional image capture and processing methods frequently fall short in low-light conditions, underscoring the urgent need for groundbreaking solutions that can effectively mitigate the challenges of diminished illumination. Existing Retinex-based methods rely on single-image inputs, limiting their ability to address varying exposure levels effectively.

The comparative analysis in Figure \ref{figure1} highlights the inherent limitations of these approaches.  Specifically, it reveals that overexposure remains a critical challenge, significantly degrading the quality of the reconstructed images. As depicted in Figure \ref{figure1}, reflectance images processed by existing methods often suffer from pronounced artifacts caused by overexposure, leading to imbalanced color distribution and loss of texture detail. This inadequacy in handling under-exposed and over-exposed regions results in outputs marred by color distortion and diminished visual fidelity.

These challenges emphasize the pressing need for more advanced and adaptive techniques that can robustly address a wide range of exposure conditions while minimizing noise and mitigating overexposure. By addressing these gaps, future methodologies can achieve superior image quality, preserving both texture and color accuracy under challenging lighting scenarios.

To address the complexities of low-light image enhancement, we propose LUMINA-Net, a novel deep learning framework that leverages paired low-light images with varying exposures. Building upon the Retinex theory, LUMINA-Net decomposes low-light images into illumination and reflectance components. A key innovation of LUMINA-Net is the integration of a Channel-Guidance (CG) Module, which employs spatial and channel attention mechanisms to refine feature extraction. This is complemented by a Color Enhancement (CE) Module that ensures natural color reproduction and an Over-Exposure Correction (OEC) Module that dynamically adjusts overexposed regions, significantly reducing artifacts and improving the visual balance of the enhanced images. Additionally, self-attention-enriched skip connections are employed to preserve global coherence and fine-grain structural details, ensuring that the enhanced outputs retain their overall context and texture fidelity.

We conducted extensive experiments on the LOL and SICE datasets to evaluate our proposed retinex-based approach for low-light image enhancement. By focusing on underexposed, well-aligned image pairs, we aim to improve visual quality and system performance in challenging lighting conditions. Leveraging deep learning and image processing techniques, our goal is to advance the state-of-the-art in low-light image enhancement and unlock the full potential of vision-based systems in low-light environments.

The contributions of this paper include,
\begin{itemize}
    \item We introduce LUMINA-Net, a retinex-based approach for enhancing image quality. Our model utilizes paired images and incorporates a Channel-Guidance (CG) Module with spatial and channel attention for refined feature extraction, along with a Color Enhancement (CE) Module to ensure natural color balance.
    \item We propose an Over-Exposure Correction (OEC) Module to dynamically adjust over-exposed regions, incorporating a self-attention-enriched skip connection for enhanced structural context and minimal artifacts.
    \item We conducted extensive experiments on the LOL and SICE datasets to demonstrate the high efficiency and superiority of our LUMINA-Net, which can effectively suppress noise and preserve fine details in the final enhanced results.
\end{itemize}
\section{Related Work}
Low-light image enhancement remains a significant challenge in computer vision and image processing, despite advancements in camera technology. Researchers have developed various solutions to address this issue, ranging from traditional methods to cutting-edge deep learning techniques, Retinex theory, and optimization algorithms. An enhanced Vector Wiener Filter (VWF) is introduced in \cite{ford1997reconstruction} that leverages photon noise correlation and Fourier domain signal-to-noise ratio enhancement, resulting in remarkable image quality and super-resolution capability.

Building on these foundations, further innovations have emerged that resulted in Histogram Equalization (HE) with adaptive illumination adjustment \cite{banik2018contrast}, an adjustable contrast stretching technique to enhance color image contrast \cite{al2018contrast}. Additionally, researchers have explored the use of deep learning techniques, such as Generative Diffusion Prior (GDP) \cite{fei2023generative} and light-effects suppression networks \cite{li2023pixel,lu2022progressive,zhou2023fusion}, to address uneven light distribution and over-enhancement. These advancements have significantly improved low-light image enhancement, but ongoing research is still needed to overcome the remaining challenges.

Recent advancements in deep learning have transformed low-light image enhancement, with state-of-the-art network architectures pushing the limits of image restoration and quality. Researchers have proposed various innovative methods, including unsupervised image de-noising using GANs \cite{lin2023unsupervised}, zero-reference approaches for noise mitigation and image enhancement \cite{cao2024zero,li2023zero1}, and networks leveraging RAW image data, Channel Guidance Net \cite{fu2023raw}, Fourier-based transforms \cite{wang2023fourllie}, UNet-based \cite{li2024color}, Two-stage Single Image De-hazing Network (TSID Net) \cite{wang2024tsid} and Adaptive Illumination Estimation Network (AIE Net) \cite{yu2024joint} architectures. Additionally, techniques such as style transfer-based data generation, teacher networks, and self-supervised approaches have been explored to address challenges like overexposure, underexposure, and noise removal, ultimately leading to significant improvements in low-light image enhancement.

Researchers have proposed various Retinex theory-based methods for low-light image enhancement, leveraging diverse datasets, multi-metric evaluations, and deep learning techniques. These methods decouple illumination and reflectance components to adjust lighting, suppress noise, and revive colors, restoring image clarity and visual fidelity. Recent approaches include combining Retinex theory with self-supervised learning \cite{wang2021seeing,rasheed2022empirical,fu2023learning}, zero-shot learning-based Retinex decomposition method (ZERRINNet) \cite{li2023zero}, and deep neural networks such as Decom-Net, Denoise-Net, Relight-Net, Diff-Retinex, DICNet, machine learning, and CNNs \cite{hai2023r2rnet,yi2023diff,li2024dark,pan2024dicnet} to achieve superior image restoration and enhancement outcomes in low-light environments.
\section{Method}
%%FIGURE2
\begin{figure*}[h]
\centering
\includegraphics[width=0.90\textwidth]{Fig2.png}
\caption{The proposed LUMINA-Net consists of three modules: the N-Net, R-Net, and L-Net for initial image decomposition and processing, followed by the CG Module for channel-guided reflectance enhancement, the CE Module for illumination cross-enhancement, and the OEC Module for over-exposure correction. In Training Phase Input images \( I_1 \) and \( I_2 \) are first passed through N-Net and the refined images are \(i_1, i_2 \) which are then decomposed into reflectance \(R_1, R_2 \) and illumination \( L_1, L_2 \). In Testing Phase final enhanced image \( I_f \) is produced after combining illumination and reflectance components, ensuring improved lighting, reduced noise, and preserved structural details. Multiple loss functions, including perceptual loss \( L_{per} \), Consistency Loss \(L_c \), Reflectance Loss \(L_R \), and Projection loss \( L_{p} \), are employed to optimize the enhancement quality.}
\label{figure2}
\end{figure*}
The proposed LUMINA-Net is a multi-stage framework designed to enhance low-light images through a synergistic approach. Initially, the Channel-Guidance (CG) Module in the Reflectance branch mitigates noise and refines features, preserving intricate details and textures often compromised in low-light conditions. Subsequently, the Color Enhancement (CE) Module in the Illumination branch modulates brightness and contrast while safeguarding texture fidelity, preventing the introduction of artificial artifacts during illumination adjustment. Finally, the Over-Exposure Correction (OEC) Module harmonizes excessively bright areas, ensuring uniform lighting and retaining information in highlighted regions, thereby producing a more balanced and visually appealing image. The overall network framework is shown in Figure \ref{figure2}.
%PAIRED IMAGES
\subsection{Preliminary}
The Retinex theory models a low-light image $I$ \cite{sun2024di} as, 
\begin{equation}
    I = L \circ R ,
    \label{Eq1}
\end{equation}
where $\circ$ denotes element-wise multiplication. Illumination $(L)$ represents the light intensity in the scene, expected to be smooth and texture-less, while reflectance $(R)$ captures the inherent properties of objects, such as textures and details. Conventional illumination and reflectance estimation techniques rely on predefined, hand-crafted priors that often do not accommodate the complexity and variability of real-world scenes and lighting conditions \cite{sethu2023comprehensive}. To address this limitation, we exploit paired low-light images, $I_1$ and $I_2$ with same reflectance $R'$ but different illuminations $L_1'$ and $L_2'$.
\begin{equation}
    I_1 = L_1' \circ R', \quad I_2 = L_2' \circ R' .
    \label{Eq2}
\end{equation}

The introduction of paired low-light images in LUMINA-Net injects additional constraints and valuable information, bolstering the robustness of illumination-reflectance decomposition.
\subsection{Proposed Method}
Our proposed LUMINA-Net image enhancement technique seamlessly integrates three synergistic components, the Channel-Guided (CG) Module for adaptive illumination-reflectance decomposition, the Color Enhancement (CE) Module for vibrant color restoration and refinement, and the Over-Exposure Correction (OEC) Module for balanced brightness and detail preservation. This combination enables the transformation of low-quality images into visually stunning and detailed outputs, yielding a robust and efficient image enhancement framework.
%CGNET
\subsubsection{\textbf{Channel-Spatial Guidance (CG) Module}} The CG Module a pivotal role in enhancing the quality of reflectance maps generated from illumination-reflectance decomposition. This module leverages both Channel Attention and Spatial Attention mechanisms to selectively emphasize crucial features and suppress noise, resulting in refined reflectance maps. Initially, the CG Module takes two low-light input images with reflectance maps $R_1$ and $R_2$ that represents the intrinsic properties of the scene under different lighting conditions.

The \textbf{Channel Attention Mechanism} then identifies significant feature channels within these reflectance maps. This is achieved through global average pooling (GAP), which generates channel descriptors that capture the importance of each channel. These descriptors are subsequently refined via a convolutional layer and sigmoid activation, producing channel-wise weights that ensure the most critical features are emphasized.

In parallel, the \textbf{Spatial Attention Mechanism} targets essential spatial regions within the reflectance maps, such as edges and textures. This is accomplished by performing both global average pooling and max pooling along the channel dimension, generating spatial descriptors. These descriptors are then processed through a convolutional layer and sigmoid activation, producing a spatial weight map that highlights key regions for refinement.

The synergistic combination of Channel Attention and Spatial Attention mechanisms yields a refined reflectance map from $R_{f1}$ and $R_{f2}$. This integrated approach preserves high-frequency details providing a more accurate and cleaner representation of reflectance, robust to both low-light and normal-light conditions. The CG Module significantly leverages the strengths of both attention mechanisms to enhance the reflectance refinement, mitigating noise and highlighting crucial features to achieve superior image reconstruction outcomes that surpass those of traditional Retinex-based approaches.
%CEM
\subsubsection{\textbf{Color Enhancement (CE) Module}} The Color Enhancement (CE) Module plays a crucial role in refining the illumination maps $L_1$ and $L_2$. Its primary objective is to enhance the brightness and contrast of the image, particularly in low-light regions, while ensuring natural and smooth lighting transitions.

To achieve this, the module employs a series of convolutional layers to improve the quality of the illumination maps. Additionally, it utilizes Adaptive Average Pooling (AAP), which produces a global descriptor for each channel. The descriptor is then processed through a fully connected layer and sigmoid activation, generating channel-wise attention weights. These weights modulate the features of the illumination maps, enhancing important features while suppressing irrelevant information.

%The refined illumination maps, $L_{f1}$ and $L_{f2}$, are computed by integrating the original illumination maps with channel-attended features. 
The refined illumination maps, $L_{f1}$ and $L_{f2}$, are computed using Adaptive Average Pooling (AAP), which adaptively aggregates spatial information to enhance the quality of the illumination estimates. This process improves the accuracy and quality of the illumination estimates by leveraging attention mechanisms, which focus on the most relevant features for illumination correction. The channel-attended features help adaptively adjust the illumination maps, refining them for better performance in both low-light and normal-light conditions. The result is a more visually appealing and well-lit image, with enhanced clarity and detail, which can be applied in various lighting scenarios while preserving the scene's natural appearance.
%OCEM
\subsubsection{\textbf{Over-Exposure Correction (OEC) Module}} The Over-Exposure Correction (OEC) Module is designed to address the issue of overexposure by refining the combined illumination $(L_f)$ and reflectance $(R_f)$ maps. This process restores details in overexposed regions, ensuring a balanced exposure through an intermediate image representation, generated by element-wise multiplying the refined illumination and reflectance map. This intermediate representation is then passed through the OEC Module, which is specifically designed to handle areas of the image that are excessively bright or saturated.

The module employs residual learning along with sigmoid activation to suppress overexposure artifacts while preserving fine image details. The output of the OEC Module is the final enhanced image $I_f$, which retains natural brightness and structure while correcting overexposed regions and resulting in a well-balanced image with improved exposure.
%LOSS
\subsection{Loss Functions}
The LUMINA-Net architecture employs a multi-faceted loss function strategy to ensure effective training and image enhancement. This integrated approach minimizes differences between predicted and ground-truth images while preserving critical image characteristics, including perceptual quality, spatial smoothness, and reflectance consistency, thereby achieving a harmonious balance between visual fidelity and structural integrity.
%Perceptual loss
%\subsubsection{\textbf{Perceptual Loss ($L_{\text{per}}$)}} The $L_{\text{per}}$ measures the similarity between the predicted and ground-truth images in a high-level feature space, capturing their perceptual quality. This loss retains essential visual details during image reconstruction by comparing features extracted from a pre-trained model.
%\begin{equation}
   % L_{\text{per}} = ||(I_1, I_f)||_2^2 ,
   % \label{Eq3}
%\end{equation}
%where $I_1$ and $I_f$ are the feature maps of the input image $I_1$ and the generated image $I_f$, respectively.
%Projection loss
\subsubsection{\textbf{Projection Loss($L_p$)}}  The projection step ensures that the input image is more suitable for decomposition under the Retinex model by removing noise and irrelevant features. The projection loss measures the difference between the original input image $I_1$ and the projected image $i_1$, guiding the transformation of the original image into a cleaner, noise-free representation that better aligns with the Retinex assumptions. This loss can be expressed as:
\begin{equation}
    L_p = ||I_1 - i_1||_2^2 .
    \label{Eq3}
\end{equation}

By reallocating the decomposition error to the projection stage, this process ensures more accurate and detailed reflectance and illumination maps, resulting in enhanced image quality and realistic reconstructions.
%Consistency Loss
\subsubsection{\textbf{Consistency Loss ($L_C$)}} The $L_C$ is derived from the Retinex theory and plays a pivotal role in maintaining consistency between the reflectance maps of paired low-light images. This consistency enforces accurate reflectance decomposition and implicitly addresses sensor noise without requiring additional handcrafted constraints. The loss is defined as:
\begin{equation}
    L_C = ||R_{f1} - R_{f2}||_2^2 ,
    \label{Eq4}
\end{equation}
where $R_{f1}$ and $R_{f2}$ are the reflectance components of the paired low-light images.

By minimizing the difference between the reflectance maps of the two images, $L_C$   leverages the randomness of noise across paired images, enabling effective noise removal while ensuring reflectance consistency. This enhances the robustness and accuracy of the decomposition process for low-light image enhancement.
%Retinex Loss
\subsubsection{\textbf{Retinex Loss ($L_R$)}}In this approach, the core concept is to decompose low-light images into two distinct components: illumination and reflectance. The goal is to estimate the reflectance map that captures the intrinsic properties of the scene and the illumination map that represents the varying lighting conditions. To achieve this, a set of constraints is applied to ensure that the decomposition is consistent and physically meaningful, resulting in a high-quality enhancement of low-light images. The loss function that facilitates this decomposition is formulated as follows:
\begin{align*}
    L_R = ||R_{f1} \circ L_{f1} - i||_2^2 
    \quad + ||R_{f1} - i/stopgrad(L_{f1})||_2^2
\end{align*}
\begin{equation}
    \quad + ||L - L_0||_2^2 + ||\nabla L||_1 ,
\label{Eq5}
\end{equation}
where \(i\) denotes the input low-light image, \(R_{f1}\) and \(L_{f1}\) are the predicted reflectance and illumination maps, \(L_0\) is the initial estimate of the illumination, and \(\nabla L\) represents the gradient of the illumination map. \( ||L - L_0||_2^2 \), forces the illumination estimate to be close to the initial illumination \(L_0\), which is computed using the maximum values of the R, G, and B channels of the input image. This initialization serves as a prior, guiding the network toward more realistic illumination estimates. \( ||\nabla L||_1 \), is a smoothness constraint that encourages the illumination map to have smooth transitions, avoiding sharp or unnatural changes in lighting. By minimizing this loss function, the network effectively separates the reflectance and illumination components, improving the overall quality and realism of the enhanced low-light image without relying on extensive handcrafted priors or ground-truth data.

%Total Variation Loss
%\subsubsection{\textbf{Total Variation Loss ($L_{\text{TV}}$)}} The $L_{\text{TV}}$ loss promotes spatial smoothness in the predicted illumination map (L) by penalizing large pixel-wise gradients. This encourages a more natural and smoother image.
%\begin{align*}
    %L_{\text{TV}} & = ||\nabla R_{f}||_1\\
   % &= \sum_{i=1}^{n} \sum_{j=1}^{m} |R_{f}(i+1,j) - R_{f}(i,j)|
%\end{align*}
%\begin{equation}
%\quad + |R_{f}(i,j+1) - R_{f}(i,j)|
%\label{Eq7}
%\end{equation}
%where $R_{f}$ is the refined reflectance map, $\nabla R_{f}$ is the gradient of the refined reflectance map and $||.||_1$ is the L1 norm.

%PerceptualLoss
\subsubsection{\textbf{Perceptual Loss ($L_{per}$)}} The $L_{per}$ measures the similarity
between the predicted and ground-truth images in a high level feature space, capturing their perceptual quality. This loss retains essential visual details during image reconstruction by comparing features extracted from a pre-trained model.
\begin{equation}
    L_{per} = ||(\phi(I_{f1}), \phi(I_{GT}))||_2^2 ,
\end{equation}
where $I_{f1}$ is the predicted image, $I_{GT}$ is the ground-truth image, and \( \phi(\cdot) \) represents the feature extraction function of the pretrained model.
%Edge Loss
%\subsubsection{\textbf{Edge Loss ($L_{\text{edge}}$)}} The $L_{\text{edge}}$ preserves crucial edge details by comparing the edge maps of predicted and target images. It utilizes Sobel filters to detect horizontal and vertical edges, ensuring sharp transitions and structural details are retained in the enhanced image.
%\begin{align*}
   % L_{\text{edge}} = ||\nabla R_{f} - \nabla GT||_1
    %&= \sum_{i=1}^{n} \sum_{j=1}^{m} |\nabla R_{f}(i,j)
%\end{align*}
%\begin{equation}
%\quad - \nabla GT(i,j)|
%\label{Eq8}
%\end{equation}
%where $R_{f}$ is the refined reflectance map, $GT$ is the ground-truth reflectance map, $\nabla$ is the gradient operator and $||.||_1$ is the L1 norm. Edge loss is minimized to produce refined reflectance maps with well-defined boundaries and detailed textures, ultimately leading to more realistic and accurate image reconstructions.
%Combined Loss
\subsection{Combined Loss}
The final loss function is a meticulously crafted weighted sum of individual losses, synergistically training the model to minimize perceptual, consistency, projection, reflectance, total variation and edge losses while preserving naturalness and spatial consistency. The model achieves a harmonious balance between visual fidelity, detail retention, and spatial coherence, ultimately enhancing its performance and generating high-quality images.
\begin{align*}
    L_{\text{All}} = w_0 \times L_p + w_1 \times L_{C} + w_2 \times L_{R}
\end{align*}
\begin{equation}
+ w_3 \times L_{per} ,
\label{Eq9}
\end{equation}
where $L_{p}$ is the projection loss, $L_{C}$ is the reflectance consistency loss, $L_{R}$ is the retinex loss, $L_{pre}$ is the perceptual loss, and $w_0, w_1, w_2, w_3$ are weights controlling the importance of each loss term. 
\section{Experiments}
%Table1
\begin{table*}[ht]
\centering
\caption{QUANTITATIVE COMPARISONS WITH STATE-OF-THE-ART METHODS ON LOL AND SICE DATASETS. “T”, “S”, and “U” REPRESENT “TRADITIONAL", “SUPERVISED", AND “UNSUPERVISED" METHODS, RESPECTIVELY. THE BEST, SECOND AND THIRD PERFORMANCES ARE MARKED IN \textcolor{red}{RED}, \textcolor{blue}{BLUE}, AND \textcolor{green}{GREEN}, RESPECTIVELY.}
\resizebox{0.80\textwidth}{!}{%
\begin{tabular}{l@{\hskip 10pt}|c@{\hskip 10pt}|ccc@{\hskip 10pt}|ccc}
\hline
\textbf{Method}        & \textbf{Type} & \multicolumn{3}{c|}{\textbf{LOL}} & \multicolumn{3}{c}{\textbf{SICE}} \\ 
\cline{3-8}
 & & \textbf{PSNR↑} & \textbf{SSIM↑} & \textbf{LPIPS↓} & \textbf{PSNR↑} & \textbf{SSIM↑} & \textbf{LPIPS↓} \\ 
\hline
SDD \cite{hao2020low}& T & 13.34 & 0.637 & 0.743 & 15.35 & 0.741 & 0.232\\ 
STAR \cite{xu2020star} & T & 12.91 & 0.518 & 0.366 & 15.17 & 0.727 & 0.246\\ 
\hline
MBLLEN \cite{lv2018mbllen} & S & 17.86 & 0.727 & 0.225 & 13.64 & 0.632 & 0.297\\ 
RetinexNet \cite{wei2018deep} & S & 17.55 & 0.648 & 0.379 & 19.89 & 0.783 & 0.276\\ 
GLADNet \cite{wang2018gladnet} & S & 19.72 & 0.680 & 0.321 & 19.98 & 0.837 & 0.203\\ 
KinD \cite{zhang2019kindling} & S & 17.65 & \textcolor{green}{0.775} & \textcolor{blue}{0.171} & \textcolor{blue}{21.10} & 0.838 & \textcolor{green}{0.195}\\ 
DRBN \cite{yang2020fidelity} & S & 16.29 & 0.551 & 0.260 & 15.58 & 0.522 & 0.289\\
URetinexNet \cite{wu2022uretinex} & S & \textcolor{green}{19.84} & \textcolor{red}{0.826} & \textcolor{red}{0.128} & \textcolor{blue}{21.64} & \textcolor{green}{0.843} & \textcolor{blue}{0.192}\\ 
\hline
ZeroDCE \cite{guo2020zero} & U & 14.86 & 0.559 & 0.335 & 18.69 & 0.810 & 0.207\\ 
RRDNet \cite{zhu2020zero} & U & 11.40 & 0.457 & 0.362 & 13.28 & 0.678 & 0.221\\ 
RUAS \cite{liu2021retinex} & U & 16.40 & 0.500 & 0.270 & 16.85 & 0.734 & 0.363\\ 
SCI \cite{ma2022toward} & U & 14.78 & 0.522 & 0.339 & 15.95 & 0.787 & 0.235\\ 
EnlightenGAN \cite{jiang2021enlightengan} & U & 17.48 & 0.651 & 0.322 & 18.73 & 0.822 & 0.216\\ 
PairLIE \cite{fu2023learning} & U & 19.51 & 0.736 & 0.248 & 21.32 & \textcolor{blue}{0.840} & 0.216\\ 
FourierDiff \cite{lv2024fourier} & U & 18.67 & 0.602 & 0.362 & 16.87 & 0.768 & 0.387\\ 
NeRco \cite{yang2023implicit} & U & \textcolor{blue}{22.94} & 0.773 & 0.327 & 17.97 & 0.756 & 0.428\\ 
%LightenDiffusion \cite{IEEEhowto:Jiang} & U & \textbf{21.09} & \textbf{0.821} & 0.310 & 19.08 & 0.776 & 0.375\\ 
LUMINA-Net (Ours) & U &\textcolor{red}{23.67} & \textcolor{blue}{0.814} & \textcolor{green}{0.194} & \textcolor{red}{22.17} & \textcolor{red}{0.857} & \textcolor{red}{0.121}\\ 
\hline
\end{tabular}}
\label{tab:comparison}
\end{table*}
%Figure3
\begin{figure*}[h]
\centering
\includegraphics[width=0.85\textwidth]{Fig3.png}
\caption{Visual comparisons of different low-light image enhancement (LIE) methods. (a) Input image. (b-j) Results from various state-of-the-art methods. (k) Our proposed method, LUMINA-Net, demonstrating superior enhancement. (l) Ground truth image for reference. The images (b-j) show the performance of existing techniques, while (k) highlights the effectiveness of LUMINA-Net in restoring details and colors in low-light conditions.}
\label{figure3}
\end{figure*}
    \subsection{Experimental Datasets}
     LUMINA-Net is trained using low-light image pairs derived from the SICE \cite{cai2018learning} and LOL datasets \cite{wei2018deep}. For evaluation, we select an additional 50 sequences (150 images) from the SICE dataset and utilize the official evaluation set (15 images) from the LOL dataset to assess the model’s performance. Both SICE and LOL contain reference images, allowing us to employ multiple metrics for objective evaluation, including PSNR, SSIM \cite{wang2004image}, LPIPS \cite{zhang2018unreasonable}. A higher PSNR or SSIM score indicates that the enhanced result is closer to the reference image in terms of fidelity and structural similarity. Conversely, lower LPIPS values signify improved enhancement quality and more accurate color reproduction. %Additionally, we use the MEF dataset \cite{ma2015perceptual} for qualitative visual comparisons, further validating the effectiveness of LUMINA-Net against other methods.
     \subsection{Implementation Details}
     We implement \textbf{LUMINA-Net} using PyTorch. During training, images are randomly cropped to a size of \(256 \times 256\) to account for spatial variations. A batch size of 1 is applied for efficient memory usage, given the high-resolution image processing requirements. The ADAM optimizer \cite{kingma2014adam} is employed with an initial learning rate of \(1 \times 10^{-4}\) , along with a cosine annealing learning rate scheduler for gradual decay. The number of training epochs is set to 400 to ensure convergence. For low-light enhancement scenarios, we empirically set the default correction factor to \(\lambda = 0.2\), as suggested in \cite{fu2023learning}. For the LOL dataset, the correction factor was adjusted to \(\lambda = 0.14\). The hyper-parameters \(w_0\), \(w_1\), \(w_2\) and \(w_3\) in the loss formulation are set to \(w_0 = 5\), \(w_1 = 1\), \(w_2 = 1\) and \(w_3 = 0.1\) based on empirical evaluation for optimal performance.  Our models are run on NVIDIA TITAN XP GPUs.
     \subsection{Comparison with state-of-the-arts methods}
     LUMINA-Net is compared with 16 state-of-the-art low-light image enhancement (LIE) methods, which can be grouped into three categories: traditional methods (SDD \cite{hao2020low}, STAR \cite{xu2020star}), supervised approaches (MBLLEN \cite{lv2018mbllen}, RetinexNet \cite{wei2018deep}, GALDNet \cite{wang2018gladnet}, KinD \cite{zhang2019kindling}, DRBN \cite{yang2020fidelity}, URetinexNet \cite{wu2022uretinex}), and unsupervised methods (Zero-DCE \cite{guo2020zero}, RRDNet \cite{zhu2020zero}, RUAS \cite{liu2021retinex}, SCI \cite{ma2022toward}, EnlightenGAN \cite{jiang2021enlightengan}, PairLIE \cite{fu2023learning}, FourierDiff \cite{lv2024fourier}, and NeRco \cite{yang2023implicit}). These comparisons are made based on the performance of each method, using their official codes with the recommended parameters to ensure fairness and consistency in the evaluation. The results obtained from these methods are used as a benchmark for assessing LUMINA-Net’s performance.

     \subsection{Quantitative Comparisons}
     Table \ref{tab:comparison} presents the quantitative performance results on the LOL and SICE datasets. Traditional and unsupervised methods show relatively poor performance, as they face challenges in learning enhancement models without reference images. Traditional methods, relying on fixed algorithms, struggle to adapt to varied lighting conditions, while unsupervised methods, lacking paired references during training, depend on indirect cues, limiting their effectiveness in diverse scenarios.
     
     Supervised methods utilize paired low-light and reference images to learn more precise mappings, yielding superior results. Despite being unsupervised, LUMINA-Net outperforms many supervised methods on both datasets. Its superior PSNR, SSIM, and LPIPS scores highlight its ability to preserve details, ensure color accuracy, and reduce artifacts, demonstrating its robustness in low-light enhancement without the need for reference images.

     %Table \ref{tab:comparison} presents the quantitative performance results on the LOL and SICE datasets. As observed, traditional and unsupervised methods demonstrate relatively poor performance. This is expected, as accurately learning an enhancement model without a reference image is challenging. Furthermore, the effectiveness of these methods heavily relies on the priors employed. However, hand-crafted features may lack adaptability under diverse lighting conditions.  

     %In Table \ref{tab:comparison}, Lumina-Net achieves the best performance among the five unsupervised methods and delivers competitive results when compared with supervised approaches. Since paired low-light images provide ample information for addressing the low-light image enhancement (LIE) task, Lumina-Net reduces the reliance on handcrafted priors. Consequently, Lumina-Net demonstrates a significant improvement in performance.  
     \subsection{Visual comparisons}
     Figure \ref{figure3} highlights the comparative performance of various low-light image enhancement methods on the LOL-real and SICE datasets. LUMINA-Net outperforms other methods by producing natural and visually balanced results with accurate brightness, color reproduction, and contrast. However, RetinexNet suffers from overexposure, which washes out image details and disrupts the natural appearance of colors, resulting in a loss of important features. This limitation is also reflected in RetinexNet's metric values, as it achieves lower performance on metrics that emphasize detail preservation and structural similarity, such as SSIM and LPIPS. 
     
     When evaluating the similarity between the enhanced image and the ground truth, LPIPS (Learned Perceptual Image Patch Similarity) is the most indicative metric. Unlike PSNR and SSIM, which focus on pixel-level accuracy and structural similarity respectively, LPIPS is designed to measure perceptual similarity by comparing high-level features extracted from a deep neural network. This makes it more aligned with human perception, highlighting the issues in RetinexNet's overexposed results. LUMINA-Net consistently achieves better LPIPS values, demonstrating its ability to produce outputs that closely resemble the ground truth in both appearance and perceptual quality.
     
     Other methods, such as ZeroDCE and RUAS, fail to enhance extremely dark regions adequately, often leaving noise and artifacts. Similarly, KinD and URetinexNet, while effective in specific datasets, lack robustness in diverse lighting conditions. LUMINA-Net, in contrast, excels across all metrics, combining detail preservation, structural similarity, and perceptual quality to deliver realistic, high-quality results that outperform competing methods in both quantitative and qualitative evaluations.

    \subsection{Ablation Studies}
    To validate the effectiveness of LUMINA-Net's components, including its modules and overall design, we conducted ablation experiments on the LOL dataset, with results presented in Table \ref{tab:ablation_lol} and Figure \ref{figure4}.

    In Figure \ref{figure4} (a), removing the Over-Exposure Correction (OEC) Module caused overexposure and uneven brightness, significantly lowering PSNR and SSIM scores, emphasizing its importance in brightness management, (b) shows that excluding the Channel-Guided (CG) Module resulted in a loss of reflectance detail and structural consistency, underlining its role in reflectance-illumination separation for sharper details, (c), removing the Color Enhancement (CE) Module led to color distortions and reduced vibrancy, highlighting its importance in color accuracy. Finally, (d), our enhanced result, demonstrates the full LUMINA-Net configuration, showing preserved fine details, accurate colors, and balanced exposure. The results confirm that OEC, CG, and CE Modules are crucial for achieving high-quality low-light image enhancement, with reflectance enhancement being essential for maintaining sharpness and detail. 

    %In the first experiment, removing the Over-Exposure Correction (OEC) Module resulted in overexposed regions and uneven brightness distribution, highlighting its critical role in controlling brightness and ensuring balanced exposure across the image. The absence of the OEC Module led to unnatural artifacts, particularly in high-intensity regions, confirming its necessity for managing bright areas and preventing overexposure. In the second experiment, excluding the Channel-Guided (CG) Module caused a loss of reflectance detail and structural inconsistency, demonstrating the importance of the CG Module in accurately separating reflectance from illumination, which is essential for preserving sharp details and texture in low-light conditions. The third experiment, which involved removing the Color Enhancement (CE) Module, resulted in noticeable color distortions, reduced vibrancy, and unnatural hues, emphasizing the critical role of this module in maintaining color fidelity and realistic illumination adjustments. Finally, removing reflectance enhancement within the CG Module led to blurred textures and a decline in fine detail preservation, showing the significant impact of reflectance enhancement on recovering sharp, realistic textures and edge details crucial for overall image clarity and quality.

%Figure4
\begin{figure*}[t]
\centering
\includegraphics[width=0.88\textwidth]{Fig4.png}
\caption{Visual comparisons of the ablation studies. (a) Without the OEC Module. (b) Without the CG Module. (c) Without the CE Module. Our result (d) demonstrates the most visually accurate restoration, closely matching the ground truth. The ablation studies highlight the contribution of each module in improving the overall enhancement performance.}
\label{figure4}
\end{figure*}

%Table2
%\setlength{\tabcolsep}{12pt}
\begin{table}[ht]
\centering
\large
\caption{QUANTITATIVE RESULTS OF ABLATION STUDIES ON LOL DATASET. THE BEST RESULTS ARE MARKED IN \textbf{BOLD}.}
\begin{tabular}{lcc}
\hline
\textbf{Method} & \textbf{PSNR$\uparrow$} & \textbf{SSIM$\uparrow$} \\
\hline
w/o OEC    & 19.70  & 0.648 \\
w/o CG     & 21.50  & 0.624 \\
w/o CE     & 22.69  & 0.784 \\
Ours       & \textbf{23.67}  & \textbf{0.814} \\
\hline
\end{tabular}
\label{tab:ablation_lol}
\end{table}

\section{Conclusion}
In this paper, to address the challenges faced by existing low-light image enhancement methods, such as preserving image details in dark regions and accurately recovering colors, we propose LUMINA-Net. This method combines three key modules: the Channel-Guided (CG) Module for enhancing reflectance in dark regions, the Color Enhancement (CE) Module for adjusting illumination and ensuring accurate color restoration, and the Over-Exposure Correction (OEC) Module for handling overexposure. These modules work together to significantly improve both the quality and the naturalness of enhanced images. Through extensive experiments on low-light datasets, LUMINA-Net consistently demonstrates superior performance over existing state-of-the-art methods, achieving better detail preservation, accurate color recovery, and reduced artifacts, making it a promising solution for real-world low-light imaging applications.

\section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.
\bibliographystyle{IEEEtran}
%{\footnotesize % Change size here
%\bibliography{references}
%}
\begin{thebibliography}{00}
\small
\bibitem{qu2024double}
J.~Qu, R.~W. Liu, Y.~Gao, Y.~Guo, F.~Zhu, and F.-Y. Wang, ``Double domain guided real-time low-light image enhancement for ultra-high-definition transportation surveillance,'' \emph{IEEE Transactions on Intelligent Transportation Systems}, vol.~25, no.~8, pp. 9550--9562, 2024.

\bibitem{liu2022attention}
R.~W. Liu, N.~Liu, Y.~Huang, and Y.~Guo, ``Attention-guided lightweight generative adversarial network for low-light image enhancement in maritime video surveillance,'' \emph{The Journal of Navigation}, vol.~75, no.~5, pp. 1100--1117, 2022.

\bibitem{li2024light}
J.~Li, B.~Li, Z.~Tu, X.~Liu, Q.~Guo, F.~Juefei-Xu, R.~Xu, and H.~Yu, ``Light the night: A multi-condition diffusion framework for unpaired low-light enhancement in autonomous driving,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 15205--15215, 2024.

\bibitem{liu2024lane}
Y.~Liu, Y.~Wang, and Q.~Li, ``Lane detection based on real-time semantic segmentation for end-to-end autonomous driving under low-light conditions,'' \emph{Digital Signal Processing}, vol.~155, p. 104752, 2024.

\bibitem{ma2021structure}
Y.~Ma, J.~Liu, Y.~Liu, H.~Fu, Y.~Hu, J.~Cheng, H.~Qi, Y.~Wu, J.~Zhang, and Y.~Zhao, ``Structure and illumination constrained GAN for medical image enhancement,'' \emph{IEEE Transactions on Medical Imaging}, vol.~40, no.~12, pp. 3955--3967, 2021.

\bibitem{gomez2019low}
P.~G{'o}mez, M.~Semmler, A.~Sch{"u}tzenberger, C.~Bohr, and M.~D{"o}llinger, ``Low-light image enhancement of high-speed endoscopic videos using a convolutional neural network,'' \emph{Medical \& Biological Engineering \& Computing}, vol.~57, pp. 1451--1463, 2019.

\bibitem{fu2022efficient}
Z.~Fu, M.~Song, C.~Ma, J.~Nasti, V.~Tyagi, G.~Lloyd, and W.~Tang, ``An efficient hybrid model for low-light image enhancement in mobile devices,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 3057--3066, 2022.

\bibitem{zhou2024real}
Y.~Zhou, C.~MacPhee, W.~Gunawan, A.~Farahani, and B.~Jalali, ``Real-time low-light video enhancement on smartphones,'' \emph{Journal of Real-Time Image Processing}, vol.~21, no.~155, p. 155, 2024.

\bibitem{hwang2006full}
B.-W. Hwang, S.~Kim, and S.-W. Lee, ``A full-body gesture database for automatic gesture recognition,'' \emph{7th International Conference on Automatic Face and Gesture Recognition (FGR06)}, pp. 243--248, 2006.

\bibitem{banik2018contrast}
P.~P. Banik, R.~Saha, and K.-D. Kim, ``Contrast enhancement of low-light image using histogram equalization and illumination adjustment,'' \emph{2018 International Conference on Electronics, Information, and Communication (ICEIC)}, pp. 1--4, 2018.

\bibitem{park2022histogram}
J.~Park, A.~G. Vien, J.-H. Kim, and C.~Lee, ``Histogram-based transformation function estimation for low-light image enhancement,'' \emph{2022 IEEE International Conference on Image Processing (ICIP)}, pp. 1--5, 2022.

\bibitem{lee2001automatic}
M.-S. Lee, Y.-M. Yang, and S.-W. Lee, ``Automatic video parsing using shot boundary detection and camera operation analysis,'' \emph{Pattern Recognition}, vol.~34, no.~3, pp. 711--719, 2001.

\bibitem{wang2021seeing}
R.~Wang, X.~Xu, C.-W. Fu, J.~Lu, B.~Yu, and J.~Jia, ``Seeing dynamic scene in the dark: A high-quality video dataset with mechatronic alignment,'' \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)}, pp. 9700--9709, 2021.

\bibitem{hai2023r2rnet}
J.~Hai, Z.~Xuan, R.~Yang, Y.~Hao, F.~Zou, F.~Lin, and S.~Han, ``R2rnet: Low-light image enhancement via real-low to real-normal network,'' \emph{Journal of Visual Communication and Image Representation}, vol.~90, p. 103712, 2023.

\bibitem{yi2023diff}
X.~Yi, H.~Xu, H.~Zhang, L.~Tang, and J.~Ma, ``Diff-retinex: Rethinking low-light image enhancement with a generative diffusion model,'' \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp. 12302--12311, 2023.

\bibitem{yang2007reconstruction}
H.-D. Yang and S.-W. Lee, ``Reconstruction of 3D human body pose from stereo image sequences based on top-down learning,'' \emph{Pattern Recognition}, vol.~40, no.~11, pp. 3120--3131, 2007.

\bibitem{ford1997reconstruction}
S.~D. Ford, B.~M. Welsh, M.~C. Roggemann, and D.~J. Lee, ``Reconstruction of low-light images by use of the vector Wiener filter,'' \emph{JOSA A}, vol.~14, no.~10, pp. 2678--2691, 1997.

\bibitem{al2018contrast}
Z.~Al-Ameen, ``Contrast enhancement for color images using an adjustable contrast stretching technique,'' \emph{International Journal of Computing}, vol.~17, no.~2, pp. 74--80, 2018.

\bibitem{fei2023generative}
B.~Fei, Z.~Lyu, L.~Pan, J.~Zhang, W.~Yang, T.~Luo, B.~Zhang, and B.~Dai, ``Generative diffusion prior for unified image restoration and enhancement,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 9935--9946, 2023.

\bibitem{li2023pixel}
X.~Li, M.~Liu, and Q.~Ling, ``Pixel-wise gamma correction mapping for low-light image enhancement,'' \emph{IEEE Transactions on Circuits and Systems for Video Technology}, vol.~34, no.~2, pp. 681--694, 2023.


\bibitem{lu2022progressive}
Y.~Lu and S.-W. Jung, ``Progressive joint low-light enhancement and noise removal for raw images,'' \emph{IEEE Transactions on Image Processing}, vol.~31, pp. 2390--2404, 2022.

\bibitem{zhou2023fusion}
L.~Zhou, F.~S. Alenezi, A.~Nandal, A.~Dhaka, T.~Wu, D.~Koundal, A.~Alhudhaif, and K.~Polat, ``Fusion of overexposed and underexposed images using caputo differential operator for resolution and texture based enhancement,'' \emph{Applied Intelligence}, vol.~53, no.~12, pp. 15836--15854, 2023.

\bibitem{lin2023unsupervised}
X.~Lin, C.~Ren, X.~Liu, J.~Huang, and Y.~Lei, ``Unsupervised image denoising in real-world scenarios via self-collaboration parallel generative adversarial branches,'' \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp. 12642--12652, 2023.

\bibitem{cao2024zero}
P.~Cao, Q.~Niu, Y.~Zhu, and T.~Li, ``A zero-reference low-light image-enhancement approach based on noise estimation,'' \emph{Applied Sciences}, vol.~14, no.~7, p. 2846, 2024.

\bibitem{li2023zero1}
Y.~Li, Y.~Niu, R.~Xu, and Y.~Chen, ``Zero-referenced low-light image enhancement with adaptive filter network,'' \emph{Engineering Applications of Artificial Intelligence}, vol.~124, p. 106611, 2023.

\bibitem{fu2023raw}
Y.~Fu, Y.~Hong, Y.~Zou, Q.~Liu, Y.~Zhang, N.~Liu, and C.~Yan, ``Raw image based over-exposure correction using channel-guidance strategy,'' \emph{IEEE Transactions on Circuits and Systems for Video Technology}, vol.~34, no.~4, pp. 2749--2762, 2023.

\bibitem{wang2023fourllie}
C.~Wang, H.~Wu, and Z.~Jin, ``Fourllie: Boosting low-light image enhancement by fourier frequency information,'' \emph{Proceedings of the 31st ACM International Conference on Multimedia}, pp. 7459--7469, 2023.

\bibitem{li2024color}
Y.~Li, K.~Xu, G.~P. Hancke, and R.~W. Lau, ``Color shift estimation-and-correction for image enhancement,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 25389--25398, 2024.

\bibitem{wang2024tsid}
S.~Wang, Q.~Hou, J.~Li, and J.~Liu, ``TSID-Net: a two-stage single image dehazing framework with style transfer and contrastive knowledge transfer,'' \emph{The Visual Computer}, pp. 1--18, 2024.

\bibitem{yu2024joint}
T.~Yu, S.~Wang, W.~Chen, F.~R. Yu, V.~C. Leung, and Z.~Tian, ``Joint self-supervised enhancement and denoising of low-light images,'' \emph{IEEE Transactions on Emerging Topics in Computational Intelligence}, vol.~8, no.~2, pp. 1800--1813, 2024.

\bibitem{rasheed2022empirical}
M.~T. Rasheed, G.~Guo, D.~Shi, H.~Khan, and X.~Cheng, ``An empirical study on retinex methods for low-light image enhancement,'' \emph{Remote Sensing}, vol.~14, no.~18, p. 4608, 2022.

\bibitem{fu2023learning}
Z.~Fu, Y.~Yang, X.~Tu, Y.~Huang, X.~Ding, and K.-K. Ma, ``Learning a simple low-light image enhancer from paired low-light instances,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 22252--22261, 2023.

\bibitem{li2023zero}
W.~Li, B.~Xiong, Q.~Ou, X.~Long, J.~Zhu, J.~Chen, and S.~Wen, ``Zero-shot enhancement of low-light image based on retinex decomposition,'' \emph{arXiv preprint arXiv:2311.02995}, 2023.

\bibitem{li2024dark}
Z.~Li, Y.~Pan, H.~Yu, and Z.~Zhang, ``DARK: Denoising, amplification, restoration kit,'' \emph{arXiv preprint arXiv:2405.12891}, 2024.

\bibitem{pan2024dicnet}
H.~Pan, B.~Gao, X.~Wang, C.~Jiang, and P.~Chen, ``DICNet: achieve low-light image enhancement with image decomposition, illumination enhancement, and color restoration,'' \emph{The Visual Computer}, pp. 1--17, 2024.

\bibitem{sun2024di}
S.~Sun, W.~Ren, J.~Peng, F.~Song, and X.~Cao, ``DI-Retinex: Digital-imaging retinex theory for low-light image enhancement,'' \emph{arXiv preprint arXiv:2404.03327}, 2024.

\bibitem{sethu2023comprehensive}
S.~Sethu, J.~Devaraj, and D.~Wang, ``A comprehensive review of deep learning based illumination estimation,'' \emph{Preprints}, vol.~10, p. 20944, 2023.

\bibitem{hao2020low}
S.~Hao, X.~Han, Y.~Guo, X.~Xu, and M.~Wang, ``Low-light image enhancement with semi-decoupled decomposition,'' \emph{IEEE Transactions on Multimedia}, vol.~22, no.~12, pp. 3025--3038, 2020.

\bibitem{xu2020star}
J.~Xu, Y.~Hou, D.~Ren, L.~Liu, F.~Zhu, M.~Yu, H.~Wang, and L.~Shao, ``Star: A structure and texture aware retinex model,'' \emph{IEEE Transactions on Image Processing}, vol.~29, pp. 5022--5037, 2020.

\bibitem{lv2018mbllen}
F.~Lv, F.~Lu, J.~Wu, and C.~Lim, ``MBLLEN: Low-light image/video enhancement using cnns,'' \emph{Proceedings of the British Machine Vision Conference (BMVC)}, vol.~220, no.~1, p. 4, 2018.

\bibitem{wei2018deep}
C.~Wei, W.~Wang, W.~Yang, and J.~Liu, ``Deep retinex decomposition for low-light enhancement,'' \emph{arXiv preprint arXiv:1808.04560}, 2018.

\bibitem{wang2018gladnet}
W.~Wang, C.~Wei, W.~Yang, and J.~Liu, ``Gladnet: Low-light enhancement network with global awareness,'' \emph{2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018)}, pp. 751--755, 2018.

\bibitem{zhang2019kindling}
Y.~Zhang, J.~Zhang, and X.~Guo, ``Kindling the darkness: A practical low-light image enhancer,'' \emph{Proceedings of the 27th ACM International Conference on Multimedia}, pp. 1632--1640, 2019.

\bibitem{yang2020fidelity}
W.~Yang, S.~Wang, Y.~Fang, Y.~Wang, and J.~Liu, ``From fidelity to perceptual quality: A semi-supervised approach for low-light image enhancement,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 3063--3072, 2020.

\bibitem{wu2022uretinex}
W.~Wu, J.~Weng, P.~Zhang, X.~Wang, W.~Yang, and J.~Jiang, ``Uretinex-net: Retinex-based deep unfolding network for low-light image enhancement,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 5901--5910, 2022.

\bibitem{guo2020zero}
C.~Guo, C.~Li, J.~Guo, C.~C. Loy, J.~Hou, S.~Kwong, and R.~Cong, ``Zero-reference deep curve estimation for low-light image enhancement,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 1780--1789, 2020.

\bibitem{zhu2020zero}
A.~Zhu, L.~Zhang, Y.~Shen, Y.~Ma, S.~Zhao, and Y.~Zhou, ``Zero-shot restoration of underexposed images via robust retinex decomposition,'' \emph{2020 IEEE International Conference on Multimedia and Expo (ICME)}, pp. 1--6, 2020.

\bibitem{liu2021retinex}
R.~Liu, L.~Ma, J.~Zhang, X.~Fan, and Z.~Luo, ``Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 10561--10570, 2021.

\bibitem{ma2022toward}
L.~Ma, T.~Ma, R.~Liu, X.~Fan, and Z.~Luo, ``Toward fast, flexible, and robust low-light image enhancement,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 5637--5646, 2022.

\bibitem{jiang2021enlightengan}
Y.~Jiang, X.~Gong, D.~Liu, Y.~Cheng, C.~Fang, X.~Shen, J.~Yang, P.~Zhou, and Z.~Wang, ``Enlightengan: Deep light enhancement without paired supervision,'' \emph{IEEE Transactions on Image Processing}, vol.~30, pp. 2340--2349, 2021.

\bibitem{lv2024fourier}
X.~Lv, S.~Zhang, C.~Wang, Y.~Zheng, B.~Zhong, C.~Li, and L.~Nie, ``Fourier priors-guided diffusion for zero-shot joint low-light enhancement and deblurring,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 25378--25388, 2024.

\bibitem{yang2023implicit}
S.~Yang, M.~Ding, Y.~Wu, Z.~Li, and J.~Zhang, ``Implicit neural representation for cooperative low-light image enhancement,'' \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp. 12918--12927, 2023.

\bibitem{cai2018learning}
J.~Cai, S.~Gu, and L.~Zhang, ``Learning a deep single image contrast enhancer from multi-exposure images,'' \emph{IEEE Transactions on Image Processing}, vol.~27, no.~4, pp. 2049--2062, 2018.

\bibitem{wang2004image}
Z.~Wang, A.~C. Bovik, H.~R. Sheikh, and E.~P. Simoncelli, ``Image quality assessment: from error visibility to structural similarity,'' \emph{IEEE Transactions on Image Processing}, vol.~13, no.~4, pp. 600--612, 2004.

\bibitem{zhang2018unreasonable}
R.~Zhang, P.~Isola, A.~A. Efros, E.~Shechtman, and O.~Wang, ``The unreasonable effectiveness of deep features as a perceptual metric,'' \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 586--595, 2018.

\bibitem{kingma2014adam}
D.~P. Kingma, ``Adam: A method for stochastic optimization,'' \emph{arXiv preprint arXiv:1412.6980}, 2014.

\end{thebibliography}
\end{document}