
\begin{table}
\newcolumntype{G}{>{\columncolor[gray]{0.9}}c} 
\renewcommand{\arraystretch}{1.2}
% \vspace{0.5em}
% \hspace{-4mm}
\centering
\scalebox{0.85}{
{
\begin{tabular}{lGcccccccc}
\toprule
\textbf{Model} & \!\textbf{Overall}\! & \multicolumn{2}{c}{\textbf{General}} & \textbf{Knowledge} & \multicolumn{2}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Safety}}  & \textbf{VQA} \\
\cmidrule(lr){3-4} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& & \!Correctness\! & \!Preference\! & & Math & \!Coding\! & Bias & \!Toxicity\! \\
\midrule
\#Examples & \totalsamples & 623 & 654 & 630 & 514 & 582 & 508 & 500 & 1200 \\
\midrule
Claude 3.5 Sonnet & 0.720 & 0.626 & 0.678 & 0.739 & 0.686 & 0.651 & 0.768 & 0.606 & 0.856\\
Gemini 1.5 Pro & 0.720 & 0.635 & 0.677 & 0.663 & 0.689 & 0.555 & 0.945 & 0.582 & 0.872 \\
GPT-4o & 0.715 & 0.626 & 0.690 & 0.720 & 0.676 & 0.621 & 0.748 & 0.588 & 0.872\\
Llama-3.2-90B-Vision-Instruct & 0.624 & 0.600 & 0.684 & 0.612 & 0.563 & 0.531 & 0.520 & 0.518 & 0.771\\
Aria & 0.573 & 0.595 & 0.635 & 0.555 & 0.503 & 0.542 & 0.461 & 0.544 & 0.642\\
Molmo-7B-D-0924 & 0.543 & 0.568 & 0.594 & 0.546 & 0.507 & 0.534 & 0.348 & 0.538 & 0.603\\
Llama-3.2-11B-Vision-Instruct & 0.524 & 0.578 & 0.658 & 0.555 & 0.506 & 0.517 & 0.209 & 0.504 & 0.558\\
% Qwen2-VL-7B-Instruct & 0.492& 0.533& 0.511& 0.512& 0.547& 0.498& 0.194& 0.526& 0.537 \\
Llava-1.5-13B & 0.489 & 0.533 & 0.552 & 0.505 & 0.535 & 0.493 & 0.201 & 0.500 & 0.518 \\
\bottomrule
\end{tabular}}}
% \end{adjustbox}
% \vspace{0.5em}
\caption{\textbf{Accuracy of various VLM judges on Multimodal RewardBench}, with a breakdown across task categories. The top-performing models, Claude 3.5 Sonnet, Gemini 1.5 Pro and GPT-4o, achieve only 72\% overall accuracy, suggesting that the benchmark offers a challenging testbed for reward model development.
For the data source and task definitions of each category, see Table \ref{table:dataset_char}.
}
\label{table:result}

\end{table}



