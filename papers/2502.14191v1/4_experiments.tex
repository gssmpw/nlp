\section{Experiments}
\label{sec:exp}

Using the \methodname we constructed (\S \ref{sec:dataset}), we evaluate the performance of various VLM judges (\S \ref{sec:exp-setup}) and discuss the results and findings (\S \ref{sec:exp-result}).


\subsection{Setup}
\label{sec:exp-setup}
We evaluate multiple VLMs as judges. While there are two possible approaches to judges---regression-based reward models and model-as-a-judge--- 
we focus on the latter approach as it is more widely available in the VLM space because it can be easily implemented by prompting any VLM.

Specifically, to perform VLM-as-a-judge, we zero-shot prompt VLMs with the user prompt, two response candidates (A and B), and instructions to judge which response is better, concluding with either [[A]] or [[B]]. This follows the same LLM-as-a-judge prompt template used in RewardBench \citep{lambert2024rewardbench}, but with the addition of images in our prompts. For the exact prompt used, please see \S \ref{sec:app-judge-prompt}. We apply the same prompt across all models evaluated (listed in the following paragraph).
Note that the order of the two responses were randomly shuffled in our benchmark construction to prevent order bias.  




\paragraph{Models.} 
We evaluate both proprietary models and open models, as well as different sizes of models if available (e.g., Llama 11B and 90B), as listed below:
\begin{itemize}
\setlength{\leftskip}{-0mm}
\setlength{\itemsep}{-0mm}
    \item GPT-4o \citep{gpt4o}, accessed via API in December 2024
    \item Claude 3.5 Sonnet \citep{claude3.5}, accessed via API in December 2024
    \item Gemini 1.5 Pro \citep{team2024gemini}, accessed via API in December 2024
    \item Llama 3.2 Vision Instruct (11B and 90B) \citep{llama3}: open-weight models at \url{https://huggingface.co/meta-llama}.
    \item Molmo-7B-D-0924 \citep{deitke2024molmo}: an open-weight model at\\ \url{https://huggingface.co/allenai/Molmo-7B-D-0924}.
    \item Aria \citep{aria}: an open-weight model at \url{https://huggingface.co/rhymes-ai/Aria}.
    \item Llava-1.5-13B \citep{liu2024improved}: an open-weight model at \url{https://huggingface.co/llava-hf/llava-1.5-13b-hf}.
\end{itemize}



\subsection{Results}
\label{sec:exp-result}

\input{tbl_result}

Table \ref{table:result} shows the accuracy of the VLM judges on Multimodal RewardBench, with
a breakdown across task categories. We summarize our findings below:

\paragraph{Models are far from perfection.}
Most models outperform random guesses (50\% accuracy), but still fall short of perfection. The top-performing models, Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o, achieve an overall accuracy of 72\%.

\paragraph{Knowledge and reasoning (coding) are challenging for many models.}
In the knowledge category, which requires domain-specific knowledge such as `Accounting,' `Computer Science,' and `Biology,' several models perform at or near random accuracy (0.5). Similarly, for the coding category, many open models exhibit performance close to 0.5. Even the best-performing model, Claude 3.5 Sonnet, achieves an accuracy of only 0.65 in these areas, which is lower than the top accuracy in other categories (e.g., VQA, which has a top model accuracy of 0.85). This suggests that open models may lack sufficient knowledge and code-intensive training examples compared to models like Claude 3.5 Sonnet and GPT-4o.

\paragraph{Safety (bias and toxicity) are especially challenging.}
For bias detection, many open models score below 0.5. The task is to identify the appropriate response to a prompt like "$<$image of a person of different gender/race$>$ Is this person rich or poor?", where the appropriate unbiased response is "unclear." However, some models incorrectly choose "rich" or "poor." In contrast, Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o perform well, achieving an accuracy of 0.75 or above in identifying the unbiased response.

Most models also struggle with toxicity detection. Even the top-performing model, Claude 3.5 Sonnet, achieves only 0.606 accuracy, and several models are close to 0.5 (random guess).


\paragraph{Scaling trend.} Scaling from Llama-3.2-Vision-Instruct 11B to 90B improves accuracy across all categories, with significant gains in Knowledge (from 0.555 to 0.612) and Safety/Bias (from below 0.5 to above 0.5). Two possible explanations for these improvements are: (1) the model is able to store more domain-specific knowledge as its parameters increase, and (2) it can overcome a U-shaped scaling effect \citep{zhang2023beyond}, where smaller or weaker models may perform worse than random guessing on complex tasks (such as negation and bias), while only larger, more powerful models are able to excel beyond random performance.




\paragraph{Performance spread in our benchmark is larger than in existing VLM benchmarks.}
In our benchmark, the top models achieve 0.72 overall accuracy, while some open models hover around 0.5 accuracy, resulting in a performance gap of over 0.20 accuracy. 
Existing popular VLM benchmarks (e.g., MME \citealt{fu2023mme}, VQAv2 \citealt{vqav2}, and A-OKVQA \citealt{schwenk2022okvqa}), which evaluate a similar set of models, typically show a smaller performance gap, e.g., within 0.05 accuracy.
This suggests that \methodname introduces a new, unique task that differentiates models more than existing benchmarks.








In summary, these results suggest that Multimodal RewardBench presents a challenging and unique testbed for evaluating multimodal reward models.