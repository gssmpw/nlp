\section{Related works}
\paragraph{Multimodal LLMs and benchmarks.}
Vision-language models (VLMs)---which take both text and images as input and generate text as output---have rapidly proliferated in recent years  \citep{liu2024visual, dai2023instructblip, deitke2024molmo, aria, yasunaga2022retrieval, zhou2024transfusion}. These models are applied to a variety of tasks, including visual question answering (VQA), image captioning, and visual instruction following.  
Numerous benchmarks evaluate the performance of VLMs on these tasks, such as LlavaBench \citep{liu2024visual}, VisitBench \citep{bitton2023visit}, Nocaps \citep{agrawal2019nocaps}, RealworldQA \citep{realworldqa}, MMBench \citep{liu2025mmbench}, SEED-Bench \cite{li2023seed}, and MMMU \citep{yue2023mmmu, yue2024mmmu}. However, these benchmarks do not evaluate the task of VLM reward modeling or judging, which is the focus of this work.  

Our benchmarking approach draws from holistic evaluation \citep{liang2022holistic, lee2024vhelm, lee2024holistic}, which assesses model capabilities and limitations across multiple dimensions and has been applied to LLMs, VLMs, and image generation models. We extend this holistic evaluation framework to reward models, examining a range of dimensions critical to VLM reward modeling: long-form generation, VQA, knowledge, reasoning, and safety.





\paragraph{Reward models and judges.}

A reward model takes a prompt and one or more responses as input, and score or rank them based on human preferences such as accuracy, helpfulness, and safety. This reward signal is used to align LLMs or VLMs with human preferences through techniques like RLHF \citep{instructgpt, bai2022constitutional, llama3}.
Common types of reward models include:
(1) Regression-based reward models \citep{stiennon2020learning, instructgpt}, which add a regression head to a base model to output reward scores, and
(2) Model-as-a-judge approaches \citep{zheng2023judging, lee2024prometheusvision, xiong2024llava, chen2024mllm}, which leverage the text generation capabilities of an LLM or VLM to produce a reward score or ranking.

Since the second type (VLM-as-a-judge) can be easily constructed using any VLM and is more widely available, this work focuses on evaluating various VLMs in the model-as-a-judge role, but our benchmark can be used for evaluating any type of reward models.


\paragraph{Benchmarking reward models.}
Developing effective reward models is essential for aligning LLMs and VLMs \citep{instructgpt, llama3}. Existing benchmarks like Anthropic Helpful and Harmless \citep{bai2022training}, OpenAI Summarization \citep{stiennon2020learning}, RewardBench \citep{lambert2024rewardbench}, and JudgeBench \citep{tan2024judgebench} have advanced reward model evaluation, with RewardBench notably providing a diverse benchmark set and leaderboard.

However, these benchmarks are limited to the text modality (LLMs). We extend this effort to multimodal reward models for VLMs. Although there is research focused on evaluating VLM judges \citep{lee2024prometheusvision, xiong2024llava, chen2024mllm, li2024vlrewardbench}, these efforts are primarily restricted to general VQA tasks. They lack coverage of expert knowledge domains (e.g., MMMU) and safety concerns (e.g., bias and toxicity), and their annotations lack rigorous inter-annotator agreement validation by human experts. (Table \ref{tbl:method_comparison}). In contrast, our \methodname provides the first comprehensive, high-quality benchmark for VLM reward models, covering six key areas---general correctness, general preference, knowledge, reasoning, safety, and VQA---annotated by domain experts.