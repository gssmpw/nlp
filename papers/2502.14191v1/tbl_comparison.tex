\begin{table*}[h]
% \begin{center}
% \hspace{-2mm}
\centering
\scalebox{0.85}{
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & ~~\textbf{Multimodal}~~ & 
~~\begin{tabular}{@{}c@{}}\vrule width 0pt depth 0pt height 10pt\textbf{Cover holistic dimensions}\\[-0pt]\vrule width 0pt depth 5pt height 0pt\textbf{(e.g. reasoning, safety)}\end{tabular}~~
& ~~\begin{tabular}{@{}c@{}}\vrule width 0pt depth 0pt height 8pt \textbf{Human expert annotated /}\\[0pt]\textbf{High inter-annotator agreement}\end{tabular}\\
\midrule
\begin{tabular}{@{}l@{}}\vrule width 0pt depth 0pt height 10pt {Anthropic HH}\\[0pt]{OpenAI Summarization}\\[0pt]{JudgeBench}\vrule width 0pt depth 5pt\end{tabular}~~~ &  & & \\
\midrule
\begin{tabular}{@{}l@{}}\vrule width 0pt depth 4pt height 9pt RewardBench\end{tabular}~~~ &  & \ding{52} & \\
\midrule
\begin{tabular}{@{}l@{}}\vrule width 0pt depth 0pt height 10pt {Prometheus-Vision}\\[0pt]{Llava-critic}\\[0pt]
VLRewardBench\\[0pt]{MLLM-as-a-Judge} \vrule width 0pt depth 5pt\end{tabular}~~~ & \ding{52} & & \\
\midrule
\begin{tabular}{@{}l@{}}\vrule width 0pt depth 0pt height 10pt \textbf{Multimodal-}\\[0pt]\textbf{RewardBench (Ours)}\vrule width 0pt depth 5pt\end{tabular}~~~ & \ding{52} & \ding{52} & \ding{52}  \\
\bottomrule
\end{tabular}}
\vspace{-1mm}
\caption{\textbf{Comparison with existing evaluation data for reward models}. Our \methodname is the first \textbf{holistic} benchmark for evaluating reward models for \textbf{multimodal} LLMs (vision-language models).
\label{tbl:method_comparison}
}
% \end{center}
% \vspace{-3mm}
\end{table*}



