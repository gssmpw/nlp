\section{Introduction}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{fig_overview.pdf}
    \caption{\textbf{Illustration of \methodname.} We build a human-annotated benchmark that consists of (multimodal prompt, chosen response, rejected response) triplets (\textbf{\textit{left}}). Using this benchmark, we evaluate the accuracy of various reward models or judges for vision-language models (\textbf{\textit{right}}).
    See \S \ref{sec:app-example} for real examples from our benchmark.
    }
    \label{fig:overview}
\end{figure}
\input{tbl_comparison}
\input{tbl_dataset_detail}



High quality reward models or judges are essential for aligning language models (LMs) and vision-language models (VLMs) \citep{instructgpt, bai2022constitutional, llama3}. 
Reward models assess the quality of model outputs, guiding models towards accurate, helpful, and safe outputs via RLHF-style algorithms. %This is typically achieved through techniques like reinforcement learning from human feedback (RLHF) \citep{instructgpt}.
%To train strong VLMs that excel across a wide range of tasks, developing accurate reward models is essential. 
%This requires establishing comprehensive benchmarks to evaluate reward models across multiple dimensions, including general accuracy, domain knowledge, reasoning, and safety \citep{lambert2024rewardbench, lee2024vhelm}.
%However, the research community currently lacks open benchmarks to evaluate reward models for VLMs. 
However, existing benchmarks for evaluating reward model quality are typically limited to the text modality~\citep{bai2022training,bai2022training,stiennon2020learning,tan2024judgebench}.
% : Anthropic’s Helpful and Harmless \citep{bai2022training}, OpenAI’s Summarization \citep{stiennon2020learning}, RewardBench \citep{lambert2024rewardbench}, and JudgeBench \citep{tan2024judgebench}. %RewardBench provides a diverse benchmark set and leaderboard, which have advanced reward model development for LLMs.
While recent work has evaluated VLM judges \citep{lee2024prometheusvision, xiong2024llava, chen2024mllm, li2024vlrewardbench}, these efforts are limited to general VQA tasks, and their labels are not annotated by human experts (Table \ref{tbl:method_comparison}). 
%As a result, there is a pressing need for a comprehensive, high-quality benchmark to evaluate and support the development of multimodal reward models.

In this paper, we introduce \textbf{Multimodal RewardBench}, a holistic, expert-annotated benchmark to evaluate reward models for VLMs (\S \ref{sec:dataset}). 
We focus on six key capabilities: general correctness, general preference, knowledge, reasoning, safety, and VQA. 
For each, we collect diverse text+image prompts from open-source datasets, responses from various VLMs (including GPT-4o, Claude, Gemini, Llama3), and human annotation to label chosen and rejected responses. We hired expert annotators to ensure high-quality annotation and high inter-annotator agreement. 
In total, our benchmark consists of \totalsamples triplets of (prompt, chosen response, rejected response) across the six areas. 
These triplets can be used directly to evaluate reward model response ranking accuracy. To support more fine grained performance measures, we cover both cases: (1) correct vs incorrect responses, and (2) human-preferred vs non-preferred responses under the condition that both responses are either correct or incorrect. 

We also analyze the performance of various VLM judges on \textbf{Multimodal RewardBench}, including proprietary models (GPT-4o, Claude, Gemini), open models (Molmo, Aria), and different sizes of Llama3 (\S \ref{sec:exp}). We find that:
\begin{itemize}
\setlength{\leftskip}{-0mm}
    \item Most models outperform random guesses (50\% accuracy), but still fall short of human performance levels. The top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve an overall accuracy of 72\%.
    \item Most models struggle on reasoning tasks (both math and coding) and safety tasks (especially toxicity detection).
\end{itemize}
These results suggest that Multimodal RewardBench presents a challenging and unique testbed for evaluating multimodal reward models.
 
In summary, our contributions are as follows:
\begin{itemize}
\setlength{\leftskip}{-0mm}
    \item[1.] We release a holistic, expert-annotated benchmark for evaluating reward models for VLMs, which spans six areas, including knowledge, reasoning, and safety—domains not covered by existing VLM reward model evaluations.
    \item[2.] We analyze the landscape of the current state-of-the-art VLM judges, highlighting trends in scaling, knowledge and reasoning capabilities, and safety (bias and toxicity detection) for VLMs.
\end{itemize}
