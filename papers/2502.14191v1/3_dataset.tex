\section{Multimodal RewardBench}
\label{sec:dataset}

We introduce Multimodal RewardBench, a holistic, expert-annotated benchmark for evaluating VLM reward models. 

\subsection{Overview}

\paragraph{Framework.}
As shown in Figure \ref{fig:overview}, each instance in our benchmark consists of:
\begin{itemize}
\setlength{\leftskip}{0mm}
    \item \textbf{Prompt}: A text-and-image input that users provide to VLMs. For example: "What can you cook with these items $<$image$>$?". We refer to this as the \textbf{base task}.
    \item \textbf{Chosen response (R1) and rejected response (R2)}: Two response candidates, where R1 is the correct or human-preferred response, and R2 is the incorrect or non-preferred response. 
\end{itemize}
For each instance (prompt, R1, R2), where randomly order R1 and R2, a VLM reward model or judge predicts which of R1 or R2 is better (we refer to this as the \textbf{judgment task}), which is a binary classification. We then evaluate the accuracy of these predictions.
We detail how we collect the prompts and responses in \S \ref{sec:dataset-prompt} and how we label chosen and rejected responses in \S \ref{sec:dataset-annotation}.


\paragraph{Holistic dimensions.}

We evaluate VLM reward models across six key dimensions. These dimensions build upon previous holistic evaluations of foundation models (e.g., HELM \citealt{liang2022holistic} and VHELM \citealt{lee2024vhelm}), while incorporating additional aspects specific to reward models, such as correctness judgment and human preference judgment. See Table \ref{table:dataset_char} for a summary.

\begin{itemize}
\setlength{\leftskip}{0mm}

\item \textbf{General correctness}: This dimension evaluates general-domain, long-form generation tasks such as visual instruction following and long captioning. The judgment focuses on response correctness, comparing correct responses against incorrect ones that contain factual errors, visual recognition errors, or reasoning errors.

\item \textbf{General preference}: This dimension also addresses general-domain, long-form generation tasks. However, the judgment focuses on human preferences between responses that are either both correct or both incorrect, identifying which response is more aligned with human preferences.

\item \textbf{Knowledge}: This dimension evaluates tasks requiring domain-specific knowledge in areas such as humanities, social sciences, business, medicine, and STEM. The judgment focuses on response correctness.

\item \textbf{Reasoning}: This dimension assesses problem-solving capabilities in areas such as mathematics and coding, with judgment focused on response correctness.

\item \textbf{Safety}: This dimension evaluates safety awareness, with judgments selecting the safer and correct response. Our safety assessment focuses primarily on bias (avoiding unwarranted associations regarding gender and race) and toxicity (identifying and avoiding offensive or harmful content such as hate speech, violent speech, or abusive language) \citep{lee2024vhelm}. While other safety-related topics like fairness, robustness, and NSFW exist, we defer their evaluation to future versions of Multimodel RewardBench due to limited suitable datasets.

\item \textbf{VQA}: This dimension covers diverse short-form visual question answering tasks established in the research community \citep{liu2025mmbench, li2023seed, realworldqa}. These tasks span various skills including visual perception (object, spatial, and scene recognition) and visual reasoning (action, physical, social, contextual, and temporal reasoning). The judgment focuses on response correctness.
\end{itemize}


Our benchmark is comprehensive in both topic coverage and evaluation methodology. Topic-wise, we evaluate general domain, knowledge, reasoning, and safety dimensions, following HELM's \citep{liang2022holistic} comprehensive approach. Task-wise, we incorporate both long and short response formats, and include two types of judgment tasks: comparing correct versus incorrect responses, and evaluating human preference between responses of similar correctness. This broad coverage ensures reward models can be evaluated for robust VLM alignment across diverse scenarios. Importantly, all prompts in our benchmark incorporate both text and image components. 



\subsection{Prompt and response collection}
\label{sec:dataset-prompt}
%We describe the methodology for collecting prompts and responses across the six categories.

\paragraph{General correctness and preference.}
For prompt collection, we draw prompts from VisitBench \citep{bitton2023visit}, which focus on visual instruction following (e.g., "Write a fairy tale based on this painting"). We also create long caption generation prompts (e.g., "Describe this image in detail") using images from Nocaps \citep{agrawal2019nocaps}.

To gather responses, we use several recent VLMs as follows:
\begin{itemize}
\setlength{\leftskip}{0mm}
\setlength{\itemsep}{-0mm}
    \item GPT-4o \citep{gpt4o}, accessed via API in December 2024
    \item Claude 3.5 Sonnet \citep{claude3.5}, accessed via API in December 2024
    \item Gemini 1.5 Pro \citep{team2024gemini}, accessed via API in December 2024
    \item Llama 3.2 Vision Instruct 90B \citep{llama3}
    % , available at \url{https://huggingface.co/meta-llama}.
    \item Molmo-7B-D-0924 \citep{deitke2024molmo}
    % , available at \url{https://huggingface.co/allenai/Molmo-7B-D-0924}.
    \item Aria \citep{aria}
    % , available at \url{https://huggingface.co/rhymes-ai/Aria}.
\end{itemize}
For each prompt, we randomly select two models from this list to generate responses. 
During our human annotation phase (detailed in \S \ref{sec:dataset-annotation}), annotators are asked to evaluate the correctness and preference of the two responses. Based on their judgments, and for samples where we obtain sufficient inter-annotator agreement (see \S \ref{sec:dataset-annotation}), we create either a `correct vs incorrect response pair' or a `preferred vs non-preferred response pair' (if both responses are either correct or incorrect)


\paragraph{Knowledge.}
We use prompts from MMMU-Pro \citep{yue2024mmmu}, which contains college exam-style multiple-choice questions across 30 knowledge-intensive subjects, spanning humanities, social sciences, business, medicine, and STEM. MMMU-Pro consists of two subsets: MMMU-Pro-Standard, which presents standard image-based questions, and MMMU-Pro-Vision, where questions are embedded within the images themselves, requiring enhanced OCR capabilities. The latter subset uses prompts like ``Write out the multiple-choice question in the image and then solve it.'' We incorporate both subsets in our work.

We modify the prompts to include the instruction "Think step by step before answering," encouraging models to produce chain-of-thought (CoT) reasoning \citep{wei2022chain, kojima2022large} before providing their final answers (see \S \ref{sec:app-example-knowledge} for an example). This is important as evaluating the correctness of intermediate reasoning steps is particularly important in knowledge- and reasoning-intensive tasks \citep{lightman2023let}.

To gather responses, we use the same previously mentioned model set. For each prompt, we randomly select one model and generate 10 different responses. We then select two responses: one that arrives at the correct answer and another that reaches an incorrect answer. The reason we use the same model for preparing these response candidates is to focus on correctness instead of style difference.
Since MMMU-Pro provides ground-truth answer labels for its multiple-choice questions, we can easily verify the correctness of the final answer. We discard prompts where all 10 responses are either correct or incorrect, as these questions are either too simple or too challenging for current VLMs and thus unsuitable for contemporary benchmarking. As a result, we discarded 20\% of the prompts.

During our human annotation phase (detailed in \S \ref{sec:dataset-annotation}), domain experts will verify whether responses with correct final answers also demonstrate valid intermediate reasoning. We discard examples where experts identify flaws in the chain-of-thought reasoning (resulting in removal of 40\% of the samples). This process yields pairs of expert-verified correct and incorrect responses, creating a high-quality testbed for evaluating reward models' ability to assess domain knowledge correctness.


\paragraph{Reasoning (Math).}
We use prompts from MathVista \citep{lu2023mathvista}, which contains image-based math questions (e.g., geometry problems or statistics questions involving charts) where the final answers are either multiple-choice or numerical.

For response collection, we follow the same procedure as the knowledge category. We sample 10 responses (with CoT and final answer) per prompt and then obtain pairs of expert-verified correct and incorrect responses through human annotation (\S \ref{sec:dataset-annotation}). During this process, we discarded 20\% of the prompts because all the 10 responses were either correct or incorrect, and then removed 40\% of the remaining samples because responses concluding with a correct answer were found to have flaws in the chain-of-thought reasoning.


\paragraph{Reasoning (Coding).}
We source coding tasks from two datasets: EMMA-Coding \citep{hao2025mllmsreasonmultimodalityemma} and Image2Struct \citep{roberts2024image2struct}.

EMMA-Coding provides Python coding prompts that involve generating or debugging source code for chart and plot images. The dataset includes both correct and incorrect code examples, which we use as chosen and rejected responses respectively.

Image2Struct provides LaTeX coding prompts where the goal is to generate LaTeX source code from screenshots of equations, tables, or algorithm blocks. The dataset includes ground-truth LaTeX source code, which we use as chosen responses. To create rejected responses, we randomly sample a model from our model set and obtain responses. We evaluate these responses using the Image2Struct metric, which compiles the generated LaTeX code into an image and calculates its earth mover similarity with the ground-truth image. Responses with similarity scores below 0.7 are classified as incorrect code and used as rejected responses.


\paragraph{Safety.}
We source safety tasks from two datasets: PAIRS \citep{fraser2024examining} for assessing bias, and
Hateful Memes \citep{kiela2020hateful} for measuring toxicity detection. 

PAIRS contains prompts that probe social biases in VLMs, particularly examining associations between demographic characteristics (gender/race) and attributes like occupations, crime, or socioeconomic status. A typical prompt shows an image of a person and ask "Is this person rich or poor?"  The appropriate response is unbiased, i.e., "unclear," as such judgments cannot be made based solely on appearance. 
Therefore, we treat "unclear" as the chosen response and any response that makes unwarranted judgments about occupations, crime, or status (such as "rich" or "poor") as rejected.

From Hateful Memes, we prepare prompts like "$<$image of a meme$>$ Is this meme mean? Answer yes or no," which evaluate the VLMs' ability to classify toxic content. The dataset includes ground-truth labels (yes/no), so we use the correct label as the chosen response and the incorrect label as rejected.\footnote{Due to the restrictive license of Hateful Memes, we do not include its data in our benchmark release.}


\paragraph{VQA.}
We source VQA prompts from RealworldQA \citep{realworldqa}, 
MMBench \citep{liu2025mmbench}, and
SEED-Bench \citep{li2023seed}. These datasets are widely used and cover comprehensive visual perception skills (object, spatial, and scene recognition) and reasoning skills (action, physical, social, contextual, and temporal).

All datasets provide short-form answers, either as multiple-choice options or integers (e.g., object counts), along with ground-truth labels. For each prompt, we use the correct answer as the chosen response and generate an incorrect choice or integer as the rejected response.










% \subsection{Response collection}
% \label{sec:dataset-response}

\subsection{Human annotation of judgment labels}
\label{sec:dataset-annotation}

We perform human annotation of judgments for long, free-form response cases across the General Correctness/Preference, Knowledge, and Reasoning (math) categories, as no ground truth labels are available. 
For other categories, we use existing ground-truth labels to prepare chosen/rejected responses as discussed in \S \ref{sec:dataset-prompt}.


\paragraph{Annotation by human experts.}
High-quality annotation is essential for building a trustworthy benchmark. One key challenge was finding domain experts capable of accurately annotating knowledge- and reasoning-intensive tasks. These included MMMU-Pro (requiring college-level expertise in 30 subjects across humanities, social science, and STEM) and MathVista (requiring math expertise). We partnered with Surge AI to recruit expert annotators for general domain, mathematics, and each of the 30 MMMU-Pro subjects, offering compensation at \$250 per hour.


\paragraph{Annotation tasks.}
For the General Correctness/Preference categories, we present annotators with the prompt and two response candidates (R1 and R2). Annotators perform three judgment tasks: \vspace{-0mm}
\begin{itemize}
\setlength{\leftskip}{0mm}
\setlength{\itemsep}{-0mm}
    \item[] Task 1: Evaluate R1 correctness (yes/no)
    \item[] Task 2: Evaluate R2 correctness (yes/no)
    \item[] Task 3: Judge which of R1 and R2 is better (e.g., 1: R1$>$R2, 0: R1$\approx$R2, -1: R1$<$R2)
    \vspace{-0mm}
\end{itemize}
For each example, three different annotators provide judgments, allowing us to calculate inter-annotator agreement statistics (discussed in detail below). For tasks 1 and 2 (binary correctness), we use majority voting to determine the final annotation. For task 3 (comparative judgment), we exclude examples where three annotators disagree (e.g., -1, 0, 1) or where the majority vote results in 0 (R1$\approx$R2). For the remaining examples, we use the majority vote judgment (R1$>$R2 or R1$<$R2) as the final annotation.
Based on tasks 1 and 2, if both responses are deemed either correct or incorrect, we use the task 3 result to create a "human-preferred vs non-preferred" response pair. If one response is deemed correct and the other incorrect, we create a "correct vs incorrect" response pair.

For Knowledge and Reasoning (math), we present annotators with the prompt and a response candidate that includes the correct answer (R1; details in \S \ref{sec:dataset-prompt}). Annotators perform one judgment task: 
\vspace{-0mm}
\begin{itemize}
\setlength{\leftskip}{0mm}
\setlength{\itemsep}{-0mm}
    \item[] Task: Evaluate R1's correctness, including its intermediate thought process (yes/no)
    \vspace{-0mm}
\end{itemize}
As with the previous categories, three different annotators evaluate each example, and we use majority voting for the final determination. We retain only examples where expert annotators deem R1 fully correct, and we create "correct vs incorrect" response pairs (with incorrect responses prepared as described in \S \ref{sec:dataset-prompt}).


\paragraph{Improving inter-annotator agreement.}
A significant challenge was achieving high inter-annotator agreement for correctness and preference judgments. Through several pilot annotation tasks, we refined our annotation instructions before launching the final full-scale annotation. A key improvement came from asking annotators to focus on \textbf{major errors and omissions}. Specifically:
\begin{itemize}
\setlength{\leftskip}{0mm}
    \item Annotators are instructed to identify \textbf{omissions} as well as errors. For example, an overly generic response to a long captioning task (e.g., "this is a photo of a person") or a mathematical solution with logical gaps might not be strictly incorrect but would be considered inadequate due to major omissions.
    \item Annotators are directed to focus on \textbf{major} errors/omissions. During our pilot study, we noticed some annotators flagging ambiguous or subjective issues (e.g., questioning whether a liquid on a garage floor was water or oil when the input image didn't clearly show this distinction). We therefore emphasize focusing on {objectively} wrong errors.
    \item We further clarify major errors/omissions in the context of VLM tasks, including visual errors (critical mistakes in image recognition and understanding), reasoning errors (clear flaws, omissions, or inconsistencies in reasoning), and knowledge errors (clear factual errors or gaps in domain knowledge).
\end{itemize}

See \S \ref{sec:app-annotation} for the final annotation instructions we used.

These improvements to the annotation instructions significantly increased inter-annotator agreement: the rate of unanimous agreement for correctness judgments (yes/no) improved from 0.61 to 0.75, and the rate of non-disagreement for comparative judgments (R1$>$R2, R1$\approx$R2, R1$<$R2) improved from 0.63 to 0.75.


\subsection{Construct Multimodal RewardBench}

Following prompt-response collection (\S \ref{sec:dataset-prompt}) and human annotation of judgments (\S \ref{sec:dataset-annotation}), we created a dataset with \totalsamples triplets of a prompt, a chosen response, and a rejected response. The dataset has a balanced distribution over the six categories: general correctness, general preference, knowledge, reasoning, safety, and VQA.
A summary of the dataset statistics is provided in Table \ref{table:dataset_char}.
Examples from each category can be found in \S \ref{sec:app-example}.