\section{Conclusion}
We present Multimodal RewardBench, a holistic benchmark for evaluating reward models in vision-language models (VLMs). Our benchmark covers six key areas with 5,150 expert-annotated triplets of prompts, chosen responses, and rejected responses. Through extensive evaluation of various VLM judges, including both proprietary and open models, we found that the best models achieved 72\% accuracy overall and that significant room for improvement remains, particularly in reasoning and safety tasks. These findings highlight the importance of holistic reward model evaluation, with our benchmark serving as a challenging testbed for future VLM development.


 
\section{Limitations and future work}
While our work represents the first holistic benchmark for VLM reward models and makes important strides in covering diverse dimensions (such as general correctness/preference, knowledge, reasoning, safety, and VQA), future work can further expand and enrich each of these dimensions by incorporating additional datasets.
For example, in the safety category, we were limited to two datasets: PAIRS (for bias) and Hateful Memes (for toxicity) due to the current scarcity of VLM datasets in this domain. As more datasets become available, future work can explore additional safety-related aspects, including prompt refusal, NSFW content detection, and harmful response identification. Similarly, while our coding evaluation focused on Python plotting and LaTeX, future work could encompass more programming languages like HTML, JavaScript and C++, and address more challenging coding problems that emphasize algorithmic problem-solving rather than rendering tasks. 

Another limitation is that our benchmark currently evaluates only VLM-as-a-judge approaches, as there are few publicly available regression/classifier-based VLM reward models, unlike the situation with LLM reward models. As the research community develops and open-sources more such models, future work will evaluate regression/classifier-based VLM reward models as well.