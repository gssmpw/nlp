\begin{table}[t]
% \vspace{0.5em}
\renewcommand{\arraystretch}{1.1}
% \hspace{-4mm}
\centering
\scalebox{0.85}{
{
\begin{tabular}{p{1.75cm}llrl}
\toprule
\textbf{Category} & \textbf{Source} & \textbf{Response Type} & \textbf{N} & \textbf{Task Description} \\ 
\midrule
General & VisitBench & Long (instruction following)\!\!\! & 221 & Correct vs incorrect response \\
Correctness  & Nocaps & Long (long caption) & 402 & Correct vs incorrect response 
\\
\textbf{623 total} & \\

\midrule
General & VisitBench & Long (instruction following)\!\!\! & 265 & Preferred vs non-preferred response\!\! \\
Preference  & Nocaps & Long (long caption) & 389 & Preferred vs non-preferred response\!\!\!  \\
\textbf{654 total} & \\

\midrule
Knowledge & MMMU-Pro-S & Long (CoT + answer) & 330 & Correct vs incorrect response \\
\textbf{630 total} & MMMU-Pro-V & Long (CoT + answer) & 300 & Correct vs incorrect response \\

\midrule
Reasoning & MathVista & Long (CoT + answer) & 514 & Correct vs incorrect response \\
\textbf{1098 total}   & EMMA-Coding & Code (Python) & 282 & Correct vs incorrect code \\
& Image2Struct & Code (LaTeX) & 300 & Correct vs incorrect code \\

\midrule
Safety & PAIRS & Short answer & 508 & Unbiased vs biased response \\
\textbf{1008 total} & Hateful Memes & Short answer & 500 & Correct vs incorrect answer \\

\midrule
VQA & RealworldQA & Short answer & 400 & Correct vs incorrect answer\\
\textbf{1200 total}   & SEED-Bench & Short answer & 400 & Correct vs incorrect answer\\
& MMBench & Short answer & 400 & Correct vs incorrect answer \\

\midrule
\textbf{Grand Total}\!\!\! &&& \textbf{\totalsamples} \\
\bottomrule
\end{tabular}}}
% \end{adjustbox}
% \vspace{0.5em}
\caption{Summary of \textbf{\methodname}, a \textbf{holistic} benchmark for evaluating reward models for vision-language models (VLMs). We cover six key areas relevant to VLMs: general correctness, general preference, knowledge, reasoning, safety, and VQA. We cover both long and short response formats, and assess both types of judge tasks: 'correct vs incorrect response' and 'human-preferred vs non-preferred' (provided both responses are either correct or incorrect). Our judge labels for long-form responses are collected by \textbf{high-quality annotation from human experts}.}
\label{table:dataset_char}

\end{table}
