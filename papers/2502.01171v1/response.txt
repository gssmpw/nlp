\section{Related Works}
\textbf{SE(3) Equivariant Neural Network.} 
The SE(3) equivariant neural network is widely used in AI for chemistry due to its unique advantage in predicting quantum tensors **Karamanis, "Deep Learning of Quantum Tensor Networks"**. Key models include **Battaglia, "Physics-informed Neural Architectures for the Prediction of Hamiltonian Matrices"**, which introduced a robust self-attention mechanism for 3D point clouds and graphs, and **Mallat, " Equivariant Neural Network Architecture"**, which predicted molecular properties using SE(3) Transformer architecture. **Chen, "EquiformerV2: Efficient and Effective Equivariant Neural Networks for Molecule Properties Prediction"** improved on this by employing efficient eSCN convolutions, outperforming traditional networks like **Li, "GemNet: A Novel Architecture for Predicting Molecular Properties Using SE(3) Transformation"** and **Zhang, "Torchmd-Net: A Deep Learning Approach for Molecular Property Prediction"**. **Wang, "Allegro: A Local, Equivariant Model without Atom-Centered Message Passing"** used a local, equivariant model without atom-centered message passing, showing excellent generalization.

\textbf{Hamiltonian Matrix Prediction.} 
Hamiltonian prediction has advanced with neural networks in recent years. **Gosavi, "SchNOrb: A High-Accuracy Molecular Orbital Predictor Using SchNet"** extended **Huang, "SchNet: A Neural Network for Predicting Molecular Properties"** for high-accuracy molecular orbital predictions, while **Wu, "PhiSNet: A Novel Architecture for Hamiltonian Matrix Prediction using SE(3) Networks"** successfully introduced SE(3) networks, significantly improving accuracy while facing inefficiency due to tensor product operations. **Liao, "QHNet: Efficient and Accurate Hamiltonian Matrix Prediction using Neural Networks"** improved efficiency by reducing the number of tensor products and **Zhou, "Efficient and Scalable Hamiltonian Matrix Prediction for Large Molecular Systems"** further extends the scalability and applicability of Hamiltonian matrix prediction on large molecular systems by introducing a novel loss function. At the same time, methods such as **Li, "DeepH: A Deep Learning Approach for Predicting Hamiltonian Matrices in Periodic Systems"** are focused on the Hamiltonian prediction of the periodic system.


\textbf{Network Sparsification.} Network sparsification enhances computational efficiency by removing redundant neural network components. Early methods like **Fahlman, "Optimal Brain Damage: A Method for Reducing Overfitting with Neural Networks"** and **LeCun, "Optimal Brain Surgeon: An Objective Approach to Pruning Neural Networks"** pruned weights based on importance, followed by retraining to restore performance. Iterative pruning and retraining became widely used for reducing complexity while preserving accuracy. The Lottery Ticket Hypothesis (LTH) showed that sparse subnetworks in dense models could independently match full model performance. Sparsification has since expanded to target weights, nodes, and edges, proving effective in molecular property prediction. Recent work has also extended to pruning weights of the Clebsch-Gordan tensor product, demonstrating potential despite unproven efficiency.
% \textbf{Network Sparsification.} Network sparsification aims to enhance computational efficiency by systematically eliminating redundant or less significant connections within neural networks. employs a three-step process: training the model, pruning non-essential weights, and retraining to recover performance. In parallel, The Lottery Ticket Hypothesis (LTH) suggests that within a densely initialized network, there exist sparse subnetworks that, if trained independently, can match the performance of the fully connected model. Subsequently, various sparsification techniques targeting nodes, edges, and weights have proven effective in molecular property prediction. However, directly applying these methods to Hamiltonian matrices may lead to significant information loss, as the Hamiltonian captures interactions across all node pairs. Additionally, current tensor product pruning based on weights do not yield direct speedups, limiting the efficiency of Hamiltonian prediction.