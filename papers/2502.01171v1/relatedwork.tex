\section{Related Works}
\textbf{SE(3) Equivariant Neural Network.} 
The SE(3) equivariant neural network is widely used in AI for chemistry due to its unique advantage in predicting quantum tensors \cite{fuchs2020se, du2022se, musaelian2023learning, liao2022equiformer, liao2023equiformerv2, batzner20223, batzner2023advancing}. Key models include SE(3)-Transformer \cite{fuchs2020se}, which introduced a robust self-attention mechanism for 3D point clouds and graphs, and Equiformer \cite{liao2022equiformer}, which predicted molecular properties using SE(3) Transformer architecture. EquiformerV2 \cite{liao2023equiformerv2} improved on this by employing efficient eSCN convolutions \cite{passaro2023reducing}, outperforming traditional networks like GemNet \cite{gasteiger2022gemnet} and Torchmd-Net \cite{tholke2022torchmd}. Allegro \cite{musaelian2023learning} used a local, equivariant model without atom-centered message passing, showing excellent generalization. 

\textbf{Hamiltonian Matrix Prediction.} 
Hamiltonian prediction has advanced with neural networks in recent years. SchNOrb\cite{schutt2019unifying} extended SchNet \cite{schutt2017schnet} for high-accuracy molecular orbital predictions, while PhiSNet \cite{unke2021se} successfully introduced SE(3) networks, significantly improving accuracy while facing inefficiency due to tensor product operations. QHNet \cite{yu2023efficient} improved efficiency by reducing the number of tensor products and \cite{huang2024enhancing} further extends the scalability and applicability of Hamiltonian matrix prediction on large molecular systems by introducing a novel loss function. At the same time, methods such as DeepH \cite{li2022deep} are focused on the Hamiltonian prediction of the periodic system.


\textbf{Network Sparsification.} Network sparsification enhances computational efficiency by removing redundant neural network components. Early methods like Optimal Brain Damage \cite{lecun1990optimal} and Optimal Brain Surgeon \cite{hassibi1993second} pruned weights based on importance, followed by retraining to restore performance. Iterative pruning and retraining \cite{han2015learning} became widely used for reducing complexity while preserving accuracy. The Lottery Ticket Hypothesis (LTH) \cite{frankle2018lottery} later showed that sparse subnetworks in dense models could independently match full model performance. Sparsification has since expanded to target weights, nodes, and edges, proving effective in molecular property prediction \cite{liu2023comprehensive,peng2022towards}. Recent work has also extended to pruning weights of the Clebsch-Gordan tensor product \cite{wang2023towards}, demonstrating potential despite unproven efficiency.
% \textbf{Network Sparsification.} Network sparsification aims to enhance computational efficiency by systematically eliminating redundant or less significant connections within neural networks. \cite{han2015learning} employs a three-step process: training the model, pruning non-essential weights, and retraining to recover performance. In parallel, the Lottery Ticket Hypothesis (LTH) \cite{frankle2018lottery} suggests that within a densely initialized network, there exist sparse subnetworks that, if trained independently, can match the performance of the fully connected model. Subsequently, various sparsification techniques targeting nodes, edges, and weights have proven effective in molecular property prediction \cite{liu2023comprehensive,peng2022towards}. However, directly applying these methods to Hamiltonian matrices may lead to significant information loss, as the Hamiltonian captures interactions across all node pairs. Additionally, current tensor product pruning based on weights \cite{wang2023towards}do not yield direct speedups, limiting the efficiency of Hamiltonian prediction.