\section{Related Works}
\textbf{SE(3) Equivariant Neural Network.} 
The SE(3) equivariant neural network is widely used in AI for chemistry due to its unique advantage in predicting quantum tensors ____. Key models include SE(3)-Transformer ____, which introduced a robust self-attention mechanism for 3D point clouds and graphs, and Equiformer ____, which predicted molecular properties using SE(3) Transformer architecture. EquiformerV2 ____ improved on this by employing efficient eSCN convolutions ____, outperforming traditional networks like GemNet ____ and Torchmd-Net ____. Allegro ____ used a local, equivariant model without atom-centered message passing, showing excellent generalization. 

\textbf{Hamiltonian Matrix Prediction.} 
Hamiltonian prediction has advanced with neural networks in recent years. SchNOrb____ extended SchNet ____ for high-accuracy molecular orbital predictions, while PhiSNet ____ successfully introduced SE(3) networks, significantly improving accuracy while facing inefficiency due to tensor product operations. QHNet ____ improved efficiency by reducing the number of tensor products and ____ further extends the scalability and applicability of Hamiltonian matrix prediction on large molecular systems by introducing a novel loss function. At the same time, methods such as DeepH ____ are focused on the Hamiltonian prediction of the periodic system.


\textbf{Network Sparsification.} Network sparsification enhances computational efficiency by removing redundant neural network components. Early methods like Optimal Brain Damage ____ and Optimal Brain Surgeon ____ pruned weights based on importance, followed by retraining to restore performance. Iterative pruning and retraining ____ became widely used for reducing complexity while preserving accuracy. The Lottery Ticket Hypothesis (LTH) ____ later showed that sparse subnetworks in dense models could independently match full model performance. Sparsification has since expanded to target weights, nodes, and edges, proving effective in molecular property prediction ____. Recent work has also extended to pruning weights of the Clebsch-Gordan tensor product ____, demonstrating potential despite unproven efficiency.
% \textbf{Network Sparsification.} Network sparsification aims to enhance computational efficiency by systematically eliminating redundant or less significant connections within neural networks. ____ employs a three-step process: training the model, pruning non-essential weights, and retraining to recover performance. In parallel, the Lottery Ticket Hypothesis (LTH) ____ suggests that within a densely initialized network, there exist sparse subnetworks that, if trained independently, can match the performance of the fully connected model. Subsequently, various sparsification techniques targeting nodes, edges, and weights have proven effective in molecular property prediction ____. However, directly applying these methods to Hamiltonian matrices may lead to significant information loss, as the Hamiltonian captures interactions across all node pairs. Additionally, current tensor product pruning based on weights ____do not yield direct speedups, limiting the efficiency of Hamiltonian prediction.