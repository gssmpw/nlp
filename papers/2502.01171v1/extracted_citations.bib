@article{batzner20223,
  title={E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials},
  author={Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger, Mario and Mailoa, Jonathan P and Kornbluth, Mordechai and Molinari, Nicola and Smidt, Tess E and Kozinsky, Boris},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={2453},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{batzner2023advancing,
  title={Advancing molecular simulation with equivariant interatomic potentials},
  author={Batzner, Simon and Musaelian, Albert and Kozinsky, Boris},
  journal={Nature Reviews Physics},
  volume={5},
  number={8},
  pages={437--438},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{du2022se,
  title={SE (3) equivariant graph neural networks with complete local frames},
  author={Du, Weitao and Zhang, He and Du, Yuanqi and Meng, Qi and Chen, Wei and Zheng, Nanning and Shao, Bin and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={5583--5608},
  year={2022},
  organization={PMLR}
}

@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@article{fuchs2020se,
  title={Se (3)-transformers: 3d roto-translation equivariant attention networks},
  author={Fuchs, Fabian and Worrall, Daniel and Fischer, Volker and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1970--1981},
  year={2020}
}

@article{gasteiger2022gemnet,
  title={Gemnet-oc: developing graph neural networks for large and diverse molecular simulation datasets},
  author={Gasteiger, Johannes and Shuaibi, Muhammed and Sriram, Anuroop and G{\"u}nnemann, Stephan and Ulissi, Zachary and Zitnick, C Lawrence and Das, Abhishek},
  journal={arXiv preprint arXiv:2204.02782},
  year={2022}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={1993}
}

@inproceedings{huang2024enhancing,
    author = {Li, Yunyang and Xia, Zaishuo and Huang, Lin and Wei, Xinran and Harshe, Samuel and Yang, Han and Luo, Erpai and Wang, Zun and Zhang, Jia and Liu, Chang and Shao, Bin and Gerstein, Mark},
    title = {Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonian for  Molecular Systems},
    booktitle={International Conference on Learning Representations},
    year = {2025}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={598--605},
  year={1990}
}

@article{li2022deep,
  title={Deep-learning density functional theory Hamiltonian for efficient ab initio electronic-structure calculation},
  author={Li, He and Wang, Zun and Zou, Nianlong and Ye, Meng and Xu, Runzhang and Gong, Xiaoxun and Duan, Wenhui and Xu, Yong},
  journal={Nature Computational Science},
  volume={2},
  number={6},
  pages={367--377},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@article{liao2022equiformer,
  title={Equiformer: Equivariant graph attention transformer for 3d atomistic graphs},
  author={Liao, Yi-Lun and Smidt, Tess},
  journal={arXiv preprint arXiv:2206.11990},
  year={2022}
}

@article{liao2023equiformerv2,
  title={Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations},
  author={Liao, Yi-Lun and Wood, Brandon and Das, Abhishek and Smidt, Tess},
  journal={arXiv preprint arXiv:2306.12059},
  year={2023}
}

@article{liu2023comprehensive,
  title={Comprehensive graph gradual pruning for sparse training in graph neural networks},
  author={Liu, Chuang and Ma, Xueqi and Zhan, Yibing and Ding, Liang and Tao, Dapeng and Du, Bo and Hu, Wenbin and Mandic, Danilo P},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}

@article{musaelian2023learning,
  title={Learning local equivariant representations for large-scale atomistic dynamics},
  author={Musaelian, Albert and Batzner, Simon and Johansson, Anders and Sun, Lixin and Owen, Cameron J and Kornbluth, Mordechai and Kozinsky, Boris},
  journal={Nature Communications},
  volume={14},
  number={1},
  pages={579},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{passaro2023reducing,
  title={Reducing so (3) convolutions to so (2) for efficient equivariant gnns},
  author={Passaro, Saro and Zitnick, C Lawrence},
  booktitle={International Conference on Machine Learning},
  pages={27420--27438},
  year={2023},
  organization={PMLR}
}

@article{peng2022towards,
  title={Towards effective sparsification for molecular graph property prediction},
  author={Peng, Yue and Zhang, Rui},
  journal={Journal of Chemical Information and Modeling},
  year={2022}
}

@article{schutt2017schnet,
  title={Schnet: A continuous-filter convolutional neural network for modeling quantum interactions},
  author={Sch{\"u}tt, Kristof and Kindermans, Pieter-Jan and Sauceda Felix, Huziel Enoc and Chmiela, Stefan and Tkatchenko, Alexandre and M{\"u}ller, Klaus-Robert},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{schutt2019unifying,
  title={Unifying machine learning and quantum chemistry with a deep neural network for molecular wavefunctions},
  author={Sch{\"u}tt, Kristof T and Gastegger, Michael and Tkatchenko, Alexandre and M{\"u}ller, K-R and Maurer, Reinhard J},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={5024},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{tholke2022torchmd,
  title={Torchmd-net: Equivariant transformers for neural network based molecular potentials},
  author={Th{\"o}lke, Philipp and De Fabritiis, Gianni},
  journal={arXiv preprint arXiv:2202.02541},
  year={2022}
}

@article{unke2021se,
  title={SE (3)-equivariant prediction of molecular wavefunctions and electronic densities},
  author={Unke, Oliver and Bogojeski, Mihail and Gastegger, Michael and Geiger, Mario and Smidt, Tess and M{\"u}ller, Klaus-Robert},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14434--14447},
  year={2021}
}

@inproceedings{wang2023towards,
  title={Towards Flexible, Efficient, and Effective Tensor Product Networks},
  author={Wang, Nanxiang and Lin, Chen and Bronstein, Michael and Torr, Philip},
  booktitle={NeurIPS 2023 Workshop: New Frontiers in Graph Learning},
  year={2023}
}

@inproceedings{yu2023efficient,
  title={Efficient and equivariant graph networks for predicting quantum Hamiltonian},
  author={Yu, Haiyang and Xu, Zhao and Qian, Xiaofeng and Qian, Xiaoning and Ji, Shuiwang},
  booktitle={International Conference on Machine Learning},
  pages={40412--40424},
  year={2023},
  organization={PMLR}
}

