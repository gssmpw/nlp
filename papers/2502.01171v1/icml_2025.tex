%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
% \usepackage[accepted]{icml2025}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell} % 方便设置自动换行


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{caption}

\captionsetup[figure]{skip=0pt}  % 减少图片标题与正文之间的间距


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\makeatletter
\renewcommand{\maketag@@@}[1]{\hbox{\m@th\normalsize\normalfont#1}}%
\makeatother

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Efficient and Scalable Density Functional Theory Hamiltonian Prediction through Adaptive Sparsity}
% \icmltitle{Adaptive Sparsity for Scalable and Efficient Density Functional Theory Hamiltonian Prediction}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
% \icmlauthor{Anonymous}{}
\icmlauthor{Erpai Luo}{equal,sch}
\icmlauthor{Xinran Wei}{equal,comp}
\icmlauthor{Lin Huang}{comp}
\icmlauthor{Yunyang Li}{sch_2}
\icmlauthor{Han Yang}{comp}
\icmlauthor{Zun Wang}{comp}
\icmlauthor{Chang Liu}{comp}
\icmlauthor{Zaishuo Xia}{sch_3}
\icmlauthor{Jia Zhang}{comp}
\icmlauthor{Bin Shao}{comp}

\end{icmlauthorlist}

\icmlaffiliation{sch}{Department of Automation, Tsinghua University, Beijing, China}
\icmlaffiliation{sch_2}{Department of Computer Science, Yale University, CT, USA}
\icmlaffiliation{sch_3}{Department of Computer Science, University of California, CA, USA}
\icmlaffiliation{comp}{Microsoft Research AI for Science, Beijing, China}

\icmlcorrespondingauthor{Xinran Wei}{weixinran@microsoft.com}
\icmlcorrespondingauthor{Jia Zhang}{jia.zhang@microsoft.com}
\icmlcorrespondingauthor{Lin Huang}{huang.lin@microsoft.com}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.1in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% SE(3)-equivariant neural networks have demonstrated impressive performance in predicting the Density Functional Theory Hamiltonian of molecules. Despite their success, the computational demands of the networks based on group representations, particularly due to the extensive use of high-order tensor products, have hampered their applicability to large molecular systems with huge amounts of basis sets. 
% To address this challenge, we propose \underline{SPH}Net, an efficient and scalable equivariant network, which infuses adaptive \underline{SP}arsity into \underline{H}amiltonian prediction networks by sparse gates. 
% These two distinct adaptive gates reduce the number of tensor products and accelerate individual tensor products by constraining non-critical interaction combinations. Meanwhile, a three-phase Three-phase Sparsity Scheduler is proposed to ensure unbiased convergence to the optimal set.  
% Without compromising accuracy, SPHNet achieves up to a 4× reduction in memory consumption and a 7.1× speedup on the evaluation datasets. 
% By virtue of its enhanced efficiency, SPHNet extends the scalability limits of Hamiltonian matrix prediction tasks for molecular systems, and demonstrates the flexibility, usability, and superior performance of adaptive sparsity in SE(3) network architectures, particularly for Hamiltonian prediction tasks with comprehensive ablation studies.
% SPHNet leverages two novel sparse gates: the Sparse Pair Gate, which reduces the number of tensor product computations by filtering out unimportant node pairs, and the Sparse TP Gate, which improves the efficiency of individual tensor operations by pruning less significant cross-order interactions. 

% The Hamiltonian matrix is a fundamental quantity in computational chemistry, as it provides access to all molecular properties computed via Density Functional Theory (DFT). 

% To ensure unbiased convergence to the optimal sparsity set, we introduce a Three-phase Sparsity Scheduler, enabling the model to maintain high performance even at sparsity rates as high as 70\%. 
Hamiltonian matrix prediction is pivotal in computational chemistry, serving as the foundation for determining a wide range of molecular properties. While SE(3) equivariant graph neural networks have achieved remarkable success in this domain, their substantial computational cost—driven by high-order tensor product (TP) operations—restricts their scalability to large molecular systems with extensive basis sets. To address this challenge, we introduce \underline{SPH}Net, an efficient and scalable equivariant network, that incorporates adaptive \underline{SP}arsity into \underline{H}amiltonian prediction. SPHNet employs two innovative sparse gates to selectively constrain non-critical interaction combinations, significantly reducing tensor product computations while maintaining accuracy. To optimize the sparse representation, we develop a Three-phase Sparsity Scheduler, ensuring stable convergence and achieving high performance at sparsity rates of up to 70\%. Extensive evaluations on QH9 and PubchemQH datasets demonstrate that SPHNet achieves state-of-the-art accuracy while providing up to a 7x speedup over existing models. Beyond Hamiltonian prediction, the proposed sparsification techniques also hold significant potential for improving the efficiency and scalability of other SE(3) equivariant networks, further broadening their applicability and impact.
% Extensive evaluations across multiple datasets demonstrate that SPHNet achieves up to 7x speedup, while outperforming existing models in accuracy. With its enhanced efficiency, its scalability advantages in predicting large Hamiltonian matrices and, through comprehensive ablation studies, highlights the feasibility, flexibility and performance of adaptive sparsity within the SE(3) network architecture, particularly for Hamiltonian prediction tasks.
% SPHNet demonstrates superior scalability for Hamiltonian matrix prediction in molecular systems. Comprehensive ablation studies further demonstrate the  feasibility, flexibility and superior performance of adaptive sparsity within the SE(3) network architecture, particularly for Hamiltonian prediction tasks.


\end{abstract}


\section{Introduction}

% However, these methods mainly focus on single molecular property, and obtaining most properties of matter still akin to Density Functional Theory (DFT) methods\cite{hohenberg1964inhomogeneous,te2020classical}, which is a time consuming process. To address this problem, numerous studies have emerged that utilize deep learning to predict Hamiltonian matrices\cite{schutt2017schnet,unke2021se,yu2023efficient,gong2023general,zhou2022deep,zhong2023transferable}. This allows for the rapid acquisition of almost all molecular properties through the predicted Hamiltonian.

The Kohn-Sham Hamiltonian matrix is a fundamental quantity in computational chemistry, as it enables the prediction of all molecular properties obtainable through traditional Density Functional Theory (DFT) calculations \cite{hohenberg1964inhomogeneous,te2020classical}. Accurately predicting the Hamiltonian matrix allows for the rapid determination of system energy, HOMO-LUMO gaps, electron density, and other important properties \cite{fang2022geometry,zang2023hierarchical,batzner20223,batatia2022mace,wang2022visnet,li2023long,chen2023geomformer}. Consequently, recent efforts have focused on using machine learning techniques to predict the Hamiltonian matrix \cite{schutt2017schnet,unke2021se,yu2023efficient,gong2023general,zhou2022deep,zhong2023transferable}. Among these, methods based on SE(3) equivariant graph neural networks \cite{unke2021se,yu2023efficient,gong2023general,yu2023efficient, huang2024enhancing} have achieved superior accuracy due to their consistency with the equivariance of the Hamiltonian matrix. These methods typically project molecular features into spherical harmonics space and use tensor product operations to interact features of different orders, which are crucial for maintaining equivariance.

% Deep learning techniques are playing an increasingly important role in the field of computational chemistry~\cite{choudhary2022recent,debus2021deep,walters2020applications}. In particular, the development of graph neural networks in recent years~\cite{reiser2022graph,fung2021benchmarking,ying2021transformers} has enabled deep learning methods to accurately predict molecular properties such as system energy, HOMO-LUMO gaps, and electron density \cite{fang2022geometry,zang2023hierarchical,batzner20223,batatia2022mace,wang2022visnet,li2023long,chen2023geomformer}. Furthermore, in recent years, numerous studies have emerged that utilize deep learning to predict Hamiltonian matrices\cite{schutt2017schnet,unke2021se,yu2023efficient,gong2023general,zhou2022deep,zhong2023transferable}, which allow for the rapid acquisition of almost all molecular properties through the predicted Hamiltonian.

% The Hamiltonian matrix is an equivariant quantity, requiring the neural network to maintain equivariance throughout the forward process. Thus, SE(3) equivariant graph neural networks have become the primary models for predicting Hamiltonian matrices \cite{unke2021se,yu2023efficient,gong2023general}. These networks project molecular features into spherical harmonics space and use tensor product operations to interact features of different orders, ensuring that the output Hamiltonian matrix transforms accordingly when the input molecule is rotated, translated, or perturbed. However, the efficiency issues introduced by tensor product operations have also gained researchers' attention. For instance, QHNet \cite{yu2023efficient, huang2024enhancing} achieves certain speed improvements by reducing the number of tensor product operations and feature dimensions, however, the training time and memory are still unacceptable when the scale of the Hamiltonian becomes larger. Therefore, there is still a critical need to further accelerate Hamiltonian prediction methods to extend their applicability to larger molecular systems and extensive basis sets. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/dilemma.png}
    % \caption{Two Dilemmas in SE(3) network. \textbf{(A)} The number of tensor products grows quadratically with the number of atoms in the layer. To solve this dilemma, we proposed Sparse Pair Gate to reduce the number of tensor products in the SE(3) network. \textbf{(B)} The time cost for the tensor product grows sixth power with the highest end of the inputs. To solve this dilemma, we proposed Sparse TP Gate to reduce the time cost of single tensor products in the SE(3) network. 
    % }
    \caption{\textbf{(A)} The number of tensor products grows quadratically with the number of atoms $N$, as the Hamiltonian includes features for all possible atomic pair combinations. \textbf{(B)} The time cost of tensor products grows with the sixth power of their order $L$, where the increase in order corresponds to the expansion in the number of orbital types in the DFT basis set. For example, the def2-SVP basis set requires a maximum order of 4, while def2-TZVP demands an order of 6. 
    }
    \label{fig:dilemma}
\end{figure}

However, the efficiency issues introduced by tensor product operations have become a significant bottleneck. On the one hand, as the number of atoms in the system increases, the number of tensor product operations grows quadratically, significantly increasing computational cost and memory requirements, as illustrated in Fig.\ref{fig:dilemma}(A). 
% On the other hand, as the basis set for DFT calculations expands, it becomes necessary to increase the order of tensor product operations to ensure corresponding representations, leading to an increase in the time consumption of a single tensor product operation, as shown at the bottom of Fig.\ref{fig:dilemma}(B). 
On the other hand, since each order in the output representation corresponds to different orbitals in the DFT basis set, the order of tensor product operations must increase as the DFT basis set expands. 
% For example, the def2-SVP basis set requires a maximum order of 4, while def2-TZVP demands an order of 6. 
As shown in Fig.~\ref{fig:dilemma}(B), the computational complexity of tensor products scales with the sixth power of the order, causing the time cost of a single tensor product operation to grow rapidly with larger basis sets.
Therefore, optimizing both the number and efficiency of tensor product operations is critical for extending Hamiltonian prediction to larger systems and larger basis sets.

% This scale limitation is primarily due to the dependency of SE(3) equivariant graph neural networks on tensor product operations. On one hand, equivariant neural networks require multiple tensor product operations to ensure adequate interactions between features of different orders within the spherical harmonics space. As the number of atoms in the computational system increases, the number of tensor product operations grows quadratically as shown in the top of Fig.\ref{fig:model_archi}(A). On the other hand, as the basis set for Density Functional Theory (DFT) methods\cite{hohenberg1964inhomogeneous,te2020classical} calculations expands, it becomes necessary to increase the order of tensor product operations to ensure corresponding representations, leading to an increase in the time consumption of a single tensor product operation, as shown at the bottom of Fig.\ref{fig:model_archi}(A). Therefore, the overall computational complexity of tensor products results in a rapid escalation of inference time and GPU memory requirements. In this context, optimizing both the number of tensor product operations and their efficiency is essential to make Hamiltonian prediction feasible for larger systems.

% To address the above challenges, we introduce adaptive sparsity into equivariant networks and propose a scalable and efficient network for Hamiltonian prediction, termed \textbf{SPHNet}. The main contribution of SPHNet can be summarized as follows:
% \begin{itemize}
 
% \item We integrate adaptive sparsity into equivariant networks by introducing two sparse gates. Specifically, we employ the \textbf{Sparse Pair Gate} to filter out unimportant node pairs, reducing the number of tensor product computations, and the \textbf{Sparse TP Gate} to prune less significant interactions across different orders in tensor product , thereby improving the efficiency of individual tensor operations. Further, to ensure the above gates converges unbiasedly to the optimal set, we propose a \textbf{three-phase training} strategy, which enables the model maintains high accuracy even at sparsity rate as high as 70\%.

% \item We propose \textbf{SPHNet}, an efficient equivariant network with above sparse gates. To further reduce the computational cost of high-order tensor products, SPHNet alleviates the need for \textbf{Spherical Node Interaction Blocks} by stacking \textbf{Vectorial Node Interaction Blocks}, which do not require tensor product computations. This approach allows the maximum order of node representations to gradually increase, thereby enhancing the overall efficiency of the method.

The recent advancements in leveraging sparsity in numerical DFT solvers \cite{doi:10.1021/ct200412r,doi:10.1021/acs.jctc.2c00509} and message-passing networks \cite{liu2023comprehensive,peng2022towards,passaro2023reducing} inspire our exploration of incorporating adaptive sparsity into Hamiltonian prediction models. To address the aforementioned challenges, we developed SPHNet, and the main contribution of this work can be summarized as follows:
\begin{itemize}
 
\item We propose a scalable and efficient network for Hamiltonian prediction, termed \textbf{SPHNet}, which introduce adaptive sparsity into equivariant networks by introducing two sparse gates. Specifically, we employ the \textbf{Sparse Pair Gate} to filter out unimportant node pairs, reducing the number of tensor product computations, and the \textbf{Sparse TP Gate} to prune less significant interactions across different orders in tensor product, thereby improving the efficiency of tensor operations. 

% \item To ensure that the aforementioned gates converge unbiasedly to the optimal set, we propose a \textbf{Three-phase Sparsity Scheduler}. 
\item To optimize the sparse representation, we develop a \textbf{Three-phase Sparsity Scheduler}, which ensures efficient weight updates for all combinations through three stages: random, adaptive, and fixed. This approach facilitates stable convergence to the optimal set while maintaining high accuracy at sparsity rates up to 70\%.

% \item We propose \textbf{SPHNet}, an efficient equivariant network with above sparse gates. 
% To further reduce the computational cost of high-order tensor products, SPHNet alleviates the need for \textbf{Spherical Node Interaction Blocks} by stacking \textbf{Vectorial Node Interaction Blocks}, which do not require tensor product computations. This approach allows the maximum order of node representations to gradually increase, thereby enhancing the overall efficiency of the method.

% \item We demonstrate the enhancements of SPHNet across multiple datasets, showing that it outperforms existing models in accuracy while achieving up to 7x acceleration and using  25\% of the memory. These improvements highlight SPHNet’s accuracy, efficiency, and scalability in Hamiltonian matrix prediction tasks for molecular systems. Furthermore, comprehensive ablation studies validate the feasibility, flexibility, and soundness of adaptive sparsification, providing valuable insights into its application for Hamiltonian prediction.

\item We demonstrate the enhancements of SPHNet across multiple datasets, showing that it outperforms existing models in accuracy on QH9 and PubchemQH, while achieving up to a 7x speedup and reducing memory usage by up to 75\%. It also demonstrates comparable performance on small molecular trajectories on the MD17 dataset. Moreover, the proposed adaptive sparsification techniques exhibit strong promise in improving the computational efficiency and scalability of other SE(3) equivariant networks, paving the way for broader applications across diverse tasks.

% \item Our enhancements demonstrated in SPHNet experiments, show over 4x acceleration and a 3x memory reduction on the QH9 dataset with better accuracy. More notable in larger systems, on the PubChemQH dataset, we saw over 7x acceleration and around a 4x cut in memory usage. These improvements significantly extend the scalability of Hamiltonian matrix prediction tasks for molecular systems, demonstrating the feasibility of adaptive sparsification in equivariant models, particularly for Hamiltonian prediction tasks.
\end{itemize}

\section{Related Works}

\textbf{SE(3) Equivariant Neural Network.} 
The SE(3) equivariant neural network is widely used in AI for chemistry due to its unique advantage in predicting quantum tensors \cite{fuchs2020se, du2022se, musaelian2023learning, liao2022equiformer, liao2023equiformerv2, batzner20223, batzner2023advancing}. Key models include SE(3)-Transformer \cite{fuchs2020se}, which introduced a robust self-attention mechanism for 3D point clouds and graphs, and Equiformer \cite{liao2022equiformer}, which predicted molecular properties using SE(3) Transformer architecture. EquiformerV2 \cite{liao2023equiformerv2} improved on this by employing efficient eSCN convolutions \cite{passaro2023reducing}, outperforming traditional networks like GemNet \cite{gasteiger2022gemnet} and Torchmd-Net \cite{tholke2022torchmd}. Allegro \cite{musaelian2023learning} used a local, equivariant model without atom-centered message passing, showing excellent generalization. 

\textbf{Hamiltonian Matrix Prediction.} 
Hamiltonian prediction has advanced with neural networks in recent years. SchNOrb\cite{schutt2019unifying} extended SchNet \cite{schutt2017schnet} for high-accuracy molecular orbital predictions, while PhiSNet \cite{unke2021se} successfully introduced SE(3) networks, significantly improving accuracy while facing inefficiency due to tensor product operations. QHNet \cite{yu2023efficient} improved efficiency by reducing the number of tensor products and \cite{huang2024enhancing} further extends the scalability and applicability of Hamiltonian matrix prediction on large molecular systems by introducing a novel loss function. At the same time, methods such as DeepH \cite{li2022deep} are focused on the Hamiltonian prediction of the periodic system.


\textbf{Network Sparsification.} Network sparsification enhances computational efficiency by removing redundant neural network components. Early methods like Optimal Brain Damage \cite{lecun1990optimal} and Optimal Brain Surgeon \cite{hassibi1993second} pruned weights based on importance, followed by retraining to restore performance. Iterative pruning and retraining \cite{han2015learning} became widely used for reducing complexity while preserving accuracy. The Lottery Ticket Hypothesis (LTH) \cite{frankle2018lottery} later showed that sparse subnetworks in dense models could independently match full model performance. Sparsification has since expanded to target weights, nodes, and edges, proving effective in molecular property prediction \cite{liu2023comprehensive,peng2022towards}. Recent work has also extended to pruning weights of the Clebsch-Gordan tensor product \cite{wang2023towards}, demonstrating potential despite unproven efficiency.
% \textbf{Network Sparsification.} Network sparsification aims to enhance computational efficiency by systematically eliminating redundant or less significant connections within neural networks. \cite{han2015learning} employs a three-step process: training the model, pruning non-essential weights, and retraining to recover performance. In parallel, the Lottery Ticket Hypothesis (LTH) \cite{frankle2018lottery} suggests that within a densely initialized network, there exist sparse subnetworks that, if trained independently, can match the performance of the fully connected model. Subsequently, various sparsification techniques targeting nodes, edges, and weights have proven effective in molecular property prediction \cite{liu2023comprehensive,peng2022towards}. However, directly applying these methods to Hamiltonian matrices may lead to significant information loss, as the Hamiltonian captures interactions across all node pairs. Additionally, current tensor product pruning based on weights \cite{wang2023towards}do not yield direct speedups, limiting the efficiency of Hamiltonian prediction.

\section{Preliminary}

% \subsection{Hamiltonian Matrix}

\textbf{DFT Hamiltonian}. Density Functional Theory (DFT), as formulated by Hohenberg and Kohn\cite{hohenberg1964inhomogeneous} and further developed through the Kohn-Sham equations by Kohn and Sham\cite{kohn1965self}, represents a key computational quantum mechanical methodology for studying the electronic structure of many-body systems, including atoms, molecules, and solids\cite{jain2016computational,te2020classical}. 
% Distinct from methodologies that necessitate explicit consideration of electron-electron interactions, DFT asserts that the entirety of a many-electron system's ground state properties can be determined via its electron density $\rho(\mathbf{r})$, which is mathematically expressed as:
% \begin{equation}
% \rho(\mathbf{r}) = \sum_i |\psi_i(\mathbf{r})|^2,
% \end{equation}
% wherein $\psi_i(\mathbf{r})$ delineates the system's orbitals. These orbitals, $\psi_i(\mathbf{r})$, are typically delineated through a linear combination of a predefined basis set $\phi_j$:
% \begin{equation}
% \psi_i(\mathbf{r}) = \sum_{j} (\mathbf{C}_{ij})\phi_j(\mathbf{r}),
% \end{equation}
% with \(\mathbf{C} \in \mathbb{R}^{n \times n}\) representing the coefficients of molecular orbitals, $n$ denotes the number of basis functions. 
The core principle of DFT lies in solving the matrix $\mathbf{C}$ via the Kohn-Sham (KS) equations, summarized briefly as\cite{szabo2012modern}:
\begin{equation}
\mathbf{H}\mathbf{C} = \mathbf{S}\mathbf{C}\boldsymbol{\epsilon},
\end{equation}
where \(\mathbf{C} \) representing the coefficients of molecular orbitals, $\mathbf{H}$ signifies the Hamiltonian matrix, $\mathbf{S}$ refers to the overlap matrix, and $\boldsymbol{\epsilon}$ is the diagonal matrix encapsulating orbital energies. Note that \(\mathbf{C} \in \mathbb{R}^{n \times n}\) where $n$ denotes the number of basis functions and \(\mathbf{H}, \mathbf{S}, \boldsymbol{\epsilon} \in \mathbb{R}^{n_o \times n_o}\) where \(n_o\) denotes the number of atomic orbitals used in DFT calculations. Primarily, an iterative self-consistent field (SCF) methodology \cite{payne1992iterative,cances2000convergence,wang2022self,kudin2002black} is utilized to solve the Kohn-Sham (KS) equations and to obtain the Hamiltonian matrix, requiring a temporal complexity of \(\mathcal{O}(n^3)\).

% Primarily, an iterative self-consistent field (SCF) methodology \cite{payne1992iterative,cances2000convergence,wang2022self,kudin2002black} is employed for the resolution of the KS equations, a process that iterates the electron density towards convergence. The temporal complexity associated with solving the KS equations is primarily depends on the numbers of the basis functions, typically scaling as $\mathcal{O}(n^3)$ with conventional algorithms.

% \subsection{SE(3) Equivariant and Tensor Product}

\textbf{SE(3) Equivariant and Tensor Product.} The coordinate system in 3D Euclidean space is the most commonly used description for molecular systems. In this space, the group of 3D translations and rotations forms the SE(3) group. A function \( f(\cdot) \) is SE(3) equivariant if it satisfies \( f(D(g)x) = D(g)f(x) \), where \( D(g) \) is a representation of SE(3) acting on \( x \). Therefore, a neural network in which every operation satisfies the definition of \( f(\cdot) \) is referred to as an SE(3) equivariant neural network. Among these operations, the tensor product operation is the most crucial. Typically, higher-order irreducible representations (irreps) are constructed using spherical harmonics. The tensor product operation combines two irreps, \( x \) and \( y \), with rotational orders \( \ell_{1} \) and \( \ell_{2} \) respectively, using the Clebsch-Gordan (CG) coefficients\cite{griffiths2018introduction}, resulting in a new irreducible representation of order \( \ell_{3} \) as:
\begin{equation}
    (x^{\ell_1} \otimes y^{\ell_2})^{\ell_3}_{m_3} = \sum_{m_1=-\ell_1}^{\ell_1} \sum_{m_2=-\ell_2}^{\ell_2} C^{(\ell_3,m_3)}_{(\ell_1,m_1),(\ell_2,m_2)} x^{\ell_1}_{m_1} y^{\ell_2}_{m_2},  
    \label{equa:tp}
\end{equation}
where $m$ denotes the $m$-th element in the irreducible representation and satisfies $-\ell \leq m \leq \ell$. The output order $\ell_3$ is restricted by $\vert \ell_{1}-\ell_{2}\vert \leq \ell_{3} \leq \vert \ell_{1}+\ell_{2}\vert$. Since each order has to interact with every other order, the computational complexity of the full tensor product using irreps up to order L have a computational complexity of $O(L^6)$, which constrains its applicability for systems with higher degrees.


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/Fig1.png}
    % \caption{\textbf{(A)} The overall architecture of SPHNet. First, the input atomic number and atom coordinates are sent to the Vectorial Node Interaction Block to obtain the atom features. Then, the Sparse Pair Gate selects the most important $\{\mathbf{x}_i,\vec{r_{ij}}\}$ pairs for the Spherical Node Interaction Block. The Spherical Node Interaction Block increases the highest order of the irreducible representation $x_i$, which will be used in the Pair Construction block to construct the pair features. Specifically, the Diagonal block in the Pair Construction block used a Sparse Tensor Product Gate to select the most valuable combinations in the tensor product and obtain the diagonal feature $\mathbf{f}_{ii}$. The Non-Diagonal block uses both the pair gate and tensor product gate to filter the important $\{\mathbf{x}_i,\mathbf{x}_j\}$ pairs and tensor product combinations, and output the pair feature $\mathbf{f}_{ij}$. These features are finally fed into the Expansion block to get the predicted Hamiltonian matrix. \textbf{(B)} The Sparse Pair Gate. It takes all paired features as input and calculates each pair's weight. The top pairs are selected based on their weight. \textbf{(C)} The Sparse Tensor Product Gate. The top combinations are selected based on the learnable weight. \textbf{(D)} Three-phase Sparsity Scheduler for the sparse gate. The first phase is pure random selection, the second phase is adaptively learning an optimal set, and the third phase is fixing the learned optimal set.
    % }
    \vspace{-5pt} 
    \caption{\textbf{(A)} The overall architecture of SPHNet. Atomic numbers and atomic coordinates are first passed through the Vectorial Node Interaction Blocks to obtain atomic features \(\mathbf{x}_i^{\ell}\). Subsequently, the Sparse Pair Gate selects the key pair set \( (i, j) \) for the Spherical Node Interaction Blocks, where the irreps \( \mathbf{x}_i^{\ell} \) are elevated from the \(\ell = 1\) to \( L_{\text{max}} \) during the interaction process. Next, the Sparse Tensor Product Gate in the construction block identifies the key cross-order combinations \( (\ell_1, \ell_2, \ell_3) \) for the diagonal blocks, yielding diagonal pair features \( \mathbf{f}_{ii} \). For non-diagonal blocks, both the Pair Gate and Tensor Product Gate are applied to select the critical pairs and tensor product combinations, producing non-diagonal pair features \( \mathbf{f}_{ij} \). Finally, these features are fed into the expansion block to construct the predicted Hamiltonian matrix.  
    \textbf{(B)} Sparse Pair Gate: It takes pairwise features as input, computes weights for each pair \( (i, j) \), and selects a optimal subset using the sparsity scheduler.
    \textbf{(C)} Sparse Tensor Product Gate: Similarly, it utilizes the sparsity scheduler to identify an optimal subset of cross-order combinations \( (\ell_1, \ell_2, \ell_3) \) based on learnable weights. 
    \textbf{(D)} Three-Phase Sparsity Scheduler: Designed for the sparse gates, it operates in three phases: random, adaptive, and fixed.
    }
    \label{fig:model_archi}
\end{figure*}

\section{Methodology}
\subsection{Three-phase Sparsity Scheduler}

To ensure stable selection of combinations in the Sparse Tensor Product Gate and Sparse Pair Gate, we propose the Three-phase Sparsity Scheduler, a three-stage selection function that always selects retained elements from the set based on a learnable weight matrix \( \mathbf{W} \). Thus, for any unsparsified set $U$, the scheduler can be defined as:
\begin{equation}
\label{equa:gate}
\mathrm{TSS}(\mathbf{W}, k) =
\begin{cases} 
\text{RANDOM}(\mathbf{W},1-k), & \text{if } \text{epoch} < t , \\
\text{TOP}(\mathbf{W}, 1-k), & \text{if } \text{epoch} = t, \\
\text{TOP}({\mathbf{W}^{p_2}}', 1-k), & \text{if } \text{epoch} > t,
\end{cases}
\end{equation}
where \( k \) represents the sparsity rate of the tensor product, $t$ is the round of the training epoch, and \(\text{TSS}(\cdot)\) always returns a subset \( U^\mathrm{TSS} \) of \( U \), containing $(1-k)|U|$ elements selected based on \(\mathbf{W}\). 

Specifically, in the first phase, \( \mathbf{W} \) is initialized as an all-ones vector, and \( \text{RANDOM}(\cdot) \) denotes a random function that selects elements from the set independently of \( \mathbf{W} \) with a given probability $1-k$. This ensures that all parameters are unbiasedly selected and updated. In the second phase, \( \text{TOP}(\cdot) \) is used to select the elements with the highest weights from the set with a given probability $1-k$, ensuring that the optimal combinations are chosen globally. In the third phase, \( {\mathbf{W}^{p_2}}' \) represents a fixed vector without gradients, which is frozen after the final backward update of the second phase. From this point onward, the selected combinations remain unchanged. This design prioritizes efficiency, as static connections typically result in faster computation.

% $ {\mathbf{W}^{e_3}}' $ represents a fixed vector after the last backward in phase 2, which means $ {\mathbf{W}_c^{e_3}}' $ will not be updated any more 

\subsection{Sparse Tenor Product Gate}
\label{sparse_tp_sec}
% Although we use the Sparse Pair Gate and Spherical Node Interaction Block to select the most valuable tensor product and redundant those less necessary tensor products, the time and computational cost of one single tensor product operation still remain very high, especially when irreps has a high maximum order. To accelerate the tensor product operation, we propose an adaptive sparse strategy called Sparse Tensor Product Gate to work with classical tensor product operation (Fig \ref{fig:model_archi}b). 

The Sparse TP Gate is designed to accelerate individual tensor product computations by filtering out unnecessary cross-order combinations in traditional tensor product operations, as illustrated in Fig.\ref{fig:model_archi}(B). By doing so, tensor product operations implemented with \texttt{e3nn} \cite{e3nn} can achieve near-linear acceleration, as demonstrated in the Appendix \ref{appendix:tp_scaling}. Moreover, this approach can be easily adapted to various tensor product implementations by simply modifying the instructions that declare cross-order combinations in the tensor product operation.

Generally, the classical tensor product operation includes the interaction of every element \( m \) in every irrep order \( \ell \). The aim of Sparse Tensor Product Gate is to learn a set of the most valuable combinations \( U_c^{\mathrm{TSS}} \) from the complete set \( U_c= \{(\ell_1, \ell_2, \ell_3) \mid \ell_3 \in [|\ell_1 - \ell_2|, \ell_1 + \ell_2] \} \) in the tensor product and remove the redundant combinations accordingly. \( U_c^{\mathrm{TSS}} \) is defined by a one-dimensional learnable value score $\mathbf{W}_c^{\ell_1, \ell_2, \ell_3}$ of length $|U_c|$ as:
\begin{equation}
\label{equa:topk}
U_c^{\mathrm{TSS}} = \{(\ell_1, \ell_2, \ell_3) \mid \mathbf{W}_c^{\ell_1, \ell_2, \ell_3} \in \text{TSS}(\mathbf{W}_c, k)\},
\end{equation}
where \(\text{TSS}(\cdot)\) is the selection scheduler defined in Equation \ref{equa:gate}.
Accordingly, we can obtain the updated combination weight from the original weights $c$ defined in classical tensor product by:
\begin{equation}
c' = \{c^{ \ell_1, \ell_2, \ell_3} \times \mathbf{W}_c^{\ell_1, \ell_2, \ell_3} \mid (\ell_1, \ell_2, \ell_3) \in U_{c}^{\mathrm{TSS}} \}.
\label{eq:sp_tp_weight}
\end{equation}
In practical experiments, the sparsity rate \( k \) is set based on the details in Section \ref{sec:sparsity_ratio}.
% To ensure that the adaptive sparsify strategy converges unbiasedly to the optimal combination set, it must be accompanied by an appropriate learning strategy. To this end, we propose a three-phase exploration strategy for $\text{TSS}(\cdot)$ in equation \ref{equa:topk} that enables the sparse tensor product gates to thoroughly explore each combination within the complete set \( U_c \) (Fig.\ref{fig:model_archi}d) as follows:
% \begin{equation}
% \label{equa:gate}
% \mathrm{TSS}(\mathbf{W}_c, k) =
% \begin{cases} 
% \text{RANDOM}(\mathbf{W}_c,(1-k)\%)  , & \text{if } t < 3, \\
% \text{TOP}(\mathbf{W}_c, (1-k)\%), & \text{if } t = 3, \\
% \text{TOP}({\mathbf{W}_c^{p2}}', (1-k)\%), & \text{if } t > 3,
% \end{cases}
% \end{equation}
% where $t$ is the round of the training epoch, $ {\mathbf{W}_c^{p2}}' $ represents a fixed vector without grad after the last backward in phase 2. 

% Accordingly, the tensor product operation, combined with the Sparse TP Gate, can be reformulated as:
% \begin{equation}
% \label{equa:sparsetp}
% \mathbf{f}_{ij}^{\ell_3} = \sum_{\{\ell_1, \ell_2, \ell_3 \} \in U_{c}^{\mathrm{TSS}}} c'^{(\ell_1, \ell_2, \ell_3)} \times (x^{\ell_1}_{i} \otimes^{\ell_3}_{\ell_1,\ell_2} w_{ij}^{\ell_1, \ell_2, \ell_3} x^{\ell_2}_j )
% \end{equation}
% where $c'$ is a updated combination weight obtained from the original weights $c$ defined in classical tensor product as:
% \begin{equation}
% c' = \{c^{ \ell_1, \ell_2, \ell_3} \times W_c^{\ell_1, \ell_2, \ell_3} \mid (\ell_1, \ell_2, \ell_3) \in U_{c}^{\mathrm{TSS}} \}
% \label{eq:sp_tp_weight}
% \end{equation}


\subsection{Sparse Pair Gate}
\label{sparse_pair_sec}
% In most of the models for predicting Hamiltonian matrices, there are many fully connected graph networks that need a tensor product operation of the order of the square number of nodes. For example, the atomic representations are obtained by calculating the tensor product of an atom feature $x_i$ with all its edge features $\{r_{ij}\}$. The same thing happens when constructing the pair feature $\mathbf{f}_{ij}$, where $n^2$ tensor products are needed in every pair construction layer. These are computational consuming processes. To reduce the number of tensor products in these fully connected graph networks, we developed the Sparse Pair Gate to adaptively select the most important edge.
Since the output of the Hamiltonian matrix encompasses interactions between all pairs of atoms, previous models typically define inter-atomic relationships as fully connected. This results in tensor product computations that scale quadratically with the number of nodes. However, we observed that not every block requires such dense connectivity. To reduce the number of tensor product operations in the network, we developed the Sparse Pair Gate, which adaptively selects the most important edges.

Similarly with the tensor product gate, the Sparse Pair Gate selects a subset from all the node pair $U_p = \{(i,j) | i \neq j\} $, and uses a linear layer \( F_p(\cdot) \) to learn the weight of each pair $\mathbf{W}_p^{ij}$ in this fully connected graph. This can be formulated as below:
\begin{equation}
\mathbf{I}_{ij} = (\mathbf{x}^0_i \| \mathbf{x}^0_j \| \langle \mathbf{x}_i, \mathbf{x}_j \rangle^0),
\end{equation}
\begin{equation}
\mathbf{W}_p^{ij} = \mathrm{Sigmoid} \left( F_p(\mathbf{I}_{ij}) \right),
\end{equation}
where $\mathbf{x}^0_i$ and $\mathbf{x}^0_j$ are the zero-order features of the irreps, $||$ is concatenation operation in the last feature dimension and \( \langle \cdot , \cdot \rangle \) stands for the inner product operation. Thus, the sparse pair gate can be formulated as:
\begin{equation}
\label{equa:topk_pair}
U_p^{\mathrm{TSS}} = \{(i, j) \mid \mathbf{W}_p^{ij} \in \text{TSS}(\mathbf{W}_p, k)\},
\end{equation}
where $\mathrm{TSS}(\cdot)$ is the same as Equation \ref{equa:gate}. Accordingly, we can get the pair weight $\mathbf{w}_{ij}$ of ${\mathbf{x}_i,\mathbf{x}_j}$ used in tensor product:
\begin{equation}
\mathbf{w}_{ij} = F_r\left(\mathrm{RBF}(\vec{r_{ij}})\right) \times F_s(\mathbf{W}_p^{ij} \times \mathbf{I}_{ij}),
\label{equ:w_ij}
\end{equation}
where $F_r(\cdot)$ and $F_s(\cdot)$ are linear layers, $\mathrm{RBF}(\cdot)$ is the radial basis function. 

% we apply Sparse Pair Gates at two locations in the SPHNet model. The first is placed before the Spherical Node Interaction Block, where it selects the most valuable node-edge pairs $(x_i, \vec{r_{ij}})$, which equals to have a subset $U_p^\mathrm{TSS}$ from $U_p$. Therefore, the tensor product operation in Spherical Node Interaction Block can be formulated as:
% \begin{equation}
%     \begin{aligned}
%     \textbf{m}^{\ell_3}_{ij} =& \sum_{\ell_1,\ell_2} x^{\ell_2}_{j} \otimes^{\ell_3}_{\ell_1,\ell_2} w_{ij}^{\ell_1,\ell_2,\ell_3} Y^{(\ell_2)} (\vec{r_{ij}}) \\
%     \text{s.t.} \quad & {i,j} \in U_p^\mathrm{TSS} \\
%     \end{aligned}
%     \label{equa:SO(3)}
% \end{equation}
% where \( Y^{(\ell_f)}_{m_f} (\cdot) \) represent the spherical harmonic projection function, $x_j$ is the output irrep of Vectorial Node Interaction Block, $\vec{r_{ij}}$ is the distance vector between atoms, where ${i,j} \in U_p^\mathrm{TSS}$. The output $ \textbf{m}_{ij}$ denote the high-order message that between noede $i$ and node $j$.

% The second Sparse Pair Gate is placed before the Non-Diagonal Pair Construction block, fused with the Sparse TP Gate defined in section \ref{sparse_tp_sec}, the tensor product should be formulated same as equation \ref{equa:sparsetp}, with additional constraint ${i,j} \in U_p^\mathrm{TSS}$. It's noteworthy that the Sparse Pair Gate is only employed from the second Spherical Node Interaction Block and the Non-Diagonal Pair Construction block to ensure complete information flow between nodes.

\subsection{SPHNet}
\label{sec: node_conv}
With the above two sparse gates, SPHNet can be formulated as four modules. The model takes atomic numbers \( Z \) and 3D coordinates of molecular systems as input features, initializes the node representation \(x_i\) with a linear function as $ x_i = F_x(Z_i) $, processes them through the Vectorial Node Interaction Block, and sequentially passes through the Spherical Node Interaction Block, Pair Construction Module, and Expansion Module to output the predicted Hamiltonian matrix $\mathbf{H}$, as illustrated in Fig.\ref{fig:model_archi}.

\textbf{Node Interaction Block}.
The Node Interaction Block aggregates information from neighboring nodes through a message-passing mechanism, thereby extracting irreducible representations of nodes. Specifically, the irreducible representation $x_i$ can be updated by aggregating the message \( m_{ij} \) as,
\begin{equation}
\label{node_inter}
\hat{\mathbf{x}}_i^{\ell} = F_{m} ( {\mathbf{x}}_i^{\ell} + \sum_j \mathbf{m}_{ij}^{\ell} ),
\end{equation}
where $F_{m}(\cdot)$ is a linear layer.

To achieve sufficiently interactive high-order information while minimizing the associated computational cost, we propose a mechanism that gradually increases the maximum order of node representations. Initially, four Vectorial Node Interaction Blocks are applied to obtain vectorial representations $\mathbf{m}^{\ell}_{ij}$, where $\ell \leq 1$, as follows:
\begin{equation}
    \mathbf{m}^{\ell}_{ij} = \mathbf{x}^{\ell}_{j} \odot (\mathbf{w}_{ij} \odot \vec{r_{ij}}), \quad \text{s.t.} \quad l \leq 1,\\\
\end{equation}
where $\mathbf{w}_{ij}$ is the weight defined in Equation \ref{equ:w_ij}, and $\vec{r_{ij}}$ is the distance vector between atoms. This representation does not involve interactions between high-order tensors, thereby eliminating tensor product operations, as detailed in Appendix \ref{appendix:featext}. 

Next, we apply two Spherical Node Interaction Blocks to capture interactions for high-order irreducible representations, increasing the highest order of the irreducible representation to match the maximum required order determined by the basis set. Specifically, these blocks project distance information $\vec{r_{ij}}$ into the high-order spherical space using spherical harmonics function \( Y^{\ell}_{m} (\cdot) \). Subsequently,  $\textbf{m}^{\ell}_{ij}$ in this block is computed under the constraints imposed by Sparse Pair Gate as:
\begin{equation}
    \begin{aligned}
    \mathbf{m}^{\ell_3}_{ij} =& \sum_{\ell_1,\ell_2} \mathbf{x}^{\ell_2}_{j} \otimes^{\ell_3}_{\ell_1,\ell_2} \mathbf{w}_{ij}^{\ell_1,\ell_2,\ell_3} Y^{\ell_2}_m (\vec{r_{ij}}), \\
    \text{s.t.} \quad & {(i,j)} \in U_p^\mathrm{TSS}, \quad \ell_1,\ell_2,\ell_3 \leq L_{\text{max}},\\
    \end{aligned}
    \label{equa:SO(3)}
\end{equation}
where $L_{\text{max}}$ represents the allowed max order of irreps in the network and $\mathbf{w}_{ij}$ is the weight defined in Equation \ref{equ:w_ij}.


% \textbf{Vectorial Node Interaction Block}. This module uses the relative 3D coordinates \( \vec{r_{ij}} \) and atomic numbers \( Z \) of the molecular system as input to extract the irreps \( x_i \) of node \( i \) via an equivariant graph neural network. In general, this process can be expressed as: 
% \begin{equation}
% \tilde{\mathbf{x}}_i^{\ell} = F_{m} ( \hat{\mathbf{x}}_i^{\ell} + \sum_j \mathbf{m}_{ij}^{\ell} ).
% \end{equation}
% where $\phi(\cdot)$ defines a message-passing framework, and $m_{ij}^0$ and ${m}_{ij}^1$ denote the scalar message and vectorial message between node $i$ and its neighbor $j$, and the highest order of \(x_i\) is limited to the first order. Importantly, this representation does not involve interactions between high-order tensors, thereby avoiding tensor product operations. This allows for the efficient extraction of more informative node representations (detailed information can be found in Appendix \ref{appendix:featext}).

% \textbf{Spherical Node Interaction Block}. Taking the node representation \(x_i\) as input, two Spherical Node Interaction Blocks are then applied to increase the highest order of the irreducible representation \(x_i\) to match the maximum required order determined by the basis set. This is achieved via Equation \ref{equa:SO(3)}, which performs tensor products on the high-order spherical harmonics projections of neighboring vectors \(\vec{r}_{ij}\). Notably, before each Spherical Node Interaction Block, a Sparse Pair Gate is employed to adaptively learn the most valuable neighboring pairs and reduce the number of tensor product operations during the ascending process. The detailed definition of the Sparse Pair Gate can be found in Section \ref{sparse_pair_sec}.
\begin{table*}[h]  
    \centering  
    \vspace{-10pt}
    \caption{Experimental results on QH9 dataset.}  
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{lccccccc}  
        \toprule
        \multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} & \textit{H}  $\downarrow$ & $\epsilon$  $\downarrow$ & $\psi$ $\uparrow$ & Memory $\downarrow$  & Speed $\uparrow$ & Speedup $\uparrow$\\ 
        & &[$10^{-6}E_h$] & [$10^{-6}E_h$] & [$10^{-2}$] & [GB/Sample] & [Sample/Sec] & Ratio \\
        \midrule\midrule 
        \multirow{3}{*}{QH9-stable iid} 
        & QHNet & 76.31 & 798.51 & 95.85 & 0.70 & 19.2 & 1.0x \\ 
        & WANet & 80.00 & 833.62 & 96.86 & N/A & N/A & N/A \\ 
        & SPHNet & \textbf{45.48} & \textbf{334.28} & \textbf{97.75} & \textbf{0.23} & \textbf{76.80} & \textbf{4.0x}\\
        \midrule  
        \multirow{2}{*}{QH9-stable ood} & QHNet & 72.11 & 644.17 & 93.68 & 0.70 & 21.12 & 1.0x\\
        & SPHNet & \textbf{43.33} & \textbf{186.40} & \textbf{98.16} & \textbf{0.23}  & \textbf{78.72} & \textbf{3.7x}\\ 
        \midrule  
        \multirow{3}{*}{QH9-dynamic geo} & QHNet & 70.03 & 408.31 & 97.13 & 0.70  & 24.68 & 1.0x \\
        & WANet & 74.74 & 416.57 & \textbf{99.68} & N/A & N/A & N/A\\ 
        & SPHNet & \textbf{52.18} & \textbf{100.88} & 99.12 & \textbf{0.23}  & \textbf{82.56} & \textbf{3.3x} \\ 
        \midrule  
        \multirow{2}{*}{QH9-dynamic mol} & QHNet & 120.10 & 2182.06 & 89.63 & 0.70 & 23.04 & 1.0x\\
        & SPHNet & \textbf{108.19} & \textbf{1724.10} & \textbf{91.49} & \textbf{0.23} & \textbf{80.64} & \textbf{3.5x} \\ 
        \bottomrule
    \end{tabular}
    } 
    \label{tab:qh9_result}  
    \vspace{-10pt}
\end{table*} 
% \vspace{-5pt}

\textbf{Pair Construction Block}. Consists of the Non-Diagonal block and the Diagonal block. The Non-Diagonal block uses these irreducible representations to compute the interaction from node $j$ to node $i$, generating the pair-wise features $\mathbf{f}_{ij}$ for the Non-Diagonal blocks of the Hamiltonian matrix. The Diagonal block calculates the self-interaction of node $i$ to produce the node-wise feature $\mathbf{f}_{ii}$ for the diagonal blocks of the Hamiltonian matrix. 

As shown in Fig.\ref{fig:model_archi}(B), we use Sparse Tensor Product Gate before both the Diagonal and Non-Diagonal block, and use Sparse Pair Gate before Non-Diagonal block, therefore the $\mathbf{f}_{ii}$, $\mathbf{f}_{ij}$ can be reformulated as:
\begin{small}
\begin{equation}
\label{eqa:fii}
\mathbf{f}_{ii}^{\ell_3} = \sum_{\{\ell_1, \ell_2, \ell_3 \} \in U_{c}^{\mathrm{TSS}}} \mathbf{W}_{ii}^{(\ell_1, \ell_2, \ell_3)} \times (\hat{\mathbf{x}}_{i}^{\ell_1} \otimes^{\ell_3}_{\ell_1,\ell_2}  \hat{\mathbf{x}}_{i}^{\ell_2} ),
\end{equation}
\end{small}
\begin{small}
\begin{equation}
\label{equa:fij}
\begin{aligned}
\mathbf{f}_{ij}^{\ell_3} & = \sum_{\{\ell_1, \ell_2, \ell_3 \} \in U_{c}^{\mathrm{TSS}}} c'^{(\ell_1, \ell_2, \ell_3)} \times (\hat{\mathbf{x}}^{\ell_1}_{i} \otimes^{\ell_3}_{\ell_1,\ell_2} \mathbf{w}_{ij}^{\ell_1, \ell_2, \ell_3} \hat{\mathbf{x}}^{\ell_2}_j ), \\
&\text{s.t.} \quad {(i,j)} \in U_p^\mathrm{TSS}, \quad \ell_1,\ell_2,\ell_3 \leq L_{\text{max}},\\
\end{aligned}
\end{equation}
\end{small}
\noindent where $W_{ii}$ is learnable parameters for each combination in $U_{c}^{\mathrm{TSS}}$, $c'$ is an updated combination weight defined in Equation \ref{eq:sp_tp_weight} and $\mathbf{w}_{ij}$ is the weight defined in Equation \ref{equ:w_ij}. 
It's noteworthy that the Sparse Pair Gate is only employed from the second Spherical Node Interaction Block and the Non-Diagonal Pair Construction block to ensure complete information flow between nodes, and more details could be found in Appendix \ref{appendix:pair_cons}.
% As shown in Fig \ref{fig:model_archi}a, we uses both Sparse Pair Gate and Sparse Tensor Product Gate before the Non-Diagonal block, to make the module with less and faster tensor product operations, and the details are described in section \ref{sparse_pair_sec} and \ref{sparse_tp_sec}.

\textbf{Expansion Block}. Finally, the expansion block utilizes the tensor expansion operation to obtain the non-diagonal Hamiltonian block $\mathbf{h}_{ij}$ and diagonal Hamiltonian block $\mathbf{h}_{ii}$. The detailed formulation can be found in Appendix \ref{appendix:expansion}. Considering the symmetry of the Hamiltonian matrix, where for any sub-block \( \mathbf{h}_{ij} \), there exists a corresponding sub-block \( \mathbf{h}_{ji} \) such that \(\mathbf{h}_{ji} = \mathbf{h}_{ij}^T \). So we can further enhance the model's efficiency by constructing the node pair features \( \mathbf{f}_{ij} \) from the upper triangular part of the Hamiltonian matrix, i.e., \(\{\mathbf{f}_{ij} \mid i < j\}\). Leveraging this symmetry allows us to halve the number of tensor products required when predicting the node pair irreps \( \mathbf{f}_{ij} \), significantly reducing the computational burden associated with constructing \( \mathbf{h}_{ij} \).


\section{Result}
We conducted a series of experiments to compare the overall performance of SPHNet with the previous state-of-the-art model, including SchNOrb\cite{schutt2019unifying}, PhiSNet\cite{unke2021se}, QHNet\cite{yu2023efficient} and WANet\cite{huang2024enhancing}, and the results listed in the table are sourced from the their papers. Note that the results for PhiSNet and SchNOrb can only be conducted on the MD17 dataset, as these models are specifically designed for trajectory datasets in conformational space. 
Additionally, since WANet has not been open-sourced, we only report results on a subset of datasets and metrics based on the information provided in their paper. Detailed experimental settings are described in Appendix \ref{sec:exp_setting}. Additional ablation studies on the Three-phase Sparsity Scheduler and Interaction Block, as well as further analysis of the experiments presented in this section, are provided in Appendix \ref{app:exp_result}. 
% The detailed experimental setup of hardware, software, and hyperparameters is provided in the Appendix \ref{sec:exp_setting}.
% Furthermore, due to the architecture of DeepH, the data utilized has been relabeled by OpenMX\cite{ozaki2004numerical}, which may result in discrepancies when compared to other models. 
% Therefore, we present only the Hamiltonian MAE results for DeepH based on the MD17 dataset. 


\textbf{Datasets.} Three datasets were used in the experiments: The MD17 dataset includes Hamiltonian matrices for the trajectory data of four molecules: water, ethanol, malondialdehyde, and uracil, containing 4,900, 30,000, 26,978, and 30,000 structures, respectively. The QH9 dataset \cite{yu2024qh9}, consisting of Hamiltonian matrices for 134k small molecules with no more than 20 atoms, and the PubChemQH dataset \cite{huang2024enhancing}, containing Hamiltonian matrices for 50k molecules with atom counts ranging from 40 to 100. The QH9 and PubChemQH use the B3LYP exchange-correlation function, and the MD17 uses the PBE exchange-correlation function. The GTO orbital basis for MD17 and QH9 is Def2-SVP while PubChemQH uses the Def2-TZVP GTO orbital basis. Compared to the MD17 and QH9 datasets, the maximum number of orbitals in the Hamiltonian of  PubChemQH dataset increases by about 15 times, resulting in a 200x difference in the number of elements in the Hamiltonian matrix.

\textbf{Evaluation metrics.} The evaluation metrics include Hamiltonian MAE (mean absolute error between predicted and Hamiltonian matrices calculated by DFT), occupied energy MAE $\epsilon$ (mean absolute error of energies of occupied molecular orbitals calculated from predicted Hamiltonian matrices), $\mathrm{\mathbf{C}}$ similarity(cosine similarity of coefficients for occupied molecular orbitals), training GPU memory (GB per sample), training speed (training samples per second) and speedup ratio compared to QHNet. 


% \subsection{Performance on MD17 and QH9 Dataset}

% We first evaluated the performance of SPHNet on the smaller-scale MD17 dataset. As presented in Table \ref{tab:md17_result}, we compared SPHNet’s performance with four baseline models across four MD17 molecules—water, ethanol, malondialdehyde, and uracil—comprising 3 to 12 atoms. The results indicated that SPHNet achieved accuracy comparable to that of the other models, suggesting its effectiveness in handling small molecular trajectory datasets.

% For the larger molecule dataset QH9, we trained SPHNet on four different split sets and compared the results with the baseline models QHNet and WANet. As shown in Table \ref{tab:qh9_result}, for two stable split sets, SPHNet attained a speedup of 3.7x to 4x over QHNet while improving the accuracy of the Hamiltonian Mean Absolute Error (MAE) and occupied energy MAE. Furthermore, SPHNet significantly reduced GPU memory usage by nearly 70\%. For two dynamic split sets, SPHNet maintained a speedup of 3.3x to 3.5x, simultaneously enhancing prediction accuracy and decreasing GPU memory usage by over 70\%. These results underscored SPHNet's efficiency, achieving substantial speedups and resource savings without sacrificing precision. Overall, these findings indicated that SPHNet can effectively learn molecular features and the Hamiltonian matrix within the small-scale dataset, establishing a solid foundation for calculations in larger-scale systems.

% \begin{table}[h]  
%     \centering  
%     \caption{Experiment results on MD17 dataset}  
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{p{1.5cm}cccc}  
%         \toprule
%         Dataset & Model & \textit{H} [$10^{-6}E_h$] $\downarrow$ & $\epsilon$ [$10^{-6}E_h$] $\downarrow$ & $\psi$ [$10^{-2}$] $\uparrow$ \\  
%         \midrule\midrule
%         \multirow{5}{*}{Water} 
%         & SchNOrb & 165.4 & 279.3 & \textbf{100.00} \\
%         & PhiSNet & \underline{15.67} & \underline{85.53} & \textbf{100.00} \\
%         & QHNet & \textbf{10.79} & \textbf{33.76} & 99.99 \\ 
%         & SPHNet & 23.18 & 182.29 & \textbf{100.00} \\
%         \midrule  
%         \multirow{5}{*}{Ethanol} 
%         & SchNOrb & 187.4 & 334.4 & \textbf{100.00} \\
%         & PhiSNet & \textbf{20.09} & 102.04 & 99.81 \\
%         & QHNet & \underline{20.91} & \textbf{81.03} & 99.99 \\
%         & SPHNet & 21.02 & \underline{82.30} & \textbf{100.00} \\ 
%         \midrule  
%         \multirow{5}{*} {\makecell{Malon-\\aldehyde}}
%         & SchNOrb & 191.1 & 400.6 & 99.00 \\
%         & PhiSNet & \underline{21.31} & 100.6 & 99.89 \\
%         & QHNet & 21.52 & \textbf{82.12} & \underline{99.92} \\
%         & SPHNet & \textbf{20.67} & \underline{95.77} & \textbf{99.99} \\ 
%         \midrule  
%         \multirow{5}{*}{Uracil} 
%         & SchNOrb & 227.8 & 1760 & 90.00 \\
%         & PhiSNet & \textbf{18.65} & 143.36 & 99.86 \\
%         & QHNet & 20.12 & \textbf{113.44} & \underline{99.89} \\
%         & SPHNet & \underline{19.36} & \underline{118.21} & \textbf{99.99} \\ 
%         \bottomrule
%     \end{tabular}
%       }
%     \label{tab:md17_result}  
% \end{table}  


% \begin{table*}[h]  
%     \centering  
%     \caption{Experiment results on QH9 dataset}  
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lccccccc}  
%         \toprule
%         % & \# of Params
%         \multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} & \textit{H}  $\downarrow$ & $\epsilon$  $\downarrow$ & $\psi$ $\uparrow$ & Memory $\downarrow$  & Speed $\uparrow$ & Speedup $\uparrow$\\ 
%         & &[$10^{-6}E_h$] & [$10^{-6}E_h$] & [$10^{-2}$] & [GB/Sample] & [Sample/Sec] & Ratio \\
%         \midrule\midrule 
%         \multirow{3}{*}{QH9-stable iid} 
%         & QHNet & 76.31 & 798.51 & 95.85 & 0.70 & 19.2 & 1.0x \\ 
%         & WANet & 80.00 & N/A & 96.86 & N/A & N/A & N/A \\ 
%         & SPHNet & \textbf{45.48} & \textbf{334.28} & \textbf{97.75} & \textbf{0.23} & \textbf{76.80} & 4.0x\\
%         \midrule  
%         \multirow{2}{*}{QH9-stable ood} & QHNet & 72.11 & 644.17 & 93.68 & 0.70 & 21.12 & 1.0x\\
%         & SPHNet & \textbf{43.33} & \textbf{186.40} & \textbf{98.16} & \textbf{0.23}  & \textbf{78.72} & 3.7x\\ 
%         \midrule  
%         \multirow{3}{*}{QH9-dynamic geo} & QHNet & 70.03 & 408.31 & 97.13 & 0.70  & 24.68 & 1.0x \\
%         & WANet & 74.74 & N/A & \textbf{99.68} & N/A & N/A & N/A\\ 
%         & SPHNet & \textbf{52.18} & \textbf{100.88} & 99.12 & \textbf{0.23}  & \textbf{82.56} & 3.3x \\ 
%         \midrule  
%         \multirow{2}{*}{QH9-dynamic mol} & QHNet & 120.10 & 2182.06 & 89.63 & 0.70 & 23.04 & 1.0x\\
%         & SPHNet & \textbf{108.19} & \textbf{1724.10} & \textbf{91.49} & \textbf{0.23} & \textbf{80.64} & 3.5x \\ 
%         % \midrule \midrule  
%         % \multirow{3}{*}{PubChemQH} & QHNet & 123.74 & 104.73 & 2.32 & 22.5 & 0.44 \quad/ \quad 1.0x\\
%         % & WANet & 99.98 & 101.32 & 3.13 & 15.0 & 1.09 \quad/ \quad 2.4x\\
%         % & SPHNet & 97.31 & N/A & N/A & 5.62 & 3.12\quad/ \quad 7.1x\\
%         \bottomrule
%     \end{tabular}
%     } 
%     \label{tab:qh9_result}  
% \end{table*}  
% \vspace{-5pt}

\subsection{Performance on QH9 Dataset}

We first evaluated the performance of SPHNet on the QH9 dataset. Noted that the QH9 dataset has two subsets and four different splits, including the stable-iid, stable-ood, dynamic-geo, and dynamic-mol. We trained SPHNet on four different split sets and compared the results with the baseline models QHNet and WANet. As shown in Table \ref{tab:qh9_result}, for two stable split sets, SPHNet attained a speedup of 3.7x to 4x over QHNet while improving the accuracy of the Hamiltonian Mean Absolute Error (MAE) and the occupied energy MAE. Furthermore, SPHNet significantly reduced GPU memory usage, requiring only 30\% memory usage of the baseline model. For two dynamic split sets, SPHNet maintained a speedup of 3.3x to 3.5x, simultaneously enhancing prediction accuracy and decreasing GPU memory usage by over 70\%. These results underscore SPHNet's efficiency, achieving substantial speedups and resource savings without sacrificing precision, indicating that SPHNet can effectively learn molecular features and the Hamiltonian matrix within the small-scale dataset, establishing a solid foundation for calculations in larger-scale systems.

% \vspace{-4pt}

\subsection{Performance on PubChemQH Dataset}

We further validated our approach on the larger-scale molecular system and trained SPHNet on the PubChemQH dataset. It was observed that SPHNet trains over 7.1x times faster than the baseline QHNet while achieving a better Hamiltonian prediction accuracy, as shown in Table \ref{tab:pubchem_result}, which has a higher speedup ratio compared to that in the smaller-scale datasets. Moreover, GPU memory consumption of SPHNet was significantly lower, at only 25\% and 37\% of the baselines.

\begin{table}[ht!]
    \centering
    \vspace{-7pt}
    \caption{Experimental results on PubChemQH dataset.}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|ccc}
        \toprule
        Model & QHNet & WANet & SPHNet \\ 
        \midrule \midrule
        \textit{H} [$10^{-6}E_h$] $\downarrow$ &  123.74 & \underline{99.98} & \textbf{97.31} \\
        $\epsilon$ [$E_h$] $\downarrow$ & 3.33 & \textbf{1.17} & \underline{2.16} \\
        $\psi$ [$10^{-2}$] $\uparrow$ & 2.32 & \textbf{3.13} & \underline{2.97} \\
        Memory [GB/Sample] $\downarrow$  & 22.5 & \underline{15.0} & \textbf{5.62} \\
        Speed [Sample/Sec] $\uparrow$ & 0.44  & \underline{1.09} & \textbf{3.12} \\  
        Speedup Ratio $\uparrow$ & 1.0x & \underline{2.4x} & \textbf{7.1x} \\
        \bottomrule
        % Other &  &  &  \\
        % \hline 
    \end{tabular}
    }
    \label{tab:pubchem_result}
    \vspace{-5pt}
\end{table}

We observed that SPHNet achieves a higher speedup on the PubChemQH dataset. As described in the dataset section, the number of orbitals in the Hamiltonians of PubChemQH is 15 times greater than in QH9. Additionally, since PubChemQH was computed using the Def2-TZVP orbital basis, the network requires a maximum output angular momentum order of \(L_{\text{max}} = 6\), whereas datasets computed with the Def2-SVP basis only require \(L_{\text{max}} = 4\). These make the number and time consumption of tensor products much higher than that in small-scale datasets. Thus, we can reasonably infer that the higher speedup achieved on the PubChemQH dataset is due to the markedly greater computational complexity of its molecules compared to those in the MD17 and QH9 datasets. This increased complexity introduces higher information redundancy, making it more amenable to adaptive sparsification. Our sparsity ablation experiments across different datasets support this inference. As molecular size and maximum angular momentum order increase, the rising sparsity rates of sparse pair gates and sparse tensor product gates generally result in smaller accuracy losses. Therefore, higher coefficient rates can be applied to SPHNet, further accelerating its performance on large-scale datasets. For more details on this ablation study, please refer to Section \ref{sec:sparsity_ratio}.

% The higher speedup ratio is due to the significantly higher computational complexity of molecules in the PubChemQH dataset compared to the MD17 and QH9 datasets. As discussed in the dataset section, the number of orbitals in the PubChemQH Hamiltonian is 15 times greater than in QH9. Additionally, since PubChemQH is computed using the Def2-TZVP orbital basis, it requires the max order of output irreps $L_{\text{max}}$ in the network to be 6, where the Def2-SVP dataset only requires \(L_{\text{max}}=4\). These make the number and time consumption of tensor products much higher than that in small-scale datasets. Thus, we can reasonably infer that SPHNet exhibits greater redundancy on the PubChemQH dataset, allowing for more aggressive coefficient optimization. This inference is supported by our sparsity ablation experiments on different datasets. As molecular size and the maximum order increase, the rising sparsity rates of sparse gates and sparse tensor product gates typically result in smaller accuracy losses. Therefore, a higher coefficient rate can be applied to SPHNet, further accelerating its performance on large-scale datasets. For more details on this ablation study, please refer to Section \ref{sec:sparsity_ratio}.

% Therefore, SPHNet has more optimization potential on the PubChemQH dataset. Besides, our ablation study on both large-scale system and small-scale systems indicated that we can set the sparsity rate of the Sparse Pair Gate and Sparse Tensor Product Gate to a higher ratio when the molecular size and maximum order become larger, further accelerating the speed of SPHNet in the large-scale dataset. The details about this ablation study are presented in section \ref{sec:sparsity_ratio}. 


% “tested the  speed and GPU memory under a batch size of 2“ in setting

\begin{table}[ht!]  
    \centering  
    \vspace{-12pt}
    \caption{Experimental results on MD17 dataset.}  
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{p{1.5cm}cccc}  
        \toprule
        Dataset & Model & \textit{H} [$10^{-6}E_h$] $\downarrow$ & $\epsilon$ [$10^{-6}E_h$] $\downarrow$ & $\psi$ [$10^{-2}$] $\uparrow$ \\  
        \midrule\midrule
        \multirow{5}{*}{Water} 
        & SchNOrb & 165.4 & 279.3 & \textbf{100.00} \\
        & PhiSNet & \underline{15.67} & \underline{85.53} & \textbf{100.00} \\
        & QHNet & \textbf{10.79} & \textbf{33.76} & 99.99 \\ 
        & SPHNet & 23.18 & 182.29 & \textbf{100.00} \\
        \midrule  
        \multirow{5}{*}{Ethanol} 
        & SchNOrb & 187.4 & 334.4 & \textbf{100.00} \\
        & PhiSNet & \textbf{20.09} & 102.04 & 99.81 \\
        & QHNet & \underline{20.91} & \textbf{81.03} & 99.99 \\
        & SPHNet & 21.02 & \underline{82.30} & \textbf{100.00} \\ 
        \midrule  
        \multirow{5}{*} {\makecell{Malon-\\aldehyde}}
        & SchNOrb & 191.1 & 400.6 & 99.00 \\
        & PhiSNet & \underline{21.31} & 100.6 & 99.89 \\
        & QHNet & 21.52 & \textbf{82.12} & \underline{99.92} \\
        & SPHNet & \textbf{20.67} & \underline{95.77} & \textbf{99.99} \\ 
        \midrule  
        \multirow{5}{*}{Uracil} 
        & SchNOrb & 227.8 & 1760 & 90.00 \\
        & PhiSNet & \textbf{18.65} & 143.36 & 99.86 \\
        & QHNet & 20.12 & \textbf{113.44} & \underline{99.89} \\
        & SPHNet & \underline{19.36} & \underline{118.21} & \textbf{99.99} \\ 
        \bottomrule
    \end{tabular}
      }
    \label{tab:md17_result}
    \vspace{-7pt}
\end{table} 



\subsection{Performance on MD17 Dataset}

We also evaluated the performance of SPHNet on the smaller-scale MD17 dataset. As presented in Table \ref{tab:md17_result}, we compared SPHNet’s performance with four baseline models across four MD17 molecules—water, ethanol, malondialdehyde, and uracil—comprising 3 to 12 atoms. The results demonstrate that SPHNet achieves accuracy comparable to other models, indicating its suitability for small molecular trajectory datasets. 

It is worth noting that predictions on the MD17 dataset represent a relatively simple task compared to other datasets, as it focuses solely on small systems and their conformational spaces. On the one hand, baseline models generally already perform well across all these datasets, leaving limited room for SPHNet to achieve significant improvements in either accuracy or speed. On the other hand, taking the water molecule with only three atoms as an example, the number of possible interaction combinations within the system is inherently small, which limits the potential benefits of adaptive sparsification.

 
\subsection{Ablation Study on Sparsity Rate}
\label{sec:sparsity_ratio}

The key idea of SPHNet is the implementation of adaptive sparsity to reduce computational demands in SE(3) equivariant networks. The most critical hyperparameter in this adaptive sparse system is the sparsity rate. To determine the optimal sparsity rate and assess its impact across various molecular systems, we conducted a series of experiments. We varied the sparsity rate from 0\% to 90\% in 10\% intervals and trained SPHNet on three datasets representing different molecular scales (Here we chose Ethanol to represent MD17 dataset). As illustrated in Fig.\ref{fig:sparse_ratio}, the predicted Hamiltonian MAE across all three datasets remained relatively stable at lower sparsity rates but increased significantly when the rate reached a specific threshold. This finding suggests that an appropriate range of sparsity has minimal impact on model accuracy while simultaneously enhancing computational speed.
\begin{figure}[h]
    \centering
    \vspace{-5pt} 
    \includegraphics[width=0.8\linewidth]{figs/sparse_ratio.png}
    \vspace{-8pt} 
    \caption{The effect of different sparsity rates on the model performance on the datasets with different scale of molecules.}
    \label{fig:sparse_ratio}
     \vspace{-7pt}
\end{figure}

Additionally, we observed that the threshold at which the MAE begins to rise significantly becomes larger as the complexity of the molecular system increases. For the smallest MD17 system, the critical turning point is at 30\%, while for the QH9 system, it is 40\%, and for the largest PubChemQH system, it rises to 70\%. In the experiments conducted in this study, we adopted the above critical turning point as the sparsity rate for training and testing for different datasets. This observation aligns with our earlier conclusion that the adaptive sparsity strategy offers greater potential in large-scale systems, as there are more calculations of lower importance that can be optimized. Consequently, the system size can serve as a rough estimate for determining the applicable sparsity rate, thereby reducing the need for extensive parameter searches.

\subsection{Analysis of Optimal Selected Set}
\label{sec:sparsity_path}

Experiments demonstrated that the two adaptive sparse gates effectively select the most important tensor products and their optimal combinations. Here, we examined the selected pairs and combinations to understand the learning outcomes of the sparse gates. 

For the optimal pair set, we analyzed the length(atomic distance) of the selected pairs, as illustrated in Fig.\ref{fig:sparse_path}(A). The results indicate that sparse gating differs from the commonly used RBF-based cutoff strategy, as it evaluates the importance of a pair based solely on its contribution to the output irreps, rather than its distance. Interestingly, as the pair length increases, the probability of a pair being selected also rises. This is likely because the features of long-range pairs are more challenging to learn, requiring as many samples as possible to achieve accurate representations. Notably, the selection probability increases nearly linearly in the distance range of 16Å to 25Å. This range corresponds to the influence of electrostatic interactions and may also include weak van der Waals forces. These findings highlight the importance of long-range interactions in Hamiltonian prediction and demonstrate the effectiveness of sparse pair gating in dynamically integrating both long-range and short-range interactions, ultimately enabling the model to achieve superior performance.
% The results indicated that the Sparse Pair Gate does not solely assess the importance of a pair based on distance, distinguishing it from a simple cutoff strategy. Instead, the probability of a pair being selected increased as the length of the pair got larger. This may be because distant pair features are more difficult to learn, requiring more computation to get more accurate features.
\begin{figure}[h]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.8\linewidth]{figs/sparse_path.png}
    \vspace{-5pt} 
    \caption{\textbf{(A)} The distribution of distance between selected node pairs and the selected proportions of different pair distances in the pair gate.  \textbf{(B)} The proportions of each output order being selected in the first Non-Diagonal block's tensor product gate. }
    \vspace{-7pt}
    \label{fig:sparse_path}
\end{figure}

Then, for the selected optimal combination set, we analyze the order of output irreps of each selected cross-order combination in tensor product operations. For a classical tensor product operation, each irreps of the output \(x^{\ell_3}\) is contributed by \(O({\ell_3}^2)\) cross-channel combinations. As shown in Fig.\ref{fig:sparse_path}(B) and Fig.\ref{fig:sparse_path_sup}, in the QH9 dataset, the proportion of each order that is selected decreases monotonically as the order increases in both Sparse Combination Gates. In the PubChemQH, the trend is the same except the order zero has a relatively lower selected ratio. These results suggest that although higher orders' output irreps contain more combinations, the importance of a single combination is weakened as the number of combinations increases, making them easier to filter out.


% \subsection{Comparison Within the Same Training Time}
% To better demonstrate the benefits of model acceleration for Hamiltonian prediction, we trained and tested QHNet and SPHNet within the same limited timeframe to evaluate prediction accuracy. As shown in Fig.\ref{fig:limit_time}, with a target Hamiltonian MAE of 200e-6 Hartree, QHNet requires approximately 3x and 6x more time on the QH9 and PubChemQH datasets than our model respectively. Notably, QHNet often requires over 250,000 iterations to converge, with the default setting described in their paper, which can take more than a week. Thus, the time savings with SPHNet are substantial given such a high iteration count. Similarly, when both models are trained for the same duration, SPHNet achieves more accurate results on both datasets compared to QHNet. This demonstrates the efficiency advantages of SPHNet, which significantly benefit both the training and inference processes.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figs/train_time_mae.png}
%     \caption{Comparison of the Hamiltonian MAE within the same training time on QH9 dataset (a) and PubChemQH dataset (b).}
%     \label{fig:limit_time}  
% \end{figure}

\subsection{Scaling with increasing size of Hamiltonian}
To further validate the limits of Hamiltonian prediction scale achievable by different models under restricted GPU memory, as well as the scaling of speed and memory consumption with increasing system size, we randomly selected 6 molecules of varying sizes from the PubChemQC PM6 dataset. We computed the labels for these molecules with the setting of PubChemQH, and conducted tests with a single A6000 card based on the outcomes. The results are illustrated in Fig.\ref{fig:scaling} and Fig.\ref{fig:scale_mem}.

Our method demonstrates better scaling than the baseline model. As the Hamiltonian size (number of atomic orbitals) increases, the speedup ratio improves while memory consumption decreases. Thanks to significant savings in both memory and time, our model can train on larger systems (around 3000 atomic orbitals) within the same memory consumption, whereas the baseline model is limited to approximately 1800 atomic orbitals.

% We observed that our method exhibits better scaling compared to the baseline model. Specifically, as the Hamiltonian size (the number of atomic orbitals in the Hamiltonian matrix) increases, the speedup ratio becomes larger with fewer memory consumption. Compared to the baseline model, our model, due to significant savings in memory and time, can train on larger systems (approximately 3000 atomic orbitals) within a similar time frame. In contrast, the baseline model manages around 1800 atomic orbitals.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/scale_up.png}
    \vspace{-7pt} 
    \caption{Comparison of the training speed with the increasing size of Hamiltonian.}
    \label{fig:scaling}
    \vspace{-10pt}
\end{figure}
% \section{Relate work}

% NN method in the field of DFT. Classical model for energy, force and other physical quantity prediction.

% Methods for hamiltonian matrix prediction. Schnetorb, PhiSNet, QHNet, other new method.

% Methods for tensor product speed up.

\section{Conclusion and Future Work}
In this work, we introduced SPHNet, an efficient and scalable SE(3) equivariant neural network for Hamiltonian prediction. By incorporating two adaptive sparse gates and a corresponding three-phase learning scheduler, SPHNet optimized both the number of tensor product operations and the efficiency of individual tensor computations. As a result, SPHNet achieved exceptional efficiency and performance in predicting Hamiltonians for larger molecular systems. Moreover, the proposed sparsification techniques demonstrated significant potential for extension to other SE(3) equivariant networks and broader prediction tasks.

This study opens several promising avenues for future work. First, the adaptive sparsity technique shows considerable potential for generalization to a wider range of tasks. Specifically, the sparse tensor product gates can be readily extended to any SE(3)-equivariant network architecture based on Clebsch-Gordan tensor products, while the sparse pair gates could be adapted to more types of pairwise interactions, such as those found in MACE \cite{batatia2022mace} for multi-body interactions. Second, integrating advanced loss functions, such as those introduced in \cite{huang2024enhancing}, could further improve the accuracy of downstream properties of the Hamiltonian matrix, particularly in enhancing the applicability for large molecular systems. These directions will guide future efforts with further experimental validation, expanding the impact of adaptive sparsification and optimizing the performance of SE(3)-equivariant networks.

% However, our work had certain limitations. First, while adaptive sparsity could naturally generalize to other SE(3) network architectures for wider applications and impacts, we did not provide extensive experimental validation in this regard. Second, due to the scope of this study, we did not integrate novel loss functions, such as those proposed in \cite{huang2024enhancing}, to further improve the accuracy of downstream properties of the Hamiltonian matrix for large systems. As a result, some evaluation metrics, while improved with SPHNet, still fell below the precision required for chemical applications. These limitations will guide our future research and drive further advancements in adaptive sparsification, broadening its impact across various tasks.

% we introduce SPHNet, an efficient SE(3)-equivariant neural network specifically designed for Hamiltonian prediction tasks in molecular systems. By employing an advanced adaptive sparsification strategy in conjunction with a three-phase learning scheduler, we optimize both the number of tensor product operations and the efficiency of individual tensor product computations. As a result, SPHNet achieves over a 7-fold speedup in prediction tasks for larger molecular systems, significantly enhancing the scalability of Hamiltonian prediction methods. The proposed sparsification techniques can also be applied to other SE(3) equivariant networks to broaden their application and impact. However, our work does have certain limitations. First, we found that the accuracy of downstream properties of the Hamiltonian matrix for large systems, despite enhancements, still falls considerably short of the precision required for chemical applications. Second, as a model trained on a single dataset, SPHNet demonstrates limited generalization capabilities, which restricts its broader applicability. These limitations will guide our future research directions. We believe that SPHNet can greatly contribute to the task of Hamiltonian prediction and bring more benefits to the field of computational chemistry.


% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.



% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{icml_2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Additional Experimental Settings}
\label{sec:exp_setting}
\subsection{Training setup.}
% 换成表
The baseline model QHNet was using its default setting. The training setting for SPHNet is shown in Table \ref{tab:model_training_settings}, most training settings were set to align with the QHNet's experiment to make a fair comparison. The batch size for training is 10 on the MD17 dataset (except Uracil which was set to 5), 32 on the QH9 dataset, and 8 on the PubChemQH dataset. The training step was 260,000 for the QH9 dataset, 200,000 for the MD17 dataset, and 300,000 for the PubChemQH dataset. There was a 1,000-step warmup with the polynomial schedule. The maximum learning rate was 1e-3 for the QH9 dataset and PubChemQH dataset and was 5e-4 for the MD17 dataset. The training and test sets in the QH9 dataset were split in the same manner as their official implementation, including four different split sets. The MD17 dataset was randomly split into the train, validation, and test sets of the same size as the QHNet experiment. The PubChemQH datasets were split randomly into train, validation, and test sets by 80\%, 10\%, and 10\%. The batch size for speed and GPU memory test was set to the maximum number that the GPU can hold to maximize its capability for fair comparison. 
%. There was a gradient clip of 5 in all experiments.

\begin{table}[h]
    \centering
    \caption{Model Training Settings for Different Datasets.}
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{lcccccccc}
        \hline
        Dataset                   & Batch Size & Training Steps & Warmup Steps & Learning Rate& $L_{\text{max}}$ & Sparsity Rate & TSS Epoch $t$ &Train/Val/Test Split Method \\
        \hline
        MD17 Water & 10 & 200,000 & 1,000 & 5e-4   & 4 & 0.1 & 3 & Random (500/500/3900)       \\
        \hline
        MD17 Ethanol & 10 & 200,000 & 1,000 & 5e-4 & 4 & 0.3 & 3 & Random (25000/500/4500)       \\
        \hline
        MD17 Malondialdehyde & 10  & 200,000 & 1,000 & 5e-4 & 4 & 0.3 & 3   & Random (25000/500/1478)       \\
        \hline
        MD17 Uracil & 5 & 200,000 & 1,000 & 5e-4 & 4 & 0.3 & 3 & Random (25000/500/4500)       \\
        \hline
        
        QH9   & 32  & 260,000  & 1,000   & 1e-3  & 4 & 0.4 & 3  & Official (4 splits)     \\
        \hline
        PubChemQH & 8  & 300,000 & 1,000 & 1e-3  & 6 & 0.7 & 3 & Random (40257/5032/5032)       \\
        \hline
    \end{tabular}
    }
    \label{tab:model_training_settings}
\end{table}


\subsection{Hardware and software.}
Our experiments are implemented based on PyTorch 2.1.0,
PyTorch Geometric 2.5.0, and \texttt{e3nn} \cite{e3nn} 0.5.1. In our experiments, the speed and GPU memory metrics are tested on a single NVIDIA RTX A6000 46GB GPU. The complete training of the models is carried on 4 $\times$ 80GB Nvidia A100 GPU. Our code is available in supplementary material.

\subsection{Problem Formulation.}
The loss function of SPHNet is the MAE plus MSE between predicted matrix $H_{pred}$ and ground truth matrix $H_{ref}$. 
\begin{equation}
    loss = MAE(\mathbf{H}_{ref},\mathbf{H}_{pred})+MSE(\mathbf{H}_{ref},\mathbf{H}_{pred}).
\end{equation}
SPHNet predicts the gap between the Hamiltonian matrix and the initial guess. The predicted target can be written as:
\begin{equation}
    \Delta \mathbf{H} = \mathbf{H}_{ref} - \mathbf{H}_{init},
\end{equation}
where $\mathbf{H}_{init}$ is the initial guess of Hamiltonian matrix get with the \texttt{pyscf} \cite{sun2020recent} by function $init\_guess\_by\_minao(\cdot)$. On one hand, we observed that the scale and variance of \( \Delta \mathbf{H} \) are approximately an order of magnitude smaller than those of \( \mathbf{H}_{\text{ref}} \) on large datasets such as PubchemQH, which facilitates more stable learning across different datasets. On the other hand, the computational cost for calculating the initial guess is very cheap, it would not take long to obtain all the initial guesses of molecules in the dataset. Thus, we switched the training target to the $\Delta \mathbf{H}$ and added the $\mathbf{H}_{init}$ back to the predicted  $\Delta \mathbf{H}$ to get the full Hamiltonian matrix. 

\subsection{Evaluation Metric}

\textbf{Mean Absolute Error (MAE) of Hamiltonian Matrix \( H \)}: This metric quantifies the Mean Absolute Error in relation to ground truth data obtained from Density Functional Theory (DFT), accounting for both diagonal and Non-Diagonal elements that reflect intra- and inter-atomic interactions, respectively. This can be presented as:
\begin{equation}
    \mathbf{H} = mean (|\mathbf{H}_{\text{pred}} - \mathbf{H}_{\text{gt}}|),
\end{equation}
where \( H_{\text{pred}} \) is the predicted Hamiltonian matrix and \( H_{\text{gt}} \) is the reference Hamiltonian matrix .

\textbf{Mean Absolute Error (MAE) of Occupied Orbital Energies \( \epsilon \)}: This metric evaluates the MAE of the occupied orbital energies, specifically the Highest Occupied Molecular Orbital (HOMO) and Lowest Unoccupied Molecular Orbital (LUMO) levels. The accuracy of these critical properties is assessed by comparing the energies derived from the predicted Hamiltonian matrix against those obtained from the reference DFT calculations. The MAE can be expressed as:
\begin{equation}
\epsilon = \frac{1}{M} \sum_{k=1}^{M} |\epsilon_k^{\text{pred}} - \epsilon_k^{\text{ref}}|,
\end{equation}
where \( M \) represents the number of occupied orbitals, $\epsilon_k^{\text{pred}}$ and $\epsilon_k^{\text{ref}}$ are the predicted and reference energy of the k-th occupied orbital.

\textbf{Cosine Similarity of Orbital Coefficients \( \psi \)}: To evaluate the similarity between the predicted and reference electronic wavefunctions, we calculate the cosine similarity of the coefficients corresponding to the occupied molecular orbitals. This similarity metric is pivotal for understanding and predicting the chemical properties of the system. The cosine similarity \( S \) can be defined as:
\begin{equation}
S(\psi^{\text{pred}}, \psi^{\text{ref}}) = \frac{\sum_{i} \psi_i^{\text{pred}} \psi_i^{\text{ref}}}{\|\psi^{\text{pred}}\| \|\psi^{\text{ref}}\|},
\end{equation}
where \( \|\psi\| \) denotes the norm of the vector \( \psi \), $\psi_i^{\text{ref}}$ and $\psi^{\text{ref}}$ are the Coefficient of the i-th occupied molecular orbital in the predicted and reference wavefunction

\section{Additional Experimental Results}
\label{app:exp_result}

\subsection{Ablation Study for Three-phase Sparsity Scheduler}
We evaluated the impact of the three-phase sparsity scheduler on model performance. We trained the SPHNet  under several settings: without the random stage, without the adaptive stage, and without the fixed stage. Each configuration was trained five times to assess overall performance. As shown in Table \ref{tab:scheduler_ablation}, models trained without the random stage tended to converge to localized optimal points, resulting in suboptimal performance. The models trained without an adaptive stage exhibited more randomness in performance, achieving significantly lower accuracy compared to the SPHNet with a complete scheduler. For the model trained without the fixed stage, although it did not show a substantial difference in accuracy metrics, the speedup ratio dropped to 5.45 due to the additional computational overhead from the sparse gate and the non-static graph implementation of tensor products following the Sparse Pair Gate, as opposed to standard implementations in the e3nn library.

\begin{table*}[h]  
    \centering  
    \caption{Ablation Experimental results on Three-phase Sparsity Scheduler.}  
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{lccccccc}  
        \toprule
        % & \# of Params
        \multirow{2}{*}{Model} & Random  & Adaptive & Fixed & \textit{H}  $\downarrow$ & Memory $\downarrow$  & Speed $\uparrow$ & Speedup $\uparrow$\\ 
         & Stage & Stage & Stage & [$10^{-6}E_h$] & [GB/Sample] & [Sample/Sec] & Ratio \\
        \midrule\midrule 
        SPHNet & \ding{51} & \ding{51} & \ding{51} &  97.31 $\pm$ 0.52 & 5.62 & 3.12 & 7.09x\\
        \midrule  
        SPHNet & \ding{55} & \ding{51} & \ding{51} &  112.68 $\pm$ 10.75 & 5.62 & 3.12 & 7.09x\\
        SPHNet & \ding{51} & \ding{55} & \ding{51} &  122.79 $\pm$ 19.02 & 5.62 & 3.12 & 7.09x\\ 
        SPHNet & \ding{51} & \ding{51} & \ding{55} &  97.11 $\pm$ 1.33 & 5.62 & 3.12 & 5.45x\\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab:scheduler_ablation}  
\end{table*} 
 
\subsection{Ablation Study for Sparse Gates and Interaction blocks}

\begin{table*}[h]  
    \centering  
    \caption{Ablation Experimental results on PubChemQH dataset.}  
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccccc}  
        \toprule
        % & \# of Params
        \multirow{2}{*}{Model} & Sparse Pair & Sparse TP & Vectorial Node & Spherical Node & \textit{H}  $\downarrow$ & Memory $\downarrow$  & Speed $\uparrow$ & Speedup $\uparrow$\\ 
         & Gate & Gate & Interaction block & Interaction block &[$10^{-6}E_h$] & [GB/Sample] & [Sample/Sec] & Ratio \\
        \midrule\midrule 
        SPHNet & \ding{51} & \ding{51} & 4 & 2 & 97.31 & 5.62 & 3.12 & 7.09x\\
        \midrule  
        SPHNet & \ding{55} & \ding{51} & 4 & 2 & 94.31 & 8.04 & 1.75 & 3.98x\\
        SPHNet & \ding{51} & \ding{55} & 4 & 2 & 87.70 & 6.98  & 1.20 & 2.73x\\ 
        SPHNet & \ding{55} & \ding{55} & 4 & 2 & 86.35 & 10.91  & 0.76 & 1.73x\\ 
        % \midrule  
        SPHNet & \ding{51} & \ding{51} & 0 & 5 & 97.08 & 8.47 & 1.08 & 2.45x\\
        \bottomrule
    \end{tabular}
    } 
    \label{tab:pub_ablation}  
\end{table*} 

To better evaluate the effects of the Sparse Pair Gate and Sparse Tensor Product Gate on the overall performance, we also trained the SPHNet on three different settings, including SPHNet without Sparse Pair Gate, SPHNet without Sparse Tensor Product Gate, and SPHNet without both Gate. As shown in Table \ref{tab:pub_ablation}, the Sparse Pair Gate could bring 78\% speedup and 30\% GPU memory reduction to the model with little loss of model accuracy. The Sparse Tensor Product Gate in our setting (remove 70\% combinations in the tensor product) would lose about 10\% accuracy, however, it could bring 160\% speedup, which was enough for us to ignore the loss it caused. It's noteworthy that when the computational resource is higher, the user can set a lower sparsity rate to improve the accuracy (details are presented in section \ref{sec:sparsity_ratio}).

We then evaluated the impact of the Vectorial Node Interaction block on model performance. As shown in Table \ref{tab:pub_ablation}, we replaced the Vectorial Node Interaction block with three Spherical Node Interaction blocks. The results indicated that the accuracy of the Hamiltonian MAE remained comparable to that of the original SPHNet, however, performance in terms of memory consumption and speed declined significantly. This suggests that, despite the absence of the tensor product operation, the Vectorial Node Interaction block can effectively extracts atomic information from the input data with much less memory consumption and much faster training speed.

% \newpage
\subsection{Selected Combination Set Details}
Here we presented the details of the selected combination sets in all Sparse TP Gate in the SPHNet. We first analysis the output order of the selected combinations. As shown in Fig.\ref{fig:sparse_path_sup}, the selected combinations' output order in the Non-Diagonal block shows a monotonically decreasing trend as analyzed in section \ref{sec:sparsity_path}, suggesting the weakening of a single combination as the number of combinations within the same order increase. In the Diagonal block, we didn't observe any apparent pattern, this might be because all the combinations have similar effects on the model performance. However, this requires further analysis in the future.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/select_comb_details.png}
    \caption{The proportions of each output order being selected in different tensor product gates. }
    \label{fig:sparse_path_sup}
\end{figure}

% \newpage
We then check the combination weights for each of the selected combinations. The details are shown in Table \ref{tab:path_weight_pubchem}, Table \ref{tab:path_weight_qh9} and Table \ref{tab:path_weight_md17}. These results are from the Sparse Tensor Product Gate before the first Non-Diagonal block. For the PubchemQH dataset, there are a total of 175 combinations in the tensor product (maximum irreps order 6), and the Sparse Pair Gate selects 30\% of these combinations. For the QH9 dataset and MD17 dataset, there are a total of 65 combinations in the tensor product (maximum irreps order 4), and the Sparse Pair Gate selects 60\% of these combinations for QH9, 70\% of these combinations for MD17. 
\begin{table}[h]
    \centering
    \caption{Weight of selected combinations in PubChemQH}
    % \begin{tabular}{@{}llllllll@{}}
    \resizebox{0.7\textwidth}{!}{
    \begin{tabular}{ccccccccc}
        \toprule
        \textbf{Rank} & \textbf{Comb}          & \textbf{Weight} & \textbf{Rank} & \textbf{Comb}          & \textbf{Weight} & \textbf{Rank} & \textbf{Comb}          & \textbf{Weight} \\ \midrule
        1               & (4, 2, 3)            & 1.1422             & 19              & (2, 5, 5)            & 1.0736       & 37              & (4, 4, 2)            & 1.0488            \\
        2               & (5, 4, 2)            & 1.1398             & 20              & (3, 0, 3)            & 1.0725             & 38              & (3, 1, 2)            & 1.0530             \\
        3               & (0, 4, 4)            & 1.1160             & 21              & (4, 3, 5)            & 1.0710             & 39              & (4, 4, 2)            & 1.0488             \\
        4               & (0, 3, 3)            & 1.1093             & 22              & (0, 5, 5)            & 1.0710             & 40              & (5, 2, 3)            & 1.0486             \\
        5               & (3, 5, 2)            & 1.1081             & 23              & (2, 3, 3)            & 1.0682             & 41              & (2, 3, 1)            & 1.0485             \\
        6               & (6, 6, 6)            & 1.1078             & 24              & (4, 6, 2)            & 1.0664             & 42              & (6, 5, 3)            & 1.0482             \\
        7               & (5, 2, 5)            & 1.1031             & 25              & (4, 6, 4)            & 1.0651             & 43              & (3, 3, 4)            & 1.0478             \\
        8               & (6, 3, 5)            & 1.1028             & 26              & (6, 3, 3)            & 1.0647             & 44              & (4, 5, 1)            & 1.0464             \\
        9               & (4, 0, 4)            & 1.1015             & 27              & (6, 5, 1)            & 1.0607             & 45              & (5, 4, 1)            & 1.0452             \\
        10              & (1, 4, 3)            & 1.0978             & 28              & (3, 5, 4)            & 1.0600             & 46              & (5, 0, 5)            & 1.0417             \\
        11              & (2, 5, 3)            & 1.0917             & 29              & (6, 4, 2)            & 1.0596             & 47              & (3, 4, 1)            & 1.0377             \\
        12              & (5, 3, 2)            & 1.0903             & 30              & (2, 0, 2)            & 1.0579             & 48              & (4, 3, 1)            & 1.0300             \\
        13              & (2, 1, 3)            & 1.0856             & 31              & (3, 2, 1)            & 1.0566             & 49              & (1, 0, 1)            & 1.0283             \\
        14              & (5, 5, 4)            & 1.0854             & 32              & (5, 4, 4)            & 1.0545             & 50              & (6, 6, 4)            & 1.0281             \\
        15              & (4, 4, 4)            & 1.0842             & 33              & (5, 6, 1)            & 1.0537             & 51              & (5, 3, 4)            & 1.0274             \\
        16              & (2, 4, 2)            & 1.0835             & 34              & (4, 3, 3)            & 1.0535             & 52              & (5, 5, 5)            & 1.0248             \\
        17              & (3, 6, 3)            & 1.0829             & 35              & (4, 5, 3)            & 1.0534             & 53              & (2, 2, 2)            & 0.9987             \\
        18              & (1, 2, 1)            & 1.0799             & 36              & (3, 1, 2)            & 1.0530             &                &              &               \\ \bottomrule
    \end{tabular}}
    \label{tab:path_weight_pubchem}
\end{table}


\begin{table}[h]
    \centering
    \caption{Weight of selected combinations in QH9}
    \resizebox{0.7\textwidth}{!}{ % Adjust the size of the table if necessary
    \begin{tabular}{ccccccccc}
        \toprule
        \textbf{Rank} & \textbf{Comb} & \textbf{Weight} & \textbf{Rank} & \textbf{Comb} & \textbf{Weight} & \textbf{Rank} & \textbf{Comb} & \textbf{Weight} \\
        \midrule
        1  & (1, 3, 2)  & 1.1748  & 14 & (3, 1, 2)  & 1.0758  & 27 & (3, 0, 3)  & 1.0154  \\
        2  & (2, 4, 2)  & 1.1722  & 15 & (2, 0, 2)  & 1.0715  & 28 & (2, 3, 4)  & 1.0150  \\
        3  & (3, 2, 1)  & 1.1661  & 16 & (2, 2, 0)  & 1.0696  & 29 & (4, 4, 1)  & 1.0145  \\
        4  & (3, 4, 1)  & 1.1433  & 17 & (3, 2, 3)  & 1.0577  & 30 & (1, 0, 1)  & 1.0145  \\
        5  & (3, 4, 3)  & 1.1284  & 18 & (2, 3, 1)  & 1.0530  & 31 & (0, 2, 2)  & 1.0133  \\
        6  & (2, 1, 3)  & 1.1240  & 19 & (0, 0, 0)  & 1.0406  & 32 & (3, 4, 2)  & 1.0117  \\
        7  & (3, 3, 2)  & 1.1233  & 20 & (4, 4, 0)  & 1.0383  & 33 & (1, 3, 3)  & 1.0108  \\
        8  & (2, 2, 2)  & 1.1163  & 21 & (4, 3, 1)  & 1.0336  & 34 & (2, 2, 1)  & 1.0099  \\
        9  & (1, 1, 0)  & 1.1157  & 22 & (1, 2, 1)  & 1.0319  & 35 & (4, 2, 3)  & 1.0097  \\
        10 & (4, 4, 2)  & 1.1085  & 23 & (1, 1, 1)  & 1.0276  & 36 & (2, 3, 2)  & 1.0090  \\
        11 & (0, 3, 3)  & 1.0972  & 24 & (3, 4, 4)  & 1.0213  & 37 & (3, 2, 2)  & 1.0059  \\
        12 & (3, 3, 1)  & 1.0853  & 25 & (0, 4, 4)  & 1.0198  & 38 & (3, 1, 4)  & 1.0057  \\
        13 & (0, 1, 1)  & 1.0788  & 26 & (3, 3, 0)  & 1.0154  & 39 & (4, 0, 4)  & 0.9912  \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:path_weight_qh9}
\end{table}


\begin{table}[h]
    \centering
    \caption{Weight of selected combinations in MD17 (Ethanol)}
    \resizebox{0.7\textwidth}{!}{ % Adjust the size of the table if necessary
\begin{tabular}{ccccccccc}
\toprule
\textbf{Rank} & \textbf{Comb}           & \textbf{Weight} & \textbf{Rank} & \textbf{Comb}           & \textbf{Weight} & \textbf{Rank} & \textbf{Comb}           & \textbf{Weight} \\
\midrule
1  & (3, 2, 1)  & 1.0755  & 16 & (3, 3, 1)  & 1.0213  & 31 & (3, 3, 0)  & 0.9894  \\
2  & (3, 4, 1)  & 1.0674  & 17 & (4, 4, 2)  & 1.0206  & 32 & (4, 4, 1)  & 0.9891  \\
3  & (2, 1, 1)  & 1.0608  & 18 & (2, 2, 1)  & 1.0141  & 33 & (4, 1, 3)  & 0.9886  \\
4  & (1, 2, 1)  & 1.0592  & 19 & (3, 1, 2)  & 1.0119  & 34 & (2, 2, 2)  & 0.9878  \\
5  & (4, 3, 1)  & 1.0449  & 20 & (2, 0, 2)  & 1.0090  & 35 & (3, 2, 2)  & 0.9856  \\
6  & (0, 2, 2)  & 1.0447  & 21 & (4, 4, 0)  & 1.0037  & 36 & (4, 3, 3)  & 0.9851  \\
7  & (0, 3, 3)  & 1.0446  & 22 & (2, 1, 3)  & 1.0016  & 37 & (3, 3, 3)  & 0.9811  \\
8  & (2, 4, 2)  & 1.0418  & 23 & (3, 3, 2)  & 1.0003  & 38 & (3, 4, 4)  & 0.9802  \\
9  & (2, 3, 1)  & 1.0417  & 24 & (3, 4, 2)  & 0.9995  & 39 & (3, 4, 3)  & 0.9795  \\
10 & (1, 3, 2)  & 1.0408  & 25 & (0, 4, 4)  & 0.9991  & 40 & (1, 3, 4)  & 0.9787  \\
11 & (0, 1, 1)  & 1.0399  & 26 & (3, 0, 3)  & 0.9978  & 41 & (2, 3, 2)  & 0.9769  \\
12 & (1, 1, 2)  & 1.0349  & 27 & (4, 3, 2)  & 0.9972  & 42 & (3, 1, 3)  & 0.9763  \\
13 & (1, 0, 1)  & 1.0330  & 28 & (3, 2, 3)  & 0.9928  & 43 & (2, 4, 3)  & 0.9736  \\
14 & (4, 2, 2)  & 1.0301  & 29 & (2, 3, 4)  & 0.9920  & 44 & (2, 3, 3)  & 0.9730  \\
15 & (1, 4, 3)  & 1.0295  & 30 & (0, 0, 0)  & 0.9911  & 45 & (1, 2, 2)  & 0.9719  \\
\bottomrule
\end{tabular}
    }
    \label{tab:path_weight_md17}
\end{table}

\newpage
\subsection{GPU Memory when Scaling with Increasing Size}
We used molecules with different sizes to test the GPU memory consumption of SPHNet and the baseline model QHNet. As shown in Fig.\ref{fig:scale_mem}, we observed that the GPU memory consumption of SPHNet increased much slower than the baseline model. When the molecule had more than 1800 atomic orbitals, the baseline model reached the maximum GPU memory, which is 6 times the memory SPHNet needs, while the SPHNet can handle molecules with around 2900 atomic orbitals, making it possible to train on larger molecule systems. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{figs/scale_up_mem.png}
    \caption{The GPU memory consumption with scaling molecule size. }
    \label{fig:scale_mem}
\end{figure}

% \newpage
\subsection{Single Tensor Product Time Scaling with Sparsity Rate}
\label{appendix:tp_scaling}
We test the time consumption for a single tensor product under different sparsity rates. As shown in Fig.\ref{fig:tp_time}, the time consumption decreases linearly with increasing sparsity, in line with our expectations. This experiment suggested that the sparsity strategy can accelerate the tensor product speed on a linear scale.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{figs/tp_time.png}
    \caption{The time consumption for a single tensor product with scaling sparsity rates. }
    \label{fig:tp_time}
\end{figure}

% \subsection{performance of DFT calculation acceleration ratio}
\section{Additional Preliminary}
\subsection{Spherical Harmonic Function}

The spherical harmonic function in SPHNet is used to project the atomic vector $\vec{r_{ij}}$ into spherical space. A real basis for spherical harmonics \( Y_{lm} : S^2 \rightarrow \mathbb{R} \) can be expressed in relation to their complex analogues \( Y^l_m : S^2 \rightarrow \mathbb{C} \). This relationship is formulated through the following equations:

\begin{equation}
Y^l_{m} = 
\begin{cases}
\frac{i}{\sqrt{2}} Y^l_{-|m|} - (-1)^{m} Y^l_{|m|}, & \text{if } m < 0. \\ 
Y^l_0, & \text{if } m = 0. \\ 
\frac{1}{\sqrt{2}} \left( Y^l_{|m|} + (-1)^{m} Y^l_{-|m|} \right), & \text{if } m > 0. 
\end{cases}
\end{equation}

To ensure both consistency and standardized treatment throughout the analysis, the Condon–Shortley phase convention is frequently utilized. The inverse relationships that define the complex spherical harmonics \( Y^l_m : S^2 \rightarrow \mathbb{C} \) in terms of the real spherical harmonics \( Y_{lm} : S^2 \rightarrow \mathbb{R} \) are outlined as follows:

\begin{equation}
Y^l_m = 
\begin{cases}
\frac{1}{\sqrt{2}} \left( Y^{|l|}_{|m|} - i Y_{l, -|m|} \right), & \text{if } m < 0. \\ 
Y_{l, 0}, & \text{if } m = 0. \\ 
(-1)^{m} \frac{1}{\sqrt{w}} \left( Y_{l, |m|} + i Y_{l, -|m|} \right), & \text{if } m > 0.
\end{cases}
\end{equation}

Established theories regarding the analytical solutions of the hydrogen atom indicate that the eigenfunctions corresponding to the angular component of the wave function are represented as spherical harmonics. Notably, in the absence of magnetic interactions, the solutions to the non-relativistic Schrödinger equation can also be represented as real functions. This aspect highlights the common use of real-valued basis functions in electronic structure calculations, as it simplifies software implementations by eliminating the need for complex algebra. It is crucial to acknowledge that these real-valued functions occupy a functional space identical to that of their complex counterparts, thus preserving generality and completeness in the solutions.


\subsection{Irreps and Tensor Products}

\textbf{Irreps Representation}: SPHNet employs the special orthogonal group \( \text{SO}(3) \) to capture the essential 3D rotational symmetries intrinsic to molecular structures. It leverages the irreducible representations (irreps) of \( \text{SO}(3) \), indexed by an integer \( l \), which are associated with spherical harmonic functions \( Y_{lm} \). These spherical harmonics impart rotational characteristics to the feature vectors, thereby ensuring that the model maintains rotational invariance and facilitates consistent assessment of geometric properties.

\textbf{Tensor Product}: Tensor product is the core operation in SPHNet. It facilitates interactions among irreps linked to distinct order (angular momentum) \( l \) and enhances expressiveness within the model. This operation merges two irreps with order \( l_1 \) and \( l_2 \) to generate a new irrep characterized by order \( l_3 \). The expansion is accomplished by utilizing Clebsch-Gordan coefficients, weighted by \( w_{m_1,m_2} \):

\begin{equation}
    (x^{\ell_1} \otimes y^{\ell_2})^{\ell_3}_{m_3} = \sum_{m_1=-\ell_1}^{\ell_1} \sum_{m_2=-\ell_2}^{\ell_2} w^{\ell_1,\ell_2,\ell_3}_{m_1,m_2} C^{(\ell_3,m_3)}_{(\ell_1,m_1),(\ell_2,m_2)} x^{\ell_1}_{m_1} y^{\ell_2}_{m_2},  
\end{equation}

% \begin{equation}
% h_{l_3,m_3} = h_{l_1,m_1} \otimes h_{l_2,m_2} = \sum_{m_1,m_2} w_{m_1,m_2} C^{l_3,m_3}_{l_1,m_1,l_2,m_2} h_{l_1,m_1} h_{l_2,m_2} .
% \end{equation}

This mathematical approach allows for the amalgamation of complex features derived from simpler ones, effectively capturing the nuanced interactions of angular momentum in the molecular context.


\section{Additional Model Details.}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/model_detail.png}
    \vspace{-5pt} 
    \caption{The components of SPHNet. \textbf{(A)} The Vectorial Node Interaction Block, which uses a long-short range message-passing mechanism. \textbf{(B)} The Expansion block.  \textbf{(C)} The Spherical Node Interaction Block. \textbf{(D)} The Diagonal block in the Pair Construction block. \textbf{(E)} The Non-Diagonal block in the Pair Construction block.}
    \label{fig:model_detail}
\end{figure}

\subsection{RBF}
The RBF here refers to the Radial Basis Functions and is used to construct nonlinear maps of vectors $r$. We used Exponential Bernstein Radial Basis Functions here to process the vector of two atoms, and the mathematical formulation is as below:

\begin{equation}  
    \text{RBF}(r) = \text{F}_{cut}(r, \text{cutoff}) \times \exp\left(\log c + n \cdot (-\alpha r) + v \cdot \log(-\exp(-\alpha r) + 1)\right)  ,
\end{equation}  

and the cutoff function $\text{F}_{cut}$ is to limit the range of action of the RBF, beyond which inputs will be ignored or reduced in impact:

\begin{equation}
    \text{F}_{cut}(x, \text{cutoff}) =   
    \begin{cases}   
    \exp\left(-\frac{x^2}{(\text{cutoff} - x)(\text{cutoff} + x)}\right), & \text{if } x < \text{cutoff}. \\  
    0, & \text{otherwise} . 
    \end{cases}  
\end{equation}



\subsection{Sph Linear}
The Sph Linear used in the SPHNet was implemented through the o3.Linear function in the \texttt{e3nn} \cite{e3nn}. It is an O(3) linear operation that takes an irrep as input and outputs the linear combination of input. Specifically, the feature of each order in the input irreps will be mapped to the feature of a specific order of the output irreps through a learnable matrix. We didn't set the instruction attribution in the \texttt{e3nn}'s \cite{e3nn} function, which resulted in a fully connected instruction and each order in the input and output irreps will have a connection.

\subsection{Vectorial Node Interaction Block}
\label{appendix:featext}

% \begin{equation}
% {x_i^0}', {x_i^1}' = \phi_x \left( {x_i^0}, {x_i^1}, \sum_{j \in \mathcal{N}(i)}m_{ij}^0, \sum_{j \in \mathcal{N}(i)}{m}_{ij}^1 \right).\\
% \end{equation}
% \begin{equation}
% {m_i^0}' = \sum_{j \in \mathcal{N}(i)} \phi_m \left({x_i^0}, {x_j^0}, |\vec{r_{ij}}| \right)
% \end{equation}
% \begin{equation}
% {{m}_i^1}' = \sum_{j \in \mathcal{N}(i)} \phi_m \left({m}_i^1, {x_i^1}, \vec{r}_{ij}\right)
% \end{equation}
As shown in Fig.\ref{fig:model_detail}(A), the Node Vectorial Node Interaction Block receives the atomic numbers $Z$ and 3D coordinates of the molecular system $\vec{r}_{ij}$ as input and extracts the representations $\mathbf{x}_i$ for node $i$. In this block, we adopted the long-short range message-passing mechanism \cite{li2023long} to capture the long-range interaction between each atom and obtain more informative node representations. There are 4 short-range message-passing modules and 4 long-range message-passing modules. Specifically, for the short-range message-passing module, the module passes the neighboring atoms' features to the center atom as defined below:
\begin{equation}
% \label{node_inter}
\hat{\mathbf{x}}_i^{\ell} = F_{short} ( {\mathbf{x}}_i^{\ell} + \sum_{j\in \mathcal{N}(i)} \mathbf{m}_{ij}^{\ell} ),
\end{equation}
where $F_{short}$ is the linear layer, $\mathbf{m}_{ij}$ is the message from atom $j$ to atom $i$, and the $\mathcal{N}(i)$ is the neighboring atoms of atom $i$. For long-range message-passing module, the module passes the neighboring groups' feature to the center atom as defined below:
\begin{equation}
% \label{node_inter}
\hat{\mathbf{x}}_i^{\ell} = F_{long} ( {\mathbf{x}}_i^{\ell} + \sum_{j\in \mathcal{N}(i)} \mathbf{m}_{ij}^{\ell} ),
\end{equation}
where $F_{long}$ is the linear layer, $\mathcal{N}(i)$ is the neighborhood group of atom $i$ on the atom-fragment bipartite graph, and $\mathbf{m}_{ij}$ is the message from neighboring group $j$ to atom $i$.

\subsection{Spherical Node Interaction Block}
As shown in Fig.\ref{fig:model_detail}(C), the Spherical Node Interaction Block takes the node feature $\mathbf{x}_i$ and 3D coordinates of the molecular system $\vec{r}_{ij}$ as input and outputs the new high-order feature through a tensor product operation. Specifically, the input atom feature $\mathbf{x}_i$ and its neighboring atom feature are first fed to the Sparse Pair Gate and obtain the selected important pair set $(\mathbf{x}'_i,(\vec{r_{ij}})')$ and their pair weight $\mathbf{W}^{ij}_p$.
This weight feature is then multiplied with the RBF of $\vec{r_{ij}}$ and gets the final tensor product weight as defined in Equation \ref{equ:w_ij} and \ref{node_inter}. The atom feature $x_i$ is finally doing tensor product with the high-order spherical harmonics projection of the selected $(\vec{r_{ij}})'$, as defined in Equation \ref{equa:SO(3)}. Last, the ascended node feature $\hat{x}_i$ is outputted after normalization and sph linear:

\begin{equation}
\mathbf{x}^l_i = 
\begin{cases}
\text{Layernorm}(\mathbf{x}^l_i), & \text{if } l = 0. \\ 
\mathbf{x}^l_i \times \frac{1}{L_{\text{max}}} \sum_{l=1}^{L_{\text{max}}} \left( \frac{1}{2l + 1} \sum_{m=-l}^{l} {\mathbf{x}^l_{i,m}}^2 \right), & \text{if } l > 0.
\end{cases}
\end{equation}

\begin{equation}
    \mathbf{x}_i = \text{Sphlinear}(\mathbf{x}_i).
\end{equation}

% \begin{equation}
%     \begin{aligned}
%     \hat{x}^{\ell_3}_i =& \sum_{\ell_1,\ell_2} x^{\ell_2}_{i} \otimes^{\ell_3}_{\ell_1,\ell_2} wt_{ij}^{\ell_1,\ell_2,\ell_3} Y^{(\ell_2)} (r'_{ij}) 
%     \end{aligned}
%     \label{equa:SO(3)}
% \end{equation}
% where $Y (\vec{r_{ij}})$ is the high-order spherical harmonics projection of $r'_{ij}$, and $\hat{x}^{\ell_3}_i$ is the ascended node feature.

There are two Spherical Node Interaction Blocks in the SPHNet. The first block increases the maximum order of input irreps from zero to the required order without utilizing the pairs selected by the Sparse Pair Gate. The second block inputs and outputs irreps with the same maximum order, and employs the pairs $(\mathbf{x}'_i,(\vec{r_{ij}})')$ selected by the Sparse Pair Gate.

\subsection{Pair Construction Block}
\label{appendix:pair_cons}
The Hamiltonian matrix contains the relational information of each pair of atoms in the molecular system. The objective of the Pair Construction Block is to extend the model's capacity to consider diagonal node pairs \( \mathbf{f}_{ii} \) and non-diagonal node pairs \( \mathbf{f}_{ij} \) in the Hamiltonian matrix. Therefore, there are two subblocks in the Pair Construction block, the Diagonal block for the diagonal feature and the Non-Diagonal block for the Non-Diagonal feature. There are two Pair Construction blocks in the SPHNet, to ensure that each subblock $h_{ij}$ in the Hamiltonian matrix is covered at least once, the first Pair Construction block does not use the pair selected by Sparse Pair Gate.

\subsubsection{Diagonal Block}
As shown in Fig.\ref{fig:model_detail}D, the Diagonal block first uses two separate sph linear layers to get two different representations from the input node feature, and then uses a self-tensor product operation to obtain \( \mathbf{f}_{ii} \). Here, the combinations in the tensor product are selected by the Sparse Tensor Product Gate:
\begin{equation}
    \mathbf{\hat{x}}_{i_1} = F_{s_1}(\mathbf{\hat{x}}_{i}), \space\space\space
    \mathbf{\hat{x}}_{i_2} = F_{s_2}(\mathbf{\hat{x}}_{i}),
\end{equation}

\begin{equation}
\mathbf{f}_{ii}^{\ell_3} = \sum_{\{\ell_1, \ell_2, \ell_3 \} \in U_c^{\mathrm{TSS}}} \mathbf{W}_{ii}^{(\ell_1, \ell_2, \ell_3)} \times (\mathbf{\hat{x}}_{i_1}^{\ell_1} \otimes^{\ell_3}_{\ell_1,\ell_2}  \mathbf{\hat{x}}_{i_2}^{\ell_2} ),
\end{equation}
where \( \mathbf{x}_i \) is the atom feature from the Spherical Node Interaction Block, and $F_{s_1}$ and $F_{s_2}$ are the sph linear layer. $\mathbf{W}_{ii}$ is the combination weight defined in Equation \ref{eqa:fii}. Last, the diagonal feature $\mathbf{f}_{ii}$ is outputted after normalization and sph linear the same as in the Spherical Node Interaction block.
 

\subsubsection{Non-Diagonal Block}
For the non-diagonal pairs, the module first uses Sparse Pair Gate to select the most valuable pair $\{\mathbf{x'}_i,\mathbf{x'}_j\}$ and their tensor product weight $\mathbf{w}_{ij}$, as defined in Equation \ref{equ:w_ij}. The tensor product weight is then multiplied with the RBF of $\vec{r'_{ij}}$ and gets the final tensor product weight as defined in Equation \ref{equa:SO(3)}. Then, the module calculates the tensor product of \( \mathbf{x'}_i \) and \( \mathbf{x'}_j \), the process is presented in Fig.\ref{fig:model_detail}E. Noted that the combinations in the tensor product are selected by the Sparse Tensor Product Gate. The result is regarded as the desired \( \mathbf{f}_{ij} \), defined in Equation \ref{equa:fij}. Last, the non-diagonal feature $\mathbf{f}_{ij}$ is outputted after normalization and sph linear the same as in the Spherical Node Interaction block.

There are two separate Pair Construction Blocks that receive atom representations from the two SO(3) Convolution Blocks respectively, and the final node pair feature is the addition of these two Pair Construction Blocks' output.

\subsection{Expansion Block}
\label{appendix:expansion}

Once we have gathered the irreps for both diagonal and non-diagonal pairs, the subsequent step involves constructing the complete Hamiltonian matrix. There are many matrix elements in the Hamiltonian matrix, each matrix element represents the interaction between two orbitals. Specifically, the block denoted as $h_{ij}$ captures all interactions between atoms $i$ and $j$. Since atoms have varying numbers of orbitals, the shape of the pair block $h_{ij}$ is different, and must be determined during the construction of the Hamiltonian matrix.

Here, same as the previous work \cite{yu2023efficient}, we introduce an intermediate block $M_{ij}$ containing the full orbitals, from which we can derive $h_{ij}$ by extracting the relevant components based on the specific atom types. For instance, in the MD17 dataset, there are four atoms—H, C, N, and O—each with its own set of orbitals. The full orbital set consists of 1s, 2s, 3s, 2p, 3p, 3d orbitals, where $M \in \mathbb{R}^{14 \times 14}$. When dealing with the hydrogen atom (H), only the 1s, 2s, and 2p orbitals are selected to contribute to the Hamiltonian matrix. By using this strategy, each node’s irrep $\mathbf{f}_{ij}$ can be converted into an intermediate block $M_{ij}$ with a predetermined structure, irrespective of the atom type. This technique is particularly advantageous as it can be easily adapted to various molecules.

To construct the intermediate pair blocks with full orbitals using pair irreducible representations, we apply a tensor expansion operation in conjunction with the filter operation. This expansion is defined by the following relation:
\begin{equation}
\left( \overline{\otimes}_{\ell_o} \mathbf{f}^{\ell_o} \right)_{(\ell_i,\ell_j)}^{(m_i,m_j)} = \sum_{m_o=-\ell_o}^{\ell_o} C_{(\ell_i,m_i),(\ell_j,m_j)}^{(\ell_o,m_o)} \mathbf{f}_{m_o}^{\ell_o},
\end{equation}
where \( C \) denotes the Clebsch-Gordan coefficients, and \( \overline{\otimes} \) symbolizes the tensor expansion which is the converse operation of the tensor product. Specifically, \( x^{\ell_i} \otimes y^{\ell_j} \) can be expressed as a sum over tensor expansions:
\begin{equation}
\mathbf{x}^{\ell_i} \otimes \mathbf{y}^{\ell_j} = \sum_{\ell_3} \mathbf{W}_{l_{i}, l_{j}, l_{o}}\overline{\otimes} \mathbf{f}^{\ell_o},
\end{equation}
subject to the angular momentum coupling constraints \( |\ell_i - \ell_j| \leq \ell_o \leq \ell_i + \ell_j \). The filter then takes the atom types as input, producing a weight for each path $(\ell_{o1}, \ell_{o2}, \ell_{in})$:

\begin{equation}
\mathbf{F}_{ij}^{(\ell_{o1}, \ell_{o2}, \ell_{in})} = f(Z_i, Z_j), \quad \mathbf{F}_{ii}^{(\ell_{o1}, \ell_{o2}, \ell_{in})} = f(Z_i), \tag{20}
\end{equation}

where $Z$ denotes the embedding of the atom types. The intermediate blocks $M$ are generated by the filter, using the node pair irreducible representations as follows:

\begin{equation}
M_{ii}^{(\ell_{o1}, \ell_{o2})} = \sum_{\ell_{in}, c'} \mathbf{F}_{ii}^{(\ell_{o1}, \ell_{o2}, \ell_{in})} \overline{\otimes}\mathbf{f}_{ii}^{(\ell_{in}, c')}, \tag{21}
\end{equation}

\begin{equation}
M_{ij}^{(\ell_{o1}, \ell_{o2})} = \sum_{\ell_{in}, c'} \mathbf{F}_{ij}^{(\ell_{o1}, \ell_{o2}, \ell_{in})} \overline{\otimes}\mathbf{f}_{ij}^{(\ell_{in}, c')}.
\end{equation}

Here, $c'$ indicates the channel index in the input irreducible representations, and $c$ represents the channel index in $M$. For instance, there are nine channels with $(\ell_1, \ell_2) = (0, 0)$ and four channels with $(\ell_1, \ell_2) = (1, 1)$. A bias term is added for the node pair representation when $\ell_{in} = 0$. 

% In the construction of final Hamiltonian blocks that encompass full orbital information using pair irreps, a tensor expansion operation is employed. This expansion is defined by the following relation:
% \begin{equation}
% \left( \overline{\otimes}_{\ell_o} f^{\ell_o} \right)_{(\ell_i,\ell_j)}^{(m_i,m_j)} = \sum_{m_o=-\ell_o}^{\ell_o} C_{(\ell_i,m_i),(\ell_j,m_j)}^{(\ell_o,m_o)} f_{m_o}^{\ell_o},
% \end{equation}
% where \( C \) denotes the Clebsch-Gordan coefficients, and \( \overline{\otimes} \) symbolizes the tensor expansion which is the converse operation of the tensor product. \( x^{\ell_i} \otimes y^{\ell_j} \) can be expressed as a sum over tensor expansions:
% \begin{equation}
% x^{\ell_i} \otimes y^{\ell_j} = \sum_{\ell_3} W_{l_{i}, l_{j}, l_{o}}\overline{\otimes} f^{\ell_o},
% \end{equation}
% subject to the angular momentum coupling constraints \( |\ell_i - \ell_j| \leq \ell_o \leq \ell_i + \ell_j \). Using this expansion block we can obtain the sub-blocks of each atom pair $h_{ij}$ in the Hamiltonian matrix.  % and finally build the completed matrix.

Last, we derive $\mathbf{h}_{ij}$ by extracting the relevant components based on the specific atom types and use these matrix elements to construct the complete Hamiltonian matrix. As shown in Fig.\ref{fig:model_detail}(B), for the Non-Diagonal block in the Hamiltonian matrix, we only construct the node pair
features $\mathbf{f}_{ij}$ from the upper triangular part of the Hamiltonian matrix. The lower triangular part is then obtained by symmetrizing the sub-blocks from the upper triangular part. 

\section{Additional Related works}

\subsection{SE(3) equivariant neural network}
The SE(3) equivariant neural network is one of the most used models in the field of AI for chemistry\cite{fuchs2020se,du2022se,musaelian2023learning,liao2022equiformer,liao2023equiformerv2,batzner20223,batzner2023advancing}. The fundamental feature of SE(3) equivariant neural network is that all the features and operations in the model are SE(3) equivariant. This is achieved by using irreducible representations (irreps) and functions of geometry built from spherical harmonics. In the network, the equivariant features propagate through each layer and interact with other features by equivariant operations, and finally obtain the desired SE(3) equivariant quantity.

SE(3)-Transformer\cite{fuchs2020se} proposed a novel self-attention mechanism for 3D point clouds and graphs that ensures robustness through continuous 3D roto-translation equivariance, which is widely used in the field of quantum chemistry. The Equiformer\cite{liao2022equiformer} introduced a novel graph neural network leveraging SE(3) Transformer architecture based on irreducible representations to predict molecule property. It demonstrated strong empirical results by incorporating tensor products and a new attention mechanism called equivariant graph attention. The EquiformerV2\cite{liao2023equiformerv2} is an improved version of Equiformer, which scales effectively to higher-order representations by replacing SO(3) convolutions with efficient eSCN convolutions\cite{passaro2023reducing}, and outperforming the traditional network such as GemNet\cite{gasteiger2022gemnet} and Torchmd-Net \cite{tholke2022torchmd} in molecular energy and force prediction and other downstream tasks. Allegro\cite{musaelian2023learning} used a strictly local, equivariant model to represent many-body potential using iterated tensor products of learned representations without atom-centered message passing. It demonstrates remarkable generalization to out-of-distribution data and accurately recovers structural and kinetic properties of amorphous electrolytes in agreement with ab initio simulations.

These SE(3) equivariant neural networks greatly improve the performance of Artificial Intelligent in the field of quantum chemistry. However, the computational complexity of tensor product operation greatly reduces the efficiency of these SE(3) equivariant models. The eSCN\cite{passaro2023reducing} presents an efficient method to perform SO(3) equivariant convolutions. It reduces the computational complexity by aligning node embeddings' primary axis with edge vectors, transforming the SO(3) convolutions into mathematically equivalent SO(2) convolutions, which decreases complexity from $O(L^6)$ to $O(L^3)$. 

\subsection{Hamiltonian matrix prediction}
Predicting the Hamiltonian matrix (also known as electronic wavefunctions) is gradually gaining more and more attention because of the wide range of potential application scenarios of the Hamiltonian matrix. There are more and more works trying to solve the problem of predicting the Hamiltonian matrix using deep learning techniques.

The SE(3) equivariant neural network is one of the most used models in the field of Hamiltonian matrix prediction\cite{unke2021se,yu2023efficient,gong2023general,atz2021geometric}. The PhiSNet \cite{unke2021se} is the first method that primarily focused on Hamiltonian matrix prediction. It leverages SE(3)-equivariant operations through its whole architecture to predict molecular wavefunctions and electronic densities and can reconstruct wavefunctions with high accuracy. The main problem for PhiSNet is its inefficiency due to the large amount of tensor product operations. To solve this problem, QHNet \cite{yu2023efficient} proposed a model with careful design to greatly reduce the number of tensor products and improve the efficiency of Hamiltonian prediction. The DeepH-E3 \cite{gong2023general} is an E(3)-equivariant model that preserves Euclidean symmetry even with spin-orbit coupling. The method allows for efficient and accurate electronic structure calculations of large-scale materials by learning from small-sized DFT data, significantly reducing computational costs.

There are also other works that try to enhance the prediction ability from different aspects. DeepH \cite{li2022deep} introduces a local coordinate system defined for each edge according to its local chemical environment to handle the gauge (or rotation) covariance of the DFT Hamiltonian matrix. This allows the Hamiltonian matrix blocks to be invariant under rotation when transformed into the local coordinate system. Self-consistency training\cite{zhang2024selfconsistency} leverages the self-consistency principle of density functional theory (DFT) to train a model without requiring the labeled Hamiltonian matrice. This enhances the generalization and efficiency and reduces reliance on costly DFT calculations for supervision.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
