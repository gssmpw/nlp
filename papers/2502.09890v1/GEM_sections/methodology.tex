\section{Method}
\label{sec:method}

We aim to model a \( \Gi \) distribution \( q(x) \) using diffusion models. In practice, we only have access to a finite set of \(N\) samples denoted as \( \{ x_0^i \}_{i=1}^N \). Let the empirical distribution be defined as \( \hat{q}(x) := \frac{1}{N} \sum_{i=1}^N \delta(x - x_0^i) \), where \( \delta \) is the Dirac delta function. Unfortunately, this empirical distribution \( \hat{q}(x) \) is not necessarily \( \Gi \), as the finite dataset \( \{x_0^i\} \) may not be closed under group action \(g \in G\). Consequently, fitting \( p_\theta(x) \) to \(\hat{q}(x)\) without enforcing equivariant constraints on \( \phi_\theta \) might yield a model that fails to generalize.


% \vinh{I should write a paragraph about group symmetrization operations and why do we need it to understand our paper. }

\mathias{Is anyone using the unconstrained minimizer? Or is this just some theoretical consideration? Generally I am missing a story here: what is the problem with existing methods? what do they do? what do we do differently? What is the insight of our method? How is it better than existing methods?}


\vinh{Minimizing the unconstrained loss does not yield an equivariant minimizer. Non-equivariant models, such as those used in the MCF paper, rely on this loss. Their success, however, is largely attributed to the fact that the training and test sets share a canonical coordinate system (as stated in their work).}  

\vinh{Next, I discussed the common approach to ensuring that the modeled distribution is invariant. This typically involves enforcing the denoiser to be an equivariant function, essentially framing the problem as a constrained optimization task.}  

\vinh{This is the standard method for achieving equivariance. However, it often leads to optimization challenges. One theoretical study~\citep{nonuniversalityequi} has highlighted the difficulty of training equivariant models with SGD. Additionally, I suspect that this loss suffers from high variance.}  

\vinh{To address this, I aim to explicitly show the minimizer of the constrained loss and demonstrate that our model, without enforcing this constraint, can achieve the same theoretical minimizer. Furthermore, our loss exhibits lower variance.}  

\vinh{In summary, using the unconstrained loss results in a non-equivariant minimizer, while the constrained loss ensures equivariance but comes with optimization difficulties. Our proposed unconstrained loss retains the same minimizer as the constrained one while being easier to optimize.}  


\paragraph{Analyzing the Unconstrained Minimizer.} The impact of unconstrained fitting becomes apparent when examining the minimizer of the empirical diffusion risk at time \(t\):
\begin{align}
    \mathcal{L}_t(\phi) &= \mathbb{E}_{(x_0, x_t) \sim \hat{q}(x_0, x_t)} \left[ \omega(t) \| \phi(x_t, t) - x_0 \|^2 \right] \nonumber \\
    &= \omega(t) \mathbb{E}_{x_t \sim \hat{q}_t(x_t)} \mathbb{E}_{x_0 \sim \hat{q}_t(x_0 \mid x_t)} \left[ \| \phi(x_t, t) - x_0 \|^2 \right] ,
    \label{eqn:loss_t}
\end{align}
The minimizer of the empirical risk function is the point where its derivative with respect to \( \phi \) becomes zero. For each \( x_t \), the gradient is given by:  
\begin{align}
    \nabla_\phi \left[ \mathbb{E}_{x_0 \sim \hat{q}_t(x_0 \mid x_t)} \left[ \| \phi(x_t, t) - x_0 \|^2 \right] \right] &= 2 \mathbb{E}_{x_0 \sim \hat{q}_t(x_0 \mid x_t)} \left[  \phi(x_t, t) - x_0  \right] \nonumber \\
    &= 2 \sum_{i=1}^N (\phi(x_t, t) - x_0^i) \hat{q}_t(x_0^i \mid x_t) . \nonumber
\end{align}  
Thus, the minimizer \( \phi^* \) of \eqref{eqn:loss_t} satisfies:  
\begin{equation}
    \phi^*(x_t, t) = \sum_{i=1}^N x_0^i \hat{q}_t(x_0^i \mid x_t) = \mathbb{E}[x_0 \mid x_t] ,
\end{equation}
which is not necessarily \( \Ge \) (see~\cref{sub_app:mini} for the proof). 

\paragraph{Constrained Optimization Challenges.} On the other hand, enforcing \( \phi \) to be \( \Ge \) guarantees that the learned distribution \( p_\theta(x) \) is \( \Gi \)~\citepcolor{Theoequi}. In this case, the optimization problem becomes:
\begin{align} 
\label{eqn:constraint_problem_}
&\text{Minimize} \quad \mathcal{L}_t(\phi)  \\
&\text{Subject to} \quad \phi(g \circ x_t, t) = g \circ \phi(x_t, t) \quad \forall x_t \in \Omega \text{ and } \forall g \in G . 
\end{align}
This formalization of the optimization problem, where equivariance constraints are explicitly imposed, has been widely adopted by the research community~\citepcolor{yim2023se, hoogeboom2022equivariant}.

Let \( \phi^*_G \) be the minimizer of this optimization problem. The function \( \phi^*_G \) is \( \Ge \) and equivalent to \( \phi^* \) under a weighted symmetrization transformation~\citepcolor{Theoequi}:
\begin{align}
    &\phi^*_G(x_t, t) = \sum_{i=1}^N S_G(x_0^i, x_t, t) q_t(x_0^i \mid x_t) , \\
    \text{where} \quad &S_G(x_0^i, x_t, t) := \int_G (g \circ x_0^i) \hat{q}^G_t(g \circ x_0^i \mid x_t) \mu_G(g) \mathrm{d} g,\label{eqn:symmetrized_function} \\
    \text{and} \quad &\hat{q}_t^G(g \circ x_0^i \mid x_t) := \frac{q_t(x_t \mid g \circ x_0^i)\hat{q}^G(g 
\circ x_0)}{\hat{q}^G_t(x_t)} = \frac{q_t(x_t \mid g \circ x_0^i)\hat{q}^G(x_0)}{\hat{q}^G_t(x_t)}.
\end{align}
Unfortunately, the optimization problem in~\eqref{eqn:constraint_problem_} is a \textbf{non-trivially constrained optimization problem}, where the constraints significantly influence the solution space, adding complexity to the problem. \mathias{First you write that this formalization of the optimization problem is widely adopted, and then you write that the optimization problem is non-trivial, hard to solve, etc. What are existing approaches doing to address this? Not clear to me. I also don't understand what equations (6)-(8) are supposed to tell me as a reader. make this more explicit. it seems the unconstrained version is equivalent to the constrained one under some transformation. But then, why do we need the constrained objective?} Moreover, since diffusion models are often trained using Monte Carlo estimation of the loss function, we argue that this approach for~\eqref{eqn:constraint_problem_} suffers from high variance. \mathias{Again, I think it is important to concretely contrast what state of the art models currently do, and how what we do is different, better.}

\paragraph{Proposed Loss Function.} To overcome these challenges, we propose a new loss function:
\begin{align} 
\mathcal{L}^G_t(\phi) = \mathbb{E}_{(x_0, x_t) \sim \hat{q}(x_0, x_t)} \left[ \omega(t) \| \phi(x_t, t) - S_G(x_0, x_t, t) \|^2 \right], 
\label{eqn:proposed_loss} 
\end{align}
where \( S_G(x_0, x_t, t) \) symmetrizes \(x_0\) over \(G\)~\eqref{eqn:symmetrized_function}. Unlike~\eqref{eqn:loss_t}, which directly uses \(x_0\), we replace it with its symmetrized counterpart \(S_G(x_0, x_t, t)\). This substitution inherently enforces equivariance without the need for explicit constraints, as \(S_G\) integrates the group action, aligning the objective with the \(\Gi\) distribution. Critically, the proposed loss retains the same minimizer \( \phi^*_G \) as the constrained problem~\eqref{eqn:constraint_problem_}. To formally establish this property, we present the following theorem, with proof provided in~\cref{sub_app:theo_1}:
\begin{tcolorbox}[title=Equivariant Minimizer of \(\mcL_t^G (\phi)\)]
\begin{theorem}
Let \( \bar{\phi}^*_G \) be the minimizer of the loss function in~\eqref{eqn:proposed_loss}. Then \( \bar{\phi}^*_G(x_t, t) \) is \( \Ge \) and 
\( \bar{\phi}^*_G(x_t, t) = \phi^*_G (x_t, t) = \sum_{i=1}^N S_G(x_0^i, x_t, t). \)
\label{the:theo_1}
\end{theorem}
\end{tcolorbox}
While \cref{the:theo_1} guarantees that the proposed loss inherently aligns with the constrained problem's minimizer, practical optimization introduces potential deviations. To ensure robustness, we therefore explicitly enforce \(\Ge\) on \(\phi\). 


\paragraph{Practical Implementation.}  
To compute \( S_G(x_0, x_t, t) \) for a sampled pair \((x_0, x_t) \sim \hat{q}_t(x_0, x_t)\), we start from its definition as a group-weighted averaged posterior expectation (see the derivation in~\cref{sub_app:SFull}):
\begin{equation}
    S_G(x_0^i, x_t, t) = C \int_G (g \circ x_0^i) \exp{\frac{-\| x_t - \alpha_t (g \circ x_0^i) \|^2}{2\sigma_t^2}} \mu_G(g) \mathrm{d}g,
    \label{eqn:final_form}
\end{equation}
where \( C \) is a normalizing constant independent of \( g \) (i.e., \( C \int_G \exp{\frac{-\| x_t - \alpha_t (g \circ x_0^i) \|^2}{2\sigma_t^2}} \mu_G(g) \mathrm{d}g = 1 \)).  

We approximate the integral using Monte Carlo sampling, as detailed in Algorithm~\ref{alg:group_symmetrization}.  

\begin{algorithm}[H]
    \caption{Monte Carlo Approximation of \( S_G(x_0^i, x_t, t) \)}
    \label{alg:group_symmetrization}
    \begin{algorithmic}[1]
        \STATE Draw \( K \) group elements \( g_k \sim \mu_G \)
        \FOR{each \( g_k \)}
            \STATE Compute transformed sample: \( x_k \leftarrow g_k \circ x_0^i \)
            \STATE Compute logit: \( R_k \leftarrow \frac{-\| x_t - \alpha_t x_k \|^2}{2\sigma_t^2} \)
        \ENDFOR
        \STATE Apply softmax over logits: \( w_k \leftarrow \frac{\exp(R_k)}{\sum_{j=1}^K \exp(R_j)} \)
        \STATE Compute approximation: \( S_G(x_0^i, x_t, t) \leftarrow \sum_{k=1}^K w_k x_k \)
    \end{algorithmic}
\end{algorithm}



The softmax normalization implicitly accounts for the constant \(C\), ensuring the weights \(w_k\) reflect the likelihood of each transformed sample \(g_k \circ x_0^i\) given \(x_t\).



A key advantage of the proposed loss function is its significantly lower gradient estimator variance compared to the constrained optimization objective in Eq.~\eqref{eqn:constraint_problem_}. This stems from the symmetrization in \(S_G(x_0, x_t, t)\), which averages out noise across group transformations. Formally:
\begin{tcolorbox}[title=Variance Reduction of the Proposed Loss]
\begin{theorem}
Let \(\mcL_t(\phi)\) and \(\mcL_t^G(\phi)\) be the Monte Carlo estimators of Eq.~\eqref{eqn:constraint_problem_} and Eq.~\eqref{eqn:proposed_loss}, respectively. Then:
\begin{equation}
    \text{\emph{Var}}(\nabla_\phi(\mcL_t(\phi))) \geq \text{\emph{Var}}(\nabla_\phi\mcL_t^G(\phi))
\end{equation}
with equality only if \(G\) acts trivially on \(x_0\). 
\label{the:theo_2}
\end{theorem}
\end{tcolorbox}
A full proof is provided in ~\cref{sub_app:theo_2}.


