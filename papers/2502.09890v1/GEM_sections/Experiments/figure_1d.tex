% \begin{figure}[H]
%     \centering
%     % Top-left figure
%     \begin{subfigure}[b]{0.33\textwidth} % Reduced width
%         \centering
%         \includegraphics[width=\textwidth]{GEM_figs/Toy_Exps/final_NEW_training_loss_nolog_comparison_non_equi.pdf}
%         \caption{Training Loss}
%         \label{fig:training_loss_nolog}
%     \end{subfigure}
%     % \hfill
%     % Top-right figure
%     \begin{subfigure}[b]{0.33\textwidth} % Reduced width
%         \centering
%         \includegraphics[width=\textwidth]{GEM_figs/Toy_Exps/final_NEW_log_validation_error_comparison_non_equi.pdf}
%         \caption{RMSD - lower is better (log scale)}
%         \label{fig:validation_error_log}
%     \end{subfigure}


%     \caption{\textbf{1D toy experiment.} Comparison of training loss and evaluation RMSD with reflection group equivariance. (a) shows the training loss over epochs, while (b) presents the evaluation RMSD for models trained with different numbers of epochs.}
%     \label{fig:1D-toy}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     % First figure
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{GEM_figs/Toy_Exps/final_NEW_training_loss_nolog_comparison_non_equi.pdf}
%         \caption{Training Loss}
%         \label{fig:training_loss_nolog}
%     \end{subfigure}
%     % Second figure
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{GEM_figs/Toy_Exps/final_NEW_log_validation_error_comparison_non_equi.pdf}
%         \caption{RMSD (log scale)}
%         \label{fig:validation_error_log}
%     \end{subfigure}
%     % Third figure
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{GEM_figs/Toy_Exps/generated_samples_plot.pdf}
%         \caption{Generated sample distribution}
%         \label{fig:validation_error_linear}
%     \end{subfigure}

%     \caption{\textbf{1D toy experiment.} Comparison of training loss and evaluation RMSD with reflection group equivariance. (a) shows the training loss over epochs, (b) presents the evaluation RMSD on a log scale, and (c) shows the distribution of generated samples.}
%     \label{fig:1D-toy}
% \end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.32\textwidth}  % 1/3 of text width
        \includegraphics[width=\textwidth]{GEM_figs/Toy_Exps/final_NEW_training_loss_nolog_comparison_non_equi.pdf}
        \caption{Training Loss.}
        \label{fig:training_loss_nolog}
    \end{minipage}
    \hfill
    \begin{minipage}{0.65\textwidth}  % 2/3 of text width
    
        \centering
        \small
        \begin{tabular}{lcc}\toprule
Models & RMSD ($\times 10^{-3}$) & W2 Distance ($\times 10^{-2}$) \\\midrule
NSD+Enet &8.3923 $\pm$ 1.8076 &0.0139 $\pm$ 0.0032 \\
\rowcolor[HTML]{EAEAEA}NSD+Enet (Ours) &\textbf{2.0486 $\pm$ 1.4348} &\textbf{0.0017 $\pm$ 0.0005} \\
\bottomrule
\end{tabular}
        \captionof{table}{1D Toy Experiment Results. We report RMSD between the generated samples and their closest target in \(\{-1, 1\}\). We also report the W2 Distance between the generated samples and the ground-truth distribution.} 
        \label{tab:1d_toy}
    \end{minipage}
\end{figure}