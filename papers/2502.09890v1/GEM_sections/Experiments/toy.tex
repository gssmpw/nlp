
\subsection{Toy Experiment}

We evaluate our loss using the most common design of equivariant diffusion models—\emph{Non-Symmetrized Diffusion} with an Equivariant Denoiser (NSD+Enet)—on two toy examples. Each example consists of a single data sample and illustrates a scenario where the empirical distribution is highly asymmetric.

\paragraph{Reflection group in 1D.} In this toy experiment, we create a dataset containing a single 1D sample with value \( 1 \). The equivariant group is the reflection group (i.e., \( g \cdot x = \pm x \)). We train a denoiser \( D_\theta(x_t, t) = f_\theta(x_t, t) - f_\theta(-x_t, t) \), which is a \emph{\( G \)-equivariant} function. The function \( f_\theta \) is a 3-layer multi-layer perceptron (MLP), with hidden sizes of 2, 10, and 5, and Tanh activation functions. 

We train the models with a batch size of 100, a learning rate of 0.001, and a total of 100,000 iterations. During training, we log the loss and periodically evaluate the model. After training, we generate 100,000 samples using 100 DDIM~\citepcolor{song2020denoising} steps and compute the root mean square deviation (RMSD) between each sample and its closest target value in \(\{-1, 1\}\). Additionally, we calculate the Wasserstein-2 (W2) distance between the generated samples and the target distribution. We repeat the evaluation three times with different random seeds and report the mean and standard deviation in~\cref{tab:1d_toy}.


\input{GEM_sections/Experiments/figure_1d}

Our results demonstrate that our loss function improves both training stability and model performance across different settings. \Cref{fig:training_loss_nolog} shows that our loss function stabilizes training, which leads to faster and more stable convergence. In contrast, training without our loss function exhibits high variance and fails to converge properly, as seen in fluctuating training loss curves.

Our model achieves significantly better results than the baseline. Specifically, our loss achieves 4 times better RMSD and 8 times better W2 Distance than the original diffusion loss. This demonstrates that our approach generates samples much closer to the target values while aligning more accurately with the ground truth distribution. 

\paragraph{Rotation and Permutation Groups in 2D.} 
In this experiment, the model is tasked with generating a point cloud of \( 4 \) points arranged in a straight line, ensuring equivariance to both rotation and permutation groups. Similar to the 1D experiment, the training dataset consists of a single sample. This sample comprises \( 4 \) points in 2D space with coordinates \((-1.0, 0.0)\), \((-0.5, 0.0)\), \((0.5, 0.0)\), and \((1.0, 0.0)\), respectively. We use EGNN~\citepcolor{satorras2021n} as the denoiser architecture, constructing a fully connected graph from the \( 4 \) points and using their coordinates as node features. 
% For the non-equivariant denoiser, we employ an MLP, utilizing the coordinates as node features. 


\input{GEM_sections/Experiments/figure_2d}

% The results in ~\cref{fig:2D-toy} demonstrate the effectiveness of our proposed loss function in improving model performance. Similar to the observation in the 1D toy experiment, without our loss, the training loss fluctuates and fails to converge, while the model with our loss converges quickly and stably, as shown in ~\cref{fig:training_loss_2D}. This improvement in training stability directly contributes to the better quality of the generated samples, which can be seen in ~\cref{fig:2D-qualitative}, where our samples are positioned closer to the circle. The improvement can also be measured quantitatively by the RMSD. Our method consistently reduces RMSD, with values approximately half of those observed without our loss, regardless of the sampling steps.

\cref{fig:2D-toy} demonstrates the effectiveness of our proposed loss function when applied to NSD+Enet. Similar to the 1D experiment, \cref{fig:training_loss_2D} shows that our loss function results in a significantly more stable training loss curve compared to the original loss function. \cref{fig:2D-rmsd} 
shows that using our loss function leads to much better sample quality and requires fewer sampling steps to achieve high-quality samples. For instance, NSD+Enet (Ours) with 15 sampling steps achieves an RMSD of around 0.15, outperforming NSD+Enet with 50 sampling steps, which results in an RMSD of 0.29.
