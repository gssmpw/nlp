\section{Related Work}

\paragraph{Equivariant Neural Networks.}  
Equivariant neural networks have attracted significant attention for tasks involving structured data, such as computer vision~\citepcolor{equiconv, harmonicnet}, 3D modeling~\citepcolor{tensorfieldnet, e3transformer, vectorneuron}, quantum mechanics and quantum field theory~\citepcolor{quantumfield, schrodinger}, and biomolecular design~\citepcolor{bioref, scaledeepequi}. These networks exploit group symmetries to ensure consistent outputs under transformations like rotations, translations, and permutations, which are commonly encountered in many scientific domains. By incorporating symmetric inductive biases, equivariant networks enhance model generalization and reduce data requirements by naturally encoding symmetry constraints. However, challenges remain, such as high computational complexity~\citepcolor{efficientequi} and difficulties in learning effectively with stochastic gradient descent (SGD)~\citepcolor{nonuniversalityequi}.

\paragraph{Equivariance and Diffusion Models.}  
Diffusion models have become a prominent class of generative models, particularly for tasks involving complex data distributions~\citepcolor{song2020denoising, ho2020denoising, karras2022elucidating}. These models transform data into noise through a forward process and learn to reverse this transformation using a neural network-based denoising function, \(\phi_\theta\). Incorporating equivariance into diffusion models can significantly improve their performance on data with inherent symmetries, enabling more efficient learning and enhancing generalization to unseen transformations~\citepcolor{hoogeboom2022equivariant, geodiff, bose2023se}. To enforce equivariance, it is common to design the denoiser network \(\phi_\theta\) to be equivariant under a group \(\Ge\). While this approach allows the learned distribution to converge to the underlying invariant distribution \(\Gi\)~\citepcolor{Theoequi}, it inherits the challenges associated with equivariant network learning. Additionally, we show in this work that the Monte Carlo estimation of the loss function, following this design, suffers from high variance, which complicates the optimization process.

% \paragraph{Non-equivariant models.} \citecolor{sareen2025symmetry} learn the canonicalization that is easy for the non-equivariant model to learn.  \citecolor{Swallow} learn a transformer model to perform molecular conformation prediction. 


\section{Theoretical Proofs and Derivations}

\subsection{Full derivation of the group-weighted averaged posterior expectation}
\label{sub_app:SFull}

To compute \( S_G(x_0, x_t, t) \) for a sampled pair \((x_0, x_t) \sim \hat{q}_t(x_0, x_t)\), we begin with its definition as a group-weighted averaged posterior expectation:
\begin{align}
    S_G(x_0^i, x_t, t) &:= \int_G (g \circ x_0^i) \hat{q}^G_t(g \circ x_0^i \mid x_t) \mu_G(g) \mathrm{d} g , \\
    &= \int_G (g \circ x_0^i) \hat{q}_t(x_t \mid g \circ x_0^i) \frac{\hat{q}^G(g \circ x_0^i)}{\hat{q}^G_t(x_t)} \mu_G(g) \mathrm{d} g \\
    &= \frac{\hat{q}^G(x_0^i)}{\hat{q}^G_t(x_t)} \int_G (g \circ x_0^i) \hat{q}_t(x_t \mid g \circ x_0^i)  \mu_G(g) \mathrm{d} g \\
    &= \frac{\hat{q}^G(x_0^i)}{\hat{q}^G_t(x_t)} \frac{1}{(2\pi\sigma_t^2)^{d/2}} \int_G (g \circ x_0^i) \exp{\frac{-\| x_t - \alpha_t (g \circ x_0^i) \|^2}{2\sigma_t^2}} \mu_G(g) \mathrm{d}g \\
    &= C \int_G (g \circ x_0^i) \exp{\frac{-\| x_t - \alpha_t (g \circ x_0^i) \|^2}{2\sigma_t^2}} \mu_G(g) \mathrm{d}g .
\end{align}
Here, \(\hat{q}^G(x) = \int_G \hat{q}(g \circ x)\mathrm{d}g \) is the symmetrized empirical distribution \(\hat{q}(x)\), and \(C\) is a normalizing constant independent of \(g\). Critically, we avoid computing \(\hat{q}^G(x)\) or \(\hat{q}_t(x_t)\), instead, we absorb them to the normalizing constant \(C\), which cancels out during normalization

\subsection{The minimizer of the empirical unconstrained diffusion risk function is not guaranteed to be G-equivariant.}
\label{sub_app:mini}

\begin{proof}
We show that \(\phi^*(x_t, t)\) is not guaranteed to be equivariant by providing a counterexample.

\paragraph{Counterexample: Translation in 1D}


\begin{enumerate}
    \item \textbf{Data}: Two points \(x_0^1 = 0\) and \(x_0^2 = 1\), with uniform empirical distribution \(\hat{q}(x_0^i) = 0.5\).
    \item \textbf{Group action}: Translation by \(a = 1\), i.e., \(g \circ x = x + 1\).
    \item \textbf{Diffusion kernel}: \(\hat{q}_t(x_t \mid x_0) = \mathcal{N}(x_t; \alpha_t x_0, \sigma_t^2)\).
\end{enumerate}

First, rewrite the minimizer:
\begin{align}
    \phi^*(x_t, t) &= \sum_{i=1}^N x_0^i \hat{q}_t(x_0^i \mid x_t) \\
    &= \sum_{i=1}^N x_0^i \hat{q}_t(x_t \mid x_0^i) \frac{\hat{q}(x_0^i)}{\hat{q}_t(x_t)} \\
    &= \frac{1}{\hat{q}_t(x_t)} \sum_{i=1}^N x_0^i \hat{q}_t(x_t \mid x_0^i) \hat{q}(x_0^i) .
\end{align}
Substituting \(x_0^1 = 0\) and \(x_0^2 = 1\):
\begin{align}
    \phi^*(x_t, t) &= \frac{1}{\hat{q}_t(x_t)} \left( 0 \cdot \mathcal{N}(x_t; 0, \sigma_t^2) \cdot 0.5 + 1 \cdot \mathcal{N}(x_t; \alpha_t, \sigma_t^2) \cdot 0.5 \right) \\
    &= \frac{0.5 \cdot \mathcal{N}(x_t; \alpha_t, \sigma_t^2)}{\hat{q}_t(x_t)} \\
    &= \frac{0.5 \cdot \mathcal{N}(x_t; \alpha_t, \sigma_t^2)}{\mathcal{N}(x_t; 0, \sigma_t^2) \cdot 0.5 + \mathcal{N}(x_t; \alpha_t, \sigma_t^2) \cdot 0.5} \\
    &= \frac{\mathcal{N}(x_t; \alpha_t, \sigma_t^2)}{\mathcal{N}(x_t; 0, \sigma_t^2) + \mathcal{N}(x_t; \alpha_t, \sigma_t^2)} .
\end{align}

Now, compute \(\phi^*(g \circ x_t, t)\). Apply \(g \circ x_t = x_t + 1\):
\begin{equation}
    \phi^*(g \circ x_t, t) = \frac{\mathcal{N}(x_t + 1; \alpha_t, \sigma_t^2)}{\mathcal{N}(x_t + 1; 0, \sigma_t^2) + \mathcal{N}(x_t + 1; \alpha_t, \sigma_t^2)} < 1 .
\end{equation}
Next, compute \(g \circ \phi^*(x_t, t)\):
\begin{equation}
    g \circ \phi^*(x_t, t) = \frac{\mathcal{N}(x_t; \alpha_t, \sigma_t^2)}{\mathcal{N}(x_t; 0, \sigma_t^2) + \mathcal{N}(x_t; \alpha_t, \sigma_t^2)} + 1 > 1 .
\end{equation}

Thus, \(\phi^*(g \circ x_t, t) < g \circ \phi^*(x_t, t)\) (since \(\phi^*(g \circ x_t, t) < 1\) and \(g \circ \phi^*(x_t, t) > 1\)). Consequently,  
\[
\phi^*(g \circ x_t, t) \neq g \circ \phi^*(x_t, t).
\]
This completes the counterexample, showing that \(\phi^*(x_t, t)\) is not necessarily equivariant.
\end{proof}

\subsection{Proof of~\cref{the:theo_1}}
\label{sub_app:theo_1}

\begingroup
\setcounter{theorem}{0} % Set to the correct theorem number
\renewcommand{\thetheorem}{\ref{the:theo_1}} % Use the existing theorem label
\begin{tcolorbox}[title=Equivariant Minimizer of \(\mcL_t^G (\phi)\)]
\begin{theorem}
Let \( \bar{\phi}^*_G \) be the minimizer of~\eqref{eqn:proposed_loss}. Then \( \bar{\phi}^*_G(x_t, t) \) is \( \Ge \) and:
\[ \bar{\phi}^*_G(x_t, t) = \phi^*_G (x_t, t) = \sum_{i=1}^N S_G(x_0^i, x_t, t). \]
\end{theorem}
\end{tcolorbox}
\endgroup

\begin{proof}
We rewrite the~\cref{eqn:proposed_loss}:
\begin{align}
    \mathcal{L}^G_t(\phi) &= \mathbb{E}_{(x_0, x_t) \sim \hat{q}(x_0, x_t)} \left[ \omega(t) \| \phi(x_t, t) - S_G(x_0, x_t, t) \|^2 \right], \\
    &= \omega(t)\mathbb{E}_{x_t \sim \hat{q}(x_t)}\mathbb{E}_{x_0 \sim \hat{q}(x_0 \mid x_t)} \left[ \| \phi(x_t, t) - S_G(x_0, x_t, t) \|^2 \right],
\end{align}
We first compute the following derivative:
\begin{align}
    \nabla_\phi \left[ \mathbb{E}_{\hat{q}_t(x_0 \mid x_t)} \left[ \| \phi(x_t, t) - S_G(x_0, x_t, t) \|^2 \right] \right] &= 2 \mathbb{E}_{x_0 \sim \hat{q}_t(x_0 \mid x_t)} \left[\phi(x_t, t) - S_G(x_0, x_t, t)  \right] \\
    &= 2 \sum_{i=1}^N (\phi(x_t, t) - S_G(x_0^i, x_t, t)) \hat{q}_t(x_0^i \mid x_t).
\end{align}
Thus, the minimizer \( \bar{\phi}^*_G \) of \(\mcL_t^G(\phi)\) satisfies:
\begin{equation}
    \phi_G^*(x_t, t) = \sum_{i=1}^N S_G(x_0^i, x_t, t) \hat{q}_t(x_0^i \mid x_t). 
\end{equation}

% We will first prove that \(S_G(x_0, x_t, t)\) is a \(\Ge\) function w.r.t \(x_t\), i.e., \(S_G(x_0, g' \circ x_t, t) = g' \circ S_G(x_0, x_t, t)\) for all \(x_t, t\) and \(g' \in G\). 



% Substituting \( S_G(x_0^i, x_t, t) := \int_G (g \circ x_0^i) \hat{q}_t(g \circ x_0^i \mid x_t) \mu_G(g) \mathrm{d} g \):
% \begin{align}
%     \phi_G^*(x_t, t) = \sum_{i=1}^N \int_G (g \circ x_0^i) \hat{q}_t(g \circ x_0^i \mid x_t) \mu_G(g) \mathrm{d} g \hat{q}_t(x_0^i \mid x_t).
% \end{align}


\end{proof}


\subsection{Proof of~\cref{the:theo_2}}
\label{sub_app:theo_2}

\begin{tcolorbox}[title=Variance Reduction of the Proposed Loss]
\begin{theorem}
Let \(\mcL_t(\phi)\) and \(\mcL_t^G(\phi)\) be the Monte Carlo estimators of Eq.~\eqref{eqn:constraint_problem_} and Eq.~\eqref{eqn:proposed_loss}, respectively. Then:
\begin{equation}
    \text{\emph{Var}}(\nabla_\phi(\mcL_t(\phi))) \geq \text{\emph{Var}}(\nabla_\phi\mcL_t^G(\phi))
\end{equation}
with equality only if \(G\) acts trivially on \(x_0\).
\end{theorem}
\end{tcolorbox}


