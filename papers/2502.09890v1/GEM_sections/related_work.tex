\section{Related Work}

\paragraph{Equivariant neural networks.} Equivariant neural networks have been successfully applied in fields such as image processing~\citepcolor{}, 3D data processing~\citepcolor{}, graph analysis~\citepcolor{}, physical simulations~\citepcolor{}, and AI for science applications such as molecular modeling and computational chemistry~\citepcolor{}, leading to improved generalization and sample efficiency. Despite their benefits, existing techniques to build equivariant neural networks face several challenges. \vinh{list the challenges.}

\paragraph{Equivariance and Diffusion Models.} Diffusion models have emerged as a powerful class of generative models, achieving remarkable success in generating high-fidelity samples across various domains~\citepcolor{ho2020denoising, song2020score, karras2022elucidating}. These models learn data distributions by reversing a diffusion process that incrementally adds Gaussian noise to the data. Incorporating equivariance into diffusion models can enhance their performance on data with inherent symmetries~\citepcolor{hoogeboom2022equivariant, geodiff, bose2023se}. Equivariant diffusion models ensure that the generative process respects the symmetries of the data, leading to more efficient learning and better generalization. However, integrating equivariance into diffusion models also inherits the challenges associated with equivariant neural networks.

\paragraph{Non-equivariant models.} \citecolor{sareen2025symmetry} learn the canonicalization that is easy for the non-equivariant model to learn.  \citecolor{Swallow} learn a transformer model to perform molecular conformation prediction. 
