\vspace{-0.5cm}
\section{Introduction}
\label{sec:intro}

% \begin{enumerate}
%     % \item Introduce diffusion models in scientific application domains. We want to respect physical law. 
%     % In these domains we need to respect equivariances. (One sentence description of what is equivariance).
%     % \item How to design a equivaraince diffusion model (what have been done). Dispite we are giving some inductive bias to the model, empirically there are some difficulty in training them. Sometimes they are even worse than non-equivariant counterparts. 
%     % When people want to enforce equivariant, they enforce the denoiser to be equivariant, and they show that it can model invariant distribution. 
%     \item What we say is that, we should really design the loss function that more tailor to this equivariant constraint. How our contribution connects to modeling side. Make it clear - people changing the model, we should also change the loss. 
% \end{enumerate}

Diffusion models are effective in modeling complex distributions~\citepcolor{ho2020denoising, song2020denoising, song2020score, karras2022elucidating}, with applications extending to domains like molecular and protein generation~\citepcolor{surveygraphdiff, digress, protein_diff}. Physical systems often exhibit symmetries (e.g., rotations or translations), requiring the distribution to be invariant to these operations~\citepcolor{hoogeboom2022equivariant, jing2022torsional}. \mathias{Give a more concrete example here as well.} To respect these symmetries, diffusion models must enforce equivariant constraints, ensuring the denoising process aligns with relevant transformations. This improves data efficiency, reduces the need for augmented training examples, and enhances generalization to unseen configurations~\citepcolor{Theoequi}.

Training diffusion models involves learning a denoiser that reverses an incremental noise addition process~\citepcolor{ho2020denoising}. The denoiser iteratively removes noise to reconstruct the original data~\citepcolor{tong2024learning, ho2020denoising}. Incorporating symmetries into the denoiser has been successfully applied in tasks like 3D molecular generation~\citepcolor{hoogeboom2022equivariant, hassan2024flow, guan20233d}, protein backbone generation~\citepcolor{bose2023se, jing2023eigenfold}, and point cloud generation~\citepcolor{luo2021diffusion, mikuni2023fast, wu2023fast}. However, these models face scalability limitations and convergence difficulties when trained with SGD~\citepcolor{doesequimatteratscale, nonuniversalityequi}.

To address these challenges, we analyze the minimizer of the diffusion loss function when the denoiser is not constrained to be equivariant. We show that the unconstrained minimizer results in a non-equivariant function, which highlights the importance of enforcing equivariance in the denoiser. In this work, we propose a novel approach by designing a loss function that explicitly enforces equivariance during the optimization process. Our method symmetrizes the target for the model's prediction via a time-dependent weighted averaging operation over the group actions. This ensures that the model respects the desired symmetries without requiring explicit constraints. To estimate the group average, we use Monte Carlo sampling, which incurs very little computational overhead, as it does not require additional neural network passes. The resulting loss function not only preserves the equivariance of the learned distribution but also reduces gradient estimator variance, enhancing both the stability and efficiency of the optimization process.

We provide a detailed analysis of our method, including a theoretical guarantee of equivariance for the minimizer of the proposed loss function. We evaluate it on synthetic datasets and the molecular conformation generation task~\citepcolor{hawkins2017conformation}, using the GEOM-QM9 dataset~\citepcolor{axelrod2022geom}. Our experiments demonstrate that the proposed loss function improves sample quality compared to existing methods. These results highlight the potential of our approach to enhance the scalability and practical use of equivariant diffusion models in various generative tasks.

% Equivariant diffusion models have proven to be effective in generative tasks involving data with inherent symmetry, such as 3D molecular generation~\citepcolor{hoogeboom2022equivariant, hassan2024flow, guan20233d, le2023navigating, jing2022torsional}, protein backbone generation~\citepcolor{bose2023se, jing2023eigenfold}, and point cloud generation~\citepcolor{luo2021diffusion, mikuni2023fast, wu2023fast}. These models capitalize on the symmetry properties of the data, ensuring that the generated samples respect the underlying geometric constraints, which is especially crucial in fields like computational chemistry and structural biology. 

% To ensure equivariance, a common approach is to design the denoiser to be equivariant~\citepcolor{hoogeboom2022equivariant, hassan2024flow, guan20233d, le2023navigating}. However, the empirical distribution provided by the dataset is often not an invariant distribution. This raises the question: is using an equivariant denoiser alone sufficient to model the underlying group-invariant distribution, or is data augmentation needed to symmetrize the observed distribution? Recent theoretical work by \citecolor{Theoequi} answers this question by demonstrating that equivariant diffusion models can learn group-invariant distributions using an equivariant score network, without requiring data augmentation.

% However, we argue that the Monte Carlo estimation of the diffusion loss function~\citepcolor{song2020denoising, song2020score, ho2020denoising} presents challenges in terms of convergence to the theoretical minimizer when applying to equivariant applications. Specifically, the training objective exhibits a slow convergence rate and high variance, which hinders the practical application of equivariant diffusion models and results in longer training times. This issue underlines the need for a loss function that can improve both the efficiency and effectiveness of training while maintaining the essential symmetry constraints.

% In this paper, we introduce a novel loss function specifically designed to address these challenges. Our loss function is unbiased, ensuring that the model learns without introducing systematic errors, while also achieving low variance to stabilize the training process. Additionally, it is computationally efficient, enabling faster training without sacrificing model performance. By integrating symmetry constraints directly into the Monte Carlo estimation function, our approach ensures that the target for the equivariant denoiser is itself an equivariant function.






%%%%%%%%%%%



