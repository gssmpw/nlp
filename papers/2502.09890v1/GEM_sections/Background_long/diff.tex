
\subsection{Diffusion Models and Equivariant Diffusion Models}

Diffusion models~\citepcolor{ho2020denoising, song2020score} are a class of generative models that construct complex data distributions by iteratively transforming simple noise distributions through a learned denoising process. The core idea is to define a forward diffusion process that incrementally corrupts the data and a reverse process that learns to reconstruct the original data. Formally, given data \( x_0 \sim q(x_0) \), the forward process generates a sequence \( x_t \) over time \( t \in [0, T] \) using a stochastic differential equation (SDE)~\citepcolor{song2020score} or a discrete Markov chain~\citepcolor{ho2020denoising}, such as:
\[
x_t = {\alpha_t} x_0 + \sigma_t \epsilon, \quad \epsilon \sim \mathcal{N}(0, I),
\]
where \( \alpha_t \) and \( \sigma_t \) are a time-dependent scaling factor and noise factor that determines the level of noise added at each step. The noise should be increasingly added to the sample so that at time $t = T$, $q(x_T) = \mcN(0, I)$.

The reverse process, parameterized by a neural network \( \phi_\theta(x_t, t) \), approximates clean sample-\( x_0 \) given the noisy version-\( x_t \) of it. The training objective typically involves minimizing a reweighted form of the denoising loss:
\begin{equation}
    \mcL = \mathbb{E}_{t \sim \mathcal{U}(0, T), (x_0, x_t) \sim q(x_0, x_t)} \left[ \omega(t) \| \phi_\theta (x_t, t) - x_0 \|^2 \right],
    \label{eqn:diff_loss}
\end{equation}
where \(\omega(t)\) is a time-dependent loss weight. To sample from the diffusion model, we begin with a noise vector \( x_T \sim \mathcal{N}(0, I) \) and iteratively apply the learned reverse process to transform it into a data sample \( x_0 \) using the trained model \( \phi_\theta \). The design of the reverse process offers significant flexibility. It can involve solving an ODE or SDE numerically~\citepcolor{song2020score, karras2022elucidating, lu2022dpm, tong2024learning}, or it may employ ancestral sampling methods~\citepcolor{ho2020denoising}.


\paragraph{Equivariant Diffusion Models.}
Many generative tasks involve data with inherent symmetries, such as rotation, reflection, or permutation invariance, which must be preserved. Equivariant diffusion models extend the standard diffusion framework by enforcing equivariance in the neural network \( \phi_\theta \). A model is said to be \(\Ge\) if the denoiser \( \phi_\theta \) satisfies \( \phi_\theta (g \circ x_t , t) = g \circ \phi_\theta (x_t, t) \), where \( g \in G \) is a symmetry transformation from the group \( G \). Importantly, diffusion models with \( \Ge \) denoisers can model \( \Gi \) distributions~\citepcolor{Theoequi}. 

% \paragraph{Group symmetrization operators.} 

These models are particularly suitable for applications in structural biology~\citepcolor{corso2024discovery, yim2023se, schneuing2024structure, igashov2024equivariant} and molecular modeling~\citepcolor{hoogeboom2022equivariant, guan20233d, le2023navigating}, where data resides in geometric domains governed by the linear isometry group. By ensuring that the generative process respects these symmetries, equivariant diffusion models produce results that are more physically meaningful and interpretable.



