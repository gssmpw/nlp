
\subsection{Toy Experiment}

\paragraph{Reflection group in 1D.} In this toy experiment, we create a dataset containing a single 1D sample with value \( 1 \). The equivariant group is the reflection group (i.e., \( g \cdot x = \pm x \)). We train a denoiser \( D_\theta(x_t, t) = f_\theta(x_t, t) - f_\theta(-x_t, t) \), which is a \emph{\( G \)-equivariant} function. The function \( f_\theta \) is a 3-layer multi-layer perceptron (MLP) with Tanh() activation functions. We train the model with a batch size of 100, a learning rate of \( 0.001 \), and for 100k iterations. During training, we log information about the loss and evaluate the model periodically. The evaluation metric is the root mean square deviation (RMSD) between the sample and the closest target value between 1 and -1. We train the model for 3 different seeds and plot the mean and error in \cref{fig:1D-toy}. We compare models using our loss (Ours), the default diffusion loss (Baseline), and the default loss with data augmentation (Baseline (augmentation)). For the sampling process, we solve the diffusion ODE using the DDIM~\citepcolor{song2020denoising} sampler in 100 steps to generate 1,000 samples. 

\input{GEM_sections/Experiments/figure_1d}

\Cref{fig:training_loss_nolog} shows the training loss over epochs, and \Cref{fig:validation_error_log} presents the evaluation RMSD for models with different training epochs. Firstly, regarding the loss, both the baseline and baseline (augmentation) exhibit the same loss trend, and the augmentation does not seem to provide any significant improvement. Additionally, both models show high variance in their loss. In contrast, our loss function provides a more stable training process, with the loss dropping significantly. 

In terms of evaluation metrics, our loss converges to a high-quality model faster than the baselines, eventually achieving better performance. While this is a simple problem, and we did not expect much improvement compared to the baseline (since the group space contains only 2 possible actions), our loss function still helps stabilize and accelerate training, leading to improved performance.

\paragraph{Rotation and Permutation Groups in 2D.} 
In this experiment, the model is tasked with generating a point cloud of \( 4 \) points arranged in a straight line, ensuring equivariance to both rotation and permutation groups. Similar to the 1D experiment, the training dataset consists of a single sample. This sample comprises \( 4 \) points in 2D space with coordinates \((-1.0, 0.0)\), \((-0.5, 0.0)\), \((0.5, 0.0)\), and \((1.0, 0.0)\), respectively. We employ EGNN~\citepcolor{satorras2021n} as the denoiser architecture, constructing a fully connected graph from the \( 4 \) points and using their coordinates as node features.


\input{GEM_sections/Experiments/figure_2d}

