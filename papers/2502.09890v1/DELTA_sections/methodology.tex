\section{Method}
\label{sec:method}

We propose a new loss function that attains the same minimizer as \emph{Symmetrized Diffusion} (SD) without requiring explicit constraints on the denoiser or an increase in training dataset size. While our approach does not rely on these techniques, they can still be incorporated if desired. In our experiments, for instance, we also use a \( G \)-equivariant denoiser. Furthermore, we show that our loss function exhibits lower gradient variance than existing approaches.


To provide a clearer foundation for our analysis, we first present a theoretical observation that establishes the relationship between the forward diffusion processes of the original empirical distribution and its symmetrized counterpart. We assume that our dataset consists of unique structures, meaning no two samples are equivalent under group transformations. Consider two forward diffusion processes: one applied to the original empirical distribution \( \hat{q}(x_0) \), generating time-dependent marginals \( \hat{q}_t(x_t) \), and the other applied to the symmetrized distribution \( \hat{q}^G(x_0) = S_G[q](x_0) \), generating \( \hat{q}_t^G(x_t) \). We show that \( \hat{q}_t^G(x_t) \) is the symmetrized version of \( \hat{q}_t(x_t) \), i.e., \( \hat{q}_t^G(x_t) = S_G[\hat{q}_t](x_t) \). The formal lemma and proof are provided in~\cref{sub_app:symmetrized}, and an illustration is shown in~\cref{fig:symmetrized_diffusion}.  
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.45\linewidth]{GEM_figs/Equi.pdf}
    \caption{The forward diffusion process applied to the symmetrized distribution \( \hat{q}^G(x_0) \) yields the symmetrized time-dependent marginal distributions \( \hat{q}_t^G(x_t) \) of the original process \( \hat{q}_t(x_t) \).}
    \label{fig:symmetrized_diffusion}
\end{figure}
Our objective is to design a loss function with the same minimizer as SD without explicitly constraining the denoiser or relying on data augmentation.


\paragraph{Analyzing the Minimizer of Symmetrized Diffusion.} We examine the minimizer of the empirical diffusion loss at time \(t\):
\begin{align}
    \mathcal{L}^G_t(\phi) &= \mathbb{E}_{(x_0, x_t) \sim \hat{q}^G(x_0, x_t)} \left[ \omega(t) \| \phi(x_t, t) - x_0 \|^2 \right] \nonumber \\
    &= \omega(t) \mathbb{E}_{x_t \sim \hat{q}^G_t(x_t)} \mathbb{E}_{x_0 \sim \hat{q}^G_t(x_0 \mid x_t)} \left[ \| \phi(x_t, t) - x_0 \|^2 \right] .
    \label{eqn:sym_loss}
\end{align}

The minimizer of the empirical loss function is the point where its derivative with respect to \( \phi \) becomes zero. For each \( x_t \), the gradient is given by:  

\begin{align}
    \nabla_\phi \left[ \mathbb{E}_{x_0 \sim \hat{q}^G_t(x_0 \mid x_t)} \left[ \| \phi(x_t, t) - x_0 \|^2 \right] \right] &= 2 \mathbb{E}_{x_0 \sim \hat{q}^G_t(x_0 \mid x_t)} \left[  \phi(x_t, t) - x_0  \right] \nonumber \\
    &= 2\left(\phi(x_t, t) -  \mathbb{E}_{x_0 \sim \hat{q}^G_t(x_0 \mid x_t)}[x_0] \right)\nonumber.
\end{align}  
Thus, the minimizer \( \phi^* \) of \cref{eqn:sym_loss} satisfies:  
\begin{align*}
    \phi^*_G(x_t, t) = \mathbb{E}_{x_0 \sim \hat{q}^G_t(x_0 \mid x_t)}[x_0]  = \int_\Omega x_0 \hat{q}^G_t(x_0 \mid x_t) \mathrm{d}x_0 = \int_\Omega x_0 q_t(x_t \mid x_0) \frac{\hat{q}^G(x_0)}{\hat{q}^G_t(x_t)} \mathrm{d}x_0 .
\end{align*}
which is \( \Ge \) (proof in~\cref{sub_app:sym_diff_mini}), while the Non-symmetrized Diffusion's minimizer is not necessarily \(\Ge\) (see~\cref{sub_app:non_sym_diff_mini} for the proof). 

Although \emph{Symmetrized Diffusion} has an equivariant minimizer, data augmentation can significantly increase the dataset size. This, in turn, requires greater computational resources. 

A common alternative is to design the denoiser architecture to naturally handle data symmetries. This often involves using specialized layers where the weight matrices and activation functions follow specific constraints~\citepcolor{hoogeboom2022equivariant, protein_diff, equi_drug_diff}. 

\paragraph{Constrained Equivariant Diffusion Models} Enforcing \( \phi \) to be \( G \)-equivariant while minimizing the diffusion loss involves solving the following constrained optimization problem:
\begin{align} 
\label{eqn:constraint_problem}
&\text{Minimize} \quad \mathcal{L}_t(\phi) = \omega(t) \mathbb{E}_{(x_0, x_t) \sim \hat{q}_t(x_0, x_t)} [\| \phi(x_t, t) - x_0 \|^2]  \\
&\text{Subject to} \quad \phi(g \circ x_t, t) = g \circ \phi(x_t, t) \quad \forall x_t \in \Omega \text{ and } \forall g \in G. \nonumber 
\end{align}
This formulation, where equivariance constraints are explicitly enforced, has gained significant attention in recent research~\citepcolor{yim2023se, hoogeboom2022equivariant}. Recent theoretical work~\citepcolor{Theoequi} demonstrates that \emph{Constrained Equivariant Diffusion Models} and \emph{Symmetrized Diffusion} share the same minimizer.


In practice, this optimization is challenging and often requires complex design choices to enforce equivariance, which can restrict the class of minimizers and complicate training~\citepcolor{relax_equi}. Furthermore, since the minimizer of \(\mathcal{L}_t(\phi)\) is not naturally equivariant (see proof in~\cref{sub_app:non_sym_diff_mini}), the equivariant constraint must actively guide the optimization process, adding to its difficulty. Additionally, we will show that the Monte Carlo estimation of the loss function exhibits high variance, further exacerbating the optimization challenge~\citepcolor{nonuniversalityequi}.


\paragraph{Proposed Loss Function.} To address the challenges of \emph{Symmetrized Diffusion} and \emph{Constrained Equivariant Diffusion Models}, we propose a novel loss function:  
\begin{align} 
\label{eqn:proposed_loss} 
&\bar{\mathcal{L}}^G_t(\phi) = \mathbb{E}_{x_0 \sim \hat{q}(x_0)} \mathbb{E}_{x_t \sim \hat{q}_t(x_t \mid x_0)} \left[ \omega(t) \| \phi(x_t, t) - \phi^*_G(x_t, t) \|^2 \right],
\end{align}
where
\begin{align}
\phi^*_G(x_t, t) = \int_\Omega x_0 \hat{q}^G_t(x_0 \mid x_t) \mathrm{d}x_0. \nonumber
\end{align}
Rather than denoising \(x_t\) directly to the target \(x_0\), this approach encourages \(\phi(x_t, t)\) to align with \(\phi^*_G(x_t, t)\), which is already a \(\Ge\) function (proof in~\cref{sub_app:sym_diff_mini}). Notably, the minimizer of this loss function is \(\phi^*_G(x_t, t)\), eliminating the need for data augmentation or explicitly enforcing equivariance in the network design.

Although computing \(\phi^*_G(x_t, t)\) analytically is intractable, we can approximate it using the Monte Carlo method outlined in~\cref{alg:group_symmetrization}. To understand how this approximation is derived, we decompose the integral over \(\Omega\) into a double integral over the quotient space \(\Omega / G\) and the group \(G\), since our dataset consists of unique structures:  
\begin{align*}
    \phi^*_G(x_t, t) &= \int_{\Omega / G} \int_G (g \circ x_0) \hat{q}^G_t(x_0 \mid x_t) \mathrm{d}\mu_G(g) \mathrm{d}x_0 \\
    &= \sum_{x_0 \in D} \int_G (g \circ x_0) \hat{q}^G_t(x_0 \mid x_t) \mathrm{d}\mu_G(g).
\end{align*}

We approximate this integral by first sampling \(x_0\) from the dataset \(D\) and then averaging over transformations \(g\) drawn from the group \(G\).

% This approximation effectively balances computational feasibility and accuracy, leveraging dataset samples and group transformations to estimate \(\phi^*_G(x_t, t)\) efficiently. Importantly, this formulation avoids the complexity of directly computing the group-equivariant target, enabling scalable training of diffusion models with symmetrized objectives.


% Since the approximation involves only simple operations, computing \(\phi^*_G(x_t, t)\) introduces negligible overhead in practice.

% The most important characteristic of our loss function is that it has the same minimizer as \emph{Symmetrized Diffusion}. The following theorem formalizes this result, showing that the minimizer of our proposed loss function is indeed equivariant and coincides with the minimizer of the Symmetrized Diffusion model. See~\cref{sub_app:theo_1} for the proof.

% \begin{tcolorbox}[title=Equivariant Minimizer of \(\bar{\mcL}_t^G (\phi)\)]
% \begin{theorem}
% Let \( \bar{\phi}^*_G \) be the minimizer of the loss function \(\bar{\mcL}_t^G\) in~\cref{eqn:proposed_loss}. Then \( \bar{\phi}^*_G(x_t, t) \) is \( G \)-equivariant and 
% \[ \bar{\phi}^*_G(x_t, t) = \phi^*_G (x_t, t) = \int_\Omega x_0 \hat{q}^G_t(x_0 \mid x_t) \mathrm{d}x_0 . \]
% \label{the:theo_1}
% \end{theorem}
% \end{tcolorbox}

While the proposed loss inherently aligns with the minimizer of the constrained problem, practical optimization may still introduce potential deviations. Therefore, our method does not rule out the use of the equivariant constraint if one chooses to apply it. There are two key benefits to using our loss function in conjunction with the equivariant constraint. 

First, the minimizer of our loss function already satisfies the equivariant constraint, meaning that the role of the constraint is no longer solely to guide the minimizer to the correct equivariant functional class. This significantly simplifies the optimization problem. Second, we will show that our loss function exhibits low variance, which further aids in stabilizing and accelerating the optimization process.


\paragraph{Practical Implementation.}  
We prove in~\cref{sub_app:SFull} that \(\phi^*_G(x_t, t)\) can be simplified to the following form:
\begin{align*}
    &\phi^*_G(x_t, t) = C \sum_{x_0 \in D} \int_G (g \circ x_0) \exp \left( \frac{-\|x_t - \alpha_t(g \circ x_0)\|^2}{2 \sigma_t^2} \right) \mathrm{d}\mu_G(g), 
    \label{eqn:final_form}
\end{align*}
where \(C\) is a normalized factor such that:
\begin{align*}
    C \sum_{x_0 \in D} \int_G \exp \left( \frac{-\|x_t - \alpha_t(g \circ x_0)\|^2}{2 \sigma_t^2} \right) \mathrm{d}\mu_G(g) = 1.
\end{align*}
    

The function \(\phi^*_G(x_t, t)\) represents a normalized expectation over transformations of data samples \(x_0 \in D\) under the action of the group \(G\). Specifically, it computes a weighted average of the transformed inputs \(g \circ x_0\), with weights determined by a Gaussian kernel that measures the similarity between the noisy sample \(x_t\) and the scaled transformed input \(\alpha_t (g \circ x_0)\). We can approximate \(\phi^*_G(x_t, t)\) using Monte Carlo sampling to estimate the group integral, as detailed in Algorithm~\ref{alg:group_symmetrization}. 

\begin{algorithm}[H]
    \caption{Approximation of \( \phi^*_G(x_t, t) \)}
    \label{alg:group_symmetrization}
    \begin{algorithmic}[1]
        \STATE Draw \( M \) samples \( x_0^m \sim \hat{q}(x_0) \), \( K \) group elements \( g_k \sim \mu_G \)
        \FOR{each \( x_0^m \), \( g_k \)}
            \STATE \( x_0^{mk} \leftarrow g_k \circ x_0^m \), \quad 
            \( R_{mk} \leftarrow \frac{-\| x_t - \alpha_t x_0^{mk} \|^2}{2\sigma_t^2} \)
        \ENDFOR
        \STATE \( w_{mk} \leftarrow \frac{\exp(R_{mk})}{\sum_{j=1}^{MK} \exp(R_j)} \), \quad
        \( \phi^*_G(x_t, t) \leftarrow \sum_{j=1}^{MK} w_j x_0^j \)
    \end{algorithmic}
\end{algorithm}

A key advantage of the proposed loss function is its significantly lower gradient estimator variance when used with a \(\Ge\) denoiser, compared to the constrained optimization objective in Eq.~\eqref{eqn:constraint_problem}. This reduction arises from the symmetrization in \(\phi^*_G(x_t, t)\), which averages out noise over group transformations. Formally:

\begin{tcolorbox}[title=Variance Reduction of the Proposed Loss]
\begin{theorem}
Let \(\mcL_t(\phi)\) and \(\bar{\mcL}_t^G(\phi)\) be the Monte Carlo estimators corresponding to Eq.~\eqref{eqn:constraint_problem} and Eq.~\eqref{eqn:proposed_loss}, respectively, where \(\phi(x_t, t)\) is \(\Ge\). Then:
\begin{equation}
    \text{\emph{Var}}(\nabla_\phi \mcL_t(\phi)) \geq \text{\emph{Var}}(\nabla_\phi \bar{\mcL}_t^G(\phi)).
\end{equation}
\label{the:theo_2}
\end{theorem}
\end{tcolorbox}


A full proof is provided in~\cref{sub_app:theo_2}.

