\vspace{-0.5cm}
\section{Introduction}
\label{sec:intro}

% \begin{enumerate}
%     % \item Introduce diffusion models in scientific application domains. We want to respect physical law. 
%     % In these domains we need to respect equivariances. (One sentence description of what is equivariance).
%     % \item How to design a equivaraince diffusion model (what have been done). Dispite we are giving some inductive bias to the model, empirically there are some difficulty in training them. Sometimes they are even worse than non-equivariant counterparts. 
%     % When people want to enforce equivariant, they enforce the denoiser to be equivariant, and they show that it can model invariant distribution. 
%     \item What we say is that, we should really design the loss function that more tailor to this equivariant constraint. How our contribution connects to modeling side. Make it clear - people changing the model, we should also change the loss. 
% \end{enumerate}

Diffusion models have proven effective in capturing complex distributions~\citepcolor{ho2020denoising, song2020denoising, song2020score, karras2022elucidating}, with notable applications in molecular and protein generation~\citepcolor{surveygraphdiff, digress, protein_diff}. Many physical systems, such as molecular structures, exhibit inherent symmetries. For instance, a molecule's spatial configuration remains invariant under rotations in three-dimensional space~\citepcolor{hoogeboom2022equivariant, jing2022torsional}.

Symmetries in data are naturally described by \(\Gi\) distributions, which remain unchanged under transformations from a symmetry group \(G\)~\citepcolor{Theoequi, equiflow}. Our goal is to model a \(\Gi\) distribution \(q(x_0)\) using diffusion models. In practice, however, we only have access to a finite dataset \(D = \{x_0^i\}_{i=1}^N\), which defines the empirical distribution \(\hat{q}(x) := \frac{1}{N} \sum_{i=1}^N \delta(x - x_0^i)\), where \(\delta\) is the Dirac delta function. Unfortunately, \(\hat{q}(x_0)\) is not necessarily \(\Gi\), as the dataset \(D\) may not be closed under the group action \(g \in G\). Consequently, fitting a model \(p_\theta(x_0)\) to \(\hat{q}(x_0)\) without enforcing equivariant constraints may result in poor generalization. We refer to this approach as \emph{Non-Symmetrized Diffusion} (NSD).

While some NSD models achieve strong performance without explicit equivariant constraints, they typically rely on datasets with a canonical coordinate system~\citepcolor{Swallow} or large datasets that mitigate the need for augmentation~\citepcolor{abramson2024accurate}. Nevertheless, incorporating equivariant inductive biases into diffusion models remains a preferred approach, as it better captures the underlying symmetries of the data. Two common strategies for achieving this are data augmentation and the design of equivariant denoisers.

Data augmentation involves constructing a symmetrized dataset \(D^G = \{g \circ x_0^i \mid g \in G, x_0^i \in D\}\), which yields a \(\Gi\) empirical distribution closer to the true distribution \(q(x_0)\)~\citepcolor{Theoequi}. However, this approach can be computationally expensive, especially when the group \(G\) is large or infinite. We refer to this method as \emph{Symmetrized Diffusion} (SD).

Alternatively, one can train diffusion models on the original dataset using an equivariant denoiser~\citepcolor{yim2023se, hoogeboom2022equivariant, jing2022torsional, protein_diff}. Since the diffusion sampling process involves repeated denoising steps, an equivariant denoiser ensures that an initially invariant prior distribution remains invariant throughout sampling. This approach has been successfully applied to tasks such as 3D molecular generation~\citepcolor{hoogeboom2022equivariant, hassan2024flow, guan20233d}, protein backbone generation~\citepcolor{bose2023se, jing2023eigenfold}, and point cloud generation~\citepcolor{luo2021diffusion, mikuni2023fast, wu2023fast}.

Formally, enforcing equivariance in diffusion models can be framed as a constrained optimization problem. However, this introduces challenges such as increased architectural complexity, higher computational costs~\citepcolor{Swallow, doesequimatteratscale}, and convergence issues due to noisy gradients~\citepcolor{nonuniversalityequi}. In this work, we demonstrate that the loss function in such approaches is inherently noisy, exacerbating these challenges.

To address these issues, we propose a novel approach that explicitly enforces equivariance through a carefully designed loss function. Our method symmetrizes the model's prediction target via a time-dependent weighted averaging operation over group actions applied to data samples. This ensures that the model respects the desired symmetries without requiring explicit architectural constraints. We estimate the average using Monte Carlo sampling, which incurs minimal computational overhead as it avoids additional neural network passes. The resulting loss function not only preserves equivariance but also reduces gradient estimator variance, improving both optimization stability and efficiency.

We provide a theoretical analysis of our method, including a guarantee of equivariance for the minimizer of the proposed loss function. Our experiments on synthetic datasets and the molecular conformation generation task~\citepcolor{hawkins2017conformation}, using the GEOM-QM9 dataset~\citepcolor{axelrod2022geom}, demonstrate that our approach improves sample quality compared to existing methods. These results underscore the potential of our method to enhance the scalability and practicality of equivariant diffusion models in generative tasks.





%%%%%%%%%%%



