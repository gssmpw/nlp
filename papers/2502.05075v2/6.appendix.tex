\onecolumn
\clearpage
\appendix
\appendixpage  % if you use a package that provides an appendix title page
\hypersetup{linkcolor=black}
\startcontents[sections]
\printcontents[sections]{l}{1}

\hypersetup{linkcolor=hrefblue}
\glsresetall

\section{Additional related works}\label{apx:related_works}

\paragraph{Knowledge distillation.}
Knowledge distillation (KD)~\citep{hinton2015distilling,gou2021knowledge} is closely connected to W2S generalization regarding the teacher-student setup, while W2S reverts the capacities of teacher and student in KD. In KD, a strong teacher model guides a weak student model to learn the teacher's knowledge. In contrast, W2S generalization occurs when a strong student model surpasses a weak teacher model under weak supervision.
\citet{phuong2019towards,stanton2021does,ojha2023knowledge,nagarajan2023student,dong2024cluster,ildiz2024high} conducted rigorous statistical analyses for the student's generalization from knowledge distillation. 
From the analysis perspective, a key difference between KD and W2S is that W2S is usually analyzed in the context of finetuning since the notions of “weak” and “strong” are built upon pretraining. This finetuning perspective introduces distinct angles from KD for examining intrinsic dimension~\citep{li2018measuring} and student-teacher correlation in W2S. 

\paragraph{Self-distillation and self-training.}
In contrast to W2S that considers distinct student and teacher models, self-distillation~\citep{zhang2019your,zhang2021self} and related paradigms such as Born-Again Networks~\citep{furlanello2018born} use the same or progressively refined architectures to iteratively distill knowledge from a ``previous version'' of the model. There have been extensive theoretical analyses toward understanding the mechanism behind self-distillation~\citep{mobahi2020self,das2023understanding,borup2023self,pareek2024understanding}.

Self-training~\citep{scudder1965probability,lee2013pseudo} is a closely related method to self-distillation that takes a single model's confident predictions to create pseudo-labels for unlabeled data and refines that model iteratively. 
\citet{wei2020theoretical,oymak2021theoretical,frei2022self} provide theoretical insights into the generalization of self-training. 
In particular, \citet{wei2020theoretical} introduced a theoretical framework based on neighborhood expansion, which was later on extended to various settings of weakly supervised learning, including domain adaptation~\citep{cai2021theory}, contrastive learning~\citep{shen2022connect}, consistency regularization~\citep{yang2023sample}, and now weak-to-strong generalization~\citep{lang2024theoretical,shin2024weak}.




\section{Proofs in \Cref{sec:single_task_ft}}

\begin{lemma}\label{lem:low_est_err_ft}    
    Given the FT approximation errors $\rho_s$ and $\rho_w$ in \Cref{def:ft_est_err}, we have
    \begin{align*}
        \rho_s(n) \le n \rho_s \quad \text{and} \quad \rho_w(n) \le n \rho_w \quad \forall\ n \in \N.
    \end{align*}
\end{lemma}

\begin{proof}[Proof of \Cref{lem:low_est_err_ft}]
    Let $\thetab_* = \argmin_{\thetab \in \R^d}\ \E_{\xb \sim \Dcal}[(\phi_w(\xb)^\top \thetab - f_*(\xb))^2]$ such that
    \begin{align*}
        \E_{\xb \sim \Dcal}[(\phi_w(\xb)^\top \thetab_* - f_*(\xb))^2] = \rho_w.
    \end{align*}
    Then, by observing that conditioned on $\Xb$,
    \begin{align*}
        \phi_w(\Xb)^\dagger f_*(\Xb) = \argmin_{\thetab \in \R^d}\ \| \phi_w(\Xb) \thetab - f_*(\Xb) \|_2^2,
    \end{align*} 
    we have
    \begin{align*}
        \rho_w(n) &= \E_{\Xb \sim \Dcal^n}\sbr{\| \phi_w(\Xb) \phi_w(\Xb)^\dagger f_*(\Xb) - f_*(\Xb) \|_2^2} \\
        &\le \E_{\Xb \sim \Dcal^n}\sbr{\| \phi_w(\Xb) \thetab_* - f_*(\Xb) \|_2^2} \\
        &= n\ \E_{\Xb \sim \Dcal^n}\sbr{\frac{1}{n} \| \phi_w(\Xb) \thetab_* - f_*(\Xb) \|_2^2} \\
        &= n\ \E_{\xb \sim \Dcal}\sbr{(\phi_w(\xb)^\top \thetab_* - f_*(\xb))^2} \\
        &= n\ \rho_w.
    \end{align*}
    The proof for $\rho_s(n)$ follows analogously.
\end{proof}



\subsection{Proof of \Cref{thm:w2s_ft}}\label{apx:pf_w2s_ft}

\begin{theorem}[Formal restatement of \Cref{thm:w2s_ft}]\label{thm:w2s_ft_formal}
    Consider $f_\wts(\xb) = \phi_s(\xb)^\top \thetab_\wts$ finetuned as in \eqref{eq:sft_weak}, \eqref{eq:w2s_ft} with both $\alpha_w, \alpha_\wts \to 0$. Under \Cref{asm:features,asm:ft_data}, when $n \ge \Omega(d_w)$, the excess risk $\exrisk(f_\wts) = \vari(f_\wts) + \bias(f_\wts)$ satisfies
    \begin{align*}
        &\bias(f_\wts) \le \frac{\rho_w(n)}{n} + \frac{\rho_s(N)}{N} \le \rho_w + \rho_s, \\
        &\vari(f_\wts) \lesssim \frac{\sigma^2}{n} \rbr{d_{s \wedge w} + \frac{d_s}{N} (d_w - d_{s \wedge w})}.
    \end{align*}
    In particular, when ${\rho_w(n)}/{n} > 0$ and $d_s < d_w$, the inequality for $\bias(f_\wts)$ is strict.

    Moreover, when $\phi_w(\xb) \sim \Ncal(\b0_d, \Sigmab_w)$, for any $n > d_w + 1$, we have 
    \begin{align*}
        &\vari(f_\wts) = \frac{\sigma^2}{n-d_w-1} \rbr{d_{s \wedge w} + \frac{d_s}{N} (d_w - d_{s \wedge w})}.
    \end{align*}
\end{theorem}

\begin{proof}[Proof of \Cref{thm:w2s_ft} and \Cref{thm:w2s_ft_formal}]
    We first observe that the solution of \eqref{eq:sft_weak} as $\alpha_w \to 0$ is given by
    \begin{align*}
        \thetab_w = \wt\Phib_w^\dagger \wt\yb = \wt\Phib_w^\dagger (\wt\fb_* + \wt\zb),
    \end{align*}
    where $\wt\zb \sim \Ncal(\b0_n, \sigma^2 \Ib_n)$.
    Meanwhile, the solution of \eqref{eq:w2s_ft} as $\alpha_\wts \to 0$ is given by
    \begin{align*}
        \thetab_\wts = \Phib_s^\dagger \Phib_w \thetab_w = \Phib_s^\dagger \Phib_w \wt\Phib_w^\dagger (\wt\fb_* + \wt\zb).
    \end{align*}  
    
    Then, the excess risk of $f_\wts$ can be decomposed into variance and bias as follows:
    \begin{align*}
        \exrisk(f_\wts) &= \E_{\xb \sim \Dcal}\sbr{\E_{f_\wts}\sbr{(f_\wts(\xb) - f_*(\xb))^2}} \\
        &= \E_{\Scal_x}\sbr{\E_{\wt\Scal}\sbr{\frac{1}{N}\nbr{\Phib_s \thetab_\wts - \fb_*}_2^2}} \\
        &=\E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{(\Phib_s \Phib_s^\dagger \Phib_w \wt\Phib_w^\dagger \wt\fb_* - \fb_*) + \Phib_s \Phib_s^\dagger \Phib_w \wt\Phib_w^\dagger \wt\zb}_2^2} \\
        &= \underbrace{\frac{1}{N} \E_{\Scal_x, \wt\Scal}\sbr{\nbr{\Phib_s \Phib_s^\dagger \Phib_w \wt\Phib_w^\dagger \wt\zb}_2^2}}_{\vari(f_\wts)} + \underbrace{\frac{1}{N} \E_{\Scal_x, \wt\Scal}\sbr{\nbr{\Phib_s \Phib_s^\dagger \Phib_w \wt\Phib_w^\dagger \wt\fb_* - \fb_*}_2^2}}_{\bias(f_\wts)}.
    \end{align*}

    \paragraph{Bias.}
    For the bias term, by observing that $\Pb_s = \Phib_s \Phib_s^\dagger$ is an $N \times N$ orthogonal projection, we can decompose the bias term as
    \begin{align*}
        \bias(f_\wts) &= \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Pb_s \rbr{\Phib_w \wt\Phib_w^\dagger \wt\fb_* - \fb_*}}_2^2} + \frac{1}{N} \E_{\Scal_x}\sbr{\nbr{\rbr{\Ib_N - \Pb_s} \fb_*}_2^2},
    \end{align*}
    where $\E_{\Scal_x}\sbr{\nbr{\rbr{\Ib_N - \Pb_s} \fb_*}_2^2} = \rho_s(N)$ by \Cref{def:ft_est_err}.

    For the first term, 
    \begin{align*}
        \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Pb_s \rbr{\Phib_w \wt\Phib_w^\dagger \wt\fb_* - \fb_*}}_2^2} &\le \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Phib_w \wt\Phib_w^\dagger \wt\fb_* - \fb_*}_2^2} \\
        &= \E_{\wt\Scal}\sbr{\frac{1}{n} \nbr{\wt\Phib_w \wt\Phib_w^\dagger \wt\fb_* - \wt\fb_*}_2^2} \\
        &= \frac{\rho_w(n)}{n}.
    \end{align*}
    Notice that when ${\rho_w(n)}/{n} > 0$, this inequality is strict if $d_s < d_w$, where $\Phib_w \wt\Phib_w^\dagger \wt\fb_* - \wt\fb_* \notin \range(\Phib_s)$ almost surely.

    Overall, we have
    \begin{align*}
        \bias(f_\wts) \le \frac{\rho_w(n)}{n} + \frac{\rho_s(N)}{N} \le \rho_w + \rho_s,
    \end{align*}
    where the second inequality follows from \Cref{lem:low_est_err_ft}.

    \paragraph{Variance.}
    For the variance term, we observe that
    \begin{align*}
    \begin{split}
        \vari(f_\wts) &= \frac{1}{N} \E_{\Scal_x, \wt\Scal}\sbr{\nbr{\Pb_s \Phib_w \wt\Phib_w^\dagger \wt\zb}_2^2} \\
        &= \frac{1}{N} \E_{\Scal_x, \wt\Scal}\sbr{\tr\rbr{\Phib_w^\top \Pb_s \Phib_w \wt\Phib_w^\dagger \wt\zb \wt\zb^\top (\wt\Phib_w^\dagger)^\top}} \\
        &= \frac{\sigma^2}{N} \E_{\Scal_x, \wt\Scal}\sbr{\tr\rbr{\Phib_w^\top \Pb_s \Phib_w (\wt\Phib_w^\top \wt\Phib_w)^\dagger}},
    \end{split}
    \end{align*}
    which implies
    \begin{align}\label{eq:pf_var_w2s}
    \begin{split}
        \vari(f_\wts) = \frac{\sigma^2}{N} \tr\rbr{\E_{\Scal_x}\sbr{\Sigmab_w^{-1/2} \Phib_w^\top \Pb_s \Phib_w \Sigmab_w^{-1/2}} \E_{\wt\Scal}\sbr{\rbr{\Sigmab_w^{-1/2} \wt\Phib_w^\top \wt\Phib_w \Sigmab_w^{-1/2}}^\dagger}}.
    \end{split}
    \end{align}

    Recall the spectral decomposition $\Sigmab_w = \Vb_w \Lambdab_w \Vb_w^\top$. 
    Since $\E_{\xb \sim \Dcal}[\phi_w(\xb) \phi_w(\xb)^\top] = \Sigmab_w$, for each $\xb \sim \Dcal$, we can write $\phi_w(\xb) = \Sigmab_w^{1/2} \gammab$, where $\gammab \in \R^{d}$ is an independent random vector that is zero-mean and isotropic (\ie $\E[\gammab] = \b0_{d}$ and $\E[\gammab \gammab^\top] = \Ib_{d}$). The same holds for $\Sigmab_s = \Vb_s \Lambdab_s \Vb_s^\top$ and $\phi_s(\xb) = \Sigmab_s^{1/2} \gammab$.

    Then, for $\Scal$ and $\wt\Scal$, there exist independent random matrices $\Gammab = [\gammab_1, \ldots, \gammab_N]^\top \in \R^{N \times d}$ and $\wt\Gammab = [\wt\gammab_1, \ldots, \wt\gammab_n]^\top \in \R^{n \times d}$ consisting of $\iid$ zero-mean isotropic rows such that
    \begin{align}\label{eq:pf_var_w2s_subgaussian_asm}
    \begin{split}
        &\Phib_w \Sigmab_w^{-1/2} = \Gammab \Sigmab_w^{1/2} \Sigmab_w^{-1/2} = \Gammab \Vb_w \Vb_w^\top, \\
        &\wt\Phib_w \Sigmab_w^{-1/2} = \wt\Gammab \Sigmab_w^{1/2} \Sigmab_w^{-1/2} = \wt\Gammab \Vb_w \Vb_w^\top, \\
        &\Phib_s \Sigmab_s^{-1/2} = \Gammab \Sigmab_s^{1/2} \Sigmab_s^{-1/2} = \Gammab \Vb_s \Vb_s^\top, \\
        &\wt\Phib_s \Sigmab_s^{-1/2} = \wt\Gammab \Sigmab_s^{1/2} \Sigmab_s^{-1/2} = \wt\Gammab \Vb_s \Vb_s^\top.
    \end{split}
    \end{align}
    Let $\Gammab_w = \Gammab \Vb_w \in \R^{N \times d_w}$ and $\wt\Gammab_w = \wt\Gammab \Vb_w \in \R^{n \times d_w}$. We observe that
    \begin{align*}
        \E_{\wt\Scal}\sbr{\rbr{\Sigmab_w^{-1/2} \wt\Phib_w^\top \wt\Phib_w \Sigmab_w^{-1/2}}^\dagger}
        = \E_{\wt\Scal}\sbr{\rbr{\Vb_w \wt\Gammab_w^\top \wt\Gammab_w \Vb_w^\top}^\dagger} 
        = \Vb_w \E_{\wt\Scal}\sbr{\rbr{\wt\Gammab_w^\top \wt\Gammab_w}^\dagger} \Vb_w^\top.
    \end{align*}

    Now, we consider the following two cases for the feature distribution of $\phi_w(\xb)$, corresponding to the distribution of $\Gammab_w$ and $\wt\Gammab_w$:
    \begin{enumerate}[label=(\alph*)]
        \item \b{Gaussian features}: In \Cref{thm:w2s_ft}, assuming $\phi_w(\xb) \sim \Ncal(\b0_d, \Sigmab_w)$ such that $\wt\Gammab_w$ consists of $\iid$ Gaussian rows, we have $\wt\gammab_i \sim \Ncal(\b0_{d_w}, \Ib_{d_w})$. Notice that under the assumption $n > d_w + 1$, $\rank(\wt\Gammab_w) = d_w$ almost surely, and therefore $\wt\Gammab_w^\top \wt\Gammab_w$ is invertible.
        
        Meanwhile, with $\wt\gammab_i \sim \Ncal(\b0_{d_w}, \Ib_{d_w})$ for all $i \in [n]$, $(\wt\Gammab_w^\top \wt\Gammab_w) \sim \Wcal(\Ib_{d_w},n)$ follows the Wishart distribution~\citep[Definition 3.4.1]{wishart1928generalised} with $n$ degrees of freedom and scale matrix $\Ib_{d_w}$. 
        Therefore, $(\wt\Gammab_w^\top \wt\Gammab_w)^{-1} \sim \Wcal^{-1}(\Ib_{d_w},n)$ follows the inverse Wishart distribution~\citep[\S 3.8]{mardia2024multivariate}, whose mean takes the form~\citep[(3.8.3)]{mardia2024multivariate}
        \begin{align*}
            \E_{\wt\Scal}\sbr{(\wt\Gammab_w^\top \wt\Gammab_w)^\dagger} = \frac{1}{n - d_w -1} \Ib_{d_w}.
        \end{align*}
        Then, we have
        \begin{align*}
            \E_{\wt\Scal}\sbr{\rbr{\Sigmab_w^{-1/2} \wt\Phib_w^\top \wt\Phib_w \Sigmab_w^{-1/2}}^\dagger}
            = \frac{1}{n - d_w -1} \Vb_w \Vb_w^\top.
        \end{align*}
        Therefore, \eqref{eq:pf_var_w2s} implies
        \begin{align}\label{eq:pf_var_w2s_1}
        \begin{split}
            \vari(f_\wts) &= \frac{\sigma^2}{N}\ \frac{1}{n - d_w -1}\ \tr\rbr{\Vb_w^\top \E_{\Scal_x}\sbr{\Sigmab_w^{-1/2} \Phib_w^\top \Pb_s \Phib_w \Sigmab_w^{-1/2}} \Vb_w} \\
            &= \frac{\sigma^2}{N}\ \frac{1}{n - d_w -1}\ \tr\rbr{\E_{\Scal_x}\sbr{\Vb_w^\top \Vb_w \Gammab_w^\top \Pb_s \Gammab_w \Vb_w^\top \Vb_w}} \\
            &= \frac{\sigma^2}{N}\ \frac{1}{n - d_w -1}\ \tr\rbr{\E_{\Scal_x}\sbr{\Gammab_w^\top \Pb_s \Gammab_w}}.
        \end{split}
        \end{align}
        Recall that $\Pb_s = \Phib_s \Phib_s^\dagger$. Let $\Gammab_s = \Gammab \Vb_s \in \R^{N \times d_s}$, and we can write
        \begin{align*}
            \Pb_s = (\Phib_s \Sigmab_s^{-1/2}) (\Phib_s \Sigmab_s^{-1/2})^\dagger = (\Gammab_s \Vb_s^\top) (\Gammab_s \Vb_s^\top)^\dagger = \Gammab_s \Gammab_s^\dagger.
        \end{align*}
        Therefore, with $\Gammab_w = \Gammab \Vb_w$ and $\Gammab_s = \Gammab \Vb_s$, we can decompose
        \begin{align*}
            \tr\rbr{\E_{\Scal_x}\sbr{\Gammab_w^\top \Pb_s \Gammab_w}} 
            &= \E_{\Scal_x}\sbr{\tr\rbr{\Gammab_w^\top \Gammab_s \Gammab_s^\dagger \Gammab_w}} \\
            &= \E_{\Scal_x}\sbr{\tr\rbr{\Vb_w^\top \Vb_s \Vb_s^\top \Vb_w \Gammab_w^\top \Gammab_s \Gammab_s^\dagger \Gammab_w}} \\
            &+ \E_{\Scal_x}\sbr{\tr\rbr{\Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top) \Vb_w \Gammab_w^\top \Gammab_s \Gammab_s^\dagger \Gammab_w}}.
        \end{align*}
        For the first term, since $\Gammab_w \Vb_w^\top \Vb_s = \Gammab \Vb_w \Vb_w^\top \Vb_s$ and $\Gammab_s = \Gammab \Vb_s$, the range of $\Gammab_w \Vb_w^\top \Vb_s$ is a subspace of that of $\Gammab_s$ and therefore,
        \begin{align*}
            \E_{\Scal_x}\sbr{\tr\rbr{\Vb_w^\top \Vb_s \Vb_s^\top \Vb_w \Gammab_w^\top \Gammab_s \Gammab_s^\dagger \Gammab_w}} 
            &= \E_{\Scal_x}\sbr{\tr\rbr{ \Vb_s^\top \Vb_w \Gammab_w^\top \Gammab_s \Gammab_s^\dagger \Gammab_w \Vb_w^\top \Vb_s}} \\
            &= \E_{\Scal_x}\sbr{\tr\rbr{ \Vb_s^\top \Vb_w \Gammab_w^\top \Gammab_w \Vb_w^\top \Vb_s}} \\
            &= \tr\rbr{\Vb_s^\top \Vb_w \E_{\Scal_x}\sbr{\Gammab_w^\top \Gammab_w} \Vb_w^\top \Vb_s}.
        \end{align*}
        Since $\E_{\Scal_x}\sbr{\Gammab_w^\top \Gammab_w} = N \Ib_{d_w}$, we have
        \begin{align*}
            \E_{\Scal_x}\sbr{\tr\rbr{\Vb_w^\top \Vb_s \Vb_s^\top \Vb_w \Gammab_w^\top \Gammab_s \Gammab_s^\dagger \Gammab_w}} 
            &= N \tr\rbr{\Vb_s^\top \Vb_w \Vb_w^\top \Vb_s} \\
            &= N \nbr{\Vb_s^\top \Vb_w}_F^2 \\
            &= N d_{s \wedge w}.
        \end{align*}
        For the second term, we first observe that the row space of $\Gammab_w \Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top)$ is orthogonal to that of $\Gammab_s = \Gammab \Vb_s$, and therefore, $\Gammab_w \Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top)$ and $\Gammab_s$ are independent, which implies
        \begin{align*}
            \E_{\Scal_x}\sbr{\tr\rbr{\Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top) \Vb_w \Gammab_w^\top \Gammab_s \Gammab_s^\dagger \Gammab_w}} 
            &= \tr\rbr{\E\sbr{\Gammab_w \Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top) \Vb_w \Gammab_w^\top} \E\sbr{\Gammab_s \Gammab_s^\dagger}}.
        \end{align*}
        Since $\Gammab$ consists of independent isotropic rows, so do $\Gammab_s = \Gammab \Vb_s \in \R^{N \times d_s}$ and $\Gammab_w = \Gammab \Vb_w \in \R^{N \times d_w}$, which implies
        \begin{align*}
            \E\sbr{\Gammab_s \Gammab_s^\dagger} = \frac{d_s}{N}\ \Ib_N \quad \t{and} \quad \E\sbr{\Gammab_w^\top \Gammab_w} = N\ \Ib_{d_w}.
        \end{align*}
        Then, we have
        \begin{align*}
            \E_{\Scal_x}\sbr{\tr\rbr{\Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top) \Vb_w \Gammab_w^\top \Gammab_s \Gammab_s^\dagger \Gammab_w}} 
            &= \tr\rbr{\E\sbr{\Gammab_w \Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top) \Vb_w \Gammab_w^\top} \E\sbr{\Gammab_s \Gammab_s^\dagger}} \\
            &= \frac{d_s}{N} \tr\rbr{\E\sbr{\Gammab_w \Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top) \Vb_w \Gammab_w^\top}} \\
            &= \frac{d_s}{N} \tr\rbr{\Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top) \Vb_w \E\sbr{\Gammab_w^\top \Gammab_w}} \\
            &= \frac{d_s}{N} N \tr\rbr{\Vb_w^\top (\Ib_d - \Vb_s \Vb_s^\top) \Vb_w} \\
            &= d_s (d_w - d_{s \wedge w}).
        \end{align*}
        Combining the two terms, we have
        \begin{align*}
            \tr\rbr{\E_{\Scal_x}\sbr{\Gammab_w^\top \Pb_s \Gammab_w}} = N d_{s \wedge w} + d_s (d_w - d_{s \wedge w}).
        \end{align*}
        Then, by \eqref{eq:pf_var_w2s_1}, the variance is exactly characterized by
        \begin{align*}
            \vari(f_\wts) 
            &= \frac{\sigma^2}{N}\ \frac{N d_{s \wedge w} + d_s (d_w - d_{s \wedge w})}{n - d_w -1} \\
            &= \frac{\sigma^2}{n-d_w-1} \rbr{d_{s \wedge w} + \frac{d_s}{N} (d_w - d_{s \wedge w})}.
        \end{align*}

        \item \b{Sub-gaussian features}: Relaxing the Gaussian feature assumption, when $\wt\Gammab_w$ consists of $\iid$ sub-gaussian random vectors that are zero-mean and isotropic (\ie $\E[\wt\gammab_i] = \b0_{d_w}$ and $\E[\wt\gammab_i \wt\gammab_i^\top] = \Ib_{d_w}$), with $n \ge \Omega(d_w)$, \Cref{lem:trace_inv_subgaussian} implies that
        \begin{align*}
            \E_{\wt\Scal}\sbr{(\wt\Gammab_w^\top \wt\Gammab_w)^\dagger} \aleq O\rbr{\frac{1}{n}} \Ib_{d_w},
        \end{align*}
        and therefore,
        \begin{align*}
            \E_{\wt\Scal}\sbr{\rbr{\Sigmab_w^{-1/2} \wt\Phib_w^\top \wt\Phib_w \Sigmab_w^{-1/2}}^\dagger} \aleq O\rbr{\frac{1}{n}} \Vb_w \Vb_w^\top.
        \end{align*}
        Then, via an analogous argument as \eqref{eq:pf_var_w2s_1}, \eqref{eq:pf_var_w2s} implies that 
        \begin{align}\label{eq:pf_var_w2s_2}
        \begin{split}
            \vari(f_\wts) \le \frac{\sigma^2}{N}\ O\rbr{\frac{1}{n}}\ \tr\rbr{\E_{\Scal_x}\sbr{\Gammab_w^\top \Pb_s \Gammab_w}}.
        \end{split}
        \end{align}
        We observe that in the analysis of the Gaussian feature case, the characterization
        \begin{align*}
            \tr\rbr{\E_{\Scal_x}\sbr{\Gammab_w^\top \Pb_s \Gammab_w}} = (N - d_s) d_{s \wedge w} + d_s d_w
        \end{align*}
        does not involve the Gaussianity of $\Gammab$ and therefore holds for general subgaussian features.
        This leads to an upper bound on the variance:
        \begin{align*}
            \vari(f_\wts) 
            &\le \frac{\sigma^2}{N}\ O\rbr{\frac{1}{n}}\ \rbr{N d_{s \wedge w} + d_s (d_w - d_{s \wedge w})} \\
            &\lesssim \frac{\sigma^2}{n} \rbr{d_{s \wedge w} + \frac{d_s}{N} (d_w - d_{s \wedge w})}.
        \end{align*}
    \end{enumerate}
\end{proof}


\begin{lemma}[Adapting \cite{vershynin2010introduction} Theorem 5.39]\label{lem:trace_inv_subgaussian}
    Let $\wt\Gammab_w = [\wt\gammab_1, \ldots, \wt\gammab_n]^\top$ be an $n \times d_w$ matrix whose rows $\wt\gammab_1, \ldots, \wt\gammab_n$ consist of $\iid$ sub-gaussian random vectors that are zero-mean and isotropic (\ie $\E[\wt\gammab_i] = \b0_{d_w}$ and $\E[\wt\gammab_i \wt\gammab_i^\top] = \Ib_{d_w}$). When $n \ge \Omega(d_w)$, we have
    \begin{align*}
        \E\sbr{\nbr{\rbr{\wt\Gammab_w^\top \wt\Gammab_w}^\dagger}_2} \le O\rbr{\frac{1}{n}},
    \end{align*}
    where $\Omega(\cdot)$ and $O(\cdot)$ suppresses constants that depend only on the sub-gaussian norm $\nbr{\wt\gammab_i}_{\psi_2} = \sup_{\vb \in \SSS^{d_w-1}} \sup_{p \ge 1} (\E[|\wt\gammab_i^\top \vb|^p])^{1/p} / \sqrt{p}$, independent of $n, d_w$.
\end{lemma}

\begin{proof}[Proof of \Cref{lem:trace_inv_subgaussian}]
    Let $\sigma_{\min}(\wt\Gammab_w^\top \wt\Gammab_w)$ be the smallest singular value of $\wt\Gammab_w^\top \wt\Gammab_w$.
    Leveraging \cite{vershynin2010introduction} Theorem 5.39, we notice that for $n \ge \Omega(d_w)$, there exist constants $c_1, c_2 > 0$ that depend only on the sub-gaussian norm $\nbr{\wt\gammab_i}_{\psi_2}$ such that
    \begin{align*}
        \Pr\sbr{\sigma_{\min}(\wt\Gammab_w^\top \wt\Gammab_w) < \rbr{\sqrt{n} - c_1\sqrt{d_w} - t}^2} \le \exp\rbr{-c_2 t^2}.
    \end{align*}
    Therefore, we have 
    \begin{align*}
        \Pr\sbr{\frac{1}{\sigma_{\min}(\wt\Gammab_w^\top \wt\Gammab_w)} > t} \le \exp\rbr{-c_2 \rbr{\sqrt{n} - c_1 \sqrt{d_w} - \sqrt{\frac{1}{t}}}^2}.
    \end{align*}

    Notice that for any non-negative random variable $Z$ with a cumulative density function $F_Z(z)$, 
    \begin{align*}
        \E\sbr{Z} &= \int_0^\infty z d F_Z(z) 
        = - \int_0^\infty z d \rbr{1 - F_Z(z)} \\
        &= \sbr{z \rbr{1 - F_Z(z)}}_0^\infty + \int_0^\infty \rbr{1 - F_Z(z)} dz \\
        &= \int_0^\infty \Pr\sbr{Z > z} dz.
    \end{align*}
    Therefore, we have
    \begin{align*}
        \E\sbr{\frac{1}{\sigma_{\min}(\wt\Gammab_w^\top \wt\Gammab_w)}} \le \int_0^\infty \exp\rbr{-c_2 \rbr{\sqrt{n} - c_1 \sqrt{d_w} - \sqrt{\frac{1}{t}}}^2} d t.
    \end{align*}
    Let $t_0 = 1 / \rbr{\sqrt{n} - c_1 \sqrt{d_w}}^2$ such that $\sqrt{n} - c_1 \sqrt{d_w} - \sqrt{\frac{1}{t}}=0$ and 
    \begin{align*}
        \int_{0}^{t_0} \exp\rbr{-c_2 \rbr{\sqrt{n} - c_1 \sqrt{d_w} - \sqrt{\frac{1}{t}}}^2} d t \le t_0
    \end{align*}
    Then, we have
    \begin{align*}
        &\E\sbr{\frac{1}{\sigma_{\min}(\wt\Gammab_w^\top \wt\Gammab_w)}} 
        \le \int_0^\infty \exp\rbr{-c_2 \rbr{\sqrt{n} - c_1 \sqrt{d_w} - \sqrt{\frac{1}{t}}}^2} d t \\
        &\le t_0 + \int_{t_0}^\infty \exp\rbr{-c_2 \rbr{\sqrt{n} - c_1 \sqrt{d_w} - \sqrt{\frac{1}{t}}}^2} d t \\
        &= t_0 + 2 \int_{0}^{\sqrt{n}-c_1\sqrt{d_w}} \exp\rbr{-c_2 u^2} \rbr{\sqrt{n} - c_1 \sqrt{d_w} - u}^{-3} d u \\
        &= t_0 + \frac{2}{\rbr{\sqrt{n} - c_1 \sqrt{d_w}}^2} \int_{0}^{1} \exp\rbr{-c_2 \rbr{\sqrt{n}-c_1\sqrt{d_w}}^2 u^2} \rbr{1 - u}^{-3} d u \\
        &= \frac{1}{\rbr{\sqrt{n} - c_1 \sqrt{d_w}}^2} + \frac{2}{\rbr{\sqrt{n} - c_1 \sqrt{d_w}}^2} \rbr{\int_{0}^{1} \exp\rbr{-\Omega\rbr{u^2}} \rbr{1 - u}^{-3} d u} \\
        &= O\rbr{\frac{1}{\rbr{\sqrt{n} - c_1 \sqrt{d_w}}^2}}.
    \end{align*}
    When $n \ge \Omega(d_w)$, we have $\sqrt{n} - c_1 \sqrt{d_w} \ge \Omega(\sqrt{n})$, and therefore ,
    \begin{align*}
        \E\sbr{\nbr{\rbr{\wt\Gammab_w^\top \wt\Gammab_w}^\dagger}_2}
        \le \E\sbr{\frac{1}{\sigma_{\min}(\wt\Gammab_w^\top \wt\Gammab_w)}} 
        \le O\rbr{\frac{1}{n}}.
    \end{align*}
\end{proof}





\subsection{Proof of \Cref{pro:sft_weak} and \Cref{cor:sft_strong}}\label{apx:pf_sft_weak}
\begin{proof}[Proof of \Cref{pro:sft_weak} and \Cref{cor:sft_strong}]
    The excess risk of the finetuned weak teacher $f_w(\xb) = \phi_w(\xb)^\top \thetab_w$ can be expressed as
    \begin{align*}
        \exrisk(f_w) &= \E_{\xb \sim \Dcal}\sbr{\E_{f_w}\sbr{(f_w(\xb) - f_*(\xb))^2}} \\
        &= \E_{\wt\Scal}\sbr{\frac{1}{n}\nbr{\wt\Phib_w \thetab_w - \wt\fb_*}_2^2},
    \end{align*}
    where $\wt\fb_* = [\fb_*(\wt\xb_1), \ldots, \fb_*(\wt\xb_n)]^\top \in \R^n$; and we recall that $\wt\Phib_w = [\phi_w(\wt\xb_1), \ldots, \phi_w(\wt\xb_n)]^\top$. Notice that the randomness of $\thetab_w$ comes from the SFT samples $\wt\Scal \sim \Dcal(f_*)^n$.

    Observe that the solution of \eqref{eq:sft_weak} as $\alpha_w \to 0$ is given by $\thetab_w = \wt\Phib_w^\dagger \wt\yb$, where $\wt\yb = \wt\fb_* + \wt\zb$ is the noisy label vector with $\wt\zb \sim \Ncal(\b0_n, \sigma^2 \Ib_n)$.
    Therefore, with the randomness over $\wt\Scal \sim \Dcal(f_*)^n$, we have
    \begin{align*}
        \exrisk(f_w) &= \E \sbr{\frac{1}{n}\nbr{\wt\Phib_w \wt\Phib_w^\dagger \wt\yb - \wt\fb_*}_2^2} \\
        &= \E \sbr{\frac{1}{n}\nbr{\wt\Phib_w \wt\Phib_w^\dagger \wt\zb + \rbr{\wt\Phib_w \wt\Phib_w^\dagger \wt\fb_* - \wt\fb_*}}_2^2} \\
        &= \underbrace{\E \sbr{\frac{1}{n}\nbr{\wt\Phib_w \wt\Phib_w^\dagger \wt\zb}_2^2}}_{\vari(f_w)} + \underbrace{\E\sbr{\frac{1}{n}\nbr{\wt\Phib_w \wt\Phib_w^\dagger \wt\fb_* - \wt\fb_*}_2^2}}_{\bias(f_w)}.
    \end{align*}
    
    For bias, by the definition of finetuning capacity (see \Cref{def:ft_est_err}), we have
    \begin{align*}
        \bias(f_w) = \frac{1}{n} \E\sbr{\nbr{\wt\Phib_w \wt\Phib_w^\dagger \wt\fb_* - \wt\fb_*}_2^2} = \frac{\rho_w(n)}{n}.
    \end{align*}
    We observe that $\bias(f_w) \le \rho_w$ by \Cref{lem:low_est_err_ft}.
    Notice that \Cref{lem:low_est_err_ft} also implies $\bias(f_s) = {\rho_s(n)}/{n} \le \rho_s$. 

    For variance, we observe that 
    \begin{align*}
        \vari(f_w) &= \frac{1}{n} \E\sbr{\nbr{\wt\Phib_w \wt\Phib_w^\dagger \wt\zb}_2^2} \\
        &= \frac{1}{n} \E\sbr{\tr\rbr{\wt\Phib_w \wt\Phib_w^\dagger \wt\zb \wt\zb^\top}} \\
        &= \frac{\sigma^2}{n} \E\sbr{\tr\rbr{\wt\Phib_w \wt\Phib_w^\dagger}}.
    \end{align*}
    By \Cref{asm:ft_data}, since $\rank(\wt\Phib_w) = d_w$ almost surely, we have
    \begin{align*}
        \vari(f_w) = \frac{\sigma^2}{n} \E\sbr{\tr\rbr{\wt\Phib_w \wt\Phib_w^\dagger}} = \frac{\sigma^2 d_w}{n}.
    \end{align*}
\end{proof}



\subsection{Proof of \Cref{cor:pgr}}\label{apx:pf_pgr}
\begin{proof}[Proof of \Cref{cor:pgr}]
    Noticing that with $\rank(\wt\Phib_w) = d_w$ and $\rank(\wt\Phib_s) = \rank(\Phib_s) = d_s$ almost surely, the excess risks of $f_w, f_s, f_c$ are characterized exactly in \Cref{pro:sft_weak} and \Cref{cor:sft_strong}, and $\exrisk(f_\wts)$ is upper bounded by \Cref{thm:w2s_ft}.
    Therefore, by directly plugging in the excess risks to the definitions of PGR and OPR, we have
    \begin{align}\label{eq:pgr_lower_tight}
    \begin{split}
        \pgr = &\frac{\exrisk(f_w) - \exrisk(f_\wts)}{\exrisk(f_w) - \exrisk(f_c)} \\
        \ge &\rbr{\sigma^2\ \frac{d_w}{n} + \frac{\rho_w(n)}{n} - \frac{\sigma^2}{n-d_w-1} \rbr{d_{s \wedge w} + \frac{d_s}{N} (d_w-d_{s \wedge w})} - \rbr{\frac{\rho_w(n)}{n} + \frac{\rho_s(N)}{N}}} \\
        &\rbr{\sigma^2\ \frac{d_w}{n} + \frac{\rho_w(n)}{n} - \sigma^2\ \frac{d_s}{N+n} - \frac{\rho_s(N+n)}{N+n}}^{-1} \\
        \ge &\rbr{\sigma^2 \frac{d_w}{n} - \sigma^2 \frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}{n-d_w-1} - \frac{\rho_s(N)}{N}} \Big/ \rbr{\sigma^2 \frac{d_w}{n} + \frac{\rho_w(n)}{n}}, \\
        \ge &\rbr{\sigma^2 \frac{d_w}{n} - \sigma^2 \frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}{n-d_w-1} - \rho_s} \Big/ \rbr{\sigma^2 \frac{d_w}{n} + \rho_w},
    \end{split}
    \end{align}
    and 
    \begin{align}\label{eq:opr_lower_tight}
    \begin{split}
        \opr = &\frac{\exrisk(f_s)}{\exrisk(f_\wts)} \\
        \ge &\rbr{\sigma^2\ \frac{d_s}{n} + \frac{\rho_s(n)}{n}} \Big/ \rbr{\sigma^2 \frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}{n-d_w-1} + \rbr{\frac{\rho_w(n)}{n} + \frac{\rho_s(N)}{N}}} \\
        \ge &\sigma^2 \frac{d_s}{n} \Big/ \rbr{\sigma^2 \frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}{n-d_w-1} + \rho_w + \rho_s}.
    \end{split}
    \end{align} 

    When taking $n = d_w + q + 1$ for some small constant $q \in \N$, we observe that 
    \begin{align*}
        \pgr &\ge \rbr{\sigma^2 \frac{d_w}{n} - \sigma^2 \frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}{n-d_w-1} - \rho_s} \Big/ \rbr{\sigma^2 \frac{d_w}{n} + \rho_w} \\
        &\ge \rbr{\frac{d_w}{d_w + q + 1} - \frac{d_{s \wedge w}}{q} - \frac{d_s}{N} \frac{d_w - d_{s \wedge w}}{q} - \frac{\rho_s}{\sigma^2}} \Big/ \rbr{\frac{d_w}{d_w + q + 1} + \frac{\rho_w}{\sigma^2}} \\
        &\ge \rbr{\frac{d_w}{d_w + q + 1} - \frac{d_{s \wedge w}}{q} - \frac{d_s}{N} \frac{d_w - d_{s \wedge w}}{q} - \frac{\rho_s}{\sigma^2} - \frac{\rho_w}{\sigma^2}} \Big/ \rbr{\frac{d_w}{d_w + q + 1} + \frac{\rho_w}{\sigma^2} - \frac{\rho_w}{\sigma^2}} \\
        &= 1 - \frac{n}{d_w} \rbr{\frac{d_{s \wedge w}}{q} + \frac{d_s}{N} \frac{d_w - d_{s \wedge w}}{q} + \frac{\rho_w + \rho_s}{\sigma^2}} \\
        &= 1 - \frac{n}{q}\ {\frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) d_s / N}{d_w}} - \frac{n}{d_w}\ {\frac{\rho_w + \rho_s}{\sigma^2}},
    \end{align*}
    and
    \begin{align*}
        \opr &\ge \sigma^2 \frac{d_s}{n} \Big/ \rbr{\sigma^2 \frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}{n-d_w-1} + \rho_w + \rho_s} \\
        &= \frac{d_s}{n} \Big/ \rbr{\frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}{q} + \frac{\rho_w + \rho_s}{\sigma^2}} \\
        &= \rbr{\frac{n}{q}\ \frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}{d_s} + \frac{n}{d_s}\ \frac{\rho_w + \rho_s}{\sigma^2}}^{-1}.
    \end{align*}
\end{proof}



\subsection{Proof of \Cref{cor:non_monotonic_scaling}}\label{apx:pf_non_monotonic_scaling}
\begin{proof}[Proof of \Cref{cor:non_monotonic_scaling}]
    Recall the notations introduced for conciseness:
    \begin{align*}
        d_\wts(N) = d_{s \wedge w} + (d_w - d_{s \wedge w}) \frac{d_s}{N}, \quad \varrho = \frac{\rho_w + \rho_s}{\sigma^2}.
    \end{align*}
    Then, the lower bounds for $\pgr$ and $\opr$ in \Cref{cor:pgr} can be expressed in terms of $d_\wts(N)$ and $\varrho$ as 
    \begin{align*}
        \pgr \ge 1 - \frac{d_\wts(N)}{d_w} - \frac{d_w + 1}{d_w} \varrho - \frac{d_w + 1}{d_w}\ \frac{d_\wts(N)}{q} - q \frac{\varrho}{d_w},
    \end{align*}
    and 
    \begin{align*}
        \opr \ge \rbr{\frac{d_\wts(N)}{d_s} + \frac{d_w + 1}{d_s} \varrho + \frac{d_\wts(N)}{d_s}\ \frac{d_w + 1}{q} + q \frac{\varrho}{d_s}}^{-1}.
    \end{align*}
    Both lower bounds are maximized when the last two terms in the expressions that involve $q$ are minimized, which is achieved when $q = \sqrt{\rbr{d_w + 1} {d_\wts(N)}/{\varrho}}$. Substituting the optimal $q$ back into the expressions yields the best lower bounds for $\pgr$ and $\opr$:
    \begin{align*}
        \pgr \ge &1 - \frac{d_\wts(N)}{d_w} - \varrho \frac{d_w + 1}{d_w} - 2 \sqrt{\varrho \frac{d_w + 1}{d_w}\ \frac{d_\wts(N)}{d_w}} \\
        = &1 - \rbr{\sqrt{\frac{d_\wts(N)}{d_w}} + \sqrt{\varrho\ \frac{d_w + 1}{d_w}}}^2,
    \end{align*}
    and 
    \begin{align*}
        \opr \ge &\rbr{\frac{d_\wts(N)}{d_s} + \varrho \frac{d_w + 1}{d_s} + 2 \sqrt{\varrho \frac{d_w + 1}{d_s}\ \frac{d_\wts(N)}{d_s}}}^{-1} \\
        = &\rbr{\sqrt{\frac{d_\wts(N)}{d_s}} + \sqrt{\varrho\ \frac{d_w + 1}{d_s}}}^{-2}.
    \end{align*}
\end{proof}




\section{Ridge regression analysis}\label{apx:ridge_regression}
In this section, we investigate the more realistic scenario where the weak and strong feature covariances are not exactly low-rank but admit small numbers of dominating eigenvalues. 

Concretely, we consider the same data distribution $(\xb, y) \sim \Dcal(f_*)$ with $y = f_*(\xb) + z$ for some independent Gaussian label noise $z \sim \Ncal(0, \sigma^2)$ and an unknown ground truth function $f_*: \Xcal \to \R$ as in \Cref{sec:ridgeless_regression}.
Under the same sub-gaussian feature assumption as in \Cref{asm:features}, we adapt \Cref{def:low_intrinsic_dim,def:correlation_dim} to the ridge regression setting as follows.
\begin{assumption}[Data distribution]\label{asm:ridge_regression}
    Let $\phi_s: \Xcal \to \R^d$ and $\phi_w: \Xcal \to \R^d$ be the strong and weak pretrained models that take $\xb \sim \Dcal$ and output pretrained features $\phi_s(\xb), \phi_w(\xb) \in \R^d$, respectively.
    \begin{enumerate}[label=(\roman*)]
        \item \b{Ground truth}: Assume $f_*$ can be expressed as a linear function over an unknown ground truth feature $\phi_*: \Xcal \to \R^d$ such that $f_*(\cdot) = \phi_*(\cdot)^\top \thetab_*$ for some fixed $\thetab_* \in \R^d$.
        \item \b{Sub-gaussian features} (\Cref{asm:features}): Let $\phi_w(\xb)$, $\phi_s(\xb)$, $\phi_*(\xb)$ be zero-mean sub-gaussian random vectors with $\E[\phi_w(\xb)] = \E[\phi_s(\xb)] = \E[\phi_*(\xb)] = \b{0}_d$, and 
        \begin{align*}
            \E[\phi_w(\xb) \phi_w(\xb)^\top] = \Sigmab_w, \quad \E[\phi_s(\xb) \phi_s(\xb)^\top] = \Sigmab_s, \quad \E[\phi_*(\xb) \phi_*(\xb)^\top] = \Sigmab_*.
        \end{align*}
        For conciseness, we assume without loss of generality that these features are roughly normalized, \ie, $\nbr{\Sigmab_w}_2 \asymp 1$, $\nbr{\Sigmab_s}_2 \asymp 1$, and $\nbr{\Sigmab_*}_2 \asymp 1$.
        \item \b{Low intrinsic dimension}: Let $\Sigmab_s$ and $\Sigmab_w$ both be \b{positive-definite} with spectral decompositions $\Sigmab_s = \Vb_s \Lambdab_s \Vb_s^\top$ and $\Sigmab_w = \Vb_w \Lambdab_w \Vb_w^\top$, where $\Lambdab_s, \Lambdab_w \in \R^{d \times d}$ are diagonal matrices with positive eigenvalues in decreasing order; while $\Vb_s \in \R^{d \times d}$ and $\Vb_w \in \R^{d \times d}$ are orthogonal matrices consisting of the corresponding orthonormal eigenvectors. The low intrinsic dimension of FT implies that $\Lambdab_s = \diag(\lambda^s_1,\cdots,\lambda^s_d)$ and $\Lambdab_w = \diag(\lambda^w_1,\cdots,\lambda^w_d)$ consist of a few dominating eigenvalues, while the rest of the eigenvalues are negligible, \ie, there exist $d_s, d_w \ll d$ such that $\sum_{i > d_s} \lambda^s_i \ll \tr(\Sigmab_s)$ and $\sum_{i > d_w} \lambda^w_i \ll \tr(\Sigmab_w)$. Here, 
        \begin{align*}
            \tr(\Sigmab_s) \lesssim d_s \quad \t{and} \quad \tr(\Sigmab_w) \lesssim d_w
        \end{align*}
        effectively measure the intrinsic dimensions of $\phi_s$ and $\phi_w$.
    \end{enumerate}
\end{assumption}

\begin{remark}[Weak-strong similarity]
    In place of correlation dimension (\Cref{def:correlation_dim}) in the ridgeless setting, for the ridge regression analysis, we measure the similarity between the weak and strong models directly through $\tr(\Sigmab_s \Sigmab_w)$. Notice that 
    \begin{align*}
        \tr(\Sigmab_s \Sigmab_w) \le \min\cbr{\tr(\Sigmab_s)\nbr{\Sigmab_w}_2, \tr(\Sigmab_w)\nbr{\Sigmab_s}_2} \lesssim \min\cbr{\tr(\Sigmab_s), \tr(\Sigmab_w)}.
    \end{align*}
    In particular, when $\Sigmab_s$ and $\Sigmab_w$ admit low intrinsic dimensions, $\tr(\Sigmab_s \Sigmab_w)$ can be much smaller than $\min\cbr{\tr(\Sigmab_s), \tr(\Sigmab_w)}$ if their eigenvectors corresponding to the dominating eigenvalues are almost orthogonal.
\end{remark}

\begin{remark}[FT approximation errors]
    It is worth noting that under the ground truth and positive-definite covariance assumptions in \Cref{asm:ridge_regression}(i, iii), the FT approximation errors in \Cref{def:ft_est_err} satisfy
    \begin{align}\label{eq:pf_ridge_ft_approx_err}
    \begin{split}
        &\rho_s = \min_{\thetab \in \R^d} \E_{\xb \sim \Dcal}\sbr{(\phi_s(\xb)^\top \thetab - f_*(\xb))^2} = 0 \quad (\t{when } \thetab = \Sigmab_s^{-1} \Sigmab_* \thetab_*), \\
        &\rho_w = \min_{\thetab \in \R^d} \E_{\xb \sim \Dcal}\sbr{(\phi_w(\xb)^\top \thetab - f_*(\xb))^2} = 0 \quad (\t{when } \thetab = \Sigmab_w^{-1} \Sigmab_* \thetab_*).
    \end{split}
    \end{align}
    In place of \Cref{def:ft_est_err}, with positive-definite covariances in \Cref{asm:ridge_regression}, we measure the alignment between the ground truth feature $\phi_*$ and the weak/strong feature $\phi_w, \phi_s$ through
    \begin{align*}
        \varrho_s = \|\Sigmab_s^{-1/2} \Sigmab_*^{1/2} \thetab_*\|_2^2, \quad \varrho_w = \|\Sigmab_w^{-1/2} \Sigmab_*^{1/2} \thetab_*\|_2^2.
    \end{align*}
    Intuitively, for $\Sigmab_s$ and $\Sigmab_w$ with a few dominating eigenvalues (\Cref{asm:ridge_regression}(iii)), $\varrho_s$ and $\varrho_w$ are small if the eigensubspace associated with non-negligible eigenvalues of $\Sigmab_*$ is fully covered by the eigensubspaces associated with the dominating eigenvalues of $\Sigmab_s$ and $\Sigmab_w$, respectively. 
\end{remark}

The W2S FT under ridge regression consists of two steps.
\begin{enumerate}[label=(\alph*)]
    \item First, the weak teacher $f_w(\xb) = \phi_w(\xb)^\top \thetab_w$ is supervisedly finetuned over $\wt\Scal$: 
    \begin{align}\label{eq:w2s_weak_ridge}
        \thetab_w = \argmin_{\thetab \in \R^d} \frac{1}{n}\nbr{\wt\Phib_w \thetab - \wt\yb}_2^2 + \alpha_w \nbr{\thetab}_2^2, \quad \alpha_w > 0.
    \end{align}
    \item Then, the W2S model $f_\wts(\xb) = \phi_s(\xb)^\top \thetab_\wts$ is finetuned over the strong feature $\phi_s$ through the unlabeled samples $\Scal_x$ and their pseudo-labels generated by the weak teacher model:
    \begin{align}\label{eq:w2s_strong_ridge}
        \thetab_\wts = \argmin_{\thetab \in \R^d} \frac{1}{N}\nbr{\Phib_s \thetab - \Phib_w \thetab_w}_2^2 + \alpha_\wts \nbr{\thetab}_2^2, \quad \alpha_\wts > 0.
    \end{align}
\end{enumerate}

\begin{theorem}[W2S under ridge regression]\label{thm:w2s_ridge}
    Let $\varrho_w = \nbr{\Sigmab_w^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2$ and $\varrho_s = \nbr{\Sigmab_s^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2$.
    Under \Cref{asm:ridge_regression}, the generalization error of W2S FT via ridge regression with fixed $\alpha_w, \alpha_\wts > 0$, $\exrisk(f_\wts) = \vari(f_\wts) + \bias(f_\wts)$, is upper bounded by
    \begin{align*}
        \vari(f_\wts) \le \frac{\sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w}}{4 (\alpha_w n) (\alpha_\wts N)}, \quad
        \bias(f_\wts) \le \alpha_w \varrho_w + \alpha_\wts \varrho_s.
    \end{align*}
    In particular, when taking  
    \begin{align*}
        \alpha_w = \rbr{\frac{\sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w}}{4 n N}\ \frac{\varrho_s}{\varrho_w^2}}^{1/3}, \quad 
        \alpha_\wts = \rbr{\frac{\sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w}}{4 n N}\ \frac{\varrho_w}{\varrho_s^2}}^{1/3},
    \end{align*}
    the excess risk of W2S FT is upper bounded by
    \begin{align*}
        \exrisk(f_\wts) \le 3 \rbr{\frac{\sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w}}{4 n N}\ \varrho_s \varrho_w}^{1/3}.
    \end{align*}
\end{theorem}

\Cref{thm:w2s_ridge} conveys a similar high-level intuition as in \Cref{thm:w2s_ft} regarding the effect of the weak-strong similarity on the generalization error of W2S FT. In particular, the larger discrepancy between $\phi_s$ and $\phi_w$ (corresponding to the smaller $\tr\rbr{\Sigmab_s \Sigmab_w}$) leads to lower variance and therefore better W2S generalization.

Meanwhile, a key difference in W2S between the ridge and ridgeless settings (\Cref{thm:w2s_ridge} versus \Cref{thm:w2s_ft}) is that the FT approximation errors in \Cref{thm:w2s_ridge}, reflected by $\varrho_s = \|\Sigmab_s^{-1/2} \Sigmab_*^{1/2} \thetab_*\|_2^2$ and $\varrho_w = \|\Sigmab_w^{-1/2} \Sigmab_*^{1/2} \thetab_*\|_2^2$, can be compensated by larger sample sizes $n, N$ and directly affect the sample complexity: 
\begin{align*}
    n N \asymp \sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w} \varrho_s \varrho_w.
\end{align*}
Such difference is a result of optimizing the regularization hyperparameters $\alpha_w, \alpha_\wts$ in ridge regression that control the variance-bias tradeoff.

\begin{proof}[Proof of \Cref{thm:w2s_ridge}]
    We first formalize some useful facts on the features and labels as in \eqref{eq:pf_var_w2s_subgaussian_asm}.
    In particular, the sub-gaussian assumption in \Cref{asm:ridge_regression}(ii) implies that for each $\xb \sim \Dcal$, the corresponding strong/weak feature $\phi_s(\xb), \phi_w(\xb) \in \R^d$, and the ground truth $f_*(\xb) \in \R$ are simultaneously characterized by an independent subgaussian random vector $\gammab \in \R^d$ with $\E[\gammab] = \b0_{d}$ and $\E[\gammab \gammab^\top] = \Ib_{d}$, \ie,
    \begin{align*}
        \phi_s(\xb) = \Sigmab_s^{1/2} \gammab, \quad \phi_w(\xb) = \Sigmab_w^{1/2} \gammab, \quad f_*(\xb) = \phi_*(\xb)^\top \thetab_* = \gammab^\top \Sigmab_*^{1/2} \thetab_*.
    \end{align*}

    Then, for $\Scal$ and $\wt\Scal$, there exist independent random matrices $\Gammab = [\gammab_1, \ldots, \gammab_N]^\top \in \R^{N \times d}$ and $\wt\Gammab = [\wt\gammab_1, \ldots, \wt\gammab_n]^\top \in \R^{n \times d}$ consisting of $\iid$ zero-mean isotropic rows such that
    \begin{align}\label{eq:pf_var_w2s_subgaussian_asm_2}
    \begin{split}
        &\Phib_s = \Gammab \Sigmab_s^{1/2} = \Gammab_s \Lambdab_s^{1/2} \Vb_s^\top, \\
        &\Phib_w = \Gammab \Sigmab_w^{1/2} = \Gammab_w \Lambdab_w^{1/2} \Vb_w^\top, \\
        &\yb = \fb_* + \zb, \quad \fb_* = \Gammab \Sigmab_*^{1/2} \thetab_*, \quad \zb \sim \Ncal(\b0_N, \sigma^2 \Ib_N), \\
        &\wt\Phib_w = \wt\Gammab \Sigmab_w^{1/2} = \wt\Gammab_w \Lambdab_w^{1/2} \Vb_w^\top, \\
        &\wt\yb = \wt\fb_* + \wt\zb, \quad \wt\fb_* = \wt\Gammab \Sigmab_*^{1/2} \thetab_*, \quad \wt\zb \sim \Ncal(\b0_n, \sigma^2 \Ib_n),
    \end{split}
    \end{align}
    where $\Gammab_s = \Gammab \Vb_s$, $\Gammab_w = \Gammab \Vb_w$, and $\wt\Gammab_w = \wt\Gammab \Vb_w$.

    \paragraph{Variance-bias decomposition.}
    Recall that the excess risk of W2S generalization $\exrisk(f_\wts)$ can be decomposed into the variance and bias terms:
    \begin{align*}
        &\vari(f_\wts) = \E_{\xb \sim \Dcal}\sbr{\E_{\Scal_x, \wt\Scal}\sbr{(f_\wts(\xb) - \E_{\Scal_x, \wt\Scal}[f_\wts(\xb)])^2}}, \\
        &\bias(f) = \E_{\xb \sim \Dcal}\sbr{(\E_{\Scal_x, \wt\Scal}[f_\wts(\xb)] - f_*(\xb))^2}.
    \end{align*}
    With $\alpha_w > 0$, \eqref{eq:w2s_weak_ridge} yields a weak teacher model $f_w(\xb) = \phi_w(\xb)^\top \thetab_w$ with 
    \begin{align*}
        \thetab_w = \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \rbr{\wt\fb_8 + \wt\zb}.
    \end{align*}
    Then, the W2S model $f_\wts(\xb) = \phi_s(\xb)^\top \thetab_\wts$ is given by \eqref{eq:w2s_strong_ridge} with $\alpha_\wts > 0$:
    \begin{align*}
        \thetab_\wts = &\rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Phib_w \thetab_w \\
        = &\rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Phib_w \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \rbr{\wt\fb_* + \wt\zb},
    \end{align*}
    which implies
    \begin{align*}
        \E_{\Scal_x, \wt\Scal}[\thetab_\wts] = \rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Phib_w \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\fb_*.
    \end{align*}
    Then, we can concretize the variance and bias terms as:
    \begin{align}\label{eq:pf_ridge_var}
    \begin{split}
        &\vari(f_\wts) = \E_{\xb \sim \Dcal}\sbr{\E_{\Scal_x, \wt\Scal}\sbr{(f_\wts(\xb) - \E_{\Scal_x, \wt\Scal}[f_\wts(\xb)])^2}} \\
        = &\E_{\Scal_x, \wt\Scal}\sbr{\nbr{\Sigmab_s^{1/2} \rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Phib_w \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\zb}_2^2},
    \end{split}
    \end{align}
    and
    \begin{align}\label{eq:pf_ridge_bias}
    \begin{split}
        &\bias(f_\wts) = \E_{\xb \sim \Dcal}\sbr{(\E_{\Scal_x, \wt\Scal}[f_\wts(\xb)] - f_*(\xb))^2} \\
        = &\E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Phib_s \rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Phib_w \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\fb_* - \fb_*}_2^2}.
    \end{split}
    \end{align}
    Now, we are ready to upper bound the variance and bias terms separately.

    \paragraph{Variance.}
    Denote $\zetab = \Lambdab_w^{1/2} \Vb_w^\top \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\zb \in \R^d$, whose randomness comes from $\wt\Scal$ only, independent of $\Scal_x$.
    Then, the variance term \eqref{eq:pf_ridge_var} can be expressed as
    \begin{align*}
        &\vari(f_\wts) = \E_{\Scal_x, \wt\Scal}\sbr{\nbr{\Sigmab_s^{1/2} \rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Phib_w \zetab}_2^2} \\
        = &\tr\rbr{\E_{\Scal_s}\rbr{\Gammab_w^\top \Phib_s \rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Sigmab_s \rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Gammab_w} \E_{\wt\Scal}\sbr{\zetab \zetab^\top}} \\
        = &\tr\rbr{\E_{\Scal_s}\rbr{\Gammab_w^\top \Gammab_s \rbr{\Gammab_s^\top \Gammab_s + \alpha_\wts N \Lambdab_s^{-1}}^{-1} \rbr{\Gammab_s^\top \Gammab_s + \alpha_\wts N \Lambdab_s^{-1}}^{-1} \Gammab_s^\top \Gammab_w} \E_{\wt\Scal}\sbr{\zetab \zetab^\top}} \\
        = &\tr\rbr{\E_{\Scal_s}\rbr{\Vb_w^\top \Gammab^\top \Gammab \rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{-2} \Gammab^\top \Gammab \Vb_w} \E_{\wt\Scal}\sbr{\zetab \zetab^\top}} \\
        = &\tr\rbr{\E_{\Scal_s}\rbr{\Gammab^\top \Gammab \rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{-2} \Gammab^\top \Gammab} \E_{\wt\Scal}\sbr{\Vb_w \zetab \zetab^\top \Vb_w^\top}}.
    \end{align*}
    Notice that $\rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{2} \succeq \alpha_\wts N \rbr{\Gammab^\top \Gammab \Sigmab_s^{-1} + \Sigmab_s^{-1} \Gammab^\top \Gammab}$.
    Since matrix inversion is convex, a Jensen-type inequality implies that
    \begin{align*}
        &\Gammab^\top \Gammab \rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{-2} \Gammab^\top \Gammab \\
        \preceq &\Gammab^\top \Gammab \rbr{\alpha_\wts N \rbr{\Gammab^\top \Gammab \Sigmab_s^{-1} + \Sigmab_s^{-1} \Gammab^\top \Gammab}}^{\dagger} \Gammab^\top \Gammab \\
        = &\frac{1}{2 \alpha_\wts N} \Gammab^\top \Gammab \rbr{\frac{1}{2} \rbr{\Gammab^\top \Gammab \Sigmab_s^{-1} + \Sigmab_s^{-1} \Gammab^\top \Gammab}}^{\dagger} \Gammab^\top \Gammab \\
        \preceq &\frac{1}{4 \alpha_\wts N} \rbr{\Gammab^\top \Gammab \Sigmab_s + \Sigmab_s \Gammab^\top \Gammab}.
    \end{align*}
    Therefore, 
    \begin{align*}
        \E_{\Scal_s}\rbr{\Gammab^\top \Gammab \rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{-2} \Gammab^\top \Gammab}
        \preceq &\frac{1}{4 \alpha_\wts N} \E_{\Scal_s}\sbr{\Gammab^\top \Gammab \Sigmab_s + \Sigmab_s \Gammab^\top \Gammab} 
        = \frac{1}{2 \alpha_\wts N} \Sigmab_s.
    \end{align*}
    Meanwhile, we observe that
    \begin{align*}
        \E_{\wt\Scal}\sbr{\Vb_w \zetab \zetab^\top \Vb_w^\top} 
        = &\E_{\wt\Scal}\sbr{\Sigmab_w^{1/2} \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\zb \wt\zb^\top \wt\Phib_w \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \Sigmab_w^{1/2}} \\
        = &\sigma^2 \E_{\wt\Scal}\sbr{\Sigmab_w^{1/2} \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\Phib_w \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \Sigmab_w^{1/2}},
    \end{align*}
    where 
    \begin{align*}
        \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\Phib_w \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1}
        \preceq &\frac{1}{2 \alpha_w n} \Ib_d.
    \end{align*}
    Therefore, we have
    \begin{align*}
        \E_{\wt\Scal}\sbr{\Vb_w \zetab \zetab^\top \Vb_w^\top} 
        \preceq &\sigma^2 \E_{\wt\Scal}\sbr{\Sigmab_w^{1/2} \rbr{\frac{1}{2 \alpha_w n} \Ib_d} \Sigmab_w^{1/2}}
        = \frac{\sigma^2}{2 \alpha_w n} \Sigmab_w.
    \end{align*}
    Overall, the variance of $f_\wts$ can be upper bounded as
    \begin{align}\label{eq:pf_ridge_var_ub}
    \begin{split}
        \vari(f_\wts) 
        = &\tr\rbr{\E_{\Scal_s}\rbr{\Gammab^\top \Gammab \rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{-2} \Gammab^\top \Gammab} \E_{\wt\Scal}\sbr{\Vb_w \zetab \zetab^\top \Vb_w^\top}} \\
        \le &\frac{\sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w}}{4 (\alpha_w n) (\alpha_\wts N)}.
    \end{split}
    \end{align}

    \paragraph{Bias.}
    Let $\xib = \Sigmab_w^{1/2} \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\fb_* \in \R^d$, whose randomness comes from $\wt\Scal$ only, independent of $\Scal_x$.
    Recall from \eqref{eq:pf_ridge_bias}, the bias term \eqref{eq:pf_ridge_bias} can be decomposed as
    \begin{align*}
        &\bias(f_\wts) = \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Phib_s \rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Phib_w \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\fb_* - \fb_*}_2^2}\\
        &= \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \rbr{\nbr{\Phib_s \rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Gammab \xib - \Phib_s \Phib_s^\dagger \fb_*}_2^2 + \nbr{\rbr{\Ib_N - \Phib_s \Phib_s^\dagger} \fb_*}_2^2}},
    \end{align*}
    where by \Cref{lem:low_est_err_ft} and \eqref{eq:pf_ridge_ft_approx_err}
    \begin{align*}
        \E_{\Scal_x}\sbr{\frac{1}{N} \nbr{\rbr{\Ib_N - \Phib_s \Phib_s^\dagger} \fb_*}_2^2}
        = \frac{\rho_s(N)}{N} \le \rho_s = 0.
    \end{align*}
    Therefore, with $\xib = \Sigmab_w^{1/2} \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\fb_*$, we have
    \begin{align*}
        \bias(f_\wts) = \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Phib_s \rbr{\Phib_s^\top \Phib_s + \alpha_\wts N \Ib_d}^{-1} \Phib_s^\top \Gammab \xib - \Phib_s \Phib_s^\dagger \fb_*}_2^2}.
    \end{align*}
    Recall that $\fb_* = \Gammab \Sigmab_*^{1/2} \thetab_*$ and $\Phib_s = \Gammab \Sigmab_s^{1/2} = \Gammab_s \Lambdab_s^{1/2} \Vb_s^\top$.
    Then, we can express the bias term as
    \begin{align*}
        \bias(f_\wts) = &\E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Gammab\rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{-1} \Gammab^\top \Gammab \xib - \Gammab \Gammab^\dagger \fb_*}_2^2} \\
        = &\E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Gammab \Sigmab_*^{1/2} \thetab_* - \Gammab\rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{-1} \Gammab^\top \Gammab \xib}_2^2} \\
        = &\E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Gammab \rbr{\Sigmab_*^{1/2} \thetab_* - \xib} + \Gammab \rbr{\Ib_d - \rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{-1} \Gammab^\top \Gammab} \xib}_2^2} \\
    \end{align*} 
    By Woodbury matrix identity, we have
    \begin{align}\label{eq:pf_ridge_bias_woodbury}
        \Ib_d - \rbr{\Gammab^\top \Gammab + \alpha_\wts N \Sigmab_s^{-1}}^{-1} \Gammab^\top \Gammab
        = \rbr{\Ib_d + \frac{1}{\alpha_\wts N} \Sigmab_s \Gammab^\top \Gammab}^{-1}.
    \end{align}
    Therefore, we have 
    \begin{align}\label{eq:pf_ridge_bias_inter1}
        \bias(f_\wts) = \E_{\Scal_x, \wt\Scal}\Bigg[\frac{1}{N} \Big\|\underbrace{\Gammab \rbr{\Sigmab_*^{1/2} \thetab_* - \xib}}_{\t{Term A}} + \underbrace{\Gammab \rbr{\Ib_d + \frac{1}{\alpha_\wts N} \Sigmab_s \Gammab^\top \Gammab}^{-1} \xib}_{\t{Term B}}\Big\|_2^2\Bigg].
    \end{align}

    For Term A, notice that $\xib = \Sigmab_w^{1/2} \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\fb_*$ implies
    \begin{align*}
        \Sigmab_*^{1/2} \thetab_* - \xib 
        = &\Sigmab_*^{1/2} \thetab_* - \Sigmab_w^{1/2} \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\fb_* \\
        = &\Sigmab_*^{1/2} \thetab_* - \rbr{\wt\Gammab^\top \wt\Gammab + \alpha_w n \Sigmab_w^{-1}}^{-1} \wt\Gammab^\top \wt\Gammab \Sigmab_*^{1/2} \thetab_* \\
        = &\rbr{\Ib_d - \rbr{\wt\Gammab^\top \wt\Gammab + \alpha_w n \Sigmab_w^{-1}}^{-1} \wt\Gammab^\top \wt\Gammab} \Sigmab_*^{1/2} \thetab_* \\
        = &\rbr{\Ib_d + \frac{1}{\alpha_w n} \Sigmab_w \wt\Gammab^\top \wt\Gammab}^{-1} \Sigmab_*^{1/2} \thetab_*,
    \end{align*}
    where the last equality follows from Woodbury matrix identity as in \eqref{eq:pf_ridge_bias_woodbury}.
    Therefore,
    \begin{align*}
        \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Gammab \rbr{\Sigmab_*^{1/2} \thetab_* - \xib}}_2^2} 
        = &\E_{\wt\Scal}\sbr{\frac{1}{n} \nbr{\wt\Gammab \rbr{\Sigmab_*^{1/2} \thetab_* - \xib}}_2^2} \\
        = &\E_{\wt\Scal}\sbr{\frac{1}{n} \nbr{\wt\Gammab \rbr{\Ib_d + \frac{1}{\alpha_w n} \Sigmab_w \wt\Gammab^\top \wt\Gammab}^{-1} \Sigmab_*^{1/2} \thetab_*}_2^2}.
    \end{align*}
    Since 
    \begin{align*}
        \rbr{\Ib_d + \frac{1}{\alpha_w n} \Sigmab_w \wt\Gammab^\top \wt\Gammab}^{-1} \wt\Gammab^\top \wt\Gammab \rbr{\Ib_d + \frac{1}{\alpha_w n} \Sigmab_w \wt\Gammab^\top \wt\Gammab}^{-1} \preceq \frac{\alpha_w n}{2} \Sigmab_w^{-1},
    \end{align*}
    we have
    \begin{align}\label{eq:pf_ridge_bias_term1}
    \begin{split}
        \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Gammab \rbr{\Sigmab_*^{1/2} \thetab_* - \xib}}_2^2} 
        \le &\frac{1}{n} \tr\rbr{\frac{\alpha_w n}{2} \Sigmab_w^{-1} \Sigmab_*^{1/2} \thetab_* \thetab_*^\top \Sigmab_*^{1/2}} \\
        = &\frac{\alpha_w}{2} \nbr{\Sigmab_w^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2.
    \end{split}
    \end{align}
    
    For Term B, leveraging Woodbury matrix identity as in \eqref{eq:pf_ridge_bias_woodbury}, we notice that 
    \begin{align*}
        &\E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Gammab \rbr{\Ib_d + \frac{1}{\alpha_\wts N} \Sigmab_s \Gammab^\top \Gammab}^{-1} \xib}_2^2} 
        \le \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \tr\rbr{\frac{\alpha_\wts N}{2} \Sigmab_s^{-1} \xib \xib^\top}} \\
        = &\frac{\alpha_\wts}{2} \E_{\Scal_x, \wt\Scal}\sbr{\nbr{\Sigmab_s^{-1/2} \Sigmab_w^{1/2} \rbr{\wt\Phib_w^\top \wt\Phib_w + \alpha_w n \Ib_d}^{-1} \wt\Phib_w^\top \wt\fb_*}_2^2} \\
        = &\frac{\alpha_\wts}{2} \E_{\Scal_x, \wt\Scal}\sbr{\nbr{\Sigmab_s^{-1/2} \rbr{\wt\Gammab^\top \wt\Gammab + \alpha_w n \Sigmab_w^{-1}}^{-1} \wt\Gammab^\top \wt\Gammab \Sigmab_*^{1/2} \thetab_*}_2^2}
    \end{align*}
    Since $\rbr{\wt\Gammab^\top \wt\Gammab + \alpha_w n \Sigmab_w^{-1}}^{-1} \wt\Gammab^\top \wt\Gammab \preceq \Ib_d$, we know that
    \begin{align}\label{eq:pf_ridge_bias_term2}
    \begin{split}
        \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Gammab \rbr{\Ib_d + \frac{1}{\alpha_\wts N} \Sigmab_s \Gammab^\top \Gammab}^{-1} \xib}_2^2} 
        \le \frac{\alpha_\wts}{2} \nbr{\Sigmab_s^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2.
    \end{split}
    \end{align}
    Combining \eqref{eq:pf_ridge_bias_inter1}, \eqref{eq:pf_ridge_bias_term1}, and \eqref{eq:pf_ridge_bias_term2}, we can upper bound the bias term as
    \begin{align}\label{eq:pf_ridge_bias_final}
    \begin{split}
        &\bias(f_\wts) = \E_{\Scal_x, \wt\Scal}\Bigg[\frac{1}{N} \Big\|\underbrace{\Gammab \rbr{\Sigmab_*^{1/2} \thetab_* - \xib}}_{\t{Term A}} + \underbrace{\Gammab \rbr{\Ib_d + \frac{1}{\alpha_\wts N} \Sigmab_s \Gammab^\top \Gammab}^{-1} \xib}_{\t{Term B}}\Big\|_2^2\Bigg] \\
        \le &2 \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Gammab \rbr{\Sigmab_*^{1/2} \thetab_* - \xib}}_2^2} + 2 \E_{\Scal_x, \wt\Scal}\sbr{\frac{1}{N} \nbr{\Gammab \rbr{\Ib_d + \frac{1}{\alpha_\wts N} \Sigmab_s \Gammab^\top \Gammab}^{-1} \xib}_2^2} \\
        \le &\alpha_w \nbr{\Sigmab_w^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2 + \alpha_\wts \nbr{\Sigmab_s^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2.
    \end{split}
    \end{align}
    
    \paragraph{Variance-bias tradeoff.}
    Overall, by \eqref{eq:pf_ridge_var_ub} and \eqref{eq:pf_ridge_bias_final}, we have
    \begin{align*}
        &\vari(f_\wts) \le \frac{\sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w}}{4 (\alpha_w n) (\alpha_\wts N)}, \\
        &\bias(f_\wts) \le \alpha_w \nbr{\Sigmab_w^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2 + \alpha_\wts \nbr{\Sigmab_s^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2.
    \end{align*}
    The upper bound the excess risk $\exrisk(f_\wts) = \vari(f_\wts) + \bias(f_\wts)$ is minimized by taking 
    \begin{align*}
        \alpha_w = \rbr{\frac{\sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w}}{4 n N}\ \frac{\nbr{\Sigmab_s^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2}{\nbr{\Sigmab_w^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^4}}^{1/3}, \ 
        \alpha_\wts = \rbr{\frac{\sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w}}{4 n N}\ \frac{\nbr{\Sigmab_w^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2}{\nbr{\Sigmab_s^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^4}}^{1/3},
    \end{align*}
    which leads to the optimal upper bound for the excess risk:
    \begin{align*}
        \exrisk(f_\wts) \le 3 \rbr{\frac{\sigma^2 \tr\rbr{\Sigmab_s \Sigmab_w}}{4 n N}\ \nbr{\Sigmab_s^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2 \nbr{\Sigmab_w^{-1/2} \Sigmab_*^{1/2} \thetab_*}_2^2}^{1/3}.
    \end{align*}
\end{proof}






\section{Canonical angles}\label{apx:canonical_angles}
In this section, we review the concept of canonical angles between two subspaces that connect the formal definition of the correlation dimension $d_{s \wedge w} = \nbr{\Vb_s^\top \Vb_w}_F^2$ in \Cref{def:correlation_dim} to the intuitive notion of the alignment between the corresponding subspaces $\Vcal_s$ and $\Vcal_w$ in the introduction: $\sum \cos(\angle(\Vcal_s, \Vcal_w)) = \nbr{\Vb_s^\top \Vb_w}_F^2$.
\begin{definition}[Canonical angles \cite{golub2013matrix}, adapting from \cite{dong2024efficient}]\label{def:canonical_angles}
    Let $\Vcal_s,\Vcal_w \subseteq \R^d$ be two subspaces with dimensions $\dim\rbr{\Vcal_s}=d_s$ and $\dim\rbr{\Vcal_w}=d_w$ (assuming $d_w \geq d_s$ without loss of generality). The canonical angles $\angle\rbr{\Vcal_s,\Vcal_w}=\diag\rbr{\nu_1,\dots,\nu_{d_s}}$ are $d_s$ angles that jointly measure the alignment between $\Vcal_s$ and $\Vcal_w$, defined recursively as follows:
    \begin{align*}
        &\ub_i, \vb_i ~\triangleq~
        \argmax~\ub_i^*\vb_i \\
        \t{s.t.}~
        &\ub_i \in \rbr{\Vcal_s \setminus \spn\cbr{\ub_{\iota}}_{\iota=1}^{i-1}} \cap \SSS^{d-1},\\ 
        &\vb_i \in \rbr{\Vcal_w \setminus \spn\cbr{\vb_{\iota}}_{\iota=1}^{i-1}} \cap \SSS^{d-1}\\
        &\cos (\nu_i) = \ub_i^* \vb_i \quad \forall~ i=1,\dots,k,
    \end{align*}
    such that $0 \leq \nu_1 \leq \dots \leq \nu_k \leq \pi/2$.

    Given two subspaces $\Vcal_s,\Vcal_w \subseteq \R^d$, let $\Vb_s \in \R^{d \times d_s}$ and $\Vb_w \in \R^{d \times d_w}$ be the matrices whose columns form orthonormal bases for $\Vcal_s$ and $\Vcal_w$, respectively. Then, the canonical angles $\angle(\Vcal_s, \Vcal_w)$ are determined by the singular values of $\Vb_s^\top \Vb_w$~\citep[\S 3]{bjorck1973numerical}:
    \begin{align*}
        \cos(\angle_i(\Vcal_s, \Vcal_w)) = \sigma_i(\Vb_s^\top \Vb_w) \quad \forall~ i=1,\dots,d_s,
    \end{align*}
    where $\sigma_i(\Vb_s^\top \Vb_w)$ denotes the $i$-th singular value of $\Vb_s^\top \Vb_w$.
\end{definition}

In particular, since $\Vb_s, \Vb_w$ consist of orthonormal columns, the singular values of $\Vb_s^\top \Vb_w$ fall in $[0,1]$, and therefore,
\begin{align*}
    d_{s \wedge w} = \sum \cos(\angle(\Vcal_s, \Vcal_w)) = \nbr{\Vb_s^\top \Vb_w}_F^2 \in [0, \min\cbr{d_s, d_w}].
\end{align*}




\section{Additional experiments}\label{apx:exp_details}

\subsection{Additional experiments and details on UTKFace regression}\label{apx:exp_img_reg}
This section provides some additional details and results for the UTKFace regression experiments in \Cref{sec:exp_img_reg}. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=\columnwidth]{fig/mse_utkface_resnet18_clipb32.pdf}%\vspace{-2em}
    \caption{Scaling for MSE on UTKFace with \texttt{CLIP-B32} as the strong student and \texttt{ResNet18} as the weak teacher}\label{fig:mse_utkface_resnet18-clip}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\columnwidth]{fig/mse_utkface_resnet50_clipb32.pdf}%\vspace{-2em}
    \caption{Scaling for MSE on UTKFace with \texttt{CLIP-B32} as the strong student and \texttt{ResNet50} as the weak teacher}\label{fig:mse_utkface_resnet50-clip}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\columnwidth]{fig/mse_utkface_resnet152_clipb32.pdf}%\vspace{-2em}
    \caption{Scaling for MSE on UTKFace with \texttt{CLIP-B32} as the strong student and \texttt{ResNet152} as the weak teacher}\label{fig:mse_utkface_resnet152-clip}
\end{figure}

We summarize the relevant dimensionality in \Cref{tab:img_reg_dim}. We observe the following:
\begin{itemize}
    \item The intrinsic dimensions of the pretrained features are significantly smaller than the ambiance feature dimensions, which is consistent with our theoretical analysis and the empirical observations in \cite{aghajanyan2020intrinsic}. 
    \item The correlation dimensions $d_{s \wedge w}$ are considerably smaller than the corresponding intrinsic dimensions, indicating that the subspaces spanned by the weak and strong features are not aligned in practice. As shown in \Cref{sec:exp_img_reg}, such discrepancies in the weak and strong features facilitate W2S generalization.
\end{itemize}

\begin{table}[!ht]
    \centering
    \caption{Summary of the pretrained feature dimensions, along with the intrinsic dimensions $d_s, d_w$ and correlation dimensions $d_{s \wedge w}$ (with respect to the strong student \texttt{CLIP-B32}) computed over the entire UTKFace dataset (including training and testing).}\label{tab:img_reg_dim}
    \begin{tabular}{c|ccc}
        \toprule
        Pretrained Model & Feature Dimension & Intrinsic Dimension ($\tau=0.01$) & Correlation Dimension \\
        \midrule
        \texttt{ResNet18} & 512 & 194 & 167.64 \\
        \texttt{ResNet34} & 512 & 150 & 129.97 \\
        \texttt{ResNet50} & 2048 & 522 & 301.06 \\
        \texttt{ResNet101} & 2048 & 615 & 354.52 \\
        \texttt{ResNet152} & 2048 & 589 & 339.90 \\
        \midrule
        \texttt{CLIP-B32} & 768 & 443 & $\times$ \\
        \bottomrule
    \end{tabular}
\end{table}

For reference, we provide the scaling for MSE losses of three representative teacher-student pairs in \Cref{fig:mse_utkface_resnet18-clip,fig:mse_utkface_resnet50-clip,fig:mse_utkface_resnet152-clip}. 
\begin{itemize}
    \item It is worth highlighting that while the MSE loss of $f_\wts$ monotonically decreases with respect to both sample sizes $n,N$, the different rates of convergence compared to $f_w$, $f_s$, and $f_c$ lead to the distinct scaling behavior of the relative W2S performance ($\pgr$ and $\opr$) with respect to $n$ versus $N$ in \Cref{fig:pgr_opr_utkface_resnet-clip,fig:pgr_opr_utkface_vardom_resnet-clip}.
    \item When the strong student has a lower intrinsic dimension than the weak teacher (\cf \Cref{fig:mse_utkface_resnet18-clip} versus \Cref{fig:mse_utkface_resnet50-clip,fig:mse_utkface_resnet152-clip}), $d_s < d_w$, the W2S model $f_\wts$ tends to achieve better generalization in terms of the test MSE. This is consistent with our analysis in \Cref{sec:generalization_errors}.
    \item When $d_s < d_w$, the W2S model $f_\wts$ tends to achieve (slightly) better generalization for (slightly) smaller correlation dimension $d_{s \wedge w}$ (\cf \Cref{fig:mse_utkface_resnet50-clip} versus \Cref{fig:mse_utkface_resnet152-clip}), again coinciding with our analysis in \Cref{sec:generalization_errors}.
    \item W2S generalization generally happens (\ie $f_\wts$ is able to outperform $f_w$) with sufficiently large sample sizes $n, N$. However, as the labeled sample size $n$ increases, the test MSE of $f_\wts$ converges slower than that of the strong baseline and ceiling models, $f_s$ and $f_c$, leading to the inverse scaling for $\pgr$ and $\opr$ with respect to $n$ in \Cref{fig:pgr_opr_utkface_resnet-clip,fig:pgr_opr_utkface_vardom_resnet-clip}. When $n$ is too large, the W2S model $f_\wts$ may not be able to achieve better generalization than the strong baseline $f_s$.
\end{itemize}




\subsection{Experiments on image classification}\label{apx:exp_img_cls}

\paragraph{Dataset.} ColoredMNIST \citep{arjovsky2019invariant} consists of groups of different colors and reassign the label to be binary (digits 0-4 vs 5-9). We pool together the groups to form one dataset. The choice is to bring diversity to the feature space with additional color features and thus potential feature discrepancies. We hold out a test set of 7000 samples and used the rest 63000 samples for training.

\paragraph{Linear probing over pretrained features.} We fix a strong student as DINOv2-s14 \citep{oquab2023dinov2} and vary the weak teacher among the ResNet-d series and ResNet series (ResNet18D, ResNet34D, ResNet101, ResNet152) \citep{he2018resnetd,he2015deepresiduallearningimage}. We replace ResNet18 and ResNet34 used in \Cref{sec:exp_img_reg} to experiment on weak models with similar intrinsic dimensions but different correlation dimensions. We treat the backbone of the models (excluding the classification layer) as $\phi_s$ and $\phi_w$ and finetune them via linear probing. We train the models with cross entropy loss and AdamW optimizer. We tune the training hyperparameters of weak and strong models using a validation set and train them for 800 steps with learning rate 1e-3 and weight decay 1e-6. 

\begin{table}[!ht]
    \centering
    \caption{Summary of the pretrained feature dimensions, along with the intrinsic dimensions $d_s, d_w$ and correlation dimensions $d_{s \wedge w}$ (with respect to the strong student \texttt{DINOv2-S14}) computed over the entire ColoredMNIST dataset (including training and testing).}\label{tab:img_cls_dim_coloredmnist}
    \begin{tabular}{c|ccc}
        \toprule
        Pretrained Model & Feature Dimension & Intrinsic Dimension ($\tau=0.01$) & Correlation Dimension \\
        \midrule
        \texttt{ResNet-18-D} & 512 & 117 & 6.23 \\
        \texttt{ResNet-34-D} & 512 & 127 & 7.07 \\
        \texttt{ResNet101} & 2048 & 121 & 1.74 \\
        \texttt{ResNet152} & 2048 & 128 & 1.88 \\
        \midrule
        \texttt{DINOv2-S14} & 384 & 28 & $\times$ \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\columnwidth]{fig/coloredmnist_lp/coloredmnist_dsw.pdf}%\vspace{-2em}
    \caption{Scaling for $\pgr$ and $\opr$ of different weak teachers with a fixed strong student on ColoredMNIST.}\label{fig:coloredmnist_dscapw}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\columnwidth]{fig/coloredmnist_lp/coloredmnist_var.pdf}%\vspace{-2em}
    \caption{Scaling for $\pgr$ and $\opr$ of W2S on ColoredMNIST with injected label noise.}\label{fig:coloredmnist_variance}
\end{figure}

\paragraph{Discrepancies lead to better W2S.}
\Cref{fig:coloredmnist_dscapw} shows the scaling of $\pgr$ and $\opr$ with respect to the sample sizes $n, N$ for different weak teachers in the ResNet series with respect to a fixed student, \texttt{CLIP-B32}. 
As in \Cref{sec:exp_img_reg}, we observe that with similar intrinsic dimensions $d_s, d_w$, W2S achieves better relative performance in terms of $\pgr$ and $\opr$ when the correlation dimension $d_{s \wedge w}$ is smaller.

\paragraph{Variance reduction is a key advantage of W2S.}
We inject noise to the labels of the original ColoredMNIST training samples by randomly flipping the ground truth labels with probability $\varsigma \in [0,1]$ (following \cite{arjovsky2019invariant}). 
\Cref{fig:coloredmnist_variance} shows the scaling of $\pgr$ and $\opr$ with respect to $n$ and $N$ when taking DINOv2-S14 as the strong student and ResNet101 as the weak teacher. We observe that the larger artificial label noise $\varsigma$ leads to higher $\pgr$ and $\opr$. 

\subsection{Experiments on sentiment classification}\label{apx:exp_nlp_cls}

\paragraph{Dataset.} The Stanford Sentiment Treebank \citep{socher-etal-2013-sst2} is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by \citet{pang-lee-2005-sst_original_corpus} and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. We conduct binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded), the so-called SST-2 dataset, and split the dataset into training and testing sets of sizes 63000 and 4349. Generalization errors are estimated with the 0-1 loss over the test set.

\paragraph{Full finetuning.} We fix the strong student as Electra-base-discriminator \citep{clark2020electra} and vary the weak teacher among the Bert series \citep{turc2019bert-tiny} (Bert-Tiny, Bert-Mini, Bert-Small, Bert-Medium). 
With manageable model sizes, we conduct full finetuning experiments following the setup in \cite{burns2023weak}.
We use the standard cross entropy loss for supervised finetuning. 
When training strong students on weak labels (W2S), we use the confidence weighted loss proposed by \cite{burns2023weak}, which is suggested to be able to improve weak-to-strong generalization on many NLP tasks.
All training is conducted via Adam optimizers~\citep{kingma2014adam} with a learning rate of 5e-5, a cosine learning rate schedule, and 40 warmup steps. We train for 3 epochs, which is sufficient for the train and validation loss to stabilize. 

\paragraph{Intrinsic dimension.} The intrinsic dimensions $d_w,d_s$ are measured based on the Structure-Aware Intrinsic Dimension (SAID) method proposed by \cite{aghajanyan2020intrinsic}. We first train the full models on the whole training set, and then train the models with only $d$ trainable parameters based on SAID transformation. The $d_w$ or $d_s$ are the smallest number of parameters under SAID that is necessary to retain 90\% accuracy of the full models. Here, the 90\% accuracy is a common threshold used to estimate intrinsic dimensions in the literature \citep{li2018measuring}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\columnwidth]{fig/sst2/sst2-dsw.pdf}%\vspace{-2em}
    \caption{Scaling for $\pgr$ and $\opr$ of different weak teachers with a fixed strong student on SST-2.}\label{fig:sst2_dsw}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\columnwidth]{fig/sst2/sst2-var.pdf}%\vspace{-2em}
    \caption{Scaling for $\pgr$ and $\opr$ of W2S on SST-2 with injected label noise.}\label{fig:sst2_var}
\end{figure}

\paragraph{Correlation Dimension.} 
Let $D_s, D_w \in \N$ be the finetunable parameter counts of the strong and weak models, respectively. For full FT whose dynamics fall in the kernel regime, as explained in \Cref{rmk:lp_to_general_ft}, the strong and weak ``features'' become the gradients\footnote{
    Notice that $f_s, f_w$ are scalar-valued functions for binary classification tasks like SST-2, and thus the gradients $\nabla_{\thetab} f_s$ and $\nabla_{\thetab} f_w$ are row vectors.
    For multi-class classification tasks where $f_s, f_w$ output vectors of logits, a common heuristic to keep $\Phib_s, \Phib_w$ as matrices of manageable sizes (in constrast to tensors) is to replace gradients of the models, $\nabla_{\thetab} f_s$ and $\nabla_{\thetab} f_w$, with gradients of MSE losses at the pretrained initialization. 
    The gradients of MSE can be viewed as a weighted sum of the model gradients for each class.
}, $\Phib_s = \nabla_{\thetab} f_s(\Xb | \theta_s^{(0)}) \in \R^{N \times D_s}$ and $\Phib_w = \nabla_{\thetab} f_w(\Xb | \theta_w^{(0)}) \in \R^{N \times D_w}$, of the respective models at the pretrained initialization, $\theta_s^{(0)} \in \R^{D_s}$ and $\theta_w^{(0)} \in \R^{D_w}$.

A practical challenge is that $D_s, D_w, N$ are all huge for full FT on most NLP tasks, making it infeasible to compute the $D_s \times D_s$ and $D_w \times D_w$ Gram matrices and their spectral decompositions. 
As a remedy, we leverage the significantly lower intrinsic dimensions $d_s \ll D_s, d_w \ll D_w$ (see \Cref{tab:img_cls_dim_coloredmnist}) to accelerate estimation of $d_{s \wedge w}$ via sketching~\citep{halko2011finding,woodruff2014sketching}.
\begin{enumerate}[label=(\roman*)]
    \item We first reduce both $D_s, D_w$ to the same lower dimension $D = 0.01 \min\{D_s, D_w\}$ (with $D \gg \max\{d_s, d_w\}$) by uniform subsampling columns of $\Phib_s, \Phib_w$ to obtain $\Phib_s', \Phib_w' \in \R^{N \times D}$.
    \item Then, we use randomized rangefinder~\citep[Algorithm 4.1]{halko2011finding} to approximate the first $d_s, d_w$ right singular vectors, $\Vb_s \in \R^{D \times d_s}$ and $\Vb_w \in \R^{D \times d_w}$, of $\Phib_s', \Phib_w'$. Taking the evaluation of $\Vb_s$ as an example, we draw a Gaussian random matrix $\Gb_s \in \R^{d_s \times D}$ and compute the orthornormalization $\Vb_s = \ortho(\Phib_s'^\top \Gb_s)$ via QR decomposition.
    \item Finally, we compute the correlation dimension $d_{s \wedge w} = \nbr{\Vb_s^\top \Vb_w}_F^2$.
\end{enumerate}

\begin{table}[!ht]
    \centering
    \caption{Summary of finetunable parameter counts $D_s, D_w$, intrinsic dimensions $d_s, d_w$, and correlation dimensions $d_{s \wedge w}$ (with respect to the strong student \texttt{Electra}) computed over the entire SST-2 dataset (including training and testing).}\label{tab:sst2_dim}
    \begin{tabular}{c|ccc}
        \toprule
        Pretrained Model & $D_s,D_w$ & Intrinsic Dimension ($\tau=0.01$) & Correlation Dimension \\
        \midrule
        \texttt{Bert-Tiny} & 4.4M & 7000 & 81.13 \\
        \texttt{Bert-Mini} & 11.2M & 8500 & 38.67 \\
        \texttt{Bert-Small} & 28.8M & 8000 & 26.19 \\
        \texttt{Bert-Medium} & 41.4M & 4000 & 8.52 \\
        \midrule
        \texttt{Electra} & 109.5M & 700 & $\times$ \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Discrepancies lead to better W2S.}
\Cref{fig:sst2_dsw} shows the scaling of $\pgr$ and $\opr$ with respect to $n$ and $N$ for different $d_{s \wedge w}$. 
As in \Cref{sec:exp_img_reg,apx:exp_img_cls}, we observe the better relative W2S performance in terms of $\pgr$ and $\opr$ when $d_{s \wedge w}/d_w$ is smaller.

\paragraph{Variance reduction is a key advantage of W2S.}
We inject noise to the labels of training samples by randomly flipping labels with probability $\varsigma = 0, 0.1, 0.2, 0.3$. 
\Cref{fig:sst2_var} shows the scaling of $\pgr$ and $\opr$ with respect to $n$ and $N$ when taking \texttt{Electra} as the strong student and \texttt{Bert-Medium} as the weak teacher. We observe that the larger artificial label noise $\varsigma$ leads to higher $\pgr$ and $\opr$. 