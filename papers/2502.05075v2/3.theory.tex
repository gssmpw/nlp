\section{Problem setup}\label{sec:ridgeless_regression}
In this section, we cast FT as a ridgeless regression problem. The setup is introduced in three parts: model capacity, FT algorithms, and metrics for W2S performance.

Consider the problem of learning an unknown data distribution $\Dcal(f_*): \Xcal \times \Ycal \to [0,1]$ (where $\Xcal$ is a set and $\Ycal \subseteq \R$) associated with a downstream task characterized by an unknown ground truth function $f_*: \Xcal \to \R$. Every sample $(\xb, y) \sim \Dcal(f_*)$ satisfies $y = f_*(\xb) + z$ where $z \sim \Ncal(0, \sigma^2)$ is an independent Gaussian label noise. Let $\Dcal: \Xcal \to [0,1]$ be the marginal distribution over $\Xcal$. We assume that $f_*$ is bounded: $\abbr{f_*(\xb)} \le 1$ for $\xb \sim \Dcal$ almost surely (under normalization without loss of generality).

For any $f: \Xcal \to \R$ learned with samples from $\Dcal(f_*)$, we measure the generalization error via excess risk:
\begin{align}
    \exrisk(f) = \E_{\xb \sim \Dcal}\sbr{\E_{f}\sbr{(f(\xb) - f_*(\xb))^2}},
\end{align}
where randomness in $f$ comes from its training samples.
Notice that $\exrisk(f)$ can be decomposed into variance and bias, $\exrisk(f) = \vari(f) + \bias(f)$,
where
\begin{align*}
    &\vari(f) = \E_{\xb \sim \Dcal}\sbr{\E_{f}\sbr{(f(\xb) - \E_{f}[f(\xb)])^2}}, \\
    &\bias(f) = \E_{\xb \sim \Dcal}\sbr{(\E_{f}[f(\xb)] - f_*(\xb))^2}.
\end{align*}




\subsection{Measures for model capacity}
Model capacity is a key notion in W2S that distinguishes the weak and strong models. Intuitively, a stronger model is capable of representing a downstream task $\Dcal(f_*)$ more accurately and efficiently. We formalize such ``accuracy'' and ``complexity'' through the notions of intrinsic dimensions and FT approximation errors, as introduced below.

Consider two pretrained models, a weak model $\phi_w$ and a strong model $\phi_s$, that output features $\Xcal \to \R^d$:
\begin{assumption}[Sub-gaussian features]\label{asm:features}
    For $\xb\sim \Dcal$, assume both $\phi_w(\xb)$ and $\phi_s(\xb)$ are zero-mean sub-gaussian random vectors with $\E[\phi_w(\xb)] = \E[\phi_s(\xb)] = \b{0}_d$, and $\E[\phi_w(\xb) \phi_w(\xb)^\top] = \Sigmab_w$, $\E[\phi_s(\xb) \phi_s(\xb)^\top] = \Sigmab_s$.
\end{assumption}

Approximation errors measure the model capacity from the ``accuracy'' perspective: how accurate can the downstream task $\Dcal(f_*)$ be represented by the pretrained features of $\phi_s$ and $\phi_w$ over the population.
\begin{definition}[FT approximation error]\label{def:ft_est_err}
   Given $\Dcal(f_*)$, let the FT approximation errors of $\phi_s$ and $\phi_w$ be
    \begin{align*}
        &\rho_s = \min_{\thetab \in \R^d} \E_{\xb \sim \Dcal}\sbr{(\phi_s(\xb)^\top \thetab - f_*(\xb))^2}, \\
        &\rho_w = \min_{\thetab \in \R^d} \E_{\xb \sim \Dcal}\sbr{(\phi_w(\xb)^\top \thetab - f_*(\xb))^2},
    \end{align*}
    such that $\rho_s, \rho_w \in [0,1]$ (given $\Pr_{\xb \sim \Dcal}[|f_*(\xb)| \le 1]=1$ by assumption). 
    We assume both $\rho_s$ and $\rho_w$ are small compared to label noise: $\rho_s + \rho_w \ll \sigma^2$; while the stronger model $\phi_s$ has a lower FT approximation error: $\rho_s < \rho_w$.
    Further, with respect to a sample size $n$, let $\rho_s(n) = \E_{\Xb \sim \Dcal^n}[\| \phi_s(\Xb) \phi_s(\Xb)^\dagger f_*(\Xb) - f_*(\Xb) \|_2^2]$ and $\rho_w(n) = \E_{\Xb \sim \Dcal^n}[\| \phi_w(\Xb) \phi_w(\Xb)^\dagger f_*(\Xb) - f_*(\Xb) \|_2^2]$, where $\phi_s(\Xb)$ and $\phi_w(\Xb)$ are $n \times d$ feature matrices; and $f_*(\Xb) \in \R^n$ is a vector of the noiseless ground truth labels.
\end{definition}
Notice that FT approximation error is different from approximation error of the full model. Precisely, \Cref{def:ft_est_err} quantifies the approximation error of \emph{finetuning the pretrained model}, whose dynamics~\citep{wei2022more,malladi2023kernel} fall in the kernel regime~\citep{jacot2018neural}. 
Since feature learning is limited in the kernel regime~\citep{woodworth2020kernel}, a low FT approximation error requires the pretrained features $\phi_s$ and $\phi_w$ to provide an expressive set of features for the downstream task $\Dcal(f_*)$.

In addition to ``accuracy'', a strong model ought to be able to represent a downstream task concisely. We quantify such ``complexity'' through intrinsic dimension -- the minimum dimension of a feature subspace that can represent the downstream task $\Dcal(f_*)$ accurately. In light of the ubiquitous observations on low intrinsic dimensions of FT~\citep{aghajanyan2020intrinsic}, we introduce a common assumption for FT~\citep{xia2024less,dong2024sketchy} that the pretrained features of $\phi_s, \phi_w$ are concentrated in low-dimensional subspaces, as formalized below.
\begin{definition}[Intrinsic dimensions]\label{def:low_intrinsic_dim}
    Let $d_s = \rank(\Sigmab_s)$ and $d_w = \rank(\Sigmab_w)$ be the \b{intrinsic dimensions} of $\phi_s$ and $\phi_w$. Assume low intrinsic dimensions\footnote{\label{fn:ridge_regression}
        In practice, $\Sigmab_s$ and $\Sigmab_w$ usually admit fast decaying eigenvalues, but not exactly low-rank. In this more realistic case, ridge regression with suitable choices of regularization hyperparameters intuitively performs ``soft'' truncation of the small singular values, effectively leading to low intrinsic dimensions $d_s, d_w \ll d$. 
        For conciseness of the main message, we focus on the ideal case of exactly low-rank $\Sigmab_s$ and $\Sigmab_w$ in the main text, while deferring the ridge regression analysis for general $\Sigmab_s$ and $\Sigmab_w$ to \Cref{apx:ridge_regression}.
    }: $d_s, d_w \ll d$.
\end{definition}
Moreover, \citet{aghajanyan2020intrinsic} observed that the stronger pretrained models tend to have lower intrinsic dimensions, \ie we often have $d_s < d_w$ in practice.

Beyond the absolute notion of model capacity in terms of intrinsic dimensions and FT approximation errors, we introduce a relative measure for the similarity between weak and strong models -- the correlation dimension characterizing the overlap between feature subspaces of $\phi_s$ and $\phi_w$.
\begin{definition}[Correlation dimension]\label{def:correlation_dim}
    Consider spectral decompositions $\Sigmab_s = \Vb_s \Lambdab_s \Vb_s^\top$ and $\Sigmab_w = \Vb_w \Lambdab_w \Vb_w^\top$, where $\Lambdab_s \in \R^{d_s \times d_s}$ and $\Lambdab_w \in \R^{d_w \times d_w}$ are diagonal matrices with positive eigenvalues in decreasing order; while $\Vb_s \in \R^{d \times d_s}$ and $\Vb_w \in \R^{d \times d_w}$ consist of the corresponding orthonormal eigenvectors.
    Let $d_{s \wedge w} = \nbr{\Vb_s^\top \Vb_w}_F^2$ be the \b{correlation dimension} between $\phi_s$ and $\phi_w$ such that $0 \le d_{s \wedge w} \le \min\cbr{d_s, d_w}$.
\end{definition}

\begin{remark}[Extension to general FT]\label{rmk:lp_to_general_ft}
    While we focus on learning $\Dcal(f_*)$ via linear probing over $\phi_w$ and $\phi_s$, since the finetuning dynamics fall approximately in the kernel regime~\citep{wei2022more,malladi2023kernel}, the linear probing analysis naturally extends to general FT. 
    Precisely, let $f_w(\cdot | \b0_d): \Xcal \to \R$ and $f_s(\cdot | \b0_d): \Xcal \to \R$ be the pretrained weak and strong models, where $d$ is the number of finetunable parameters. By denoting $\phi_w(\xb) = \nabla_{\thetab} f_w(\xb | \b0_d)$ and $\phi_s(\xb) = \nabla_{\thetab} f_s(\xb | \b0_d)$, the general FT process effectively reduces to linear probing over $\phi_w$ and $\phi_s$.
\end{remark}



\subsection{W2S and supervised finetuning}\label{sec:ft_algorithms}
With the task $\Dcal(f_*)$ and models $\phi_s, \phi_w$ specified, we are ready to formalize the data and algorithms for FT.

We consider two sample sets drawn $\iid$ from $\Dcal(f_*)$: a small labeled set $\wt\Scal = \csep{(\wt\xb_i,\wt{y}_i)}{i \in [n]} \sim \Dcal(f_*)^n$ and a large sample set $\Scal = \csep{(\xb_i, y_i)}{i \in [N]} \sim \Dcal(f_*)^N$ where the labels $y_i$ are inaccessible, denoting the unlabeled part as $\Scal_x = \csep{\xb_i}{i \in [N]}$.
The goal is to learn a function $f: \Xcal \to \R$ using $\wt\Scal$ and $\Scal_x$ that generalizes well to $\Dcal(f_*)$.

For $\wt\Scal$, let $\wt\Phib_w = [\phi_w(\wt\xb_1),...,\phi_w(\wt\xb_n)]^\top$, $\wt\Phib_s = [\phi_s(\wt\xb_1),...,\phi_s(\wt\xb_n)]^\top \in \R^{n \times d}$ be the weak and strong features with associated labels $\wt\yb = [\wt{y}_1, \ldots, \wt{y}_n]^\top \in \R^n$. 
Analogously for $\Scal$, let $\Phib_w = [\phi_w(\xb_1), \ldots, \phi_w(\xb_N)]^\top$, $\Phib_s = [\phi_s(\xb_1), \ldots, \phi_s(\xb_N)]^\top \in \R^{N \times d}$ be the weak and strong features with unknown labels $\yb = [y_1, \ldots, y_N]^\top \in \R^N$.
For conciseness of notations, we introduce a mild regularity assumption on the ranks of these feature matrices.
\begin{assumption}[Sufficient finetuning data]\label{asm:ft_data}
    Assume $\wt\Scal$ and $\Scal$ are sufficiently large such that $\rank(\wt\Phib_w) = \rank(\Phib_w) = d_w$ and $\rank(\wt\Phib_s) = \rank(\Phib_s) = d_s$ almost surely\footnote{
        Assuming distributions of $\phi_w(\xb)$ and $\phi_s(\xb)$ are absolutely continuous with respect to the Lebesgue measure, for any $n \ge d_w$ and $n \ge d_s$, $\rank(\wt\Phib_w)=\rank(\Phib_w)=d_w$ and $\rank(\wt\Phib_s)=\rank(\Phib_s)=d_s$ almost surely~\citep[\S 3.3.1]{vershynin2018high}.
    }.
\end{assumption}

Given regularization hyperparameters $\alpha_w, \alpha_\wts, \alpha_s, \alpha_c > 0$, we consider the following FT algorithms: 
\begin{enumerate}[label=(\alph*)]
    \item \b{Weak teacher model} $f_w(\xb) = \phi_w(\xb)^\top \thetab_w$ is supervisedly finetuned over $\wt\Scal$: 
    \begin{align}\label{eq:sft_weak}
        \thetab_w = \argmin_{\thetab \in \R^d} \frac{1}{n}\nbr{\wt\Phib_w \thetab - \wt\yb}_2^2 + \alpha_w \nbr{\thetab}_2^2.
    \end{align}
    \item \b{W2S model} $f_\wts(\xb) = \phi_s(\xb)^\top \thetab_\wts$ is finetuned over the strong feature $\phi_s$ through $\Scal_x$ and their pseudo-labels generated by the weak teacher model:
    \begin{align}\label{eq:w2s_ft}
        \thetab_\wts = \argmin_{\thetab \in \R^d} \frac{1}{N}\nbr{\Phib_s \thetab - \Phib_w \thetab_w}_2^2 + \alpha_\wts \nbr{\thetab}_2^2
    \end{align}
    \item \b{Strong SFT model} $f_s(\xb) = \phi_s(\xb)^\top \thetab_s$ is a strong baseline where the strong feature $\phi_s$ is supervisedly finetuned over the small labeled set $\wt\Scal$ directly:
    \begin{align}\label{eq:sft_strong}
        \thetab_s = \argmin_{\thetab \in \R^d} \frac{1}{n}\nbr{\wt\Phib_s \thetab - \wt\yb}_2^2 + \alpha_s \nbr{\thetab}_2^2.
    \end{align}

    \item \b{Strong ceiling model} $f_c(\xb) = \phi_s(\xb)^\top \thetab_c$ is a reference for the ceiling performance where $\phi_s$ is supervisedly finetuned over $\Scal \cup \wt\Scal$, assuming access to the unknown labels $\yb = [y_1, \ldots, y_N]^\top$:
    \begin{align}\label{eq:sft_ceiling}
        \thetab_c = \argmin_{\thetab \in \R^d} \frac{1}{n+N}\nbr{\bmat{\wt\Phib_s \\ \Phib_s} \thetab - \bmat{\wt\yb \\ \yb}}_2^2 + \alpha_c \nbr{\thetab}_2^2.
    \end{align}
\end{enumerate}
For underdetermined/overparametrized problems\footnote{
    While the feature dimension $d$ can be either larger (overparametrized) or smaller (underparametrized) than the sample sizes $n, N, n+N$, with the low intrinsic dimensions $d_s, d_w \ll d$, \cref{eq:sft_weak,eq:w2s_ft,eq:sft_strong,eq:sft_ceiling} are always underdetermined.
}, {even without explicit regularization, gradient descent implicitly biases toward the minimum $\ell_2$-norm solutions in the kernel regime~\citep{woodworth2020kernel}, equivalent to solving \cref{eq:sft_weak,eq:w2s_ft,eq:sft_strong,eq:sft_ceiling} with $\alpha_w, \alpha_\wts, \alpha_s, \alpha_c \to 0$. Therefore, we focus on ridgeless regression in the analysis.}



\subsection{Metrics for W2S performance}\label{sec:w2s_metrics}
In addition to the absolute generalization error of W2S, $\exrisk(f_\wts)$, we quantify the W2S performance of $f_\wts$ relative to $f_w$, $f_s$, and $f_c$ through the following metrics:
\begin{enumerate}[label=(\alph*)]
    \item \b{Performance gap recovery (PGR)} introduced in \cite{burns2023weak} measures the ratio between excess risk reductions from the weak teacher $f_w$ of the W2S model $f_\wts$ and the strong ceiling model $f_c$:
    \begin{align}
        \pgr = \frac{\exrisk(f_w) - \exrisk(f_\wts)}{\exrisk(f_w) - \exrisk(f_c)}.
    \end{align}
    In practice, $\exrisk(f_\wts)$ typically falls between $\exrisk(f_c)$ and $\exrisk(f_w)$~\citep{burns2023weak}. Therefore, it usually holds that $0 \le \pgr \le 1$. A higher $\pgr$ indicates better W2S generalization: the W2S model $f_\wts$ can recover more of the excess risk gap between the weak teacher $f_w$ and the strong ceiling model $f_c$.
    \item \b{Outperforming ratio (OPR)} compares excess risks of the strong baseline $f_s$ and the W2S model $f_\wts$:
    \begin{align}\label{eq:w2s_gain}
        \opr = \exrisk(f_s) / \exrisk(f_\wts).
    \end{align}
    A higher $\opr$ implies better W2S generalization: $f_\wts$ outperforms $f_s$ when $\opr > 1$. 
    This metric could be of interest in practice when the labeled samples $\wt\Scal$ are limited -- if $\opr < 1$, SFT the strong model over $\wt\Scal$ would be a better choice than W2S both in terms of generalization and computational efficiency.
\end{enumerate}




\section{Main results}\label{sec:single_task_ft}
In this section, we first analyze the generalization errors of W2S and its reference models in \Cref{sec:generalization_errors}. Then in \Cref{sec:w2s_performance}, we conduct a case study on the W2S performance in terms of the metrics introduced in \Cref{sec:w2s_metrics}.

\subsection{Generalization errors}\label{sec:generalization_errors}
We start with the W2S model $f_\wts(\xb) = \phi_s(\xb)^\top \thetab_\wts$ finetuned as in \eqref{eq:sft_weak}, \eqref{eq:w2s_ft} with both $\alpha_w, \alpha_\wts \to 0$. For demonstration purposes, we consider an idealized Gaussian feature case in the main text, where the variance of $f_\wts$ can be exactly characterized (instead of upper bounded)\footnote{\label{fn:gaussian_features}
    The analogous generalization bound holds up to constants for sub-gaussian features in \Cref{asm:features}, see \Cref{thm:w2s_ft_formal}.
}. 
\begin{theorem}[W2S model (formally in \ref{apx:pf_w2s_ft})]\label{thm:w2s_ft}
    Assuming \Cref{asm:features,asm:ft_data} and $\phi_w(\xb) \sim \Ncal(\b0_d, \Sigmab_w)$, for $n > d_w + 1$, $\exrisk(f_\wts) = \vari(f_\wts) + \bias(f_\wts)$ satisfies
    \begin{align*}
        &\vari(f_\wts) = \frac{\sigma^2}{n-d_w-1} \rbr{d_{s \wedge w} + \frac{d_s}{N} (d_w-d_{s \wedge w})}, \\
        &\bias(f_\wts) \le \frac{\rho_w(n)}{n} + \frac{\rho_s(N)}{N} \le \rho_w + \rho_s,
    \end{align*}
    where the inequality for $\bias(f_\wts)$ is strict if ${\rho_w(n)}/{n} > 0$ and $d_s < d_w$.
\end{theorem}

\begin{remark}[Discrepancy is virtue]\label{rmk:variance_decomposition}
    Notice that $\vari(f_\wts)$ consists of two terms.
    (a) In the overlapped subspace $\range(\Sigmab_s) \cap \range(\Sigmab_w)$ with correlation dimension $d_{s \wedge w}$, the variance $\sigma^2 d_{s \wedge w} / (n-d_w-1)$ mimics that of the weak teacher, where more pseudo-labels $N$ fail to reduce the variance.
    (b) Whereas variance in the subspace of discrepancy $\range(\Sigmab_w) \setminus \range(\Sigmab_s)$ takes the form $\sigma^2 (d_s / N)(d_w - d_{s \wedge w}) / (n-d_w-1)$, reduced by a factor of $d_s/N$ and vanishing as $N$ grows.
\end{remark}

As a reference, we also look into the weak teacher model $f_w(\xb) = \phi_w(\xb)^\top \thetab_w$ in \eqref{eq:sft_weak} with $\alpha_w \to 0$:
\begin{proposition}[Weak teacher (\ref{apx:pf_sft_weak})]\label{pro:sft_weak}
    With \Cref{asm:features,asm:ft_data}, $\exrisk(f_w) = \vari(f_w) + \bias(f_w)$ has
    \begin{align*}
        \vari(f_w) = \sigma^2\ \frac{d_w}{n}, \quad 
        \bias(f_w) = \frac{\rho_w(n)}{n} \le \rho_w.
    \end{align*}
\end{proposition}

To measure the W2S performance in a relative sense, another two necessary references are the strong SFT baseline $f_s(\xb) = \phi_s(\xb)^\top \thetab_s$ in \eqref{eq:sft_strong} and strong ceiling model $f_c(\xb) = \phi_s(\xb)^\top \thetab_c$ in \eqref{eq:sft_ceiling}, with both $\alpha_s, \alpha_c \to 0$:
\begin{corollary}[Strong SFT and ceiling models]\label{cor:sft_strong}
    Under \Cref{asm:features,asm:ft_data}, $\exrisk(f_s) = \vari(f_s) + \bias(f_s)$ and $\exrisk(f_c) = \vari(f_c) + \bias(f_c)$ satisfy
    \begin{align*}
    \begin{split}
        & \vari(f_s) = \sigma^2\ \frac{d_s}{n}, \quad 
        \bias(f_s) = \frac{\rho_s(n)}{n} \le \rho_s, \\
        &\vari(f_c) = \sigma^2\ \frac{d_s}{N+n}, \quad
        \bias(f_c) = \frac{\rho_s(N+n)}{N+n} \le \rho_s,
    \end{split}
    \end{align*}
\end{corollary}

\paragraph{W2S in variance.}
Assuming $\rho_s + \rho_w \ll \sigma^2$ (\Cref{def:ft_est_err}), variance dominates the generalization error. 
\Cref{thm:w2s_ft} and \Cref{pro:sft_weak} suggest that W2S generalization occurs in variance, \ie $\vari(f_\wts) < \vari(f_w)$, if
\begin{align*}
    n (1 - d_s / N) > (d_w + 1) d_w / (d_w - d_{s \wedge w}),
\end{align*}
which holds broadly when $n, N$ are large enough. For example, with a low correlation dimension $d_{s \wedge w} < d_w$, 
\begin{align*}
    n > (d_w + 1)^2 / (d_w - d_{s \wedge w}) \quad\t{and}\quad N \ge (d_w + 1) d_s
\end{align*}
are sufficient.
Notice that when $d_s < d_w$, $d_{s \wedge w} < d_w$ always holds because $d_{s \wedge w} \le d_s < d_w$.
Meanwhile, with a lower correlation dimension $d_{s \wedge w}$, W2S in variance is more pronounced and takes place at smaller $n$.

\paragraph{W2S in bias.}
Comparing the biases in \Cref{thm:w2s_ft} and \Cref{pro:sft_weak}, when the strong student has zero FT approximation error $\rho_s = 0$, and the FT approximation error of the weak teacher is non-negligible ${\rho_w(n)}/{n} > 0$, as long as the strong student has a lower intrinsic dimension $d_s < d_w$, W2S also enjoys a strictly lower bias than the weak teacher: $\bias(f_\wts) < \bias(f_w)$\footnote{\label{fn:bias_strong}
    Notice that quantifying such advantage of W2S in bias requires further assumptions on the downstream task $\Dcal(f_*)$ and the covariance matrices $\Sigmab_w, \Sigmab_s$, analogous to the settings in \cite{ildiz2024high,wu2024provable}, which is deviating from our focus on variance but could be an interesting future direction.
}.



\subsection{W2S performance: a case study}\label{sec:w2s_performance}
With the generalization analysis, we are ready to take a closer look at the W2S performance in terms of $\pgr$ and $\opr$ defined in \Cref{sec:w2s_metrics}. 

\begin{proposition}[$\pgr$ and $\opr$ lower bounds (\ref{apx:pf_pgr})]\label{cor:pgr}
    Given $f_w, f_\wts, f_c$, and $f_s$ as in \Cref{thm:w2s_ft}, \Cref{pro:sft_weak}, and \Cref{cor:sft_strong}, under \Cref{asm:features,asm:ft_data}, assuming $\phi_w(\xb) \sim \Ncal(\b0_d, \Sigmab_w)$\footnoteref{fn:gaussian_features}, with $n = d_w + q + 1$ for some constant $q \in \N$, we have $\pgr \ge$
    \begin{align*}
        1 - \frac{n}{q}\ {\frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) d_s / N}{d_w}} - \frac{n}{d_w}\ {\frac{\rho_w + \rho_s}{\sigma^2}},
    \end{align*}
    and $\opr \ge$
    \begin{align*}
        \rbr{\frac{n}{q}\ \frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}{d_s} + \frac{n}{d_s}\ \frac{\rho_w + \rho_s}{\sigma^2}}^{-1}.
    \end{align*}
\end{proposition}

We recall from \Cref{sec:w2s_metrics} that the larger $\pgr$ and $\opr$ imply better W2S generalization. Then, a natural question hinted by \Cref{cor:pgr} is \emph{how do the sample sizes $n, N$ affect the W2S performance?} The concrete answers to this question depend on the relative magnitude of the FT approximation errors and label noise, $(\rho_s + \rho_w)/\sigma^2$. 

\paragraph{Case I: negligible FT approximation error.}
In the ideal case where the FT approximation errors are negligible compared to label noise, $(\rho_s+\rho_w)/\sigma^2 \to 0$, \Cref{cor:pgr} suggests better lower bounds for $\pgr$, $\opr$ as $n,N$ increase:
\begin{align*}
    \pgr &\ge 1 - \frac{n}{n - d_w - 1}\ {\frac{d_{s \wedge w} + (d_w - d_{s \wedge w}) d_s / N}{d_w}}, \\
    \opr &\ge \frac{n - d_w - 1}{n}\ \frac{d_s}{d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}}.
\end{align*}
Depending on $d_{s \wedge w}$, we have the following cases:
\begin{enumerate}[label=(\alph*)]
    \item When $d_{s \wedge w} > 0$, with sample sizes $n \gtrsim d_w$ and $N \gtrsim (d_w / d_{s \wedge w} - 1) d_s$, $\pgr \ge 1 - O(d_{s \wedge w}/d_w)$ and $\opr \ge \Omega(d_s / d_{s \wedge w})$ imply good W2S performance if $d_{s \wedge w} \ll \min\cbr{d_s, d_w}$.
    \item When $d_{s \wedge w} = 0$, a labeled sample size of $n \gtrsim d_w$ leads to $\pgr \ge 1 - O(d_s/N)$ and $\opr \ge \Omega(N/d_w)$, implying good W2S performance when $N \gg \max\cbr{d_w, d_s}$.
\end{enumerate}

\paragraph{Case II: small non-negligible FT approximation error.}
In a more realistic scenario where $0 < (\rho_s + \rho_w)/\sigma^2 \ll 1$ is small but non-negligible, the trade-off between variance and bias brings about a non-monotonic scaling of the $\pgr$ and $\opr$ lower bounds with respect to $n$:
\begin{corollary}[Non-monotonic scaling \wrt $n$ (\ref{apx:pf_non_monotonic_scaling})]\label{cor:non_monotonic_scaling}
    For conciseness, denote $d_\wts(N) = d_{s \wedge w} + (d_w - d_{s \wedge w}) {d_s}/{N}$ and $\varrho = (\rho_w + \rho_s)/\sigma^2$. Under the same setting as \Cref{cor:pgr}, with $n = d_w + q + 1$ for some $q \in \N$, the lower bounds of $\pgr$ and $\opr$ are maximized when $q = \sqrt{\rbr{d_w + 1} {d_\wts(N)}/{\varrho}}$, where
    \begin{align*}
        \pgr \ge &1 - d_w^{-1} \rbr{\sqrt{d_\wts(N)} + \sqrt{\varrho \rbr{d_w + 1}}}^2, \\
        \opr \ge &d_s \rbr{\sqrt{d_\wts(N)} + \sqrt{\varrho \rbr{d_w + 1}}}^{-2}.
    \end{align*}
\end{corollary}
Such non-monotonic scaling for $\pgr$ with respect to $n$ coincides with some empirical observations in \cite{burns2023weak} on NLP tasks.
While the variance of $f_\wts$ in \Cref{thm:w2s_ft} decreases monotonically as $n$ grows, so do those of the reference models $f_w$, $f_s$, and $f_c$. With non-negligible FT approximation errors, as $n$ increases, the $\pgr$ and $\opr$ lower bounds decrease with the improvements in bias but increase with the improvements in variance.
Therefore, the optimal $n$ for the lower bounds of $\pgr$ and $\opr$ is determined by the trade-off between variance and bias.

Assuming $\rho_s + \rho_w \ll \sigma^2$ in \Cref{def:ft_est_err}, we have $\varrho \ll 1$.
Again, consider two cases depending on $d_{s \wedge w}$:
\begin{enumerate}[label=(\alph*)]
    \item If $d_{s \wedge w} > 0$, we have $d_\wts(N) \lesssim d_{s \wedge w}$ when $N \gtrsim (d_w / d_{s \wedge w} - 1) d_s$, implying large $\pgr$ and $\opr$ when $d_{s \wedge w} \ll \min\cbr{d_s, d_w}$.
    \item If $d_{s \wedge w} = 0$, we have $d_\wts(N) = d_w d_s / N$, implying large $\pgr$ and $\opr$ when $N \gg \max\cbr{d_w, d_s}$.
\end{enumerate}
