\section{Related works}
This section provides a brief review of literature that is directly related to our focus on weak-to-strong (W2S) generalization and intrinsic dimension. We defer discussions on other related topics to \Cref{apx:related_works}.

\paragraph{W2S alignment: emergence \& growing influence.}
W2S generalization, where a strong student surpasses a weak teacher's performance under weak supervision, was first introduced by ____, offering a promising avenue for aligning superhuman models. Since then, a rapidly expanding body of work has empirically validated this phenomenon across diverse tasks in vision and language models. ____ and ____ propose loss functions and multi-teacher algorithms but do not analyze the underlying mechanisms. ____ and ____ refine training data to improve W2S alignment, while ____ and ____ use weak models for data filtering and reranking.
However, W2S generalization is not without challenges. ____ highlight the issue of W2S deception, where strong models superficially align with weak teachers but fail in new or conflicting cases, an issue that worsens as the capacity gap increases. This underscores the need for improved methods to mitigate misalignment and calls for a theoretical understanding of the true factors that lead to W2S generalization.

\paragraph{Theoretical perspectives on W2S generalization.} 
The empirical successes of W2S have spurred a growing interest in understanding the theoretical underpinnings of this phenomenon. Existing theories on W2S interpret the difference between strong and weak models in terms of the quality of their representations (from the bias perspective in our context). 
____ study W2S in classification through the lens of neighborhood expansion____ where model capacity is interpreted as the robustness to perturbation.
Within this framework, ____ highlights the importance of data selection in W2S while proposing metrics and algorithms for data selection in W2S.
In the same classification setting, ____ takes a transfer learning perspective and highlights the limitation of naive FT in W2S.
____ take a benign overfitting____ perspective and show the asymptotic transition between W2S generalization and random guessing.
For regression tasks, ____ reveals the connection between W2S gain and misfit error of the strong student on weak pseudo-labels.
____ treat W2S as a special case of knowledge distillation, showing its limitation in terms of improving the data scaling law____.
We consider a similar setting of ridgeless regression as ____ but look into a fundamentally different aspect -- variance reduction. This offers a fresh take on the roles of intrinsic dimension and student-teacher correlation in W2S. 

\paragraph{Intrinsic dimension.} 
There has been prevailing empirical and theoretical evidence that natural high-dimensional systems often exhibit low-dimensional structures____.
The concept of intrinsic dimension has been widely studied in manifold learning____, dimensionality reduction____, and representation learning____.
In the context of neural network training, ____ propose a method to measure the intrinsic dimension of the objective landscape based on the Johnson-Lindenstrauss-type transforms____. 
This offers a structural perspective on task complexity, which is largely absent from prior W2S studies. 
____ investigate the intrinsic dimensions of FT, showing that FT over large models usually has surprisingly low intrinsic dimensions, while good pretraining tends to reduce the intrinsic dimension.
Our work extends these insights by linking the intrinsic dimension to W2S, decomposing generalization error into bias and variance, and building upon findings from ____ on variance-dominated risks in learning from noisy labels.