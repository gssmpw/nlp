\subsection{Related works}
This section provides a brief review of literature that is directly related to our focus on weak-to-strong (W2S) generalization and intrinsic dimension. We defer discussions on other related topics to \Cref{apx:related_works}.

\paragraph{W2S alignment: emergence \& growing influence.}
W2S generalization, where a strong student surpasses a weak teacher's performance under weak supervision, was first introduced by \cite{burns2023weak}, offering a promising avenue for aligning superhuman models. Since then, a rapidly expanding body of work has empirically validated this phenomenon across diverse tasks in vision and language models. \citet{guo2024visionsuperalignmentweaktostronggeneralization} and \citet{liu2024cosupervisedlearningimprovingweaktostrong} propose loss functions and multi-teacher algorithms but do not analyze the underlying mechanisms. \citet{guo2024improvingweaktostronggeneralizationreliabilityaware} and \citet{yang2024weaktostrongreasoning} refine training data to improve W2S alignment, while \citet{li2024superfilteringweaktostrongdatafiltering} and \citet{sun2024easytohardgeneralizationscalablealignment} use weak models for data filtering and reranking.
However, W2S generalization is not without challenges. \citet{yang2024superficialalignmentstrongmodelsdeceive} highlight the issue of W2S deception, where strong models superficially align with weak teachers but fail in new or conflicting cases, an issue that worsens as the capacity gap increases. This underscores the need for improved methods to mitigate misalignment and calls for a theoretical understanding of the true factors that lead to W2S generalization.

\paragraph{Theoretical perspectives on W2S generalization.} 
The empirical successes of W2S have spurred a growing interest in understanding the theoretical underpinnings of this phenomenon. Existing theories on W2S interpret the difference between strong and weak models in terms of the quality of their representations (from the bias perspective in our context). 
\citet{lang2024theoretical} study W2S in classification through the lens of neighborhood expansion~\citep{wei2020theoretical,cai2021theory} where model capacity is interpreted as the robustness to perturbation.
Within this framework, \citet{shin2024weak} highlights the importance of data selection in W2S while proposing metrics and algorithms for data selection in W2S.
In the same classification setting, \citet{somerstep2024statistical} takes a transfer learning perspective and highlights the limitation of naive FT in W2S.
\citet{wu2024provable} take a benign overfitting~\citep{bartlett2020benign,muthukumar2021classification} perspective and show the asymptotic transition between W2S generalization and random guessing.
For regression tasks, \citet{Charikar2024QuantifyingTG} reveals the connection between W2S gain and misfit error of the strong student on weak pseudo-labels.
\citet{ildiz2024high} treat W2S as a special case of knowledge distillation, showing its limitation in terms of improving the data scaling law~\citep{spigler2020asymptotic,bahri2024explaining}.
We consider a similar setting of ridgeless regression as \citet{ildiz2024high} but look into a fundamentally different aspect -- variance reduction. This offers a fresh take on the roles of intrinsic dimension and student-teacher correlation in W2S. 

\paragraph{Intrinsic dimension.} 
There has been prevailing empirical and theoretical evidence that natural high-dimensional systems often exhibit low-dimensional structures~\citep{udell2019big}.
The concept of intrinsic dimension has been widely studied in manifold learning~\citep{tenenbaum2000global}, dimensionality reduction~\citep{van2008visualizing}, and representation learning~\citep{bengio2013representation}.
In the context of neural network training, \citet{li2018measuring} propose a method to measure the intrinsic dimension of the objective landscape based on the Johnson-Lindenstrauss-type transforms~\citep{johnson1984extensions}. 
This offers a structural perspective on task complexity, which is largely absent from prior W2S studies. 
\citet{aghajanyan2020intrinsic} investigate the intrinsic dimensions of FT, showing that FT over large models usually has surprisingly low intrinsic dimensions, while good pretraining tends to reduce the intrinsic dimension.
Our work extends these insights by linking the intrinsic dimension to W2S, decomposing generalization error into bias and variance, and building upon findings from \citet{yang2020rethinking, amari2020does} on variance-dominated risks in learning from noisy labels.



\subsection{Notations}
Given any $n \in \Z_+$, we denote $[n] = \cbr{1,\cdots,n}$. 
Let $\eb_n$ be the $n$-th canonical basis of the conformable dimension; $\Ib_n$ be the $n \times n$ identity matrix; and $\b0_n, \b1_n \in \R^n$ being vectors with all zeroes and ones, respectively. 
% Let $\Delta_n \dfeq \csepp{\pb \in [0,1]^b}{\nbr{\pb}_1 = 1}$ be the dimension-$n$ probability simplex. 
For any distribution $p$ and $n \in \Z_+$, let $p^n \dfeq \bigotimes_{i=1}^n p$ as the $n$-fold product distribution of $p$, sampling which yields $n$ $\iid$ samples from $p$.
For any matrix $\Ab \in \R^{n \times d}$, let $\Ab^\dagger$ be the Moore-Penrose pseudoinverse.
% Let $\sval{\Ab}{1} \ge \cdots \ge \sval{\Ab}{\rank(\Ab)} \ge 0$ be the singular values
We adapt the standard asymptotic notations: for any functions $f,g: \R_+ \to \R_+$, we write $f = O\rbr{g}$ or $f \lesssim g$ if there exists some constant $C>0$ such that $f(x) \leq C g(x)$ for all $x \in \R_+$; $f = \Omega\rbr{g}$ or $f \gtrsim g$ if $g = O\rbr{f}$; $f \asymp g$ if $f = O\rbr{g}$ and $f = \Omega\rbr{g}$. Also, we denote $f = o(g)$ or $f/g = o_x(1)$ if $\lim_{x \to \infty} f(x)/g(x) = 0$.
% Additionally, for any $k \le \rank\rbr{\Ab}$, let $\tsvd{\Ab}{k} = \argmin_{\Bb:\ \rank\rbr{\Bb} \le k} \nbr{\Ab - \Bb}_F$ be the optimal rank-$k$ approximation of $\Ab$ (characterized by the rank-$k$ truncated SVD).
% For any symmetric matrices $\Ab, \Bb \in \R^{d \times d}$, we write $\Ab \ageq \Bb$ or $\Ab - \Bb \ageq 0$ if $\Ab - \Bb$ is positive semidefinite.