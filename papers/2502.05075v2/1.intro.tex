\section{Introduction}
As the capabilities of modern machine learning models grow and exceed human performance in many domains, an emerging problem is whether it would be possible to align the strong superhuman models with weaker supervisors such as human feedback. The weak-to-strong (W2S) framework introduced in \cite{burns2023weak} is a feasible analogy for this problem, inquiring how much capacity of a strong student model can be evoked under the supervision of a weak teacher model.
W2S is related to various learning paradigms like co-training~\citep{blum1998combining}, self-training~\citep{scudder1965probability}, knowledge distillation~\citep{hinton2015distilling}, and self-distillation~\citep{zhang2019your,zhang2021self}, yet being critically dissimilar. 

Formalizing the \emph{discrepancy} between the student and the teacher in their \emph{model capacities} is essential for understanding W2S. 
Most existing theories for W2S treat model capacity as an absolute notion with respect to the downstream task, \eg the weak teacher lacks the robustness to perturbation~\citep{lang2024theoretical,shin2024weak} or the ability to fit the target function~\citep{ildiz2024high,wu2024provable}. 
Nevertheless, empirical observations suggest W2S models also surpass weak model's performance on less challenging tasks~\citep{burns2023weak}, where the weak teacher has sufficient capacity to achieve good performance.
This gap of understanding motivates some natural questions:
\begin{center}
    \emph{Why W2S happens when both the teacher and student have sufficient capacities for the downstream task? \\
    What affects W2S generalization beyond the absolute notion of model capacity?}
\end{center}
To answer the above questions, we analyze W2S generalization through the lens of intrinsic dimension beyond the absolute notion of model capacity. We develop a theoretical framework that incorporates student-teacher correlation, providing a more nuanced explanation of when and why W2S model surpasses the weak teacher's performance.  

Our framework is built on two inspiring observations on finetuning (FT): 
\begin{enumerate*}[label=(\roman*)]
    \item FT tends to fall in the kernel regime~\citep{jacot2018neural,wei2022more,malladi2023kernel}; and
    \item for a downstream task, relevant features in a stronger pretrained model tend to concentrate in a subspace of lower dimension, known as the \emph{intrinsic dimension}, even when FT is highly overparametrized~\citep{aghajanyan2020intrinsic}. 
\end{enumerate*}
Leveraging these properties, we cast FT as a ridgeless regression problem over subgaussian features.
In particular, we consider two subspaces $\Vcal_s, \Vcal_w \subset \R^d$ of low dimensions $d_s, d_w \ll d$ that encapsulate relevant features in the strong student and weak teacher, respectively.
The ``absolute'' model capacities are measured from two aspects: 
\begin{enumerate*}[label=(\roman*)]
    \item the intrinsic dimensions $d_s, d_w$ that quantify the representation ``complexity'' and
    \item the approximation errors $\rho_s < \rho_w$ that quantify the representation ``accuracy''
\end{enumerate*}
of the strong and weak models, respectively.
In addition, the student-teacher correlation is measured by alignment between the strong and weak feature subspaces through their canonical angles (see \Cref{apx:canonical_angles}), $d_{s \wedge w} = \sum \cos(\angle(\Vcal_s, \Vcal_w))$ such that $d_{s \wedge w} \in [0, \min\{d_s, d_w\}]$.

This framework reveals the roles of low intrinsic dimensions and student-teacher correlation in W2S.
Decomposing the W2S generalization error into variance and bias, the bias is due to the approximation errors, $\rho_s, \rho_w$, which are low when both student and teacher have sufficient capabilities; whereas the variance comes from noise in the labeled samples for finetuning the weak teacher.
When finetuning the strong student with $N \gtrsim d_s$ pseudo-labels generated by a weak teacher supervisedly finetuned with $n \gtrsim d_w$ noisy labels, the variance of W2S is proportional to:
\begin{align*}
    \tikz[baseline=(A.base)]{
    \node[fill=red!10, draw, rounded corners, inner sep=2pt] (A)
       {$\displaystyle \frac{d_{s \wedge w}}{n}$};
    \node[below=15pt, anchor=north] {\footnotesize \red{Var. in $\Vcal_s \cap \Vcal_w$}};
    }
    + 
    \tikz[baseline=(A.base)]{
    \node[draw, rounded corners, inner sep=2pt] (A)
       {$\displaystyle \frac{d_s}{N}$};
    \node[below=15pt, anchor=north] {\footnotesize W2S};
    }
    \tikz[baseline=(B.base)]{
    \node[fill=blue!10, draw, rounded corners, inner sep=2pt] (B)
       {$\displaystyle \frac{d_w - d_{s \wedge w}}{n}$}; 
    \node[below=15pt, anchor=north] {\footnotesize\blue{Var. in $\Vcal_w \setminus \Vcal_s$}};
    }.
\end{align*}
Specifically, the student mimics variance of the weak teacher in the overlapped feature subspace $\Vcal_s \cap V_w$ but reduces the variance by a factor of $d_s/N$ in the discrepancy between $\Vcal_w$ and $\Vcal_s$.
Compared to the weak teacher variance that scales as $d_w/n$, W2S happens (\ie the student outperforms its weak teacher) with sufficient sample sizes $n, N$ when:
\begin{enumerate*}[label=(\roman*)]
    \item the strong student has a lower intrinsic dimension, $d_s < d_w$ (as empirically observed in \cite{aghajanyan2020intrinsic} on NLP tasks), or
    \item the student-teacher correlation is low, $d_{s \wedge w} < d_w$.
\end{enumerate*}
This unveils the benefit of discrepancy between the teacher and student features for W2S: 
\begin{center}
    \emph{In the variance-dominated regime, W2S comes from variance reduction in the discrepancy of weak teacher features from strong student features}.
\end{center}


To provide intuitions for such variance reduction, let's consider $\Vcal_s$ and $\Vcal_w$ with large discrepancy as two distinct aspects of a downstream task that both provide sufficient information. For example, to classify the brand of a car in an image, one can use either the simple information in the logo (strong features $\Vcal_s$ with a lower intrinsic dimension $d_s$) or the complex information in the design (weak features $\Vcal_w$ with a higher intrinsic dimension $d_w$). In a high-dimensional feature space, $\Vcal_s$ and $\Vcal_w$ that encode irrelevant information are likely almost orthogonal, leading to a small $d_{s \wedge w}$. Then, the error of weak teacher induced by noise in the $n$ labeled samples is only correlated to the design features in $\Vcal_w$ but almost independent of the logo features in $\Vcal_s$. {Therefore, the error in weak supervision can be viewed as independent label noise for the student. With an intrinsic dimension of $d_s$, the generalization error of student induced by such independent noise vanishes at a rate of $O(d_s/N)$.}    

Our main contributions are summarized as follows:
\begin{itemize}
    \item We introduce a theoretical framework for W2S based on the low intrinsic dimensions of FT, where we characterize model capacities from three aspects: approximation errors for ``accuracy'', intrinsic dimensions for ``complexity'', and student-teacher correlation for ``alignment'' (\Cref{sec:ridgeless_regression}).
    
    \item We provide a generalization analysis for W2S with an exact characterization of the variance under a Gaussian feature assumption, unveiling the virtue of discrepancy between the student and teacher in W2S (\Cref{sec:generalization_errors}).
    
    \item We further investigate the relative W2S performance in terms of performance gap recovery (PGR)~\citep{burns2023weak} and outperforming ratio (OPR) compared to the strong baseline model supervisedly finetuned with $n$ labels. A case study provides insights into the scaling of PGR and OPR with respect to the sample sizes $n, N$ and sample complexities in W2S (\Cref{sec:w2s_performance}).
\end{itemize}




