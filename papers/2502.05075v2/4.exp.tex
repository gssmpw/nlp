\section{Experiments}\label{sec:experiments}
We conduct experiments to validate the theoretical findings on both synthetic and real tasks. In this section, we focus on two illustrative settings: synthetic regression (\Cref{sec:exp_synthetic}) and real-world image regression (\Cref{sec:exp_img_reg}). For brevity, we defer more experiments on image and sentiment classification tasks to \Cref{apx:exp_img_cls,apx:exp_nlp_cls}, respectively.

\subsection{Synthetic regression}\label{sec:exp_synthetic}
We start by grounding the theoretical framework introduced in \Cref{sec:ridgeless_regression} with synthetic regression tasks. 

\paragraph{Setup.}
We concretize the downstream task $\Dcal(f_*)$ as a regression problem over Gaussian features. 
Let $f_*: \R^d \to \R$ be a linear function in a high-dimensional feature space $d=20,000$ of form $f_*(\xb) = \xb^\top \Lambda_*^{1/2} \thetab_*$ where $\Lambda_* = \diag(\lambda^*_1, \cdots, \lambda^*_d)$ is a diagonal matrix with a low rank $d_* = 300$ such that $\lambda^*_i = i^{-1}$ for $i \le d_*$ and $\lambda^*_i = 0$ otherwise; and $\thetab_* \in \R^{d}$ is a random unit vector.
Every sample $(\xb, y) \sim \Dcal(f_*)$ is generated by $\xb \sim \Ncal(\b0_d, \Ib_d)$ and $y = f_*(\xb) + z$ with $z \sim \Ncal(0, \sigma^2)$.
Given $\xb$, the associated strong and weak features in \Cref{asm:features} are generated by $\phi_s(\xb) = \Sigmab_s^{1/2} \xb$ and $\phi_w(\xb) = \Sigmab_w^{1/2} \xb$, with intrinsic dimensions $d_s = 100$ and $d_w = 200$ such that $\Sigmab_s = \sum_{i=1}^{d_s} \lambda^*_i \eb_i \eb_i^\top$ and $\Sigmab_w = \sum_{i=d_s - d_{s \wedge w} + 1}^{d_w + d_s - d_{s \wedge w}} \lambda^*_i \eb_i \eb_i^\top$. 
For all synthetic experiments, we have $\rho_s + \rho_w < 0.0004$.

In the experiments, we vary $d_{s \wedge w}$ to control the student-teacher correlation and $\sigma^2$ to control the dominance of variance over bias (characterized by $\rho_s, \rho_w$). Each error bar reflects the standard deviation over $40$ runs. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{fig/exrisk_dsw10.pdf}%\vspace{-2em}
    \caption{Scaling for excess risks on the synthetic regression task in a \emph{variance-dominated regime} with a \emph{low correlation dimension}.}\label{fig:exrisk_dsw10}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{fig/exrisk_dsw90.pdf}%\vspace{-2em}
    \caption{Scaling for excess risks on the synthetic regression task in a \emph{variance-dominated regime} with a \emph{high correlation dimension}.}\label{fig:exrisk_dsw90}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{fig/exrisk_dsw90_biased.pdf}%\vspace{-2em}
    \caption{Scaling for excess risks on the synthetic regression task when \emph{the variance is not dominant}, $\sigma^2 \approx \rho_s + \rho_w$.}\label{fig:exrisk_dsw90_biased}
\end{figure}

\paragraph{Scaling for generalization errors.}
\Cref{fig:exrisk_dsw10,fig:exrisk_dsw90,fig:exrisk_dsw90_biased} show scaling for $\exrisk(f_\wts)$ (W2S), $\exrisk(f_w)$ (Weak), $\exrisk(f_s)$ (S-Baseline), and $\exrisk(f_c)$ (S-Ceiling) with respect to the sample sizes $n, N$. The dashes show theoretical predictions in \Cref{thm:w2s_ft,pro:sft_weak,cor:sft_strong}, consistent with the empirical measurements shown in the solid lines.
In particular, we consider three cases:
\begin{itemize}%[label=(\alph*)]
    \item \Cref{fig:exrisk_dsw10}: When variance dominates ($\sigma^2 = 0.01 \gg \rho_w + \rho_s$), with a low correlation dimension $d_{s \wedge w} = 10$, $f_\wts$ outperforms both $f_w$ and $f_s$ for a moderate $n$ and a large enough $N$. However, larger sample sizes do not necessarily lead to better W2S generalization in a relative sense. For example, when $n$ keeps increasing, the strong baseline $f_s$ eventually outperforms $f_\wts$.
    \item \Cref{fig:exrisk_dsw90}: When variance dominates, with a high correlation dimension $d_{s \wedge w} = 90$, $f_\wts$ still generalizes better than $f_w$ but fails to outperform the strong baseline $f_s$. 
    \item \Cref{fig:exrisk_dsw90_biased}: When the variance is low (not dominant, \eg $\sigma^2 = 0.0004 \approx \rho_s + \rho_w$), $f_\wts$ can fail to outperform $f_w$. This suggests that variance reduction is a key advantage of W2S over supervised FT.
\end{itemize}



\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{fig/pgr_opr_vardom.pdf}%\vspace{-2em}
    \caption{Scaling for $\pgr$ and $\opr$ under different $d_{s \wedge w}$ on the synthetic regression task in a \emph{variance-dominated regime}.}\label{fig:pgr_opr_vardom}
\end{figure}

\paragraph{Scaling for $\pgr$ and $\opr$.}
\Cref{fig:pgr_opr_vardom} show the scaling for $\pgr$ and $\opr$ with respect to sample sizes $n, N$ in the variance-dominated regime (with small non-negligible FT approximation errors), at three different correlation dimensions $d_{s \wedge w} = 90, 50, 10$. The solid and dashed lines represent the empirical measurements and lower bounds in \eqref{eq:pgr_lower_tight}, \eqref{eq:opr_lower_tight}, respectively.
\begin{itemize}
    \item Coinciding with the theoretical predictions in \Cref{cor:non_monotonic_scaling} and the performance gaps between W2S and the references in \Cref{fig:exrisk_dsw10}, we observe that the relative W2S performance in terms of $\pgr$ and $\opr$ can degenerate as $n$ increases, while the larger $N$ generally leads to better W2S generalization in the relative sense. 
    \item The lower correlation dimension $d_{s \wedge w}$ leads to higher $\pgr$ and $\opr$, \ie larger discrepancy between the strong and weak features improves W2S generalization.
\end{itemize}



\subsection{UTKFace regression}\label{sec:exp_img_reg}
Beyond the synthetic regression, we investigate W2S on a real-world image regression task -- age estimation on the UTKFace dataset~\citep{zhang2017age}. Each error bar in this section reflects standard deviation of $10$ runs. 

\paragraph{Dataset.} 
UTKFace (Aligned \& Cropped)~\citep{zhang2017age} consists of $23,708$ face images with age labels ranging from $0$ to $116$. We preprocess the images to $224 \times 224$ pixels and split the dataset into training and testing sets of sizes $20,000$ and $3,708$.
Generalization errors are estimated with the mean squared error (MSE) over the test set. 

\paragraph{Linear probing over pretrained features.}
We fix the strong student as CLIP ViT-B/32~\citep{radford2021learning} (\texttt{CLIP-B32}) and vary the weak teacher among the ResNet series~\citep{he2015deepresiduallearningimage} (\texttt{ResNet18}, \texttt{ResNet34}, \texttt{ResNet50}, \texttt{ResNet101}, \texttt{ResNet152}). We treat the backbones of these models (excluding the classification layers) as $\phi_s,\phi_w$ and finetune them via linear probing. We use ridge regression with a small fixed regularization hyperparameter $\alpha_w, \alpha_\wts, \alpha_s, \alpha_c = 10^{-6}$, close to the machine epsilon of single precision floating point numbers.

\paragraph{Intrinsic dimension.}
The intrinsic dimensions $d_w, d_s$ are measured based on the empirical covariance matrices $\Sigmab_w, \Sigmab_s$ of the weak and strong features over the entire dataset (including training and testing).
As mentioned in \Cref{fn:ridge_regression}, these covariances generally have fast decaying eigenvalues (but not exactly low-rank) in practice, effectively leading to low intrinsic dimensions under ridge regression. We estimate such low intrinsic dimensions as the minimum rank for the best low-rank approximation of $\Sigmab_w, \Sigmab_s$ with a relative error in trace less than $\tau=0.01$.

\paragraph{Correlation dimension.}
The pretrained feature dimensions (or the finetunable parameter counts) of the weak and strong models can be different in practice (see \Cref{apx:exp_img_reg}, \Cref{tab:img_reg_dim}). 
We introduce an estimation for $d_{s \wedge w}$ in this case.
Consider the (truncated) spectral decompositions $\tsvd{\Sigmab_s}{d_s} = \Vb_s \Lambdab_s \Vb_s^\top$ and $\tsvd{\Sigmab_w}{d_w} = \Vb_w \Lambdab_w \Vb_w^\top$ of two empirical covariances with different feature dimensions $D_s, D_w$ such that $\Vb_s \in \R^{D_s \times d_s}$ and $\Vb_w \in \R^{D_w \times d_w}$ consists of the top $d_s, d_w$ orthonormal eigenvectors, respectively. We estimate the correlation dimension $d_{s \wedge w}$ under different feature dimensions $D_s \ne D_w$ by matching the dimensions through a random unitary matrix~\citep{vershynin2018high} $\Gammab \in \R^{D_s \times D_w}$: $d_{s \wedge w} = \|\Vb_s^\top \Gammab \Vb_w\|_F^2$. This provides a good estimation for $d_{s \wedge w}$ because with low intrinsic dimensions $\max\{d_s, d_w\} \ll D_s, D_w$ in practice, mild dimension reduction through $\Gammab$ well preserves the essential information in $\Vb_s, \Vb_w$.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{fig/pgr_opr_utkface_resnet-clip.pdf}%\vspace{-2em}
    \caption{Scaling for $\pgr$ and $\opr$ of different weak teachers with a fixed strong student on UTKFace. The legends show the comparison between $d_{s \wedge w}$ and $d_w$.}\label{fig:pgr_opr_utkface_resnet-clip}
\end{figure}

\paragraph{Discrepancies lead to better W2S.}
\Cref{fig:pgr_opr_utkface_resnet-clip} shows the scaling of $\pgr$ and $\opr$ with respect to the sample sizes $n, N$ for different weak teachers in the ResNet series with respect to a fixed student, \texttt{CLIP-B32}. 
We first observe that the relative W2S performance in terms of $\pgr$ and $\opr$ is closely related to the correlation dimension $d_{s \wedge w}$ and the intrinsic dimensions $d_s, d_w$. 
\begin{itemize}
    \item When the strong student has a lower intrinsic dimension than the weak teacher (as widely observed in practice~\citep{aghajanyan2020intrinsic}), \ie $d_s < d_w$, the relative W2S performance tends to be better than when $d_s > d_w$.
    \item The relative W2S performance tends to be better when $d_{s \wedge w}/d_w$ is lower, \ie the larger discrepancy between weak and strong features leads to better W2S generalization.
\end{itemize}
Meanwhile, both $\pgr$ and $\opr$ scale inversely with the labeled sample size $n$ and exhibit diminishing return with respect to the increasing pseudolabel size $N$, consistent with the theoretical predictions in \Cref{cor:non_monotonic_scaling} and the synthetic experiments in \Cref{fig:pgr_opr_vardom}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{fig/pgr_opr_utkface_vardom_resnet-clip.pdf}%\vspace{-2em}
    \caption{Scaling for $\pgr$ and $\opr$ on UTKFace with injected label noise: $y_i \gets y_i + \zeta_i$ where $\zeta_i \sim \Ncal(0, \varsigma^2)~\iid$.}\label{fig:pgr_opr_utkface_vardom_resnet-clip}
\end{figure}

\paragraph{Variance reduction is a key advantage of W2S.}
To investigate the impact of variance on W2S generalization, we inject noise to the training label by $y_i \gets y_i + \zeta_i$ where $\zeta_i \sim \Ncal(0, \varsigma^2)~\iid$, and $\varsigma$ controls the injected labels noise level.
In \Cref{fig:pgr_opr_utkface_vardom_resnet-clip}, we show the scaling for $\pgr$ and $\opr$ with respect to the sample sizes $n, N$ under different noise levels $\varsigma$. We observe that the relative W2S performance in terms of $\pgr$ and $\opr$ improves as the noise level $\varsigma$ increases. This provides empirical evidence that variance reduction is a key advantage of W2S over supervised FT, highlighting the importance of understanding the mechanisms of W2S in the variance-dominated regime.

