
\section{Safety Challenges Specific to LLM Agents}\label{sec:safety_challenges}



High-stakes financial systems require stringent demands on reliability, accuracy, and robustnessâ€”qualities that are not always guaranteed by current LLMs~\citep{nie2024survey}. We categorize the risks associated with LLMs in finance into two classes: intrinsic challenges that originate from the models themselves (Section~\ref{sec:intrinsic_risks}) and external challenges that arise from the interaction of LLMs with external systems and workflows (Section~\ref{sec:external_risks}). We provide an overview of the risks and challenges in Figure~\ref{fig:limitations}.



\subsection{Intrinsic Risks from LLM Agents}\label{sec:intrinsic_risks}

LLMs generate outputs based on patterns learned from large-scale text corpora~\citep{naveed2023comprehensive,zhao2023survey,liu2024understanding}. While this results in fluent language, it can also lead to meaningful errors~\citep{marchisio2024understanding}. In casual usage like chatbots, these mistakes might not matter much. However, in finance, even small errors can lead to serious consequences, including financial losses, legal risks, and damage to reputation.

\paragraph{Illusory Confidence}

In finance, decision-making is rooted in uncertainty quantification and risk assessment~\citep{fadina2024framework}, as formalized in modern portfolio theory~\citep{elton2009modern} and utility theory~\citep{stigler1950development}. They emphasize the need for precise confidence intervals to build investment strategies. However, LLMs often output results with unwarranted certainty, regardless of factual correctness~\citep{tanneru2024quantifying,xiongcan}. 
This behavior contradicts the principles of probabilistic risk assessment~\citep{bedford2001probabilistic} for financial systems. 
For instance, a model's overconfident erroneous market trend analysis may lead to portfolio misallocation, undermining an investor's risk-adjusted return expectations.

\paragraph{Hallucination}

LLMs' hallucination poses a fundamental challenge to the integrity of financial analytics~\citep{kang2023deficiency,roychowdhury2023hallucination}. 
Although minor hallucinations may appear harmless in casual scenarios, they can have severe consequences in finance. For instance, fabricating earnings data can lead to false signals that influence the algorithmic trading systems and human decision-making. Given that finance is predicated on accurate information, hallucinations may lead to financial losses and legal liabilities. 


\paragraph{Lack of Temporal Awareness}

The dynamic nature of financial markets requires continuous adaptation, as reflected in dynamic portfolio optimization~\citep{nystrup2018dynamic}. LLMs are trained on static corpora and lack mechanisms to update their knowledge in real time. This temporal misalignment means that models may rely on outdated economic conditions or regulatory landscapes, resulting in analyses that fail to account for present market realities. For instance, LLM agent may generate trading recommendations overlook recent central bank policy changes, leading to erroneous decision-making. In markets where timely and precise information is critical, this limitation can significantly threaten the reliability of LLM-driven financial systems.

\paragraph{Poor Handling of Domain-Specific Reasoning}

Financial decision-making often involves interpreting domain-specific language~\citep{ke2025demystifying}, complex contracts~\citep{lai2024large}, and regulatory documents~\citep{cao2024large}. LLMs trained on general-purpose text frequently struggle to differentiate between domain-specific terminologies or rules, leading to errors in domain-specific reasoning. For example, LLM agents might misinterpret the purpose of a smart contract in blockchain transactions~\citep{ressi2024ai}. The lack of domain-specific reasoning can result in misleading decisions, particularly in areas where precise understanding of terminology and mechanics is essential.


\paragraph{Adversarial Vulnerabilities}

Adversaries can exploit LLM agents through carefully designed prompts that manipulate their outputs~\citep{xullm,schulhoff2023ignore}. For example, subtle input modifications may cause an agent to generate biased market analyses. This is particularly concerning in trading systems, where such manipulations can influence downstream processes, leading to flawed strategies or improper risk assessments.

LLM agents operating autonomously are especially susceptible to cascading errors~\citep{hongmetagpt}. In multi-step workflows, adversarially manipulated inputs can propagate through decision pipelines, increase the impact of initial errors. For instance, an agent tasked with analyzing market sentiment and generating investment recommendations might amplify false information from an initial manipulated source, resulting in systemic failures.

\paragraph{Dependency on Prompt Design}

Users of LLM agents in financial domain may encounter variability in the outputs depending on how prompts are phrased~\citep{zhao2024optimizing}. The unpredictability complicates their practical use, particularly in tasks requiring consistent and reliable results. For instance, slight changes in how a user phrases a risk assessment query can lead to different outputs, even when the task remains the same~\citep{Yu2023BenchmarkingLL}.
This inconsistency poses a challenge for users who rely on LLM agents for critical decision-making. In automated trading pipelines, such variability can lead to inefficiencies, errors, or missed opportunities. 

\paragraph{Lack of Interpretability}

The risk management frameworks require transparency and accountability in decision-making processes~\citep{schuett2024risk}. However, an LLM's ``black-box'' nature makes it challenging to explain how it derives its outputs. Regulators and institutional investors emphasize the need for explainable models to ensure traceability and compliance. The inability to clarify how LLMs produce their outputs creates challenges for adoption in high-stakes domains, where decision-making processes must be fully understood and auditable to users~\citep{hung2023walking}. 

\paragraph{Limitations in Multimodal Integration}

Financial decision-making relies on the integration of multimodal data, combining textual analysis, numerical computations, and visual representations such as stock charts~\citep{zhang2024finagent}. Decision science highlights the importance of processing diverse and complex information in dynamic environments~\citep{ijcai2024p875}. However, current LLMs are primarily text-focused and often fail to integrate textual, numerical, and visual data effectively.
This limitation is evident in quantitative research and algorithmic trading, where understanding relationships across varied data types is essential. The lack of robust multimodal capabilities reduces the analytical potential of LLM agent and limits their ability to provide actionable insights for financial decisions.




\subsection{Risks from External Interactions}\label{sec:external_risks}
\paragraph{API/Tool Dependency}
LLM agents frequently rely on external APIs and tools~\citep{Shen2025shortcutsbench}. For example, to retrieve live market data, execute trades, or query regulatory information. While such integrations expand a LLM's capabilities, they also involve dependencies on systems that can be error-prone~\citep{zhao2024attacks}. A malfunctioning data feed might provide inaccurate prices~\citep{tivnan2018price}, or a compromised API could intentionally inject misleading content~\citep{zhao2024attacks}. Because the LLM treats these external tools as supplementary knowledge sources, any errors or manipulations can propagate unchecked.


\paragraph{Multi-Step Tasks}
Financial operations often involve complex, multi-step workflows: evaluating a company's fundamentals, performing sentiment analysis on news, applying risk models, and ultimately placing trades~\citep{nie2024survey}. Small errors at any step in these workflows may accumulate into larger failures. For example, an incorrect news interpretation might lead to a flawed risk assessment, which in turn could trigger an inappropriate trade.
Unlike single-turn tasks that can be manually reviewed, multi-step tasks can magnify small inaccuracies, culminating in decisions that carry substantial monetary risks.

