\section{Conclusion}
% The current landscape of financial LLM agent benchmarks is fundamentally flawed, prioritizing performance while overlooking critical safety risks. Vulnerabilities, such as hallucinations, adversarial manipulation, and temporal misalignment, poses systemic threats in high-stakes financial environments. To address this gap, we introduce the Safety-Aware Evaluation Agent (SAEA), a comprehensive framework that assesses LLM agents across three evaluation dimensions: model, workflow, and system levels. Our empirical findings reveal that even high-performing models can exhibit hidden failure modes, underscoring the need for a paradigm shift in LLM evaluation. Moving forward, the financial AI community must adopt risk-aware benchmarking practices to ensure the safe and reliable deployment of LLM agents. Without such measures, the integration of AI into financial decision-making remains fraught with unchecked dangers.

The current approach to benchmarking financial LLM agents is biased, as it prioritizes performance while neglecting critical safety risks. Vulnerabilities such as hallucinations, adversarial manipulation, and lack of temporal awareness pose systemic threats in high-stakes financial environments. To address this gap, we introduce the \textbf{S}afety-\textbf{A}ware \textbf{E}valuation \textbf{A}gent \textbf{(SAEA)}, a comprehensive framework that evaluates LLM agents across three key dimensions: model, workflow, and system level. 
Our findings demonstrate that existing evaluation methods overlook crucial safety concerns, making risk-aware benchmarking essential for the responsible deployment of LLM agents in finance. Without such measures, AI-driven financial decision-making remains exposed to significant, unchecked risks.
% Our findings demonstrate that existing evaluation methods overlook crucial safety concerns, underscoring the need for a paradigm shift in LLM evaluation. Moving forward, the community must adopt risk-aware benchmarking practices to ensure the safe and reliable deployment of LLM agents. Without such measures, AI-driven financial decision-making remains exposed to significant, unchecked risks.