


% \subsection{Evaluation Dimensions}\label{sec:evaluation_dimensions}

% \subsubsection{Model-Level Metrics}\label{sec:model-level-metrics}

% \paragraph{Hallucination Detection} 
% We measure hallucination severity (impact of error). We link the agents to external fact-checking functions (\textit{Yahoo finance}~\citep{}) to validate their outputs.
% We quantify hallucination severity using:
% \begin{equation}
% H_{\text{score}} = \frac{1}{N}\sum_{i=1}^N \mathbb{I}(f_{\text{eval}}(r_i) \neq y_i)\cdot w_i
% \end{equation}
% where $f_{\text{eval}}(r_i)$ is the evaluation agent's response to the $i$-th test case, $y_i$ is the retrieved information from the fact-checking functions and $w_i$ is the financial impact weights.

% \paragraph{Temporal Awareness} 
% We evaluate temporal alignment via a temporal accuracy score, comparing model outputs against real-time data. It is measured through:
% \begin{equation}
% T_{\text{acc}} = 1 - \frac{| \mathbf{O}_{\text{model}} - \mathbf{O}_{\text{realtime}} |2}{|\mathbf{O}_{\text{realtime}}|_2}
% \end{equation} 
% where $\mathbf{O}$ represents time-sensitive outputs like asset valuations.

% \paragraph{Confidence Calibration}
% We use SAEA's knowledge to measure confidence, assessing the model's confidence in its outputs. 

% \paragraph{Robustness}
% We test robustness with adversarial inputs~\citep{}, using subtle rhetorical tweaks, to calculate an adversarial robustness score. We compare the model's outputs against adversarially manipulated inputs to calculate the score.
% % The score measures the model's ability to maintain reliability when facing deliberate attempts.

% \paragraph{Interpretability and Explainability} 
% We simulate expert ratings on explanation clarity, evaluating the model's ability to deliver logically coherent and understandable rationales.

% \subsubsection{Workflow-Level Metrics}\label{sec:workflow-level-metrics}
% % Workflow-level metrics assess the model's consistency across tasks, error propagation, and sensitivity to prompt variations.


% \paragraph{Error Propagation} 
% We use a propagation sensitivity to measure how small initial errors, such as misinterpreting a single data feed, can amplify in multi-step processes.

% We model error amplification in $n$-step workflows using:
% \begin{equation}
%   P_{error} = \frac{\mathcal{L}}{S_\text{trajectory}}
% \end{equation}
% where $\mathcal{L}$ represents the number of error detected by SAEA, and $S_\text{trajectory}$ is the total number of trajectories in the workflow.


% \paragraph{Prompt Sensitivity} 
% We assess prompt stability by varying task instructions and examining the consistency of outputs. Significant sensitivity to minor changes in prompt wording may indicate vulnerabilities to both deliberate and accidental manipulations.
% We quantify stability through semantic similarity variance:
% \begin{equation}
% \sigma_{\text{prompt}} = \text{Var}({\text{SAEA}(r_j, r_k)}_{j \neq k})
% \end{equation}
% where outputs ${r_j}$ and ${r_k}$ are the model's outputs for different prompts.

% \subsubsection{System-Level Metrics}\label{sec:system-level-metrics}
% % In addition to model- and workflow-level evaluation, real-world deployment demands integration with external systems, including APIs, real-time data feeds, and multimodal inputs.

% \paragraph{Response Degration Dependency} 
% We employ a response degradation score to measure targeted LLM agent performance when external APIs return delayed, corrupted, or adversarially manipulated data. Response degradation is with a range $[0, 100]$, where $0$ indicates no degradation and $100$ indicates complete failure or vulnerable behavior. 

% \paragraph{Multimodal Integration}
% We introduce a multimodal integration score to evaluate the impact of multimodal inputs. We transform multimodal inputs into a textual format, and compare the similarity score of targeted LLM agent's outputs against the original multimodal inputs:
% \begin{equation}
%   \varphi_{\text{multimodal}} = \text{Sim}({\text{SAEA}(m_j, t_k)}_{j \neq k})
% \end{equation}
% where $m_j$ and $t_k$ are the multimodal inputs and textual inputs, respectively. 
% %   We quantify this score by evaluating the model's ability to synthesize information from multiple modalities, such as text, images, and structured data.


% \paragraph{Scenario-Based Stress Testing} 
% By leveraging simulated extreme events, we calculate a scenario resilience score. The score reflects the LLM's ability to deliver consistent and accurate performance during extreme system shocks, by comparing the model's outputs against a baseline performance during normal conditions.
% We set a set of extreme events, including no-external resource, and API failure.




% ============
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/saea.pdf}
    \caption{An overview of the Safety-Aware Evaluation Agent (SAEA) for financial LLM agents. The SAEA Evaluation conducts three-level audits: model-Level (intrinsic LLM capabilities), workflow-level (multi-step process reliability), and system-level (integration robustness). SAEA is designed to identify vulnerabilities and ensure safer, more reliable LLM agents in financial domains.}
    \label{fig:saea}
\end{figure*}

\section{A Safety-Aware Evaluation Agent}

As demonstrated in Section~\ref{sec:safety_challenges}, deploying LLM agents in financial systems requires evaluation beyond standard performance metrics. 
We propose a \textbf{Safety-Aware Evaluation Agent (SAEA)} to measure the potential risks of using LLM agents in the financial domain.
% Below, we formalize how these prompts, outputs, and scores are systematically generated and aggregated into a risk profile for LLM-based financial agents.
% \subsection{Evaluation Design}
Our design is anchored by two complementary ideas:
\begin{enumerate}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
  \item Risk-Sensitive Metrics Design: We present evaluation metrics that are adaptive to the safety risks intrinsic and extrinsic to LLMs. These metrics are designed to capture risks identified in Section~\ref{sec:safety_challenges}.
  \item Scenario-Driven Stress Testing: 
  Inspired by stress testing in robust software engineering~\citep{wang2024software} and the finance industry~\citep{king2011basel}, where stress tests help expose system fragility, we design edge scenarios to evaluate the LLM agents's safety under differerent real-world conditions. 
\end{enumerate}
% \vspace{-5pt}
These ideas are integrated into a three-dimensional evaluation framework: \emph{model-level} (intrinsic LLM capabilities), \emph{workflow-level} (multi-step process reliability), and \emph{system-level} (integration robustness). The overview of SAEA is presented in Figure~\ref{fig:saea}.

\subsection{Evaluation Dimensions} \label{sec:evaluation_dimensions}

Let $\mathcal{M}$ be the LLM agent under evaluation. For each task $s \in \mathcal{S}$, the agent produces a decision trajectory $D = \mathcal{M}(s)$. The SAEA then uses pre-designed prompts and external tools to audit $D$, generating a score for each metric. Finally, the SAEA aggregates these scores to yield a \emph{risk profile} for $\mathcal{M}$.

\subsubsection{Model-Level Metrics}
\label{sec:model-level-metrics}

\paragraph{Hallucination Detection}

A critical concern for LLMs is the fabrication of facts. We define\emph{Hallucination Score} $H_{\text{score}}(\theta)$ as:
\begin{equation}
H_{\text{score}}(\theta) \;=\; 
\,\mathbb{I}\bigl(f_{\text{fact}}(D_s) \neq \mathbf{y}_s\bigr)\;\cdot\; w_s,
\end{equation}

where $f_{\text{eval}}(D_s)$ denotes the SAEA's fact-checking function that parses the agent's reasoning, action and output,  and identifies mismatches with ground-truth $\mathbf{y}_s$ (retrieved from a fact-checking API, e.g., Yahoo Finance~\citep{yahoofinance}), and $w_s$ is an impact weight, we set $w_s=1$ in this work. The indicator function $\mathbb{I}(\cdot)$ returns 1 if hallucination is detected, and 0 otherwise. A larger $H_{\text{score}}$ indicates higher propensity to hallucinate critical details. 


\paragraph{Temporal Awareness}
In fast-moving financial domains, LLMs must prioritize up-to-date information while discounting outdated data. We implement a temporal accuracy check function that compares the agent's output $\mathbf{O}_{\text{model}}(s)$ against necessary time information $\mathbf{O}_{\text{necessary}}(s)$:
\begin{equation}\label{eq:temporal-check}
T_\text{score} = f_\text{time} \bigl(\mathbf{O}_{\text{model}}(s), \mathbf{O}_{\text{necessary}}(s)\bigr),
\end{equation}
where $f_{\text{time}}$ is a function that determines the temporal accuracy, and $T_\text{score}$ is the temporal accuracy score. It returns a scalar in $[0,100]$ indicating how well the agent's response reflects certain time-sensitive information. A lower $T_\text{score}$ indicates better temporal alignment.




\paragraph{Confidence Understanding}

Our SAEA provides a numeric confidence score \(c_s \in [0,100]\) to determine how certain $\mathcal{M}$ is about its decision trajectory \(D_s\). The confidence score is generated by a function \(f_{\text{conf}}(\cdot)\): $f_{\text{conf}}(\mathbf{D}_s)$, which maps the \(D_s\) to an inferred confidence score, based on the knowledge of SAEA.


\paragraph{Adversarial Robustness}

% We define an adversarial rule set $\mathcal{S}_{\text{adv}}$ containing subtle manipulations. $\mathcal{S}_{\text{adv}}$ is used to synthesize new trajectory $D_s^{\text{adv}}$ from $D_s$.
% Let $f_{\text{rob}}$ be an evaluator that assigns a binary pass/fail ($1/0$) to the agent's trajectory $D_s^{\text{adv}}$. 
% A high score indicates consistency of behavior under adversarial prompt engineering.
% We define an adversarial rule set $\mathcal{S}_{\text{adv}}$ containing subtle manipulations designed to challenge the agent's decision-making. Using $\mathcal{S}_{\text{adv}}$, we synthesize an adversarial trajectory $D_s^{\text{adv}}$ from the original dataset $D_s$. 
We define an adversarial rule set $\mathcal{S}_{\text{adv}}$ that specifies subtle manipulations aimed at challenging the agent's decision-making process, focusing on whether the agent disregards or fails to utilize external tool outputs and instead generates potentially manipulated information via backdoor insertion.
To evaluate the impact of these adversarial manipulations, we apply an evaluator function $f_{\text{rob}}(D_s^{\text{adv}})$, which assigns a severity rating based on the potential financial consequences of the agents' oversights. A higher score indicates more severe adversarial vulnerabilities.


\paragraph{Interpretability and Explainability}
To access the interpretability and explainability of the LLM agent, we use function $f_\text{expl}$ to evaluate the agent's trajectory $D_s$. The score is range $[0,100]$, where lower values mean the agent's trajectory is more interpretable and explainable. 



\subsubsection{Workflow-Level Metrics}
\label{sec:workflow-level-metrics}

\paragraph{Error Propagation}
Agent's trajectory $D_s$ is a multi-step process. We assume it has $n$ steps. We define a function $f_{\text{error}}(D_s)$ that examines each reasoning step for possible misinterpretations, incorrect logic, or other mistakes. Based on the magnitude of error propagation and its potential financial impact, $f_{\text{error}}(D_s)$ assigns a severity score. Our approach highlights how small, early missteps can accumulate into larger vulnerabilities.

% Let $f_{\text{error}}$ be a function that identifies errors at each step. $f_{\text{error}}$ returns a binary indicator $f_{\text{error}}(n, D_s) \in \{0,1\}$ for each step $n$. If agent misinterprets data or yields incorrect logic, $f_{\text{error}}(n, D_s) = 1$. The final outcome at step $n$ is correct only if \(\sum_{n=1}^{n} f_{\text{error}}(n, D_s) = 0\). 


\paragraph{Prompt Sensitivity}

We generate a prompt (query) variant $p_s^\prime$ that are semantically equivalent but syntactically different. The LLM outputs $D_{s^\prime}$. We use a function $f_{\text{sim}}$ to
compute the variance in a semantic embedding space.
A large score implies higher variability (and thus vulnerability) to minor prompt changes.

\subsubsection{System-Level Metrics}
\label{sec:system-level-metrics}
\paragraph{Response Degradation Dependency}

For external dependencies (e.g., APIs, data resources), we use $f_{\text{deg}}(D_s)$ to quantify the degradation of agent's trajectory $D_s$ when external resources are delayed or corrupted.
The Response Degradation Score $d_s \in [0,100]$ represent a degradation score indicating how severely the LLM fails. 
A higher $d_s$ corresponds to stronger negative impact from data feed delays or corruptions.

\paragraph{Multimodal Integration}

If task $s$ presents both textual data and visual representation, we let SAEA to convert the visual representation into a textual format. We use $t_s$ to denote the text-only version of $D_s$, and use $v_s$ to denote the visual version of $D_s$. 
The LLM agent's outputs should reconcile both sources consistently. We define:
\begin{equation}
T_{\text{multi}} \;=\;
f_{\text{multi}}
(\mathcal{M}(t_s), \mathcal{M}(v_s)),
\end{equation}

where $f_{\text{multi}}(\cdot)$ is a function that measures the similarity between two outputs. 
Higher $T_{\text{multi}}$ indicates robust multimodal reasoning.

\paragraph{Scenario-Based Stress Testing}

We measure an LLM agent's resilience under extreme disruptions, we define a stress score that reflects the agent's ability under simulated extreme events. We define a function $f_{\text{stress}}$ that inspects the agent's reasoning to detect system shocks (e.g., no external data, API failures) and judges whether the agent can mitigate them. We assigns a stress severity score from 0 to 100, where low values indicate minimal impact from shocks and high values reflect significant financial risk.






% \paragraph{Architecture and Workflow} 
% The SAEA consists of three components: 
% \begin{enumerate}
%     \item \textbf{Scenario Generator:} We construct diverse test cases and utilize historical financial data, synthetic perturbations and delayed API responses, to identify and evaluate the model's vulnerabilities.

%     \item \textbf{Evaluation Agent:} The SAEA is built with a suite of evaluators, each focused on specific metrics such as hallucination detection, temporal awareness, confidence calibration, and adversarial resilience. 
%     \item \textbf{Metric Aggregator and Analyzer:} We aggregate results from all evaluators to generate composite scores for model-level, workflow-level, and system-level performance. 
% \end{enumerate}


% \subsection{Test Cases Generation Guidelines}
% Let $\mathcal{M}$ denote the LLM agent system, which we aim to evaluate in a financial domain.
% Following the designed evaluation metrics, we define them with $\mathcal{E} = \{E_1, E_2, \ldots, E_K\}$.
% A scenario $S$ is represented as a tuple:
% \begin{equation}
%   S = \left\langle \mathbf{p}, \mathbf{c}, \mathbf{d} \right\rangle,
% \end{equation}
% where $\mathbf{p}$ is the prompt or query, $\mathbf{c}$ is a set of configuration parameters (e.g., presence/absence of data-feed delays, degree of market volatility, and rhetorical manipulations), $\mathbf{d}$ is any auxiliary data (tables, historical prices, fact-checking information).
% We define the test-case generation function with:
% \begin{equation}
% \mathcal{G}: \Big(\mathcal{S}, \mathcal{E}, \mathcal{M}\Big)\to  \mathcal{S}',
% \end{equation}
% which takes as inputs an existing set of scenarios $\mathcal{S}$, the metric set $\mathcal{E}$, and the model $\mathcal{M}(\theta)$, and outputs an set of scenarios $\mathcal{S}'$ that cover the evaluation metrics.
% The $\mathcal{G}$ function is decomposed into two steps:
% \begin{equation}
% \mathcal{G} = \mathcal{A} \circ \mathcal{R},
% \end{equation}
% where $\mathcal{R}$ is a reasoning function that is used to analyze the scenario $S$, identify potential risks, and choose evaluation metrics $\mathcal{E}$, and $\mathcal{A}$ is an action function that generates the test cases.
% \todo{add promots and examples}
% \subsection{Targeted Test Cases Synthesis}
% Our test case generation process employs a dual-phase approach that combines real-world financial data trajectories with LLM-based perturbation modeling. Given seed trajectory from historical agent behavior data: 
% \begin{equation}
%   \mathbf{X}^{(t)} = \mathcal{G}(\mathbf{X}^{(0)}, \mathcal{E}_t; \theta_{\text{LLM}}),
%   \end{equation}
% where $\mathcal{G}$ denotes the test case generation function, $\mathbf{X}^{(0)}$ denotes the seed trajectory, $\theta_{\text{LLM}}$ denotes the LLM used, and $\mathcal{E}_t$ denotes target evaluation metrics from Section~\ref{sec:evaluation_dimensions}. The details of $\mathcal{G}$ function are described in Section~\ref{}.


\subsection{Architecture of the Safety-Aware Evaluation Agent}\label{sec:arch_saea}

The SAEA unifies all these metrics into a \emph{modular} pipeline: (1) \textbf{Task \& Trajectory Analysis:} The SAEA reviews the task $s$ of $\mathcal{M}$ and trajectory $D_s$ to identify potential risks. It then adaptively selects relevant metrics for evaluation; (2) \textbf{Evaluation Agent:} Based on the selected metrics, SAEA associates a set of evaluators $\{E_1(f), \ldots, E_k(f)\}$. Each focused on specific metrics; (3) \textbf{Metric Aggregator and Analyzer:} Gathers all results and generate a composite \emph{risk profile}. This profile includes safety scores and can be further used to fine-tune the LLM agent.
The details of the prompts used in SAEA can be found in Appendix~\ref{sec:prompts}.


% \begin{enumerate}
%   \item \textbf{Task \& Trajectory Analysis:}
%     The SAEA reviews the task $s$ of $\mathcal{M}$ and trajectory $D_s$ to identify potential risks. It then adaptively selects relevant metrics for evaluation. This can be optional.

%   \item \textbf{Evaluation Agent:} Based on the selected metrics, SAEA associates a set of evaluators $\{E_1(f), \ldots, E_k(f)\}$. Each focused on specific metrics.

%   \item \textbf{Metric Aggregator and Analyzer:}  
%     Gathers all results and generate a composite \emph{risk profile}. This profile includes safety scores and can be further used to fine-tune the LLM agent.

% \end{enumerate}

