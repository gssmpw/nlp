\section{Introduction}


The financial domain has long been recognized where precision, safety, and trust are important~\citep{vcernevivciene2024explainable,bussmann2021explainable,buckley2021regulating}. In recent years, the development of large language models (LLMs) has unlocked huge potential for many tasks across finance, from regulatory compliance and market forecasting to advanced analytics of high-volume textual data~\citep{guo2024fine,shaffer2024scaling,kim2024financial}. Benchmarks such as InvestorBench~\citep{li2024investorbenchbenchmarkfinancialdecisionmaking} and Pixiu~\citep{xie2024pixiu} have emerged as an important way for evaluating LLM agents in finance. 




Yet, as LLM agents become ever more integrated into critical financial workflows, an urgent question arises: 
\begin{quote}
    \textbf{\textit{Do current benchmarks adequately assess the safety, reliability, and robustness of LLMs for high-stakes financial tasks?}}
\end{quote}
We argue that the \textbf{answer is no}. Existing benchmarks typically focus on task-specific metrics such as accuracy, F1 score, or ROUGE~\citep{xie2024pixiu}, which capture financial performance but ignore the unique safety challenges of finance. Unlike many standard LLM settings, financial systems are highly fragile and uncertain, where even minor errors can lead to cascading failures and substantial losses~\citep{desai2024opportunities,li2024mars}. 
% In such environments, a model that appears ``successful'' under conventional benchmarks may still fail in real-world scenarios, incurring systemic consequences~\citep{mcintosh2024inadequacies}.

\begin{table*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcllcl}
    \hline
    \textbf{Name} & \textbf{Year} & \textbf{Task} & \textbf{Evaluation Metrics} & \textbf{Risk Awareness} & \textbf{Potential Safety Issues} \\
    \hline
    PIXIU~\citep{xie2024pixiu} & 2024 & Multiple financial NLP tasks, stock prediction & 
    ACC, F1, MCC & \color{red_}{\ding{55}} & 
    \ding{193}, \ding{194}, \ding{196}, \ding{201} \\
    FLARE-ES~\citep{zhang2024dolares} & 2024 & Multiple financial NLP tasks & 
    Accuracy, ROUGE, MCC, BERTScore & \color{red_}{\ding{55}} & 
    \ding{192}, \ding{197}, \ding{198}, \ding{201} \\
    Hirano~\citep{hirano-2024-construction} & 2024 & Multiple financial NLP tasks & ACC & \color{red_}{\ding{55}} & 
    \ding{192}, \ding{194}, \ding{201} \\
    EconLogicQA~\citep{quan-liu-2024-econlogicqa} & 2024 & Financial question answering & ACC & \color{red_}{\ding{55}} & 
    \ding{192}, \ding{193}, \ding{198}, \ding{201} \\
    R-Judge~\citep{yuan-etal-2024-r} & 2024 & Safety judgment, risk identification & 
    F1, Recall, Specificity, Validity, Effectiveness, Alertness & \color{green_}{\ding{51}} & 
    \ding{196}, \ding{197}, \ding{198}, \ding{201} \\
    AlphaFin~\citep{li2024alphafin} & 2024 & Financial question answering, stock prediction & 
    ARR, ACC, 
    AERR, ANVOL, SR, MD, CR, MDD, ROUGE & \color{red_}{\ding{55}} & 
    \ding{194}, \ding{193}, \ding{201} \\
    InvestorBench~\citep{li2024investorbenchbenchmarkfinancialdecisionmaking} & 2024 & Trading task & 
    CR, SR, AV, MDD & \color{red_}{\ding{55}} & 
    \ding{192}, \ding{194}, \ding{197}, \ding{201} \\
    FinCoin~\citep{yu2024finconsynthesizedllmmultiagent} & 2024 & Stock prediction & CR, SR, MDD & \color{red_}{\ding{55}} & 
    \ding{193}, \ding{194}, \ding{196}, \ding{201} \\
    \hline
    \end{tabular}
    }
    \caption{Benchmarks of LLMs on Financial Applications and Associated Risks. The potential safety issues are: 
    \ding{192} Illusory Confidence, \ding{193} Hallucination, \ding{194} Lack of Temporal Awareness, 
    \ding{195} Poor Handling of Domain-Specific Reasoning, \ding{196} Adversarial Vulnerabilities, 
    \ding{197} Dependency on Prompt Design, \ding{198} Lack of Interpretability, 
    \ding{199} Limitations in Multimodal Integration, \ding{200} API/Tool Dependency, 
    \ding{201} Multi-Step Tasks.}
    \label{tab:financial_benchmarks}
\end{table*}
\vspace{-5pt}

The safety of LLM agents extends beyond their ability to generate accurate outputs; it includes their resilience in navigating the intricacies and dynamics of financial systems~\citep{ullah2024llms,boi2024smart}. 
A model that appears ``successful'' under conventional benchmarks may still fail in real-world scenarios, incurring systemic consequences~\citep{mcintosh2024inadequacies}.
For example, the Freysa AI agent lost \textbf{\$47,000} due to a security vulnerability, where users exploited attack prompts to manipulate the model into bypassing security checks and executing unauthorized transactions~\citep{park2024}. Similarly, a user lost \textbf{\$2,500} due to GPT-generated~\citep{achiam2023gpt} phishing content, which recommended a fraudulent site while writing a transaction bot~\citep{mitchell2024}. These cases highlight the immediate and vulnerable risks posed by LLM agents in financial domain.
% The dangers of illusory confidence and hallucinated reasoning have become increasingly evident, as demonstrated in the news of the Freysa AI agent losing \$47,000 due to a security vulnerability~\citep{park2024}. User utilized attack prompts to manipulate the model into bypassing security checks and executing unauthorized transactions.
% Similarly, a user lost \$2,500 due to GPT-generated phishing content, which which recommended a fraudulent site while writing a transaction bot, highlighting the immediate and vulnerable risks posed by LLMs~\citep{mitchell2024}. 

LLM agents are fundamentally predictive models that generate outputs based on statistical patterns in training data~\citep{yang2024unleashing,cheng2024exploring}. While this enables impressive performance across many tasks, it also introduces unique vulnerabilities. 
A significant issue is illusory confidence, where agents generate outputs with a confident tone despite reasoning errors~\citep{yadkori2024believe,Chen2025EngagingWA}. Such misplaced confidence can lead to incorrect recommendations~\citep{yang2024can,zhou2024large}. Similarly, hallucination~\citep{huang2024survey}, an inherent behavior of LLM agent, carries a clear risk. For example, fabricated financial metrics can influence decision-making systems and lead to monetary losses~\citep{rangapur2023investigating}.



Beyond these direct errors, LLM agents lack temporal awareness, which is a critical limitation in dynamic environments like finance. The models struggle to adapt to rapidly evolving markets, producing outputs based on outdated information~\citep{qiu2023large}.
The inability to handle domain-specific reasoning further increase risks, as financial systems require understanding of domain-specific language, regulations, and data structures~\citep{ke2025demystifying,zhao2023knowledgemath}. Another vulnerability is adversarial manipulation~\citep{xullm}. Even subtle changes to prompts or malicious inputs can exploit weaknesses in the agent system, resulting in misleading outputs~\citep{10.1145/3689217.3690621}.

% add metrics and evaluation
Current evaluation metrics for LLM agents in financial applications fall short. Accuracy-based benchmarks assess task performance but ignore fundamental safety concerns -- robustness, reliability, and resilience in high-stakes environments.
Financial systems demand more: an evaluation framework that puts agents to the test in dynamic environments, under adversarial conditions, and while meeting the practical requirements of real-world tasks.~\citep{zhao2024revolutionizing,chen2024a}.


In this work, we challenge the machine learning community to rethink how LLMs are evaluated for high-stakes domains. In finance, as in other critical domains, traditional benchmarks are insufficient to ensure safety.
\textbf{We posit a shift in focus from what LLM agents can do to what they must not do in financial domains}. Our work provides a roadmap
for developing LLM agents that are not only powerful but also risk-aware, ensuring safer deployment in financial decision-making. Our key contributions include:
\begin{itemize}
\setlength{\itemsep}{0pt}
    \item A study of financial LLM benchmarks, identifying critical safety gaps and introducing three-dimensional risk-aware evaluation metrics (model-level, workflow-level and system-level).
    \item An empirical evaluation of both API-based and open-weight LLM agents under our proposed risk metrics, revealing risks overlooked by traditional benchmarks.
    \item The introduction of a Safety-Aware Evaluation Agent (SAEA), to provide a comprehensive assessment of LLM agents in financial applications.
\end{itemize}

% we argue for a risk-aware evaluation framework adapted to the unique safety challenges posed by LLMs in finance. Our contributions include:
% \begin{itemize}
%     \item We study XX papers that evaluate LLMs in financial applications, identifying critical safety gaps and limitations in current benchmarks. 
%     \item By evaluating general-purpose LLMs and financial-specific LLMs, we reveal critical safety gaps masked by traditional benchmarks, emphasizing the need for more robust evaluation strategies.
%     \item We introduce adaptive evaluation agent that assess the safety, reliability, and robustness of LLMs in financial applications. With the diverse metrics, we aim to provide a comprehensive evaluation that captures the unique risks in financial systems.
% \end{itemize}





