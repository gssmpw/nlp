\section{Evaluation}
\input{main/results_table}
In this section, we validate our safety-aware evaluation framework on diverse financial tasks with multiple LLM agents. We include a comparison between traditional benchmark performance and our proposed risk-aware metrics. 


\subsection{Evaluated LLM Agents and Tasks}
We consider API-based and open-weights LLMs as the agents' backbone. 
For the API-based agents, we use GPT-4o~\citep{achiam2023gpt} and Claude-3.5-Sonnet~\citep{TheC3}. For the open-weights models, we use Llama-3.3-70b, Llama-3.1-8b~\citep{dubey2024llama}, and DeepSeek-R1~\citep{guo2025deepseek}.
We evaluate these agents on three categories of high-impact financial tasks to capture real-world complexities. The tasks are collected from R-Judge~\citep{yuan-etal-2024-r}: (1) Finance management, cryptocurrency use-cases (Bitcoin, Ethereum, Binance), (2) Webshop automation, online shop and Shopify integrations, (3) Transactional services, bank and PayPal scenarios. To ensure comparability, all models are tested on the same prompts, temperature, and data. We use zero-shot chain-of-thought (CoT) ~\citep{Wei2022ChainOT} for all tasks. For a fair comparison, we standardized all scores to a 0-100 scale.



\subsection{Results}
% \subsubsection{Performance on Traditional Benchmarks}

% We first benchmark each LLM agents on standard task-specific metrics: F1 score, Recall, Specificity, and Validity~\citep{yuan-etal-2024-r}. The F1 score measures the model's overall effectiveness in identifying risks. Recall focuses on the model's ability to correctly identify unsafe samples, while specificity evaluates its performance in recognizing safe cases. Validity assesses the proportion of samples where the model provides a definitive (``unsafe'' and ``safe'') label.
% Table~\ref{} provides a high-level summary. 

% % key insights and findings


% \paragraph{Proposed Risk-Aware Evaluation}
We apply our SAEA framework to assess how well different models manage risks at three levels: model-level (Section~\ref{sec:model-level-metrics}), workflow-level (Section~\ref{sec:workflow-level-metrics}), and system-level (Section~\ref{sec:system-level-metrics}). The evaluation is conducted on agent trajectories from three tasks using five different LLMs in SAEA, measuring performance across multiple dimensions. The examples of using SAEA to evaluate the LLM agents are shown in Appendix~\ref{sec:appendix:saea_examples}.

To analyze risk handling, we categorize the trajectories into two groups based on ground truth labels: \textit{``safe''} and \textit{``unsafe''}. The evaluation results are presented in Table~\ref{tab:llm_comparison}, where each score is reported as a pair, with the value to the left of the slash (``/'') corresponding to \textit{``safe''} trajectories and the value to the right corresponding to \textit{``unsafe''} trajectories. The scores range from $0$ to $100$, where lower scores indicate lower risk presence or greater robustness against risks.

Our SAEA framework assigns lower scores to \textit{``safe''} trajectories compared to \textit{``unsafe''} trajectories, indicating that SAEA can accurately evaluate an agent's risk based on its actions and decision trajectories. We also observe that the size of the evaluation LLM may influence the results; for instance, a smaller LLM, {Llama-3.1-8b}, tends to assign higher scores to \textit{``safe''} trajectories, leading to less stable evaluations. Additionally, the open-weight LLMs {DeepSeek-R1} and {Llama-3.3-70b} exhibit similar behavior to API-based LLMs such as {GPT-4o} and {Claude-3.5-Sonnet}, supporting the consistency of SAEA across different evaluation LLMs. 



% key insights and findings

\vspace{-10pt}
\section{Discussion}


\subsection{Lessons Learned}

% \paragraph{The Accuracy-Safety Dichotomy.}
% Our findings indicate that traditional benchmarks fail to capture the real-world risks of LLM agents. For instance, agents achieving XX\% accuracy exhibit XX\% failure rate under our stress tests. Similarly, the \textit{confidence score} observed in our experiments highlight a critical gap: XX\% of hallucinated trajectory were presented with $>90\%$ confidence scores. \todo{need revise}

\paragraph{Real-World Uncertainty}  
Traditional benchmarks fall short in capturing the real-world risks faced by LLM agents. Existing benchmarks often impose restrictions on both the scenarios and the sources of risk, limiting their applicability. However, real-world applications encounter unpredictable risks and unknown attack vectors, introducing what we refer to as open-world risks.  
An LLM agent operates within a complex system composed of multiple components, each of which is susceptible to different types of failures and vulnerabilities. As a result, evaluating an agentâ€™s robustness requires a more comprehensive framework that accounts for these diverse and evolving real-world threats.



\paragraph{LLM Agent Vulnerabilities Are Domain-Sensitive}

Our empirical findings indicate that even top-tier LLM agents exhibit different failure modes under adversarial vs.\ normal scenarios. For instance, \emph{Hallucination} may remain relatively low during benign queries but explode in complex prompts that combine partial truths with fabricated data. This suggests that context matters immensely: evaluations that only measure performance on query, risk overlooking critical vulnerabilities in real-world use.


\paragraph{High-Performance Models Can Still Lack Robustness}

Though certain LLM agents achieved strong scores on standard metrics (e.g., F1), they struggled to maintain reliability when facing malicious prompts. This tension highlights that \emph{accuracy does not equate to safety}. As the financial domain demands both precision and trustworthiness, future LLM agents design must incorporate explicit mechanisms (e.g., domain-specific verification modules, real-time anomaly detection) to address scenario-driven risks.


\paragraph{SAEA Reveals Hidden Failures}
Our SAEA uncovered hidden failure modes, especially when multiple perturbations (e.g., data-feed delays, rhetorical manipulation, inconsistent time-stamped information) were combined. Critical flaws can be detected, without having to wait until the actual deployment. 



\subsection{Challenges in Implementation}

\paragraph{Accessing Real-Time Financial Data}
Many LLMs lack the ability to integrate up-to-date market feeds. APIs are often restricted, and data vendors charge high fees for real-time financial information. Ensuring timely data updates without overcomplicating system design is a nontrivial challenge.

\paragraph{Constructing Risk-Focused Benchmarks}
Designing test sets that reflect the risks in financial systems is both logistically and ethically challenging. 
On one hand, scenarios must be realistic to expose weaknesses; on the other, they must be wide-ranging enough to cover edge cases. Achieving this balance often requires human-in-the-loop oversight to inject domain expertise into synthetic data generation and adversarial prompt design. 


\paragraph{Performance vs.\ Safety Trade-offs}

By employing SAEA to audit decision in the LLM agents, financial institutions can more effectively detect potential safety failures. However, this auditing can introduce overheads, which can increase response latency to constraints on generative breadth and may reduce raw performance on conventional benchmarks. For example, real-time inspections of reasoning constraint checks might slow down high-frequency trading systems. Thus, financial institutions face a trade-off: achieving maximum throughput and fluency versus ensuring robust safeguards against manipulation or error propagation. 
We posit that in high-stakes environments, even a marginal improvement in risk mitigation can far outweigh modest dips in standard metrics, given the outsized reputational and monetary damage unsafe outputs can cause. 






\paragraph{Regulatory and Compliance Overhead}
Financial organizations operate under strict regulations (e.g., SEC in the US). Integrating LLM agents into compliance processes is nontrivial, as it necessitates explainable outputs and reliable auditing trails. Many current LLM agent architectures struggle with consistent, legally admissible rationales. Bridging this gap may require hybrid systems that combine symbolic reasoning components during the reasoning.


\subsection{Post-Hoc Analysis and Traceable Pipelines via SAEA}

Conventional metrics (e.g., accuracy) offer static snapshots of performance but rarely reveal how small missteps escalate into systemic faults. 
For example, the 2007 subprime mortgage crisis caused partly by rating-model oversights and lax underwriting, illustrates the need for retrospective inquiry~\citep{reinhart20082007}. By the time negative outcomes surfaced, investigators had to reconstruct a cascade of failures spanning multiple stakeholders. It highlights the importance of post-hoc analysis in finance: it enables practitioners to identify root causes and track how minor lapses can lead to broad disruptions.

\paragraph{SAEA for Risk Explanation and Analysis}
Our SAEA anchors the post-hoc diagnostic process. We preserve a comprehensive audit trail of every operational step, including reasoning records, prompt interpretations, and references to external data sources. Our structured log makes it possible to revisit an unexpected outcome, such as an anomalous portfolio allocation or questionable market advisory, and determine exactly where the system deviated.
SAEA collects the agent data and analyzes its safety. If the agent cites outdated interest rates during a risk calculation, SAEA flags the inconsistency between the model's stated value and a verified historical database. In subsequent investigations, developers can identify which version of the LLM agents led to the flawed conclusion. Such traceability is important when a single erroneous subtask feeds into sequential tasks like volatility forecasting. These findings can guide targeted improvements, such as refined prompts, additional training data, or domain-specific filters. Instead of relying on reactive patches, system architects gain a continuous feedback loop that systematically addresses known failure modes and raises overall robustness.

\paragraph{Long-Term Reliability and Compliance}

Most financial authorities require auditable models for tasks ranging from credit assessment to algorithmic trading~\citep{}. Our logs satisfy such requirements by documenting each step the agent takes, its data inputs, and the transformations. Our transparency clarifies accountability and helps institutions demonstrate that they can govern their AI-driven strategies.
Over extended periods, post-hoc analysis exposes slow-burning issues that might otherwise go undetected. SAEA's logging supports time-series scrutiny, allowing teams to detect subtle action drift or environmental changes before they degrade performance. Our layered monitoring guards against both sudden and incremental threats to long-term reliability.


