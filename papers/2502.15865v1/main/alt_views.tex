\section{Alternative Views}
\label{sec:alt-views}

While our position highlights the distinct risks posed by LLM agents in finance and the need for risk-aware evaluations, we acknowledge two prevalent perspectives that question this stance. 

\subsection{View 1: Standard Benchmarks Already Capture LLM Agent Reliability}

One perspective states that traditional NLP evaluation metrics, such as accuracy, F1 scores, and even domain-specific financial indicators (e.g., sentiment classification accuracy in earnings call analyses), are sufficient for assessing the overall reliability of LLMs in real-world financial applications.
Advocates of this perspective argue:
\begin{itemize}
    \item \emph{Correlation with Agent Competence:} 
    Strong performance on established benchmarks is interpreted as a proxy for an LLM agent's overall capability, implying that models excelling on standard datasets exhibit fewer intrinsic failure modes (e.g., hallucination, flawed reasoning) in production~\citep{xiao2025tradingagentsmultiagentsllmfinancial,wu2023bloomberggpt}.
    
    \item \emph{Incremental Benchmark Evolution:} Standardized benchmarks can be continuously refined by incorporating new challenge sets (e.g., adversarial question-answering tasks)~\citep{xie2024finben,islam2023financebench}, progressively mitigating known weaknesses in LLMs.

    \item \emph{Ease of Comparison:} 
    The well-established metrics enable efficient performance comparisons across multiple LLM agents, which simplifies evaluation and adoption for financial stakeholders seeking to identify the most suitable model without the complexity of domain-specific assessment methodologies~\citep{xie2024pixiu,li2024investorbenchbenchmarkfinancialdecisionmaking,xie2024finben}.
    \end{itemize}

\paragraph{Counterarguments.}
We do not deny the usefulness of these benchmarks for basic proficiency. However, purely accuracy-driven evaluations overlook LLM-specific vulnerabilities, such as:
\vspace{-5pt}
\begin{itemize}
    \item \emph{Language-Induced Attack Vectors:} Subtle rhetorical or ``jailbreak'' prompts can bypass standard filters, leading to unsafe outputs even if the model scores highly on standardized datasets~\citep{shen2024anything}.
    \item \emph{Systematic Hallucination Patterns:} Traditional metrics rarely detect when an LLM concocts facts under complex or misleading instructions~\citep{kang2023deficiency}. A flaw that can lead to severe decisions if the agent is tasked with autonomously analyzing financial data~\citep{roychowdhury2023hallucination}.
    \item \emph{Unpredictable Decision Pathways:} CoT or ReAct reasoning remains opaque in standard evaluations~\citep{Wei2022ChainOT,yao2022react}. Agents can arrive at correct answers for the ``wrong reasons,'' which masks potential misalignment~\citep{turpin2024language}.
\end{itemize}
Therefore, while standard benchmarks remain a valuable baseline, they do not fully expose the agent-level risks that arise from an LLM's generative and self-reasoning capabilities.

\subsection{View 2: Engineering Solutions Can Mitigate LLM Agent Risks}

Another perspective argues that direct engineering interventions can mitigate the inherent risks associated with LLM agents, such as guardrail modules~\citep{chu2024causal}, advanced prompt filtering~\citep{gao2024brief}, and extended fine-tuning~\citep{qi2024finetuning}. Thus, it is sufficient to rely on these solutions to ensure the safety of LLMs in financial applications. Advocates reason that:
\vspace{-5pt}
\begin{itemize}
    \item \emph{Agent-Level Guardrails:} 
    Mechanisms such as content filtering systems and CoT verification modules can intercept and correct potentially unsafe model outputs~\citep{ling2023deductive}, thereby reducing the likelihood of misinformation or harmful decision-making.
    \item \emph{Alignment with Minimal Overhead:} 
    Approaches such as reinforcement learning from human feedback (RLHF)~\citep{ouyang2022training} or fine-tuning with domain-specific datasets~\citep{li2023large} can enhance factual reliability, without requiring stress testing of every decision-making pathway within the model.
    \item \emph{Rapid Iteration:} Continuous updates to the LLM's weights, and carefully engineered prompts may adapt the agent's behaviors faster than risk metrics~\citep{chen2023unleashing}. 
    
\end{itemize}

\paragraph{Counterarguments.}
We support robust engineering interventions. However, even comprehensive alignment strategies may not capture all vulnerabilities inherent to LLMs, unless they are repeatedly tested under realistic, agent-centric scenarios:
\vspace{-5pt}
\begin{itemize}
    \item \emph{Emerging Exploitation Strategies:} 
    Attackers continuously develop new techniques to manipulate an LLM's conversational logic, such as chaining multi-step misleading queries~\citep{xhonneux2024efficient}. Without specialized evaluations designed to simulate these adversarial interactions, critical security loopholes may remain undetected.
    
    \item \emph{Hallucination in Edge Cases:} 
    When exposed to incomplete data, LLMs may exhibit misplaced confidence in their reasoning, leading to erroneous conclusions~\citep{Chen2025EngagingWA}. Engineering solutions typically address known failure modes but may overlook emergent anomalies that arise when the model is required to make complex, interdependent decisions.
    \item \emph{The Challenge of Continuous Adaptation:} Real-world usage of LLM agents evolves rapidly; the system may be extended for new tasks or connected to external APIs. Without an ongoing risk-centric evaluation pipeline, engineering fixes become outdated as the agent's operational scope expands.
\end{itemize}
Therefore, while engineering solutions are essential, they are not sufficient to guarantee the safety. Agent-level vulnerabilities require domain-sensitive validation to ensure that new functionalities do not introduce dangerous behaviors.

