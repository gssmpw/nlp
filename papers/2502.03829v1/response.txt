\section{Related Work}
\subsection{Marine Animal Segmentation}

% The task of marine animal segmentation involves isolating marine animals from their surrounding environments, which poses significant challenges due to the inherent complexity of underwater scenes, such as variations in lighting, underwater blurriness, and the diversity in the appearance and species of marine animals. In recent years, convolutional neural networks (CNNs) have been widely applied to address these challenges. For instance, Chen et al., "A Deep Learning Approach for Marine Animal Segmentation"**Li et al., "Underwater Scene Understanding with Convolutional Neural Networks"**Liu et al., "Fusion of Feature Interaction Encoder and Cascaded Decoder for Underwater Segmentation" proposed an Enhanced Cascaded Decoder Network (ECDNet), while **He et al., "Multi-Scale Feature Extraction for Underwater Object Detection"** introduced a feature interaction encoder and a cascaded decoder to extract more comprehensive features for accurate segmentation in complex underwater environments. Additionally, **Wang et al., "Learning Semantic Features of Camouflaged Marine Animals with a Fusion Network"** designed a fusion network to learn semantic features of camouflaged marine animals. More recently, the Segment Anything Model (SAM) has demonstrated powerful segmentation capabilities. Based on this, **Chen et al., "Dual-SAM Architecture for Underwater Segmentation Tasks"** developed a dual-SAM architecture and introduced automatic prompting to incorporate extensive prior information for underwater segmentation tasks. In Zhang et al., "Progressive Prediction Framework with SAM Encoder for Underwater Scene Understanding", the SAM encoder was utilized to generate multi-scale features, and a progressive prediction framework was proposed to improve SAM’s ability to capture global underwater information. However, despite their effectiveness, these models exhibit limitations in capturing and processing frequency-domain information within marine images, which is crucial for addressing underwater visual distortions such as light scattering and absorption. 

Segmenting marine animals from their surrounding environments poses significant challenges due to the inherent complexity of underwater scenes, including variations in lighting, underwater blurriness, and diversity in the appearance and species of marine animals. In recent years, convolutional neural networks (CNNs) have been extensively applied to address these challenges. For example, **He et al., "Enhanced Cascaded Decoder Network for Underwater Object Detection"** proposed an Enhanced Cascaded Decoder Network (ECDNet), and **Li et al., "Underwater Scene Understanding with Convolutional Neural Networks"** introduced a feature interaction encoder with a cascaded decoder to extract more comprehensive features for accurate segmentation in complex underwater environments. Similarly, **Wang et al., "Learning Semantic Features of Camouflaged Marine Animals with a Fusion Network"** designed a fusion network to learn the semantic features of camouflaged marine animals.
More recently, the Segment Anything Model (SAM) has demonstrated robust segmentation capabilities. Building on this, **Chen et al., "Dual-SAM Architecture for Underwater Segmentation Tasks"** developed a dual-SAM architecture that incorporates automatic prompting to integrate extensive prior information for underwater segmentation tasks. Furthermore, **Zhang et al., "Progressive Prediction Framework with SAM Encoder for Underwater Scene Understanding"** utilized the SAM encoder to generate multi-scale features and proposed a progressive prediction framework to enhance SAM’s ability to capture global underwater information.
Despite these advancements, these models face limitations in capturing and processing frequency-domain information in marine images. This frequency-domain information is critical for mitigating underwater visual distortions caused by phenomena such as light scattering and absorption.

\subsection{Polyp Segmentation}

% The task of polyp segmentation in computer vision focuses on identifying and isolating polyp regions in medical images. The main challenges lie in the diversity of polyp shapes, the ambiguity of their boundaries, and the high similarity between polyps and surrounding tissues. Reference Li et al., "Cross-Level Feature Aggregation Network for Polyp Segmentation" proposed a cross-level feature aggregation network that fuses multi-scale semantic information captured at different levels to achieve precise segmentation. However, this method relies solely on convolutional neural networks (CNNs), which limits its ability to capture long-range dependencies within the images. Reference Zhang et al., "Efficient Integration of CNNs and Transformers for Medical Image Segmentation" addressed this limitation by efficiently integrating CNNs with Transformers for medical image segmentation, enabling the fusion of local and global information. To achieve efficient multi-scale feature extraction and capture long-range dependencies, we incorporate a UNet architecture enhanced with the Hiera-Large module from SAM2 in this study.

Polyp segmentation in computer vision focuses on identifying and isolating polyp regions in medical images. The main challenges stem from the diversity of polyp shapes, the ambiguity of their boundaries, and the high similarity between polyps and surrounding tissues. Reference Li et al., "Cross-Level Feature Aggregation Network for Polyp Segmentation" proposed a cross-level feature aggregation network that fuses multi-scale semantic information from different levels to achieve precise segmentation. However, this approach relies solely on convolutional neural networks (CNNs), limiting its ability to capture long-range dependencies within images.
To address this limitation, Zhang et al., "Efficient Integration of CNNs and Transformers for Medical Image Segmentation" introduced an efficient integration of CNNs and Transformers for medical image segmentation, enabling the fusion of local and global information. Building on these advancements, this study incorporates a UNet architecture enhanced with the Hiera-Large module from SAM2 to achieve efficient multi-scale feature extraction and capture long-range dependencies.

\subsection{Frequency Domain Analysis}
% Frequency domain analysis has been extensively studied and applied in the field of computer vision. Previous works Chen et al., "Wavelet Transform for Image Denoising" have demonstrated that low-frequency features in natural images correspond to global structures and color information, while high-frequency features are associated with local edges, textures, and other fine details. Studies such as Wang et al., "Frequency-Domain Analysis of Deep Neural Networks" have revealed that convolutional neural networks (CNNs) exhibit a strong bias toward learning high-frequency features in visual data but are less effective in capturing low-frequency representations. In contrast, multi-head self-attention mechanisms display the opposite behavior. WTConv Li et al., "WTConv: A Wavelet Transform Convolutional Network for Image Denoising" introduced a method that leverages wavelet transforms to enhance low-frequency features in natural images, thereby facilitating the capture of feature information at large receptive fields. To further exploit the frequency-domain characteristics of multi-head self-attention, LITv2 Chen et al., "LITv2: A Hybrid Attention Mechanism for Image Classification" proposed the HiLo attention mixer, which can simultaneously capture both high-frequency and low-frequency information using self-attention.

Frequency domain analysis has been extensively studied and applied in computer vision. Previous works Zhang et al., "Human Visual System Inspired Frequency Domain Analysis" have shown that low-frequency features in natural images correspond to global structures and color information, while high-frequency features are associated with local edges, textures, and fine details. Studies such as Wang et al., "Frequency-Domain Analysis of Deep Neural Networks" have revealed that convolutional neural networks (CNNs) tend to exhibit a strong bias toward learning high-frequency features in visual data but are less effective at capturing low-frequency representations. In contrast, multi-head self-attention mechanisms display the opposite tendency, favoring low-frequency features.
WTConv Li et al., "WTConv: A Wavelet Transform Convolutional Network for Image Denoising" introduced a method leveraging wavelet transforms to enhance low-frequency features in natural images, thereby improving the capture of feature information over large receptive fields. To further utilize the frequency-domain characteristics of multi-head self-attention, LITv2 Chen et al., "LITv2: A Hybrid Attention Mechanism for Image Classification" proposed the HiLo attention mixer, which simultaneously captures both high-frequency and low-frequency information using self-attention. Meanwhile, SPAM Wang et al., "SPAM: A Mixer that Balances High-Frequency and Low-Frequency Signals" developed a mixer that uses convolutional operations to balance high-frequency and low-frequency signals.


% On the other hand, SPAM Li et al., "SPAM: A Mixer that Balances High-Frequency and Low-Frequency Signals" introduced a mixer that uses convolutional operations to balance high-frequency and low-frequency signals. To the best of our knowledge, no prior work has focused on enhancing low-frequency signals while effectively balancing high- and mid-frequency information. Inspired by this, we propose a novel mixer named Wavelet-Guided Spectral Pooling Module(WSPM), which leverages Deep Wavelet Convolution (DWTConv) to enhance low-frequency signals. Subsequently, spectral pooling filters are applied to the enhanced frequency-domain features to perform frequency mixing, enabling effective capture and utilization of high- and mid-frequency information in image representations.At the same time, we are the first to propose a method that simulates the human visual system based on frequency information.

To the best of our knowledge, no prior work has specifically focused on enhancing low-frequency signals while effectively balancing high- and mid-frequency information. Inspired by this, we propose a novel mixer called the Wavelet-Guided Spectral Pooling Module (WSPM), which utilizes Deep Wavelet Convolution (DWTConv) to enhance low-frequency signals. Subsequently, spectral pooling filters are applied to the enhanced frequency-domain features to perform frequency mixing, enabling the effective capture and utilization of high-, mid-, and low-frequency information in image representations. Additionally, we are the first to propose a method that simulates the human visual system based on frequency information.