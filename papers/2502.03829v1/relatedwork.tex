\section{Related Work}
\subsection{Marine Animal Segmentation}

% The task of marine animal segmentation involves isolating marine animals from their surrounding environments, which poses significant challenges due to the inherent complexity of underwater scenes, such as variations in lighting, underwater blurriness, and the diversity in the appearance and species of marine animals. In recent years, convolutional neural networks (CNNs) have been widely applied to address these challenges. For instance, \cite{ECDNet} proposed an Enhanced Cascaded Decoder Network (ECDNet), while \cite{Marine_Related_2} introduced a feature interaction encoder and a cascaded decoder to extract more comprehensive features for accurate segmentation in complex underwater environments. Additionally, \cite{MASNet} designed a fusion network to learn semantic features of camouflaged marine animals. More recently, the Segment Anything Model (SAM) has demonstrated powerful segmentation capabilities. Based on this, \cite{Dual-SAM} developed a dual-SAM architecture and introduced automatic prompting to incorporate extensive prior information for underwater segmentation tasks. In \cite{MAS-SAM}, the SAM\cite{SAM} encoder was utilized to generate multi-scale features, and a progressive prediction framework was proposed to improve SAM’s ability to capture global underwater information. However, despite their effectiveness, these models exhibit limitations in capturing and processing frequency-domain information within marine images, which is crucial for addressing underwater visual distortions such as light scattering and absorption. 

Segmenting marine animals from their surrounding environments poses significant challenges due to the inherent complexity of underwater scenes, including variations in lighting, underwater blurriness, and diversity in the appearance and species of marine animals. In recent years, convolutional neural networks (CNNs) have been extensively applied to address these challenges. For example, \cite{ECDNet} proposed an Enhanced Cascaded Decoder Network (ECDNet), and \cite{Marine_Related_2} introduced a feature interaction encoder with a cascaded decoder to extract more comprehensive features for accurate segmentation in complex underwater environments. Similarly, \cite{MASNet} designed a fusion network to learn the semantic features of camouflaged marine animals.
More recently, the Segment Anything Model (SAM) has demonstrated robust segmentation capabilities. Building on this, \cite{Dual-SAM} developed a dual-SAM architecture that incorporates automatic prompting to integrate extensive prior information for underwater segmentation tasks. Furthermore, \cite{MAS-SAM} utilized the SAM encoder to generate multi-scale features and proposed a progressive prediction framework to enhance SAM’s ability to capture global underwater information.
Despite these advancements, these models face limitations in capturing and processing frequency-domain information in marine images. This frequency-domain information is critical for mitigating underwater visual distortions caused by phenomena such as light scattering and absorption.

\subsection{Polyp Segmentation}

% The task of polyp segmentation in computer vision focuses on identifying and isolating polyp regions in medical images. The main challenges lie in the diversity of polyp shapes, the ambiguity of their boundaries, and the high similarity between polyps and surrounding tissues. Reference \cite{CFANet} proposed a cross-level feature aggregation network that fuses multi-scale semantic information captured at different levels to achieve precise segmentation. However, this method relies solely on convolutional neural networks (CNNs), which limits its ability to capture long-range dependencies within the images. Reference \cite{H2Former} addressed this limitation by efficiently integrating CNNs with Transformers for medical image segmentation, enabling the fusion of local and global information. To achieve efficient multi-scale feature extraction and capture long-range dependencies, we incorporate a UNet architecture enhanced with the Hiera-Large module from SAM2 in this study.

Polyp segmentation in computer vision focuses on identifying and isolating polyp regions in medical images. The main challenges stem from the diversity of polyp shapes, the ambiguity of their boundaries, and the high similarity between polyps and surrounding tissues. Reference \cite{CFANet} proposed a cross-level feature aggregation network that fuses multi-scale semantic information from different levels to achieve precise segmentation. However, this approach relies solely on convolutional neural networks (CNNs), limiting its ability to capture long-range dependencies within images.
To address this limitation, \cite{H2Former} introduced an efficient integration of CNNs and Transformers for medical image segmentation, enabling the fusion of local and global information. Building on these advancements, this study incorporates a UNet architecture enhanced with the Hiera-Large module from SAM2 to achieve efficient multi-scale feature extraction and capture long-range dependencies.

\subsection{Frequency Domain Analysis}
% Frequency domain analysis has been extensively studied and applied in the field of computer vision. Previous works \cite{Frequency_related_1,Frequency_related_2} have demonstrated that low-frequency features in natural images correspond to global structures and color information, while high-frequency features are associated with local edges, textures, and other fine details. Studies such as \cite{Frequency_related_3,Frequency_related_4} have revealed that convolutional neural networks (CNNs) exhibit a strong bias toward learning high-frequency features in visual data but are less effective in capturing low-frequency representations. In contrast, multi-head self-attention mechanisms display the opposite behavior. WTConv \cite{Frequency_related_7} introduced a method that leverages wavelet transforms to enhance low-frequency features in natural images, thereby facilitating the capture of feature information at large receptive fields. To further exploit the frequency-domain characteristics of multi-head self-attention, LITv2 \cite{Frequency_related_5} proposed the HiLo attention mixer, which can simultaneously capture both high-frequency and low-frequency information using self-attention.

Frequency domain analysis has been extensively studied and applied in computer vision. Previous works \cite{Frequency_related_1,Frequency_related_2} have shown that low-frequency features in natural images correspond to global structures and color information, while high-frequency features are associated with local edges, textures, and fine details. Studies such as \cite{Frequency_related_3,Frequency_related_4} have revealed that convolutional neural networks (CNNs) tend to exhibit a strong bias toward learning high-frequency features in visual data but are less effective at capturing low-frequency representations. In contrast, multi-head self-attention mechanisms display the opposite tendency, favoring low-frequency features.
WTConv \cite{Frequency_related_7} introduced a method leveraging wavelet transforms to enhance low-frequency features in natural images, thereby improving the capture of feature information over large receptive fields. To further utilize the frequency-domain characteristics of multi-head self-attention, LITv2 \cite{Frequency_related_5} proposed the HiLo attention mixer, which simultaneously captures both high-frequency and low-frequency information using self-attention. Meanwhile, SPAM \cite{SPANet} developed a mixer that uses convolutional operations to balance high-frequency and low-frequency signals.


% On the other hand, SPAM \cite{SPANet} introduced a mixer that uses convolutional operations to balance high-frequency and low-frequency signals. To the best of our knowledge, no prior work has focused on enhancing low-frequency signals while effectively balancing high- and mid-frequency information. Inspired by this, we propose a novel mixer named Wavelet-Guided Spectral Pooling Module(WSPM), which leverages Deep Wavelet Convolution (DWTConv) to enhance low-frequency signals. Subsequently, spectral pooling filters are applied to the enhanced frequency-domain features to perform frequency mixing, enabling effective capture and utilization of high- and mid-frequency information in image representations.At the same time, we are the first to propose a method that simulates the human visual system based on frequency information.

To the best of our knowledge, no prior work has specifically focused on enhancing low-frequency signals while effectively balancing high- and mid-frequency information. Inspired by this, we propose a novel mixer called the Wavelet-Guided Spectral Pooling Module (WSPM), which utilizes Deep Wavelet Convolution (DWTConv) to enhance low-frequency signals. Subsequently, spectral pooling filters are applied to the enhanced frequency-domain features to perform frequency mixing, enabling the effective capture and utilization of high-, mid-, and low-frequency information in image representations. Additionally, we are the first to propose a method that simulates the human visual system based on frequency information.