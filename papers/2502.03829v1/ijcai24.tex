%%%% ijcai24.tex

    \typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{multicol}
\usepackage{multirow}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subcaption}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{
% FE-UNet: Frequency Domain Enhanced U-Net with Segment Anything Capability
FE-UNet: Frequency Domain Enhanced U-Net with Segment Anything Capability for Versatile Image Segmentation}


% Single author syntax
% \author{
% Guohao Huo
% \affiliations
% School of Information and Software Engineering, University of Electronic Science and Technology of China
% \emails
% gh.huo513@gmail.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Guohao Huo$^1$\,
Ruiting Dai$^1$\,
Ling Shao$^2$\,
Hao Tang$^3$\thanks{Corresponding author}\\
\affiliations
$^1$University of Electronic Science and Technology of China\\
$^2$University of Chinese Academy of Sciences\\
$^3$Peking University\\
\emails{
gh.huo513@gmail.com,
rtdai@uestc.edu.cn,
ling.shao@ieee.org,
hao.tang@vision.ee.ethz.ch
}}
% \fi
\begin{document}

\maketitle

\begin{abstract}
    % Image segmentation plays a crucial role in visual understanding. Convolutional Neural Networks (CNNs) are highly inclined to learn high-frequency features in images, whereas transformers exhibit the opposite behavior. In this paper, we experimentally quantify the contrast sensitivity function of CNNs and compare it with the contrast sensitivity function of the human visual system derived through extensive experiments by Mannos and Sakrison. Based on these insights, we propose the Wavelet-Guided Spectral Pooling Module(WSPM) to enhance and balance image features in the frequency domain. Furthermore, to simulate the human visual system, we introduce the the Frequency Domain Enhanced Receptive Field Block (FE-RFB) module, which integrates the WSPM to extract features from the enhanced frequency domain in a manner similar to the human visual system. We also construct the FE-UNet model, using SAM2 as the backbone and incorporating Hiera-Large as the pre-trained block, aiming to improve the model's generalization capability while maintaining segmentation accuracy. Preliminary tests show that our model achieves state-of-the-art (SOTA) performance in tasks such as marine animal segmentation and polyp segmentation.

Image segmentation is a critical task in visual understanding. Convolutional Neural Networks (CNNs) are predisposed to capture high-frequency features in images, while Transformers exhibit a contrasting focus on low-frequency features. In this paper, we experimentally quantify the contrast sensitivity function of CNNs and compare it with that of the human visual system, informed by the seminal experiments of Mannos and Sakrison.
Leveraging these insights, we propose the Wavelet-Guided Spectral Pooling Module (WSPM) to enhance and balance image features across the frequency domain. To further emulate the human visual system, we introduce the Frequency Domain Enhanced Receptive Field Block (FE-RFB), which integrates WSPM to extract enriched features from the frequency domain.
Building on these innovations, we develop FE-UNet, a model that utilizes SAM2 as its backbone and incorporates Hiera-Large as a pre-trained block, designed to enhance generalization capabilities while ensuring high segmentation accuracy. Experimental results demonstrate that FE-UNet achieves state-of-the-art performance in diverse tasks, including marine animal and polyp segmentation, underscoring its versatility and effectiveness.
\end{abstract}

\section{Introduction}

% Image segmentation is a fundamental task in computer vision, serving as the basis for subsequent image analysis and understanding. By extracting key features and structural information from images, segmentation has enabled a wide range of downstream applications in natural and medical fields, such as marine animal segmentation and polyp segmentation. While numerous specialized architectures have been proposed to achieve outstanding performance on these diverse tasks, challenges remain due to the complex frequency-domain information inherent in natural images. Enhancing image features in the frequency domain to improve segmentation performance continues to be a significant hurdle.

Image segmentation is a cornerstone task in computer vision, forming the foundation for advanced image analysis and understanding. By isolating key features and structural details within images, segmentation has empowered numerous applications across diverse domains, including natural and medical fields such as marine animal segmentation and polyp segmentation. Despite the development of various specialized architectures achieving exceptional performance, significant challenges persist due to the complex frequency-domain characteristics of natural images. Enhancing image features in the frequency domain to boost segmentation performance remains a critical obstacle.

% Many methods leveraging deep convolutional neural networks (CNNs) have achieved remarkable improvements in segmentation accuracy. However, these methods often favor learning high-frequency features from natural images, leading to suboptimal performance when dealing with images dominated by low-frequency information—for instance, images of marine animals with large regions of uniform color. In marine animal segmentation tasks, the underwater environment introduces non-uniform illumination and scattering effects, resulting in hazy and blurred images that heavily distort frequency-domain information. Similarly, in the case of polyp segmentation, uneven illumination from endoscopic devices and imaging noise make low-frequency information more prominent than high-frequency details, posing significant challenges for fine-grained segmentation.

Deep convolutional neural networks (CNNs) have substantially advanced segmentation accuracy. However, CNNs are inherently biased toward learning high-frequency features, often leading to suboptimal outcomes when processing images dominated by low-frequency information. For example, in marine animal segmentation, underwater environments introduce non-uniform illumination and scattering effects, causing hazy and blurred images that distort frequency-domain information. Similarly, in polyp segmentation tasks, uneven illumination from endoscopic devices and imaging noise highlight low-frequency components while diminishing high-frequency details, posing challenges for achieving precise segmentation.

% To address these issues, we propose a novel feature learning framework called FE-UNet, designed for natural image segmentation. Specifically, we employ a Deep Wavelet Convolution (DWTConv) mechanism to enhance low-frequency information in image features. Following this, a spectral pooling filter is used to balance high- and low-frequency components, simulating the human visual system's heightened sensitivity to mid-frequency information. Additionally, to better capture multi-scale image features, we design an Frequency Domain Enhanced Receptive Field Block (FE-RFB) module integrated with a Wavelet-Guided Spectral Pooling Module(WSPM), enabling simultaneous enhancement of frequency-domain information and simulation of the relationship between the receptive field and eccentricity in the human visual system. This approach leverages the complementary properties of convolutional neural networks and the human visual system’s contrast sensitivity to improve segmentation effectiveness.

To address these challenges, we propose a novel feature learning framework called FE-UNet, specifically designed for natural image segmentation. The framework incorporates a Deep Wavelet Convolution (DWTConv) mechanism to enhance low-frequency information in image features. Subsequently, a spectral pooling filter is applied to balance high- and low-frequency components, emulating the human visual system's heightened sensitivity to mid-frequency information.
To further improve the capture of multi-scale image features, we introduce the Frequency Domain Enhanced Receptive Field Block (FE-RFB), which integrates the Wavelet-Guided Spectral Pooling Module (WSPM). This integration enables simultaneous enhancement of frequency-domain information and simulates the relationship between receptive field size and eccentricity in the human visual system. By leveraging the complementary strengths of convolutional neural networks and the human visual system’s contrast sensitivity, our approach effectively improves segmentation performance.

In summary, our contributions are as follows:
(1)
    % We introduced a novel feature learning framework called FE-UNet, designed for natural image segmentation. By enhancing frequency-domain information in natural images, it achieves high-performance segmentation results.
    We propose FE-UNet, a frequency-domain-enhanced segmentation framework, designed to improve segmentation performance by leveraging balanced feature extraction across high- and low-frequency components in natural images.  
(2)
    % We proposed the the Frequency Domain Enhanced Receptive Field Block (FE-RFB), which aggregates multi-scale receptive fields and eccentricity-aware features from frequency-enhanced information, simulating the mechanisms of the human visual system.
    We introduce the FE-RFB, which aggregates multi-scale receptive fields and eccentricity-aware features, inspired by the mechanisms of the human visual system, to improve feature extraction and segmentation effectiveness.
(3)
    % We developed the Wavelet-Guided Spectral Pooling Module(WSPM), which enhances low-frequency information in natural images and uses a spectral pooling filter to balance high- and low-frequency components, facilitating subsequent feature aggregation.
    We develop the WSPM, which enhances low-frequency information and balances it with high-frequency features, providing a robust foundation for frequency-domain-aware feature learning.
    (4)
    % Extensive experiments were conducted to validate the effectiveness and segmentation accuracy of the proposed model. Our method achieved state-of-the-art performance across four marine animal segmentation datasets and two polyp segmentation datasets.
    Extensive experiments on four marine animal segmentation datasets and two polyp segmentation datasets demonstrate the state-of-the-art performance of FE-UNet, showcasing its versatility and effectiveness in addressing diverse segmentation challenges.

\section{Related Work}

\subsection{Marine Animal Segmentation}

% The task of marine animal segmentation involves isolating marine animals from their surrounding environments, which poses significant challenges due to the inherent complexity of underwater scenes, such as variations in lighting, underwater blurriness, and the diversity in the appearance and species of marine animals. In recent years, convolutional neural networks (CNNs) have been widely applied to address these challenges. For instance, \cite{ECDNet} proposed an Enhanced Cascaded Decoder Network (ECDNet), while \cite{Marine_Related_2} introduced a feature interaction encoder and a cascaded decoder to extract more comprehensive features for accurate segmentation in complex underwater environments. Additionally, \cite{MASNet} designed a fusion network to learn semantic features of camouflaged marine animals. More recently, the Segment Anything Model (SAM) has demonstrated powerful segmentation capabilities. Based on this, \cite{Dual-SAM} developed a dual-SAM architecture and introduced automatic prompting to incorporate extensive prior information for underwater segmentation tasks. In \cite{MAS-SAM}, the SAM\cite{SAM} encoder was utilized to generate multi-scale features, and a progressive prediction framework was proposed to improve SAM’s ability to capture global underwater information. However, despite their effectiveness, these models exhibit limitations in capturing and processing frequency-domain information within marine images, which is crucial for addressing underwater visual distortions such as light scattering and absorption. 

Segmenting marine animals from their surrounding environments poses significant challenges due to the inherent complexity of underwater scenes, including variations in lighting, underwater blurriness, and diversity in the appearance and species of marine animals. In recent years, convolutional neural networks (CNNs) have been extensively applied to address these challenges. For example, \cite{ECDNet} proposed an Enhanced Cascaded Decoder Network (ECDNet), and \cite{Marine_Related_2} introduced a feature interaction encoder with a cascaded decoder to extract more comprehensive features for accurate segmentation in complex underwater environments. Similarly, \cite{MASNet} designed a fusion network to learn the semantic features of camouflaged marine animals.
More recently, the Segment Anything Model (SAM) has demonstrated robust segmentation capabilities. Building on this, \cite{Dual-SAM} developed a dual-SAM architecture that incorporates automatic prompting to integrate extensive prior information for underwater segmentation tasks. Furthermore, \cite{MAS-SAM} utilized the SAM encoder to generate multi-scale features and proposed a progressive prediction framework to enhance SAM’s ability to capture global underwater information.
Despite these advancements, these models face limitations in capturing and processing frequency-domain information in marine images. This frequency-domain information is critical for mitigating underwater visual distortions caused by phenomena such as light scattering and absorption.

\subsection{Polyp Segmentation}

% The task of polyp segmentation in computer vision focuses on identifying and isolating polyp regions in medical images. The main challenges lie in the diversity of polyp shapes, the ambiguity of their boundaries, and the high similarity between polyps and surrounding tissues. Reference \cite{CFANet} proposed a cross-level feature aggregation network that fuses multi-scale semantic information captured at different levels to achieve precise segmentation. However, this method relies solely on convolutional neural networks (CNNs), which limits its ability to capture long-range dependencies within the images. Reference \cite{H2Former} addressed this limitation by efficiently integrating CNNs with Transformers for medical image segmentation, enabling the fusion of local and global information. To achieve efficient multi-scale feature extraction and capture long-range dependencies, we incorporate a UNet architecture enhanced with the Hiera-Large module from SAM2 in this study.

Polyp segmentation in computer vision focuses on identifying and isolating polyp regions in medical images. The main challenges stem from the diversity of polyp shapes, the ambiguity of their boundaries, and the high similarity between polyps and surrounding tissues. Reference \cite{CFANet} proposed a cross-level feature aggregation network that fuses multi-scale semantic information from different levels to achieve precise segmentation. However, this approach relies solely on convolutional neural networks (CNNs), limiting its ability to capture long-range dependencies within images.
To address this limitation, \cite{H2Former} introduced an efficient integration of CNNs and Transformers for medical image segmentation, enabling the fusion of local and global information. Building on these advancements, this study incorporates a UNet architecture enhanced with the Hiera-Large module from SAM2 to achieve efficient multi-scale feature extraction and capture long-range dependencies.

\subsection{Frequency Domain Analysis}
% Frequency domain analysis has been extensively studied and applied in the field of computer vision. Previous works \cite{Frequency_related_1,Frequency_related_2} have demonstrated that low-frequency features in natural images correspond to global structures and color information, while high-frequency features are associated with local edges, textures, and other fine details. Studies such as \cite{Frequency_related_3,Frequency_related_4} have revealed that convolutional neural networks (CNNs) exhibit a strong bias toward learning high-frequency features in visual data but are less effective in capturing low-frequency representations. In contrast, multi-head self-attention mechanisms display the opposite behavior. WTConv \cite{Frequency_related_7} introduced a method that leverages wavelet transforms to enhance low-frequency features in natural images, thereby facilitating the capture of feature information at large receptive fields. To further exploit the frequency-domain characteristics of multi-head self-attention, LITv2 \cite{Frequency_related_5} proposed the HiLo attention mixer, which can simultaneously capture both high-frequency and low-frequency information using self-attention.

Frequency domain analysis has been extensively studied and applied in computer vision. Previous works \cite{Frequency_related_1,Frequency_related_2} have shown that low-frequency features in natural images correspond to global structures and color information, while high-frequency features are associated with local edges, textures, and fine details. Studies such as \cite{Frequency_related_3,Frequency_related_4} have revealed that convolutional neural networks (CNNs) tend to exhibit a strong bias toward learning high-frequency features in visual data but are less effective at capturing low-frequency representations. In contrast, multi-head self-attention mechanisms display the opposite tendency, favoring low-frequency features.
WTConv \cite{Frequency_related_7} introduced a method leveraging wavelet transforms to enhance low-frequency features in natural images, thereby improving the capture of feature information over large receptive fields. To further utilize the frequency-domain characteristics of multi-head self-attention, LITv2 \cite{Frequency_related_5} proposed the HiLo attention mixer, which simultaneously captures both high-frequency and low-frequency information using self-attention. Meanwhile, SPAM \cite{SPANet} developed a mixer that uses convolutional operations to balance high-frequency and low-frequency signals.


% On the other hand, SPAM \cite{SPANet} introduced a mixer that uses convolutional operations to balance high-frequency and low-frequency signals. To the best of our knowledge, no prior work has focused on enhancing low-frequency signals while effectively balancing high- and mid-frequency information. Inspired by this, we propose a novel mixer named Wavelet-Guided Spectral Pooling Module(WSPM), which leverages Deep Wavelet Convolution (DWTConv) to enhance low-frequency signals. Subsequently, spectral pooling filters are applied to the enhanced frequency-domain features to perform frequency mixing, enabling effective capture and utilization of high- and mid-frequency information in image representations.At the same time, we are the first to propose a method that simulates the human visual system based on frequency information.

To the best of our knowledge, no prior work has specifically focused on enhancing low-frequency signals while effectively balancing high- and mid-frequency information. Inspired by this, we propose a novel mixer called the Wavelet-Guided Spectral Pooling Module (WSPM), which utilizes Deep Wavelet Convolution (DWTConv) to enhance low-frequency signals. Subsequently, spectral pooling filters are applied to the enhanced frequency-domain features to perform frequency mixing, enabling the effective capture and utilization of high-, mid-, and low-frequency information in image representations. Additionally, we are the first to propose a method that simulates the human visual system based on frequency information.


\section{The Proposed Method}
\subsection{The Band-Pass Characteristics of CNNs and Visual Sensitivity}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{image/fig1.png}
    \caption{The Contrast Sensitivity Function model of the human visual system (HVS-CSF) and the Contrast Sensitivity Function model of convolutional neural networks(CNN-CSF), with the horizontal axis representing normalized spatial frequency and the vertical axis representing sensitivity.}
    \label{fig:1}
\end{figure}

% The human visual system's ability to discern details is related to the relative contrast of the observed area, typically represented by the Contrast Sensitivity Function (CSF)\cite{CSF_3}. The CSF is a function of spatial frequency and exhibits band-pass characteristic. Based on extensive experiments, Mannos and Sakrison proposed a classic model for the Contrast Sensitivity Function:

The human visual system's ability to discern details is closely related to the relative contrast of the observed area, typically represented by the Contrast Sensitivity Function (CSF) \cite{CSF_3}. The CSF is a function of spatial frequency and exhibits a band-pass characteristic. Based on extensive experiments, Mannos and Sakrison proposed a classic model for the Contrast Sensitivity Function:
\begin{align}
    H(f)=2.6*(0.192+0.114)*e^{\left[-(0.114f)^{1.1}\right]},
\end{align}%
where the spatial frequency is:
%
\begin{align}
    f = \left( f_x^2 + f_y^2 \right)^{0.5},
\end{align}%
% In the above formula, $f_x$ and $f_y$ represent the spatial frequencies in the horizontal and vertical directions, respectively. And based on this, plot the Contrast Sensitivity Function (HVS-CSF) curve of the human visual system (see Figure \ref{fig:1}).
% To compare the frequency characteristics of convolutional neural networks with the human visual system, we designed a simple CIFAR-10 \cite{CIFAR-10} classification experiment. We used the pre-trained ResNet18 model on ImageNet for feature extraction and inference. For each channel of the image features, we sequentially applied the Fourier transform and circular masking.
where $f_x$ and $f_y$ represent the spatial frequencies in the horizontal and vertical directions, respectively, based on this, we plotted the Contrast Sensitivity Function (HVS-CSF) curve of the human visual system (see Figure \ref{fig:1}).
To compare the frequency characteristics of convolutional neural networks with those of the human visual system, we designed a simple classification experiment using the CIFAR-10 dataset \cite{CIFAR-10}. We employed a pre-trained ResNet18 model on ImageNet for feature extraction and inference. For each channel of the image features, we sequentially applied the Fourier transform and circular masking.
\begin{equation}
\begin{aligned}
    F(u,v)&=\int_{-\infty}^\infty\int_{-\infty}^\infty f(x,y)e^{-2\pi i(ux+vy)}dxdy, \\
    M(u,v) &=
    \begin{cases}
    1 & \text{if } r \leq \mathbf{R}; \\
    0 & \text{if } r > \mathbf{R}.
    \end{cases}
\end{aligned}%
\end{equation}
Filter the image with different cutoff frequencies, and then apply the inverse Fourier transform.
\begin{align}
    f_{\mathrm{filtered}}(x,y)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}F_{\mathrm{filtered}}(u,v)e^{2\pi i(ux+vy)}dudv,
\end{align}%
% Convert the frequency domain features back to the spatial domain. Then, measure the model's classification accuracy at different cutoff frequencies and plot the Contrast Sensitivity Function (CNN-CSF) curve of the convolutional neural network (see Figure \ref{fig:1}).Based on Figure \ref{fig:1}, we can draw the following conclusions:
Convert the frequency domain features back to the spatial domain, then measure the model's classification accuracy at different cutoff frequencies. Plot the Contrast Sensitivity Function (CNN-CSF) curve of the convolutional neural network in Figure \ref{fig:1}. We can draw the following conclusions:
% The human visual system is more sensitive to mid-frequency signals, while its sensitivity to low-frequency and high-frequency signals is lower.
(i) The human visual system is most sensitive to mid-frequency signals, with lower sensitivity to both low-frequency and high-frequency signals.
    % The convolutional neural network also shows low sensitivity to low-frequency signals, while it is more sensitive to mid-high frequency signals, with slightly higher sensitivity to mid-frequency signals compared to high-frequency signals.
(ii) Similarly, convolutional neural networks exhibit low sensitivity to low-frequency signals. They are more responsive to mid-to-high-frequency signals, with a slightly greater sensitivity to mid-frequency signals compared to high-frequency signals.
% Based on this, we propose the Frequency Domain Enhanced Receptive Field Module (FE-RFB) to enhance low-frequency signals through a DWTConv. This is followed by mixing operations using a spectral pooling filter to blend high-frequency and low-frequency signals into the mid-frequency range, fully leveraging the convolutional module's high sensitivity to mid-frequency signals.

Based on this, we propose the Frequency Domain Enhanced Receptive Field Block (FE-RFB), which enhances low-frequency signals using a DWTConv. This is followed by mixing operations with a spectral pooling filter to blend high-frequency and low-frequency signals into the mid-frequency range, fully leveraging the convolutional module's high sensitivity to mid-frequency signals.

% Additionally, to further simulate the relationship between the receptive field and eccentricity in the human visual system, we combine the multi-scale frequency-domain enhancement of perceptual field and eccentricity methods, aiming to fully exploit the frequency domain characteristics of convolutional operations for simulating the human visual system.Based on the Frequency Domain Enhanced Receptive Field Block (FE-RFB) architecture, Hiera-L Block, and U-Shape architecture, we have innovatively proposed the FE-UNet architecture.

Furthermore, to simulate the relationship between the receptive field and eccentricity in the human visual system, we integrate multi-scale frequency-domain enhancement with perceptual field and eccentricity methods. This approach aims to fully exploit the frequency-domain characteristics of convolutional operations to better mimic the human visual system.
Building on the FE-RFB, the Hiera-L Block, and a U-shaped architecture, we have innovatively developed the FE-UNet architecture.

\subsection{FE-UNet}
% The original SAM2 model produces segmentation results that are class-agnostic; without manual specific prompts, SAM2 cannot generate segmentation results for designated classes. To enhance the specificity of SAM2 and better adapt it to specific downstream tasks while efficiently utilizing pre-trained parameters, we propose the FE-UNet architecture (as shown in Figure \ref{fig:2}(a)). This aims to improve model performance while reducing memory usage.

The original SAM2 model generates segmentation results that are class-agnostic. Without manual prompts for specific classes, SAM2 cannot produce segmentation results for designated categories. To enhance the specificity of SAM2 and better adapt it to specific downstream tasks while efficiently utilizing pre-trained parameters, we propose the FE-UNet architecture (as shown in Figure \ref{fig:2}(a)). This architecture is designed to improve model performance while reducing memory usage.
\begin{figure*}
    \centering
    \includegraphics[width=0.825\textwidth]{image/fig2.png}
    \caption{Figure (a) shows the architecture of our proposed FE-UNet model, Figure (b) illustrates the architecture of the proposed Frequency Domain Enhanced Receptive Field Block (FE-RFB), and Figure (c) depicts the architecture of our proposed Wavelet-Guided Spectral Pooling Module (WSPM) module.}
    \label{fig:2}
\end{figure*}

\noindent\textbf{Encoder.}
%  FE-UNet utilizes the pre-trained Hiera-L backbone network from SAM2. The attention mechanisms within the Hiera backbone compensate for the traditional convolutional neural networks' limitations in capturing long-range contextual features. Additionally, the hierarchical structure of the Hiera module is beneficial for capturing multi-scale features, which supports the design of U-shaped networks.
FE-UNet leverages the pre-trained Hiera-L backbone network from SAM2. The attention mechanisms within the Hiera backbone address the limitations of traditional convolutional neural networks in capturing long-range contextual features. Furthermore, the hierarchical structure of the Hiera module facilitates the capture of multi-scale features, making it well-suited for designing U-shaped networks.

% To achieve parameter-efficient fine-tuning, we introduce a trainable Adapter module before the Hiera Block and freeze the parameters of the Hiera Block itself. This approach avoids fine-tuning the Hiera Block, thereby reducing memory usage.

To enable parameter-efficient fine-tuning, we introduce a trainable Adapter module positioned before the Hiera Block, while keeping the parameters of the Hiera Block frozen. This approach eliminates the need to fine-tune the Hiera Block, significantly reducing memory usage.
Given an input image $I \in {R}^{3 \times H \times W}$, where $H$ and $W$ represent the height and width of the image, Hiera outputs four levels of hierarchical features $X_i \in {R}^{C_i \times \frac{H}{2^{i+1}} \times \frac{W}{2^{i+1}}} \quad (i \in \{1, 2, 3, 4\})$ The channel counts for each level are $C_i \in \{144, 288, 576, 1152\}$

\noindent\textbf{Adapter.}
% We referenced the work in \cite{Adapter_1,Adapter_2} to design the Adapter module, which sequentially includes a linear layer for downsampling, a GeLU activation function, a linear layer for upsampling, and another GeLU activation function. This design enables efficient fine-tuning of the Hiera Block while reducing memory usage.
We drew inspiration from \cite{Adapter_1,Adapter_2} to design the Adapter module, which consists of a sequential structure: a linear layer for downsampling, a GeLU activation function, a linear layer for upsampling, and another GeLU activation function. This design enables efficient fine-tuning of the Hiera Block while minimizing memory usage.

\noindent\textbf{FE-RFB.}
% After feature extraction during the encoder stage, these features undergo multi-channel feature fusion through deep wise convolution, reducing the channel count of the U-shaped network's hierarchical features to 64. This reduction helps minimize the memory consumption of the FE-RFB module. Subsequently, the reduced channel features are passed into our designed FE-RFB module for frequency domain information enhancement, simulating aspects of the human visual system.
After feature extraction during the encoder stage, the features undergo multi-channel fusion using depthwise convolution, which reduces the channel count of the U-shaped network's hierarchical features to 64. This reduction minimizes the memory consumption of the FE-RFB. The reduced-channel features are then passed through the FE-RFB, which is designed to enhance frequency domain information while simulating aspects of the human visual system.

\noindent\textbf{Decoder.}
We made adjustments to the decoder part of the traditional UNet architecture, utilizing the same upsampling operations. However, we implemented a customized DoubleConv module, which consists of two identical convolution—batch normalization—ReLU activation function combinations. The convolution operations use a kernel size of 3×3. Each decoder output feature is processed through a 1×1 convolutional segmentation head to generate segmentation results $S_i(i \in \left\{1, 2, 3\right\})$. These segmentation results are then upsampled and supervised against the ground truth segmentation masks.

\noindent\textbf{Loss Function.}
Each hierarchical structure loss function in FE-UNet is composed of a weighted Intersection over Union (IoU) and Binary Cross-Entropy (BCE) loss. The specific single-level loss function is defined as follows:
%
\begin{align}
    L = L_{IoU}^w + L_{BCE}^w.
\end{align}%
Since we employ deep supervision, the final loss function for FE-UNet is expressed as the sum of the individual hierarchical losses:
%
\begin{align}
    L_{total} = \sum_{i=1}^{3} L(G, S_i).
\end{align}%

\subsection{FE-RFB}
The human eye's ability to perceive spatial changes or spatial frequency contrast sensitivity, varies across frequency ranges. Generally, the eye is most sensitive to mid-frequency signals, with higher sensitivity to low-frequency signals compared to high-frequency signals. In contrast, convolution operations typically exhibit greater sensitivity to mid-frequency signals than to low-frequency ones.

To fully leverage the characteristics of convolution operations and the human eye's sensitivity to mid-frequency signals, we utilize the Wavelet-Guided Spectral Pooling Module(WSPM) to enhance low-frequency signals and perform mixing operations with high-frequency signals. This process shifts the frequency of the image towards the mid-frequency range, thereby enhancing the original RFB module's simulation effects related to the human visual field and eccentricity.

 To achieve multi-scale receptive field capture, we employ the Wavelet-Guided Spectral Pooling Module(WSPM) with different depths and convolution kernel sizes. In the WSPM, n represents the radius size of the low-frequency region $\mathbf{A}^{lf}$ centered at the origin, which is $2 ^ n$, The depth convolution part of the WSPM is configured with kernel sizes of 1×n and n×1. Subsequently, the padding numbers and dilation rates for the different branches of the dilated convolutions are set to $rate = {1, 3, 5,7}$. This configuration facilitates the expansion of the receptive field and aligns the feature sizes, making it convenient for the subsequent concatenation operations. As a result, we propose the FE-RFB, with the structural diagram illustrated in Figure \ref{fig:2}(b).

\subsection{WSPM}

In the field of computer vision, two common image filtering methods are used: one involves kernel convolution in the spatial domain, while the other utilizes Fourier transform for filtering in the frequency domain. The method proposed in this paper also operates in the frequency domain, but to achieve simple and efficient deep aggregation of spectral information under different receptive fields, we employ wavelet filtering. By applying a multi-branch spectral pooling filter followed by mixing operations on the Deep Wavelet Convolution (DWTConv), we introduce the Wavelet-Guided Spectral Pooling Modulation (WSPM). The module architecture is shown in Figure \ref{fig:2}(c).

\noindent\textbf{DWTConv.}
To fully utilize low-frequency features, we employ specific cascaded deep wavelet convolution operations. We use the Haar wavelet transform for simplicity and efficiency while utilizing four sets of filters for filtering in different frequency bands.
%
\begin{align}
    f_{LL} &= \frac{1}{2} \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix},  \quad
    f_{LH} = \frac{1}{2} \begin{bmatrix} 1 & -1 \\ 1 & -1 \end{bmatrix}\nonumber, \\
    f_{HL} &= \frac{1}{2} \begin{bmatrix} 1 & 1 \\ -1 & -1 \end{bmatrix},  \quad
    f_{HH} = \frac{1}{2} \begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix}.
\end{align}%
Among them, 
$f_{LL}$is the low-pass filter, while the others are high-pass filters. Subsequently, a convolution with a kernel size of 1 is used for deep aggregation operations. For different input channels, the output is
%
\begin{align}
    &[X_{LL}, X_{LH}, X_{HL}, X_{HH}] = \nonumber\\
     &\mathrm{Conv}_{(1 \times 1)} \left( [f_{LL}, f_{LH}, f_{HL}, f_{HH}], X \right).
\end{align}%
To align the output with the input dimensions, we use inverse wavelet transform to aggregate the features after wavelet decomposition, thus constructing the output
%
\begin{align}
    Y=\mathrm{IWT}(\mathrm{Conv}_{(1\times1)}(W,\mathrm{WT}(X))).
\end{align}%
The above formula represents only a single-level wavelet decomposition and aggregation operation.

In the WSPM, we employ cascaded wavelet decomposition to sequentially decompose the low-frequency signal $X_{LL}^{(i)}$
(where $i$ indicates the level). This enhances the low-frequency features while simultaneously reducing the spatial resolution to some extent. The process of cascaded wavelet decomposition and aggregation is as follows:
%
\begin{align}
    & X_{LL}^{(i)},X_H^{(i)}=\mathrm{WT}(X_{LL}^{(i-1)}),\\
    & Y_{LL}^{(i)},Y_H^{(i)}=\mathrm{Conv}_{(1\times1)}(W^{(i)},(X_{LL}^{(i)},X_H^{(i)})),\\
    &z^{(i)}=\mathrm{IWT}(Y_{LL}^{(i)}+z^{(i+1)},Y_H^{(i)}).
\end{align}%
Note that the above inverse wavelet transform formula simplifies using the theorem that states the inverse wavelet transform is a linear operation:
%
\begin{align}
    \mathrm{IWT}(X+Y)=\mathrm{IWT}(X)+\mathrm{IWT}(Y).
\end{align}%
In the DWTConv module, we employ deep convolution operations with a receptive field size of n×n to simulate the different receptive field features captured by the human eye. To reduce the parameter count of the model without compromising its performance, we use deep convolution operations with kernel sizes of 1×n and n×1.

\noindent\textbf{SPF.}
Based on the inverse power law, the most important visual information in natural images is concentrated in the mid-frequency region. After using the DWTConv, we employ spectral pooling filters to perform mixing operations on the low-frequency and high-frequency components in the spectrum, thereby increasing the weight of the low-frequency components.
First, we use a two-dimensional DFT to map the features obtained after deep convolution from the spatial domain to the frequency domain:
\begin{align}
    Z = \mathcal{F}(z) \in \mathbb{C}^{H \times W}.
\end{align}%
In the above formula, $\mathcal{F}(\cdot)$ represents the two-dimensional DFT operation. Next, we perform a shifting operation to move the low-frequency components to the center of the spectrum. We then use a Fourier transform centering function to remove the remaining parts outside of the low-frequency subset.
%
\begin{align}
    S^{lf} =
    \begin{cases}
    \mathcal{G}(Z)(u,v), & \text{if } (u,v) \in \mathbf{A}^{lf} \\
    0, & \text{else}
\end{cases}
\end{align}%
In the above formula, $\mathcal{G}(\cdot)$is the Fourier transform centering function,$(u,v)$is a pair of positions in the frequency domain, and $\mathbf{A}^{lf}\in\mathbb{R}^{2}$ represents the low-frequency region centered at the origin.

High-pass filters are the opposite of low-pass filters, so high-frequency components can be directly obtained by removing low-frequency components from the input feature map
\begin{align}
    S^{hf}=\mathcal{G}(Z)-S^{lf}.
\end{align}%
Finally, by sequentially applying the inverse transformation and inverse DFT operation to the high-frequency and low-frequency components, we can obtain the spectral pooled feature map
%
\begin{align}
    f_{lp}(Z) &= \mathcal{F}^{-1}(\mathcal{G}^{-1}({S}^{lf})) \in \mathbb{R}^{H \times W} \\
    f_{hp}(Z) &= \mathcal{F}^{-1}(\mathcal{G}^{-1}({S}^{hf})) \in \mathbb{R}^{H \times W}
\end{align}%
We mix the visual features of different frequency bands obtained from the decomposition using a combination filter, which can be represented by the following formula:
%
\begin{align}
    \tilde{Z} = \lambda f_{lp}(Z) + (1 - \lambda) f_{hp}(Z) \in \mathbb{R}^{H \times W}.
\end{align}%
Since $\mathcal{F(\cdot)}$ and $\mathcal{G}(\cdot)$, as well as their inverses, are linear operations, they satisfy the principle of superposition. The above formula is equivalent to:
%
\begin{align}
    \tilde{Z} = \mathcal{F}^{-1} \left( \mathcal{G}^{-1} \left( \lambda S^{lf} + (1 - \lambda) S^{hf} \right) \right),
\end{align}%
where $\lambda \in  [0, 1]$ is a balancing parameter. We can now manipulate the frequency spectrum of visual features by adjusting $\lambda$ to control the balance between high-frequency and low-frequency components.

\section{Experiments}
\noindent\textbf{Datasets and Evaluation Metrics.}
Following the convention \cite{DataSet_1,SPANet}, we experimentally validated the effectiveness of FE-UNet on two tasks: marine animal segmentation and polyp segmentation. The experimental datasets for both tasks are detailed in the Appendix.

% \subsubsection{Marine Animal Segmentation}
% The goal is to separate marine animals from the background in natural images. For this task, we utilized four public benchmarks: MAS3K, RMAS, UFO120, and RUWI datasets.
% \begin{itemize}
%     \item MAS3K: This dataset contains 3,103 high-quality annotated images. We followed the default split, using 1,769 images for training and 1,141 images for testing, while excluding the remaining 193 images that contained only backgrounds.
%     \item RMAS: This dataset includes 3,014 marine images. We used 2,514 images to train the model and 500 images to test the model's performance.
%     \item UFO120: This dataset consists of 1,620 underwater images featuring various scenes. We followed the default split, using 1,500 images for training and 120 images to evaluate the model's performance.
%     \item RUWI: This dataset comprises real underwater images captured under complex lighting conditions, containing 700 images. Unlike the original paper, we used 525 images for model training and 175 images for testing.
% \end{itemize}

% \subsubsection{Polyp Segmentation}
% In medical image analysis, the objective is to accurately segment polyps from the colon or other tissue structures. For this task, we used Kvasir-SEG and CVC-ClinicDB as training sets, and extract 100 images from the Kvasir dataset to test the performance of the model. 

% Additionally, we utilized CVC-ColonDB, CVC-300, and ETIS datasets as test sets to validate the model's generalization capability and assess its performance on these datasets. To evaluate the model's effectiveness in the polyp segmentation task, we used two metrics: mean Dice score (mDice) and mean Intersection over Union (mIoU).

\noindent\textbf{Comparison with State-of-the-Arts.}
In this section, we compare our method with other approaches on four public marine animal segmentation datasets and four public polyp segmentation datasets. The quantitative and qualitative results clearly demonstrate the significant advantages of our proposed method.

\begin{table*}[!htbp] 
    \centering
    \caption{Marine animal segmentation performance on MAS3K and RMAS datasets.}
    \resizebox{1\linewidth}{!}{% 
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
        \hline
        \multirow{2}{*}{Category} & \multirow{2}{*}{Method} & \multicolumn{5}{c|}{MAS3K} & \multicolumn{5}{c}{RMAS}  \\
        \cline{3-12}
        & & mIoU & $\mathbf{S}_{\alpha}$ & $\mathbf{F}^w_{\beta}$ & $\mathbf{mE}_{\phi}$ & MAE & mIoU & $\mathbf{S}_{\alpha}$ & $\mathbf{F}^w_{\beta}$ & $\mathbf{mE}_{\phi}$ & MAE \\
        \hline
        \multirow{13}{*}{CNN} 
        & PFANet \cite{PFANet} & 0.405 & 0.690 & 0.471 & 0.768 & 0.086 & 0.556 & 0.767 & 0.582 & 0.810 & 0.051 \\
        & SCRN \cite{SCRN} & 0.693 & 0.839 & 0.730 & 0.869 & 0.041 & 0.695 & 0.842 & 0.731 & 0.878 & 0.030 \\
        & UNet++ \cite{UNet++} & 0.506 & 0.726 & 0.552 & 0.790 & 0.083 & 0.558 & 0.763 & 0.644 & 0.835 & 0.046 \\
        & U2Net \cite{U2-Net} & 0.654 & 0.812 & 0.711 & 0.851 & 0.047 & 0.676 & 0.830 & 0.762 & 0.904 & 0.029 \\
        & SINet \cite{SINet} & 0.658 & 0.820 & 0.725 & 0.884 & 0.039 & 0.684 & 0.835 & 0.780 & 0.908 & 0.025 \\
        & BASNet \cite{BASNet} & 0.677 & 0.826 & 0.724 & 0.862 & 0.046 & 0.707 & 0.847 & 0.771 & 0.907 & 0.032 \\
        & PFNet \cite{PFNet} & 0.695 & 0.839 & 0.746 & 0.890 & 0.039 & 0.694 & 0.843 & 0.771 & 0.922 & 0.026 \\
        & RankNet \cite{RankNet} & 0.658 & 0.812 & 0.722 & 0.867 & 0.043 & 0.704 & 0.846 & 0.772 & 0.927 & 0.026 \\
        & C2FNet \cite{C2FNet} & 0.717 & 0.851 & 0.761 & 0.894 & 0.038 & 0.721 & 0.858 & 0.788 & 0.923 & 0.026 \\
        & ECDNet \cite{ECDNet} & 0.711 & 0.850 & 0.766 & 0.901 & 0.036 & 0.664 & 0.823 & 0.689 & 0.854 & 0.036 \\
        & OCENet \cite{OCENet} & 0.667 & 0.824 & 0.703 & 0.868 & 0.052 & 0.680 & 0.836 & 0.752 & 0.900 & 0.030 \\
        & ZoomNet \cite{ZoomNet} & 0.736 & 0.862 & 0.780 & 0.898 & 0.032 & 0.728 & 0.855 & 0.795 & 0.915 & \underline{0.022} \\
        & MASNet \cite{MASNet} & 0.742 & 0.864 & 0.788 & 0.906 & 0.032 & 0.731 & 0.862 & 0.801 & 0.920 & 0.024\\
        \hline
        \multirow{3}{*}{Transformer} 
        & SETR \cite{SETR} & 0.715 & 0.855 & 0.789 & 0.917 & 0.030 & 0.654 & 0.818 & 0.747 & 0.933 & 0.028 \\
        & TransUNet \cite{TransUNet} & 0.739 & 0.861 & 0.805 & 0.919 & 0.029 & 0.688 & 0.832 & 0.776 & 0.941 & 0.025 \\
        & H2Former \cite{H2Former} & 0.748 & 0.865 & 0.810 & 0.925 & 0.028& 0.717 & 0.844 & 0.799 & 0.931 & 0.023 \\
        \hline
        \multirow{8}{*}{SAM} 
        & SAM \cite{SAM} & 0.566 & 0.763 & 0.656 & 0.807 & 0.059 & 0.445 & 0.697 & 0.534 & 0.790 & 0.053 \\
        & Med-SAM \cite{Med-SAM} & 0.739 & 0.861 & 0.811 & 0922 & 0 031 & 0.678 & 0.832 & 0.778 & 0.920 & 0.027 \\
        & SAM-Adapter \cite{SAM-Adapter} & 0.714 & 0.847 & 0.782 & 0.914 & 0.033 & 0.656 & 0.816 & 0.752 & 0.927 & 0.027 \\
        & SAM-DADF \cite{SAM-DADF} & 0.742 & 0.866 & 0.806 & 0.925 & 0.028 & 0.686 & 0.833 & 0.780 & 0.926 & 0.024 \\
        & I-MedSAM \cite{I-MedSAM} & 0.698 & 0.835 & 0.759 & 0.889 & 0.039 & 0.633 & 0.803 & 0.699 &0.893 & 0.035 \\
        & Dual-SAM \cite{Dual-SAM} & \underline{0.789} & 0.884 & 0.838 &\underline{0.933} & \underline{0.023} & 0.735 & 0.860 & \underline{0.812} & \underline{0.944} & \underline{0.022} \\
        & MAS-SAM \cite{MAS-SAM} & 0.788 & \underline{0.887} & \underline{0.840} & \textbf{0.938} & 0.025 & \underline{0.742} & \underline{0.865} & \textbf{0.819} & \textbf{0.948} & \textbf{0.021} \\
        \cline{2-12}
        & FE-UNet (Ours) & \textbf{0.815} &  \textbf{0.900} & \textbf{0.848} & 0.928 &  \textbf{0.022} & \textbf{0.758} & \textbf{0.874} & 0.811 & 0.938 & \textbf{0.021}\\
        \hline
    \end{tabular}}
    \label{tab:MAS1}
    \vspace{-0.4cm}
\end{table*}

\begin{table*}[!htbp]
    \centering
    \caption{Marine animal segmentation performance on UFO120 and RUWI datasets.}
    \resizebox{1\linewidth}{!}{% 
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
        \hline
        \multirow{2}{*}{Category} & \multirow{2}{*}{Method}     & \multicolumn{5}{c|}{UFO120} & \multicolumn{5}{c}{RUWI}  \\
        \cline{3-12}
        & &mIoU & $\mathbf{S}_{\alpha}$ & $\mathbf{F}^w_{\beta}$ & $\mathbf{mE}_{\phi}$ & MAE & mIoU & $\mathbf{S}_{\alpha}$ & $\mathbf{F}^w_{\beta}$ & $\mathbf{mE}_{\phi}$ & MAE \\
        \hline
        \multirow{13}{*}{CNN} 
        &PFANet \cite{PFANet} & 0.677 & 0.752& 0.723 & 0.815 & 0.129  & 0.773  & 0.765 & 0.811 & 0.867 & 0.096 \\
       & SCRN \cite{SCRN} & 0.678 & 0.783 & 0.760 & 0.839 & 0.106 & 0.830 & 0.847 & 0.883 & 0.925 & 0.059 \\
       &UNet++ \cite{UNet++} & 0.412 & 0.459 & 0.433 & 0.451 & 0.409 & 0.586 & 0.714 & 0.678 & 0.790 & 0.145 \\
       & U2Net \cite{U2-Net} & 0.680 & 0.792 & 0.709 & 0.811 & 0.134 & 0.841 & 0.873 & 0.861 & 0.786 & 0.074 \\
       &SINet \cite{SINet} & 0.767 & 0.837 & 0.834 & 0.890 & 0.079 & 0.785 & 0.789 & 0.825 & 0.872 & 0.096 \\
       &BASNet \cite{BASNet} & 0.710 & 0.809& 0.793 & 0.865 & 0.097 & 0.841 & 0.871 & 0.895 & 0.922 &  0.056\\
       & PFNet \cite{PFNet} & 0.570 & 0.708 & 0.550 & 0.683 & 0.216 & 0.864 & 0.883 & 0.870 & 0.790 & 0.062 \\
        &RankNet \cite{RankNet} &0.739  & 0.823 & 0.772 & 0.828 & 0.101 & 0.865 & 0.886 & 0.889 & 0.759 & 0.056 \\
        &C2FNet \cite{C2FNet} & 0.747 & 0.826 & 0.806 & 0.878 & 0.083 & 0.840  & 0.830 & 0.883 & 0.924 & 0.060 \\
        &ECDNet \cite{ECDNet} & 0.693 & 0.783 & 0.768 & 0.848 & 0.103 & 0.829 & 0.812 & 0.871 & 0.917 & 0.064 \\
        &OCENet \cite{OCENet} & 0.605 & 0.725 & 0.668 & 0.773 & 0.161 & 0.763  & 0.791 & 0.798 & 0.863 & 0.115 \\
        &ZoomNet \cite{ZoomNet} & 0.616 & 0.702 & 0.670 & 0.815 & 0.174 & 0.739& 0.753 & 0.771 & 0.817 & 0.137 \\
       & MASNet \cite{MASNet} & 0.754 & 0.827 & 0.820 & 0.879 & 0.083 &  0.865& 0.880 & 0.913 & 0.944 & 0.047\\ 
        \hline
        \multirow{3}{*}{Transformer} 
        &SETR \cite{SETR} & 0.711 & 0.811 & 0.796 &0.871  & 0.089 &0.832  & 0.864 & 0.895 & 0.924 & 0.055 \\ 
        &TransUNet \cite{TransUNet} & 0.752 & 0.825 & 0.827 & 0.888 & 0.079 & 0.854 & 0.872 & 0.910 & 0.940 & 0.048 \\
        &H2Former \cite{H2Former} & 0.780 & 0.844 & 0.845 & \underline{0.901} & 0.070 & 0.871 & 0.884 & 0.919 & 0.945 & 0.045 \\ 
        \hline
        \multirow{7}{*}{SAM}
        &SAM \cite{SAM} & 0.681 & 0.768 & 0.745 & 0.827 & 0.121 & 0.849 & 0.855 & 0.907 & 0.929 & 0.057 \\
        &Med-SAM \cite{Med-SAM} & 0.774 & 0.842 & 0.839 & 0.899 & 0.072 & 0.877 & 0.885 & 0.921 & 0.942 & 0.045 \\
        &SAM-Adapter \cite{SAM-Adapter} & 0.757 & 0.829 & 0.834 & 0.884 & 0.081 & 0.867 & 0.878 & 0.913 & 0.946 & 0.046 \\
        &SAM-DADF \cite{SAM-DADF} & 0.768 & 0.841 & 0.836 & 0.893 & 0.073 & 0.881 & 0.889 & 0.925 & 0.940 & 0.044 \\
        &I-MedSAM \cite{I-MedSAM} & 0.730 & 0.818 & 0.788 & 0.865 & 0.084 & 0.844 & 0.849 & 0.897 & 0.923 & 0.050 \\
        &Dual-SAM \cite{Dual-SAM} & 0.810 & 0.856 & \textbf{0.864} & \textbf{0.914} & \underline{0.064} & \underline{0.904} & \underline{0.903} & \underline{0.939} & \underline{0.959} & 0.035 \\
        &MAS-SAM \cite{MAS-SAM} & 0.807 & \underline{0.861} & \textbf{0.864} & \textbf{0.914} & \textbf{0.063} & 0.902 & 0.894 & \textbf{0.941} & \textbf{0.961} & \textbf{0.035} \\
        \cline{2-12}
        & FE-UNet (Ours) & \textbf{0.821} & \textbf{0.871} & \underline{0.856} & \textbf{0.914} & 0.067 & \textbf{0.914} & \textbf{0.912} & 0.936 & \underline{0.959} & \underline{0.037} \\
        \hline
    \end{tabular}}
    \label{tab:MAS2}
        \vspace{-0.4cm}
\end{table*}

% \caption{Polyp segmentation performance on Kvasir-SEG \cite{17}, CVC-ClinicDB \cite{1}, CVC-ColonDB \cite{43}, CVC-300 \cite{39}, and ETIS \cite{39} datasets.}
\begin{table*}[!htbp]
    \centering
    \caption{Polyp segmentation performance on Kvasir-SEG, CVC-ColonDB , CVC-300 , and ETIS  datasets.}
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
        \hline
        \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Kvasir} & \multicolumn{2}{c|}{CVC-ColonDB} & \multicolumn{2}{c|}{CVC-300} & \multicolumn{2}{c}{ETIS} \\
        \cline{2-9}
        & mDice & mIoU & mDice & mIoU & mDice & mIoU & mDice & mIoU \\
        \hline
        UNet \cite{U-Net} & 0.818 & 0.746 & 0.504 & 0.436 & 0.710 & 0.627 & 0.398 & 0.335 \\
        SFA \cite{SFA} & 0.723 & 0.611 & 0.456 & 0.337 & 0.467 & 0.329 & 0.297 & 0.217 \\
        UNet++ \cite{UNet++} & 0.821 & 0.744  & 0.482 & 0.408 & 0.707 & 0.624 & 0.401 & 0.344 \\
        PraNet \cite{PraNet} & 0.898 & 0.840  & 0.709 & 0.640 & 0.871 & 0.797 & 0.628 & 0.567 \\
        EU-Net \cite{EU-Net} & 0.908 & 0.854  & 0.756 & 0.681 & 0.837 & 0.765 & 0.687 & 0.609 \\
        SANet \cite{SANet} & 0.904 & 0.847  & 0.752 & 0.669 & 0.888 & 0.815 & 0.750 & 0.654 \\
        MSNet \cite{MSNet} & 0.905 & 0.849  & 0.751 & 0.671 & 0.865 & 0.799 & 0.723 & 0.652 \\
        C2FNet \cite{C2FNet} & 0.886 & 0.831  & 0.724 & 0.650 & 0.874 & 0.801 & 0.699 & 0.624 \\
        MSEG \cite{MSEG} & 0.897 & 0.839  & 0.735 & 0.666 & 0.874 & 0.804 & 0.700 & 0.630 \\
        DCRNet \cite{DCRNet} & 0.886 & 0.825  & 0.704 & 0.631 & 0.856 & 0.788 & 0.556 & 0.496 \\
        LDNet \cite{LDNet} & 0.887 & 0.821  & 0.740 & 0.652 & 0.869 & 0.793 & 0.645 & 0.551 \\
        FAPNet \cite{FAPNet} & 0.902 & 0.849  & 0.731 & 0.658 & 0.893 & 0.826 & 0.717 & 0.643 \\
        ACSNet \cite{ACSNet} & 0.898 & 0.838  & 0.716 & 0.649 & 0.863 & 0.787 & 0.578 & 0.509 \\
        H2Former \cite{H2Former} & 0.910 & 0.858  & 0.719 & 0.642 & 0.856 & 0.793 & 0.614 & 0.547 \\
        CaraNet \cite{CaraNet} & 0.913 & 0.859  & 0.775 & 0.700 & \underline{0.902} & \underline{0.836} & 0.740 & 0.660 \\
        CFA-Net \cite{CFANet} & \underline{0.915} & \underline{0.861}  & 0.743 & 0.665 & 0.893 & 0.827 & 0.732 & 0.655 \\
        I-MedSAM \cite{I-MedSAM} & 0.839 & 0.759  & \textbf{0.885} & \textbf{0.800} & 0.900 & 0.822 & \textbf{0.874} & \textbf{0.791} \\
        \hline
        FE-UNet (Ours) & \textbf{0.929} & \textbf{0.883}  & \underline{0.804} & \underline{0.729} & \textbf{0.909} & \textbf{0.847} & \underline{0.787} & \underline{0.712}\\
        \hline
    \end{tabular}
            \vspace{-0.2cm}
    \label{tab:polyp_segmentation}
\end{table*}

% \subsubsection{Quantitative Comparisons}
% \textbf{Marine Animal Segmentation Task:}
Tables \ref{tab:MAS1} and \ref{tab:MAS2} present the quantitative comparisons on typical marine animal segmentation datasets. Compared with CNN-based methods, our method significantly improves performance. On the challenging MAS3K dataset, our method achieves the highest scores across all metrics, delivering a 4-6\% improvement. Moreover, our method consistently outperforms others on additional MAS datasets. Compared to state-of-the-art marine animal segmentation models, our model achieves a 1-3\% improvement in mIoU and $\mathbf{S}_{\alpha}$ metrics. When compared with Transformer-based methods, our method achieves a 3-6\% improvement on the MAS3K dataset. Furthermore, compared with other SAM-based methods,  our model achieves a 1-2\% improvement in mIoU scores as well as $\mathbf{S}_{\alpha}$ compared to current SOTA methods.

% \textbf{Polyp Segmentation Task:}
We follow \cite{SOTA_Polyp_1}, including the same comparison methods and tools. Table \ref{tab:polyp_segmentation} shows the performance of our model on four polyp segmentation test datasets. On the Kvasir and CVC-300 datasets, our model achieved SOTA performance, with a 1-2\% improvement over the second-best method. Furthermore, on the CVC-ColonDB and ETIS datasets, our model demonstrated the second-best segmentation performance.

% \subsubsection{Qualitative Comparisons}
\begin{figure}[!tbp]
    \centering
    \includegraphics[width=1\linewidth]{image/fig3.png}
    \caption{In the marine animal segmentation task, predictions were generated using different models, and the visualized prediction masks were compared. Best view by zooming in.}
    \label{fig:3}
\end{figure}

\begin{figure}[!tbp]
    \centering
    \includegraphics[width=1\linewidth]{image/fig4.png}
    \caption{In the polyp segmentation task, predictions were generated using different models, and the visualized prediction masks were compared. Best view by zooming in. }
    \label{fig:4}
\end{figure}

Figures \ref{fig:3} and \ref{fig:4} illustrate some visual examples from the marine animal segmentation and polyp segmentation tasks, respectively, to further verify the effectiveness of our method. Compared with previous approaches, our method produces segmentation results that are highly similar to the ground truth in simpler tasks. Moreover, on challenging images with cluttered backgrounds and rich details, our method consistently generates more accurate and refined segmentation masks. More visual results demonstrating the superior performance of our model are presented in the Appendix.

\section{Conclution}
In this work, we propose a novel feature learning framework named FE-UNet for natural image segmentation. Specifically, we introduce the Frequency Domain Enhanced Receptive Field Block (FE-RFB), which aggregates frequency-domain information enhanced by multi-scale WSPM modules through the integration of multi-scale receptive fields and eccentricity-aware mechanisms. This design simulates the human visual system's heightened sensitivity to mid-frequency features. Our method extracts richer frequency-domain information that is highly beneficial for fine-grained image segmentation. As a result, it achieves state-of-the-art (SOTA) performance on four marine animal segmentation tasks and polyp segmentation tasks. Our framework design is not only applicable to marine animal segmentation and polyp segmentation scenarios but also lays a solid foundation for image segmentation research in other complex scenarios, providing a broader space for exploration.






%% The file named.bst is a bibliography style file for BibTeX 0.99c
\clearpage
\bibliographystyle{named}
\bibliography{ijcai24}

\newpage 
\section{Appendix}


\subsection{Datasets}

\begin{table}[htbp]
    \centering
    \caption{Task Set}
    \begin{tabular}{c|c|c|c}
        \hline
        Segmentation Tasks     & Dataset & Train Set & Test Set  \\
        \hline
        \multirow{4}{*}{Marine Animal}     & MAS3K & 1769 & 1141 \\
                                                        & RMAS & 2514 & 500\\
                                                        & UFO120 & 1500 & 120\\
                                                        & RUWI & 525 & 175\\
        \hline
        \multirow{5}{*}{Polyp}             & Kvasir-SEG & 900 & 100 \\ 
                                                        & CvC-ClinicDB & 550 & - \\
                                                        & CVC-ColonDB & - & 380 \\
                                                        & CVC-300  & - & 60 \\
                                                        & ETIS & - & 196 \\
        \hline
    \end{tabular}
    \label{tab:datasets}
\end{table}
The content in Table \ref{tab:datasets} shows the dataset configuration used in our experiments.
\subsubsection{Marine Animal Segmentation}
The goal is to separate marine animals from the background in natural images. For this task, we utilized four public benchmarks: MAS3K, RMAS, UFO120, and RUWI datasets.
\begin{itemize}
    \item MAS3K: This dataset contains 3,103 high-quality annotated images. We followed the default split, using 1,769 images for training and 1,141 images for testing, while excluding the remaining 193 images that contained only backgrounds.
    \item RMAS: This dataset includes 3,014 marine images. We used 2,514 images to train the model and 500 images to test the model's performance.
    \item UFO120: This dataset consists of 1,620 underwater images featuring various scenes. We followed the default split, using 1,500 images for training and 120 images to evaluate the model's performance.
    \item RUWI: This dataset comprises real underwater images captured under complex lighting conditions, containing 700 images. Unlike the original paper, we used 525 images for model training and 175 images for testing.
\end{itemize}

\subsubsection{Polyp Segmentation}
In medical image analysis, the objective is to accurately segment polyps from the colon or other tissue structures. For this task, we used Kvasir-SEG and CVC-ClinicDB as training sets, and extracted 100 images from the Kvasir dataset to test the performance of the model. 

Additionally, we utilized CVC-ColonDB, CVC-300, and ETIS datasets as test sets to validate the model's generalization capability and assess its performance on these datasets. To evaluate the model's effectiveness in the polyp segmentation task, we used two metrics: mean Dice score (mDice) and mean Intersection over Union (mIoU).

\subsection{Implementation Details}
For the Wavelet-Guided Spectral Pooling Module(WSPM), we set the specificity of the cascaded depth wavelet convolution kernel to 1×1 with a stride of 1. Additionally, we employ two parallel SPF modules, configuring $\lambda$ to 0.7 and 0.8, respectively.

The model is implemented based on the PyTorch framework, which is widely used in the field of deep learning due to its strong flexibility and ease of use.

The model is trained using the AdamW optimizer, an improved version of the Adam optimizer that includes a weight decay mechanism to better prevent overfitting. The initial learning rate for AdamW is set to 0.001.

We use a batch size of 12, which determines the number of samples used for each training iteration when updating the model parameters, influencing the model's convergence speed and training efficiency.

The training is set for 20 epochs to better adapt to the marine animal segmentation and polyp segmentation tasks.

All input images are resized to 350×350, which helps reduce computational overhead while ensuring that image information is preserved.

The model employs a cosine decay learning rate strategy to gradually decrease the learning rate during the later stages of training, ensuring more stable training and avoiding oscillations or overfitting.

All experiments were conducted on a system equipped with an Intel(R) Xeon(R) Platinum 8462Y+ CPU, 8 NVIDIA A800-SXM4-80GB GPUs, and 1TB of RAM.

\subsection{Ablation Study}
Since both marine animal segmentation and polyp segmentation are segmentation tasks, we chose polyp segmentation as the primary focus for the ablation experiments.
\subsubsection{Effect of FE-RFB}
As shown in Figure \ref{fig:Ablation1}, we investigated the performance of the FE-UNet model with and without the FE-RFB across different configurations of branch0, using the Kvasir, CVC-ClinicDB, and CVC-300 datasets. The results indicate that the FE-UNet model with simple skip connections outperformed the model with the RFB module on the CVC-ClinicDB and CVC-300 datasets, but it was inferior to the performance of the model enhanced by the frequency domain using the FE-RFB.
We configured four different Frequency Domain Enhanced Receptive Field Block (FE-RFB) branch0 setups:
\begin{itemize}
    \item FE-RFB: Using the same configuration as the previous RFB module.
    \item FE-RFB-1: Adopting a setup similar to PraNet.
    \item FE-RFB-2: Configured similarly to other branches, incorporating our designed WSPM module.
    \item FE-RFB-3: Based on FE-RFB-2, replacing WSPM with DWTConv.
\end{itemize}
From the analysis of these four configurations, we found that the FE-RFB with branch0 configured as Conv 1×1 — Conv 3×3 yielded the best results. This suggests that the original spatial frequency domain information is essential for guiding the multi-branch WSPM module in the fusion of frequency domain information.

\subsubsection{Effect of Different Levels of FE-RFB}
As shown in Figure \ref{fig:Ablation2}, we explored the effects of the FE-RFB at different levels within the FE-UNet model. We experimented with various combinations of FE-RFB across different levels, using the Kvasir and CVC-300 datasets.

Our findings indicate that the performance of the FE-UNet model is optimized when the FE-RFB module is applied at all levels of the U\-Shape architecture. Notably, the FE-RFB at the second and third levels of the U\-Shape architecture has a significant and indispensable impact on the model's performance.

\begin{figure*}[!tbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/Ab_1_1.png}
        \caption{Kvasir}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/Ab_1_2.png}
        \caption{CVC-300}
        \label{fig:sub2}
    \end{subfigure}
    \caption{Visualization of Ablation Experiment Results for FE-RFB}
    \label{fig:Ablation1}
\end{figure*}

\begin{figure*}[!tbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/Ab_2_1.png}
        \caption{Kvasir}
        \label{fig:sub3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/Ab_2_2.png}
        \caption{CVC-300}
        \label{fig:sub4}
    \end{subfigure}
    \caption{Visualization of Ablation Experiment Results for Different Levels of FE-RFB Effects}
    \label{fig:Ablation2}
\end{figure*}

\subsection{Visualization Results}
Figures \ref{fig:7} and \ref{fig:8} present additional visualization results to demonstrate the superior performance of our model, leveraging insights from machine learning.

\begin{figure*}[!tbp]
    \centering
    \includegraphics[width=1\linewidth]{image/fig7.png}
    \caption{Additional visualization results of different models on the marine animal segmentation task. }
    \label{fig:7}
\end{figure*}
\begin{figure*}[!tbp]
    \centering
    \includegraphics[width=1\linewidth]{image/fig8.png}
    \caption{Additional visualization results of different models on the polyp segmentation task. }
    \label{fig:8}
\end{figure*}

\end{document}


