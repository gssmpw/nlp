% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
\newtcbtheorem[]{prompt}{{ \textbf{Template to Obtain \texttt{Val\_PPL}}}}%
{colback=gray!10, colframe=gray!50!black, 
 left=0in, right=0in, bottom=0.02in, top=0.02in, fontupper=\large}{th}

\newtcbtheorem[]{promptcond}{{ \textbf{Template for Computing Conditional}}}%
{colback=gray!10, colframe=gray!50!black, 
 left=0in, right=0in, bottom=0.02in, top=0.02in, fontupper=\large}{th}



% personal packages
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

% \def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
% \def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{adjustbox}
\usepackage{marvosym}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage{arydshln}
\usepackage{makecell}
\usepackage{bbm}
\usepackage{wrapfig} 
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}

\usepackage{xcolor}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}
\newcommand{\tcb}{\textcolor{blue}} 

\definecolor{highlightcolor}{gray}{.9}
\newcommand{\highlight}[1]{\cellcolor{highlightcolor}{#1}}
\definecolor{baselinecolor}{gray}{.9}
\definecolor{backcolor}{RGB}{232, 242, 255}


\definecolor{color1}{HTML}{1f77b4}
\definecolor{color2}{HTML}{ff7f0e}
\definecolor{color3}{HTML}{2ca02c}
\definecolor{color4}{HTML}{d62728}
\definecolor{color5}{HTML}{9467bd}

\definecolor{colora}{rgb}{0.4, 0.7607843137254902, 0.6470588235294118}
\definecolor{colorb}{rgb}{0.9882352941176471, 0.5529411764705883, 0.3843137254901961}

\def\vw{{\bm{w}}}
\def\gL{{\mathcal{L}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % Disable the original \footnote command (make it do nothing)
% \let\oldfootnote\footnote
% \renewcommand{\footnote}[1]{} % Disable footnote (do nothing)

% % Create a new \myfootnote command that behaves like the original \footnote
% \newcommand{\myfootnote}[1]{\oldfootnote{#1}} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in Visual Instruction Tuning}



\author{
 \textbf{Yunhao Gou\textsuperscript{1,2}},
 \textbf{Hansi Yang\textsuperscript{2}},
 \textbf{Zhili Liu\textsuperscript{2,3}},
 \textbf{Kai Chen\textsuperscript{2}},
 \textbf{Yihan Zeng\textsuperscript{3}},
 \textbf{Lanqing Hong\textsuperscript{3}},
\\
 \textbf{Zhenguo Li \textsuperscript{3}},
 \textbf{Qun Liu\textsuperscript{3}},
 \textbf{James T. Kwok\textsuperscript{2}},
 \textbf{Yu Zhang\textsuperscript{1,4}}
\\
\\
 \textsuperscript{1}Southern University of Science and Technology,
 \\
 \textsuperscript{2}The Hong Kong University of Science and Technology,
 \\
 \textsuperscript{3}Huawei Noah’s Ark Lab,
\\
 \small{
   \textbf{Correspondence:} \href{mailto:yu.zhang.ust@gmail.com}{yu.zhang.ust@gmail.com}
 }
}


\begin{document}
\maketitle
\begin{abstract}
Visual Instruction Tuning (VIT) enhances Multimodal Large Language Models (MLLMs) but it is hindered by corrupted datasets containing hallucinated content, incorrect responses, and poor OCR quality. While prior works focus on dataset refinement through high-quality data collection or rule-based filtering, they are costly or limited to specific types of corruption. To deeply understand how corrupted data affects MLLMs,
in this paper, we systematically investigate this issue and find that while corrupted data degrades the performance of MLLMs, its effects are largely superficial in that the performance of MLLMs can be largely restored by either disabling a small subset of parameters or post-training with a small amount of clean data. Additionally, corrupted MLLMs exhibit improved ability to distinguish clean samples from corrupted ones, enabling the dataset cleaning without external help. Based on those insights, we propose a corruption-robust training paradigm combining self-validation and post-training, which significantly outperforms existing corruption mitigation strategies.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Visual Instruction Tuning (VIT)~\citep{liu2023llava} has been actively explored to enhance the visual processing capabilities of Multimodal Large Language Models (MLLMs), extending beyond basic vision-language tasks to more complex domains such as geometric problem-solving~\citep{gao2025gllava} and chart interpretation~\citep{li2024multimodal}. To support these advancements, large-scale visual instruction datasets are either crawled from the Internet or synthesized using generative AI models. However, those datasets often contain corrupted data—such as repetitive~\citep{chen2024expanding} or incorrect responses~\citep{dubey2024llama}, hallucinated content, and poor OCR quality~\citep{wu2024deepseek}—which can degrade the performance of MLLMs or even cause abnormal behaviors. Ensuring high-quality VIT datasets with accurate samples is therefore essential.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!]
    \centering
    \includegraphics[width=\linewidth]{figs/intro.pdf}
    \vspace{-.3in}
    \caption{\textbf{Top}: Examples of corrupted samples in VIT. \textbf{Bottom left}: Average task performance of MLLMs with various corruption ratios. Though simple fine-tuning suffers from performance drop, disabling corruption-related parameters (1.4\%) can largely restore the performance. 
    \textbf{Bottom right}: MLLM's (fine-tuned with corrupted samples) precisions of classifying clean and corrupted samples. We found corrupted data improve classification ability. Details are in Appendix \ref{app:detail_intro}.}
    \label{fig:intro}
\end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


To mitigate dataset corruption, various refinement strategies have been proposed. LLaMA-3~\citep{dubey2024llama} conducts \emph{quality tuning} with verified samples. Molmo~\citep{deitke2024molmo} employs human annotators to curate 1M high-quality image captions. DeepSeek-VL2~\citep{wu2024deepseek} refines responses using meta-information. However, collecting high-quality data or meta-information is expensive. To reduce such costs, Eagle-2~\citep{li2025eagle} and Intern-VL2.5~\citep{chen2024expanding} adopt heuristic rule-based filtering, but they are limited to specific types of corruptions and hence cannot generalize to other scenarios.

A deeper understanding of how corrupted data affects MLLMs is essential for developing more effective and cost-efficient mitigation strategies. For instance, can an MLLM fine-tuned on corrupted data still identify clean samples? Moreover, when faced with a corrupted model, is it necessary to retrain the MLLM from scratch with clean data, or can some corrective method restore the performance of MLLMs efficiently? To address those questions, we aim to investigate: (1) \emph{how does data corruption impact the performance and behavior of MLLMs}? (2) \emph{how can such understanding be leveraged for better mitigation}?

In this paper, we delve deeper into the data corruption of VIT. Through experiments with meticulously designed corrupted datasets, we discover the following intriguing findings that lead to efficient corruption-robust training strategies:

\paragraph{Corrupted data hurt but only superficially.}     
According to the bottom left of Figure~\ref{fig:intro} showing the model performance under various corruption levels, we find that while corrupted training data significantly degrade the performance of MLLMs, the damage is largely superficial. 
That is, simply disabling 1.4\% of parameters in the MLLM fine-tuned on corrupted data
could largely restore the performance of MLLMs. This suggests a fast and cost-effective fix for corrupted models.

\paragraph{Corrupted model differentiates clean and corrupted samples better.}
As shown in the bottom right of Figure~\ref{fig:intro}, models fine-tuned on corrupted data exhibit significant improvement in distinguishing clean samples from corrupted ones when compared to a clean model. We also provide empirical explanations for this phenomenon in Sec. \ref{sec:method}.
As a result, dataset cleaning can be performed effectively without external intervention.

Leveraging those findings, we propose a corruption-robust training paradigm
with \textbf{self-validation} and \textbf{post-training} to mitigate the impact of corrupted data. We compare it against noise-robust loss functions and sample selection methods, demonstrating its superior effectiveness. Our key contributions are summarized as follows.

\begin{itemize}[leftmargin=*]
    \item We are the first to systematically study the impact of corrupted data in VIT, revealing its detrimental yet superficial effect on model performance.
    \item We show that fine-tuning with corrupted data enhances the ability of MLLMs to identify clean samples and provide empirical analysis.
    \item We introduce a corruption-robust training paradigm for VIT that outperforms all existing approaches.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\subsection{Data Enhancement For MLLM}
Data corruption in VIT—such as repetitive~\citep{chen2024expanding,li2025eagle}, hallucinated responses, poor OCR quality~\citep{wu2024deepseek}, and incorrect answers~\citep{dubey2024llama}—degrades the model performance. To improve dataset quality, \textbf{Oracle model} methods regenerate~\citep{chen2023sharegpt4v,chen2024allava} or filter~\citep{fu2025tldr,xiong2024llava} clean samples but rely on costly clean models. \textbf{Heuristic} methods detect corruption via patterns like repetition and image resolution~\citep{chen2024expanding,li2025eagle} but fail to address hallucinations and incorrect responses. In addition, the lack of a comprehensive understanding of how corruption affects MLLMs limits the development of more effective mitigation strategies. Our work fills this gap by analyzing the impact of corrupted samples and proposing a robust solution.



\subsection{Learning with Noisy Labels (LNL)}
Our study is related to learning with noisy labels (LNL) in machine learning, which aims to mitigate the effect of mis-labeled data when training a classification model. It can be generally categorized into three main approaches~\citep{han2020survey}:
designing special loss functions that can be robust to possible wrong supervision~\cite{ghosh2017robust,zhang2018generalized,menon2019can}, 
correcting wrong supervision with model prediction~\citep{tanaka2018joint,yi2019probabilistic,zhang2020distilling},
and sample selection, which identifies noisy samples from the training data and then makes them less influential in the training process~\cite{jiang2018mentornet, han2018co,wei2020combating}. 
Among those approaches, the sample selection approach based on memorization effect~\cite{arpit2017closer,zhang2016understanding} generally achieves the best overall performance.  
This approach considers samples with small loss values as clean samples~\citep{han2018co,jiang2018mentornet,yao2020searching,10509799}. 
Nevertheless, these approaches cannot effectively leverage instruction following abilities of MLLM models and we empirically find those approaches less effective for corrupted data in the era of large language models (LLMs). 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/effect_nr_task.pdf}
\vspace{-.3in}
\caption{\textbf{Effects of corruption on LLaVA-1.5 (LLaMA-3.1-8B).} The evaluation datasets are shown in 3 groups: VQA, Conversation and MC-VQA. The corruption ratio here is 60\%. }
\label{fig:effect_nr_task}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries}
\label{sec:preliminary}

\paragraph{Notations.}
Given an image \( x_v \) and the corresponding instruction \( x_q \),
an MLLM parameterized by \( \bm{\theta} \) predicts a response \( x_y \).
For simplicity of notations, let \( x_c \equiv (x_v, x_q) \). The fine-tuning process optimizes the model on a dataset \( \mathcal{D} = \{(x_c, x_y)\} \) by minimizing the loss \( \ell(x_y |x_c;\bm{\theta}) = -\log p(x_y | x_c; \bm{\theta}) \).  For convenience, we sometimes use \( x\) to denote \( (x_c, x_y) \) and simplify the loss as \( \ell(x; \bm{\theta}) = \ell(x_y | x_c; \bm{\theta}) \).

\paragraph{Corruption under Investigation.} In this paper, we mainly focus on the corruption on the image-text alignment, that is, the response of an image given the instruction could be incorrect. This covers common corruptions in VIT such as incorrect and hallucinated responses, while text-only corruptions such as grammar errors and repetitions are not the focus of this paper.

To construct such corruption, we use the GPT-4o~\citep{hurst2024gpt} to generate incorrect responses to replace the correct ones. 
Specifically, let $z \in \{0,1\}$ denote the correctness of a sample \(x_c\). The dataset with corruptions is defined as  $\mathcal{\Tilde{D}} = \left\{ (x_c, \tilde{x}_y) \right\}$, with 
$$\tilde{x}_y = \begin{cases} 
x_y & \text{if } z = 1 \ \text{(clean)}\\
g(x_c, x_y) & \text{if } z = 0 \  \text{(corrupted)}
\end{cases},$$
where $g$ denotes the GPT-4o model (the prompt used and examples of corrupted data are shown in Appendix~\ref{app:data_corrupt}).
We define the corruption ratio $cr$ as the proportion of corrupted data (i.e., those with $z=0$) in $\Tilde{\mathcal{D}}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Experimental Setup.}
We conduct experiments following the setup of LLaVA-1.5 \citep{liu2023improved}. Specifically, we begin by pre-training the vision projectors, which connect the CLIP visual encoder ViT-L/14 \citep{radford2021learningtransferablevisualmodels} to LLMs (e.g., LLaMA-3.1-8B~\citep{dubey2024llama} and Qwen-2.5 0.5B/3B/7B models~\citep{qwen2.5}), using approximately 600K image-text caption pairs. Then, we perform supervised fine-tuning (SFT) with a 100K instruction-tuning dataset, which is uniformly sampled from LLaVA-665K~\citep{liu2023improved} that comprises 665K text-only and vision-language instances.

Following LLaVA-1.5, we evaluate the performance 
%of fine-tuned MLLMs 
on 11 standard evaluation datasets, which are 
divided into the following groups based on their response formatting prompts: (i) VQA (visual question answering);
(ii) MC-VQA (multiple-choice VQA); and
(iii) Conversation.
More details on the 
%taxonomy of the 
datasets and performance metrics are put in Appendices \ref{app:data_taxonomy} and \ref{app:data_metrics}, respectively.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Effect of Corrupted Data on VIT}

Sec. \ref{sec:effect_benchmark} 
first examines the effect of corrupted data on the performance of MLLMs, and finds that its impact may be superficial. 
Sec. \ref{sec:effect_parameter} then studies this 
by analyzing the parameter space of MLLMs.

\subsection{Superficial Impact of Data Corruption}
%on  Model Performance}
\label{sec:effect_benchmark}

We consider two ways of injecting corruption into the training data. \textbf{Uniform Corruption} uniformly samples clean samples from all datasets and replaces them with corrupted ones. \textbf{Selective Corruption} selectively corrupts clean samples from specific datasets. 
Specifically,
we consider the following dataset variants:
(i) \texttt{uniform}: corruption\footnote{In the following, we use $cr=60\%$. Results with different corruption levels can be found in Appendix \ref{app:effect}.}
is uniformly injected into all datasets; (ii) \texttt{no\_vqa}: 
corruption is injected into all datasets except the VQA datasets; (iii)
\texttt{no\_mc-vqa} (resp. \texttt{no\_conv}) in which the corruption is injected into all datasets except the multiple-choice VQA (resp. conversation) datasets; and 
(iv) \texttt{clean}, where no corruption is introduced. 

Figure~\ref{fig:effect_nr_task} shows the 
performance 
of MLLMs (fine-tuned on those dataset variants)
on clean datasets.
As can be seen, models fine-tuned on \texttt{clean} always outperform those fine-tuned on \texttt{uniform}, demonstrating the negative effects of corrupted training data. However, we observe that on VQA tasks, models fine-tuned on \texttt{no\_vqa} perform comparably to those trained on \texttt{clean} and considerably better than those trained on \texttt{uniform}. This observation also holds for models trained on \texttt{no\_mc-vqa} and \texttt{no\_conv}. In other words, even though the majority of training data are corrupted ($cr=60\%$), the model can still maintain its performance on the evaluation set as long as the corresponding training tasks are not corrupted. 
This leads to a bold hypothesis on the effect of corrupted training data on MLLMs: \emph{Corrupted data superficially impacts MLLMs by encouraging the behavior of producing incorrect answers; however, this effect is reversible, and the model retains its ability to generate correct responses on the evaluation tasks.}

    
\subsection{Analyzing the Superficial Impact of Corrupted Data}
\label{sec:effect_parameter}
To validate the above hypothesis,
we examine the reversibility of the effect of corrupted responses.
First, we try to identify weights in the MLLM that are particularly responsible for generating corrupted responses. 
Specifically, following~\citet{wei2024assessing}, we select weights with top-\(q\%\) influence scores on a \textbf{corrupted} dataset
but remove those that overlap with weights with top-\(p\%\) influence on a \textbf{clean} dataset.
This ensures that only weights contributing specifically to corrupted samples are considered. We use the SNIP score ~\citep{lee2018snip} to compute the influence. Specifically, for any linear layer 
in $\bm{\theta}$ 
with weight matrix \(W \)
and a dataset $D^{*}$
the influence score is computed as
$I(W) = \mathbb{E}_{x \sim D^{*}} |W \odot \nabla_{W} \ell(x;\bm\theta)|$,
where  \(\odot\) denotes element-wise multiplication. 
For more background and details, please refer to Appendix \ref{app:prune}.

In the following, we study  the questions: (i)  Are 
the corruption-related weights 
sparse 
(i.e., they act as behavioral triggers rather than encoding erroneous fundamental knowledge)? 
and (ii)  Can  the  model performance be restored
after disabling these weights?
If affirmative,
these would provide strong evidence that the effect of corrupted samples is both reversible and superficial.


We start with an MLLM fine-tuned with 100K data at \(cr=60\%\). With different choices 
of $(p,q)$
(detailed in Appendix \ref{app:prune}), we disable weights that are only related to the corrupted data 
and report the performance. For comparison, we include results from 
models fine-tuned with 100K and 40K clean data
(denoted by \texttt{Clean} and \texttt{Clean(40K)},
respectively).

Table~\ref{tab:param}
shows the performance on evaluation datasets.
As can be seen, by disabling fewer than \textbf{1.4\%} of the parameters, the corrupted model 
can restore its performance from 39.1 to 57.1, which is close to 
that
(i.e., 59.3) 
of the model fine-tuned with 40K clean data. This confirms our hypothesis in Sec.~\ref{sec:effect_benchmark} on the superficiality and reversibility of the 
effect 
of corrupted samples
on MLLMs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule
Model               & Avg. Score & \% Disabled & ($p$, $q$)        \\ \midrule
\texttt{Clean}               & 61.7      & -          & -             \\ 
\texttt{Clean (40K)}         & 59.3      & -          & -             \\ \midrule
\multirow{4}{*}{\setlength\extrarowheight{0pt}\begin{tabular}[l]{@{}l@{}}\texttt{Corrupted}\\ \texttt{($cr=60\%$, 100K)}\end{tabular}}      & 39.1      & 0          & -             \\ 
                            & 51.4      & 0.84     & (12, 10)      \\ 
                            & 55.2      & 1.16     & (17, 15)      \\ 
                            & 57.1      & 1.39     & (22, 20)      \\ \bottomrule


\end{tabular}%
}
\caption{\textbf{
Performance of LLaVA-1.5 (LLaMA-3.1-8B) with corruption-related weights disabled}.
}
\label{tab:param}
\end{table}



\section{Defending against Corrupted Data} 
\label{sec:method} 
In Sec. \ref{sec:ft_clean}, we show that \textbf{post-training} with a small set of clean data can effectively restore the performance of corrupted models. However, this raises an important question: How can we identify clean samples in a corrupted dataset? To answer this, in Sec. \ref{sec:filter}, we assess the ability of MLLMs to identify the cleanness of training samples and introduce \textbf{self-validation} as a robust solution and provide empirical understanding for it in Sec. \ref{sec:filter_analysis}. Finally, we combine post-training and self-validation as an effective corruption-robust training strategy in Sec. \ref{sec:final_method}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\centering
\includegraphics[width=0.8\linewidth]{figs/ft_clean.pdf}
\vspace{-.2in}
\caption{\textbf{Average performance of the corrupted LLaVA-1.5 (LLaMA-3.1-8B) post-trained with an increasing number of clean data.} 0 in x-axis indicates the original corrupted model without post-training.}
\label{fig:ft_clean}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\centering
\centering
\includegraphics[width=\linewidth]{figs/precision_50_qwen.pdf}

\caption{\textbf{Precision-recall curves of MLLM's predictions on the correctness of 100K samples ($cr=50\%$).} $x$-axis: recall; $y$-axis: precision. Solid and dotted line denote \texttt{Val\_PPL} and \texttt{PPL}, respectively.  Color represents the corruption ratio of the training dataset: \textcolor{black}{\Large{$\bullet$}}\textcolor{black}{0\%}, \textcolor{color1}{\Large{$\bullet$}}\textcolor{color1}{10\%}, \textcolor{color2}{\Large{$\bullet$}}\textcolor{color2}{20\%} \textcolor{color3}{\Large{$\bullet$}}\textcolor{color3}{30\%}, \textcolor{color4}{\Large{$\bullet$}}\textcolor{color1}{40\%}, \textcolor{color5}{\Large{$\bullet$}}\textcolor{color5}{50\%}. }
\label{fig:precision_50_qwen}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Post-training with Clean Data}
\label{sec:ft_clean}
In this section, we exploit the superficial impact of corrupted samples, as identified in Sec. \ref{sec:effect_benchmark}, to restore the performance of corrupted models through post-training with clean data. The disabling of weights as discussed in Sec. \ref{sec:effect_parameter} is another promising direction for performance recovery
and we will leave it for future research. Specifically, for models fine-tuned on corrupted datasets with varying corruption ratios, we continue to fine-tune them with the same hyper-parameters as before. Note that here we use the original clean samples uniformly sampled from all datasets for post-training. However, we will introduce in Sec. \ref{sec:filter} on how to obtain clean samples using the corrupted models. 

Figure~\ref{fig:ft_clean} shows
the average evaluation performance for models fine-tune with varying corruption and post-trained with clean data at different sizes. As can be seen,
even with only 20K clean samples, all
models exhibit performance improvement after post-training. %the amount of 
With more post-training 
data,
the performance quickly converges to that of the clean model. This suggests post-training is a fast and cost-effective approach for performance recovery on corrupted models, without the need of re-training with clean data from scratch.

\subsection{Can MLLMs Identify Corrupted Samples?}
\label{sec:filter}
In this section, we show that MLLMs fine-tuned on corrupted training datasets can be used to identify corrupted samples.
We use
MLLMs that have been fine-tuned on datasets with corruption ratios from 0\% to 50\%, and run them on 
a dataset with 100K samples and \( cr=50\% \).
Each MLLM predicts a score 
(namely, Perplexity and Validation Perplexity to be presented
in the following)
on whether the response in each sample is corrupted or not.
Samples with scores smaller than a 
threshold are considered clean.
Details are in Appendix \ref{app:classify}.

The scores used are: 
\begin{enumerate}[leftmargin=*]
    \item 
Perplexity
(\texttt{PPL}):
This quantifies how ``surprised'' or ``uncertain'' a model is when generating a response conditioned on an instruction~\citep{marion2023less}. 
Samples with higher perplexities 
%(more uncertain) 
are considered
as more likely to be corrupted.
Note that \texttt{PPL} is proportional to the MLLM's training loss (with their formulations in Appendix \ref{app:ppl}) and treating large-loss samples as noisy samples in widely adopted in LNL~\citep{han2018co}.
\item Validation Perplexity
(\texttt{Val\_PPL}):
Using the following template,
we 
directly prompt the corrupted model to predict whether a sample is corrupted (self-validation).
The perplexity of the predicted word “No” 
is used
as the score.
\end{enumerate}


\begin{prompt*}{}{}
{\textit{<image>Query:} $\left\{\text{instruction text}\right\}$ \\
\textit{Response:} $\left\{\text{response text}\right\}$ \\
\textit{Is the response correct? Answer yes or no:}
}
\end{prompt*}


\paragraph{Effectiveness of 
\texttt{PPL} and
\texttt{Val\_PPL}.}
Figure~\ref{fig:precision_50_qwen} compares the recalls and precisions of the scores across varying corruption levels. As can be seen, \texttt{Val\_PPL} (solid curves) is effective in identifying clean data when trained with corrupted data. 
Specifically, at all corruption levels, these models (except
the ones trained on clean data (black curves)) can achieve a precision of over 0.9 
at a recall of 0.25. The reason why we focus on such a small recall is that Sec. \ref{sec:ft_clean} shows post-training with only a small amount of clean samples (\eg, 20K) can largely restore the model's performance.
This demonstrates the effectiveness of self-validation, which can be used in the proposed post-training approach.
In contrast, \texttt{PPL} (dotted curves) performs worse. For example, at a corruption rate of 50\% and recall of 0.25, \texttt{PPL} can only achieve a precision of less than 0.7. For the small-sized LLM (i.e., Qwen-2.5-0.5B), \texttt{Val\_PPL} does not demonstrate much advantage. This suggests that self-validation is an emergent ability that can only be observed in larger language models. 


\begin{figure*}[t]
\centering
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/posterior.pdf}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/cond_likelihood_clean.pdf}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/frac.pdf}
\end{subfigure}
\caption{\textbf{Left}: Distribution of classification probability
\( \hat{p}(z=1|x_c, \tilde{x}_y; \bm{\theta}) \).
Color represents corruption ratios of datasets the model is fine-tuned on: \textcolor{black}{\Large{$\bullet$}}\textcolor{black}{0\%},\textcolor{color1}{\Large{$\bullet$}}\textcolor{color1}{10\%}, \textcolor{color2}{\Large{$\bullet$}}\textcolor{color2}{20\%}. \textbf{Middle}: Mean of $\hat{p}(\tilde{x}_y | x_c, z=0; \bm{\theta})$ and $\hat{p}(\tilde{x}_y | x_c; \bm{\theta})$ of corrupted samples. \textbf{Right}: Mean ratios of $\hat{p}(\tilde{x}_y | x_c, z=0; \bm{\theta})$ to $\hat{p}(\tilde{x}_y | x_c; \bm{\theta})$  for clean and corrupted samples. All results here are obtained from LLaMA-3.1-8B on a dataset with 100K samples and $cr=50\%$.}
\label{fig:classify_anaylsis}
\end{figure*}

\paragraph{How Robust are Scores under Corruption?}
As shown in Figure \ref{fig:precision_50_qwen}, the \texttt{Val\_PPL} curves 
(non-black solid curves) 
at different  corruption ratios
are consistent 
and close to each other.
In contrast, \texttt{PPL} deteriorates sharply as the corruption ratio increases, which is expected since training with corrupted data explicitly lowers the perplexity of the MLLM on corrupted samples, making clean and corrupted data less distinguishable.


\subsection{Analysis on \texttt{Val\_PPL}}
\label{sec:filter_analysis}
An intriguing observation from
Figure \ref{fig:precision_50_qwen} is that
the solid black curves are always below the solid non-black curves, in other words, 
the corrupted model is even better than the clean model in identifying corrupted samples. We provide empirical analysis for this phenomenon in this section. 

\paragraph{Probabilistic Modeling of Self-Validation.}
Recall that
the \texttt{Val\_PPL} template asks the MLLM to predict correctness of a sample. Essentially, the MLLM estimates 
\( p(z=1|x_c, \tilde{x}_y) \), the ground-truth probability that response $\tilde{x}_y$ is correct,
with its output probability 
\( \hat{p}(z=1|x_c, \tilde{x}_y; \bm{\theta}) \).
Figure~\ref{fig:classify_anaylsis} (left)
shows the distributions of 
\( \hat{p}\)
for models
fine-tuned from datasets
with different corruption levels.
As can be seen, for
predictions on clean samples (solid curves),
corruption in the fine-tuning data 
has little effect on the  output probability distribution. In contrast,
significant distribution 
shift is observed on that of the corrupted samples (dashed curves). 

To understand, 
we 
rewrite 
    $p(z = 1 | x_c, \tilde{x}_y)$
as:
\begin{align} 
    p(z = 1 | x_c, \tilde{x}_y)-1 \propto - \frac{p(\tilde{x}_y | x_c, z = 0)}{p(\tilde{x}_y | x_c)}.\label{eq:bayes}
\end{align}
Proof is in Appendix~\ref{app:proof_bayes}.
This shows that 
\( p(z = 1 | x_c, \tilde{x}_y) -1\) is proportional
to the negative of the ratio between 
\( p(\tilde{x}_y | x_c, z = 0) \) 
and 
\( p(\tilde{x}_y | x_c) \). 
Similar to \texttt{Val\_PPL}, 
\( p(\tilde{x}_y | x_c, z = 0) \) 
can be obtained by explicitly prompting the MLLM as follows:

\begin{promptcond*}{}{}
{\textit{<image>Give me an \textcolor{red}{incorrect} answer for the following question.} \\$\left\{\text{instruction text}\right\}$}
\end{promptcond*}  

Figure~\ref{fig:classify_anaylsis} (middle) shows the mean of $\hat{p}(\tilde{x}_y | x_c, z=0; \bm{\theta})$ and $\hat{p}(\tilde{x}_y | x_c; \bm{\theta})$ at \mbox{varying} corruption levels for corrupted samples. 
We observe a sharp rise in $\hat{p}(\tilde{x}_y | x_c, z=0; \bm{\theta})$ (\textcolor{color1}{blue}) when transitioning from a clean model to one trained with slight corruption (\( cr=10\% \)), followed by a slower growth as the corruption increases further. In contrast, $\hat{p}(\tilde{x}_y | x_c; \bm{\theta})$ (\textcolor{color2}{orange}) exhibits a modest increase and remains consistently lower than the other probability. This discrepancy\footnote{A symmetrical but less pronounced trend is observed for clean samples in Appendix \ref{app:classify_add}.} results in a sharp increase in their ratio when the corruption is first introduced, followed by a gradual decline with additional corruption, which is confirmed in Figure~\ref{fig:classify_anaylsis} (right) that visualizes the mean of the ratios. 


\paragraph{Impact of Ratio on Sample Separability.}
The gap between the mean ratios for clean and corrupted samples reflects the separability of MLLMs on those samples using \texttt{Val\_PPL}. As can be seen, this gap initially widens with small corruption, suggesting improved separability, but then narrows as higher corruption increases the likelihood of corrupted data, shrinking the ratio. This aligns with the trends in Figure \ref{fig:precision_50_qwen}, where slight corruption enhances classification while excessive corruption degrades it.


\paragraph{Asymmetric Growth of Two Probabilities.}  
The probability $\hat{p}(\tilde{x}_y| x_c, z=0; \bm{\theta})$ for corrupted data increases rapidly with slight corruption, while $\hat{p}(\tilde{x}_y | x_c; \bm{\theta})$ requires more corruptions to grow. This asymmetry stems from two factors. 
(i) Fine-tuning on corrupted data refines MLLM's understanding of incorrect responses thereby sharply boosts $\hat{p}(\tilde{x}_y | x_c, z=0; \bm{\theta})$. (ii) $\hat{p}(\tilde{x}_y | x_c; \bm{\theta})$ grows more slowly as fine-tuning optimizes  both the clean and corrupted data, leading to competing objectives. In contrast, $\hat{p}(\tilde{x}_y | x_c, z=0; \bm{\theta})$ is conditioned on $z=0$, evolves independently and increases more freely.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
Methods & \textbf{Avg.} & GQA & MME\_P & MME\_C & POPE & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}LLaVA\\ Wild\end{tabular} & MM-Vet & MMB & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}SEED\\ IMG\end{tabular}  & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}SciQA\\ IMG\end{tabular}  &  \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}Text\\ VQA\end{tabular} & OKVQA \\ \midrule


\rowcolor{baselinecolor}
Clean & 61.65 & 59.18 & 1480.36 & 342.86 & 84.25 & 66.30 & 34.68 & 66.92 & 66.91 & 75.51 & 55.25 & 52.28  \\ 
None (CE) & 49.13  & 41.87  & 668.17  & 253.21  & 62.90  & 57.30  & 23.47  & 63.83  & 63.26  & 74.02  & 50.01  & 38.67   \\ \midrule
\multicolumn{12}{l}{\textit{Noise-robust loss functions}} \\
GCE & 50.95  & 40.67  & 751.27  & 240.00  & 69.37  & 62.00  & 23.85  & \underline{65.81}  & \underline{64.84}  & \textbf{74.81}  & 50.05  & \underline{41.51}   \\ 
% Unhinged & 27.81  & 38.46  & 484.80  & 200.00  & 0.00  & 58.70  & 27.11  & 4.47  & 36.69  & 52.70  & 34.53  & 4.00   \\ 
Phuber CE & 47.28  & 37.24  & 595.69  & 258.21  & 46.77  & 59.90  & 26.93  & 61.51  & 63.49  & 73.82  & 48.24  & 40.16   \\ \midrule
\multicolumn{12}{l}{\textit{Sample selection (online)}} \\
MentorNet & 46.21  & 40.07  & 746.12  & 261.43  & 69.67  & 60.20  & 27.84  & 46.13  & 49.39  & 62.82  & 47.57  & 34.69   \\ 
Co-teaching & 47.97  & 39.95  & 583.35  & 253.93  & 66.38  & 57.60  & \underline{29.63}  & 55.58  & 60.91  & 72.14  & 48.83  & 35.68   \\ 
JoCoR & 47.00  & 39.23  & 571.96  & 245.36  & 59.09  & 58.40  & 27.80  & 55.33  & 60.28  & 72.19  & 48.69  & 36.76   \\ \midrule
\multicolumn{12}{l}{\textit{Sample selection (post-training)}} \\
EL2N & 47.07  & 49.34  & \underline{1357.43}  & 269.64  & \textbf{83.97}  & 58.20  & 27.34  & 12.97  & 43.49  & 51.96  & \underline{50.62}  & 38.27   \\ 
GradNorm & \underline{55.77}  & \underline{50.06}  & 1342.40  & \textbf{318.93}  & 76.86  & \underline{66.50}  & 27.25  & 59.79  & 64.01  & 73.43  & 49.07  & 39.48   \\ 
Entropy & 48.60  & 43.76  & 1118.39  & 261.79  & 73.53  & 60.30  & 27.57  & 42.10  & 52.68  & 63.01  & 48.12  & 34.94   \\ 
PPL & 54.69  & 47.54  & 1222.56  & 279.64  & \underline{82.65}  & 60.50  & 25.69  & 64.86  & 62.83  & 73.38  & 49.04  & 39.02   \\ 
\rowcolor{backcolor}
Val\_PPL & \textbf{60.17}  & \textbf{56.65}  & \textbf{1510.48}  & \underline{297.50}  & 82.18  & \textbf{69.30}  & \textbf{31.51}  & \textbf{67.18}  & \textbf{65.48}  & \underline{74.62}  & \textbf{53.48}  & \textbf{48.76}   \\ 
\bottomrule
\end{tabular}%
}
\caption{\textbf{Comparisons of different corruption-robust strategies on LLaVA-1.5 (LLaMA-3.1-8B)}, where \textbf{Avg.} refers to the average performance on 11 benchmarks (normalized to 0-100). Best results are in \textbf{bold}, and the second best ones are \underline{underlined}. Results for Qwen-2.5 series can be found in Tables \ref{tab:main_qwen_0.5b}, \ref{tab:main_qwen_3b}, and \ref{tab:main_qwen_7b} in the appendix.}
\label{tab:main_llama3}
\end{table*}

%%% gce:0.3, phuber: 10, jocor:07, co-teaching: 01, self-teaching: 05

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/ablate.pdf}
\vspace{-.4in}
\caption{\textbf{Average task performance of models post-trained (or fine-tuned from scratch) on data from different sources}.} 
\label{fig:ablate}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Post-training with Self-Validated Samples}
\label{sec:final_method}
The findings post-training in Sec.~\ref{sec:ft_clean} and self-validation in Sec.~\ref{sec:filter} shed lights on an effective strategy when fine-tuned MLLM with corrupted datasets. In detail, we can first fine-tune an MLLM on corrupted datasets.  
Then we use self-validation to filter clean samples and finally conduct post-training on the fine-tuned MLLM with those filtered samples to obtain the final MLLM, which is used for evaluation. 


\section{Experiments}
In this section, we evaluate the effectiveness of the corruption-robust training paradigm proposed in Sec. \ref{sec:method}. 
Sec. \ref{sec:baseline} compares existing corruption-robust strategies to ours and Sec. \ref{sec:ablation} performs ablation studies of the post-training and self-validation components.

\subsection{Comparisons with Existing Methods}
\label{sec:baseline}
We conduct experiments on the 11 benchmark datasets introduced before with $cr=50\%$, by using LLaVA-1.5 built on LLama-3.1-8B and Qwen-2.5 series models.
We compare with the following baselines.
\begin{enumerate}[leftmargin=*]
\item Noise-robust loss functions:
By default, the MLLM is trained with the Cross-Entropy (CE) loss, denoted by \texttt{None (CE)}. We also consider two noise-robust loss functions from \citet{menon2019can}: the Generalized Cross-Entropy (\texttt{GCE}) loss~\citep{zhang2018generalized}, which combines the CE loss and mean absolute error (MAE) 
through the Box-Cox transformation~\citep{boxcox1964}, as MAE has shown to have better robustness to label noise~\citep{ghosh2017robust}; and the Phuber Cross-Entropy loss~\citep{menon2019can} (\texttt{Phuber CE}), which incorporates gradient clipping into CE. Their formulations are provided in Appendix \ref{app:baseline}.

\item Online sample selection methods:
They focus on selecting clean samples during training. \texttt{MentorNet} \citep{jiang2018mentornet} identifies small-loss samples as clean. \texttt{Co-teaching} \citep{han2018co} trains two networks and exchanges small-loss samples between them to avoid error accumulation. \texttt{JoCoR}~\citep{wei2020combating} enforces agreement between networks to prevent biased selection. Their implementations are detailed in Appendix~\ref{app:baseline}.

\item Sample selection methods based on post-training: Different from online sample selection methods, the proposed \texttt{PPL} and \texttt{Val\_PPL} methods select samples after the model is trained.
Moreover, we also use several scores: \texttt{EL2N} and \texttt{GradNorm}, which measure the $\ell_2$-norm of the output error vector and the gradient, respectively~\citep{paul2021deep}; and \texttt{Entropy}~\citep{Coleman2020Selection}, which reflects uncertainty in the output probabilities. Formal definitions of these scores are provided in Appendix \ref{app:baseline}.
\end{enumerate}


\paragraph{Results.}
Table \ref{tab:main_llama3} 
compares the various corruption-robust strategies.
As can be seen, \texttt{Val\_PPL} significantly restores the performance of a corrupted model, improving it from 49.13 to 60.2 on average, where the clean model achieves 61.65. Moreover, \texttt{Val\_PPL} outperforms all the baselines on average and achieves the best results on 8 out of 11 evaluation tasks. 


\subsection{Ablation Study}
\label{sec:ablation}
To assess the effectiveness of self-validation, we conduct post-training on LLaVA-1.5 fine-tuned with 50\% corruption ratio\footnote{Results for other corruption levels are in Figure~\ref{fig:ablate_more} in the appendix.}
using ``clean data" from various sources. (i) \texttt{GT}: Clean data with responses directly from the dataset.
This can be regarded as the ``best" clean data possible);
(ii) \texttt{PPL} (we choose it as it is widely adopted in LNL);
and (iii) the proposed \texttt{Val\_PPL}. In addition, to demonstrate the advantage of post-training, we also experiment with 
(iv) \texttt{Val\_PPL(Scratch)}, which fine-tunes the model from scratch using the samples selected by \texttt{Val\_PPL}
rather than post-training. 

Figure~\ref{fig:ablate} shows
the average task performance of these models post-trained (or fine-tuned from scratch) 
using ``clean data'' from various sources with different sizes.
As can be seen, for the 3B and larger models, \texttt{Val\_PPL} performs as well as \texttt{GT} across different data sizes, rapidly approaching the performance of the clean model. However, for Qwen-2.5-0.5B, only \texttt{GT} can restore the model performance. This is consistent with our observation in Figure \ref{fig:precision_50_qwen} that the self-validation is an emergent ability for larger LLMs. Further, we find that \texttt{Val\_PPL(Scratch)} is consistently outperformed by its post-trained counterparts, demonstrating the sample-efficiency of post-training.  

\section{Conclusions}
In this paper, we show that while corrupted data hampers the performance of MLLMs, its impact is superficial. By disabling corruption-related parameters or post-training with clean data, the performance of MLLMs can be largely restored. Additionally, the corrupted MLLMs can effectively distinguish between clean and corrupted samples via self-validation, enabling self-cleaning of datasets. Building on this, the proposed corruption-robust training paradigm significantly outperforms existing strategies.

%\newpage
\section*{Limitations}
One limitation of this paper is that we did not study MLLMs with larger LLMs (\eg, 70B and 400B) due to limited computational resources. 

\section*{Acknowledgment}
We gratefully acknowledge the support of MindSpore, CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research.

\bibliography{custom}

% Start of the appendix
\appendix
\renewcommand*\contentsname{Appendix}
\clearpage

\setcounter{tocdepth}{-1}  

% Now, generate a new ToC for the appendix
\tableofcontents  % This will now show only appendix sections

\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

\section{Details in Figure \ref{fig:intro}}
\label{app:detail_intro}
The ``Simple FT'' results in Figure \ref{fig:intro} are aggregated from Figure \ref{fig:effect_nr} with LLaMA-3.1-8B (experiment details in Sec. \ref{sec:effect_benchmark}). Results of ``Ours'' are aggregated from Figure \ref{fig:ablate} and \ref{fig:ablate_more}  (experiment details in Sec. \ref{sec:baseline}). The results of ``Disable Params.'' is taken from Table \ref{tab:param} with details in Sec. \ref{sec:effect_parameter}. The figure of precision under various corruption level is obtained from Figure \ref{fig:precision_50_qwen} using LLaMA-3.1-8B at a recall of 0.5 (check Sec. \ref{sec:filter} for more details). 

\section{Dataset Details}
\subsection{Corrupted Datasets}
\label{app:data_corrupt}
The prompt used by GPT (described in Sec \ref{sec:preliminary}) to generate corrupted samples is shown in Figure \ref{fig:prompt}. We provide one example 
for each of the datasets used, an example of corrupted sample is shown
in Figures
\ref{fig:exp_llava150}-\ref{fig:exp_textcaps}. As can be seen, GPT produces corrupted samples in the following ways: changing the option letter in multiple choice VQA (Figure \ref{fig:exp_aokvqa}), replacing the correct answer in VQA with a plausible but incorrect one (Figures \ref{fig:exp_gqa}-\ref{fig:exp_okvqa}), inducing object hallucination in a conversation (Figure \ref{fig:exp_llava150}) and generating wrong captions (Figure \ref{fig:exp_textcaps}). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figs/prompt.pdf}
\vspace{-.3in}
\caption{\textbf{Prompts for generating corrupted data.}}
\label{fig:prompt}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/exp_llava150.pdf}
\vspace{-.3in}
\caption{Example of corrupted sample in dataset (\textbf{LLaVA-158K}).}
\label{fig:exp_llava150}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/exp_aokvqa.pdf}
\vspace{-.3in}
\caption{Example of corrupted sample in datasets \textbf{A-OKVQA} and \textbf{RefCOCO}. 
}
\label{fig:exp_aokvqa}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/exp_gqa.pdf}
\vspace{-.3in}
\caption{Example of corrupted sample in dataset \textbf{GQA}.}
\label{fig:exp_gqa}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/exp_vqav2.pdf}
\vspace{-.3in}
\caption{Example of corrupted sample in dataset \textbf{VQAv2}.}
\label{fig:exp_vqav2}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/exp_ocrvqa.pdf}
\vspace{-.3in}
\caption{Example of corrupted sample in dataset \textbf{OCRVQA}.}
\label{fig:exp_ocrvqa}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/exp_okvqa.pdf}
\vspace{-.3in}
\caption{Example of corrupted sample in dataset \textbf{OKVQA}.}
\label{fig:exp_okvqa}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/exp_textcaps.pdf}
\vspace{-.3in}
\caption{Example of corrupted sample in dataset \textbf{TextCaps}.}
\label{fig:exp_textcaps}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Task Taxonomy}
\label{app:data_taxonomy}
To better understand the effect of corruption on the fine-tuning process of MLLMs, we categorize the training and evaluation tasks into 3
classes according to their response formatting prompts. These prompts specify how the MLLM should format a response when queried with a question.
\begin{itemize}
    \item VQA (visual question answering): The MLLM is prompted to answer shortly via \emph{``Answer the question using a single word or phrase.''}
    \item MC-VQA (multiple-choice VQA): The MLLM is prompted to answer only the option via \emph{``Answer with the option's letter from the given choices directly.''}
    \item Conversation: The MLLM receives no format prompt. It responds to the question in a verbose way like a conversation. 
    \item Others (not categorized): The format prompt of this task falls into none of VQA, MC-VQA and Conversation. These format prompts only appear in the training dataset.
\end{itemize}
Based on the above categories, we list the training and evaluation datasets along with our taxonomy in Tables \ref{tab:training_taxonomy} and \ref{tab:evaluation_taxonomy}.  



\begin{table}[t]
\centering
\setlength{\tabcolsep}{8pt} % Adjust column separation for better readability
\renewcommand{\arraystretch}{1.3} % Adjust row height for improved aesthetics
\begin{tabular}{@{}p{2cm}p{5cm}@{}}
\toprule
\textbf{Category} & \textbf{Training Datasets} \\
\midrule
\textbf{VQA}       & VQAv2~\citep{goyal2017making}, GQA~\citep{hudson2019gqa}, OKVQA~\citep{marino2019ok}, OCRVQA~\citep{8978122}       \\
\textbf{MC-VQA}    & A-OKVQA~\citep{schwenk2022okvqa}                   \\
\textbf{Conversation} & LLaVA-158K~\citep{liu2023llava}, ShareGPT~\citep{sharegpt}                         \\
\textbf{Others (not categorized)}    & TextCaps~\citep{sidorov2020textcaps}, VG~\citep{krishna2017visual}, RefCOCO~\citep{kazemzadeh-etal-2014-referitgame,mao2016generation}     \\
\bottomrule
\end{tabular}
\caption{\textbf{Taxonomy of 10 Training Datasets.} Note that no corruption is injected into ShareGPT in all our experiments as it is a text-only dataset.}
\label{tab:training_taxonomy}
\end{table}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{8pt} % Adjust column separation for better readability
\renewcommand{\arraystretch}{1.3} % Adjust row height for improved aesthetics
\label{tab:evaluation_taxonomy}
\begin{tabular}{@{}p{2cm}p{5cm}@{}}
\toprule
\textbf{Category} & \textbf{Evaluation Datasets} \\
\midrule
\textbf{VQA} & GQA~\citep{hudson2019gqa},  MME~\citep{fu2024mmecomprehensiveevaluationbenchmark}, POPE~\citep{li2023evaluating}, OKVQA~\citep{marino2019ok},  TextVQA~\citep{singh2019textvqa} \\ 
\textbf{MC-VQA} & MMB~\citep{liu2023mmbench}, SEED-IMG~\citep{li2023seed}, SciQA-IMG~\citep{lu2022scienceqa}    \\
\textbf{Conversation} & LLaVA-Wild~\citep{liu2023llava}, MM-Vet~\citep{yu2023mmvet} \\
\bottomrule
\end{tabular}
\caption{\textbf{Taxonomy of the 11 Evaluation Datasets.} Note that MME can be split into the perception set MME\_P and the cognition set MME\_C. }
\label{tab:evaluation_taxonomy}
\end{table}

\subsection{Evaluation Metrics}
\label{app:data_metrics}
The score ranges for MME\_P and MME\_C are $[0, 2000]$ and $[0, 800]$, respectively. In our paper, we report both their original values and the normalized values (scaled to $[0, 100]$) interchangeably. All the remaining datasets have a metric range of $[0, 100]$. We also report the average performance by taking the mean scores (normalized) of the 11 tasks in some experiments. 

\section{More on Effect of Corruption}
\label{app:effect}
For experiments with uniform corruption, we vary the corruption ratio ($cr$) from 0\% to 60\% and construct a reference dataset with those corrupted samples removed to see whether they are contributing negatively. Figure~\ref{fig:effect_nr} shows the model's performance under uniform corruption across different ratios on various benchmarks. 

As can be seen in Figure~\ref{fig:effect_nr}, for most tasks, corrupting the data results in worse performance than simply removing them. This degradation worsens with increasing $cr$, indicating the negative effect of corrupted data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/effect_nr.pdf}
\vspace{-.3in}
\caption{\textbf{Performance (y-axis) of LLaVA-1.5 (LLaMA-3.1-8B) under different corruption ratios (x-axis).}}
\label{fig:effect_nr}
\end{figure*}

\section{Details on Identifying Important and Corruption-related Weights}
\label{app:prune}

\subsection{Identifying Important Weights}
Following~\cite{wei2024assessing}, we use the SNIP score~\citep{lee2018snip} to quantify weight importance. For any linear layer with a weight matrix \(W \in \mathbb{R}^{d_{\mathrm{out}} \times d_{\mathrm{in}}}\), the importance score of a weight entry \(W_{ij}\) is given by:
\[
I(W_{ij}, x) = |W_{ij} \cdot \nabla_{W_{ij}} \ell(x;\bm{\theta})|,
\]
which is a first-order Taylor approximation of the change in the loss when \(W_{ij}\) is set to zero. In matrix form, this extends to:
\[
I(W, x) = |W \odot \nabla_{W} \ell(x;\bm{\theta})|,
\]
where \(\odot\) denotes element-wise multiplication. Given a dataset \(D^{*}\) of interest, we aggregate importance scores over all instances:
\[
I(W) =  \mathbb{E}_{x \sim D^{*}} |W \odot \nabla_{W} \ell(x;\bm{\theta})|.
\]
Intuitively, \(I(W)\) measures how critical each weight is for the model's predictions on $D^{*}$. A small \(I(W)_{ij}\) indicates that setting \(W_{ij}\) to zero has minimal impact on the loss.

\subsection{Identifying Corruption-related Weights}
A straightforward approach to identifying corruption-related weights is to compute \(I(W)\) using a corrupted dataset and select the highest-ranked weights. However, some of these weights may also contribute to correct predictions. To address this, we adopt the approach proposed in \citet{wei2024assessing} to identify weights that are \textbf{specific} to corrupted data by leveraging set difference. 


Specifically, we compute \(I^c\) using a model trained only with clean data and a small dataset consisting of 1K clean samples as $D^{*}$. Similarly, we compute \(I^n\) using a model trained on dataset with 60\% corruption and an 1K corrupted dataset as $D^{*}$. For any pair of sparsity levels \((p, q)\), we define the top-\(p\%\) important weights \(S^c(p)\) for \textbf{clean} samples as the weights whose \(I^c_{i,j}\) scores rank within the top \(p\%\) of the \(i\)-th row of \(I^c\)~\citep{sun2024a}:
\[
S^c(p) = \{(i,j) | I^c_{i,j} \text{ is among the top } p\% \text{ of } I^c_i\}.
\]
Similarly, we define the top-\(q\%\) important weights \(S^n(q)\) for \textbf{corrupted} samples as :
\[
S^n(q) = \{(i,j) | I^n_{i,j} \text{ is among the top } q\% \text{ of } I^n_i\}.
\]
Finally, the isolated weights \(S(p,q)\) are defined as the set difference between \(S^n(q)\) and \(S^c(p)\):
\[
S(p,q) = S^n(q) - S^c(p).
\]
This approach isolates weights specific to corrupted samples while filtering out those that are also important for producing clean samples.

\paragraph{Choices of $(p,q)$.}
We begin with small, identical values for $(p,q)$ and gradually increase them until we observe a significant performance improvement compared to the original model. To prevent the model from collapsing due to the removal of critical weights essential for correct predictions, we set $p$ to be slightly larger than $q$. This ensures that more of the weights responsible for generating correct, clean responses are preserved. We find that this approach leads to better overall performance, with fewer parameters disabled.

\section{Details on Identifying Correctness using MLLM}

\subsection{Decision Rule and Evaluation}
\label{app:classify}
Giving a score (\eg, \texttt{PPL} and \texttt{Val\_PPL}) and the dataset (100K samples with $cr=50\%$), the following is conducted: 
\begin{enumerate}
    \item Computing the score for all the samples in a dataset.
    \item Choosing thresholds \( \tau \) starting from the 1st to the 100th lower percentiles of the score. Then, for each threshold \( \tau \), we define: $\hat{z}_i = \mathbbm{1} \left( \text{score}(x_i) < \tau \right),$ where \( \mathbbm{1}(\cdot) \) is the indicator function. Intuitively, a sample $x_i$ is predicted as clean $\hat{z}_i = 1$ if its score is lower than the threshold. 
    \item For $N$ samples in total, we compute precision ($P$) and recall ($R$) scores for all thresholds as follows:
\[
P = \frac{ \sum_{i=1}^N \mathbbm{1}(z_i = 1 \text{ and } \hat{z}_i = 1) }{ \sum_{i=1}^N \mathbbm{1}( \hat{z}_i = 1 ) }.
\]

$$
R = \frac{ \sum_{i=1}^N \mathbbm{1}(z_i = 1 \text{ and } \hat{z}_i = 1) }{ \sum_{i=1}^N \mathbbm{1}(z_i = 1) }.
$$
    \item We draw the precision-recall curve.
\end{enumerate}

\subsection{Details on Scores}
\label{app:ppl}
\paragraph{Perplexity}
For a sample $x$, its perplexity is computed as : 
\begin{equation}
    \texttt{PPL}(x) = \exp\left(-\frac{1}{n} \sum_{t=1}^{n} \log p(x_y^t | x_y^{<t}, x_c; \bm{\theta})\right),
\end{equation}
where $n$ is the length of the response.
It quantifies how ``surprised'' or ``uncertain'' a model is when generating a response conditioned on an instruction. A lower PPL indicates higher confidence, while a higher PPL suggests that the response is less probable under the model’s learned distribution. Intuitively, a higher PPL indicates the sample is more likely to be corrupted according to the models' knowledge.  

The loss for this sample is $\log{\texttt{PPL}(x)}$. 

\subsection{Proof of Eq. \eqref{eq:bayes}}
\label{app:proof_bayes}
By Bayes' theorem, 
\begin{eqnarray}
\lefteqn{p(z = 0 | x_c, \tilde{x}_y) } \nonumber
\\
& =  &
    \frac{p(\tilde{x}_y | z = 0, x_c)}{p(\tilde{x}_y | x_c)} 
    \cdot p(z = 0 | x_c). \label{eq:tmp}
\end{eqnarray}
Since corruption is uniformly added
on all samples,
the correctness label \( z \) is independent of \( x_c \), and so
\( p(z = 0 | x_c) = p(z = 0) \).
Substituting this into  (\ref{eq:tmp}),
we obtain
\begin{align*}
    p(z = 1 | x_c, \tilde{x}_y) &= 
    1 - c \cdot \frac{p(\tilde{x}_y | x_c, z = 0)}{p(\tilde{x}_y | x_c)},
\end{align*}
where \( c = p(z = 0) \). Hence, we have
\begin{align*}
    p(z = 1 | x_c, \tilde{x}_y)-1 \propto 
    - \frac{p(\tilde{x}_y | x_c, z = 0)}{p(\tilde{x}_y | x_c)}.
\end{align*}

\subsection{Additional Results}
\label{app:classify_add}
Figure \ref{fig:cond_likelihood_noisy} shows the mean of $\hat{p}(\tilde{x}_y | x_c, z=0; \bm{\theta})$ and $\hat{p}(\tilde{x}_y | x_c; \bm{\theta})$ at \mbox{varying} corruption levels for \textbf{clean} samples. Compared to Figure \ref{fig:classify_anaylsis} (Middle), a symmetrical but less pronounced trend is observed for clean samples. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figs/cond_likelihood_noisy.pdf}
\caption{Mean of $\hat{p}(\tilde{x}_y | x_c, z=0; \bm{\theta})$ and $\hat{p}(\tilde{x}_y | x_c; \bm{\theta})$ of corrupted samples.}
\label{fig:cond_likelihood_noisy}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Implementation Details on Baselines}
\label{app:baseline}
\subsection{Noise-robust Loss Functions}
The mathematical formulations for the loss functions and their corresponding hyper-parameters are provided in Table \ref{tab:loss}. For GCE, a larger $q$ ($q \in (0, 1]$) reduces the sensitivity of the loss to over-confident yet incorrect predictions. In the case of Phuber CE, the hyper-parameter $\tau$ ($\tau > 1$) determines the threshold for gradient clipping when the model produces over-confident but incorrect predictions, thereby enhancing its robustness to potential data noise. Overall, these loss functions aim to mitigate the adverse impact of small predicted probabilities for the target classes (see loss visualizations in Figure \ref{fig:robust_loss}), which often arise due to corrupted data. To optimize performance, we conduct a hyper-parameter search for $q$ and $\tau$ in Figure \ref{fig:hyper}.



\subsection{Sample Selection Methods}
For MentorNet, Co-teaching and JoCoR (Algorithms \ref{alg:mentornet}, \ref{alg:coteaching} and \ref{alg:jocor}). We assume access to an estimated noise level $\alpha$ for the dataset and drop certain amount of data (which is linearly warmed up from 0 to $\alpha$ in $T_k$ steps) according to the defined criterion in the algorithm. Recall the definition of loss in Sec. \ref{sec:preliminary}, we let $\gL(\mathcal{B}; \bm{\theta})$ be the batch-wise formulation $\sum_{x\in \mathcal{B}}\frac{1}{|\mathcal{B}|}\ell(x;\bm{\theta})$. We search for the best $\frac{T_k}{T}$ in Figure \ref{fig:hyper}.


\paragraph{MentorNet} While various implementations of MentorNet exist, we follow prior works on learning with label noise~\cite{han2018co, wei2020combating} and adopt a simple selection criterion based on self-computed loss. Specifically, we retain only samples with small loss for model training.

\paragraph{Co-teaching} Originally, Co-teaching utilizes two randomly initialized networks to generate diverse predictions and mitigate error accumulation. However, in the era of MLLM, pre-trained LLMs are required for initialization. To introduce diversity in model predictions, we train them using two independently shuffled data-loaders (with different random seeds). This strategy is also employed in JoCoR, which similarly relies on two distinct networks.

\paragraph{JoCoR}
It was originally designed for image classification tasks, where the consistency loss minimizes the divergence between two models' class predictions $y$ for images $x$:
\[
\ell_{con}^{\text{cls}}(x; \bm{\theta}_f, \bm{\theta}_g) = D_{KL}(p_f \| p_g) + D_{KL}(p_g \| p_f), 
\]
\[
p_f = p(y | x; \bm{\theta}_f), \quad p_g = p(y | x; \bm{\theta}_g).
\]
For autoregressive sequence generation, the model predicts a sequence token by token, requiring consistency at each step:
\[
\ell_{con}^{\text{seq}}(x; \bm{\theta}_f) = \sum_{t=1}^{T} D_{KL}(p_f^t \| p_g^t) + D_{KL}(p_g^t \| p_f^t).
\]
where
\[
p_f^t = p(x_y^t | x_y^{<t}, x_c; \bm{\theta}_f), 
\]
\[
p_g^t = p(x_y^t | x_y^{<t}, x_c; \bm{\theta}_g).
\]
This ensures divergence minimization at each decoding step rather than a single output. Therefore, we use $\ell_{con}^{\text{cls}}$ as $\ell_{con}$ and $\gL_{con}(\mathcal{B}; \bm{\theta}_f, \bm{\theta}_g)$ its batch-wise formulation.



\subsection{Scores Used for Sample Selection in Post-training}
Let \( p_t(i) = p(x_y^t = i | x_y^{<t}, x_c; \bm{\theta}) \) denote the model's predicted probability for token \( i \) at timestep \( t \). 

\paragraph{Entropy}
The entropy is computed as   
\[
H(x; \bm{\theta}) = - \frac{1}{T} \sum_{t=1}^{T} \sum_{i=1}^{V} p_t(i) \log p_t(i),
\]
which measures the model’s uncertainty, averaged over tokens.

\paragraph{EL2N}
Denoting one-hot indicator vector for the true token \( x_y^t \) as  
\[
\mathbf{1}_{x_y^t} \in \mathbb{R}^{V}, \quad \text{where} \quad \mathbf{1}_{x_y^t, i} =
\begin{cases}
1, & \text{if } x_y^t = i, \\
0, & \text{otherwise}.
\end{cases}
\]
where \( V \) is the vocabulary size. 
The  L2-norm of the output error (EL2N) is computed as:  
\[
\text{EL2N}(x; \bm{\theta}) = \frac{1}{T} \sum_{t=1}^{T} \sqrt{\sum_{i=1}^{V} \left( p_t(i) - \mathbf{1}_{x_y^t, i} \right)^2}.
\]
which quantifies the deviation of the predicted probability distribution from the ground truth, averaged over tokens.

\paragraph{GradNorm} It is defined as the L2-norm of the gradient vector that is formed by stacking the flattened gradient of each trainable parameter in the model. 
\[
\text{GradNorm}(x; \bm{\theta}) = ||\nabla_{\bm{\theta}}\ell(x_y| x_c; \bm{\theta})||_2
\]

When using these scores for sample selection in post-training, we experimented with both higher and lower values and found that selecting samples with lower values yielded better results (Table \ref{tab:ablation_lower_higher}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h!]
\centering
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule
Loss    & Definition  & Hyper-parameters \\ 
\midrule
CE & $-\log(p)$ & - \\
GCE   & $(1-p^q)/q$   & $q \in (0,1]$           \\ 
% Unhinged     & $1-p$  & - \\ 
Phuber CE    & $\begin{cases}
    -\log(p) & p > \frac{1}{\tau} \\
    1 + \log(\tau) - \tau*p  & p \le \frac{1}{\tau}
\end{cases} $     & $\tau > 1$ \\ 
                            \bottomrule
\end{tabular}%
\caption{Mathematical definition for noise robust loss functions. $p$ denotes the probability of predicting specific tokens.}
\label{tab:loss}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figs/robust_loss.pdf}
\vspace{-.3in}
\caption{\textbf{Visualization of noise-robust loss functions.}}
\label{fig:robust_loss}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{algorithm*}[ht]
	\caption{MentorNet}
	\begin{algorithmic}[1]
		\STATE Let $\bm{\theta}$ be the MLLM parameters, $T$ the total number of training steps, $\mathcal{D}$ the corrupted dataset, $\alpha$ the estimated corruption level, $\eta$ the learning rate, and $T_k$ the warm-up steps;
            \STATE Shuffle training data $\mathcal{D}$;
		\FOR{$t = 0, \dots, T-1$}
            \STATE $R(t) = 1 - \min \left\{\frac{t}{T_k} \cdot \alpha, \alpha\right\}$;
		\STATE Draw a mini-batch $\mathcal{B}$ from the dataset $\mathcal{D}$;
            \STATE Select $R(t) \cdot |\mathcal{B}|$ small-loss samples $\hat{\mathcal{B}}$ from $\mathcal{B}$ based on $\ell(x_i; \bm{\theta})$;
		\STATE Update the parameters: $\bm{\theta} = \bm{\theta} - \eta \nabla_{\bm{\theta}} \gL(\hat{\mathcal{B}}; \bm{\theta})$;
		\ENDFOR
	\end{algorithmic}
	\label{alg:mentornet}
\end{algorithm*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{algorithm*}[ht]
	\caption{Co-teaching}
	\begin{algorithmic}[1]
		\STATE Let $\bm{\theta}_f$ and $\bm{\theta}_g$ be the parameters for two identical MLLMs,  $T$ the total number of training steps, $\mathcal{D}$ the corrupted dataset, $\alpha$ the estimated corruption level, $\eta$ the learning rate, and $T_k$ the warm-up steps;
		\FOR{$t = 0, \dots, T-1$}
            \STATE Shuffle training data $\mathcal{D}$ twice with different seeds to get $\mathcal{D}_f$ and $\mathcal{D}_g$;
            \STATE $R(t) = 1 - \min \left\{\frac{t}{T_k} \cdot \alpha, \alpha\right\}$;
		\STATE Draw mini-batches $\mathcal{B}_f$ and $\mathcal{B}_g$ from datasets $\mathcal{D}_f$ and $\mathcal{D}_g$;
            \STATE Select $R(t) \cdot |\mathcal{B}_g|$ small-loss samples $\hat{\mathcal{B}}_g$ from $\mathcal{B}_g$ based on $\ell(x_i; \bm{\theta}_f)$;
            \STATE Select $R(t) \cdot |\mathcal{B}_f|$ small-loss samples $\hat{\mathcal{B}}_f$ from $\mathcal{B}_f$ based on $\ell(x_i; \bm{\theta}_g)$;
		\STATE Update the parameters: $\bm{\theta}_f = \bm{\theta}_f - \eta \nabla_{\bm{\theta}_f} \gL(\hat{\mathcal{B}_f}; \bm{\theta}_f)$;
            \STATE Update the parameters: $\bm{\theta}_g = \bm{\theta}_g - \eta \nabla_{\bm{\theta}_g} \gL(\hat{\mathcal{B}_g}; \bm{\theta}_f)$;
		\ENDFOR
	\end{algorithmic}
	\label{alg:coteaching}
\end{algorithm*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm*}[ht]
	\caption{JoCoR}
	\begin{algorithmic}[1]
		\STATE Let $\bm{\theta}_f$ and $\bm{\theta}_g$ be the parameters for two identical MLLMs,  $T$ the total number of training steps, $\mathcal{D}$ the corrupted dataset, $\alpha$ the estimated corruption level, $\eta$ the learning rate, $\lambda$ the weighting co-efficient, and $T_k$ the warm-up steps;
		\FOR{$t = 0, \dots, T-1$}
            \STATE Shuffle training data $\mathcal{D}$ twice with different seeds to get $\mathcal{D}_f$ and $\mathcal{D}_g$;
            \STATE $R(t) = 1 - \min \left\{\frac{t}{T_k} \cdot \alpha, \alpha\right\}$;
		\STATE Draw mini-batches $\mathcal{B}_f$ and $\mathcal{B}_g$ from datasets $\mathcal{D}_f$ and $\mathcal{D}_g$;
            \STATE Select $R(t) \cdot |\mathcal{B}_g|$ small-loss samples $\hat{\mathcal{B}}_g$ from $\mathcal{B}_g$ based on $(1-\lambda)\ell(x_i; \bm{\theta}_f) + \lambda \ell_{con}(x_i; \bm{\theta}_f, \bm{\theta}_g)$;
            \STATE Select $R(t) \cdot |\mathcal{B}_f|$ small-loss samples $\hat{\mathcal{B}}_f$ from $\mathcal{B}_f$ based on $(1-\lambda)\ell(x_i; \bm{\theta}_g) + \lambda\ell_{con}(x_i; \bm{\theta}_g, \bm{\theta}_f)$;
		\STATE Update the parameters: $\bm{\theta}_f = \bm{\theta}_f - \eta \nabla_{\bm{\theta}_f} \left((1-\lambda)\gL(\hat{\mathcal{B}_f}; \bm{\theta}_f) + \lambda\gL_{con}(\hat{\mathcal{B}_f}; \bm{\theta}_g, \bm{\theta}_f)\right)$;
            \STATE Update the parameters: $\bm{\theta}_g = \bm{\theta}_g - \eta \nabla_{\bm{\theta}_g} \left((1-\lambda)\gL(\hat{\mathcal{B}_g}; \bm{\theta}_g) + \lambda\gL_{con}(\hat{\mathcal{B}_g}; \bm{\theta}_f, \bm{\theta}_g)\right)$;
		\ENDFOR
	\end{algorithmic}
	\label{alg:jocor}
\end{algorithm*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
Methods & \textbf{Avg.} & GQA & MME\_P & MME\_C & POPE & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}LLaVA\\ Wild\end{tabular} & MM-Vet & MMB & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}SEED\\ IMG\end{tabular}  & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}SciQA\\ IMG\end{tabular}  &  \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}Text\\ VQA\end{tabular} & OKVQA \\ \midrule


\rowcolor{baselinecolor}
Clean & 45.78  & 47.88  & 1140.91  & 257.86  & 81.75  & 46.60  & 14.72  & 45.19  & 51.14  & 60.44  & 36.61  & 30.02   \\ 
None (CE) & 36.01  & 37.06  & 670.50  & 217.86  & 48.25  & 40.80  & 15.87  & 33.25  & 48.61  & 55.33  & 32.21  & 23.95   \\ \midrule
\multicolumn{12}{l}{\textit{Noise-robust loss functions}} \\
GCE & 38.83  & 38.23  & 735.12  & 254.29  & 46.56  & \textbf{44.80}  & \textbf{19.95}  & \textbf{40.98}  & \textbf{50.17}  & \textbf{60.59}  & \textbf{36.32}  & 20.94   \\ 
% Unhinged & 36.54  & 38.18  & 881.97  & \underline{255.71}  & 61.12  & 42.10  & 15.83  & 23.20  & 42.57  & 51.31  & 33.38  & 18.15   \\ 
Phuber CE & 37.61  & 36.42  & 776.03  & 211.43  & 55.75  & \underline{44.20}  & 16.10  & 33.59  & 49.10  & \underline{59.44}  & 33.99  & 19.93   \\ \midrule
\multicolumn{12}{l}{\textit{Sample selection (online)}} \\
MentorNet & 39.18  & 37.54  & 882.57  & \textbf{268.57}  & 68.96  & 36.00  & 13.44  & 36.60  & 48.02  & 55.18  & 32.49  & 25.10   \\ 
Co-teaching & 39.76  & 36.85  & 884.94  & 235.71  & 69.98  & 39.20  & 18.67  & \underline{37.97}  & 48.18  & 55.13  & 32.52  & 25.20   \\ 
JoCoR & 39.26  & 36.80  & 844.99  & 239.29  & 70.40  & 38.10  & 15.37  & 36.43  & 47.61  & 56.72  & 32.95  & \underline{25.29}   \\ \midrule
\multicolumn{12}{l}{\textit{Sample selection (post-training)}} \\
EL2N & 36.88  & 37.19  & 646.61  & 205.36  & 61.13  & 35.00  & 16.01  & 36.86  & 48.35  & 54.73  & 33.96  & 24.40   \\ 
GradNorm & \underline{40.32}  & \underline{40.90}  & 863.06  & 233.57  & 72.08  & 43.80  & 16.42  & 36.68  & \underline{49.32}  & 57.86  & 32.80  & 21.33   \\ 
Entropy & 30.18  & 37.92  & 841.09  & 253.21  & 27.87  & 38.80  & 16.01  & 0.26  & 42.13  & 46.41  & 29.20  & 19.69   \\ 
PPL & 39.26  & 39.86  & \underline{913.69}  & 227.14  & \underline{74.46}  & 41.20  & 16.70  & 26.12  & 46.71  & 54.54  & 33.86  & 24.32   \\ 
\rowcolor{backcolor}
Val\_PPL & \textbf{42.28}  & \textbf{43.62}  & \textbf{1076.64}  & 226.79  & \textbf{77.19}  & 42.70  & \underline{18.72}  & 34.97  & 49.10  & 55.68  & \underline{35.02}  & \textbf{25.86}   \\ 
\bottomrule

\end{tabular}%
}
\caption{\textbf{Comparisons of different corruption-robust strategies on Qwen-2.5-0.5B.} Here, \textbf{Avg.} refers to the average performance on 11 benchmarks (normalized to 0-100). Best results are \textbf{Bold}, second best are \underline{underlined}.}
\label{tab:main_qwen_0.5b}
\end{table*}

%%% gce:0.7, phuber: 5, jocor:01, co-teaching: 01, self-teaching: 07


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
Methods & \textbf{Avg.} & GQA & MME\_P & MME\_C & POPE & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}LLaVA\\ Wild\end{tabular} & MM-Vet & MMB & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}SEED\\ IMG\end{tabular}  & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}SciQA\\ IMG\end{tabular}  &  \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}Text\\ VQA\end{tabular} & OKVQA \\ \midrule


\rowcolor{baselinecolor}
Clean & 55.84  & 53.78  & 1299.26  & 288.93  & 83.41  & 62.80  & 30.05  & 62.80  & 63.50  & 72.98  & 48.26  & 35.62   \\ 
None (CE) & 43.71  & 37.40  & 792.42  & 243.93  & 32.06  & 59.00  & 19.54  & 56.62  & 62.86  & 71.64  & 42.07  & 29.56   \\ \midrule
\multicolumn{12}{l}{\textit{Noise-robust loss functions}} \\
GCE & \underline{51.11}  & 44.26  & \underline{1203.31}  & 249.64  & 76.49  & 58.30  & \underline{28.26}  & 56.62  & 56.55  & 68.22  & 45.40  & \underline{36.79}   \\ 
% Unhinged & 47.84  & \underline{45.22}  & 1025.60  & 286.79  & 75.51  & 51.40  & \textbf{29.45}  & 56.10  & 55.69  & 69.96  & 44.77  & 11.01   \\ 
Phuber CE & 44.00  & 37.25  & 795.80  & 234.64  & 21.00  & 60.00  & 24.95  & 60.22  & 63.10  & 72.04  & \underline{45.50}  & 30.77   \\ \midrule
\multicolumn{12}{l}{\textit{Sample selection (online)}} \\
MentorNet & 47.24  & 37.75  & 944.09  & 249.29  & 61.47  & 57.00  & 24.04  & 56.53  & 56.71  & \underline{73.28}  & 42.62  & 31.92   \\ 
Co-teaching & 47.70  & 37.32  & 805.62  & 252.50  & 58.10  & \underline{60.40}  & 27.71  & 59.79  & 60.60  & \textbf{73.82}  & 42.28  & 32.85   \\ 
JoCor & 45.84  & 37.46  & 658.20  & 221.43  & 57.53  & 57.50  & 24.68  & 58.59  & 61.04  & 72.04  & 42.67  & 32.10   \\ 
\midrule
\multicolumn{12}{l}{\textit{Sample selection (post-training)}} \\
EL2N & 51.06  & 42.49  & 1060.59  & 265.36  & 74.88  & 57.70  & 23.72  & 63.83  & \underline{63.16}  & 72.83  & 43.11  & 33.75   \\ 
GradNorm & 49.49  & 43.60  & 909.99  & \textbf{303.93}  & \underline{80.27}  & 55.00  & 24.86  & 56.44  & 61.90  & 70.90  & 42.95  & 24.96   \\ 
Entropy & 48.81  & 42.11  & 996.08  & 266.43  & 78.76  & 55.50  & 20.37  & 54.30  & 59.33  & 70.05  & 42.98  & 30.45   \\ 
PPL & 46.87  & 40.17  & 1126.94  & 231.79  & 35.05  & 57.60  & 24.86  & \textbf{65.64}  & 63.11  & 72.63  & 41.77  & 29.37   \\ 
\rowcolor{backcolor}
Val\_PPL & \textbf{55.70}  & \textbf{52.03}  & \textbf{1254.83}  & \underline{287.14}  & \textbf{82.66}  & \textbf{60.60} & 27.11  & \underline{65.29}  & \textbf{63.51}  & 72.63  & \textbf{48.21}  & \textbf{42.07}   \\ 
\bottomrule
        
\end{tabular}%
}
\caption{\textbf{Comparisons of different corruption-robust strategies on Qwen-2.5-3B.} Here, \textbf{Avg.} refers to the average performance on 11 benchmarks (normalized to 0-100). Best results are \textbf{Bold}, second best are \underline{underlined}.}
\label{tab:main_qwen_3b}
\end{table*}

%%% gce:0.9, phuber: 10, jocor:07, co-teaching: 01, self-teaching: 03
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
Methods & \textbf{Avg.} & GQA & MME\_P & MME\_C & POPE & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}LLaVA\\ Wild\end{tabular} & MM-Vet & MMB & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}SEED\\ IMG\end{tabular}  & \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}SciQA\\ IMG\end{tabular}  &  \setlength\extrarowheight{0pt}\begin{tabular}[c]{@{}c@{}}Text\\ VQA\end{tabular} & OKVQA \\ \midrule


\rowcolor{baselinecolor}
Clean & 59.75  & 56.94  & 1479.14  & 292.50  & 84.76  & 65.00  & 34.08  & 70.19  & 66.16  & 76.30  & 52.71  & 40.57   \\ 
None (CE) & 45.56  & 38.65  & 765.30  & 273.57  & 26.31  & 55.20  & 23.49  & 64.78  & \underline{65.64}  & 73.72  & 46.25  & 34.62   \\ \midrule
\multicolumn{12}{l}{\textit{Noise-robust loss functions}} \\
GCE & 51.34  & 43.66  & 1129.36  & 291.43  & 68.83  & 57.70  & 27.80  & 57.73  & 57.15  & 72.24  & \underline{49.38}  & 37.34   \\ 
% Unhinged & 47.59  & 41.25  & 695.05  & 216.43  & 68.75  & 49.10  & 24.59  & 59.62  & 55.09  & 73.82  & 47.31  & \underline{42.14}   \\ 
Phuber CE & 47.32  & 39.73  & 848.28  & 269.64  & 31.47  & \underline{60.10}  & \underline{28.35}  & 64.78  & 64.46  & 73.57  & 49.04  & 32.93   \\  \midrule
\multicolumn{12}{l}{\textit{Sample selection (online)}} \\
MentorNet & 48.22  & 40.98  & 1055.65  & 280.36  & 52.27  & 54.30  & 26.88  & 55.33  & 57.10  & 74.02  & 45.81  & 35.92   \\ 
Co-teaching & 47.23  & 38.44  & 668.22  & 235.00  & 60.50  & 54.30  & 23.44  & 65.03  & 61.66  & 75.06  & 45.97  & 32.31   \\ 
JoCor & 46.73  & 38.56  & 633.31  & 259.64  & 51.35  & 53.10  & 23.12  & 63.06  & 61.34  & 74.67  & 47.13  & 37.63   \\  \midrule
\multicolumn{12}{l}{\textit{Sample selection (post-training)}} \\
EL2N & 50.52  & 43.96  & 1046.46  & 261.43  & 52.98  & 57.30  & 26.42  & 64.95  & 64.81  & 74.62  & 47.11  & 38.62   \\ 
GradNorm & \underline{54.69}  & \underline{47.92}  & 1190.76  & 303.21  & \textbf{84.32}  & 56.20  & 26.65  & 65.98  & 64.91  & 75.36  & 48.60  & 34.20   \\ 
Entropy & 49.71  & 43.82  & 1159.33  & \underline{305.00}  & 42.42  & 57.20  & 27.11  & 61.86  & 63.09  & 72.43  & 45.07  & 37.72   \\ 
PPL & 54.68  & 45.52  & \underline{1305.64}  & 296.79  & 77.34  & 53.50  & 27.48  & \textbf{68.64}  & 64.91  & \underline{75.56}  & 47.68  & 38.45   \\ 
\rowcolor{backcolor}
Val\_PPL & \textbf{59.15}  & \textbf{52.02}  & \textbf{1417.26}  & \textbf{307.14}  & \underline{83.19}  & \textbf{66.50}  & \textbf{32.61}  & \underline{68.38}  & \textbf{66.22}  & \textbf{76.10}  & \textbf{52.02}  & \textbf{44.29}   \\ 
\bottomrule
\end{tabular}%
}
\caption{\textbf{Comparisons of different corruption-robust strategies on Qwen-2.5-7B.} Here, \textbf{Avg.} refers to the average performance on 11 benchmarks (normalized to 0-100). Best results are \textbf{Bold}, second best are \underline{underlined}.}
\label{tab:main_qwen_7b}
\end{table*}

%%% gce:0.9, phuber: 15, jocor:05, co-teaching: 07, self-teaching: 05

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/ablate_more.pdf}
\vspace{-.3in}
\caption{\textbf{Average model performance of models post-trained (or fine-tuned from scratch) on data from data from different sources}. Post-trained models are fine-tuned on dataset with various corruption levels (10\%-40\%) at first. Results with 50\% corruption are in Figure~\ref{fig:ablate}.}
\label{fig:ablate_more}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[!ht]
    \centering
    % \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccc}
        \toprule
        \multirow{2}{*}{Models} & \multicolumn{2}{c}{EL2N} &  \multicolumn{2}{c}{GradNorm} &  \multicolumn{2}{c}{Entropy}    \\ 
        & $\downarrow$  & $\uparrow$ & $\downarrow$ & $\uparrow$ & $\downarrow$ & $\uparrow$  \\ \midrule
        Qwen-2.5-0.5B & \textbf{36.88}  & 28.11  & \textbf{47.07}  & 40.32  & \textbf{30.18}  & 31.67   \\ 
        Qwen-2.5-3B & \textbf{51.06}  & 30.41  & \textbf{49.49}  & 34.09  & \textbf{48.81}  & 38.01   \\ 
        Qwen-2.5-7B & \textbf{50.52}  & 23.14  & \textbf{54.69}  & 38.52  & \textbf{49.71}  & 39.98   \\ 
        LLaMA-3.1-8B & \textbf{47.07}  & 23.32  & \textbf{55.77} & 39.81  & \textbf{48.60}  & 30.73   \\  \bottomrule
    \end{tabular}
    % }
    \caption{\textbf{Post-training on the bottom ($\downarrow$) and top ($\uparrow$) 30\% subsets based on EL2N, GradNorm, and Entropy scores.} The reported results represent the average performance. All models are initially fine-tuned on data with a corruption ratio of $cr=50\%$.}
    \label{tab:ablation_lower_higher}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/hyper.pdf}
\vspace{-.3in}
\caption{\textbf{Ablations on the choices of hyper-parameters for noise-robust loss functions and sample selection methods.} Average performance are reported. All experiments are conducted on datasets with $cr=50\%$ and best results are indicated by red dashed lines and reported in Tables \ref{tab:main_llama3}, \ref{tab:main_qwen_0.5b}, \ref{tab:main_qwen_3b} and \ref{tab:main_qwen_7b}}
\label{fig:hyper}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Extended Related Work on MLLMs} MLLMs integrate the vision modality into LLMs~\citep{touvron2023llama,chen2023gaining}, enabling the advanced understanding and reasoning over visual instructions~\citep{liu2023llava,bai2023qwen,gou2023mixture,gou2024eyes,chen2024emova}.
Recent VLLM works can be categorized into three directions, 1) \textit{Vision encoders}~\citep{oquab2023dinov2,chen2021multisiam,chen2023mixed} are enhanced and aggregated for robust representations~\citep{lin2023sphinx,li2024mini,tong2024cambrian}.
2) \textit{High-resolution} methods are proposed to overcome the fixed resolution of pre-trained vision encoders 
(e.g., $336 \times 336$ for CLIP~\citep{radford2021learningtransferablevisualmodels}), enabling LLMs to perceive fine-grained visual information~\citep{liu2024llavanext,dong2024xcomposer2-4khd,huang2024hires,luo2024feast}.
3) \textit{High-quality instruction data} is essential for VLLMs to generate accurate and well-formed responses~\citep{deitke2024molmo,chen2024expanding,li2025eagle}.
This paper is related to the third directions. However, rather than constructing high-quality data, we study study the effect of corrupted data on MLLMs and its mitigation. 
\end{document}


