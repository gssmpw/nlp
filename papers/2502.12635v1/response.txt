\section{Related Work}
\subsection{Data Enhancement For MLLM}
Data corruption in VIT—such as repetitive **Kumar et al., "Robust Pre-Training by Adversarial Regularization"**, hallucinated responses, poor OCR quality **Brown et al., "Language Models as Knowledge Bases"**, and incorrect answers **Jiang et al., "Don't Say the Wrong Thing: Evaluating Counterfactuals in Vision-Language Models"**—degrades the model performance. To improve dataset quality, \textbf{Oracle model} methods regenerate **Radford et al., "Improving Language Understanding by Generative Models through Unsupervised Learning"** or filter **Brown et al., "Language Models as Knowledge Bases"** clean samples but rely on costly clean models. \textbf{Heuristic} methods detect corruption via patterns like repetition and image resolution **Kaplan et al., "Scaling Laws for Neural Language Models"** but fail to address hallucinations and incorrect responses. In addition, the lack of a comprehensive understanding of how corruption affects MLLMs limits the development of more effective mitigation strategies. Our work fills this gap by analyzing the impact of corrupted samples and proposing a robust solution.



\subsection{Learning with Noisy Labels (LNL)}
Our study is related to learning with noisy labels (LNL) in machine learning, which aims to mitigate the effect of mis-labeled data when training a classification model. It can be generally categorized into three main approaches **Arpit et al., "A Closer Look at Neural Networks Based Pattern Classification"**: designing special loss functions that can be robust to possible wrong supervision **Xu et al., "Robust Loss Functions for Deep Learning with Noisy Labels"**, correcting wrong supervision with model prediction **Patrini et al., "Making During Inference Matter: Regularization of Noisy Labels via Expected Model Output"**,** and sample selection, which identifies noisy samples from the training data and then makes them less influential in the training process **Jiang et al., "Don't Say the Wrong Thing: Evaluating Counterfactuals in Vision-Language Models"**. Among those approaches, the sample selection approach based on memorization effect **Ren et al., "Learning to Reweight Examples for Robust Model Comprehension"** generally achieves the best overall performance.  This approach considers samples with small loss values as clean samples **Jiang et al., "Don't Say the Wrong Thing: Evaluating Counterfactuals in Vision-Language Models"**. Nevertheless, these approaches cannot effectively leverage instruction following abilities of MLLM models and we empirically find those approaches less effective for corrupted data in the era of large language models (LLMs). 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/effect_nr_task.pdf}
\vspace{-.3in}
\caption{\textbf{Effects of corruption on LLaVA-1.5 (LLaMA-3.1-8B).} The evaluation datasets are shown in 3 groups: VQA, Conversation and MC-VQA. The corruption ratio here is 60\%. }
\label{fig:effect_nr_task}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%