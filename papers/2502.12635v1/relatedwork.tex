\section{Related Work}
\subsection{Data Enhancement For MLLM}
Data corruption in VIT—such as repetitive~\citep{chen2024expanding,li2025eagle}, hallucinated responses, poor OCR quality~\citep{wu2024deepseek}, and incorrect answers~\citep{dubey2024llama}—degrades the model performance. To improve dataset quality, \textbf{Oracle model} methods regenerate~\citep{chen2023sharegpt4v,chen2024allava} or filter~\citep{fu2025tldr,xiong2024llava} clean samples but rely on costly clean models. \textbf{Heuristic} methods detect corruption via patterns like repetition and image resolution~\citep{chen2024expanding,li2025eagle} but fail to address hallucinations and incorrect responses. In addition, the lack of a comprehensive understanding of how corruption affects MLLMs limits the development of more effective mitigation strategies. Our work fills this gap by analyzing the impact of corrupted samples and proposing a robust solution.



\subsection{Learning with Noisy Labels (LNL)}
Our study is related to learning with noisy labels (LNL) in machine learning, which aims to mitigate the effect of mis-labeled data when training a classification model. It can be generally categorized into three main approaches~\citep{han2020survey}:
designing special loss functions that can be robust to possible wrong supervision~\cite{ghosh2017robust,zhang2018generalized,menon2019can}, 
correcting wrong supervision with model prediction~\citep{tanaka2018joint,yi2019probabilistic,zhang2020distilling},
and sample selection, which identifies noisy samples from the training data and then makes them less influential in the training process~\cite{jiang2018mentornet, han2018co,wei2020combating}. 
Among those approaches, the sample selection approach based on memorization effect~\cite{arpit2017closer,zhang2016understanding} generally achieves the best overall performance.  
This approach considers samples with small loss values as clean samples~\citep{han2018co,jiang2018mentornet,yao2020searching,10509799}. 
Nevertheless, these approaches cannot effectively leverage instruction following abilities of MLLM models and we empirically find those approaches less effective for corrupted data in the era of large language models (LLMs). 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!]
\centering
\includegraphics[width=\linewidth]{figs/effect_nr_task.pdf}
\vspace{-.3in}
\caption{\textbf{Effects of corruption on LLaVA-1.5 (LLaMA-3.1-8B).} The evaluation datasets are shown in 3 groups: VQA, Conversation and MC-VQA. The corruption ratio here is 60\%. }
\label{fig:effect_nr_task}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%