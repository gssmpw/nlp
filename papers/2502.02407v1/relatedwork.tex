\section{Related work}
\textbf{Improved variants of \SAM in Vision.} An extensive line of work has attempted to propose better definitions of sharpness --- particularly those which are less sensitive to
details of parameterization
\citep{kwon2021asam,tahmasebi2024universalclasssharpnessawareminimization,li2024enhancing}. Some of these methods 
have shown small improvements on vision tasks. \textit{We believe our decomposition approach is orthogonal to this line of research.} Therefore, the obtained \funcSAM algorithm is substantially different from  prior work.



\textbf{Studies exploring preconditioning for \SAM.} Other work has also suggested that the perturbation step in \SAM should be taken in an alternative geometry. Our approach to preconditioning is most
similar to Fisher \SAM~\citep{kim2022fisher}, and the concurrent work of~\citet{zhang2025preconditioned} which describes a more
general preconditioning scheme for \SAM. Our key insight is that it is useful to take the \SAM perturbation in the \emph{exact same geometry}
used by the optimizer, which can be accomplished for negligible cost in the case of Adam and its variants.
Furthermore, our work is \textit{primarily driven by the problem of making \SAM work in language modeling}, which is far from the focus of these other works.\looseness=-1

\textbf{Role of the indefinite Hessian term.} At a more conceptual level, our study aligns with recent works~\citep{singh2021analytic,dauphin2024neglected} which underscore paying more importance to the functional Hessian, the understudied indefinite term of the Hessian in the Gauss-Newton decomposition, as opposed to focusing solely on the positive semi-definite GGN term as suggested by prior studies~\citep{sagun2018empiricalanalysishessianoverparametrized,papyan2019spectrumdeepnethessiansscale,jacot2020asymptoticspectrumhessiandnn}. The decomposition of the sharpness gradient into logit and functional modes, along with the demonstrated significance of \funcSAM in NLP tasks (which is closely tied to the functional Hessian),\textit{ highlights the risks of over-reliance on the GGN and the corresponding outlier spectrum of the Hessian} --- 
especially in the context of \emph{regularization} of sharpness.

\textbf{\SAM in NLP.} Prior works building on \SAM in NLP have been restricted to the fine-tuning setting~\citep{bahri2022sharpnessawareminimizationimproveslanguage}, domain transfer~\citep{sherborne2024trambridgingtrustregions} or on small-scale machine translation setups~\citep{li2024enhancing}.
To the best of our knowledge, \SAM has hitherto not been successfully applied to language modeling, particularly in any scaling
setting.