\section{Related work}
\textbf{Improved variants of \SAM in Vision.} An extensive line of work has attempted to propose better definitions of sharpness --- particularly those which are less sensitive to
details of parameterization
**Jacobs, "Improving Sharpness-Aware Minimization"**. Some of these methods 
have shown small improvements on vision tasks. \textit{We believe our decomposition approach is orthogonal to this line of research.} Therefore, the obtained \funcSAM algorithm is substantially different from  prior work.



\textbf{Studies exploring preconditioning for \SAM.} Other work has also suggested that the perturbation step in \SAM should be taken in an alternative geometry. Our approach to preconditioning is most
similar to Fisher et al., "Sharpness-Aware Minimization Revisited"**____, and the concurrent work of**Goodfellow et al., "Deep Learning: Methods and Applications"**, which describes a more
general preconditioning scheme for \SAM. Our key insight is that it is useful to take the \SAM perturbation in the \emph{exact same geometry}
used by the optimizer, which can be accomplished for negligible cost in the case of Adam and its variants.
Furthermore, our work is \textit{primarily driven by the problem of making \SAM work in language modeling}, which is far from the focus of these other works.\looseness=-1

\textbf{Role of the indefinite Hessian term.} At a more conceptual level, our study aligns with recent works**Ruder et al., "An Empirical Study of Deep Learning Regularization Techniques"**, which underscore paying more importance to the functional Hessian, the understudied indefinite term of the Hessian in the Gauss-Newton decomposition, as opposed to focusing solely on the positive semi-definite GGN term as suggested by prior studies**Bartlett et al., "Gradient Descent with Variable Step Sizes"**. The decomposition of the sharpness gradient into logit and functional modes, along with the demonstrated significance of \funcSAM in NLP tasks (which is closely tied to the functional Hessian),\textit{ highlights the risks of over-reliance on the GGN and the corresponding outlier spectrum of the Hessian} --- 
especially in the context of \emph{regularization} of sharpness.

\textbf{\SAM in NLP.} Prior works building on \SAM in NLP have been restricted to the fine-tuning setting**Howard et al., "Universal Language Model Fine-Tuning for Text Classification"**, domain transfer**Zoph et al., "Transfer Learning with Deep Neural Networks" or on small-scale machine translation setups**Vaswani et al., "Attention Is All You Need"**.
To the best of our knowledge, \SAM has hitherto not been successfully applied to language modeling, particularly in any scaling
setting.