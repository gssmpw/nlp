% This class has a lot of options, so please check deepmind.cls for more details.
% This is a minimal set for most needs.
\documentclass[11pt, a4paper, logo, twocolumn, copyright,]{googledeepmind}

% Omit dates for reproducibility.
\pdfinfoomitdate 1
\pdftrailerid{redacted}

\makeatletter
\renewcommand\bibentry[1]{\nocite{#1}{\frenchspacing\@nameuse{BR@r@#1\@extra@b@citeb}}}
\makeatother

\usepackage{kantlipsum, lipsum}
\usepackage{dsfont}
\usepackage{gdm-colors}


\usepackage[authoryear, sort&compress, round]{natbib}

\graphicspath{{figures/}}

\title{Avoiding spurious sharpness minimization broadens applicability of \SAM}

\correspondingauthor{contact@sidakpal.com, hmobahi@google.com, thetish@google.com, ynd@google.com}

\keywords{Sharpness Aware Minimization, Hessian, Generalization, LLMs}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage{subcaption}

\usepackage{multirow}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\input{dimensions}
\input{math_defs}
\usepackage{makecell}

\newcommand{\rad}{\rho} % \SAM radius
\renewcommand{\th}{\m{\theta}}
\newcommand{\lr}{\eta}
\newcommand{\eps}{\m{\epsilon}}
\newcommand{\J}{\m{J}}
% \newcommand{\z}{\m{z}}
\newcommand{\dz}{\nabla_{\z}\Loss}
\newcommand{\g}{\m{g}}
\renewcommand{\H}{\m{H}}
\newcommand{\G}{\m{G}}
\newcommand{\N}{\m{N}}
\newcommand{\logitText}{\textbf{\texttt{logit}}\xspace}
\newcommand{\funcText}{\textbf{\texttt{func}}\xspace}
\newcommand{\crossText}{\textbf{\texttt{cross}}\xspace}
\newcommand{\glog}{\mathbf{\delta}_{\logitText}} % \g
\newcommand{\gfun}{\mathbf{\delta}_{\funcText}}
\newcommand{\tlog}{\tau_{\logitText}}
\newcommand{\tfn}{\tau_{\funcText}}
\newcommand{\tc}{\tau_{\crossText}}
\newcommand{\SP}{\textbf{\texttt{SP}}\xspace}


%%%%%%%%%%%% for the code

\usepackage{listings}
% \usepackage[scr=false]{rsfso} % Prevents defining \mathscr
%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
%   numbers=left,                    
%   numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%%%%%%%%%%%%%%%%

\newif\ifcomments
% \commentstrue % uncomment to turn all colored comments on
\commentsfalse % uncomment to turn all colored comments off



\ifcomments
\newcommand{\ynd}[1]{{\color{blue}[YD: #1]}}
\newcommand{\hmb}[1]{{\color{purple}[HM: #1]}}
\newcommand{\aga}[1]{{\color{red}[AA: #1]}}
\newcommand{\sps}[1]{{\color{olive}[SPS: #1]}}
\else
\newcommand{\ynd}[1]{}
\newcommand{\hmb}[1]{}
\newcommand{\aga}[1]{}
\newcommand{\sps}[1]{}
\fi
\newcommand{\sidak}[1]{\todo[color=orange!20,size=\footnotesize,caption={}]{Sidak: #1}{}}
\usepackage[dvipsnames]{xcolor}


% \usepackage{tcolorbox}
\usepackage[most]{tcolorbox}
\usepackage{fontawesome} % for the Light bulb icon
\usepackage[]{enumitem}

\reportnumber{} % Leave blank if n/a

% Assign your own date to the report.
% Can comment out if not needed or leave blank if n/a.
% \renewcommand{\today}{2000-01-01}


\author[*,1]{Sidak Pal Singh}
\author[2]{Hossein Mobahi}
\author[2]{Atish Agarwala}
\author[2]{Yann Dauphin}

% Affiliations *must* come after the declaration of \author[]
\affil[*]{Work done during internship at Google DeepMind}
\affil[1]{ETH Zürich}
\affil[2]{Google DeepMind}


\begin{abstract}
Curvature regularization techniques like Sharpness Aware Minimization (\SAM) have shown great promise in improving generalization on vision tasks. However, we find that \SAM performs poorly in domains like natural language processing (NLP), often degrading performance --- even with twice the compute budget. We investigate the discrepancy across domains and find that \textit{in the NLP setting, \SAM is dominated by regularization of the logit statistics --— instead of improving the geometry of the function itself}. We use this observation to develop an alternative algorithm we call \funcSAM , which regularizes curvature only through modification of the statistics of the overall function implemented by the neural network, and avoids spurious minimization through logit manipulation. Furthermore, we argue that preconditioning the \SAM perturbation also prevents spurious minimization, and when combined with \funcSAM, it gives further improvements. Our proposed algorithms show improved performance over \adamw and \SAM baselines when trained for an equal number of steps, in both fixed-length and Chinchilla-style training settings, at various model scales (including \textit{billion-parameter scale}). On the whole, our work highlights the importance of more precise characterizations of sharpness in broadening the applicability of curvature regularization to large language models (LLMs).\looseness=-1\vspace{-0.5em}
\end{abstract}

\begin{document}

\maketitle

% Incude paper content from external files
\input{template_content}

% Bibliography components
\bibliographystyle{abbrvnat}
\nobibliography*
\bibliography{template_refs}



\appendix
\onecolumn

\section*{Supplementary Materials}
\section{Additional Results}

\label{app:sharp_plots}

\begin{figure}[ht!]
\centering
\begin{tabular}{cc}
\includegraphics[height=0.33\linewidth]{figures/vit_imagenet1k_contributions.pdf} & \includegraphics[height=0.33\linewidth]{figures/vit_imagenet21k_contributions.pdf} \\
\includegraphics[height=0.33\linewidth]{figures/vit_jft_contributions.pdf} &
\includegraphics[height=0.33\linewidth]{figures/nanodo_c4_contributions.pdf} \\
\includegraphics[height=0.33\linewidth]{figures/nanodo_c4_big_contributions.pdf}   
% \includegraphics[height=0.25\linewidth]{figures/nanodo_c4_billion_param_model_contributions.pdf}
\end{tabular}
\caption{Sharpness contributions $\lcolor{\tlog}$, $\fcolor{\tfn}$ and $\tc$ for various datasets. $\tc$ tends to be negative for most of training.\looseness=-1}
\label{fig:sam_contribution_app}
\end{figure}


% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=.5\textwidth]{figures/vit_imagenet21k_contributions.pdf}}
% \caption{ViT, ImageNet21K.}
% \label{fig:vit_i21k_imagnet_contributions}
% \end{center}
% \vskip -0.2in
% \end{figure}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[ht!]
\caption{Comparison of \SAM variants across non-linearities for a Nanodo model with 2M (non-embedding parameters) on C4 dataset.}
\label{tab:non-linearities}
\centering
\begin{tabular}{@{}llc@{}}
\toprule
\textsc{Non-Linearity} & \textsc{Method}                                                        & \textsc{Eval Loss} \\ \midrule
GeLU          & \begin{tabular}[c]{@{}l@{}}\precond\\\funcSAM \end{tabular}                                                & 3.8614    \\[4mm]
GeLU          & \begin{tabular}[c]{@{}l@{}}\precond\\\SAM \end{tabular} & 3.8894    \\[4mm]
GeLU          & \adamw                                                         & 3.9069    \\[2mm] \midrule
ReLU          & \begin{tabular}[c]{@{}l@{}}\precond\\\funcSAM\end{tabular}                                                & 3.8777    \\[4mm]
ReLU          & \begin{tabular}[c]{@{}l@{}}\precond\\\SAM\end{tabular}  & 3.8937    \\[4mm]
ReLU          & \adamw                                                         & 3.9145    \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Comparison of different methods based on Hessian ${\HL}$ and GGN ${\HG}$ metrics for the $117.9$M model trained as per Chinchilla like training setup.}
    % \vspace{1em}
    \renewcommand{\arraystretch}{1.2} % Adjust row spacing
\setlength{\tabcolsep}{4pt} % Reduce column spacing
    \begin{tabular}{lcccc}
        \toprule
        \textsc{Method} & \textsc{Eval Loss} & $\lambda_{\text{max}}({\HL})$ & $\mathrm{tr}({\HL})$ & $\mathrm{tr}({\HG})$ \\
        \midrule
        \makecell[l]{\precond\\ \funcSAMbrief} & \textbf{3.096} & 8.884 & \textit{2790.650} & 2746.559 \\[4mm]
        \makecell[l]{\precond \\ \SAM}  & 3.108 & \textit{7.415} & 2920.445 & \textit{2060.214} \\[4mm]
        \SAM& 3.126 & \textbf{3.381} & \textbf{2235.759} & \textbf{2222.779} \\[2mm]
        \adamw& 3.120 & 9.259 & 3299.497 & 3285.881 \\
        \bottomrule
    \end{tabular}
    \label{tab:hessian_comparison_117.9M}
\end{table}

\begin{figure*}[h]
\centering
\begin{tabular}{ccc}
\includegraphics[height=0.25\linewidth]{figures/pruning_20.png} & \includegraphics[height=0.25\linewidth]{figures/pruning_30-90.png}
\end{tabular}
\caption{\textit{Effect of one-shot (unstructured) global magnitude pruning:} We see that sharpness minimization methods tend to degrade more gracefully as increasing number of parameters are pruned. Also, from this figure we can see that the performance gained imparted by \funcSAM over \adamw is equivalent to setting about $25\%$ parameters of zero, and is thus significant.}
\label{fig:pruning}
\end{figure*}

\vspace{-1em}
\subsection{Architectural Details.}\label{app:archi}
The $23.9$M, $42.5$M, $117.9$M, and $1208$M models have the same depth of $6$, and whose width has been scaled together with the number of heads. In particular, these correspond to $h=9$, $12$, $20$, and $64$ heads per block and the width $m$ scales as $m=64\times h$, and the MLP dimension is $f=4\times m$. The $2$M model used for prototyping has depth $3$, $4$ heads per block, width $m=256$ and MLP dimension $f=1024$.\looseness=-1
\clearpage

\section{Additional theory}

\subsection{Argument for reducing matrix-vector products with inverse preconditioning}

\label{app:mvp-precond-argument}

We will use a random matrix model to reason about the effects of preconditioning with a matrix inverse. Random matrices have been used
to model the spectra of the Hessian of the large models found in machine learning, and in particular treating the Gauss-Newton and the functional Hessian/NME as
independent yields quantitative insights about the structure of the overall Hessian \citep{pennington17rmt}.


Consider two $N\times N$ symmetric
invertible matrices $\m{A}$ and $\m{B}$.
Suppose $\m{A}$ and $\m{B}$ are random and freely independent.
Free independence is the non-commutative analog to classical independence, implied by classical independence of entries in the limit of large matrix size \citep{pennington17rmt}.
The key feature it induces is
\begin{equation}
\mathbb{E}[\tr[\m{A}^{j}\m{B}^{k}]] = \mathbb{E}[\tr[\m{A}^{j}]]\mathbb{E}[\tr[\m{B}^{k}]]
\end{equation}
for any $j$, $k$, where $\tr$ is the trace.

Given a random unit vector $\m{v}$, the expected squared lengths of the matrix vector products with $\m{A}$ and $\m{B}$ are given by 
\begin{equation}
\mathbb{E}[\|\m{A}\m{v}\|^{2}] = \frac{1}{N}\mathbb{E}[\tr[\m{A}^{2}]],~\mathbb{E}[\|\m{B}\m{v}\|^{2}] = \frac{1}{N}\mathbb{E}[\tr[\m{B}^{2}]]
\end{equation}
The ratio $r_{1}$ of magnitudes is given by
\begin{equation}
r_{1} \equiv \frac{\mathbb{E}[\|\m{B}\m{v}\|^{2}]}{\mathbb{E}[\|\m{A}\m{v}\|^{2}]} = \frac{\mathbb{E}[\tr[\m{B}^{2}]]}{\mathbb{E}[\tr[\m{A}^{2}]]}
\end{equation}

Now consider the norm of $\m{w} = \m{A}^{-1}\m{v}$ passed through each matrix:
\begin{equation}
\mathbb{E}[\|\m{A}\m{w}\|^{2}] = \frac{1}{N},~\mathbb{E}[\|\m{B}\m{w}\|^{2}] = \frac{1}{N}\mathbb{E}[\tr[\m{A}^{-1}\m{B}^{2}\m{A}^{-1}]]
\end{equation}
The new ratio of magnitudes $r_{2}$ is given by
\begin{equation}
r_{2} \equiv \frac{\mathbb{E}[\|\m{B}\m{w}\|^{2}]}{\mathbb{E}[\|\m{A}\m{w}\|^{2}]} = \mathbb{E}[\tr[\m{A}^{-1}\m{B}^{2}\m{A}^{-1}]] = \frac{\mathbb{E}[\tr[\m{B}^{2}]]}{\mathbb{E}[\tr[\m{A}^{-2}]]^{-1}}
\end{equation}
From Jensen's inequality, $\tr[\m{A}^{2}]\geq \tr[\m{A}^{-2}]^{-1}$. Therefore $r_{2}>r_{1}$; preconditioning by the inverse of $\m{A}$
causes matrix-vector products to upweigh products with $\m{B}$ relative to products with $\m{A}$, relative to the unpreconditioned product.

In neural network settings, there are non-trivial relationships between the gradient, the Gauss-Newton matrix $\HG$ and the functional Hessian $\HF$; however,
the eigenvectors and eigenvalues of $\HG$ and $\HF$ are only weakly related. Therefore even though the exact calculations in the example
above don't hold, we suspect that generically preconditioning by $\HG^{-1}$ will downweigh $\HG\eps^{*}$ compared to $\HF\eps^{*}$.


\clearpage


\section{Code Snippets for \SAM and \funcSAM}\label{app:code}
\input{code}

\clearpage
\section{\angleSAM}\label{app:angle-sam}

In this section, we present a general variant of \SAM, which includes both \funcSAM and \SAM as its particular instantiations.  The core idea is that once we have been able to decompose the sharpness gradient into those arising from logit and functional paths, we can design our bespoke or custom versions of \SAM which lean more or less towards one path than another. 

To do so, let's assume that out custom path is at an angle $\phi$ with the functional path. We weigh the functional path by a factor of $\cos(\phi)$, while we weigh the logit path by $\sin(\phi)$. Then the gradient update in \angleSAM can be described as:

\begin{align}
     &\m{g}_{\, \angleSAM} := \nabla_\btheta \Loss(\btheta) + \sin(\phi) \,\cdot \, \m{g}_\logitText + \cos(\phi) \, \cdot \,\m{g}_\funcText \\
     & = \nabla_\btheta\Loss(\btheta) + \sin(\phi) \,\cdot \,\left[ \nabla_\btheta \boldsymbol{F}(\btheta)\cdot  \nabla_{\boldsymbol{F}}\Loss(\btheta+ \rad \, \beps^\ast)- \nabla_\btheta\Loss(\btheta)\right] + \cos(\phi) \,\cdot \, \left[\nabla_\btheta \boldsymbol{F}(\btheta+\rad\,\beps^\ast) \cdot  \nabla_{\boldsymbol{F}}\Loss(\btheta) - \nabla_\btheta\Loss(\btheta)\right]\\
     & = \nabla_\btheta\Loss(\btheta) + \rho \sin(\phi) \,\cdot \, \lcolor{\HG}\,\cdot\, \beps^\ast + \rho \cos(\phi) \,\cdot \, \fcolor{\HF}\,\cdot\, \beps^\ast  + \mathcal{O}(\rho^2)
\end{align}

We see that $\phi=\frac{\pi}{4}$ recovers \SAM upto first order in $\rho$, while $\phi=0$ would yield \funcSAM and $\phi=\frac{\pi}{2}$ would result in a variant that optimizes solely along the logit path and which can thus be called \logitSAM. \textit{All in all, this shows how \angleSAM is a clean generalization of \SAM, that equips it with a perturbation angle in addition to the usual perturbation radius $\rho$.}

At some level, this above formulation could be viewed as making an assumption that these two paths are orthogonal\footnote{This is not far-fetched.
As we noted in our experiments, these two paths tend to be anti-correlated, but often the correlation is quite small in magnitude and and approaches zero.}. On another level, one can simply think of these weights as a mere strategy to obtain convenient weight settings that have their sum of squares as unity.   

\textit{We expect that this approach might pay dividends in different model-dataset-optimizer triples, and we expect this to be an interesting direction for future work.}



\end{document}
