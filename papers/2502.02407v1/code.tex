
%"mystyle" code listing set
\lstset{style=mystyle}
\begin{lstlisting}[language=Python, caption={Illustration of how to get the gradients in the two methods. Functional SAM differs from the SAM implementation only in the last couple lines, where the effect of perturbation is made to reside only in the function Jacobian part.}]
from jax import grad, vjp
from jax.tree_util import tree_map
from utils import normalize_grad

def sam_gradients(params, loss_fn, rho):

    # compute the usual loss gradient
    dL_dtheta = grad(loss_fn)(params)
    
    # normalize the gradients
    dL_dtheta = normalize_grad(dL_dtheta)
    
    # perturb the parameters
    perturbed_params = tree_map(lambda a, b: a + rho * b, params, dL_dtheta)
    
    # compute the gradient as by SAM
    sam_grad = grad(loss_fn)(perturbed_params)
    
    return sam_grad
    

def functional_sam_gradients(params, loss_fn, network_fn, rho):
    
    # compute the usual loss gradient, but also extract dL_dlogits 
    (dL_dlogits), dL_dtheta = grad(loss_fn, hax_aux=True)(params)
    
    # normalize the gradients
    dL_dtheta = normalize_grad(dL_dtheta)
    
    # perturb the parameters
    perturbed_params = tree_map(lambda a, b: a + rho * b, params, dL_dtheta)
    
    # set up the VJP at the perturbed parameters
    _, dF_dtheta_fn = vjp(lambda theta: network_fn(theta), perturbed_params)
    
    # do the VJP with the (unperturbed) dL_dlogits
    functional_sam_grad = dF_dtheta_fn(dL_dlogits)[0]
    
    return functional_sam_grad
    
    
\end{lstlisting}
