@inproceedings{
singh2021analytic,
title={Analytic Insights into Structure and Rank of Neural Network Hessian Maps},
author={Sidak Pal Singh and Gregor Bachmann and Thomas Hofmann},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=otDgw7LM7Nn}
}
@inproceedings{
zhao2024theoretical,
title={Theoretical Characterisation of the Gauss Newton Conditioning in Neural Networks},
author={Jim Zhao and Sidak Pal Singh and Aurelien Lucchi},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=fpOnUMjLiO}
}
@article{Schraudolph2002FastCM,
  title={Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent},
  author={Nicol N. Schraudolph},
  journal={Neural Computation},
  year={2002},
  volume={14},
  pages={1723-1738}
}
@article{beaglehole2024gradient,
  title={Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks},
  author={Beaglehole, Daniel and Mitliagkas, Ioannis and Agarwala, Atish},
  journal={arXiv preprint arXiv:2402.05271},
  year={2024}
}
@article{barrett2020implicit,
  title={Implicit gradient regularization},
  author={Barrett, David GT and Dherin, Benoit},
  journal={arXiv preprint arXiv:2009.11162},
  year={2020}
}
@inproceedings{gretton2005measuring,
  title={Measuring statistical dependence with Hilbert-Schmidt norms},
  author={Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  booktitle={International conference on algorithmic learning theory},
  pages={63--77},
  year={2005},
  organization={Springer}
}
@inproceedings{mooij2009regression,
  title={Regression by dependence minimization and its application to causal inference in additive noise models},
  author={Mooij, Joris and Janzing, Dominik and Peters, Jonas and Sch{\"o}lkopf, Bernhard},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={745--752},
  year={2009}
}
@article{smith2021origin,
  title={On the origin of implicit regularization in stochastic gradient descent},
  author={Smith, Samuel L and Dherin, Benoit and Barrett, David GT and De, Soham},
  journal={arXiv preprint arXiv:2101.12176},
  year={2021}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}

@misc{singh2021analytic,
      title={Analytic Insights into Structure and Rank of Neural Network Hessian Maps}, 
      author={Sidak Pal Singh and Gregor Bachmann and Thomas Hofmann},
      year={2021},
      eprint={2106.16225},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@misc{agarwala2024high,
      title={High dimensional analysis reveals conservative sharpening and a stochastic edge of stability}, 
      author={Atish Agarwala and Jeffrey Pennington},
      year={2024},
      eprint={2404.19261},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@inproceedings{
singh2024closed,
title={Closed form of the Hessian spectrum for some Neural Networks},
author={Sidak Pal Singh and Thomas Hofmann},
booktitle={High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning},
year={2024},
url={https://openreview.net/forum?id=gW30Rx4eZP}
}
@inproceedings{
Jastrzebski2020The,
title={The Break-Even Point on Optimization Trajectories of Deep Neural Networks},
author={Stanislaw Jastrzebski and Maciej Szymczak and Stanislav Fort and Devansh Arpit and Jacek Tabor and Kyunghyun Cho* and Krzysztof Geras*},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1g87C4KwB}
}
@article{ujvary2022rethinking,
	title={Rethinking Sharpness-Aware Minimization as Variational Inference},
	author={Ujv{\'a}ry, Szilvia and Telek, Zsigmond and Kerekes, Anna and M{\'e}sz{\'a}ros, Anna and Husz{\'a}r, Ferenc},
	journal={arXiv preprint arXiv:2210.10452},
	year={2022}
}
@article{hochreiter1997flat,
	title={Flat minima},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={1},
	pages={1--42},
	year={1997},
	publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}
@article{keskar2016large,
	title={On large-batch training for deep learning: Generalization gap and sharp minima},
	author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	journal={arXiv preprint arXiv:1609.04836},
	year={2016}
}
@article{foret2020sharpness,
	title={Sharpness-aware minimization for efficiently improving generalization},
	author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	journal={arXiv preprint arXiv:2010.01412},
	year={2020}
}
@inproceedings{
singh2022phenomenology,
title={Phenomenology of Double Descent in Finite-Width Neural Networks},
author={Sidak Pal Singh and Aurelien Lucchi and Thomas Hofmann and Bernhard Sch{\"o}lkopf},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=lTqGXfn9Tv}
}
@inproceedings{
singh2021analytic,
title={Analytic Insights into Structure and Rank of Neural Network Hessian Maps},
author={Sidak Pal Singh and Gregor Bachmann and Thomas Hofmann},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=otDgw7LM7Nn}
}
@misc{https://doi.org/10.48550/arxiv.1910.05929,
  doi = {10.48550/ARXIV.1910.05929},
  
  url = {https://arxiv.org/abs/1910.05929},
  
  author = {Fort, Stanislav and Ganguli, Surya},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Emergent properties of the local geometry of neural loss landscapes},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@software{nanodo,
  author = {Peter J. Liu and Roman Novak and Jaehoon Lee and Mitchell Wortsman and Lechao Xiao and Katie Everett and Alexander A. Alemi and  Mark Kurzeja and Pierre Marcenac and Izzeddin Gur and Simon Kornblith and Kelvin Xu and Gamaleldin Elsayed and Ian Fischer and Jeffrey Pennington and Ben Adlam and Jascha-Sohl Dickstein},
  title = {NanoDO: A minimal Transformer decoder-only language model implementation in {JAX}.},
  url = {http://github.com/google-deepmind/nanodo},
  version = {0.1.0},
  year = {2024},
}
@article{gur2018gradient,
  title={Gradient descent happens in a tiny subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  journal={arXiv preprint arXiv:1812.04754},
  year={2018}
}

@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}
Loading...
@misc{papyan2019spectrumdeepnethessiansscale,
      title={The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size}, 
      author={Vardan Papyan},
      year={2019},
      eprint={1811.07062},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.07062}, 
}
@misc{sagun2018empiricalanalysishessianoverparametrized,
      title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks}, 
      author={Levent Sagun and Utku Evci and V. Ugur Guney and Yann Dauphin and Leon Bottou},
      year={2018},
      eprint={1706.04454},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.04454}, 
}
@misc{haas2024boldsymbolmumathbfp2effectivesharpnessaware,
      title={$\boldsymbol{\mu}\mathbf{P^2}$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling}, 
      author={Moritz Haas and Jin Xu and Volkan Cevher and Leena Chennuru Vankadara},
      year={2024},
      eprint={2411.00075},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.00075}, 
}
@misc{yang2022tensorprogramsvtuning,
      title={Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer}, 
      author={Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki and Weizhu Chen and Jianfeng Gao},
      year={2022},
      eprint={2203.03466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.03466}, 
}
@misc{ormaniec2024doesmeantransformerinsights,
      title={What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis}, 
      author={Weronika Ormaniec and Felix Dangel and Sidak Pal Singh},
      year={2024},
      eprint={2410.10986},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.10986}, 
}
@article{jiang2024does,
  title={How does adaptive optimization impact local neural network geometry?},
  author={Jiang, Kaiqi and Malik, Dhruv and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{pan2023toward,
  title={Toward understanding why adam converges faster than sgd for transformers},
  author={Pan, Yan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2306.00204},
  year={2023}
}
@article{liu2020understanding,
  title={Understanding the difficulty of training transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal={arXiv preprint arXiv:2004.08249},
  year={2020}
}
@article{noci2022signal,
  title={Signal propagation in transformers: Theoretical perspectives and the role of rank collapse},
  author={Noci, Lorenzo and Anagnostidis, Sotiris and Biggio, Luca and Orvieto, Antonio and Singh, Sidak Pal and Lucchi, Aurelien},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27198--27211},
  year={2022}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@article{everett2024scaling,
  title={Scaling exponents across parameterizations and optimizers},
  author={Everett, Katie and Xiao, Lechao and Wortsman, Mitchell and Alemi, Alexander A and Novak, Roman and Liu, Peter J and Gur, Izzeddin and Sohl-Dickstein, Jascha and Kaelbling, Leslie Pack and Lee, Jaehoon and others},
  journal={arXiv preprint arXiv:2407.05872},
  year={2024}
}

@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
Loading...
@misc{ridnik2021imagenet21kpretrainingmasses,
      title={ImageNet-21K Pretraining for the Masses}, 
      author={Tal Ridnik and Emanuel Ben-Baruch and Asaf Noy and Lihi Zelnik-Manor},
      year={2021},
      eprint={2104.10972},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.10972}, 
}
@misc{sun2017revisitingunreasonableeffectivenessdata,
      title={Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}, 
      author={Chen Sun and Abhinav Shrivastava and Saurabh Singh and Abhinav Gupta},
      year={2017},
      eprint={1707.02968},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1707.02968}, 
}
@inproceedings{ciregan2012multi,
  title={Multi-column deep neural networks for image classification},
  author={Ciregan, Dan and Meier, Ueli and Schmidhuber, J{\"u}rgen},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={3642--3649},
  year={2012},
  organization={IEEE}
}
@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}


@inproceedings{kim2022fisher,
  title={Fisher sam: Information geometry and sharpness aware minimisation},
  author={Kim, Minyoung and Li, Da and Hu, Shell X and Hospedales, Timothy},
  booktitle={International Conference on Machine Learning},
  pages={11148--11161},
  year={2022},
  organization={PMLR}
}
@misc{jacot2020asymptoticspectrumhessiandnn,
      title={The asymptotic spectrum of the Hessian of DNN throughout training}, 
      author={Arthur Jacot and Franck Gabriel and ClÃ©ment Hongler},
      year={2020},
      eprint={1910.02875},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.02875}, 
}
@article{zhang2025preconditioned,
  title={Preconditioned Sharpness-Aware Minimization: Unifying Analysis and a Novel Learning Algorithm},
  author={Zhang, Yilang and Li, Bingcong and Giannakis, Georgios B},
  journal={arXiv preprint arXiv:2501.06603},
  year={2025}
}

@misc{haas2024boldsymbolmumathbfp2effectivesharpnessaware,
      title={$\boldsymbol{\mu}\mathbf{P^2}$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling}, 
      author={Moritz Haas and Jin Xu and Volkan Cevher and Leena Chennuru Vankadara},
      year={2024},
      eprint={2411.00075},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.00075}, 
}
@misc{tahmasebi2024universalclasssharpnessawareminimization,
      title={A Universal Class of Sharpness-Aware Minimization Algorithms}, 
      author={Behrooz Tahmasebi and Ashkan Soleymani and Dara Bahri and Stefanie Jegelka and Patrick Jaillet},
      year={2024},
      eprint={2406.03682},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03682}, 
}
@misc{wen2023sharpnessminimizationalgorithmsminimize,
      title={Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization}, 
      author={Kaiyue Wen and Zhiyuan Li and Tengyu Ma},
      year={2023},
      eprint={2307.11007},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.11007}, 
}
@inproceedings{kwon2021asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={International Conference on Machine Learning},
  pages={5905--5914},
  year={2021},
  organization={PMLR}
}

@misc{sherborne2024trambridgingtrustregions,
      title={TRAM: Bridging Trust Regions and Sharpness Aware Minimization}, 
      author={Tom Sherborne and Naomi Saphra and Pradeep Dasigi and Hao Peng},
      year={2024},
      eprint={2310.03646},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.03646}, 
}
@misc{becker2024momentumsamsharpnessawareminimization,
      title={Momentum-SAM: Sharpness Aware Minimization without Computational Overhead}, 
      author={Marlon Becker and Frederick Altrock and Benjamin Risse},
      year={2024},
      eprint={2401.12033},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.12033}, 
}
@misc{andriushchenko2022understandingsharpnessawareminimization,
      title={Towards Understanding Sharpness-Aware Minimization}, 
      author={Maksym Andriushchenko and Nicolas Flammarion},
      year={2022},
      eprint={2206.06232},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.06232}, 
}
@misc{bartlett2023dynamicssharpnessawareminimizationbouncing,
      title={The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima}, 
      author={Peter L. Bartlett and Philip M. Long and Olivier Bousquet},
      year={2023},
      eprint={2210.01513},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.01513}, 
}
@article{li2024enhancing,
  title={Enhancing sharpness-aware optimization through variance suppression},
  author={Li, Bingcong and Giannakis, Georgios},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{du2022sharpness,
  title={Sharpness-Aware Training for Free},
  author={Du, Jiawei and Zhou, Daquan and Feng, Jiashi and Tan, Vincent YF and Zhou, Joey Tianyi},
  journal={arXiv preprint arXiv:2205.14083},
  year={2022}
}
@misc{
liu2023same,
title={Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models},
author={Hong Liu and Sang Michael Xie and Zhiyuan Li and Tengyu Ma},
year={2023},
url={https://openreview.net/forum?id=F5uYcwABMu}
}
@inproceedings{Na_2022,
   title={Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models},
   url={http://dx.doi.org/10.18653/v1/2022.findings-emnlp.361},
   DOI={10.18653/v1/2022.findings-emnlp.361},
   booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
   publisher={Association for Computational Linguistics},
   author={Na, Clara and Mehta, Sanket Vaibhav and Strubell, Emma},
   year={2022},
   pages={4909â€“4936} }

@misc{keskar2017largebatchtrainingdeeplearning,
      title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, 
      author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
      year={2017},
      eprint={1609.04836},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.04836}, 
}
@misc{xie2024sampasharpnessawareminimizationparallelized,
      title={SAMPa: Sharpness-aware Minimization Parallelized}, 
      author={Wanyun Xie and Thomas Pethick and Volkan Cevher},
      year={2024},
      eprint={2410.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.10683}, 
}
@inproceedings{liu2022towards,
  title={Towards efficient and scalable sharpness-aware minimization},
  author={Liu, Yong and Mai, Siqi and Chen, Xiangning and Hsieh, Cho-Jui and You, Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12360--12370},
  year={2022}
}
@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}
@misc{dziugaite2017computingnonvacuousgeneralizationbounds,
      title={Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data}, 
      author={Gintare Karolina Dziugaite and Daniel M. Roy},
      year={2017},
      eprint={1703.11008},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1703.11008}, 
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@misc{wu2020adversarialweightperturbationhelps,
      title={Adversarial Weight Perturbation Helps Robust Generalization}, 
      author={Dongxian Wu and Shu-tao Xia and Yisen Wang},
      year={2020},
      eprint={2004.05884},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2004.05884}, 
}
@article{krogh1991simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John},
  journal={Advances in neural information processing systems},
  volume={4},
  year={1991}
}
@misc{gouk2020regularisationneuralnetworksenforcing,
      title={Regularisation of Neural Networks by Enforcing Lipschitz Continuity}, 
      author={Henry Gouk and Eibe Frank and Bernhard Pfahringer and Michael J. Cree},
      year={2020},
      eprint={1804.04368},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1804.04368}, 
}
@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}
@article{rissanen1978modeling,
  title={Modeling by shortest data description},
  author={Rissanen, Jorma},
  journal={Automatica},
  volume={14},
  number={5},
  pages={465--471},
  year={1978},
  publisher={Elsevier}
}
@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}
@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}
@misc{chaudhari2017entropysgdbiasinggradientdescent,
      title={Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}, 
      author={Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
      year={2017},
      eprint={1611.01838},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.01838}, 
}
@article{DBLP:journals/corr/abs-2006-07897,
  author       = {Fabrizio Pittorino and
                  Carlo Lucibello and
                  Christoph Feinauer and
                  Enrico M. Malatesta and
                  Gabriele Perugini and
                  Carlo Baldassi and
                  Matteo Negri and
                  Elizaveta Demyanenko and
                  Riccardo Zecchina},
  title        = {Entropic gradient descent algorithms and wide flat minima},
  journal      = {CoRR},
  volume       = {abs/2006.07897},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.07897},
  eprinttype    = {arXiv},
  eprint       = {2006.07897},
  timestamp    = {Wed, 17 Jun 2020 14:28:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-07897.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{jiang2020neurips2020competitionpredicting,
      title={NeurIPS 2020 Competition: Predicting Generalization in Deep Learning}, 
      author={Yiding Jiang and Pierre Foret and Scott Yak and Daniel M. Roy and Hossein Mobahi and Gintare Karolina Dziugaite and Samy Bengio and Suriya Gunasekar and Isabelle Guyon and Behnam Neyshabur},
      year={2020},
      eprint={2012.07976},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.07976}, 
}
@misc{jiang2019fantasticgeneralizationmeasures,
      title={Fantastic Generalization Measures and Where to Find Them}, 
      author={Yiding Jiang and Behnam Neyshabur and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
      year={2019},
      eprint={1912.02178},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.02178}, 
}
@article{vapnik1991principles,
  title={Principles of risk minimization for learning theory},
  author={Vapnik, Vladimir},
  journal={Advances in neural information processing systems},
  volume={4},
  year={1991}
}
@misc{bahri2022sharpnessawareminimizationimproveslanguage,
      title={Sharpness-Aware Minimization Improves Language Model Generalization}, 
      author={Dara Bahri and Hossein Mobahi and Yi Tay},
      year={2022},
      eprint={2110.08529},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.08529}, 
}
@software{flax2020github,
  author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
  title = {{F}lax: A neural network library and ecosystem for {JAX}},
  url = {http://github.com/google/flax},
  version = {0.10.2},
  year = {2024},
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/jax-ml/jax},
  version = {0.3.13},
  year = {2018},
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}


@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}
@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@article{GurAri2018GradientDH,
  title={Gradient Descent Happens in a Tiny Subspace},
  author={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.04754}
}

@article{Li2020HessianBA,
  title={Hessian based analysis of SGD for Deep Nets: Dynamics and Generalization},
  author={Xinyan Li and Qilong Gu and Yingxue Zhou and Tiancong Chen and Arindam Banerjee},
  journal={ArXiv},
  year={2020},
  volume={abs/1907.10732}
}
@misc{foret2021sharpnessaware,
      title={Sharpness-Aware Minimization for Efficiently Improving Generalization}, 
      author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
      year={2021},
      eprint={2010.01412},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@misc{dauphin2024neglected,
      title={Neglected Hessian component explains mysteries in Sharpness regularization}, 
      author={Yann N. Dauphin and Atish Agarwala and Hossein Mobahi},
      year={2024},
      eprint={2401.10809},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@inproceedings{DBLP:conf/aistats/ThomasPMMBR20,
  author={Valentin Thomas and Fabian Pedregosa and Bart van MerriÃ«nboer and Pierre-Antoine Manzagol and Yoshua Bengio and Nicolas Le Roux},
  title={On the interplay between noise and curvature and its effect on optimization and generalization},
  year={2020},
  cdate={1577836800000},
  pages={3503-3513},
  url={http://proceedings.mlr.press/v108/thomas20a.html},
  booktitle={AISTATS},
  crossref={conf/aistats/2020}
}


@article{Hochreiter1997FlatM,
  title={Flat Minima},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1-42},
  url={https://api.semanticscholar.org/CorpusID:733161}
}

@inproceedings{10.5555/3618408.3619732,
author = {Singh, Sidak Pal and Hofmann, Thomas and Sch\"{o}lkopf, Bernhard},
title = {The Hessian perspective into the nature of convolutional neural networks},
year = {2023},
publisher = {JMLR.org},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1324},
numpages = {39},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}
@InProceedings{pennington17rmt,
  title = 	 {Geometry of Neural Network Loss Surfaces via Random Matrix Theory},
  author =       {Jeffrey Pennington and Yasaman Bahri},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2798--2806},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/pennington17a.html},
}

@misc{zhang2024transformersneedadamhessian,
      title={Why Transformers Need Adam: A Hessian Perspective}, 
      author={Yushun Zhang and Congliang Chen and Tian Ding and Ziniu Li and Ruoyu Sun and Zhi-Quan Luo},
      year={2024},
      eprint={2402.16788},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.16788}, 
}