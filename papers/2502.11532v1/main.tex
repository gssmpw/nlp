\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{cleveref}
\usepackage{array} 
\usepackage{amsmath}
\usepackage{booktabs} 



\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Control-CLIP: Decoupling Category and Style Guidance in CLIP for Specific-Domain Generation}

\author{\IEEEauthorblockN{
Zexi Jia\IEEEauthorrefmark{2}, 
Chuanwei Huang\IEEEauthorrefmark{2},
Hongyan Fei, 
Yeshuang Zhu, 
Zhiqiang Yuan, 
Jinchao Zhang*, 
Jie Zhou* }
\IEEEauthorblockA{Peking University, Beijing, China \\  Wechat AI, Tencent, China}
}


\maketitle


\begin{abstract}
Text-to-image diffusion models have shown remarkable capabilities of generating high-quality images closely aligned with textual inputs. However, the effectiveness of text guidance heavily relies on the CLIP text encoder, which is trained to pay more attention to general content but struggles to capture semantics in specific domains like styles. 
As a result, generation models tend to fail on prompts like "a photo of a cat in Pokemon style" in terms of simply producing images depicting "a photo of a cat".
To fill this gap, we propose Control-CLIP, a novel decoupled CLIP fine-tuning framework that enables the CLIP model to learn the meaning of category and style in a complement manner. With specially designed fine-tuning tasks on minimal data and a modified cross-attention mechanism, Control-CLIP can precisely guide the diffusion model to a specific domain. Moreover, the parameters of the diffusion model remain unchanged at all, preserving the original generation performance and diversity. Experiments across multiple domains confirm the effectiveness of our approach, particularly highlighting its robust plug-and-play capability in generating content with various specific styles.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}


\input{1_intro}
\input{2_related_work}
\input{3_method}
\input{4_experiments}
\input{5_conclusion}






\bibliographystyle{IEEEbib}
\bibliography{icme2025references}


\end{document}
