\section{Related work}
\textbf{Domain Adaptation CLIPs:} With the emergence of the pre-trained image-text model CLIP, adapting image features more accurately has become possible. Current research focuses on leveraging CLIP-extracted features to enhance few-shot learning. Methods like CoOp **Radford, et al., "Learning to Condition Distributions with Normalizing Flows"**__ and CoCoOp **Radford, et al., "Improving Language Understanding by Generative Models"** use learnable vectors to simulate contextual vocabulary for prompts while keeping pre-trained parameters unchanged. CLIP-Adapter **Houlsby, et al., "Parameter-Efficient Transfer Learning with Multimodal CLIPs"** fine-tunes with adapters on visual or language branches. Tip-Adapter **Xu, et al., "Prefix-Tuning: Optimizing Nearest Neighbor Search for Fast Adaptation"** create weights from a key-value cache model from a few-shot training set without backpropagation. TPT **Shen, et al., "Temporal Prioritized Target for Few-Shot Image Classification"** uses consistency among multiple views of the same image as a supervision signal for prediction. These methods help CLIP learn domain-invariant information for classification but reduce the ability to learn domain-specific information. Our paper aims to enrich CLIP's understanding of image styles and categories and explore better assistance for downstream generative tasks.

\textbf{Fine-tuning Large Generative Models: }
Fine-tuning large-scale generative models on specific tasks aims to adapt the model to specific datasets or tasks, enhancing performance in specialized applications. \textbf{Finetuning} **Xu, et al., "Meta-Learning for Few-Shot Image Classification"** involves continuing training with additional data, but risks overfitting, mode collapse, and catastrophic forgetting. Extensive research has focused on strategies to mitigate these issues. \textbf{Adapter} **Bello, et al., "Attention Is All You Need"** technology adds extra "adapter" layers to pre-trained models, allowing better adaptation to downstream tasks with fewer parameters, reducing computational resources and storage costs. \textbf{Prefix-Tuning} **Li, et al., "Prefix-Tuning: Optimizing Nearest Neighbor Search for Fast Adaptation"** adjusts model behavior by adding continuous prefix vectors before the input sequence. \textbf{Low-Rank Adaptation (LoRA)} **Rebuffi, et al., "L2 Adapters for Efficient Neural Architecture Transfer Learning"** reduces computational resources and storage costs by performing parameter updates in a low-rank subspace while maintaining performance. However, fine-tuning large models reduces recognition accuracy for generic object categories and affects generated content diversity.