\section{Introduction}
\label{sec:intro}

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=0.9\linewidth]{first.pdf} 
\end{center}
   \caption{Comparison of different approaches to leveraging in-domain text and image data for fine-tuning. Previously common fine-tuning methods include: (a) fine-tuning only the generator, (b) fine-tuning only the text encoder, and (c) using an adapter to decouple category features from the text encoder and combine them with the original text features. (d) Our Control-CLIP utilizes a decoupled design to learn both category and style features for generation models.}
\label{fig:first}
\end{figure*}

In recent years, there have been significant advancements in text-controlled image generation. With the emergence of text-to-image models~\cite{rombach2022high, jia2024finger,huang2025semantic,jia2023fingerstr,wang2023improving}, we can now create visually stunning images by providing textual prompts. 
Apart from generating high quality images, accurately controlling and adapting the style of generated images via text is also a crucial requirement in AI-based artistic creation. A challenge lies in leveraging domain-specific text-image data to help pre-trained generative models better understand the stylistic nuances of a particular domain and accurately generate images based on textual descriptions.
To tackle this problem, methods have been proposed to fine-tune the diffusion generation model~\cite{zhai2023investigating} or incorporating various control conditions~\cite{tumanyan2023plug}, as illustrated in Fig.~\ref{fig:first} (a) and (b).

Contrastive Language-Image Pre-training (CLIP)~\cite{radford2021learning} effectively bridges the gap between the language and image domains and serves as the foundational network for most diffusion-based image generation models.

However, despite the growing use of CLIP models in generative tasks like text-to-image and text-to-video generation, the need for domain adaptability in CLIP models and its impact on generative models has been under-explored. We propose that domain adaptation of text-to-image diffusion models can be effectively achieved by fine-tuning only the CLIP component on a small amount of in-domain data, thus preserving the general-domain generation capability of pre-trained diffusion models.

In this paper, we consider style features as domain-specific and object categories as domain-invariant. As noted in domain adaptation~\cite{wang2018deep}, each domain has unique characteristics, while all domains share invariant features. For instance, we can differentiate images like cartoons, sketches, and real photos based on color strokes and texture, which are domain-specific features. However, all styles of images can depict the same object, such as a cat or a dog, determined by category features independent of the domain.

Previous research on fine-tuning the CLIP model for domain adaptation mainly focused on learning category features to improve classification accuracy within specific domains, as shown in Fig.~\ref{fig:first}(c). However, these methods do not explicitly separate style features from category features in their loss functions. While effective in some cases, they inadvertently reduce the model's ability to recognize stylistic nuances. In the application of text-to-image generation, a depth understanding of nuanced domain-specific styles is demanded. Traditional fine-tuning methods of CLIP models focus on learning domain-invariant features for classification has inadvertently suppressed the model's sensitivity to style-specific attributes. This tendency has impeded the fine-tuned CLIP models in recognizing various stylistic forms and has constrained their ability to accurately interpret text descriptions about domain-specific styles.

This paper introduces a novel approach called \emph{Control-CLIP}, designed to explicitly disentangle style and category features to enhance the representation capacity of CLIP. Our framework uses two distinct text encoders—style and category encoders—fine-tuned independently. As shown in Fig.~\ref{fig:first}(d), this integration improves the CLIP model's ability to interpret textual descriptions of object categories and styles. By decoupling domain features, the CLIP model gains enhanced resilience in style and category perception, excelling in precise few-shot classification and style discrimination tasks within specific domains. This improved semantic understanding also allows the CLIP model to better control image generation, producing images that align more accurately with language descriptions. We develop two variations of the Control-CLIP model to cater to diverse datasets. One variant uses cross-entropy-based loss functions for datasets with style labels, while the other employs triplet loss for datasets with only text description labels. This adaptability demonstrates the method's efficacy in enhancing the CLIP model's comprehension of both style and category information.

Additionally, we introduce a novel cross-attention mechanism that combines the fine-tuned outcomes of the style and category encoders with the Stable Diffusion~\cite{rombach2022high} model. This refined structure integrates seamlessly with Stable Diffusion in a plug-and-play manner, eliminating the need to train the generative model. Incorporating the Control-CLIP model enhances the generative model's understanding of textual style and category descriptions, improving the generation of in-domain images that closely match the provided descriptions.



In summary, the main contributions of this paper include:
\begin{itemize}
\item We propose Control-CLIP, a CLIP based text-image alignment model that explicitly decouples style and category features.
\item  Control-CLIP can be fine-tuned on specific domain datasets, and it can enhance the category discrimination ability and style discrimination ability of the CLIP model simultaneously in specific domains. 
\item We propose a new form of cross-attention that combines Control-CLIP into the Stable Diffusion model. This integration helps generate images that align with semantic descriptions without adjusting the generative model.
\end{itemize}
% (1) We propose Control-CLIP, the first CLIP based text-image alignment model that explicitly decouples style and category features. (2) Control-CLIP can be fine-tuned on specific domain datasets, and it can enhance the category discrimination ability and style discrimination ability of the CLIP model simultaneously in specific domains. (3) We propose a new form of cross-attention that combines Control-CLIP into the Stable Diffusion model. This integration helps generate images that align with semantic descriptions without adjusting the generative model.