% \vspace{-8px}
\section{Related work}

\textbf{Domain Adaptation CLIPs:} With the emergence of the pre-trained image-text model CLIP, adapting image features more accurately has become possible. Current research focuses on leveraging CLIP-extracted features to enhance few-shot learning. Methods like CoOp~\cite{zhou2022learning} and CoCoOp~\cite{zhou2022conditional} use learnable vectors to simulate contextual vocabulary for prompts while keeping pre-trained parameters unchanged. CLIP-Adapter~\cite{gao2023clip} fine-tunes with adapters on visual or language branches. Tip-Adapter~\cite{zhang2021tip} create weights from a key-value cache model from a few-shot training set without backpropagation. TPT~\cite{shu2022test} uses consistency among multiple views of the same image as a supervision signal for prediction. These methods help CLIP learn domain-invariant information for classification but reduce the ability to learn domain-specific information. Our paper aims to enrich CLIP's understanding of image styles and categories and explore better assistance for downstream generative tasks.

\textbf{Fine-tuning Large Generative Models: }
Fine-tuning large-scale generative models on specific tasks aims to adapt the model to specific datasets or tasks, enhancing performance in specialized applications. \textbf{Finetuning}~\cite{kumar2022fine} involves continuing training with additional data, but risks overfitting, mode collapse, and catastrophic forgetting. Extensive research has focused on strategies to mitigate these issues. \textbf{Adapter}~\cite{chen2022vision} technology adds extra "adapter" layers to pre-trained models, allowing better adaptation to downstream tasks with fewer parameters, reducing computational resources and storage costs. \textbf{Prefix-Tuning}~\cite{li2021prefix} adjusts model behavior by adding continuous prefix vectors before the input sequence. \textbf{Low-Rank Adaptation (LoRA)}~\cite{hu2022lora} reduces computational resources and storage costs by performing parameter updates in a low-rank subspace while maintaining performance. However, fine-tuning large models reduces recognition accuracy for generic object categories and affects generated content diversity. 

