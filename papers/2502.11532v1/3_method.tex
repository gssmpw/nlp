
\section{Method}


\subsection{Decoupling Style and Category Features}

Inspired by the work of Adversarial Representation Learning (ARL)~\cite{ding2022domain}, our approach involves a dual strategy of minimizing cross-entropy associated with target labels and maximizing cross-entropy for non-target labels. This method aims to separate domain-specific features and domain-invariant features effectively. To achieve this, we utilize two distinct CLIP-based fine-tuning networks, referred to as the style encoder and the category encoder, respectively. We use $f$ to represent the output of CLIP, and $f_{s}$ to represent the output of the style encoder, $f_{c}$ to represent the output of the category encoder, and $f_i$ to represent the output of image encoder. The primary objective is to endow each fine-tuned model with discriminative capabilities pertinent to its designated domain. Specifically, the style encoder is fine-tuned to minimize style classification errors while intentionally maximizing category classification errors. This approach is designed to encourage predictions for categories to approximate a uniform distribution. Conversely, the category encoder is fine-tuned to exhibit the opposite behavior, prioritizing accuracy in category classification and less focus on style differentiation. The training pipeline is illustrated in Fig.~\ref{fig:train}.

\textbf{Labelled Data}: When applying fine-tuning on datasets with style labels, we leverage two cross-entropy loss functions to effectively distinguish between style and category attributes. The style encoder's objective is to minimize style-related loss while concurrently maximizing category-related loss. The formulation of this approach is as follows:
\begin{equation}
\begin{aligned}
&\mathcal{L}_{1}=-\sum \operatorname{softmax}(f_{s}) \log y_{s} \\
&\mathcal{L}_{2}=\sum \operatorname{softmax}(f_{s}) \log {y_{c}},
\end{aligned}
\end{equation}
where ${y_s}$ and $y_c$ represent the style label and category label of the image, respectively. During the training process, we simultaneously constrain both of them, resulting in the total loss function:
\begin{equation}
\mathcal{L}_{style}^{label}=\mathcal{L}_{1}+ {\lambda}_{1} \mathcal{L}_{2}.
\end{equation}
Correspondingly, the loss function for the category encoder during fine-tuning is as follows:
\begin{equation}
\begin{aligned}
&\mathcal{L}_{1}^{'}=-\sum \operatorname{softmax}(f_{c}) \log y_{c} \\
&\mathcal{L}_{2}^{'}=\sum \operatorname{softmax}(f_{c}) \log y_{s},
\end{aligned}
\end{equation}
and the total loss function for the category encoder is:
\begin{equation}
\mathcal{L}_{category}^{label}=\mathcal{L}_{1}^{'}+ {\lambda}_{2} \mathcal{L}_{2}^{'}.
\end{equation}

% where $n_1$ and $n_2$ are constant terms, and throughout this paper, both are set to 1.

\textbf{Unlabelled Data}: Most fine-tuning datasets are comprised of image-text pairs without explicit style labels. 
Generally, in generative scenarios, we can obtain captions corresponding to images. For images without captions, Vision-Language Models (VLM) can be utilized to generate them. We apply a strategy where we filter out category-related words using regular expression techniques, leaving behind primarily style-related text. The style-related text is then inputted into the text encoder of the CLIP model, extracting the \textbf{style feature} $f_{s}$, and feeding the category-related text to obtain the \textbf{category feature} $f_{c}$. 
% Then, we use triplet loss~\cite{schroff2015facenet} to distinguish between image descriptors and image categories. Throughout the process of training the style encoder using a caption-based approach, image-caption pairs are treated as positive pairings, while image-category pairs are perceived as negative pairings. 
Due to the absence of corresponding style labels, we utilize $f_i$ as an anchor to ensure the retention of style information in $f_s$. Simultaneously, by increasing the distance between $f_s$ and $f_c$, we aim to eliminate category information from $f_s$. More specifically, we employ triplet loss~\cite{schroff2015facenet} to implement the aforementioned constraints for the style encoder:
% The training methodology for the style encoder can then be expressed as follows: 
\begin{equation}
\mathcal{L}_{style}^{unlabel}= \sum \max (\|f_{s}-f_{i}\|_2 - \|f_{s}-f_{c}\|_2+m_1, 0).
\end{equation}
Correspondingly, the loss function for category encoder is:
\begin{equation}
\mathcal{L}_{category}^{unlabel}= \sum \max (\|f_{c}-f_{i}\|_2 - \|f_{c}-f_{s}\|_2+m_2, 0).
\end{equation}
$m_1$ and $m_2$ represent the margins in the triplet loss. In this paper, we set $m_1 = m_2 = 0.3$.
% \subsection{Control-CLIP}

In our model, we introduce an Adapter module consisting of two learnable linear layers with activation functions. This compact module efficiently captures domain-specific style and category characteristics, streamlining the learning process and reducing overfitting with minimal parameters.

For few-shot classification, we combine outputs from the category encoder and CLIP's text encoder to obtain finely tuned category-specific features. In the image style discrimination task, we use a weighted sum of the style encoder and CLIP's text encoder outputs. For generation tasks, we guide the process using both style and category features in a plug-and-play manner within the diffusion model.

\begin{figure*}[h!]
\begin{center}
\includegraphics[width=0.9
\linewidth]{training.pdf}
\end{center}
   \caption{Training style encoder in Control-CLIP. Left: On datasets with style labels, Control-CLIP separates category and style features through cross-entropy loss, achieving better performance but requires higher annotation requirements.  Right: On datasets without style labels, we can utilize image captions to obtain style information. Control-CLIP uses a triplet loss function to distinguish between category and style features. The category encoder is trained similarly with an inverted loss function.
}
\label{fig:train}
\end{figure*}

\subsection{Using Control-CLIP as Generation Condition}
The research on Stable Diffusion~\cite{rombach2022high} demonstrates that diffusion models can model conditional distributions of the form $p(z|y)$. This can be achieved through the use of a conditional denoising autoencoder $\theta(z_t, t, y)$. 
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{control.pdf}
\end{center}
   \caption{We replace the input of the K and V matrices in the attention mechanism with the outputs of Style-Control and Category-Control CLIP.}
\label{fig:free}
\end{figure}
In this paper, a new control method is proposed, where we expand $\theta(z_t, t, y)$ to $\theta(z_t, t,f_s,f_c)$, allowing for joint control of image generation through decoupled style and category features, as shown in Fig.~\ref{fig:free}. 
% In its implementation, 
We draw inspiration from Stable Diffusion and use a cross-attention mechanism to integrate style and category control.
% as shown in Fig.~\ref{fig:free}
% Regarding the attention mechanism:
Regarding the original attention mechanism in Transformer:
\begin{equation}
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right) \cdot V,
\end{equation}
where $Q$,$K$,$V$originate from the same input.
%HCW
However, in Stable Diffusion, $Q$ is derived from the noise prediction network $\varphi_i\left(z\right)$, while $K$ and $V$ originate from the same control condition encoder $\tau_\theta(y)$:
\begin{equation}
Q=W_Q^{(i)} \cdot \varphi_i\left(z\right), K=W_K^{(i)} \cdot \tau_\theta(y), V=W_V^{(i)} \cdot \tau_\theta(y).
\end{equation}

In this paper, we distinguish the sources of the K and V matrices. Specifically, the K matrix is derived from the style encoder $\tau_{style}$, whereas the V matrix is obtained from the category encoder $\tau_{category}$ , as illustrated in the formula:

\begin{equation}
K=W_K^{(i)} \cdot \tau_{style}(y), V=W_V^{(i)} \cdot \tau_{category}(y).
\end{equation}
By default, we employ style encoder $\tau_{style}$ and category encoder $\tau_{category}$ to control the generation process together. 
%TODO alpha
% Note that the weight coefficient $\alpha$ is uniformly set to 0.1 in the generation task. 
The extracted $f_s$ and $f_c$ are aggregated with the original CLIP text feature $f_{text}$ through residual connections:

\begin{equation}
    \begin{aligned}
    &\tau_{style}(y) = \alpha f_{s} + (1- \alpha) f_{text},\\
    &\tau_{category}(y) = \alpha f_{c} + (1- \alpha) f_{text}.
    \end{aligned}
\label{eq:alpha}
\end{equation}
For generation task, the residual ratio $\alpha$ is set to 0.1.

% Besides, we can tailor different generative needs by combining various K and V matrices. For instance, when we require greater output diversity, we can only use the style features as K and V.

