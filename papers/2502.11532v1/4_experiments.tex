\section{Experiments}


\begin{table*}[]
\begin{center}
\caption{The category and style classification accuracy of CLIP and CLIP-based fine-tuned models on 4 datasets. The comparative work is based on training with style labels.}
\label{table:more}
\setlength{\tabcolsep}{3.5mm}{
\begin{tabular}{lcccccccc}
\toprule
Top-1 Accuracy  & \multicolumn{2}{c}{ImageNet-R} & \multicolumn{2}{c}{PACS} & \multicolumn{2}{c}{VLCS} & \multicolumn{2}{c}{OfficeHome} \\
\midrule
    & Style       & Category  & Style       & Category       & Style   & Category      & Style     & Category   \\
    \midrule
CLIP~\cite{radford2021learning}  &56.0 &46.5  &  69.8            & 88.3             &    25.0       &  69.8           &       50.3    & 71.8             \\
CoOp~\cite{zhou2022learning}    &57.8 &66.4     &   82.1      &  92.3              &   90.0        &73.6         &        86.7   & 74.0            \\
CoCoOp~\cite{zhou2022conditional} &57.5 &66.2  &      80.5        &   \textbf{93.7}        &    88.2       &  74.8           &   87.1        & 73.5             \\
CLIP-Adapter~\cite{gao2023clip} &63.1 &65.3   &      79.8        & 91.2              &     85.6      & 75.1           &        85.5   & 74.3 \\
TPT~\cite{shu2022test}   &59.1 &50.2   &       69.8       &      90.9           &      25.0     &  74.8         &    50.3       & 73.6 \\   
\midrule
Control-CLIP  &\textbf{77.4} &\textbf{70.5} &          \textbf{83.4}    &    92.4            &  \textbf{90.2}        &  \textbf{75.9}            &     \textbf{87.7}      & \textbf{74.7}  \\   
Control-CLIP(caption)  &76.9 &64.2 &       78.9       &  91.7               &   83.1        & 72.4             &      81.2     &72.9  \\   
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}

% \vspace{-3px}
\subsection{Experimental Settings}
\textbf{Datasets:} We conduct experiments with Control-CLIP on four image classification datasets with well-defined styles and category labels, namely ImageNet-R~\cite{hendrycks2021many}, PACS~\cite{li2017deeper}, VLCS~\cite{li2017deeper}, and OfficeHome~\cite{venkateswara2017deep}. These data include two different labels, one representing categories and the other representing styles. For example, in ImageNet-R, there are 200 object category labels that are the same as those in ImageNet, as well as 16 different style labels such as cartoon, video game, painting, and so on.

\textbf{Training:} All methods are trained with 16 samples and evaluated on the full test set. We use manually crafted hard prompts for text input, following CLIP's approach. For datasets with both style and semantic categories, like ImageNet-R, we validate our style-based and description-based models separately. The style-based model uses the prompt ``a photo of {STYLE} {Category}," while the description-based model uses randomly selected common image description templates. We use CLIP's ResNet-50 and VIT-B/16 as visual encoders and a Transformer as the text encoder, keeping pre-trained weights fixed. Data pre-processing follows CLIP's protocols, including random cropping, resizing, and horizontal flipping..
% \vspace{-3px}
\subsection{Performance on Discrimination}
We first evaluate our method on the discriminative task of category and style recognition. We fine-tuned the model on a 16-shots setting of ImageNet-R with 30 epochs and tested its performance on the remaining data. As Tab.~\ref{table:more} shows, our method simultaneously improves the accuracy of category recognition and style recognition, achieving the highest accuracy in style classification. For a fair comparison, we also use category prompts and style prompts respectively for other CLIP-based fine-tuning methods. It is important to note that cache-based models, such as TIP~\cite{zhang2021tip} and APE~\cite{zhu2023not}, struggle to cache multiple attributes of images at the same time, and therefore cannot perform both category and style recognition simultaneously. Compared to full fine-tuning of CLIP, our method only needs to adjust the parameters of two fully connected layers, significantly reducing the number of parameters during training.

Tab.~\ref{table:more} also shows the results of style and category classification on more datasets. We used a 16-shot training set and trained the CLIP fine-tuning methods with style prompts with 30 epochs for fair comparison. The experimental results show that Control-CLIP achieves higher accuracy scores on both style and category across several datasets.

\begin{table}[]
% \caption{Ablations on varying the residual ratio $\alpha$.}
\caption{Recognition accuracy with varying residual ratio $\alpha$.}
\setlength{\tabcolsep}{3.2mm}{
\begin{tabular}{lcccccc}
\toprule
Ratio $\alpha$                  & 0 & 0.2 & 0.4 & 0.6 & 0.8 & 1 \\
\midrule
Category & 56.0  & 62.5    &  \textbf{63.9}   & 63.9    & 63.5   & 63.2  \\
Style   & 46.5  &  66.3   &   70.3  &  70.5   & \textbf{71.4}   & 71.3 \\
\midrule
Ratio $\lambda$                  & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\
\midrule
Category & 63.1  &  63.5   & \textbf{63.9}    &  63.7   & 63.0    & 62.4  \\
Style    & 65.3  &  66.1   &  67.9   &  \textbf{68.1}   & 67.4    & 63.1 \\
\bottomrule

\end{tabular}}
\label{table:alpha}
\end{table}


\subsection{Performance on Generation}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{show.pdf}
\end{center}
   \caption{With the guidance of Control-CLIP, Stable Diffusion can generate images closely aligned with the prompts while maintaining the diversity of the generated contents. Compared to LoRA, Control-CLIP's generated results have better style and content diversity. Prompts: (a) \textcolor{blue}{A photo of Pokemon style cat;} (b) \textcolor{brown}{A photo of Tom and Jerry style dog;} (c) \textcolor{orange}{A photo of SpongeBob style tiger.} For each cartoon domain, we use the same set for LoRA and Control-CLIP of 10 images for fine-tuning.}
\label{fig:show}
\end{figure*}

We employed a multi-style cartoon dataset~\cite{kaggle} to evaluate the generation of diverse styles. This dataset comprises cartoon images in 10 distinct styles sourced from a range of classic animated series, such as Pokemon, Smurfs, South Park, SpongeBob SquarePants, Tom and Jerry, etc. From each style class, we randomly chose 10 images and labeled their corresponding category information to train the Control-CLIP model. After training, we integrated Control-CLIP into the Stable Diffusion v1.5 to test the efficacy of our model in guiding the generative process.

Fig.~\ref{fig:show} 
shows generation results under three cartoon-style datasets. These generation results are all generated from the same prompt template ``a photo of \{STYLE\} style \{Category\}", for example, ``a photo of Pokemon style cat". Control-CLIP demonstrates more accurate content and semantic understanding capabilities, resulting in more accurate generation results. 

As our work involved fine-tuning CLIP and enhancing its style understanding capabilities, objective metrics like FID and CLIP score are challenging to evaluate our generative work. Therefore, we use Average Human Ranking (AHR)~\cite{zhang2023adding} as a preference metric, where users rate each result on a scale from 1 to 5 (the higher the score, the better the generated quality). We invited 10 volunteers, who did not participate in this work, to score the generative quality and description fit of 10 results for each of 10 cartoon categories. The Control-CLIP guidance helped Stable-Diffusion's generative quality average score increase from 1.84 to 2.70, and the description fit score from 2.52 to 3.78.

\subsection{Ablation Study}

\textbf{Residual ratio $\alpha$: } We study the residual ratio $\alpha$ in Eq.~\ref{eq:alpha}. Tab.~\ref{table:alpha} shows that on ImageNet-R, the optimal $\alpha$ is 0.8 for style recognition and 0.4 for category recognition, indicating style adaptation needs more new knowledge. When $\alpha=0$, Control-CLIP becomes the original CLIP, and performance drops, showing successful decoupling of style and category features. At $\alpha=1.0$, reliance on the new CLIP model leads to overfitting.

\textbf{LOSS ratio $\lambda$: } We study the loss weight $\lambda$ to assess our adversarial training strategy. Tab.~\ref{table:alpha} shows that adversarial training improves recognition accuracy for both categories and styles. When $\lambda=0$, Control-CLIP becomes CLIP-Adapter, causing a performance drop.