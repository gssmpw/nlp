\section{Related work}
\textbf{Domain Adaptation CLIPs:} With the emergence of the pre-trained image-text model CLIP, adapting image features more accurately has become possible. Current research focuses on leveraging CLIP-extracted features to enhance few-shot learning. Methods like CoOp____ and CoCoOp____ use learnable vectors to simulate contextual vocabulary for prompts while keeping pre-trained parameters unchanged. CLIP-Adapter____ fine-tunes with adapters on visual or language branches. Tip-Adapter____ create weights from a key-value cache model from a few-shot training set without backpropagation. TPT____ uses consistency among multiple views of the same image as a supervision signal for prediction. These methods help CLIP learn domain-invariant information for classification but reduce the ability to learn domain-specific information. Our paper aims to enrich CLIP's understanding of image styles and categories and explore better assistance for downstream generative tasks.

\textbf{Fine-tuning Large Generative Models: }
Fine-tuning large-scale generative models on specific tasks aims to adapt the model to specific datasets or tasks, enhancing performance in specialized applications. \textbf{Finetuning}____ involves continuing training with additional data, but risks overfitting, mode collapse, and catastrophic forgetting. Extensive research has focused on strategies to mitigate these issues. \textbf{Adapter}____ technology adds extra "adapter" layers to pre-trained models, allowing better adaptation to downstream tasks with fewer parameters, reducing computational resources and storage costs. \textbf{Prefix-Tuning}____ adjusts model behavior by adding continuous prefix vectors before the input sequence. \textbf{Low-Rank Adaptation (LoRA)}____ reduces computational resources and storage costs by performing parameter updates in a low-rank subspace while maintaining performance. However, fine-tuning large models reduces recognition accuracy for generic object categories and affects generated content diversity.