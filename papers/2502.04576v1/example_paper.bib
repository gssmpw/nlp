@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}



@inproceedings{
ruan2024identifyingsandbox,
title={Identifying the Risks of {LM} Agents with an {LM}-Emulated Sandbox},
author={Yangjun Ruan and Honghua Dong and Andrew Wang and Silviu Pitis and Yongchao Zhou and Jimmy Ba and Yann Dubois and Chris J. Maddison and Tatsunori Hashimoto},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=GEcwtMk1uA}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@article{hendrycks2021would,
  title={What would jiminy cricket do? towards agents that behave morally},
  author={Hendrycks, Dan and Mazeika, Mantas and Zou, Andy and Patel, Sahil and Zhu, Christine and Navarro, Jesus and Song, Dawn and Li, Bo and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2110.13136},
  year={2021}
}

@article{o2018scalable,
  title={Scalable end-to-end autonomous vehicle testing via rare-event simulation},
  author={O'Kelly, Matthew and Sinha, Aman and Namkoong, Hongseok and Tedrake, Russ and Duchi, John C},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{miller2019explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{min2025situated,
  title={Situated instruction following},
  author={Min, So Yeon and Puig, Xavi and Chaplot, Devendra Singh and Yang, Tsung-Yen and Rai, Akshara and Parashar, Priyam and Salakhutdinov, Ruslan and Bisk, Yonatan and Mottaghi, Roozbeh},
  booktitle={European Conference on Computer Vision},
  pages={202--228},
  year={2025},
  organization={Springer}
}

@inproceedings{yang2024sweagent,
  title={{SWE}-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
  author={John Yang and Carlos E Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik R Narasimhan and Ofir Press},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://arxiv.org/abs/2405.15793}
}

@article{gehring2024rlef,
  title={Rlef: Grounding code llms in execution feedback with reinforcement learning},
  author={Gehring, Jonas and Zheng, Kunhao and Copet, Jade and Mella, Vegard and Cohen, Taco and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2410.02089},
  year={2024}
}


@inproceedings{zhai2024finetuning,
  title={Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning},
  author={Zhai, Yuexiang and Bai, Hao and Lin, Zipeng and Pan, Jiayi and Tong, Shengbang and Zhou, Yifei and Suhr, Alane and Xie, Saining and LeCun, Yann and Ma, Yi and Levine, Sergey},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=nBjmMF2IZU}
}

@article{ufo,
  title={{UFO: A UI-Focused Agent for Windows OS Interaction}},
  author={Zhang, Chaoyun and Li, Liqun and He, Shilin and Zhang, Xu and Qiao, Bo and  Qin, Si and Ma, Minghua and Kang, Yu and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei and  Zhang, Qi},
  journal={arXiv preprint arXiv:2402.07939},
  year={2024}
}

@article{chen2024self,
  title={Self-play fine-tuning converts weak language models to strong language models},
  author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:2401.01335},
  year={2024}
}

@misc{openhands,
      title={{OpenHands: An Open Platform for AI Software Developers as Generalist Agents}},
      author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
      year={2024},
      eprint={2407.16741},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2407.16741},
}

@inproceedings{gurrealwebagent,
  title={A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis},
  author={Gur, Izzeddin and Furuta, Hiroki and Huang, Austin V and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={68539--68551},
  year={2023}
}

@article{cheng2024seeclick,
  title={Seeclick: Harnessing gui grounding for advanced visual gui agents},
  author={Cheng, Kanzhi and Sun, Qiushi and Chu, Yougang and Xu, Fangzhi and Li, Yantao and Zhang, Jianbing and Wu, Zhiyong},
  journal={arXiv preprint arXiv:2401.10935},
  year={2024}
}

@article{wang2024executable,
  title={Executable code actions elicit better llm agents},
  author={Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2402.01030},
  year={2024}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@article{shavit2023practices,
  title={Practices for governing agentic AI systems},
  author={Shavit, Yonadav and Agarwal, Sandhini and Brundage, Miles and Adler, Steven and Oâ€™Keefe, Cullen and Campbell, Rosie and Lee, Teddy and Mishkin, Pamela and Eloundou, Tyna and Hickey, Alan and others}
}

@inproceedings{yao2023react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{shneiderman2020human,
  title={Human-centered artificial intelligence: Reliable, safe \& trustworthy},
  author={Shneiderman, Ben},
  journal={International Journal of Human--Computer Interaction},
  volume={36},
  number={6},
  pages={495--504},
  year={2020},
  publisher={Taylor \& Francis}
}


@article{diaz2023connectingtrustworthy,
  title={Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation},
  author={D{\'\i}az-Rodr{\'\i}guez, Natalia and Del Ser, Javier and Coeckelbergh, Mark and de Prado, Marcos L{\'o}pez and Herrera-Viedma, Enrique and Herrera, Francisco},
  journal={Information Fusion},
  volume={99},
  pages={101896},
  year={2023},
  publisher={Elsevier}
}

@article{huang2023surveyhallucination,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  year={2023},
  publisher={ACM New York, NY}
}


@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9426--9439},
  year={2024}
}

@article{brundage2018malicious,
  title={The malicious use of artificial intelligence: Forecasting, prevention, and mitigation},
  author={Brundage, Miles and Avin, Shahar and Clark, Jack and Toner, Helen and Eckersley, Peter and Garfinkel, Ben and Dafoe, Allan and Scharre, Paul and Zeitzoff, Thomas and Filar, Bobby and others},
  journal={arXiv preprint arXiv:1802.07228},
  year={2018}
}

@article{hendrycks2021unsolved,
  title={Unsolved problems in ml safety},
  author={Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2109.13916},
  year={2021}
}

@article{andriushchenko2024agentharm,
  title={Agentharm: A benchmark for measuring harmfulness of llm agents},
  author={Andriushchenko, Maksym and Souly, Alexandra and Dziemian, Mateusz and Duenas, Derek and Lin, Maxwell and Wang, Justin and Hendrycks, Dan and Zou, Andy and Kolter, Zico and Fredrikson, Matt and others},
  journal={arXiv preprint arXiv:2410.09024},
  year={2024}
}

@inproceedings{
madaan2023selfrefine,
title={Self-Refine: Iterative Refinement with Self-Feedback},
author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=S37hOerQLB}
}

@inproceedings{tian-etal-2023-just,
    title = "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
    author = "Tian, Katherine  and
      Mitchell, Eric  and
      Zhou, Allan  and
      Sharma, Archit  and
      Rafailov, Rafael  and
      Yao, Huaxiu  and
      Finn, Chelsea  and
      Manning, Christopher",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.330/",
    doi = "10.18653/v1/2023.emnlp-main.330",
    pages = "5433--5442",
    abstract = "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model`s conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50{\%}."
}

@inproceedings{
xiong2024can,
title={Can {LLM}s Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in {LLM}s},
author={Miao Xiong and Zhiyuan Hu and Xinyang Lu and YIFEI LI and Jie Fu and Junxian He and Bryan Hooi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gjeQKFxFpZ}
}

@article{
lin2024generating,
title={Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models},
author={Zhen Lin and Shubhendu Trivedi and Jimeng Sun},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=DWkJCSxKU5},
note={}
}

@inproceedings{ye-etal-2021-towards,
    title = "Towards More Fine-grained and Reliable {NLP} Performance Prediction",
    author = "Ye, Zihuiwen  and
      Liu, Pengfei  and
      Fu, Jinlan  and
      Neubig, Graham",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.324/",
    doi = "10.18653/v1/2021.eacl-main.324",
    pages = "3703--3714",
    abstract = "Performance prediction, the task of estimating a system`s performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU, but also \textit{fine-grained} performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the \textit{reliability} of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future."
}

@article{jiang-etal-2021-know,
    title = "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
    author = "Jiang, Zhengbao  and
      Araki, Jun  and
      Ding, Haibo  and
      Neubig, Graham",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.57/",
    doi = "10.1162/tacl_a_00407",
    pages = "962--977",
    abstract = "Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, {\textquotedblleft}How can we know when language models know, with confidence, the answer to a particular query?{\textquotedblright} We examine this question from the point of view of calibration, the property of a probabilistic model`s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models{---}T5, BART, and GPT-2{---}and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at \url{https://github.com/jzbjyb/lm-calibration}."
}

@article{
lin2022teaching,
title={Teaching Models to Express Their Uncertainty in Words},
author={Stephanie Lin and Jacob Hilton and Owain Evans},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=8s8K2UZGTZ},
note={}
}

@inproceedings{zhou-etal-2023-navigating,
    title = "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
    author = "Zhou, Kaitlyn  and
      Jurafsky, Dan  and
      Hashimoto, Tatsunori",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.335/",
    doi = "10.18653/v1/2023.emnlp-main.335",
    pages = "5506--5524",
    abstract = "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like {\textquotedblleft}I`m sure it`s{\textquotedblright}, {\textquotedblleft}I think it`s{\textquotedblright}, or {\textquotedblleft}Wikipedia says it`s{\textquotedblright} affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80{\%}. Surprisingly, we find that expressions of high certainty result in a 7{\%} decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty."
}


@misc{yang2023improvingreliabilitylargelanguage,
      title={Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning}, 
      author={Yuchen Yang and Houqiang Li and Yanfeng Wang and Yu Wang},
      year={2023},
      eprint={2310.04782},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.04782}, 
}

@inproceedings{wang-etal-2023-know,
    title = "Know What {I} don`t Know: Handling Ambiguous and Unknown Questions for Text-to-{SQL}",
    author = "Wang, Bing  and
      Gao, Yan  and
      Li, Zhoujun  and
      Lou, Jian-Guang",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.352/",
    doi = "10.18653/v1/2023.findings-acl.352",
    pages = "5701--5714",
    abstract = "The task of text-to-SQL aims to convert a natural language question into its corresponding SQL query within the context of relational tables. Existing text-to-SQL parsers generate a plausible SQL query for an arbitrary user question, thereby failing to correctly handle problematic user questions. To formalize this problem, we conduct a preliminary study on the observed ambiguous and unanswerable cases in text-to-SQL and summarize them into 6 feature categories. Correspondingly, we identify the causes behind each category and propose requirements for handling ambiguous and unanswerable questions. Following this study, we propose a simple yet effective counterfactual example generation approach that automatically produces ambiguous and unanswerable text-to-SQL examples. Furthermore, we propose a weakly supervised DTE (Detecting-Then-Explaining) model for error detection, localization, and explanation. Experimental results show that our model achieves the best result on both real-world examples and generated examples compared with various baselines. We release our data and code at: \url{https://github.com/wbbeyourself/DTE}."
}

@inproceedings{
kuhn2023semantic,
title={Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation},
author={Lorenz Kuhn and Yarin Gal and Sebastian Farquhar},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=VD-AYtP0dve},
abstract={We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of "semantic equivalence"â€”different sentences can mean the same thing. To overcome these challenges we introduce semantic entropyâ€”an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.}
}

@inproceedings{mallen-etal-2023-trust,
    title = "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
    author = "Mallen, Alex  and
      Asai, Akari  and
      Zhong, Victor  and
      Das, Rajarshi  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.546/",
    doi = "10.18653/v1/2023.acl-long.546",
    pages = "9802--9822",
    abstract = "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary."
}



@article{DBLP:journals/corr/abs-2406-02378,
  publtype={informal},
  author={Guangliang Liu and Haitao Mao and Bochuan Cao and Zhiyu Xue and Kristen Marie Johnson and Jiliang Tang and Rongrong Wang},
  title={On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2406.02378},
  url={https://doi.org/10.48550/arXiv.2406.02378}
}


@article{Kadavath2022LanguageM,
  title={Language Models (Mostly) Know What They Know},
  author={Saurav Kadavath and Tom Conerly and Amanda Askell and Tom Henighan and Dawn Drain and Ethan Perez and Nicholas Schiefer and Zachary Dodds and Nova Dassarma and Eli Tran-Johnson and Scott Johnston and Sheer El-Showk and Andy Jones and Nelson Elhage and Tristan Hume and Anna Chen and Yuntao Bai and Sam Bowman and Stanislav Fort and Deep Ganguli and Danny Hernandez and Josh Jacobson and John Kernion and Shauna Kravec and Liane Lovitt and Kamal Ndousse and Catherine Olsson and Sam Ringer and Dario Amodei and Tom B. Brown and Jack Clark and Nicholas Joseph and Benjamin Mann and Sam McCandlish and Christopher Olah and Jared Kaplan},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.05221},
  url={https://api.semanticscholar.org/CorpusID:250451161}
}





@inproceedings{
xie2022when,
title={When to Ask for Help: Proactive Interventions in Autonomous Reinforcement Learning},
author={Annie Xie and Fahim Tajwar and Archit Sharma and Chelsea Finn},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=L9EXtg7h6XE}
}

@inproceedings{
ren2023robots,
title={Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners},
author={Allen Z. Ren and Anushri Dixit and Alexandra Bodrova and Sumeet Singh and Stephen Tu and Noah Brown and Peng Xu and Leila Takayama and Fei Xia and Jake Varley and Zhenjia Xu and Dorsa Sadigh and Andy Zeng and Anirudha Majumdar},
booktitle={7th Annual Conference on Robot Learning},
year={2023},
url={https://openreview.net/forum?id=4ZK8ODNyFXx}
}





@inproceedings{
singh2022askhelp,
title={Ask4Help: Learning to Leverage an Expert for Embodied Tasks},
author={Kunal Pratap Singh and Luca Weihs and Alvaro Herrasti and Jonghyun Choi and Aniruddha Kembhavi and Roozbeh Mottaghi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_bqtjfpj8h}
}

@article{Nguyen2021LearningWA,
  title={Learning When and What to Ask: a Hierarchical Reinforcement Learning Framework},
  author={Khanh Nguyen and Yonatan Bisk and Hal Daum'e},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.08258},
  url={https://api.semanticscholar.org/CorpusID:239016703}
}



@inproceedings{Chi2019JustAA,
  title={Just Ask: An Interactive Learning Framework for Vision and Language Navigation},
  author={Ta-Chung Chi and Mihail Eric and Seokhwan Kim and Minmin Shen and Dilek Z. Hakkani-T{\"u}r},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:208527038},
  abstract={In the vision and language navigation task (Anderson et al. 2018), the agent may encounter ambiguous situations that are hard to interpret by just relying on visual information and natural language instructions. We propose an interactive learning framework to endow the agent with the ability to ask for users' help in such situations. As part of this framework, we investigate multiple learning approaches for the agent with different levels of complexity. The simplest model-confusion-based method lets the agent ask questions based on its confusion, relying on the predefined confidence threshold of a next action prediction model. To build on this confusion-based method, the agent is expected to demonstrate more sophisticated reasoning such that it discovers the timing and locations to interact with a human. We achieve this goal using reinforcement learning (RL) with a proposed reward shaping term, which enables the agent to ask questions only when necessary. The success rate can be boosted by at least 15% with only one question asked on average during the navigation. Furthermore, we show that the RL agent is capable of adjusting dynamically to noisy human responses. Finally, we design a continual learning strategy, which can be viewed as a data augmentation method, for the agent to improve further utilizing its interaction history with a human. We demonstrate the proposed strategy is substantially more realistic and data-efficient compared to previously proposed pre-exploration techniques.}
}

@inproceedings{liu2022asking,
  title={Asking for knowledge (afk): Training rl agents to query external knowledge using language},
  author={Liu, Iou-Jen and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Oudeyer, Pierre-Yves and Schwing, Alexander},
  booktitle={International Conference on Machine Learning},
  pages={14073--14093},
  year={2022},
  organization={PMLR}
}


@inproceedings{Xiao2022UncertaintyQW,
  title={Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis},
  author={Yuxin Xiao and Paul Pu Liang and Umang Bhatt and Willie Neiswanger and Ruslan Salakhutdinov and Louis-Philippe Morency},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247613322}
}

@inproceedings{
asai2024selfrag,
title={Self-{RAG}: Learning to Retrieve, Generate, and Critique through Self-Reflection},
author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=hSyW5go0v8}
}

@inproceedings{Paul2023REFINERRF,
  title={REFINER: Reasoning Feedback on Intermediate Representations},
  author={Debjit Paul and Mete Ismayilzada and Maxime Peyrard and Beatriz Borges and Antoine Bosselut and Robert West and Boi Faltings},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:257921623}
}

@inproceedings{qiao-etal-2024-making,
    title = "Making Language Models Better Tool Learners with Execution Feedback",
    author = "Qiao, Shuofei  and
      Gui, Honghao  and
      Lv, Chengfei  and
      Jia, Qianghuai  and
      Chen, Huajun  and
      Zhang, Ningyu",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.195/",
    doi = "10.18653/v1/2024.naacl-long.195",
    pages = "3550--3568",
    abstract = "Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed by further analysis, show that TRICE can make the large language model selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools."
}

@inproceedings{gou2024critic,
  title={{CRITIC}: Large Language Models Can Self-Correct with Tool-Interactive Critiquing},
  author={Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Yang, Yujiu and Duan, Nan and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@misc{wang2023shepherdcriticlanguagemodel,
      title={Shepherd: A Critic for Language Model Generation}, 
      author={Tianlu Wang and Ping Yu and Xiaoqing Ellen Tan and Sean O'Brien and Ramakanth Pasunuru and Jane Dwivedi-Yu and Olga Golovneva and Luke Zettlemoyer and Maryam Fazel-Zarandi and Asli Celikyilmaz},
      year={2023},
      eprint={2308.04592},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.04592}, 
}

@inproceedings{ke-etal-2024-critiquellm,
    title = "{C}ritique{LLM}: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
    author = "Ke, Pei  and
      Wen, Bosi  and
      Feng, Andrew  and
      Liu, Xiao  and
      Lei, Xuanyu  and
      Cheng, Jiale  and
      Wang, Shengyuan  and
      Zeng, Aohan  and
      Dong, Yuxiao  and
      Wang, Hongning  and
      Tang, Jie  and
      Huang, Minlie",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.704/",
    doi = "10.18653/v1/2024.acl-long.704",
    pages = "13034--13054",
    abstract = "Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4`s direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT."
}

% SELF-CORRECT PAPER:
ICLR 2023 Paper (https://iclr.cc/virtual/2023/poster/11103)
@inproceedings{welleck2023generating,
  title={Generating Sequences by Learning to Self-Correct},
  author={Welleck, Sean and Lu, Ximing and West, Peter and Brahman, Faeze and Shen, Tianxiao and Khashabi, Daniel and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year={2023}
}





@inproceedings{
anonymous2025training,
title={Training Language Models to Self-Correct via Reinforcement Learning},
author={Anonymous},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=CjwERcAU7w}
}


@inproceedings{
qu2024recursive,
title={Recursive Introspection: Teaching Language Model Agents How to Self-Improve},
author={Yuxiao Qu and Tianjun Zhang and Naman Garg and Aviral Kumar},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=DRC9pZwBwR}
}

@inproceedings{
liu2024agentbench,
title={AgentBench: Evaluating {LLM}s as Agents},
author={Xiao Liu and Hao Yu and Hanchen Zhang and Yifan Xu and Xuanyu Lei and Hanyu Lai and Yu Gu and Hangliang Ding and Kaiwen Men and Kejuan Yang and Shudan Zhang and Xiang Deng and Aohan Zeng and Zhengxiao Du and Chenhui Zhang and Sheng Shen and Tianjun Zhang and Yu Su and Huan Sun and Minlie Huang and Yuxiao Dong and Jie Tang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=zAdUB0aCTQ}
}

@inproceedings{
anonymous2025livecodebench,
title={LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code},
author={Anonymous},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=chfJJYC3iL}
}

@inproceedings{
chen2024teaching,
title={Teaching Large Language Models to Self-Debug},
author={Xinyun Chen and Maxwell Lin and Nathanael Sch{\"a}rli and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KuPixIqPiq}
}

@inproceedings{
lightman2024lets,
title={Let's Verify Step by Step},
author={Hunter Lightman and Vineet Kosaraju and Yuri Burda and Harrison Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=v8L0pN6EOi}
}

@misc{uesato2022solvingmathwordproblems,
      title={Solving math word problems with process- and outcome-based feedback}, 
      author={Jonathan Uesato and Nate Kushman and Ramana Kumar and Francis Song and Noah Siegel and Lisa Wang and Antonia Creswell and Geoffrey Irving and Irina Higgins},
      year={2022},
      eprint={2211.14275},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.14275}, 
}

@inproceedings{
xie2023selfevaluation,
title={Self-Evaluation Guided Beam Search for Reasoning},
author={Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and Min-Yen Kan and Junxian He and Qizhe Xie},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=Bw82hwg5Q3}
}

@inproceedings{
zelikman2022star,
title={{ST}aR: Bootstrapping Reasoning With Reasoning},
author={Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah Goodman},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_3ELRdg2sgI}
}


@inproceedings{
hu2024uncertainty,
title={Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in {LLM}s},
author={Zhiyuan Hu and Chumin Liu and Xidong Feng and Yilun Zhao and See-Kiong Ng and Anh Tuan Luu and Junxian He and Pang Wei Koh and Bryan Hooi},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=CVpuVe1N22}
}