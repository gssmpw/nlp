\section{Related Work}
\textbf{LLM agents }  Recent breakthroughs in LLM agents ____  have allowed the creation of AI systems which can complete a range of real-world tasks in an open-ended environment ____.
Most previous work on training such LLM agents focuses on SFT for tool-use ____, prompting closed-source LLMs ____, or applying RL in domains with a clear objective such as code generation or math ____. We instead focus on applying RL techniques to an environment with ambiguous instructions ____. Although previous works attempt to train LLM agents in such environments ____, they do not address how to enable them to request interventions, which is the central focus of our work.

\textbf{Safe and Trustworthy AI } Prior work on AI safety often focuses on \emph{value alignment}—ensuring AI systems follow human values ____—and \emph{AI security}—ensuring robustness to adversarial attacks ____. However, these alone may not guarantee safety in high-stakes contexts, where an agent’s limited \emph{capabilities} can lead to harmful failures (e.g., prescribing the wrong medicine ____). We therefore situate our work under the more expansive concpet of \emph{Trustworthy AI} ____, which includes the requirement that agents pursue tasks robustly without unintended failures. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/sif_task.pdf}
    \vspace{-2em}
    \caption{(a) A SIF task requires the agent to locate objects, interact with humans, and perform household tasks in a sequence of discrete actions. Assuming perfect visual perception, the relevant segment is highlighted in orange; states are represented in text. (b) A brief overview of Self-Regulation and Requesting Intervention, in comparison to the base agent.}
    \label{fig:sif}
    \vspace{-1em}
    %\vspace{-1em}
\end{figure*}

\textbf{Self-improvement for LLMs }Previous work in self-improvement has explored the potential of enhancing LLM responses. 
Environmental feedback____ and model-generated critiques____ have enabled models to perform better in subsequent iterations. Reward models combined with search algorithms further guide decoding toward better answers____. However, most such methods assume the model can inherently solve the task, with the challenge lying in eliciting that capability. When a task exceeds the model’s ability, intervention of more capable models/augmented compute is needed.

\textbf{Confidence Estimation \& Meta-cognition.} Meta-cognitive agents that recognize their own limitations can guide human trust and seek external knowledge to improve accuracy____. Previous work estimates confidence via semantic entropy____, logit values____, direct questioning____, or conformal prediction____. Although these methods can help decide when to intervene, their estimates are calculated from logits, and may be biased by training data and fail out-of-distribution____. Another approach is learning an RL policy that treats assistance seeking as an action____. In contrast, we use a process reward model with tabular RL to flexibly adapt to varying budget constraints without additional training.