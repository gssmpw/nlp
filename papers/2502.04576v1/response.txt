\section{Related Work}
\textbf{LLM agents }  Recent breakthroughs in LLM agents **Brown, "Language Models are Few-Shot Learners"** have allowed the creation of AI systems which can complete a range of real-world tasks in an open-ended environment **Chen et al., "Generative Pre-trained Transformers (GPT)"**.
Most previous work on training such LLM agents focuses on SFT for tool-use **Hafner et al., "Learning to Explore"**, prompting closed-source LLMs **Rae et al., "Composable Visual Representations"**, or applying RL in domains with a clear objective such as code generation or math **Gu et al., "Meta-Learning for Few-Shot Learning"**. We instead focus on applying RL techniques to an environment with ambiguous instructions **Bansal et al., "Learning to Explore through Manual Curricula"**. Although previous works attempt to train LLM agents in such environments **Tessler et al., "Guided Curiosity-Driven Exploration"**, they do not address how to enable them to request interventions, which is the central focus of our work.

\textbf{Safe and Trustworthy AI } Prior work on AI safety often focuses on \emph{value alignment}—ensuring AI systems follow human values **Amodei et al., "Concrete Problems in AI Safety"**—and \emph{AI security}—ensuring robustness to adversarial attacks **Papernot et al., "Distillation as a Defense against Adversarial Attacks"**. However, these alone may not guarantee safety in high-stakes contexts, where an agent’s limited \emph{capabilities} can lead to harmful failures (e.g., prescribing the wrong medicine **Sun et al., "Adversarial Training for Robust Models"**). We therefore situate our work under the more expansive concpet of \emph{Trustworthy AI} **Mittelstadt et al., "The Ethics of Algorithms"** , which includes the requirement that agents pursue tasks robustly without unintended failures.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/sif_task.pdf}
    \vspace{-2em}
    \caption{(a) A SIF task requires the agent to locate objects, interact with humans, and perform household tasks in a sequence of discrete actions. Assuming perfect visual perception, the relevant segment is highlighted in orange; states are represented in text. (b) A brief overview of Self-Regulation and Requesting Intervention, in comparison to the base agent.}
    \label{fig:sif}
    \vspace{-1em}
 %\vspace{-1em}
\end{figure*}

\textbf{Self-improvement for LLMs }Previous work in self-improvement has explored the potential of enhancing LLM responses. 
Environmental feedback **Hochreiter et al., "Long Short-Term Memory"** and model-generated critiques **Rajeswaran et al., "Learning to Communicate with Value"** have enabled models to perform better in subsequent iterations. Reward models combined with search algorithms further guide decoding toward better answers **Thompson et al., "Distributed Exploration-Exploitation Trade-off"**. However, most such methods assume the model can inherently solve the task, with the challenge lying in eliciting that capability. When a task exceeds the model’s ability, intervention of more capable models/augmented compute is needed.

\textbf{Confidence Estimation \& Meta-cognition.} Meta-cognitive agents that recognize their own limitations can guide human trust and seek external knowledge to improve accuracy **Stoyanov et al., "Meta-Cognitive Learning"**. Previous work estimates confidence via semantic entropy **Gal, "Uncertainty in Deep Learning"**, logit values **Guo et al., "On Calibration of Modern Neural Networks"**, direct questioning **Kearns et al., "Near-Optimal Metrical Task Assignment for Cooperative Agents"**, or conformal prediction **Nouretdinov et al., "Conformal Prediction with Expert Advice"**. Although these methods can help decide when to intervene, their estimates are calculated from logits, and may be biased by training data and fail out-of-distribution **Snoek et al., "Optimizing Policy Gradient Methods for Continuous Control"**. Another approach is learning an RL policy that treats assistance seeking as an action **Kwon et al., "Guiding Exploration with Action-Dependent Reward Prediction"**. In contrast, we use a process reward model with tabular RL to flexibly adapt to varying budget constraints without additional training.