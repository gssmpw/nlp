\section{Related Work}
\textbf{LLM agents }  Recent breakthroughs in LLM agents \citep{yao2023react,yang2024sweagent,shinn2024reflexion}  have allowed the creation of AI systems which can complete a range of real-world tasks in an open-ended environment \citep{nakano2021webgpt, schick2023toolformer, openhands}.
Most previous work on training such LLM agents focuses on SFT for tool-use \citep{schick2023toolformer}, prompting closed-source LLMs \citep{yang2024sweagent,wang2024executable,openhands}, or applying RL in domains with a clear objective such as code generation or math \citep{dubey2024llama,chen2024self}. We instead focus on applying RL techniques to an environment with ambiguous instructions \citep{min2025situated}. Although previous works attempt to train LLM agents in such environments \citep{zhai2024finetuning,gehring2024rlef}, they do not address how to enable them to request interventions, which is the central focus of our work.

\textbf{Safe and Trustworthy AI } Prior work on AI safety often focuses on \emph{value alignment}—ensuring AI systems follow human values \citep{gabriel2020artificial,christiano2017deep,bai2022constitutional}—and \emph{AI security}—ensuring robustness to adversarial attacks \citep{hendrycks2021unsolved,brundage2018malicious}. However, these alone may not guarantee safety in high-stakes contexts, where an agent’s limited \emph{capabilities} can lead to harmful failures (e.g., prescribing the wrong medicine \citep{ruan2024identifyingsandbox}). We therefore situate our work under the more expansive concpet of \emph{Trustworthy AI} \citep{diaz2023connectingtrustworthy}, which includes the requirement that agents pursue tasks robustly without unintended failures. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/sif_task.pdf}
    \vspace{-2em}
    \caption{(a) A SIF task requires the agent to locate objects, interact with humans, and perform household tasks in a sequence of discrete actions. Assuming perfect visual perception, the relevant segment is highlighted in orange; states are represented in text. (b) A brief overview of Self-Regulation and Requesting Intervention, in comparison to the base agent.}
    \label{fig:sif}
    \vspace{-1em}
    %\vspace{-1em}
\end{figure*}

\textbf{Self-improvement for LLMs }Previous work in self-improvement has explored the potential of enhancing LLM responses. 
Environmental feedback~\cite{gou2024critic,qiao-etal-2024-making,liu2024agentbench,chen2024teaching,anonymous2025livecodebench} and model-generated critiques~\cite{madaan2023selfrefine,wang2023shepherdcriticlanguagemodel,welleck2023generating,lin2024generating,qu2024recursive} have enabled models to perform better in subsequent iterations. Reward models combined with search algorithms further guide decoding toward better answers~\cite{nakano2021webgpt,uesato2022solvingmathwordproblems,zelikman2022star,xie2023selfevaluation,lightman2024lets}. However, most such methods assume the model can inherently solve the task, with the challenge lying in eliciting that capability. When a task exceeds the model’s ability, intervention of more capable models/augmented compute is needed.

\textbf{Confidence Estimation \& Meta-cognition.} Meta-cognitive agents that recognize their own limitations can guide human trust and seek external knowledge to improve accuracy~\cite{mallen-etal-2023-trust,asai2024selfrag}. Previous work estimates confidence via semantic entropy~\cite{kuhn2023semantic}, logit values~\cite{jiang-etal-2021-know,Kadavath2022LanguageM}, direct questioning~\cite{zhou-etal-2023-navigating,lin2024generating,xiong2024can}, or conformal prediction~\cite{ren2023robots}. Although these methods can help decide when to intervene, their estimates are calculated from logits, and may be biased by training data and fail out-of-distribution~\cite{Xiao2022UncertaintyQW,zhou-etal-2023-navigating}. Another approach is learning an RL policy that treats assistance seeking as an action~\cite{Chi2019JustAA,Nguyen2021LearningWA,liu2022asking,singh2022askhelp,xie2022when,hu2024uncertainty}. In contrast, we use a process reward model with tabular RL to flexibly adapt to varying budget constraints without additional training.