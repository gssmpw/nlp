\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc}
%\usepackage[danish]{babel}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage[breakable]{tcolorbox}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{breakcites}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[normalem]{ulem}
\usepackage{actuarialangle}
\usepackage[nottoc]{tocbibind}
\usepackage[mathscr]{euscript}
\usepackage{geometry}
\usepackage{bm}
\usepackage{bbm}
%\geometry{a4paper, left=25mm, right=25mm, bottom=34mm}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\sectionmark}[1]{\markright{#1}{}}
\fancyhead[L]{\color{gray} \nouppercase{\rightmark}}
\setcounter{tocdepth}{4}
%
\newtcolorbox{ntcolorbox}[1][]{%
    breakable
}
%
\newcommand*{\fakebreak}{\par\vspace{\textheight minus \textheight}\pagebreak}
\allowdisplaybreaks
%
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\usepackage{todonotes}
\newcommand{\NicoComm}[1]{\todo[linecolor=orange, backgroundcolor=orange,
bordercolor=yellow, textcolor=white, size=footnotesize]{#1}}
\newcommand{\MieComm}[1]{\todo[linecolor=purple, backgroundcolor=purple,
bordercolor=yellow, textcolor=white, size=footnotesize]{#1}}

\usepackage{makecell}
\setlength\parindent{0pt}
%
\begin{document}
%
%
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{defin}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{calc}[thm]{Calculation}
\newtheorem{choice}[thm]{Choice}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{ques}[thm]{Question}
\newtheorem{claim}[thm]{Claim}
\let\oldcalc\calc
\renewcommand{\calc}{\oldcalc\normalfont}
\let\oldexample\example
\renewcommand{\example}{\oldexample\normalfont}
\let\oldrmk\rmk
\renewcommand{\rmk}{\oldrmk\normalfont}
\numberwithin{equation}{subsection}

\newcommand{\bR}{\mathbb{R}}
\newcommand{\bT}{\mathbb{T}}

\newcommand\xqed[1]{%
  \leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
  \quad\hbox{#1}}
\newcommand\demo{\xqed{$\circ$}}
%
\fancyfoot[C]{\thepage{}}
%
\title{Signature Reconstruction from Randomized Signatures}
\author{\textbf{Mie Glückstad} \\ Mathematical Institute \\ University of Oxford \\ \emph{mie.gluckstad@exeter.ox.ac.uk} \and
\textbf{Nicola Muça Cirone}\\ Department of Mathematics \\ Imperial College London \\ \emph{n.muca-cirone22@imperial.ac.uk}
\and
\textbf{Josef Teichmann}\\ Department of Mathematics \\ ETH Zürich \\ \emph{jteichma@math.ethz.ch}}
\date{}
%
\maketitle
%
\begin{abstract}
\noindent Controlled ordinary differential equations driven by continuous bounded variation curves can be considered a continuous time analogue of recurrent neural networks for the construction of expressive features of the input curves. We ask up to which extent well known signature features of such curves can be reconstructed from controlled ordinary differential equations with (untrained) random vector fields. The answer turns out to be algebraically involved, but essentially the number of signature features, which can be reconstructed from the non-linear flow of the controlled ordinary differential equation, is exponential in its hidden dimension, when the vector fields are chosen to be neural with depth two. Moreover, we characterize a general linear independence condition on arbitrary vector fields, under which the signature features up to some fixed order can always be reconstructed. Algebraically speaking this complements in a quantitative manner several well known results from the theory of Lie algebras of vector fields and puts them in a context of machine learning.
\end{abstract}
%
\newpage
\tableofcontents
%
\newpage
%
\section{Introduction}
Controlled ordinary differential equation (CDEs)
$$
Y_t = y + \sum_{i=1}^d \int_0^t V_i(Y_s) d X^i_s
$$
driven by a continuous bounded variation input path $X: [0,T] \to \mathbb{R}^d $ are a continuous time analogue of recurrent neural networks. Here $V_i: \mathbb{R}^N \to \mathbb{R}^N$, $i=1,\ldots,d$ are smooth vector fields with globally bounded derivatives on $\mathbb{R}^N$. $N$ is referred to as the \emph{hidden dimension}. The vector fields correspond to the (recurrent) cell of the network. Each component of $Y$ is called a feature of $X$. In machine learning applications the regressions on features are performed to approximate non-linear functionals of $X$ up to time $T$. \\

It is an important question whether solutions of a fixed controlled differential equation at time $T$ provide, when one varies over all intial values $Y_0 = y$, an exhaustive set of (non-linear) features of $X$, where exhaustive refers to density properties in spaces of path space functionals. A more specific question is whether one can approximate by linear combinations of solutions $Y_T$ for different intial values of a given controlled differential equation, the well known signature features of $X$ (at time $T$), which are dense in multiple senses (see, e.g., the recent work \cite{CucSchTei:23}).\\

%Heuristically one can understand easily why controlled differential equations can provide an exhaustive set of (non-linear) featuers of $X$: by Norbert Wiener's version of Tauber's theorem the shifts $ \sigma(.+u)$ of a function $ \sigma \in L^1(\mathbb{R})$ span a dense subset if and only if the Fourier transform of $\sigma$ does not have a zero. Consider the controlled differential equation with three hidden dimensions for a smooth function $\sigma \in L^1(\mathbb{R})$ with nonwhere vanishing Fourier transform
%\begin{align*}
%d Y^0_t = 1 \, , \\
%d Y^1_t = \sigma(Y^0_t ) d X_t \, , \\
%d Y^2_t = \sigma'(Y^1_t) \sigma(Y^0_t) d X_t \, ,
%\end{align*}
%then linear combinations of $ Y^2_T = \sigma\big( Y^1_0 + \int_0^t \sigma(Y_0 +s) dX_s \big) $ for all $Y^1_0$


In \cite{AkyildirimTeichmann} it has been conjectured that the signature features, see Subsection \ref{subs:sigs}, of a bounded variation path $X: [0,T] \rightarrow \mathbb{R}^d$ can almost surely be reconstructed from its randomized signature, i.e., from the collection of solutions $(Y_T^y)_{y \in \mathbb{R}^N}$ at time $T$ to the controlled differential equation with randomly chosen vector fields of neural network type
$$ 
Y_t := y + \sum_{i=1}^d\int_0^t \sigma(A_i Y_s + b_i) dX_s^i \quad \text{ for } t \in [0,T],
$$
considered across all different initial values $y \in \mathbb{R}^N$. Here $A_i \in \mathbb{R}^{N \times N}$ and $b_i \in \mathbb{R}^N$ are samples of random matrices, and $\sigma$ is an activation function applied entry-wise. 
\\
Our original aim was to give a detailed proof of this conjecture, based on a sketch from \cite{AkyildirimTeichmann}, which suggested using a Taylor expansion to explicitly relate the signature components to the solutions of the randomized signature (see Section \ref{subsec:TaylorExp}). The sketch, however, relied on a crucial assumption of linear independence between the iterated vector fields generated by $V_i := \sigma(A_i \cdot + b_i)$ for $i \in \{1, \dots, d\}$, and this question of linear independence turned out to be algebraically richer and more delicate, than we had originally anticipated. In particular it is only true up to a certain depth of iterations but in general wrong (on finite dimensional input spaces). We also refer here to important algebraic or geometric work by \cite{Bah:21} and in particular \cite{Mol:87}, where natural identities of Lie polynomials are described and derived. Our work provides an alternative, more quantitative perspective on these seminal results. \\

%On the other hand, as outlined in Subsection \ref{subsec:directapproach}, there are certain controlled differential equations from which one can reconstruct the whole path and whose hidden dimension is finite (an example with hidden dimension is $d+4$ is provided).\\

In Section \ref{sec:linindep} we study the structure of the iterated vector fields for general choices of vector fields $V_1, \dots, V_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$, which need not be of the 'randomized signature form', and observe that they can be represented graphically as vertex- and edge-labelled planar recursive trees, inspired by the similar tree-like representations from \cite{mclachlan2017butcherseriesstoryrooted} and \cite{GUBINELLI}. Indeed, the question of linear independence of the iterated vector fields, boils down to a question of linear independence between the associated \emph{tree-like vector fields}, as defined in Section \ref{subsec:TreeVF}.\\

We illustrate by an example in Remark \ref{rmk:Dimensions}, that having linear independence between the iterated vector fields up to order $m$ may be entirely impossible (no matter the choice of vector fields) if the dimension $N$ of the input space is not chosen to be sufficiently large compared to $m$. We also see in Remark \ref{rmk:NoDepth} that the tree-like vector fields are, perhaps surprisingly, \emph{not} linearly independent, when $V_1, \dots, V_d$ are chosen to be of the classical randomized signature form.\\

In order to break the algebraic relations which arise between the tree-like vector fields in the randomized signature case, we add \emph{depth} to the randomized signature (see also Section \ref{subsec:RanSig}). Namely, we show that when $N \geq m-1$, choosing $V_1, \dots, V_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$ to be randomized signatures of \emph{depth two} with exponential activation function, we are able to obtain linear independence of the tree-like vector fields up to order $m$. In Section \ref{sec:SigRec} we outline the signature reconstruction results, inspired by the conjecture from \cite{AkyildirimTeichmann}, and see that it is possible to reconstruct the signature components up to order $m$ if linear independence between the tree-like vector fields up to order $m$ is satisfied. This, of course, in particular implies that we are able to reconstruct the signature components up to order $m$, for the randomized signature with depth two and exponential activation when $N \geq m-1$. We emphasize here that since the number of signature components of order $m$ is $d^m$, the bound $N \geq m-1$ is indeed logarithmically small in terms of the number of reconstructed siganture components (when $d \geq 2$). In other words: as the hidden dimension $N$ increases linearly, the number of signature components which can be reconstructed by this method increases exponentially.\\

In Section \ref{subsec:sigreconstructLie} we briefly discuss modifications of the signature reconstruction result to the case where the vector fields $V_1, \dots, V_d: G \rightarrow TG$ are defined on some finite-dimensional Lie group. Proving the linear independence results in this case should follow by analogous arguments to what we have done in Section \ref{sec:linindep}, and provides the added benefit of having the solutions of the differential equations contained on a (possibly compact) Lie group.
%
\section{Setting}
%
\subsection{Signatures and rough paths}
\label{subs:sigs}

The study of signatures originates from the theory of rough paths, see \cite{LyonsRCDE}, or the more introductory resources \cite{Allan}, \cite{LyonsRoughDE} \cite{FrizHairer} for an overview of this. 
Its reconstruction is of particular interest since by the classical Stone-Weierstrass theorem, linear functionals are dense in the space of continuous real-valued functions defined on compact sets of un-parameterized paths. 
This density property makes signature coefficients well-suited as feature representations for machine learning tasks involving sequential data \cite{fermanian2023new, cass2024lecture}, and as a consequence related techniques have been implemented across diverse domains. Their applications span deep learning \cite{kidger2019deep,  cirone2024theoretical, issa2024non, barancikova2024sigdiffusions}, kernel methods \cite{salvi2021signature, lemercier2021distribution}, and quantitative finance \cite{AkyildirimTeichmann, arribas2020sigsdes, horvath2023optimal, pannier2024path, cirone2025rough}. 
%Additionally, signature methods have proven valuable in information theory \cite{salvi2023structure, shmelev2024sparse}, cybersecurity applications \cite{cochrane2021sk}, and computational neuroscience \cite{holberg2024exact}, demonstrating their versatility across scientific disciplines.
\\
%
\\
Let $X: [0,T] \rightarrow \mathbb{R}^d$ be a Lipschitz continuous path and consider its real-valued coordinate functions $X^i: [0,T] \rightarrow \mathbb{R}$. The study of signatures deals with the iterated integals of these coordinate functions. Let $w_1, \dots, w_n \in \{1, \dots, d\}$ be indices and consider the corresponding \emph{word} $w= (w_1, \dots, w_n)$ given through the associated $n$-tuple. Then the iterated integral of $X$ associated to the word $w$ over the subinterval $[s,t] \subseteq [0,T]$ is given by
$$ \int_{\Delta_{[s,t]}^n} dX_r^w := \int_s^t \int_s^{r_n} \cdots \int_s^{r_3} \int_s^{r_2} dX_{r_1}^{w_1} dX_{r_2}^{w_2} \cdots dX_{r_n}^{w_n}, $$
where we denote the set of possible $n$-step interval partitions by
$$ \Delta_{[s,t]}^n :=  \{ s \leq r_1 \leq r_2 \leq \dots \leq r_n \leq t \} \quad \textrm{for each } n \in \mathbb{N}. $$
We denote by $\mathscr{W}_n := \{ w = (w_1, \dots, w_n) ~|~ w_1, \dots, w_n \in \{1, \dots, d\}\}$ the collection of words of a fixed length $n$, and set for each $w = (w_1, \dots, w_n) \in \mathscr{W}_n$
$$ e_w := e_{w_1} \otimes e_{w_2} \otimes \dots \otimes e_{w_n}, $$
where $e_{w_i}$ is the standard basis vector in $\mathbb{R}^d$ in direction $w_i$ for each $i \in \{1, \dots, n\}$. In this case, we can consider the collection of all $n$'th order iterated integrals as an element in the $n$'th order tensor space $(\mathbb{R}^d)^{\otimes n}$, given by
$$ \int_{\Delta_{[s,t]}^n} dX_{r_1} \otimes \cdots \otimes dX_{r_n} = \sum_{w \in \mathscr{W}_n} \int_{\Delta_{[s,t]}^n} dX_r^w~ e_w, $$
where we have considered the canonical tensor product on $\mathbb{R}^d$. The collection of all finite order iterated integrals of $X$ is referred to as the \emph{signature} of $X$ (with the convention that the zero'th order iterated integral is just 1). 
\begin{defin}[Signature]
The signature $S(X)$ of the path $X$ is a two-parameter function $S(X): \Delta_{[0,T]}^2 \rightarrow T((\mathbb{R}^d))$ given by
$$ S(X)_{s,t} := 1 + \sum_{n=1}^{\infty} \int_{\Delta_{[s,t]}^n} dX_{r_1} \otimes \dots \otimes dX_{r_n}, $$
for every $(s,t) \in \Delta_{[0,T]}^2$, where $T((\mathbb{R}^d))$ denotes the extended tensor algebra on $\mathbb{R}^d$. We refer to $\int_{\Delta_{[s,t]}^n} dX_{r_1} \otimes \cdots \otimes dX_{r_n}$ as the \emph{$n$'th order signature component of $X$}.
\end{defin}
%
Here, the extended tensor algebra $T((\mathbb{R}^d))$ should not be confused with the usual tensor algebra $T(\mathbb{R}^d)$ which is the countable direct sum $ T(\mathbb{R}^d) = \bigoplus_{n=0}^{\infty} (\mathbb{R}^d)^{\otimes n} = \mathbb{R} \oplus \mathbb{R}^d \oplus (\mathbb{R}^d)^{\otimes 2} \oplus (\mathbb{R}^d)^{\otimes 3} \oplus \dots$. Instead, the \emph{extended} tensor algebra can be considered as the closure of $T(\mathbb{R}^d)$, and we can in particular express it by
$$ T((\mathbb{R}^d)) := \left\{ (x_0, x_1, x_2, \dots) = \sum_{n=0}^{\infty} x_n ~\Big{|}~ \forall n \in \mathbb{N}_0: ~ x_n \in (\mathbb{R}^d)^{\otimes n} \right\}.$$
%
The most common setting is that of level two rough paths, in which a meaningful notion of integration is introduced for paths of Hölder-regularity $\alpha \in \left( \frac{1}{3}, \frac{1}{2} \right]$ to which both the Riemann-Stieltjes and Young integration theories do not apply. This is done by equipping these paths with an extra piece of information, the \emph{enhancement}, mimicking the behaviour of a second order iterated integral.
More precisely, the idea goes as follows: consider first a path $X$ of Hölder-regularity $\alpha \in \left( \frac{1}{2}, 1 \right]$, an appropriate twice continuously differentiable function $f$ and a sequence of partitions
$$ \pi^n = \{ s = r_0^n < r_1^n < \dots < r_{N_n}^n = t \} $$
with mesh $|\pi^n| \rightarrow 0$. Applying a second order Taylor expansion to $f$ and using the usual approximation results for Young integrals, it is seen that
\begin{equation}
\label{eq:roughpathidea}
 \int_s^t f(X_r) dX_r = \lim_{n \rightarrow \infty} \sum_{i=0}^{N_n - 1} f(X_{r_i^n})(X_{r_{i+1}^n} - X_{r_i^n}) + Df(X_{r_i^n})\int_{r_i^n}^{r_{i+1}^n} \int_{r_i^n}^u dX_u \otimes dX_r.
\end{equation}
This gives us an expression for the Young integral which does not just depend on the usual approximation terms in the first part of the sum, but also on terms which are expressed through the second order iterated integral of $X$.\\
When the path $X$ is of lower Hölder regularity ($\alpha \in \left( \frac{1}{3}, \frac{1}{2} \right]$) and the usual approximation of the integral via the sum $\sum_{i=0}^{N_n - 1} f(X_{r_i^n}) (X_{r_{i+1}^n} - X_{r_i^n})$ does not work, the idea is then to equip $X$ with an enhancement $\mathbb{X}$ (setting $\mathbf{X} = (X, \mathbb{X})$), and letting $\mathbb{X}$ play the role of the second order iterated integral in \eqref{eq:roughpathidea}. This is done, first of all, by ensuring that $\mathbb{X}$ behaves similarly to a second order iterated integral (through \emph{Chen's relation}), and secondly by imposing sufficient regularity conditions on $\mathbb{X}$ to ensure that the limit $\lim_{n \rightarrow \infty} \sum_{i=0}^{N_n - 1} f(X_{r_i^n})(X_{r_{i+1}^n} - X_{r_i^n}) + Df(X_{r_i^n}) \mathbb{X}_{r_i^n, r_{i+1}^n}$ exists and is well-defined, in which case this limit is referred to as a \emph{rough integral} and is denoted by $\int_s^t f(X_r) d\mathbf{X}_r$.\\ 

For paths of even lower Hölder regularity, $\alpha \in \left( 0, \frac{1}{3} \right]$, a similar approach can be implemented, this time using a higher order Taylor expansion and thus equipping the path $X$ with higher order enhancements, say, $\mathbb{X}^2, \dots, \mathbb{X}^n$ (where $\mathbb{X}^2$ is the second order enhancement from before), with each $\mathbb{X}^i$ mimicking the behaviour of the $i$'th order iterated integral $\int_{\Delta_{[\cdot, \cdot]}^i} dX_{r_1} \otimes \cdots \otimes dX_{r_i} $. In this case, the level $n$ rough path will precisely be the collection $\mathbf{X} = (X, \mathbb{X}^2, \dots, \mathbb{X}^n)$, mimicking the signature components up to order $n$.\\

 From the initial study of rough paths, the relationship between a path and its signature has been further explored, and it has been shown that many of the geometric properties of a path $X$ can in most cases be recovered from its signature $S(X)$. In some sense, the signature can be seen to \emph{encode} the information of the full underlying path $X$, when evaluated in its terminal value, up to pieces of $X$ "backtracking" onto themselves \cite{HamblyLyons}. It is also shown in \cite{LyonsInversion} that if $X: [0,T] \rightarrow \mathbb{R}^d$ is a $\mathcal{C}^1$-path, then the path $X$ can approximately be reconstructed from the signature $S(X)_{0,T}$ by an inversion method - and it is possible to explicitly quantify the error of this approximation. Inversion of signatures is also studied in \cite{chang2019insertion} and \cite{lyons2017hyperbolic}. Results such as these suggest that it will often be useful to study the signature $S(X)_{0,T}$ if we want to understand the underlying path $X$, and indeed signature-based methods have gained traction for applications in various fields, as outlined earlier. For an introductory overview of some applications of signatures in machine learning, see also \cite{ChevyrevK}.
%
\subsection{Taylor expansions}
\label{subsec:TaylorExp}
%
\subsubsection{The $\mathbb{R}^N$-case}
%
Let $X: [0,T] \rightarrow \mathbb{R}^d$ be a Lipschitz continuous path, and let $V_1, \dots, V_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$ be smooth vector fields. We consider for initial values $y \in \mathbb{R}^N$, controlled differential equations (CDEs) of the form
\begin{equation}
\label{eq:RNDE}
 Y_t = y + \sum_{i=1}^d \int_0^t V_i(Y_s) dX_s^i,
\end{equation}
and denote by $Y^y: [0,T] \rightarrow \mathbb{R}^N$ the unique solution associated to the initial value $y$ (supposing that the vector fields are chosen in a way such that a unique solution exists).\\

Let $g: \mathbb{R}^N \rightarrow \mathbb{R}$ be a smooth function, such that $g \in C^{\infty}(\mathbb{R}^N)$ with the notation from Appendix \ref{appendix:vectorfields}. By Proposition \ref{prop:mapXf}, we can then for each $i \in \{1, \dots, d\}$ apply $g$ to the vector field $V_i: \mathbb{R}^N \rightarrow \mathbb{R}^N$, in order to obtain a map $V_ig \in C^{\infty}(\mathbb{R}^N)$, which evaluates to:
$$ V_ig(y) = \underbrace{\nabla g(y)^T}_{1 \times n} \underbrace{V_i(y)}_{n \times 1} \quad \textrm{for every } y \in \mathbb{R}^N. $$
As $V_i g \in C^{\infty}(\mathbb{R}^N)$ we can again apply a vector field $V_j$ to this function, and this can be done repeatedly as many times as wanted. For any word $w=(w_1, \dots, w_k) \in \mathscr{W}_k$ and $g \in C^{\infty}(\mathbb{R}^N)$, we can thus construct a well-defined map $V_wg := V_{w_1} \cdots V_{w_k} g  \in C^{\infty}(\mathbb{R}^N)$ by this procedure. Indeed, $V_w: C^{\infty}(\mathbb{R}^N) \rightarrow C^{\infty}(\mathbb{R}^N)$ is seen to be the operator obtained by composing the derivations of the associated vector fields in the iteration, i.e. 
$$V_w = \mathscr{D}_{V_{w_1}} \circ \dots \circ \mathscr{D}_{V_{w_n}},$$
using the notation from Appendix \ref{appendix:vectorfields}.\\

Using a Picard iteration, e.g., as in \cite{BaudoinZhang}, we now obtain the following Taylor expansion.
%
\begin{thm}
\label{thm:BaudoinZhang}
Let $Y: [0,T] \rightarrow \mathbb{R}^N$ denote the unique solution to the differential equation \eqref{eq:RNDE} with initial value $y \in \mathbb{R}^N$, and let $g \in C^{\infty}(\mathbb{R}^N)$. Then:
\begin{align*} 
g(Y_t) = g(y) + \sum_{k=1}^{\infty} \sum_{w \in \mathscr{W}_k} V_w g(y)\int_{\Delta_{[0,t]}^k} dX_r^w \quad \textrm{for every } t \in [0,T].
\end{align*}
\end{thm}
%
\begin{rmk}
\label{rmk:TaylorConv}
Convergence of the Taylor expansion in Theorem \ref{thm:BaudoinZhangLie} is not needed for our results in Section \ref{subsec:sigreconstruct}, as any remainder terms from the Picard iteration disappear when taking derivatives and evaluating in $r =0$ under our reparametrization method proposed in Section \ref{subsec:sigreconstruct}. As such, we will refrain from going into a lengthy discussion of any such convergence. For a discussion of the convergence radius of the Taylor expansion, see for instance \cite{BaudoinZhang}.
\demo
\end{rmk}
%
\subsubsection{The Lie group case}
%
A similar construction can be made when considering smooth vector fields $V_1, \dots, V_d: G \rightarrow TG$ on a finite-dimensional Lie group $G$. In this case, we consider for initial values $z \in G$, CDEs of the form
\begin{equation}
\label{eq:LieDE} 
Z_t = z + \sum_{i=1}^d \int_0^t V_i(Z_s) dX_s^i \quad \textrm{for every } t \in [0,T].
\end{equation}
If the vector fields are chosen appropriately, such that a unique solution exists, it is well-known that this solution takes value on the Lie group $G$. We denote by $Z^z: [0,T] \rightarrow G$ the unique solution associated to the initial value $z$. When $G$ is a matrix Lie group, and the vector fields $V_1, \dots, V_d$ are chosen to be linear maps, the solutions $(Z^z)_{z \in G}$ are referred to as \emph{path-developments} on the Lie group $G$, see for instance \cite{Lou}.\\

As previously, we can apply the vector fields $V_1, \dots, V_d$ repeatedly to any function $g \in C^{\infty}(G)$ using the construction from Proposition \ref{prop:mapXf}. This yields, following a Picard iteration similar to the one in \cite{BaudoinZhang}, a Taylor expansion as in the $\mathbb{R}^N$-case.
\begin{thm}
\label{thm:BaudoinZhangLie}
Let $Z: [0,T] \rightarrow G$ denote the unique solution to the differential equation \eqref{eq:LieDE} with initial value $z \in G$. Then it holds for any $g \in C^{\infty}(G)$ that
$$ Z_t = z + \sum_{k=1}^{\infty} \sum_{w \in \mathscr{W}_k} V_w g (z) \int_{\Delta_{[0,t]}^k} dX_r^w \quad \textrm{for every } t \in [0,T]. $$
\end{thm}
%
\begin{rmk}
    As in the $\mathbb{R}^N$-case, we have here slightly abused notation without taking into account the convergence of the Taylor expansion. As in the $\mathbb{R}^N$-case, our approach in Section \ref{subsec:sigreconstructLie} does not require the Taylor expansion to be convergent.
    \demo
\end{rmk}
%
\subsection{Randomized signatures with depth}
\label{subsec:RanSig}
%
Let $X: [0,T] \rightarrow \mathbb{R}^d$ be a Lipschitz continuous path, and let $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ be a real analytic function with infinite radius of convergence, different from the null-map (see e.g., \cite{Krantz} for a detailed discussion of real analytic functions). We apply $\sigma$ component-wise to vectors, such that
$$ \sigma(x_1, \dots, x_N) = \left( \sigma(x_1), \dots, \sigma(x_N)\right) \quad \textrm{for every } x = (x_1, \dots, x_N) \in \mathbb{R}^N.$$
Suppose that $A_1, \dots, A_d$ are random matrices in $\mathbb{R}^{N \times N}$ and $b_1, \dots, b_d$ are random vectors in $\mathbb{R}^N$, generated in a way such that all entries are i.i.d. with distribution absolutely continuous with respect to the Lebesgue measure. Define for each $i \in \{1, \dots, d\}$, the random neural vector field $V_i: \mathbb{R}^N \rightarrow \mathbb{R}^N$ by:
    $$ V_i(x) = \sigma(A_i x + b_i) \quad \text{for every } x \in \mathbb{R}^N. $$
Consider for each $y \in \mathbb{R}^N$ the random differential equation from \eqref{eq:RNDE}:
$$ Y_t^y = y + \sum_{i=1}^d \int_0^t \sigma(A_i Y_s + b_i) ~dX_s^i \quad \text{for } t \in [0,T].$$
Then we call the collection of solutions $(Y^y)_{y \in \mathbb{R}^N}$ a \emph{randomized signature of $X$}. Randomized signatures were originally introduced in \cite{CuchieroTeichmann1} and \cite{CuchieroTeichmann2}, and have later been studied and applied in works such as \cite{AkyildirimTeichmann}, \cite{Gonon24}, \cite{CompagnoniTeichmann}, \cite{MucaCirone23}, and \cite{Gruber23}. \\

For our purposes here, the shift-vectors $b_1, \dots, b_d$ will not be of much use, and so we ignore them. Inspired by neural activation functions, we can of course add depth to the vector fields $V_1, \dots, V_d$. Let $\sigma$ and $A_1, \dots, A_d$ be given as before, and let $D_1, \dots, D_d$ be random \emph{diagonal} matrices in $\mathbb{R}^{N \times N}$ with i.i.d. entries having distribution absolutely continuous with respect to the Lebesgue measure. Then we can for each $i \in \{1, \dots, d\}$, associate a random \emph{depth two} neural vector field $V_i: \mathbb{R}^N \rightarrow \mathbb{R}^N$ given by:
\begin{equation}
\label{eq:VFDepth}
    V_i(x) = \sigma(A_i \sigma(D_ix)) \quad \text{for every } x \in \mathbb{R}^N.
\end{equation}
Considering as before the random differential equations
$$ Y_t^y =  y + \sum_{i=1}^d \int_0^t \sigma(A_i\sigma(D_iY_s)) ~dX_s^i \quad \text{for } t \in [0,T],$$
we refer to the collection of solutions $(Y^y)_{y \in \mathbb{R}^N}$ as a randomized signature of $X$ with depth two, or simply as a \emph{randomized signature with depth}.

\newpage
%
\section{Linear independence of iterated vector fields on $\mathbb{R}^N$}
\label{sec:linindep}

From now on consider $d$ fixed smooth vector fields $\{V_i : \bR^N \to \bR^N ~|~ i=1,\dots,d ~\}$, and let $\mathscr{W}$ be the set of words in the alphabet $\{1,\dots,d\}$. Our main results will rely on utilizing the Taylor expansions from Section \ref{subsec:TaylorExp} to relate the collection of solutions $(Y^y)_{y \in \mathbb{R}^N}$ of a CDE \eqref{eq:RNDE} to the iterated integrals (i.e., signature components) of the underlying path $X$. In order to do this, we will need to study the iterated vector fields $V_w: C^{\infty}(\mathbb{R}^N) \rightarrow C^{\infty}(\mathbb{R}^N)$ which arise in the Taylor expansions. In particular, we will want to show that these operators are \emph{linearly independent} for words of the same length. We note of course, that the iterated vector fields can be defined recursively on word length, by setting for $i \in \{1, \dots, d\}$, $x \in \mathbb{R}^N$, $w \in \mathscr{W}$, and $\phi \in C^{\infty}(\bR^N)$:
        \begin{gather*}
            V_{i}\phi(x) = (\nabla \phi(x))^T V_i(x), \quad 
            V_{wi}\phi(x) := V_{i}(V_w\phi)(x)= (\nabla (V_w\phi)(x))^T V_i(x),
        \end{gather*}
        with the convention that in the one-letter case (i.e., $w=i$ for some $i$), $V_i$ may refer to either a vector field $\mathbb{R}^N \rightarrow \mathbb{R}^N$ or an operator $C^{\infty}(\mathbb{R}^N) \rightarrow C^{\infty}(\mathbb{R}^N)$, depending on the context.\\


As it is clear from an attempt at writing the explicit formulation of $V_{w}$ where $|w| \geq 3$, the form of the $V_{w}$ quickly spirals out of control. 
Fortunately such terms present a rich combinatorial structure which can be leveraged to effectively handle the constituent parts.
The natural language for this is that of \emph{trees}, as is customary in the differential equations literature under the guise of \emph{Butcher} expansions \cite{mclachlan2017butcherseriesstoryrooted}, or in the related Branched Rough Paths of \cite{GUBINELLI}.
%
\subsection{Tree-like vector fields}
\label{subsec:TreeVF}
Start by recalling the notion of a \emph{labeled recursive tree of order $m$}: this is a connected graph $\tau$ with vertex set $\{1, \dots, m\}$, and precisely $m-1$ edges, which is built recursively by letting vertex 1 denote the root, and adding in step $l+1$ the vertex labeled $l+1$ to the existing tree by connecting it with an edge to one of the vertices $\{1, \dots, l\}$. In addition to the natural labeling of a recursive tree, we will add extra vertex labels corresponding to letters from the alphabet $\{1, \dots, d\}$. Given a recursive tree of order $m$, we will typically just highlight these letter-labels by denoting them by $(w_1, \dots, w_m) \in \mathscr{W}_m$ such that the letter $w_l$ is the label of vertex $l$ for each $l \in \{1, \dots, m\}$. We denote by $\bT$ be the set of letter-labeled recursive trees, and note that these are {planar} graphs meaning that the order of the branches at any vertex is ignored (\emph{cf.} Figure \ref{fig:planar_tree_ex}).
\begin{figure}[h!]
        \centering
        \input{Figures/planar_tree_ex2}
        \caption{Example of a planar rooted tree in $\bT$ with vertices $\{1, \dots, 7\}$ labeled by the word $w=(w_1, \dots, w_7) \in \mathscr{W}_7$.}
        \label{fig:planar_tree_ex}
\end{figure}
\ \\
Given trees $\tau_1,\dots,\tau_k \in \bT$, let $[\tau_1 \cdots \tau_k]_{\bullet_i}$ be the tree obtained by attaching the roots of $\tau_1,\dots,\tau_k$ to a new vertex $\bullet_i$; then every $\tau \in \bT$ can be constructed by starting with its leaves and recursively using the "graphical" operation $[\cdot]$ (only attaching vertices of higher index to lower index vertices, from the top down). For example the tree of Figure \ref{fig:planar_tree_ex} can be constructed as 
$[[\bullet_{w_3}]_{\bullet_{w_2}} ~ [[\bullet_{w_6} ~ \bullet_{w_7}]_{\bullet_{w_5}}]_{\bullet_{w_4}}]_{\bullet_{w_1}}$. For any given word $w \in \mathscr{W}$, we denote by $\mathbb{T}_w$ the collection of letter-labeled recursive trees of order $|w|$ with letter-labels given by the letters of the word $w$. Formally, we define it as follows.
\begin{defin}[Letter-labeled recursive trees]
    Define the following sets of recursive trees with vertices labeled by $\{0, 1, \dots, d\}$: 
    \begin{itemize}
        \item For $i \in \{1, \dots, d\}$ let $\bT_{i} := \{ \bullet_i \}$.
        \item For $i \in \{1, \dots, d\}$ and $w \in \mathscr{W}$ define $\bT_{wi}$ from $\bT_{w}$ by attaching, one by one, to the vertices of its trees a new edge having leaf $\bullet_i$ \emph{cf.} Figure \ref{fig:tree_spaces}.
    \end{itemize}
\end{defin}

\begin{figure}[h!]
        \centering
        \input{Figures/tree_spaces2}
        \caption{Left to right, the spaces $\bT_{w_1}$, $\bT_{w_1w_2}$, $\bT_{w_1w_2w_3}$, and $\bT_{w_1w_2w_3w_4}$. }
        \label{fig:tree_spaces}
\end{figure}
Denote by $\pi_1, \dots, \pi_N: \mathbb{R}^N \rightarrow \mathbb{R}$ the canonical coordinate projections, $\pi_i: x \mapsto x_i$.

\begin{defin}[Tree-like vector fields]
\label{def:TreeLikeVF}
    To each tree $\tau \in \bT$ we associate the vector field $V_{\tau}: \bR^N \to \bR^N$ defined recursively, for $i \in \{1, \dots, d\}$, and  $\tau_1, \dots, \tau_k \in \mathbb{T}$, as 
    \begin{align}
        V_{\bullet_i}(x) &= V_i(x),
        \\ \label{eq:TreeLikeVF}
        V_{[\tau_1 \cdots \tau_k]_{\bullet_i}}(x) &= \sum_{j_1,\dots, j_k=1}^N \frac{\partial^k}{\partial x_{j_k} \cdots \partial x_{j_1}}V_i(x) \cdot \pi_{j_1}(V_{\tau_1}(x)) \cdots \pi_{j_k}(V_{\tau_k}(x))        .
    \end{align}

    For each fixed $m \in \mathbb{N}$ we call $\{ V_{\tau} ~|~ \tau \in \mathbb{T}_w, w \in \mathscr{W}_m \}$ the \emph{collection of tree-like vector fields of order $m$}. Note how $V_{[\tau_1 \cdots \tau_k]_{\bullet_i}}(x) = ( d^k V_i )_x [V_{\tau_1}(x), \cdots, V_{\tau_k}(x)]$.
\end{defin}
Letting $\tau$ be given as in Figure \ref{fig:planar_tree_ex}, we see that the associated vector field is:
\begin{align*}
    V_{\tau}(x) = \sum_{j_1, \dots, j_6=1}^N& \frac{\partial^2}{\partial x_{j_3} \partial x_{j_1}} V_{w_1}(x) \cdot \pi_{j_1}\left(\frac{\partial}{\partial x_{j_2}} V_{w_2}(x)\right) \cdot \pi_{j_2}(V_{w_3}(x)) \cdot \pi_{j_3}\left(\frac{\partial}{\partial x_{j_4}}V_{w_3}(x)\right)\\
    &\quad \cdot \pi_{j_4}\left(\frac{\partial^2}{\partial x_{j_5} \partial x_{j_6}} V_{w_5}(x) \right) \cdot \pi_{j_5}(V_{w_6}(x)) \cdot \pi_{j_6}(V_{w_7}(x))
\end{align*}
\begin{figure}[h!]
  \centering
  \input{Figures/treeSketch1}
  \caption{The planar rooted tree from Figure \ref{fig:planar_tree_ex} with assigned edge-directions $j_1, \dots, j_6 \in \{1, \dots, N\}$.}
\end{figure}
\\
Let $w \in \mathscr{W}_m$, let $\tau \in \mathbb{T}_m$ and let $\mathbf{j} = (j_1, \dots, j_{m-1}) \in \{1, \dots, N\}^{m-1}$ be a set of edge directions arising in the expression of $V_{\tau}$ obtained from \ref{eq:TreeLikeVF}. We will order the elements in $\mathbf{j}$ such that $j_k$ denotes the $k$'th edge added to the tree in its recursive construction. The edge directions $\mathbf{j}$ then have the following interpretation in the expression for $V_{\tau}$: Consider for some $l \in \{1, \dots, m\}$, the $l$'th vertex of the tree $\tau$, with associated label $w_l \in \{1, \dots, d\}$. Then:
\begin{itemize}
    \item The ingoing edge into vertex $l$ (label $w_l$) will always have direction $j_{l-1}$, and determines the direction of the coordinate projection $\pi_{j_{l-1}}$ associated to $V_{w_{l}}$ in the expression of $V_{\tau}$;
    \item The directions of the outgoing edges of a vertex $w_l$ determine the directions in which derivatives are taken with respect to $V_{w_l}$ in the expression of $V_{\tau}$.
\end{itemize}
%
\subsection{Linear independence of iterated vector fields}
%
We will use the association between trees and vector fields to gain an explicit expansion of the operators $V_w$. Here, the tree-like vector fields $V_{\tau}$ will come in handy, but first we will need to appropriately 'lift' them into actual operators on $C^{\infty}(\mathbb{R}^N)$. Let $w =(w_1, \dots, w_m) \in \mathscr{W}_m$ be a fixed word, and denote by $\mathbb{T}_w^0$ the collection of letter-labeled recursive trees of order $m+1$ with the root vertex having label $0$ and the remaining vertices having labels $(w_1, \dots, w_m)$. Let $\mathbb{T}^0 = \{ \tau ~|~ \tau \in \mathbb{T}_w^0 \text{ for some } w \in \mathscr{W}\}$ be the collection of all such letter-labeled recursive trees.
\begin{figure}[h!]
        \centering
        \input{Figures/tree_spaces}
        \caption{Left to right, the spaces $\bT_{w_1}^0$, $\bT_{(w_1,w_2)}^0$, $\bT_{(w_1, w_2, w_3)}^0$.
        }
        \label{fig:tree_spaces}
\end{figure}
%
\begin{defin}[Tree-like operators]
Let $\tau^0 \in \mathbb{T}^0$. Then $\tau^0 = [\tau_1  \cdots  \tau_k]_{\bullet_0}$ for some $\tau_1, \dots, \tau_k \in \mathbb{T}$. We define an associated operator $V_{\tau^0}: C^{\infty}(\mathbb{R}^N) \rightarrow C^{\infty}(\mathbb{R}^N)$ by setting:
\begin{align*}
    V_{\bullet_0} \phi(x) &= \phi(x) \quad \text{and} \\ V_{[\tau_1 \cdots \tau_k]_{\bullet_0}}\phi(x) &= \sum_{j_0, \dots, j_{k-1} = 1}^N \frac{\partial^k}{\partial x_{j_{k-1}} \cdots \partial x_{j_0}} \phi(x) \cdot \pi_{j_0}(V_{\tau_1}(x)) \cdots \pi_{j_{k-1}}(V_{\tau_k}(x))
\end{align*}
for all $\phi \in C^{\infty}(\mathbb{R}^N)$ and $x \in \mathbb{R}^N$. Here, $V_{\tau_1}, \dots, V_{\tau_k}$ denote the tree-like vector fields obtained from Definition \ref{def:TreeLikeVF}.
\end{defin}
%
With the addition of a root vertex $0$, we see that if $\tau^0 \in \mathbb{T}^0$ is a letter-labeled recursive tree of order $m+1$ with labels $(0, w_1, \dots, w_m)$, then it can be expressed as a sum over edge directions $\mathbf{j} = (j_0, \dots, j_{m-1}) \in \{1, \dots, N\}^m$ where $j_1, \dots, j_{m-1}$ play the same role as before, and $j_0$ denotes the first edge going out of the root $0$. The following lemma allows us to express the operator $V_w$ as an expansion of tree-like operators associated to the word $w$.

\begin{lemma}
\label{lemma:Nicola1}
We can expand, for $w \in \mathscr{W}$, the operators $V_w$ in terms of tree-like operators as
\begin{equation}
    V_w = \sum_{\tau^0 \in \bT_w^0} V_{\tau^0}.
\end{equation}
\end{lemma}
\begin{proof}
    This follows from a simple application of the chain rule and an induction on word length.
\end{proof}

\begin{example}
As an example, we see that for $w_1, w_2 \in \{1, \dots, d\}$ and $\phi \in C^{\infty}(\mathbb{R}^N)$:
    $$
    V_{w_1}\phi(x) = (\nabla \phi(x))^T V_{w_1}(x) = \sum_{j_0=1}^N \frac{\partial}{\partial x_{j_0}}\phi(x) \cdot \pi_{j_0}(V_{w_1}(x)) = V_{[\bullet_{w_1}]_{\bullet_0}}\phi(x)$$
    and
    \begin{align*}
    V_{w_1w_2}\phi(x) &= (\nabla V_{w_1}\phi(x))^T V_{w_2}(x) \\
    &= \sum_{j_0,j_1=1}^N \frac{\partial^2}{\partial x_{j_1} \partial x_{j_0}} \phi(x) \cdot \pi_{j_0}(V_{w_1}(x)) \cdot \pi_{j_1}(V_{w_2}(x))\\ 
    &\qquad \qquad + \frac{\partial}{\partial x_{j_0}}\phi(x) \cdot \pi_{j_0}\left( \frac{\partial}{\partial x_{j_1}} V_{w_1}(x) \right) \cdot \pi_{j_1}(V_{w_2}(x)) \\
    &= V_{[\bullet_{w_1}~ \bullet_{w_2}]_{\bullet_0}}\phi(x) +  V_{[[\bullet_{w_2}]_{\bullet_{w_1}}]_{\bullet_0}}\phi(x)  
    \end{align*}
    \demo 
\end{example}
%
\begin{prop}
\label{prop:LinIndep}
    Fix integers $d, N, m \geq 1$. Let $V_1, \dots, V_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$ be smooth vector fields, and suppose that the tree-like vector fields
    \begin{equation} 
    \label{eq:assumption}
    \{ V_{\tau}: \mathbb{R}^N \rightarrow \mathbb{R}^N ~|~ \tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m \}
    \end{equation}
    are linearly independent. Then it follows that the operators
    $$ \{V_w: C^{\infty}(\mathbb{R}^N) \rightarrow C^{\infty}(\mathbb{R}^N) ~|~ w \in \mathscr{W}_m \}$$
    are linearly independent.
\end{prop}
%
\begin{proof}
For any word $w \in \mathscr{W}_m$, we have by the "tree expansion" of $V_w$ from Lemma \ref{lemma:Nicola1} that:
$$ V_w = \sum_{\tau^0 \in \mathbb{T}_w^0} V_{\tau^0}$$
Start by observing that if $\tau^0 \in \mathbb{T}_w^0$, then the degree of the tree at the root (denoted by $\deg_{\tau^0}(0)$) determines the order of the differential operator applied to $\phi \in C^{\infty}(\mathbb{R}^N)$ in the expression of $V_{\tau^0}$. As the differential operators $\partial^k: C^{\infty}(\mathbb{R}^N) \rightarrow C^{\infty}(\mathbb{R}^N)$ for different $k \in \mathbb{N}$ are linearly independent, it will be sufficient for us to establish linear independence of the tree-like operators associated to the first order differential operator $\partial^1 : C^{\infty}(\mathbb{R}^N) \rightarrow C^{\infty}(\mathbb{R}^N)$. Set $\Pi := \{\tau^0 \in \mathbb{T}^0 ~|~ \tau^0 \in \mathbb{T}_w^0, ~ w \in \mathscr{W}_m, ~ \deg_{\tau^0}(0) = 1 \}$. Then the tree-like operators associated to $\partial^1$ are precisely given by the collection $\{V_{\tau^0} ~|~ \tau^0 \in \Pi \}$.
\begin{figure}[h!]
  \centering
  %\input{Figures/TreeSketch3}
  \includegraphics[width=0.7\textwidth]{Figures/Fig5.png}
  \caption{Form of the trees $\tau^0 \in \Pi$. }
  \label{fig:TreeSketch3}
\end{figure}
\ \\
By the recursive tree construction, the first order differential operator must always arise from first edge attached to the root, and thus have direction $j_0$. As no more edges can be added to the root without changing the order of the differential operator, a recursive tree is now built from the first vertex attached to the root. In other words, if $\tau^0 \in \mathbb{T}_w^0 \cap \Pi$, then $\tau^0 = [\tau]_{\bullet_0}$ for some $\tau \in \mathbb{T}_w$ which uniquely determines $\tau^0$, as seen on Figure \ref{fig:TreeSketch3}. We thus see that for all $\phi \in C^{\infty}(\mathbb{R}^N)$ and $x \in \mathbb{R}^N$:
$$ V_{\tau^0}\phi(x) = (\nabla \phi(x))^T V_{\tau}(x) = \sum_{j_0=1}^N \frac{\partial}{\partial x_{j_0}} \phi(x) \cdot \pi_{j_0}(V_{\tau}(x)) \quad \text{for all } \tau^0 \in \Pi,$$
and so linear independence of the vector fields $\{V_{\tau} ~|~ \tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m\}$ directly implies linear independence of the operators $\{V_{\tau^0} ~|~ \tau^0 \in \Pi\}$. As
$$ V_w = \underbrace{\sum_{\tau^0 \in \mathbb{T}_w^0 \cap \Pi} V_{\tau^0}}_{(1)} + \underbrace{\sum_{\tau^0 \in \mathbb{T}_w^0 \cap \Pi^c} V_{\tau^0}}_{(2)}$$
with all operators of type $(1)$ being linearly independent from all operators of type $(2)$, and all operators of type $(1)$ being linearly independent across different words $w \in \mathscr{W}_m$, we conclude that the operators $\{V_w ~|~ w \in \mathscr{W}_m\}$ must be linearly independent, as desired.
\end{proof}
%
In Section \ref{subsec:NestedExpo} we will introduce a class of vector fields, namely the ones obtained through \emph{nested exponentials}, which satisfy assumption \eqref{eq:assumption} when chosen such that $N \geq m-1$. The overall approach here should be generally applicable to other choices of vector fields as well. For every word $w \in \mathscr{W}_m$ and every associated letter-labeled recursive tree $\tau \in \mathbb{T}_w$, denote for each $\mathbf{j} = (j_1, \dots, j_{m-1}) \in \{1, \dots, N\}^{m-1}$ the associated term in the expression of $V_{\tau}$ from Definition \ref{def:TreeLikeVF} by $V_{\tau}^{\mathbf{j}}$, such that
\begin{equation} 
\label{eq:JExpansion}
V_{\tau} = \sum_{\mathbf{j} = (j_1, \dots, j_{m-1})} V_{\tau}^{\mathbf{j}}.
\end{equation}
The approach is then as follows:
\begin{enumerate}
    \item Take $N \geq m-1$, and consider the (non-empty) set of edge directions $\mathbf{j} = (j_1, \dots, j_{m-1}) \in \{1, \dots, N\}^{m-1}$ with $j_1, \dots, j_{m-1}$ distinct;
    \item Let $\tau \in \mathbb{T}_w^0$, and let $\mathbf{j} = (j_1, \dots, j_{m-1})$ have distinct directions. Then even if the word $w$ has the same letter appearing multiple times, the associated vertices can be distinguished in their vector-field form, since the in- and out-going edge directions will be distinct (corresponding to having different directions of the projections and derivatives of the associated vector fields);
    \item If the vector fields $V_1, \dots, V_d$ are chosen suitably, then the previous point should be enough to show that the vector fields
     $$ \left\{ V_{\tau}^{\mathbf{j}}: \mathbb{R}^N \rightarrow \mathbb{R}^N ~\Big{|}~ \begin{array}{l}
    \mathbf{j} = (j_1, \dots, j_{m-1}) \in \{1, \dots, N\}^{m-1} \text{ with } j_1, \dots, j_{m-1} \text{ distinct} \\
    \tau \in \mathbb{T}_w \text{ for some } w \in \mathscr{W}_m
    \end{array} \right\}$$
    are linearly independent among themselves, and indeed also linearly independent from 
    $$\left\{ V_{\tau}^{\mathbf{j}}: \mathbb{R}^N \rightarrow \mathbb{R}^N ~\Big{|}~ \begin{array}{l}
    \mathbf{j} = (j_1, \dots, j_{m-1}) \in \{1, \dots, N\}^{m-1} \text{ with } j_1, \dots, j_{m-1} \text{ not distinct} \\
    \tau \in \mathbb{T}_w \text{ for some } w \in \mathscr{W}_m
    \end{array} \right\}.$$
\end{enumerate}
The third point will then be sufficient to conclude that the vector fields $\{V_{\tau} ~|~ \tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m\}$ are linearly independent, using the expansion \eqref{eq:JExpansion} and a similar argument to the one used in the proof of Proposition \ref{prop:LinIndep}.
%
\begin{rmk}
\label{rmk:Dimensions}
    In general a dimensional requirement relating $N$ and $m$ is needed in order for the vector fields $V_1, \dots, V_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$ to satisfy the assumption \eqref{eq:assumption}. Consider for instance the case where $m = 3$, $N = 1$ and $d = 3$. Then for $i,j,k \in \{1,2,3\}$ we have 
    \begin{align*}
        V_{kji} =& ( V_i'' V_j V_k +  V_i' V_j' V_k) \partial_x  
        + ( 2V_i' V_j V_k + V_i V_j' V_k) \partial^2_x
    + (V_i V_j V_k) \partial^3_x
    \end{align*}
    and the non-trivial relationship
    \[
    V_{123}+V_{231}+V_{312} = V_{132}+V_{213}+V_{321} \, ,
    \]
    so $\{V_w: C^{\infty}(\mathbb{R}) \rightarrow C^{\infty}(\mathbb{R}) ~|~ w \in \mathscr{W}_3\}$ is \emph{not} linearly independent (no matter our choice of vector fields $V_1, \dots, V_d: \mathbb{R} \rightarrow \mathbb{R}$).\\
    
    The relationship needed between $N$ and $m$ to avoid breaking the possibility of linear independence between the operators $\{V_w: C^{\infty}(\mathbb{R}^N) \rightarrow C^{\infty}(\mathbb{R}^N) ~|~ w \in \mathscr{W}_m\}$ is yet to be exactly determined - moreover, this relationship should also to some extent be dependent on the dimension $d$. For instance, if $d = 1$ then linear independence can easily be obtained for all $N, m \in \mathbb{N}$ (given suitable choices of vector fields). As noted earlier, we shall see in Section \ref{subsec:NestedExpo} that the assumption $N \geq m-1$ (for any choice of $d \in \mathbb{N}$) will also be sufficient to construct a working example where the operators $\{V_w ~|~ w \in \mathscr{W}_m\}$ become linearly independent.
    \demo
\end{rmk}
%
\subsection{Example: Nested exponentials}
\label{subsec:NestedExpo}
%
First, recall the definition of algebraic independence over the field of rational numbers $\mathbb{Q}$. For a more general algebraic definition, see e.g., \cite{Morandi}[Def. 19.1].
%
\begin{defin}[Algebraic independence over $\mathbb{Q}$]
Let $\alpha_1, \dots, \alpha_n \in \mathbb{R}$. We say that the set $\{\alpha_1, \dots, \alpha_n\}$ is \emph{algebraically independent over $\mathbb{Q}$} if
$$\forall p \in \mathbb{Q}[x_1, \dots, x_n] \setminus \{0\}: \quad p(\alpha_1, \dots, \alpha_n) \neq 0,$$
i.e., if $\alpha_1, \dots, \alpha_n$ cannot be made to cancel out through any polynomial equation with coefficients in $\mathbb{Q}$, other than of course the trivial zero polynomial $p \equiv 0$.\\

More generally, a set $A \subseteq \mathbb{R}$ is said to be \emph{algebraically independent over $\mathbb{Q}$}, if any finite subset of $A$ is algebraically independent over $\mathbb{Q}$. If a set is not algebraically independent over $\mathbb{Q}$, we say that it is \emph{algebraically dependent (over $\mathbb{Q}$)}.
\end{defin}
%
We will start by examining the random neural depth two vector fields given by \eqref{eq:VFDepth}, and then show that when $N \geq m-1$ and $\sigma(x) = \exp(x)$, these vector fields indeed satisfy the assumption of linear independence between tree-like vector fields of order $m$ from Proposition \ref{prop:LinIndep}. We note that if the random matrices $A_1, \dots, A_d$ and $D_1, \dots, D_d$ have entries which are i.i.d. with distribution absolutely continuous with respect to the Lebesgue measure, then all entries must almost surely be algebraically independent. It will thus be sufficient for us to study the setting where $A_i, D_i \in \mathbb{R}^{N \times N}$ are deterministic, and given by:
$$ A_i = \left( \begin{array}{cccc}
\alpha_{11}(i) & \alpha_{12}(i) & \dots & \alpha_{1N}(i) \\
\alpha_{21}(i) & \alpha_{22}(i) & \dots & \alpha_{2N}(i) \\
\vdots & \vdots & \ddots & \vdots \\
\alpha_{N1}(i) & \alpha_{N2}(i) & \dots & \alpha_{NN}(i)
\end{array}\right) \quad \text{and} \quad D_i = \left( \begin{array}{cccc}
d_1(i) & 0 & \dots & 0 \\
0 & d_2(i) & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & d_N(i)
\end{array}\right),$$
where the coefficients $\{ \alpha_{jk}(i) ~|~ i \in \{1, \dots, d\}, j,k \in \{1, \dots, N\}\} \cup \{d_j(i) ~|~ i \in \{1, \dots, d\}, j \in \{1, \dots, N\} \}$ are algebraically independent over $\mathbb{Q}$.
%
\subsubsection{Neural depth-two vector fields}
\label{subsub:depth-two}
Start by letting $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ be a real analytic function with infinite radius of convergence, and define the vector fields $V_1, \dots, V_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$ by:
$$ V_i(x) = \sigma(A_i \sigma(D_i x)) \quad \text{for } x \in \mathbb{R}^N.$$
As we know that the tree-like vector fields $\{V_{\tau} ~|~ \tau \in \mathbb{T}\}$ will be products of different order partial derivatives (and coordinate projections) of the vector fields $V_1, \dots, V_d$, we first make a brief observation about the form of these partial derivatives. Suppose that $N \geq m-1$, such that the edge directions $j_1, \dots, j_{m-1} \in \{1, \dots, N\}$ can be chosen to be distinct. Then we have for all $k \in \{1, \dots, N\}$, $i \in \{1, \dots, d\}$, and $j_1, \dots, j_{m-1} \in \{1, \dots, N\}$ distinct, that:
\begin{align*}
    \pi_k(V_i)(x) &= \sigma(\pi_k(A_i \sigma(D_ix))), \\
    \partial_{j_1} \pi_k(V_i)(x) &= \sigma'(\pi_k(A_i \sigma(D_i x))) \alpha_{kj_1} \sigma'(d_{j_1}(i) x_{j_1}), \\
    \frac{\partial^2}{\partial j_2 \partial j_1} \pi_k(V_i)(x) &= \sigma''(\pi_k(A_i\sigma(D_ix))) \alpha_{kj_1} \alpha_{kj_2} \sigma'(d_{j_1}(i)x_{j_1}) \sigma'(d_{j_2}(i)x_{j_2}), \\
    \vdots \quad &\qquad \vdots \\
    \frac{\partial^{(m-1)}}{\partial j_{m-1} \cdots \partial j_1} \pi_k(V_i)(x) &= \sigma^{(m-1)}(\pi_k(A_i\sigma(D_ix))) \prod_{l=1}^{m-1} \alpha_{kj_l} \sigma'(d_{j_l}(i) x_{j_l})
\end{align*}
Take some word $w=(w_1, \dots, w_m)$, some tree $\tau \in \mathbb{T}_w$, and a tuple of \emph{distinct} edge directions $\mathbf{j} = (j_1, \dots, j_{m-1}) \in \{1, \dots, N\}^{m-1}$. Then as the tree $\tau$ has $m-1$ edges, there will be $m-1$ partial derivatives taken in the expression of $V_{\tau, \mathbf{j}}$, and each of these will be taken in a different direction associated to one of the entries of $\mathbf{j}$. Denote by $\mathbf{i} = (i_1, \dots, i_{m-1})$ the indices of the vertices (i.e., the indices of the letters in $w \in \mathscr{W}_m$) with respect to which the derivative in each direction $(j_1, \dots, j_{m-1})$ is taken (see Figure \ref{fig:TreeSketch4}). Then, as we have by the recursive construction of trees that the edge with direction $j_1$ must necessarily be attached to the root vertex $w_1$, the edge with direction $j_2$ must be attached to one of the vertices $\{w_1, w_2\}$, the edge with direction $j_3$ must be attached to one of the vertices $\{w_1, w_2, w_3\}$, and so on, we must have that the vertex indices $\mathbf{i} = (i_1, \dots, i_{m-1})$ must be contained in the collection:
\begin{equation}
\label{eq:derivvector}
    \{(i_1, \dots, i_{m-1}) ~|~ i_1 \in \{1\}, i_2 \in \{1,2\}, i_3 \in \{1,2,3\}, \dots, i_{m-1} \in \{1, \dots, m-1\} \}.
\end{equation}
\begin{figure}[h!]
  \centering
  \input{Figures/treeSketch4}
  \caption{Illustration of how the indices $\mathbf{i} = (i_1, \dots, i_{m-1})$ are obtained in a specific recursive tree with $m = 5$.}
  \label{fig:TreeSketch4}
\end{figure}
\ \\
As each element in the collection \eqref{eq:derivvector} precisely determines the structure of a recursive tree (as it explicitly pinpoints the vertices to which each edge in the recursive construction is attached), the cardinality of the set $\eqref{eq:derivvector}$ is exactly the number of recursive trees with $m$ vertices. In particular a tree $\tau \in \mathbb{T}_w$, $w \in \mathscr{W}_m$ can be uniquely represented by the pair $(\mathbf{i}, w)$ (with the possible add-on of edge directions $\mathbf{j}$).\\

Suppose that we are given a pair $(\mathbf{i}, w) = ((i_1, \dots, i_{m-1}), (w_1, \dots, w_m))$ uniquely determining a letter-labeled recursive tree $\tau \in \mathbb{T}_w$ of order $m$, and let $\mathbf{j} = (j_1, \dots, j_{m-1})$ be distinct edge directions (supposing $N \geq m-1$). 
\begin{itemize}
    \item We denote for each vertex index $i \in \{1, \dots, m\}$ by $n_i$ the number of times the index appears in the $(m-1)$-tuple $\mathbf{i}=(i_1, \dots, i_{m-1})$. Then $n_i$ is precisely the number of out-going edges of the vertex with index $i$ in the tree $\tau$, and so $\sum_{i=1}^{m-1} n_i = m-1$. In the example on Figure \ref{fig:TreeSketch4} we would have $(n_1, n_2, n_3, n_4, n_5) = (1, 2, 0, 1, 0)$.

    \item We split the edge directions of $\mathbf{j}$ into tuples associated to each vertex, such that $\mathbf{j}^{(1)}, \dots, \mathbf{j}^{(m)}$ denote the edge directions going out of vertices $w_1, \dots, w_m$, respectively (with $\mathbf{j}^{(m)} = ()$ by construction). In the example on Figure \ref{fig:TreeSketch4} this would correspond to having $\mathbf{j}^{(1)} = (j_1)$, $\mathbf{j}^{(2)} = (j_2, j_3)$, $\mathbf{j}^{(3)} = ()$, $\mathbf{j}^{(4)} = (j_4)$, $\mathbf{j}^{(5)} = ()$, and by construction we naturally have that $|\mathbf{j}^{(i)}| = n_i$.
\end{itemize}
For every index $i \in \{1, \dots, m\}$ we can then write $\frac{\partial^{n_i}}{\partial x_{\mathbf{j}^{(i)}}}$ for the $n_i$'th order partial derivative operator taken in the directions determined by $\mathbf{j}^{(i)}$. If $n_i = 0$ (and so $\mathbf{j}^{(i)} = ()$) we let $\frac{\partial^{n_i}}{\partial x_{\mathbf{j}^{(i)}}}$ be the identity operator. By the recursive construction of $V_{\tau}$ from Definition \ref{def:TreeLikeVF} and the form of the partial derivatives from earlier, we now see that for any $j_0 \in \{1, \dots, N\}$:
\begin{align}
\label{eq:TreeVFExpression}
\begin{split}
    \pi_{j_0}(V_{\tau}^{\mathbf{j}}) &= \prod_{i=1}^m \frac{\partial^{n_i}}{\partial x_{\mathbf{j}^{(i)}}} \pi_{j_{i-1}}(V_{w_i}) \\
    &= \prod_{i=1}^m \sigma^{(n_i)}(\pi_{j_{i-1}}(A_{w_i} \sigma(D_{w_i}x))) \prod_{j \in \mathbf{j}^{(i)}} \alpha_{j_{i-1}j} \sigma'(d_j(w_i)x_j)\\
    &=  \sigma^{{\color{blue}(n_1)}}(\pi_{j_0}(A_{w_1}\sigma(D_{w_1}x))) \sigma^{{\color{blue}(n_2)}}(\pi_{j_1}(A_{w_2}\sigma(D_{w_2}x))) \cdots \sigma^{{\color{blue}(n_m)}}(\pi_{j_{m-1}}(A_{w_m}\sigma(D_{w_m}x)))\\
    &\qquad \times \alpha_{j_{i_1 -1}j_1}({\color{blue}w_{i_1}}) \alpha_{j_{i_2 - 1}j_2}({\color{blue}w_{i_2}}) \cdots \alpha_{j_{i_{m-1}-1}j_{m-1}}({\color{blue}w_{i_{m-1}}})\\
    &\qquad \times \sigma'(d_{j_1}({\color{blue}w_{i_1}})x_{j_1}) \sigma'(d_{j_2}({\color{blue}w_{i_2}})x_{j_2}) \cdots \sigma'(d_{j_{m-1}}({\color{blue}w_{i_{m-1}}})x_{j_{m-1}})
    \end{split}
\end{align}
%
\subsubsection{Nested exponentials}
%
Suppose now that $\sigma(x) = e^x$, such that the vector fields $V_1, \dots, V_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$ are given by
$$ V_i(x) = \exp(A_i \exp(D_i x)) \quad \text{for } x \in \mathbb{R}^N. $$
Then the first line in \eqref{eq:TreeVFExpression} will be the same for all $\tau \in \mathbb{T}_w$ (given fixed distinct edge directions $\mathbf{j}$), since $\sigma^{(n)}(x) = \exp(x) = \sigma(x)$ for all $n \in \mathbb{N}_0$. In particular, we get that:
\begin{align*}
    \pi_{j_0}(V_{\tau}^{\mathbf{j}}) &=  \exp\left( \sum_{i=1}^ m \pi_{j_{i-1}}(A_{w_i}\exp(D_{w_i}x))\right) \\
    &\qquad \times \alpha_{j_{i_1 -1}j_1}({\color{blue}w_{i_1}}) \alpha_{j_{i_2 - 1}j_2}({\color{blue}w_{i_2}}) \cdots \alpha_{j_{i_{m-1}-1}j_{m-1}}({\color{blue}w_{i_{m-1}}})\\
    &\qquad \times \exp\left( \sum_{n=1}^{m-1} d_{j_n}({\color{blue}w_{i_n}})x_{j_n} \right).
\end{align*}
The important parts of the expression here are the exponential factors (especially the one on the last line), as we do not care particularly about the coefficients in front when dealing with linear independence. As the constants $\{d_j(i) ~|~ j \in \{1, \dots, N\}, i \in \{1, \dots, d\}\} \cup \{\alpha_{kj}(i)~|~ k,j \in \{1, \dots, N\}, i \in \{1, \dots, d\}\}$ are algebraically independent over $\mathbb{Q}$, we see that each pair of $(m-1)$-tuples $(\mathbf{i}, \mathbf{j}) = ((i_1, \dots, i_{m-1}), (j_1, \dots, j_{m-1}))$ with $j_1, \dots, j_{m-1}$ distinct, will yield a \emph{unique} pair, consisting of the sum $\sum_{i=1}^m \pi_{j_{i-1}}(A_{w_i} \exp(D_{w_i}x)) = \sum_{j=1}^N \sum_{i=1}^m \alpha_{j_{i-1}j}(w_i) \exp(d_j(w_i)x_j)$ and a linear combination $\sum_{n=1}^{m-1} d_{j_n}(w_{i_n}) x_{j_n}$. As the functions $\{e^{c_1x_1 + \dots + c_{m-1} x_{m-1}} ~|~ c_1, \dots, c_{m-1} \in \mathbb{R}\}$ and $$\{ e^{\alpha_1 e^{\beta_1x_j} + \dots + \alpha_{(m-1)} e^{\beta_{m-1}x_j}} ~|~ \alpha_{1}, \dots, \alpha_{(m-1)}, \beta_{1}, \dots, \beta_{(m-1)}\in \mathbb{R} \}$$ are linearly independent for all $j \in \{1, \dots, N\}$, this implies that the above maps are indeed linearly independent - and indeed they are also linearly independent from any maps for which the edge directions $\mathbf{j} = (j_1, \dots, j_{m-1})$ are \emph{not} distinct. In other words, whenever $N \geq m-1$, the maps
$$ \left\{ \pi_{j_0}(V_{\tau}^{\mathbf{j}}): \mathbb{R}^N \rightarrow \mathbb{R} ~\Big{|}~ \begin{array}{l}
\tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m, ~ j_0 \in \{1, \dots, N\}\\
\mathbf{j} = (j_1, \dots, j_{m-1}) \in \{1, \dots, N\}^{m-1} \text{ with distinct entries}
\end{array} \right\}$$
are linearly independent. To briefly see that these maps are indeed also linearly independent from any $\pi_{j_0}(V_{\tau}^{\mathbf{j}})$ with $\mathbf{j}$ \emph{not} distinct, fix $\mathbf{j} = (j_1, \dots, j_{m-1})$ with \emph{distinct} entries and $\mathbf{j}' = (j_1', \dots, j_{m-1}')$ with \emph{non-distinct} entries. Then as $\mathbf{j}'$ will have at least one repeat of entries, we must necessarily have that there exists an entry, say $j^*$ in $\mathbf{j}$, such that $j^*$ does \emph{not} appear anywhere in $\mathbf{j}'$, i.e., $j^* \neq j_n'$ for all $n \in \{1, \dots, m-1\}$. As the vector fields $V_{\tau}^{\mathbf{j}'}$ associated to $\mathbf{j}'$ will thus \emph{never} take derivatives in direction $j^*$, there is no way of obtaining a factor of the type $\exp\left( d_{j^*}(w_{i^*}) x_{j^*} \right)$ in the expression of $V_{\tau}^{\mathbf{j}'}$ (with $i^*$ denoting whatever index is associated to $j^*$ in $\mathbf{i} = (i_1, \dots, i_{m-1})$). Utilizing the linear independence of exponential functions from before, this gives the desired.
\\

Removing the initial coordinate projection yields of course also that the vector fields
$$ \left\{ V_{\tau}^{\mathbf{j}}: \mathbb{R}^N \rightarrow \mathbb{R}^N ~\Big{|}~ \begin{array}{l}
\tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m\\
\mathbf{j} = (j_1, \dots, j_{m-1}) \in \{1, \dots, N\}^{m-1} \text{ with distinct entries}
\end{array} \right\}$$
are linearly independent among themselves, and also linearly independent from the collection of vector fields
$$ \left\{ V_{\tau}^{\mathbf{j}}: \mathbb{R}^N \rightarrow \mathbb{R}^N ~\Big{|}~ \begin{array}{l}
\tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m\\
\mathbf{j} = (j_1, \dots, j_{m-1}) \in \{1, \dots, N\}^{m-1} \text{ with non-distinct entries}
\end{array} \right\}.$$
Utilizing finally the fact that
$$ V_{\tau} = \sum_{\mathbf{j} = (j_1, \dots, j_{m-1})} V_{\tau}^{\mathbf{j}},$$
we conclude that the collection $\{V_{\tau} ~|~ \tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m\}$ is linearly independent when $N \geq m-1$ and $\sigma(x) = e^x$. This implies by Proposition \ref{prop:LinIndep} that the iterated vector fields $\{V_w ~|~ w \in \mathscr{W}_m\}$ must also be linearly independent for every fixed $m \leq N+1$.
%
\begin{rmk}[The general depth-two case]
    Note here that we have only specified $\sigma(x) = \exp(x)$ in order to make quick and direct conclusions about linear independence, utilizing well-known results for exponential functions. It should be possible to extend this to many other suitable choices of real analytic activation functions (for instance chosen \emph{generically}, i.e., such that all coefficients in the power series expansion are algebraically independent over $\mathbb{Q}$).
    \demo
\end{rmk}
%
\begin{rmk}[The no-depth case]
\label{rmk:NoDepth}
    One might wonder whether the strategy outlined here would not also work in the case where the vector fields $V_1, \dots, V_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$ are chosen \emph{without depth}, i.e., in the classical 'randomized signature form':
    $$ V_i (x) = \sigma(A_i x) \quad \text{for } x \in \mathbb{R}^N.$$
    In this case, however, one sees that for $j, k \in \{1, \dots, N\}$ the partial derivatives will be of the form:
    $$ \partial_j \pi_k(V_i)(x) = \sigma'(\pi_k(A_ix)) \alpha_{kj}(i),$$
    i.e., the only way in which the directions of the derivatives is recorded is through the coefficients $\alpha_{kj}$ appearing out front. These are of course not very helpful when establishing linear independence, and indeed it can be checked that the tree-like vector fields $\{ V_{\tau}^{\mathbf{j}} ~|~ \tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m, ~ \mathbf{j} \text{ distinct} \}$ are \underline{not} linearly independent in this case.\\
    
    To see this, take for instance two words in $\mathscr{W}_m$:
    $$ w = (w_1, \dots, w_m) \quad \text{and} \quad w' = (w_1, w_{m-1}, w_{m-2}, \dots, w_3, w_2, w_m),$$
    and let $\tau \in \mathbb{T}_w$ and $\tau' \in \mathbb{T}_{w'}$ denote the fixed recursive trees corresponding to a ladder with letter-labels given by each of these words. I.e., the structure of $\tau$ and $\tau'$ is determined uniquely by the indices $(i_1, i_2, \dots, i_{m-1}) = (1, 2, \dots, m-1)$ and $(n_1, n_2, \dots, n_{m-1}, n_m) = (1,1, \dots, 1,0)$. Set 
    $$\mathbf{j} = (1,2,3,4,\dots, m-1) \quad \text{and} \quad \mathbf{j}' = (m-2, m-3, \dots, 2, 1, m-1).$$ 
    \begin{figure}[h!]
  \centering
  \input{Figures/TreeSketch5}
  \caption{The 'ladder'-like trees $(\tau, w, \mathbf{j})$ and $(\tau', w', \mathbf{j}')$ }
  \label{fig:TreeSketch5}
\end{figure}
\ \\
    Then:
    \begin{align*}
        V_{\tau}^{\mathbf{j}} &= c_{\tau, \mathbf{j}} \sigma'(A_{w_1}x) \sigma'(\pi_1(A_{w_2}x)) \cdots \sigma'(\pi_{m-2}(A_{w_{m-1}}x))\sigma(\pi_{m-1}(A_{w_m}x)) \quad \text{and}\\
        V_{\tau'}^{\mathbf{j}'} &= c_{\tau', \mathbf{j}'} \sigma'(A_{w_1}x) \sigma'(\pi_{m-2}(A_{w_{m-1}}x)) \cdots \sigma'(\pi_1(A_{w_2}x)) \sigma(\pi_{m-1}(A_{w_m}x)),
    \end{align*}
    and so $V_{\tau}^{\mathbf{j}} = \frac{c_{\tau, \mathbf{j}}}{c_{\tau', \mathbf{j}'}} V_{\tau'}^{\mathbf{j'}}$, showing that the two tree-like vector fields are \emph{linearly dependent}, even though they are associated to different words and have distinct edge directions.
    \demo
\end{rmk}
%
\newpage
%
\section{Signature reconstruction}
\label{sec:SigRec}

%\subsection{Signature reconstruction: a direct approach}
%\label{subsec:directapproach}
%
%Even though the paradigm of reservoir computing or direct learning advocates the use of generic or random vector fields to obtain as many as possible directions in terms of Lie brackets in the space of vector fields, there are also direct, neither random nor generic approaches, which allow for signature reconstruction. Proof techniques are different though and not related to Taylor expansions.
%
%The following system of equations on $\mathbb{C} \times   \mathbb{R}^{d+2}$ leads
%\begin{align*}
%   d Z_t & = 1 \, , \\
%    d u_t & = 0  \, , \\
%    d \lambda_t & = 0 \, , \\
%    d V_t & = \mathrm{i} V_t \exp(\mathrm{i} u_t Z_t ) \langle \lambda_t , d X_t \rangle
%\end{align*}

%For initial value $Z_0=0$, $V_0=1$ and $u_0$, $\lambda_0$ we obtain
%$$
%V_t = \exp \big(\mathrm{i} \langle \lambda_0 , \int_0^t \exp(\mathrm{i} u_0 s) dX_s \rangle \big) 
%$$
%Obviously a solution of this system yields through limits of %linear combinations \emph{all} polynomials of Fourier %coefficients of the first derivative of $X^i$, $i=1,\ldots,d$, which is enough to reconstruct the path (if it starts at $0$) and whence signature itself.

\subsection{Signature reconstruction on $\mathbb{R}^N$}
\label{subsec:sigreconstruct}
Consider the CDE \eqref{eq:RNDE} from the $\mathbb{R}^N$-case. It was first suggested in \cite{AkyildirimTeichmann}
that the information obtained by considering such CDEs across all different initial values $y \in \mathbb{R}^N$ given a specified choice of random neural vector fields $V_1, \dots, V_d$ would allow for the reconstruction of the components of the signature $S(X)$ almost surely. We provide here a detailed proof of this fact for general vector fields satisfying an assumption of linear independence between the associated tree-like vector fields up to a certain order $L$, but note that the reconstruction is then only possible up to the $L$'th order signature component. We note also that the assumption of linear independence between the associated tree-like vector fields implicitly depends on the dimension of the hidden space $\mathbb{R}^N$ of the CDE (see Remark \ref{rmk:Dimensions}).
%
\begin{thm}[Signature reconstruction on $\mathbb{R}^N$]
\label{thm:sigreconRN}
Fix some $N, L \in \mathbb{N}$, let $V_1, \dots, V_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$ be smooth vector fields, and suppose that for all fixed $m \leq L$, the associated tree-like vector fields
$$ \{ V_{\tau} ~|~ \tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m\}$$
are linearly independent. Consider for each $y \in \mathbb{R}^N$, the CDE:
\begin{equation}
\label{eq:SigDE}
Y_t = y + \sum_{i=1}^d \int_0^t V_i(Y_s) dX_s^i \quad \text{for every } t \in [0,T]
\end{equation}
from \eqref{eq:RNDE}, and denote the solution associated to the initial value $y$ by $Y^{y}: [0,T]\rightarrow \mathbb{R}^N$. Then the signature components of $S(X)$ up to order $L$ can be uniquely reconstructed from the collection of solutions $(Y^y)_{y \in \mathbb{R}^N}$
\end{thm}
%
\begin{proof}
We start by slightly modifying the differential equation \eqref{eq:SigDE}. In particular, we let $\eta \in \mathbb{R}^N$ and $r \in \mathbb{R}$ and consider the differential equation
$$ Y_t^{\eta,r}  = \eta + \sum_{i=1}^d \int_0^t r V_i(Y_s^{\eta,r}) dX_s^i \quad \textrm{for } t \in [0,T]. $$
We immediately see that for every $\eta \in \mathbb{R}^N$ and $r \neq 0$ it holds that $Y_t^{\eta, r} = rY_t^{\frac{\eta}{r}}$, so each solution $Y^{\eta, r}$ (for $r \neq 0$) can indeed be obtained from the collection of solutions $(Y^y)_{y \in \mathbb{R}^N}$ to the differential equation \eqref{eq:SigDE} under appropriate scalar-multiplication with $r$. We of course also have the trivial correspondence $Y^{\eta, 0} \equiv \eta$. \\

Now, let $g \in C^{\infty}(\mathbb{R}^N)$, and define the $r$-scaled vector fields $\tilde{V}_1, \dots, \tilde{V}_d: \mathbb{R}^N \rightarrow \mathbb{R}^N$ by
$$\tilde{V}_i(x) = r V_i(x) \quad \textrm{for every } x \in \mathbb{R}^N. $$
Then it is easily verified for any $w = (w_1, \dots, w_k) \in \mathscr{W}_k$ that $ \tilde{V}_w g = r^k  V_w g$, so in particular Theorem \ref{thm:BaudoinZhang} gives the Taylor expansion
$$ g(Y_t^{\eta, r}) = g(\eta) + \sum_{k=1}^{\infty} r^k \sum_{w \in \mathscr{W}_k} V_w g(\eta) \int_{\Delta_{[0,t]}^k} dX_r^w\quad \textrm{for every } t \in [0,T].$$
For any $m\in \mathbb{N}$ we can thus take the $m$'th derivative with respect to $r$, and evaluate in $r =0$, in order to obtain that
$$ \frac{d^m}{dr^m}(g(Y_t^{\eta, r}))|_{r=0} = \sum_{w \in \mathscr{W}_m} V_w g(\eta) \int_{\Delta_{[0,t]}^m} dX_r^w \quad \textrm{for every } t \in [0,T]. $$
We now obtain from Proposition \ref{prop:LinIndep} that whenever $m \leq L$, the operators $\{V_w ~|~ w \in \mathscr{W}_m\}$ are linearly independent, due to our assumption of linear independence of the tree-like vector fields up to order $L$. Hence, choosing some appropriate distinct initial values $\eta_1, \dots, \eta_{d^m}$ (noting that $|\mathscr{W}_m| = d^m$), will by linear independence allow us to consider the above expression as a system of linear equations which is uniquely solvable with respect to the iterated integrals of $X$. Solving this linear equation system will then allow us to express all elements of the $m$'th signature component of $X$, in terms of the solutions $Y^{\eta_1, r}, \dots, Y^{\eta_{d^m}, r}$ for $r \in \mathbb{R}$, concluding the proof.
\end{proof}
%
The result can of course, among other examples, be applied to the randomized nested exponential vector fields from Section \ref{subsec:NestedExpo}, i.e., the vector fields given by
$$ V_i(x) = \exp(A_i \exp(D_i x)) \quad \text{for } x \in \mathbb{R}^N,$$
with the matrices $A_1, \dots, A_d$ and diagonal matrices $D_1, \dots, D_d$ being randomly generated by sampling the entries independently according to a distribution which is absolutely continuous with respect to the Lebesgue measure. In this case Theorem \ref{thm:sigreconRN} states that if $N \geq L-1$, then the signature components up to order $L$ of a path $X$ can be almost surely uniquely reconstructed from the randomized signatures of $X$ with depth two and exponential activation function.
%
\subsection{Signature reconstruction on Lie groups}
\label{subsec:sigreconstructLie}
%
Consider the CDE \eqref{eq:LieDE} from the Lie group case. We wish to extend the signature reconstruction result from Theorem \ref{thm:sigreconRN} to this setting. Letting $G$ be a Lie group of dimension $N$, and letting $V_1, \dots, V_d: G \rightarrow TG$ be smooth vector fields, Proposition \ref{prop:LinIndep} (linear independence of tree-like vector fields) should be possible to extend to the Lie group setting. In this case we can utilize the Taylor expansion from Theorem \ref{thm:BaudoinZhangLie} as before.
%
\begin{thm}[Signature reconstruction on $G$]
\label{thm:sigreconLie}
Fix some $N,L \in \mathbb{N}$, let $V_1, \dots, V_d: G \rightarrow TG$ be smooth vector fields, and suppose that for all fixed $m \leq L$, the associated tree-like vector fields
$$ \{ V_{\tau}: G \rightarrow TG ~|~ \tau \in \mathbb{T}_w, ~ w \in \mathscr{W}_m \}$$
are linearly independent. Consider for each $\zeta \in G$ and $r \in \mathbb{R}$, the $r$-scaled CDE
\begin{equation} 
Z_t^{\zeta, r} = \zeta + \sum_{i=1}^d \int_0^t rV_i(Z_s^{\zeta, r}) dX_s^i \quad \text{for every } t \in [0,T]
\end{equation}
from \eqref{eq:LieDE}. Then the signature components of $S(X)$ up to order $L$ can be uniquely reconstructed from the collection of solutions $(Z^{\zeta, r})_{\zeta \in G, r \in \mathbb{R}}$.
\end{thm}
%
\begin{rmk}
It is sufficient to consider $r \in (-\varepsilon, \varepsilon)$ for some $\varepsilon > 0$, as we are only interested in choices of $r$ close to zero.
\demo
\end{rmk}
%
\begin{proof}
Write $\tilde{V}_i(x) = rV_i(x)$ for every $i \in \{1, \dots, d\}$ and $x \in G$, and consider some $g \in C^{\infty}(G)$ non-constant. Then it can easily be verified for any word $w = (w_1, \dots, w_k) \in \mathscr{W}_k$ that $ \tilde{V}_w g = r^k V_wg$, so in particular we get the Taylor expansion
\begin{align*}
g(Z_t^{\zeta, r}) = g(\zeta) + \sum_{k=1}^{\infty} r^k \sum_{w \in \mathscr{W}_k} V_w g(\zeta) \int_{\Delta_{[0,t]}^k} dX_r^w \quad \textrm{for every } t \in [0,T],
\end{align*}
from Theorem \ref{thm:BaudoinZhangLie}. For any $m \in \mathbb{N}$ we can thus  take the $m$'th derivative with respect to $r$ and evaluate in $r=0$, in order to obtain that
$$ \frac{d^m}{dr^m} (g(Z_t^{\zeta, r}))|_{r=0} = \sum_{w \in \mathscr{W}_m} V_w g(\zeta) \int_{\Delta_{[0,t]}^k} dX_r^w \quad \textrm{for every } t \in [0,T].$$
Under a similar claim of linear independence of the operators $\{V_w ~|~ w \in \mathscr{W}_m\}$ whenever $m \leq L$ as in Proposition \ref{prop:LinIndep}, we can choose appropriate distinct initial values $\zeta_1, \dots, \zeta_{d^m}$, which allow us to consider the above expression as a linear equation system which is uniquely solvable with respect to the iterated integrals of $X$. Solving this linear equation system will allow us to express all elements of the $m$'th signature component of $X$, in terms of the solutions $Z^{\zeta_1, r}, \dots, Z^{\zeta_{d^m}, r}$ for $r \in \mathbb{R}$ under $g$. Hence all signature components of $S(X)$ can be reconstructed from the solutions $(Z^{\zeta, r})_{\zeta \in G, r \in \mathbb{R}}$, as desired. The claim of linear independence should be provable in a similar fashion to the $\mathbb{R}^N$-case covered in Section \ref{sec:linindep} - we will however not go into the details of this here.
\end{proof}
%
\newpage
%
\newpage
\section{Further perspectives}

The findings and discussions presented in this paper lay the groundwork for future exploration in the understanding of \emph{randomized signatures}, of their expressive power and stability. Some promising directions for future research and practical applications can be identified:

\begin{itemize}
    \item \textbf{Log-Signature:} In this work, our primary goal was reconstructing the signature. A significant challenge we encountered stemmed from inherent, unavoidable algebraic relations between generic vector fields (\emph{cf.} Remark \ref{rmk:Dimensions}), which fundamentally arise from the properties of the algebra of words.
    These relations are closely tied to those of the signature entries themselves, which are not independent but exhibit similar algebraic dependencies. It is known (\cite{LyonsRoughDE, morrill2021neural}) that such dependencies can be eliminated by shifting the focus from the signature to the \emph{log-signature}. This raises a natural question about the recovery of this lossless compression.
    Would such an approach resolve the algebraic complexities encountered so far, or would it introduce new challenges arising from the Lie-algebraic nature of the log-signature? We leave this question open for further investigation.

    \item \textbf{The Role of Depth:} We have demonstrated that the notion of depth in vector fields—analogous to the depth of neural networks—serves as a powerful tool in obtaining explicit results. This depth provides a “leveled” structure, reminiscent of that found in tensor algebras.
    It seems plausible that greater depth corresponds to a higher degree of independence, thereby enabling the recovery of more signature terms for a fixed hidden dimension. We believe that a thorough understanding of this phenomenon would be of significant interest and value for future research.

    \item \textbf{Alternative approaches to signature reconstruction:} %As seen in Section \ref{subsec:directapproach} it is possible to construct systems of equations for which the signature can explicitly be reconstructed without having to use Taylor expansions, thus circumventing the main challenge of this paper, namely the study of linear independence between iterated vector fields. 
    It might be possible to approach the question of signature reconstruction from an entirely different direction than the one applied in this project, by avoiding the use of Taylor expansions, and thus circumventing the challenges of linear independence between vector fields. Some alternative approaches to signature reconstruction that were explored in the making of this paper, but which were ultimately abandoned, were approaches using Baire's category theorem or probabilistic arguments. A different line of inquiry might be looking into more direct constructions using Fourier coefficients.

\end{itemize}
%
\newpage
%
\section{Acknowledgements}
%
MG and NMC are supported by the EPSRC Centre for Doctoral
Training in Mathematics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1).\\

The authors would like to thank Cristopher Salvi for his feedback on an earlier version of this work, and for suggesting the investigation of randomized signatures with depth. The authors would also like to thank Thomas Mikosch for facilitating the joint master thesis of MG between University of Copenhagen and ETH Zürich, on which parts of this paper are loosely based.
%

\bibliographystyle{apalike}
\bibliography{references}
%
\newpage
%
\appendix
%
\section{Appendix: Vector fields}
\label{appendix:vectorfields}
%
\subsection{Tangent spaces and tangent mappings}
Let $M$ be a smooth manifold, such that $M$ is locally homeomorphic to $\mathbb{R}^n$ for some fixed $n \in \mathbb{N}$. Denote
$$ C^{\infty}(M) := \{ f: M \rightarrow \mathbb{R} ~|~ f \textrm{ is smooth} \}. $$ 
For any two maps $f,g \in C^{\infty}(M)$ we define addition, scalar multiplication and multiplication pointwise in $C^{\infty}(M)$ by
\begin{align*}
(f+g)(x) &= f(x) + g(x),\\
(\alpha f)(x) &= \alpha f(x), \\
(f \cdot g)(x) &= f(x)g(x),
\end{align*}
for all $x \in M$, $\alpha \in \mathbb{R}$. Equipped with these operations, $C^{\infty}(M)$ forms an algebra (for which $+$ and $\cdot$ are particularly commutative by construction).\\

For each $x \in M$ we associate a \emph{tangent space}
$$T_xM = \left\{ X_x: C^{\infty}(M) \rightarrow \mathbb{R} \textrm{ linear} ~\Big{|}~ \begin{array}{c}
X_x(f \cdot g) = X_x(f)g(x) + f(x) X_x(g), \\
X_x \textrm{ satisfies the germ condition}
\end{array} \right\}, $$
where the germ condition (referring to the formal definition of tangent spaces in terms of \emph{germs}) states that for all $f,g \in C^{\infty}(M)$:
$$\textrm{If } \exists U \subseteq M \textrm{ open neighbourhood of } x \textrm{ such that } f|_U = g|_U \textrm{ then } X_x(f) = X_x(g).$$
Suppose that $(U,u)$ is a chart around $x$, such that $U$ is open with $x \in U$, and $u: U \rightarrow u(U) \subseteq \mathbb{R}^n$ is a homeomorphism. Denoting the coordinate functions of $u$ by $u^1, \dots, u^n$, it is then known that the tangent space $T_xM$ has canonical basis vectors $\left( \frac{\partial}{\partial u^i} \right)_{i=1}^n$, so in particular $T_xM$ is linearly isomorphic to $\mathbb{R}^n$ for each $x \in M$.
\begin{defin}[Tangent mapping]
Let $M$ and $N$ be smooth manifolds, and let $\varphi: M \rightarrow N$ be a smooth mapping. For every $x \in M$ we define the \emph{tangent mapping induced by $\varphi$} as
$$ T_x\varphi: T_xM \rightarrow T_{\varphi(x)}N \quad \textrm{given by} \quad X_x \mapsto X_x(\cdot \circ \varphi). $$
\end{defin}
\begin{prop}[Properties of tangent mappings]
\label{prop:propertiestangent}
Let $M$, $N$ and $P$ be smooth manifolds and consider smooth mappings $\varphi: M \rightarrow N$ and $\psi: N \rightarrow P$. For every fixed $x \in M$ it holds that
\begin{itemize}
\item[\textbf{\emph{(i)}}] The tangent mapping $T_x\varphi: T_xM \rightarrow T_{\varphi(x)}N$ is linear;
\item[\textbf{\emph{(ii)}}] $T_x(\psi \circ \varphi) = T_{\varphi(x)}\psi \circ T_x\varphi$;
\item[\textbf{\emph{(iii)}}] $T_x\textrm{id}_M = \textrm{id}_{T_xM}$ where $\textrm{id}$ denotes the identity map on the indicated space.
\end{itemize}
\end{prop}
%
We especially notice that if
\begin{itemize}
\item $M$ is associated to $\mathbb{R}^m$ and $N$ is associated to $\mathbb{R}^n$;
\item $(U,u)$ is a chart on $M$ with $x \in U$ and $(V,v)$ is a chart on $N$ with $\varphi(x) \in V$;
\end{itemize}
we can consider the canonical bases $\left( \frac{\partial}{\partial u_i}|_x \right)_{i=1}^m$ for $T_xM$ and $\left(\frac{\partial}{\partial v_j}|_{\varphi(x)} \right)_{j=1}^n$ for $T_{\varphi(x)}N$. Hence, linearity of the tangent mapping $T_x\varphi: T_xM \rightarrow T_{\varphi(x)}N$ implies that it can be represented by a real-valued $n \times m$-matrix given with respect to the canonical bases.
\begin{prop}
Let $M$ and $N$ be smooth manifolds, fix some $x \in M$ and consider a smooth mapping $\varphi: M\rightarrow N$. Then the real-valued matrix representing the linear map $T_x\varphi: T_xM \rightarrow T_{\varphi(x)}N$ with respect to the canonical choice of basis vectors is precisely given by the Jacobi matrix
$$D(v \circ \varphi \circ u^{-1})(u(x)) \in \mathbb{R}^{n \times m}$$
for any choice of charts $(U,u)$ in $M$ and $(V,v)$ in $N$ with $x \in U$ and $\varphi(x) \in V$.\footnote{Note here that $v \circ \varphi \circ u^{-1}: \underbrace{u(U)}_{\subseteq \mathbb{R}^m} \rightarrow \mathbb{R}^n$ is a well-defined map between Euclidean spaces.}
\end{prop}
%
In particular this means that we obtain the linear combinations:
$$ T_x\varphi\left( \frac{\partial}{\partial u^i}|_x \right) = \sum_{j=1}^n \frac{\partial (v^j \circ \varphi \circ u^{-1})}{\partial x^i}(u(x)) \frac{\partial}{\partial v^j}|_{\varphi(x)} \quad \textrm{for } i=1, \dots, m. $$
\ \\
We denote by $TM = \{ (x,X_x) ~|~ x \in M, X_x \in T_xM\}$ the \emph{tangent bundle} on $M$. For any smooth mapping $\varphi: M \rightarrow N$ considered as above, we can associate the total mapping $T\varphi: TM \rightarrow TN$ given by $(x, X_x) \mapsto (\varphi(x), T_x\varphi(X_x))$.
%
\subsection{Vector fields and pushforwards}
\begin{defin}[Vector field]
A \emph{vector field} on $M$ is a map $X: M \rightarrow TM$ which associates to each $x \in M$ a tangent vector $X(x) \in T_xM$. I.e., the map $X$ is a vector field if it satisfies the identity
$$ \pi_M \circ X = \textrm{id}_M $$
where $\pi_M: TM \rightarrow M$ is the projection mapping $(x, X_x) \mapsto x$ and $\textrm{id}_M$ is the identity mapping on $M$.
\end{defin}
The set of all vector fields on $M$ is denoted
$$ \mathfrak{X}(M) := \{ X: M \rightarrow TM ~|~ X \textrm{ is a vector field}\}, $$
and $\mathfrak{X}(M)$ forms a vector space when equipped with pointwise addition and scalar multiplication.
%
\begin{prop}[The map $Xf$]
\label{prop:mapXf}
Let $X \in \mathfrak{X}(M)$ be a vector field and let $f \in C^{\infty}(M)$ be a smooth function. Then the map
$$ Xf: M \rightarrow \mathbb{R} \quad \textrm{given by} \quad Xf(x) := X(x)(f) $$
is smooth, i.e., $Xf \in C^{\infty}(M)$.
\end{prop}
%
For a vector field $X: M \rightarrow TM$ in $\mathfrak{X}(M)$, we define the map 
$$\mathcal{D}_X: C^{\infty}(M) \rightarrow C^{\infty}(M) \quad \textrm{given by} \quad f \mapsto Xf \quad \textrm{for all } x \in M, $$
and it can then be shown that $\mathscr{D}_X$ is a \emph{derivation}. Indeed, there is a unique correspondence between a vector field $X$ and its derivation, in the sense that the map $X \mapsto \mathscr{D}_X$ is bijective.\\

We will now define the notion of a \emph{pushforward} of a vector field.
%
\begin{defin}[$\varphi$-related vector fields]
Let $M$ and $N$ be smooth manifolds, and consider a smooth mapping $\varphi: M \rightarrow N$. If $X \in \mathfrak{X}(M)$ and $Y \in \mathfrak{X}(N)$ are vector fields on $M$ and $N$, respectively, we say that $X$ and $Y$ are \emph{$\varphi$-related} if
$$ T\varphi \circ X = Y \circ \varphi. $$
\end{defin}
%
\begin{prop}[Pushforward of a vector field]
\label{prop:pushforward}
Let $M$ and $N$ be smooth manifolds, and suppose that $\varphi: M \rightarrow N$ is a smooth diffeomorphism. Then for every $X \in \mathfrak{X}(M)$ there exists a unique vector field $Y \in \mathfrak{X}(N)$ such that $X$ and $Y$ are $\varphi$-related. We write $\varphi_*X := Y$ and call $\varphi_*X$ the \emph{pushforward} of $X$ by $\varphi$. In particular, the pushforward is explicitly given by
$$ \varphi_*X = T\varphi \circ X \circ \varphi^{-1}. $$
\end{prop}
%
\subsection{Velocity vectors and integral curves}
Let as usual $M$ be a smooth manifold and let $J \subseteq \mathbb{R}$ be some open interval. We will be interested in considering smooth curves $\gamma: J \rightarrow M$, and will typically assume that $0 \in J$, such that $\gamma(0)$ denotes the \emph{starting point} of the curve.
%
\begin{defin}[Velocity vector]
Consider some smooth curve $\gamma: J \rightarrow M$ and fix some time $t_0 \in J$. Then, considering the tangent mapping $T_{t_0} \gamma: T_{t_0} J \rightarrow T_{\gamma(t_0)} M$, we define the \emph{velocity vector} of $\gamma$ at $t_0$ as the tangent vector
$$ \gamma'(t_0) := T_{t_0} \gamma \left( \frac{d}{dt}|_{t_0} \right) \in T_{\gamma(t_0)} M. $$
\end{defin}
%
We note in particular that evaluating the tangent vector $\gamma'(t_0) \in T_{\gamma(t_0)} M$ in any function $f \in C^{\infty}(M)$ yields
$$ \gamma'(t_0)(f) = T_{t_0} \gamma \left( \frac{d}{dt}|_{t_0} \right)(f) = (f \circ \gamma)'(t_0).$$
In other words, the velocity vector can be considered as a derivation which maps every $f \in C^{\infty}(M)$ to its derivative along the curve $\gamma$.\\

One can now ask, whether we, given some vector field $X: M \rightarrow TM$ can construct a curve $\gamma: J \rightarrow M$ such that $X$ precisely maps every point of the curve to its corresponding velocity vector. Such a curve will be referred to as an \emph{integral curve}.
%
\begin{defin}[Integral curve]
Let $M$ be a smooth manifold and let $X \in \mathfrak{X}(M)$ be a vector field. We say that a smooth curve $\gamma: J \rightarrow M$ is an \emph{integral curve of $X$} if it satisfies that
$$ \gamma'(t) = X(\gamma(t)) \quad \textrm{for every } t \in J.$$
\end{defin}
%
\begin{thm}[Existence and uniqueness of integral curves]
Let $X \in \mathfrak{X}(M)$ be a vector field on the smooth manifold $M$ and fix some point $x \in M$. Then there exists an open interval $J \subseteq \mathbb{R}$ with $0 \in J$ and a smooth curve $\gamma: J \rightarrow M$ such that $\gamma$ is an integral curve of $X$ with starting point $\gamma(0) = x$. If we can choose $J = \mathbb{R}$, it furthermore holds that the curve $\gamma$ is unique.
\end{thm}
%
\begin{defin}[Maximal integral curve]
Let $X \in \mathfrak{X}(M)$ and let $\gamma: J \rightarrow \mathbb{R}$ be an integral curve of $X$. We say that $\gamma$ is a \emph{maximal integral curve of $X$} if it cannot be extended to a larger integral curve of $X$, i.e., if there exist no smooth curves $\tilde{\gamma}: \tilde{J} \rightarrow M$ satisfying that
\begin{enumerate}
    \item $\tilde{J} \subseteq \mathbb{R}$ is an open interval with $J \subseteq \tilde{J}$.
    \item $\tilde{\gamma}|_J = \gamma$.
    \item $\tilde{\gamma}$ is an integral curve of $X$.
\end{enumerate}
\end{defin}
%
\section{Lie groups and Lie algebras}
%
For this section it may be useful to refer to Appendix \ref{appendix:vectorfields} for conventions of notation. See \cite{Hall}, \cite{JohnLee}, \cite{Michor}, \cite{Samelson} and \cite{Warner}, for thorough introductions to Lie group theory.
%
\subsection{Basic definitions and relations}
\begin{defin}[Lie group]
Let $G$ be a smooth manifold, and let $\cdot: G \times G \rightarrow G$ be a smooth map on $G$. If the pair $(G, \cdot)$ constitutes a group, then we say that $(G, \cdot)$ is a \emph{Lie group}. We denote the identity element in $(G, \cdot)$ by $e$.
\end{defin}
Let $G$ be a Lie group. For any fixed $a \in G$, the left- and right-translation operators $L_a: G \rightarrow G$ and $R_a: G \rightarrow G$ are given by
$$ L_a(b) = a b \quad \textrm{and} \quad R_a(b) = b a \quad \textrm{for any } b \in G. $$
A map $\varphi: G \rightarrow H$ between two Lie groups $G$ and $H$ is called a \emph{Lie group homomorphism} if it is a smooth group homomorphism.
%
\begin{defin}[Lie algebra]
A vector space $\mathfrak{g}$, equipped with a bilinear operation $[\cdot, \cdot]: \mathfrak{g} \times \mathfrak{g} \rightarrow \mathfrak{g}$, satisfying the two properties
\begin{itemize}
\item[\textbf{\emph{(i)}}] $[X,Y] = - [Y,X]$;
\item[\textbf{\emph{(ii)}}] $[X, [Y,Z]] = [[X,Y], Z] + [Y, [X,Z]]$ (the Jacobi identity);
\end{itemize} 
for all $X,Y,Z \in \mathfrak{g}$ is called a \emph{Lie algebra}. The operation $[\cdot, \cdot]$ is called the \emph{Lie bracket} associated to the Lie algebra.
\end{defin}
%
\noindent As one would expect, a map $\phi: \mathfrak{g} \rightarrow \mathfrak{h}$ between two Lie algebras is called a \emph{Lie algebra homomorphism} if it is an algebra homomorphism between $\mathfrak{g}$ and $\mathfrak{h}$, i.e., if it is linear and satisfies the homomorphism relation
$$ \phi([X,Y]) = [\phi(X), \phi(Y)] \quad \textrm{for all } X,Y \in \mathfrak{g}.$$
%
\begin{prop}[Lie subalgebra]
Let $(\mathfrak{g}, [\cdot, \cdot])$ be a Lie algebra, and suppose that $\mathfrak{h} \subseteq \mathfrak{g}$ is a subspace. If $\mathfrak{h}$ is closed under the Lie bracket, i.e.,
$$ X,Y \in \mathfrak{h} \quad \Rightarrow \quad [X,Y] \in \mathfrak{h}, $$
then $(\mathfrak{h}, [\cdot, \cdot]|_{\mathfrak{h}})$ forms a Lie algebra, where $[\cdot, \cdot]|_{\mathfrak{h}}: \mathfrak{h} \times \mathfrak{h} \rightarrow \mathfrak{h}$ denotes the restriction of the Lie bracket to $\mathfrak{h}$. We call $(\mathfrak{h}, [\cdot, \cdot]|_{\mathfrak{h}})$ a \emph{Lie subalgebra} of $\mathfrak{g}$.
\end{prop}
%
Let $G$ be a Lie group. Then the space $\mathfrak{X}(G)$ of all vector fields on $G$ can be equipped with a canonical Lie bracket under which it forms a Lie algebra.
%
\begin{thm}[Lie bracket on $\mathfrak{X}(G)$]
The canonical choice of Lie bracket on $\mathfrak{X}(G)$ is the bilinear map $[\cdot, \cdot]: \mathfrak{X}(G) \times \mathfrak{X}(G) \rightarrow \mathfrak{X}(G)$ given by the representation 
$$ [X,Y](x) = X(x) \circ \mathcal{D}_Y - Y(x) \circ \mathcal{D}_X \quad \textrm{for all } x \in G \textrm{ and } X,Y \in \mathfrak{X}(G). $$
Equipped with this bracket, the pair $(\mathfrak{X}(G), [\cdot,\cdot])$ forms a Lie algebra. The maps $\mathcal{D}_X$ and $\mathcal{D}_Y$ are defined as in Appendix \ref{appendix:vectorfields}.
\end{thm}
%
Given a Lie group $(G, \cdot)$, we can now associate to it a canonical Lie algebra, namely, the Lie subalgebra of $(\mathfrak{X}(G), [\cdot, \cdot])$ consisting of all \emph{left-invariant} vector fields.
%
\begin{prop}[Left-invariant vector field]
Let $G$ be a Lie group, and let $X \in \mathfrak{X}(G)$ be a vector field on $G$. We say that the vector field $X$ is \emph{left-invariant} if
\begin{equation}
(L_a)_* X = TL_a \circ X \circ L_{a^{-1}} = X  \quad \textrm{for all } a \in G.
\end{equation}
We denote the set of left-invariant vector fields on $G$ by $\mathfrak{X}_L(G)$.
\end{prop}
%
By Proposition \ref{prop:pushforward} we see that a vector field is left-invariant if and only if it is $L_a$-related to itself for every $a \in G$. It can also easily be verified that the collection $\mathfrak{X}_L(G)$ of left-invariant vector fields on $G$ is a Lie subalgebra of $\mathfrak{X}(G)$. We shall refer to this Lie subalgebra as the \emph{Lie algebra generated by $G$}, and write $\textrm{Lie}(G) = \mathfrak{X}_L(G)$.
%
\begin{thm}
\label{thm:Liealgisomorph}
Let $G$ be a Lie group and denote by $\varepsilon: \textrm{Lie}(G) \rightarrow T_eG$ the evaluation map $X \mapsto X(e)$. Then $\varepsilon$ is a linear isomorphism between vector spaces, and we write
$$ \textrm{Lie}(G) \cong T_e G.$$
\end{thm}
From Theorem \ref{thm:Liealgisomorph} one immediately obtains a natural way of constructing a Lie algebra homomorphism from a Lie group homomorphism.
\begin{thm}[Induced Lie algebra homomorphism]
Let $\varphi: G \rightarrow H$ be a Lie group homomorphism between two Lie groups $G$ and $H$, and denote their Lie associated Lie algebras by $\mathfrak{g} = \textrm{Lie}(G)$ and $\mathfrak{h} = \textrm{Lie}(H)$, respectively. Then for every left-invariant vector field $X \in \mathfrak{g}$ there exists a unique $Y \in \mathfrak{h}$ such that $X$ and $Y$ are $\varphi$-related, and we write $\varphi_*X := Y$. In particular, the map $\varphi_*: \mathfrak{g} \rightarrow \mathfrak{h}$ obtained from this construction is a Lie algebra homomorphism.
\end{thm}
%
\subsection{The exponential map}
We start by introducing the notion of a \emph{one-parameter subgroup}.
\begin{defin}[One-parameter subgroup]
Let $G$ be a Lie group and note that the pair $(\mathbb{R}, +)$ naturally constitutes a Lie group as well. We will refer to any Lie group homomorphism $\gamma: \mathbb{R} \rightarrow G$ as a \emph{one-parameter subgroup of $G$}.
\end{defin}
%
\begin{thm}
Let $G$ be a Lie group with neutral element $e$. Then $\gamma: \mathbb{R} \rightarrow G$ is a one-parameter subgroup of $G$ if and only if there exists some left-invariant vector field $X \in \textrm{Lie}(G)$ such that $\gamma$ is the maximal integral curve of $X$ satisfying the initial condition $\gamma(0) = e$. In particular, we will say that $\gamma$ is the \emph{one-parameter subgroup generated by $X$}, and sometimes denote it by $\gamma_X$.
\end{thm}
In particular, we obtain the bijective correspondences
\begin{align*} 
\begin{array}{ccccc}
\{\textrm{one-parameter subgroups of } G\} & \longleftrightarrow & \textrm{Lie}(G) & \longleftrightarrow & T_eG, \\
\gamma_X & \leftrightarrow & X & \leftrightarrow & X(e).
\end{array}
\end{align*}
%
\begin{defin}[Exponential map]
Let $G$ be a Lie group with Lie algebra $\mathfrak{g} = \textrm{Lie}(G)$. Then the exponential map $\exp: \mathfrak{g} \rightarrow G$ is defined by
$$ \exp(X) = \gamma_X(1) \quad \textrm{for all } X \in \mathfrak{g},$$
where $\gamma_X$ denotes the one-parameter subgroup generated by $G$.
\end{defin}
%
The exponential map provides us with a canonical way of expressing the one-parameter subgroup associated to a left-invariant vector field.
%
\begin{prop}
Let $G$ be a Lie group. Then for every left-invariant vector field $X \in \textrm{Lie}(G)$, the one-parameter subgroup generated by $X$ is the curve $\gamma_X: \mathbb{R} \rightarrow G$ defined by
$$ \gamma_X(s) = \exp(sX) \quad \textrm{for every } s \in \mathbb{R}.$$
\end{prop}
%
We end this section by stating some useful properties of the exponential map.
%
\begin{prop}[Properties of the exponential map]
Let $G$ be a Lie group with Lie algebra $\mathfrak{g} = \textrm{Lie}(G)$. Then the exponential map $\exp: \mathfrak{g} \rightarrow G$ is smooth, and satisfies all of the following properties.
\begin{enumerate}
    \item For all $X \in \mathfrak{g}$: $\exp((s+t)X) = \exp(sX)\exp(tX)$ for all $s,t \in \mathbb{R}$-
    \item For all $X \in \mathfrak{g}$: $\exp(X)^{-1} = \exp(-X)$.
    \item For all $X \in \mathfrak{g}$: $\exp(X)^n = \exp(nX)$.
    \item The tangent mapping $T_0\exp: T_0\mathfrak{g} \rightarrow T_eG$ is the identity mapping.
    \item There exists an open neighbourhood $U \subseteq \mathfrak{g}$ with $0 \in U$ such that $\exp(U) \subseteq G$ is open with $e \in \exp(U)$, and
    $$ \exp|_U: U \rightarrow \exp(U) \quad \textrm{is a diffeomorphism}.$$
    \item If $\varphi: G \rightarrow H$ is a Lie group homomorphism into some other Lie group $H$ with Lie algebra $\mathfrak{h} = \textrm{Lie}(H)$, then it holds that
    $$ \exp \circ \varphi_* = \varphi \circ \exp \quad \textrm{as a map } \mathfrak{g} \rightarrow H,$$
    where $\varphi_*: \mathfrak{g} \rightarrow \mathfrak{h}$ denotes the Lie algebra homomorphism induced from $\varphi$.
\end{enumerate}
\end{prop}
%
\end{document}