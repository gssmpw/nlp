
In this paper, we model routers in networking as agents using simple learning algorithms to find the best way to get their packets served. This simple, myopic and distributed multi-agent decision system makes large queuing systems simple to operate, but at the same time, the system needs more capacity than would be required if all traffic were centrally coordinated. In a recent paper, \cite{DBLP:conf/sigecom/GaitondeT20,DBLP:journals/jacm/GaitondeT23}, the authors initiated the study of such systems, modeling them as an infinitely repeated game in which routers compete for servers and the system maintains a state (number of packets held by each queue) that results from outcomes of previous rounds. Routers get to send a packet at each step at which they received a packet or have one in their queue\footnote{The players are routers with states, the length of their queues; the terms {\em router} and {\em queue} are used interchangeably.} to one of the servers, and 
each server attempts to process only one of the packets arriving to it. However, the model of \cite{DBLP:conf/sigecom/GaitondeT20,DBLP:journals/jacm/GaitondeT23} assumes that servers have no buffers at all, so queues have to resend all packets that were not served successfully.  They show that, in their system, even with hugely increased server capacity relative to what is needed in the centrally-coordinated case, 
the use of timestamps and priority for older packets is required to ensure that the system is stable, 
i.e., that queue lengths do not diverge with time (see Section \ref{sec:model} for a formal definition). 

In networking systems, servers accepting packets typically have a very small buffer and can hold on to a few packets to be served later. In this paper, we consider the analog of the model of  \cite{DBLP:conf/sigecom/GaitondeT20,DBLP:journals/jacm/GaitondeT23} in such systems with a tiny buffer at each server. We show that even with a buffer capable of holding only a single packet, the service capacity of the system greatly increases even when agents are learning.  To see this, consider a system with 
a single queue receiving packets at a high rate, say with probability 
$\lambda=\frac12$ at each time step, and multiple servers, each with a service rate (the probability of successfully serving a packet), say
$\mu=\frac13$. Without buffers, the condition for stability of such a system is that there is a single server with a service rate of $\mu>\lambda$, as a queue can only send a packet to one server at a time.  So even with a large number of servers, each with service rate $\mu=\frac13$, and thus, an arbitrarily large sum of service capacities, it would not be feasible to keep the system stable.
Also, the learning algorithm of this  single queue is facing a classical multi-arm bandit problem, aiming to learn which of the servers is the best one to send their packets to. By contrast, we observe that with a buffer of even just one packet at each server, a large enough number of servers with low capacity will always ensure stability, as the 
learning agent can now take advantage of low capacity servers as well, sending packets to them rarely enough 
such that they are likely to clear in time before the next packet is sent to them.
% to have it likely \ykedit{that they} clear in time before the next packet is sent. 

Back to our example of servers with $\mu=\frac13$ and $\lambda = \frac{1}{2}$, which, as we saw above, is not stable without buffers for any number of servers, with buffers, two such servers will make the system stable if the queue alternates sending its packets to them. In light of this observation, the goal of a learning algorithm even with just one queue and many servers is now more complex: it should learn to distribute its packets in a way that takes advantage of the capacity of all the servers.

Our main result, formally stated in Theorem \ref{thm:main}, is to show that 
when multiple queues compete for service while using learning algorithms to identify good servers, a small constant-factor increase in server capacity---relative to what would be required under centralized coordination---suffices to maintain the system stable, even if servers randomly select among simultaneously arriving packets.
Specifically, suppose we have $n$ queues with packet arrival rates of $\lambda_1,\ldots,\lambda_n$, and $m$ servers of service rates $\mu_1,\ldots,\mu_m$. Obviously, we must have $\sum_i\lambda_i < \sum_j \mu_j$, else the system cannot be stable even if fully coordinated. We show that this condition is not strong enough to ensure stability even with full coordination due to the small buffers.\footnote{This condition allows to create a fractional assignment such that $\sum_j x_{ij} >\lambda_i$ and $\sum_i x_{ij}<\mu_j$. Using the matching decomposition of this assignment in a coordinated schedule will guarantee a bounded expected number of packets both at the queues and servers.}  However, our result shows that if $\lambda_i<\frac12$ for all $i$, all queues use a learning algorithm that guarantees the no-regret condition with high probability, and  $3\sum_i\lambda_i < \sum_j \mu_j$, this guarantees that the system remains stable. While we do not know whether the factor of 3 in the required capacity constraint is tight, we show that to guarantee that a no-regret outcome of learning by the queues remains stable, it is required to have at least $2\sum_i\lambda_i < \sum_j \mu_j$  (see Remark \ref{remark:lambda-dependent-factor} and Theorem \ref{thm:lower-bound-example} for more details).