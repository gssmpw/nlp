\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[review]{acl}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[most]{tcolorbox}
\usepackage{multirow} 
\usepackage{makecell}
\usepackage{colortbl}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{placeins}
\usepackage{times}
\usepackage{latexsym}
\newcommand{\modelname}{PathRAG }
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{hyperref}
\title{PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths}

\author{
Boyu Chen\textsuperscript{1}, 
Zirui Guo\textsuperscript{1,2}, 
Zidan Yang\textsuperscript{1,3}, 
Yuluo Chen\textsuperscript{1},  
Junze Chen\textsuperscript{1},
\\
\textbf{Zhenghao Liu\textsuperscript{3}},  
\textbf{Chuan Shi\textsuperscript{1}},  
\textbf{Cheng Yang\textsuperscript{1}} 
\\
\textsuperscript{1} Beijing University of Posts and Telecommunications  \\
\textsuperscript{2} University of Hong Kong 
\textsuperscript{3} Northeastern University
\\
\texttt{chenbys4@bupt.edu.cn,yangcheng@bupt.edu.cn} 
}

\AtBeginDocument{\nolinenumbers}

\begin{document}
\maketitle

\begin{abstract}

Retrieval-augmented generation (RAG) improves the response quality of large language models (LLMs) by retrieving knowledge from external databases. Typical RAG approaches split the text database into chunks, organizing them in a flat structure for efficient searches. To better capture the inherent dependencies and structured relationships across the text database, researchers propose to organize textual information into an indexing graph, known as \textit{graph-based RAG}. However, we argue that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance. To overcome these limitations, we propose PathRAG, which retrieves key relational paths from the indexing graph, and converts these paths into textual form for prompting LLMs. Specifically, PathRAG effectively reduces redundant information with flow-based pruning, while guiding LLMs to generate more logical and coherent responses with path-based prompting. Experimental results show that PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions. The code is available at the following link: 
\href{github link}{https://github.com/BUPT-GAMMA/PathRAG}

\end{abstract}

\section{Introduction}
\begin{figure}[h]
    \includegraphics[width=\linewidth]{Figs/introduction.pdf}
    \caption{Comparison between different graph-based RAG methods. GraphRAG~\cite{edge2024graphrag} uses all the information within certain communities, while LightRAG~\cite{guo2024lightrag} uses all the immediate neighbors of query-related nodes. In contrast, our PathRAG focuses on key relational paths between query-related nodes to alleviate noise and reduce token consumption. }
    \label{fig:Ego graph information vs Path information}
\end{figure}


Retrieval-augmented generation (RAG) empowers large language models (LLMs) to access up-to-date or domain-specific knowledge from external databases, enhancing the response quality without additional training~\cite{gao2022HyDE,gao2023retrieval-naiverag,RAG—survey-2024,kg-survey-2024}. Most RAG approaches divide the text database into chunks, organizing them in a flat structure to facilitate efficient and precise searches~\cite{finardi2024chronicles,yepes2024financial,lyu2024crud}.

To better capture the inherent dependencies and structured relationships across texts in a database, researchers have introduced graph-based RAG~\cite{edge2024graphrag,guo2024lightrag}, which organizes textual information into an indexing graph. In this graph, nodes represent entities extracted from the text, while edges denote the relationships between these entities. Traditional RAG~\cite{liu2021kg,yasunaga2021kg-rag,gao2021kg-rag} usually focuses on questions that can be answered with local information about a single entity or relationship. In contrast, graph-based RAG targets on  global-level questions that need the information across a database to generate a summary-like response. For example, GraphRAG \cite{edge2024graphrag} first applies community detection on the graph, and then gradually summarizes the information in each community. The final answer is generated based on the most query-relevant communities. LightRAG \cite{guo2024lightrag} extracts both local and global keywords from input queries, and retrieves relevant nodes and edges using these keywords. The ego-network information of the retrieved nodes is then used as retrieval results. 

However, we argue that the information considered in previous graph-based RAG methods is often redundant, which can introduce noise, degrade model performance, and increase token consumption. As shown in Figure ~\ref{fig:Ego graph information vs Path information} (a), GraphRAG method uses all the information from the nodes and edges within certain communities. Similarly, as shown in Figure ~\ref{fig:Ego graph information vs Path information} (b), LightRAG retrieves the immediate neighbors of query-related nodes to generate answers. The redundant information retrieved in these two methods may act as noise, and negatively impact the subsequent generation. Moreover, both methods adopt a flat structure to organize retrieved information in the prompts, \textit{e.g.,} directly concatenating the textual information of all retrieved nodes and edges, resulting in answers with suboptimal logicality and coherence.

To overcome the above limitations, we propose PathRAG, which performs key path retrieval among retrieved nodes and converts these paths into textual form for LLM prompting. As shown in Figure ~\ref{fig:Ego graph information vs Path information} (c), we focus on the key relational paths between retrieved nodes to alleviate noise and reduce token consumption. Specifically, we first retrieve relevant nodes from the indexing graph based on the keywords in the query. Then we design a flow-based pruning algorithm with distance awareness to identify the key relational paths between each pair of retrieved nodes. The pruning algorithm enjoys low time complexity, and can assign a reliability score to each retrieved path. Afterward, we sequentially concatenate the node and edge information alongside each path as textual relational paths. Considering the ``lost in the middle'' issue of LLMs~\cite{liu2024lost}, we place the textual paths into the prompt in ascending order of reliability scores for better answer generation. To evaluate the effectiveness of PathRAG, we follow the four benchmark datasets used in previous work~\cite{qian2024memorag}, and additionally explore two larger ones. Experimental results on six datasets show that PathRAG generates better answers across all five evaluation dimensions compared to the state-of-the-art baselines. Compared to GraphRAG and LightRAG, the average win rates of PathRAG are 60.44\% and 58.46\%, respectively. The advantages of PathRAG are more significant for larger datasets, making it better aligned with real-world applications. The contributions of this work are as follows:

$\bullet$ We highlight that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance.

$\bullet$ We propose PathRAG, which efficiently retrieves key relational paths from an indexing graph with flow-based pruning, and effectively generates answers with path-based LLM prompting. 

$\bullet$ PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions. Extensive experiments further validate the design of PathRAG.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figs/model.pdf}
    \caption{The overall framework of our proposed PathRAG with three main stages. 1) Node Retrieval Stage: Relevant nodes are retrieved from the indexing graph based on the keywords in the query; 2) Path Retrieval Stage: We design a flow-based pruning algorithm to extract key relational paths between each pair of retrieved nodes, and then retrieve paths with the highest reliability scores; 3) Answer Generation Stage: The retrieved paths are placed into prompts in ascending order of reliability scores, and finally fed into an LLM for answer generation.}
    \label{fig:model_overview}
\end{figure*}

\section{Related Work}

\textbf{Text-based RAG}.
To improve text quality~\cite{fang2024enhancing,xu2024unsupervised,zhu2024information} and mitigate hallucination effects \cite{lewis2020retrieval,guu2020retrieval}, retrieval-augmented generation (RAG) is widely used in large language models (LLMs) by leveraging external databases. These databases primarily store data in textual form, containing a vast amount of domain knowledge that LLMs can directly retrieve. We refer to such systems as text-based RAG. Based on different retrieval mechanisms \cite{RAG—survey-2024}, text-based RAG can be broadly classified into two categories: \textbf{sparse vector retrieval} \cite{alon2022neuro,schick2023toolformer,jiang2023active,cheng2024lift} and \textbf{dense vector retrieval} \cite{lewis2020retrieval,hofstatter2023fid,li2024llama2vec,zhang2024arl2}. Sparse vector retrieval typically identifies the most representative words in each text segment by word frequency, and retrieves relevant text for a specific query based on keyword matching. In contrast, dense vector retrieval addresses issues like lexical mismatches and synonyms by encoding both query terms and text into vector embeddings. It then retrieves relevant content based on the similarity between these embeddings. However, most text-based RAG methods use a flat organization of text segments, and fail to capture essential relationships between chunks (\textit{e.g.,} the contextual dependencies), limiting the quality of LLM-generated responses~\cite{edge2024graphrag,guo2024lightrag}.

\textbf{KG-RAG}.
Besides text databases, researchers have proposed retrieving information from knowledge graphs (KGs), known as KG-RAG ~\cite{yasunaga2021kg-rag,gao2021kg-rag,li2024subgraphrag,kg-survey-2024,he2025g}. These methods can utilize existing KGs~\cite{wen2023mindmap,dehghan2024ewek} or their optimized versions~\cite{fang2024reano,panda2024holmes}, and enable LLMs to retrieve information of relevant entities and their relationships. Specifically, KG-RAG methods typically extract a local subgraph from the KG~\cite{bordes2015large,talmor2018web,gu2021beyond}, such as the immediate neighbors of the entity mentioned in a query. However, most KG-RAG methods focus on addressing questions that can be answered with a single entity or relation in the KG~\cite{joshi2017triviaqa,yang2018hotpotqa,kwiatkowski2019natural,ho2020constructing}, narrowing the scope of their applicability. 

\textbf{Graph-based RAG}. Instead of utilizing pre-constructed KGs,
graph-based RAG~\cite{edge2024graphrag,guo2024lightrag} typically organizes text databases as text-associated graphs, and focuses on global-level questions that need the information from multiple segments across a database. The graph construction process often involves extracting entities from the text and identifying relationships between these entities. Also, contextual information is included as descriptive text to minimize the information loss during the text-to-graph conversion. GraphRAG \cite{edge2024graphrag} first applies community detection algorithms on the graph, and then gradually aggregates the information from sub-communities to form higher-level community information. LightRAG \cite{guo2024lightrag} adopts a dual-stage retrieval framework to accelerate the retrieval process. First, it extracts both local and global keywords from the question. Then, it retrieves relevant nodes and edges using these keywords, treating the ego-network information of the retrieved nodes as the final retrieval results. This approach simplifies the retrieval process and effectively handles global-level tasks. However, the retrieved information covers all immediate neighbors of relevant nodes, which may introduce noise harming the answer quality. We also notice a concurrent work MiniRAG~\cite{fan2025minirag} that leverages path information to assist retrieval. But they focus on addressing questions that can be answered by the information of a specific node, and thus explore paths between query-related and answer-related nodes like KG reasoning~\cite{yasunaga2021kg-rag,liu2021kg,tian2022knowledge}. Their implementation details such as path discovery and integration are also quite different from ours.

\section{Preliminaries}

In this section we will introduce and formalize the workflow of a graph-based RAG system.

Instead of storing text chunks as an unordered collection, graph-based RAG automatically structures a text database into an \textbf{indexing graph} as a preprocessing step. Given a text database, the entities and their interrelations within the textual content are identified by LLMs, and utilized to construct the node set \(\mathcal{V}\) and edge set \(\mathcal{E}\). Specifically, each node \(v \in \mathcal{V}\) represents a distinct entity with an identifier \(k_v\) (\textit{e.g.,} entity name) and a textual chunk \(t_v\) (\textit{e.g.,} associated text snippets), while each edge \(e \in \mathcal{E}\) represents the relationship between entity pairs with a descriptive textual chunk \(t_e\) to enrich relational context. We denote the indexing graph as \(\mathcal{\mathcal G = (\mathcal V, \mathcal E, \mathcal K_{\mathcal V}, \mathcal T)}\), where \(\mathcal K_{\mathcal V}\) represent the collection of node identifiers and \(\mathcal T\) is the collection of textual chunks in the indexing graph.

Given a query \( q\), a graph-oriented retriever extracts relevant nodes and edges in the indexing graph. Then the textual chunks of retrieved elements are integrated with query \( q \) to obtain the answer by an LLM generator. The above process can be simplified as:
\begin{equation}
    \mathcal{A}(q, \mathcal G) = \mathcal F \circ \mathcal M (q;\mathcal R(q, \mathcal G)),
\end{equation}
where \( \mathcal A \) denotes the augmented generation with retrieval results, \(\mathcal R\) means the graph-oriented retriever, \( \mathcal M \) and \( \mathcal F \) represent the prompt template and the LLM generator, respectively. In this paper, we primarily focus on designing a more effective graph-oriented retriever and the supporting prompt template to achieve a better graph-based RAG.

\section{Methodology}

In this section, we propose a novel graph-based RAG framework with the path-based retriever and a tailored prompt template, formally designated as PathRAG. As illustrated in Figure \ref{fig:model_overview}, the proposed framework operates on an indexing graph through three sequential stages: node retrieval, path retrieval, and answer generation.

\subsection{Node Retrieval}

In this stage, we identify keywords from the input query by LLMs, and accordingly extract relevant nodes from the indexing graph. Given a query \(q\), an LLM is utilized to extract keywords from the query text. The collection of keywords extracted from query \( q \) is denoted as \( \mathcal{K}_{q} \). Based on the extracted keywords, dense vector matching is employed to retrieve related nodes in the indexing graph \( \mathcal{G} \). In dense vector matching, the relevance between a keyword and a node is calculated by their similarity in the semantic embedding space, where the commonly used cosine similarity is adopted in our method. Specifically, we first encode both node identifiers and the extracted keywords using a semantic embedding model \( f: \mathcal{K}_{q} \cup \mathcal{K}_{\mathcal{V}} \rightarrow \mathcal{X}_{q} \cup \mathcal{X}_{\mathcal{V}} \), where \( \mathcal{X}_{\mathcal{V}} = \{x_{v}\}_{v \in \mathcal{V}} \) represents the embeddings of node identifiers, and \( \mathcal{X}_{q} = \{x_{q,i}\}_{i=1}^{|\mathcal{K}_{q}|} \) denotes the embeddings of the extracted keywords. Based on the obtained embeddings above, we then iterate over \( \mathcal{X}_{q} \) to search the most relevant nodes among \( \mathcal{X}_{\mathcal{V}} \) with the embedding similarity, until a predefined number \( N \) of nodes is reached. The resulting subset of retrieved nodes is denoted as \( \mathcal{V}_{q} \subseteq \mathcal{V} \).

\subsection{Path Retrieval}
In this subsection, we introduce the path retrieval module that aggregates textual chunks in the form of relational paths to capture the connections between retrieved nodes.

Given two distinct retrieved nodes \(v_\text{start}, v_\text{end}\in \mathcal{V}_q\), there could be many reachable paths between them. Since not all paths are helpful to the task, further refinement is needed to enhance both effectiveness and efficiency. Inspired by the resource allocation strategy~\cite{lu2011link,lin2015flowing}, we propose a flow-based pruning algorithm with distance awareness to extract key paths.

\iffalse
(TODO: xx algorithm) is first applied to search for the reachable paths between them. We denote each reachable path as an ordered sequence 
\( P = v_{0} \xrightarrow{e_{0}} \cdots v_{i} \xrightarrow{e_{i}} \cdots  = (\mathcal V_{P}, \mathcal E_{P})\), where \(v_{i}\) and \(e_{i}\) represent the \(i\)-th node and directed edge, and \(\mathcal V_{P}\) and \(\mathcal E_{P}\) represent the set of nodes and edges in the path \(\mathcal P\), respectively. Notably, the path is treated as directed to model more fine-grained relations.

We collect all reachable paths between this node pair and denote them as \(\mathcal P\).


Considering that not all paths are task-relevant, further refinement of these paths are necessary. Inspired by the resource allocation strategy~\cite{lu2011link,lin2015flowing}, we propose a distance-aware flow-based pruning algorithm to extract high-reliability relational paths. We integrate all reachable paths between node pairs into a directed graph, which includes all nodes \(\mathcal V_{\mathcal P}\) and directed edges \(\mathcal E_{\mathcal P}\) appearing in the reachable path set, denoted as \(\mathcal G_{\mathcal P} = (\mathcal V_{\mathcal P}, \mathcal E_{\mathcal P})\). 
\fi

Formally, we denote the sets of nodes pointing to \(v_i\) and nodes pointed by \(v_i\) as \(\mathcal N(v_i,\cdot)\) and \(\mathcal N(\cdot, v_i)\), respectively. We define the resource of node \(v_i\) as \(\mathcal{S}(v_i)\). We set \(\mathcal{S}(v_\text{start}) = 1\) and initialize other resources to $0$, followed by propagating the resources through the neighborhood. The resource flowing to \(v_i\) is defined as:
\begin{equation}
  \label{eq:flow-algorithm }
  \mathcal{S}(v_i) = \sum_{v_j \in \mathcal N(\cdot,v_i)}\frac{\alpha \cdot \mathcal{S}(v_j)}{\lvert \mathcal N(v_j,\cdot)\rvert},
\end{equation}
where $\alpha$ represents the decay rate of information propagation along the edges. Based on the assumption that the closer two nodes are in the indexing graph, the stronger their connection will be, we introduce this penalty mechanism to enable the retriever to perceive distance. It is crucial to emphasize that our approach differs from strictly sorting paths with a limited number of hops. Detailed comparative experiments will be presented in subsequent sections. 

Notably, due to the decay penalty and neighbor allocation, nodes located far from the initial node are assigned with negligible resources.
Therefore, we introduce an early stopping strategy to prune paths in advance when 
\begin{equation}
    \frac{\mathcal{S}(v_i)}{\lvert\mathcal N(v_i,\cdot) \rvert}<\theta,
\end{equation}
where \(\theta\) is the pruning threshold. This ensures that the algorithm terminates early for nodes that contribute minimally to the overall propagation. For efficiency concerns, we update the resource of a node at most once. 


We denote each path as an ordered sequence 
\( P = v_{0} \xrightarrow{e_{0}} \cdots v_{i} \xrightarrow{e_{i}} \cdots  = (\mathcal V_{P}, \mathcal E_{P})\), where \(v_{i}\) and \(e_{i}\) represent the \(i\)-th node and directed edge, and \(\mathcal V_{P}\) and \(\mathcal E_{P}\) represent the set of nodes and edges in the path \(\mathcal P\), respectively. 
For each path $P=(\mathcal{V}_P,\mathcal E_{P})$, we calculate the average resource values flowing through its edges as the measurement of reliability, which can be formulated as:
\begin{equation}
  \label{eq:avg-equation }
  \mathcal S(P) = \frac{1}{{\lvert \mathcal E_{P}\rvert}}\sum_{v_i \in \mathcal{V}_P}S(v_i),
\end{equation}
where \(|\mathcal E_{P}|\) is the number of edges in the path. Then, we sort these paths based on the reliability \(\mathcal S(P)\) and retain only the most reliable relational paths for this node pair. These paths are added to the global candidate pool in the form of path-reliability pair \((P, \mathcal S(P))\). We repeat the above process for each distinct node pair, ultimately obtaining all candidate paths. Then the top-\(K\) reliable paths can be obtained from the candidate pool to serve as the retrieval information of query \(q\) for subsequent generation, which we denote as \(\mathcal P_{q}\).

\subsection{Answer Generation}
For better answer generation, we establish path prioritization based on their reliability, then strategically position these paths to align with LLMs' performance patterns~\cite{qin2023large,liu2024lost,cuconasu2024power}. 

Formally, for each retrieved relational path, we concatenate the textual chunks of all nodes and edges within the path to obtain a textual relational path, which can be formulated as:
\begin{equation}
    t_{P} = \operatorname{concat}([\cdots;t_{v_i}; t_{e_i};t_{v_{i+1}};\cdots]),
\end{equation}
where \(\operatorname{concat}(\cdot)\) denotes the concatenation operation, \(v_i\) and \(e_i\) are the \(i\)-th node and edge in the path \(P\), respectively. 

Considering the ``lost in the middle'' issue~\cite{liu2024lost,cao2024graphinsight,firooz2024lost} for LLMs in long-context scenarios, directly aggregating the query with different relational paths may lead to suboptimal results. Therefore, we position the most critical information at the two ends of the template, which is regarded as the golden memory region for LLM comprehension. Specifically, we place the query at the beginning of the template and organize the textual relational paths in a reliability ascending order, ensuring that the most reliable relational path is positioned at the end of the template. The final prompt can be denoted as:
\begin{equation}
    \mathcal M (q;\mathcal R(q, \mathcal G))= \operatorname{concat}([q;t_{P_K};\cdots;t_{P_1}]),
\end{equation}
where \(P_1\) is the most reliable path and \(P_K\) is the \(K\)-th reliable path. This simple prompting strategy can significantly improve the response performance of LLM compared with placing the paths in a random or reliability ascending order in our experiments.

{
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{3pt}
\FloatBarrier
\begin{table*}[ht]
\caption{Performance across six datasets and five evaluation dimensions in terms of win rates.}
\vspace{-0.4cm}
\centering
\normalsize
    \resizebox{\textwidth}{!}{
    \begin{tabular}
    {lcccccccccccc}
    \Xhline{0.8pt}
     & \multicolumn{2}{c}{\textbf{Agriculture}} &  \multicolumn{2}{c}{\textbf{Legal}} & \multicolumn{2}{c}{\textbf{History}} & \multicolumn{2}{c}{\textbf{CS}} & \multicolumn{2}{c}{\textbf{Biology}} & \multicolumn{2}{c}{\textbf{Mix}}	  \\ 
     \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
     & NaiveRAG & \textbf{PathRAG} & NaiveRAG & \textbf{PathRAG} & NaiveRAG & \textbf{PathRAG} & NaiveRAG & \textbf{PathRAG} & NaiveRAG & \textbf{PathRAG} & NaiveRAG & \textbf{PathRAG} 
     \\ 
    \hline
    Comprehensiveness & \cellcolor{gray!30} 37.60\% & \underline{62.40\%} & \cellcolor{gray!30} 31.45\% & \underline{68.55\%} & \cellcolor{gray!30} 33.87\% & \underline{66.13\%} & \cellcolor{gray!30}39.52\% & \underline{60.48\%} & \cellcolor{gray!30}35.48\% & \underline{64.52\%} & \cellcolor{gray!30}41.60\% & \underline{58.40\%} 
    \\
    Diversity & \cellcolor{gray!30}32.26\% & \underline{67.74\%} & \cellcolor{gray!30}24.39\% & \underline{75.61\%} & \cellcolor{gray!30}36.29\% & \underline{63.71\%} & \cellcolor{gray!30}42.40\% & \underline{57.60\%} & \cellcolor{gray!30}41.13\% & \underline{58.87\%} & \cellcolor{gray!30}33.06\% & \underline{66.94\%}
    \\
    Logicality & \cellcolor{gray!30}35.48\% & \underline{64.52\%} & \cellcolor{gray!30}35.20\% & \underline{64.80\%} & \cellcolor{gray!30}43.55\% & \underline{56.45\%} & \cellcolor{gray!30}36.29\% & \underline{63.71\%} & \cellcolor{gray!30}44.35\% & \underline{55.65\%} & \cellcolor{gray!30}43.20\% & \underline{56.80\%}
    \\
    Relevance & \cellcolor{gray!30}40.80\% & \underline{59.20\%} & \cellcolor{gray!30}26.61\% & \underline{73.39\%} & \cellcolor{gray!30}42.40\% & \underline{57.60\%} & \cellcolor{gray!30}37.39\% & \underline{62.61\%} & \cellcolor{gray!30}34.67\% & \underline{65.33\%} & \cellcolor{gray!30}41.94\% & \underline{58.06\%}
    \\
    Coherence & \cellcolor{gray!30}38.21\% & \underline{61.79\%} & \cellcolor{gray!30}33.06\% & \underline{66.94\%} & \cellcolor{gray!30}44.00\% & \underline{56.00\%} & \cellcolor{gray!30}38.71\% & \underline{61.29\%} & \cellcolor{gray!30}34.68\% & \underline{65.32\%} & \cellcolor{gray!30}37.60\% & \underline{62.40\%}
    \\
    \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
     & HyDE & \textbf{PathRAG} & HyDE & \textbf{PathRAG} & HyDE & \textbf{PathRAG} & HyDE & \textbf{PathRAG} & HyDE & \textbf{PathRAG} & HyDE & \textbf{PathRAG} \\
    \hline
    Comprehensiveness & \cellcolor{gray!30}38.02\% & \underline{61.98\%} & \cellcolor{gray!30}38.40\% & \underline{61.60\%} & \cellcolor{gray!30}34.68\% & \underline{65.32\%} & \cellcolor{gray!30}40.80\% & \underline{59.20\%} & \cellcolor{gray!30}33.06\% & \underline{66.94\%} & \cellcolor{gray!30}42.74\% & \underline{57.26\%}
    \\
    Diversity & \cellcolor{gray!30}36.29\% & \underline{63.71\%} & \cellcolor{gray!30}21.60\% & \underline{78.40\%} & \cellcolor{gray!30}34.68\% & \underline{65.32\%} & \cellcolor{gray!30}39.52\% & \underline{60.48\%} & \cellcolor{gray!30}36.00\% & \underline{64.00\%} & \cellcolor{gray!30}33.87\% & \underline{66.13\%}
    \\
    Logicality & \cellcolor{gray!30}44.00\% & \underline{56.00\%} & \cellcolor{gray!30}30.33\% & \underline{69.67\%} & \cellcolor{gray!30}38.21\% & \underline{61.79\%} & \cellcolor{gray!30}38.71\% & \underline{61.29\%} & \cellcolor{gray!30}45.08\% & \underline{54.92\%} & \cellcolor{gray!30}45.53\% & \underline{54.47\%}
    \\
    Relevance & \cellcolor{gray!30}39.34\% & \underline{60.66\%} & \cellcolor{gray!30}35.48\% & \underline{64.52\%} & \cellcolor{gray!30}35.77\% & \underline{64.23\%} & \cellcolor{gray!30}37.39\% & \underline{62.61\%} & \cellcolor{gray!30}46.34\% & \underline{53.66\%} & \cellcolor{gray!30}43.55\% & \underline{56.45\%}
    \\
    Coherence & \cellcolor{gray!30}41.46\% & \underline{58.54\%} & \cellcolor{gray!30}41.94\% & \underline{58.06\%} & \cellcolor{gray!30}40.32\% & \underline{59.68\%} & \cellcolor{gray!30}37.60\% & \underline{62.40\%} & \cellcolor{gray!30}41.94\% & \underline{58.06\%} & \cellcolor{gray!30}45.60\% & \underline{54.40\%}
    \\
    \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
     & GraphRAG & \textbf{PathRAG} & GraphRAG & \textbf{PathRAG} & GraphRAG & \textbf{PathRAG} & GraphRAG & \textbf{PathRAG} & GraphRAG & \textbf{PathRAG} & GraphRAG & \textbf{PathRAG} \\
    \hline
    Comprehensiveness & \cellcolor{gray!30}44.72\% & \underline{55.28\%} & \cellcolor{gray!30}33.87\% & \underline{66.13\%} & \cellcolor{gray!30}41.13\% & \underline{58.87\%} & \cellcolor{gray!30}37.60\% & \underline{62.40\%} & \cellcolor{gray!30}39.52\% & \underline{60.48\%} & \cellcolor{gray!30}41.13\% & \underline{58.87\%}
    \\
    Diversity & \cellcolor{gray!30}45.97\% & \underline{54.03\%} & \cellcolor{gray!30}29.84\% & \underline{70.16\%} & \cellcolor{gray!30}36.59\% & \underline{63.41\%} & \cellcolor{gray!30}42.74\% & \underline{57.26\%} & \cellcolor{gray!30}38.21\% & \underline{61.79\%} & \cellcolor{gray!30}36.29\% & \underline{63.71\%} 
    \\
    Logicality & \cellcolor{gray!30}32.52\% & \underline{67.48\%} & \cellcolor{gray!30}41.60\% & \underline{58.40\%} & \cellcolor{gray!30}43.55\% & \underline{56.45\%} & \cellcolor{gray!30}37.39\% & \underline{62.61\%} & \cellcolor{gray!30}34.45\% & \underline{65.55\%} & \cellcolor{gray!30}41.94\% & \underline{58.06\%}
    \\
    Relevance & \cellcolor{gray!30}43.09\% & \underline{56.91\%} & \cellcolor{gray!30}40.65\% & \underline{59.35\%} & \cellcolor{gray!30}43.55\% & \underline{56.45\%} & \cellcolor{gray!30}34.68\% & \underline{65.32\%} & \cellcolor{gray!30}42.28\% & \underline{57.72\%} & \cellcolor{gray!30}40.32\% & \underline{59.68\%}
    \\
    Coherence & \cellcolor{gray!30}41.13\% & \underline{58.87\%} & \cellcolor{gray!30}38.21\% & \underline{61.79\%} & \cellcolor{gray!30}40.80\% & \underline{59.20\%} & \cellcolor{gray!30}38.02\% & \underline{61.98\%} & \cellcolor{gray!30}43.55\% & \underline{56.45\%} & \cellcolor{gray!30}41.60\% & \underline{58.40\%}
    \\
    \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
     & LightRAG & \textbf{PathRAG} & LightRAG & \textbf{PathRAG} & LightRAG & \textbf{PathRAG} & LightRAG & \textbf{PathRAG} & LightRAG & \textbf{PathRAG} & LightRAG & \textbf{PathRAG} \\
    \hline
    Comprehensiveness & \cellcolor{gray!30}41.94\% & \underline{58.06\%} & \cellcolor{gray!30}36.29\% & \underline{63.71\%} & \cellcolor{gray!30}42.74\% & \underline{57.26\%} & \cellcolor{gray!30}43.20\% & \underline{56.80\%} & \cellcolor{gray!30}44.72\% & \underline{55.28\%} & \cellcolor{gray!30}44.80\% & \underline{55.20\%}
    \\
    Diversity & \cellcolor{gray!30}41.46\% & \underline{58.54\%} & \cellcolor{gray!30}36.49\% & \underline{63.51\%} & \cellcolor{gray!30}43.90\% & \underline{56.10\%} & \cellcolor{gray!30}45.16\% & \underline{54.84\%} & \cellcolor{gray!30}43.09\% & \underline{56.91\%} & \cellcolor{gray!30}42.74\% & \underline{57.26\%}
    \\
    Logicality & \cellcolor{gray!30}43.09\% & \underline{56.91\%} & \cellcolor{gray!30}39.84\% & \underline{60.16\%} & \cellcolor{gray!30}38.71\% & \underline{61.29\%} & \cellcolor{gray!30}44.72\% & \underline{55.28\%} & \cellcolor{gray!30}45.60\% & \underline{54.40\%} & \cellcolor{gray!30}41.94\% & \underline{58.06\%}
    \\
    Relevance & \cellcolor{gray!30}39.20\% & \underline{60.80\%} & \cellcolor{gray!30}37.81\% & \underline{62.19\%} & \cellcolor{gray!30}41.13\% & \underline{58.87\%} & \cellcolor{gray!30}41.46\% & \underline{58.54\%} & \cellcolor{gray!30}42.28\% & \underline{57.72\%} & \cellcolor{gray!30}40.65\% & \underline{59.35\%}
    \\
    Coherence & \cellcolor{gray!30}40.80\% & \underline{59.20\%} & \cellcolor{gray!30}36.29\% & \underline{63.71\%} & \cellcolor{gray!30}41.46\% & \underline{58.54\%} & \cellcolor{gray!30}41.60\% & \underline{58.40\%} & \cellcolor{gray!30}43.55\% & \underline{56.45\%} & \cellcolor{gray!30}39.52\% & \underline{60.48\%}
    \\
    \hline
    \end{tabular}
}
\vspace{-0.2cm}
\label{main_results}
\end{table*}
}


\subsection{Discussion}
\textbf{Complexity Analysis of Path Retrieval.} After the $i$-th step of resource propagation, there are at most $\frac{\alpha^i}{\theta}$ nodes alive due to the decay penalty and early stopping. Hence the total number of nodes involved in this propagation is at most $\sum_{i=0}^\infty \alpha^i/\theta=\frac{1}{(1-\alpha)\theta}$. Thus the complexity of extracting candidate paths between all node pairs is $\mathcal{O}(\frac{N^2}{(1-\alpha)\theta})$. In our settings, the number of retrieved nodes $N\in [10,60]$ is much less than the total number of nodes in the indexing graph $|\mathcal{V}|\sim 10^4$. Thus the time complexity is completely acceptable.

\textbf{Necessity of Path-based Prompting.} Note that different retrieved paths may have shared nodes or edges. To reduce the prompt length, it is possible to flatten the paths and remove duplications as a set of nodes and edges. However, this conversion will lose the semantic relations between the two endpoints of each path. We also validate the necessity of path-based prompting in the experiments.


\section{Experiments}
We conduct extensive experiments to answer the following research questions (\textbf{RQs}): \textbf{RQ1:} How effective is our proposed \modelname compared to the state-of-the-art baselines? 
\textbf{RQ2:} How do different values of key hyperparameters influence the method's performance? 
\textbf{RQ3:} Has each component of our framework played its role effectively? 
\textbf{RQ4:} How much token cost does \modelname require to achieve the performance of other baselines? 
\textbf{RQ5:} Do the RAG response and its evaluation of \modelname offer some interpretability?

\subsection{Experimental Setup}
\subsubsection{\textbf{Datasets}}
We follow the settings of LightRAG~\cite{guo2024lightrag} and evaluate our model using the UltraDomain benchmark \cite{qian2024memorag}. The UltraDomain data is sourced from 428 college textbooks across 18 distinct domains. Besides the four domains used in LightRAG’s evaluation (Agriculture, Legal, Computer Science, and Mix), we extend two more domains (History and Biology), and consider six datasets in total. The token counts of the six datasets range from $600,000$ to $5,000,000$. We also follow the standardized process from GraphRAG and LightRAG for dataset preprocessing. Detailed information about the datasets can be found in the Appendix~\ref{sec:appendix-datasets}.

\subsubsection{\textbf{Baselines}}
We compare PathRAG with four state-of-the-art methods: NaiveRAG \cite{gao2023retrieval-naiverag}, HyDE \cite{gao2022HyDE}, GraphRAG \cite{edge2024graphrag}, and LightRAG \cite{guo2024lightrag}. These methods cover cutting-edge text-based and graph-based RAG approaches. Detailed descriptions of the baselines can be found in the Appendix~\ref{sec:appendix-baselines}.

\subsubsection{\textbf{Implementation Details}} 
To ensure fairness and consistency across experiments, we uniformly use ``GPT-4o-mini'' for all LLM-related components across both the baseline methods and our approach. Also, the indexing graphs for different graph-based RAG methods are the same as GraphRAG~\cite{edge2024graphrag}. Retrieved edges that correspond to global keywords of LightRAG are placed after the query. For the key hyperparameters of PathRAG, the number of retrieval nodes \(N\) is selected from \(\{10,20,30,40,50,60\}\), the number of paths \(K\) is varied within \(\{5,10,15,20,25\}\), the decay rate \(\alpha\) is chosen from \(\{0.6,0.7,0.8,0.9,1.0\}\), and the threshold $\theta$ is fixed as $0.05$. 


\subsubsection{\textbf{Evaluation Metrics}}

Due to the absence of ground truth answers, we follow the LLM-based evaluation procedures as GraphRAG and LightRAG. Specifically, we utilize ``GPT-4o-mini'' to evaluate the generated answers across multiple dimensions. The evaluation dimensions are based on those from GraphRAG and LightRAG, including Comprehensiveness and Diversity, while also incorporating three new dimensions from recent advances in LLM-based evaluation~\cite{chan2023chateval}, namely Logicality, Relevance, and Coherence. We compare the answers generated by each baseline and our method and conduct win-rate statistics. A higher win rate indicates a greater performance advantage over the other. Note that the presentation order of two answers will be alternated, and the average win rates will be reported. Detailed descriptions of these evaluation dimensions can be found in Appendix~\ref{sec:appendix-evaluation dimensionsy}.


\begin{figure*}[ht]
    \centering    \includegraphics[width=0.95\textwidth,height=0.14\textheight]{Figs/parameters.pdf}
    \caption{Impact of three hyperparameters in PathRAG on the Legal dataset.}
    \label{fig:Parameter}
\end{figure*}

\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{3pt}
\begin{table*}[ht]
\caption{Ablation study on the path retrieval algorithm of PathRAG.}
\normalsize
\vspace{-0.4cm}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}
    {lcccccccccccc}
    \Xhline{0.8pt}
     & \multicolumn{2}{c}{\textbf{Agriculture}} &  \multicolumn{2}{c}{\textbf{Legal}} & \multicolumn{2}{c}{\textbf{History}} & \multicolumn{2}{c}{\textbf{CS}} & \multicolumn{2}{c}{\textbf{Biology}} & \multicolumn{2}{c}{\textbf{Mix}}	  \\ 
     \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
     & Random & \textbf{Flow-based} & Random & \textbf{Flow-based} & Random & \textbf{Flow-based} & Random & \textbf{Flow-based} & Random & \textbf{Flow-based} & Random & \textbf{Flow-based} \\
    \hline
    Comprehensiveness & \cellcolor{gray!30}44.80\% & \underline{55.20\%} & \cellcolor{gray!30}46.77\% & \underline{53.23\%} & \cellcolor{gray!30}45.97\% & \underline{54.03\%} & \cellcolor{gray!30}38.40\% & \underline{61.60\%} & \cellcolor{gray!30}44.00\% & \underline{56.00\%} & \cellcolor{gray!30}42.74\% & \underline{57.26\%}
    \\ 
    Diversity & \cellcolor{gray!30}38.40\% & \underline{61.60\%} & \cellcolor{gray!30}49.19\% & \underline{50.81\%} & \cellcolor{gray!30}31.45\% & \underline{68.55\%} & \cellcolor{gray!30}37.70\% & \underline{62.30\%} & \cellcolor{gray!30}29.84\% & \underline{70.16\%} & \cellcolor{gray!30}47.58\% & \underline{52.42\%}
    \\ 
    Logicality & \cellcolor{gray!30}47.97\% & \underline{52.03\%} & \cellcolor{gray!30}46.77\% & \underline{53.23\%} & \cellcolor{gray!30}44.00\% & \underline{56.00\%} & \cellcolor{gray!30}44.63\% & \underline{55.37\%} & \cellcolor{gray!30}41.94\% & \underline{58.06\%} & \cellcolor{gray!30}46.40\% & \underline{53.60\%}
    \\ 
    Relevance & \cellcolor{gray!30}45.45\% & \underline{54.55\%} & \cellcolor{gray!30}44.80\% & \underline{55.20\%} & \cellcolor{gray!30}45.97\% & \underline{54.03\%} & \cellcolor{gray!30}41.46\% & \underline{58.54\%} & \cellcolor{gray!30}45.83\% & \underline{54.17\%} & \cellcolor{gray!30}48.39\% & \underline{51.61\%} 
    \\ 
    Coherence & \cellcolor{gray!30}44.35\% & \underline{55.65\%} & \cellcolor{gray!30}44.60\% & \underline{55.40\%} & \cellcolor{gray!30}40.98\% & \underline{59.02\%} & \cellcolor{gray!30}38.40\% & \underline{61.60\%} & \cellcolor{gray!30}41.46\% & \underline{58.54\%} & \cellcolor{gray!30}47.15\% & \underline{52.85\%}
    \\ 
    \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
     & Hop-first & \textbf{Flow-based} & Hop-first & \textbf{Flow-based} & Hop-first & \textbf{Flow-based} & Hop-first & \textbf{Flow-based} & Hop-first & \textbf{Flow-based} & Hop-first & \textbf{Flow-based} \\
    \hline
    Comprehensiveness & \cellcolor{gray!30}48.78\% & \underline{51.22\%} & \cellcolor{gray!30}44.35\% & \underline{55.65\%} & \cellcolor{gray!30}45.83\% & \underline{54.17\%} & \cellcolor{gray!30}47.15\% & \underline{52.85\%} & \cellcolor{gray!30}48.80\% & \underline{51.20\%} & \cellcolor{gray!30}43.20\% & \underline{56.80\%}
    \\ 
    Diversity & \cellcolor{gray!30}42.98\% & \underline{57.02\%} & \cellcolor{gray!30}36.00\% & \underline{64.00\%} & \cellcolor{gray!30}49.59\% & \underline{50.41\%} & \cellcolor{gray!30}43.55\% & \underline{56.45\%} & \cellcolor{gray!30}45.97\% & \underline{54.03\%} & \cellcolor{gray!30}47.58\% & \underline{52.42\%}
    \\ 
    Logicality & \cellcolor{gray!30}47.58\% & \underline{52.42\%} & \cellcolor{gray!30}45.16\% & \underline{54.84\%} & \cellcolor{gray!30}41.13\% & \underline{58.87\%} & \cellcolor{gray!30}40.80\% & \underline{59.20\%} & \cellcolor{gray!30}44.80\% & \underline{55.20\%} & \cellcolor{gray!30}43.44\% & \underline{56.56\%}
    \\ 
    Relevance & \cellcolor{gray!30}44.72\% & \underline{55.28\%} & \cellcolor{gray!30}43.44\% & \underline{56.56\%} & \cellcolor{gray!30}45.97\% & \underline{54.03\%} & \cellcolor{gray!30}41.46\% & \underline{58.54\%} & \cellcolor{gray!30}37.40\% & \underline{62.60\%} & \cellcolor{gray!30}41.46\% & \underline{58.54\%}
    \\ 
    Coherence & \cellcolor{gray!30}39.34\% & \underline{60.66\%} & \cellcolor{gray!30}41.13\% & \underline{58.87\%} & \cellcolor{gray!30}39.84\% & \underline{60.16\%} & \cellcolor{gray!30}48.80\% & \underline{51.20\%} & \cellcolor{gray!30}42.74\% & \underline{57.26\%} & \cellcolor{gray!30}44.72\% & \underline{55.28\%}
    \\ 
    \hline
    \end{tabular}
}
\vspace{-0.3cm}
\label{table:sorting algorithm}
\end{table*}

\subsection{Main Results (RQ1)}

As shown in Table~\ref{main_results}, \textbf{PathRAG consistently outperforms the baselines across all evaluation dimensions and datasets}.

From the perspective of evaluation dimensions, compared to all baselines, PathRAG shows an average win rate of 60.88\% in Comprehensiveness, 62.75\% in Diversity, 59.78\% in Logicality, 60.47\% in Relevance, and 59.93\% in Coherence on average. These advantages highlight the effectiveness of our proposed path-based retrieval, which contributes to better performance across multiple aspects of the generated responses. From the dataset perspective, \modelname has a win rate of 60.13\% in Agriculture, 60.26\% in CS and 59.02\% in Mix on average. For the larger three datasets, \modelname shows greater advantages, with an average win rate of 65.53\% in Legal, 60.13\% in History and 59.50\% in Biology. This indicates that our proposed PathRAG effectively reduces the impact of irrelevant information when handling larger datasets, making it more aligned with real-world applications and offering stronger practical significance compared to existing RAG baselines.

\subsection{Hyperparameter Analysis (RQ2)}

We adjust one hyperparameter at a time  on the Legal dataset, and then calculate the win rates compared with LightRAG, the best baseline. 

\textbf{Number of retrieved nodes ($N$)}. As shown on the left side of Figure ~\ref{fig:Parameter}, we observe that as $N$ increases, the average win rate gradually improves, peaking at \( N = 40 \), followed by a slight decline. 

This is because the retrieved path information becomes increasingly sufficient as the number of nodes grows. However, as $N$ continues to increase, the retrieved nodes are less relevant to the question and negatively impact the performance.

\textbf{Number of retrieved paths ($K$)}. 
As shown in the middle of Figure ~\ref{fig:Parameter}, 

we observe that as \( K \) increases, the average win rate reaches its peak at \( K = 15 \). When \( K = 25 \), the average win rate drops, meaning that additional retrieved paths can not bring further improvement to the model. In practice, larger datasets prefer larger values of $K$. 

\textbf{Decay rate $\alpha$}. As shown on the right side of Figure ~\ref{fig:Parameter}, when $\alpha = 0.6$, the pruning algorithm prioritizes shorter paths, resulting in an average win rate of only $0.57$. As $\alpha$ increases, the average win rate peaks at $0.63$ when $\alpha = 0.8$, but then begins to decline. At $\alpha = 1.0$, where the decay rate is completely ignored, the average win rate significantly drops. This suggests that prioritizing shorter paths with a proper $\alpha$ serves as effective prior knowledge for the pruning process. 

\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{3pt}
\begin{table*}[ht]
\caption{Ablation study on the prompt format of PathRAG.}
\normalsize
\vspace{-0.4cm}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}
    {lcccccccccccc}
    \Xhline{0.8pt}
     & \multicolumn{2}{c}{\textbf{Agriculture}} &  \multicolumn{2}{c}{\textbf{Legal}} & \multicolumn{2}{c}{\textbf{History}} & \multicolumn{2}{c}{\textbf{CS}} & \multicolumn{2}{c}{\textbf{Biology}} & \multicolumn{2}{c}{\textbf{Mix}}	  \\
     \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
     & Flat & \textbf{Path-based} & Flat & \textbf{Path-based} & Flat & \textbf{Path-based} & Flat & \textbf{Path-based} & Flat & \textbf{Path-based} & Flat & \textbf{Path-based} \\
    \hline
    Comprehensiveness & \cellcolor{gray!30}45.60\% & \underline{54.40\%} & \cellcolor{gray!30}39.52\% & \underline{60.48\%} & \cellcolor{gray!30}48.80\% & \underline{51.20\%} & \cellcolor{gray!30}41.13\% & \underline{58.87\%} & \cellcolor{gray!30}45.53\% & \underline{54.47\%} & \cellcolor{gray!30}49.59\% & \underline{50.41\%}
    \\ 
    Diversity & \cellcolor{gray!30}44.72\% & \underline{55.28\%} & \cellcolor{gray!30}41.94\% & \underline{58.06\%} & \cellcolor{gray!30}39.52\% & \underline{60.48\%} & \cellcolor{gray!30}40.80\% & \underline{59.20\%} & \cellcolor{gray!30}44.35\% & \underline{55.65\%} & \cellcolor{gray!30}43.09\% & \underline{56.91\%}
    \\ 
    Logicality & \cellcolor{gray!30}46.40\% & \underline{53.60\%} & \cellcolor{gray!30}37.19\% & \underline{62.81\%} & \cellcolor{gray!30}45.53\% & \underline{54.47\%} & \cellcolor{gray!30}43.55\% & \underline{56.45\%} & \cellcolor{gray!30}47.97\% & \underline{52.03\%} & \cellcolor{gray!30}41.94\% & \underline{58.06\%}
    \\ 
    Relevance & \cellcolor{gray!30}39.52\% & \underline{60.48\%} & \cellcolor{gray!30}44.72\% & \underline{55.28\%} & \cellcolor{gray!30}48.39\% & \underline{51.61\%} & \cellcolor{gray!30}44.35\% & \underline{55.65\%} & \cellcolor{gray!30}47.58\% & \underline{52.42\%}  & \cellcolor{gray!30}44.80\% & \underline{55.20\%}
    \\ 
    Coherence & \cellcolor{gray!30}41.13\% & \underline{58.87\%} & \cellcolor{gray!30}39.20\% & \underline{60.80\%} & \cellcolor{gray!30}45.60\% & \underline{54.40\%} & \cellcolor{gray!30}46.34\% & \underline{53.66\%} & \cellcolor{gray!30}44.72\% & \underline{55.28\%} & \cellcolor{gray!30}42.28\% & \underline{57.72\%}
    \\ 
    \hline
    \end{tabular}
}
\vspace{-0.2cm}
\label{table:prompt structure}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=0.92\linewidth]{Figs/case_study2.pdf}
    \caption{Case study comparing the answers generated by PathRAG and the best baseline LightRAG.}
    \label{fig:case study2}
    \vspace{-0.5cm}
\end{figure*}

\subsection{Ablation Study (RQ3)}

We conduct ablation experiments to validate the design of PathRAG. A detailed introduction to the variants can be found in Appendix~\ref{sec:appendix-ablation study}.


\textbf{Necessity of path ordering}. We consider two different strategies to rank the retrieved paths in the prompt, namely random and hop-first. As shown in the Table ~\ref{table:sorting algorithm}, the average win rates of PathRAG compared to the random and hop-first variants are respectively 56.75\% and 56.08\%, indicating the necessity of path ordering in the prompts. 

\textbf{Necessity of path-based prompting}. 
While retrieval is conducted using paths, the retrieved information in the prompts does not necessarily need to be organized in the same manner. To assess the necessity of path-based organization, we compare prompts structured by paths with those using a flat organization. As shown in Table~\ref{table:prompt structure}, path-based prompts achieve an average win rate of 56.14\%, outperforming the flat format. In PathRAG, node and edge information within a path is inherently interconnected, and separating them can result in information loss. Therefore, after path retrieval, prompts should remain structured to preserve contextual relationships and enhance answer quality.

\subsection{Token Cost Analysis (RQ4)}

For a fair comparison focusing on token consumption, we also consider a lightweight version of PathRAG with $N=20$ and $K=5$, dubbed as PathRAG-lt. PathRAG-lt performs on par with LightRAG in overall performance, achieving an average win rate of 50.69\%. The average token consumptions per question for LightRAG, PathRAG and PathRAG-lt are $15,837$, $13,318$ and  $8,869$, respectively. Hence PathRAG reduces 16\% token cost with much better performance, and the corresponding monetary cost is only $0.002\$$. PathRAG-lt reduces 44\% tokens while maintaining comparable performance to LightRAG.  These results demonstrate the token efficiency of our method.

\iffalse
\begin{table}
  \centering
  \footnotesize
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Method} & \textbf{LightRAG} & \textbf{PathRAG-lt} & \textbf{PathRAG} \\
    \hline
    \textbf{token cost} & 15,837 & 8,869 & 13,318\\
    \hline
    \textbf{monetary cost} & $2.38 \times 10^{-3}\$ $ & $1.33 \times 10^{-3}\$$ & $2.00 \times 10^{-3}\$$ \\
    \hline
  \end{tabular}
  \caption{Comparison of LightRAG, PathRAG-lt and PathRAG in terms of token, time, and monetary cost.}
  \label{table:cost of PathRAG}
\end{table}
\fi

\subsection{Case Study (RQ5)}
To provide a more intuitive demonstration of the evaluation process, we present a case study from the Agriculture dataset. Given the same question, both LightRAG and PathRAG generate responses based on the retrieved text. The responses are then evaluated by GPT-4o-mini across five dimensions, with justifications provided, as shown in Figure~\ref{fig:case study2}. We highlight the key points in the answers in bold, with LLM justification for winning judgments displayed in blue and losing judgments in purple. The case study demonstrates that our proposed PathRAG provides comprehensive support for answer generation, with clear advantages in all five dimensions.

\section{Conclusion}

In this paper, we propose PathRAG, a novel graph-based RAG method that focuses on retrieving key relational paths from the indexing graph to alleviate noise. PathRAG can efficiently identify key paths with a flow-based pruning algorithm, and effectively generate answers with path-based LLM prompting. Experimental results demonstrate that PathRAG consistently outperforms baseline methods on six datasets. In future work, we will optimize the indexing graph construction process, and consider to collect more human-annotated datasets for graph-based RAG. It is also possible to explore other substructures besides paths. 

\section{Limitations}
This work focuses on how to retrieve relevant information from an indexing graph for answering questions. For a fair comparison with previous methods, the indexing graph construction process is not explored. Also, we prioritize simplicity in our proposed PathRAG, and thus the path retrieval algorithm involves no deep neural networks or parameter training, which may limit the performance. Besides, we follow the evaluation protocol of previous graph-based RAG methods, and the metrics are relative rather than absolute. We will consider to collect more datasets and design new metrics for graph-based RAG in future work.

\bibliography{acl_latex}

\newpage

\appendix

\section{Dataset Descriptions}
\label{sec:appendix-datasets}

We conduct experiments on the following six datasets, and the statistics of each dataset and corresponding indexing graph are shown in Table ~\ref{table:dataset information}.

$\bullet$ Agriculture dataset: This dataset focuses on the agricultural domain, covering various aspects of agricultural practices, such as beekeeping, crop cultivation, and farm management.

$\bullet$ Legal dataset: This dataset focuses on the legal domain, covering various aspects of legal practices, such as case law, legal regulations, and judicial procedures.

$\bullet$ History dataset: This dataset focuses on the field of history, covering various periods, events, and figures throughout time. It includes historical texts, articles, and documents related to world history, significant historical movements, and important historical figures from different regions and cultures.

$\bullet$ CS dataset: This dataset focuses on the field of computer science, covering multiple subfields such as algorithms, data structures, artificial intelligence, machine learning, and computer networks. It particularly provides various practical application examples in the areas of machine learning and big data.

$\bullet$ Biology dataset: This dataset focuses on the field of biology, covering a wide range of topics such as plants, animals, insects, and more. It provides detailed information about the physical characteristics, behaviors, ecosystems, and other aspects of various organisms.

$\bullet$ Mix dataset: This dataset contains a variety of literary classics, including essays, poetry, and biographies, covering multiple fields such as philosophy, history, and literature.

\begin{table*}
  \centering
  \footnotesize
    \caption{Dataset statistics.}
  \begin{tabular}{c|cccccc}
    \toprule
    \textbf{Datasets} & \textbf{Agriculture} & \textbf{Legal} 
    & \textbf{History} & \textbf{CS} & \textbf{Biology} &\textbf{Mix}\\
    \midrule
    \textbf{Number of documents} & 12 & 94 & 26 & 10 & 27 & 61 \\
    \midrule
    \textbf{Number of tokens} & 1,923,163 & 4,719,555 & 5,088,196
& 2,039,199 & 3,234,487 & 602,537
\\
    \midrule
    \textbf{Number of nodes in the indexing graph} & 22,973 & 20,772 & 63,051 & 20,286 & 41,968 & 10,657 \\

    \bottomrule
  \end{tabular}

  \label{table:dataset information}
\end{table*}

\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{3pt}
\begin{table*}[ht]
\caption{Comparison between PathRAG-lt and LightRAG in terms of win rates.}
\normalsize
\vspace{-0.4cm}
    \resizebox{\textwidth}{!}{
    \begin{tabular}
    {lcccccccccccc}
    \Xhline{0.8pt}
     & \multicolumn{2}{c}{Agriculture} &  \multicolumn{2}{c}{Legal} & \multicolumn{2}{c}{History} & \multicolumn{2}{c}{CS} & \multicolumn{2}{c}{Biology} & \multicolumn{2}{c}{Mix}	  \\ 
    \hline
     & LightRAG & \textbf{PathRAG-lt} & LightRAG & \textbf{PathRAG-lt} & LightRAG & \textbf{PathRAG-lt} & LightRAG & \textbf{PathRAG-lt} & LightRAG & \textbf{PathRAG-lt} & LightRAG & \textbf{PathRAG-lt} \\
    \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
    Comprehensiveness & \cellcolor{gray!30}\underline{56.45\%} & 43.55\% &\cellcolor{gray!30} 47.58\% & \underline{52.42\%} &\cellcolor{gray!30} \underline{57.72\%} & 42.28\% &\cellcolor{gray!30} \underline{52.89\%} & 47.11\% &\cellcolor{gray!30} 49.60\% & \underline{50.40\%} &\cellcolor{gray!30} 41.46\% & \underline{58.54\%}
    \\
    Diversity &\cellcolor{gray!30} \underline{52.00\%} & 48.00\% &\cellcolor{gray!30} \underline{56.10\%} & 43.90\% &\cellcolor{gray!30} \underline{54.03\%} & 45.97\% &\cellcolor{gray!30} 48.80\% & \underline{51.20\%} &\cellcolor{gray!30} \underline{52.89\%} & 47.11\% &\cellcolor{gray!30} \underline{52.42\%} & 47.58\%
    \\
    Logicality &\cellcolor{gray!30} 45.16\% & \underline{54.84\%} &\cellcolor{gray!30} 43.09\% & \underline{56.91\%} &\cellcolor{gray!30} 48.80\% & \underline{51.20\%} &\cellcolor{gray!30} 45.60\% & \underline{54.40\%} &\cellcolor{gray!30} 48.78\% & \underline{51.22\%} &\cellcolor{gray!30} 41.94\% & \underline{58.06\%}
    \\
    Relevance &\cellcolor{gray!30} 49.60\% & \underline{50.40\%} &\cellcolor{gray!30} 47.58\% & \underline{52.42\%} &\cellcolor{gray!30} 45.53\% & \underline{54.47\%} &\cellcolor{gray!30} \underline{52.89\%} & 47.11\% &\cellcolor{gray!30} \underline{53.66\%} & 46.34\% &\cellcolor{gray!30} 35.48\% & \underline{64.52\%}
    \\
    Coherence &\cellcolor{gray!30} \underline{52.89\%} & 47.11\% &\cellcolor{gray!30} 47.15\% & \underline{52.85\%} &\cellcolor{gray!30} \underline{52.42\%} & 47.58\% &\cellcolor{gray!30} \underline{51.20\%} & 48.80\% &\cellcolor{gray!30} \underline{52.89\%} & 47.11\% &\cellcolor{gray!30} 42.74\% & \underline{57.26\%}
    \\
    \hline
    \end{tabular}
}
\vspace{-0.2cm}
\label{table:lightweight PathRAG}
\end{table*}


\section{Baseline Descriptions}
\label{sec:appendix-baselines}
The detailed baseline descriptions are as follows:

$\bullet$ \textbf{NaiveRAG}: This method is mainly used for retrieving information from text databases by splitting the text into chunks for storage. During the storage process, the chunks are embedded using text embeddings. For a query, the question is converted into a text embedding, and retrieval is performed based on maximum similarity between the query embedding and the text chunks, enabling efficient and direct access to answers.

$\bullet$ \textbf{HyDE}: This model shares a similar storage framework with NaiveRAG. However, during the query phase, it uses an LLM to generate a hypothetical document based on the question, which is then used to retrieve relevant text chunks and generate the final answer.

$\bullet$ \textbf{GraphRAG}: This is a graph-based RAG. It uses an LLM to extract entities and relationships from the text, representing them as nodes and edges, with descriptions from the original text attached as features to reduce information loss. For each question, a community detection algorithm is applied to summarize and generalize the information contained in the nodes from the bottom up, forming new community descriptions. Finally, the results of the community detection are used to answer global summarization questions. 

$\bullet$ \textbf{LightRAG}: This is also a graph-based RAG, inheriting the graph construction method mentioned in GraphRAG. However, considering the high cost of retrieval in GraphRAG, LightRAG cleverly employs a dual-level retrieval framework, performing more detailed and precise searches in the graph at both local and global levels, significantly reducing token and time consumption.


\section{Evaluation Dimensions}
\label{sec:appendix-evaluation dimensionsy}

LLM will evaluate RAG responses based on the following five dimensions:

$\bullet$ Comprehensiveness: How much detail does the answer provide to cover all aspects and details of the question?

$\bullet$ Diversity: How varied and rich is the answer in providing different perspectives and insights on the question?

$\bullet$ Logicality: How logically does the answer respond to all parts of the question?

$\bullet$ Relevance: How relevant is the answer to the question, staying focused and addressing the intended topic or issue?

$\bullet$ Coherence: How well does the answer maintain internal logical connections between its parts, ensuring a smooth and consistent structure?

\section{Details of Ablated Variants}
\label{sec:appendix-ablation study}

\subsection{Path Ordering}
\label{sec:ranking algorithms}
$\bullet$ \textbf{Random ordering}. We randomly select $K$ paths and place them into the prompt.

$\bullet$ \textbf{Hop-first ordering}. Paths are sorted based on the number of hops. Paths with fewer hops are considered to have more direct relevance. Within the same hop count, paths are randomly ordered. Finally, $K$ paths are selected and arranged in ascending order, placing the most important paths at the end of the prompt to enhance memory retention.

\subsection{Prompt Format}
\label{sec:appendix-prompt organization}

$\bullet$ \textbf{Flat organization}. In this setting, the retrieved paths are decomposed into individual nodes and edges. The order of nodes and edges is randomized and not structured based on their original paths.

\section{Detailed Comparison between PathRAG-lt and LightRAG}
Table~\ref{table:lightweight PathRAG} presents the win rates of PathRAG-lt against LightRAG on six datasets. PathRAG-lt has an overall win rate of 50.69\%.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figs/case_study.pdf}
    \caption{Case study comparing the answers generated by PathRAG and the best baseline LightRAG on the CS dataset.}
    \label{fig:case study}
\end{figure*}

\section{Additional Case study}

We also provide an additional case study comparing PathRAG and LightRAG on the CS dataset. Given the question, ``What derived features should be considered to enhance the dataset's predictive power?
'', both LightRAG and PathRAG generate responses based on the retrieved text. These responses are then evaluated by GPT-4o-mini across five dimensions, with justifications provided, as shown in Figure~\ref{fig:case study}. We highlight the key points in the answers in bold, with LLM justification for winning judgments displayed in blue and losing judgments in purple. The case study demonstrates that our proposed path information retrieval method provides comprehensive support for answer generation. \modelname exhibits clear advantages in all five dimensions.
\label{sec:appendix-Case study}

\end{document}
