\section{Related Work}
\paragraph{Theoretical Understanding of Scratchpads and CoT}
The first theoretical study of scratchpads for transformers is by ____, who showed that transformers can autoregressively simulate the computations of a Turing machine. %
More recently, work has shown that polynomial-length CoT can allow transformers to transcend the complexity class $TC^0$ ____, and express all problems in PTIME, which by widely believed but unproven conjectures far exceeds $TC^0$.
There are also lower bounds for scratchpads  for \emph{single-layer} transformers ____, showing that a one-layer transformer (both the input itself and the CoT are processed by a single layer) requires a substantial number of steps to solve certain problems, such as iterated function composition.
____ extended this line of work to multi-layer transformers, proving a benefit for CoT in iterated function composition.



Another angle to understanding CoTs is via learnability arguments ____.
%
A particularly promising angle is the \emph{globality degree} ____, with potentially broad implications for scratchpads, though resulting lower-bounds remain largely conjectural beyond special cases (Appendix~\ref{app:globality}).
Interestingly, the globality degree is linked to our sensitivity-based techniques and could in principle lead to stronger bounds; a future full proof of the main conjecture of ____ would, in combination with our unconditional results here, entail even further results (Appendix~\ref{app:globality}).

%
%
%

\paragraph{Lower bounds for transformers}
Lower bounds for UHAT transformers primarily rest on random restrictions, either directly ____ or via reduction to known circuit bounds proven with those ____.
A key technical step in our work is to expand the reach of the random restriction technique to transformers including CoTs.
%
Another approach to obtaining lower bounds relies on circuit conjectures from computational complexity ____ and is thus conditional on these (widely believed) conjectures.
There are further techniques for single-layer transformers ____, for autoregressive transformers ____, and for length-generalizng transformers ____ relying on communication complexity or VC dimension. 
%

\paragraph{Relation to Results on Subset Parities}
Our results concerning \textsc{Parity} are distinct from results on \emph{Subset Parities}.
A long string of results has established difficulty of learning subset parities -- that is, functions $\chi_U$ for $U \subseteq \{1, \dots, N\}$ -- in various setups, and established benefits for providing intermediate steps ____.
Intuitively, the difficulty of learning subset parities emerges from the fact that one has to identify the set $U$ from the exponentially-large power-set.
In learning such functions, there is a provable benefit to providing intermediate steps in training ____.
However, this argument does not readily imply that causal transformers will find the \textsc{Parity} function difficult to learn and that CoTs will help there, because the \textsc{Parity} function applies to a very specific set ($U = \{1, \dots, N\}$).
Our results thus are distinct from results on subset parities due to ____.
%
Interestingly, ____ show that \textsc{Parity} may be easy to learn for MLPs with Rademacher initialization; however, this requires a weight norm substantially growing with $N$, making transformer length generalization unlikely ____.

%


%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%



%


%

%

%


%


%


%
%
%

%

%

%

%


%

%
%


%



%
%
%
%
%
%


%
%
%
%
%
%
%