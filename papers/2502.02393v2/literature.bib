

@article{vasudeva2024simplicity,
  title={Simplicity Bias of Transformers to Learn Low Sensitivity Functions},
  author={Vasudeva, Bhavya and Fu, Deqing and Zhou, Tianyi and Kau, Elliott and Huang, Youqi and Sharan, Vatsal},
  journal={arXiv preprint arXiv:2403.06925},
  year={2024}
}


@article{kozachinskiy2025completely,
  title={A completely uniform transformer for parity},
  author={Kozachinskiy, Alexander and Steifer, Tomasz},
  journal={arXiv preprint arXiv:2501.02535},
  year={2025}
}



@article{han2024token,
  title={Token-Budget-Aware LLM Reasoning},
  author={Han, Tingxu and Fang, Chunrong and Zhao, Shiyu and Ma, Shiqing and Chen, Zhenyu and Wang, Zhenting},
  journal={arXiv preprint arXiv:2412.18547},
  year={2024}
}

@article{r1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and others},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}


@article{yang2023gpt,
  title={GPT Can Solve Mathematical Problems Without a Calculator},
  author={Yang, Zhen and Ding, Ming and Lv, Qingsong and Jiang, Zhihuan and He, Zehai and Guo, Yuyi and Bai, Jinfeng and Tang, Jie},
  journal={arXiv preprint arXiv:2309.03241},
  year={2023}
}

@article{wei2022statistically,
  title={Statistically meaningful approximation: a case study on approximating turing machines with transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12071--12083},
  year={2022}
}

@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}



@inproceedings{
merrill2024the,
title={\href{https://openreview.net/forum?id=QZgo9JZpLq}{The Illusion of State in State-Space Models}},
author={William Merrill and Jackson Petty and Ashish Sabharwal},
booktitle={International Conference on Machine Learning},
year={2024},
%url={https://openreview.net/forum?id=QZgo9JZpLq}
}

@inproceedings{lehnert2024beyond,
  title={Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping},
year={2024},
  author={Lehnert, Lucas and Sukhbaatar, Sainbayar and McVay, Paul and Rabbat, Michael and Tian, Yuandong},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents}
}

@article{cooley1965algorithm,
  title={An algorithm for the machine calculation of complex Fourier series},
  author={Cooley, James W and Tukey, John W},
  journal={Mathematics of computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  publisher={JSTOR}
}


@inproceedings{
hsieh2024ruler,
title={\href{https://openreview.net/forum?id=kIoBbc76Sy}{{RULER}: What{\textquoteright}s the Real Context Size of Your Long-Context Language Models?}},
author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Boris Ginsburg},
booktitle={First Conference on Language Modeling, COLM},
year={2024},
  doi          = {10.48550/ARXIV.2404.06654}
}

@inproceedings{kim2023entity,
    title = "Entity Tracking in Language Models",
    author = "Kim, Najoung  and
      Schuster, Sebastian",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Annual Meeting of the Association for Computational Linguistics, ACL",
    month = jul,
    year = "2023",
    %address = "Toronto, Canada",
    %publisher = "Association for Computational Linguistics",
    %url = "https://aclanthology.org/2023.acl-long.213",
    doi = "10.18653/v1/2023.acl-long.213",
    pages = "3835--3855",
    abstract = "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface.",
}


@article{
chiang2025transformers,
title={Transformers in Uniform {TC}\${\textasciicircum}0\$},
author={David Chiang},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2025},
url={https://openreview.net/forum?id=ZA7D4nQuQF},
note={}
}

@article{grazzi2024unlocking,
  title={Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues},
  author={Grazzi, Riccardo and Siems, Julien and Franke, J{\"o}rg KH and Zela, Arber and Hutter, Frank and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:2411.12537},
  year={2024}
}


@article{deng2024explicit,
  title={From explicit {C}o{T} to implicit {C}o{T}: Learning to internalize {C}o{T} step by step},
  author={Deng, Yuntian and Choi, Yejin and Shieber, Stuart},
  journal={arXiv preprint arXiv:2405.14838},
  year={2024}
}


@inproceedings{
malach2024autoregressive,
title={Auto-Regressive Next-Token Predictors are Universal Learners},
author={Eran Malach},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=i56plqPpEa}
}

@article{barcelo2025ehrenfeucht,
  title={Ehrenfeucht-Haussler Rank and Chain of Thought},
  author={Barcel{\'o}, Pablo and Kozachinskiy, Alexander and Steifer, Tomasz},
  journal={arXiv preprint arXiv:2501.12997},
  year={2025}
}

@article{huang2024formal,
  title={A Formal Framework for Understanding Length Generalization in Transformers},
  author={Huang, Xinting and Yang, Andy and Bhattamishra, Satwik and Sarrof, Yash and Krebs, Andreas and Zhou, Hattie and Nakkiran, Preetum and Hahn, Michael},
  journal={arXiv preprint arXiv:2410.02140},
  year={2024}
}

@inproceedings{schonhage1982asymptotically,
  title={Asymptotically fast algorithms for the numerical muitiplication and division of polynomials with complex coefficients},
  author={Sch{\"o}nhage, Arnold},
  booktitle={European Computer Algebra Conference},
  pages={3--15},
  year={1982},
  organization={Springer}
}


@inproceedings{merrill2024little,
  title={A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers},
  author={Merrill, William and Sabharwal, Ashish},
  booktitle={NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning},
year={2024}
}
@inproceedings{sanford2024transformers,
  title={Transformers, parallel computation, and logarithmic depth},
  author={Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
  booktitle={Forty-first International Conference on Machine Learning},
year={2024}
}



@article{harvey2021integer,
  title={Integer multiplication in time {O}(n log n)},
  author={Harvey, David and Van Der Hoeven, Joris},
  journal={Annals of Mathematics},
  volume={193},
  number={2},
  pages={563--617},
  year={2021},
  publisher={Department of Mathematics, Princeton University Princeton, New Jersey, USA}
}


@inproceedings{abbe2022merged,
  title={The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks},
  author={Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
  booktitle={Conference on Learning Theory},
  pages={4782--4887},
  year={2022},
  organization={PMLR}
}

@article{huang2024formal,
  title={A Formal Framework for Understanding Length Generalization in Transformers},
  author={Huang, Xinting and Yang, Andy and Bhattamishra, Satwik and Sarrof, Yash and Krebs, Andreas and Zhou, Hattie and Nakkiran, Preetum and Hahn, Michael},
  journal={arXiv preprint arXiv:2410.02140},
  year={2024}
}



@article{jones1976new,
  title={New problems complete for nondeterministic log space},
  author={Jones, Neil D and Lien, Y Edmund and Laaser, William T},
  journal={Mathematical systems theory},
  volume={10},
  pages={1--17},
  year={1976},
  publisher={Springer}
}


@inproceedings{ebrahimi2020can,
  title={How Can Self-Attention Networks Recognize Dyck-n Languages?},
  author={Ebrahimi, Javid and Gelda, Dhruv and Zhang, Wei},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4301--4306},
  year={2020}
}


@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{butoi2024training,
  title={Training Neural Networks as Recognizers of Formal Languages},
  author={Butoi, Alexandra and Khalighinejad, Ghazal and Svete, Anej and Valvoda, Josef and Cotterell, Ryan and DuSell, Brian},
  journal={arXiv preprint arXiv:2411.07107},
  year={2024}
}


@inproceedings{clark2019bert,
  title={What Does {BERT} Look At? {A}n Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of BlackboxNLP 2019},
  year={2019}
}



@inproceedings{voita2019analyzing,
  author    = {Elena Voita and
               David Talbot and
               Fedor Moiseev and
               Rico Sennrich and
               Ivan Titov},
  title     = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
               Lifting, the Rest Can Be Pruned},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {5797--5808},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/VoitaTMST19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Merrill2023LogPrecision,
  author       = {William Merrill and
                  Ashish Sabharwal},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {A Logic for Expressing Log-Precision Transformers},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/a48e5877c7bf86a513950ab23b360498-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/MerrillS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{feng2024numerical,
  title={How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs},
  author={Feng, Guhao and Yang, Kai and Gu, Yuntian and Ai, Xinyue and Luo, Shengjie and Sun, Jiacheng and He, Di and Li, Zhenguo and Wang, Liwei},
  journal={arXiv preprint arXiv:2410.13857},
  year={2024}
}

@article{cho2024arithmetic,
  title={Arithmetic transformers can length-generalize in both operand length and count},
  author={Cho, Hanseul and Cha, Jaeyoung and Bhojanapalli, Srinadh and Yun, Chulhee},
  journal={arXiv preprint arXiv:2410.15787},
  year={2024}
}

@article{sabbaghi2024explicitly,
  title={Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks},
  author={Sabbaghi, Mahdi and Pappas, George and Hassani, Hamed and Goel, Surbhi},
  journal={arXiv preprint arXiv:2406.01895},
  year={2024}
}

@article{zhang2024counting,
  title={Counting Ability of Large Language Models and Impact of Tokenization},
  author={Zhang, Xiang and Cao, Juntai and You, Chenyu},
  journal={arXiv preprint arXiv:2410.19730},
  year={2024}
}

@book{cassandras2007introduction,
	title="Introduction to Discrete Event Systems",
	author="Christos G. {Cassandras} and Stephane {Lafortune}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1587663028",
	year="2007"
}

@book{oksendal2013stochastic,
	title="Stochastic Differential Equations: An Introduction with Applications",
	author="Bernt Karsten {Øksendal}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2170316986",
	year="2013"
}

@article{giscard2013evaluating,
	title="Evaluating Matrix Functions by Resummations on Graphs: The Method of Path-Sums",
	author="Pierre-Louis {Giscard} and S. J. {Thwaite} and D. {Jaksch}",
	journal="SIAM Journal on Matrix Analysis and Applications",
	volume="34",
	number="2",
	pages="445--469",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3099383185",
	year="2013"
}

@article{grassmann1977transient,
	title="Transient solutions in markovian queueing systems",
	author="Winfried K. {Grassmann}",
	journal="Computers \& Operations Research",
	volume="4",
	number="1",
	pages="47--53",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1967986582",
	year="1977"
}

@book{odonnell2014analysis,
	title="Analysis of Boolean Functions",
	author="Ryan {O'Donnell}",
	publisher={Cambridge University Press},
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1891181203",
	year="2014"
}

@article{furst1984parity,
  title={Parity, circuits, and the polynomial-time hierarchy},
  author={Furst, Merrick and Saxe, James B and Sipser, Michael},
  journal={Mathematical systems theory},
  volume={17},
  number={1},
  pages={13--27},
  year={1984},
  publisher={Springer}
}

@book{mitzenmacherprobability,
  title={Probability and Computing},
  year={2005},
  author={Mitzenmacher, Michael and Upfal, Eli},
  publisher={Cambridge University Press},
  address={Cambridge}
}

@book{minsky1969perceptrons,
author = {Minsky, Marvin and Papert, Seymour A.},
title = {Perceptrons: An Introduction to Computational Geometry},
year = {1969},
publisher = {The MIT Press},
}



@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={740--745},
  year={2018}
}


@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of {LSTM}s to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}


@article{suzgun2019evaluating,
  title={On Evaluating the Generalization of {LSTM} Models in Formal Languages},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
  journal={Proceedings of the Society for Computation in Linguistics (SCiL)},
  pages={277--286},
  year={2019}
}


@article{merrill2019sequential,
  title={Sequential neural networks as automata},
  author={Merrill, William},
  journal={arXiv preprint arXiv:1906.01615},
  year={2019}
}



@article{bernardy2018can,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  author={Bernardy, Jean-Philippe},
  journal={LiLT (Linguistic Issues in Language Technology)},
  volume={16},
  number={1},
  year={2018}
}


@article{perez2019turing,
  title={On the Turing Completeness of Modern Neural Network Architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}



@article{chen2017recurrent,
  title={Recurrent neural networks as weighted language recognizers},
  author={Chen, Yining and Gilroy, Sorcha and Maletti, Andreas and May, Jonathan and Knight, Kevin},
  journal={arXiv preprint arXiv:1711.05408},
  year={2017}
}



@article{siegelman1991neural,
  title={On    the    computational    power of   neural   nets},
  author={Siegelman, Hava and Sontag, Eduardo D},
  journal={Journal of Computer and System Sciences},
  year={1995},
  volume={50},
  issue={1},
  pages={132--150}
}



@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}


@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@article{tran2018importance,
  title={The importance of being recurrent for modeling hierarchical structure},
  author={Tran, Ke and Bisazza, Arianna and Monz, Christof},
  journal={arXiv preprint arXiv:1803.03585},
  year={2018}
}

@article{yang2019assessing,
  title={Assessing the Ability of Self-Attention Networks to Learn Word Order},
  author={Yang, Baosong and Wang, Longyue and Wong, Derek F and Chao, Lidia S and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:1906.00592},
  year={2019}
}

@inproceedings{yao1986separating,
author={A. C. {Yao}},
booktitle={26th Annual Symposium on Foundations of Computer Science (sfcs 1985)},
title={Separating the polynomial-time hierarchy by oracles},
year={1985},
volume={},
number={},
pages={1-10},
keywords={Polynomials;Circuits;Erbium;Frequency selective surfaces;Computer science;Complexity theory;Computational complexity},
doi={10.1109/SFCS.1985.49},
ISSN={0272-5428},
month={Oct},}

@inproceedings{futrell2019neural,
    title = "Neural language models as psycholinguistic subjects: Representations of syntactic state",
    author = "Futrell, Richard  and
      Wilcox, Ethan  and
      Morita, Takashi  and
      Qian, Peng  and
      Ballesteros, Miguel  and
      Levy, Roger",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1004",
    doi = "10.18653/v1/N19-1004",
    pages = "32--42",
    abstract = "We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.",
}


@article{hastad1994optimal,
  title={Optimal Depth, Very Small Size Circuits for Symmetrical Functions in AC0},
  author={Hastad, Johan and Wegener, Ingo and Wurm, Norbert and Yi, Sang-Zin},
  journal={Information and Computation},
  volume={108},
  number={2},
  pages={200--211},
  year={1994},
  publisher={Elsevier}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{kirov2012processing,
  title={Processing of nested and cross-serial dependencies: an automaton perspective on SRN behaviour},
  author={Kirov, Christo and Frank, Robert},
  journal={Connection Science},
  volume={24},
  number={1},
  pages={1--24},
  year={2012},
  publisher={Taylor \& Francis}
}


@inproceedings{kirov2013bayesian,
  title={Bayesian speech production: Evidence from latency and hyperarticulation},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={35},
  number={35},
  year={2013}
}


@inproceedings{kirov2012specificity,
  title={The specificity of online variation in speech production},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}


@article{shen2018reinforced,
  title={Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Wang, Sen and Zhang, Chengqi},
  journal={arXiv preprint arXiv:1801.10296},
  year={2018}
}


@inproceedings{shen2018disan,
  title={Disan: Directional self-attention network for rnn/cnn-free language understanding},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}


@article{chen2018best,
  title={The best of both worlds: Combining recent advances in neural machine translation},
  author={Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Schuster, Mike and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:1804.09849},
  year={2018}
}



@article{hao2019modeling,
  title={Modeling Recurrence for Transformer},
  author={Hao, Jie and Wang, Xing and Yang, Baosong and Wang, Longyue and Zhang, Jinfeng and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:1904.03092},
  year={2019}
}


@article{paulus2017deep,
  title={A deep reinforced model for abstractive summarization},
  author={Paulus, Romain and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1705.04304},
  year={2017}
}


@article{cheng2016long,
  title={Long short-term memory-networks for machine reading},
  author={Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
  journal={arXiv preprint arXiv:1601.06733},
  year={2016}
}


@article{lin2017structured,
  title={A structured self-attentive sentence embedding},
  author={Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.03130},
  year={2017}
}


@article{parikh2016decomposable,
  title={A decomposable attention model for natural language inference},
  author={Parikh, Ankur P and T{\"a}ckstr{\"o}m, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1606.01933},
  year={2016}
}


@article{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}


@inproceedings{clark2019bert,
  title={What Does {BERT} Look At? An Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of BlackboxNLP 2019},
  year={2019}
}



@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Cohen, William W and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{gulordava2018colorless,
  title={Colorless green recurrent networks dream hierarchically},
  author={Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  journal={arXiv preprint arXiv:1803.11138},
  year={2018}
}

@article{linzen2016assessing,
	Author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	Journal = {Transactions of the Association for Computational Linguistics},
	Title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
    Volume = {4},
    Pages = {521--535},
	Year = {2016}
}

@inproceedings{skachkova2018closing,
  title={Closing Brackets with Recurrent Neural Networks},
  author={Skachkova, Natalia and Trost, Thomas and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}


@article{tabor2000fractal,
  title={Fractal encoding of context-free grammars in connectionist networks},
  author={Tabor, Whitney},
  journal={Expert Systems},
  volume={17},
  number={1},
  pages={41--56},
  year={2000},
  publisher={Wiley Online Library}
}


@article{gruning2006stack,
  title={Stack-like and queue-like dynamics in recurrent neural networks},
  author={Gr{\"u}ning, Andr{\'e}},
  journal={Connection Science},
  volume={18},
  number={1},
  pages={23--42},
  year={2006},
  publisher={Taylor \& Francis}
}


@incollection{chomsky1963algebraic,
  title={The algebraic theory of context-free languages},
  author={Chomsky, Noam and Sch{\"u}tzenberger, Marcel P},
  booktitle={Studies in Logic and the Foundations of Mathematics},
  volume={35},
  pages={118--161},
  year={1963},
  publisher={Elsevier}
}

@article{gers2001lstm,
  title={{LSTM} recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, E},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}
@inproceedings{kalinke1998computation,
  title={Computation in recurrent neural networks: From counters to iterated function systems},
  author={Kalinke, Yvonne and Lehmann, Helko},
  booktitle={Australian Joint Conference on Artificial Intelligence},
  pages={179--190},
  year={1998},
  organization={Springer}
}



@article{everaert2015structures,
  title={Structures, not strings: linguistics as part of the cognitive sciences},
  author={Everaert, Martin BH and Huybregts, Marinus AC and Chomsky, Noam and Berwick, Robert C and Bolhuis, Johan J},
  journal={Trends in cognitive sciences},
  volume={19},
  number={12},
  pages={729--743},
  year={2015},
  publisher={Elsevier}
}





@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}


@article{cartling2008implicit,
  title={On the implicit acquisition of a context-free grammar by a simple recurrent neural network},
  author={Cartling, Bo},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1527--1537},
  year={2008},
  publisher={Elsevier}
}

@incollection{montague1973proper,
  title={The proper treatment of quantification in ordinary English},
  author={Montague, Richard},
  booktitle={Approaches to natural language},
  pages={221--242},
  year={1973},
  publisher={Springer}
}

@article{marvin2018targeted,
  title={Targeted syntactic evaluation of language models},
  author={Marvin, Rebecca and Linzen, Tal},
  journal={arXiv preprint arXiv:1808.09031},
  year={2018}
}

@article{lewis2005activation,
  title={An activation-based model of sentence processing as skilled memory retrieval},
  author={Lewis, Richard L and Vasishth, Shravan},
  journal={Cognitive science},
  volume={29},
  number={3},
  pages={375--419},
  year={2005},
  publisher={Wiley Online Library}
}


@article{lin2019open,
  title={Open Sesame: Getting Inside {BERT}'s Linguistic Knowledge},
  author={Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
  journal={arXiv preprint arXiv:1906.01698},
  year={2019}
}

@inproceedings{devlin2019bert,
	title="{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author="Jacob {Devlin} and Ming-Wei {Chang} and Kenton {Lee} and Kristina {Toutanova}",
	booktitle="NAACL-HLT 2019: Annual Conference of the North American Chapter of the Association for Computational Linguistics",
	pages="4171--4186",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963341956",
	year="2019"
}

@article{tenney2019bert,
  title={Bert rediscovers the classical nlp pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  journal={arXiv preprint arXiv:1905.05950},
  year={2019}
}


@incollection{miller-finitary-1963,
        title = {Finitary models of language users},
        booktitle = {Handbook of {Mathematical} {Psychology}},
        author = {Miller, George A and Chomsky, Noam},
        year = {1963}
}

@article{gibson1999memory,
  title={Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical},
  author={Gibson, Edward and Thomas, James},
  journal={Language and Cognitive Processes},
  volume={14},
  number={3},
  pages={225--248},
  year={1999},
  publisher={Taylor \& Francis}
}


@inproceedings{gopalan2016smooth,
  title={Smooth boolean functions are easy: Efficient algorithms for low-sensitivity functions},
  author={Gopalan, Parikshit and Nisan, Noam and Servedio, Rocco A and Talwar, Kunal and Wigderson, Avi},
  booktitle={Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
  pages={59--70},
  year={2016},
  organization={ACM}
}


@article{de2018deep,
  title={Deep neural networks are biased towards simple functions},
  author={De Palma, Giacomo and Kiani, Bobak Toussi and Lloyd, Seth},
  journal={arXiv preprint arXiv:1812.10156},
  year={2018}
}

@inproceedings{gilmer2015new,
  title={A new approach to the sensitivity conjecture},
  author={Gilmer, Justin and Kouck{\`y}, Michal and Saks, Michael E},
  booktitle={Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
  pages={247--254},
  year={2015},
  organization={ACM}
}


@article{rossman2018average,
  title={The average sensitivity of bounded-depth formulas},
  author={Rossman, Benjamin},
  journal={computational complexity},
  volume={27},
  number={2},
  pages={209--223},
  year={2018},
  publisher={Springer}
}



@article{boppana1997average,
  title={The average sensitivity of bounded-depth circuits},
  author={Boppana, Ravi B},
  journal={Information processing letters},
  volume={63},
  number={5},
  pages={257--261},
  year={1997},
  publisher={Elsevier}
}



@article{miller2018recurrent,
  title={When Recurrent Models Don't Need To Be Recurrent},
  author={Miller, John and Hardt, Moritz},
  journal={arXiv preprint arXiv:1805.10369},
  year={2018}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}


@book{mcnaughton1971counter,
  title={Counter-Free Automata (MIT research monograph no. 65)},
  author={McNaughton, Robert and Papert, Seymour A},
  year={1971},
  publisher={The MIT Press}
}

@article{parker2017cue,
  title={The cue-based retrieval theory of sentence comprehension: New findings and new challenges},
  author={Parker, Dan and Shvartsman, Michael and Van Dyke, Julie A},
  journal={Language processing and disorders},
  pages={121--144},
  year={2017},
  publisher={Cambridge Scholars Publishing Newcastle}
}




@article{barrington1992regular,
  title={Regular languages in NC1},
  author={Barrington, David A Mix and Compton, Kevin and Straubing, Howard and Th{\'e}rien, Denis},
  journal={Journal of Computer and System Sciences},
  volume={44},
  number={3},
  pages={478--499},
  year={1992},
  publisher={Elsevier}
}

@article{korsky2019computational,
  title={On the Computational Power of RNNs},
  author={Samuel A. Korsky and Robert C. Berwick},
  journal={arXiv preprint arXiv:1906.06349},
  year={2019}
}


@article{venkatesh1998pseudo,
  title={Pseudo-average block sensitivity equals average sensitivity},
  author={Venkatesh, Srinivasan},
  journal={Information processing letters},
  volume={68},
  number={2},
  pages={93--95},
  year={1998},
  publisher={Elsevier}
}

@book{odonnell2014analysis,
	title="Analysis of Boolean Functions",
	author="Ryan {O'Donnell}",
	publisher={Cambridge University Press},
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1891181203",
	year="2014"
}


@article{bernasconi1996sensitivity,
	title="Sensitivity vs. block sensitivity (an average-case study)",
	author="A. {Bernasconi}",
	journal="Information Processing Letters",
	volume="59",
	number="3",
	pages="151--157",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2079828635",
	year="1996"
}


@article{nisan1991crew,
	title="{CREW} {PRAMs} and decision trees",
	author="Noam {Nisan}",
	journal="SIAM Journal on Computing",
	volume="20",
	number="6",
	pages="999--1007",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1867758558",
	year="1991"
}


@article{venkatesh1998pseudo,
	title="Pseudo-average block sensitivity equals average sensitivity",
	author="S. {Venkatesh}",
	journal="Information Processing Letters",
	volume="68",
	number="2",
	pages="93--95",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2045361120",
	year="1998"
}

@inproceedings{kahn1988the,
	title="The influence of variables on Boolean functions",
	author="J. {Kahn} and G. {Kalai} and N. {Linial}",
	booktitle="[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science",
	pages="68--80",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2103749128",
	year="1988"
}

@book{jukna2012boolean,
	title="Boolean Function Complexity: Advances and Frontiers",
	author="Stasys {Jukna}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/112858668",
	year="2012"
}


@inproceedings{kaushik2020learning,
	title="Learning The Difference That Makes A Difference With Counterfactually-Augmented Data",
	author="Divyansh {Kaushik} and Eduard {Hovy} and Zachary {Lipton}",
	booktitle="ICLR 2020: Eighth International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2994934025",
	year="2020"
}

@article{gardner2020evaluating,
	title="Evaluating {NLP} Models via Contrast Sets",
	author="Matt {Gardner} and Yoav {Artzi} and Victoria {Basmova} and Jonathan {Berant} and Ben {Bogin} and Sihao {Chen} and Pradeep {Dasigi} and Dheeru {Dua} and Yanai {Elazar} and Ananth {Gottumukkala} and Nitish {Gupta} and Hanna {Hajishirzi} and Gabriel {Ilharco} and Daniel {Khashabi} and Kevin {Lin} and Jiangming {Liu} and Nelson F. {Liu} and Phoebe {Mulcaire} and Qiang {Ning} and Sameer {Singh} and Noah A. {Smith} and Sanjay {Subramanian} and Reut {Tsarfaty} and Eric {Wallace} and Ally {Zhang} and Ben {Zhou}",
	journal="arXiv preprint arXiv:2004.02709",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3014564055",
	year="2020"
}

@inproceedings{jia2017adversarial,
	title="Adversarial Examples for Evaluating Reading Comprehension Systems",
	author="Robin {Jia} and Percy {Liang}",
	booktitle="Proceedings of the 2017 Conference on Empirical Methods in Natural
      Language Processing",
	pages="2021--2031",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963969878",
	year="2017"
}


@article{hatami2010variations,
	title="Variations on the Sensitivity Conjecture",
	author="Pooya {Hatami} and Raghav {Kulkarni} and Denis {Pankratov}",
	journal="Theory of Computing",
	volume="4",
	pages="1--27",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963994076",
	year="2010"
}

@inproceedings{kim2014convolutional,
	title="Convolutional Neural Networks for Sentence Classification",
	author="Yoon {Kim}",
	booktitle="Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	pages="1746--1751",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1832693441",
	year="2014"
}



@inproceedings{levy2018long,
	title="Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum",
	author="Omer {Levy} and Kenton {Lee} and Nicholas {FitzGerald} and Luke {Zettlemoyer}",
	booktitle="ACL 2018: 56th Annual Meeting of the Association for Computational Linguistics",
	pages="732--739",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963410683",
	year="2018"
}

@article{hahn2020theoretical,
	title="Theoretical Limitations of Self-Attention in Neural Sequence Models",
	author="Michael {Hahn}",
	journal="Transactions of the Association for Computational Linguistics",
	volume="8",
	pages="156--171",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3014096773",
	year="2020"
}

@inproceedings{vaswani2017attention,
	title="Attention is All You Need",
	author="Ashish {Vaswani} and Noam {Shazeer} and Niki {Parmar} and Jakob {Uszkoreit} and Llion {Jones} and Aidan N. {Gomez} and Lukasz {Kaiser} and Illia {Polosukhin}",
	booktitle="Proceedings of the 31st International Conference on {N}eural {I}nformation {P}rocessing {S}ystems",
	pages="5998--6008",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963403868",
	year="2017"
}

@article{hochreiter1997long,
	title="Long short-term memory",
	author="Sepp {Hochreiter} and Jürgen {Schmidhuber}",
	journal="Neural Computation",
	volume="9",
	number="8",
	pages="1735--1780",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2064675550",
	year="1997"
}


@inproceedings{wang2019glue,
	title="GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
	author="Alex {Wang} and Amanpreet {Singh} and Julian {Michael} and Felix {Hill} and Omer {Levy} and Samuel R. {Bowman}",
	booktitle="ICLR 2019: 7th International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963310665",
	year="2019"
}



@article{liu2019roberta,
	title="{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach",
	author="Yinhan {Liu} and Myle {Ott} and Naman {Goyal} and Jingfei {Du} and Mandar {Joshi} and Danqi {Chen} and Omer {Levy} and Mike {Lewis} and Luke {Zettlemoyer} and Veselin {Stoyanov}",
	journal="arXiv preprint arXiv:1907.11692",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2965373594",
	year="2019"
}

@inproceedings{yang2019xlnet,
	title="{XLNet}: Generalized Autoregressive Pretraining for Language Understanding",
	author="Zhilin {Yang} and Zihang {Dai} and Yiming {Yang} and Jaime {Carbonell} and Ruslan {Salakhutdinov} and Quoc V {Le}",
	booktitle="NeurIPS 2019: Thirty-third Conference on {N}eural {I}nformation {P}rocessing {S}ystems",
	pages="5753--5763",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2970597249",
	year="2019"
}

@article{wolf2019huggingface,
	title="HuggingFace's Transformers: State-of-the-art Natural Language Processing",
	author="Thomas {Wolf} and Lysandre {Debut} and Victor {Sanh} and Julien {Chaumond} and Clement {Delangue} and Anthony {Moi} and Pierric {Cistac} and Tim {Rault} and Rémi {Louf} and Morgan {Funtowicz} and Jamie {Brew}",
	journal="arXiv preprint arXiv:1910.03771",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2980282514",
	year="2019"
}

@inproceedings{ott2019fairseq,
	title="fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
	author="Myle {Ott} and Sergey {Edunov} and Alexei {Baevski} and Angela {Fan} and Sam {Gross} and Nathan {Ng} and David {Grangier} and Michael {Auli}",
	booktitle="NAACL-HLT 2019: Annual Conference of the North American Chapter of the Association for Computational Linguistics",
	pages="48--53",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2933138175",
	year="2019"
}


@inproceedings{wang2019superglue,
	title="SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
	author="Alex {Wang} and Yada {Pruksachatkun} and Nikita {Nangia} and Amanpreet {Singh} and Julian {Michael} and Felix {Hill} and Omer {Levy} and Samuel R. {Bowman}",
	booktitle="Advances in {N}eural {I}nformation {P}rocessing {S}ystems",
	pages="3266--3280",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2990704537",
	year="2019"
}



@inproceedings{silveira2014a,
	title="A Gold Standard Dependency Corpus for English",
	author="Natalia {Silveira} and Timothy {Dozat} and Marie-Catherine de {Marneffe} and Samuel {Bowman} and Miriam {Connor} and John {Bauer} and Chris {Manning}",
	booktitle="Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)",
	pages="2897--2904",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2250263931",
	year="2014"
}


@inproceedings{marvin2018targeted,
	title="Targeted Syntactic Evaluation of Language Models",
	author="Rebecca {Marvin} and Tal {Linzen}",
	booktitle="EMNLP 2018: 2018 Conference on Empirical Methods in Natural Language Processing",
	pages="1192--1202",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2888922637",
	year="2018"
}

@inproceedings{Gauthier:et-al:2020:syntaxgym,
  author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
  title = {{SyntaxGym}: An online platform for targeted evaluation of language models},
  booktitle = {Proceedings of the Association for Computational Linguistics: System Demonstrations (ACL 2020)},
  year = {2020}
}

@article{hu2020a,
	title="A Closer Look at the Performance of Neural Language Models on Reflexive Anaphor Licensing",
	author="Jennifer {Hu} and Sherry Y {Chen} and Roger P. {Levy}",
	journal="Proceedings of the Society for Computation in Linguistics",
	volume="3",
	number="1",
	pages="382--392",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2994726368",
	year="2020"
}

@inproceedings{socher2013recursive,
	title="Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
	author="Richard {Socher} and Alex {Perelygin} and Jean {Wu} and Jason {Chuang} and Christopher D. {Manning} and Andrew {Ng} and Christopher {Potts}",
	booktitle="Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
	pages="1631--1642",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2251939518",
	year="2013"
}

@article{dagan2009recognizing,
	title="Recognizing textual entailment: Rational, evaluation and approaches",
	author="Ido {Dagan} and Bill {Dolan} and Bernardo {Magnini} and Dan {Roth}",
	journal="Natural Language Engineering",
	volume="15",
	number="4",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2118707092",
	year="2009"
}

@inproceedings{howard2018universal,
	title="Universal Language Model Fine-tuning for Text Classification",
	author="Jeremy {Howard} and Sebastian {Ruder}",
	booktitle="ACL 2018: 56th Annual Meeting of the Association for Computational Linguistics",
	volume="1",
	pages="328--339",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963026768",
	year="2018"
}

@inproceedings{peters2018deep,
	title="DEEP CONTEXTUALIZED WORD REPRESENTATIONS",
	author="Matthew E {Peters} and Mark {Neumann} and Mohit {Iyyer} and Matt {Gardner} and Christopher {Clark} and Kenton {Lee} and Luke {Zettlemoyer}",
	booktitle="NAACL HLT 2018: 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	volume="1",
	pages="2227--2237",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2962739339",
	year="2018"
}



@inproceedings{wang2012baselines,
	title="Baselines and Bigrams: Simple, Good Sentiment and Topic Classification",
	author="Sida {Wang} and Christopher {Manning}",
	booktitle="Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
	volume="2",
	pages="90--94",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2154359981",
	year="2012"
}


@article{wiebe2005annotating,
	title="Annotating Expressions of Opinions and Emotions in Language",
	author="Janyce {Wiebe} and Theresa {Wilson} and Claire {Cardie}",
	journal="language resources and evaluation",
	volume="39",
	number="2",
	pages="165--210",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2014902591",
	year="2005"
}


@inproceedings{hu2004mining,
	title="Mining and summarizing customer reviews",
	author="Minqing {Hu} and Bing {Liu}",
	booktitle="Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining",
	pages="168--177",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2160660844",
	year="2004"
}

@inproceedings{pang2004a,
	title="A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts",
	author="Bo {Pang} and Lillian {Lee}",
	booktitle="Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL'04), Main Volume",
	pages="271--278",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2114524997",
	year="2004"
}

@inproceedings{pang2005seeing,
	title="Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales",
	author="Bo {Pang} and Lillian {Lee}",
	booktitle="Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)",
	pages="115--124",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2163455955",
	year="2005"
}


@inproceedings{lei2018simple,
	title="Simple Recurrent Units for Highly Parallelizable Recurrence.",
	author="Tao {Lei} and Yu {Zhang} and Sida I. {Wang} and Hui {Dai} and Yoav {Artzi}",
	booktitle="Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	pages="4470--4481",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963174729",
	year="2018"
}


@article{qi2020stanza,
	title="Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
	author="Peng {Qi} and Yuhao {Zhang} and Yuhui {Zhang} and Jason {Bolton} and Christopher D. {Manning}",
	journal="arXiv preprint arXiv:2003.07082",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3011573503",
	year="2020"
}

@inproceedings{qi2018universal,
	title="Universal Dependency Parsing from Scratch",
	author="Peng {Qi} and Timothy {Dozat} and Yuhao {Zhang} and Christopher D. {Manning}",
	booktitle="Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
	pages="160--170",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2899024931",
	year="2018"
}

@book{minsky1969perceptrons,
	title="Perceptrons: An Introduction to Computational Geometry",
	author="Marvin Lee {Minsky} and Seymour {Papert}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2086789740",
	year="1969"
}

@inproceedings{hsieh-etal-2019-robustness,
    title = "On the Robustness of Self-Attentive Models",
    author = "Hsieh, Yu-Lun  and
      Cheng, Minhao  and
      Juan, Da-Cheng  and
      Wei, Wei  and
      Hsu, Wen-Lian  and
      Hsieh, Cho-Jui",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1147",
    doi = "10.18653/v1/P19-1147",
    pages = "1520--1529",
    abstract = "This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims.",
}

@article{qiu2024ask,
  title={Ask, and it shall be given: Turing completeness of prompting},
  author={Qiu, Ruizhong and Xu, Zhe and Bao, Wenxuan and Tong, Hanghang},
  journal={arXiv preprint arXiv:2411.01992},
  year={2024}
}

@article{mcleish2024transformers,
  title={Transformers Can Do Arithmetic with the Right Embeddings},
  author={McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and others},
  journal={arXiv preprint arXiv:2405.17399},
  year={2024}
}


@article{kozachinskiy2024lower,
  title={Lower bounds on transformers with infinite precision},
  author={Kozachinskiy, Alexander},
  journal={arXiv preprint arXiv:2412.20195},
  year={2024}
}
@inproceedings{
anonymous2024chainofthought,
title={Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=N6pbLYLeej},
note={under review}
}

@article{furst1984parity,
  title={Parity, circuits, and the polynomial-time hierarchy},
  author={Furst, Merrick and Saxe, James B and Sipser, Michael},
  journal={Mathematical systems theory},
  volume={17},
  number={1},
  pages={13--27},
  year={1984},
  publisher={Springer}
}

@book{mitzenmacherprobability,
  title={Probability and Computing},
  year={2017},
  author={Mitzenmacher, Michael and Upfal, Eli},
  publisher={Cambridge University Press},
  address={Cambridge},
  edition={2nd}
}

@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={740--745},
  year={2018}
}


@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of {LSTM}s to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}


@article{suzgun2019evaluating,
  title={On Evaluating the Generalization of {LSTM} Models in Formal Languages},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
  journal={Proceedings of the Society for Computation in Linguistics (SCiL)},
  pages={277--286},
  year={2019}
}

@inproceedings{merrill2019sequential,
    title = "Sequential Neural Networks as Automata",
    author = "Merrill, William",
    booktitle = "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges",
    month = aug,
    year = "2019",
    address = "Florence",
    publisher = "Association for Computational Linguistics",
    pages = "1--13",
    abstract = "This work attempts to explain the types of computation that neural networks can perform by relating them to automata. We first define what it means for a real-time network with bounded precision to accept a language. A measure of network memory follows from this definition. We then characterize the classes of languages acceptable by various recurrent networks, attention, and convolutional networks. We find that LSTMs function like counter machines and relate convolutional networks to the subregular hierarchy. Overall, this work attempts to increase our understanding and ability to interpret neural networks through the lens of theory. These theoretical insights help explain neural computation, as well as the relationship between neural networks and natural language grammar.",
}

@article{bernardy2018can,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  author={Bernardy, Jean-Philippe},
  journal={LiLT (Linguistic Issues in Language Technology)},
  volume={16},
  number={1},
  year={2018}
}

@inproceedings{
perez2019turing,
title={On the {Turing} Completeness of Modern Neural Network Architectures},
author={Jorge Pérez and Javier Marinković and Pablo Barcel{\'o}},
booktitle={International Conference on Learning Representations},
year={2019}
}


@nproceedings{chen2017recurrent,
    title = "Recurrent Neural Networks as Weighted Language Recognizers",
    author = "Chen, Yining  and
      Gilroy, Sorcha  and
      Maletti, Andreas  and
      May, Jonathan  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1205",
    doi = "10.18653/v1/N18-1205",
    pages = "2261--2271",
    abstract = "We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.",
}



@article{siegelman1991neural,
  title={On    the    computational    power of   neural   nets},
  author={Siegelman, Hava and Sontag, Eduardo D},
  journal={Journal of Computer and System Sciences},
  year={1995},
  volume={50},
  issue={1},
  pages={132--150}
}



@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}


@inproceedings{dehghani2018universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019}
}}

@inproceedings{tran2018importance,
    title = "The Importance of Being Recurrent for Modeling Hierarchical Structure",
    author = "Tran, Ke  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "4731--4736",
    abstract = "Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures{---}recurrent versus non-recurrent{---}with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at \url{https://github.com/ ketranm/fan_vs_rnn}",
}

@inproceedings{yang2019assessing,
  author    = {Baosong Yang and
               Longyue Wang and
               Derek F. Wong and
               Lidia S. Chao and
               Zhaopeng Tu},
  title     = {Assessing the Ability of Self-Attention Networks to Learn Word Order},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {3635--3644},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/YangWWCT19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yao1986separating,
author={A. C. {Yao}},
booktitle={26th Annual Symposium on Foundations of Computer Science (SFCS 1985)},
title={Separating the polynomial-time hierarchy by oracles},
year={1985},
volume={},
number={},
pages={1-10}}


@article{hastad1994optimal,
  title={Optimal Depth, Very Small Size Circuits for Symmetrical Functions in {AC}0},
  author={Hastad, Johan and Wegener, Ingo and Wurm, Norbert and Yi, Sang-Zin},
  journal={Information and Computation},
  volume={108},
  number={2},
  pages={200--211},
  year={1994},
  publisher={Elsevier}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{kirov2012processing,
  title={Processing of nested and cross-serial dependencies: an automaton perspective on {SRN} behaviour},
  author={Kirov, Christo and Frank, Robert},
  journal={Connection Science},
  volume={24},
  number={1},
  pages={1--24},
  year={2012},
  publisher={Taylor \& Francis}
}


@inproceedings{kirov2013bayesian,
  title={Bayesian speech production: Evidence from latency and hyperarticulation},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={35},
  number={35},
  year={2013}
}


@inproceedings{kirov2012specificity,
  title={The specificity of online variation in speech production},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}


@inproceedings{shen2018reinforced,
  title={Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Wang, Sen and Zhang, Chengqi},
  booktitle={IJCAI'18 Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  year={2018},
  pages={4345--4352}
}


@inproceedings{shen2018disan,
  title={Disan: Directional self-attention network for {RNN}/{CNN}-free language understanding},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}


@inproceedings{chen2018best,
    title = "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
    author = "Chen, Mia Xu  and
      Firat, Orhan  and
      Bapna, Ankur  and
      Johnson, Melvin  and
      Macherey, Wolfgang  and
      Foster, George  and
      Jones, Llion  and
      Schuster, Mike  and
      Shazeer, Noam  and
      Parmar, Niki  and
      Vaswani, Ashish  and
      Uszkoreit, Jakob  and
      Kaiser, Lukasz  and
      Chen, Zhifeng  and
      Wu, Yonghui  and
      Hughes, Macduff",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    pages = "76--86",
    abstract = "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT{'}14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",
}



@inproceedings{hao2019modeling,
    title = "Modeling Recurrence for Transformer",
    author = "Hao, Jie  and
      Wang, Xing  and
      Yang, Baosong  and
      Wang, Longyue  and
      Zhang, Jinfeng  and
      Tu, Zhaopeng",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "1198--1207",
    abstract = "Recently, the Transformer model that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence modeling hinders its further improvement of translation capacity. In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention models and recurrent networks. Experimental results on the widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences with a single recurrent layer, which outperforms its deep counterpart.",
}

@inproceedings{
paulus2017deep,
title={A Deep Reinforced Model for Abstractive Summarization},
author={Romain Paulus and Caiming Xiong and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018}
}


@inproceedings{cheng2016long,
    title = "Long Short-Term Memory-Networks for Machine Reading",
    author = "Cheng, Jianpeng  and
      Dong, Li  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    pages = "551--561",
}


@inproceedings{lin2017structured,
  title={A structured self-attentive sentence embedding},
  author={Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2017}
}


@inproceedings{parikh2016decomposable,
	    title = "A Decomposable Attention Model for Natural Language Inference",
	    author = {Parikh, Ankur  and
	      T{\"a}ckstr{\"o}m, Oscar  and
	      Das, Dipanjan  and
	      Uszkoreit, Jakob},
	    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
	    month = nov,
	    year = "2016",
	    address = "Austin, Texas",
	    publisher = "Association for Computational Linguistics",
	    pages = "2249--2255",
	}

@inproceedings{voita2019analyzing,
  author    = {Elena Voita and
               David Talbot and
               Fedor Moiseev and
               Rico Sennrich and
               Ivan Titov},
  title     = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
               Lifting, the Rest Can Be Pruned},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {5797--5808},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/VoitaTMST19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{clark2019bert,
  title={What Does {BERT} Look At? {A}n Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of BlackboxNLP 2019},
  year={2019}
}



@inproceedings{dai2019transformer,
  author    = {Zihang Dai and
               Zhilin Yang and
               Yiming Yang and
               Jaime G. Carbonell and
               Quoc Viet Le and
               Ruslan Salakhutdinov},
  title     = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {2978--2988},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/DaiYYCLS19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{gulordava2018colorless,
    title = "Colorless Green Recurrent Networks Dream Hierarchically",
    author = "Gulordava, Kristina  and
      Bojanowski, Piotr  and
      Grave, Edouard  and
      Linzen, Tal  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    pages = "1195--1205"
}

@article{linzen2016assessing,
	Author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	Journal = {Transactions of the Association for Computational Linguistics},
	Title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
    Volume = {4},
    Pages = {521--535},
	Year = {2016}
}

@inproceedings{skachkova2018closing,
  title={Closing Brackets with Recurrent Neural Networks},
  author={Skachkova, Natalia and Trost, Thomas and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}


@article{tabor2000fractal,
  title={Fractal encoding of context-free grammars in connectionist networks},
  author={Tabor, Whitney},
  journal={Expert Systems},
  volume={17},
  number={1},
  pages={41--56},
  year={2000},
  publisher={Wiley Online Library}
}


@article{gruning2006stack,
  title={Stack-like and queue-like dynamics in recurrent neural networks},
  author={Gr{\"u}ning, Andr{\'e}},
  journal={Connection Science},
  volume={18},
  number={1},
  pages={23--42},
  year={2006},
  publisher={Taylor \& Francis}
}


@incollection{chomsky1963algebraic,
  title={The algebraic theory of context-free languages},
  author={Chomsky, Noam and Sch{\"u}tzenberger, Marcel P},
  booktitle={Studies in Logic and the Foundations of Mathematics},
  volume={35},
  pages={118--161},
  year={1963},
  publisher={Elsevier}
}

@article{gers2001lstm,
  title={{LSTM} recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}
@inproceedings{kalinke1998computation,
  title={Computation in recurrent neural networks: From counters to iterated function systems},
  author={Kalinke, Yvonne and Lehmann, Helko},
  booktitle={Australian Joint Conference on Artificial Intelligence},
  pages={179--190},
  year={1998},
  organization={Springer}
}



@article{everaert2015structures,
  title={Structures, not strings: linguistics as part of the cognitive sciences},
  author={Everaert, Martin BH and Huybregts, Marinus AC and Chomsky, Noam and Berwick, Robert C and Bolhuis, Johan J},
  journal={Trends in cognitive sciences},
  volume={19},
  number={12},
  pages={729--743},
  year={2015},
  publisher={Elsevier}
}





@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}


@article{cartling2008implicit,
  title={On the implicit acquisition of a context-free grammar by a simple recurrent neural network},
  author={Cartling, Bo},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1527--1537},
  year={2008},
  publisher={Elsevier}
}

@incollection{montague1973proper,
  title={The proper treatment of quantification in ordinary {E}nglish},
  author={Montague, Richard},
  booktitle={Approaches to natural language},
  pages={221--242},
  year={1973},
  publisher={Springer}
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@article{lewis2005activation,
  title={An activation-based model of sentence processing as skilled memory retrieval},
  author={Lewis, Richard L and Vasishth, Shravan},
  journal={Cognitive science},
  volume={29},
  number={3},
  pages={375--419},
  year={2005},
  publisher={Wiley Online Library}
}



@inproceedings{lin2019open,
    title = "Open {S}esame: Getting inside {BERT}{'}s Linguistic Knowledge",
    author = "Lin, Yongjie  and
      Tan, Yi Chern  and
      Frank, Robert",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "241--253",
    abstract = "How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT{'}s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT{'}s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT{'}s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.",
}


@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{tenney2019bert,
  author    = {Ian Tenney and
               Dipanjan Das and
               Ellie Pavlick},
  title     = {{BERT} Rediscovers the Classical {NLP} Pipeline},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {4593--4601},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/TenneyDP19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{miller-finitary-1963,
        title = {Finitary models of language users},
        booktitle = {Handbook of {Mathematical} {Psychology}},
        author = {Miller, George A and Chomsky, Noam},
	editor={R. Duncan Luce and Robert R. Bush and Eugene Galanter},
	pages={419--492},
	publisher={John Wiley},
        year = {1963}
}

@article{gibson1999memory,
  title={Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical},
  author={Gibson, Edward and Thomas, James},
  journal={Language and Cognitive Processes},
  volume={14},
  number={3},
  pages={225--248},
  year={1999},
  publisher={Taylor \& Francis}
}


@inproceedings{gopalan2016smooth,
  title={Smooth boolean functions are easy: Efficient algorithms for low-sensitivity functions},
  author={Gopalan, Parikshit and Nisan, Noam and Servedio, Rocco A and Talwar, Kunal and Wigderson, Avi},
  booktitle={Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
  pages={59--70},
  year={2016},
  organization={ACM}
}


@article{de2018deep,
  title={Deep neural networks are biased towards simple functions},
  author={De Palma, Giacomo and Kiani, Bobak Toussi and Lloyd, Seth},
  journal={arXiv preprint arXiv:1812.10156},
  year={2018}
}

@inproceedings{gilmer2015new,
  title={A new approach to the sensitivity conjecture},
  author={Gilmer, Justin and Kouck{\`y}, Michal and Saks, Michael E},
  booktitle={Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
  pages={247--254},
  year={2015},
  organization={ACM}
}


@article{rossman2018average,
  title={The average sensitivity of bounded-depth formulas},
  author={Rossman, Benjamin},
  journal={Computational Complexity},
  volume={27},
  number={2},
  pages={209--223},
  year={2018},
  publisher={Springer}
}



@article{boppana1997average,
  title={The average sensitivity of bounded-depth circuits},
  author={Boppana, Ravi B},
  journal={Information processing letters},
  volume={63},
  number={5},
  pages={257--261},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{
miller2018recurrent,
title={Stable Recurrent Models},
author={John Miller and Moritz Hardt},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}


@book{mcnaughton1971counter,
  title={Counter-Free Automata (MIT research monograph no. 65)},
  author={McNaughton, Robert and Papert, Seymour A},
  year={1971},
  publisher={The MIT Press}
}

@article{parker2017cue,
  title={The cue-based retrieval theory of sentence comprehension: New findings and new challenges},
  author={Parker, Dan and Shvartsman, Michael and Van Dyke, Julie A},
  journal={Language processing and disorders},
  pages={121--144},
  year={2017},
  publisher={Cambridge Scholars Publishing Newcastle}
}




@article{barrington1992regular,
  title={Regular languages in {NC}1},
  author={Barrington, David A Mix and Compton, Kevin and Straubing, Howard and Th{\'e}rien, Denis},
  journal={Journal of Computer and System Sciences},
  volume={44},
  number={3},
  pages={478--499},
  year={1992},
  publisher={Elsevier}
}

@article{korsky2019computational,
  title={On the Computational Power of {RNN}s},
  author={Samuel A. Korsky and Robert C. Berwick},
  journal={arXiv preprint arXiv:1906.06349},
  year={2019}
}

@inproceedings{hsieh2019robustness,
    title = "On the Robustness of Self-Attentive Models",
    author = "Hsieh, Yu-Lun and
      Cheng, Minhao  and
      Juan, Da-Cheng  and
      Wei, Wei  and
      Hsu, Wen-Lian  and
      Hsieh, Cho-Jui",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "1520--1529"
}

@inproceedings{levy2018long,
  title={Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum},
  author={Levy, Omer and Lee, Kenton and FitzGerald, Nicholas and Zettlemoyer, Luke},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={732--739},
  year={2018}
}

@article{ostmeyer2019machine,
  title={Machine learning on sequential data using a recurrent weighted average},
  author={Ostmeyer, Jared and Cowell, Lindsay},
  journal={Neurocomputing},
  volume={331},
  pages={281--288},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{lei2017deriving,
  title={Deriving neural architectures from sequence and graph kernels},
  author={Lei, Tao and Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2024--2033},
  year={2017},
  organization={JMLR. org}
}
@article{yin2017comparative,
  title={Comparative Study of CNN and RNN for Natural Language Processing},
  author={Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1702.01923},
  year={2017}
}
@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={933--941},
  year={2017},
  organization={JMLR. org}
}
@article{bradbury2016quasi,
  title={Quasi-recurrent neural networks},
  author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01576},
  year={2016}
}
@article{lei2015semi,
  title={Semi-supervised question retrieval with gated convolutions},
  author={Lei, Tao and Joshi, Hrishikesh and Barzilay, Regina and Jaakkola, Tommi and Tymoshenko, Katerina and Moschitti, Alessandro and Marquez, Lluis},
  journal={arXiv preprint arXiv:1512.05726},
  year={2015}
}
@inproceedings{sundermeyer2013comparison,
  title={Comparison of feedforward and recurrent neural network language models},
  author={Sundermeyer, Martin and Oparin, Ilya and Gauvain, J-L and Freiberg, Ben and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8430--8434},
  year={2013},
  organization={IEEE}
}
@inproceedings{balduzzi2016strongly,
  title={Strongly-Typed Recurrent Neural Networks},
  author={Balduzzi, David and Ghifary, Muhammad},
  booktitle={International Conference on Machine Learning},
  pages={1292--1300},
  year={2016}
}


@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={arXiv preprint arXiv:1905.10650},
  year={2019}
}

@book{chomsky1957syntactic,
  title={Syntactic structures.},
  author={Chomsky, Noam},
  year={1957},
  publisher={Mouton},
  address={The Hague}
}


@article{keener1992limit,
  title={Limit theorems for random walks conditioned to stay positive},
  author={Keener, Robert W and others},
  journal={The Annals of Probability},
  volume={20},
  number={2},
  pages={801--824},
  year={1992},
  publisher={Institute of Mathematical Statistics}
}


@article{chi1999statistical,
  title={Statistical properties of probabilistic context-free grammars},
  author={Chi, Zhiyi},
  journal={Computational Linguistics},
  volume={25},
  number={1},
  pages={131--160},
  year={1999},
  publisher={MIT Press}
}

@incollection{shieber1985evidence,
  title={Evidence against the context-freeness of natural language},
  author={Shieber, Stuart M},
  booktitle={Philosophy, Language, and Artificial Intelligence},
  pages={79--89},
  year={1985},
  publisher={Springer}
}

@inproceedings{svete2024transformers,
  title={Transformers Can Represent n-gram Language Models},
  author={Svete, Anej and Cotterell, Ryan},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={6841--6874},
  year={2024}
}


@inproceedings{kuncoro2018lstms,
  title={Lstms can learn syntax-sensitive dependencies well, but modeling structure makes them better},
  author={Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1426--1436},
  year={2018}
}



@article{mccoy2019berts,
  title={BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance},
  author={McCoy, R Thomas and Min, Junghyun and Linzen, Tal},
  journal={arXiv preprint arXiv:1911.02969},
  year={2019}
}

@inproceedings{yang2024masked,
  title={Masked hard-attention transformers recognize exactly the star-free languages},
  author={Yang, Andy and Chiang, David and Angluin, Dana},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{hahn-rofin-2024-sensitive,
    title = "Why are Sensitive Functions Hard for Transformers?",
    author = "Hahn, Michael  and
      Rofin, Mark",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.800/",
    doi = "10.18653/v1/2024.acl-long.800",
    pages = "14973--15008",
    abstract = "Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding transformers' inductive biases requires studying not just their in-principle expressivity, but also their loss landscape."
}

@misc{kozachinskiy2024flann,
author={Alexander Kozachinskiy},
title={Logical Languages Accepted by Transformer Encoders with Hard Attention. Presentation at Formal Languages and Neural Networks Discord},
year={2024},
url={https://www.youtube.com/watch?v=h1bgvUHc4-c}
}

@inproceedings{goyal2024think,
  title={Think before you speak: Training Language Models With Pause Tokens},
year={2024},
  author={Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  booktitle={The Twelfth International Conference on Learning Representations}
}


@article{zhang2024counting,
  title={Counting Ability of Large Language Models and Impact of Tokenization},
  author={Zhang, Xiang and Cao, Juntai and You, Chenyu},
  journal={arXiv preprint arXiv:2410.19730},
  year={2024}
}
@article{kozachinskiy2025completely,
  title={A completely uniform transformer for parity},
  author={Kozachinskiy, Alexander and Steifer, Tomasz},
  journal={arXiv preprint arXiv:2501.02535},
  year={2025}
}

@inproceedings{
sanford2024transformers,
title={Transformers, parallel computation, and logarithmic depth},
author={Clayton Sanford and Daniel Hsu and Matus Telgarsky},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=QCZabhKQhB}
}

@article{zhang2024autoregressive,
  title={Autoregressive+ Chain of Thought= Recurrent: Recurrence's Role in Language Models' Computability and a Revisit of Recurrent Transformer},
  author={Zhang, Xiang and Abdul-Mageed, Muhammad and Lakshmanan, Laks VS},
  journal={arXiv preprint arXiv:2409.09239},
  year={2024}
}

@article{abbe2024learning,
  title={Learning High-Degree Parities: The Crucial Role of the Initialization},
  author={Abbe, Emmanuel and Cornacchia, Elisabetta and H{\k{a}}z{\l}a, Jan and Kougang-Yombi, Donald},
  journal={arXiv preprint arXiv:2412.04910},
  year={2024}
}



@article{sabbaghi2024explicitly,
  title={Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks},
  author={Sabbaghi, Mahdi and Pappas, George and Hassani, Hamed and Goel, Surbhi},
  journal={arXiv preprint arXiv:2406.01895},
  year={2024}
}



@article{cho2024arithmetic,
  title={Arithmetic transformers can length-generalize in both operand length and count},
  author={Cho, Hanseul and Cha, Jaeyoung and Bhojanapalli, Srinadh and Yun, Chulhee},
  journal={arXiv preprint arXiv:2410.15787},
  year={2024}
}


@inproceedings{
pfau2024lets,
title={Let{\textquoteright}s Think Dot by Dot: Hidden computation in transformer language models},
author={Jacob Pfau and William Merrill and Samuel R. Bowman},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=NikbrdtYvG}
}

@inproceedings{allender2006grid,
  title={Grid graph reachability problems},
  author={Allender, Eric and Chakraborty, Tanmoy and Barrington, David A Mix and Datta, Samir and Roy, Sambuddha},
  booktitle={21st Annual IEEE Conference on Computational Complexity (CCC'06)},
  pages={15--pp},
  year={2006},
  organization={IEEE}
}


@article{kulkarni2016on,
	title="On Fractional Block Sensitivity.",
	author="Raghav {Kulkarni} and Avishay {Tal}",
	journal="Chicago Journal of Theoretical Computer Science",
	volume="2016",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2578797763",
	year="2016"
}

@inproceedings{
saxton2018analysing,
title={Analysing Mathematical Reasoning Abilities of Neural Models},
author={David Saxton and Edward Grefenstette and Felix Hill and Pushmeet Kohli},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1gR5iR5FX},
}

@book{minsky1969perceptrons,
	title="Perceptrons: An Introduction to Computational Geometry",
	author="Marvin Lee {Minsky} and Seymour {Papert}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2086789740",
	year="1969"
}

@article{greenbury2016genetic,
	title="Genetic Correlations Greatly Increase Mutational Robustness and Can Both Reduce and Enhance Evolvability",
	author="Sam F. {Greenbury} and Steffen {Schaper} and Sebastian E. {Ahnert} and Ard A. {Louis}",
	journal="PLOS Computational Biology",
	volume="12",
	number="3",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2210383493",
	year="2016"
}

@article{wegener1987the,
	title="The Complexity of Symmetric Boolean Functions",
	author="Ingo {Wegener}",
	journal="Computation Theory and Logic, In Memory of Dieter Rödding",
	pages="433--442",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1587602953",
	year="1987"
}

@article{kulkarni2016on,
	title="On Fractional Block Sensitivity.",
	author="Raghav {Kulkarni} and Avishay {Tal}",
	journal="Chicago Journal of Theoretical Computer Science",
	volume="2016",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2578797763",
	year="2016"
}

@inproceedings{cheng1990an,
	title="An entropy measure for the complexity of multi-output Boolean functions",
	author="Kwang-Ting {Cheng} and Vishwani D. {Agrawal}",
	booktitle="Proceedings of the 27th ACM/IEEE Design Automation Conference on ",
	pages="302--305",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2148482302",
	year="1990"
}

@article{franco2005role,
	title="Role of function complexity and network size in the generalization ability of feedforward networks",
	author="Leonardo {Franco} and José M. {Jerez} and José M. {Bravo}",
	journal="international conference on artificial neural networks",
	pages="1--8",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1533354148",
	year="2005"
}

@inproceedings{franco2004on,
	title="On a generalization complexity measure for Boolean functions",
	author="L. {Franco} and M. {Anthony}",
	booktitle="2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)",
	volume="2",
	pages="973--978",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2117549320",
	year="2004"
}

@article{franco2006generalization,
	title="Generalization ability of Boolean functions implemented in feedforward neural networks",
	author="Leonardo {Franco}",
	journal="Neurocomputing",
	volume="70",
	number="1",
	pages="351--361",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2122613489",
	year="2006"
}

@article{franco2006the,
	title="The influence of oppositely classified examples on the generalization complexity of Boolean functions",
	author="L. {Franco} and M. {Anthony}",
	journal="IEEE Transactions on Neural Networks",
	volume="17",
	number="3",
	pages="578--590",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2123660490",
	year="2006"
}

@article{gomez2014the,
	title="The generalization complexity measure for continuous input data.",
	author="Iván {G{\'o}mez} and Sergio A. {Cannas} and Omar {Osenda} and José M. {Jerez} and Leonardo {Franco}",
	journal="The Scientific World Journal",
	volume="2014",
	pages="815156--815156",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2053167324",
	year="2014"
}

@article{franco2003the,
	title="The influence of opposite examples and randomness on the generalization complexity of Boolean functions",
	author="Leonardo {Franco} and Martin {Anthony}",
	journal="",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2112845300",
	year="2003"
}

@inproceedings{gomez2010extension,
	title="Extension of the generalization complexity measure to real valued input data sets",
	author="Iván {G{\'o}mez} and Leonardo {Franco} and José M. {Jerez} and José L. {Subirats}",
	booktitle="ISNN'10 Proceedings of the 7th international conference on Advances in Neural Networks - Volume Part I",
	pages="86--94",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2130900802",
	year="2010"
}

@article{franco2001a,
	title="A measure for the complexity of Boolean functions related to their implementation in neural networks",
	author="Leonardo {Franco}",
	journal="arXiv preprint arXiv:cond-mat/0111169",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1509570633",
	year="2001"
}

@article{chande1997number,
	title="Number of Restrictions of a Boolean Function act as a Complexity Measure",
	author="Vinay {Chande} and P G {Poonacha}",
	journal="Iete Journal of Research",
	volume="43",
	number="1",
	pages="3--10",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2078940883",
	year="1997"
}


@article{nakkiran2019sgd,
	title="{SGD} on Neural Networks Learns Functions of Increasing Complexity.",
	author="Preetum {Nakkiran} and Gal {Kaplun} and Dimitris {Kalimeris} and Tristan {Yang} and Benjamin L. {Edelman} and Fred {Zhang} and Boaz {Barak}",
	journal="arXiv preprint arXiv:1905.11604",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2947785987",
	year="2019"
}

@inproceedings{valle-perez2019deep,
	title="Deep learning generalizes because the parameter-function map is biased towards simple functions",
	author="Guillermo {Valle-Perez} and Chico Q. {Camargo} and Ard {Louis}",
	booktitle="ICLR 2019: 7th International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2964122761",
	year="2019"
}

@article{linial1993constant,
	title="Constant depth circuits, Fourier transform, and learnability",
	author="Nathan {Linial} and Yishay {Mansour} and Noam {Nisan}",
	journal="Journal of the ACM",
	volume="40",
	number="3",
	pages="607--620",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2042194938",
	year="1993"
}

@inproceedings{kahn1988the,
	title="The influence of variables on Boolean functions",
	author="J. {Kahn} and G. {Kalai} and N. {Linial}",
	booktitle="[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science",
	pages="68--80",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2103749128",
	year="1988"
}

@article{samorodnitsky2009gowers,
	title="Gowers Uniformity, Influence of Variables, and PCPs",
	author="Alex {Samorodnitsky} and Luca {Trevisan}",
	journal="SIAM Journal on Computing",
	volume="39",
	number="1",
	pages="323--360",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2055711302",
	year="2009"
}

@article{bruck1992polynomial,
	title="Polynomial threshold functions, AC 0 functions, and spectral norms",
	author="Jehoshua {Bruck} and Roman {Smolensky}",
	journal="SIAM Journal on Computing",
	volume="21",
	number="1",
	pages="33--42",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2117508909",
	year="1992"
}

@article{torvik2002minimizing,
	title="Minimizing the Average Query Complexity of Learning Monotone Boolean Functions",
	author="Vetle I. {Torvik} and Evangelos {Triantaphyllou}",
	journal="Informs Journal on Computing",
	volume="14",
	number="2",
	pages="144--174",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2151393344",
	year="2002"
}

@article{anthony1995on,
	title="On specifying Boolean functions by labelled examples",
	author="Martin {Anthony} and Graham {Brightwell} and John {Shawe-Taylor}",
	journal="Discrete Applied Mathematics",
	volume="61",
	number="1",
	pages="1--25",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1974820192",
	year="1995"
}

@article{franco2001generalization,
	title="Generalization properties of modular networks: implementing the parity function",
	author="L. {Franco} and S.A. {Cannas}",
	journal="IEEE Transactions on Neural Networks",
	volume="12",
	number="6",
	pages="1306--1313",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2155258457",
	year="2001"
}

@article{gál2002a,
	title="A Theorem on Sensitivity and Applications in Private Computation",
	author="Anna {Gál} and Adi {Rosén}",
	journal="SIAM Journal on Computing",
	volume="31",
	number="5",
	pages="1424--1437",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1980995759",
	year="2002"
}



@inproceedings{nakkiran2020deep,
	title="Deep Double Descent: Where Bigger Models and More Data Hurt",
	author="Preetum {Nakkiran} and Gal {Kaplun} and Yamini {Bansal} and Tristan {Yang} and Boaz {Barak} and Ilya {Sutskever}",
	booktitle="ICLR 2020: Eighth International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2996603747",
	year="2020"
}

@inproceedings{lee2019wide,
	title="Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
	author="Jaehoon {Lee} and Lechao {Xiao} and Samuel {Schoenholz} and Yasaman {Bahri} and Roman {Novak} and Jascha {Sohl-Dickstein} and Jeffrey {Pennington}",
	booktitle="NeurIPS 2019: Thirty-third Conference on {N}eural {I}nformation {P}rocessing {S}ystems",
	pages="8572--8583",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2970217468",
	year="2019"
}

@inproceedings{chatterjee2020coherent,
	title="Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization",
	author="Sat {Chatterjee}",
	booktitle="ICLR 2020: Eighth International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2995185977",
	year="2020"
}

@article{cao2019towards,
	title="Towards Understanding the Spectral Bias of Deep Learning",
	author="Yuan {Cao} and Zhiying {Fang} and Yue {Wu} and Ding-Xuan {Zhou} and Quanquan {Gu}",
	journal="arXiv preprint arXiv:1912.01198",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2994110075",
	year="2019"
}

@inproceedings{gissin2020the,
	title="The Implicit Bias of Depth: How Incremental Learning Drives Generalization",
	author="Daniel {Gissin} and Shai {Shalev-Shwartz} and Amit {Daniely}",
	booktitle="ICLR 2020: Eighth International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2996210821",
	year="2020"
}




@inproceedings{rahaman2019on,
	title="On the Spectral Bias of Neural Networks",
	author="Nasim {Rahaman} and Aristide {Baratin} and Devansh {Arpit} and Felix {Draxler} and Min {Lin} and Fred {Hamprecht} and Yoshua {Bengio} and Aaron {Courville}",
	booktitle="ICML 2019: Thirty-sixth International Conference on Machine Learning",
	pages="5301--5310",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2897097528",
	year="2019"
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{jaeger2012formal,
	title="Formal language theory: refining the Chomsky hierarchy",
	author="Gerhard {Jäger} and James {Rogers}",
	journal="Philosophical Transactions of the Royal Society B",
	volume="367",
	number="1598",
	pages="1956--1970",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2115912164",
	year="2012"
}



@article{suzgun2019on,
	title="On Evaluating the Generalization of {LSTM} Models in Formal Languages",
	author="Mirac {Suzgun} and Yonatan {Belinkov} and Stuart M. {Shieber}",
	journal="Proceedings of the Society for Computation in Linguistics",
	volume="2",
	number="1",
	pages="277--286",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963723151",
	year="2019"
}


@article{merrill2020a,
	title="A Formal Hierarchy of RNN Architectures.",
	author="William {Merrill} and Gail {Weiss} and Yoav {Goldberg} and Roy {Schwartz} and Noah A. {Smith} and Eran {Yahav}",
	journal="arXiv preprint arXiv:2004.08500",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3017116062",
	year="2020"
}


@article{jeretic2020are,
	title="Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition.",
	author="Paloma {Jeretic} and Alex {Warstadt} and Suvrat {Bhooshan} and Adina {Williams}",
	journal="arXiv preprint arXiv:2004.03066",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3015830489",
	year="2020"
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@inproceedings{xu2019training,
  title={Training behavior of deep neural network in frequency domain},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Xiao, Yanyang},
  booktitle={International Conference on Neural Information Processing},
  pages={264--274},
  year={2019},
  organization={Springer}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  pages={9461--9471},
  year={2018}
}
@article{novak2018sensitivity,
  title={Sensitivity and generalization in neural networks: an empirical study},
  author={Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1802.08760},
  year={2018}
}

@article{wieting2015towards,
  title={Towards universal paraphrastic sentence embeddings},
  author={Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
  journal={arXiv preprint arXiv:1511.08198},
  year={2015}
}
@inproceedings{arora2016simple,
	title="A Simple but Tough-to-Beat Baseline for Sentence Embeddings",
	author="Sanjeev {Arora} and Yingyu {Liang} and Tengyu {Ma}",
	booktitle="ICLR 2017: International Conference on Learning Representations 2017",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2752172973",
	year="2017"
}

@article{gissin2019implicit,
  title={The Implicit Bias of Depth: How Incremental Learning Drives Generalization},
  author={Gissin, Daniel and Shalev-Shwartz, Shai and Daniely, Amit},
  journal={arXiv preprint arXiv:1909.12051},
  year={2019}
}

@inproceedings{horne1994bounds,
  title={Bounds on the complexity of recurrent neural network implementations of finite state machines},
  author={Horne, Bill G and Hush, Don R},
  booktitle={Advances in {N}eural Information Processing Systems},
  pages={359--366},
  year={1994}
}



@article{bourgain1992influence,
  title={The influence of variables in product spaces},
  author={Bourgain, Jean and Kahn, Jeff and Kalai, Gil and Katznelson, Yitzhak and Linial, Nathan},
  journal={Israel Journal of Mathematics},
  volume={77},
  number={1-2},
  pages={55--64},
  year={1992},
  publisher={Springer}
}

@article{hatami2009decision,
  title={Decision trees and influences of variables over product probability spaces},
  author={Hatami, Hamed},
  journal={Combinatorics, Probability and Computing},
  volume={18},
  number={3},
  pages={357--369},
  year={2009},
  publisher={Cambridge University Press}
}

@inproceedings{mossel2005noise,
  title={Noise stability of functions with low influences: invariance and optimality},
  author={Mossel, Elchanan and O'Donnell, Ryan and Oleszkiewicz, Krzysztof},
  booktitle={46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)},
  pages={21--30},
  year={2005},
  organization={IEEE}
}

@article{keller2011influences,
  title={On the influences of variables on Boolean functions in product spaces},
  author={Keller, Nathan},
  journal={Combinatorics, Probability and Computing},
  volume={20},
  number={1},
  pages={83--102},
  year={2011},
  publisher={Cambridge University Press}
}

@inproceedings{petrov2012a,
	title="A Universal Part-of-Speech Tagset",
	author="Slav {Petrov} and Dipanjan {Das} and Ryan {McDonald}",
	booktitle="Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)",
	pages="2089--2096",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2143995218",
	year="2012"
}


@inproceedings{nivre2016universal,
	title="Universal Dependencies v1: A Multilingual Treebank Collection",
	author="Joakim {Nivre} and Marie-Catherine de {Marneffe} and Filip {Ginter} and Yoav {Goldberg} and Jan {Hajic} and Christopher D. {Manning} and Ryan T. {McDonald} and Slav {Petrov} and Sampo {Pyysalo} and Natalia {Silveira} and Reut {Tsarfaty} and Daniel {Zeman}",
	booktitle="Tenth International Conference on Language Resources and Evaluation (LREC 2016)",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2579343286",
	year="2016"
}


@article{grimmett2016influence,
	title="Influence in product spaces",
	author="Geoffrey R. {Grimmett} and Svante {Janson} and James R. {Norris}",
	journal="Advances in Applied Probability",
	volume="48",
	pages="145--152",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2247076637",
	year="2016"
}

@article{graham2006influence,
	title="Influence and sharp-threshold theorems for monotonic measures",
	author="B. T. {Graham} and G. R. {Grimmett}",
	journal="Annals of Probability",
	volume="34",
	number="5",
	pages="1726--1745",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2034647305",
	year="2006"
}

@article{graham2011sharp,
	title="Sharp thresholds for the random-cluster and Ising models",
	author="Benjamin T. {Graham} and Geoffrey {Grimmett}",
	journal="Annals of Applied Probability",
	volume="21",
	number="1",
	pages="240--265",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2004060089",
	year="2011"
}

@inproceedings{rajpurkar2016squad,
	title="SQuAD: 100,000+ Questions for Machine Comprehension of Text",
	author="Pranav {Rajpurkar} and Jian {Zhang} and Konstantin {Lopyrev} and Percy {Liang}",
	booktitle="Proceedings of the 2016 Conference on Empirical Methods in Natural
      Language Processing",
	pages="2383--2392",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963748441",
	year="2016"
}

@inproceedings{williams2018a,
	title="A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE",
	author="Adina {Williams} and Nikita {Nangia} and Samuel {Bowman}",
	booktitle="NAACL HLT 2018: 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	volume="1",
	pages="1112--1122",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963846996",
	year="2018"
}

@inproceedings{levesque2012the,
	title="The Winograd schema challenge",
	author="Hector J. {Levesque} and Ernest {Davis} and Leora {Morgenstern}",
	booktitle="KR'12 Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning",
	pages="552--561",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1599016936",
	year="2012"
}

@article{warstadt2019neural,
	title="Neural Network Acceptability Judgments",
	author="Alex {Warstadt} and Amanpreet {Singh} and Samuel R. {Bowman}",
	journal="Transactions of the Association for Computational Linguistics",
	volume="7",
	pages="625--641",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2978670439",
	year="2019"
}

@incollection{dolan2005automatically,
  author    = {William B. Dolan and
               Chris Brockett},
  title     = {Automatically Constructing a Corpus of Sentential Paraphrases},
  booktitle = {Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP
               2005, Jeju Island, Korea, October 2005, 2005},
  publisher = {Asian Federation of Natural Language Processing},
  year      = {2005},
  url       = {https://www.aclweb.org/anthology/I05-5002/},
  timestamp = {Tue, 17 Sep 2019 17:11:58 +0200},
  biburl    = {https://dblp.org/rec/conf/acl-iwp/DolanB05.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bjerva2017cross,
	title="Cross-lingual Learning of Semantic Textual Similarity with Multilingual Word Representations",
	author="Johannes {Bjerva} and Robert {Östling}",
	journal="Proceedings of the 21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May 2017, Gothenburg, Sweden",
	number="131",
	pages="211--215",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2680186315",
	year="2017"
}

@inproceedings{cer2017semeval,
	title="SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
	author="Daniel M. {Cer} and Mona T. {Diab} and Eneko {Agirre} and Iñigo {Lopez-Gazpio} and Lucia {Specia}",
	booktitle="Proceedings of the 11th International Workshop on Semantic Evaluation
      (SemEval-2017)",
	pages="1--14",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2739351760",
	year="2017"
}

@article{chomsky1956three,
	title="Three models for the description of language",
	author="N. {Chomsky}",
	journal="IEEE Transactions on Information Theory",
	volume="2",
	number="3",
	pages="113--124",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2124479173",
	year="1956"
}

@book{li1993an,
	title="An introduction to Kolmogorov complexity and its applications",
	author="Ming {Li} and Paul {Vitányi}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1638203394",
	publisher={Springer},
	year="1993"
}

@article{dingle2018input,
	title="Input-output maps are strongly biased towards simple outputs.",
	author="Kamaludin {Dingle} and Chico Q. {Camargo} and Ard A. {Louis}",
	journal="Nature Communications",
	volume="9",
	number="1",
	pages="761--761",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2794316594",
	year="2018"
}

@inproceedings{pennington2014glove,
	title="Glove: Global Vectors for Word Representation",
	author="Jeffrey {Pennington} and Richard {Socher} and Christopher {Manning}",
	booktitle="Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	pages="1532--1543",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2250539671",
	year="2014"
}

@inproceedings{ethayarajh2018unsupervised,
	title="Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline",
	author="Kawin {Ethayarajh}",
	booktitle="Proceedings of The Third Workshop on Representation Learning for NLP",
	pages="91--100",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2887005207",
	year="2018"
}

@inproceedings{kaushik2018how,
	title="How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks",
	author="Divyansh {Kaushik} and Zachary C. {Lipton}",
	booktitle="EMNLP 2018: 2018 Conference on Empirical Methods in Natural Language Processing",
	pages="5010--5015",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2962727366",
	year="2018"
}

@inproceedings{gururangan2018annotation,
	title="Annotation artifacts in natural language inference data",
	author="Suchin {Gururangan} and Swabha {Swayamdipta} and Omer {Levy} and Roy {Schwartz} and Samuel {Bowman} and Noah A. {Smith}",
	booktitle="NAACL HLT 2018: 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	volume="2",
	pages="107--112",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2962736243",
	year="2018"
}


@inproceedings{poliak2018hypothesis,
	title="Hypothesis Only Baselines in Natural Language Inference",
	author="Adam {Poliak} and Jason {Naradowsky} and Aparajita {Haldar} and Rachel {Rudinger} and Benjamin Van {Durme}",
	booktitle="
          Proceedings of the Seventh Joint Conference on Lexical and
          Computational Semantics
        ",
	pages="180--191",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2962843521",
	year="2018"
}


@inproceedings{liao2020probabilistically,
	title="Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order",
	author="Yi {Liao} and Xin {Jiang} and Qun {Liu}",
	booktitle="Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	pages="263--274",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3034878914",
	year="2020"
}




@article{datta2020geometry,
  author    = {Debajyoti Datta and
               Shashwat Kumar and
               Laura E. Barnes and
               Tom Fletcher},
  title     = {Geometry matters: Exploring language examples at the decision boundary},
  journal   = {CoRR},
  volume    = {abs/2010.07212},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.07212},
  archivePrefix = {arXiv},
  eprint    = {2010.07212},
  timestamp = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-07212.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{smith2013effect,
  author =        {Smith, Nathaniel J. and Levy, Roger},
  journal =       {Cognition},
  number =        {3},
  pages =         {302--319},
  publisher =     {Elsevier},
  title =         {The effect of word predictability on reading time is
                   logarithmic},
  volume =        {128},
  year =          {2013},
}


@inproceedings{hale2001probabilistic,
  author =        {Hale, John T.},
  booktitle =     {Proceedings of the Second Meeting of the North
                   American Chapter of the Association for Computational
                   Linguistics and Language Technologies},
  pages =         {1--8},
  title =         {A probabilistic {Earley} parser as a psycholinguistic
                   model},
  year =          {2001},
}

@article{levy2008expectation,
  author =        {Levy, Roger},
  journal =       {Cognition},
  number =        {3},
  pages =         {1126--1177},
  publisher =     {Elsevier},
  title =         {Expectation-based syntactic comprehension},
  volume =        {106},
  year =          {2008},
}

@article{gibson1998linguistic,
  author =        {Gibson, Edward},
  journal =       {Cognition},
  number =        {1},
  pages =         {1--76},
  title =         {Linguistic complexity: Locality of syntactic
                   dependencies},
  volume =        {68},
  year =          {1998},
}

@inproceedings{nangia2019human,
	title="Human vs. Muppet: A conservative estimate of human performance on the GLUE benchmark",
	author="Nikita {Nangia} and Samuel R. {Bowman}",
	booktitle="Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	pages="4566--4575",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2945067664",
	year="2019"
}

@inproceedings{chelba2014one,
	title="One billion word benchmark for measuring progress in statistical language modeling.",
	author="Ciprian {Chelba} and Tomas {Mikolov} and Mike {Schuster} and Qi {Ge} and Thorsten {Brants} and Phillipp {Koehn} and Tony {Robinson}",
	booktitle="INTERSPEECH",
	pages="2635--2639",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2611669587",
	year="2014"
}

@article{hahn2020sensitivity,
  author    = {Michael Hahn and
               Dan Jurafsky and Richard Futrell},
  title     = {Sensitivity as a Complexity Measure for Sequence Classification Tasks},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {9},
  pages = {891--908},
  year      = {2021},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00403/106992/Sensitivity-as-a-Complexity-Measure-for-Sequence},
  preprint       = {https://arxiv.org/abs/2104.10343},
  github = {https://github.com/m-hahn/sensitivity},
  slides = {files/sensitivity-slides.pdf}
}



@inproceedings{
zhou2023understanding,
title={Understanding Length Generalization by Thinking Like Transformers},
author={Hattie Zhou and Arwen Bradley and Etai Littwin and Noam Razin and Omid Saremi and Joshua Susskind and Samy Bengio and Preetum Nakkiran},
booktitle={The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23},
year={2023},
url={https://openreview.net/forum?id=tEUJiua8ir}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}


@article{merrill2023parallelism,
  title={The parallelism tradeoff: Limitations of log-precision transformers},
  author={Merrill, William and Sabharwal, Ashish},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={531--545},
  year={2023},
  publisher={MIT Press}
}
@inproceedings{merrill2023logic,
  title={A Logic for Expressing Log-Precision Transformers},
  author={Merrill, William and Sabharwal, Ashish},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}



@inproceedings{chiang2023tighter,
  author       = {David Chiang and
                  Peter Cholak and
                  Anand Pillay},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Tighter Bounds on the Expressivity of Transformer Encoders},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {5544--5562},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/chiang23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/0001CP23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{hao2022formal,
  title={Formal language recognition by hard attention transformers: Perspectives from circuit complexity},
  author={Hao, Yiding and Angluin, Dana and Frank, Robert},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={800--810},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}



@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}



@inproceedings{bhattamishra2022simplicity,
  author       = {Satwik Bhattamishra and
                  Arkil Patel and
                  Varun Kanade and
                  Phil Blunsom},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Simplicity Bias in Transformers and their Ability to Learn Sparse
                  Boolean Functions},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada,
                  July 9-14, 2023},
  pages        = {5767--5791},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.acl-long.317},
  doi          = {10.18653/V1/2023.ACL-LONG.317},
  timestamp    = {Thu, 10 Aug 2023 12:36:04 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/BhattamishraPKB23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{abbe2023generalization,
  author       = {Emmanuel Abbe and
                  Samy Bengio and
                  Aryo Lotfi and
                  Kevin Rizk},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Generalization on the Unseen, Logic Reasoning and Degree Curriculum},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {31--60},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/abbe23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/AbbeBLR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}






@inproceedings{bhattamishra2020ability,
  author       = {Satwik Bhattamishra and
                  Kabir Ahuja and
                  Navin Goyal},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {On the Ability and Limitations of Transformers to Recognize Formal
                  Languages},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {7096--7116},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.576},
  doi          = {10.18653/V1/2020.EMNLP-MAIN.576},
  timestamp    = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/BhattamishraAG20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{chiang2022overcoming,
  author       = {David Chiang and
                  Peter Cholak},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {Overcoming a Theoretical Limitation of Self-Attention},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {7654--7664},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.acl-long.527},
  doi          = {10.18653/V1/2022.ACL-LONG.527},
  timestamp    = {Mon, 01 Aug 2022 16:27:46 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/0001C22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{deletang2022neural,
  author       = {Gr{\'{e}}goire Del{\'{e}}tang and
                  Anian Ruoss and
                  Jordi Grau{-}Moya and
                  Tim Genewein and
                  Li Kevin Wenliang and
                  Elliot Catt and
                  Chris Cundy and
                  Marcus Hutter and
                  Shane Legg and
                  Joel Veness and
                  Pedro A. Ortega},
  title        = {Neural Networks and the Chomsky Hierarchy},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=WbxHAzkeQcn},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DeletangRGGWCCH23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@misc{ruoss2023randomized,
      title={Randomized Positional Encodings Boost Length Generalization of Transformers}, 
      author={Anian Ruoss and Grégoire Delétang and Tim Genewein and Jordi Grau-Moya and Róbert Csordás and Mehdi Bennani and Shane Legg and Joel Veness},
      year={2023},
      eprint={2305.16843},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yun2019transformers,
    title={Are Transformers universal approximators of sequence-to-sequence functions?},
    author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
    year={2019},
    eprint={1912.10077},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{ahn2023linear,
      title={Linear attention is (maybe) all you need (to understand transformer optimization)}, 
      author={Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra},
      year={2023},
      eprint={2310.01082},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Yao_2021,
   title={Self-Attention Networks Can Process Bounded Hierarchical Languages},
   url={http://dx.doi.org/10.18653/v1/2021.acl-long.292},
   DOI={10.18653/v1/2021.acl-long.292},
   booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
   year={2021} }

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}


@article{lu2023prompts,
  title={How are Prompts Different in Terms of Sensitivity?},
  author={Lu, Sheng and Schuff, Hendrik and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2311.07230},
  year={2023}
}



@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}


@article{strobl2023averagehard,
  author       = {Lena Strobl},
  title        = {Average-Hard Attention Transformers are Constant-Depth Uniform Threshold
                  Circuits},
  journal      = {CoRR},
  volume       = {abs/2308.03212},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.03212},
  doi          = {10.48550/ARXIV.2308.03212},
  eprinttype    = {arXiv},
  eprint       = {2308.03212},
  timestamp    = {Mon, 21 Aug 2023 17:38:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-03212.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{strobl2023survey,
  author       = {Lena Strobl and
                  William Merrill and
                  Gail Weiss and
                  David Chiang and
                  Dana Angluin},
  title        = {Transformers as Recognizers of Formal Languages: {A} Survey on Expressivity},
  journal      = {CoRR},
  volume       = {abs/2311.00208},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.00208},
  doi          = {10.48550/ARXIV.2311.00208},
  eprinttype    = {arXiv},
  eprint       = {2311.00208},
  timestamp    = {Wed, 08 Nov 2023 09:50:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-00208.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{carbery2004multilinear,
  title={A multilinear generalisation of the Cauchy-Schwarz inequality},
  author={Carbery, Anthony},
  journal={Proceedings of the American Mathematical Society},
  volume={132},
  number={11},
  pages={3141--3152},
  year={2004}
}


@inproceedings{merrill2023expresssive,
title={The Expressive Power of Transformers with Chain of Thought},
author={William Merrill and Ashish Sabharwal},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NjNGlPh8Wh}
}

@inproceedings{feng2023towards,
title={Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective},
author={Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=qHrADgAdYu}
}


@article{wies2022sub,
  title={Sub-task decomposition enables learning in sequence to sequence tasks},
  author={Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2204.02892},
  year={2022}
}


@inproceedings{liu2022transformers,
title={Transformers Learn Shortcuts to Automata},
author={Bingbin Liu and Jordan T. Ash and Surbhi Goel and Akshay Krishnamurthy and Cyril Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=De4FYqjFueZ}
}


@InProceedings{li2023stability,
  title = 	 {Transformers as Algorithms: Generalization and Stability in In-context Learning},
  author =       {Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {19565--19594},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/li23l/li23l.pdf},
  url = 	 {https://proceedings.mlr.press/v202/li23l.html},
  abstract = 	 {In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.}
}


@book{bullen2013handbook,
  title={Handbook of means and their inequalities},
  author={Bullen, Peter S},
  volume={560},
  year={2013},
  publisher={Springer Science \& Business Media}
}



@article{witkowski2004new,
  title={A new proof of the monotonicity of power means},
  author={Witkowski, Alfred},
  journal={J. Ineq. Pure and Appl. Math},
  volume={5},
  number={1},
  year={2004}
}





@article{DBLP:journals/corr/abs-2211-05729,
  author       = {Kaiyue Wen and
                  Tengyu Ma and
                  Zhiyuan Li},
  title        = {How Does Sharpness-Aware Minimization Minimize Sharpness?},
  journal      = {CoRR},
  volume       = {abs/2211.05729},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.05729},
  doi          = {10.48550/ARXIV.2211.05729},
  eprinttype    = {arXiv},
  eprint       = {2211.05729},
  timestamp    = {Fri, 29 Sep 2023 12:48:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-05729.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{takase2022layer,
  title={On layer normalizations and residual connections in transformers},
  author={Takase, Sho and Kiyono, Shun and Kobayashi, Sosuke and Suzuki, Jun},
  journal={arXiv preprint arXiv:2206.00330},
  year={2022}
}



@article{DBLP:journals/corr/BaKH16,
  author       = {Lei Jimmy Ba and
                  Jamie Ryan Kiros and
                  Geoffrey E. Hinton},
  title        = {Layer Normalization},
  journal      = {CoRR},
  volume       = {abs/1607.06450},
  year         = {2016},
  url          = {http://arxiv.org/abs/1607.06450},
  eprinttype    = {arXiv},
  eprint       = {1607.06450},
  timestamp    = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{andriushchenko2023modern,
  author       = {Maksym Andriushchenko and
                  Francesco Croce and
                  Maximilian M{\"{u}}ller and
                  Matthias Hein and
                  Nicolas Flammarion},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {A Modern Look at the Relationship between Sharpness and Generalization},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {840--902},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/andriushchenko23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/AndriushchenkoC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}


@inproceedings{jiang2019fantastic,
title={Fantastic Generalization Measures and Where to Find Them},
author={Yiding Jiang and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgIPJBFvH}
}

@inproceedings{kaur2023maximum,
  title={On the maximum hessian eigenvalue and generalization},
  author={Kaur, Simran and Cohen, Jeremy and Lipton, Zachary Chase},
  booktitle={Proceedings on},
  pages={51--65},
  year={2023},
  organization={PMLR}
}

@article{li2010concise,
  title={Concise formulas for the area and volume of a hyperspherical cap},
  author={Li, Shengqiao},
  journal={Asian Journal of Mathematics \& Statistics},
  volume={4},
  number={1},
  pages={66--70},
  year={2010},
  publisher={Science Alert}
}


@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}



@article{de2008brief,
  title={A brief introduction to Fourier analysis on the Boolean cube},
  author={De Wolf, Ronald},
  journal={Theory of Computing},
  pages={1--20},
  year={2008},
  publisher={Theory of Computing Exchange}
}



@inproceedings{Damian2023SelfStab,
  author       = {Alex Damian and
                  Eshaan Nichani and
                  Jason D. Lee},
  title        = {Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge
                  of Stability},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=nhKHA59gXz},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DamianNL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{pytorch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Loshchilov2017DecoupledWD,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:53592270}
}



@InProceedings{pmlr-v97-rahaman19a,
  title = 	 {On the Spectral Bias of Neural Networks},
  author =       {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5301--5310},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/rahaman19a.html},
  abstract = 	 {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions – i.e. functions that vary globally without local fluctuations – which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.}
}


@inproceedings{NEURIPS2019_b432f34c,
 author = {Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SGD on Neural Networks Learns Functions of Increasing Complexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/b432f34c5a997c8e7c806a895ecc5e25-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2022_306264db,
 author = {Fridovich-Keil, Sara and Gontijo Lopes, Raphael and Roelofs, Rebecca},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {7368--7382},
 publisher = {Curran Associates, Inc.},
 title = {Spectral Bias in Practice: The Role of Function Frequency in Generalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/306264db5698839230be3642aafc849c-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}




@article{DBLP:journals/tacl/MerrillSS22,
  author       = {William Merrill and
                  Ashish Sabharwal and
                  Noah A. Smith},
  title        = {Saturated Transformers are Constant-Depth Threshold Circuits},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {10},
  pages        = {843--856},
  year         = {2022},
  url          = {https://transacl.org/ojs/index.php/tacl/article/view/3465},
  timestamp    = {Wed, 26 Oct 2022 16:52:10 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/MerrillSS22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}






@article{bhattamishra2020computational,
  title={On the computational power of transformers and its implications in sequence modeling},
  author={Bhattamishra, Satwik and Patel, Arkil and Goyal, Navin},
  journal={arXiv preprint arXiv:2006.09286},
  year={2020}
}



@inproceedings{
kim2024transformers,
title={Transformers Provably Solve Parity Efficiently with Chain of Thought},
author={Juno Kim and Taiji Suzuki},
booktitle={NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning},
year={2024},
url={https://openreview.net/forum?id=E7HwPhfX1B}
}


@article{hou2024universal,
  title={Universal length generalization with turing programs},
  author={Hou, Kaiying and Brandfonbrener, David and Kakade, Sham and Jelassi, Samy and Malach, Eran},
  journal={arXiv preprint arXiv:2407.03310},
  year={2024}
}

@article{DBLP:journals/mst/KrebsLR07,
  author       = {Andreas Krebs and
                  Klaus{-}J{\"{o}}rn Lange and
                  Stephanie Reifferscheid},
  title        = {Characterizing {TC}\({}^{\mbox{0}}\) in Terms of Infinite Groups},
  journal      = {Theory Comput. Syst.},
  volume       = {40},
  number       = {4},
  pages        = {303--325},
  year         = {2007},
  url          = {https://doi.org/10.1007/s00224-006-1310-2},
  doi          = {10.1007/S00224-006-1310-2},
  timestamp    = {Sun, 28 May 2017 13:18:25 +0200},
  biburl       = {https://dblp.org/rec/journals/mst/KrebsLR07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{qiu2024ask,
  title={Ask, and it shall be given: Turing completeness of prompting},
  author={Qiu, Ruizhong and Xu, Zhe and Bao, Wenxuan and Tong, Hanghang},
  journal={arXiv preprint arXiv:2411.01992},
  year={2024}
}




@article{furst1984parity,
  title={Parity, circuits, and the polynomial-time hierarchy},
  author={Furst, Merrick and Saxe, James B and Sipser, Michael},
  journal={Mathematical systems theory},
  volume={17},
  number={1},
  pages={13--27},
  year={1984},
  publisher={Springer}
}

@book{mitzenmacherprobability,
  title={Probability and Computing},
  year={2017},
  author={Mitzenmacher, Michael and Upfal, Eli},
  publisher={Cambridge University Press},
  address={Cambridge},
  edition={2nd}
}

@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={740--745},
  year={2018}
}


@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of {LSTM}s to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}


@article{suzgun2019evaluating,
  title={On Evaluating the Generalization of {LSTM} Models in Formal Languages},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
  journal={Proceedings of the Society for Computation in Linguistics (SCiL)},
  pages={277--286},
  year={2019}
}


@article{merrill2019sequential,
  title={Sequential neural networks as automata},
  author={Merrill, William},
  journal={arXiv preprint arXiv:1906.01615},
  year={2019}
}



@article{bernardy2018can,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  author={Bernardy, Jean-Philippe},
  journal={LiLT (Linguistic Issues in Language Technology)},
  volume={16},
  number={1},
  year={2018}
}

@inproceedings{
perez2019turing,
title={On the {Turing} Completeness of Modern Neural Network Architectures},
author={Jorge Pérez and Javier Marinković and Pablo Barcel{\'o}},
booktitle={International Conference on Learning Representations},
year={2019}
}


@nproceedings{chen2017recurrent,
    title = "Recurrent Neural Networks as Weighted Language Recognizers",
    author = "Chen, Yining  and
      Gilroy, Sorcha  and
      Maletti, Andreas  and
      May, Jonathan  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1205",
    doi = "10.18653/v1/N18-1205",
    pages = "2261--2271",
    abstract = "We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.",
}



@article{siegelman1991neural,
  title={On    the    computational    power of   neural   nets},
  author={Siegelman, Hava and Sontag, Eduardo D},
  journal={Journal of Computer and System Sciences},
  year={1995},
  volume={50},
  issue={1},
 pages={132--150}
}



@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}


@inproceedings{dehghani2018universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019}
}}

@inproceedings{tran2018importance,
    title = "The Importance of Being Recurrent for Modeling Hierarchical Structure",
    author = "Tran, Ke  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "4731--4736",
    abstract = "Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures{---}recurrent versus non-recurrent{---}with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at \url{https://github.com/ ketranm/fan_vs_rnn}",
}

@inproceedings{yang2019assessing,
  title={Assessing the Ability of Self-Attention Networks to Learn Word Order},
  author={Yang, Baosong and Wang, Longyue and Wong, Derek F and Chao, Lidia S and Tu, Zhaopeng},
  booktitle={57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019}
}

@inproceedings{yao1986separating,
author={A. C. {Yao}},
booktitle={26th Annual Symposium on Foundations of Computer Science (SFCS 1985)},
title={Separating the polynomial-time hierarchy by oracles},
year={1985},
volume={},
number={},
pages={1-10}}


@article{hastad1994optimal,
  title={Optimal Depth, Very Small Size Circuits for Symmetrical Functions in {AC}0},
  author={Hastad, Johan and Wegener, Ingo and Wurm, Norbert and Yi, Sang-Zin},
  journal={Information and Computation},
  volume={108},
  number={2},
  pages={200--211},
  year={1994},
  publisher={Elsevier}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{kirov2012processing,
  title={Processing of nested and cross-serial dependencies: an automaton perspective on {SRN} behaviour},
  author={Kirov, Christo and Frank, Robert},
  journal={Connection Science},
  volume={24},
  number={1},
  pages={1--24},
  year={2012},
  publisher={Taylor \& Francis}
}


@inproceedings{kirov2013bayesian,
  title={Bayesian speech production: Evidence from latency and hyperarticulation},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={35},
  number={35},
  year={2013}
}


@inproceedings{kirov2012specificity,
  title={The specificity of online variation in speech production},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}


@inproceedings{shen2018reinforced,
  title={Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Wang, Sen and Zhang, Chengqi},
  booktitle={IJCAI'18 Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  year={2018},
  pages={4345--4352}
}


@inproceedings{shen2018disan,
  title={Disan: Directional self-attention network for rnn/cnn-free language understanding},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}


@inproceedings{chen2018best,
    title = "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
    author = "Chen, Mia Xu  and
      Firat, Orhan  and
      Bapna, Ankur  and
      Johnson, Melvin  and
      Macherey, Wolfgang  and
      Foster, George  and
      Jones, Llion  and
      Schuster, Mike  and
      Shazeer, Noam  and
      Parmar, Niki  and
      Vaswani, Ashish  and
      Uszkoreit, Jakob  and
      Kaiser, Lukasz  and
      Chen, Zhifeng  and
      Wu, Yonghui  and
      Hughes, Macduff",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    pages = "76--86",
    abstract = "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT{'}14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",
}



@inproceedings{hao2019modeling,
    title = "Modeling Recurrence for Transformer",
    author = "Hao, Jie  and
      Wang, Xing  and
      Yang, Baosong  and
      Wang, Longyue  and
      Zhang, Jinfeng  and
      Tu, Zhaopeng",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "1198--1207",
    abstract = "Recently, the Transformer model that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence modeling hinders its further improvement of translation capacity. In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention models and recurrent networks. Experimental results on the widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences with a single recurrent layer, which outperforms its deep counterpart.",
}

@inproceedings{
paulus2017deep,
title={A Deep Reinforced Model for Abstractive Summarization},
author={Romain Paulus and Caiming Xiong and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018}
}


@inproceedings{cheng2016long,
    title = "Long Short-Term Memory-Networks for Machine Reading",
    author = "Cheng, Jianpeng  and
      Dong, Li  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    pages = "551--561",
}


@inproceedings{lin2017structured,
  title={A structured self-attentive sentence embedding},
  author={Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2017}
}


@inproceedings{parikh2016decomposable,
	    title = "A Decomposable Attention Model for Natural Language Inference",
	    author = {Parikh, Ankur  and
	      T{\"a}ckstr{\"o}m, Oscar  and
	      Das, Dipanjan  and
	      Uszkoreit, Jakob},
	    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
	    month = nov,
	    year = "2016",
	    address = "Austin, Texas",
	    publisher = "Association for Computational Linguistics",
	    pages = "2249--2255",
	}

@inproeedings{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}


@inproceedings{clark2019bert,
  title={What Does {BERT} Look At? {A}n Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of BlackboxNLP 2019},
  year={2019}
}



@article{dai2019transformer,
  title={Transformer-{XL}: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Cohen, William W and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{gulordava2018colorless,
    title = "Colorless Green Recurrent Networks Dream Hierarchically",
    author = "Gulordava, Kristina  and
      Bojanowski, Piotr  and
      Grave, Edouard  and
      Linzen, Tal  and
      Baroni, Marco",
    booktitle = "2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2018",
    pages = "1195--1205"
}

@article{linzen2016assessing,
	Author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	Journal = {Transactions of the Association for Computational Linguistics},
	Title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
    Volume = {4},
    Pages = {521--535},
	Year = {2016}
}

@inproceedings{skachkova2018closing,
  title={Closing Brackets with Recurrent Neural Networks},
  author={Skachkova, Natalia and Trost, Thomas and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}


@article{tabor2000fractal,
  title={Fractal encoding of context-free grammars in connectionist networks},
  author={Tabor, Whitney},
  journal={Expert Systems},
  volume={17},
  number={1},
  pages={41--56},
  year={2000},
  publisher={Wiley Online Library}
}


@article{gruning2006stack,
  title={Stack-like and queue-like dynamics in recurrent neural networks},
  author={Gr{\"u}ning, Andr{\'e}},
  journal={Connection Science},
  volume={18},
  number={1},
  pages={23--42},
  year={2006},
  publisher={Taylor \& Francis}
}


@incollection{chomsky1963algebraic,
  title={The algebraic theory of context-free languages},
  author={Chomsky, Noam and Sch{\"u}tzenberger, Marcel P},
  booktitle={Studies in Logic and the Foundations of Mathematics},
  volume={35},
  pages={118--161},
  year={1963},
  publisher={Elsevier}
}

@article{gers2001lstm,
  title={LSTM recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, E},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}
@inproceedings{kalinke1998computation,
  title={Computation in recurrent neural networks: From counters to iterated function systems},
  author={Kalinke, Yvonne and Lehmann, Helko},
  booktitle={Australian Joint Conference on Artificial Intelligence},
  pages={179--190},
  year={1998},
  organization={Springer}
}



@article{everaert2015structures,
  title={Structures, not strings: linguistics as part of the cognitive sciences},
  author={Everaert, Martin BH and Huybregts, Marinus AC and Chomsky, Noam and Berwick, Robert C and Bolhuis, Johan J},
  journal={Trends in cognitive sciences},
  volume={19},
  number={12},
  pages={729--743},
  year={2015},
  publisher={Elsevier}
}





@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}


@article{cartling2008implicit,
  title={On the implicit acquisition of a context-free grammar by a simple recurrent neural network},
  author={Cartling, Bo},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1527--1537},
  year={2008},
  publisher={Elsevier}
}

@incollection{montague1973proper,
  title={The proper treatment of quantification in ordinary {E}nglish},
  author={Montague, Richard},
  booktitle={Approaches to natural language},
  pages={221--242},
  year={1973},
  publisher={Springer}
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@article{lewis2005activation,
  title={An activation-based model of sentence processing as skilled memory retrieval},
  author={Lewis, Richard L and Vasishth, Shravan},
  journal={Cognitive {S}cience},
  volume={29},
  number={3},
  pages={375--419},
  year={2005},
  publisher={Wiley Online Library}
}



@inproceedings{lin2019open,
    title = "Open {S}esame: Getting inside {BERT}{'}s Linguistic Knowledge",
    author = "Lin, Yongjie  and
      Tan, Yi Chern  and
      Frank, Robert",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "241--253",
    abstract = "How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT{'}s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT{'}s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT{'}s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.",
}


@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{tenney2019bert,
  title={{BERT} rediscovers the classical {NLP} pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}


@incollection{miller-finitary-1963,
        title = {Finitary models of language users},
        booktitle = {Handbook of {Mathematical} {Psychology}},
        author = {Miller, George A and Chomsky, Noam},
        year = {1963},
        editor = {R. D. Luce and R. R. Bush and E. Galanter},
        pages={269--321}
}

@article{gibson1999memory,
  title={Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical},
  author={Gibson, Edward and Thomas, James},
  journal={Language and Cognitive Processes},
  volume={14},
  number={3},
  pages={225--248},
  year={1999},
  publisher={Taylor \& Francis}
}


@inproceedings{gopalan2016smooth,
  title={Smooth boolean functions are easy: Efficient algorithms for low-sensitivity functions},
  author={Gopalan, Parikshit and Nisan, Noam and Servedio, Rocco A and Talwar, Kunal and Wigderson, Avi},
  booktitle={Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
  pages={59--70},
  year={2016},
  organization={ACM}
}


@article{de2018deep,
  title={Deep neural networks are biased towards simple functions},
  author={De Palma, Giacomo and Kiani, Bobak Toussi and Lloyd, Seth},
  journal={arXiv preprint arXiv:1812.10156},
  year={2018}
}

@inproceedings{gilmer2015new,
  title={A new approach to the sensitivity conjecture},
  author={Gilmer, Justin and Kouck{\`y}, Michal and Saks, Michael E},
  booktitle={Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
  pages={247--254},
  year={2015},
  organization={ACM}
}


@article{rossman2018average,
  title={The average sensitivity of bounded-depth formulas},
  author={Rossman, Benjamin},
  journal={Computational Complexity},
  volume={27},
  number={2},
  pages={209--223},
  year={2018},
  publisher={Springer}
}



@article{boppana1997average,
  title={The average sensitivity of bounded-depth circuits},
  author={Boppana, Ravi B},
  journal={Information processing letters},
  volume={63},
  number={5},
  pages={257--261},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{
miller2018recurrent,
title={Stable Recurrent Models},
author={John Miller and Moritz Hardt},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}


@book{mcnaughton1971counter,
  title={Counter-Free Automata (MIT research monograph no. 65)},
  author={McNaughton, Robert and Papert, Seymour A},
  year={1971},
  publisher={The MIT Press}
}

@incollection{Parker2017TheCR,
  title={The cue-based retrieval theory of sentence comprehension: New findings and new challenges},
  author={Parker, Dan and Shvartsman, Michael and Van Dyke, Julie A},
  booktitle={Language processing and disorders},
  pages={121--144},
  editor={Escobar, L. and Torrens, V. and Parodi, T.},
  year={2017},
  publisher={Cambridge Scholars Publishing},
  address={Newcastle},
}


@incollection{parker2017cue,
  title={The cue-based retrieval theory of sentence comprehension: New findings and new challenges},
  author={Parker, Dan and Shvartsman, Michael and Van Dyke, Julie A},
  booktitle={Language processing and disorders},
  pages={121--144},
  editor={Escobar, L. and Torrens, V. and Parodi, T.},
  year={2017},
  publisher={Cambridge Scholars Publishing},
  address={Newcastle},
}





@proceedings{hsieh2019robustness,
title	= {On the Robustness of Self-Attentive Models},
editor	= {Yu-Lun Hsieh and Minhao Cheng and Da-Cheng Juan and Wei Wei and Wen-Lian Hsu and Cho-Jui Hsieh},
year	= {2019},
booktitle	= {Annual Meeting of the Association for Computational Linguistics (ACL)}
}

@inproceedings{levy2018long,
  title={Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum},
  author={Levy, Omer and Lee, Kenton and FitzGerald, Nicholas and Zettlemoyer, Luke},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={732--739},
  year={2018}
}

@article{ostmeyer2019machine,
  title={Machine learning on sequential data using a recurrent weighted average},
  author={Ostmeyer, Jared and Cowell, Lindsay},
  journal={Neurocomputing},
  volume={331},
  pages={281--288},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{lei2017deriving,
  title={Deriving neural architectures from sequence and graph kernels},
  author={Lei, Tao and Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2024--2033},
  year={2017},
  organization={JMLR. org}
}
@article{yin2017comparative,
  title={Comparative Study of CNN and RNN for Natural Language Processing},
  author={Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1702.01923},
  year={2017}
}
@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={933--941},
  year={2017},
  organization={JMLR. org}
}
@article{bradbury2016quasi,
  title={Quasi-recurrent neural networks},
  author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01576},
  year={2016}
}
@article{lei2015semi,
  title={Semi-supervised question retrieval with gated convolutions},
  author={Lei, Tao and Joshi, Hrishikesh and Barzilay, Regina and Jaakkola, Tommi and Tymoshenko, Katerina and Moschitti, Alessandro and Marquez, Lluis},
  journal={arXiv preprint arXiv:1512.05726},
  year={2015}
}
@inproceedings{sundermeyer2013comparison,
  title={Comparison of feedforward and recurrent neural network language models},
  author={Sundermeyer, Martin and Oparin, Ilya and Gauvain, J-L and Freiberg, Ben and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8430--8434},
  year={2013},
  organization={IEEE}
}
@inproceedings{balduzzi2016strongly,
  title={Strongly-Typed Recurrent Neural Networks},
  author={Balduzzi, David and Ghifary, Muhammad},
  booktitle={International Conference on Machine Learning},
  pages={1292--1300},
  year={2016}
}


@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={arXiv preprint arXiv:1905.10650},
  year={2019}
}

@book{chomsky1957syntactic,
  title={Syntactic structures.},
  author={Chomsky, Noam},
  year={1957},
  publisher={Mouton},
  address={The Hague}
}


@article{keener1992limit,
  title={Limit theorems for random walks conditioned to stay positive},
  author={Keener, Robert W and others},
  journal={The Annals of Probability},
  volume={20},
  number={2},
  pages={801--824},
  year={1992},
  publisher={Institute of Mathematical Statistics}
}


@article{chi1999statistical,
  title={Statistical properties of probabilistic context-free grammars},
  author={Chi, Zhiyi},
  journal={Computational Linguistics},
  volume={25},
  number={1},
  pages={131--160},
  year={1999},
  publisher={MIT Press}
}

@incollection{shieber1985evidence,
  title={Evidence against the context-freeness of natural language},
  author={Shieber, Stuart M},
  booktitle={Philosophy, Language, and Artificial Intelligence},
  pages={79--89},
  year={1985},
  publisher={Springer}
}


@inproceedings{kuncoro2018lstms,
  title={Lstms can learn syntax-sensitive dependencies well, but modeling structure makes them better},
  author={Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1426--1436},
  year={2018}
}


@article{seo2017neural,
  title={Neural speed reading via skim-{RNN}},
  author={Seo, Minjoon and Min, Sewon and Farhadi, Ali and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1711.02085},
  year={2017}
}


@article{hansen2019neural,
  title={Neural Speed Reading with Structural-Jump-{LSTM}},
  author={Hansen, Christian and Hansen, Casper and Alstrup, Stephen and Simonsen, Jakob Grue and Lioma, Christina},
  journal={arXiv preprint arXiv:1904.00761},
  year={2019}
}


@article{yu2017learning,
  title={Learning to skim text},
  author={Yu, Adams Wei and Lee, Hongrae and Le, Quoc V},
  journal={arXiv preprint arXiv:1704.06877},
  year={2017}
}

@article{lei2016rationalizing,
  title={Rationalizing neural predictions},
  author={Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:1606.04155},
  year={2016}
}


@InProceedings{hahn_modeling_2016,
  author = {Hahn, Michael and Keller, Frank},
  title = {Modeling {Human} {Reading} with {Neural} {Attention}},
  booktitle = {Proceedings of {EMNLP}},
  year = {2016}
}


@article{miao2016language,
  title={Language as a latent variable: Discrete generative models for sentence compression},
  author={Miao, Yishu and Blunsom, Phil},
  journal={arXiv preprint arXiv:1609.07317},
  year={2016}
}

@Article{hahn2019estimating,
  author = {Hahn, Michael and Futrell, Richard},
  title = {Estimating Predictive Rate-Distortion Curves via Neural Variational Inference },
  journal = {Entropy},
  volume = {21},
  number = {7},
  pages = {640},
  year = {2019},
}

@article{lee2019learning,
  title={Learning Autocomplete Systems as a Communication Game},
  author={Lee, Mina and Hashimoto, Tatsunori B and Liang, Percy},
  journal={arXiv preprint arXiv:1911.06964},
  year={2019}
}


@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}
@inproceedings{futrell2017noisy,
  title={Noisy-context surprisal as a human sentence processing cost model},
  author={Futrell, Richard and Levy, Roger},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  pages={688--698},
  year={2017}
}


@article{vasishth2010short,
	title="Short-term forgetting in sentence comprehension: Crosslinguistic evidence from verb-final structures",
	author="Shravan {Vasishth} and Katja {Suckow} and Richard L. {Lewis} and Sabine {Kern}",
	journal="Language and Cognitive Processes",
	volume="25",
	number="4",
	pages="533--567",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2166544825",
	year="2010"
}


@article{boyce2020maze,
	title="Maze Made Easy: Better and easier measurement of incremental processing difficulty",
	author="Veronica {Boyce} and Richard {Futrell} and Roger P. {Levy}",
	journal="Journal of Memory and Language",
	volume="111",
	pages="104082",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2995543961",
	year="2020"
}

@article{lau2017grammaticality,
	title="Grammaticality, Acceptability, and Probability: A Probabilistic View of Linguistic Knowledge.",
	author="Jey Han {Lau} and Alexander {Clark} and Shalom {Lappin}",
	journal="Cognitive Science",
	volume="41",
	number="5",
	pages="1202--1241",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2531882892",
	year="2017"
}



@incollection{frazier1985syntactic,
  author =        {Frazier, Lyn},
  booktitle =       {Natural language parsing: {P}sychological,
                   computational, and theoretical perspectives},
  pages =         {129--189},
  publisher =     {Cambridge University Press New York},
  editor={D. R. Dowty and L. Karttunen and A. M. Zwicky},
  title =         {Syntactic complexity},
  year =          {1985},
}


@article{christiansen1999toward,
  title={Toward a connectionist model of recursion in human linguistic performance},
  author={Christiansen, Morten H and Chater, Nick},
  journal={Cognitive Science},
  volume={23},
  number={2},
  pages={157--205},
  year={1999},
  publisher={Wiley Online Library}
}



@article{macdonald2002reassessing,
  title={Reassessing working memory: Comment on Just and Carpenter (1992) and Waters and Caplan (1996).},
  author={MacDonald, Maryellen C and Christiansen, Morten H},
  year={2002},
  publisher={American Psychological Association}
}

@article{christiansen1999toward,
  title={Toward a connectionist model of recursion in human linguistic performance},
  author={Christiansen, Morten H and Chater, Nick},
  journal={Cognitive Science},
  volume={23},
  number={2},
  pages={157--205},
  year={1999},
  publisher={Wiley Online Library}
}


@article{christiansen2009usage,
  title={A usage-based approach to recursion in sentence processing},
  author={Christiansen, Morten H and MacDonald, Maryellen C},
  journal={Language Learning},
  volume={59},
  pages={126--161},
  year={2009},
  publisher={Wiley Online Library}
}


@inproceedings{engelmann2009processing,
  title={Processing grammatical and ungrammatical center embeddings in {E}nglish and {G}erman: A computational model},
  author={Engelmann, Felix and Vasishth, Shravan},
  booktitle={Proceedings of the Ninth International Conference on Cognitive Modeling, Manchester, UK},
  pages={240--45},
  year={2009}
}


@article{frank2019judgements,
  title={Judgements about double-embedded relative clauses differ between languages},
  author={Frank, Stefan L. and Ernst, Patty},
  journal={Psychological research},
  volume={83},
  number={7},
  pages={1581--1593},
  year={2019},
  publisher={Springer}
}


@article{frank2016cross,
  title={Cross-linguistic differences in processing double-embedded relative clauses: Working-memory constraints or language statistics?},
  author={Frank, Stefan L. and Trompenaars, Thijs and Vasishth, Shravan},
  journal={Cognitive Science},
  volume={40},
  number={3},
  pages={554--578},
  year={2016},
  publisher={Wiley Online Library}
}



@inproceedings{futrell-noisy-context-2017,
	title = {Noisy-context surprisal as a human sentence processing cost model},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Futrell, Richard and Levy, Roger},
	year = {2017},
	pages = {688--698},
	file = {Noisy-context surprisal as a human sentence processing cost model - E17-1065:/home/user/Zotero/storage/MI7EK2ZN/E17-1065.pdf:application/pdf}
}


@book{huddleston2002the,
	title="The Cambridge Grammar of the English Language",
	author="Rodney {Huddleston} and Geoffrey K. {Pullum}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2013833248",
	year="2002"
}

@article{gibson1999memory,
	title="Memory Limitations and Structural Forgetting: The Perception of Complex Ungrammatical Sentences as Grammatical",
	author="Edward {Gibson} and James {Thomas}",
	journal="Language and Cognitive Processes",
	volume="14",
	number="3",
	pages="225--248",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2160732323",
	year="1999"
}


@article{marcus-building-1993,
	title = {Building a large annotated corpus of {English}: {The} {Penn} {Treebank}},
	volume = {19},
	number = {2},
	journal = {Computational {L}inguistics},
	author = {Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
	year = {1993},
	pages = {313--330}
}

@inproceedings{silveira2014a,
	title="A Gold Standard Dependency Corpus for {E}nglish",
	author="Natalia {Silveira} and Timothy {Dozat} and Marie-Catherine de {Marneffe} and Samuel {Bowman} and Miriam {Connor} and John {Bauer} and Chris {Manning}",
	booktitle="Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)",
	pages="2897--2904",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2250263931",
	year="2014"
}

@inproceedings{taule2008ancora,
	title="{A}n{C}ora: Multilevel Annotated Corpora for {C}atalan and {S}panish",
	author="Mariona {Taulé} and Maria Antònia {Martí} and Marta {Recasens}",
	booktitle="Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08)",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/38462120",
	year="2008"
}

@inproceedings{volker2019hdt,
	title="{HDT-UD}: A very large Universal Dependencies Treebank for {G}erman",
	author="Emanuel Borges {Völker} and Maximilian {Wendt} and Felix {Hennig} and Arne {Köhn}",
	booktitle="Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)",
	pages="46--57",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2996400930",
	year="2019"
}


@article{gibson2013rational,
	title="Rational integration of noisy evidence and prior semantic expectations in sentence interpretation",
	author="Edward A. {Gibson} and Leon {Bergen} and Steven T. {Piantadosi}",
	journal="Proceedings of the National Academy of Sciences of the United States of America",
	volume="110",
	number="20",
	pages="8051--8056",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2132684680",
	year="2013"
}

@article{ferreira2002good,
	title="Good-Enough Representations in Language Comprehension",
	author="Fernanda {Ferreira} and Karl G.D. {Bailey} and Vittoria {Ferraro}",
	journal="Current Directions in Psychological Science",
	volume="11",
	number="1",
	pages="11--15",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2130822992",
	year="2002"
}

@article{ferreira2007the,
	title="The ‘Good Enough’ Approach to Language Comprehension",
	author="Fernanda {Ferreira} and Nikole D. {Patson}",
	journal="Language and Linguistics Compass",
	volume="1",
	pages="71--83",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1967852455",
	year="2007"
}


@article{hoffman2013stochastic,
	title="Stochastic variational inference",
	author="Matthew D. {Hoffman} and David M. {Blei} and Chong {Wang} and John {Paisley}",
	journal="Journal of Machine Learning Research",
	volume="14",
	number="1",
	pages="1303--1347",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2166851633",
	year="2013"
}

@article{blei2017variational,
	title="Variational Inference: A Review for Statisticians",
	author="David M. {Blei} and Alp {Kucukelbir} and Jon D. {McAuliffe}",
	journal="Journal of the American Statistical Association",
	volume="112",
	number="518",
	pages="859--877",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2225156818",
	year="2017"
}

@inproceedings{miao2016neural,
	title="Neural variational inference for text processing",
	author="Yishu {Miao} and Lei {Yu} and Phil {Blunsom}",
	booktitle="ICML'16 Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48",
	pages="1727--1736",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963773425",
	year="2016"
}


@inproceedings{kingma2014auto,
	title="Auto-Encoding Variational {B}ayes",
	author="Diederik P {Kingma} and Max {Welling}",
	booktitle="ICLR 2014 : International Conference on Learning Representations (ICLR) 2014",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1959608418",
	year="2014"
}

@inproceedings{rezende2014stochastic,
	title="Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
	author="Danilo Jimenez {Rezende} and Shakir {Mohamed} and Daan {Wierstra}",
	booktitle="Proceedings of The 31st International Conference on Machine Learning",
	pages="1278--1286",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2962897886",
	year="2014"
}

@inproceedings{mnih2014neural,
	title="Neural Variational Inference and Learning in Belief Networks",
	author="Andriy {Mnih} and Karol {Gregor}",
	booktitle="Proceedings of The 31st International Conference on Machine Learning",
	pages="1791--1799",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2122262818",
	year="2014"
}


@article{grodner-consequences-2005,
	title = {Consequences of the serial nature of linguistic input for sentenial complexity},
	volume = {29},
	number = {2},
	journal = {Cognitive science},
	author = {Grodner, Daniel and Gibson, Edward},
	year = {2005},
	pages = {261--290}
}

@article{macdonald2002reassessing,
  title={Reassessing working memory: Comment on Just and Carpenter (1992) and Waters and Caplan (1996).},
  author={MacDonald, Maryellen C and Christiansen, Morten H},
  year={2002},
  publisher={American Psychological Association}
}


@article{christiansen2009usage,
  title={A usage-based approach to recursion in sentence processing},
  author={Christiansen, Morten H and MacDonald, Maryellen C},
  journal={Language Learning},
  volume={59},
  pages={126--161},
  year={2009},
  publisher={Wiley Online Library}
}


@article{wells2009experience,
  title={Experience and sentence processing: Statistical learning and relative clause comprehension},
  author={Wells, Justine B and Christiansen, Morten H and Race, David S and Acheson, Daniel J and MacDonald, Maryellen C},
  journal={Cognitive psychology},
  volume={58},
  number={2},
  pages={250--271},
  year={2009},
  publisher={Elsevier}
}

@article{yngve1960model,
  title={A model and an hypothesis for language structure},
  author={Yngve, Victor H},
  journal={Proceedings of the American philosophical society},
  volume={104},
  number={5},
  pages={444--466},
  year={1960},
  publisher={JSTOR}
}

@article{mcelree2000sentence,
  title={Sentence comprehension is mediated by content-addressable memory structures},
  author={McElree, Brian},
  journal={Journal of {P}sycholinguistic {R}esearch},
  volume={29},
  number={2},
  pages={111--123},
  year={2000},
  publisher={Springer}
}

@article{mcelree-memory-2003,
	title = {Memory structures that subserve sentence comprehension},
	volume = {48},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X02005156},
	doi = {10.1016/S0749-596X(02)00515-6},
	abstract = {Measures of the speed and accuracy of processing sentences with nonadjacent dependencies derived from the response-signal speed-accuracy tradeoﬀ procedure were used to examine the nature of the memory system that underlies sentence comprehension. Three experiments with diﬀerent sentence structures demonstrated that the accuracy of processing a dependency decreased as more material was interpolated between nonadjacent constituents. However, processing speed was unaﬀected by the amount of interpolated material, indicating that memory representations for previously processed constituents can be accessed directly. These results suggest that a content-addressable memory system mediates sentence comprehension, in which syntactic and semantic information provide direct access to memory representations without the need to search through extraneous representations. Notably, content-addressability appears to underlie the interpretation of sentence structures that also require the recovery of order information, a type of operation that has been shown to necessitate a slow search process in list-learning experiments (McElree, 2001; McElree \& Dosher, 1993).},
	language = {en},
	number = {1},
	urldate = {2018-04-17},
	journal = {Journal of Memory and Language},
	author = {McElree, Brian and Foraker, Stephani and Dyer, Lisbeth},
	month = jan,
	year = {2003},
	pages = {67--91},
	file = {McElree et al. - 2003 - Memory structures that subserve sentence comprehen.pdf:/home/user/Zotero/storage/2QK6PUUX/McElree et al. - 2003 - Memory structures that subserve sentence comprehen.pdf:application/pdf}
}

@book{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={A. Radford and Jeffrey Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  publisher={OpenAI}
}

@article{griffiths2015rational,
	title="Rational Use of Cognitive Resources: Levels of Analysis Between the Computational and the Algorithmic",
	author="Thomas L. {Griffiths} and Falk {Lieder} and Noah D. {Goodman}",
	journal="Topics in Cognitive Science",
	volume="7",
	number="2",
	pages="217--229",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2141467654",
	year="2015"
}



@book{schlesinger1968sentence,
	title="Sentence structure and the reading process",
	author="Izchak M. {Schlesinger}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2041450877",
	year="1968",
	publisher={Mouton},
	address={The Hague}
}


@article{fodor1967some,
	title="Some syntactic determinants of sentential complexity",
	author="J. A. {Fodor} and M. {Garrett}",
	journal="Attention Perception \& Psychophysics",
	volume="2",
	number="7",
	pages="289--296",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1996776436",
	year="1967"
}

@article{fodor1968some,
	title="Some syntactic determinants of sentential complexity, II : Verb structure",
	author="J. A. {Fodor} and M. {Garrett} and T. G. {Bever}",
	journal="Attention Perception \& Psychophysics",
	volume="3",
	number="6",
	pages="453--461",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2007512328",
	year="1968"
}


@article{frank2021the,
	title="The missing-VP effect in readers of English as a second language.",
	author="Stefan L. {Frank} and Patty {Ernst} and Robin L. {Thompson} and Rein {Cozijn}",
	journal="Memory \& Cognition",
	pages="1--16",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2973071686",
	year="2021"
}

@article{Haussler2015AnIA,
	title="An interference account of the missing-{VP} effect.",
	author="Jana {H{\"a}ussler} and Markus {Bader}",
	journal="Frontiers in Psychology",
	volume="6",
	pages="766--766",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1570883930",
	year="2015"
}



@article{frank2019judgements,
	title="Judgements about double-embedded relative clauses differ between languages.",
	author="Stefan L. {Frank} and Patty {Ernst}",
	journal="Psychological Research-psychologische Forschung",
	volume="83",
	number="7",
	pages="1581--1593",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2801043313",
	year="2019"
}


@article{witzel2012ComparisonsOO,
  title={Comparisons of Online Reading Paradigms: eye tracking, moving-window, and maze},
  author={Naoko Witzel and Jeffrey Witzel and K. Forster},
  journal={Journal of Psycholinguistic Research},
  year={2012},
  volume={41},
  pages={105-128}
}


@article{forster2009TheMT,
  title={The maze task: Measuring forced incremental sentence processing time},
  author={K. Forster and Christine Guerrera and Lisa Elliot},
  journal={Behavior Research Methods},
  year={2009},
  volume={41},
  pages={163-171}
}


@article{Lakretz2021MechanismsFH,
  title={Mechanisms for handling nested dependencies in neural-network language models and humans.},
  author={Yair Lakretz and Dieuwke Hupkes and A. Vergallito and M. Marelli and Marco Baroni and S. Dehaene},
  journal={Cognition},
  year={2021},
  pages={
          104699
        }
}

@book{Boyd2006ConvexO,
  title={Convex Optimization},
  author={Stephen P. Boyd and L. Vandenberghe},
  year={2006}
}


@article{Huang2021WhenMN,
  title={When missing NPs make double center-embedding sentences acceptable},
  author={Nick Huang and C. Phillips},
  journal={Glossa},
  year={2021},
  volume={6},
  pages={37}
}

@article{Amir2015PastfutureIB,
  title={Past-future Information Bottleneck for linear feedback systems},
  author={Nadav Amir and Stas Tiomkin and Naftali Tishby},
  journal={2015 54th IEEE Conference on Decision and Control (CDC)},
  year={2015},
  pages={5737-5742}
}

@article{Marzen2016PredictiveRF,
  title={Predictive Rate-Distortion for Infinite-Order Markov Processes},
  author={S. Marzen and J. Crutchfield},
  journal={Journal of Statistical Physics},
  year={2016},
  volume={163},
  pages={1312-1338}
}
@article{Still2010OptimalCI,
  title={Optimal causal inference: estimating stored information and approximating causal architecture.},
  author={Susanne Still and J. Crutchfield and Christopher J. Ellison},
  journal={Chaos},
  year={2010},
  volume={20 3},
  pages={
          037111
        }
}
@article{Wang2019PastfutureIB,
  title={Past-future information bottleneck for sampling molecular reaction coordinate simultaneously with thermodynamics and kinetics},
  author={Yihang Wang and Jo{\~a}o Marcelo Lamim Ribeiro and P. Tiwary},
  journal={Nature Communications},
  year={2019},
  volume={10}
}
@article{Creutzig2009PastfutureIB,
  title={Past-future information bottleneck in dynamical systems.},
  author={F. Creutzig and A. Globerson and Naftali Tishby},
  journal={Physical review. E, Statistical, nonlinear, and soft matter physics},
  year={2009},
  volume={79 4 Pt 1},
  pages={
          041925
        }
}


@article{Levy2009EyeME,
  title={Eye movement evidence that readers maintain and act on uncertainty about past linguistic input},
  author={R. Levy and K. Bicknell and T. Slattery and K. Rayner},
  journal={Proceedings of the National Academy of Sciences},
  year={2009},
  volume={106},
  pages={21086--21090}
}

@article{Gibson2013RationalIO,
  title={Rational integration of noisy evidence and prior semantic expectations in sentence interpretation},
  author={E. Gibson and Leon Bergen and S. Piantadosi},
  journal={Proceedings of the National Academy of Sciences},
  year={2013},
  volume={110},
  pages={8051 - 8056}
}

@article{Levy2008ExpectationbasedSC,
  title={Expectation-based syntactic comprehension},
  author={R. Levy},
  journal={Cognition},
  year={2008},
  volume={106},
  pages={1126-1177}
}

@inproceedings{Hale2001APE,
  author    = {John Hale},
  title     = {A Probabilistic Earley Parser as a Psycholinguistic Model},
  booktitle = { The Second Meeting of the North American
               Chapter of the Association for Computational Linguistics, {NAACL}
               2001},
  publisher = {The Association for Computational Linguistics},
  year      = {2001},
  url       = {https://aclanthology.org/N01-1021/},
  timestamp = {Fri, 06 Aug 2021 00:41:29 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/Hale01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{Boston2011ParallelPA,
  title={Parallel processing and sentence comprehension difficulty},
  author={Marisa Ferrara Boston and John Hale and S. Vasishth and R. Kliegl},
  journal={Language and Cognitive Processes},
  year={2011},
  volume={26},
  pages={301 - 349}
}

@article{Demberg2008DataFE,
  title={Data from eye-tracking corpora as evidence for theories of syntactic processing complexity},
  author={V. Demberg and Frank Keller},
  journal={Cognition},
  year={2008},
  volume={109},
  pages={193-210}
}

@article{Demberg2013IncrementalPP,
  title={Incremental, Predictive Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar},
  author={V. Demberg and Frank Keller and Alexander Koller},
  journal={Computational Linguistics},
  year={2013},
  volume={39},
  pages={1025-1066}
}


@article{Gibson1998LinguisticCL,
  title={Linguistic complexity: locality of syntactic dependencies},
  author={E. Gibson},
  journal={Cognition},
  year={1998},
  volume={68},
  pages={1-76}
}

@article{Just1992ACT,
  title={A capacity theory of comprehension: individual differences in working memory.},
  author={M. Just and P. Carpenter},
  journal={Psychological review},
  year={1992},
  volume={99 1},
  pages={
          122-49
        }
}




@article{Futrell2020LossyContextSA,
  title={Lossy-Context Surprisal: An Information-Theoretic Model of Memory Effects in Sentence Processing},
  author={Richard Futrell and E. Gibson and R. Levy},
  journal={Cognitive Science},
  year={2020},
  volume={44}
}

@article{Lieder2019ResourcerationalAU,
  title={Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources},
  author={Falk Lieder and T. Griffiths},
  journal={Behavioral and Brain Sciences},
  year={2019},
  volume={43}
}

@article{yngve1960,
  title={A model and an hypothesis for language structure},
  author={Yngve, Victor H},
  journal={Proceedings of the American philosophical society},
  volume={104},
  number={5},
  pages={444--466},
  year={1960},
  publisher={JSTOR}
}

@inproceedings{Dozat2017DeepBA,
  author    = {Timothy Dozat and
               Christopher D. Manning},
  title     = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Hk95PK9le},
  timestamp = {Thu, 25 Jul 2019 14:25:56 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DozatM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Sutskever2014SequenceTS,
  title={Sequence to Sequence Learning with Neural Networks},
  author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  booktitle={NIPS},
  year={2014}
}
@inproceedings{Luong2015EffectiveAT,
  author    = {Thang Luong and
               Hieu Pham and
               Christopher D. Manning},
  editor    = {Llu{\'{\i}}s M{\`{a}}rquez and
               Chris Callison{-}Burch and
               Jian Su and
               Daniele Pighin and
               Yuval Marton},
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2015, Lisbon, Portugal, September 17-21,
               2015},
  pages     = {1412--1421},
  publisher = {The Association for Computational Linguistics},
  year      = {2015},
  url       = {https://doi.org/10.18653/v1/d15-1166},
  doi       = {10.18653/v1/d15-1166},
  timestamp = {Fri, 06 Aug 2021 00:40:22 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/LuongPM15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Anderson1991ReflectionsOT,
  title={Reflections of the Environment in Memory},
  author={John R. Anderson and L. Schooler},
  journal={Psychological Science},
  year={1991},
  volume={2},
  pages={396 - 408}
}
@article{Anderson1989HumanMA,
  title={Human memory: An adaptive perspective.},
  author={John R. Anderson and R. Milson},
  journal={Psychological Review},
  year={1989},
  volume={96},
  pages={703-719}
}
@article{Bates2020EfficientDC,
  title={Efficient data compression in perception and perceptual memory.},
  author={Christopher Bates and R. Jacobs},
  journal={Psychological {R}eview},
  year={2020},
  volume={127},
  issue={5},
  pages={891--917}
}
@article{Sims2016RatedistortionTA,
  title={Rate–distortion theory and human perception},
  author={C. Sims},
  journal={Cognition},
  year={2016},
  volume={152},
  pages={181-198}
}
@article{Sims2012AnIO,
  title={An ideal observer analysis of visual working memory.},
  author={C. Sims and R. Jacobs and D. Knill},
  journal={Psychological {R}eview},
  year={2012},
  volume={119 4},
  pages={
          807-30
        }
}
@article{Yoo2018StrategicAO,
  title={Strategic allocation of working memory resource},
  author={Aspen H. Yoo and Zuzanna Klyszejko and C. Curtis and W. Ma},
  journal={Scientific Reports},
  year={2018},
  volume={8}
}

@article{Forster2009TheMT,
  title={The maze task: Measuring forced incremental sentence processing time},
  author={K. Forster and Christine Guerrera and Lisa Elliot},
  journal={Behavior Research Methods},
  year={2009},
  volume={41},
  pages={163-171}
}

@article{Witzel2012ComparisonsOO,
  title={Comparisons of Online Reading Paradigms: Eye Tracking, Moving-Window, and Maze},
  author={Naoko Witzel and Jeffrey Witzel and K. Forster},
  journal={Journal of Psycholinguistic Research},
  year={2012},
  volume={41},
  pages={105-128}
}


@article{Glanzer1976AnalysisOT,
  title={Analysis of the word-frequency effect in recognition memory},
  author={M. Glanzer and N. L. Bowles},
  journal={Journal of Experimental Psychology: Human Learning \& Memory},
  year={1976},
  volume={2},
  pages={21-31}
}

@article{Gorman1961RecognitionMF,
  title={Recognition memory for nouns as a function of abstractness and frequency.},
  author={A. Gorman},
  journal={Journal of experimental psychology},
  year={1961},
  volume={61},
  pages={
          23-9
        }
}


@article{Frank2018JudgementsAD,
  title={Judgements about double-embedded relative clauses differ between languages},
  author={S. Frank and Patty Ernst},
  journal={Psychological Research},
  year={2018},
  volume={83},
  pages={1581 - 1593}
}
@article{Christiansen2009AUA,
  title={A Usage-Based Approach to Recursion in Sentence Processing},
  author={Morten H. Christiansen and M. MacDonald},
  journal={Language Learning},
  year={2009},
  volume={59},
  pages={126-161}
}
@article{Gimenes2009WhenAM,
  title={When a missing verb makes a {F}rench sentence more acceptable},
  author={M. Gimenes and F. Rigalleau and D. Gaonac'h},
  journal={Language and Cognitive Processes},
  year={2009},
  volume={24},
  pages={440 - 449}
}

@article{Blaubergs1974ShorttermML,
  title={Short-term memory limitations on decoding self-embedded sentences},
  author={M. S. Blaubergs and M. Braine},
  journal={Journal of Experimental Psychology},
  year={1974},
  volume={102},
  pages={745-748}
}

@article{Blaubergs1976EncodingSS,
  title={Encoding Self-Embedded Sentences},
  author={M. S. Blaubergs},
  journal={Language and Speech},
  year={1976},
  volume={19},
  pages={1 - 8}
}

@article{Blumenthal1966ObservationsWS,
  title={Observations with self-embedded sentences},
  author={A. L. Blumenthal},
  journal={Psychonomic Science},
  year={1966},
  volume={6},
  pages={453-454}
}

@article{Hakes1970SentenceCA,
  title={Sentence comprehension and relative pronouns},
  author={D. Hakes and H. Cairns},
  journal={Perception \& Psychophysics},
  year={1970},
  volume={8},
  pages={5-8}
}
@article{Hakes1976UnderstandingSW,
  title={Understanding sentences with relative clauses},
  author={D. Hakes and J. S. Evans and L. L. Brannon},
  journal={Memory \& Cognition},
  year={1976},
  volume={4},
  pages={283-290}
}

@article{Marks1968ScalingOG,
  title={Scaling of grammaticalness of self-embedded English sentences},
  author={L. Marks},
  journal={Journal of Verbal Learning and Verbal Behavior},
  year={1968},
  volume={7},
  pages={965-967}
}

@article{Miller1964FreeRO,
  title={Free Recall of Self-Embedded English Sentences},
  author={G. A. Miller and S. Isard},
  journal={Inf. Control.},
  year={1964},
  volume={7},
  pages={292-303}
}

@article{Wang1970TheRO,
  title={The role of syntactic complexity as a determiner of comprehensibility},
  author={Marilyn D. Wang},
  journal={Journal of Verbal Learning and Verbal Behavior},
  year={1970},
  volume={9},
  pages={398-404}
}

@article{Kimball1973SevenPO,
  title={Seven principles of surface structure parsing in natural language},
  author={J. Kimball},
  journal={Cognition},
  year={1973},
  volume={2},
  pages={15-47}
}

@article{Lewis1996InterferenceIS,
  title={Interference in short-term memory: The magical number two (or three) in sentence processing},
  author={R. Lewis},
  journal={Journal of Psycholinguistic Research},
  year={1996},
  volume={25},
  pages={93-115}
}

@article{Staub2018RelativeCA,
  title={Relative clause avoidance: Evidence for a structural parsing principle},
  author={A. Staub and F. Foppolo and C. Donati and C. Cecchetto},
  journal={Journal of Memory and Language},
  year={2018},
  volume={98},
  pages={26-44}
}

@inproceedings{Qi2020StanzaAP,
  title={Stanza: A Python Natural Language Processing Toolkit for Many Human Languages},
  author={Peng Qi and Yuhao Zhang and Yuhui Zhang and Jason Bolton and Christopher D. Manning},
  booktitle={ACL},
  year={2020}
}

@article{Bates2014FittingLM,
  title={Fitting Linear Mixed-Effects Models Using lme4},
  author={D. Bates and M. Machler and B. Bolker and Steven C. Walker},
  journal={Journal of Statistical Software},
  year={2014},
  volume={67},
  pages={1-48}
}

@inproceedings{Akaike1973InformationTA,
  title={Information Theory and an Extension of the Maximum Likelihood Principle},
  author={H. Akaike},
  year={1973},
  editor = {Petrov, B.N. and Csaki, F.},
  booktitle = {International Symposium on Information Theory}
}

@Article{gronau2020bridgesampling,
  title = {{bridgesampling}: An {R} Package for Estimating
    Normalizing Constants},
  author = {Quentin F. Gronau and Henrik Singmann and Eric-Jan
    Wagenmakers},
  journal = {Journal of Statistical Software},
  year = {2020},
  volume = {92},
  number = {10},
  pages = {1--29},
  doi = {10.18637/jss.v092.i10},
}

@inproceedings{Ferraresi2008IntroducingAE,
  title={Introducing and evaluating {ukWaC} , a very large web-derived corpus of English},
  author={A. Ferraresi and E. Zanchetta and Marco Baroni and Silvia Bernardini},
  year={2008}
}
@inproceedings{Davies2012TheCO,
  title={The {C}orpus of {C}ontemporary {A}merican {E}nglish ({COCA})},
  author={Mark Davies},
  year={2012}
}
@article{Husain2014StrongEC,
  title={Strong Expectations Cancel Locality Effects: Evidence from Hindi},
  author={Samar Husain and S. Vasishth and N. Srinivasan},
  journal={PLoS ONE},
  year={2014},
  volume={9}
}
@inproceedings{Sharma2021ClauseFV,
  title={Clause Final Verb Prediction in Hindi: Evidence for Noisy Channel Model of Communication},
  author={Kartik Sharma and Niyati Bafna and Samar Husain},
  booktitle={CMCL},
  year={2021}
}
@article{Safavi2016DependencyRD,
  title={Dependency Resolution Difficulty Increases with Distance in Persian Separable Complex Predicates: Evidence for Expectation and Memory-Based Accounts},
  author={M. S. Safavi and Samar Husain and S. Vasishth},
  journal={Frontiers in Psychology},
  year={2016},
  volume={7}
}

@inproceedings{Paszke2019PyTorchAI,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Adam Paszke and S. Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and N. Gimelshein and L. Antiga and Alban Desmaison and Andreas K{\"o}pf and E. Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  booktitle={NeurIPS},
  year={2019}
}

@article{Williams1992SimpleSG,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Ronald J. Williams},
  journal={Machine Learning},
  year={1992},
  volume={8},
  pages={229-256}
}

@article{Culbertson2020FROMTW,
  title={FROM THE WORLD TO WORD ORDER: DERIVING BIASES IN NOUN PHRASE ORDER FROM STATISTICAL PROPERTIES OF THE WORLD},
  author={Jennifer Culbertson and M. Schouwstra and S. Kirby},
  journal={Language},
  year={2020}
}
@article{Bartek2011InSO,
  title={In search of on-line locality effects in sentence comprehension.},
  author={Brian Bartek and Richard L. Lewis and S. Vasishth and Mason Smith},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={2011},
  volume={37 5},
  pages={
          1178-98
        }
}

@article{Staub2017TheMV,
  title={The Matrix Verb as a Source of Comprehension Difficulty in Object Relative Sentences.},
  author={A. Staub and Brian Dillon and C. Clifton},
  journal={Cognitive science},
  year={2017},
  volume={41 Suppl 6},
  pages={
          1353-1376
        }
}

@article{Staub2010EyeMA,
  title={Eye movements and processing difficulty in object relative clauses},
  author={A. Staub},
  journal={Cognition},
  year={2010},
  volume={116},
  pages={71-86}
}

@article{Forster2009TheMT,
  title={The maze task: Measuring forced incremental sentence processing time},
  author={K. Forster and Christine Guerrera and Lisa Elliot},
  journal={Behavior Research Methods},
  year={2009},
  volume={41},
  pages={163-171}
}

@inproceedings{vani-etal:2021-using-interpolated-maze,
  year = {2021},
  title = {Using the Interpolated Maze task to Assess Incremental Processing in English Relative Clauses},
  booktitle = {Proceedings of the 43rd Annual Meeting of the Cognitive Science Society},
  author = {Vani, Pranali and Wilcox, Ethan Gotlieb and P. Levy, Roger}
}

@inproceedings{Yadav2021IsSI,
  title={Is similarity-based interference caused by lossy compression or cue-based retrieval? A computational evaluation},
  author={Himanshu Yadav and Garrett Smith and S. Vasishth},
  year={2021}
}

@inproceedings{Gauthier2020SyntaxGymAO,
  title={SyntaxGym: An Online Platform for Targeted Evaluation of Language Models},
  author={Jon Gauthier and Jennifer Hu and Ethan Gotlieb Wilcox and Peng Qian and R. Levy},
  booktitle={ACL},
  year={2020}
}

@inproceedings{Arehalli2020NeuralLM,
  title={Neural Language Models Capture Some, But Not All Agreement Attraction Effects},
  author={Suhas Arehalli and Tal Linzen},
  booktitle={CogSci},
  year={2020}
}
@article{Tabor2004EffectsOM,
  title={Effects of merely local syntactic coherence on sentence processing},
  author={W. Tabor and Bruno Galantucci and Daniel C. Richardson},
  journal={Journal of Memory and Language},
  year={2004},
  volume={50},
  pages={355-370}
}
@article{Tabor2004EvidenceFS,
  title={Evidence for self-organized sentence processing: digging-in effects.},
  author={W. Tabor and Sean Hutchins},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={2004},
  volume={30 2},
  pages={
          431-50
        }
}

@incollection{Gibson2000TheDL,
  title={The dependency locality theory: A distance-based theory of linguistic complexity},
  author={Edward Gibson},
  year={2000},
  editor={A. Marantz, Y. Miyashita and W. O'Neil},
  booktitle = {Image, Language, Brain},
  pages={95--126}
}

@article{Barr2013RandomES,
  title={Random effects structure for confirmatory hypothesis testing: Keep it maximal.},
  author={D. Barr and R. Levy and Christoph Scheepers and Harry J. Tily},
  journal={Journal of memory and language},
  year={2013},
  volume={68 3}
}

@article{Staub2006SyntacticPI,
  title={Syntactic prediction in language comprehension: evidence from either...or.},
  author={A. Staub and C. Clifton},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={2006},
  volume={32 2},
  pages={
          425-36
        }
}

@article{Dotlacil2020Parsing,
author = {Dotla{\v c}il, Jakub},
title = {Parsing as a Cue-Based Retrieval Model},
journal = {Cognitive Science},
volume = {45},
number = {8},
pages = {e13020},
keywords = {Computational psycholinguistics, Cue-based retrieval, Memory retrieval, ACT-R, Modeling reading data, Processing},
doi = {https://doi.org/10.1111/cogs.13020},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13020},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13020},
abstract = {Abstract This paper develops a novel psycholinguistic parser and tests it against experimental and corpus reading data. The parser builds on the recent research into memory structures, which argues that memory retrieval is content-addressable and cue-based. It is shown that the theory of cue-based memory systems can be combined with transition-based parsing to produce a parser that, when combined with the cognitive architecture ACT-R, can model reading and predict online behavioral measures (reading times and regressions). The parser's modeling capacities are tested against self-paced reading experimental data (Grodner \& Gibson, 2005), eye-tracking experimental data (Staub, 2011), and a self-paced reading corpus (Futrell et al., 2018).},
year = {2021}
}


@article{Dotlacil2021ParsingMA,
  title={Parsing Model and a Rational Theory of Memory},
  author={Jakub Dotlacil and Puck de Haan},
  journal={Frontiers in Psychology},
  year={2021},
  volume={12}
}

@article{DBLP:journals/corr/abs-2103-04469,
  author    = {Adam Goodkind and
               Klinton Bicknell},
  title     = {Local word statistics affect reading times independently of surprisal},
  journal   = {CoRR},
  volume    = {abs/2103.04469},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.04469},
  eprinttype = {arXiv},
  eprint    = {2103.04469},
  timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-04469.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/cogsci/WilcoxGHQL20,
  author    = {Ethan Wilcox and
               Jon Gauthier and
               Jennifer Hu and
               Peng Qian and
               Roger Levy},
  editor    = {Stephanie Denison and
               Michael Mack and
               Yang Xu and
               Blair C. Armstrong},
  title     = {On the Predictive Power of Neural Language Models for Human Real-Time
               Comprehension Behavior},
  booktitle = {Proceedings of the 42th Annual Meeting of the Cognitive Science Society
               - Developing a Mind: Learning in Humans, Animals, and Machines, CogSci
               2020, virtual, July 29 - August 1, 2020},
  publisher = {cognitivesciencesociety.org},
  year      = {2020},
  url       = {https://cogsci.mindmodeling.org/2020/papers/0375/index.html},
  timestamp = {Fri, 19 Feb 2021 16:21:31 +0100},
  biburl    = {https://dblp.org/rec/conf/cogsci/WilcoxGHQL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/cogsci/SchijndelL21,
  author    = {Marten van Schijndel and
               Tal Linzen},
  title     = {Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic
               Disambiguation Difficulty},
  journal   = {Cogn. Sci.},
  volume    = {45},
  number    = {6},
  year      = {2021},
  url       = {https://doi.org/10.1111/cogs.12988},
  doi       = {10.1111/cogs.12988},
  timestamp = {Mon, 13 Sep 2021 12:00:07 +0200},
  biburl    = {https://dblp.org/rec/journals/cogsci/SchijndelL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/acl/WilcoxVL20,
  author    = {Ethan Wilcox and
               Pranali Vani and
               Roger Levy},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {A Targeted Assessment of Incremental Processing in Neural Language
               Models and Humans},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational
               Linguistics and the 11th International Joint Conference on Natural
               Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
               Event, August 1-6, 2021},
  pages     = {939--952},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.acl-long.76},
  doi       = {10.18653/v1/2021.acl-long.76},
  timestamp = {Mon, 09 Aug 2021 16:25:37 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/WilcoxVL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{buerkner2017brms,
   author = {Paul-Christian B{\"u}rkner},
   title = {brms: An {R} Package for Bayesian Multilevel Models Using {S}tan},
   journal = {Journal of Statistical Software, Articles},
   volume = {80},
   number = {1},
   year = {2017},
   pages = {1--28}
}

@article{burkner2018advanced,
  title={Advanced Bayesian Multilevel Modeling with the R Package brms},
  author={B{\"u}rkner, Paul-Christian},
  journal={The R Journal},
  volume={10},
  number={1},
  pages={395--411},
  year={2018}
}

@book{gelman1995bayesian,
	title="Bayesian Data Analysis",
	author="Andrew {Gelman} and John B. {Carlin} and Hal S. {Stern} and David B. {Dunson} and Aki {Vehtari} and Donald B. {Rubin}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2045656233",
	year="1995"
}

@article{homan2014the,
	title="The No-{U}-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo",
	author="Matthew D. {Homan} and Andrew {Gelman}",
	journal="Journal of Machine Learning Research",
	volume="15",
	number="1",
	pages="1593--1623",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963977107",
	year="2014"
}
@article{carpenter2017stan,
	title="Stan: A Probabilistic Programming Language",
	author="Bob {Carpenter} and Andrew {Gelman} and Matthew D. {Hoffman} and Daniel {Lee} and Ben {Goodrich} and Michael {Betancourt} and Marcus A. {Brubaker} and Jiqiang {Guo} and Peter {Li} and Allen {Riddell}",
	journal="Journal of Statistical Software",
	volume="76",
	number="1",
	pages="1--32",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2577537660",
	year="2017"
}

@article{lewandowski2009generating,
	title="Generating random correlation matrices based on vines and extended onion method",
	author="Daniel {Lewandowski} and Dorota {Kurowicka} and Harry {Joe}",
	journal="Journal of Multivariate Analysis",
	volume="100",
	number="9",
	pages="1989--2001",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2084045976",
	year="2009"
}

@inproceedings{DBLP:conf/acl-cmcl/GoodkindB18,
  author    = {Adam Goodkind and
               Klinton Bicknell},
  editor    = {Asad B. Sayeed and
               Cassandra Jacobs and
               Tal Linzen and
               Marten van Schijndel},
  title     = {Predictive power of word surprisal for reading times is a linear function
               of language model quality},
  booktitle = {Proceedings of the 8th Workshop on Cognitive Modeling and Computational
               Linguistics, {CMCL} 2018, Salt Lake City, Utah, USA, January 7, 2018},
  pages     = {10--18},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/w18-0102},
  doi       = {10.18653/v1/w18-0102},
  timestamp = {Mon, 13 Sep 2021 12:00:07 +0200},
  biburl    = {https://dblp.org/rec/conf/acl-cmcl/GoodkindB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{DBLP:series/synthesis/2017Goldberg,
  author    = {Yoav Goldberg},
  title     = {Neural Network Methods for Natural Language Processing},
  series    = {Synthesis Lectures on Human Language Technologies},
  publisher = {Morgan {\&} Claypool Publishers},
  year      = {2017},
  url       = {https://doi.org/10.2200/S00762ED1V01Y201703HLT037},
  doi       = {10.2200/S00762ED1V01Y201703HLT037},
  timestamp = {Mon, 11 Sep 2017 11:42:02 +0200},
  biburl    = {https://dblp.org/rec/series/synthesis/2017Goldberg.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/neco/HochreiterS97,
  author    = {Sepp Hochreiter and
               J{\"{u}}rgen Schmidhuber},
  title     = {Long Short-Term Memory},
  journal   = {Neural Comput.},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  year      = {1997},
  url       = {https://doi.org/10.1162/neco.1997.9.8.1735},
  doi       = {10.1162/neco.1997.9.8.1735},
  timestamp = {Tue, 01 Sep 2020 13:12:40 +0200},
  biburl    = {https://dblp.org/rec/journals/neco/HochreiterS97.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hammersley1954PoorMM,
  title={Poor Man's {M}onte {C}arlo},
  author={J. M. Hammersley and K. W. Morton},
  journal={Journal of the royal statistical society series b-methodological},
  year={1954},
  volume={16},
  pages={23-38}
}

@article{Leech1992100MW,
  title={100 Million Words of {E}nglish: The {B}ritish National Corpus ({BNC})},
  author={Geoffrey Leech},
  journal={Second Language Research},
  year={1992},
  volume={28},
  pages={1-13}
}
@inproceedings{Akaike1973InformationTA,
  title={Information Theory and an Extension of the Maximum Likelihood Principle},
  author={Hirotugu Akaike},
  year={1973}
}

@inproceedings{DBLP:conf/cogsci/WilcoxGHQL20,
  author    = {Ethan Wilcox and
               Jon Gauthier and
               Jennifer Hu and
               Peng Qian and
               Roger Levy},
  editor    = {Stephanie Denison and
               Michael Mack and
               Yang Xu and
               Blair C. Armstrong},
  title     = {On the Predictive Power of Neural Language Models for Human Real-Time
               Comprehension Behavior},
  booktitle = {Proceedings of the 42th Annual Meeting of the Cognitive Science Society
               - Developing a Mind: Learning in Humans, Animals, and Machines, CogSci
               2020, virtual, July 29 - August 1, 2020},
  publisher = {cognitivesciencesociety.org},
  year      = {2020},
  url       = {https://cogsci.mindmodeling.org/2020/papers/0375/index.html},
  timestamp = {Fri, 19 Feb 2021 16:21:31 +0100},
  biburl    = {https://dblp.org/rec/conf/cogsci/WilcoxGHQL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/acl-cmcl/GoodkindB18,
  author    = {Adam Goodkind and
               Klinton Bicknell},
  editor    = {Asad B. Sayeed and
               Cassandra Jacobs and
               Tal Linzen and
               Marten van Schijndel},
  title     = {Predictive power of word surprisal for reading times is a linear function
               of language model quality},
  booktitle = {Proceedings of the 8th Workshop on Cognitive Modeling and Computational
               Linguistics, {CMCL} 2018, Salt Lake City, Utah, USA, January 7, 2018},
  pages     = {10--18},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/w18-0102},
  doi       = {10.18653/v1/w18-0102},
  timestamp = {Mon, 13 Sep 2021 12:00:07 +0200},
  biburl    = {https://dblp.org/rec/conf/acl-cmcl/GoodkindB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{VanDyke2011CuedependentII,
  title={Cue-dependent interference in comprehension.},
  author={Julie A. Van Dyke},
  journal={Journal of memory and language},
  year={2011},
  volume={65 3},
  pages={
          247-263
        }
}

@article{VanDyke2006RetrievalII,
  title={Retrieval interference in sentence comprehension},
  author={Julie A. Van Dyke and Brian McElree},
  journal={Journal of {M}emory and {L}anguage},
  year={2006},
  volume={55},
  issue={2},
  pages={
          157-166
        }
}


@article{VanDyke2007InterferenceEF,
  title={Interference effects from grammatically unavailable constituents during sentence processing.},
  author={Julie A. Van Dyke},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={2007},
  volume={33 2},
  pages={
          407-30
        }
}

@article{Gordon2006SimilaritybasedID,
  title={Similarity-based interference during language comprehension: Evidence from eye tracking during reading.},
  author={Peter C. Gordon and Randall Hendrick and Marcus Johnson and Yoonhyoung Lee},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={2006},
  volume={32},
  issue={6},
  pages={
          1304-21
        }
}



@inproceedings{DBLP:conf/lrec/PetrovDM12,
  author    = {Slav Petrov and
               Dipanjan Das and
               Ryan T. McDonald},
  editor    = {Nicoletta Calzolari and
               Khalid Choukri and
               Thierry Declerck and
               Mehmet Ugur Dogan and
               Bente Maegaard and
               Joseph Mariani and
               Jan Odijk and
               Stelios Piperidis},
  title     = {A Universal Part-of-Speech Tagset},
  booktitle = {Proceedings of the Eighth International Conference on Language Resources
               and Evaluation, {LREC} 2012, Istanbul, Turkey, May 23-25, 2012},
  pages     = {2089--2096},
  publisher = {European Language Resources Association {(ELRA)}},
  year      = {2012},
  url       = {http://www.lrec-conf.org/proceedings/lrec2012/summaries/274.html},
  timestamp = {Mon, 19 Aug 2019 15:23:03 +0200},
  biburl    = {https://dblp.org/rec/conf/lrec/PetrovDM12.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Nicenboim2016WhenHR,
  title={When High-Capacity Readers Slow Down and Low-Capacity Readers Speed Up: Working Memory and Locality Effects},
  author={Bruno Nicenboim and Pavel Logacev and Carolina Andrea Gattei and Shravan Vasishth},
  journal={Frontiers in Psychology},
  year={2016},
  volume={7}
}

@book{Boyd2006ConvexO,
  title={Convex Optimization},
  author={Stephen P. Boyd and Lieven Vandenberghe},
  year={2006}
}

@inproceedings{Cover2005ElementsOI,
  title={Elements of Information Theory},
  author={Thomas M. Cover and Joy A. Thomas},
  year={2005}
}

@article{Futrell2020LossyContextSA,
  title={Lossy-Context Surprisal: An Information-Theoretic Model of Memory Effects in Sentence Processing},
  author={Richard Futrell and Edward Gibson and Roger P. Levy},
  journal={Cognitive Science},
  year={2020},
  volume={44}
}
@article{barnard2000modeling,
        title="Modeling covariance matrices in terms of standard deviations and correlations, with application to shrinkage",
        author="John {Barnard} and Robert {McCulloch} and Xiao Li {Meng}",
        journal="Statistica Sinica",
        volume="10",
        number="4",
        pages="1281--1311",
        notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1249696972",
        year="2000" 
} 
@misc{boyce2020amlap,
	title="A-maze of Natural Stories: Texts are comprehensible using the Maze task",
	author={Veronica Boyce and Roger Levy},
howpublished={Paper presented at: Architectures and Mechanisms for Language Processing (AMLaP)},
	year={2020}
}

@article{Rasmussen2018LeftCornerPW,
  title={Left-Corner Parsing With Distributed Associative Memory Produces Surprisal and Locality Effects.},
  author={Nathan Rasmussen and William Schuler},
  journal={Cognitive science},
  year={2018},
  volume={42},
  pages={
          1009-1042
        }
}

@article{Dotlacil2021ParsingAA,
  title={Parsing as a Cue‐Based Retrieval Model},
  author={Jakub Dotlacil},
  journal={Cognitive Science},
  year={2021},
  volume={45}
}

@book{Jackendoff1980XSA,
  title={X Syntax: A Study of Phrase Structure},
  author={Ray Jackendoff},
  year={1980}
}

@book{Anderson1990adaptive,
	title={The Adaptive Character of Thought},
	year={1990},
	author={John R. Anderson},
	address={Hillsdale, NJ},
	publisher={Erlbaum}
}

@book{Ebbinghaus18852013MemoryAC,
  title={Memory: A Contribution to Experimental Psychology},
  author={Hermann Ebbinghaus},
  year={1913}
}



@inproceedings{levy:2008emnlp,
    title = "A Noisy-Channel Model of Human Sentence Comprehension under Uncertain Input",
    author = "Levy, Roger",
    booktitle = "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2008",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D08-1025",
    pages = "234--243",
}


@inproceedings{levy:2011acl,
  author    = {Roger Levy},
  title     = {Integrating surprisal and uncertain-input models in online sentence
               comprehension: formal techniques and empirical results},
  booktitle = {The 49th Annual Meeting of the Association for Computational Linguistics},
  pages     = {1055--1065},
  year      = {2011},
  url       = {https://aclanthology.org/P11-1106/},
  timestamp = {Fri, 06 Aug 2021 00:41:04 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/Levy11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{ryskin-etal:2018comprehenders,
	author = {Ryskin, Rachel and Futrell, Richard and Kiran, Swathi and Gibson, Edward},
	date-added = {2018-10-25 15:44:50 -0400},
	date-modified = {2018-10-25 15:44:56 -0400},
	journal = {Cognition},
	pages = {141--150},
	publisher = {Elsevier},
	title = {Comprehenders model the nature of noise in the environment},
	volume = {181},
	year = {2018}}


@article{karlsson:2007-constraints,
	author = {Karlsson, Fred},
	date-added = {2020-09-23 09:18:04 -0400},
	date-modified = {2020-09-23 09:18:11 -0400},
	journal = {Journal of Linguistics},
	pages = {365--392},
	publisher = {JSTOR},
	title = {Constraints on multiple center-embedding of clauses},
	year = {2007}}

@article{Huang2021,
title = {When missing NPs make double center-embedding sentences acceptable},
author = {Nick Huang and Colin Phillips},
url = {http://www.colinphillips.net/wp-content/uploads/2021/02/huang2021.pdf, When missing NPs make double center-embedding sentences acceptable
http://www.colinphillips.net/wp-content/uploads/2021/02/huang2021_appendix1.pdf, Appendix 1 (additional judgment analyses)
http://www.colinphillips.net/wp-content/uploads/2021/02/huang2021_appendix2.pdf, Appendix 2 (experimental materials)},
year = {2021},
date = {2021-02-02},
journal = {Glossa},
note = {revised submission, to appear in Glossa},
keywords = {},
pubstate = {published},
tppubtype = {article}
}

@article{Parker2016,
title = {Negative polarity illusions and the format of hierarchical encodings in memory},
author = {Dan Parker and Colin Phillips},
url = {http://www.colinphillips.net/wp-content/uploads/2016/11/parkerphillips2016.pdf, Negative polarity illusions and the format of hierarchical encodings in memory
http://www.colinphillips.net/wp-content/uploads/2015/12/parker2015_npi_supp.pdf, Supplementary materials},
year = {2016},
date = {2016-10-07},
journal = {Cognition},
volume = {157},
pages = {321-339},
keywords = {},
pubstate = {published},
tppubtype = {article}
}

@article{Keshev2021NoisyIB,
  title={Noisy is better than rare: Comprehenders compromise subject-verb agreement to form more probable linguistic structures},
  author={Maayan Keshev and Aya Meltzer-Asscher},
  journal={Cognitive Psychology},
  year={2021},
  volume={124}
}


@article{Jger2017SimilaritybasedII,
  title={Similarity-based interference in sentence comprehension: Literature review and Bayesian meta-analysis},
  author={Lena A. J{\"a}ger and Felix Engelmann and Shravan Vasishth},
  journal={Journal of Memory and Language},
  year={2017},
  volume={94},
  pages={316-339}
}

@article{Bloom1980CompletionNF,
  title={Completion norms for 329 sentence contexts},
  author={Paul Alexander Bloom and Ira Fischler},
  journal={Memory \& Cognition},
  year={1980},
  volume={8},
  pages={631-642}
}

@article{Block2010ClozePA,
  title={Cloze probability and completion norms for 498 sentences: Behavioral and neural validation using event-related potentials},
  author={Cady K Block and Carryl L. Baldwin},
  journal={Behavior Research Methods},
  year={2010},
  volume={42},
  pages={665-670}
}


@article{Taylor1953ClozePA,
  title={``{C}loze {P}rocedure'': A New Tool for Measuring Readability},
  author={Wilson L. Taylor},
  journal={Journalism \& Mass Communication Quarterly},
  year={1953},
  volume={30},
  pages={415 - 433}
}


@article{Dyke2003DistinguishingEO,
  title={Distinguishing effects of structure and decay on attachment and repair: A cue-based parsing account of recovery from misanalyzed ambiguities},
  author={Julie A. Van Dyke and Richard L. Lewis},
  journal={Journal of Memory and Language},
  year={2003},
  volume={49},
  pages={285-316}
}


@article{Frank2011InsensitivityOT,
  title={Insensitivity of the Human Sentence-Processing System to Hierarchical Structure},
  author={Stefan Frank and Rens Bod},
  journal={Psychological Science},
  year={2011},
  volume={22},
  pages={829 - 834}
}

@article{MacDonald2002ReassessingWM,
  title={Reassessing Working Memory: Comment on {J}ust and {C}arpenter (1992) and {W}aters and {C}aplan (1996)},
  author={Maryellen C. MacDonald and Morten H. Christiansen},
  journal={Psychological Review},
  year={2002},
  volume={109},
  pages={35-54}
}

@misc{gibson2011domain,
author={Edward Gibson and E Fedorenko},
year={2011},
title={The domain-generality of working memory resources for language},
howpublished={Paper presented at: Architectures and Mechanisms for Language Processing (AMLaP)}
}




@inproceedings{Shain2016Memory,
  author    = {Cory Shain and
               Marten van Schijndel and
               Richard Futrell and
               Edward Gibson and
               William Schuler},
  title     = {Memory access during incremental sentence processing causes reading
               time latency},
  booktitle = {Proceedings of the Workshop on Computational Linguistics for Linguistic
               Complexity},
  pages     = {49--58},
  year      = {2016},
  url       = {https://aclanthology.org/W16-4106/},
  timestamp = {Mon, 13 Sep 2021 12:00:07 +0200},
  biburl    = {https://dblp.org/rec/conf/acl-cl4lc/ShainSFGS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{Staub2015TheEO,
  title={The Effect of Lexical Predictability on Eye Movements in Reading: Critical Review and Theoretical Interpretation},
  author={Adrian Staub},
  journal={Lang. Linguistics Compass},
  year={2015},
  volume={9},
  pages={311-327}
}

@article{Kuperberg2016WhatDW,
  title={What do we mean by prediction in language comprehension?},
  author={Gina R. Kuperberg and T. Florian Jaeger},
  journal={Language, Cognition and Neuroscience},
  year={2016},
  volume={31},
  pages={32 - 59}
}

@article{Rayner1996EffectsOC,
  title={Effects of contextual constraint on eye movements in reading: A further examination},
  author={Keith Rayner and Arnold D. Well},
  journal={Psychonomic Bulletin \& Review},
  year={1996},
  volume={3},
  pages={504-509}
}


@article{Smith2013TheEO,
  title={The effect of word predictability on reading time is logarithmic},
  author={Nathaniel J. Smith and R. Levy},
  journal={Cognition},
  year={2013},
  volume={128},
  pages={302-319}
}


@incollection{Jurafsky2006ProbabilisticMI,
  title={Probabilistic Modeling in Psycholinguistics: Linguistic Comprehension and Production},
  author={Dan Jurafsky},
  year={2002},
  booktitle={Probabilistic Linguistics},
  editor={Rens Bod and Jennifer Hay and Stefanie Jannedy},
  pages={39--95}
}


@article{Lewandowsky2000ARA,
  title={A redintegration account of the effects of speech rate, lexicality, and word frequency in immediate serial recall},
  author={Stephan Lewandowsky and Simon Farrell},
  journal={Psychological Research},
  year={2000},
  volume={63},
  pages={163-173}
}


@article{Hulme1997WordfrequencyEO,
  title={Word-frequency effects on short-term memory tasks: evidence for a redintegration process in immediate serial recall.},
  author={Charles Hulme and Steven Roodenrys and Richard Schweickert and G D A Brown and M Martin and George P. Stuart},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={1997},
  volume={23 5},
  pages={
          1217-32
        }
}

@article{Norris2019ChunkingAR,
  title={Chunking and Redintegration in Verbal Short-Term Memory},
  author={Dennis Norris and Kristjan Kalm and Jane Hall},
  journal={Journal of Experimental Psychology. Learning, Memory, and Cognition},
  year={2019},
  volume={46},
  pages={872 - 893}
}

@article{Jones2018DoesSB,
  title={Does syntax bias serial order reconstruction of verbal short-term memory?},
  author={Timothy Jones and Simon Farrell},
  journal={Journal of Memory and Language},
  year={2018}
}


@article{Hsiao2016ProductionPC,
  title={Production predicts comprehension: Animacy effects in Mandarin relative clause processing},
  author={Yaling Hsiao and Maryellen C. MacDonald},
  journal={Journal of Memory and Language},
  year={2016},
  volume={89},
  pages={87-109}
}

@article{Gennari2012AnimacyAC,
  title={Animacy and competition in relative clause production: A cross-linguistic investigation},
  author={Silvia P. Gennari and Jelena Mirkovic and Maryellen C. MacDonald},
  journal={Cognitive Psychology},
  year={2012},
  volume={65},
  pages={141-176}
}

@article{Schwering2020VerbalWM,
  title={Verbal Working Memory as Emergent from Language Comprehension and Production},
  author={Steven Schwering and Maryellen C. MacDonald},
  journal={Frontiers in Human Neuroscience},
  year={2020},
  volume={14}
}

@article{Gennari2008SemanticII,
  title={Semantic indeterminacy in object relative clauses.},
  author={Silvia P. Gennari and Maryellen C. MacDonald},
  journal={Journal of memory and language},
  year={2008},
  volume={58 4},
  pages={
          161-187
        }
}


@article{Schweickert1993AMP,
  title={A multinomial processing tree model for degradation and redintegration in immediate recall},
  author={Richard Schweickert},
  journal={Memory \& Cognition},
  year={1993},
  volume={21},
  pages={168-175}
}

@article{Schweickert1993AMP,
  title={A multinomial processing tree model for degradation and redintegration in immediate recall},
  author={Richard Schweickert},
  journal={Memory \& Cognition},
  year={1993},
  volume={21},
  pages={168-175}
}

@article{Brown1995ModelingIL,
  title={Modeling Item Length Effects in Memory Span: No Rehearsal Needed?},
  author={Gordon D. A. Brown and Charles Hulme},
  journal={Journal of Memory and Language},
  year={1995},
  volume={34},
  pages={594-621}
}

@article{Jones2018DoesSB,
  title={Does syntax bias serial order reconstruction of verbal short-term memory?},
  author={Timothy Jones and Simon Farrell},
  journal={Journal of Memory and Language},
  year={2018}
}
@article{Smucker2018OptimalED,
  title={Optimal experimental design},
  author={Byran J. Smucker and Martin Krzywinski and Naomi Altman},
  journal={Nature Methods},
  year={2018},
  volume={15},
  pages={559-560}
}


%Oh, B.-D., & Schuler, W. (2022). Entropy- and distance-based predictors from GPT-2 attention patterns predict reading times over and above GPT-2 surprisal. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.
@inproceedings{Oh2022EntropyAD,
  title={Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal},
  author={Byung-Doh Oh and William Schuler},
  year={2022}
}
@article{Smith2013TheEO,
  title={The effect of word predictability on reading time is logarithmic},
  author={Nathaniel J. Smith and R. Levy},
  journal={Cognition},
  year={2013},
  volume={128},
  pages={302-319}
}

@article{Brothers2021WordPE,
  title={Word predictability effects are linear, not logarithmic: Implications for probabilistic models of sentence comprehension.},
  author={Trevor Brothers and Gina R. Kuperberg},
  journal={Journal of memory and language},
  year={2021},
  volume={116}
}

@article{Szewczyk2021ContextbasedFO,
  title={Context-based facilitation of semantic access follows both logarithmic and linear functions of stimulus probability},
  author={Jakub M. Szewczyk and Kara D. Federmeier},
  journal={Journal of memory and language},
  year={2021},
  volume={123}
}

@article{Meister2021RevisitingTU,
  title={Revisiting the Uniform Information Density Hypothesis},
  author={Clara Meister and Tiago Pimentel and Patrick Haller and Lena Jager and Ryan Cotterell and Roger Philip Levy},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.11635}
}

@article{Bhatia2020PreverbalSC,
  title={Preverbal syntactic complexity leads to local coherence effects in Hindi},
  author={Sakshi Bhatia and Samar Husain},
  journal={Language, Cognition and Neuroscience},
  year={2020}
}

@article{Vasishth2010ShorttermFI,
  title={Short-term forgetting in sentence comprehension: Crosslinguistic evidence from verb-final structures},
  author={Shravan Vasishth and Katja Suckow and Richard L. Lewis and Sabine Kern},
  journal={Language and Cognitive Processes},
  year={2010},
  volume={25},
  pages={533 - 567}
}

@article{Huang2021WhenMN,
  title={When missing NPs make double center-embedding sentences
 acceptable},
  author={Nick Huang and Colin Phillips},
  journal={Glossa: a journal of general linguistics},
  year={2021}
}

@article{Gibson2013RationalIO,
  title={Rational integration of noisy evidence and prior semantic expectations in sentence interpretation},
  author={Edward Gibson and Leon Bergen and Steven T. Piantadosi},
  journal={Proceedings of the National Academy of Sciences},
  year={2013},
  volume={110},
  pages={8051 - 8056}
}

@article{ZHANG2023105346,
title = {A noisy-channel approach to depth-charge illusions},
journal = {Cognition},
volume = {232},
pages = {105346},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2022.105346},
url = {https://www.sciencedirect.com/science/article/pii/S0010027722003353},
author = {Yuhan Zhang and Rachel Ryskin and Edward Gibson},
keywords = {Depth-charge sentence, Semantic illusion, Noisy-channel framework, Language comprehension},
abstract = {The “depth-charge” sentence, No head injury is too trivial to be ignored, is often interpreted as “no matter how trivial head injuries are, we should not ignore them” while the literal meaning is the opposite – “we should ignore them”. Four decades of research have failed to resolve the source of this entrenched semantic illusion. Here we adopt the noisy-channel framework for language comprehension to provide a potential explanation. We hypothesize that depth-charge sentences result from inferences whereby comprehenders derive the interpretation by weighing the plausibility of possible readings of the depth-charge sentences against the likelihood of plausible sentences being produced with errors. In four experiments, we find that (1) the more plausible the intended meaning of the depth-charge sentence is, the more likely the sentence is to be misinterpreted; and (2) the higher the likelihood of our hypothesized noise operations, the more likely depth-charge sentences are to be misinterpreted. These results suggest that misinterpretation is affected by both world knowledge and the distance between the depth-charge sentence and a plausible alternative, which is consistent with the noisy-channel framework.}
}


@thesis{DembergWinterfors2010BroadcoverageMO,
  title={Broad-coverage model of prediction in human sentence processing},
  author={Vera {Demberg-Winterfors}},
  year={2010}
}

@article{Paape2021DoesLC,
  title={Does Local Coherence Lead to Targeted Regressions and Illusions of Grammaticality?},
  author={Dario Paape and Shravan Vasishth and Ralf Engbert},
  journal={Open Mind : Discoveries in Cognitive Science},
  year={2021},
  volume={5},
  pages={42 - 58}
}


@article{hahn2022resource,
	title={A resource-rational model of human processing of recursive linguistic structure},
	year={2022},
        journal = {Proceedings of the National Academy of Sciences of the United States of America},
	volume={119},
	number={43},
	pages={e2122602119},
	author={Michael Hahn and Richard Futrell and Roger Levy and Edward Gibson},
	doi={https://doi.org/10.1073/pnas.2122602119},
	url={https://doi.org/10.1073/pnas.2122602119}
}


@article{Hahn2020modeling,
  author    = {Michael Hahn and
               Judith Degen and Richard Futrell},
  title     = {Modeling word and morpheme order in natural language as an efficient tradeoff of memory and surprisal},
  journal   = {Psychological Review},
  volume={128},
  issue={4},
  month = {December},
  pages={726--756},
  year      = {2021},
  archivePrefix = {psyarxiv},
  doi = {10.1037/rev0000269},
  url = {files/hahn_psychreview_2021_final.pdf},
}

@article{Hahn-Keller-2018-arxiv,
  author    = {Michael Hahn and
               Frank Keller},
  title     = {Modeling task effects in human reading with neural network-based attention},
  journal   = {Cognition},
  year      = {2023},
  volume = {230},
  pages = {105289},
  archivePrefix = {arXiv},
  eprint    = {1808.00054},
  URL = {https://authors.elsevier.com/a/1fsml2Hx2pj5L}
}

@INPROCEEDINGS{Hahn-etal-2019-arxiv,
  author    = {Michael Hahn and
               Frank Keller and Yonatan Bisk and Yonatan Belinkov},
  title     = {Character-based surprisal as a model of human reading in the presence of errors},
  booktitle = {Proceedings of the 41st Annual Meeting of the Cognitive Science Society (CogSci)},
  year      = {2019},
  url = {https://cogsci.mindmodeling.org/2019/papers/0089/0089.pdf},
  archivePrefix = {arXiv},
  eprint    = {1902.00595},
}

@article{Ferreira2003TheMO,
  title={The misinterpretation of noncanonical sentences},
  author={Fernanda Ferreira},
  journal={Cognitive Psychology},
  year={2003},
  volume={47},
  pages={164-203}
}

@inproceedings{poppels-levy:2016cogsci,
  year = {2016},
  title = {Structure-sensitive Noise Inference: Comprehenders Expect Exchange Errors},
  pages = {378–383},
  howpublished = {Poster presentation},
  booktitle = {Proceedings of the 38th Annual Meeting of the Cognitive Science Society},
  author = {Poppels, Till and Levy, Roger}
}


@inproceedings{clark-etal:2022-evidence-for-availability,
	author = {Thomas Hikaru Clark and Ethan Gotlieb Wilcox and Edward Gibson and Roger P. Levy},
	booktitle = cogsciConference2022,
	date-added = {2022-05-12 15:42:36 -0400},
	date-modified = {2022-05-12 15:46:59 -0400},
	title = {Evidence for Availability Effects on Speaker Choice in the {Russian} Comparative Alternation},
	year = {2022},
	url = {https://escholarship.org/uc/item/1q19f8vt},
}




@inproceedings{zhan2018comparing,
    title = "Comparing Theories of Speaker Choice Using a Model of Classifier Production in {M}andarin {C}hinese",
    author = "Zhan, Meilin  and
      Levy, Roger",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1181",
    doi = "10.18653/v1/N18-1181",
    pages = "1997--2005",
    abstract = "Speakers often have more than one way to express the same meaning. What general principles govern speaker choice in the face of optionality when near semantically invariant alternation exists? Studies have shown that optional reduction in language is sensitive to contextual predictability, such that more predictable a linguistic unit is, the more likely it is to get reduced. Yet it is unclear whether these cases of speaker choice are driven by audience design versus toward facilitating production. Here we argue that for a different optionality phenomenon, namely classifier choice in Mandarin Chinese, Uniform Information Density and at least one plausible variant of availability-based production make opposite predictions regarding the relationship between the predictability of the upcoming material and speaker choices. In a corpus analysis of Mandarin Chinese, we show that the distribution of speaker choices supports the availability-based production account and not the Uniform Information Density.",
}


@article{liu2020mixed,
  title={Mixed evidence for crosslinguistic dependency length minimization},
  author={Liu, Zoey},
  journal={STUF-Language Typology and Universals},
  volume={73},
  number={4},
  pages={605--633},
  year={2020},
  publisher={De Gruyter}
}



@article{hahn2022crosslinguistic,
    title={Crosslinguistic word order variation reflects evolutionary pressures of dependency and information locality},
    author={Hahn, Michael and Xu, Yang},
    journal={Proceedings of the National Academy of Sciences},
    volume={119},
    number={24},
    pages={e2122604119},
    year={2022},
    publisher={National Academy of Sciences},
    doi = {10.1073/pnas.2122604119},
}

@inproceedings{Levy_Jaeger_2006,
    author = {Levy, Roger and Jaeger, T. Florian},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
    pages = {},
    publisher = {MIT Press},
    title = {Speakers optimize information density through syntactic reduction},
    url = {https://proceedings.neurips.cc/paper/2006/file/c6a01432c8138d46ba39957a8250e027-Paper.pdf},
    volume = {19},
    year = {2006}
}

@article{de_Marneffe_Manning_Nivre_Zeman_2021, title={Universal Dependencies}, volume={47}, ISSN={0891-2017}, DOI={10.1162/coli_a_00402}, abstractNote={Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate–argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.}, number={2}, journal={Computational Linguistics}, author={de Marneffe, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel}, year={2021}, month={Jul}, pages={255–308} }



@article{ferreira2002good,
  title={Good-enough representations in language comprehension},
  author={Ferreira, Fernanda and Bailey, Karl GD and Ferraro, Vittoria},
  journal={Current Directions in Psychological Science},
  volume={11},
  number={1},
  pages={11--15},
  year={2002},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{koranda2021good,
  title={Good-enough production: {S}electing easier words instead of more accurate ones},
  author={Koranda, Mark J and Zettersten, Martin and MacDonald, Maryellen C},
  journal={Psychological Science},
  pages={09567976221089603},
  year={2021},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{ferreira2016prediction,
  title={Prediction, information structure, and good-enough language processing},
  author={Ferreira, Fernanda and Lowder, Matthew W},
  journal={Psychology of Learning and Motivation},
  volume={65},
  pages={217--247},
  year={2016},
  publisher={Elsevier}
}

@article{ferreira2007good,
  title={The `good enough' approach to language comprehension},
  author={Ferreira, Fernanda and Patson, Nikole D},
  journal={Language and Linguistics Compass},
  volume={1},
  number={1-2},
  pages={71--83},
  year={2007},
  publisher={Wiley Online Library}
}



@article{ferreira2003given,
  title={Given-new ordering effects on the production of scrambled sentences in {J}apanese},
  author={Ferreira, Victor S and Yoshita, Hiromi},
  journal={Journal of Psycholinguistic Research},
  volume={32},
  number={6},
  pages={669--692},
  year={2003},
  publisher={Springer}
}

@article{bock1980syntactic,
  title={Syntactic effects of information availability in sentence production},
  author={Bock, J. Kathryn and Irwin, David E.},
  journal={Journal of Verbal Learning and Verbal Behavior},
  volume={19},
  number={4},
  pages={467--484},
  year={1980},
  publisher={Elsevier}
}




@article{futrell2021information,
  title={An information-theoretic account of semantic interference in word production},
  author={Futrell, Richard},
  journal={Frontiers in Psychology},
  volume={12},
  pages={672408},
  year={2021},
  publisher={Frontiers Media SA}
}


@article{boyce2020maze,
author={Veronica Boyce and Richard Futrell and Roger P. Levy},
year={2020},
title={Maze Made Easy: Better and easier measurement of incremental processing difficulty},
journal={Journal of Memory and Language},
volume={111},
pages={104082}}



@article{rohde2021whats,
  title={What's new? {A} comprehension bias in favor of informativity},
  author={Rohde, Hannah and Futrell, Richard and Lucas, Christopher G},
  journal={Cognition},
  volume={209},
  pages={104491},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{meister2021revisiting,
    title = "Revisiting the {U}niform {I}nformation {D}ensity Hypothesis",
    author = {Meister, Clara  and
      Pimentel, Tiago  and
      Haller, Patrick  and
      J{\"a}ger, Lena  and
      Cotterell, Ryan  and
      Levy, Roger},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.74",
    doi = "10.18653/v1/2021.emnlp-main.74",
    pages = "963--980",
}


@inproceedings{pimentel2019meaning,
    title = "Meaning to Form: Measuring Systematicity as Information",
    author = "Pimentel, Tiago  and
      McCarthy, Arya D.  and
      Blasi, Damian  and
      Roark, Brian  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1171",
    doi = "10.18653/v1/P19-1171",
    pages = "1751--1764",
    abstract = "A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram {`}gl{'} have any systematic relationship to the meaning of words like {`}glisten{'}, {`}gleam{'} and {`}glow{'}? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small{---}despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language.",
}




@incollection{givon1985iconicity,
  title={Iconicity, isomorphism and non-arbitrary coding in syntax},
  author={Giv{\'o}n, Talmy},
  booktitle={Iconicity in syntax},
  editor={John Haiman},
  pages={187--220},
  year={1985},
  address={Amsterdam},
  publisher={John Benjamins}
}


@incollection{dryer2008branching,
title={The Branching Direction Theory of Word Order Correlations Revisited},
author={Matthew S. Dryer},
year={2011},
booktitle={Universals of Language Today},
editor={Sergio Scalise and Elisabetta Magni and Antonietta Bisetto},
address={Berlin},
publisher={Springer}
}


@article{dryer1991svo,
title = {SVO Languages and the {OV:VO} Typology},
author = {Dryer, Matthew S.},
journal = {Journal of Linguistics},
volume = {27},
number = {2},
pages = {443--482},
year = {1991},
}



@article{Jing2021DependencylengthMA,
  title={Dependency-length minimization and its limits: A possible role for a probabilistic version of the final-over-final condition},
  author={Yingqi Jing and Dami{\'a}n E. Blasi and Balthasar Bickel},
  journal={Language},
  year={2021},
  volume={98},
  pages={397 - 418}
}

@inproceedings{Sheehan2017TheFC,
  title={The Final-Over-Final Condition: A Syntactic Universal},
  author={Michelle Sheehan and Theresa Biberauer and Ian G. Roberts and Anders Holmberg},
  year={2017}
}


@inproceedings{Hawkins2004EfficiencyAC,
  title={Efficiency and complexity in grammars},
  author={John A. Hawkins},
  year={2004}
}


@inproceedings{Hawkins2014CrossLinguisticVA,
  title={Cross-Linguistic Variation and Efficiency},
  author={John A. Hawkins},
  year={2014}
}

@article{Clem2021DisharmonyAT,
  title={Disharmony and the Final-Over-Final Condition in Amahuaca},
  author={Emily Clem},
  journal={Linguistic Inquiry},
  year={2021},
  volume={53},
  pages={809-822}
}

@article{Futrell2020DependencyLA,
  title={Dependency locality as an explanatory principle for word order},
  author={Richard Futrell and Roger Philip Levy and Edward Gibson},
  journal={Language},
  year={2020},
  volume={96},
  pages={371 - 412}
}

@article{Arehalli2022SyntacticSF,
  title={Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities},
  author={Suhas Arehalli and Brian Dillon and Tal Linzen},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.12187}
}

@article{vanSchijndel2020SingleStagePM,
  title={Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty},
  author={Marten van Schijndel and Tal Linzen},
  journal={Cognitive science},
  year={2020},
  volume={45 6},
  pages={
          e12988
        }
}

@inproceedings{huang2022spr,
	author={Kuan-Jung Huang and Suhas Arehalli and Mari Kugemoto and Grusha Prasad and Christian Muxica and Brian Dillon and Tal Linzen},
	year ={2022},
	title={SPR mega-benchmark shows surprisal tracks construction- but not item-level difficulty},
	booktitle={35th Annual Conference on Human Sentence Processing}
}

@article{Chen2005OnlineSS,
  title={Online Syntactic Storage Costs in Sentence Comprehension.},
  author={Evan Chen and Edward Gibson and Florian Wolf},
  journal={Journal of Memory and Language},
  year={2005},
  volume={52},
  pages={144-169}
}

@article{Husain2014StrongEC,
  title={Strong Expectations Cancel Locality Effects: Evidence from Hindi},
  author={Samar Husain and Shravan Vasishth and Narayanan Srinivasan},
  journal={PLoS ONE},
  year={2014},
  volume={9}
}

@article{Futrell2020TheNS,
  title={The Natural Stories corpus: a reading-time corpus of English texts containing rare syntactic constructions},
  author={Richard Futrell and Edward Gibson and Harry J. Tily and Idan Asher Blank and Anastasia Vishnevetsky and Steven T. Piantadosi and Evelina Fedorenko},
  journal={Language Resources and Evaluation},
  year={2020},
  volume={55},
  pages={63 - 77}
}

@article{kennedy2005parafoveal,
  author = {Kennedy, Alan and Pynte, Joel},
  title = {Parafoveal-on-foveal effects in normal reading},
  journal = {Vision Research},
  year = 2005,
  volume = 45,
  number = 2,
  pages = {153--168}
}

@book{graff-switchboard-2-1998,
        address = {Philadelphia},
        title = {Switchboard-2 {Phase} {I} {LDC}98S75. {DVD}},
        url = {https://catalog.ldc.upenn.edu/LDC98S75},
        urldate = {2017-11-06},
        publisher = {Linguistic Data Consortium},
        author = {Graff, David and Canavan, Alexandra and Zipperlen, George},
        year = {1998},
        file = {Switchboard-2 Phase I - Linguistic Data Consortium:/home/user/Zotero/storage/QB347XNU/LDC98S75.html:text/html}
}

@inproceedings{ryu2022transformer,
	title={Using Transformer Language model to Integrate Surprisal, Entropy, and Working Memory Retrieval Accounts of Sentence Processing},
	author={Soo Hyun Ryu and Richard Lewis},
	booktitle={35th Annual Conference on Human Sentence Processing},
	year={2022}
}

@article{Oh2022ComparisonOS,
  title={Comparison of Structural Parsers and Neural Language Models as Surprisal Estimators},
  author={Byung-Doh Oh and Christian Clark and William Schuler},
  journal={Frontiers in Artificial Intelligence},
  year={2022},
  volume={5}
}

@inproceedings{ryu-lewis-2021-accounting,
    title = {Accounting for Agreement Phenomena in Sentence Comprehension with Transformer Language Models: Effects of Similarity-based Interference on Surprisal and Attention},
    author = {Ryu, Soo Hyun  and
      Lewis, Richard},
    booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
    year = {2021},
    publisher = {Association for Computational Linguistics},
    pages = {61--71}
}

@article{Oh2022WhyDS,
  title={Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?},
  author={Byung-Doh Oh and William Schuler},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.12131}
}

@article{liu2020mixed,
	title={Mixed evidence for crosslinguistic dependency length minimization},
	author={Zoey Liu},
	journal={STUF-Language Typology and Universals},
       issue={73},
  number={4},
  pages={605--633},
  year={2020}
}

@article{Bock1982TowardAC,
  title={Toward a Cognitive Psychology of Syntax: Information Processing Contributions to Sentence Formulation},
  author={Kathryn Bock},
  journal={Psychological Review},
  year={1982},
  volume={89},
  pages={1-47}
}

@book{Ariel1990AccessingNA,
  title={Accessing Noun-Phrase Antecedents},
  author={Mira Ariel},
  year={1990}
}

@article{Chang2009LearningTO,
  title={Learning to order words: A connectionist model of heavy NP shift and accessibility effects in Japanese and English},
  author={Franklin Chang},
  journal={Journal of Memory and Language},
  year={2009},
  volume={61},
  pages={374-397}
}

@article{Staub2011TheEO,
  title={The effect of lexical predictability on distributions of eye fixation durations},
  author={Adrian Staub},
  journal={Psychonomic Bulletin \& Review},
  year={2011},
  volume={18},
  pages={371-376}
}

@article{Fedorov2010OptimalED,
  title={Optimal experimental design},
  author={Valerii Fedorov},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  year={2010},
  volume={2}
}

@inproceedings{Zhou2022teasing,
	Author = {Zhou, Irene and Hu, Jennifer and Levy, Roger P. and Zaslavsky, Noga},
	Title = {Teasing apart models of pragmatics using optimal reference game design},
	Booktitle = {Proceedings of the 44th Annual Meeting of the Cognitive Science Society},
	Year = {2022},
}

@inproceedings{foster2019variational, author = {Foster, Adam and Jankowiak, Martin and Bingham, Elias and Horsfall, Paul and Teh, Yee Whye and Rainforth, Thomas and Goodman, Noah}, booktitle = {Advances in Neural Information Processing Systems}, date-added = {2020-09-05 00:58:02 +0000}, date-modified = {2020-09-05 00:58:02 +0000}, pages = {14036--14047}, title = {Variational Bayesian optimal experimental design}, website = {https://arxiv.org/abs/1903.05480}, year = {2019} } 

@inproceedings{Ouyang2018, author = {Ouyang, Long and Tessler, Michael Henry and Ly, D and Goodman, Noah D}, booktitle = {Proceedings of the Fortieth Annual Conference of the Cognitive Science Society}, date-added = {2018-07-03 02:42:41 +0000}, date-modified = {2018-07-03 02:47:16 +0000}, title = {webppl-oed: A practical optimal experiment design system}, website = {papers/OuyangTessler2018-oed-cogsci-final.pdf}, year = {2018} }

@article{Myung2009OptimalED,
  title={Optimal experimental design for model discrimination.},
  author={Jay I. Myung and Mark A. Pitt},
  journal={Psychological review},
  year={2009},
  volume={116 3},
  pages={
          499-518
        }
}

@inproceedings{liu2022onesize,
	title={Does One Size Fit all in Crosslinguistic Dependency Length Minimization?},
	author={Zoey Liu and Ria Upreti and Matthew A. Kramer and Savithry Namboodiripad},
	booktitle={Proceedings of the 44th Annual Meeting of the Cognitive Science Society},
	year={2022}
}

@inproceedings{Bever2013TheCB,
  title={The cognitive basis for linguistic structures},
  author={Thomas G. Bever},
  year={1970}
}

@article{Konieczny2000LocalityAP,
  title={Locality and Parsing Complexity},
  author={Lars Konieczny},
  journal={Journal of Psycholinguistic Research},
  year={2000},
  volume={29},
  pages={627-645}
}

@article{Jger2015TheSA,
  title={The subject-relative advantage in Chinese: Evidence for expectation-based processing},
  author={Lena A. J{\"a}ger and Zhong Chen and Qiang Li and Chien-Jer Charles Lin and Shravan Vasishth},
  journal={Journal of Memory and Language},
  year={2015},
  volume={79},
  pages={97-120}
}


@article{Husain2014StrongEC,
  title={Strong Expectations Cancel Locality Effects: Evidence from Hindi},
  author={Samar Husain and Shravan Vasishth and Narayanan Srinivasan},
  journal={PLoS ONE},
  year={2014},
  volume={9}
}

@article{Levy2012TheSC,
  title={The syntactic complexity of Russian relative clauses},
  author={Roger Philip Levy and Evelina Fedorenko and Edward Gibson},
  journal={Journal of memory and language},
  year={2012},
  volume={69},
  pages={461 - 496}
}

@article{Levy2013ExpectationAL,
  title={Expectation and Locality Effects in German Verb-final Structures.},
  author={Roger Philip Levy and Frank Keller},
  journal={Journal of memory and language},
  year={2013},
  volume={68 2},
  pages={
          199-222
        }
}
@article{Paape2021DoesLC,
  title={Does Local Coherence Lead to Targeted Regressions and Illusions of Grammaticality?},
  author={Dario Paape and Shravan Vasishth and Ralf Engbert},
  journal={Open Mind : Discoveries in Cognitive Science},
  year={2021},
  volume={5},
  pages={42 - 58}
}

@article{Cutter2022DoRM,
  title={Do readers maintain word-level uncertainty during reading? A pre-registered replication study},
  author={Michael G Cutter and Ruth Filik and Kevin B. Paterson},
  journal={Journal of Memory and Language},
  year={2022}
}

@incollection{Levy2013MemoryAS,
  title={Memory and surprisal in human sentence comprehension},
  author={Roger Levy},
  year={2013},
  booktitle={Sentence Processing},
  editor={Roger P. G. van Gompel}
}

@article{Christianson2017WhyRE,
  title={Why reread? Evidence from garden-path and local coherence structures},
  author={Kiel Christianson and Steven G. Luke and Erika K. Hussey and Kacey L Wochna},
  journal={Quarterly Journal of Experimental Psychology},
  year={2017},
  volume={70},
  pages={1380 - 1405}
}


@inproceedings{Bicknell2009AMO,
  title={A model of local coherence effects in human sentence processing as consequences of updates from bottom-up prior to posterior beliefs},
  author={K. Bicknell and R. Levy},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2009}
}

@inproceedings{Bicknell2009CorrectingTI,
  title={Correcting the Incorrect: Local Coherence Effects Modeled with Prior Belief Update},
  author={K. Bicknell and R. Levy and Vera Demberg},
  year={2009}
}

@inproceedings{bicknell-levy-demberg:2009,
  year = {2009},
  title = {Correcting the incorrect: Local coherence effects modeled with prior belief update},
  pages = {13–24},
  booktitle = {Proceedings of the 35th Annual Meeting of the Berkeley Linguistics Society},
  author = {Bicknell, Klinton and Levy, Roger and Demberg, Vera}
}

@article{Frank2013ReadingTD,
  title={Reading time data for evaluating broad-coverage models of English sentence processing},
  author={S. Frank and Irene Fernandez Monsalve and Robin L. Thompson and Gabriella Vigliocco},
  journal={Behavior Research Methods},
  year={2013},
  volume={45},
  pages={1182-1190}
}

@article{Kliegl2006TrackingTM,
  title={Tracking the mind during reading: the influence of past, present, and future words on fixation durations.},
  author={Reinhold Kliegl and Antje Nuthmann and Ralf Engbert},
  journal={Journal of experimental psychology. General},
  year={2006},
  volume={135 1},
  pages={
          12-35
        }
}
@article{Kamide2018TheIO,
  title={The Influence of Globally Ungrammatical Local Syntactic Constraints on Real-Time Sentence Comprehension: Evidence From the Visual World Paradigm and Reading},
  author={Yuki Kamide and Anuenue Kukona},
  journal={Cognitive science},
  year={2018},
  volume={42 8},
  pages={
          2976-2998
        }
}

@inproceedings{Mueller2019effect,
	author={H.M. M{\"u}ller and L. Konieczny, L.},
       year={2019},
       title={The effect of context on local syntactic coherency processing},
       booktitle={Proceedings of the 32nd annual CUNY sentence processing conference}
}

@inproceedings{Kitaev2018ConstituencyPW,
  title={Constituency Parsing with a Self-Attentive Encoder},
  author={Nikita Kitaev and Dan Klein},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018}
}

@inproceedings{kauf2022local,
	title={No local coherence effects in the absence of high surprisal},
	author={Carina Kauf and Roger Levy},
	booktitle={35th Annual Conference on Human Sentence Processing},
	year={2022}
}

@article{hahn2022resource,
	title={A resource-rational model of human processing of recursive linguistic structure},
	year={2022},
        journal = {Proceedings of the National Academy of Sciences of the United States of America},
	volume={119},
	number={43},
	pages={e2122602119},
	author={Michael Hahn and Richard Futrell and Roger Levy and Edward Gibson},
	doi={https://doi.org/10.1073/pnas.2122602119},
	url={https://doi.org/10.1073/pnas.2122602119},
	month={oct},
	github={https://gitlab.com/m-hahn/resource-rational-surprisal},
        supplement = {https://www.pnas.org/doi/suppl/10.1073/pnas.2122602119/suppl_file/pnas.2122602119.sapp.pdf}
}

@unpublished{smith2022software,
	title={A software toolkit for modeling human sentence parsing: An approach using continuous-time, discrete-state stochastic dynamical systems},
	author={Garrett Smith and Shravan Vasishth},
	year={2022},
	note={https://psyarxiv.com/dtazq/}
}


@article{Crocker2000WideCoveragePS,
  title={Wide-Coverage Probabilistic Sentence Processing},
  author={Matthew W. Crocker and T. Brants},
  journal={Journal of Psycholinguistic Research},
  year={2000},
  volume={29},
  pages={647-669}
}


@article{Brouwer2021NeurobehavioralCO,
  title={Neurobehavioral Correlates of Surprisal in Language Comprehension: A Neurocomputational Model},
  author={Harm Brouwer and Francesca Delogu and Noortje J. Venhuizen and Matthew W. Crocker},
  journal={Frontiers in Psychology},
  year={2021},
  volume={12}
}


@article{Frank2011InsensitivityOT,
  title={Insensitivity of the Human Sentence-Processing System to Hierarchical Structure},
  author={S. Frank and Rens Bod},
  journal={Psychological Science},
  year={2011},
  volume={22},
  pages={829 - 834}
}

@article{Brouwer2021NeurobehavioralCO,
  title={Neurobehavioral Correlates of Surprisal in Language Comprehension: A Neurocomputational Model},
  author={Harm Brouwer and Francesca Delogu and Noortje J. Venhuizen and Matthew W. Crocker},
  journal={Frontiers in Psychology},
  year={2021},
  volume={12}
}

@article{Frank2015TheER,
  title={The ERP response to the amount of information conveyed by words in sentences},
  author={S. Frank and Leun J. Otten and Giulia Galli and Gabriella Vigliocco},
  journal={Brain and Language},
  year={2015},
  volume={140},
  pages={1-11}
}

@article{hahn-2022-crosslinguistic,
	author= {Michael Hahn and Yang Xu},
	title = {Crosslinguistic word order variation reflects evolutionary pressures of dependency and information locality},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  year = {2022},
  URL = {https://www.pnas.org/doi/10.1073/pnas.2122604119},
  doi = {10.1073/pnas.2122604119},
  volume={119},
  number={24},
  pages={e2122604119},
  github = {https://gitlab.com/m-hahn/efficiency-basic-word-order/},
  preprint = {https://arxiv.org/abs/2206.04239v1},
  supplement = {https://www.pnas.org/doi/suppl/10.1073/pnas.2122604119/suppl_file/pnas.2122604119.sapp.pdf},
  month={July},
png={figs/hahn2022crosslinguistic.png}
}

@article{hahn2020sensitivity,
  author    = {Michael Hahn and
               Dan Jurafsky and Richard Futrell},
  title     = {Sensitivity as a complexity measure for sequence classification tasks},
  png = {figs/sensirivity.png},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {9},
  pages = {891--908},
  year      = {2021},
  month = {November},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00403/106992/Sensitivity-as-a-Complexity-Measure-for-Sequence},
  preprint       = {https://arxiv.org/abs/2104.10343},
  github = {https://github.com/m-hahn/sensitivity},
  slides = {files/sensitivity-slides.pdf}
}

@article{Hahn-2019-arxiv,
author = {Michael Hahn},
title = {Theoretical limitations of self-attention in neural sequence models},
journal = {Transactions of the Association for Computational Linguistics},
volume = {8},
number = {},
pages = {156-171},
year = {2020},
month = {October},
doi = {10.1162/tacl\_a\_00306},
png = {figs/transformers.png},
URL = {
        https://doi.org/10.1162/tacl_a_00306
},
	slides={files/acl2020-selfattention.pdf},
  preprint = {https://arxiv.org/abs/1906.06755},
  supplement = {files/transformers-proof.pdf},
    abstract = { Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics. }
}

@article{hahn-universals-2019,
  author = {Michael Hahn and Jurafsky, Dan and Futrell, Richard},
  title = {Universals of word order reflect optimization of grammars for efficient communication},
  png = {figs/pnas2020.png},
  pages={2347--2353},
  volume={117},
  issue={5},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  year = {2020},
  month = {December},
  URL = {https://www.pnas.org/content/early/2020/01/16/1910923117},
  github = {https://github.com/m-hahn/grammar-optim},
  supplement = {https://www.pnas.org/content/pnas/suppl/2020/01/17/1910923117.DCSupplemental/pnas.1910923117.sapp.pdf},
  abstract = {Human languages share many grammatical properties. We show that some of these properties can be explained by the need for languages to offer efficient communication between humans given our cognitive constraints. Grammars of languages seem to find a balance between two communicative pressures: to be simple enough to allow the speaker to easily produce sentences, but complex enough to be unambiguous to the hearer, and this balance explains well-known word-order generalizations across our sample of 51 varied languages. Our results offer quantitative and computational evidence that language structure is dynamically shaped by communicative and cognitive pressures.}
}

@inproceedings{hewitt-rnns-2020,
  author    = {John Hewitt and Michael Hahn and Surya Ganguli and Percy Liang and Christopher Manning},
  title     = {{RNN}s can generate bounded hierarchical languages with optimal memory},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)},
  year      = {2020},
  pages={1978--2010},
  month = {January},
  png = {figs/rnns.png},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.156.pdf},
  preprint = {https://arxiv.org/abs/2010.07515},
  github = {https://github.com/john-hewitt/dyckkm-constructions/}
}

@Article{hahn2019tabula,
   author = {Michael Hahn and Marco Baroni},
   title = {Tabula nearly rasa: Probing the linguistic knowledge of character-level neural language models trained on unsegmented text},
   journal = {Transactions of the Association for Computational Linguistics},
   year = {2019},
   volume = {7},
   pages = {467--484},
   github = {https://github.com/m-hahn/tabula-rasa-rnns},
   URL = {https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00283},
   preprint = {https://arxiv.org/abs/1906.07285},
   png = {figs/baroni.png},
   abstract = {Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our "near tabula rasa" RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage.}
}

@article{hahn2023theory,
	title={A theory of emergent in-context learning as implicit structure induction},
	author={Michael Hahn and Navin Goyal},
	journal={arXiv Preprint},
	year={2023},
	     url={https://arxiv.org/abs/2303.07971}
}


@article{Wei2022Chain,
  author    = {Jason Wei and
               Xuezhi Wang and
               Dale Schuurmans and
               Maarten Bosma and
               Ed H. Chi and
               Quoc Le and
               Denny Zhou},
  title     = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal   = {CoRR},
  volume    = {abs/2201.11903},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.11903},
  eprinttype = {arXiv},
  eprint    = {2201.11903},
  timestamp = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-11903.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{BrownMRSKDNSSAA20,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:56:50 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{SchickS21,
  author    = {Timo Schick and
               Hinrich Sch{\"{u}}tze},
  editor    = {Kristina Toutanova and
               Anna Rumshisky and
               Luke Zettlemoyer and
               Dilek Hakkani{-}T{\"{u}}r and
               Iz Beltagy and
               Steven Bethard and
               Ryan Cotterell and
               Tanmoy Chakraborty and
               Yichao Zhou},
  title     = {It's Not Just Size That Matters: Small Language Models Are Also Few-Shot
               Learners},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2021, Online, June 6-11, 2021},
  pages     = {2339--2352},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.naacl-main.185},
  doi       = {10.18653/v1/2021.naacl-main.185},
  timestamp = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/SchickS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}






@inproceedings{DBLP:conf/iclr/XieRL022,
  author    = {Sang Michael Xie and
               Aditi Raghunathan and
               Percy Liang and
               Tengyu Ma},
  title     = {An Explanation of In-context Learning as Implicit Bayesian Inference},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=RdJVFCHjUMI},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/XieRL022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{DBLP:journals/corr/abs-2211-15661,
  author    = {Ekin Aky{\"{u}}rek and
               Dale Schuurmans and
               Jacob Andreas and
               Tengyu Ma and
               Denny Zhou},
  title     = {What learning algorithm is in-context learning? Investigations with
               linear models},
  journal   = {CoRR},
  volume    = {abs/2211.15661},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.15661},
  doi       = {10.48550/arXiv.2211.15661},
  eprinttype = {arXiv},
  eprint    = {2211.15661},
  timestamp = {Tue, 29 Nov 2022 17:41:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-15661.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}






@article{DBLP:journals/corr/abs-2202-12837,
  author    = {Sewon Min and
               Xinxi Lyu and
               Ari Holtzman and
               Mikel Artetxe and
               Mike Lewis and
               Hannaneh Hajishirzi and
               Luke Zettlemoyer},
  title     = {Rethinking the Role of Demonstrations: What Makes In-Context Learning
               Work?},
  journal   = {CoRR},
  volume    = {abs/2202.12837},
  year      = {2022},
  url       = {https://arxiv.org/abs/2202.12837},
  eprinttype = {arXiv},
  eprint    = {2202.12837},
  timestamp = {Wed, 02 Mar 2022 16:35:04 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2202-12837.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Oswald2022TransformersLI,
  title={Transformers learn in-context by gradient descent},
  author={Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and Joao Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
  year={2022}
}




@article{DBLP:journals/corr/abs-2208-01066,
  author    = {Shivam Garg and
               Dimitris Tsipras and
               Percy Liang and
               Gregory Valiant},
  title     = {What Can Transformers Learn In-Context? {A} Case Study of Simple Function
               Classes},
  journal   = {CoRR},
  volume    = {abs/2208.01066},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2208.01066},
  doi       = {10.48550/arXiv.2208.01066},
  eprinttype = {arXiv},
  eprint    = {2208.01066},
  timestamp = {Tue, 09 Aug 2022 17:15:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2208-01066.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhang2022RobustnessOD,
  title={Robustness of Demonstration-based Learning Under Limited Data Scenario},
  author={Hongxi Zhang and Yanzhe Zhang and Ruiyi Zhang and Diyi Yang},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.10693}
}



@inproceedings{DBLP:conf/acl/BenderK20,
  author    = {Emily M. Bender and
               Alexander Koller},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  title     = {Climbing towards {NLU:} On Meaning, Form, and Understanding in the
               Age of Data},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages     = {5185--5198},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.acl-main.463},
  doi       = {10.18653/v1/2020.acl-main.463},
  timestamp = {Fri, 06 Aug 2021 00:41:06 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/BenderK20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{DBLP:journals/tacl/MerrillGSS21,
  author    = {William Merrill and
               Yoav Goldberg and
               Roy Schwartz and
               Noah A. Smith},
  title     = {Provable Limitations of Acquiring Meaning from Ungrounded Form: What
               Will Future Language Models Understand?},
  journal   = {Trans. Assoc. Comput. Linguistics},
  volume    = {9},
  pages     = {1047--1060},
  year      = {2021},
  url       = {https://doi.org/10.1162/tacl\_a\_00412},
  doi       = {10.1162/tacl\_a\_00412},
  timestamp = {Fri, 02 Sep 2022 10:50:08 +0200},
  biburl    = {https://dblp.org/rec/journals/tacl/MerrillGSS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@book{chomsky1957syntactic,
  title={Syntactic structures.},
  author={Chomsky, Noam},
  year={1957},
  publisher={Mouton},
  address={The Hague}
}

@incollection{montague1973proper,
  title={The proper treatment of quantification in ordinary {E}nglish},
  author={Montague, Richard},
  booktitle={Approaches to natural language},
  pages={221--242},
  year={1973},
  publisher={Springer}
}

@article{Frank2012PredictingPR,
  title={Predicting Pragmatic Reasoning in Language Games},
  author={Michael C. Frank and Noah D. Goodman},
  journal={Science},
  year={2012},
  volume={336},
  pages={998 - 998}
}

@article{Goodman2016PragmaticLI,
  title={Pragmatic Language Interpretation as Probabilistic Inference},
  author={Noah D. Goodman and Michael C. Frank},
  journal={Trends in Cognitive Sciences},
  year={2016},
  volume={20},
  pages={818-829}
}

@article{Piantadosi2012BootstrappingIA,
  title={Bootstrapping in a language of thought: A formal model of numerical concept learning},
  author={Steven T. Piantadosi and Joshua B. Tenenbaum and Noah D. Goodman},
  journal={Cognition},
  year={2012},
  volume={123},
  pages={199-217}
}

@article{Piantadosi2016FourPS,
  title={Four Problems Solved by the Probabilistic Language of Thought},
  author={Steven T. Piantadosi and Robert A. Jacobs},
  journal={Current Directions in Psychological Science},
  year={2016},
  volume={25},
  pages={54 - 59}
}

@article{Overlan2017LearningAV,
  title={Learning abstract visual concepts via probabilistic program induction in a Language of Thought},
  author={Matthew Overlan and Robert A. Jacobs and Steven T. Piantadosi},
  journal={Cognition},
  year={2017},
  volume={168},
  pages={320-334}
}



@article{karlsson:2007-constraints,
        author = {Karlsson, Fred},
        date-added = {2020-09-23 09:18:04 -0400},
        date-modified = {2020-09-23 09:18:11 -0400},
        journal = {Journal of Linguistics},
        pages = {365--392},
        publisher = {JSTOR},
        title = {Constraints on multiple center-embedding of clauses},
        year = {2007}}



@inproceedings{DBLP:conf/acl/BlasiCWSBB19,
  author    = {Dami{\'{a}}n E. Blasi and
               Ryan Cotterell and
               Lawrence Wolf{-}Sonkin and
               Sabine Stoll and
               Balthasar Bickel and
               Marco Baroni},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {On the Distribution of Deep Clausal Embeddings: {A} Large Cross-linguistic
               Study},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {3938--3943},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1384},
  doi       = {10.18653/v1/p19-1384},
  timestamp = {Fri, 06 Aug 2021 00:41:01 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/BlasiCWSBB19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{Wolf2019Huggingface,
  author    = {Thomas Wolf and
               Lysandre Debut and
               Victor Sanh and
               Julien Chaumond and
               Clement Delangue and
               Anthony Moi and
               Pierric Cistac and
               Tim Rault and
               R{\'{e}}mi Louf and
               Morgan Funtowicz and
               Jamie Brew},
  title     = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1910.03771},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.03771},
  eprinttype = {arXiv},
  eprint    = {1910.03771},
  timestamp = {Tue, 02 Jun 2020 12:49:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-03771.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={A. Radford and Jeffrey Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  publisher={OpenAI}
}



@article{Andreas2022LanguageModels,
  author    = {Jacob Andreas},
  title     = {Language Models as Agent Models},
  journal   = {CoRR},
  volume    = {abs/2212.01681},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.01681},
  doi       = {10.48550/arXiv.2212.01681},
  eprinttype = {arXiv},
  eprint    = {2212.01681},
  timestamp = {Thu, 08 Dec 2022 15:26:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-01681.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{DBLP:conf/acl/LiNA20,
  author    = {Belinda Z. Li and
               Maxwell I. Nye and
               Jacob Andreas},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {Implicit Representations of Meaning in Neural Language Models},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational
               Linguistics and the 11th International Joint Conference on Natural
               Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
               Event, August 1-6, 2021},
  pages     = {1813--1827},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.acl-long.143},
  doi       = {10.18653/v1/2021.acl-long.143},
  timestamp = {Mon, 09 Aug 2021 16:25:37 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/LiNA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{DBLP:journals/corr/abs-2208-02957,
  author    = {Steven T. Piantadosi and
               Felix Hill},
  title     = {Meaning without reference in large language models},
  journal   = {CoRR},
  volume    = {abs/2208.02957},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2208.02957},
  doi       = {10.48550/arXiv.2208.02957},
  eprinttype = {arXiv},
  eprint    = {2208.02957},
  timestamp = {Tue, 16 Aug 2022 09:53:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2208-02957.bib}
}

@article{DBLP:journals/corr/abs-2206-07682,
  author    = {Jason Wei and
               Yi Tay and
               Rishi Bommasani and
               Colin Raffel and
               Barret Zoph and
               Sebastian Borgeaud and
               Dani Yogatama and
               Maarten Bosma and
               Denny Zhou and
               Donald Metzler and
               Ed H. Chi and
               Tatsunori Hashimoto and
               Oriol Vinyals and
               Percy Liang and
               Jeff Dean and
               William Fedus},
  title     = {Emergent Abilities of Large Language Models},
  journal   = {CoRR},
  volume    = {abs/2206.07682},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2206.07682},
  doi       = {10.48550/arXiv.2206.07682},
  eprinttype = {arXiv},
  eprint    = {2206.07682},
  timestamp = {Tue, 21 Jun 2022 17:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2206-07682.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{DBLP:journals/corr/abs-2211-12588,
  author    = {Wenhu Chen and
               Xueguang Ma and
               Xinyi Wang and
               William W. Cohen},
  title     = {Program of Thoughts Prompting: Disentangling Computation from Reasoning
               for Numerical Reasoning Tasks},
  journal   = {CoRR},
  volume    = {abs/2211.12588},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.12588},
  doi       = {10.48550/arXiv.2211.12588},
  eprinttype = {arXiv},
  eprint    = {2211.12588},
  timestamp = {Tue, 29 Nov 2022 17:41:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-12588.bib},
}

@article{DBLP:journals/corr/abs-2205-05055,
  author    = {Stephanie C. Y. Chan and
               Adam Santoro and
               Andrew K. Lampinen and
               Jane X. Wang and
               Aaditya Singh and
               Pierre H. Richemond and
               Jay McClelland and
               Felix Hill},
  title     = {Data Distributional Properties Drive Emergent In-Context Learning
               in Transformers},
  journal   = {CoRR},
  volume    = {abs/2205.05055},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2205.05055},
  doi       = {10.48550/arXiv.2205.05055},
  eprinttype = {arXiv},
  eprint    = {2205.05055},
  timestamp = {Mon, 30 May 2022 08:41:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2205-05055.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{oai_o3_pub_breakthrough,
  author       = {François Chollet},
  title        = {OpenAI o3 Breakthrough High Score on ARC-AGI-Pub},
  year         = {2025},
  month        = {December},
  url          = {https://arcprize.org/blog/oai-o3-pub-breakthrough},
  note         = {Accessed: 2025-01-25}
}


@misc{beurerkellner2022prompting,
    title={Prompting Is Programming: A Query Language For Large Language Models},
    author={Luca Beurer-Kellner and Marc Fischer and Martin Vechev},
    year={2022},
    eprint={2212.06094},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{Bommasani2021OnTO,
  title={On the Opportunities and Risks of Foundation Models},
  author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen A. Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel J. Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.07258}
}

@article{Chan2022TransformersGD,
  title={Transformers generalize differently from information stored in context vs in weights},
  author={Stephanie C. Y. Chan and Ishita Dasgupta and Junkyung Kim and Dharshan Kumaran and Andrew Kyle Lampinen and Felix Hill},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.05675}
}


@article{Ullman2012TheoryLA,
  title={Theory learning as stochastic search in the language of thought},
  author={Tomer David Ullman and Noah D. Goodman and Joshua B. Tenenbaum},
  journal={Cognitive Development},
  year={2012},
  volume={27},
  pages={455-480}
}

@article{Yang2017OneMF,
  title={One model for the learning of language},
  author={Yuan Yang and Steven T. Piantadosi},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2017},
  volume={119}
}

@article{Dehaene2022SymbolsAM,
  title={Symbols and mental programs: a hypothesis about human singularity},
  author={Stanislas Dehaene and Fosca Al Roumi and Yair Lakretz and Samuel Planton and Mathias Sabl{\'e}-Meyer},
  journal={Trends in Cognitive Sciences},
  year={2022},
  volume={26},
  pages={751-766}
}

@article{Olsson2022IncontextLA,
  title={In-context Learning and Induction Heads},
  author={Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and T. J. Henighan and Benjamin Mann and Amanda Askell and Yushi Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and John Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom B. Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Christopher Olah},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.11895}
}



@article{DBLP:journals/corr/abs-2212-10001,
  author    = {Boshi Wang and
               Sewon Min and
               Xiang Deng and
               Jiaming Shen and
               You Wu and
               Luke Zettlemoyer and
               Huan Sun},
  title     = {Towards Understanding Chain-of-Thought Prompting: An Empirical Study
               of What Matters},
  journal   = {CoRR},
  volume    = {abs/2212.10001},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.10001},
  doi       = {10.48550/arXiv.2212.10001},
  eprinttype = {arXiv},
  eprint    = {2212.10001},
  timestamp = {Tue, 03 Jan 2023 15:59:43 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-10001.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{solomonoff1964formal,
	author={Ray J. Solomonoff},
	year={1964},
       title={A Formal Theory of Inductive Inference, Part II},
	journal={Information and Control},
		volume={7},
		number={2},
		pages={224--254}
}

@article{Dingle2018InputoutputMA,
  title={Input–output maps are strongly biased towards simple outputs},
  author={Kamaludin Dingle and Chico Q. Camargo and Ard A. Louis},
  journal={Nature Communications},
  year={2018},
  volume={9}
}

@book{LiVitanyi2008introduction,
	author={Ming Li and Paul Vit{\'a}nyi},
	title={An Introduction to Kolmogorov Complexity and its Applications},
	edition={3},
	year={2008}
}

@article{Hewitt2019DesigningAI,
  title={Designing and Interpreting Probes with Control Tasks},
  author={John Hewitt and Percy Liang},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.03368}
}



@inproceedings{DBLP:conf/emnlp/VoitaT20,
  author    = {Elena Voita and
               Ivan Titov},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {Information-Theoretic Probing with Minimum Description Length},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {183--196},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.14},
  doi       = {10.18653/v1/2020.emnlp-main.14},
  timestamp = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/VoitaT20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Goodman2008ARA,
  title={A Rational Analysis of Rule-Based Concept Learning},
  author={Noah D. Goodman and Joshua B. Tenenbaum and Jacob Feldman and Thomas L. Griffiths},
  journal={Cognitive science},
  year={2008},
  volume={32 1},
  pages={
          108-54
        }
}

@article{Ellis2021DreamCoderBI,
  title={DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning},
  author={Kevin Ellis and Catherine Wong and Maxwell Nye and Mathias Sabl{\'e}-Meyer and Lucas Morales and Luke B. Hewitt and Luc Cary and Armando Solar-Lezama and Joshua B. Tenenbaum},
  journal={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  year={2021}
}

@article{Power2022GrokkingGB,
  title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  author={Alethea Power and Yuri Burda and Harrison Edwards and Igor Babuschkin and Vedant Misra},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.02177}
}


@article{yang203large,
  title={Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning},
  author = {Xinyi Wang and Wanrong Zhu and William Yang Wang},
  year={2023},
  journal={ArXiv},
  volume={arXiv:2301.11916}
}

@article{Abney1996StochasticAG,
  title={Stochastic Attribute-Value Grammars},
  author={Steven P. Abney},
  journal={ArXiv},
  year={1996},
  volume={cmp-lg/9610003}
}

@article{Chi1999StatisticalPO,
  title={Statistical Properties of Probabilistic Context-Free Grammars},
  author={Zhiyi Chi},
  journal={Comput. Linguistics},
  year={1999},
  volume={25},
  pages={131-160}
}

@inproceedings{Hunter2013DistributionsOM,
  title={Distributions on Minimalist Grammar Derivations},
  author={Tim Hunter and Chris Dyer},
  booktitle={Mathematics of Language},
  year={2013}
}

@inproceedings{Resnik1992ProbabilisticTG,
  title={Probabilistic Tree-Adjoining Grammar as a Framework for Statistical Natural Language Processing},
  author={Philip Resnik},
  booktitle={International Conference on Computational Linguistics},
  year={1992}
}

@inproceedings{Kallmeyer2010ParsingBC,
  title={Parsing Beyond Context-Free Grammars},
  author={Laura Kallmeyer},
  booktitle={Cognitive Technologies},
  year={2010}
}

@inproceedings{Torr2019WideCoverageNA,
  title={Wide-Coverage Neural A* Parsing for Minimalist Grammars},
  author={John Torr and Milos Stanojevi{\'c} and Mark Steedman and Shay B. Cohen},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}



@inproceedings{DBLP:conf/emnlp/LampinenDCMTCMW22,
  author    = {Andrew K. Lampinen and
               Ishita Dasgupta and
               Stephanie C. Y. Chan and
               Kory W. Mathewson and
               Mh Tessler and
               Antonia Creswell and
               James L. McClelland and
               Jane Wang and
               Felix Hill},
  editor    = {Yoav Goldberg and
               Zornitsa Kozareva and
               Yue Zhang},
  title     = {Can language models learn from explanations in context?},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
               2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022},
  pages     = {537--563},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.findings-emnlp.38},
  timestamp = {Tue, 07 Feb 2023 17:10:51 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/LampinenDCMTCMW22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Stabler1996DerivationalM,
  title={Derivational Minimalism},
  author={E. Stabler},
  booktitle={Logical Aspects of Computational Linguistics},
  year={1996}
}

@article{Marcus1998RethinkingEC,
  title={Rethinking Eliminative Connectionism},
  author={Gary F. Marcus},
  journal={Cognitive Psychology},
  year={1998},
  volume={37},
  pages={243-282}
}


@inproceedings{Kamp1993FromDT,
  title={From Discourse to Logic - Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory},
  author={Hans Kamp and Uwe Reyle},
  booktitle={Studies in Linguistics and Philosophy},
  year={1993}
}


@article{Seki1991OnMC,
  title={On Multiple Context-Free Grammars},
  author={Hiroyuki Seki and Takashi Matsumura and Mamoru Fujii and Tadao Kasami},
  journal={Theor. Comput. Sci.},
  year={1991},
  volume={88},
  pages={191-229}
}




@inproceedings{DBLP:conf/lacl/Michaelis01,
  author    = {Jens Michaelis},
  editor    = {Philippe de Groote and
               Glyn Morrill and
               Christian Retor{\'{e}}},
  title     = {Transforming Linear Context-Free Rewriting Systems into Minimalist
               Grammars},
  booktitle = {Logical Aspects of Computational Linguistics, 4th International Conference,
               {LACL} 2001, Le Croisic, France, June 27-29, 2001, Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {2099},
  pages     = {228--244},
  publisher = {Springer},
  year      = {2001},
  url       = {https://doi.org/10.1007/3-540-48199-0\_14},
  doi       = {10.1007/3-540-48199-0\_14},
  timestamp = {Tue, 14 May 2019 10:00:51 +0200},
  biburl    = {https://dblp.org/rec/conf/lacl/Michaelis01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{DBLP:conf/lacl/Michaelis98,
  author    = {Jens Michaelis},
  editor    = {Michael Moortgat},
  title     = {Derivational Minimalism Is Mildly Context-Sensitive},
  booktitle = {Logical Aspects of Computational Linguistics, Third International
               Conference, LACL'98, Grenoble, France, December 14-16, 1998, Selected
               Papers},
  series    = {Lecture Notes in Computer Science},
  volume    = {2014},
  pages     = {179--198},
  publisher = {Springer},
  year      = {1998},
  url       = {https://doi.org/10.1007/3-540-45738-0\_11},
  doi       = {10.1007/3-540-45738-0\_11},
  timestamp = {Tue, 14 May 2019 10:00:51 +0200},
  biburl    = {https://dblp.org/rec/conf/lacl/Michaelis98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@book{Chesi2022MinimalistPE,
  title={Minimalist parsing},
   editor={Robert C. Berwick and Edward P. Stabler},
  year={2019}
}

@book{Deransart1988AttributeGD,
  title={Attribute Grammars: Definitions, Systems and Bibliography},
  author={Pierre Deransart and Martin Jourdan and Bernard Lorho},
  year={1988}
}

@article{Giegerich1988CompositionAE,
  title={Composition and evaluation of attribute coupled grammars},
  author={Robert Giegerich},
  journal={Acta Informatica},
  year={1988},
  volume={25},
  pages={355-423}
}

@article{Engelfriet1986TheCO,
  title={The complexity of Languages Generated by Attribute Grammars},
  author={Joost Engelfriet},
  journal={SIAM J. Comput.},
  year={1986},
  volume={15},
  pages={70-86}
}



@article{Hale2006UncertaintyAT,
  title={Uncertainty About the Rest of the Sentence},
  author={John Hale},
  journal={Cognitive science},
  year={2006},
  volume={30 4},
  pages={
          643-72
        }
}

@thesis{Pollard1984GeneralizedPS,
  title={Generalized phrase structure grammars, head grammars, and natural language},
  author={Carl Pollard},
  address={Stanford University},
  year={1984}
}

@article{VijayShanker1994TheEO,
  title={The equivalence of four extensions of context-free grammars},
  author={K. Vijay-Shanker and David J. Weir},
  journal={Mathematical systems theory},
  year={1994},
  volume={27},
  pages={511-546}
}

@inproceedings{Joshi1985NaturalLP,
  title={Natural language parsing: Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions?},
  author={Aravind K. Joshi},
  year={1985}
}

@inproceedings{VijayShanker1987CharacterizingSD,
  title={Characterizing Structural Descriptions produced by Various Grammatical Formalisms},
  author={K. Vijay-Shanker and David J. Weir and Aravind K. Joshi},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={1987}
}

@inproceedings{Chomsky1992TheMP,
  title={The Minimalist Program},
  author={Noam Chomsky},
  year={1992}
}

@article{Yang2022UnsupervisedDC,
  title={Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars},
  author={Songlin Yang and R. Levy and Yoon Kim},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.09140}
}

@article{Kim2020COGSAC,
  title={COGS: A Compositional Generalization Challenge Based on Semantic Interpretation},
  author={Najoung Kim and Tal Linzen},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.05465}
}

@thesis{Weir1988CharacterizingMC,
  title={Characterizing mildly context-sensitive grammar formalisms},
  author={David J. Weir},
  url={http://users.sussex.ac.uk/~davidw/resources/papers/dissertation.pdf},
  institution={University of Pennsylvania},
  year={1988}
}

@article{Chi1999StatisticalPO,
  title={Statistical Properties of Probabilistic Context-Free Grammars},
  author={Zhiyi Chi},
  journal={Comput. Linguistics},
  year={1999},
  volume={25},
  pages={131-160}
}

@inproceedings{Manning1999FoundationsOS,
  title={Foundations of statistical natural language processing},
  author={Christopher D. Manning and Hinrich Sch{\"u}tze},
  year={1999}
}


@article{Cremers1996CountingCC,
  title={Counting Coordination Categorially},
  author={Crit Cremers and Maarten Hijzelendoorn},
  journal={ArXiv},
  year={1996},
  volume={cmp-lg/9605011}
}


@unpublished{Frank2000CoordinatingPA,
  title={Coordinating Parsing and Grammar},
  author={Robert Frank},
  year={2000}
}

@article{Steedman1990GappingAC,
  title={Gapping as constituent coordination},
  author={Mark Steedman},
  journal={Linguistics and Philosophy},
  year={1990},
  volume={13},
  pages={207-263}
}


@inproceedings{Ross1970GAPPINGAT,
  title={GAPPING AND THE ORDER OF CONSTITUENTS},
  author={John Robert Ross},
  year={1970}
}

@article{Steedman1990GappingAC,
  title={Gapping as constituent coordination},
  author={Mark Steedman},
  journal={Linguistics and Philosophy},
  year={1990},
  volume={13},
  pages={207-263}
}

@book{Heim1998SemanticsIG,
  title={Semantics in generative grammar},
  author={Irene Heim and Angelika Kratzer},
  year={1998}
}

@inproceedings{Boullier2000RangeCG,
  title={Range Concatenation Grammars},
  author={Pierre Boullier},
  booktitle={International Workshop/Conference on Parsing Technologies},
  year={2000}
}

@inproceedings{Boullier1999ChineseNM,
  title={Chinese Numbers, MIX, Scrambling, and Range Concatenation Grammars},
  author={Pierre Boullier},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  year={1999}
}




@article{DBLP:journals/corr/abs-2212-09597,
  author    = {Shuofei Qiao and
               Yixin Ou and
               Ningyu Zhang and
               Xiang Chen and
               Yunzhi Yao and
               Shumin Deng and
               Chuanqi Tan and
               Fei Huang and
               Huajun Chen},
  title     = {Reasoning with Language Model Prompting: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2212.09597},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.09597},
  doi       = {10.48550/arXiv.2212.09597},
  eprinttype = {arXiv},
  eprint    = {2212.09597},
  timestamp = {Thu, 26 Jan 2023 17:27:32 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-09597.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}






@article{DBLP:journals/corr/abs-2212-10403,
  author    = {Jie Huang and
               Kevin Chen{-}Chuan Chang},
  title     = {Towards Reasoning in Large Language Models: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2212.10403},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.10403},
  doi       = {10.48550/arXiv.2212.10403},
  eprinttype = {arXiv},
  eprint    = {2212.10403},
  timestamp = {Wed, 04 Jan 2023 16:01:37 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-10403.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{DBLP:journals/corr/abs-2112-00114,
  author    = {Maxwell I. Nye and
               Anders Johan Andreassen and
               Guy Gur{-}Ari and
               Henryk Michalewski and
               Jacob Austin and
               David Bieber and
               David Dohan and
               Aitor Lewkowycz and
               Maarten Bosma and
               David Luan and
               Charles Sutton and
               Augustus Odena},
  title     = {Show Your Work: Scratchpads for Intermediate Computation with Language
               Models},
  journal   = {CoRR},
  volume    = {abs/2112.00114},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.00114},
  eprinttype = {arXiv},
  eprint    = {2112.00114},
  timestamp = {Fri, 29 Apr 2022 17:42:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-00114.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}






@article{DBLP:journals/corr/abs-2212-10559,
  author    = {Damai Dai and
               Yutao Sun and
               Li Dong and
               Yaru Hao and
               Zhifang Sui and
               Furu Wei},
  title     = {Why Can {GPT} Learn In-Context? Language Models Secretly Perform Gradient
               Descent as Meta-Optimizers},
  journal   = {CoRR},
  volume    = {abs/2212.10559},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.10559},
  doi       = {10.48550/arXiv.2212.10559},
  eprinttype = {arXiv},
  eprint    = {2212.10559},
  timestamp = {Wed, 04 Jan 2023 16:01:37 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-10559.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{DBLP:conf/naacl/ShinLAKKKCLPHS22,
  author    = {Seongjin Shin and
               Sang{-}Woo Lee and
               Hwijeen Ahn and
               Sungdong Kim and
               HyoungSeok Kim and
               Boseop Kim and
               Kyunghyun Cho and
               Gichang Lee and
               Woo{-}Myoung Park and
               Jung{-}Woo Ha and
               Nako Sung},
  editor    = {Marine Carpuat and
               Marie{-}Catherine de Marneffe and
               Iv{\'{a}}n Vladimir Meza Ru{\'{\i}}z},
  title     = {On the Effect of Pretraining Corpora on In-context Learning by a Large-scale
               Language Model},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL} 2022, Seattle, WA, United States, July 10-15, 2022},
  pages     = {5168--5186},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://doi.org/10.18653/v1/2022.naacl-main.380},
  doi       = {10.18653/v1/2022.naacl-main.380},
  timestamp = {Tue, 07 Feb 2023 11:24:37 +0100},
  biburl    = {https://dblp.org/rec/conf/naacl/ShinLAKKKCLPHS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{Rissanen2010MinimumDL,
  title={Minimum Description Length Principle},
  author={Jorma Rissanen},
  booktitle={Encyclopedia of Machine Learning},
  year={2010}
}



@inproceedings{DBLP:conf/acl/Mueller0LWS22,
  author    = {Aaron Mueller and
               Robert Frank and
               Tal Linzen and
               Luheng Wang and
               Sebastian Schuster},
  editor    = {Smaranda Muresan and
               Preslav Nakov and
               Aline Villavicencio},
  title     = {Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive
               Bias to Sequence-to-sequence Models},
  booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2022,
               Dublin, Ireland, May 22-27, 2022},
  pages     = {1352--1368},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://doi.org/10.18653/v1/2022.findings-acl.106},
  doi       = {10.18653/v1/2022.findings-acl.106},
  timestamp = {Wed, 07 Dec 2022 23:10:03 +0100},
  biburl    = {https://dblp.org/rec/conf/acl/Mueller0LWS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}






@inproceedings{DBLP:conf/emnlp/PapadimitriouJ20,
  author    = {Isabel Papadimitriou and
               Dan Jurafsky},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {Learning Music Helps You Read: Using Transfer to Study Linguistic
               Structure in Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {6829--6839},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.554},
  doi       = {10.18653/v1/2020.emnlp-main.554},
  timestamp = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/PapadimitriouJ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{wang-etal-2022-iteratively,
    title = "Iteratively Prompt Pre-trained Language Models for Chain of Thought",
    author = "Wang, Boshi  and
      Deng, Xiang  and
      Sun, Huan",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.174",
    pages = "2714--2730",
    abstract = "While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex {\&} multi-step reasoning. Similar to how humans develop a {``}chain of thought{''} for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference. We identify key limitations of existing prompting methods, namely they are either restricted to queries with a single identifiable relation/predicate, or being agnostic to input contexts, which makes it difficult to capture variabilities across different inference steps. We propose an iterative context-aware prompter, which addresses these limitations by learning to dynamically synthesize prompts conditioned on the current step{'}s contexts. Experiments on three datasets involving multi-step reasoning show the effectiveness of the iterative scheme and the context-aware prompter design.",
}


@inproceedings{Joshi1990TheCO,
  title={The convergence of mildly context-sensitive grammar formalisms},
  author={Aravind K. Joshi and K. Vijay-Shanker},
  year={1990}
}


@inproceedings{Mery2006ACS,
  title={A Case Study of the Convergence of Mildly Context-Sensitive Formalisms for Natural Language Syntax: from Minimalist Grammars to Multiple Context-Free Grammars},
  author={Bruno Mery and M. Amblard and Ir{\`e}ne Durand and Christian Retor{\'e}},
  year={2006}
}


@article{Kallmeyer2010OnMC,
  title={On Mildly Context-Sensitive Non-Linear Rewriting},
  author={Laura Kallmeyer},
  journal={Research on Language and Computation},
  year={2010},
  volume={8},
  pages={341-363}
}

@inproceedings{Joshi1985NaturalLP,
  title={Natural language parsing: Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions?},
  author={Aravind K. Joshi},
  year={1985}
}




@article{DBLP:journals/corr/abs-1710-11350,
  author    = {Eva Portelance and
               Leon Bergen and
               Chris Bruno and
               Timothy J. O'Donnell},
  title     = {Mildly context sensitive grammar induction and variational bayesian
               inference},
  journal   = {CoRR},
  volume    = {abs/1710.11350},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.11350},
  eprinttype = {arXiv},
  eprint    = {1710.11350},
  timestamp = {Mon, 13 Aug 2018 16:46:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-11350.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}






@article{DBLP:journals/corr/abs-2212-10502,
  author    = {Li Du and
               Lucas Torroba Hennigen and
               Tiago Pimentel and
               Clara Meister and
               Jason Eisner and
               Ryan Cotterell},
  title     = {A Measure-Theoretic Characterization of Tight Language Models},
  journal   = {CoRR},
  volume    = {abs/2212.10502},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.10502},
  doi       = {10.48550/arXiv.2212.10502},
  eprinttype = {arXiv},
  eprint    = {2212.10502},
  timestamp = {Wed, 04 Jan 2023 16:01:37 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-10502.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{DBLP:conf/emnlp/ZhaoT20,
  author    = {Yanpeng Zhao and
               Ivan Titov},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {Visually Grounded Compound PCFGs},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {4369--4379},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.354},
  doi       = {10.18653/v1/2020.emnlp-main.354},
  timestamp = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/ZhaoT20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{DBLP:journals/corr/abs-2103-02298,
  author    = {Yanpeng Zhao and
               Ivan Titov},
  title     = {An Empirical Study of Compound PCFGs},
  journal   = {CoRR},
  volume    = {abs/2103.02298},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.02298},
  eprinttype = {arXiv},
  eprint    = {2103.02298},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-02298.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{DBLP:conf/acl/KimDR19,
  author    = {Yoon Kim and
               Chris Dyer and
               Alexander M. Rush},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {Compound Probabilistic Context-Free Grammars for Grammar Induction},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {2369--2385},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1228},
  doi       = {10.18653/v1/p19-1228},
  timestamp = {Sat, 09 Apr 2022 12:33:45 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/KimDR19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{Goldberg2006ConstructionsAW,
  title={Constructions at Work: The Nature of Generalization in Language},
  author={Adele E. Goldberg},
  year={2006}
}

@article{Barron1991MinimumCD,
  title={Minimum complexity density estimation},
  author={Andrew R. Barron and Thomas M. Cover},
  journal={IEEE Trans. Inf. Theory},
  year={1991},
  volume={37},
  pages={1034-1054}
}


@inproceedings{Gulwani2011AutomatingSP,
  title={Automating string processing in spreadsheets using input-output examples},
  author={Sumit Gulwani},
  booktitle={ACM-SIGACT Symposium on Principles of Programming Languages},
  year={2011}
}

@article{Lake2015HumanlevelCL,
  title={Human-level concept learning through probabilistic program induction},
  author={Brenden M. Lake and Ruslan Salakhutdinov and Joshua B. Tenenbaum},
  journal={Science},
  year={2015},
  volume={350},
  pages={1332 - 1338}
}



@inproceedings{DBLP:conf/iclr/PatelP22,
  author    = {Roma Patel and
               Ellie Pavlick},
  title     = {Mapping Language Models to Grounded Conceptual Spaces},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=gJcEM8sxHK},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/PatelP22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{Sgaard2023GroundingTV,
  title={Grounding the Vector Space of an Octopus: Word Meaning from Raw Text},
  author={Anders S{\o}gaard},
  journal={Minds and Machines},
  year={2023}
}

@article{Gonen2022DemystifyingPI,
  title={Demystifying Prompts in Language Models via Perplexity Estimation},
  author={Hila Gonen and Srini Iyer and Terra Blevins and Noah A. Smith and Luke Zettlemoyer},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.04037}
}




@article{DBLP:journals/jair/HupkesDMB20,
  author    = {Dieuwke Hupkes and
               Verna Dankers and
               Mathijs Mul and
               Elia Bruni},
  title     = {Compositionality Decomposed: How do Neural Networks Generalise?},
  journal   = {J. Artif. Intell. Res.},
  volume    = {67},
  pages     = {757--795},
  year      = {2020},
  url       = {https://doi.org/10.1613/jair.1.11674},
  doi       = {10.1613/jair.1.11674},
  timestamp = {Wed, 15 Apr 2020 17:53:14 +0200},
  biburl    = {https://dblp.org/rec/journals/jair/HupkesDMB20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{Hutter2004UniversalAI,
  title={Universal Artificial Intelligence},
  author={Marcus Hutter},
  booktitle={Texts in Theoretical Computer Science. An EATCS Series},
  year={2004}
}

@article{Suzgun2022ChallengingBT,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Mirac Suzgun and Nathan Scales and Nathanael Scharli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed Huai-hsin Chi and Denny Zhou and Jason Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.09261}
}



@inproceedings{DBLP:conf/iclr/0005HPGH22,
  author    = {Samuel M{\"{u}}ller and
               Noah Hollmann and
               Sebastian Pineda{-}Arango and
               Josif Grabocka and
               Frank Hutter},
  title     = {Transformers Can Do Bayesian Inference},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=KSugKcbNf9},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/0005HPGH22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}






@inproceedings{DBLP:conf/icml/NguyenG22,
  author    = {Tung Nguyen and
               Aditya Grover},
  editor    = {Kamalika Chaudhuri and
               Stefanie Jegelka and
               Le Song and
               Csaba Szepesv{\'{a}}ri and
               Gang Niu and
               Sivan Sabato},
  title     = {Transformer Neural Processes: Uncertainty-Aware Meta Learning Via
               Sequence Modeling},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {16569--16594},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/nguyen22b.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/NguyenG22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@unpublished{pesut2022models,
	note={https://www.alignmentforum.org/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of},
	title={Who models the models that model models? An exploration of GPT-3's in-context model fitting ability},
	author={Lovre Pesut},
	year={2022}
}



@inproceedings{DBLP:conf/emnlp/RazeghiL0022,
  author    = {Yasaman Razeghi and
               Robert L. Logan IV and
               Matt Gardner and
               Sameer Singh},
  editor    = {Yoav Goldberg and
               Zornitsa Kozareva and
               Yue Zhang},
  title     = {Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
               2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022},
  pages     = {840--854},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.findings-emnlp.59},
  timestamp = {Tue, 07 Feb 2023 17:10:51 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/RazeghiL0022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@unpublished{rong21extrapolating,
title={Extrapolating to Unnatural Language Processing with GPT-3's In-context Learning: The Good, the Bad, and the Mysterious},
author={Frieda Rong},
note={\url{https://ai.stanford.edu/blog/in-context-learning/#:~:text=Extrapolating%20to%20Unnatural%20Language%20Processing%20with%20GPT-3%27s%20In-context,GPT-3%27s%20ability%20to%20extrapolate%20to%20less%20natural%20inputs}},
year={2021}
}






@article{DBLP:journals/corr/abs-2206-06565,
  author    = {Tuan Dinh and
               Yuchen Zeng and
               Ruisu Zhang and
               Ziqian Lin and
               Michael Gira and
               Shashank Rajput and
               Jy{-}yong Sohn and
               Dimitris S. Papailiopoulos and
               Kangwook Lee},
  title     = {{LIFT:} Language-Interfaced Fine-Tuning for Non-Language Machine Learning
               Tasks},
  journal   = {CoRR},
  volume    = {abs/2206.06565},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2206.06565},
  doi       = {10.48550/arXiv.2206.06565},
  eprinttype = {arXiv},
  eprint    = {2206.06565},
  timestamp = {Tue, 21 Jun 2022 17:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2206-06565.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{Liu2022TowardsUG,
  title={Towards Understanding Grokking: An Effective Theory of Representation Learning},
  author={Ziming Liu and Ouail Kitouni and Niklas Stefan Nolte and Eric J. Michaud and Max Tegmark and Mike Williams},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.10343}
}



@article{DBLP:journals/corr/abs-2302-03025,
  author    = {Bilal Chughtai and
               Lawrence Chan and
               Neel Nanda},
  title     = {A Toy Model of Universality: Reverse Engineering How Networks Learn
               Group Operations},
  journal   = {CoRR},
  volume    = {abs/2302.03025},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2302.03025},
  doi       = {10.48550/arXiv.2302.03025},
  eprinttype = {arXiv},
  eprint    = {2302.03025},
  timestamp = {Fri, 10 Feb 2023 12:26:39 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2302-03025.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{DBLP:journals/corr/abs-2301-07067,
  author    = {Yingcong Li and
               M. Emrullah Ildiz and
               Dimitris S. Papailiopoulos and
               Samet Oymak},
  title     = {Transformers as Algorithms: Generalization and Implicit Model Selection
               in In-context Learning},
  journal   = {CoRR},
  volume    = {abs/2301.07067},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2301.07067},
  doi       = {10.48550/arXiv.2301.07067},
  eprinttype = {arXiv},
  eprint    = {2301.07067},
  timestamp = {Thu, 19 Jan 2023 15:40:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2301-07067.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{DBLP:journals/corr/abs-2301-07067,
  author    = {Yingcong Li and
               M. Emrullah Ildiz and
               Dimitris S. Papailiopoulos and
               Samet Oymak},
  title     = {Transformers as Algorithms: Generalization and Implicit Model Selection
               in In-context Learning},
  journal   = {CoRR},
  volume    = {abs/2301.07067},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2301.07067},
  doi       = {10.48550/arXiv.2301.07067},
  eprinttype = {arXiv},
  eprint    = {2301.07067},
  timestamp = {Thu, 19 Jan 2023 15:40:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2301-07067.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{Hollmann2022TabPFNAT,
  title={TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second},
  author={Noah Hollmann and Samuel Muller and Katharina Eggensperger and Frank Hutter},
  year={2022}
}

@article{Shieber1985EvidenceAT,
  title={Evidence against the context-freeness of natural language},
  author={Stuart M. Shieber},
  journal={Linguistics and Philosophy},
  year={1985},
  volume={8},
  pages={333-343}
}
@article{Jger2012FormalLT,
  title={Formal language theory: refining the Chomsky hierarchy},
  author={Gerhard J{\"a}ger and James Rogers},
  journal={Philosophical Transactions of the Royal Society B: Biological Sciences},
  year={2012},
  volume={367},
  pages={1956 - 1970}
}

@book{Steedman2004TheSP,
  title={The syntactic process},
  author={Mark Steedman},
  year={2001}
}

@incollection{Stabler2011ComputationalPO,
  title={Computational Perspectives on Minimalism},
  author={Edward Stabler},
  booktitle={The Oxford Handbook of Linguistic Minimalism},
  editor={Cedric Boeckx},
  year={2011}
}
@inproceedings{Clark2021StrongLO,
  title={Strong Learning of some Probabilistic Multiple Context-Free Grammars},
  author={Alexander Clark},
  booktitle={Mathematics of Language},
  year={2021}
}
@inproceedings{Wei2023LargerLM,
  title={Larger language models do in-context learning differently},
  author={Jerry W. Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
  year={2023}
}


@article{kobele2005features,
  title={Features moving madly: A formal perspective on feature percolation in the minimalist program},
  author={Kobele, Gregory M},
  journal={Research on Language and Computation},
  volume={3},
  number={2-3},
  pages={391--410},
  year={2005},
  publisher={Springer}
}



@book{pollard1994head,
  title={Head-driven phrase structure grammar},
  author={Pollard, Carl and Sag, Ivan A},
  year={1994},
  publisher={University of Chicago Press}
}



@inproceedings{Mller2020GrammaticalT,
  title={Grammatical theory: From transformational grammar to constraint-based approaches},
  edition={4},
  author={Stefan M{\"u}ller},
  publisher={Language Science Press},
  year={2020},
  url={https://library.oapen.org/handle/20.500.12657/46939}
}


@article{Jger2012FormalLT,
  title={Formal language theory: refining the Chomsky hierarchy},
  author={Gerhard J{\"a}ger and James Rogers},
  journal={Philosophical Transactions of the Royal Society B: Biological Sciences},
  year={2012},
  volume={367},
  pages={1956 - 1970}
}

@inproceedings{Bresnan1987LexicalfunctionalG,
  title={Lexical Functional Syntax},
  author={Joan Bresnan},
  year={2000}
}

@inproceedings{Kim2008EnglishSA,
  title={English Syntax: An Introduction},
  author={Jong-Bok Kim and Peter Sells},
  year={2008}
}

@inproceedings{Boas2012SignBasedCG,
  title={Sign-Based Construction Grammar},
  author={Hans Christian Boas and Ivan A. Sag},
  year={2012}
}

@inproceedings{Ginzburg2001InterrogativeIT,
  title={Interrogative Investigations: The Form, Meaning, and Use of English Interrogatives},
  author={Jonathan Ginzburg and Ivan A. Sag},
  year={2001}
}

@inproceedings{Kallmeyer2004LTAGSW,
  title={LTAG Semantics with Semantic Unification},
  author={Laura Kallmeyer and Maribel Romero},
  booktitle={Tag},
  year={2004}
}

@inproceedings{Ginzburg2012TheIS,
  title={The interactive stance : meaning for conversation},
  author={Jonathan Ginzburg},
  year={2012}
}


@misc{wies2023learnability,
    title={The Learnability of In-Context Learning},
    author={Noam Wies and Yoav Levine and Amnon Shashua},
    year={2023},
    eprint={2303.07895},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@misc{zhao2023transformers,
    title={Do Transformers Parse while Predicting the Masked Word?},
    author={Haoyu Zhao and Abhishek Panigrahi and Rong Ge and Sanjeev Arora},
    year={2023},
    eprint={2303.08117},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@article{Ye2023InContextIL,
  title={In-Context Instruction Learning},
  author={Seonghyeon Ye and Hyeonbin Hwang and Sohee Yang and Hyeongu Yun and Yireun Kim and Minjoon Seo},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.14691}
}



@article{Wang2022TowardsUC,
  title={Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},
  author={Boshi Wang and Sewon Min and Xiang Deng and Jiaming Shen and You Wu and Luke Zettlemoyer and Huan Sun},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.10001}
}


@inproceedings{DBLP:conf/emnlp/BhattamishraAG20,
  author    = {Satwik Bhattamishra and
               Kabir Ahuja and
               Navin Goyal},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {On the Ability and Limitations of Transformers to Recognize Formal
               Languages},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {7096--7116},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.576},
  doi       = {10.18653/v1/2020.emnlp-main.576},
  timestamp = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/BhattamishraAG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{DBLP:journals/tacl/HaoAF22,
  author    = {Yiding Hao and
               Dana Angluin and
               Robert Frank},
  title     = {Formal Language Recognition by Hard Attention Transformers: Perspectives
               from Circuit Complexity},
  journal   = {Trans. Assoc. Comput. Linguistics},
  volume    = {10},
  pages     = {800--810},
  year      = {2022},
  url       = {https://transacl.org/ojs/index.php/tacl/article/view/3765},
  timestamp = {Thu, 27 Oct 2022 08:34:08 +0200},
  biburl    = {https://dblp.org/rec/journals/tacl/HaoAF22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{DBLP:conf/acl/YaoPPN20,
  author    = {Shunyu Yao and
               Binghui Peng and
               Christos H. Papadimitriou and
               Karthik Narasimhan},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {Self-Attention Networks Can Process Bounded Hierarchical Languages},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational
               Linguistics and the 11th International Joint Conference on Natural
               Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
               Event, August 1-6, 2021},
  pages     = {3770--3785},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.acl-long.292},
  doi       = {10.18653/v1/2021.acl-long.292},
  timestamp = {Thu, 12 Aug 2021 17:51:20 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/YaoPPN20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{DBLP:conf/acl/0001C22,
  author    = {David Chiang and
               Peter Cholak},
  editor    = {Smaranda Muresan and
               Preslav Nakov and
               Aline Villavicencio},
  title     = {Overcoming a Theoretical Limitation of Self-Attention},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
               May 22-27, 2022},
  pages     = {7654--7664},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://doi.org/10.18653/v1/2022.acl-long.527},
  doi       = {10.18653/v1/2022.acl-long.527},
  timestamp = {Mon, 01 Aug 2022 16:27:46 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/0001C22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{hahn2022resource,
	title={A resource-rational model of human processing of recursive linguistic structure},
	year={2022},
        journal = {Proceedings of the National Academy of Sciences of the United States of America},
	volume={119},
	number={43},
	pages={e2122602119},
	author={Michael Hahn and Richard Futrell and Roger Levy and Edward Gibson},
	doi={https://doi.org/10.1073/pnas.2122602119},
	url={https://doi.org/10.1073/pnas.2122602119},
	month={oct},
	github={https://gitlab.com/m-hahn/resource-rational-surprisal},
        supplement = {https://www.pnas.org/doi/suppl/10.1073/pnas.2122602119/suppl_file/pnas.2122602119.sapp.pdf},
	commentary={https://www.pnas.org/doi/10.1073/pnas.2217108119},
	png={figs/hahn2022resource.png}
}




@article{DBLP:journals/corr/abs-2108-07258,
  author    = {Rishi Bommasani and
               Drew A. Hudson and
               Ehsan Adeli and
               Russ B. Altman and
               Simran Arora and
               Sydney von Arx and
               Michael S. Bernstein and
               Jeannette Bohg and
               Antoine Bosselut and
               Emma Brunskill and
               Erik Brynjolfsson and
               Shyamal Buch and
               Dallas Card and
               Rodrigo Castellon and
               Niladri S. Chatterji and
               Annie S. Chen and
               Kathleen Creel and
               Jared Quincy Davis and
               Dorottya Demszky and
               Chris Donahue and
               Moussa Doumbouya and
               Esin Durmus and
               Stefano Ermon and
               John Etchemendy and
               Kawin Ethayarajh and
               Li Fei{-}Fei and
               Chelsea Finn and
               Trevor Gale and
               Lauren Gillespie and
               Karan Goel and
               Noah D. Goodman and
               Shelby Grossman and
               Neel Guha and
               Tatsunori Hashimoto and
               Peter Henderson and
               John Hewitt and
               Daniel E. Ho and
               Jenny Hong and
               Kyle Hsu and
               Jing Huang and
               Thomas Icard and
               Saahil Jain and
               Dan Jurafsky and
               Pratyusha Kalluri and
               Siddharth Karamcheti and
               Geoff Keeling and
               Fereshte Khani and
               Omar Khattab and
               Pang Wei Koh and
               Mark S. Krass and
               Ranjay Krishna and
               Rohith Kuditipudi and
               et al.},
  title     = {On the Opportunities and Risks of Foundation Models},
  journal   = {CoRR},
  volume    = {abs/2108.07258},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.07258},
  eprinttype = {arXiv},
  eprint    = {2108.07258},
  note = {Version 3, dated 2022. Retrieved Fri, 17 Feb 2023 09:02:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-07258.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{DBLP:journals/tacl/MerrillSS22,
  author    = {William Merrill and
               Ashish Sabharwal and
               Noah A. Smith},
  title     = {Saturated Transformers are Constant-Depth Threshold Circuits},
  journal   = {Trans. Assoc. Comput. Linguistics},
  volume    = {10},
  pages     = {843--856},
  year      = {2022},
  url       = {https://transacl.org/ojs/index.php/tacl/article/view/3465},
  timestamp = {Wed, 26 Oct 2022 16:52:10 +0200},
  biburl    = {https://dblp.org/rec/journals/tacl/MerrillSS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{DBLP:conf/lics/0001KS18,
  author       = {Michael Hahn and
                  Andreas Krebs and
                  Howard Straubing},
  editor       = {Anuj Dawar and
                  Erich Gr{\"{a}}del},
  title        = {Wreath Products of Distributive Forest Algebras},
  booktitle    = {Proceedings of the 33rd Annual {ACM/IEEE} Symposium on Logic in Computer
                  Science, {LICS} 2018, Oxford, UK, July 09-12, 2018},
  pages        = {512--520},
  publisher    = {{ACM}},
  year         = {2018},
  url          = {https://doi.org/10.1145/3209108.3209158},
  doi          = {10.1145/3209108.3209158},
  timestamp    = {Wed, 21 Nov 2018 12:44:18 +0100},
  biburl       = {https://dblp.org/rec/conf/lics/0001KS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/mfcs/HahnKLL15,
  author       = {Michael Hahn and
                  Andreas Krebs and
                  Klaus{-}J{\"{o}}rn Lange and
                  Michael Ludwig},
  editor       = {Giuseppe F. Italiano and
                  Giovanni Pighizzini and
                  Donald Sannella},
  title        = {Visibly Counter Languages and the Structure of NC\({}^{\mbox{1}}\)},
  booktitle    = {Mathematical Foundations of Computer Science 2015 - 40th International
                  Symposium, {MFCS} 2015, Milan, Italy, August 24-28, 2015, Proceedings,
                  Part {II}},
  series       = {Lecture Notes in Computer Science},
  volume       = {9235},
  pages        = {384--394},
  publisher    = {Springer},
  year         = {2015},
  url          = {https://doi.org/10.1007/978-3-662-48054-0\_32},
  doi          = {10.1007/978-3-662-48054-0\_32},
  timestamp    = {Tue, 14 May 2019 10:00:37 +0200},
  biburl       = {https://dblp.org/rec/conf/mfcs/HahnKLL15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@book{cassandras2007introduction,
	title="Introduction to Discrete Event Systems",
	author="Christos G. {Cassandras} and Stephane {Lafortune}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1587663028",
	year="2007"
}

@book{oksendal2013stochastic,
	title="Stochastic Differential Equations: An Introduction with Applications",
	author="Bernt Karsten {Øksendal}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2170316986",
	year="2013"
}

@article{giscard2013evaluating,
	title="Evaluating Matrix Functions by Resummations on Graphs: The Method of Path-Sums",
	author="Pierre-Louis {Giscard} and S. J. {Thwaite} and D. {Jaksch}",
	journal="SIAM Journal on Matrix Analysis and Applications",
	volume="34",
	number="2",
	pages="445--469",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3099383185",
	year="2013"
}

@article{grassmann1977transient,
	title="Transient solutions in markovian queueing systems",
	author="Winfried K. {Grassmann}",
	journal="Computers \& Operations Research",
	volume="4",
	number="1",
	pages="47--53",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1967986582",
	year="1977"
}

@book{odonnell2014analysis,
	title="Analysis of Boolean Functions",
	author="Ryan {O'Donnell}",
	publisher={Cambridge University Press},
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1891181203",
	year="2014"
}

@article{furst1984parity,
  title={Parity, circuits, and the polynomial-time hierarchy},
  author={Furst, Merrick and Saxe, James B and Sipser, Michael},
  journal={Mathematical systems theory},
  volume={17},
  number={1},
  pages={13--27},
  year={1984},
  publisher={Springer}
}

@book{mitzenmacherprobability,
  title={Probability and Computing},
  year={2005},
  author={Mitzenmacher, Michael and Upfal, Eli},
  publisher={Cambridge University Press},
  address={Cambridge}
}

@book{minsky1969perceptrons,
author = {Minsky, Marvin and Papert, Seymour A.},
title = {Perceptrons: An Introduction to Computational Geometry},
year = {1969},
publisher = {The MIT Press},
}



@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={740--745},
  year={2018}
}


@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of {LSTM}s to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}


@article{suzgun2019evaluating,
  title={On Evaluating the Generalization of {LSTM} Models in Formal Languages},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
  journal={Proceedings of the Society for Computation in Linguistics (SCiL)},
  pages={277--286},
  year={2019}
}


@article{merrill2019sequential,
  title={Sequential neural networks as automata},
  author={Merrill, William},
  journal={arXiv preprint arXiv:1906.01615},
  year={2019}
}



@article{bernardy2018can,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  author={Bernardy, Jean-Philippe},
  journal={LiLT (Linguistic Issues in Language Technology)},
  volume={16},
  number={1},
  year={2018}
}


@article{perez2019turing,
  title={On the Turing Completeness of Modern Neural Network Architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}



@article{chen2017recurrent,
  title={Recurrent neural networks as weighted language recognizers},
  author={Chen, Yining and Gilroy, Sorcha and Maletti, Andreas and May, Jonathan and Knight, Kevin},
  journal={arXiv preprint arXiv:1711.05408},
  year={2017}
}



@article{siegelman1991neural,
  title={On    the    computational    power of   neural   nets},
  author={Siegelman, Hava and Sontag, Eduardo D},
  journal={Journal of Computer and System Sciences},
  year={1995},
  volume={50},
  issue={1},
  pages={132--150}
}



@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}


@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@article{tran2018importance,
  title={The importance of being recurrent for modeling hierarchical structure},
  author={Tran, Ke and Bisazza, Arianna and Monz, Christof},
  journal={arXiv preprint arXiv:1803.03585},
  year={2018}
}

@article{yang2019assessing,
  title={Assessing the Ability of Self-Attention Networks to Learn Word Order},
  author={Yang, Baosong and Wang, Longyue and Wong, Derek F and Chao, Lidia S and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:1906.00592},
  year={2019}
}

@inproceedings{yao1986separating,
author={A. C. {Yao}},
booktitle={26th Annual Symposium on Foundations of Computer Science (sfcs 1985)},
title={Separating the polynomial-time hierarchy by oracles},
year={1985},
volume={},
number={},
pages={1-10},
keywords={Polynomials;Circuits;Erbium;Frequency selective surfaces;Computer science;Complexity theory;Computational complexity},
doi={10.1109/SFCS.1985.49},
ISSN={0272-5428},
month={Oct},}

@inproceedings{futrell2019neural,
    title = "Neural language models as psycholinguistic subjects: Representations of syntactic state",
    author = "Futrell, Richard  and
      Wilcox, Ethan  and
      Morita, Takashi  and
      Qian, Peng  and
      Ballesteros, Miguel  and
      Levy, Roger",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1004",
    doi = "10.18653/v1/N19-1004",
    pages = "32--42",
    abstract = "We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.",
}


@article{hastad1994optimal,
  title={Optimal Depth, Very Small Size Circuits for Symmetrical Functions in AC0},
  author={Hastad, Johan and Wegener, Ingo and Wurm, Norbert and Yi, Sang-Zin},
  journal={Information and Computation},
  volume={108},
  number={2},
  pages={200--211},
  year={1994},
  publisher={Elsevier}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{kirov2012processing,
  title={Processing of nested and cross-serial dependencies: an automaton perspective on SRN behaviour},
  author={Kirov, Christo and Frank, Robert},
  journal={Connection Science},
  volume={24},
  number={1},
  pages={1--24},
  year={2012},
  publisher={Taylor \& Francis}
}


@inproceedings{kirov2013bayesian,
  title={Bayesian speech production: Evidence from latency and hyperarticulation},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={35},
  number={35},
  year={2013}
}


@inproceedings{kirov2012specificity,
  title={The specificity of online variation in speech production},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}


@article{shen2018reinforced,
  title={Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Wang, Sen and Zhang, Chengqi},
  journal={arXiv preprint arXiv:1801.10296},
  year={2018}
}


@inproceedings{shen2018disan,
  title={Disan: Directional self-attention network for rnn/cnn-free language understanding},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}


@article{chen2018best,
  title={The best of both worlds: Combining recent advances in neural machine translation},
  author={Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Schuster, Mike and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:1804.09849},
  year={2018}
}



@article{hao2019modeling,
  title={Modeling Recurrence for Transformer},
  author={Hao, Jie and Wang, Xing and Yang, Baosong and Wang, Longyue and Zhang, Jinfeng and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:1904.03092},
  year={2019}
}


@article{paulus2017deep,
  title={A deep reinforced model for abstractive summarization},
  author={Paulus, Romain and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1705.04304},
  year={2017}
}


@article{cheng2016long,
  title={Long short-term memory-networks for machine reading},
  author={Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
  journal={arXiv preprint arXiv:1601.06733},
  year={2016}
}


@article{lin2017structured,
  title={A structured self-attentive sentence embedding},
  author={Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.03130},
  year={2017}
}


@article{parikh2016decomposable,
  title={A decomposable attention model for natural language inference},
  author={Parikh, Ankur P and T{\"a}ckstr{\"o}m, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1606.01933},
  year={2016}
}


@article{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}


@inproceedings{clark2019bert,
  title={What Does {BERT} Look At? An Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of BlackboxNLP 2019},
  year={2019}
}



@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Cohen, William W and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{gulordava2018colorless,
  title={Colorless green recurrent networks dream hierarchically},
  author={Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  journal={arXiv preprint arXiv:1803.11138},
  year={2018}
}

@article{linzen2016assessing,
	Author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	Journal = {Transactions of the Association for Computational Linguistics},
	Title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
    Volume = {4},
    Pages = {521--535},
	Year = {2016}
}

@inproceedings{skachkova2018closing,
  title={Closing Brackets with Recurrent Neural Networks},
  author={Skachkova, Natalia and Trost, Thomas and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}


@article{tabor2000fractal,
  title={Fractal encoding of context-free grammars in connectionist networks},
  author={Tabor, Whitney},
  journal={Expert Systems},
  volume={17},
  number={1},
  pages={41--56},
  year={2000},
  publisher={Wiley Online Library}
}


@article{gruning2006stack,
  title={Stack-like and queue-like dynamics in recurrent neural networks},
  author={Gr{\"u}ning, Andr{\'e}},
  journal={Connection Science},
  volume={18},
  number={1},
  pages={23--42},
  year={2006},
  publisher={Taylor \& Francis}
}


@incollection{chomsky1963algebraic,
  title={The algebraic theory of context-free languages},
  author={Chomsky, Noam and Sch{\"u}tzenberger, Marcel P},
  booktitle={Studies in Logic and the Foundations of Mathematics},
  volume={35},
  pages={118--161},
  year={1963},
  publisher={Elsevier}
}

@article{gers2001lstm,
  title={{LSTM} recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, E},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}
@inproceedings{kalinke1998computation,
  title={Computation in recurrent neural networks: From counters to iterated function systems},
  author={Kalinke, Yvonne and Lehmann, Helko},
  booktitle={Australian Joint Conference on Artificial Intelligence},
  pages={179--190},
  year={1998},
  organization={Springer}
}



@article{everaert2015structures,
  title={Structures, not strings: linguistics as part of the cognitive sciences},
  author={Everaert, Martin BH and Huybregts, Marinus AC and Chomsky, Noam and Berwick, Robert C and Bolhuis, Johan J},
  journal={Trends in cognitive sciences},
  volume={19},
  number={12},
  pages={729--743},
  year={2015},
  publisher={Elsevier}
}





@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}


@article{cartling2008implicit,
  title={On the implicit acquisition of a context-free grammar by a simple recurrent neural network},
  author={Cartling, Bo},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1527--1537},
  year={2008},
  publisher={Elsevier}
}

@incollection{montague1973proper,
  title={The proper treatment of quantification in ordinary English},
  author={Montague, Richard},
  booktitle={Approaches to natural language},
  pages={221--242},
  year={1973},
  publisher={Springer}
}

@article{marvin2018targeted,
  title={Targeted syntactic evaluation of language models},
  author={Marvin, Rebecca and Linzen, Tal},
  journal={arXiv preprint arXiv:1808.09031},
  year={2018}
}

@article{lewis2005activation,
  title={An activation-based model of sentence processing as skilled memory retrieval},
  author={Lewis, Richard L and Vasishth, Shravan},
  journal={Cognitive science},
  volume={29},
  number={3},
  pages={375--419},
  year={2005},
  publisher={Wiley Online Library}
}


@article{lin2019open,
  title={Open Sesame: Getting Inside {BERT}'s Linguistic Knowledge},
  author={Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
  journal={arXiv preprint arXiv:1906.01698},
  year={2019}
}

@inproceedings{devlin2019bert,
	title="{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author="Jacob {Devlin} and Ming-Wei {Chang} and Kenton {Lee} and Kristina {Toutanova}",
	booktitle="NAACL-HLT 2019: Annual Conference of the North American Chapter of the Association for Computational Linguistics",
	pages="4171--4186",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963341956",
	year="2019"
}

@article{tenney2019bert,
  title={Bert rediscovers the classical nlp pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  journal={arXiv preprint arXiv:1905.05950},
  year={2019}
}


@incollection{miller-finitary-1963,
        title = {Finitary models of language users},
        booktitle = {Handbook of {Mathematical} {Psychology}},
        author = {Miller, George A and Chomsky, Noam},
        year = {1963}
}

@article{gibson1999memory,
  title={Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical},
  author={Gibson, Edward and Thomas, James},
  journal={Language and Cognitive Processes},
  volume={14},
  number={3},
  pages={225--248},
  year={1999},
  publisher={Taylor \& Francis}
}


@inproceedings{gopalan2016smooth,
  title={Smooth boolean functions are easy: Efficient algorithms for low-sensitivity functions},
  author={Gopalan, Parikshit and Nisan, Noam and Servedio, Rocco A and Talwar, Kunal and Wigderson, Avi},
  booktitle={Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
  pages={59--70},
  year={2016},
  organization={ACM}
}


@article{de2018deep,
  title={Deep neural networks are biased towards simple functions},
  author={De Palma, Giacomo and Kiani, Bobak Toussi and Lloyd, Seth},
  journal={arXiv preprint arXiv:1812.10156},
  year={2018}
}

@inproceedings{gilmer2015new,
  title={A new approach to the sensitivity conjecture},
  author={Gilmer, Justin and Kouck{\`y}, Michal and Saks, Michael E},
  booktitle={Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
  pages={247--254},
  year={2015},
  organization={ACM}
}


@article{rossman2018average,
  title={The average sensitivity of bounded-depth formulas},
  author={Rossman, Benjamin},
  journal={computational complexity},
  volume={27},
  number={2},
  pages={209--223},
  year={2018},
  publisher={Springer}
}



@article{boppana1997average,
  title={The average sensitivity of bounded-depth circuits},
  author={Boppana, Ravi B},
  journal={Information processing letters},
  volume={63},
  number={5},
  pages={257--261},
  year={1997},
  publisher={Elsevier}
}



@article{miller2018recurrent,
  title={When Recurrent Models Don't Need To Be Recurrent},
  author={Miller, John and Hardt, Moritz},
  journal={arXiv preprint arXiv:1805.10369},
  year={2018}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}




@article{barrington1992regular,
  title={Regular languages in {NC}1},
  author={Barrington, David A Mix and Compton, Kevin and Straubing, Howard and Th{\'e}rien, Denis},
  journal={Journal of Computer and System Sciences},
  volume={44},
  number={3},
  pages={478--499},
  year={1992},
  publisher={Elsevier}
}

@article{korsky2019computational,
  title={On the Computational Power of RNNs},
  author={Samuel A. Korsky and Robert C. Berwick},
  journal={arXiv preprint arXiv:1906.06349},
  year={2019}
}


@article{venkatesh1998pseudo,
  title={Pseudo-average block sensitivity equals average sensitivity},
  author={Venkatesh, Srinivasan},
  journal={Information processing letters},
  volume={68},
  number={2},
  pages={93--95},
  year={1998},
  publisher={Elsevier}
}

@book{odonnell2014analysis,
	title="Analysis of Boolean Functions",
	author="Ryan {O'Donnell}",
	publisher={Cambridge University Press},
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1891181203",
	year="2014"
}


@article{bernasconi1996sensitivity,
	title="Sensitivity vs. block sensitivity (an average-case study)",
	author="A. {Bernasconi}",
	journal="Information Processing Letters",
	volume="59",
	number="3",
	pages="151--157",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2079828635",
	year="1996"
}


@article{nisan1991crew,
	title="{CREW} {PRAMs} and decision trees",
	author="Noam {Nisan}",
	journal="SIAM Journal on Computing",
	volume="20",
	number="6",
	pages="999--1007",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1867758558",
	year="1991"
}


@article{venkatesh1998pseudo,
	title="Pseudo-average block sensitivity equals average sensitivity",
	author="S. {Venkatesh}",
	journal="Information Processing Letters",
	volume="68",
	number="2",
	pages="93--95",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2045361120",
	year="1998"
}

@inproceedings{kahn1988the,
	title="The influence of variables on Boolean functions",
	author="J. {Kahn} and G. {Kalai} and N. {Linial}",
	booktitle="[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science",
	pages="68--80",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2103749128",
	year="1988"
}

@book{jukna2012boolean,
	title="Boolean Function Complexity: Advances and Frontiers",
	author="Stasys {Jukna}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/112858668",
	year="2012"
}


@inproceedings{kaushik2020learning,
	title="Learning The Difference That Makes A Difference With Counterfactually-Augmented Data",
	author="Divyansh {Kaushik} and Eduard {Hovy} and Zachary {Lipton}",
	booktitle="ICLR 2020: Eighth International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2994934025",
	year="2020"
}

@article{gardner2020evaluating,
	title="Evaluating {NLP} Models via Contrast Sets",
	author="Matt {Gardner} and Yoav {Artzi} and Victoria {Basmova} and Jonathan {Berant} and Ben {Bogin} and Sihao {Chen} and Pradeep {Dasigi} and Dheeru {Dua} and Yanai {Elazar} and Ananth {Gottumukkala} and Nitish {Gupta} and Hanna {Hajishirzi} and Gabriel {Ilharco} and Daniel {Khashabi} and Kevin {Lin} and Jiangming {Liu} and Nelson F. {Liu} and Phoebe {Mulcaire} and Qiang {Ning} and Sameer {Singh} and Noah A. {Smith} and Sanjay {Subramanian} and Reut {Tsarfaty} and Eric {Wallace} and Ally {Zhang} and Ben {Zhou}",
	journal="arXiv preprint arXiv:2004.02709",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3014564055",
	year="2020"
}

@inproceedings{jia2017adversarial,
	title="Adversarial Examples for Evaluating Reading Comprehension Systems",
	author="Robin {Jia} and Percy {Liang}",
	booktitle="Proceedings of the 2017 Conference on Empirical Methods in Natural
      Language Processing",
	pages="2021--2031",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963969878",
	year="2017"
}


@article{hatami2010variations,
	title="Variations on the Sensitivity Conjecture",
	author="Pooya {Hatami} and Raghav {Kulkarni} and Denis {Pankratov}",
	journal="Theory of Computing",
	volume="4",
	pages="1--27",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963994076",
	year="2010"
}

@inproceedings{kim2014convolutional,
	title="Convolutional Neural Networks for Sentence Classification",
	author="Yoon {Kim}",
	booktitle="Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	pages="1746--1751",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1832693441",
	year="2014"
}



@inproceedings{levy2018long,
	title="Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum",
	author="Omer {Levy} and Kenton {Lee} and Nicholas {FitzGerald} and Luke {Zettlemoyer}",
	booktitle="ACL 2018: 56th Annual Meeting of the Association for Computational Linguistics",
	pages="732--739",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963410683",
	year="2018"
}

@article{hahn2020theoretical,
	title="Theoretical Limitations of Self-Attention in Neural Sequence Models",
	author="Michael {Hahn}",
	journal="Transactions of the Association for Computational Linguistics",
	volume="8",
	pages="156--171",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3014096773",
	year="2020"
}

@inproceedings{vaswani2017attention,
	title="Attention is All You Need",
	author="Ashish {Vaswani} and Noam {Shazeer} and Niki {Parmar} and Jakob {Uszkoreit} and Llion {Jones} and Aidan N. {Gomez} and Lukasz {Kaiser} and Illia {Polosukhin}",
	booktitle="Proceedings of the 31st International Conference on {N}eural {I}nformation {P}rocessing {S}ystems",
	pages="5998--6008",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963403868",
	year="2017"
}

@article{hochreiter1997long,
	title="Long short-term memory",
	author="Sepp {Hochreiter} and Jürgen {Schmidhuber}",
	journal="Neural Computation",
	volume="9",
	number="8",
	pages="1735--1780",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2064675550",
	year="1997"
}


@inproceedings{wang2019glue,
	title="GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
	author="Alex {Wang} and Amanpreet {Singh} and Julian {Michael} and Felix {Hill} and Omer {Levy} and Samuel R. {Bowman}",
	booktitle="ICLR 2019: 7th International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963310665",
	year="2019"
}



@article{liu2019roberta,
	title="{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach",
	author="Yinhan {Liu} and Myle {Ott} and Naman {Goyal} and Jingfei {Du} and Mandar {Joshi} and Danqi {Chen} and Omer {Levy} and Mike {Lewis} and Luke {Zettlemoyer} and Veselin {Stoyanov}",
	journal="arXiv preprint arXiv:1907.11692",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2965373594",
	year="2019"
}

@inproceedings{yang2019xlnet,
	title="{XLNet}: Generalized Autoregressive Pretraining for Language Understanding",
	author="Zhilin {Yang} and Zihang {Dai} and Yiming {Yang} and Jaime {Carbonell} and Ruslan {Salakhutdinov} and Quoc V {Le}",
	booktitle="NeurIPS 2019: Thirty-third Conference on {N}eural {I}nformation {P}rocessing {S}ystems",
	pages="5753--5763",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2970597249",
	year="2019"
}

@article{wolf2019huggingface,
	title="HuggingFace's Transformers: State-of-the-art Natural Language Processing",
	author="Thomas {Wolf} and Lysandre {Debut} and Victor {Sanh} and Julien {Chaumond} and Clement {Delangue} and Anthony {Moi} and Pierric {Cistac} and Tim {Rault} and Rémi {Louf} and Morgan {Funtowicz} and Jamie {Brew}",
	journal="arXiv preprint arXiv:1910.03771",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2980282514",
	year="2019"
}

@inproceedings{ott2019fairseq,
	title="fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
	author="Myle {Ott} and Sergey {Edunov} and Alexei {Baevski} and Angela {Fan} and Sam {Gross} and Nathan {Ng} and David {Grangier} and Michael {Auli}",
	booktitle="NAACL-HLT 2019: Annual Conference of the North American Chapter of the Association for Computational Linguistics",
	pages="48--53",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2933138175",
	year="2019"
}


@inproceedings{wang2019superglue,
	title="SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
	author="Alex {Wang} and Yada {Pruksachatkun} and Nikita {Nangia} and Amanpreet {Singh} and Julian {Michael} and Felix {Hill} and Omer {Levy} and Samuel R. {Bowman}",
	booktitle="Advances in {N}eural {I}nformation {P}rocessing {S}ystems",
	pages="3266--3280",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2990704537",
	year="2019"
}



@inproceedings{silveira2014a,
	title="A Gold Standard Dependency Corpus for English",
	author="Natalia {Silveira} and Timothy {Dozat} and Marie-Catherine de {Marneffe} and Samuel {Bowman} and Miriam {Connor} and John {Bauer} and Chris {Manning}",
	booktitle="Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)",
	pages="2897--2904",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2250263931",
	year="2014"
}


@inproceedings{marvin2018targeted,
	title="Targeted Syntactic Evaluation of Language Models",
	author="Rebecca {Marvin} and Tal {Linzen}",
	booktitle="EMNLP 2018: 2018 Conference on Empirical Methods in Natural Language Processing",
	pages="1192--1202",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2888922637",
	year="2018"
}

@inproceedings{Gauthier:et-al:2020:syntaxgym,
  author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
  title = {{SyntaxGym}: An online platform for targeted evaluation of language models},
  booktitle = {Proceedings of the Association for Computational Linguistics: System Demonstrations (ACL 2020)},
  year = {2020}
}

@article{hu2020a,
	title="A Closer Look at the Performance of Neural Language Models on Reflexive Anaphor Licensing",
	author="Jennifer {Hu} and Sherry Y {Chen} and Roger P. {Levy}",
	journal="Proceedings of the Society for Computation in Linguistics",
	volume="3",
	number="1",
	pages="382--392",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2994726368",
	year="2020"
}

@inproceedings{socher2013recursive,
	title="Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
	author="Richard {Socher} and Alex {Perelygin} and Jean {Wu} and Jason {Chuang} and Christopher D. {Manning} and Andrew {Ng} and Christopher {Potts}",
	booktitle="Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
	pages="1631--1642",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2251939518",
	year="2013"
}

@article{dagan2009recognizing,
	title="Recognizing textual entailment: Rational, evaluation and approaches",
	author="Ido {Dagan} and Bill {Dolan} and Bernardo {Magnini} and Dan {Roth}",
	journal="Natural Language Engineering",
	volume="15",
	number="4",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2118707092",
	year="2009"
}

@inproceedings{howard2018universal,
	title="Universal Language Model Fine-tuning for Text Classification",
	author="Jeremy {Howard} and Sebastian {Ruder}",
	booktitle="ACL 2018: 56th Annual Meeting of the Association for Computational Linguistics",
	volume="1",
	pages="328--339",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963026768",
	year="2018"
}

@inproceedings{peters2018deep,
	title="DEEP CONTEXTUALIZED WORD REPRESENTATIONS",
	author="Matthew E {Peters} and Mark {Neumann} and Mohit {Iyyer} and Matt {Gardner} and Christopher {Clark} and Kenton {Lee} and Luke {Zettlemoyer}",
	booktitle="NAACL HLT 2018: 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	volume="1",
	pages="2227--2237",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2962739339",
	year="2018"
}



@inproceedings{wang2012baselines,
	title="Baselines and Bigrams: Simple, Good Sentiment and Topic Classification",
	author="Sida {Wang} and Christopher {Manning}",
	booktitle="Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
	volume="2",
	pages="90--94",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2154359981",
	year="2012"
}


@article{wiebe2005annotating,
	title="Annotating Expressions of Opinions and Emotions in Language",
	author="Janyce {Wiebe} and Theresa {Wilson} and Claire {Cardie}",
	journal="language resources and evaluation",
	volume="39",
	number="2",
	pages="165--210",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2014902591",
	year="2005"
}


@inproceedings{hu2004mining,
	title="Mining and summarizing customer reviews",
	author="Minqing {Hu} and Bing {Liu}",
	booktitle="Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining",
	pages="168--177",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2160660844",
	year="2004"
}

@inproceedings{pang2004a,
	title="A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts",
	author="Bo {Pang} and Lillian {Lee}",
	booktitle="Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL'04), Main Volume",
	pages="271--278",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2114524997",
	year="2004"
}

@inproceedings{pang2005seeing,
	title="Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales",
	author="Bo {Pang} and Lillian {Lee}",
	booktitle="Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)",
	pages="115--124",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2163455955",
	year="2005"
}


@inproceedings{lei2018simple,
	title="Simple Recurrent Units for Highly Parallelizable Recurrence.",
	author="Tao {Lei} and Yu {Zhang} and Sida I. {Wang} and Hui {Dai} and Yoav {Artzi}",
	booktitle="Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	pages="4470--4481",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963174729",
	year="2018"
}


@article{qi2020stanza,
	title="Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
	author="Peng {Qi} and Yuhao {Zhang} and Yuhui {Zhang} and Jason {Bolton} and Christopher D. {Manning}",
	journal="arXiv preprint arXiv:2003.07082",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3011573503",
	year="2020"
}

@inproceedings{qi2018universal,
	title="Universal Dependency Parsing from Scratch",
	author="Peng {Qi} and Timothy {Dozat} and Yuhao {Zhang} and Christopher D. {Manning}",
	booktitle="Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
	pages="160--170",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2899024931",
	year="2018"
}

@book{minsky1969perceptrons,
	title="Perceptrons: An Introduction to Computational Geometry",
	author="Marvin Lee {Minsky} and Seymour {Papert}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2086789740",
	year="1969"
}

@inproceedings{hsieh-etal-2019-robustness,
    title = "On the Robustness of Self-Attentive Models",
    author = "Hsieh, Yu-Lun  and
      Cheng, Minhao  and
      Juan, Da-Cheng  and
      Wei, Wei  and
      Hsu, Wen-Lian  and
      Hsieh, Cho-Jui",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1147",
    doi = "10.18653/v1/P19-1147",
    pages = "1520--1529",
    abstract = "This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims.",
}

@article{kulkarni2016on,
	title="On Fractional Block Sensitivity.",
	author="Raghav {Kulkarni} and Avishay {Tal}",
	journal="Chicago Journal of Theoretical Computer Science",
	volume="2016",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2578797763",
	year="2016"
}

@book{minsky1969perceptrons,
	title="Perceptrons: An Introduction to Computational Geometry",
	author="Marvin Lee {Minsky} and Seymour {Papert}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2086789740",
	year="1969"
}

@article{greenbury2016genetic,
	title="Genetic Correlations Greatly Increase Mutational Robustness and Can Both Reduce and Enhance Evolvability",
	author="Sam F. {Greenbury} and Steffen {Schaper} and Sebastian E. {Ahnert} and Ard A. {Louis}",
	journal="PLOS Computational Biology",
	volume="12",
	number="3",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2210383493",
	year="2016"
}

@article{wegener1987the,
	title="The Complexity of Symmetric Boolean Functions",
	author="Ingo {Wegener}",
	journal="Computation Theory and Logic, In Memory of Dieter Rödding",
	pages="433--442",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1587602953",
	year="1987"
}

@article{kulkarni2016on,
	title="On Fractional Block Sensitivity.",
	author="Raghav {Kulkarni} and Avishay {Tal}",
	journal="Chicago Journal of Theoretical Computer Science",
	volume="2016",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2578797763",
	year="2016"
}

@inproceedings{cheng1990an,
	title="An entropy measure for the complexity of multi-output Boolean functions",
	author="Kwang-Ting {Cheng} and Vishwani D. {Agrawal}",
	booktitle="Proceedings of the 27th ACM/IEEE Design Automation Conference on ",
	pages="302--305",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2148482302",
	year="1990"
}

@article{franco2005role,
	title="Role of function complexity and network size in the generalization ability of feedforward networks",
	author="Leonardo {Franco} and José M. {Jerez} and José M. {Bravo}",
	journal="international conference on artificial neural networks",
	pages="1--8",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1533354148",
	year="2005"
}

@inproceedings{franco2004on,
	title="On a generalization complexity measure for Boolean functions",
	author="L. {Franco} and M. {Anthony}",
	booktitle="2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)",
	volume="2",
	pages="973--978",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2117549320",
	year="2004"
}

@article{franco2006generalization,
	title="Generalization ability of Boolean functions implemented in feedforward neural networks",
	author="Leonardo {Franco}",
	journal="Neurocomputing",
	volume="70",
	number="1",
	pages="351--361",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2122613489",
	year="2006"
}

@article{franco2006the,
	title="The influence of oppositely classified examples on the generalization complexity of Boolean functions",
	author="L. {Franco} and M. {Anthony}",
	journal="IEEE Transactions on Neural Networks",
	volume="17",
	number="3",
	pages="578--590",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2123660490",
	year="2006"
}

@article{gomez2014the,
	title="The generalization complexity measure for continuous input data.",
	author="Iván {G{\'o}mez} and Sergio A. {Cannas} and Omar {Osenda} and José M. {Jerez} and Leonardo {Franco}",
	journal="The Scientific World Journal",
	volume="2014",
	pages="815156--815156",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2053167324",
	year="2014"
}

@article{franco2003the,
	title="The influence of opposite examples and randomness on the generalization complexity of Boolean functions",
	author="Leonardo {Franco} and Martin {Anthony}",
	journal="",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2112845300",
	year="2003"
}

@inproceedings{gomez2010extension,
	title="Extension of the generalization complexity measure to real valued input data sets",
	author="Iván {G{\'o}mez} and Leonardo {Franco} and José M. {Jerez} and José L. {Subirats}",
	booktitle="ISNN'10 Proceedings of the 7th international conference on Advances in Neural Networks - Volume Part I",
	pages="86--94",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2130900802",
	year="2010"
}

@article{franco2001a,
	title="A measure for the complexity of Boolean functions related to their implementation in neural networks",
	author="Leonardo {Franco}",
	journal="arXiv preprint arXiv:cond-mat/0111169",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1509570633",
	year="2001"
}

@article{chande1997number,
	title="Number of Restrictions of a Boolean Function act as a Complexity Measure",
	author="Vinay {Chande} and P G {Poonacha}",
	journal="Iete Journal of Research",
	volume="43",
	number="1",
	pages="3--10",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2078940883",
	year="1997"
}


@article{nakkiran2019sgd,
	title="{SGD} on Neural Networks Learns Functions of Increasing Complexity.",
	author="Preetum {Nakkiran} and Gal {Kaplun} and Dimitris {Kalimeris} and Tristan {Yang} and Benjamin L. {Edelman} and Fred {Zhang} and Boaz {Barak}",
	journal="arXiv preprint arXiv:1905.11604",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2947785987",
	year="2019"
}

@inproceedings{valle-perez2019deep,
	title="Deep learning generalizes because the parameter-function map is biased towards simple functions",
	author="Guillermo {Valle-Perez} and Chico Q. {Camargo} and Ard {Louis}",
	booktitle="ICLR 2019: 7th International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2964122761",
	year="2019"
}

@article{linial1993constant,
	title="Constant depth circuits, Fourier transform, and learnability",
	author="Nathan {Linial} and Yishay {Mansour} and Noam {Nisan}",
	journal="Journal of the ACM",
	volume="40",
	number="3",
	pages="607--620",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2042194938",
	year="1993"
}

@inproceedings{kahn1988the,
	title="The influence of variables on Boolean functions",
	author="J. {Kahn} and G. {Kalai} and N. {Linial}",
	booktitle="[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science",
	pages="68--80",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2103749128",
	year="1988"
}

@article{samorodnitsky2009gowers,
	title="Gowers Uniformity, Influence of Variables, and PCPs",
	author="Alex {Samorodnitsky} and Luca {Trevisan}",
	journal="SIAM Journal on Computing",
	volume="39",
	number="1",
	pages="323--360",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2055711302",
	year="2009"
}

@article{bruck1992polynomial,
	title="Polynomial threshold functions, AC 0 functions, and spectral norms",
	author="Jehoshua {Bruck} and Roman {Smolensky}",
	journal="SIAM Journal on Computing",
	volume="21",
	number="1",
	pages="33--42",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2117508909",
	year="1992"
}

@article{torvik2002minimizing,
	title="Minimizing the Average Query Complexity of Learning Monotone Boolean Functions",
	author="Vetle I. {Torvik} and Evangelos {Triantaphyllou}",
	journal="Informs Journal on Computing",
	volume="14",
	number="2",
	pages="144--174",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2151393344",
	year="2002"
}

@article{anthony1995on,
	title="On specifying Boolean functions by labelled examples",
	author="Martin {Anthony} and Graham {Brightwell} and John {Shawe-Taylor}",
	journal="Discrete Applied Mathematics",
	volume="61",
	number="1",
	pages="1--25",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1974820192",
	year="1995"
}

@article{franco2001generalization,
	title="Generalization properties of modular networks: implementing the parity function",
	author="L. {Franco} and S.A. {Cannas}",
	journal="IEEE Transactions on Neural Networks",
	volume="12",
	number="6",
	pages="1306--1313",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2155258457",
	year="2001"
}

@article{gál2002a,
	title="A Theorem on Sensitivity and Applications in Private Computation",
	author="Anna {Gál} and Adi {Rosén}",
	journal="SIAM Journal on Computing",
	volume="31",
	number="5",
	pages="1424--1437",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1980995759",
	year="2002"
}



@inproceedings{nakkiran2020deep,
	title="Deep Double Descent: Where Bigger Models and More Data Hurt",
	author="Preetum {Nakkiran} and Gal {Kaplun} and Yamini {Bansal} and Tristan {Yang} and Boaz {Barak} and Ilya {Sutskever}",
	booktitle="ICLR 2020: Eighth International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2996603747",
	year="2020"
}

@inproceedings{lee2019wide,
	title="Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
	author="Jaehoon {Lee} and Lechao {Xiao} and Samuel {Schoenholz} and Yasaman {Bahri} and Roman {Novak} and Jascha {Sohl-Dickstein} and Jeffrey {Pennington}",
	booktitle="NeurIPS 2019: Thirty-third Conference on {N}eural {I}nformation {P}rocessing {S}ystems",
	pages="8572--8583",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2970217468",
	year="2019"
}

@inproceedings{chatterjee2020coherent,
	title="Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization",
	author="Sat {Chatterjee}",
	booktitle="ICLR 2020: Eighth International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2995185977",
	year="2020"
}

@article{cao2019towards,
	title="Towards Understanding the Spectral Bias of Deep Learning",
	author="Yuan {Cao} and Zhiying {Fang} and Yue {Wu} and Ding-Xuan {Zhou} and Quanquan {Gu}",
	journal="arXiv preprint arXiv:1912.01198",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2994110075",
	year="2019"
}

@inproceedings{gissin2020the,
	title="The Implicit Bias of Depth: How Incremental Learning Drives Generalization",
	author="Daniel {Gissin} and Shai {Shalev-Shwartz} and Amit {Daniely}",
	booktitle="ICLR 2020: Eighth International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2996210821",
	year="2020"
}




@inproceedings{rahaman2019on,
	title="On the Spectral Bias of Neural Networks",
	author="Nasim {Rahaman} and Aristide {Baratin} and Devansh {Arpit} and Felix {Draxler} and Min {Lin} and Fred {Hamprecht} and Yoshua {Bengio} and Aaron {Courville}",
	booktitle="ICML 2019: Thirty-sixth International Conference on Machine Learning",
	pages="5301--5310",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2897097528",
	year="2019"
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{jaeger2012formal,
	title="Formal language theory: refining the Chomsky hierarchy",
	author="Gerhard {Jäger} and James {Rogers}",
	journal="Philosophical Transactions of the Royal Society B",
	volume="367",
	number="1598",
	pages="1956--1970",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2115912164",
	year="2012"
}



@article{suzgun2019on,
	title="On Evaluating the Generalization of {LSTM} Models in Formal Languages",
	author="Mirac {Suzgun} and Yonatan {Belinkov} and Stuart M. {Shieber}",
	journal="Proceedings of the Society for Computation in Linguistics",
	volume="2",
	number="1",
	pages="277--286",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963723151",
	year="2019"
}


@article{merrill2020a,
	title="A Formal Hierarchy of RNN Architectures.",
	author="William {Merrill} and Gail {Weiss} and Yoav {Goldberg} and Roy {Schwartz} and Noah A. {Smith} and Eran {Yahav}",
	journal="arXiv preprint arXiv:2004.08500",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3017116062",
	year="2020"
}


@article{jeretic2020are,
	title="Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition.",
	author="Paloma {Jeretic} and Alex {Warstadt} and Suvrat {Bhooshan} and Adina {Williams}",
	journal="arXiv preprint arXiv:2004.03066",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3015830489",
	year="2020"
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@inproceedings{xu2019training,
  title={Training behavior of deep neural network in frequency domain},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Xiao, Yanyang},
  booktitle={International Conference on Neural Information Processing},
  pages={264--274},
  year={2019},
  organization={Springer}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  pages={9461--9471},
  year={2018}
}
@article{novak2018sensitivity,
  title={Sensitivity and generalization in neural networks: an empirical study},
  author={Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1802.08760},
  year={2018}
}

@article{wieting2015towards,
  title={Towards universal paraphrastic sentence embeddings},
  author={Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
  journal={arXiv preprint arXiv:1511.08198},
  year={2015}
}
@inproceedings{arora2016simple,
	title="A Simple but Tough-to-Beat Baseline for Sentence Embeddings",
	author="Sanjeev {Arora} and Yingyu {Liang} and Tengyu {Ma}",
	booktitle="ICLR 2017: International Conference on Learning Representations 2017",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2752172973",
	year="2017"
}

@article{gissin2019implicit,
  title={The Implicit Bias of Depth: How Incremental Learning Drives Generalization},
  author={Gissin, Daniel and Shalev-Shwartz, Shai and Daniely, Amit},
  journal={arXiv preprint arXiv:1909.12051},
  year={2019}
}

@inproceedings{horne1994bounds,
  title={Bounds on the complexity of recurrent neural network implementations of finite state machines},
  author={Horne, Bill G and Hush, Don R},
  booktitle={Advances in {N}eural Information Processing Systems},
  pages={359--366},
  year={1994}
}



@article{bourgain1992influence,
  title={The influence of variables in product spaces},
  author={Bourgain, Jean and Kahn, Jeff and Kalai, Gil and Katznelson, Yitzhak and Linial, Nathan},
  journal={Israel Journal of Mathematics},
  volume={77},
  number={1-2},
  pages={55--64},
  year={1992},
  publisher={Springer}
}

@article{hatami2009decision,
  title={Decision trees and influences of variables over product probability spaces},
  author={Hatami, Hamed},
  journal={Combinatorics, Probability and Computing},
  volume={18},
  number={3},
  pages={357--369},
  year={2009},
  publisher={Cambridge University Press}
}

@inproceedings{mossel2005noise,
  title={Noise stability of functions with low influences: invariance and optimality},
  author={Mossel, Elchanan and O'Donnell, Ryan and Oleszkiewicz, Krzysztof},
  booktitle={46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)},
  pages={21--30},
  year={2005},
  organization={IEEE}
}

@article{keller2011influences,
  title={On the influences of variables on Boolean functions in product spaces},
  author={Keller, Nathan},
  journal={Combinatorics, Probability and Computing},
  volume={20},
  number={1},
  pages={83--102},
  year={2011},
  publisher={Cambridge University Press}
}

@inproceedings{petrov2012a,
	title="A Universal Part-of-Speech Tagset",
	author="Slav {Petrov} and Dipanjan {Das} and Ryan {McDonald}",
	booktitle="Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)",
	pages="2089--2096",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2143995218",
	year="2012"
}


@inproceedings{nivre2016universal,
	title="Universal Dependencies v1: A Multilingual Treebank Collection",
	author="Joakim {Nivre} and Marie-Catherine de {Marneffe} and Filip {Ginter} and Yoav {Goldberg} and Jan {Hajic} and Christopher D. {Manning} and Ryan T. {McDonald} and Slav {Petrov} and Sampo {Pyysalo} and Natalia {Silveira} and Reut {Tsarfaty} and Daniel {Zeman}",
	booktitle="Tenth International Conference on Language Resources and Evaluation (LREC 2016)",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2579343286",
	year="2016"
}


@article{grimmett2016influence,
	title="Influence in product spaces",
	author="Geoffrey R. {Grimmett} and Svante {Janson} and James R. {Norris}",
	journal="Advances in Applied Probability",
	volume="48",
	pages="145--152",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2247076637",
	year="2016"
}

@article{graham2006influence,
	title="Influence and sharp-threshold theorems for monotonic measures",
	author="B. T. {Graham} and G. R. {Grimmett}",
	journal="Annals of Probability",
	volume="34",
	number="5",
	pages="1726--1745",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2034647305",
	year="2006"
}

@article{graham2011sharp,
	title="Sharp thresholds for the random-cluster and Ising models",
	author="Benjamin T. {Graham} and Geoffrey {Grimmett}",
	journal="Annals of Applied Probability",
	volume="21",
	number="1",
	pages="240--265",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2004060089",
	year="2011"
}

@inproceedings{rajpurkar2016squad,
	title="SQuAD: 100,000+ Questions for Machine Comprehension of Text",
	author="Pranav {Rajpurkar} and Jian {Zhang} and Konstantin {Lopyrev} and Percy {Liang}",
	booktitle="Proceedings of the 2016 Conference on Empirical Methods in Natural
      Language Processing",
	pages="2383--2392",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963748441",
	year="2016"
}

@inproceedings{williams2018a,
	title="A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE",
	author="Adina {Williams} and Nikita {Nangia} and Samuel {Bowman}",
	booktitle="NAACL HLT 2018: 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	volume="1",
	pages="1112--1122",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963846996",
	year="2018"
}

@inproceedings{levesque2012the,
	title="The Winograd schema challenge",
	author="Hector J. {Levesque} and Ernest {Davis} and Leora {Morgenstern}",
	booktitle="KR'12 Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning",
	pages="552--561",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1599016936",
	year="2012"
}

@article{warstadt2019neural,
	title="Neural Network Acceptability Judgments",
	author="Alex {Warstadt} and Amanpreet {Singh} and Samuel R. {Bowman}",
	journal="Transactions of the Association for Computational Linguistics",
	volume="7",
	pages="625--641",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2978670439",
	year="2019"
}

@incollection{dolan2005automatically,
  author    = {William B. Dolan and
               Chris Brockett},
  title     = {Automatically Constructing a Corpus of Sentential Paraphrases},
  booktitle = {Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP
               2005, Jeju Island, Korea, October 2005, 2005},
  publisher = {Asian Federation of Natural Language Processing},
  year      = {2005},
  url       = {https://www.aclweb.org/anthology/I05-5002/},
  timestamp = {Tue, 17 Sep 2019 17:11:58 +0200},
  biburl    = {https://dblp.org/rec/conf/acl-iwp/DolanB05.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bjerva2017cross,
	title="Cross-lingual Learning of Semantic Textual Similarity with Multilingual Word Representations",
	author="Johannes {Bjerva} and Robert {Östling}",
	journal="Proceedings of the 21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May 2017, Gothenburg, Sweden",
	number="131",
	pages="211--215",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2680186315",
	year="2017"
}

@inproceedings{cer2017semeval,
	title="SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
	author="Daniel M. {Cer} and Mona T. {Diab} and Eneko {Agirre} and Iñigo {Lopez-Gazpio} and Lucia {Specia}",
	booktitle="Proceedings of the 11th International Workshop on Semantic Evaluation
      (SemEval-2017)",
	pages="1--14",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2739351760",
	year="2017"
}

@article{chomsky1956three,
	title="Three models for the description of language",
	author="N. {Chomsky}",
	journal="IEEE Transactions on Information Theory",
	volume="2",
	number="3",
	pages="113--124",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2124479173",
	year="1956"
}

@book{li1993an,
	title="An introduction to Kolmogorov complexity and its applications",
	author="Ming {Li} and Paul {Vitányi}",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1638203394",
	publisher={Springer},
	year="1993"
}

@article{dingle2018input,
	title="Input-output maps are strongly biased towards simple outputs.",
	author="Kamaludin {Dingle} and Chico Q. {Camargo} and Ard A. {Louis}",
	journal="Nature Communications",
	volume="9",
	number="1",
	pages="761--761",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2794316594",
	year="2018"
}

@inproceedings{pennington2014glove,
	title="Glove: Global Vectors for Word Representation",
	author="Jeffrey {Pennington} and Richard {Socher} and Christopher {Manning}",
	booktitle="Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	pages="1532--1543",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2250539671",
	year="2014"
}

@inproceedings{ethayarajh2018unsupervised,
	title="Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline",
	author="Kawin {Ethayarajh}",
	booktitle="Proceedings of The Third Workshop on Representation Learning for NLP",
	pages="91--100",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2887005207",
	year="2018"
}

@inproceedings{kaushik2018how,
	title="How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks",
	author="Divyansh {Kaushik} and Zachary C. {Lipton}",
	booktitle="EMNLP 2018: 2018 Conference on Empirical Methods in Natural Language Processing",
	pages="5010--5015",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2962727366",
	year="2018"
}

@inproceedings{gururangan2018annotation,
	title="Annotation artifacts in natural language inference data",
	author="Suchin {Gururangan} and Swabha {Swayamdipta} and Omer {Levy} and Roy {Schwartz} and Samuel {Bowman} and Noah A. {Smith}",
	booktitle="NAACL HLT 2018: 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	volume="2",
	pages="107--112",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2962736243",
	year="2018"
}


@inproceedings{poliak2018hypothesis,
	title="Hypothesis Only Baselines in Natural Language Inference",
	author="Adam {Poliak} and Jason {Naradowsky} and Aparajita {Haldar} and Rachel {Rudinger} and Benjamin Van {Durme}",
	booktitle="
          Proceedings of the Seventh Joint Conference on Lexical and
          Computational Semantics
        ",
	pages="180--191",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2962843521",
	year="2018"
}


@inproceedings{liao2020probabilistically,
	title="Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order",
	author="Yi {Liao} and Xin {Jiang} and Qun {Liu}",
	booktitle="Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	pages="263--274",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3034878914",
	year="2020"
}




@article{datta2020geometry,
  author    = {Debajyoti Datta and
               Shashwat Kumar and
               Laura E. Barnes and
               Tom Fletcher},
  title     = {Geometry matters: Exploring language examples at the decision boundary},
  journal   = {CoRR},
  volume    = {abs/2010.07212},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.07212},
  archivePrefix = {arXiv},
  eprint    = {2010.07212},
  timestamp = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-07212.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{smith2013effect,
  author =        {Smith, Nathaniel J. and Levy, Roger},
  journal =       {Cognition},
  number =        {3},
  pages =         {302--319},
  publisher =     {Elsevier},
  title =         {The effect of word predictability on reading time is
                   logarithmic},
  volume =        {128},
  year =          {2013},
}


@inproceedings{hale2001probabilistic,
  author =        {Hale, John T.},
  booktitle =     {Proceedings of the Second Meeting of the North
                   American Chapter of the Association for Computational
                   Linguistics and Language Technologies},
  pages =         {1--8},
  title =         {A probabilistic {Earley} parser as a psycholinguistic
                   model},
  year =          {2001},
}

@article{levy2008expectation,
  author =        {Levy, Roger},
  journal =       {Cognition},
  number =        {3},
  pages =         {1126--1177},
  publisher =     {Elsevier},
  title =         {Expectation-based syntactic comprehension},
  volume =        {106},
  year =          {2008},
}

@article{gibson1998linguistic,
  author =        {Gibson, Edward},
  journal =       {Cognition},
  number =        {1},
  pages =         {1--76},
  title =         {Linguistic complexity: Locality of syntactic
                   dependencies},
  volume =        {68},
  year =          {1998},
}

@inproceedings{nangia2019human,
	title="Human vs. Muppet: A conservative estimate of human performance on the GLUE benchmark",
	author="Nikita {Nangia} and Samuel R. {Bowman}",
	booktitle="Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	pages="4566--4575",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2945067664",
	year="2019"
}

@inproceedings{chelba2014one,
	title="One billion word benchmark for measuring progress in statistical language modeling.",
	author="Ciprian {Chelba} and Tomas {Mikolov} and Mike {Schuster} and Qi {Ge} and Thorsten {Brants} and Phillipp {Koehn} and Tony {Robinson}",
	booktitle="INTERSPEECH",
	pages="2635--2639",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2611669587",
	year="2014"
}

@article{hahn2020sensitivity,
  author    = {Michael Hahn and
               Dan Jurafsky and Richard Futrell},
  title     = {Sensitivity as a Complexity Measure for Sequence Classification Tasks},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {9},
  pages = {891--908},
  year      = {2021},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00403/106992/Sensitivity-as-a-Complexity-Measure-for-Sequence},
  preprint       = {https://arxiv.org/abs/2104.10343},
  github = {https://github.com/m-hahn/sensitivity},
  slides = {files/sensitivity-slides.pdf}
}



@inproceedings{
zhou2023understanding,
title={Understanding Length Generalization by Thinking Like Transformers},
author={Hattie Zhou and Arwen Bradley and Etai Littwin and Noam Razin and Omid Saremi and Joshua Susskind and Samy Bengio and Preetum Nakkiran},
booktitle={The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23},
year={2023},
url={https://openreview.net/forum?id=tEUJiua8ir}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}


@article{merrill2023parallelism,
  title={The parallelism tradeoff: Limitations of log-precision transformers},
  author={Merrill, William and Sabharwal, Ashish},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={531--545},
  year={2023},
  publisher={MIT Press}
}
@inproceedings{merrill2023logic,
  title={A Logic for Expressing Log-Precision Transformers},
  author={Merrill, William and Sabharwal, Ashish},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}




@inproceedings{chiang2023tighter,
  author       = {David Chiang and
                  Peter Cholak and
                  Anand Pillay},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Tighter Bounds on the Expressivity of Transformer Encoders},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {5544--5562},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/chiang23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/0001CP23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{hao2022formal,
  title={Formal language recognition by hard attention transformers: Perspectives from circuit complexity},
  author={Hao, Yiding and Angluin, Dana and Frank, Robert},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={800--810},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}


@inproceedings{bhattamishra2022simplicity,
  author       = {Satwik Bhattamishra and
                  Arkil Patel and
                  Varun Kanade and
                  Phil Blunsom},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Simplicity Bias in Transformers and their Ability to Learn Sparse
                  Boolean Functions},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada,
                  July 9-14, 2023},
  pages        = {5767--5791},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.acl-long.317},
  doi          = {10.18653/V1/2023.ACL-LONG.317},
  timestamp    = {Thu, 10 Aug 2023 12:36:04 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/BhattamishraPKB23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{abbe2023generalization,
  author       = {Emmanuel Abbe and
                  Samy Bengio and
                  Aryo Lotfi and
                  Kevin Rizk},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Generalization on the Unseen, Logic Reasoning and Degree Curriculum},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {31--60},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/abbe23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/AbbeBLR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}






@inproceedings{bhattamishra2020ability,
  author       = {Satwik Bhattamishra and
                  Kabir Ahuja and
                  Navin Goyal},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {On the Ability and Limitations of Transformers to Recognize Formal
                  Languages},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {7096--7116},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.576},
  doi          = {10.18653/V1/2020.EMNLP-MAIN.576},
  timestamp    = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/BhattamishraAG20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{chiang2022overcoming,
  author       = {David Chiang and
                  Peter Cholak},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {Overcoming a Theoretical Limitation of Self-Attention},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {7654--7664},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.acl-long.527},
  doi          = {10.18653/V1/2022.ACL-LONG.527},
  timestamp    = {Mon, 01 Aug 2022 16:27:46 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/0001C22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{deletang2022neural,
  author       = {Gr{\'{e}}goire Del{\'{e}}tang and
                  Anian Ruoss and
                  Jordi Grau{-}Moya and
                  Tim Genewein and
                  Li Kevin Wenliang and
                  Elliot Catt and
                  Chris Cundy and
                  Marcus Hutter and
                  Shane Legg and
                  Joel Veness and
                  Pedro A. Ortega},
  title        = {Neural Networks and the Chomsky Hierarchy},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=WbxHAzkeQcn},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DeletangRGGWCCH23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{ruoss2023randomized,
      title={Randomized Positional Encodings Boost Length Generalization of Transformers}, 
      author={Anian Ruoss and Grégoire Delétang and Tim Genewein and Jordi Grau-Moya and Róbert Csordás and Mehdi Bennani and Shane Legg and Joel Veness},
      year={2023},
      journal={arXiv preprint},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yun2019transformers,
    title={Are Transformers universal approximators of sequence-to-sequence functions?},
    author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
    year={2019},
    eprint={1912.10077},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{ahn2023linear,
      title={Linear attention is (maybe) all you need (to understand transformer optimization)}, 
      author={Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra},
      year={2023},
      eprint={2310.01082},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Yao_2021,
   title={Self-Attention Networks Can Process Bounded Hierarchical Languages},
   url={http://dx.doi.org/10.18653/v1/2021.acl-long.292},
   DOI={10.18653/v1/2021.acl-long.292},
   booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
   year={2021} }

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}


@article{lu2023prompts,
  title={How are Prompts Different in Terms of Sensitivity?},
  author={Lu, Sheng and Schuff, Hendrik and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2311.07230},
  year={2023}
}



@inproceedings{zhou2023algorithms,
  author       = {Hattie Zhou and
                  Arwen Bradley and
                  Etai Littwin and
                  Noam Razin and
                  Omid Saremi and
                  Joshua M. Susskind and
                  Samy Bengio and
                  Preetum Nakkiran},
  title        = {What Algorithms can Transformers Learn? {A} Study in Length Generalization},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=AssIuHnmHX},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ZhouBLRSSBN24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{strobl2023averagehard,
  author       = {Lena Strobl},
  title        = {Average-Hard Attention Transformers are Constant-Depth Uniform Threshold
                  Circuits},
  journal      = {CoRR},
  volume       = {abs/2308.03212},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.03212},
  doi          = {10.48550/ARXIV.2308.03212},
  eprinttype    = {arXiv},
  eprint       = {2308.03212},
  timestamp    = {Mon, 21 Aug 2023 17:38:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-03212.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{strobl2023survey,
    author = {Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
    title = "{What Formal Languages Can Transformers Express? A Survey}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {543-561},
    year = {2024},
    month = {05},
    abstract = "{As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00663},
    url = {https://doi.org/10.1162/tacl\_a\_00663},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00663/2370911/tacl\_a\_00663.pdf},
}


@article{carbery2004multilinear,
  title={A multilinear generalisation of the Cauchy-Schwarz inequality},
  author={Carbery, Anthony},
  journal={Proceedings of the American Mathematical Society},
  volume={132},
  number={11},
  pages={3141--3152},
  year={2004}
}


@inproceedings{merrill2023expresssive,
title={The Expressive Power of Transformers with Chain of Thought},
author={William Merrill and Ashish Sabharwal},
booktitle={NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning},
year={2023},
url={https://openreview.net/forum?id=CDmerQ37Zs}
}

@inproceedings{feng2023towards,
title={Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective},
author={Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=qHrADgAdYu}
}


@article{wies2022sub,
  title={Sub-task decomposition enables learning in sequence to sequence tasks},
  author={Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2204.02892},
  year={2022}
}


@inproceedings{liu2022transformers,
title={Transformers Learn Shortcuts to Automata},
author={Bingbin Liu and Jordan T. Ash and Surbhi Goel and Akshay Krishnamurthy and Cyril Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=De4FYqjFueZ}
}


@InProceedings{li2023stability,
  title = 	 {Transformers as Algorithms: Generalization and Stability in In-context Learning},
  author =       {Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {19565--19594},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/li23l/li23l.pdf},
  url = 	 {https://proceedings.mlr.press/v202/li23l.html},
  abstract = 	 {In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.}
}


@book{bullen2013handbook,
  title={Handbook of means and their inequalities},
  author={Bullen, Peter S},
  volume={560},
  year={2013},
  publisher={Springer Science \& Business Media}
}



@article{witkowski2004new,
  title={A new proof of the monotonicity of power means},
  author={Witkowski, Alfred},
  journal={J. Ineq. Pure and Appl. Math},
  volume={5},
  number={1},
  year={2004}
}





@article{DBLP:journals/corr/abs-2211-05729,
  author       = {Kaiyue Wen and
                  Tengyu Ma and
                  Zhiyuan Li},
  title        = {How Does Sharpness-Aware Minimization Minimize Sharpness?},
  journal      = {CoRR},
  volume       = {abs/2211.05729},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.05729},
  doi          = {10.48550/ARXIV.2211.05729},
  eprinttype    = {arXiv},
  eprint       = {2211.05729},
  timestamp    = {Fri, 29 Sep 2023 12:48:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-05729.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{takase2022layer,
  title={On layer normalizations and residual connections in transformers},
  author={Takase, Sho and Kiyono, Shun and Kobayashi, Sosuke and Suzuki, Jun},
  journal={arXiv preprint arXiv:2206.00330},
  year={2022}
}



@article{DBLP:journals/corr/BaKH16,
  author       = {Lei Jimmy Ba and
                  Jamie Ryan Kiros and
                  Geoffrey E. Hinton},
  title        = {Layer Normalization},
  journal      = {CoRR},
  volume       = {abs/1607.06450},
  year         = {2016},
  url          = {http://arxiv.org/abs/1607.06450},
  eprinttype    = {arXiv},
  eprint       = {1607.06450},
  timestamp    = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{andriushchenko2023modern,
  author       = {Maksym Andriushchenko and
                  Francesco Croce and
                  Maximilian M{\"{u}}ller and
                  Matthias Hein and
                  Nicolas Flammarion},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {A Modern Look at the Relationship between Sharpness and Generalization},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {840--902},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/andriushchenko23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/AndriushchenkoC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}


@inproceedings{jiang2019fantastic,
title={Fantastic Generalization Measures and Where to Find Them},
author={Yiding Jiang and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgIPJBFvH}
}

@inproceedings{kaur2023maximum,
  title={On the maximum hessian eigenvalue and generalization},
  author={Kaur, Simran and Cohen, Jeremy and Lipton, Zachary Chase},
  booktitle={Proceedings on},
  pages={51--65},
  year={2023},
  organization={PMLR}
}

@article{li2010concise,
  title={Concise formulas for the area and volume of a hyperspherical cap},
  author={Li, Shengqiao},
  journal={Asian Journal of Mathematics \& Statistics},
  volume={4},
  number={1},
  pages={66--70},
  year={2010},
  publisher={Science Alert}
}


@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}



@article{de2008brief,
  title={A brief introduction to Fourier analysis on the Boolean cube},
  author={De Wolf, Ronald},
  journal={Theory of Computing},
  pages={1--20},
  year={2008},
  publisher={Theory of Computing Exchange}
}



@inproceedings{Damian2023SelfStab,
  author       = {Alex Damian and
                  Eshaan Nichani and
                  Jason D. Lee},
  title        = {Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge
                  of Stability},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=nhKHA59gXz},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DamianNL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{pytorch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Loshchilov2017DecoupledWD,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:53592270}
}



@InProceedings{pmlr-v97-rahaman19a,
  title = 	 {On the Spectral Bias of Neural Networks},
  author =       {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5301--5310},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/rahaman19a.html},
  abstract = 	 {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions – i.e. functions that vary globally without local fluctuations – which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.}
}


@inproceedings{NEURIPS2019_b432f34c,
 author = {Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SGD on Neural Networks Learns Functions of Increasing Complexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/b432f34c5a997c8e7c806a895ecc5e25-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2022_306264db,
 author = {Fridovich-Keil, Sara and Gontijo Lopes, Raphael and Roelofs, Rebecca},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {7368--7382},
 publisher = {Curran Associates, Inc.},
 title = {Spectral Bias in Practice: The Role of Function Frequency in Generalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/306264db5698839230be3642aafc849c-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}




@article{DBLP:journals/tacl/MerrillSS22,
  author       = {William Merrill and
                  Ashish Sabharwal and
                  Noah A. Smith},
  title        = {Saturated Transformers are Constant-Depth Threshold Circuits},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {10},
  pages        = {843--856},
  year         = {2022},
  url          = {https://transacl.org/ojs/index.php/tacl/article/view/3465},
  timestamp    = {Wed, 26 Oct 2022 16:52:10 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/MerrillSS22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{frieder2024mathematical,
  title={Mathematical capabilities of chatgpt},
  author={Frieder, Simon and Pinchetti, Luca and Griffiths, Ryan-Rhys and Salvatori, Tommaso and Lukasiewicz, Thomas and Petersen, Philipp and Berner, Julius},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{liu2023goat,
  title={Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks},
  author={Liu, Tiedong and Low, Bryan Kian Hsiang},
  journal={arXiv preprint arXiv:2305.14201},
  year={2023}
}


@article{yuan2023well,
  title={How well do Large Language Models perform in Arithmetic tasks?},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang},
  journal={arXiv preprint arXiv:2304.02015},
  year={2023}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{wang2024can,
  title={Can language models solve graph problems in natural language?},
  author={Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{peng2024limitations,
  title={On Limitations of the Transformer Architecture},
  author={Peng, Binghui and Narayanan, Srini and Papadimitriou, Christos},
  journal={arXiv preprint arXiv:2402.08164},
  year={2024}
}

@article{guo2024mitigating,
  title={Mitigating Reversal Curse via Semantic-aware Permutation Training},
  author={Guo, Qingyan and Wang, Rui and Guo, Junliang and Tan, Xu and Bian, Jiang and Yang, Yujiu},
  journal={arXiv preprint arXiv:2403.00758},
  year={2024}
}


@article{kalimeris2019sgd,
  title={Sgd on neural networks learns functions of increasing complexity},
  author={Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@misc{chen2024sudden,
      title={Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs}, 
      author={Angelica Chen and Ravid Shwartz-Ziv and Kyunghyun Cho and Matthew L. Leavitt and Naomi Saphra},
      year={2024},
      eprint={2309.07311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhao2024explainability,
  title={Explainability for large language models: A survey},
  author={Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={2},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{stolfo-etal-2023-mechanistic,
    title = "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
    author = "Stolfo, Alessandro  and
      Belinkov, Yonatan  and
      Sachan, Mrinmaya",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.435",
    doi = "10.18653/v1/2023.emnlp-main.435",
    pages = "7035--7052",
    abstract = "Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.",
}


@article{belrose2023eliciting,
  title={Eliciting latent predictions from transformers with the tuned lens},
  author={Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2303.08112},
  year={2023}
}



@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}



@inproceedings{welleck2022symbolic,
  title={Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics},
  author={Welleck, Sean and West, Peter and Cao, Jize and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={8},
  pages={8629--8637},
  year={2022}
}


@article{dziri2024faith,
  title={Faith and fate: Limits of transformers on compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{lee2023teaching,
  title={Teaching arithmetic to small transformers},
  author={Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D and Lee, Kangwook and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2307.03381},
  year={2023}
}


@inproceedings{kim2021have,
  title={Have you seen that number? investigating extrapolation in question answering models},
  author={Kim, Jeonghwan and Hong, Giwon and Kim, Kyung-min and Kang, Junmo and Myaeng, Sung-Hyon},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7031--7037},
  year={2021}
}



@article{nogueira2021investigating,
  title={Investigating the limitations of transformers with simple arithmetic tasks},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
  journal={arXiv preprint arXiv:2102.13019},
  year={2021}
}



@inproceedings{hahn2024sensitive,
    title={Why are Sensitive Functions Hard for Transformers?},
    author={Michael Hahn and Mark Rofin},
    year={2024},
	booktitle={Proceedings of the 2024 Annual Conference of the Association for Computational Linguistics (ACL 2024)},
    note={arXiv Preprint 2402.09963},
    primaryClass={cs.LG}
}




@article{Sarrof2024SSMs,
  author       = {Yash Sarrof and
                  Yana Veitsman and
                  Michael Hahn},
  title        = {The Expressive Capacity of State Space Models: {A} Formal Language
                  Perspective},
  journal      = {CoRR},
  volume       = {abs/2405.17394},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.17394},
  doi          = {10.48550/ARXIV.2405.17394},
  eprinttype    = {arXiv},
  eprint       = {2405.17394},
  timestamp    = {Tue, 18 Jun 2024 16:10:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-17394.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{kazemnejad2024impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@phdthesis{krebs2008typed,
  title={Typed semigroups, majority logic, and threshold circuits},
  author={Krebs, Andreas},
  year={2008},
  school={Universit{\"a}t T{\"u}bingen}
}
@article{ahuja2024provable,
  title={On provable length and compositional generalization},
  author={Ahuja, Kartik and Mansouri, Amin},
  journal={arXiv preprint arXiv:2402.04875},
  year={2024}
}



@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}

@article{liu2024exposing,
  title={Exposing attention glitches with flip-flop language modeling},
  author={Liu, Bingbin and Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@book{li2013introduction,
  title={An Introduction to Kolmogorov Complexity and Its Applications},
  author={Li, Ming and Vitanyi, Paul},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{
yang2024counting,
title={Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers},
author={Andy Yang and David Chiang},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=FmhPg4UJ9K}
}

@inproceedings{lange2004some,
  title={Some results on majority quantifiers over words},
  author={Lange, K-J},
  booktitle={Proceedings. 19th IEEE Annual Conference on Computational Complexity, 2004.},
  pages={123--129},
  year={2004},
  organization={IEEE}
}


@InProceedings{10.1007/978-3-540-74456-6_15,
author="Behle, Christoph
and Krebs, Andreas
and Mercer, Mark",
editor="Ku{\v{c}}era, Lud{\v{e}}k
and Ku{\v{c}}era, Anton{\'i}n",
title="Linear Circuits, Two-Variable Logic and Weakly Blocked Monoids",
booktitle="Mathematical Foundations of Computer Science 2007",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="147--158",
abstract="Following recent works connecting two-variable logic to circuits and monoids, we establish, for numerical predicate sets satisfying a certain closure property, a one-to-one correspondence between {\$}FO[<,{\backslash}ensuremath{\{}{\backslash}mathfrak{\{}P{\}}{\}}]{\$}-uniform linear circuits, two-variable formulae with {\$}{\backslash}ensuremath{\{}{\backslash}mathfrak{\{}P{\}}{\}}{\$}predicates, and weak block products of monoids. In particular, we consider the case of linear TC0, majority quantifiers, and finitely typed monoids. This correspondence will hold for any numerical predicate set which is FO[{\thinspace}<{\thinspace}]-closed and whose predicates do not depend on the input length.",
isbn="978-3-540-74456-6"
}

@InProceedings{10.1007/978-3-642-02737-6_7,
author="Behle, Christoph
and Krebs, Andreas
and Reifferscheid, Stephanie",
editor="Diekert, Volker
and Nowotka, Dirk",
title="Regular Languages Definable by Majority Quantifiers with Two Variables",
booktitle="Developments in Language Theory",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="91--102",
abstract="In this paper we consider the class of all regular languages definable by the extended majority quantifier and the order predicate but using only two variables. The main part of the paper is the presentation of a geometric method which is used to show that a given regular language cannot be defined by such formulas. Applying this method we can give a necessary condition in terms of an equation as well as an upper and a lower bound for the corresponding class of monoids. As a consequence we obtain that FO{\thinspace}+{\thinspace}MAJ2[{\thinspace}<{\thinspace}] does not contain FO{\thinspace}+{\thinspace}MOD2[{\thinspace}<{\thinspace}].",
isbn="978-3-642-02737-6"
}





@inproceedings{DBLP:conf/iclr/BrunnerLPRCW20,
  author       = {Gino Brunner and
                  Yang Liu and
                  Damian Pascual and
                  Oliver Richter and
                  Massimiliano Ciaramita and
                  Roger Wattenhofer},
  title        = {On Identifiability in Transformers},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=BJg1f6EFDB},
  timestamp    = {Thu, 07 May 2020 17:11:47 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/BrunnerLPRCW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{sanford2024onelayer,
    title={One-layer transformers fail to solve the induction heads task},
    author={Clayton Sanford and Daniel Hsu and Matus Telgarsky},
    year={2024},
    journal={arXiv preprint},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}




@article{shazeer2020glu,
  title={{GLU} variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{sanford2023representational,
  title={Representational strengths and limitations of transformers},
  author={Sanford, Clayton and Hsu, Daniel J and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{Bhattamishra2024Separations,
  author       = {Satwik Bhattamishra and
                  Michael Hahn and
                  Phil Blunsom and
                  Varun Kanade},
  title        = {Separations in the Representational Capabilities of Transformers and
                  Recurrent Architectures},
  journal      = {CoRR},
  volume       = {abs/2406.09347},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.09347},
  doi          = {10.48550/ARXIV.2406.09347},
  eprinttype    = {arXiv},
  eprint       = {2406.09347},
  timestamp    = {Mon, 15 Jul 2024 16:11:03 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-09347.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{Chang2024Language,
  author       = {Yingshan Chang and
                  Yonatan Bisk},
  title        = {Language Models Need Inductive Biases to Count Inductively},
  journal      = {CoRR},
  volume       = {abs/2405.20131},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.20131},
  doi          = {10.48550/ARXIV.2405.20131},
  eprinttype    = {arXiv},
  eprint       = {2405.20131},
  timestamp    = {Mon, 24 Jun 2024 10:16:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-20131.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{WangJW0GZ0W24,
  author       = {Jie Wang and
                  Tao Ji and
                  Yuanbin Wu and
                  Hang Yan and
                  Tao Gui and
                  Qi Zhang and
                  Xuanjing Huang and
                  Xiaoling Wang},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {Length Generalization of Causal Transformers without Position Encoding},
  booktitle    = {Findings of the Association for Computational Linguistics, {ACL} 2024,
                  Bangkok, Thailand and virtual meeting, August 11-16, 2024},
  pages        = {14024--14040},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.findings-acl.834},
  timestamp    = {Tue, 27 Aug 2024 17:38:11 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/WangJW0GZ0W24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{KazemnejadPRDR23,
  author       = {Amirhossein Kazemnejad and
                  Inkit Padhi and
                  Karthikeyan Natesan Ramamurthy and
                  Payel Das and
                  Siva Reddy},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {The Impact of Positional Encoding on Length Generalization in Transformers},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/4e85362c02172c0c6567ce593122d31c-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/KazemnejadPRDR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@article{Zhou2024Transformers,
  author       = {Yongchao Zhou and
                  Uri Alon and
                  Xinyun Chen and
                  Xuezhi Wang and
                  Rishabh Agarwal and
                  Denny Zhou},
  title        = {Transformers Can Achieve Length Generalization But Not Robustly},
  journal      = {CoRR},
  volume       = {abs/2402.09371},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.09371},
  doi          = {10.48550/ARXIV.2402.09371},
  eprinttype    = {arXiv},
  eprint       = {2402.09371},
  timestamp    = {Tue, 20 Feb 2024 11:28:09 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-09371.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{Awasthi2023Improving,
  author       = {Pranjal Awasthi and
                  Anupam Gupta},
  title        = {Improving Length-Generalization in Transformers via Task Hinting},
  journal      = {CoRR},
  volume       = {abs/2310.00726},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.00726},
  doi          = {10.48550/ARXIV.2310.00726},
  eprinttype    = {arXiv},
  eprint       = {2310.00726},
  timestamp    = {Thu, 19 Oct 2023 10:30:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-00726.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{Jelassi2023Length,
  author       = {Samy Jelassi and
                  St{\'{e}}phane d'Ascoli and
                  Carles Domingo{-}Enrich and
                  Yuhuai Wu and
                  Yuanzhi Li and
                  Fran{\c{c}}ois Charton},
  title        = {Length Generalization in Arithmetic Transformers},
  journal      = {CoRR},
  volume       = {abs/2306.15400},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.15400},
  doi          = {10.48550/ARXIV.2306.15400},
  eprinttype    = {arXiv},
  eprint       = {2306.15400},
  timestamp    = {Fri, 30 Jun 2023 15:53:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-15400.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{JelassiBKM24,
  author       = {Samy Jelassi and
                  David Brandfonbrener and
                  Sham M. Kakade and
                  Eran Malach},
  title        = {Repeat After Me: Transformers are Better than State Space Models at
                  Copying},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=duRRoGeoQT},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/JelassiBKM24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@article{Xiao2023Conditions,
  author       = {Changnan Xiao and
                  Bing Liu},
  title        = {Conditions for Length Generalization in Learning Reasoning Skills},
  journal      = {CoRR},
  volume       = {abs/2311.16173},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.16173},
  doi          = {10.48550/ARXIV.2311.16173},
  eprinttype    = {arXiv},
  eprint       = {2311.16173},
  timestamp    = {Mon, 04 Dec 2023 10:53:08 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-16173.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{Hou2024Universal,
  author       = {Kaiying Hou and
                  David Brandfonbrener and
                  Sham M. Kakade and
                  Samy Jelassi and
                  Eran Malach},
  title        = {Universal Length Generalization with Turing Programs},
  journal      = {CoRR},
  volume       = {abs/2407.03310},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.03310},
  doi          = {10.48550/ARXIV.2407.03310},
  eprinttype    = {arXiv},
  eprint       = {2407.03310},
  timestamp    = {Wed, 07 Aug 2024 21:29:40 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-03310.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}


@inproceedings{wang2024transformers,
title={Transformers Provably Learn Sparse Token Selection While Fully-Connected Nets Cannot},
author={Zixuan Wang and Stanley Wei and Daniel Hsu and Jason D. Lee},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=qjqlhWDcId}
}



@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}


@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}


@inproceedings{tomita1982dynamic,
  title={Dynamic construction of finite-state automata from examples using hill-climbing.},
  author={Tomita, Masaru},
  booktitle={Proceedings of the Fourth Annual Conference of the Cognitive Science Society},
  pages={105--108},
  year={1982}
}


@article{edelman2024evolution,
  title={The evolution of statistical induction heads: In-context learning markov chains},
  author={Edelman, Benjamin L and Edelman, Ezra and Goel, Surbhi and Malach, Eran and Tsilivis, Nikolaos},
  journal={arXiv preprint arXiv:2402.11004},
  year={2024}
}





@article{schutzenberger1965finite,
  title={On finite monoids having only trivial subgroups},
  author={Sch{\"u}tzenberger, Marcel Paul},
  journal={Inf. Control.},
  volume={8},
  number={2},
  pages={190--194},
  year={1965}
}





@inproceedings{barcelo2024logical,
title={Logical Languages Accepted by Transformer Encoders with Hard Attention},
author={Pablo Barcel{\'o} and Alexander Kozachinskiy and Anthony Widjaja Lin and Vladimir Podolskii},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gbrHZq07mq}
}

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@book{de2010grammatical,
  title={Grammatical inference: learning automata and grammars},
  author={De la Higuera, Colin},
  year={2010},
  publisher={Cambridge University Press}
}




@article{CadilhacP22,
  author       = {Micha{\"{e}}l Cadilhac and
                  Charles Paperman},
  title        = {The regular languages of wire linear {AC}\({}^{\mbox{0}}\)},
  journal      = {Acta Informatica},
  volume       = {59},
  number       = {4},
  pages        = {321--336},
  year         = {2022},
  url          = {https://doi.org/10.1007/s00236-022-00432-2},
  doi          = {10.1007/S00236-022-00432-2},
  timestamp    = {Sat, 10 Sep 2022 21:00:07 +0200},
  biburl       = {https://dblp.org/rec/journals/acta/CadilhacP22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}






@incollection{tesson2002diamonds,
  title={Diamonds are forever: The variety {DA}},
  author={Tesson, Pascal and Th{\'e}rien, Denis},
  booktitle={Semigroups, algorithms, automata and languages},
  pages={475--499},
  year={2002},
  publisher={World Scientific}
}




@inproceedings{JacotHG18,
  author       = {Arthur Jacot and
                  Cl{\'{e}}ment Hongler and
                  Franck Gabriel},
  editor       = {Samy Bengio and
                  Hanna M. Wallach and
                  Hugo Larochelle and
                  Kristen Grauman and
                  Nicol{\`{o}} Cesa{-}Bianchi and
                  Roman Garnett},
  title        = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  booktitle    = {Advances in Neural Information Processing Systems 31: Annual Conference
                  on Neural Information Processing Systems 2018, NeurIPS 2018, December
                  3-8, 2018, Montr{\'{e}}al, Canada},
  pages        = {8580--8589},
  year         = {2018},
  url          = {https://proceedings.neurips.cc/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/JacotHG18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{yang2022tensor,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}

@article{conmy2023towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={16318--16352},
  year={2023}
}



@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@inproceedings{li2024chain,
  title={Chain of Thought Empowers Transformers to Solve Inherently Serial Problems},
  author={Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}




@article{peng2024limitations,
  title={On limitations of the transformer architecture},
  author={Peng, Binghui and Narayanan, Srini and Papadimitriou, Christos},
  journal={arXiv preprint arXiv:2402.08164},
  year={2024}
}


@inproceedings{
abbe2024how,
title={How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad},
author={Emmanuel Abb{\'e} and Samy Bengio and Aryo Lotfi and Colin Sandon and Omid Saremi},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=FoGwiFXzuN}
}

@inproceedings{
cho2024position,
title={Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure},
author={Hanseul Cho and Jaeyoung Cha and Pranjal Awasthi and Srinadh Bhojanapalli and Anupam Gupta and Chulhee Yun},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=5cIRdGM1uG}
}


@article{chen2024theoretical,
  title={Theoretical limitations of multi-layer Transformer},
  author={Chen, Lijie and Peng, Binghui and Wu, Hongxun},
  journal={arXiv preprint arXiv:2412.02975},
  year={2024}
}
@inproceedings{
bergstrasser2024power,
title={The Power of Hard Attention Transformers on Data Sequences: A formal language theoretic perspective},
author={Pascal Bergstr{\"a}{\ss}er and Chris K{\"o}cher and Anthony Widjaja Lin and Georg Zetzsche},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=NBq1vmfP4X}
}

@inproceedings{fan24looped,
  title={Looped Transformers for Length Generalization},
  author={Fan, Ying and Du, Yilun and Ramchandran, Kannan and Lee, Kangwook},
year={2024},
  booktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24}
}


@inproceedings{
cabannes2024iteration,
title={Iteration Head: A Mechanistic Study of Chain-of-Thought},
author={Vivien Cabannes and Charles Arnal and Wassim Bouaziz and Xingyu Alice Yang and Francois Charton and Julia Kempe},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=QBCxWpOt5w}
}


@article{hao2024training,
  title={Training Large Language Models to Reason in a Continuous Latent Space},
  author={Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.06769},
  year={2024}
}





@inproceedings{
kim2024transformers,
title={Transformers Provably Solve Parity Efficiently with Chain of Thought},
author={Juno Kim and Taiji Suzuki},
booktitle={NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning},
year={2024},
url={https://openreview.net/forum?id=E7HwPhfX1B}
}