\documentclass{article}
\usepackage[T1]{fontenc}

\usepackage{microtype}
\usepackage{graphicx}
%
\usepackage{booktabs} %

%
%
%
%
\usepackage{hyperref}


\usepackage{tocloft} %
\usepackage{etoolbox} %


\renewcommand{\contentsname}{Appendix Contents}

%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
\usepackage[accepted]{icml2025}

%
%

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%
\usepackage[capitalize,noabbrev]{cleveref}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Open Problem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{conjecture}[theorem]{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}


%
%
%

\usepackage{amsmath,amsfonts,bm}

%
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

%
\newcommand{\newterm}[1]{{\bf #1}}


%
\def\figref#1{figure~\ref{#1}}
%
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
%
\def\secref#1{section~\ref{#1}}
%
\def\Secref#1{Section~\ref{#1}}
%
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
%
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
%
\def\eqref#1{equation~\ref{#1}}
%
\def\Eqref#1{Equation~\ref{#1}}
%
\def\plaineqref#1{\ref{#1}}
%
\def\chapref#1{chapter~\ref{#1}}
%
\def\Chapref#1{Chapter~\ref{#1}}
%
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
%
\def\algref#1{algorithm~\ref{#1}}
%
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
%
\def\partref#1{part~\ref{#1}}
%
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


%
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
%
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

%
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

%
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

%
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

%
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

%
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

%
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

%
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

%
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


%
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

%
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
%
%
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

%
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

%
%
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

%
\newcommand{\pdata}{p_{\rm{data}}}
%
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
%
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
%
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} %

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
%
%
%
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} %

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}


\usepackage{pifont}  %


\newcommand{\redcross}{\textcolor{red}{\ding{55}}}  %
\newcommand{\greencheck}{\textcolor{green}{\ding{51}}}

\newcommand{\ParallelSetup}{\textsc{Parallel}}%
\newcommand{\AutoregressiveSetup}{\textsc{Autoregressive}}%

\usepackage{amsmath, amssymb}
%
\usepackage{amsthm}
\usepackage{xcolor}

%
%
%
%
%
%

\usepackage{hyperref}
\usepackage{url}

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{nicematrix}




\newcommand{\Prob}{\mathbb{P}}%
\allowdisplaybreaks


\newcommand{\ypost}{\vy}%
\newcommand{\ypre}{Y}%
\newcommand{\logit}{a}%
\newcommand{\head}{\vb}%
\newcommand{\headmac}[3]{b_{#1,#2}^{(#3)}}%
\newcommand{\seqlen}{n}%
\newcommand{\normFactor}{N}
\newcommand{\blowupInLemma}{N}%
\newcommand{\weight}{\widehat{a}}%
\newcommand{\weightmac}[4]{\widehat{a}_{#1,#2}^{(#3,#4)}}%
\newcommand{\influence}{\operatorname{I}}%
\newcommand{\Blowup}{\operatorname{Blowup}}%

\newcommand{\flipBit}[1]{^{\oplus #1}}



\newcommand{\logMatPosPos}{A}%
\newcommand{\logMatPosTok}{C}%
\newcommand{\logMatTokPos}{B}%
\newcommand{\logMatTokTok}{W}%

\newcommand{\numLayers}{L}%
 


\icmltitlerunning{Lower Bounds for CoT Reasoning}

\begin{document}

\twocolumn[
\icmltitle{Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention Transformers}


\begin{icmlauthorlist}
\icmlauthor{Alireza Amiri}{sharif}
\icmlauthor{Xinting Huang}{saarland}
\icmlauthor{Mark Rofin}{saarland}
\icmlauthor{Michael Hahn}{saarland}
\end{icmlauthorlist}

\icmlaffiliation{sharif}{Sharif University of Technology. Work done in part while interning at Saarland University.}
\icmlaffiliation{saarland}{Saarland University}

\icmlcorrespondingauthor{Michael Hahn}{mhahn@lst.uni-saarland.de}


\vskip 0.3in
]

%

%
%
%
%
%

%
\printAffiliationsAndNotice{} %

%
%

%
\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}
%

%


\begin{abstract}
Chain-of-thought reasoning and scratchpads have emerged as critical tools for enhancing the computational capabilities of transformers. While theoretical results show that polynomial-length scratchpads can extend transformers' expressivity from $TC^0$ to $PTIME$, their required length remains poorly understood. Empirical evidence even suggests that transformers need scratchpads even for many problems in $TC^0$, such as \textsc{Parity} or \textsc{Multiplication}, challenging optimistic bounds derived from circuit complexity. In this work, we initiate the study of systematic lower bounds for the number of CoT steps across different algorithmic problems, in the hard-attention regime. 
We study a variety of algorithmic problems, and provide bounds that are tight up to logarithmic factors.
Overall, these results contribute to emerging understanding of the power and limitations of chain-of-thought reasoning.
\end{abstract}



%
%
%



\section{Introduction}

%

%

%
Chain-of-Thought reasoning (CoT) has become a standard practice for solving hard problems with LLMs, enhancing the capabilities of Transformers \cite{nye2021show, Wei2022Chain} and powering a new generation of state-of-the-art models, such as OpenAI o1 \cite{jaech2024openai} and DeepSeek-R1 \cite{r1}. Models trained under this paradigm are optimized to generate a long CoT before answering any user’s request, which significantly elevates their reasoning abilities.
%
However, the use of CoT may substantially increase the number of tokens produced by the model and raise the inference costs \cite{han2024token}, in some cases reaching millions of tokens for a single task \cite{oai_o3_pub_breakthrough}.
Hence, shortening the generated CoT sequences without compromising quality became an important research direction \cite{deng2024explicit}. At the same time, there is so far little understanding of the minimal sufficient length of the CoT in a given case; thus the theoretical limits of CoT compression are unclear.




\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{plots/CoT_chart.pdf}
    \caption{High-sensitivity problems such as \textsc{Parity} %
    are difficult to learnably and generalizably represent for transformers.
    Such problems can be solved by CoTs decomposing them into a sequence of local steps. We study the length that such CoTs need to have, as a function of the input length.
    %
%
}
    \label{fig:motivating-figure}
\end{figure}
%




To gain understanding of those limits, we take up the problem of provably bounding the length of CoT sequences sufficient to solve various algorithmic problems with transformers.
%
%
By providing the lower bounds on the length of the CoT, our paper complements an established line of research on the expressive power of transformers that has focused on the model size when the answer is provided without CoT \citep[e.g.][]{strobl2023survey, bhattamishra2020ability, sanford2023representational, chen2024theoretical}.

%
%
%
%
%

%


A simple example is the \textsc{Parity} function -- deciding whether the number of 1's in a bit string is even or odd.
Transformers empirically struggle to learn it on longer inputs \citep[e.g.][]{bhattamishra2020ability, deletang2022neural, butoi2024training,hahn2024sensitive}, and this difficulty is resolved by a CoT consisting of the parities of increasing prefixes, as in Figure~\ref{fig:motivating-figure} \citep[e.g.][]{anil2022exploring}.
Here, every decoding step depends only on a bounded number of tokens. 
However, for an input of length $N$, this CoT requires $\Theta(N)$ extra decoding steps, a substantial computational burden.

An interesting and pertinent question is thus whether such a linear-length CoT is optimal, or whether a shorter CoT might exist.
The goal of this paper is to
    \emph{develop explicit and unconditional bounds on the length of CoTs}. %
This question is analogous to the classical question of bounding the time complexity of Turing machines.
We argue that it is of foundational interest in the context of LLM-based systems relying on CoT reasoning \cite{jaech2024openai, r1}.

%


%
%
%
%
%
 %
%
%

%

We study this question in the \emph{unique-hard attention} (UHAT) regime, a popular theoretical abstraction  of self-attention, in which every attention head attends to the unique position where attention weights are maximized \citep[e.g.][]{hahn2020theoretical, hao2022formal, barcelo2024logical, svete2024transformers, bergstrasser2024power,  yang2024masked,barcelo2025ehrenfeucht}.
In this model, we rigorously prove that the CoT for \textsc{Parity} described above is optimal up to constant factors (Theorem~\ref{thm:parity-bound}).
%
%
%
%
%
%
%
%
 %
  %
   %
    %
%
Besides \textsc{Parity}, we consider three other tasks (\textsc{Multiplication}, \textsc{Median}, \textsc{Reachability}) that are also empirically challenging for transformers to solve without CoT.
Across tasks, we  show that CoT lengths need to scale at least linearly in the size of the original problem.
We further show that our bounds are \emph{optimal up to logarithmic factors}.
Overall, these lower bounds place broad constraints on the inference-time compute needed to enable transformers to solve these problems.
%

%


%


%



%



%

%






%
%

%
%


%

%




\section{Background}



\subsection{Sensitive Functions Require CoTs}\label{sec:sensitivity}



Prior work has often studied the abilities of transformers through the lens of \emph{circuit complexity} \citep[e.g.][]{hao2022formal, merrill2023logic, strobl2023averagehard, feng2023towards, li2024chain, chiang2025transformers, merrill2023parallelism}.
Transformers express a subset of the circuit complexity class $TC^0$, which covers problems solvable by bounded-depth threshold circuits \citep{merrill2023parallelism, strobl2023averagehard, chiang2025transformers}.
By the unproven conjecture $TC^0 \neq NC^1$, many problems, such as graph reachability, are thus not solvable by transformers without CoTs.
Hence, the success of CoT has been linked to its ability to expand expressiveness beyond $TC^0$ \citep{feng2023towards, merrill2023expresssive, li2024chain}.

However, CoT is empirically beneficial  even for tasks well in $TC^0$, such as \textsc{Parity} \citep{anil2022exploring}:
\textsc{Parity} is expressible in $TC^0$ and also by certain models of transformers \citep{chiang2022overcoming, kozachinskiy2024lower}, but practical transformers consistently struggle to learn it via SGD in long inputs \citep[e.g.][]{bhattamishra2020ability} unless a CoT is provided \citep[e.g.][]{anil2022exploring}.
%
\citet{hahn2024sensitive} show that this fact can be explained in terms of the loss landscape: Transformers require very sharp minima to represent functions that, like \textsc{Parity}, are simultaneously sensitive to every input bit, and hence do not practically learn them on long inputs; sharpness is starkly reduced in the presence of a CoT.
%
%
%
%
In fact, the result of \citet{hahn2024sensitive} extends beyond \textsc{Parity}, and 
%
 is grounded in a foundational concept in the analysis of Boolean Functions, \emph{sensitivity}.
%
For a boolean function $f : \{0,1\}^* \rightarrow \{0,1\}$, the \emph{sensitivity} counts how many Hamming neighbors have the opposite output:
\begin{equation}
    s(f,x) = \left| \{ i=1,\dots,|x| \text{ such that } f(x) \neq f(x^{\oplus i})\}\right|
\end{equation}
where $x^{\oplus i}$ is obtained from $x$ by flipping the $i$-th bit.
%
%
%
%
The \emph{average sensitivity} is defined as the average over the Hamming cube, at string length $N$  \citep[e.g.][]{odonnell2014analysis}: %
\begin{equation}
    as_N(f) = \frac{1}{2^N} \sum_{x \in \{0,1\}^N} s(f,x)
\end{equation}
In \textsc{Parity}, flipping any bit flips the output, hence $as_N(\textsc{Parity}) = N$.
\citet{hahn2024sensitive} show that such linear growth of sensitivity with input length produces high sharpness in transformers' loss landscapes.
This unifies a string of empirical results finding that transformers have an inductive bias towards low average sensitivity \citep{bhattamishra2022simplicity, vasudeva2024simplicity, abbe2023generalization}. %
%
%
%
%
%
 %
%
%
%
%
%
%
%
%
%
%
%
\citet{hahn2024sensitive} applied this result to the \textsc{Parity} function; but, in this paper, we identify a range of other tasks also facing linear growth of sensitivity, including tasks within $TC^0$ (\textsc{Multiplication} and \textsc{Median}), and a task conjectured not to be in $TC^0$ (\textsc{Reachability}).
%
%
%
We thus overall
\begin{center}
    \emph{focus on algorithmic tasks with $as_N(f) = \Theta(N)$}.
\end{center}
including both tasks within $TC^0$, and tasks conjectured to be outside it.




\subsection{Model of Transformers}\label{sec:model-transformers}

The theoretical literature on transformers has developed various formal abstractions of transformers.
In this paper, we study the regime of \emph{unique hard attention} (UHAT), a popular theoretical model of self-attention, where every attention head attends to the unique position where attention scores are maximized \citep[e.g.][]{hahn2020theoretical, hao2022formal, barcelo2024logical, svete2024transformers, bergstrasser2024power,  yang2024masked,barcelo2025ehrenfeucht}.
%
UHAT is an appealing modeling choice, both because strong techniques for proving lower bounds are available (see Section~\ref{sec:cot-bounds}), and because interpretability work shows that language models heavily rely on heads focusing their attention on few positions \citep[e.g.][]{cabannes2024iteration, Olsson2022IncontextLA, clark2019bert, voita2019analyzing, ebrahimi2020can}.

We now introduce the relevant notions and notation.
We assume a finite alphabet $\Sigma$, with token embeddings $e(\sigma) \in \mathbb{R}^d$.
There further are positional encodings $\vp_1, \vp_2, \vp_3, \dots, \vp_{n_{max}} \in \mathbb{R}^d$, where $n_{max}$ is the maximal context size of the transformer.
We consider an input string $x \in \Sigma^*$, with length $|x|=N$.
%
We define the activations $\ypost_i^{(k)} \in \mathbb{R}^d$ at position $i$ of the $k$-th layer ($k=1, \dots, \numLayers$) as follows.
The zero-th layer consists of token and positional encodings: $\ypost_i^{(0)} := e(x_i) + \vp_i$ ($i=1, \dots, N$).
%
In each layer $l=1, \dots, L$, we first compute attention scores for the $h$-th head ($h=1,\dots,H$):
\begin{align*}
    a_{i,j}^{(k,h)} =&  (\vy_j^{(k-1)})^T K_{k,h}^T Q_{k,h} \vy_i^{(k-1)} 
    \end{align*}
    where $K_{k,h}$ (``key''), $Q_{k,h}$ (``query'') are $\in \mathbb{R}^{d\times d}$.
In softmax attention, the attention weights $\weightmac{i}{j}{k}{h}$ are then obtained via the softmax transform. In the  UHAT model, these are idealized as one-hot weights:
    \begin{align}
    \weightmac{i}{j}{k}{h} =& \frac{\exp(a_{i,j}^{(k,h)})}{\sum_{s=1}^i \exp(a_{i,s}^{(k,h)})} &&[\text{Softmax}] \\
    \weightmac{i}{j}{k}{h} =& \begin{cases} 1 & j=\arg_{s\leq i}\max a_{i,s}^{(k,h)} \\ 0 & else \end{cases} &&[\text{UHAT}] \label{eq:uhat}
\end{align}
The UHAT model can be viewed as the limit of the Softmax model when one attention score $a_{i,s}$ far exceeds the others.
If more than one $s$ attains the maximal attention score, ties are broken according to some fixed rule (e.g., choosing the left- or right-most match, \citet{yang2024masked}).
The output of the attention block is computed by weighting according to  attention weights $\hat{a}_{i,j}^{(k,h)}$ ($j \leq i$)\footnote{We assume causal masking in line with standard language model architectures, in order to allow autoregressive generation of the CoT. Our techniques are also applicable to the setting where the input itself is processed with bidirectional attention as assumed by \citet{abbe2024how} (Remark~\ref{remark:causal}).}, and applying a linear transformation $V$ (``value''); these are then aggregated across heads and combined with a skip-connection:
%
%
%
%
\begin{equation}\label{eq:def:ypre}
\ypost_i^{(k)} :=  f^{MLP}\left(\ypost_i^{(k-1)} +  \sum_{h=1}^H \sum_{j=1}^{i}\hat{a}_{i,j}^{(k,h)}  V_{k,h} \ypost_j^{(k-1)} \right)
\end{equation}
where $f^{MLP} : \mathbb{R}^d \rightarrow \mathbb{R}^d$. For our purposes here, $f^{MLP}$ may be arbitrary.\footnote{Transformers additionally implement layer norm \citep{DBLP:journals/corr/BaKH16}. Our bounds are robust to such position-wise operations. For instance,  layer norm following the MLP can be absorbed into $f^{MLP}$ for the purposes of our theorems.}
%
Finally, next-token predictions are made by $T := \mO \cdot \ypost_\seqlen^{(\numLayers)}$ for some parameter $\mO \in \mathbb{R}^{|\Sigma| \times d}$, where we are assuming some numbering of the alphabet $\Sigma$. 
%




%
%
%
%
%
%
%

%

%
%
%


\subsection{Formalizing CoTs}\label{sec:formalizing-cots}

\begin{definition}\label{def:cot}
Let $\Sigma$ be a finite alphabet.
    Given a function $f : \Sigma^* \rightarrow \Sigma^+$ and an alphabet $\Xi \supset \Sigma$, a \emph{chain-of-thought} (CoT) is a map  $g : \Sigma^* \rightarrow \Xi^+$ ending in the suffix $f(x)$.
    
    We write $\Xi_n$ to be the (finite) set of symbols appearing in at least one string $xg(x)$ for $x$ with $|x| \leq n$.
\end{definition}
We note that $\Xi$ is not restricted to be finite.
Some theoretical work has allowed the CoT voabulary to grow with the input length \citep{bhattamishra2020computational,abbe2024how}; our lower-bounds are robust to this.

%
%
%
%
%
%
%
%
%
%
%




%

%

We now formalize what it means for a CoT to be expressible in UHAT.
For maximal generality, we assume a relaxed definition: rather than requiring that a single transformer perform the task across all input lengths (which makes it hard to distinguish unboundedly many positions), we only ask that a \emph{family} of transformers perform the task with a bounded number of layers and heads:
\begin{definition}\label{def:uhat-computable}
We say that a transformer $T$ computes the CoT $g(x)$ on input $x \in \Sigma^*$ if all symbols in $x g(x)$ appear in the vocabulary of $T$ and $|x g(x)|$ is bounded by the maximal context window, and if, when $T$ is run on $x g(x)$, its output at position $|x|+i-1$ is a one-hot vector with ``one'' at the index corresponding to $g(x)_i \in \Sigma$, for $i = 1, \dots, |g(x)|$. 
%
%


%
    We say that the CoT $g(x)$ is \emph{expressible in UHAT} across input lengths if, for each input length $n$, there is a UHAT transformer $T_n$ computing $g(x)$ for all $|xg(x)| = n$, and the numbers of layers and heads are uniformly bounded.
\end{definition}
%
While we bound layers and heads, we do not expect the width to necessarily stay bounded, which allows positional encodings to keep all unboundedly many positions distinct. This is a very weak requirement, a necessary precondition for the existence of a single length-generalizing transformer across input lengths, and much more relaxed than the degree of uniformity often assumed in lower bounds for transformers \citep[e.g.][]{merrill2023logic, huang2024formal}. 
It is nonetheless \emph{sufficient for proving essentially matching lower and upper bounds}.



%
%

%



%
%
%
%

%
 %
   %
   %
   %
   %
   %
    %
    %
     %
      %
       %
    %

%


%
%

%
%

%
%
%
%


%
%







%
\section{Results: Generic CoT Bounds}\label{sec:cot-bounds}

Our goal is to study the required length of the CoT $|g(x)|$ as a function of the input length $|x|$, under the constraint that the CoT is expressible in the sense of Def.~\ref{def:uhat-computable}.
Following standard practice in computational complexity, we focus on the worst-case complexity, which -- for a given input length $N$, amounts to $\max_{x : |x| = N} |g(x)|$. %

It is well-known that transformer CoTs are universal in the sense that they can simulate Turing machines \citep{perez2019turing, bhattamishra2020computational, hou2024universal, merrill2023expresssive, malach2024autoregressive, wei2022statistically, qiu2024ask}.
Constructions use a variety of assumptions about attention; we first note that this property holds in our setup:
%
%
%
%
%
\begin{fact}[Universality of UHAT CoTs]\label{lemma:universality}
    Consider a Turing machine that terminates within $\leq \tau(N)$ steps on all inputs of length $\leq N$.
    Then there is a CoT $g(x)$ over a countable alphabet $\Xi$ computing the output of the Turing machine, with length $|g(x)| = \mathcal{O}(\tau(|x|))$, expressible in UHAT.
    %
    %
\end{fact}
The construction is discussed in Appendix~\ref{app:universality}.
%
%
%
%
As a consequence, all PTIME problems have polynomial-length CoTs, and problems outside of PTIME cannot have polynomial-length CoTs \citep{merrill2023expresssive}.
More generally, CoT is upper-bounded by Turing machine time complexity.
The converse does not hold: Many problems have efficient parallel algorithms that can be expressed by self-attention without any CoT. As a simple example, the Boolean AND function of $N$ variables requires $\Omega(N)$ steps with a Turing machine, simply because every input needs to be considered in the worst case, but it can be represented by a very simple transformer with just one attention head, and no CoT.
Our main technical contribution in this paper is to show that a diverse set of algorithmic problems \emph{do} require long CoTs, of length linear in the input. %


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Our key technical tool, and first central result, is a generic CoT lower bound (Theorem~\ref{thm:uhat-cot-bound}). In order to state and prove this bound, we use the method of \emph{random restrictions}, a key technique for understanding bounded-depth circuits \citep[e.g.][]{hastad1994optimal, furst1984parity, boppana1997average} and transformers \citep{hahn2020theoretical}.
\begin{definition}\label{def:restriction}
    Let $N$ be an input length.
    A \emph{restriction} $\rho$ is a a family of maps $\rho_N : [1,N] \rightarrow \Sigma \cup \{*\}$, for $N=1,2,\dots$.
    %
    We write $\rho\Sigma^*$ for the set of strings    $x \in \Sigma^*$ where $x_i = \rho_{|x|}(i)$ whenever $\rho_{|x|}(i) \neq *$.
    %
%
%
%
%
%
\end{definition}
We show that the existence of a sublinear-length CoT has strong implications about the function computed.
\begin{theorem}[Generic CoT Bound]\label{thm:uhat-cot-bound}
Assume that $f$ has a UHAT-expressible CoT $g(x)$ of length $|g(x)| = o\left(|x|\right)$.
%
Choose any $C \in (0,1)$. Then there is a restriction $\rho$ such that
\begin{enumerate}
        \item $|\{i \leq N : \rho_N(i) = *\}| \geq CN$ for  sufficiently large $N$
        \item For each $N$, $f$ is constant on $\Sigma^N \cap \rho\Sigma^*$
    \end{enumerate}


%
%
%
\end{theorem}
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
The first condition says that $\rho$ leaves a large fraction of input positions open; the second condition says that it is nonetheless sufficient for fixing the output.

A simple example of a UHAT-computable function is the AND function, where fixing a single bit to $0$ fixes the output to $0$; it is easily computed by a UHAT transformer where a single attention head attends to an occurrence of $0$ if any exists. Even though every bit can potentially matter to the output, this is easily computed in UHAT without CoT.
On the other hand, the \textsc{Parity} function cannot be fixed by fixing any strict subset of the input bits; hence, Theorem~\ref{thm:uhat-cot-bound} entails a linear-length lower bound for a CoT there.



%


%
%
%
%
%
%

%
%

Theorem~\ref{thm:uhat-cot-bound} is shown via Lemma~\ref{lemma:restriction-strengthened}, an analogue of the classical Switching Lemma \citep{hastad1994optimal} for $AC^0$ circuits, which permits collapsing bounded-depth circuits into shallow circuits on $\rho\Sigma^*$. Like that classical lemma, it is based on randomizing $\rho$ and showing that the probability of satisfying the desired properties is nonzero; hence, a satisfying $\rho$ exists.
\begin{lemma}\label{lemma:restriction-strengthened}
    Given a UHAT transformer $T$ operating over the finite alphabet $\Sigma$, and $C \in (0,1)$, there is $c \in \mathbb{N}$, $K \in \mathbb{N}$ ($c$ and $K$ each only depending on the number of layers and heads, and $|\Sigma|$), and a restriction $\rho$ such that
    \begin{enumerate}
        \item $|\{i \leq N : \rho_n(i) = *\}| \geq CN$ if $n \geq K$
        \item On $\Sigma^N \cap \rho \Sigma^*$, each activation $\vy_i^{(l)}$ ($i \in [1,N]$, $l\in[1,L]$) depends only on at most $c$ input positions
    \end{enumerate}
\end{lemma}
The lemma is a strengthening of Theorem~1 of \citet{hahn2020theoretical}, the proof is in Appendix~\ref{app:uhat-cot-bound}. Note that Lemma~\ref{lemma:restriction-strengthened} applies to transformer computations without a CoT, whereas Theorem~\ref{thm:uhat-cot-bound} broadens the scope to functions computable with a $o(N)$-length CoT.
In order to deduce  Theorem~\ref{thm:uhat-cot-bound}, we first apply Lemma~\ref{lemma:restriction-strengthened} to obtain a restriction on the input $x$ itself (not the CoT tokens).
We then show that it is possible to fix $c \cdot H \cdot L \cdot |g(x)|$  further input symbols to fix \emph{every} symbol appearing in the CoT, and hence the function output $f(x)$. If $|g(x)| = o(|x|)$, the resulting restriction still leaves a constant fraction of input positions free (say, $\frac{9}{10} C N$ positions).
We provide the full argument in Appendix~\ref{app:uhat-cot-bound}.

%
%
%
%
%
%
%
%
%
%
%






%
%

%


\section{Results: Application to Algorithmic Problems}
We now proceed to proving explicit lower bounds for various algorithmic problems.
For each problem, we first examine sensitivity to establish difficulty for transformers based on the results reviewed in Section~\ref{sec:sensitivity} (independent of unproven conjectures about $TC^0$).
For high-sensitivity problems, we then use Theorem~\ref{thm:uhat-cot-bound} to lower-bound CoT length, and construct an explicit CoT that matches the bound up to logarithmic factors.

\subsection{Lower Bounds for Regular Languages}\label{sec:parity}

\begin{definition}
    \textsc{Parity} takes an input $x \in \{0,1\}^N$, and decides if the number of 1's is even or odd.
    %
\end{definition}

\textsc{Parity} is the archetype of a highly-sensitive function, with $as_n(\textsc{Parity}) = n$, and
%
it has long been documented empirically that transformers struggle with it \citep[e.g.][]{bhattamishra2020ability, deletang2022neural, anil2022exploring, butoi2024training}:
We provide the following lower bound:
\begin{theorem}\label{thm:parity-bound}
Any UHAT CoT for \textsc{Parity} has length $\Omega\left(N\right)$.
%
%
%
%
\end{theorem}
The proof is a direct consequence of Theorem~\ref{thm:uhat-cot-bound}: as long as one does not fix \emph{all} input symbols, one can never fix the output of \textsc{Parity}.
%
%
The UHAT bound is tight, and is attained by the straightforward CoT consisting of the parities of increasing prefixes of the input.
%
%
%


%
We note that Theorem~\ref{thm:parity-bound} is substantively different from bounds based on learnability arguments applying to \emph{subset parities} \citep[e.g.][]{kim2024transformers, abbe2024how, abbe2023generalization, wies2022sub}, distinct from the \textsc{Parity} function applying to the full input (see Section~\ref{sec:disucssion} for discussion).
%
%
%

\paragraph{Why is Theorem~\ref{thm:parity-bound} nontrivial?}
At first sight, one might wonder if Theorem~\ref{thm:parity-bound} is trivial: Every bit matters for \textsc{Parity}, and a hard attention head can only attend to a single position; hence, one might be tempted to argue that a linear number of CoT steps is trivially needed to take every bit into consideration. However, importantly, the behavior of the attention heads itself is input-dependent and can in principle be influenced by every input bit. While each head in UHAT attends to only one position, the entire input globally determines which position this is (e.g., for the boolean AND function of $N$ bits, even though every bit can matter, a single head is enough).
Key to showing Theorem~\ref{thm:parity-bound} is that it is sufficient to fix a fraction of input bits to ``distract'' all attention heads and prevent them from considering the full input.
More broadly, our results establish a dichotomy on simulating finite-state automata:
\begin{corollary}\label{thm:finite-state}
    Let $f$ be the membership problem of a finite-state language $L$. Then exactly one of the following holds:
    \begin{enumerate}
        \item $L \in AC^0$ and $f$ is expressible in UHAT without CoT
        \item $L \not\in AC^0$, and any UHAT CoT for $f$ has length $\Omega(N)$
    \end{enumerate}
\end{corollary}
The proof is in Appendix~\ref{app:finite-state}.
Importantly, $f$ is affected by the sensitivity-based barrier discussed in Section~\ref{sec:sensitivity} if and only $L \not\in AC^0$ (Remark~\ref{remark:finite-state-sensitivity}).
Overall, this result complements results on transformer shortcuts to automata \citep{liu2022transformers} by establishing when a shortcut (i.e., a solution with CoT) can be represented well by transformers.
%


%


%
%
 %

%
%

%
%
%


%

%

%


%
%

%
%
%
%
%
 %
%
%

%
 %
  %

%
%
%
%
%
%
%
 %
%


%


\paragraph{Experiments}
%
%
%
We trained transformers on \textsc{Parity} on lengths between 1 and 500 (Figure~\ref{fig:parity} and Appendix~\ref{app:parity-experiment}; results averaged across three runs). 
We considered the full CoT from Figure~\ref{fig:motivating-figure} (dots along diagonal line).
We also considered CoTs where only every $k$-th step of the CoT was provided, shortening the CoT by $1/k$ (dots below diagonal line).
Across input lengths, the model is successful when $k \lessapprox 3$.
%
We also verified that two LLMs (DeepSeek-R1, \citet{r1}, and o1-mini, \citet{jaech2024openai}) produce CoTs of at least linear length (Appendix~\ref{app:pretrained}).



\begin{figure}
\centering
Full CoT \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Dot-by-Dot CoT

\includegraphics[width=0.23\textwidth]{plots/parity.pdf}
\includegraphics[width=0.23\textwidth]{plots/parity_Dot.pdf}


    \caption{Results for \textsc{Parity}. Left: CoTs consisting of prefix sum parities, accuracy as a function of input and CoT lengths. CoTs of length $\Theta(N)$ succeed even on very long inputs. Right: Dot-by-dot scratchpads help much less, even when much longer then the input (Theorem~\ref{theorem:dot-by-dot}). %
    }\label{fig:parity}
\end{figure}



\subsection{Multiplication}\label{sec:multiplication}


We next examine basic arithmetic operations. For $N$-digit numbers, both addition and multiplication are in $TC^0$, and both operations can in principle be expressed by soft-attention transformers \citep{feng2024numerical}, but empirical research has found transformers to succeed much better at addition than at multiplication, which remains hard for transformers \citep{yang2023gpt}.
Addition has low sensitivity and can be represented in UHAT and fixed precision \citep{feng2024numerical};
%
multiplication is shown expressible by unbounded-precision transformers but not fixed-precisioin transformers \citep{feng2024numerical}. %
%
%
We examine the difficulty of computing each of the $2N$ output digits in multiplication.
\begin{definition}
Given two $N$-digit integers $X, Y$ encoded in binary,
let $M_k$ be the $k$-th bit of the binary representation of the product $XY$.
\end{definition}
Multiplication has high sensitivity; most importantly and interestingly, it turns out that digits in the middle of the result are particularly sensitive (Figure~\ref{fig:sensitivity-multiplication}). For these:
\begin{theorem}\label{thm:multiplication}
%
%
A UHAT CoT for $M_N$ requires length $\Omega(N)$.
%
\end{theorem}
The proof is in Appendix~\ref{sec:multiplication-lower-bound-proof}.
Importantly, addition faces no such hurdle, as any digit has at most polylogarithmic average sensitivity due to the existence of an $AC^0$ (and UHAT) construction for adding $N$-digit numbers without CoT \citep{feng2024numerical}; thus, transformers can output each digit in parallel better for addition than for multiplication (Figure~\ref{fig:arithmetic-comparison}).


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{plots/Digit_sensitivity_autoregressive.png}
%
    \caption{Accuracy of a transformer trained on autoregressive 12-digit multiplication (most significant digit at the left, zero-padded on the left), on a test set with 10K number pairs. High-sensitivity digits in the middle show substantially decreased accuracy even if digits at beginning and end are predicted exactly.
    See Figure~\ref{fig:multiplication-accuracy-by-digit} for more.
    }
    \label{fig:multiplication-accuracy-by-digit-main}
\end{figure}


Given our $\Omega(N)$ lower bound (Theorem~\ref{thm:multiplication}), what is the best upper bound?
Existing work providing scratchpads for transformers performing multiplication \citep{hou2024universal} uses the naive grade-school algorithm, which has $\Theta(N^2)$ complexity, much  larger than the $\Omega(N)$ lower bound. In fact, there is a UHAT scratchpad matching the $\Omega(N)$ lower bound up to a logarithmic factor:
\begin{theorem}\label{thm:fast-multipl-scratchpad}
    There is a UHAT scratchpad for the full product, $M_1 \dots M_{2N}$, with length $\Theta(N \log N)$.
\end{theorem}
The construction is based on the Fourier transform (Appendix~\ref{app:mult-scratchpad}).
%


\paragraph{Remarks} Theorem~\ref{thm:multiplication} concerns direct decoding of an individual digit $M_N$; we comment on barriers on \emph{autoregressive} decoding of products in Appendix~\ref{app:autoregressive-mult}.
Recent work has found that custom positional encodings which match structurally corresponding digits to help with addition \citep{zhou2023algorithms,mcleish2024transformers,cho2024position,cho2024arithmetic,sabbaghi2024explicitly}.
Theorem~\ref{thm:multiplication} holds for arbitrary positional encoding vectors, suggesting that, while beneficial in the case of \textsc{Addition}, specialized positional encodings are insufficient to overcome the difficulty of \textsc{Multiplication}.

\paragraph{Experiments}
We trained small transformers (2 layers, 2 heads) to multiply binary numbers with up to 16 digits, either directly, or with the $\mathcal{O}(N \log N)$ CoT from Theorem~\ref{thm:fast-multipl-scratchpad}.
Multiplication failed to generalize when no CoT was given (Figure~\ref{fig:arithmetic-comparison}), with difficulty driven by the high-sensitivity middle digits (Figure~\ref{fig:multiplication-accuracy-by-digit-main}). The CoT succeeded at all lengths (Table~\ref{tab:scratchpad_accuracy}).
See Appendix~\ref{app:experiments-multiplication} for details.
%
%
%
We also verified that two LLMs (DeepSeek-R1 \cite{r1} and o1-mini \cite{jaech2024openai}) produce CoTs of at least linear length (Appendix~\ref{app:pretrained}).


%
%
 %
%

%
%
%
%

%
%
%
%
%


\subsection{Order Statistics}\label{sec:order-statistics}
We next consider another task in $TC^0$, which concerns finding order statistics.
%
\begin{definition}
%
Consider the \textsc{Median} task, where the input consists of $N$ numbers, each with $B$ bits, 
%
and the target is the $\lfloor N/2\rfloor$-th number when the list is sorted.

%
%
 %

    %
\end{definition}
$TC^0$ circuits solve this by selecting the number that is simultaneously smaller than and greater than $\lfloor N/2 \rfloor$ other numbers. %
%
However, the median, especially its last bit, is sensitive to alterations of the $N$ integers (Appendix~\ref{app:theory-order-statistics}). %
We obtain a CoT bound:
\begin{theorem}\label{thm:median}
%
%
For the \textsc{Median} task, a UHAT scratchpad requires length $\Omega(N)$. This bound is attained. %
%
%
%
%
%
%
\end{theorem}
The proof is in Appendix~\ref{app:theory-order-statistics}; 
%
%
%
%
the bound is attained by a CoT enumerating the lowest $\lfloor N/2\rfloor$ numbers. %

%


%

\paragraph{Experiments} 
We trained transformers on \textsc{Median} on input lengths between 6 and 398 (Figure~\ref{fig:median} and Appendix~\ref{app:median-exp-detail}), corresponding to 1 to 99 decimal numbers (each with $B$ digits). We considered the full CoT (upper most dots in each input length). Like \textsc{Parity}, we also considered CoTs where only every $k$-th step of the CoT was provided (starting from the smallest number), shortening the CoT by $1/k$ (dots below the upper most dots in each input length).
Across input lengths, the model is always successful when using the full CoT, and there is a chance of failure when $k\geq2$ (accuracies are averaged over 3 runs). %

\subsection{Graph Reachability}\label{sec:reachability}
Another foundational aspect of reasoning is graph reachability. %
\begin{definition}
    Given a set of vertices $V$ and a set of directed edges $E \subseteq V\times V$, the Reachability task is defined as follows.  \emph{Input}: Given some numbering of $V$, we assume a list of all edges $E$, each coded as a pair of $\log V$-digit binary numbers.
    We also assume a query pair of two vertices $i, j$.
\emph{Output}: A binary label, indicating if there is a path from $i$ to $j$.
\end{definition}
The reachability problem in graphs, even in directed acyclic graphs (DAG), is ${\bf NL}$-complete \citep{jones1976new}; %
hence, by the unproven conjecture $TC^0\neq NC^1$, is expected not to be solved by transformers without CoTs. We provide an \emph{unconditional lower bound}, even for a sub-family of graphs for which the reachability problem is solvable in $TC^0$, by observing that \textsc{Parity} is reducible to DAG reachability:

\begin{theorem}\label{thm:reachability}
There is a family $\mathfrak{G}$ of DAGs inside which reachability is solvable in $TC^0$, but cannot be represented by a transformer at sublinear average sensitivity. A UHAT CoT needs length $\Omega\left(|E| \log |V|\right)$. 
This bound is attained.
%
\end{theorem}
The proof proceeds by coding \textsc{Parity} into DAGs with $2N$ vertices (lower bound), and coding breadh-first search into a CoT (upper bound), see Appendix~\ref{app:theory-dag-reachability}.
%

\paragraph{Experiments} We generate random DAGs with sizes ranging from 5 to 35 vertices and use them to train Transformers to solve a DAG reachability task in the general case with a $\mathcal{O}(|E| \log |V|)$-length scratchpad. The results, presented in Figure \ref{fig:dag}, show that a scratchpad of this length is indeed sufficient to solve the task. In contrast, a model trained without a scratchpad performs at chance level.
%
See Appendix \ref{app:dag-experiments} for details.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{plots/dag_reachability.pdf}}
\caption{Results for DAG reachability. Without a CoT, the model performs at chance level for all but very small graphs. An $\mathcal{O}(|E| \log |V|)$-length CoT leads to a perfect accuracy independent of the graph size.}
\label{fig:dag}
\end{center}
\vskip -0.2in
\end{figure}

%


%

%
%
%

%

%
 %

 %

    
%

%

%



\subsection{Limitations of Dot-by-Dot CoTs}\label{sec:dot-by-dot}

Recent work has observed a benefit even for CoTs consisting just of repetition of a single token (``pause token''), which might give a transformer the opportunity to perform extra computations even without outputting any intermediate steps \cite{goyal2024think,pfau2024lets}.
While the power of such ``dot-by-dot'' CoTs remains in $TC^0$, provided their length is polynomially bounded, they may still enable additional computations \cite{pfau2024lets}, which raises the potential challenge of unauditable unobservable computations in LLMs.
Our methods result in additional barriers for such CoTs, suggesting that high-sensitivity computations require a substantial degree of reliance on \emph{explicit} CoT tokens. Specifically, for \textsc{Parity}, such a CoT cannot have polynomially bounded length: %
%
\begin{theorem}\label{theorem:dot-by-dot}
Consider a UHAT-expressible CoT for \textsc{Parity} where $g(x)$ has the form $.\dots . \# f(x)$. (``dot-by-dot CoT'').
Then $|g(x)| = \omega(|x|^k)$ for any $k \in \mathbb{N}$.
%
%
 %
    %
  %
%
\end{theorem}
The proof is in  Appendix~\ref{app:theory-dot-by-dot}.
Complementing this lower bound, an exponentially-sized UHAT CoT does exist in principle (Appendix~\ref{app:theory-dot-by-dot}).
Overall, we have thus a super-polynomial separation between the lengths required for full CoTs ($\Theta(N)$) and dot-by-dot CoTs ($\omega(poly(N))$)
\paragraph{Experiments}
Figure~\ref{fig:parity}B shows that a dot-by-dot variant of the CoT for \textsc{Parity} is not successful at expanding the lengths at which a transformer is learned successfully.

%
%
%


%

%

%




%
%
  %
%



\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{plots/median.pdf}
    \caption{Results for \textsc{Median}: CoTs consisting of the sorted first $\lfloor N/2 \rfloor$ numbers. Accuracy as a function of input and CoT lengths. 
    %
    }
\label{fig:median}
\end{figure}


%
%
%
%

%
%

 %
%



%
%

%
%

%
%

%

\section{Discussion}\label{sec:disucssion}



%
%

%

%
%
%

%

\subsection{Implications}
Across Sections~\ref{sec:parity}--\ref{sec:reachability}, our lower bounds are tight up to polylogarithmic factors.
By showing that the UHAT CoT length must scale with the input length, they provide barriers on the possibility for self-attention to solve these tasks with small inference-time compute in generating additional tokens.

An attractive approach for shortening CoT reasoning and reducing inference-time compute is by fine-tuning transformers to perform reasoning in fewer and fewer steps \citep{deng2024explicit}.
Our results show that such a process is likely to run up against representational limitations of the architecture. 
%
The findings demonstrate efficiency limitations to CoT-based reasoning that might be overcome via tool use or stronger architectures.
Logarithmic growth of the depth may be one way to overcome these \cite{merrill2024little}; within fixed-depth architectures,  certain SSM architectures overcome at least the difficulty of \textsc{Parity} \citep{grazzi2024unlocking}. It remains to be understood if this applies to the other problems from Sections~\ref{sec:parity}--\ref{sec:reachability}.

%


%

%
%
%
%

\subsection{Related Work}


\paragraph{Theoretical Understanding of Scratchpads and CoT}
The first theoretical study of scratchpads for transformers is by \citet{perez2019turing}, who showed that transformers can autoregressively simulate the computations of a Turing machine. %
More recently, work has shown that polynomial-length CoT can allow transformers to transcend the complexity class $TC^0$ \citep{feng2023towards, merrill2023expresssive, li2024chain}, and express all problems in PTIME, which by widely believed but unproven conjectures far exceeds $TC^0$.
There are also lower bounds for scratchpads  for \emph{single-layer} transformers \cite{peng2024limitations,barcelo2025ehrenfeucht}, showing that a one-layer transformer (both the input itself and the CoT are processed by a single layer) requires a substantial number of steps to solve certain problems, such as iterated function composition.
\citet{chen2024theoretical} extended this line of work to multi-layer transformers, proving a benefit for CoT in iterated function composition.



Another angle to understanding CoTs is via learnability arguments \citep{wies2022sub,kim2024transformers,hahn2023theory,abbe2024how}.
%
A particularly promising angle is the \emph{globality degree} \cite{abbe2024how}, with potentially broad implications for scratchpads, though resulting lower-bounds remain largely conjectural beyond special cases (Appendix~\ref{app:globality}).
Interestingly, the globality degree is linked to our sensitivity-based techniques and could in principle lead to stronger bounds; a future full proof of the main conjecture of \citet{abbe2024how} would, in combination with our unconditional results here, entail even further results (Appendix~\ref{app:globality}).

%
%
%

\paragraph{Lower bounds for transformers}
Lower bounds for UHAT transformers primarily rest on random restrictions, either directly \citep{hahn2020theoretical, barcelo2024logical} or via reduction to known circuit bounds proven with those \citep{hao2022formal}.
A key technical step in our work is to expand the reach of the random restriction technique to transformers including CoTs.
%
Another approach to obtaining lower bounds relies on circuit conjectures from computational complexity \citep{merrill2023parallelism, sanford2024transformers} and is thus conditional on these (widely believed) conjectures.
There are further techniques for single-layer transformers \citep{kozachinskiy2024lower, peng2024limitations, sanford2023representational, Bhattamishra2024Separations, barcelo2025ehrenfeucht}, for autoregressive transformers \citep{chen2024theoretical}, and for length-generalizng transformers \citep{huang2024formal} relying on communication complexity or VC dimension. 
%

\paragraph{Relation to Results on Subset Parities}
Our results concerning \textsc{Parity} are distinct from results on \emph{Subset Parities}.
A long string of results has established difficulty of learning subset parities -- that is, functions $\chi_U$ for $U \subseteq \{1, \dots, N\}$ -- in various setups, and established benefits for providing intermediate steps \citep[e.g.][]{wies2022sub, abbe2023generalization, kim2024transformers, anonymous2024chainofthought}.
Intuitively, the difficulty of learning subset parities emerges from the fact that one has to identify the set $U$ from the exponentially-large power-set.
In learning such functions, there is a provable benefit to providing intermediate steps in training \citep{wies2022sub, abbe2023generalization, kim2024transformers, anonymous2024chainofthought}.
However, this argument does not readily imply that causal transformers will find the \textsc{Parity} function difficult to learn and that CoTs will help there, because the \textsc{Parity} function applies to a very specific set ($U = \{1, \dots, N\}$).
Our results thus are distinct from results on subset parities due to \citet{wies2022sub, abbe2023generalization, kim2024transformers, anonymous2024chainofthought}.
%
Interestingly, \citet{abbe2024learning} show that \textsc{Parity} may be easy to learn for MLPs with Rademacher initialization; however, this requires a weight norm substantially growing with $N$, making transformer length generalization unlikely \citep{huang2024formal}.

%


%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%



%


%

%

%


%


%


%
%
%

%

%

%

%


%

%
%


%



%
%
%
%
%
%


%
%
%
%
%
%
%

\subsection{Limitations and Open Questions}
One limitation of our bounds is that they apply in the worst-case setting. Providing average-case bounds is an interesting problem for future research.
A second limitation of our results is that they focus on the UHAT model, while there are other formal models of transformers (e.g. log-precision transformers, \citet{merrill2023logic}).
Our results pose the open question whether some other model might enable substantially shorter but still learnable CoTs:
\begin{problem}\label{prob:sublinear}
    Are there sublinear-length CoTs for any high-sensitivity problem from Sections~\ref{sec:parity}--\ref{sec:reachability} that are expressible by some formal model of transformers (other than UHAT; e.g., log-precision transformers), while being practically learnable?
\end{problem}
Solving this problem in the affirmative might have substantial implications for real-world LLMs, and any such solution is likely to require further progress in understanding the learnability properties of transformers.
%
%
On the other hand, a negative answer might suggest that tool use or architectural improvements are likely to be unavoidable for transformer-based LLMs even on many $TC^0$ problems, due to architectural limitations that make successful CoTs impractically long.



\section{Conclusion}
The success of scratchpad and chain-of-thought techniques raises the need to understand their strengths and limitations.
Here, we have provided lower bounds for the number of steps required for CoT reasoning in various fundamental algorithmic problems, in the UHAT model.
These bounds are tight up to polylogarithmic factors, and are attained by realistically trainable models.
%
%
Taken together, our results provide theoretical bounds on the ability of transformers to solve reasoning problems with CoT.

%
%
 
\section*{Author Contributions}
MH coordinated the project.
All authors contributed to the conceptual framework.
Experiments were contributed by AA (Section~\ref{sec:multiplication}), 
XH (Section~\ref{sec:order-statistics}), MR (Section~\ref{sec:reachability}), and MH (Sections~\ref{sec:cot-bounds}, \ref{sec:parity}, and \ref{sec:dot-by-dot}).
MH contributed the bounds for UHAT transformers, with input from the other authors.
AA and MH jointly developed theory in Section~\ref{sec:multiplication}.

\section*{Acknowledgments}
Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 232722074 – SFB 1102.
The authors thank Satwik Bhattamishra, Will Merrill, Sophie Hao, Anej Svete, Franz Novak, Ryan Cotterell, and Andy Yang for discussion, and Yash Sarrof and Entang Wang for feedback on the paper.



\bibliography{literature}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn


\cleardoublepage

%
%
\addtocontents{toc}{\protect\setcounter{tocdepth}{3}} %
\renewcommand{\contentsname}{Appendix Contents} %
\tableofcontents %



%


\section{FAQ}

\begin{enumerate}

\item \textit{Isn't it obvious that one needs at least a linear number of steps to solve these algorithmic problems?}

In general, serial computation models such as Turing machines need at least a linear number of steps in order to solve tasks that require knowing the full input.
The situation is different for parallel computation as performed by transformers: Many problems do have direct parallel solutions. For instance, for all regular languages expressible in the circuit complexity class $AC^0$, transformers can express these in the UHAT model without CoT (Theorem~\ref{thm:finite-state}), essentially via parallel `shortcuts'. Our results are notable in that they establish that many algorithmic problems do require linear-length CoTs, with barriers on the possibility of substantial parallel shortcuts.

\item \textit{All lower bounds in this paper are essentially on the order of $\Omega(N)$ or $\Omega(N \log N)$. Do any algorithmic problems require substantially super-linear (e.g., quadratic) CoTs?}

This question is closely linked to deep unsolved questions at the heart of computational complexity. Due to the Turing completeness of transformer CoTs, proving such  lower bounds on CoTs would entail superlinear (e.g., quadratic) lower bounds on multitape Turing machine time complexity, which has been extremely challenging, even for NP-complete problems.

\item \textit{Transformers are already known to be Turing-complete. Why does this paper construct CoTs for the algorithmic problems -- isn't reducing to known Turing machine constructions enough?}

Compared to modern random-access models, single-tape Turing machines (as used in typical Turing machine completeness proofs for transformers) require substantial overhead to process data structures central to many algorithmic problems. For instance, a straightforward implementation of the queue used in breadth-first search (Theorem~\ref{thm:reachability}) takes a quadratic number of steps on a single-tape Turing machine, as the head needs to repeatedly move between start and end position of the queue. It is thus not immediate from Turing completeness that these problems can all be solved at (near-)linear CoT lengths. Multi-tape Turing machines (which can also be coded into CoTs) may provide substantially better bounds, but CoTs derived from Turing machine constructions are still likely to have substantial overhead (even if just constant factors). In contrast, we show that explicit constructions on the four algorithmic problems studied can be practically learned, confirming that the upper bounds are meaningful.


%

\item \textit{Implementations of self-attention already have quadratic complexity. Why should one be concerned with a further linear number of CoT steps?}

It is true that generating $N$ tokens, with KV-Cache, will have just $\mathcal{O}(N^2)$ complexity, asymptotically the same as directly providing an answer. However, it can still lead to substantial overhead, even if just by a constant factor, if the number of CoT tokens grows appreciably with $N$ (compare Figure~\ref{fig:llm-parity-multiplication}). Thus, any potential sublinear CoTs would be of great interest in increasing efficiency; our results demonstrate barriers to such solutions. 



%
%



\item \textit{What is the practical impact of the results? Do the results relate to any specific NLP tasks?}


\textsc{Parity}, \textsc{Multiplication}, \textsc{Median}, \textsc{Reachability} are foundational problems, which instantiate simple models of reasoning problems that have been of broad interest.
For instance, \textsc{Parity} is a simple example of state tracking, a family of reasoning problems that have been of substantial interests and that still pose challenges for LLMs \citep[e.g.][]{merrill2024the,kim2023entity, hsieh2024ruler}.
There also has been a large amount of interest in the ability of transformers and LLMs to perform arithmetic such as \textsc{Multiplication}, as it is a fundamental ingredient of mathematical reasoning.
\textsc{Reachability} is a simple case of search, which is foundational to many aspects of reasoning, and transformers' abilities to perform such problems have been an object of interest \citep[e.g.][]{lehnert2024beyond}.
%
On all these problems, our results entail barriers on the possibility of transformer algorithms avoiding substantial CoTs.

\item \textit{Definition~\ref{def:cot} enforces a finite input alphabet, but allows an infinite CoT alphabet. Why?}

The input alphabet needs to be finite for Theorem~\ref{thm:uhat-cot-bound} to go through, because the proof of Lemma~\ref{lemma:depth-reduction-lemma} by \citet{hahn2020theoretical} involves a union bound over the alphabet.
On the other hand, this is not needed for the CoT tokens.
Some prior work \citep{bhattamishra2020computational, abbe2024how} allows the CoT vocabulary to grow with the input length. Theorem~\ref{thm:uhat-cot-bound} can accommodate this, and we indeed find this useful in our constructions.


\item \textit{Definition~\ref{def:uhat-computable} allows using different transformers $T$ at every input length. Isn't this unrealistic? What about the role of length generalization?}

It is true that, in practice, one will expect transformers to perform the tasks across input lengths.
However, the theoretical literature on transformers has developed different approaches to formalize this.
Notably, a single transformer at fixed width may have trouble solving a single task across unboundedly many input lengths unless very specific positional encodings are used \citep[e.g.][]{merrill2023expresssive}, and work has thus often considered cases where the width may grow with the length \citep[e.g.][]{liu2022transformers, huang2024formal, Bhattamishra2024Separations}.
To accommodate such variability, we aim for the most general setup under which we can prove lower bounds. We hence expect only the depth and the number of heads to stay constant, but allow everything else to grow with the input length. This is the most general setup under which our techniques allow us to show Theorem~\ref{thm:uhat-cot-bound}.

\item \textit{ %
The results in this paper are proven for a hard-attention model, whereas real-world implementations use softmax attention, which can express functions that hard-attention transformers cannot. In particular, UHAT is bounded by $AC^0$, whereas softmax transformers are bounded by $TC^0$. Given this difference, what do the results entail about real-world LLMs?}

It is true that softmax attention can express some functions that hard attention (and $AC^0$) cannot, such as the \textsc{Majority} function.
On the other hand, $TC^0$ is an overly optimistic upper bound on practical abilities of transformers, as many $TC^0$ functions are not practically learned by transformers due to the sensitivity limitations discussed in Section~\ref{sec:sensitivity}.
Hence, we focused on tasks for which existing results  based on average sensitivity \citep{hahn2024sensitive} entail that transformers  will struggle across different formal models of self-attention, irrespective of differences in expressive capacity (Section~\ref{sec:sensitivity}), including problems within $TC^0$.
Thus, for the tasks considered here, a CoT is practically needed irrespective of the specific formalization of self-attention; this is also confirmed by our experiments, where transformers consistently failed in the absence of CoTs.
We further note that existing CoT constructions from the literature can by and large be expressed well using hard-attention operations (Appendices~\ref{app:cot-literature} and \ref{app:universality}).
In LLM experiments, we observed algorithms in line with our theoretical constructions (Appendix~\ref{app:pretrained}).
Our results thus place strong constraints on any possibility of evading linear lower bounds.
In principle, it is possible that other formal models of transformers allow substantially faster CoTs on some algorithmic problems while maintaining practical learnability (Problem~\ref{prob:sublinear}); rigorously proving or refuting the existence of such solutions is likely to require substantial advances in understanding transformers' learnability properties.

%
%
%
%

 

%




\item \textit{CoT lower bounds are proven for hard attention, while experiments are conducted with softmax attention. Isn't there a mismatch between theory (hard attention) and experiments (softmax transformers)?}

%

It is true that we do not conduct experiments in the hard-attention setup, because there is no efficient training procedure for that setup.
Our experiments, conducted in the practical softmax attention setting, complement the theoretical results in two ways:

\begin{enumerate}
%
\item First, transformers consistently failed in the absence of CoT on the four algorithmic tasks. This supports the theoretical prediction that a CoT improves transformers' ability to solve these tasks.
\item Second, by showing that CoTs implementing the theoretical upper bounds can be practically learned, we demonstrate the real-world meaningfulness of our theoretical bounds.

%
%
\end{enumerate}



   %
\end{enumerate}








\section{General Theoretical Results}




\subsection{Proof of Theorem~\ref{thm:uhat-cot-bound}: Generic CoT  Bound (Main Result)}\label{app:uhat-cot-bound}

Here, we prove our main lower bound (Theorem~\ref{thm:uhat-cot-bound}), which provides a generic condition under which sublinear CoTs can be possible. Our techniques build on lower-bounding methods for bounded-depth circuits \citep{furst1984parity, hastad1994optimal}.
Recall the definition of restrictions (Definition~\ref{def:restriction}).


%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
 %
%

%

%

\begin{definition}
    We say $\rho' \succ \rho$ if, whenever $\rho'_n(i) = *$, then $\rho_n(i) = *$ ($1 \leq i \leq n$).
\end{definition}

\begin{definition}[$c$-Transformer]
A $c$-Transformer conforms to the definition of transformers from Section~\ref{sec:model-transformers}, except that each $\vy_i^{(0)}$ is a function of $\leq c$ input positions.
That is, there are functions $f_i$ such that: %
\begin{equation}
\vy_i^{(0)} = f_i(x_{1\dots N})
\end{equation}
and $f_i$ depends on at most $c$ of its inputs.
\end{definition}

We build on the following lemma shown by \citet{hahn2020theoretical},  rephrased for self-containedness. It can be viewed as a transformers analogue of the Switching Lemma \citep{hastad1994optimal} for bounded-depth circuits:
\begin{lemma}[Depth Reduction Lemma]\label{lemma:depth-reduction-lemma}
    Given a UHAT $c$-Transformer $T$ with $L$ layers, a constant $C \in (0,1]$, and  restrictions $\rho_n = \rho_1, \rho_2, \dots$ such that
\begin{equation}
|\{i \leq n: \rho_n(i) = *\}| \geq Cn
\end{equation}
%
for all sufficiently large $n$.

Choose any $C' \in (0,C)$.
Then there are restrictions $\rho_1' \succ \rho_1, \rho_2' \succ \rho_2, \dots$ 
such that
\begin{equation}
|\{i \leq n: \rho'_n(i) = *\}| \geq C'n
\end{equation}
for all sufficiently large $n$, 
and such that there is a $ c\cdot(|\Sigma|^ckH+1)$-transformer $T'$ with $L-1$ layers, for some integer $k$ (depending on $C'$), %
such that, for each $i \in [1,n]$ and each $l \in [1,L]$,
\begin{center}
$\vy_i^{(l)}$ in $T$
\end{center}
equals
\begin{center}
$\vy_i^{(l-1)}$ in $T'$
\end{center}
whenever the input is in $\rho'\Sigma^*$. %
\end{lemma}
\begin{proof}
    This is a stronger statement of Lemma 4 of \citet{hahn2020theoretical}.
    That lemma only stated that the final output $\vy_n^{(L)}$ was preserved in $T'$. However, by the way $T'$ is obtained in that proof\footnote{``We can thus remove layer 0, convert layer-1 activations $\vy_j^{(1)}$ into layer-0 activations $\vy_j^{(0)}$, and obtain a $(c\cdot(2^ckH+1))$-transformer performing the same computation as before when $\rho^{(3)}$ is applied.'' \citep[quoted from][]{hahn2020theoretical}. Here, ``$\rho^{(3)}$'' corresponds to the restriction $\rho'$ from the statement of the lemma.}, the stronger statement for all activations follows.
    We also note that the original statement considered $\Sigma = \{0,1\}$, but the proof transfers to arbitrary finite alphabets $\Sigma$ without change, except that $T'$ is now a  $ c\cdot(|\Sigma|^ckH+1)$-transformer, rather than a $ c\cdot(2^ckH+1)$-transformer.

    We note that \citet{hahn2020theoretical} did not assume causal masking, whereas we are assuming it by default (Section~\ref{sec:model-transformers}). Causal masking can be easily simulated in UHAT, by setting up positional encodings in such a way that $a_{i,j}^{(k,h)}$ is extremely small whenever $j > i$. Hence, the proof of the Depth Reduction Lemma continues to work in the presence of hard-coded causal masking.
\end{proof}
This lemma entails the following fact, a strengthening of Theorem 1 of \citet{hahn2020theoretical}:
\begin{lemma}[Restated from Lemma~\ref{lemma:restriction-strengthened}]
\label{app:lemma-restriction-strengthened}
    Let $T$ be a UHAT transformer operating over a finite alphabet.
    Take any is $C \in (0,1)$.
    Then there is $c \in \mathbb{N}$, $k \in \mathbb{N}$ such that, for each $N > k$, there is $\rho$ such that
    \begin{enumerate}
    \item $\left|\{i : \rho(i) = * \}\right| \geq CN$
        \item within $\rho\Sigma^*$, each $\vy_i^{(l)}$ is determined by $\leq c$ input positions
    \end{enumerate}
\end{lemma}
\begin{proof}
By applying Lemma~\ref{lemma:depth-reduction-lemma} iteratively, $L$ times.
\end{proof}

\begin{remark}
    A reader might wonder why, instead of introducing Lemma \ref{app:lemma-restriction-strengthened}, one cannot simply apply Theorem 1 of \cite{hahn2020theoretical} independently to each layer of a Transformer. The reason is that the latter approach would only ensure the existence of $L$ independent restrictions -- one for each layer --whereas Lemma \ref{app:lemma-restriction-strengthened} guarantees that a single restriction fixes each $y_i^{(l)}$.
\end{remark}

We now conclude the key result, Theorem~\ref{thm:uhat-cot-bound}:
\begin{theorem}[Restated from Theorem \ref{thm:uhat-cot-bound}]
Assume that $f$ has a UHAT-expressible CoT $g(x)$ of length $|g(x)| = o\left(|x|\right)$.
%
Choose any $C \in (0,1)$. Then there is a restriction $\rho$ such that
\begin{enumerate}
        \item $|\{i \leq N : \rho_n(i) = *\}| \geq Cn$ for  sufficiently large $N$
        \item For each $N \in \mathbb{N}$, $f$ is constant on $\Sigma^N \cap \rho\Sigma^*$
    \end{enumerate}


%
%
%
\end{theorem}
\begin{proof}
First, we obtain a restriction $\rho^{(0)}$, and integers $c$, $k$ from Lemma~\ref{lemma:restriction-strengthened}, applying $T$ only on the input $x \in \Sigma^*$ itself.
%

For any $n > k$, we now consider the following. Let $M := \max_{|x| = N} |g(x)|$. Without loss of generality, we can pad the CoTs for all $x$ with $|x|=N$ to have length $M$.

Let $g(x) = g_1 \dots g_M \in \Xi^M$.


We now construct a sequence $\rho^{(0)} \prec \rho^{(1)} \prec \rho^{(2)} \prec \rho^{(3)} \prec ... \rho^{(M)}$ while maintaining the following properties for $k = 1\dots M$ ($\dagger$):
\begin{enumerate}
    \item $\left|\{i : \rho^{(k)}_N(i) = * \}\right| \geq CN - k \cdot c\cdot H \cdot L$
    \item For each $l \in [0,L]$, $i \in [1,k-1]$, the activation $\vy^{(l)}_{N+i}$ is constant on all strings in $\Sigma^N \cap \rho^{(k)}_N\Sigma^*$.
    \item $g_1 \dots g_k \in \Xi^k$ is constant on all strings in $\Sigma^N \cap \rho^{(k)}_N\Sigma^*$.
\end{enumerate}
We prove this by induction.

\textbf{Inductive Base ($k=1$)} 
We can fix $\vy_{N}^{(1)}, \dots, \vy_{N}^{(L)}$ by fixing at most $\leq c\cdot L$ input positions, obtaining $\rho^{(1)} \succ \rho$, with
\begin{equation}
    \left|\{i : \rho^{(1)}(i) = * \}\right| \geq CN - c \cdot L \geq CN - 1 \cdot c \cdot H \cdot L
\end{equation}
Since $g_1$ is determined by $\vy_{N}^{(L)}$, this also fixes $g_1$.

\textbf{Inductive Step ($k>1$)}
Assume that the claim has been shown for all $k' < k$ (Inductive Hypothesis).
Note that $g_k$ is determined by $\vy_{n+k-1}^{(L)}$.
By the Inductive Hypothesis, $g_{k-1}$ is fixed by $\rho^{(k-1)}$; hence, $\vy_{N+k-1}^{(0)}$ also is.
We now perform the following construction for each layer $l=1, 2, \dots, L$, and -- within each layer -- for each head $h=1, \dots, H$.
We iteratively expand $\rho^{(k-1)}$ by restricting more input tokens.

%
At a given layer $l$ and head $h$, let $\tilde{\rho} \succ \rho^{(k-1)}$ be the restriction obtained after treating layers $1, \dots, l-1$ and, within layer $l$, heads $1, \dots, h-1$.
The activation $\vy_{N+k-1}^{(l-1)}$ is now already fixed by $\tilde{\rho}$.
Our goal is now to restrict a few more input positions to fix the attention of this head to a specific position, and thus fix its output.
For this, determine the maximal value of any attention score over all inputs satisfying the restriction, and fix input tokens so that this value is achieved, forcing the attention head to attend to a specific position.
Formally, for each $j \in [1,N+k-1]$, we consider
\begin{equation}
A_{j} := \max_{x \in \Sigma^N \cap \rho^{(k-1)}\Sigma^*} a_{n+1,j}^{(l,h)}
\end{equation}
Now let $\widehat{j}$ be the $j$ that maximizes $A_j$ (under the tie-breaking procedure, e.g. choosing the leftmost one, if more than one attain the same $A_j$).
One possibility is that $\widehat{j} \in [N+1, \dots, N+k-1]$; then attention is guaranteed to fall onto this position because each relevant activation is constant across $\rho^{(k-1)} \Sigma^*$ by Inductive Hypothesis.
The more interesting possibility is when $\widehat{j} \in [1,N]$.
Note that $\vy_{\widehat{j}}^{(l-1)}$ depends only on $\leq c$ input tokens in $\rho \Sigma^*$; hence, we can expand the restriction $\tilde{\rho}$ by fixing $\leq c$ additional input symbols to force $\vy_{\widehat{j}}^{(l-1)}$ to take on a value leading to that maximal attention score $a_{N+k-1,{\widehat{j}}}^{(l,h)}$.
Overall, performing this sequentially on each layer and head, we ultimately fix all activations $\vy_{N+k-1}^{(l)}$ and hence $g_k$, with a restriction $\rho^{(k)}$ that restricts at most $\leq c \cdot L \cdot H$ further input tokens beyond $\rho^{(k-1)}$.

This concludes the inductive proof of the claim ($\dagger$).
%
%
%
%
%
Overall, by taking $\rho := \rho^{(M)}$, we have fixed an additional $\leq c\cdot H\cdot L \cdot |g(x)| = o(N)$ input positions to fix all CoT tokens.
As the output $f(x)$ is part of the CoT, $f(x)$ must also be constant across all $x \in \Sigma^N \cap \rho_{M}\Sigma^*$.
\end{proof}


\begin{remark}\label{remark:causal}
We assume, by default, causally masked attention throughout the transformer (Section~\ref{sec:model-transformers}), for consistency with typical modern language models.
Our results are also compatible with setups where attention is bidirectional on the input and causal masking at most applies during CoT generation, which is the setup assumed by \citet{abbe2024how}.
As explained in the proof of the Depth Reduction Lemma, it holds independently of whether causal masking is applied to the input or not.
\end{remark}



\subsection{Proof of Theorem~\ref{theorem:dot-by-dot} (Barriers for Dot-by-Dot Scratchpads)}\label{app:theory-dot-by-dot}

\begin{theorem}[Repeated from Theorem~\ref{theorem:dot-by-dot}]
Consider a UHAT-expressible CoT for \textsc{Parity} where $g(x)$ has the form
\begin{equation}
.\dots . \# f(x) \ \ \ \ \ \ \ \ \ \ \ \ \text{(``dot-by-dot CoT'')}
\end{equation}
    This CoT has length $\omega(|x|^k)$ for all $k > 0$.
\end{theorem}


%



\begin{proof}
In such a CoT, we can consider the suffix $f(x)$ as the target of prediction given the input $x \dots \#$ of length $n := |x| + |g(x)| - 2$, with $N := |x|$.
The function predicting $f(x)$ based on a prefix $ x \dots \#$ acts on an input of length $n = N + |g(x)| -2$, where $N = |x|$, and has average sensitivity $N$. 
We note the following fact: Due to the inclusion of UHAT in $AC^0$ \citep{hao2022formal}, and the known bound on the average sensitivity of $AC^0$ circuits (Corollary 12.14 in \citet{jukna2012boolean}); originally due to \citet{linial1993constant,boppana1997average}),  we have $as_N(f) = \mathcal{O}(poly(\log(n)))$.
        Hence, $N = \mathcal{O}(poly(\log(N + |g(x)|)))$; hence $|g(x)|$ cannot be bounded by a polynomial of $N$.
\end{proof}

\begin{theorem}[Exponentially-Sized Dot-by-Dot Scratchpad for \textsc{Parity}]
    \textsc{Parity} has a UHAT dot-by-dot CoT of length $\mathcal{O}(\exp(N))$
\end{theorem}

\begin{proof}
    We lay out a construction for a dot-by-dot scratchpad of length $2^N + 2$, expressible in a UHAT Transformer with two layers, one attention head, and $d = 2N$ hidden dimensions. 
    %
    %
    Each of the first $2^N$ positions in a scratchpad is assigned a unique bitstring $\xi^{(i)} \in \{0,1\}^N$ ($i \in [1,2^N]$): that is, $i$-th token in a scratchpad (or $N+i$-th token overall) corresponds to a binary encoding of $i - 1$.

    The construction works as follows. In the first layer, each of the $2^N$ scratchpad tokens assigned with $\xi^{(i)}$ determines whether the input string is equal to $\xi^{(i)}$. In the second layer, the $(2N + 1)$-th scratchpad token gathers information from them and uses it to predict the hard-coded value of parity for the input string (which is the $(2N + 2)$-th scratchpad token).
    
    In the first layer, at the position corresponding to string $\xi^{(i)}$, an attention head sends out a query looking for positions $j$ where $x_j \neq \xi^{(i)}_j$.
For this, the query corresponding to bit string $\xi^{(i)}$, emanating from the $i$-th position in the CoT (overall, index $N+i$), has the form
\begin{equation}
    \left(\begin{matrix}
        \xi^{(i)}_1 &\dots & \xi^{(i)}_N & (1-\xi^{(i)}_1) & \dots & (1-\xi^{(i)}_N)
    \end{matrix}\right)
\end{equation}
At this position, the key and value vectors are zero.

The key and value corresponding to the $j$-th bit in the input has the form $K_j$:
\begin{equation}
    K_j = \begin{cases}
        e_j & x_j = 0 \\
        e_{N+j} & x_j = 1
    \end{cases}
\end{equation}
where $e_{\dots} \in \mathbb{R}^{2N}$ are one-hot vectors.

Keys of all tokens in the scratchpad are constant vectors of -1.

Then, the attention score 
\begin{equation}
    a_{N+i,j} = \begin{cases}
        1 & x_j \neq \xi^{(i)}_j \\
        0 & x_j = \xi^{(i)}_j \\
        < 0 & j > N
    \end{cases}
\end{equation}
If one position $j$ achieves a $1$, one of these will be selected by tie-breaking; if none does, one of the $N$ input bits will be selected. 
The MLP at the query position checks if the retrieved value vector indeed indicates a mismatch $x_j \neq \xi^{(i)}_j$, using knowledge about $\xi^{(i)}$ forwarded via the residual connection. If no mismatch is found, the transformer can conclude that $x = \xi^{(i)}$, and use that in the second layer.

%
In the second layer, we are interested in the $(2^N + 1)$-th scratchpad token, superceeding all positions corresponding to $\xi^{(i)}$.
At this position, the second-layer attention head attends to the unique position at which the first layer found no mismatch, and retrieves the hard-coded answer computed from the positional embedding of that position.
\end{proof}
%
    
%


%
%
    
%

\subsection{Proof of Fact~\ref{lemma:universality} (Universality of UHAT CoTs)}\label{app:universality}

A substantial number of constructions coding Turing machine computations into transformer CoTs have been presented in the literature \citep[e.g.][]{perez2019turing, wei2022statistically, bhattamishra2020computational, hou2024universal, qiu2024ask, malach2024autoregressive}, but they all use different assumptions about the formal model of transformers.
For self-containedness, we here provide a simple construction. 
Our construction is largely equivalent to that of \citet{wei2022statistically}, but we present it in a simplified and self-contained manner in the notation used in our paper.
We also note that constructions from \citet{bhattamishra2020computational,hou2024universal,qiu2024ask,malach2024autoregressive} can also be expressed straightforwardly in UHAT.
We discuss existing constructions from the literature at the end of this subsection.

\paragraph{Construction}

Consider a Turing machine defined by
\begin{enumerate}
    \item a finite tape alphabet $\Sigma$
    \item a finite state set $Q$
    \item the action set
    \begin{equation}
        \mathcal{A} := \{\text{LEFT}, \text{RIGHT}\} \cup \{\text{WRITE}(\sigma) : \sigma \in \Sigma\}
    \end{equation}
    \item a transition function $\delta$, mapping $\Sigma \times 
    \mathcal{Q}$ to $\mathcal{A} \times \mathcal{Q}$
    \item a start state $s_0 \in Q$
    \item a terminating set $T \subset Q$
\end{enumerate}
with the following computation:
\begin{enumerate}
\item We consider a tape with positions $0, 1, 2, \dots$.
    \item At the beginning, the machine starts at position 0 and in state $s_0$; the tape holds a finite input word starting at $0$, ending in a separator symbol. All remaining tape positions hold a blank symbol $\text{BLANK} \in \Sigma$.
    \item At each step, the next action and state are decided based on $\delta$ applied to the current state and the current tape symbol
    \item The machine stops when a state from $T$ is reached
    \item The final state indicates whether the input word was accepted or rejected
\end{enumerate}
We encode the computations as follows.
We first encode the input word as a string over the input alphabet $\Sigma$, followed by a separator symbol. 
We then construct a CoT over the infinite alphabet
\begin{equation}
    \Xi := \mathbb{N} \times  \mathcal{A} \times \mathcal{Q}
\end{equation}
For each state transition, we record (i) the  tape position after carrying out $\delta$ (an element of $\mathbb{N})$, (ii) the output of $\delta$ (an element of $\mathcal{A} \times \mathcal{Q}$).
%

We relate this construction to other constructions in Appendix~\ref{app:relation-literature}.

%
%

%
We now show that this is implemented by UHAT as defined in Section~\ref{sec:formalizing-cots}.
We need to define a set of UHATs $T_N$, operating on inputs with length $\leq N$, with uniformly bounded number of layers and heads.
We define
\begin{equation}
    \Xi_N := [0,N] \times \mathcal{A} \times \mathcal{Q}
\end{equation}

First, let $N$ be a bound on the input length.
%
At each position,
\begin{enumerate}
    \item The input token $\xi = (i,a,q) \in [0,N] \times \mathcal{A} \times \mathcal{Q}$ provides the  tape position $i$, the output $a$ of $\delta$, and the resulting state $q$.
    %
    %
    \item An attention head attends to the last step at which the Turing machine head had been at tape position $i$ while doing a WRITE operation.
    
    The key, given input token $\xi' = (i',a',q')$, is the one-hot vector indicating the tape position $i'$, an indicator for $a' \in \{\text{WRITE}(\sigma) : \sigma\}$, and a scalar indicating the position $i'$.
\begin{equation}
    \left(\begin{matrix}
        e_{i'} \in \mathbb{R}^N \\
        1_{\exists \sigma: \delta_i = \text{WRITE}(\sigma)} \in \mathbb{R} \\
        i \in \mathbb{R} 
    \end{matrix}\right)
\end{equation}
The query, given input token $\xi = (i,a,q)$, is 
\begin{equation}
    \left(\begin{matrix}
        3N e_{i} \in \mathbb{R}^N \\
        2N \in \mathbb{R} \\
        1 \in \mathbb{R} 
    \end{matrix}\right)
\end{equation}
%
If the tape position has previously appeared, the attention score will be maximized by the most recent position at which a write operation occurred; otherwise, it will fall somewhere else.
We define the value, given input token $\xi' = (i',a',q')$, as
\begin{equation}
    \left(\begin{matrix}
        e_{i'} \\
        e_{a'}
    \end{matrix}\right)
\end{equation}
The MLP then checks if the value is in 
\begin{equation}
    \left\{ \left(\begin{matrix}
        e_{i} \\
        e_{\text{WRITE}(\sigma)}
    \end{matrix}\right) : \sigma \in \Sigma \right\}
\end{equation}
If not, the symbol at  tape position $i$ must be what it was set to before the computation started.
Else, the symbol at the position must be as given by the action $a' \in \mathcal{A}$ retrieved.

Simultaneously, a second head attends to position $i$ and check whether it is part of the original input (i.e., precedes the separator); if it is, that position provides the symbol; if it is not, the tape position has not yet been written to.

Based on the information gathered by these two heads, the MLP then computes the next action, and outputs the new tape position, the action, and the new state.
\end{enumerate}


\paragraph{Comparison to other constructions}
Our construction is closest to that of \citet{wei2022statistically}.
It is also similar to that of \citet{qiu2024ask}.
The original Turing completeness proof for scratchpads \cite{perez2019turing} recomputed the current tape position in every step, an idea used by \citet{merrill2023expresssive}; our translation differs by keeping the tape position explicitly in the CoT, rather than recomputing it in every step; we believe that this reduces some technical challenges in the construction (at the price of making the transformer's width and parameters dependent on the input length).
\citet{bhattamishra2020computational} noted that the classical RNN construction of \citet{siegelman1991neural} can be straightforwardly replicated in a transformer using hard attention and unbounded-precision activations. This construction also is expressible in the UHAT model.
A related translation, albeit in the special case where the Turing machine produces no repeated strings, is developed in Theorem 4.1 of \citet{hou2024universal}; hence our lower bounds also hold for the ``Turing programs'' scratchpad technique proposed by \citet{hou2024universal}.
Another translation (applicable to, but not specific to transformers) is used by \citet{malach2024autoregressive}, it uses even simpler computations in each step, but may suffer a polynomial slowdown compared to Turing machines.

\section{Applications to Algorithmic Tasks}


\subsection{\textsc{Parity} and other Finite-State Languages}

\subsubsection{Proof of Corollary~\ref{thm:finite-state}:  Characterization for Finite-State Languages}\label{app:finite-state}


\begin{corollary}[Restated from Corollary~\ref{thm:finite-state}]
    Let $f$ be the membership problem of a finite-state language $L$. Then $f$ exactly one of the following holds:
    \begin{enumerate}
        \item $L \in AC^0$ and $f$ is expressible in UHAT without CoT
        \item $L$ is not decidable in $AC^0$, and any UHAT CoT for $f$ has length $\Omega(N)$
    \end{enumerate}
%
\end{corollary}
\begin{proof}
%
We show this using the characterization of regular languages in $AC^0$ by \citet{barrington1992regular} and the follow-up result on hard-attention transformers by \citet{yang2024masked}.

First, if $L \in AC^0$, then it is definable in UHAT (without CoT) by Corollary~8 of \citet{yang2024masked}.

Conversely, if $L \not\in AC^0$, then by Theorem 3 of \citet{barrington1992regular}, the syntactic morphism of $L$ is not quasi-aperiodic.
That is, if $\eta_L$ is the syntactic morphism of $L$\footnote{We refer to \citet{barrington1992regular} for the relevant definition.}, then there is $t \in \mathbb{N}$ such that $\eta_L(\Sigma^t)$ contains a nontrivial group $G$.
As in the proof of Theorem 3 of \citet{barrington1992regular}, we now consider an element $m \in G$ of order $k>0$, and define $M' := \{m, m^2, \dots, m^k\}$ to be the subgroup defined by $m$; its identity element is $m^k$.
We can then find strings $u, v \in \Sigma^t$ such that $\eta_L(u) = m$, $\eta_L(v) = m^k$.

Namely, there are strings $\alpha, \beta \in \Sigma^*$ such that either $\alpha u \beta \in L$, $\alpha v \beta \not\in L$, or the other opposite holds. By passing from $L$ to its complement if needed, we may assume without loss of generality that $\alpha u \beta \in L$, $\alpha v \beta \not\in L$.
We now consider inputs of the form
\begin{equation}
    \alpha (u|v)^* \beta
\end{equation}
Now assume that a UHAT CoT with length $|g(x)| = o(|x|)$ exists for deciding, on such inputs, membership in $L$. 
By hard-coding the (fixed) strings $\alpha, \beta$, we can convert this into a UHAT CoT performing the same problem with only the inner part, a word in $(u|v)^*$, given as input. As $|u|=|v|$, we can further convert this into a UHAT CoT over a new alphabet $\Sigma' = \{a, b\}$, translating $u \in \Sigma^t$ to $a \in \Sigma'$ and $v \in \Sigma^t$ to $b \in \Sigma'$, cutting the string length by a constant factor of $t = |u| = |v|$.
We now reach a contradiction: On inputs of the chosen form, checking membership in $L$ amounts to counting how often $u$ (or, after changing the alphabet, $a$) appears modulo $k$. Fixing any constant fraction of symbols cannot fix the result. By Theorem~\ref{thm:uhat-cot-bound}, the UHAT CoT cannot have had length $|g(x)| = o(|x|)$ in the first place.
Hence, any UHAT CoT for deciding membership in $L$ has length $\Omega(N)$.
\end{proof}

\begin{remark}\label{remark:finite-state-sensitivity}
We also mentioned, below Theorem~\ref{thm:finite-state}, that  $f$ is affected by the sensitivity-based barrier discussed in Section~\ref{sec:sensitivity} if and only $L \not\in AC^0$.
This is seen as follows.
First, if $L \in AC^0$, then by the result of \citet{boppana1997average}, $as_N(f) = \mathcal{O}(poly(\log(N))) = o(N)$.
Second, if $L \not\in AC^0$, then the deciding membership of strings from $\alpha(u|v)^* \beta$ as defined in the proof of Theorem~\ref{thm:finite-state} amounts to counting how often $u$ appears modulo $k$.
Any function $\Sigma^* \rightarrow \{0,1\}$ that exhibits this behavior on strings from $\alpha(u|v)^* \beta$ must have a high sensitivity.
Overall, this means that shortcuts to finite-state automata without linear-length CoT \citep{liu2022transformers} are likely to be learned well on long inputs if and only the automaton can be simulated in $AC^0$.
\end{remark}

\subsubsection{Experiment}\label{app:parity-experiment}
We trained a 2-layer 2-head transformer using the GPT-2 architecture \citep{radford2019language}, with $d=256$.
%
%
%
%
Each model was trained at a fixed input and CoT length.
Each training and testing input was generated randomly on the fly.
Training was for 30K steps, at a batch size of 64, with AdamW (learning rate 1e-4).

%







\subsection{Arithmetic}
\subsubsection{Proof of Theorem~\ref{thm:multiplication} (Lower Bound for \textsc{Multiplication)})}\label{sec:multiplication-lower-bound-proof}




\begin{figure}
    \centering
    \textsc{Addition} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \textsc{Multiplication}
    
    \includegraphics[width=0.45\linewidth]{plots/length-sensitivity-by-position-addition.pdf}
    \includegraphics[width=0.45\linewidth]{plots/length-sensitivity-by-position.pdf}
    \caption{Arithmetic.
    Average sensitivity (Section~\ref{sec:sensitivity}) of the results digits as a function of the length $N$ of the operands (colored lines) and the position $k$ (x-axis).  The most significant digit is at position $k=0$.
    \newline
    Left: \textsc{Addition}.     Sensitivity grows sublinearly with $n$.
    \newline
    Right: \textsc{Multiplication}. Sensitivity grows rapidly with the length of the multiplicands, and is highest in the middle, where sensitivity shows linear growth with $n$.
    }
    \label{fig:sensitivity-multiplication}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{plots/Digit_sensitivity_autoregressive.png}
        \includegraphics[width=0.45\linewidth]{plots/Digit_sensitivity_encoder.png}
    \caption{Accuracy of transformers trained on 12-digit multiplication (most significant digit at the left, zero-padded on the left), either autoregressively (left) or in parallel (right), on a test set with 10K number pairs. In both setups, the high-sensitivity digits in the middle (compare Figure~\ref{fig:sensitivity-multiplication} Right) show substantially decreased accuracy even if digits at beginning and end are predicted exactly.}
    \label{fig:multiplication-accuracy-by-digit}
\end{figure}








\begin{theorem}
%
%
A UHAT CoT for $M_N$ requires length $\Omega(N)$.
%
\end{theorem}

\begin{proof}
In order to understand the difficulty of different digits in multiplication, we first review the connection between multiplication and bit string convolution.
%
The integers $X$, $Y$, $XY$ can be written in terms of binary coefficients $\xi_i, \eta_i \in \{0,1\}$ as:
\begin{align*}
    X =& \sum_{i=0}^N 2^i \xi_i \\
    Y =& \sum_{i=0}^N 2^i \eta_i \\
    XY =& \left(\sum_{i=0}^N 2^i \xi_i\right) \cdot \left(\sum_{i=0}^N 2^i \eta_i\right) \\
    = & \sum_{i,j} 2^{i+j} \xi_i \eta_j \\
    = & \sum_{k} 2^{k} \sum_{i+j=k} \xi_i \eta_j \\
\end{align*}
Hence, the $N$-th digit of $XY$, $M_N$, is determined by, on the one hand, the high-order parity $\bigoplus_{i+j=k} \xi_i \eta_j$, XOR-red with carries obtained from terms at lower $k$'s.
Now if one fixes $CN$ input bits, for $C$ small, the result cannot be fixed.
%
\end{proof}

\begin{remark}
Regarding $as(M_N)$, we note that the innermost digits $M_N$ are, up the impact of carries, derived from high-degree parities as discussed in the proof of Theorem~\ref{thm:multiplication}.
In accordance with this, we find that $as(M_k)$ is approximately $\min(k, 2N-k)$ and in particular peaks at $k\approx N$ (Figure~\ref{fig:sensitivity-multiplication}).
Empirically, the middle digits are the hardest for transformers (Figure~\ref{fig:multiplication-accuracy-by-digit}).

\end{remark}

\subsubsection{Remark: Autoregressive Multiplication}\label{app:autoregressive-mult}


Theorem~\ref{thm:multiplication} shows that querying individual digits of a product of $N$-digit numbers requires at least a linear-length CoT.
An interesting question is whether autoregressive decoding can help, i.e., the earlier digits in the result can serve as an effective chain-of-thought for later digits.
Empirical results cast doubt on such a possibility, because transformers struggle substantially more with multiplication than addition even in autoregressive generation (Figure~\ref{fig:arithmetic-comparison}).






\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/100sample_full/100sample-25-include-t.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/100sample_full/100sample-25-include-t-zoomed.png}
    \end{minipage}

    \caption{Numerical Experiments for Conjecture~\ref{conj:product-xor}:  100 different combinations samples for each T. %
    Blue dots indicate values for (\ref{eq:xor-conjecture}) for 100 different sampled $(A, B, C)$.
    Right: Zoomed in on T = 7,8,9.
    }\label{fig:conj-test}
\end{figure}


%
 %
  %
   %
    %
    %
    %
     %
      %
%

 %
%



Here, we outline a path to rigorously showing this.
The key idea is the following conjecture, which is of purely combinatorial nature:
\begin{conjecture}\label{conj:product-xor}
Given two binary encodings $X, Y \in \{0,1\}^N$ of $N$-bit integers, let $M_i(X,Y)$ be the $i$-th bit of the product $XY$.
We assume standard binary encoding with the most significant bit on the left.

Then, there is a constant $D>0$ such that for any $T \in [1,N]$, sets $A, B \subseteq [1,N]$, $C \subseteq [1,T-1]$, we have
    \begin{equation}\label{eq:xor-conjecture}
        \left|\mathbb{E}\left[\hat{M}_T(X,Y)   \prod_{i\in A} \hat{X}_i \prod_{j\in B} \hat{Y}_j  \prod_{k \in C} \hat{M}_k(X,Y)\right] \right| \leq 2^{-D\cdot T}
    \end{equation}
    where $\hat{X}_i = 2X_i-1 \in \{-1,1\}$, and similar for $\hat{Y}$, $\hat{M}$.
\end{conjecture}
This statement intuitively states that digits of the product are not substantively correlated with combinations of individual digits in the inputs or preceding digits in the result.
%
We provide numerical evidence for this conjecture in Figure~\ref{fig:conj-test}.
Any proof of this conjecture would imply lower bounds on autoregregressive multiplication. Formally, we are interested in autoregressive predictors
\begin{equation}
    F_T : \{0,1\}^{2N+T-1} \rightarrow \{0,1\}
\end{equation}
with the property that
\begin{equation}
    M_T = F_T(X_1, X_2, \dots, X_N, Y_1, Y_2, \dots, Y_N, M_1, \dots, M_{T-1})
\end{equation}
We then have bounds based on sensitivity (hence learnability by Section~\ref{sec:sensitivity}) and UHAT expressiveness:
\begin{lemma}
    Assume Conjecture~\ref{conj:product-xor}.
    Then any Boolean function implementing $F_N$ needs to satisfy\footnote{In fact, the stronger bound \begin{equation}
    as(F_T) = \Omega(T)
    \end{equation} can be concluded uniformly for $T$, independently of $N$. This is because $F_T$ is only relatively weakly dependent on digits at positions much larger than $T$, making products trivially close to zero in Conjecture~\ref{conj:product-xor} when $A$ or $B$ include indices substantially greater than $T$.} 
    \begin{equation}
    as(F_N) = \Omega(N)
    \end{equation}
        A CoT directly outputting $M_1 \dots M_{2N}$, without $\omega(1)$ further intermediate steps, cannot be implemented in UHAT.
\end{lemma}
\begin{proof}
    It is sufficient to show the lower bound on average sensitivity; the claim about UHAT follows from the inclusion of UHAT in $AC^0$ and Boppana's upper bound on the average sensitivity of $AC^0$ functions \citep{boppana1997average}.
We refer to \citet{odonnell2014analysis} to background on Fourier analysis of Boolean functions.
We consider the Fourier transform of $F_N$ as a Boolean function on $\{-1,1\}^{3N-1}$:
    \begin{equation}
        \hat{F}_N(X,Y,M_{1\dots N-1}) = \sum_{A, B \subseteq [1,N]; C \subseteq [1,N-1]} \lambda_{A,B,C} \cdot \chi_{A,B,C}(X,Y,M)
    \end{equation}
for unique coefficients $\lambda_{A,B,C} \in [-1,1]$ and
\begin{equation}
\chi_{A,B,C}(X,Y,M) := \prod_{i\in A} \hat{X}_i \prod_{j\in B} \hat{Y}_j  \prod_{k \in C} \hat{M}_k
\end{equation}
%
%
 %
%

We now investigate, for $d \in (0,1)$, the space $\mathfrak{F}_d$ of polynomials $\{-1,1\}^{3N-1} \rightarrow \{-1,1\}$ of degree at most $\leq 3dN$. Then
\begin{align*}
& \sup_{f \in \mathfrak{F}_d} \mathbb{E}_{X,Y}[f(X,Y,M(X,Y)) \hat{F}_N(X,Y,M(X,Y))] \\
    \leq & \sum_{A,B,C : |A|+|B|+|C| \leq 3dN} \left|\mathbb{E}_{X,Y}[\hat{F}_N(X,Y,M(X,Y)) \chi_{A,B,C}(X,Y,M(X,Y))]\right| \\
    \leq &  2^{-DN} \sum_{l=1}^{3dN} {3N-1 \choose 3dN}
\end{align*}
Choosing $d>0$ sufficiently small (in relation to $D$) makes this bound exponentially small in $N$.
This entails that at least a constant fraction of the Fourier mass of $\hat{F}_N$ is on degrees of order $\sim N$. A linear average sensitivity follows.
\end{proof}








\subsubsection{Construction of $\mathcal{O}(N \log N)$-length CoT for Multiplication}\label{app:mult-scratchpad}

%

Whereas the traditional approach for multiplying two $N$-digit numbers requires $\Theta(N^2)$ steps, modern algorithms are more efficient for large inputs, and achieve an asymptotic runtime up to $\Theta(N \log N)$ \citep[e.g.][]{schonhage1982asymptotically,harvey2021integer}.
%
We illustrate this by demonstrating a scratchpad whose length, $\mathcal{O}(N \log N)$, matches a widely conjectured lower bound on the asymptotic time complexity of multiplication in the Turing machine model.
It is based on the Sch{\"o}nhage-Strassen algorithm \cite{schonhage1982asymptotically}.\footnote{The Sch{\"o}nhage-Strassen algorithm \cite{schonhage1982asymptotically} requires $\mathcal{O}(N \log N \log\log N)$ steps, slightly different from the $\mathcal{O}(N \log N)$ CoT length. This is because we use an unbounded CoT alphabet to encode and perform arithmetic on the entries of the NTT. Moving to a constant-size CoT alphabet would incur a $\log\log N$ factor as in the standard Sch{\"o}nhage-Strassen algorithm. The algorithm of \citet{harvey2021integer} attains $\mathcal{O}(N \log N)$ complexity, but is substantially more complex.}

\paragraph{Background: Number Theoretic Transform (NTT)}
The Number Theoretic Transform is an analogue of the Discrete Fourier Transform (DFT) that operates over finite fields. While the DFT leverages the complex roots of unity, the NTT employs roots of unity in a modular arithmetic setting. Formally, let \( F_q \) denote a finite field of size \( q \), where \( q = p^k \) for some prime \( p \) and integer \( k \). Consider a prime \( p \) such that there exists a primitive \( n \)-th root of unity \( \omega \in F_q \), \( \omega^n \equiv 1 \pmod{p} \) and for all \( 1 \leq k < n \), \( \omega^k \neq 1 \). The NTT is then defined as a mapping of an input vector of length \( n \), say \( a = (a_0, a_1, \dots, a_{n-1}) \), to an output vector \( A = (A_0, A_1, \dots, A_{n-1}) \), where:
\[
A_k = \sum_{j=0}^{n-1} a_j \omega^{jk} \pmod{p}, \quad \text{for} \, k = 0, 1, \dots, n-1.
\]
This transformation can be seen as a modular version of the classical DFT. The inverse of the NTT (INTT) is similarly defined, allowing for the reconstruction of the original vector from its transformed version. The INTT is expressed as:
\[
a_j = \frac{1}{n} \sum_{k=0}^{n-1} A_k \omega^{-jk} \pmod{p}, \quad \text{for} \, j = 0, 1, \dots, n-1,
\]
where \( \frac{1}{n} \) refers to the modular multiplicative inverse of \( n \) modulo \( p \), which must exist for the INTT to be valid.  In the context of multiplication, the use of NTT allows for a scratchpad with \( O(n \log n) \) length, compared to the \( O(n^2) \) length of the naive scratchpad.

%

\paragraph{CoT Construction and Analysis}
%
%
%
The Schönhage-Strassen algorithm provides a method to perform integer multiplication efficiently \citep{schonhage1982asymptotically}. To translate this into a scratchpad for use in a UHAT framework, the steps of the algorithm are defined here. We note that the required attention pattern is, for any input length $N$, input-independent, which ensures that the CoT is expressible in UHAT.
The number of CoT steps scales as $\mathcal{O}(N \log N)$ because the NTT is computed using the divide-and-conquer Fast Fourier Transform strategy \citep[Cooley-Tukey Algorithm, ][]{cooley1965algorithm}.

We use ``-1'' as separator token.
\paragraph{Input encoding} We encode the input:
%
 %
    \[
    [-1] \ \ [\text{ First Number }] \ \ [-1] \ \ [\text{ Second Number }] \ \ [-1]
    \]

  %

    
\paragraph{Input Reversal and Padding} Given two binary numbers, we first reverse their bit sequences and pad them to the nearest power of two, $2^n$, where $n$ is chosen such that $2^n$ is greater than or equal to the length of the input sequences. This ensures that the sequences are of a length suitable for efficient NTT computation.

  \[
    \begin{aligned}
    & [-1]\ \ [\text{ First Number Reversed and Padded }] \\
    & [-1]\ \ [\text{ Second Number Reversed and Padded }] \ \\\
    \end{aligned}
    \]

\paragraph{NTT Transformation} The padded sequences are then transformed using the NTT, which operates in a finite field. The output is a sequence $A_0, \dots, A_{n-1} \in \{0, \dots, p-1\}$.

%

  \[
    \begin{aligned}
    & [-1]\ \ [\text{ NTT of First Number }] \ \ [-1] \ \ [\text{ NTT of Second Number }]\\
%
 %
    \end{aligned}
    \]

    This part takes $\mathcal{O}(N \log N)$ steps because the NTT is computed using the divide-and-conquer Fast Fourier Transform strategy (Cooley-Tukey Algorithm). Every step combines two fixed previously computed results; the attention pattern can be hard-coded into positional encodings and the modular arithmetic required can be hard-coded into the MLP map $f^{MLP}$ (Section~\ref{sec:model-transformers}).

    

\paragraph{Convolution} In the NTT domain, the two transformed sequences are multiplied pointwise, modulo our prime number. This convolution corresponds to a multiplication in the original domain.
We found that training was improved by first copying the NTTs with each digit marked with an index hint \citep{zhou2023algorithms} indicating its position, as a way of indicating which pairs of numbers to match in pointwise multiplication.

%

  \[
    \begin{aligned}
    & [-1]\ \ %
    [\text{ Convolution }] \ \ [-1] \\
    \end{aligned}
    \]

        This part takes just $\mathcal{O}(N)$ steps.
        
\paragraph{Inverse NTT} After the convolution, we apply the inverse NTT to convert the sequence back to the domain.

  \[
    \begin{aligned}
    &[-1] \ \ [\text{ Result of Inverse NTT} ] \ \ [-1] \ \ 
    \end{aligned}
    \]

    Again involving a divide-and-conquer FFT, this takes $\mathcal{O}(N \log N)$ steps.
    
\paragraph{Recombination} Finally, we recombine the elements of the transformed sequence to produce the final multiplication result. Starting from the leftmost element, we multiply by $2^0$, the next by $2^1$, and so on, summing these weighted values to get the desired output.

  \[
    \begin{aligned}
    &[\text{ Recombined Final Result }] \ \ [-1]
    \end{aligned}
    \]

%

 %
  %
   %
   %
   %
   %
   %
   %
   %
   %
   %
   %
   %
%

%

\paragraph{Example} Below is an example of the full CoT for 2 digit binary multiplication (10 * 11) using prime $p=5$.

\begin{itemize}
    \item \textbf{Input}: 
        \[ {-1} \ \ 1 \ \ 0 \ \ {-1} \ \ 1 \ \ 1 \ \ {-1} \]

    \item \textbf{Target}:
    \[
        \begin{aligned}
         & {-1} \  \underbrace{0 \ \ 1\ \ 0\ \ 0}_{\text{First Number Reverse and Pad}} \ \ {-1}\ \underbrace{1 \ \ 1\ \ 0\ \ 0}_{\text{Second Number Reverse and Pad}}\ {-1} \underbrace{1 \ \ 2\ \ 4\ \ 3}_{\text{NTT of First Number}}  \\
        &  {-1}  \underbrace{2 \ \ 3\ \ 0\ \ 4}_{\text{NTT of Second Number}} {-1}  \ \ \underbrace{a \ \ 1\ \ b \ \ 2\ \ c \ \ 4\ \ d \ \ 3}_{\text{NTT of First with Index Hints}} \ \ {-1} \underbrace{a \ \ 2\ \ b \ \ 3\ \ c \ \ 0\ \ d \ \ 4}_{\text{NTT of Second with Index Hints}}\\ 
        & {-1}  \ \ \underbrace{a \ \ 2\ \ b \ \ 1\ \ c \ \ 0\ \ d \ \ 2}_{\text{Convolution with Index Hints}} \ \ {-1} \ \ \underbrace{2\ \ 1\ \ 0\ \ 2}_{\text{Convolution}} \ \ {-1} \underbrace{0\ \ 1\ \ 1 \ \ 0 }_{\text{INTT of Convolution}} {-1}  \underbrace{0 \ \ 1 \ \ 1 \ \ 0 }_{\text{Result (Recombined)}}{-1}
        \end{aligned}
     \]   
\end{itemize}

%

%

%


%

%



%

%

%

%

%
%
%

%

%



\subsubsection{Experiments}\label{app:experiments-multiplication}



\paragraph{Setup}
All experiments were conducted on NVIDIA A100 GPUs (40GB memory each).
We experimented with two different transformer architectures: 
a BART-based encoder-decoder model (for autoregressive decoding of multiplication) and an encoder-only model (for parallel decoding).
%

\paragraph{Model Configurations}
We experiment with three setups:
\begin{enumerate}
    \item Direct parallel decoding of each digit $M_1, \dots, M_{2N}$ (and analogously for addition) in parallel. To avoid training separate transformers for each digit, we use a transformer encoder (i.e., transformer with bidirectional attention) reading in the two $N$-digt operands, and providing predictions for each of the $2N$ results digit at the top layer.
Due to high sensitivity of the middle digits (Figure~\ref{fig:sensitivity-multiplication}) and Theorem~\ref{thm:multiplication}, we expect that this setup will be difficult for \textsc{Multiplication}, though not for \textsc{Addition}.
    
    \item Autoregressive decoding of the result $M_1 \dots M_{2N}$ (and analogously for addition). We expect that this setup is still difficult for \textsc{Multiplication} as discussed in Appendix~\ref{app:autoregressive-mult}.

    \item Autoregressive decoding of the $\mathcal{O}(N\log N)$-length CoT. We expect that this setup will make the task feasible for transformers.
\end{enumerate}




\textit{Encoder-only Model for Direct Parallel Decoding}: We employed a transformer encoder with 2 layers, 2 attention heads, a feed-forward network dimension of 512, and a model dimension of 128. The vocabulary size was determined by the tokenizer used for encoding the digits and arithmetic symbols.

\textit{Encoder-Decoder Model for Autoregressive Decoding}:
In autoregressive decoding, we slightly deviated from the theoretical setup in Section~\ref{sec:model-transformers} in allowing bidirectional attention on the input (though not on the CoT) as in \citet{abbe2024how}, for consistency with the Direct Parallel Decoding setup, where we use bidirectional attention for efficiency, as described above. 
We built on the BART architecture, with a 1-layer encoder and a 2-layer decoder. Both the encoder and decoder had 2 attention heads, and the decoder's feed-forward network dimension was  512. The model dimension for both encoder and decoder was 128.

\textit{Note on Bidirectional Attention}:
As the results in \citet{hahn2020theoretical} do not presuppose causal masking, our lower bounds are also applicable in the setup with bidirectiobnal attention (Remark~\ref{remark:causal}).

%
%
%
%
%
%
%


%
%


\paragraph{Datasets}
All experiments were conducted on binary numbers.
We trained and evaluated both models without CoT on datasets consisting of 2-digit, 4-digit, and 6-digit arithmetic problems for both addition and multiplication. 
%
%
For the CoT, we used a training dataset of 5 million samples and 10,000 test samples. The dataset sizes for 8-digit to 12-digit multiplication were smaller due to the limited number of possible combinations, with 50,000 samples for training and 10,000 for testing.
In both cases, the test data is held-out, without overlap with the training data.

\paragraph{Training Configurations}
For the CoT multiplication experiments, we used the following training configuration:
- Number of training epochs: 20
- Batch size (per device): 8 for both training and evaluation
- Gradient accumulation steps: 4
- Learning rate: 0.0001

%





\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Multiplication Task} & \textbf{Training Set Size} & \textbf{Accuracy (\%)} \\ \hline
8-digit       & 50,000                    & 99.83                   \\ \hline
9-digit      & 50,000                    & 99.87                   \\ \hline
10-digit       & 50,000                    & 99.82                   \\ \hline
11-digit      & 50,000                    & 99.85                   \\ \hline
12-digit     & 5,000,000                    & 99.84                   \\ \hline
13-digit     & 5,000,000                    & 99.82                   \\ \hline
14-digit      & 5,000,000                    & 99.96                   \\ \hline
15-digit      & 5,000,000                    & 99.91                   \\ \hline
16-digit     & 5,000,000                 & 99.83                   \\ \hline
\end{tabular}
\caption{Accuracy results of the $\mathcal{O}(N \log N)$-length CoT for \textsc{Multiplication} for different digit sizes. All accuracies are computed on tests sets disjoint from the training set.}
\label{tab:scratchpad_accuracy}
\end{table}










%

%


\subsection{\textsc{Median} Task}\label{app:theory-order-statistics}

\subsubsection{Theory}


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{plots/median-sensitivity.pdf}
    \caption{Average sensitivity of the last digit of \textsc{Median} as a function of $N$ when $B \sim 1+\lceil\log N\rceil$. We estimate average sensitivity by sampling 200 input strings at each $N$, and 200 bit flips for each input. As predicted theoretically (Appendix~\ref{app:theory-order-statistics}), average sensitivity grows linearly in this regime where $\exp(B) \gtrapprox N$.}
    \label{fig:median-sensitivity}
\end{figure}

We first establish the claim that the last digit of \textsc{Median} is sensitive to changes of the integers, in the regime where $N << \exp(B)$.\footnote{If $N$ is on the order of or even larger than $\exp(B)$, then uniform sampling of numbers (even with replacement) will have a high chance of producing the same number and low sensitivity. We hence focus on the regime where $N << \exp(B)$.}
    Changing the first digit in any of the numbers with order rank $< \lfloor\frac{N}{2}\rfloor$ can get it to have rank $> \lfloor\frac{N}{2}\rfloor$. 
    Under uniform sampling of the numbers, %
    the chance that this will change the last digit is $\approx \frac{1}{2}$.
    Changing other digits will additionally have some nonzero (though smaller) chance of flipping the last output digit.
    Hence, the average sensitivity of the last digit is $\gtrapprox \frac{N}{2}$.
    This is illustrated in Figure~\ref{fig:median-sensitivity}.
    Hence, the task is challenging for transformers by the results reviewed in Section~\ref{sec:sensitivity}.

    

Now we proceed to showing the CoT bounds.
\begin{theorem}[Restated from Theorem~\ref{thm:median}]
    For the \textsc{Median} task, a UHAT scratchpad requires length $\Omega(N)$. This bound is attained.
\end{theorem}
    \begin{proof}
    We note that the input length is $BN$, where $B$ is the number of bits in each integer.
    To show the lower bound by applying Theorem~\ref{thm:uhat-cot-bound}, we note that fixing, say, $\frac{1}{10} N = \frac{BN}{10B}$ digits cannot fix the median.
    
    
%
    
    
 %


    

Now we show that the bound is attained.
Recall that the input consists of $N$ unique numbers in binary encoding, each with $B$ bits. %
Let $a_1, \dots, a_N$ be the integers in the input, and let $a_{i_1}, \dots, a_{i_N}$ be the same numbers ordered by magnitude.
Now the CoT simply consists of 
\begin{equation}
    a_{i_1} \dots a_{i_{\lfloor N/2 \rfloor}} %
\end{equation}
To implement this in UHAT, we first use $B$ attention heads in order to aggregate each $a_i$ on the basis of the $B$ bits entering its binary representations:
\begin{equation}
    a_i = \sum_{j=0}^{B-1} 2^j \lambda_{B(i-1)+j+1}
\end{equation}
where $\lambda \in \{0,1\}^{BN}$ is the input string consisting of $N$ integers with $B$ digits each.
This number $a_i$ is stored, e.g. as a one-hot encoding, in the activation $\vy^{(1)}$ at the last digit of its binary representation in the input.
The CoT then orders the numbers by magnitude, stopping at the $k$-th index.
Importantly, this is expressible with a single attention head as follows: If the integer $i$ is represented as the one-hot vector $e_i$, then $e_i^T K^T Q e_j$ produces the desired behavior when\footnote{We note that definitions of autoregressive sorting by transformers are also given in RASP-L \cite{zhou2023algorithms} and C-RASP \cite{huang2024formal}.}
\begin{equation}
    (K^TQ)_{ij} = \begin{cases}
        -1 & \text{if } i \leq j \\ 
        N + j - i & \text{else}
    \end{cases}
\end{equation}
Stopping at $\lfloor N/2 \rfloor$ can be hard-coded via positional encodings.
%
\end{proof}
    

%
%
%

%
%

%

%

%

 %

%
%


%
%
%



\subsubsection{Experiment}
\label{app:median-exp-detail}

To simplify the setup, we deviated from the theoretical construction by using decimal numbers instead of binary numbers, and also using decimal numbers instead of atomic encodings in the CoT. %
We considered three-digit numbers ($B=3$, but with decimal instead of binary encoding).\footnote{In preliminary experiments, we found that it is important to have diverse inputs (i.e., $B$ not too small in relation to $N$) to avoid learning a ``increment-by-1" algorithm (imagine when $N=100$, we sample 100 number from [0, 99] without replacement, which means the sorted list and the median is always the same).}

%
%
%
%
%
%
%
%
%
%
%

We experiment with $N = \{1, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99\}$. The resulting input length is between 6 and 398 (including BOS and SEP token). The $N$ numbers are randomly sampled from $\{0, 1, \cdots, 999\}$ with replacement. We train separate models for each $N$, and test on the same length. Note that the size of the test set varies with $N$, concretely it is $(N+10)\times20$. Because when $N=1$, there are only 1000 possible inputs. The test examples are excluded from training set. The input format is as follows (for the case $N=3$), including full scratchpad:
\begin{center}
\texttt{
\setlength{\tabcolsep}{3pt}
\small
\begin{NiceTabular}{*{23}{c}}[hvlines]
BOS & 3 & 4 & 3 & ; & 0 & 1 &9 & ; & 8 & 5 & 2 & ; & SEP & 0 & 1 & 9 & ; & 3 & 4 &3 & ; & EOS
\end{NiceTabular}
}.
\end{center}
We use ``;" to separate each number. As mentioned, we experiment with the full scratchpad and without any scratchpad, as well as every $k$th element in the full scratchpad, where $k=\{2,3,4,5,6,9,12\}$. The models are trained with cross entropy loss on tokens after SEP (excluding itself). The input length in figure~\ref{fig:median} refers to number of tokens before and including SEP, and CoT length refers to number of token after SEP and before the final answer (last 3 digits). While models are trained to predict both the scratchpad and the answer, we do not consider scratchpad during testing. In other words, to evaluate the models, we give them tokens up to SEP, and let them generate with greedy search, and take the last generated number before EOS as the prediction. The prediction is considered correct only when it matches all digits in the answer. We run each experiment (each $N$ and each kind of scratchpad) with 3 random seeds, and report the average accuracy.

Regarding hyperparameters, we use the same model architecture for all experiments in this section, which has 3 layers, 4 attention heads in each layer, and model dimensionality of 256. We use batch size of 64, train models for 50k steps, use learning rate starting from $3\times10^{-4}$ and decreasing linearly. We use AdamW optimizer, with $\beta_1=0.9, \beta_2=0.999$ and weight decay of 0.01. All dropout rates are set to zero. Note that we observe that low accuracy is always accompanied with big training loss.



\subsection{Graph Reachability}\label{app:theory-dag-reachability}

\subsubsection{Proof of Theorem~\ref{thm:reachability}}

\begin{theorem}[Restated from Theorem~\ref{thm:reachability}]
\label{thm:app-reachability}
There is a family $\mathfrak{G}$ of DAGs inside which reachability is solvable in $TC^0$, but cannot be represented by a transformer at sublinear average sensitivity. A UHAT CoT needs length $\Omega\left(|E| \log |V| \right)$. 
This bound is attained.
\end{theorem}


\begin{proof}
For the lower bound, the proof proceeds by coding $\textsc{Parity}_n$ into DAGs with $2n$ vertices.
For $i=1,\dots, N+1$, we introduce two vertices $v_{i,odd}$ and $v_{i,even}$.
Whenever $x_i=1$, we add edges $v_{i,odd}\rightarrow v_{i+1,even}$ and $v_{i,odd}\rightarrow v_{i+1,even}$.
When $x_i=0$, the edges instead connect $v_{i,odd}\rightarrow v_{i+1,odd}$ and $v_{i,even}\rightarrow v_{i+1,even}$.
Then, $x$ has even parity if and only if there is a path from $v_{1,even}$ to $v_{N+1, even}$.
We define $\mathfrak{G}$ as the set of these graphs.
If $G$ is guaranteed to be in $\mathfrak{G}$, then a $TC^0$ circuit is sufficient for deciding membership.
As the graphs in $\mathfrak{G}$ code \textsc{Parity}, reachability cannot be represented by transformers at low sensitivity.
Also, fixing a small constant fraction of edges, while staying within $\mathfrak{G}$, cannot fix the reachability. %
%
%


To show that the $\Omega\left(|E| \log |V|\right)$ bound is attainable, we note that a CoT of length $\mathcal{O}(|E| \log |V|)$ can encode breadth-first search, a generalization of the DFS/BFS CoT for the cycle task in \citet{abbe2024how}.
%
%
The input consists of the edges and the query, such as:

\begin{center}
\texttt{
\setlength{\tabcolsep}{3pt}
\small
\begin{NiceTabular}{*{25}{c}}[hvlines]
BOS & 06 & 08 & ; & 04 & 01 & ; & 04 & 05 & ; & 01 & 03 & ; & 08 & 00 & ; & 01 & 05 & ; & ... & QUERY1 & 03 & QUERY2 & 04 & SEP
\end{NiceTabular}
}
\end{center}

Whereas the input codes each vertex as a $\lceil \log |V| \rceil$-length number (keeping the input alphabet finite), each vertex is coded as an atomic token inside the CoT.
At the beginning of the CoT, each edge is translated from binary representations to this atomic representation. This takes $\mathcal{O}(|E| \log |V|)$ steps.\footnote{Formally, $a = \sum_{i=0}^{B} \lambda_i 2^i$, define $\widehat{a}_j := \sum_{i=0}^j \lambda_i 2^i$. The CoT then translates the binary representation of each vertex index  $a \in [1,|V|]$ into the atomic representation by enumerating $0 \widehat{a_1}_0 \dots \widehat{a_1}_{\lceil \log |V| \rceil}$}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

We then implement a standard first-in-first-out queue.
As nothing can be deleted from the CoT sequence once it has been generated, we maintain a pointer $\in \mathbb{N}$ that indicates the current head of the queue.
The CoT then starts with the first query vertex.
We copy into the CoT all the edges starting with the end of the edge. We then move to the second edge in the CoT and write down all the edges starting with the end of that edge, and so on. If at any point we encounter the target vertex, we exit. Throughout, in every CoT step, we additionally maintain the pointer $\in \mathbb{N}$ indicating the current head of the queue.
%
%
\end{proof}


%

 %
  %
   %
    %

\subsubsection{Experiment}
\label{app:dag-experiments}

We show that the CoT described in Theorem \ref{thm:app-reachability} is sufficient for Transformers to solve the problem of DAG reachability in the general case. For that, we train Transformers to predict the reachability of two vertices in a random DAG, both with and without a CoT.

\paragraph{Data generation.} Each random DAG is generated by sampling a random lower triangular adjacency matrix and instantiating a DAG from it. We then compute the distances between all pairs of vertices in the graph. When sampling examples for training or evaluation, we select, with equal probability, either an unconnected pair labeled as 0 or a connected pair labeled as 1. For connected pairs, we sample the pairs such that the distribution of possible distances is uniform.

For example, in a chain graph $A \to B \to C$, the unconnected pairs $(C, B)$, $(C, A)$, and $(B, A)$ are sampled with a probability of 1/6 each. The connected pairs $(A, B)$ and $(B, C)$ are sampled with a probability of 1/8 each (distance = 1), while the pair $(A, C)$ is sampled with a probability of 1/4 (distance = 2). This way, we avoid the bias toward shorter distances.

For each generated DAG, we compute the Weisfeiler-Lehman hash and ensure that the hashes of the training and test DAGs do not overlap. When provided to the model, the input data is encoded as described in Theorem \ref{thm:app-reachability}.

\paragraph{Model and training.} We use a decoder-only Transformer based on the GPT-2 architecture \cite{Radford2019LanguageMA}. The model has 4 layers, 4 attention heads per layer, and 256 hidden dimensions. The model is trained for 50k steps using AdamW with a batch size of 64 samples.

We train the models in two regimes: with and without a CoT. In the CoT regime, we generate a BFS-based CoT as described in Theorem \ref{thm:app-reachability} and append it to the input data.
To simplify and reduce computational cost, we represented all vertex indices as decimal numbers, eschewing conversion to atomic symbols.
During training, we optimize the next-token prediction loss on the CoT and answer parts of the sequence, while ignoring the predictions for the input tokens. During evaluation, the model autoregressively generates the CoT and the final prediction. The no-CoT regime is similar, but the CoT is not appended to the input, requiring the model to generate the answer directly.

Evaluation accuracy is checked every 9k steps. If the evaluation accuracy exceeds 99.5\%, training is terminated early.

\paragraph{Results.} The model's accuracy for various input sizes, corresponding to DAGs with 5 to 35 vertices, is shown in Figure \ref{fig:dag}. All values are averaged over three runs.

For all graph sizes except one, Transformers with CoT achieve near-perfect accuracy, while Transformers without CoT perform at chance level. The exception is for graphs with the smallest size (5 vertices), where both regimes achieve approximately 90\% accuracy. This discrepancy may be due to the limited number of distinct DAGs of this size, leading to insufficient training signal for the model to learn the algorithm of CoT construction.


\begin{figure}[H]
    \centering
    \begin{subfigure}{0.3\linewidth}
        \centering
\ \ \ \ \ \ \ \         \textsc{Parity} \\
        \includegraphics[width=\linewidth]{plots/llms/parity-r1-o1.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\linewidth}
        \centering
    \ \ \ \ \ \ \ \     \textsc{Multiplication} \\
        \includegraphics[width=\linewidth]{plots/llms/mult-r1-o1.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\linewidth}
        \centering
    \ \ \ \ \ \ \ \     \textsc{Median} \\
        \includegraphics[width=\linewidth]{plots/llms/median-r1-o1.pdf}
    \end{subfigure}

    \caption{Length of reasoning traces of DeepSeek R1 \citep{r1} and o1-mini \citep{jaech2024openai} LLMs prompted to solve the tasks of parity (left), multiplication (center), and median (right). 
    We only include traces leading to the correct answer.
    In both cases, the growth of CoT appears at least linear.
    Manual inspection of reasoning traces of DeepSeek R1 shows that the model perform the counting of all ones for \textsc{Parity}, and the naive quadratic algorithm for \textsc{Multiplication}.
    %
    }
    \label{fig:llm-parity-multiplication}
\end{figure}


\section{Experiments with Pretrained LLMs}\label{app:pretrained}

\paragraph{Approach.} To test our predictions on the necessary CoT length, we run experiments with state-of-the-art LLMs trained to generate CoT reasoning before responding to a user’s request: DeepSeek-R1 \cite{r1} and o1-mini \cite{jaech2024openai}. If, contrary to our predictions, a sub-linear algorithm for any of the discussed problems exists, these models might discover it and solve the task with a CoT of sub-linear size. Verifying this serves as a basic sanity check for our theory.

We tested the models on three tasks: parity, multiplication, and median. For the parity task, we generated random bitstrings of various lengths, ranging from 10 to 70, and asked the models to calculate their parity. We then selected the CoTs that led to correct answers and calculated their average size for each input length. The prompt provided to both models was: \texttt{You will receive a string. You have to manually calculate its parity. Finish your response with 1 if the parity is odd, and 0 if the parity is even.}

The approach for the multiplication task was similar. We generated random pairs of numbers with lengths ranging from 3 to 9 digits, prompted the models to multiply them, and computed the sizes of the correct CoTs. The prompt provided was: \texttt{You will receive two numbers. You have to multiply them manually. Finish your response with the precise result of the multiplication.}

For the median task, we generated sequences of odd length and prompted the models to find the median. The numbers were generated uniformly from 1 to $10^5$. The prompt provided was: \texttt{You will receive a sequence of numbers. You have to manually compute its median. Finish your response with the value of the median of this sequence.}

\paragraph{Results.} The results of the experiment are shown in Figure \ref{fig:llm-parity-multiplication}. For all three tasks, the size of the CoTs grew at least linearly, supporting the theoretical prediction that no CoT exists to solve parity, multiplication, or median in a sub-linear number of steps.

A qualitative inspection of the reasoning traces of DeepSeek-R1 revealed that, when computing the parity of a bit sequence, it copied every bit, counted the number of ones, and then determined the evenness of that count. For multiplication, it used a naive quadratic algorithm. However, the performance of DeepSeek-R1 dropped significantly for numbers with 10 digits or more, suggesting that its multiplication algorithm is not truly length-generalizable.




\section{Further Discussion on Related Work}\label{app:relation-literature}

%

%

%
%
%
%
%
%

%
%

%

\subsection{CoT Constructions in the Literature}\label{app:cot-literature}


%

%

%

Besides CoTs emulating Turing machines (Appendix~\ref{app:universality}), various other more specialized CoT constructions have been considered in the literature. \citet{feng2023towards} provide a general construction of transformers for a very broad class of dynamic programming (DP) algorithms.
This construction essentially only uses hard-attention operations, and can be expressed in UHAT as long as the aggregation function used in the DP algiorithm can be expressed. This includes, for instance, evaluating Boolean formulas.
\citet{cabannes2024iteration} analyzed the algorithm learned by a transformer trained on scratchpads for various algorithmic problems, identifying an ``iteration head'' attention pattern. This head effectively computes a unique hard attention pattern iterating through the input, combining material from the input with the last CoT step.
Relatedly, the Inductive Scratchpad \citet{abbe2024how} is a construction where each step is a function specifically of the previous CoT step and the input.

%

%
%




\subsection{Relation to Globality Degree}\label{app:globality}

Here, we discuss the relation of our results to the work by \citet{abbe2024how} on the Globality Degree.
We recall their definition, rephrased for self-containedness:
\begin{definition}[rephrased from Definition 2 of \citet{abbe2024how}]
For an alphabet $\mathcal{A}$ of cardinality $\mathcal{O}(poly(n))$, and  a distribution $D$ on $\mathcal{A}^n \times \mathcal{A}$, $D$ has \emph{constant globality} if and only if there is $k \in \mathbb{N}$ such that, for each $n$, there exists $S \subseteq [n]$, $|S|=k$, such that
\begin{equation}
    \operatorname{I}\left[(X[S], \hat{P}_X); Y\right] = n^{-O(1)}
\end{equation}
where $(X,Y) \sim D$, and $\hat{P}_X$ is the histogram of tokens in $X$.
\end{definition}
Here, we think of $(X,Y) \in \mathcal{A}^n \times \mathcal{A}$ as a pair of input and label; hence, $Y$ will generally be a function of $X$.

If one assumes $|\mathcal{A}| = O(1)$ and puts aside the knowledge of the token histogram, then constant globality is equivalent to the presence of nontrivial correlations with the circuit complexity class $NC^0$ (Lemma 6 in \citet{abbe2024how}), a highly restricted class contained within $AC^0$ and easily simulated by UHAT.

The key conjecture of \citet{abbe2024how} states:
\begin{conjecture}[Conjecture 1 of \citet{abbe2024how}]\label{conj:abbe}
    A distribution $P_{X,Y}$ with well-behaved $P_X$\footnote{This is defined in Definition 6 in \cite{abbe2024how}. A simple example is the uniform distribution over $\{0,1\}^N$.} is efficiently weakly learnable by a T-regular Transformer\footnote{Informally, this is a bidirectional transformer under a standard initialization (Definition 4 in \cite{abbe2024how}).} if and only if $P_{X,Y}$ has constant globality.
\end{conjecture}
For CoTs, the implied prediction is that (i) tasks with nonconstant globality require a CoT for efficient learning, (ii) a CoT is learnable if and only if each of its intermediate steps has constant globality.
A version of Conjecture~\ref{conj:abbe} is shown formally for a cycle classification task, though the conjecture makes far-reaching predictions beyond that. The practical predictions made by this conjecture are related to those of our theory, but not equivalent; for instance: %
\begin{enumerate}
%
\item (diverging prediction) \textsc{Parity} has low globality, because the conditioning on the histogram is not constrained. Conjecture~\ref{conj:abbe} thus provides a more optimistic prediction here than our results.
%
\item (converging prediction) The Cycle task of \citet{abbe2024how} requires a linear-length CoT by Theorem~\ref{thm:uhat-cot-bound}, and can also be shown to not be representable at sub-linear average sensitivity under a reasonable input encoding. Difficulty is predicted under both perspectives.
%
%
 %
%

%

%
%
%
%
%
%
%

%
 %
%
    %
   %
  %
 %
%

%
%
%
%
%
\end{enumerate}
%





Overall, there are similarities and differences between the practical predictions made by Conjecture~\ref{conj:abbe} and our unconditional lower bounds.
On a technical level, the theoretical arguments of \citet{abbe2024how}  rely on \emph{bidirectional} attention at least on the original input (if not the scratchpad), different from the causally masked transformers generally used in LMs. Expanding those arguments to causally masked transformers  is an interesting problem for further research.
%


%
%

%
%


%


%



%
%
%
%
%


%




%


%
%
%

%

%

%

%

%

%

%
 %
  %
   %
    %
        %
    %
    %
     %
      %
        %
    %
    %
    %
    %
    %

%
%
%


\begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]
        {plots/Number-of-token-comparision.png}
        \caption{Multiplication: Comparing the number of tokens used in the $\mathcal{O}(N\log N)$ NTT CoT (Appendix~\ref{app:mult-scratchpad}) vs a CoT implementation of the naive $\mathcal{O}(N^2)$ algorithm.
        The token count of the NTT CoT is piecewise constant because the length only depends on the prime $p$ used in the construction; length is asymptotically $\mathcal{O}(N\log N)$.
        }
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]
        {plots/Comparisions_new/4digit_iid.png}
        \subcaption{4D In-Domain Autoregressive}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/Comparisions_new/6digit_iid.png}
        \subcaption{6D In-Domain Autoregressive}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/Comparisions_new/8digit_iid.png}
        \subcaption{8D In-Domain Autoregressive}
    \end{minipage}

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]
        {plots/Comparisions_new/4digit_ood.png}
        \subcaption{4D Held-Out Autoregressive}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/Comparisions_new/6digit_ood.png}
        \subcaption{6D Held-Out  Autoregressive}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/Comparisions_new/8digit_ood.png}
        \subcaption{8D Held-Out Autoregressive}
    \end{minipage}

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]
        {plots/Comparisions_new/4digit_ood_enc.png}
        \subcaption{4D Held-Out Direct}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/Comparisions_new/6digit_ood_enc.png}
        \subcaption{6D Held-Out Direct}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/Comparisions_new/8digit_ood_enc.png}
        \subcaption{8D Held-Out Direct}
    \end{minipage}
    
    \caption{Arithmetic: Results on \textsc{Addition} and \textsc{Multiplication} of binary numbers, no CoT. Top row: In-Domain test data, Middle row: Held-Out test data (autoregressive decoding), bottom row: Held-Out test data (direct parallel decoding). Addition is learned well, even when the digits are directly decoded. In contrast, multiplication does poorly.}
    \label{fig:arithmetic-comparison}
\end{figure}








\end{document}
