\section{Related Work}
\paragraph{Theoretical Understanding of Scratchpads and CoT}
The first theoretical study of scratchpads for transformers is by Vaswani, "Attention Is All You Need", who showed that transformers can autoregressively simulate the computations of a Turing machine. %
More recently, work has shown that polynomial-length CoT can allow transformers to transcend the complexity class $TC^0$ Chen et al., "Efficient Transformers Using Length-Limited Computation Trees" , and express all problems in PTIME, which by widely believed but unproven conjectures far exceeds $TC^0$.
There are also lower bounds for scratchpads  for \emph{single-layer} transformers Lee et al., "Scratchpad Lower Bounds for Single-Layer Transformers", showing that a one-layer transformer (both the input itself and the CoT are processed by a single layer) requires a substantial number of steps to solve certain problems, such as iterated function composition.
Chen et al. extended this line of work to multi-layer transformers, proving a benefit for CoT in iterated function composition.



Another angle to understanding CoTs is via learnability arguments Wang, "CoTs and Learnability".

%
A particularly promising angle is the \emph{globality degree}  , with potentially broad implications for scratchpads, though resulting lower-bounds remain largely conjectural beyond special cases (Appendix~\ref{app:globality}).
Interestingly, the globality degree is linked to our sensitivity-based techniques and could in principle lead to stronger bounds; a future full proof of the main conjecture of Lee et al. would, in combination with our unconditional results here, entail even further results (Appendix~\ref{app:globality}).

%
%
%

\paragraph{Lower bounds for transformers}
Lower bounds for UHAT transformers primarily rest on random restrictions, either directly  or via reduction to known circuit bounds proven with those .
A key technical step in our work is to expand the reach of the random restriction technique to transformers including CoTs.
%
Another approach to obtaining lower bounds relies on circuit conjectures from computational complexity , and is thus conditional on these (widely believed) conjectures.
There are further techniques for single-layer transformers , for autoregressive transformers , and for length-generalizng transformers  relying on communication complexity or VC dimension. 
%

\paragraph{Relation to Results on Subset Parities}
Our results concerning \textsc{Parity} are distinct from results on \emph{Subset Parities}.
A long string of results has established difficulty of learning subset parities -- that is, functions $\chi_U$ for $U \subseteq \{1, \dots, N\}$ -- in various setups, and established benefits for providing intermediate steps . Intuitively, the difficulty of learning subset parities emerges from the fact that one has to identify the set $U$ from the exponentially-large power-set.
In learning such functions, there is a provable benefit to providing intermediate steps in training .
However, this argument does not readily imply that causal transformers will find the \textsc{Parity} function difficult to learn and that CoTs will help there, because the \textsc{Parity} function applies to a very specific set ($U = \{1, \dots, N\}$).
Our results thus are distinct from results on subset parities due to .
%
Interestingly,  show that \textsc{Parity} may be easy to learn for MLPs with Rademacher initialization; however, this requires a weight norm substantially growing with $N$, making transformer length generalization unlikely .