@article{Bhattamishra2024Separations,
  author       = {Satwik Bhattamishra and
                  Michael Hahn and
                  Phil Blunsom and
                  Varun Kanade},
  title        = {Separations in the Representational Capabilities of Transformers and
                  Recurrent Architectures},
  journal      = {CoRR},
  volume       = {abs/2406.09347},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.09347},
  doi          = {10.48550/ARXIV.2406.09347},
  eprinttype    = {arXiv},
  eprint       = {2406.09347},
  timestamp    = {Mon, 15 Jul 2024 16:11:03 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-09347.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{abbe2023generalization,
  author       = {Emmanuel Abbe and
                  Samy Bengio and
                  Aryo Lotfi and
                  Kevin Rizk},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Generalization on the Unseen, Logic Reasoning and Degree Curriculum},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {31--60},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/abbe23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/AbbeBLR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{abbe2024learning,
  title={Learning High-Degree Parities: The Crucial Role of the Initialization},
  author={Abbe, Emmanuel and Cornacchia, Elisabetta and H{\k{a}}z{\l}a, Jan and Kougang-Yombi, Donald},
  journal={arXiv preprint arXiv:2412.04910},
  year={2024}
}

@inproceedings{barcelo2024logical,
title={Logical Languages Accepted by Transformer Encoders with Hard Attention},
author={Pablo Barcel{\'o} and Alexander Kozachinskiy and Anthony Widjaja Lin and Vladimir Podolskii},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gbrHZq07mq}
}

@article{barcelo2025ehrenfeucht,
  title={Ehrenfeucht-Haussler Rank and Chain of Thought},
  author={Barcel{\'o}, Pablo and Kozachinskiy, Alexander and Steifer, Tomasz},
  journal={arXiv preprint arXiv:2501.12997},
  year={2025}
}

@article{chen2024theoretical,
  title={Theoretical limitations of multi-layer Transformer},
  author={Chen, Lijie and Peng, Binghui and Wu, Hongxun},
  journal={arXiv preprint arXiv:2412.02975},
  year={2024}
}

@inproceedings{feng2023towards,
title={Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective},
author={Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=qHrADgAdYu}
}

@article{hahn2020theoretical,
	title="Theoretical Limitations of Self-Attention in Neural Sequence Models",
	author="Michael {Hahn}",
	journal="Transactions of the Association for Computational Linguistics",
	volume="8",
	pages="156--171",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3014096773",
	year="2020"
}

@article{hahn2023theory,
	title={A theory of emergent in-context learning as implicit structure induction},
	author={Michael Hahn and Navin Goyal},
	journal={arXiv Preprint},
	year={2023},
	     url={https://arxiv.org/abs/2303.07971}
}

@article{hao2022formal,
  title={Formal language recognition by hard attention transformers: Perspectives from circuit complexity},
  author={Hao, Yiding and Angluin, Dana and Frank, Robert},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={800--810},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{huang2024formal,
  title={A Formal Framework for Understanding Length Generalization in Transformers},
  author={Huang, Xinting and Yang, Andy and Bhattamishra, Satwik and Sarrof, Yash and Krebs, Andreas and Zhou, Hattie and Nakkiran, Preetum and Hahn, Michael},
  journal={arXiv preprint arXiv:2410.02140},
  year={2024}
}

@article{kozachinskiy2024lower,
  title={Lower bounds on transformers with infinite precision},
  author={Kozachinskiy, Alexander},
  journal={arXiv preprint arXiv:2412.20195},
  year={2024}
}

@inproceedings{li2024chain,
  title={Chain of Thought Empowers Transformers to Solve Inherently Serial Problems},
  author={Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@inproceedings{merrill2023expresssive,
title={The Expressive Power of Transformers with Chain of Thought},
author={William Merrill and Ashish Sabharwal},
booktitle={NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning},
year={2023},
url={https://openreview.net/forum?id=CDmerQ37Zs}
}

@article{merrill2023parallelism,
  title={The parallelism tradeoff: Limitations of log-precision transformers},
  author={Merrill, William and Sabharwal, Ashish},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={531--545},
  year={2023},
  publisher={MIT Press}
}

@article{peng2024limitations,
  title={On limitations of the transformer architecture},
  author={Peng, Binghui and Narayanan, Srini and Papadimitriou, Christos},
  journal={arXiv preprint arXiv:2402.08164},
  year={2024}
}

@article{perez2019turing,
  title={On the Turing Completeness of Modern Neural Network Architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}

@article{sanford2023representational,
  title={Representational strengths and limitations of transformers},
  author={Sanford, Clayton and Hsu, Daniel J and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{sanford2024transformers,
  title={Transformers, parallel computation, and logarithmic depth},
  author={Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
  booktitle={Forty-first International Conference on Machine Learning},
year={2024}
}

@article{wies2022sub,
  title={Sub-task decomposition enables learning in sequence to sequence tasks},
  author={Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2204.02892},
  year={2022}
}

