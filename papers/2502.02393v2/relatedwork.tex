\section{Related Work}
\paragraph{Theoretical Understanding of Scratchpads and CoT}
The first theoretical study of scratchpads for transformers is by \citet{perez2019turing}, who showed that transformers can autoregressively simulate the computations of a Turing machine. %
More recently, work has shown that polynomial-length CoT can allow transformers to transcend the complexity class $TC^0$ \citep{feng2023towards, merrill2023expresssive, li2024chain}, and express all problems in PTIME, which by widely believed but unproven conjectures far exceeds $TC^0$.
There are also lower bounds for scratchpads  for \emph{single-layer} transformers \cite{peng2024limitations,barcelo2025ehrenfeucht}, showing that a one-layer transformer (both the input itself and the CoT are processed by a single layer) requires a substantial number of steps to solve certain problems, such as iterated function composition.
\citet{chen2024theoretical} extended this line of work to multi-layer transformers, proving a benefit for CoT in iterated function composition.



Another angle to understanding CoTs is via learnability arguments \citep{wies2022sub,kim2024transformers,hahn2023theory,abbe2024how}.
%
A particularly promising angle is the \emph{globality degree} \cite{abbe2024how}, with potentially broad implications for scratchpads, though resulting lower-bounds remain largely conjectural beyond special cases (Appendix~\ref{app:globality}).
Interestingly, the globality degree is linked to our sensitivity-based techniques and could in principle lead to stronger bounds; a future full proof of the main conjecture of \citet{abbe2024how} would, in combination with our unconditional results here, entail even further results (Appendix~\ref{app:globality}).

%
%
%

\paragraph{Lower bounds for transformers}
Lower bounds for UHAT transformers primarily rest on random restrictions, either directly \citep{hahn2020theoretical, barcelo2024logical} or via reduction to known circuit bounds proven with those \citep{hao2022formal}.
A key technical step in our work is to expand the reach of the random restriction technique to transformers including CoTs.
%
Another approach to obtaining lower bounds relies on circuit conjectures from computational complexity \citep{merrill2023parallelism, sanford2024transformers} and is thus conditional on these (widely believed) conjectures.
There are further techniques for single-layer transformers \citep{kozachinskiy2024lower, peng2024limitations, sanford2023representational, Bhattamishra2024Separations, barcelo2025ehrenfeucht}, for autoregressive transformers \citep{chen2024theoretical}, and for length-generalizng transformers \citep{huang2024formal} relying on communication complexity or VC dimension. 
%

\paragraph{Relation to Results on Subset Parities}
Our results concerning \textsc{Parity} are distinct from results on \emph{Subset Parities}.
A long string of results has established difficulty of learning subset parities -- that is, functions $\chi_U$ for $U \subseteq \{1, \dots, N\}$ -- in various setups, and established benefits for providing intermediate steps \citep[e.g.][]{wies2022sub, abbe2023generalization, kim2024transformers, anonymous2024chainofthought}.
Intuitively, the difficulty of learning subset parities emerges from the fact that one has to identify the set $U$ from the exponentially-large power-set.
In learning such functions, there is a provable benefit to providing intermediate steps in training \citep{wies2022sub, abbe2023generalization, kim2024transformers, anonymous2024chainofthought}.
However, this argument does not readily imply that causal transformers will find the \textsc{Parity} function difficult to learn and that CoTs will help there, because the \textsc{Parity} function applies to a very specific set ($U = \{1, \dots, N\}$).
Our results thus are distinct from results on subset parities due to \citet{wies2022sub, abbe2023generalization, kim2024transformers, anonymous2024chainofthought}.
%
Interestingly, \citet{abbe2024learning} show that \textsc{Parity} may be easy to learn for MLPs with Rademacher initialization; however, this requires a weight norm substantially growing with $N$, making transformer length generalization unlikely \citep{huang2024formal}.

%


%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%



%


%

%

%


%


%


%
%
%

%

%

%

%


%

%
%


%



%
%
%
%
%
%


%
%
%
%
%
%
%