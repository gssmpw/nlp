

\section{Experiments} \label{sec6}
We investigate the empirical performance of our new procedures in various experiments to demonstrate their effectiveness.
%To demonstrate the effectiveness of our new procedures, we investigate their empirical performance in the following experiments. 
Recall that our procedures are developed for two distinct goals, namely estimation of the optimal trade-off curve $T$ (see Section \ref{sec:4}) and auditing a privacy claim $T^{(0)}$ (see Section \ref{sec:goal2}). We will run experiments for both of these objectives. \\
%These goals correspond to Sections \ref{sec:4} and \ref{sec:goal2} respectively. \\
%This section aims to validate the theoretical results presented in Section~\todo{cite section} and Section~\todo{cite section}. \\
\textbf{Experiment Setting:} 
%We have outlined two distinct objectives along with their corresponding methodologies:
%\begin{description}
 %   \item[\textbf{Goal 1: Uniform Estimation of the Privacy Curve $T$}]  
 %   The first objective is to uniformly estimate an unknown privacy curve $T$, as stated in Theorem~\ref{theo:1}. To validate not only the theoretical correctness but also the practical effectiveness of this estimation approach, we conducted a simulation study on all four mechanisms. The results of this study are presented in \todo{Table~\ref{tab:estimation_f_curves} and Figure~\ref{fig:todo}.}
 %   \item[\textbf{Goal 2: Detection of Privacy Violations}] 
 %   The second objective is inferential in nature. As formulated in Theorem~\ref{theo:auditor}, the goal is to detect privacy violations for a predefined false positive rate. To demonstrate the effectiveness of this methodology, we constructed faulty algorithms and analyzed their behavior. The results of this analysis are depicted in Figure~\ref{fig:todo}.
%\end{description}
Throughout the experiments, we consider databases $\DB,\DB' \in [0,1]^r$, where the participant number is always $r=10$. As discussed in Section \ref{sec:overview_techniques}, we first choose a pair of neighboring datasets such that there is a large difference in the output distributions of $\Mech(D)$ and $\Mech(D')$. We can achieve this by simply choosing $D$ and $D'$ to be as far apart as possible (while still remaining neighbors) and we settle on the choice 
%As typical in the privacy validation literature, we consider two neighboring databases that are far apart. On the $r$-dimensional cube $[0,1]^r$ we make the natural choice of
\begin{equation}\label{eq_databases}
    \DB=(0,\hdots, 0)\quad \textnormal{and} \quad \DB'=(1,0,\hdots, 0)
\end{equation}
for all our experiments.
%and notice that similar results as the ones below hold for other pairs of databases. %Our methods do however work just as well for other data bases $D$ and $D'$.
%Additionally, for data lying in the unit cube, this choice is natural, as these two databases are far apart on the unit cube.

\subsection{Mechanisms}\label{sec:algorithms}
In this section, we test our methods on two frequently encountered mechanisms from the auditing literature: the Gaussian mechanism and differentially private Stochastic Gradient Descent (DP-SGD). We study two other prominent DP algorithms -- the Laplace and Subsampling mechanism -- in Appendix \ref{AppB}. \\
%We apply our methods to four mechanisms frequently encountered in the privacy literature: the Gaussian mechanism, the Laplace mechanism, the Subsampling mechanism, and, most notably, the Noisy Stochastic Gradient Descent (DP-SGD) mechanism. These algorithms are quite  heterogeneous and hence collectively form a good benchmark to evaluate our methods. We quickly review these mechanisms and specify parameter settings. \\

\noindent \textbf{Gaussian mechanism.}
We consider the summary statistic $S(x)= \sum_{i=1}^{10} x_i$ and the mechanism
\begin{equation*}
    M(x):= S(x)+Y~,
\end{equation*}
where $Y\sim \mathcal N (0, \sigma^2)$. The statistic $S(x)$ is privatized by the random noise $Y$ if the variance $\sigma^2$ of the Normal distribution is appropriately scaled. We choose $\sigma = 1$ for our experiments and note that - in our setting - the optimal trade-off curve is given by 
\begin{align*}
     T_{Gauss}(\alpha)= \Phi(\Phi^{-1}(1-\alpha)- 1).
\end{align*}
We point the reader to \cite{Dong2022} for more details. \\








%\textbf{Additive Noise Mechanisms}
%The most prominent DP algorithms are the Laplace and the Gaussian Mechanism. If we consider the summary statistic $S(x)= \sum_{i=1}^{10} x_i$, the output can be described by
%\begin{equation*}
%    M(x):= S(x)+Y~,
%\end{equation*}
%where $Y\sim Lap(0,b)$ or $Y\sim \mathcal N (0, \sigma^2)$, respectively. Here $b>0$ denotes the scale parameter in the Laplace distribution and $\sigma^2$ the variance for the normal distribution. Given the structure of $M$, these mechanisms are generically referred to as "additive noise mechanisms". Appropriately scaled, both mechanisms fulfill $f$-DP. For the Gaussian mechanism we set $\sigma=1$, for which \cite{Dong2022} derived the trade-off curve
%\begin{equation*}
%    T_{Gauss}(\alpha)= \Phi(\Phi^{-1}(1-\alpha)-\mu)
%\end{equation*}
%as an explicit expression of the optimal trade-off function between $P = \mathcal N(0,1)$ and $Q = \mathcal N(\mu,1)$. For the Laplace mechanism, we set $b=1$, and note again that \cite{Dong2022} derived an explicit formula given by
%\begin{equation*}
%    T_{Lap}(\alpha)=\begin{cases}
%        1-\exp(\mu)\alpha,  &\alpha<\exp(-\mu)/2~,\\
%        \exp(-\mu)/4 \alpha,  &\exp(-\mu)/2\leq \alpha\leq 1/2~,\\
%        \exp(-\mu)(1-\alpha), &\alpha>1/2
 %   \end{cases}
%\end{equation*}
%for the optimal trade-off function between $P = Lap(0,1)$ and $Q = Lap(\mu,1)$.\\
%After the simpler additive noise mechanisms, we turn to the more advanced mechanisms of subsampling and the  DP-SGD, both of which play a role in the context of private machine learning.\\
%\textbf{Subsampling Mechanism:} Random subsampling provides an effective way to enhance the privacy of a DP mechanism $M$. We only provide a rough outline here and refer for details to \cite{Dong2022}.
%In simple words, we choose an integer $m$ with  $1\leq m< r$, where $r$ is the size of the databases $D$. In subsampling, we extract random subsamples of size $m$ of these databases, giving us the smaller databases $\bar D$. The mechanism $M$ is then applied to  $\bar D$ instead of $D$, providing an additional layer of protection to users. If a user is not part of $\bar D$, their privacy cannot be compromised. The amplifying effect of subsampling is visible in the optimal trade-off curve: If $M$ implies the curve $T$, it turns out that $M(\bar D)$ implies the curve
%\begin{equation*}
%    \bar T(\alpha)=  \frac{m}{r}T(\alpha)+\frac{r-m}{r}(1-\alpha),
%\end{equation*}
%which is strictly more private than $T$ for any $m<r$. A minor technical peculiarity of subsampling is that the resulting curve $\bar T$ is generally not symmetrical, even if $T$ is (see \cite{Dong2022} for details on the symmetry of trade-off functions). Trade-off curves are usually considered to be symmetrical and one can symmetrize $\bar T$ by applying a symmetrizing operator $\mathbf{C}$ (again, see \cite{Dong2022}). In our simulations we will adhere to this procedure and depict the symmetrized version $\mathbf{C}[\bar T]$ together with a symmetrized estimator. Further details on the symmetrization can be found in Appendix \ref{AppB}.
%At first glance, this result may seem complicated, but it is in fact quite natural. Suppose we select $m$ entries randomly for the two neighboring databases $\DB,\DB'$ with each of size $k$. All entries in  $\DB,\DB'$ except one are identical - so the chance of selecting the entry where they differ (in $m$ draws) is $p=m/k$. 
%Conversely the probability of not selecting named entry is $(1-p)$.  
%This characterizes the construction of $T_p$, and the factor $(1-\alpha)$ corresponds to perfect privacy, as in that case $\alpha+\beta=\alpha+1-\alpha=1$. With that in hand, $C_p(T)=\min\{T_p,T_p^{-1}\}^{**}$ (again see in \cite{Dong2022}) is simply a symmetrization of $T_p$ in a sense that it is the greatest convex minorant of the minimum $T_p$ and $T_p^{-1}$ (\todo{maybe cite sth}). Here, for a function $T$, $T^{**}$ denotes the twice convex conjugate of $T$.\\  
%For the following experiments involving subsampling, we use the Gaussian mechanism as $M$ (with $\sigma=1$) and obtain the subsampled version $M'$, by fixing the parameter $m=5$ (recall that $r=10$). \\
%Observing only independent outputs of $M(x)$ will only yield an estimator for $\hat T_p$. As an additional contribution, we approximate $T_{sub}(\alpha)$ by incorporating a numerical approximation based on $\hat T_p$. \todo{should we expand here?}\\

\noindent \textbf{DP-SGD.} The DP-SGD mechanism is designed to (privately) approximate a solution for the empirical risk minimization problem
\begin{equation*}
\theta^*=argmin_{\theta\in \Theta} \mathcal L_x(\theta) \quad \text{with} \quad \mathcal L_x(\theta)=\frac{1}{r}\sum_{i=1}^{r} \ell(\theta, x_i)~.
\end{equation*}
Here, $\ell$ denotes a loss function, $\Theta$ a closed convex set and $\theta^*\in \Theta$ the unique optimizer. For sake of brevity, we provide a description of DP-SGD in the appendix (see Algorithm \ref{alg:noisy_sgd}). In our setting, we consider the loss function $\ell(\theta, x_i)=\frac{1}{2} (\theta-x_i)^2$, initial model $\theta_0=0$ and $\Theta=\mathbb{R}$. The remaining parameters are fixed as $\sigma=0.2, \rho = 0.2, \tau = 10, m=5$. In order to have a theoretical benchmark for our subsequent empirical findings, we also derive the theoretical trade-off curve $T_{SGD}$ analytically for our setting and choice of databases (see Appendix \ref{AppB} for details). Our calculations yield
%For the choice of databases as in equation \eqref{eq_databases}, one can compute the trade-off curve $T_{SGD}$ analytically: 
\begin{equation*}
    T_{SGD}(\alpha)=\sum_{I\subset \{1,\hdots, \tau \}} \frac{1}{2^{\tau}}\Phi\Big(\Phi^{-1} (1-\alpha)-\frac{\mu_I}{\bar\sigma}\Big)~.
\end{equation*}
where $\mu_I$ is chosen as in \eqref{mu_I} and $\bar{\sigma}$ as in \eqref{sigma_bar}.

\subsection{Simulations}
We begin by outlining the parameter settings of our KDE and $k$-NN methods for our simulations. We then discuss the metrics employed to validate our theoretical findings and, in a last step, present and analyze our simulation results.\\
\textbf{Parameter settings:}
%For the subsequent simulations we always use the same parameters across all algorithms, acknowledging the black-box setting. 
For the KDEs, we consider different sample sizes of $n_1=10^2,10^3,10^4,10^5,10^6$ and we fix the perturbation parameter at $h=0.1$. For the bandwidth parameter $b$ (see Sec. \ref{sec:kde}), we use the method of \cite{bandwidth}. To approximate the optimal trade-off curve, we use $1000$ equidistant values for $\eta$ between $0$ and $15$ (see Algorithm \ref{alg:pointwise_KDE_estimator} for details on the procedure). For the $k$-NN, we set the training sample size to $n_2=10^6,10^7,10^8$ and testing sample size to $10^6$. \\
%\todo{Yu: are you sure that this is sufficient?}\\

\noindent \textbf{Estimation}
The first goal of this work is estimation of the optimal trade-off curve $T$. In our experiments, we want to illustrate the uniform convergence of the estimator $\hat T_h$ to the optimal curve $T$, derived in Theorem \ref{theo:1}. Therefore, we consider increasing sample sizes $n_1$ to study the decreasing error. The distance of $\hat T_h$ and $T$ in each simulation run is measured by the  uniform distance\footnote{Of course, one cannot practically maximize over all (infinitely many) arguments $\alpha \in [0,1]$. The estimator $\hat T_h$ is made for a grid of values for $\eta$ (see our parameter settings above) and we maximize over all gridpoints.} %maximum distance on a grid $G$ 
%We repeatedly estimate the respective trade-off curves of the four mechanism introduced in Section \ref{sec:algorithms} and computed 
\[
    Error_T:=\sup_{\alpha \in [0,1]}|\hat T_h(\alpha)-T(\alpha)|.
\]
%on a grid $G$ of $[0,1]$. 
%In our setting, we defined $G$ as the grid given by the KDE. Since, we choose $1000$ $\eta$ equidistant, we will get $1000$ $\alpha$ values. However, they do not have to be equidistant nor unique. 
To study not only the distance in one simulation run, but across many, we calculate $Error_T$ in $1000$ independent runs and take the (empirical) mean squared error
\begin{equation}\label{eq:mse}
    MSE(Error_T):= \Ex{Error_T^2}
    %\mathbb{E}\mathbb Var(Error_G)+\mathbb E[Error_G]^2~.
\end{equation}
The results are depicted in Figure \ref{fig:estimation_mse} for the DP algorithms described in this section and the appendix. On top of that, we also construct figures that upper and lower bound the worst case errors for the Gaussian mechanism and DP-SGD over the $1000$ simulation runs. These plots visually show how the error of the estimator $\hat T_h$ shrinks as $n_1$ grows. 
% for the sample size $n_1=1000$. 
%For that, we computed the worst estimation point wise on an equidistant discretization of $[0,1]$ and interpolated the curves linearly. 
The results are summarized in Figures \ref{fig:gaussian}-\ref{fig:sgd}.\\
\begin{figure}
\centering\includegraphics[width=0.75\linewidth]{Figures/plot_table.png}
    \caption{\centering
    Empirical MSE defined in \eqref{eq:mse} to empirically validate Theorem \ref{theo:1} for varying sample sizes $n_1$ and over $1000$ simulation runs each.}\label{fig:estimation_mse}
\end{figure}
\begin{figure*}[h!]
    \centering
    \subfloat[$n_1=10^3$]{\includegraphics[width=0.3\textwidth]{Figures/Gaussian_shade_1000.png}}
    \hfill
    \subfloat[$n_1=10^4$]{\includegraphics[width=0.3\textwidth]{Figures/Gaussian_shade_10000.png}}
    \hfill
    \vspace{-0.2cm}
    \subfloat[$n_1=10^5$]{\includegraphics[width=0.3\textwidth]{Figures/Gaussian_shade_100000.png}}
    \caption{Estimation of the Gaussian Trade-off curve $T_{Gauss}$ for varying sample sizes and $\mu=1$. Min- and Max Curve lower- and upper bound the worst point-wise deviation from the true curve $T_{Gauss}$ over $1000$ simulations.}
    \label{fig:gaussian}
\vspace{-0.1cm}
\centering
    \subfloat[$n_1=10^3$]{\includegraphics[width=0.3\textwidth]{Figures/SGD_shade_1000.png}}
    \hfill
    \subfloat[$n_1=10^4$]{\includegraphics[width=0.3\textwidth]{Figures/SGD_shade_10000.png}}
    \hfill
    \vspace{-0.2cm}
    \subfloat[$n_1=10^5$]{\includegraphics[width=0.3\textwidth]{Figures/SGD_shade_100000.png}}
    \caption{Estimation of the DP-SGD Trade-off curve $T_{SGD}$ for varying sample sizes. Min- and Max Curve lower- and upper bound the worst point-wise deviation from the true curve $T_{SGD}$ over $1000$ simulations.}
    \label{fig:sgd}
\end{figure*}


\noindent {\textbf{Inference}\label{Inference}}
Next, we turn to the second goal of this work: Auditing a $T^{(0)}$-DP claim for a postulated trade-off curve $T^{(0)}$. 
The theoretical foundations of our auditor can be found in Theorem \ref{theo:auditor}. The theorem makes two guarantees: First, that for a mechanism $M$ satisfying $T^{(0)}$-DP the auditor will (correctly) not detect a violation, except with low, user-determined probability $\gamma$. Second, if $M$ violates  $T^{(0)}$-DP, the auditor will (correctly) detect the violation for sufficiently large sample sizes $n_1,n_2$. Together, these results mean that if a violation of $T^{(0)}$-DP is detected by the auditor, the user can have high confidence that $M$ does indeed not satisfy $T^{(0)}$-DP. 
%To begin, we examine the first result, which ensures, informally speaking that the auditor will not generate more than $\gamma>0$ false positives, when auditing a mechanism $M$. The second result guarantees that if the claimed privacy does not hold, the auditor will eventually identify this with probability $1$ as the sample size increases.
For the first part, we consider a scenario, where the claimed trade-off curve $T^{(0)}$ is the correct one $T^{(0)}=T$ ($M$ does not violate $T^{(0)}$-DP). For the second part, we choose a function $T^{(0)}$ above the true curve $T$ ($M$ violates $T^{(0)}$-DP). We will consider both scenarios for the Gaussian mechanism and DP-SGD.
%We will use two of the four mechanism for illustration: First, the standard Gaussian mechanism, as an example of an additive noise mechanism and second the DP-SGD mechanism, as an example of a machine learning mechanism.
%We start with auditing correctly claimed curves $T^{(0)}$. For that purpose, 
We run our auditor (Algorithm \ref{alg:auditor}) with parameters $n_1=10^4$ and $\gamma=0.05$ fixed. The choice of $\gamma=0.05$ is standard for confidence regions in statistics and we further explore the impact of $n_1$ and $\gamma$ in additional experiments in Appendix \ref{AppB}. Here, we focus on the most impactful parameter, the sample size $n_2$ and study values of  $n_2 = 10^6,10^7,10^8$. \\
Technically, the auditor only outputs a binary response that indicates whether a violation is detected or not. However, in our below experiments, we depict the inner workings of the auditor and geometrically illustrate how a decision is reached. More precisely, in Figure \ref{fig:not_faulty_sgd_gauss} we depict the claimed trade-off curve $T^{(0)}$ as a blue line. The auditor makes an estimate for the true trade-of curve $T$, namely $\hat T_h$ depicted as the orange line. The location, where the orange line (estimated DP) and the blue line (claimed DP) are the furthest apart is indicated by the vertical, dashed green line. This position is associated with the threshold $\hat \eta^*$ in Algorithm \ref{alg:pointwise_KDE_estimator}. As a second step, $\hat \eta^*$ is used in the $k$NN method to make a confidence region, depicted as a purple square (this is $\square_\gamma$ from \eqref{e:defsq}). If the square is fully below the claimed curve $T^{(0)}$, a violation is detected (Figure \ref{fig:faulty_sgd_gauss}) and if not, then no violation is detected (Figures \ref{fig:gaussian} and \ref{fig:sgd}). As we can see, detecting violations requires $n_2$ to be large enough, especially when $T^{(0)}$ and $T$ are close to each other. \\
For the incorrect $T^{(0)}$-DP claims, we have done the following: For the Gaussian case (Figure \ref{fig:faulty_sgd_gauss}), we have used a trade-off curve with parameter $\mu=0.5$ instead of the true $\mu=1$. For DP-SGD, we have used the trade-off curve corresponding to $\tau = 5$ instead of the true $\tau =10$ iterations (Figure \ref{fig:faulty_sgd_gauss}). 

%the trade-off curve of DP-SGD with a correctly claimed privacy curve and false claim. The correctly claimed can be found in Figure \ref{fig:not_faulty_sgd_gauss}. For the incorrectly claimed curve, it was stated that DP-SGD ran for only $t_{-}=5$ iterations, accessing the data just five times and thereby reinforcing the privacy guarantee. However, in reality it ran $t_{-}=10$ times, leading to a privacy breach. The results are depicted in Figure \ref{fig:faulty_sgd_gauss}.
%In this algorithm, we Algorithm \ref{alg:KDE_estimator} as a subroutine to derive an estimate $\hat \eta^*$ and we use the sample size $n_1=10^4$. Second, we run the $k$-NN with different sample sizes $n_2=10^6,10^7,10^8$. For the confidence level, we set $\gamma=0.05$, which yields confidence squares induced by $w(\gamma)$ defined in equation \eqref{e:wgamma}. 
%For the first audit in displayed in Figure \ref{fig:not_faulty_sgd_gauss}, we have audited a correctly claimed trade-off curve $T_0$. We detect a faulty mechanism, whenever the purple square $\square_\gamma=\square_{0.05}$ is disjoint from the claimed curve $T_0$. For a faulty implementation, we have use $\mu=0.5$ for the claimed curve $T_0$, while in reality the true curve only fulfills $\mu=1$. The results are displayed in Figure \ref{fig:faulty_sgd_gauss}. To complement these results with the DP-SGD, we also considered DP-SGD with a correctly claimed privacy curve and false claim. The correctly claimed can be found in Figure \ref{fig:not_faulty_sgd_gauss}. For the incorrectly claimed curve, it was stated that DP-SGD ran for only $t_{-}=5$ iterations, accessing the data just five times and thereby reinforcing the privacy guarantee. However, in reality it ran $t_{-}=10$ times, leading to a privacy breach. The results are depicted in Figure \ref{fig:faulty_sgd_gauss}.
\begin{figure*}[h!]
    \centering
    \subfloat[\centering $n_2=10^6$,\textbf{Ground Truth:} No Violation; \newline \textbf{Decision:} \textcolor{green}{"No Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Figures/gauss_100.png}}
    \hfill
    \subfloat[\centering $n_2=10^7$,\textbf{ Ground truth:} No Violation; \newline \textbf{Decision:} \textcolor{green}{"No Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Figures/gauss_100_7.png}}
    \hfill
    \subfloat[\centering $n_2=10^8$, \textbf{ Ground truth:}No Violation; \newline \textbf{Decision:} \textcolor{green}{"No Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Figures/gauss_100_8.png}}
   \vspace{-1em}
    \subfloat[\centering $n_2=10^6$, \textbf{Ground truth:} No Violation; \newline \textbf{Decision:} \textcolor{green}{"No Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Figures/sgd_100.png}}
    \hfill
    \subfloat[\centering $n_2=10^7$, \textbf{Ground truth:} No Violation; \newline \textbf{Decision:} \textcolor{green}{"No Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Figures/sgd_100_7.png}}
    \hfill
    \subfloat[\centering $n_2=10^8$, \textbf{Ground truth:} No Violation; \newline \textbf{Decision:} \textcolor{green}{"No Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Figures/sgd_100_8.png}} \caption{\textbf{Auditing a correct Mechanism:} Claimed curve $\textcolor{blue}{T^{(0)}} = T_{Gauss}$ (a,b,c) and $\textcolor{blue}{T^{(0)}} = T_{SGD}$ (d,e,f). Obtain critical vertical line with step 3 in Algorithm \ref{alg:auditor} with intercept $(\hat\alpha(\hat\eta^*), \hat \beta(\hat \eta^*))$, $k$-NN point estimator \small{\textcolor{purple}{\ding{108}}} $(\tilde\alpha(\hat\eta^*), \tilde \beta(\hat\eta^*))$ and confidence region $\textcolor{purple}{\square}$. The sample size for the KDE is $n_1=10^4$ and the confidence parameter is $\gamma=0.05$.}
    \label{fig:not_faulty_sgd_gauss}
\end{figure*}
\begin{figure*}[h!]
    \centering
    \subfloat[\centering $n_2=10^6$, \textbf{Ground truth:} Violation; \newline \textbf{Decision:} \textcolor{red}{"No Violation"}{\textcolor{red}{\scalebox{1.5}{\ding{55}}}}]{\includegraphics[width=0.3\textwidth]{Figures/gauss_faulty_100.png}}
    \hfill
    \subfloat[\centering $n_2=10^7$, \textbf{Ground truth:} Violation; \newline \textbf{Decision:} \textcolor{green}{"Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Figures/gauss_faulty_100_7.png}}
    \hfill
    \subfloat[\centering $n_2=10^8$, \textbf{Ground truth:} Violation; \newline \textbf{Decision:} \textcolor{green}{"Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Figures/gauss_faulty_100_8.png}}
    \vspace{-1em}
    \subfloat[\centering $n_2=10^6$, \textbf{Ground truth:} Violation; \newline \textbf{Decision:} \textcolor{red}{"No Violation"}{\textcolor{red}{\scalebox{1.5}{\ding{55}}}}]
    {\includegraphics[width=0.3\textwidth]{Figures/sgd_faulty_100.png}}
    \hfill
    \subfloat[\centering $n_2=10^7$, \textbf{Ground truth:} Violation; \newline \textbf{Decision:} \textcolor{red}{"No Violation"}{\textcolor{red}{\scalebox{1.5}{\ding{55}}}}]{\includegraphics[width=0.3\textwidth]{Figures/sgd_faulty_100_7.png}}
    \hfill
    \subfloat[\centering $n_2=10^8$, \textbf{Ground truth:} Violation; \newline \textbf{Decision:} \textcolor{green}{"Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Figures/sgd_faulty_100_8.png}}
     \caption{\textbf{Auditing a faulty Mechanism:} Claimed Curve $\textcolor{blue}{T^{(0)}} = T_{Gauss}$ (a,b,c) with $\mu=0.5$ and $\textcolor{blue}{T^{(0)}} = T_{SGD}$ (d,e,f) with $t_{-}=5$. Both mechanisms assume stronger privacy ($\mu=0.5<1$ and $t_{-}=5<10$). Critical vertical line derived by KDEs using step 3 in Algorithm \ref{alg:auditor} with intercept $(\hat\alpha(\hat\eta^*), \hat \beta(\hat \eta^*))$, $k$-NN point estimator {\textcolor{purple}{\ding{108}}} $(\tilde\alpha(\hat\eta^*), \tilde \beta(\hat\eta^*))$ and confidence region $\textcolor{purple}{\square}$. The sample size for KDE is $n_1=10^4$ and the confidence parameter is $\gamma=0.05$.}
    \label{fig:faulty_sgd_gauss}
\end{figure*}

\noindent\textbf{Implementation Details} The implementation is done using python and R. \footnote{\scriptsize{\url{https://github.com/stoneboat/fdp-estimation}}}. For the simulations, we have used a local device and a server. All runtimes were collected on a local device with an Intel Core i5-1135G7 processor (2.40 GHz), 16 GB of memory, and running Ubuntu 22.04.5, averaged over $10$ simulations. Thus, we demonstrate fast runtimes even on a standard personal computer.
Additionally, we used a server with four AMD EPYC 7763 64-Core (3.5 GHz) processors and 2 TB of memory and running Ubuntu 22.04.4 was used for repetitive simulations. For python, we have used Python 3.10.12 and the libraries "numpy" \cite{2020NumPy-Array}, "scikit-learn" \cite{pedregosa2011scikit} and "scipy" \cite{2020SciPy-NMeth}. For R, we used R version 4.3.1 and the libraries "fdrtool" \cite{fdrtool} and "Kernsmooth" \cite{Kernsmooth}. 
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Algorithm}                           & \textbf{Runtime in seconds} \\ \hline
Gaussian mechanism              &  26.3                                                    \\ \hline
Laplace mechanism            &    30.51                                                     \\ \hline
Subsampling mechanism         &   27.82                                                      \\ \hline
DP-SGD           &              61.1                                         \\ \hline
 
\end{tabular}
\caption{Average runtimes of Algorithm \ref{alg:pointwise_KDE_estimator} for $n_1=10^5$ over $10$ runs to obtain the full trade-off curve $T$.}
\label{tab:running_times_KDE}
\end{table}
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Algorithm}                           & \textbf{Runtime in seconds} \\ \hline
Gaussian mechanism              &    62.63                                                  \\ \hline
Laplace mechanism            &        67.04                                                 \\ \hline
Subsampling mechanism         &     66.98                                                  \\ \hline
DP-SGD           &    114.86                                                 \\ \hline
 
\end{tabular}
\caption{Average runtimes of Algorithm \ref{alg: general BayBox estimator} for $n_2=10^6$ over $5$ runs to obtain one point of the trade-off curve $T$ with confidence region.} %\\[-8ex]} 
\label{tab:running_times_kNN}
\end{table}

\subsection{Interpretation of the results}
Our experiments empirically showcase details of our methods' concrete performance. 
%refine our understanding of certain details of our methods. 
For Goal 1 (estimation), we see in Figure \ref{fig:estimation_mse} the fast decay of the estimation error of $\hat T_h$ for the optimal trade-off curve. The estimation error decays fast in $n_1$, regardless of whether there are plateau values in the sense of Assumption \ref{ass1} (e.g. Laplace Mechanism) or not (e.g. Gaussian Mechanism).
These quantitative results are supplemented by the visualizations in  
Figures~\ref{fig:gaussian}--\ref{fig:sgd}, where we depict the largest distance of $\hat T_h$ and $T$ in $1000$ simulation runs (captured by the red band). Even for the modest sample size of $n_1 = 10^3$, this band is fairly tight and for $n_1 = 10^5$ the estimation error is almost too minute to plot. We find this convergence astonishingly fast. It may be partly explained by the estimator $\hat T_h$ being structurally similar to $T$ -  after all $\hat T_h$ is also designed to be a trade-off curve for an almost optimal LR test.
The approximation over the entire unit interval corresponds to the uniform convergence guarantee in Theorem~\ref{theo:1}. 
%demonstrate that even with relatively small sample sizes, such as $n_1 = 10^3$, the worst global error across 1000 simulations remains notably small. For that observe that the combined worst deviation from the true curve $T_0$ across $1000$ simulations is already for $n_1=10^4$ almost negligible. As the sample size increases even further, the error converges to zero for all $\alpha\in[0,1]$ as visible in Figures~\ref{fig:gaussian}--\ref{fig:sgd} (c). This aligns with the uniform convergence established in Theorem~\ref{theo:1}. In addition to that, throughout all mechanisms, Figure \ref{fig:estimation_mse} illustrates the convergence for the same set of parameters, highlighting the robustness and adaptability necessary in a black-box setting. Here, we also emphasize that the MSEs are computed on an equidistant grid evaluated on the KDEs. This distinction is important because, in principle, for an equidistant $\eta$, the distribution of the $\alpha$ values could theoretically concentrate on a few points. This behavior is especially observable whenever the quotient of the densities is constant for a non-negligible subset of $[0,1]$. An example of that would be the Laplace mechanism.
%To address this potential issue, we have evaluated the error on both the grid implied by the KDE and an equidistant grid. Through this comparison, we have observed that such concentration does not impact the estimation. If this phenomenon arises, a linear interpolation is sufficient, as the trade-off curve is also linear on that subset, and if it does not arise, then the $\alpha$'s are also evenly distributed. \todo{@Tim: do you agree with me? I think it is important to make this remark, as someone could be a bit confused when observing this clustered results for the Laplacian algorithm}
% \\

For Goal 2 (inference), we recall that a  $T^{(0)}$-DP violation is detected if the box $\square_\gamma$ (purple) lies completely below the postulated curve $T^{(0)}$ (blue). In Figure \ref{fig:not_faulty_sgd_gauss} we consider the case of no violation where $T=T^{(0)}$, and we expect not to detect a violation. This is indeed what happens, since $\square_\gamma$ intersects with the curve $T^{(0)}$ in all considered cases. Interestingly, we observe that $\square_\gamma$ has a center close to $\alpha=0$ in the cases where no violation occurs (such a behavior might give additional visual evidence to users that no violation occurs).
%In principal, one would reject the privacy curve, whenever the purple square $\textcolor{purple}{\square}$ is disjoint from the blue \textcolor{blue}{curve}, i.e.
%\begin{equation*}
%    \textcolor{purple}{\square} \cap \textcolor{blue}{\textnormal{curve}}=\emptyset~.
%\end{equation*}
%In Figure \ref{fig:not_faulty_sgd_gauss} and \ref{fig:not_faulty_sgd_gauss} we have displayed the case where the claimed privacy indeed holds, so we expect to not detect a violation. For both mechanism, we can observe that for any sample size, we correctly do not reject that claim, as the $\textcolor{purple}{\square}$ and the $\textcolor{blue}{curve}$ are not disjoint. 
In Figure \ref{fig:faulty_sgd_gauss}, we display the case of faulty claims, where the privacy breach is caused by a smaller variance for both mechanisms under investigation. In accordance with Theorem \ref{theo:auditor}, we expect a detection of a violation if $n_2$ is large enough. This is indeed what happens, at a sample size of $n_2=10^7$ for the Gaussian mechanism and at  $n_2=10^8$ for DP-SGD. As we can see, larger samples $n_2$ are needed to expose claims $T^{(0)}$ that are closer to the truth $T$ (as for DP-SGD in our example). For larger $n_2$ the square $\square_\gamma$ shrinks (see eq. \eqref{e:defsq}) leading to a higher resolution of the auditor. 
%While for both mechanisms, we do not detect a statistical significant deviation for $n_2=10^6$ (since the $\textcolor{purple}{\square}$ is not disjoint from the blue \textcolor{blue}{curve}), already for $n_2=10^7$, the auditor detects the privacy violation for the Gaussian mechanism. Regarding the DP-SGD case, we have to increase the sample size to $n_2=10^8$ to detect the violation. The clear message here is that smaller privacy violations (curves are closer to each other), the larger the sample size $n_2$ has to be, to obtain small confidence regions. This is a classical pattern in statistics and aligns with Theorem \ref{theo:auditor} part (2). The main reason here is that higher $n_2$ significantly shrinks the confidence square (recall Theorem \label{thm: accuracy stat of kNN BayBox estimator}
%), which eventually will be fully below the \textcolor{blue}{curve}, indicating a statistically significant difference. Here, we explicitly want to point out that even though e.g. Figure \ref{fig:faulty_sgd_gauss} (a) was not significant, one can already take the deviation from the claimed curve as a first indication and just increase the sample size for a stronger evidence. Consequently, we strongly encourage users to also consider the point estimator derived from the $k$-NN algorithm as a potential indicator of faulty implementations. In fact, our empirical evidence indicates that the confidence interval for our $k$-NN based point estimator is significantly narrower than the theoretical bound we derived. This discrepancy arises because the theoretical convergence rate of the $k$-NN algorithm is generally not tight; in practice, the performance of the $k$-NN algorithm converges more rapidly than the theoretical rate suggests. To put this into more practical observations, the difference of the estimators derived for $n_2=10^6,10^7,10^8$ were for all mechanism negligible.