\section{Introduction}

%\todo{DP and f-DP; positives of f-DP}


Differential privacy (DP) \cite{Dwork2006} is a widely-used framework to quantify and limit information leakage of a data-release {\em mechanism} $\Mech$ via privacy parameters $\varepsilon > 0$ and $\delta \in [0,1]$. Mechanisms that are differentially private for a suitable choice of $\varepsilon$ and $\delta$ mask the contribution of individuals to their output. As a consequence, DP has been adopted by companies and public institutions to ensure user privacy \cite{Erlingsson2014,Holohan2019,Abowd2018}.



%Formally, given any two {\em neighboring datasets} $\DB$, $\DB'$ that differ exactly in one person's data, and any set $S$, we say $\Mech$ is $(\epsilon, \delta)$-DP if: \todo{Note: Brought this back because otherwise we don't have DP definition anywhere; i also need it to describe why we consider a pair of neighbors only}
%\begin{equation*}    
%\Pr(\Mech(\DB) \in S) \leq e^\epsilon \Pr(\Mech(\DB') \in S) + \delta~.
%\end{equation*}
%The level of privacy and amount of leakage for a data-release {\em mechanism} $\Mech$ is described by privacy parameters $\varepsilon > 0$ and $\delta \in [0,1]$. 
%Mechanisms satisfying  suitable choice of $\varepsilon$ and $\delta$ will bound and reduce the information leakage of individuals whose data is processed.
%Several $(\epsilon, \delta)$-DP mechanisms have been proposed that allow data analysts to balance between the level of privacy (lack of information leakage), versus the accuracy/utility of the data analysis. As a consequence, DP has been adopted by companies and public institutions to ensure user privacy \cite{Erlingsson2014,Holohan2019,Abowd2018}.}


%Differential privacy (DP) is one of the most widely-accepted notions of privacy, used to describe the level of privacy for any data-release {\em mechanism} $\Mech(\cdot)$ that take as input a dataset (collection of individuals' data). Formally, given two {\em neighboring datasets} $\DB$, $\DB'$ that differ exactly in one person's data, and any set $S$, we say $\Mech$ is $(\epsilon, \delta)$-DP if: 
%\begin{equation*}
%    \Pr(\Mech(\DB) \in S) \leq e^\epsilon \Pr(\Mech(\DB') \in S) + \delta
%\end{equation*}
% which is defined as any function that takes as input a {\em dataset} (a collection of individuals' data). This level of privacy describes how much the output of $M$ leaks information about any individual (e.g., Alice) in the dataset.

Over the years,  variants and relaxations of DP have been proposed, to address specific needs and challenges. Of these, the recent notion of $f$-DP~\cite{Dong2022} is one of the most notable, due to its attractive properties such as a tight composition theorem, and applications such as providing an improved, simpler analysis of privatized stochastic gradient descent (Noisy or DP-SGD), the most prominent privacy-preserving algorithm in machine learning. $f$-DP is grounded on the hypothesis testing interpretation of DP \footnote{\noindent For a rigorous introduction to hypothesis testing and $f$-DP we refer to Section \ref{sec:prelim}.} and describes the privacy of mechanism $\Mech$ in terms of a real-valued function $f$ on the unit interval $[0,1]$. Several mechanisms~\cite{Dong2022} have been shown to achieve $f$-DP. However, the process of designing privacy-preserving mechanisms and turning them into real-world implementations is susceptible to errors that can lead to so-called `privacy violations' \cite{Lyu2017,mattjj2023fixprng,Mironov2012}. Worse, checking such claims may be difficult, as some implementations may only allow for limited, {\em black-box} access.  %This is exemplified in the case of DP mechanisms (whose privacy can also be represented by $f$-DP), when the achieved privacy parameter is worse than what is claimed due to e.g., a buggy implementation~\cite{lyu2017understanding}. 
This problem has motivated the proposal of methods that assess the privacy of a mechanism $M$ with only black-box access.

Within the plethora of works on privacy validation, most approaches study mechanisms through the lens of standard DP \cite{StatDP, DP-Finder, CheckDP, DP-Sniper, Dette2022, Liu2019, Lu2024, Chadha2023, Lokna2023, Kifer2017, Kifer2019, Tschantz2011, Barthe2012,Barthe2014, Barthe2015, Barthe2016, Barthe2016_B}. In contrast, comparatively few methods examine $f$-DP~\cite{Nasr2023,Annamalai2024, Annamalai2024_B, Annamalai2024_C, Mahloujifar2024,Koskela2024}. Moreover, most of the procedures that feature $f$-DP are tailored to audit the privacy claims of a specific algorithm, namely DP-SGD \cite{Nasr2023,Annamalai2024,Annamalai2024_B,Annamalai2024_C}. Our goal is to devise methods that are not specific to a single mechanism, but are instead applicable to a broad class of algorithms, while only requiring black-box access. We formulate our two objectives: \\[-3.5ex]%that our methods for investigating $f$-DP should tackle:
\begin{itemize}
    \item \textbf{Estimation:} Given black-box access to a mechanism $M$, estimate its true privacy parameter (i.e., the function $f$ in $f$-DP).\\[-3.5ex]
    \item \textbf{Auditing:} Given black-box access to a mechanism $M$ and a target privacy  $f$, check whether $M$ violates the targeted privacy level (i.e., given $f$, does $M$ satisfy $f$-DP?).\\[-3.5ex]
\end{itemize}
Estimation is useful  when we do not have an initial conjecture regarding $M$'s privacy. It can thus be used as, e.g.,  preliminary exploration into the privacy of $M$. Auditing, on the other hand, can check whether an algorithm meets a specific target privacy  $f$ and is therefore designed to detect flaws or overly optimistic privacy guarantees.





%\todo{Intuition of f-DP.} 
%\textcolor{orange}{\textbf{YUN: I have shortened the discussion - we should move the details of f-DP to the Prelims as you suggested.}}
%$f$-DP describes the privacy of mechanism $\Mech$ through the hypothesis testing interpretation of DP.%, which is used across all empirical sciences% which are used across all empirical sciences to extract information from random samples
%\footnote{For a rigorous introduction to hypothesis testing and $f$-DP we refer to Section \ref{sec:prelim}.}. 
%The `$f$' in $f$-DP is a tradeoff curve/function between the Type I error $\alpha$ and Type II error $\beta = f(\alpha)$ of hypothesis tests trying to distinguish whether some individual (e.g., Alice) is in the database or not. Intuitively, a larger $f$ curve implies higher privacy (i.e., it is {\em harder} to distinguish an individual's data), and we say that $M$ is $f$-DP if its privacy is at least as good as (i.e., incurs hypothesis testing error of at least) $f$. The {\em optimal} tradeoff curve for $M$ is the largest curve $f$ such that $M$ is $f$-DP.
%Essentially, a hypothesis test is a function $S$\todo{Maybe change? In Gaussian DP paper $S$ is the database I think (also, in DP papers usually $S$ is a set of mech outputs} that takes a sample as an input and gives a binary output. The outputs are interpreted as follows. \todo{I'd prefer directly introduce the two hypotheses using the example of Alice and the neighboring databases $x$, $x'$ (e.g., similar to my previous version) instead of introducing hypothesis testing generally---since we'll introduce the general version in the prelim} \begin{itemize}
 %   \item[] $0$: The data suggests no departure from a hypothesized baseline state (default).
%    \item[] $1$: The data is strongly incompatible with the hypothesized baseline state.
%\end{itemize} 
%If the output is $1$, the user has detected something "significant" - in the context of privacy usually some private information about membership in a database\todo{<-- regarding this statement: the user/attacker can detect something significant outputting 0 as well. The detection comes from whether their output accurately matches the neighboring database used.}. Let us make this more precise: Consider two {\em neighboring} datasets $x$, $x'$, i.e. datasets which differ in exactly one record/one individual's data. Say, Alice is a member of $x$ but not of $x'$. 
%A mechanism $M$ runs on one of these datasets $y \in \{x,x'\}$ and its output is made public. $f$-DP now states that the algorithm $M$ grants high privacy, if there is no hypothesis test $S(M(y))$ that can reliably distinguish $y=x'$ (the default hypothesis) from $y=x$ (the alternative hypothesis). In the context of testing, the default hypothesis is typically called "null-hypothesis" and denoted by $H_0$, while the alternative hypothesis is denoted by $H_1$.
%- nothing is detected that indicates a violation of a hypothesized base state. and $1$ if there is strong evidence that . A rigorous description of hypothesis tests can be found in section 
%As with DP-inspired definitions, $f$-DP is a property of a statistical analysis function/algorithm $T(\cdot)$ \todo{TODO: Do we want to use $T$ or $M$ for the mechanism? I think $M$ is used by the f-DP paper and $T$ is something else} which takes as input a dataset (a collection of records). We consider {\em neighboring} datasets $x$, $x'$, which differ in exactly one record/one individual's data (say, Alice's). These two neighboring datasets represents two possibilities: {\em is Alice's data the one in $x$, or the one in $x'$?}. To measure the information leaked by analysis $T(\cdot)$, $f$-DP answers the question: Given just one output of of $T$ on either $x$ or $x'$, can an attacker distinguish between two hypotheses:
%\begin{equation*}
 %   H_0: T \text{ was evaluated on } x; \quad vs. \quad H_1: T \text{ was evaluated on } x'~?
%\end{equation*}

%Several mechanisms~\cite{Dong2022} have been shown to achieve $f$-DP. However, \textcolor{blue}{the process of designing DP mechanism and} turning \textcolor{blue}{them} into real-world implementations \textcolor{blue}{is susceptible to errors} and can at times lead to so-called `privacy violations' \cite{Lyu2017,mattjj2023fixprng,stevens2022backpropagation}. Worse, checking such claims may be difficult, as some implementations may only allow for limited, {\em black-box} access.  %This is exemplified in the case of DP mechanisms (whose privacy can also be represented by $f$-DP), when the achieved privacy parameter is worse than what is claimed due to e.g., a buggy implementation~\cite{lyu2017understanding}. 
%This has spurred on the proposal of (often black-box) {\color{blue} methods that investigate (i.e., either {\em estimate} or {\em audit}) the privacy of a given (implementation of a) mechanism $M$. %In parallel to the growing literature and research on privacy-preserving mechanisms, a number of (black-box) estimating and auditing methods have been put forward to evaluate the privacy of various algorithms and their implementations. 
%The goals of a (black-box) privacy estimator/auditor are as follows:
%\begin{itemize}
%    \item \textbf{Estimator:} Given (black-box access to) a mechanism $M$ estimate its true privacy parameter (e.g., the $f$ in $f$-DP).
%    \item \textbf{Auditor:} Given (black-box access to) a mechanism $M$ and a target privacy parameter, verify whether $M$ violates the target privacy (e.g., given $f$, does $M$ satisfy $f$-DP?).
%\end{itemize}


%Within the plethora of works in this area, most specifically study mechanisms within the standard DP definition \cite{StatDP, DP-Finder, CheckDP, DP-Sniper, Dette2022, Liu2019, Lu2024, Chadha2023, Lokna2023, Kifer2017, Kifer2019, Tschantz2011, Barthe2012,Barthe2014, Barthe2015, Barthe2016, Barthe2016_B}. In contrast, comparatively few  examine $f$-DP~\cite{Nasr2023,Annamalai2024, Mahloujifar2024,Koskela2024} both in the black-box and white-box setting. Yet, these $f$-DP works either audits standard DP  \cite{Koskela2024} or are specific to DP-SGD \cite{Mahloujifar2024, Nasr2023,Annamalai2024}. %In this work, we introduce novel $f$-DP estimation and auditing  methods that are more generally applicable to various algorithms.  

%black-boxprivacy {\em estimators} and {\em auditors}% (e.g., ~\cite{} and more)\todo{Cite black box DP and f-DP estimators we discuss as related work}. We present the goals of these constructions informally, since previous works on these constructions each has their own formal statement on their accuracy. (Looking ahead, in order to compare our constructions with previous works, we will employ empirical experiments.)
%, a wide range of works have proposed methods that, with black-box access to an algorithm, either empirically {\em estimate} its (optimal) privacy parameters~\cite{}, or more commonly, given a target set of privacy parameters, {\em audit} the algorithm to confirm if it achieves these target parameters~\cite{}. 
%\todo{TODO Please check  paragraph for correctness. This paragraph discusses (1) the technique of using specific membership inference etc. attacks to audit f-DP (the unpublished work), and (2) why we cannot directly use the connection between f-DP and DP to estimate f-DP using a DP-estimator}
%\todo{TODO: after related works is settled, put more related works here}
 %  We briefly discuss below how our work's goals and/or techniques differ from related works. %whose techniques differ from ours and why we have chosen to pursue our own techniques for $f$-DP estimation/auditing. 
%   We leave a more comprehensive description of our techniques in Sec.~\ref{sec:overview_techniques} and of related works in Sec.~\ref{sec:relatedwork}. 
%The  work that most relates to our paper's goals~\cite{}\todo{Note, this is the unpublished `Auditing f-DP in One Run' paper}\footnote{We note~\cite{} is currently under review and has not yet been published.} with a similar goal to ours (auditing $f$-DP) makes use of a technique in classical DP auditing---that is, using specific (e.g., membership inference) attacks to test the audited mechanism. However, this method implies that their theoretical guarantees are one-sided (Def. 6,~\cite{})---the auditor accepts truly $f$-DP mechanisms with provably high probability, but only rejects non-$f$-DP mechanisms with {\em empirically/experimentally-demonstrated} accuracy. 
%Another candidate technique for $f$-DP we did not use, is the well-known connection~\cite{} between  hypothesis-testing and DP (with this connection predating $f$-DP itself). 
%We know that hypothesis testing has been used as a method to audit {\em standard  DP} parameters~\cite{}. Thus, one may wonder if we can directly estimate $f$-DP by using an existing DP estimator, e.g.,~\cite{}. However, this known connection (e.g., Thm....,~\cite{}) is  between the set of all $(\epsilon, \delta)$-(standard) DP parameter pairs, and the $f$-DP curve of the mechanism. Thus, estimating $f$ appears to require {\em at minimum} an estimator for {\em all} optimal $(\epsilon, \delta)$-DP privacy parameter pairs, whereas  as far as we know, we only have a point-wise DP estimator. 
%To summarize, there remains the following gap in the literature, which formulates the following research question: 
%\begin{quote}
 %   {\em Research Question: Can we, with tight accuracy guarantees and practical efficiency, estimate and audit the $f$-DP of any black-box mechanism?}
%\end{quote}
\subsubsection*{Contributions}


We construct a `general-purpose' $f$-DP {\em estimator} and {\em auditor} for both objectives, where:\\[-3.5ex]
%With this work, we begin to fill the gap in the literature by constructing the first  `general-purpose' $f$-DP {\em estimator} and {\em auditor} of any mechanism, given only black-box access, where: %Specifically, we develop the first general-purpose black-box \(f\)-DP estimator and auditor, where: %\textcolor{orange}{
\begin{itemize}
        \item[(1)] The estimator approximates the entire true \(f\)-DP curve of a given mechanism $M$. \\[-3.5ex]
        \item[(2)] Given a target \(f\)-DP curve, the auditor statistically detects whether $M$ violates $f$-DP.  The auditor involves a tuneable confidence parameter to control the false detection rate. \\[-3.5ex]
\end{itemize}
%Experimentally, our black-box estimator achieves high accuracy for both estimation and auditing. \todo{TODO @Yun: Make more high-level and merge into Contributions}
A methodological advantage of our methods is that they come with strong mathematical performance guarantees (both, for the estimator and the auditor). Such guarantees seem warranted when making claims about the performance and correctness of a mechanism. A practical advantage of our methods is their efficiency: Our experiments (Sec.~\ref{sec6}) demonstrate high accuracy at typical runtimes of 1-2 minutes on a standard personal device.
%We show that our estimator converges uniformly to the targeted privacy parameter $f$ and prove that our auditor can reliably detect privacy violations with some\todo{tuneable?} statistical certainty. 
%An important feature of our methods is their efficiency: our experiments (Sec.~\ref{sec6}) demonstrate both short runtimes (<1 minute for estimation, and <2 minutes for auditing with good confidence region),  and can provide reliable estimates, even for comparatively small sample sizes.

%Our experiments (Sec. \ref{sec6}) demonstrate the effectiveness of our approach. Defying the challenges of the full black-box setting, our methods deliver precise estimators 
%using KDEs, 
%even for small sample sizes, within mere seconds (e.g., Figure \ref{fig:estimation_mse} and Table \ref{tab:running_times_KDE}). %We note our estimator, after one run, can estimate {\em the entire $f$-DP curve}, rather than requiring lengthy computation for each point on the curve.
%%This marks a significant advantage over the state-of-the-art DP estimator~\cite{Lu2024} requiring new computation for each estimated $(\epsilon, \delta)$ point.
%This marks a significant improvement over existing estimation methods for other privacy definition, which fail to recover all potential pairs, such as in $(\varepsilon, \delta)$-DP. 
%Our estimates are supplemented by an effective auditor (e.g., Figure \ref{fig:faulty_gauss} and Table \ref{tab:running_times_kNN}) that can accurately detect privacy violations, while controlling the risk of false detections. 
%auditor also demonstrates efficiency in addition to accuracy in detecting privacy violations (see Table \ref{tab:running_times_kNN}). 
%To further validate our results, we employ the $k$-NN algorithm to reliably 
%Our auditor's computational complexity is also low. 
%Intuitively, the auditor first pinpoints the maximum vulnerability of a mechanism using density estimation. Second, it leverages a reformulation of $f$-DP (derived in this paper) as a classification task to decide with a classifier whether an actual violation has occurred or not.
%it does so by first using a density estimator to pinpoint the most vulnerable point on the trade-off curve, then audit only this point, via a pointwise binary classifier-based estimator.
%runs the binary classifier $k$-NN on a fixed point in the trade-off curve.
%By combining these two steps, we get the best of both worlds---efficiency, as well as the tuneable confidence region associated with optimal classifiers. \textcolor{orange}{\textbf{All:} Do we need the last sentence here?}

%By combining KDE and $k$-NN, we equip ourselves with the best of both approaches: an efficiently computed estimator with finite sample guarantees for constructing confidence regions.

$ $\\[-1.5ex]
\noindent \textbf{Paper Organization} 
Preliminaries are introduced in Sec.~\ref{sec:prelim}. In Sec.~\ref{sec:overview_techniques} we give an overview of techniques. We propose our $f$-DP curve estimator in Sec.~\ref{sec:4} and auditor in Sec.~\ref{sec:goal2}. We evaluate the effectiveness of both estimator and auditor %validate and benchmark our constructions%
in Sec.~\ref{sec6} using various mechanisms from the DP literature, including DP-SGD. We delve into more detail on related work in Sec.~\ref{sec:relatedwork} and conclude in Sec.~\ref{sec:summary_discussion}. A table of notations, proofs and technical details can be found in the Appendix.

%\todo{Everyone: We say that an $f$-curve is "a trade-off curve" while the one of the optimal test is "the optimal trade-off curve". Please modify accordingly.}


%We note the distinction of contribution (1) and (2) (estimation vs. auditing). At first glance it may appear that an $f$-DP estimator is already itself an auditor (informally, we can estimate the $f$-DP of an mechanism, then check whether it violates the given curve). However, we note that while our estimate curve converges to the real $f$-DP curve of the algorithm, we do not know the rate of convergence and thus cannot directly use it to give a confidence bound on the audit. We emphasize however that our construction is nevertheless the first estimator for $f$-DP, and finding an estimator with known convergence rate for the whole curve is a worthwhile open problem.





%\todo{TODO: THIS PART IS MERGED with the prelim f-DP section 3.3}
%The goal of a hypothesis test is to, given two `hypotheses' denoted $H_0$ and $H_1$, and a sample, determine whether $H_0$ or $H_1$ is true. In the case of $f$-DP: Consider two {\em neighboring} datasets $\DB$ and $\DB'$. Say, Alice's data is in $D$ but not in $\DB'$, but otherwise these two datasets are the same. Now, we pick one of $\DB$ and $\DB'$, run $M$ on the chosen dataset once and obtain an output $X$. The observation $X$ serves as  sample for the hypothesis tester. The two hypotheses are:
%\begin{equation*}
%    H_0: X \sim \Mech(\DB), \,\,\text{ and } \,\, H_1: X \sim \Mech(\DB').
%\end{equation*}
%$f$-DP then says that, $M$ provides strong privacy if even the best hypothesis tester cannot reliably distinguish $H_0$ and $H_1$ --- no one can tell if Alice is in the dataset or not \footnote{Of course, $f$-DP also requires this hardness to distinguish to hold for any individual that can occur in any dataset, not just Alice.}! 
%If it is hard to distinguish $H_0$ and $H_1$, then it is hard for an attacker to compromise Alice's data. This `hardness to distinguish' is captured by a pair of errors: false positive rate (FPR)/Type I error $\alpha$ and false negative rate (FNR)/Type II error $\beta$. Each (hypothesis) test performed by the attacker to distinguish $H_0, H_1$ is associated with  an error  pair $(\alpha \in [0,1], \beta \in [0,1])$. For example, it is always possible to have $\alpha = 0$ for some $\beta \leq 1$, by having the hypothesis test always output $H_0$ (by never rejecting $H_0$, we never have a false positive); similarly it is always possible to have a test where $\beta = 0$. Of course, typically the attacker strives for a trade-off between the two errors. In the worst case scenario, the attacker employs an optimal test $S^*$, which is associated with a pair $(\alpha, \beta^*(\alpha))$ where no pair of errors $(\alpha, \beta')$  is attainable with $\beta'<\beta^*(\alpha)$.
%The set of errors incurred by optimal tests distinguishing $H_0$ and $H_1$ forms a so-called {\em trade-off curve} and we denote it by the function $\alpha \mapsto T(\alpha)$. Now, consider a curve $f:[0,1]\to [0,1]$. We say that an analysis $M$ is $f$-DP if its trade-off curve $T$ is {\em above} the function $f$ 
%\[
%T(\alpha)\ge f(\alpha), \qquad \forall \alpha \in [0,1].
%\]
%In other words, the errors in distinguishing   $H_0$ and $H_1$ is {\em at least} $f$. Intuitively, this means that an attacker must ``make at least error $f$" when trying to break Alice's privacy.