\section{Old Introduction}
\begin{itemize}
    \item Introduce/motivate DP and other variants, in particular $f$-DP.

    
    \item Introduce/motivate DP estimation
    \item Motivation for estimating/detecting violation of $f$-DP
    \item \textbf{Contributions:} \begin{itemize}
        \item Definitions of what it means to estimate an f-DP curve and to detect violation of an $f$-DP curve 
        \item Two distinct goals: (1) estimate the f-DP curve (the estimate converges to the real curve but we don't know the rate) (we are the first to do so) (2) given a curve and an algorithm, answer whether `I'm sure the algorithm violates this curve' or `I'm not sure whether it violates the curve'. Emphasize how these goals are distinct: In particular, why an answer to goal (1) does not automatically give us an answer to goal (2). In (1) we don't get a confidence bound (at least not for the whole curve). Thus (2) is not automatically satisfied.
    \end{itemize}
    \item \textbf{Overview of Techniques:} KDE curve; relationship between $\eta$ and $\alpha$ and $\beta$ using similar technique to Eureka DP estimator
\end{itemize}
\textbf{Differential Privacy}
\textbf{f-Differential Privacy}
Story: \begin{itemize}
    \item usually comparing distributions (classical dp, divergence based dp)
    \item what does specific values of e.g. $\varepsilon$ imply for individuals.
    \item hypothesis testing (first by wassermann 2010, now gaussian dp)
    \item making privacy work in practice: 1) tight analysis
    2) privacy amplifications
    \item f-DP allows for tight analysis of SGD, the most prominent algorithm (including privacy amplification of subsampling)
\end{itemize}
In recent years, $f$-Differential Privacy (f-DP) marked a key advancement in data privacy, offering a versatile tool to quantify the privacy loss of individuals. {\color{red} Yun: I would maybe add here some words on why f-DP is useful over just describing privacy loss with $(\epsilon,\delta)$-DP}The conceptional idea boils down to classical statistical notions: As in all privacy notions in DP, we are considering neighboring data sets $x,x'$, and wish that the statistical analysis performed on $x,x'$, say $T(x)$, only leaks limited information about any distinct individual. In \todo{wassermann2010, f-dp} they formulate a hypotheses testing approach: Whenever information is leaked about the distinct individual in $x, x'$ based on $T(x)$, then the attacker has the ability to distinguish between
\begin{equation*}
    H_0: T \text{ was evaluated on } x; \quad vs. \quad H_1: T \text{ was evaluated on } x'~,
\end{equation*}
where the attack is based on only one output $T(\cdot)$. Loosely speaking, if that task is difficult, then the distinct individual can participate in $T$ without compromising its privacy.  In classical DP notions, this is usually captured in one or multiple privacy parameters, say $\varepsilon$, which usually upper bounds a privacy loss. Later, this $\varepsilon$ can be used to assess how difficulty of distinguishing $H_0$ and $H_1$. To measure the difficulty, in classical statistical terminology, one considers two errors: Firstly $\alpha$, the probability of falsely rejecting $H_0$  (type-1 error) and secondly $\beta$, the probability of mistakenly failing to reject $H_0$. Given an $\varepsilon$ induced by the privacy loss, \todo{wassermann2010} allowed to derive a pair $(\alpha,\beta(\alpha, \varepsilon))$ to lower bound the $\beta$ given any pair of $\varepsilon$ and $\alpha$. In that spirit \todo{cite gdp} extended the work of \todo{wassermann2010}, by 


\textbf{Estimation of Differential Privacy}
Story:
\begin{itemize}
    \item DP is the standard notion to guarantee some type of privacy in statistical analysis.
    \item has a wide range of applications and therefore mistakes can happened. refer to examples in the introduction
    \item while we have formal guarantees: faulty constructions or implementations can yield privacy breaches.
    \begin{itemize}
        \item https://ar5iv.org/html/1709.02753 - Apple's DP implementation claimed a particular DP parameter but because multiple data points are shared per day, the real privacy loss is greater than claimed.
    \end{itemize}
    \item detect these breaches through consistent estimation of privacy parameter ($(\varepsilon,\delta)$, $(\lambda,\varepsilon)-$RDP, $f-$DP,...)
    \begin{itemize}
        \item Motivation for f-DP (or DP-variants) estimation: \item Renyi DP - DP auditorium, Lower Bounds for Renyi Differential Privacy in a Black-Box Setting
        \item f-DP -(Auditing $f$-Differential Privacy in One Run; submitted to ICLR 2025) - This paper is not accepted yet(?) but seems relevant enough we should use some space to compare with them
    \end{itemize}
    \item {\color{red}If there are so many DP estimators and there is a link from DP to f-DP, what are we doing? -  *Ans*: The minimum requirement to even use the link would be to have a estimator for all $\epsilon$, $\delta$. Because so far we only know point-wise estimate of eps, delta, it is hard to use the equivalence of f-DP and eps/delta from the Gaussian DP paper. 
    }
    \item already sneak peak for the second goal?!
\end{itemize}