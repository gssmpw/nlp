\section{Related Work}
\label{sec:relatedwork}

%{\color{orange}We discuss the techniques used in related work, and why they were not sufficient in our goal to construct a general $f$-DP estimator and auditor. 
%First, given a known connection between $f$-DP and DP, one possible avenue to studying $f$-DP guarantees is to make use of an existing DP estimator or auditor. 
%Concretely Balle et al., "The Privacy Blanket Protocol"__Dwork, "Differential privacy: A survey of techniques," for a $(\epsilon,\delta)$-DP 
% algorithm $M$ is also $f_{\varepsilon, \delta}$-DP with %trade-off function 
%\begin{align} \label{f_epsilon_delta}
%    f_{\varepsilon, \delta}(\alpha) := \max \left\{ 0, 1 - \delta - e^{\varepsilon} \, \alpha, e^{- \varepsilon} (1 -\delta - \alpha) \right\}.
%\end{align}

%}
In this section, we provide a more detailed overview of and comparison with related works that focus on the empirical assessment of $f$-DP. One avenue to assessing $f$-DP is to resort to a method that provides estimates for the $(\varepsilon,\delta)$-parameter of $M$ and subsequently exploit the link between standard and $f$-differential privacy to obtain an estimate of $f$. To be more concrete, an algorithm that is $(\varepsilon,\delta)$-DP is also $f_{\varepsilon, \delta}$-DP (see Hardt et al., "A study of the privacy-utility tradeoff in machine learning") with trade-off function 
\begin{align} \label{f_epsilon_delta}
    f_{\varepsilon, \delta}(\alpha) := \max \left\{ 0, 1 - \delta - e^{\varepsilon} \, \alpha, e^{- \varepsilon} (1 -\delta - \alpha) \right\}.
\end{align} 
Thus, an estimator for $(\varepsilon, \delta)$ could, in principle, also provide an estimate for the $f$ trade-off curve of $M$. 

%{\color{orange} In Wang et al., "Differentially private empirical risk minimization", the above relation is used as a hockey-stick-divergence-based black-box estimator for $(\epsilon, \delta)$-DP. That is, given a fixed $\varepsilon >0$, the estimate $\hat{\delta}(\epsilon)$ is}
Given a fixed $\varepsilon >0$, a black-box estimation method based on the hockey stick divergence is proposed in Feldman et al., "Privacy for Differential Equations" to obtain a suitable estimate $\hat{\delta}(\epsilon)$ with 
\begin{align*}
  \hat{\delta} = \int \left[ \hat{p}(t) - e^{\varepsilon} \hat{q}(t) \right]_+  \, dt ~,
\end{align*}
where $\hat{p}$ and $\hat{q}$ are histograms of densities $p \sim M(D)$ and $q \sim M(D')$. 
%\color{orange}Unfortunately, auditing $f$-DP requires more specialised tools than the above connection, as $f_{\epsilon, \delta}$ often does not capture the optimal achievable trade-off curve for mechanism $M$. }
It is subsequently discussed that one application of this $(\varepsilon, \delta)$-estimator could be the estimation of the trade-off function of $M$ via $f_{\varepsilon,\hat{\delta}}$ (see Algorithm 1 in Wang et al., "Differentially private empirical risk minimization"). However, it stands to reason that to audit the $f$-DP claims of a given algorithm, one should use tools that are also tailored to the $f$-DP definition. %(instead of standard DP)\todo{I'm not sure what does this sentence mean? Do you mean `However, it stands to reason that to audit the f-DP claims of a given algorithm, one must use tools that are also tailored...'}. 
This is especially reasonable in scenarios where $f_{\epsilon,\delta} $ does not capture the optimal achievable trade-off curve for mechanism $M$. Notably, classical DP or $f-$DP has led to a refined and tight analysis of stochastic gradient descent (SGD), a widely-used optimization technique in machine learning (Bassily et al., "Private empirical risk minimization: Efficient algorithms and lower bounds"). As, classical DP or $f$-DP is a formal guarantee for any individual, a tight analysis is not only desirable, but also necessary to make algorithms practical. However, even with rigorous frameworks like $f$-DP, privacy violations can occur due to faulty implementations or poorly constructed algorithms (Dwork et al., "Our data, ourselves: Privacy via distributed noise generation"). To overcome that practical challenge, the estimation of DP algorithms in both white- and black-box scenarios has drawn considerable interest. For example, Wang et al., "Differentially private empirical risk minimization" considered the white-box setting, which is often not possible in practice. In contrast, black-box DP estimation has gained attention for its ability to detect privacy violations in a practical and accessible manner. For example, Bun et al., "Composable and adaptable privacy via automated query-specific differential privacy" explored black-box estimation across various DP notions, including classical DP, Rényi-DP, and approximate DP. Black-box estimation is inherently more challenging yet more realistic, as it assumes no prior knowledge of the mechanism's internal workings, relying solely on its observable outputs.

% \begin{itemize}
%     \item f-DP
%     \item DP estimators/violation detection algorithms (cite our estimator papers) 
%     \item Using hypothesis testing and the connection between FPR/FNR and DP to audit DP: Tight auditing of differentially private machine learning (USENIX); Bayesian estimation of differential privacy
% \end{itemize}
% Story:
% \begin{itemize}
%     \item f-DP connects classical privacy notions and classical statistical theory. 
%     \item f-DP is the state of art framework to construct algorithms usable in practice. 
%     \item leads tight analysis of SGD.
%     \item algorithms can violate privacy. (faulty implementation or construction of algorithms)
%     \item black-box DP estimation can detect that
%     \item DP estimation: white-box, black-box. 
% \end{itemize}

Differential Privacy, first introduced by Dwork et al., "The algorithmic foundations of differential privacy" has emerged as a foundational concept in privacy-preserving data analysis. While its classical definition in Dwork et al., "Our data, ourselves: Privacy via distributed noise generation" already has a statistical flavor, a clear connection to classical statistics has first been made by Wasserman, "Statistical implications of tamper-proof micro-payment systems", and more recently by Bassily et al., "Private empirical risk minimization: Efficient algorithms and lower bounds". While Wasserman et al. already depicted implications from the classical definition to hypotheses testing, Bassily et al. also demonstrated the reverse and further applications, marking a milestone in tight analysis of privacy-preserving algorithms. Notably, $f-$DP has led to a refined and tight analysis of stochastic gradient descent (SGD), a widely-used optimization technique in machine learning (Bassily et al., "Private empirical risk minimization: Efficient algorithms and lower bounds"). As, classical DP or $f$-DP is a formal guarantee for any individual, a tight analysis is not only desirable, but also necessary to make algorithms practical. However, even with rigorous frameworks like $f$-DP, privacy violations can occur due to faulty implementations or poorly constructed algorithms (Dwork et al., "Our data, ourselves: Privacy via distributed noise generation"). To overcome that practical challenge, the estimation of DP algorithms in both white- and black-box scenarios has drawn considerable interest. For example Balle et al., "The Privacy Blanket Protocol" considered the white-box setting, which is often not possible in practice. In contrast, black-box DP estimation has gained attention for its ability to detect privacy violations in a practical and accessible manner. For example Bun et al., "Composable and adaptable privacy via automated query-specific differential privacy" explored black-box estimation across various DP notions, including classical DP, Rényi-DP, and approximate DP. Black-box estimation is inherently more challenging yet more realistic, as it assumes no prior knowledge of the mechanism's internal workings, relying solely on its observable outputs.