\subsection{Proofs for Goal 2 (Auditing)}
Before we proceed to the proofs, we state a simple but useful consequence of the Neyman-Pearson Lemma.
\begin{cor}
   \label{corollary: NP lemma}
    Let set $\cS_{\eta} = \{x: p(x)/q(x) \leq \eta \}$.
    For $\alpha \in [0,1]$, if there exists $\eta$ such that $\prdis{\rX \sim P}{\rX \in \cS_{\eta}} = \alpha$, then it holds that
    \begin{align*}
    \beta(\alpha) = 1 - \prdis{\rX \sim Q}{\rX \in \cS_{\eta}}.
    \end{align*}
% where $\cS_{\eta}$ is chosen such that $\prdis{\rX \sim P}{\rX \in \cS_{\eta}} = \alpha$.
\end{cor}

\begin{proof}[\textbf{Proof of Lemma~\ref{lemma: accuracy stat of general BayBox estimator}}]
    \label{proof: for lemma: accuracy stat of general BayBox estimator}
    We prove the statement that $\abs{\tilde{\alpha}(\eta) - \alpha(\eta)} \leq \sqrt{\frac{1}{2n}\ln{\frac{2}{\gamma}}}$ if $\eta \geq 1$. The proof of the second statement follows a similar approach. We begin with a few definitions.
    Let the observation set be defined as  
    \begin{align*}
        \cO := \Supp{P} \cup \Supp{Q} \cup \{\bot\},
    \end{align*}
    i.e. the range of observation. Define the indicator function $\mathbb{I}_{\cS_{\eta}} : \cO \mapsto \bits$, which takes as input an observation $x$ from the observation set $\cO$, outputting 1 if $x \in \cS_\eta$ and $0$ otherwise. Also, recall the definition of the set $\cS_{\eta} = \{x: p(x)/q(x) \leq \eta \}$ 
    % \todo{is it maybe $<$ instead of $\leq$? maybe doesn't matter} 
    as the set of all observation $x \in \cO$ where $p(x)$ is less than or equal to $\eta q(x)$ (as before $p, q$ are the densities of distributions $P, Q$).
    
    We first show that $\mathbb{I}_{\cS_{\eta}}$ is exactly the Bayes classifier $\phi^{*}$ for the Bayesian binary classification problem $\bbcP{\MixtureD{P}{\eta}}{Q}$. We prove this by showing for every $x \in \cO$, $\phi^{*}(x) = \mathbb{I}_{\cS_{\eta}}(x)$. Therefore, consider the tuple of random variable $(\rX, \rY) \sim \bbcP{\MixtureD{P}{\eta}}{Q}$. Then, for every observation $x \in \cO \setminus \{\bot\}$, we have  
    \begin{align*}
       \phi^{*}(x) ={}& \argmax_{\bits} \{\pr{\rY = 0 | \rX = x}, \pr{\rY = 1 | \rX = x}\} \tag{by Bayes classifier $\phi^{*}$'s construction}\\
        ={}& \argmax_{\bits} \{\pr{\rY = 0, \rX = x}, \pr{\rY = 1, \rX = x}\} \tag{by Bayes Theorem}\\
        ={}& \argmax_{\bits} \{\frac{1}{\eta}p(x), q(x)\} \\
        ={}& \mathbb{I}_{\cS_{\eta}}(x) \tag{by $\mathbb{I}_{\cS_{\eta}}$'s definition}.
    \end{align*}
    For an observation $x = \bot$, it is easy to check $\phi^{*}(x) = \mathbb{I}_{\cS_{\eta}}(x) = 0,$ as $q(x) = 0.$

    Then, we also observe that  
    \begin{align*}
        \alpha(\eta) ={}& \prdis{\rX \sim P}{\rX \in \cS_{\eta}} \tag{By Corollary~\ref{corollary: NP lemma}}\\
        ={}& \prdis{\rX \sim P}{\mathbb{I}_{\cS_{\eta}}(\rX) = 1} \\
        ={}& \prdis{\rX \sim P}{\phi^*(\rX) = 1} \tag{$\phi^* = \mathbb{I}_{\cS_{\eta}}$}\\
        ={}& \Exf{\rX \sim P}{\phi^*(\rX)}
    \end{align*}

    Recall that in algorithm~\ref{alg: general BayBox estimator}, BayBox estimatior $\bbe{\phi^*}$ computes the empirical mean of $\phi^*(\rX)$, i.e., $\tilde{\alpha}(\eta)$, as the estimate of $\alpha(\eta)$. By Hoeffding's Inequality, we finally conclude that
    \begin{align*}
           &\pr{\abs{\tilde{\alpha}(\eta) - \alpha(\eta)} > \sqrt{\frac{1}{2n}\ln{\frac{2}{\gamma}}}} \\
        ={}&\pr{\abs{\frac{1}{n}\rSum{i}{1}{n}\rZ_i - \Ex{\frac{1}{n}\rSum{i}{1}{n}\rZ_i}} > \sqrt{\frac{1}{2n}\ln{\frac{2}{\gamma}}}} \tag{$\rZ_i \defin \phi^*(\rX_i), \rX_i \overset{\text{i.i.d.}}{\sim} P$}\\
        \leq{}& \gamma.
    \end{align*}
\end{proof}

\begin{proof}[\textbf{Proof of Theorem~\ref{thm: accuracy stat of kNN BayBox estimator}}]
    \label{proof: for thm: accuracy stat of kNN BayBox estimator}
    We prove the first statement $\abs{\tilde{\alpha}(\eta) - \alpha(\eta)} \leq \sqrt{\frac{1}{2n}\ln{\frac{4}{\gamma}}} + \sqrt{\frac{144c_d^2}{n}\ln{\frac{4}{\gamma}}}$, and the second statement follows by a similar approach. 
    
    With probability at least $1 - \gamma$, we have 
    \begin{align*}
           & \abs{\tilde{\alpha}(\eta) - \alpha(\eta)}\\
        ={}& \abs{\frac{1}{n}\rSum{i}{1}{n}\kNNclassifier{n}(\rX_i) - \Ex{\frac{1}{n}\rSum{i}{1}{n}\phi^*(\rX_i)}} \tag{$\rX_i \overset{\text{i.i.d.}}{\sim} P$}\\
        ={}& \abs{\frac{1}{n}\rSum{i}{1}{n}\kNNclassifier{n}(\rX_i) - \Ex{\phi^*(\rX)}} \tag{$\rX \sim P$}\\
        \leq{}& \abs{\frac{1}{n}\rSum{i}{1}{n}\kNNclassifier{n}(\rX_i) - \Ex{\kNNclassifier{n}(\rX)}} + \abs{\Ex{\kNNclassifier{n}(\rX)} - \Ex{\phi^*(\rX)}} \\
        \leq{}& \sqrt{\frac{1}{2n}\ln{\frac{4}{\gamma}}} + \abs{\Ex{\kNNclassifier{n}(\rX)} - \Ex{\phi^*(\rX)}} \tag{by Hoeffding's Inequality}\\
        ={}& \sqrt{\frac{1}{2n}\ln{\frac{4}{\gamma}}} + \abs{ \pr{\kNNclassifier{n}(\rX) = 1} - \pr{\phi^*(\rX) = 1} }\\
        ={}& \sqrt{\frac{1}{2n}\ln{\frac{4}{\gamma}}} + \abs{ \pr{\kNNclassifier{n}(\rX) \neq 0} - \pr{\phi^*(\rX) \neq 0} }\\
        \leq{}& \sqrt{\frac{1}{2n}\ln{\frac{4}{\gamma}}} + 2|R(\kNNclassifier{n}) - R(\phi^{*})|\\
        \leq{}& \sqrt{\frac{1}{2n}\ln{\frac{4}{\gamma}}} + 12\sqrt{\frac{2c_d^2}{n}\ln{\frac{4}{\gamma}}}\tag{by Theorem~\ref{thm:covergence of kNN}}.
    \end{align*}

    We note that the first equality follows the idea in the proof of Lemma~\ref{lemma: accuracy stat of general BayBox estimator}, by just replacing the Bayes classifier with the concrete $k$-NN classifier.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem~\ref{theo:auditor}}] Recall the notation of Section \ref{sec:hyp}. In this proof, we will additionally assume that $T^{(0)}$ is strictly decaying, to make the presentation of our arguments slightly more easy to understand.\\
Now, consider  $\hat \eta^* \ge 0$ and the corresponding pair $(\alpha(\hat \eta^* ), \beta(\hat \eta^*))$ on the optimal trade-off curve.\footnote{Formally, we condition on $\hat \eta^* $, which is generated by the first part of the algorithm using KDEs. Since the coins from the KDE and the $k$-NN part of the algorithm are independent, we can simply treat  $\hat \eta^* $ as fixed.} According to Theorem \ref{thm: accuracy stat of kNN BayBox estimator} the probability that 
\begin{align} \label{e:gcon}
|\alpha(\hat \eta^* ) - \tilde \alpha(\hat \eta^* )|, |\beta(\hat \eta^* )-\tilde \beta(\hat \eta^* )| \le w(\gamma) 
\end{align}
is eventually (as $n_2\to \infty$) $\ge 1- \gamma$. Let us now condition on the event where \eqref{e:gcon} holds. The algorithm detects a violation, if
\[
i^* > \tilde{\alpha}(\hat{\eta}^*) + w(\gamma),
\]
where $i^*$ solves the equation $T^{(0)}(i^*) = \tilde{\beta}(\hat{\eta}^*) + w(\gamma)$.
We apply $T^{(0)}$ on both sides, which gives us the detection condition
\begin{align} \label{e:condition}
\tilde{\beta}(\hat{\eta}^*) + w(\gamma) < T^{(0)}(\tilde{\alpha}(\hat{\eta}^*) + w(\gamma)).
\end{align}
On the event characterized by \eqref{e:gcon} we have
\[
\tilde{\beta}(\hat{\eta}^*) + w(\gamma) \ge \beta(\hat{\eta}^*) 
\]
and, using the strict monotonicity of the trade-off curve $T^{(0)}$ 
\begin{align*}
    T^{(0)}(\tilde{\alpha}(\hat{\eta}^*) + w(\gamma)) \le T^{(0)}(\alpha(\hat{\eta}^*)).
\end{align*}
Now, in part 1) of the Theorem, we assume that $T^{(0)}(\alpha) \le T(\alpha)$ and hence
\[
T^{(0)}(\alpha(\hat{\eta}^*)) \le T(\alpha(\hat{\eta}^*)) = \beta(\hat{\eta}^*).
\]
But this means that the condition \eqref{e:condition} can only be met if $\beta(\hat{\eta}^*)>\beta(\hat{\eta}^*)$, which is false. Hence, conditional on \eqref{e:gcon}, which asymptotically holds with probability $\ge 1-\gamma$ the algorithm does not (falsely) detect a privacy violation and 
\[
\liminf_{n_2 \to \infty} \,\,\Pr\Big[ A = "\textnormal{No Violation}"\Big]=1-\gamma_{n_1}\ge 1-\gamma.
\]
It follows directly that
\[
\liminf_{n_1 \to \infty} 1-\gamma_{n_1} \ge  1-\gamma
\]
showing the first part of the theorem.\\
Now, in part 2), suppose that there exists a privacy violation. The trade-off function is strictly convex and it is not hard to see that this implies that it equals the set $\{(\alpha(\eta), \beta(\eta): \eta \ge 0\}$ in this case (the constant $\lambda$ in the Neyman-Pearson test can be set to $0$ everywhere). We also define the maximum violation
\[
v^* = \sup_{\alpha \in [0,1]}\big[ T^{(0)}(\alpha)-T(\alpha)\big]
\]
and the set of thresholds
\[
\Psi:= \big\{\eta \ge 0:T^{(0)}(\alpha(\eta))-T(\alpha(\eta)) \ge  v^*/2\big\}.
\]
It holds by the proof of Theorem \ref{theo:1} case 1) that 
\[
\sup_\eta |\hat \alpha_h(\eta) - \alpha(\eta)|\overset{P}{\to} 0, \quad as \,\,\, n_1 \to \infty.
\]
In particular, it follows that
\[
\Pr[\hat \eta^* \in \Psi]= 1-r_{n_1},
\]
where $r_{n_1} \to 0$ as $n_1 \to \infty$.
If the above statement were false, it would follow on an event with asymptotically positive probability that 
\[
T^{(0)}(\alpha(\hat \eta^*))-T(\alpha(\hat \eta^*)) \le (1/2) v^*
\]
leading to a contradiction with Proposition \ref{prop1}. Now, we condition on the event $\{\hat \eta^* \in \Psi\}$ and pass the parameter to the BayBox estimator, which returns the estimator pair $(\tilde{\alpha}(\hat{\eta}^*), \tilde{\beta}(\hat{\eta}^*))$. Now, keeping $n_1$ fixed and letting $n_2 \to \infty$ it follows that
\begin{align*}
   & \tilde{\alpha}(\hat{\eta}^*) + w(\gamma) \overset{P}{\to} \alpha(\hat{\eta}^*), \quad  \tilde{\beta}(\hat{\eta}^*) + w(\gamma) \to \beta(\hat{\eta}^*).
\end{align*}
Given the continuity of the function $T^{(0)}$ (every trade-off function is continuous) it follows that conditionally on $\Psi$
\begin{align*}
&T^{(0)}(\tilde{\alpha}(\hat{\eta}^*) + w(\gamma)) \to T^{(0)}(\alpha(\hat{\eta}^*)) \ge T(\alpha(\hat{\eta}^*) +v^*/2\\
= &\beta(\hat{\eta}^*) +\nu^*/2>  \beta(\hat{\eta}^*)
\end{align*}
and the detection condition in \eqref{e:condition} is asymptotically fulfilled as $n_2 \to \infty$. Thus, we have 
\[
\lim_{n_2 \to \infty}\Pr[A = \textnormal{"Violation"}| \{\hat \eta^* \in \Psi\}]  =1
\] 
and hence
\[
\liminf_{n_2 \to \infty}\Pr[A = \textnormal{"Violation"}]\ge 1-r_{n_1}.
\]
Taking the limit $n_1 \to \infty$ we have $r_{n_1} \to 0$ and the result follows.
\end{proof}

\section{Additional Experiments and Details} \label{AppB}

In this section, we provide some additional details on our experiments and implementations.

\subsection{Implementation details}

Algorithm \ref{alg:pointwise_KDE_estimator} gives a pseudo-code of our trade-off curve estimator $\hat T_h$, presented in Section \ref{sec:4}. 

\begin{algorithm}[h]
\footnotesize
\algorithmicrequire \; \parbox[t]{\dimexpr0.9\linewidth-\algorithmicindent}{Black-box access to $\Mech$; Threshold $\eta > 0$; Sample size $n$, databases $\DB, \DB'$.}\\[0.1cm]
\algorithmicensure \, An estimate $(\hat{\alpha}(\eta), \hat{\beta}(\eta))$ of $(\alpha(\eta), \beta(\eta))$ for tuple $(P, Q)$, where $\Mech(\DB)$ and $\Mech(\DB')$ are distributed to $P, Q$, respectively.
\begin{algorithmic}[1]
    \State Choose perturbation parameter $h$. 
    \State Set the density estimation algorithm $\cA$. By default, use the KDE algorithm.
    \Function{\textnormal{PTLR Estimatior} $\ptlr{h}{\cA}(M, \DB, \DB', \eta,n)$}{}
    \State Compute the estimated densities $\hat{p}$ and $\hat{q}$ by running $\mathcal{A}$ on $n$ independent copys of $\Mech(\DB)$ and $\Mech(\DB')$, respectively.
    % based on outputs of $M$ by running $\cA$ with a sample size of $n$.
    \State Compute $\hat{\alpha}(\eta) \leftarrow \int_{x \in [-h/2,h/2]} \frac{1}{h}\int_{\hat q /\hat p  > \eta +x} \hat p$ 
    \State Compute $\hat{\beta}(\eta) \leftarrow \int_{x \in [-h/2,h/2]} \frac{1}{h}  \int_{\hat q /\hat p  > \eta +x} \hat q$ 
    \State Return $(\hat{\alpha}(\eta), \hat{\beta}(\eta))$
    \EndFunction
\end{algorithmic}
\caption{PTLR: A Perturbed Likelihood Ratio Test Algorithm for $f$-DP Estimation}
\label{alg:pointwise_KDE_estimator}
\end{algorithm}

Next, we turn to the DP-SGD algorithm from our Experiments section. The pseudocode for that algorithm can be found in Algorithm \ref{alg:noisy_sgd} below. Note that we add Gaussian noise $Z_t \sim \mathcal{N}(0, \sigma^2)$ to the parameter $\theta_t$ at each iteration of DP-SGD. The operator $\Pi_{\Theta}$ projects the averaged and perturbed gradient onto the space $\Theta$ and is thus similar to clipping that gradient. We can derive the exact trade-off function of this algorithm for our choice of databases in \eqref{eq_databases} and our specifications from Section \ref{sec:algorithms}. More concretely, we first consider the distribution of DP-SGD on $D = (0, \dots, 0)$ and note that 
\begin{align*}
    \theta_{t+1} = \theta_t - \rho \, (\theta_t + Z_{t+1}) 
\end{align*}
for each $t \in \{0, \dots, \tau\}$. Some calculations then yield that $\Theta_{\tau} \sim \mathcal{N}(0, \bar{\sigma}^2)$ with 
\begin{align} \label{sigma_bar}
    \bar{\sigma}^2 = \rho^2 \, \sigma^2 \, \frac{1 - (1 - \rho)^{2 \tau}}{1 - (1 - \rho)^{2}}.
\end{align}
Similarly, we have for $D' = (1, 0, \dots, 0)$ that 
\begin{align*}
     \theta_{t+1} = (1 - \rho) \, \theta_t + \rho \,  Z_{t+1} 
\end{align*}
for each $t \in \{0, \dots, \tau\}$. Here, $Z_t$ is a Gaussian mixture with 
\begin{align*}
    Z_t \sim \frac{1}{2} \, \mathcal{N}\left(0,\sigma^2\right) + \frac{1}{2} \, \mathcal{N}\left(\frac{1}{m},\sigma^2\right).
\end{align*}
We can then see that $ \theta_{\tau} = \tilde{Z}_1 + \dots + \tilde{Z}_{\tau} $
where the $\tilde{Z}_t$ are independent Gaussian mixtures with
\begin{align*}
    \tilde{Z}_t & \sim \frac{1}{2} \, \mathcal{N}\left(0, \rho^2 \, (1- \rho)^{2 (\tau - t)} \, \sigma^2\right) \\ & + \frac{1}{2} \, \mathcal{N}\left(\frac{\rho (1 - \rho)^{\tau - t}}{m}, \rho^2 \, (1- \rho)^{2 (\tau - t)} \, \sigma^2\right).
\end{align*}
By defining 
\begin{align} \label{mu_I}
    \mu_I := \sum\limits_{t \in I} \frac{\rho (1 - \rho)^{\tau - t}}{m}
\end{align}
and choosing $\bar{\sigma}$ as in \eqref{sigma_bar}, we get that
\begin{align*}
    \theta_{\tau} \sim \sum\limits_{t \subset \{1, \dots, \tau\}} \frac{1}{2^{\tau}} \mathcal{N}(\mu_I, \bar{\sigma}^2).
\end{align*}
Having derived the distribution of $M(D)$ and $M(D')$, we take a look at the corresponding LR-test $g$ and note that it can be expressed as
\begin{align*}
    g(x) = \begin{cases}
       1  & x > c \\
       0  & x \leq c \\
    \end{cases}
\end{align*}
for some threshold $c$. A few calculations then yield the trade-off curve
\begin{align*}
    T_{SGD}(\alpha)=\sum_{I\subset \{1,\hdots, \tau\}} \frac{1}{2^{\tau}}\Phi\Big(\Phi^{-1} (1-\alpha)-\frac{\mu_I}{\bar\sigma}\Big)~.
\end{align*}

\begin{algorithm}[h]
\footnotesize
\algorithmicrequire \; \parbox[t]{\dimexpr0.9\linewidth-\algorithmicindent}{Dataset $x = (x_1, \ldots, x_r)$, loss function $\ell(\theta, x)$,\\ Parameters: initial state $\theta_0$, learning rate $\rho$, batch size $m$, time horizon $\tau$, noise scale $\sigma$, closed and convex space $\Theta$.}\\[0.1cm]
\algorithmicensure \, Final parameter $\theta_{\tau}$.
\begin{algorithmic}[1]
    \For{$t = 1, \ldots, \tau$}
        \State \textbf{Subsampling:} Take a uniformly random subsample $I_t \subseteq \{1, \ldots, r\}$ with batch size $m$.
        \For{$i \in I_t$}
            \State \textbf{Compute gradient:} $v_t^{(i)} \leftarrow \nabla_\theta \ell(\theta_t, x_i)$
           % \State \textbf{Clip gradient:} $v_t^{(i)} \leftarrow v_t^{(i)} / \max\{1, \|v_t^{(i)}\|_2 / C\}$
        \EndFor
        \State \textbf{Average, perturb, and descend:}
        \[
        \theta_{t+1} \leftarrow \theta_t - \rho \; \Pi_{\Theta} \left( \frac{1}{m} \sum_{i \in I_t} v_t^{(i)} + Z_t \right)
        \]
    \EndFor
    \State \textbf{Output:} $\theta_{\tau}$
\end{algorithmic}
\caption{DP-SGD Algorithm}
\label{alg:noisy_sgd}
\end{algorithm}



\begin{figure*}[h!]
    \centering
    \subfloat[$n_1=10^3$]{\includegraphics[width=0.3\textwidth]{Figures/Laplace_shade_1000.png}}
    \hfill
    \subfloat[$n_1=10^4$]{\includegraphics[width=0.3\textwidth]{Figures/Laplace_shade_10000.png}}
    \hfill
    \vspace{-0.2cm}
    \subfloat[$n_1=10^5$]{\includegraphics[width=0.3\textwidth]{Figures/Laplace_shade_100000.png}}
  \caption{Estimation of the Laplace Trade-off curve $T_{Lap}$ for varying sample sizes and $\mu=1$. Min- and Max Curve lower- and upper bound the worst point-wise deviation from the true curve $T_{Lap}$ over $1000$ simulations.}
    \label{fig:laplace}
\vspace{-0.1cm}
    \centering
    \subfloat[$n_1=10^3$]{\includegraphics[width=0.3\textwidth]{Figures/Subsampling_shade_1000.png}}
    \hfill
    \subfloat[$n_1=10^4$]{\includegraphics[width=0.3\textwidth]{Figures/Subsampling_shade_10000.png}}
    \hfill
    \vspace{-0.2cm}
    \subfloat[$n_1=10^5$]{\includegraphics[width=0.3\textwidth]{Figures/Subsampling_shade_100000.png}}
     \caption{Estimation of the Subsampling Trade-off curve $T_{Sub}$ with the Gaussian mechanism for $\mu=1$ and varying sample sizes and $\mu=1$. Min- and Max Curve lower- and upper bound the worst point-wise deviation from the true curve $T_{Sub}$ over $1000$ simulations.}
    \label{fig:subsampling}
\end{figure*}





\subsection{Additional Algorithms}
We test our estimation procedure on the Laplace and Subsampling algorithm, which often serve as building blocks in more sophisticated privacy mechanisms. We select the same setting for our experiments as in Section \ref{sec6} and choose $D$ and $D'$ as in \eqref{eq_databases}. \\

\noindent \textbf{Laplace mechanism.} We consider the summary statistic $S(x)= \sum_{i=1}^{10} x_i$ and the mechanism
\begin{equation*}
    M(x):= S(x)+Y~,
\end{equation*}
where $Y\sim \mathcal Lap(0, \sigma)$. The statistic $S(x)$ is privatized by the random noise $Y$ if the scale parameter $\sigma > 0$ of the Laplace distribution is chosen appropriately. We choose $\sigma = 1$ for our experiments and observe that the optimal trade-off curve is given by 
\begin{align*}
    T_{Lap}(\alpha)=\begin{cases}
        1- e \, \alpha,  &\alpha<e^{-1}/2~,\\
        e^{-1}/4 \alpha,  &e^{-1}/2\leq \alpha\leq 1/2~,\\
        e^{-1}(1-\alpha), &\alpha>1/2.
    \end{cases}
\end{align*}
We point the interested reader to \cite{Dong2022} for more details on how to derive $T_{Lap}$. \\

\noindent \textbf{Subsampling algorithm.} Random subsampling provides an effective way to enhance the privacy of a DP mechanism $M$. We only provide a rough outline here and refer for details to \cite{Dong2022}.
In simple words, we choose an integer $m$ with  $1\leq m< r$, where $r$ is the size of the database $D$. We then draw a random subsample of size $m$ from $D$, giving us the smaller database $\bar D$ of size $m$. The mechanism $M$ is then applied to $\bar D$ instead of $D$, providing users with an additional layer of privacy (if a user is not part of $\bar D$, their privacy cannot be compromised). The amplifying effect that subsampling has on privacy is visible in the optimal trade-off curve: If $M$ has the trade-off curve $T$, then $M(\bar D)$ has the trade-off curve
\begin{equation*}
    \bar T(\alpha)=  \frac{m}{r}T(\alpha)+\frac{r-m}{r}(1-\alpha),
\end{equation*}
which is strictly more private than $T$ for any $m<r$. A minor technical peculiarity of subsampling is that the resulting curve $\bar T$ is not necessarily symmetric, even if $T$ is (see \cite{Dong2022} for details on the symmetry of trade-off functions). Trade-off curves are usually considered to be symmetric and one can symmetrize $\bar T$ by applying a symmetrizing operator $\mathbf{C}$ with 
\begin{equation*}
    \mathbf{C}[T](x)=\begin{cases}
         T(x), \quad &x\in [0,x^*]\\
        x^*+ T (x^*)-x, \quad &x\in [x^*, T(x^*)]\\
         T^{-1}(x), \quad &x\in [ T(x^*),1],
    \end{cases}
\end{equation*}
where $x^*$ is the unique fix-point of $T$ with $T(x^*)=x^*$ (for more details we refer to \cite{Dong2022}). Another mathematical representation of $\mathbf{C}$ that we use in our code is 
$\mathbf{C}(T)=\min\{T,T^{-1}\}^{**}$, where the index $**$ signifies double convex conjugation. We incorporate this operation into our estimation procedure by simply applying $\mathbf{C}$ to our estimate of the trade-off function $T$. For our experiments involving subsampling, we use the Gaussian mechanism for $M$ (with $\sigma=1$) and obtain the subsampled version $M'$ by fixing the parameter $m=5$ (recall that $r=10$). \\




\noindent Similar to the experiments section, we construct figures that upper and lower bound the worst case errors for the Laplace mechanism and the Subsampling algorithm over $1000$ simulation runs. We can see again that the error of the estimator $\hat T_h$ shrinks significantly, as $n_1$ grows. 




\subsection{Additional simulations}

We present some results that complement the main findings in our experiment section. We use the same setup as described in our experiments and investigate a faulty implementation of the Gaussian mechanism. We study two things: First, the impact of the parameter $\gamma$, where we vary $\gamma$ between very small and relatively large values. As we can see, smaller values of $\gamma$ lead to larger boxes $\square_\gamma$ which make it harder for the auditor to detect violations. Secondly, we consider the impact of the sample size $n_1$ ranging from the very modest value of $10^2$ up to $10^4$. We see that the sample size has very little impact on the performance of the procedure and it already works well for fairly small samples $n_1$ ($n_2$ has a greater impact, as we have seen in our experiments).
\begin{figure*}[t!]
    \centering
    \subfloat[\centering $\gamma=0.001$, \textbf{Ground truth:} Violation; \textbf{Decision:} \textcolor{red}{"No Violation"}{\textcolor{red}{\scalebox{1.5}{\ding{55}}}}]{\includegraphics[width=0.3\textwidth]{Additional_Figures/gauss_faulty_gamma_0.001_mu_0.2.png}}
    \hfill
    \subfloat[\centering $\gamma=0.01$, \textbf{Ground truth:} Violation; \textbf{Decision:} \textcolor{red}{"No Violation"}{\textcolor{red}{\scalebox{1.5}{\ding{55}}}}]{\includegraphics[width=0.3\textwidth]{Additional_Figures/gauss_faulty_gamma_0.01_mu_0.2.png}}
    \hfill
    \subfloat[\centering $\gamma=0.1$, \textbf{Ground truth:} Violation; \textbf{Decision:} \textcolor{green}{"Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Additional_Figures/gauss_faulty_gamma_0.1_mu_0.2.png}}
    \hfill
    \subfloat[\centering $n_2=10^2$,
  \textbf{Ground truth:} Violation; \textbf{Decision:} \textcolor{green}{"Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]
  {\includegraphics[width=0.3\textwidth]{Additional_Figures/gauss_faulty_gamma_0.05_mu_0.2_n_2.png}}
    \hfill
    \subfloat[\centering $n_1=10^3$, \textbf{Ground truth:} Violation; \textbf{Decision:} \textcolor{green}{"Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Additional_Figures/gauss_faulty_gamma_0.05_mu_0.2_n_3.png}}
    \hfill
    \subfloat[\centering $n_1=10^4$, \textbf{Ground truth:} Violation; \textbf{Decision:} \textcolor{green}{"Violation"}{\textcolor{green}{\scalebox{1.5}{\ding{51}}}}]{\includegraphics[width=0.3\textwidth]{Additional_Figures/gauss_faulty_gamma_0.05_mu_0.2_n_4.png}}
        \caption{\textbf{Auditing a faulty Mechanism:} Claimed Curve $\textcolor{blue}{T^{(0)}} = \textcolor{blue}{T_{Gauss}}$ with $\mu=0.2$, but in reality $\mu=1$. For (a),(b),(c) we consider $n_1=10^4$, and for (d),(e),(f) we have considered various sample sizes for the KDEs, respectively. Throughout the simulations we keep $n_2=10^6$ fix.}\label{fig:additional_experiments}
       % \caption{\textbf{Auditing a faulty Mechanism:} Claimed Curve $\textcolor{blue}{T_0} = \textcolor{blue}{T_{SGD}}$ with $t_{-}=5$ iteration, but in reality data is accessed $t_{-}=10$ times. Estimate $\textcolor{blue}{T_0}$ by $\textcolor{orange}{\hat T_0}$.  With critical vertical line with intercept $(\hat\alpha(\hat\eta^*), \hat \beta(\hat \eta^*))$, $k$-NN point estimator {\textcolor{purple}{\ding{108}}} $(\tilde\alpha(\hat\eta^*), \tilde \beta(\hat\eta^*))$ and confidence region $\textcolor{purple}{\square}$. The sample size for KDE is $n_1=10^2$ and the confidence parameter is $\gamma=0.05$.}
\end{figure*}
%The derivation of this result is quite involved. 

$ $\\[1000ex] $ $ \newpage

%the subsampling mechanism uniformly samples a subset of size $m$ out of the $k$ individuals and performs the statistical analysis based on that subset instead of the whole database. For an $f-$DP mechanism $M$ acting on a database with participant number $k$, \cite{Dong2022} characterize the privacy guarantee by an operator $C_p$ acting on trade-off function. In particular, we have for $p=m/k$ that
%\begin{equation*}
 %   T_{Sub}(\alpha)= C_p[T]%(\alpha)~,
%\end{equation*}
%where 
%\begin{equation*}
%    C_p[T](\alpha)=\begin{cases}
  %      T_p(x), \quad &x\in [0,x^*]\\
  %      x^*+T_p(x^*)-x, \quad &x\in [x^*,T_p(x^*)]\\
   %     T_p^{-1}(x), \quad &x\in [T_p(x^*),1]
  %  \end{cases}
%\end{equation*}
%Roughly speaking $p =m/k$ corresponds to the probability of drawing a specific individual from a database of size $k$ in $m$ draws (and $(1-p)$ is the probability of not-drawing that individual). This motivates the specific form of $T_p$. With that in hand, the formula of $C_p$ follows from the (more general) expression  $C_p(T)=\min\{T_p,T_p^{-1}\}^{**}$ in \cite{Dong2022}. Here for a curve $f$, the function $f^*$ is the convex conjugate. is simply a symmetrization of $T_p$ in a sense that it is the greatest convex minorant of the minimum $T_p$ and $T_p^{-1}$ (\todo{maybe cite sth}). Here, for a function $T$, $T^{**}$ denotes the twice convex conjugate of $T$.