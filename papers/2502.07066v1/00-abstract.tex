\begin{abstract}
In this paper we propose new methods to statistically assess $f$-Differential Privacy ($f$-DP), a recent refinement of differential privacy (DP) that remedies certain weaknesses of standard DP (including tightness under algorithmic composition). A challenge when deploying differentially private mechanisms is that DP is hard to validate, especially in the black-box setting. This has led to numerous empirical methods for auditing standard DP, while $f$-DP remains less explored. We introduce new black-box methods for $f$-DP that, unlike existing approaches for this privacy notion, do not require prior knowledge of the investigated algorithm. Our procedure yields a complete estimate of the $f$-DP trade-off curve, with theoretical guarantees of convergence. Additionally, we propose an efficient auditing method that empirically detects $f$-DP violations with statistical certainty, merging 
techniques from non-parametric estimation and optimal classification theory. Through experiments on a range of DP mechanisms, we demonstrate the effectiveness of our estimation and auditing procedures. 


\end{abstract}


%    Differential Privacy (DP) is currently the most widely-used way to describe the privacy of data-processing algorithms. However, there are several challenges in the deployment of DP. 
%First, the standard notion of DP has well-acknowledged limitations, e.g. when describing the privacy of multi-step (composed) mechanisms. This has prompted interest in refinements of DP - one of the most recent and well-discussed ones being $f$-DP, which has several attractive properties including `lossless’ composition. The second difficulty in practical deployments is that DP is hard to verify, especially in the black-box setting. This has led to a plethora of works that aim to (empirically) either directly `estimate’ the level of privacy of an implementation, or `audit’ whether it actually achieves a claimed level of privacy. 

%In this work we explore the problems of {\em estimating} and {\em auditing} $f$-DP. We will both leverage and expand upon known techniques in DP, as well as develop new techniques to handle the unique challenges of working with $f$-DP.


%    Differential privacy (DP) is currently the most widely-used measure of privacy. However, there are several challenges in the deployment of DP. 
%First, the DP privacy definition has well-acknowledged limitations, such as difficulty in describing the privacy of multiple composed mechanisms. This prompted several generalisations of DP, one of the most recent and well-discussed of which is $f$-DP, which has several attractive properties including `lossless’ composition. The second difficulty in practical deployments is that DP is hard to verify, especially in the black-box setting. This has led to a plethora of works that aim to (empirically) either directly `estimate’ the level of privacy of an implementation, or `audit’ whether it actually achieves a claimed level of privacy. 

%In this work we explore the problems of {\em estimating} and {\em auditing} $f$-DP. We will both leverage and expand upon known techniques in DP, as well as develop new techniques to handle the unique challenges of working with $f$-DP.
