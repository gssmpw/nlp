\section{Overview of Techniques}\label{sec:overview_techniques}
Our goal is to provide an estimation and auditing procedure for the optimal privacy curve $f$ of a mechanism $\Mech$. This task can be broken down into two parts: (1) Selecting datasets $\DB,\DB'$ that cause the largest difference in $\Mech$'s output distributions and (2) Developing an estimator/auditor for the trade-off curve given that choice of $\DB, \DB'$. %for the trade-off function $T = T(M(D),M(D'))$. 
In line with previous works on black-box estimation/auditing, we focus on task (2). The selection of $\DB,\DB'$ has been studied in the black-box setting and can typically be guided by simple heuristics \cite{StatDP, DP-Sniper, Lokna2023}.% We will therefore focus on part (2) and rely on output samples gathered from $M$ on $D,D'$ for this.


Our proposed estimator of a trade-off curve relies on KDEs. Density estimation in general and KDE in particular is 
an important tool in the black box assessment of DP. For some examples, we refer to \cite{Liu2019}, \cite{Dette2022} and \cite{Kutta2024}. The reason is that DP can typically be expressed as some transformation of the density ratio $p/q$ -- this is true for standard DP (a supremum), RÃ©nyi DP (an integral) and, as we exploit in this paper, $f$-DP via the Neyman-Pearson test. A feature of our new approach is that we do not simply plug in our estimators in the definition of $f$-DP, but rather use them to make a novel, approximately optimal test. This test is not only easier to analyze than the standard likelihood ratio (LR) test but also retains similar properties (see the next section for details).
% , that is easier to study than the standard LR test, but has similar properties (see the next section).\\
%We will work with a carefully randomized approximation of the optimal Neyman-Pearson test in order avoid estimating level sets where $p = q$\todo{This might be a bit specific for reader? e.g., suggest `to avoid issues in Neyman-Pearson when $p = q$'.} We then obtain an estimate for entire trade-off curve $f$ by plugging our KDEs into this randomized test and evaluating it over over a grid in $[0,1]$. Our estimator converges uniformly to the optimal parameter $f$.
%\todo{Overview of techniques was moved here because it seems you might need some of the preliminaries to understand it. }
%Intuitively, our techniques combine the well-known LR (likelihood ratio) test in statistics (Prelim.~\ref{sec:optimal}), with techniques from binary classification (Prelim.~\ref{sec:class}), the latter of which extends the results from a recent DP estimator~\cite{Lu2024}. 
%\todo{Why we focus on a pair of neighbors for fDP} 
%In the stronger black-box setting, it is generally unknown for which datasets a mechanism may fail to satisfy privacy---similar to DP, $f$-DP requires the privacy parameter $f$ to hold for all pairs of neighbors $D, D'$. As such, in our knowledge all previous black-box (and even some white-box, e.g.~\cite{Nasr2023}) works focus on these tasks under one (or a limited number of) pair of neighboring datasets. 
%We discuss the estimator first: given a mechanism $M$ and a pair of neighboring datasets $D, D'$, the goal is to estimate the true trade-off curve between errors $(\alpha, \beta)$ for any test for hypotheses $H_0, H_1$, corresponding to $M(D), M(D')$ (Prelim.~\ref{sec:fdp}). This is done by introducing a {\em new} technique, which we term the {\em perturbed} LR test. This test approximates the well-known, possibly random, optimal LR test, but circumvents the knowledge of the distributions of $M(D), M(D')$ by construction. Under natural assumptions (Assumption~\ref{ass1}) on the densities  of these two distributions, 
%we construct an algorithm which converges to the optimal $f$-DP curve for these two hypotheses. That is, after estimating the curve, we can, given any $\alpha$, output the corresponding optimal $\beta(\alpha)$. \todo{<---Please correct the statements in this  paragraph if they are incorrect.}
%\MD{Do we need this here? I think we already capture the above in the Contributions, which we might want to extend instead of writing this here.}


Our second goal (Sec.~\ref{sec:audit}) is to audit whether a mechanism~$\Mech$ satisfies a claimed trade-off $f$, given datasets $\DB$ and $\DB'$. At a high level, we address this task by identifying and studying the \emph{most vulnerable point} on the trade-off curve $T$ of $\Mech$ --- the point most likely to violate $f$-DP. We begin by using our $f$-DP estimator to compute a value $\eta$ (from the Neyman-Pearson framework in Sec.~\ref{sec:hyp}), which defines a point $\bigl(\alpha(\eta), \beta(\eta)\bigr)$ on the true privacy curve $T$ of the mechanism~$\Mech$. $\eta$ is chosen such that $\bigl(\alpha(\eta), \beta(\eta)\bigr)$ has the largest distance from the claimed trade-off curve~$f$ asymptotically, which we prove in Prop.~\ref{prop1}.
%We prove that it converges to the point in the curve with the largest difference when subtracting from the claimed trade-off curve $f$ (Prop.~\ref{prop1}). Intuitively, this is the {\em most vulnerable point} that is  most likely to violate $f$-DP. 
Next, by extending a technique proposed in~\cite{Lu2024}, we express $\bigl(\alpha(\eta), \beta(\eta)\bigr)$ in terms of the Bayes risk of a carefully constructed Bayesian classification problem, and approximate that Bayes risk using a feasible binary classifier (e.g., $k$-nearest neighbors). By deploying the $k$-NN classifier we obtain a confidence interval that contains our vulnerable point $(\alpha,\beta)$ with high probability.
% We then examine this $\eta$ value of interest further, by extending a technique used in~\cite{Lu2024}. Specifically, our auditor employs a Bayes (optimal) binary classifier (which can be approximated by, e.g., $k$-nearest neighbor) to approximate the corresponding true $(\alpha, \beta)$ value. 
Finally, our auditor decides whether to reject (or fail to reject) the claimed $f$ curve by checking whether the corresponding point $(\alpha, \beta')$ on $f$ with $f(\alpha) = \beta'$ is contained in this interval or not.
% We then compare our estimate of $(\alpha, \beta)$ to the corresponding point $(\alpha, \beta')$ on the given claimed trade-off curve $f$---note that we will compare using the same $\alpha$. %on the $x$-axis.  In other words, we check whether $f$-DP is satisfied, by checking this 'worst' point. 
Leveraging the convergence properties of $k$-NN, our auditor provides a provable and tuneable confidence region that depends on sample size. We also note that the connection between Bayes classifiers and $f$-DP that underpins our auditor may be of independent interest, as it offers a new interpretation of $f$-DP by framing it in terms of Bayesian classification problems.
% In addition to this practical perspective, our newly-forged connection between (Bayes) classifiers and $f$-DP is a highly non-trivial theoretical contribution. It extends the recently-shown~\cite{Lu2024} connection between classifiers and standard DP; the combination of these two results may be of independent interest.


%Our second goal (Sec.~\ref{sec:audit}) is to audit whether, given $D, D'$ and a claimed trade-off $f$, the mechanism $M$ satisfies $f$-DP. This problem is relevant, since typically a DP algorithm is accompanied with a corresponding privacy claim. To audit, we first use our $f$-DP curve estimator from above to identify a pair $(\alpha, \beta)$ with high vulnerability. On a technical level, we search for a threshold $\eta$, where the Neyman-Pearson test produces particularly small errors. ${}$ We then examine the pair $(\alpha(\eta), \beta(\eta))$ extending a technique used in~\cite{Lu2024}. Specifically, we reformulate $f$-DP as an optimal classification problem, which is a contribution of independent interest. We then employ an approximation of the Bayes optimal classifier to solve this problem. The result is a small (square) range of values $\square \subset [0,1^2]$ and we know with high confidence that $(\alpha(\eta), \beta(\eta)) \in \square$. If values in $\square$ are incompatible with the claimed curve, the auditor detects a privacy violation. More precisely, the curve in $f$-DP identifies a region where $(\alpha(\eta), \beta(\eta))$ should be and if this region and the confidence range $\square$ are non-overlapping we have a contradiction (since both should include $(\alpha(\eta), \beta(\eta))$). If this outcome occurs, the auditor reject the privacy claim $f$.
%approximate the corresponding true $(\alpha, \beta)$ value for the audited mechanism. We then compare our estimate of $(\alpha, \beta)$ to the corresponding point $(\alpha, \beta')$ on the given claimed trade-off curve $f$---note that we must compare using the same $\alpha$ on the $x$-axis. In other words, we check whether $f$-DP is satisfied, by checking this 'worst' point. The use of $k$-NN in our auditor gives us a provable and tuneable confidence region that depends on sample size. 
%\todo{added this part about classifiers connection w/dp}
%In addition to this practical perspective, 
%our newly-forged connection between (Bayes) classifiers and $f$-DP is a highly non-trivial theoretical contribution. It  extends the recently-shown~\cite{Lu2024} connection between classifiers and standard DP; the combination of these two results may be of independent interest.


%\textbf{Classifiers in DP} The recent Eureka estimator~\cite{Lu2024} related Bayes classifiers to DP. Through the established connection between DP and $f$-DP~\cite{Dong2022}, their results imply an indirect link between Bayes classifiers and $f$-DP already. However, this connection is somewhat obscure, as fully recovering the $f$ function requires knowledge of every parameter tuple $(\varepsilon, \delta)$ for $\varepsilon \geq 0$ in approximate DP, which appears computationally infeasible. In this paper, we study (optimal) classifiers directly within the framework of $f$-DP, forging a new and explicit link between the two.


%\todo{TODO: Here, say something about the optimal test for the hypothesis pair being LR test and cite relevent paper(s)}.

%\todo{TODO: Say here how to get the worst $\eta$}

%\todo{TODO: estimating w/confidence bound using Bayes estimator}

%\todo{TODO: some words here on application/experiments}




% \paragraph{Paper Organisation} 
% Preliminaries are introduced in Sec.~\ref{sec:prelim}. We propose our $f$-DP curve estimator in Sec.~\ref{sec:4} and auditor in Sec.~\ref{sec:goal2}. We validate and benchmark our constructions in Sec.~\ref{sec6} using the standard Laplace/Gaussian mechanisms, as well as privacy amplification by subsampling and  DP-SGD. We delve into more detail on related work in Sec.~\ref{sec:relatedwork} 