 We compared human interpretation data for numerical utterances to LLMs' interpretations, finding substantial differences.
This manifests in LLMs' tendency toward literal interpretations, reversed halo effects (preferring exact interpretations for round rather than sharp numbers; Exp.~1), and inconsistent affect attribution between literal and hyperbolic utterances (Exp.~2), despite human-like prior representations (Exp.~3).
These findings point to a disconnect in LLM pragmatic reasoning --- despite possessing accurate prior knowledge about prices, affect and utterance probabilities --- and despite this knowledge being structured in a way that could support human-like inference when processed through an RSA framework --- LLMs fail to consistently leverage this information when directly prompted to make pragmatic interpretations. 

Our findings highlight an important methodological contribution for understanding LLM behaviors: by systematically decomposing pragmatic reasoning into testable components (priors, affect mappings, utterance likelihoods, and interpretations), we can precisely locate differences between human and AI reasoning. 
This approach extends beyond traditional behavioral comparisons, allowing us to identify whether differences stem from knowledge gaps or reasoning mechanisms. Such detailed cognitive modeling approaches may prove valuable for understanding other aspects of LLM behavior, particularly in cases where surface-level performance masks deeper processing differences from human cognition.
Importantly, our results demonstrate that cognitively-inspired chain-of-thought prompting can help bridge this gap between knowledge and application. We achieved improved correlations with human judgments by decomposing the RSA model's computational steps into natural language reasoning chains. This success suggests that while LLMs may not naturally develop human-like pragmatic reasoning through training alone, they can successfully implement such reasoning when given appropriate computational frameworks that mirror human cognitive processes.

Based on our results, future research could address several important follow-up questions. For instance, potential training modifications to help LLMs better integrate their prior knowledge and context when interpreting hyperbole could be analyzed. Identifying factors that influence how LLMs apply this knowledge in context is also an open question. We report some exploratory analyses (see supplementary materials) that begin to probe these questions through variations in prompting of the models.

Ultimately, our work demonstrates that evaluating LLMs through the lens of cognitive modeling provides a nuanced understanding of how these models deviate from human understanding. By integrating LLMs with cognitive models of pragmatic language use, we can both critically assess the models' internal consistency and provide a framework for improving their performance in interpreting non-literal language.
