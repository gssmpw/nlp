\subsection{Eliciting LLMs' Utterance Likelihoods}
In Experiment~3, we focused on assessing key aspects that might be the root cause of LLMs' deficiencies in non-lieral number interpretation, informed by the RSA model: 
(1) the price priors that LLMs assign to for each item, (2) the priors of affective responses conditional on item prices, and (3) the LLMs' representations of speaker likelihoods of uttering different prices $u$, given different true price states $s$ and speaker goals. 
The low correlation of LM-RSA, zero-shot LLM and human predictions (Experiment 3) revealed that the challenge in achieving human-like pragmatic interpretation of hyperbole and halo was not due to lack in the modelsâ€™ price and affect priors.
Therefore, we investigated whether the LLM might lack the third computational component that constitutes the pragmatic interpreter in the RSA model: the pragmatic speaker $S_1$.  
Specifically, we tested whether the LLMs captured the likelihoods of different utterances $u \in U$ conveying the speaker's communicative goal and intended meaning about an item with sufficient accuracy. 

We collected likelihoods of uttering a price $u \in U$, given different states and goals of the speaker based on \textit{zero-shot prompting}, as shown in~\autoref{prompt:affect}.
\input{figs/affect}
We used these raw LLM predictions to calculate the speaker likelihood $P_{LLM}(u \mid s, a, g)$, where $g \in G$ is the speaker goal (communicating exact or round price, and communicating the price, the affect, or both), $a$ represented affect and could take on the values 0 or 1, resulting in 12 condition, some of which are identical.
We calculated the final $P_{LLM}(u \mid s, a, g)$ by aggregating over the raw LLM predictions over the irrelevant meaning dimensions as suggested in the RSA model (repeated from main text): 
$$S_1 (u \mid s, a, g) \propto \sum_{s', a'} \delta_{g(s, a) = g(s', a')}L_0(s', a' \mid u) \cdot e^{-c(u)}$$
That is, for instance, when the goal $g$ is to convey affect only, the scores are aggregated and renormalized across the values of $S$.  

We investigated the accuracy of the utterance likelihood representations of the LLMs by fitting a \textit{full LM RSA model}.
The $P_{LLM}$ was used together with LLM priors elicited from the same LLM in Experiment 3 to fit this RSA model (via enumeration) and predict posteriors of different meanings, given utterances. 
 
We report the correlation of the full LM RSA posteriors of all $(u, s)$ with human data in \autoref{tab:gpt-prompting-comparison} (``full LM-based RSA''). 
We found that the full LM-RSA model showed notably higher correlation with human data than the zero-shot predictions of the same LLM models under zero-shot prompting (see~\autoref{tab:cor_table} in the main text).
The correlations are also higher than for the one-shot RSA CoT based results (\autoref{tab:cor_table}), suggesting that more explicit RSA-based task decomposition might guide LLMs towards more human-like interpretation in a more stringent way.
In other words, the computational components of the RSA model provide a structure which allows to build a fully LLM-based system that rationalizes an observed utterance in a more human-like way, based  only on assuming the space of possible speaker goals.
At the same time, the correlations of the full LM-RSA models are lower than for the LM-RSA model (based only on LLM priors), suggesting that LLMs' utterance likelihoods might be less human-like than LLMs' priors. 
Additioanlly, the correlations of full LM-RSA models are ordinally the same as the zero-shot predictions (from Gemini to Claude). 
Taken together, these comparisons suggest that the lack in representing human-like utterance likelihoods might contribute to the LLMs' interpretation difficulties in a zero-shot setting

These detailed analyses open up interesting avenues for investigating why LLM utterance likelihoods differ compared to humans, and whether, e.g., particular training or dataset aspects lead to the discrepancy between humans and LLMs.
We turn to some explorations in the next section.

\subsection{Free Generation of Prices}
Based on the main results, we report another exploratory investigation as to \textit{why} the LLMs may have deficient performance on hyperbole and halo, despite correctly representing component information suggested by then RSA model. 

One potential reason could be that the materials from \citet{kao2014nonliteral} used in Experiment 1 as the state space $S$ were out-of-distribution for the LLMs.
Focusing on GPT-4o-mini, we ran an exploratory free generation study, identifying which prices the LLM would generate under $\tau=1$ when prompted to complete the speaker's utterance about the price of an item, given the speaker's goal (e.g., to convey the state, the affect, or both; being precise or imprecise about the price). 
We then qualitatively assessed whether the produced numbers differed from $U$ = \{50 + k, 500 + k, 1000 + k, 5000 + k, 10000 + k\}, with $k \in \{0, 1, 2, 3\}$.
Additionally, we compared whether LLMs freely generate higher prices when prompted to convey affect (i.e., that the item was expensive), than when prompted without affect . The full prompt can be found in~\autoref{prompt:free-generation}. 
\input{figs/free_generation}

Based on one exploratory simulation for all items and prices, we found that LLM produced different prices than in $U$. For instance, the predicted utterances for the electric kettle ranged from \$30 to \$150. The LLM produced higher prices when prompted to communicate that the speaker think the item is expensive (under the goal to communicate affect or both meaning dimensions), than when the affect was not present (mean predicted prices across items: \$342~vs.~\$590). These results suggests that the LLM are, in principle, able to generate hyperbolic utterances, but may be sensitive to the exact price numbers when prompted to interpret utterances or assess the likelihood of particular utterances.
