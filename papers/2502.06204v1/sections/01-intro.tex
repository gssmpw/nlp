\begin{table*}[t]

\centering
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{p{3.2cm}p{18cm}}
\toprule
\textbf{Experiment} & \textbf{Prompt} \\
\midrule

\textbf{1: Hyperbole \& Halo} & 
In each scenario, two friends are talking about the price of an item.
Please read the scenarios carefully and provide the probability that the item has the described price.
Provide the estimates on a continuous scale between 0 and 1, where 0 stands for "impossible" and 1 stands for "extremely likely".
\textit{Daniel} bought a new \textit{electric kettle}. A friend asked him, ``Was it expensive?'' \textit{Daniel} said, ``It cost \textit{\$47}.''  Please provide the probability that the \textit{electric kettle} costs \textit{\$50}.
\\
\hline
\textbf{2: Affective Subtext} & In each scenario, a person has just bought an item and is talking to a friend about the price.
Please read the scenarios carefully and provide the probability that the person thinks that the item is expensive.
Provide the estimates on a continuous scale between 0 and 1, where 0 stands for "impossible" and 1 stands for "absolutely certain".
\textit{Daniel} bought a new \textit{electric kettle}. It cost \textit{\$47}. A friend asked him, ``Was it expensive?'' \textit{Daniel} said, ``It cost \textit{\$47}.''   Please provide the probability that \textit{Daniel} thinks that the \textit{electric kettle} is expensive. 
\\
\hline 
\textbf{3a: Price Prior} & Each scenario is about the price of an item.
Please read the scenarios carefully and provide the probability that someone buys the item with the given price.
Provide the estimates on a continuous scale between 0 and 1, where 0 stands for "impossible" and 1 stands for "extremely likely". 
\textit{Daniel} bought a new \textit{electric kettle}. It cost \textit{\$50}. Please provide the probability that someone buys the \textit{electric kettle} with this price.
\\
\hline 
\textbf{3b: Affect Prior} & In each scenario, someone has just bought an item.
Please read the scenarios carefully and provide the probability that the buyer thinks that the item is expensive.
Provide the estimates on a continuous scale between 0 and 1, where 0 stands for "impossible" and 1 stands for "absolutely certain".
\textit{Daniel} bought a new \textit{electric kettle}. It cost \textit{\$50}.  Please provide the probability that the buyer thinks that the \textit{electric kettle} is expensive.
\\
\bottomrule
\end{tabular}}
\caption{
Example prompts used in each experiment. The constant sentences were used as the system prompt. \textit{Italicized} segments varied in each trial.}
\label{tab:prompts}
\end{table*}

A friend exclaims, ``This coffee cost me a million dollars!'' We instantly understand the intended meaning: the coffee was surprisingly expensive (but not a million dollars). Humans often \emph{interpret words non-literally}, effortlessly integrating context, world knowledge, and speaker intent to grasp the meaning behind expressions \citep{gibbs2006figurative}. As large language models (LLMs) become increasingly integrated into our daily lives, three crucial questions emerge: 1) Do LLMs understand literal and non-literal utterances as humans do? 2) Can we use computational models of human cognition to systematically analyze how LLMs interpret non-literal utterances? 3) Can we use cognitive models of human pragmatic language understanding to guide LLMs to interpret meaning in a more human-like way?

In this work, we address these questions by focusing on two common phenomena in the interpretation of \textit{number} words: \textit{hyperbole}, the deliberate use of extreme numerical exaggeration to convey emotion or emphasis (see Ex.~\ref{ex:hyperbole}), and the \textit{pragmatic halo effect}, the tendency to interpret round numbers imprecisely (Ex.~\ref{ex:halo}) and sharp numbers precisely (Ex.~\ref{ex:exact}) \citep{lasersohn1999pragmatic,Krifka2007:Approximate-Int}: 

\ex.\label{ex1} Bob bought a kettle. Bob said: 
\a.\label{ex:hyperbole} `It cost \$10000.' $\leadsto$ \textit{Too expensive.} \hfill (hyperbole)
\b.\label{ex:halo} `It cost \$50.' $\leadsto$ \textit{It cost around \$50.} \hfill (imprecise)
\c.\label{ex:exact} `It cost \$48.' $\leadsto$ \textit{It cost exactly \$48.} \hfill (exact)

Language models trained to auto-regressively predict the next word and subsequently fine-tuned through human feedback have produced impressive performance in many areas \citep[among many others]{srivastava2023-BIGbench}. However, it remains unclear to what extent such training leads to nuanced distinction of literal and non-literal language in LLMs.
Recent work has explored non-literal language interpretation in LLMs, from metaphor comprehension \citep{ tong-etal-2021-recent,liu-etal-2022-testing,prystawski2023psychologically} to pragmatic inference \citep{ jeretic-etal-2020-natural, ruis2024goldilocks}. 
While benchmarking efforts have revealed persistent gaps between human and model performance \citep{sravanthi-etal-2024-pub}, we still lack a comprehensive understanding of when and why language models fail at interpreting non-literal language. Understanding these limitations is crucial both for improving models and for insights into how meaning is captured through large-scale language modeling.

To investigate whether LLMs interpret number words in a human-like way, as in Example~\ref{ex1}, we compare their  interpretations with human judgment data from \citet{kao2014nonliteral}. Specifically, we elicit LLMs' likelihood estimates for different prices given an uttered number, allowing us to compute the probability of hyperbolic interpretation (\ie interpreting the price as lower than the stated amount). 

The cognitive model of non-literal language interpretation in \citet{kao2014nonliteral}, suggests that human interpretation is driven by prior knowledge interacting with inferences about the goals of the speaker. We find an interesting disconnect --- while LLMs demonstrate human-like prior knowledge about typical prices and what constitutes ``expensive'', they tend toward more literal interpretations of numerical expressions. This suggests that despite having acquired accurate world knowledge through training, LLMs may lack the pragmatic reasoning mechanisms that humans use to bridge between literal meanings and intended interpretations. 

We then explore whether insights from cognitive science toward more human-like interpretations of hyperbole and pragmatic halo, comparing two approaches: cognitive model-inspired chain-of-thought prompting and direct implementation of computational reasoning steps with an LLM. Through both methods—providing explicit reasoning chains and implementing Rational Speech Act framework \citep{goodman2016pragmatic} computations—we demonstrate that LLMs can achieve more human-like interpretations of non-literal language.