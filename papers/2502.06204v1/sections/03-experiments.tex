

\renewcommand{\arraystretch}{1.5}
\begin{table*}[ht!]
    \centering
    \small
    \begin{tabularx}{\textwidth}{l*{7}{>{\centering\arraybackslash}X}}
        \toprule    
        \multicolumn{1}{l}{ } & \multicolumn{2}{c}{GPT-4o-mini} & \multicolumn{2}{c}{Claude-3.5 Sonnet} & \multicolumn{2}{c}{Gemini-1.5-pro} \\
        
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        
        Prompt & 0-shot & 1-shot RSA CoT & 0-shot & 1-shot RSA CoT & 0-shot & 1-shot RSA CoT\\
        \midrule
         $R$ with humans & 0.41 & 0.579 & 0.528 & 0.558 & 0.365 & 0.603 \\
        \bottomrule
    \end{tabularx}
    \caption{Correlations between human data and LLM predictions of probabilities of all utterance-meaning ($(s, u)$) pairs. 0-shot indicates correlations of human results with LLM results under 0-shot prompting, 1-shot RSA CoT indicates correlations of human results with LLM results under one-shot RSA-based CoT prompting.
    \label{tab:cor_table}}
\end{table*}
 
We closely follow the procedure and the scenarios presented in \cite{kao2014nonliteral}, about three daily items: an electric kettle, a watch, and a laptop. We study three LLMs in our experiments: GPT-4o-mini, Claude-3.5-sonnet, and and Gemini-1.5-pro. We sample responses from the LLMs with temperature $\tau=1$ for $n=10$ times for each query and average predictions across runs.\footnote{All materials and data are available at \href{https://sites.google.com/view/pragmatic-lms/home}{https://sites.google.com/view/pragmatic-lms/home}.} 


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/expt1hyp.pdf} 
    \caption{Probability of hyperbolic interpretation, i.e., $u > s$, averaged over sharp and round values of $u$. 
    }
    \label{fig:hyperbole}
\end{figure*}


\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/prag2.pdf} 
    \caption{Bias towards pragmatic halo interpretation, calculated by subtracting the probability of a fuzzy interpretation from the probability of the exact interpretation.
    } 
    \label{fig:halo}
\end{figure*}

\subsection{Experiment 1: Hyperbole and Halo}
In this experiment, we examine how LLMs interpret price-related utterances, comparing their behavior to human patterns. For hyperbole understanding, we expect LLMs to assign lower probabilities to literal interpretations when they are contextually implausible—for instance, the likelihood of a literal \$10,000 interpretation should be low when discussing an electric kettle's price. To assess pragmatic halo effects, we compare interpretations of sharp versus round numbers, hypothesizing that exact interpretations should be more probable for precise utterances (e.g., ``\$51'') than for round ones (e.g., ``\$50''). To quantify human-likeness, we correlate the LLMs' probability distributions over different price states $s$ given an utterance $u$ with human judgments collected by \citet{kao2014nonliteral}, for both hyperbolic and halo effects.
%Further, the likelihood of hyperbole should increase with higher uttered prices. 

\paragraph{Materials and Procedure.}  
The prompts for all experiments were kept as close as possible to original human experiments. Following \citet{kao2014nonliteral}, we used the following sets of price states and utterances $U = S$ = \{50 + k, 500 + k, 1000 + k, 5000 + k, 10000 + k\}, with $k \in \{0, 1, 2, 3\}$ to create exact and round prices.
The same procedure was applied to all three items.
For each utterance with $u \in U$ of the form ``It cost \$\textit{u}.'', the LLMs were prompted to predict the probability that the item had each price $s \in S$. The probabilities were then renormalized over $S$ for each $u \in U$. 

First, we use a \textit{zero-shot} prompt to generate probabilities of possible price states, given the utterance; examples are presented in \autoref{tab:prompts} (1: Hyperbole \& Halo). 
Second, we guide the models using a \textit{one-shot chain-of-thought (CoT) prompt} \citep{nye2021show,wei2022chain}. We construct this prompt by translating the computational steps of the RSA model into natural language reasoning for an example scenario \citep[following][]{prystawski2023psychologically}. This RSA-inspired chain-of-thought is appended to our original system prompt as shown in \autoref{tab:prompts} (1), followed by the context and target task.\footnote{The full CoT prompt is in our supplementary materials.} 

\paragraph{Results.}
Results from our zero-shot evaluation reveal significant disparities between LLM and human interpretations of price-related utterances, as shown in \autoref{tab:cor_table}. 
When examining correlations with human data, we find that LLMs generally default to literal interpretations, with even the best-performing model (Claude-3.5-sonnet) achieving only moderate correlation. Different models exhibited distinct behavioral patterns: GPT-4o-mini tended to assign inflated probabilities to individual utterance-meaning pairs, while Gemini-1.5-pro generally exhibited a bimodal distribution of ratings at the ends of the scale.\footnote{0-shot distributional results are in the supplementary materials.}
For hyperbolic interpretation (\autoref{fig:hyperbole}), we analyzed the probability of hyperbolic meaning by summing probabilities of states where the utterance exceeds the true state ($u > s$), averaging across both round and sharp values (e.g., \$50 and \$49). Only Claude-3.5-sonnet demonstrated a consistent pattern of increasing hyperbolic interpretation probability with higher utterances, though this pattern matched human behavior most closely in the electric kettle domain (cf. \autoref{fig:hyperbole}, left). Other models consistently underestimated hyperbolic interpretations compared to human benchmarks.

The halo effect analysis (\autoref{fig:halo}) revealed an even more striking divergence from human behavior. We quantified halo bias by calculating the difference between exact interpretation probabilities ($s = u$) and fuzzy interpretation probabilities ($s \neq u$ and $s \in [u-3, u+3]$).
While humans showed a small preference for exact interpretations with sharp numbers, LLMs displayed a large effect in the opposite direction, favoring exact interpretations for round numbers. 
These findings demonstrate that contemporary LLMs fail to capture human-like pragmatic reasoning in hyperbole and halo interpretation.

\paragraph{RSA-like Chain-of-Thought.} We explored if a one-shot Chain of Thought (CoT) prompt, implementing the computational process of an RSA model would make LLM responses more human-like. We found that the 1-shot RSA prompt improved correlations between model predictions and human data for GPT-4o-mini and Gemini-1.5-pro (\autoref{tab:cor_table}, 1-shot RSA CoT). 
Interestingly, additional ablation studies showed that prompts explaining only a few components of the RSA model (e.g., mentioning only speaker goals or only price priors) achieved comparable  improvements (see supplementary material for full details).
These ablation results suggest that while RSA-inspired prompting can improve LLM performance, the minimal components sufficient for this improvement differ from the full computational process required to explain human behavior.


\subsection{Experiment 2: Affective Subtext}
Next, we assess the inferred probability that a speaker thinks an item is strikingly expensive (\ie expressed affect), given the description of the true price $s$ and the speaker's statement $u$. If the LLMs interpret hyperbole as conveying affect, the likelihood of affect will be higher for hyperbolic utterances (i.e., $u > s$) than for literal utterances (i.e., $u = s$).

\paragraph{Materials and Procedure.} 
We use the same procedure as in Experiment 1 to retrieve the probability of affect, given a \textit{zero-shot prompt} as exemplified in \autoref{tab:prompts} (2). We use the same sets $S$ and $U$ as in Experiment 1.
Following the original experiment, we then round all the states, since we do not predict differences in affect between round and sharp utterances, and calculate the average probabilities of affect for literal utterances (where $s=u$) and hyperbolic utterances (where $u > s$). %We don't use trials where $u < s$.

\paragraph{Results.}
Results are shown in \autoref{fig:expt2}. While humans in the experiment from \citet{kao2014nonliteral} robustly inferred a distinction between literal and hyperbolic utterances, predicting higher probability of affect given hyperbolic than literal utterances (leftmost facets), LLMs did not. GPT-4o-mini overestimated affect compared to humans, mostly collapsing across literal and hyperbolic utterances. Gemini-1.5-pro treated literal and non-literal utterances more distinctly, but also overestimated affect. Claude was more conservative than humans for hyperbolic utterances, but overestimated affect for literal utterances, with an opposite pattern to humans.

Overall, LLMs did not capture human patterns well when predicting affect probability, given the true price, $s$, and the uttered price, $u$. This suggests that LLMs do not map between utterances and affect in a human-like way. This failure may be ancillary to the failure to capture hyperbolic interpretations or may reflect further difficulties with affect (though see \citet{gandhi2024human}, where LLMs demonstrate some human-like affective cognition in other contexts).

\begin{table}[ht!]
\centering
\begin{tabular}{llll}
\toprule
 \textbf{LLM} & GPT & Claude & Gemini  \\ \midrule
 Price prior  & 0.889 & 0.93 &  0.92  \\ 
 Affect prior & 0.95 & 0.973 & 0.779 \\
 \bottomrule
\end{tabular}
\caption{Correlations of human judgments and different LLM predictions, for price and conditional affect prior probabilities of different prices, across items.
These priors were used to fit the respective LM-RSA models in Experiment 3. \label{tab:priors}}
\end{table}

\subsection{Experiment 3: Price and Affect Priors}

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/prag3.pdf} 
    \caption{Probability of speaker affect (y-axis), given a price $s$ and an utterance $u$, predicted by LLMs with zero-shot prompting (with $\tau=1$) and by humans in Experiment 2 (columns), for different items (rows). Affect is rated by for literal utterances where $u=s$ and hyperbolic utterances where $u > s$. 
    }
    \label{fig:expt2}
\end{figure*}
Given that Experiments 1 and 2 revealed significant differences between LLM and human behavior in processing hyperbole, halo effects, and affective predictions, we designed Experiment 3 to investigate a potential root cause: the accuracy of LLMs' prior knowledge about price distributions and price-affect relationships. Previous work by \citet{kao2014nonliteral} has demonstrated that these priors strongly influence human pragmatic inference. Our experiment focuses on two key aspects: (1) the probability distributions that LLMs assign to different price states for each item, and (2) their predictions of affective responses conditional on item prices. By comparing these model-generated priors against human benchmarks, we can assess whether deficiencies in base knowledge might explain the models' poor performance in pragmatic reasoning tasks. 

Strong correlations between LLM and human probability estimates would suggest that pragmatic failures stem from reasoning mechanisms rather than knowledge gaps, while weak correlations would point to fundamental limitations in the models' basic priors about price and affect. By fitting an RSA model using LLM-generated priors, we can quantitatively assess the models' internal consistency—specifically, whether their predictions align with their own stated priors. This analysis provides a systematic approach for isolating reasoning deficiencies independent of the accuracy of the priors themselves. To enable even more precise analysis of the LLMs' priors and reasoning, we additionally fit an RSA model incorporating both LLM-generated priors and conditional utterance likelihoods. If both RSA models demonstrate strong alignment with human data, the reasoning deficiency could be localized specifically to the model's ability to reason about a speaker's intentions.

\paragraph{Materials and Procedure.} 
We use a fixed set of price states $S$ = \{50 + k, 500 + k, 1000 + k, 5000 + k, 10000 + k\}, where $k$ was  selected from the set \{0, 1\}. 
For price priors, we retrieve LLM predictions with the \textit{zero-shot} prompt asking the LLM to assess the probability of each price $s \in S$ (\autoref{tab:prompts} (3a)). We renormalize the predictions over all prices $S$ for each item. 
To retrieve priors over affect, we prompt the LLM \textit{zero-shot} to provide the probability that a person thinks an item is expensive, given the price $s\in S$ of that item (\autoref{tab:prompts} (3b)). We treat the predictions for each price $s$ as the probability of affect $P(a \mid s)$. 
To retrieve the conditional probabilities of different utterances, we construct a prompt verbalizing the state and different goals of the RSA speaker $S_1$.\footnote{Details can be found in the supplementary materials.}  

Finally, to investigate to which extent LLMs are consistent with their own priors, we use the predicted \textit{LLM priors} for parameterizing the $P_{S}$ and $P_A$ in the RSA model.
We call the resulting models \textit{LM-RSA} and implement them using a probabilistic programming language WebPPL \citep{dippl}. 
\begin{figure*}[tbp]
    \centering
    \includegraphics[width = 0.75\textwidth]{figs/prag4.pdf} 
    \caption{Correlation of predicted probabilities of each pair of $(u, s)$. The plots in the upper panel show predictions of the RSA model with LLM priors (x-axis) against predictions of the same LLM under zero-shot prompting (y-axis). The plot in the lower panel shows predictions of the RSA model with LLM priors (x-axis) against human results (y-axis).}
    \label{fig:lm-rsa-claude-gemini}
\end{figure*}

\paragraph{Results.}

 We find that LLM-predicted priors show strong correlation with human data ($r>0.7$) across price distributions and affect relationships (\autoref{tab:priors}). This indicates that LLMs possess the prior knowledge that should, in principle, enable them to perform human-like pragmatic inference for both hyperbole and halo effects. We observed a systematic relationship between prior accuracy and zero-shot performance: models with stronger correlations to human priors (progressing from Gemini to Claude) demonstrated correspondingly better zero-shot performance. However, this alignment in prior knowledge, while necessary, proved insufficient to guarantee human-like pragmatic reasoning under prompt-based evaluation.


 Do deviations in interpretation nonetheless derive from the small deviations in prior knowledge?
 We compared models' zero-shot behavior against predictions from RSA models fitted to each LLM's own priors (\autoref{fig:lm-rsa-claude-gemini}, upper panel). The results exposed a fundamental inconsistency: LLMs' zero-shot predictions showed relatively weak correlations with their corresponding RSA predictions, with even the best-performing model (GPT-4o-mini) achieving only moderate correlation (0.635). Yet, these same RSA models showed strong correlations with human judgments (\autoref{fig:lm-rsa-claude-gemini}, lower panel) showing that LLM priors are aligned with human priors for price and affect  ($R>0.9$). The LM-RSA models which also included LLM-generated utterance probabilities showed comparable correlations with human judgments  ($R>0.7$).\footnote{Full results are provided in the supplement.}
 This suggests that the challenge in achieving human-like pragmatic reasoning lies not in the models' priors, but in their ability to systematically apply them during inference.
