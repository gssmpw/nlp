
\section{\method} %: Effective Tool Unlearning for LLMs


We develop \method--an effective tool unlearning approach that removes the capability of using tools marked for unlearning ($\mathcal{T}_f$) or solving tasks that depend on them, while preserving the ability of using the remaining tools ($\mathcal{T}_r$) and performing general tasks such as text and code generation. \method implements three key properties for effective tool unlearning: \looseness-1

% \subsection{Required Properties for Effective Tool Unlearning}
\subsection{Tool Knowledge Deletion}
% The tool-augmented model $f$ gains its knowledge of \tf through tool learning. 
Unlearning requires completely removing the knowledge of \tf that $f$ gained during tool learning, ideally as if \tf had never been part of the training set. In other words, knowledge about \tf is successfully removed if the unlearned model $f'$ has no more knowledge than the tool-free model $f_0$ about \tf. \looseness-1

\begin{definition}[Tool Knowledge Deletion (TKD)]
Let $t_i \in \mathcal{T}_f$ denote a tool to be unlearned and $g$ be a function that quantifies the amount of knowledge a model has about a tool. The unlearned model $f'$ satisfies tool knowledge deletion if:
\begin{equation}\label{eq:prop1}
    \mathop\mathbb{E}_{t_i \in \mathcal{T}_f} [ g(f_0, t_i) - g(f', t_i) ] \geq 0.
\end{equation}
\end{definition}
% so that $f'$ retains no more knowledge of $T_f$ than $f_0$.
This formulation allows users to control the extent of knowledge removal from $f'$. For instance, when we unlearn a ``malicious'' tool that calls a malignant program, we may require $f'$ retains no knowledge of this tool, i.e. $g(f', t_i) = 0$. In less critical cases, users can choose to reset $f'$'s knowledge to {\em pre}-tool augmentation level, i.e. $g(f', t_i) = g(f_0, t_i)$

To measure tool knowledge in LLMs, we follow previous works that used prompting to probe LLMs' knowledge~\citep{gpt3,singhal2023large}, i.e. adopting the output of LLMs as their knowledge on a given tool. For each $t_i \in \mathcal{T}_f$ and its associated demonstrations $\{ \mathcal{Q}_i, \mathcal{Y}_i \}$, we query the tool-free LLM $f_0$ with $\mathcal{Q}_i$ and collect its responses $\mathcal{Y}'_i = f_0(Q_i)$. Since $f_0$ has never seen $t_i$ or $\{ \mathcal{Q}_i, \mathcal{Y}_i \}$, $\mathcal{Y}'_i$ represents the \textbf{tool-free response}. We then constrain the unlearned model $f'$ to generate responses similar to $\mathcal{Y}'_i$ to prevent it from retaining knowledge of $t_i$.


% \paragraph{Various levels of knowledge removal}
% Knowledge removal can happen in different cases for tool learning / unlearning. 
% \begin{itemize}
%     \item do not choose any tool.
%     \item choose a different tool. 
%     \item choose the right tool but wrong arguments. 
%     \item say I don't know how to solve this task based on internal knowledge / learned tools. Depends on evaluation.
% \end{itemize}
% Note that the difference between 1) and 4) is that 1) predicts that $x_i$ requires no tool to solve. While 4) encourages the LLM to answer "I don't know.".

% Most existing tool-augmented LLMs are trained in a Supervised Fine-Tuning manner (SFT), where language modeling objective is optimized query-response pairs $(q_i, y_i)$.


\subsection{Tool Knowledge Retention}
The unlearning process should preserve model's knowledge of tools in $T_r$. Ideally, all knowledge gained on $T_r$ during tool learning should be retained after unlearning. 


\begin{definition}[Tool Knowledge Retention (TKR)]
Let $t_m \in T_r$ denote a retained tool, and let $g$ be a function that quantifies the amount of knowledge a model has about a tool. The unlearned model $f'$ satisfies tool knowledge retention if:\looseness-1
\begin{equation}
    \mathop\mathbb{E}_{t_m \in \mathcal{T}_r} [ g(f, t_m) - g(f', t_m) ] = \epsilon,
    \label{eq:prop2}
\end{equation} 
where $\epsilon$ is an infinitesimal constant, so that $f'$ retains the same knowledge of tools in $T_r$ as the original model $f$.
\end{definition}
For effective tool knowledge retention, $f'$ is further fine-tuned using demonstrations associated with $\mathcal{T}_r$, or, more practically, a subset of $\mathcal{T}_r$ proportional to $\mathcal{T}_f$ for efficiency.



\subsection{General Capability Retention via Task Arithmetic}
Optimizing the above objectives can lead to effective unlearning, but it may not be sufficient to maintain the general capabilities of the unlearned model $f'$. As a foundation model, $f'$ is expected to retain abilities such as text and code generation, question answering, instruction-following, and basic mathematical reasoning. These capabilities either existed in $f_0$ prior to tool augmentation or do not depend on specific tools. Therefore, preserving the general capabilities of $f'$ is essential to guarantee that tool unlearning does not compromise the overall functionality of the model. 

\begin{definition}[General Capability Retention (GCR)]
Let $\mathcal{T}_G$ denote the general tasks used to evaluate LLMs. The unlearned model $f'$ satisfies general capability retention if it preserves the knowledge on $T_G$ that it originally obtained prior to tool learning:
\begin{equation}
    \mathop\mathbb{E}_{t_g \in \mathcal{T}_G} [ g(f_0, t_g) - g(f', t_g) ] = \epsilon,
    \label{eq:prop3}
\end{equation} where $\epsilon$ is an infinitesimal constant.
\end{definition}

We propose to use task arithmetic~\citep{ilharco2023editing,barbulescu2024textual} as an efficient and effective approach to preserving the general capabilities of the unlearned model. Our objective is that $f'$ retains as much general knowledge as $f_0$, the instruction tuned LLM trained from a randomly initialized model $f_R$. 
Let $\theta_0$ and $\theta_R$ denote the parameters of $f_0$ and $f_R$ respectively. The difference vector $\theta_0 - \theta_R$ captures the direction of general knowledge acquisition. We apply this adjustment to $\theta'$ (the parameters of $f'$) to preserve its general knowledge:
\begin{equation}
    \theta'^* \leftarrow \theta' + (\theta_0 - \theta_R).
\end{equation}
% This approach allows $f'$ to retain its general capabilities while effectively unlearning specific tools.

% This can be achieved through additional pre-training or instruction tuning. However, such explicit training methods may pose two difficulties in practice. 1) The pre-training or instruction tuning dataset may not be easily accessible at the stage of tool unlearning, making explicit training impossible. 2) Training on a subset of pre-training / instruction tuning dataset together with $ D^0_f \cup D_r $ at the same time may become prohibitively expensive and difficult, given the distinctiveness of these datasets. To account for these difficulties, we adopt task arithmetic~\citep{} to maintain the general utilities of $f'$.

\paragraph{Why Task Arithmetic?}
% While knowledge in LLMs can be highly nonlinear, task arithmetic assumes a linear transformation in the parameter space, which may not always hold. In addition, the vector difference $(\theta_0 - \theta_R)$ may over-correct or under-adjust general abilities, which may leave some tool-related knowledge in the model.
%
% Despite these limitations, 
Task arithmetic is efficient, practical, effective for preserving general capabilities~\citep{ilharco2023editing,barbulescu2024textual}: 
\textbf{Efficiency}: the vector operation does not scale with dataset size, making it significantly more efficient than retraining on large datasets. 
% This cost does not scale with the size of the dataset, which can be considered as static and offline. While explicitly training on the pre-training and instruction-tuning datasets is significantly more expensive.
% 
\textbf{Practicality}: general capabilities obtained from pre-training and instruction tuning~\citep{zhou2024lima} are often impractical to replicate due to the size and limited availability of data--even in some open-source LLMs~\citep{touvron2023llama2}, the actual pre-training data is not fully open-source. In addition, reintroducing general knowledge from alternative datasets can lead to data imbalances and distributional biases. 
%
\textbf{Effectiveness}: applying $\theta_0 - \theta_R$ largely restores the foundational abilities of $f'$, such as text generation and instruction-following, without requiring expensive and time-consuming retraining on large datasets.



\subsection{Training Details}
To obtain the unlearned model $f'$, we solve:
\begin{equation}
    % \theta'^* = \arg \min_{\theta'} \\
    % \underbrace{\mathbb{E}_{t_i \in \mathcal{T}_f} [ g(f_0, t_i) - g(f', t_i) ]}_{\text{Optimization}} + \underbrace{\mathbb{E}_{t_m \in \mathcal{T}_r} [ g(f, t_m) - g(f', t_m) ]}_{\text{Optimization}} + \\ 
    % \underbrace{\alpha (\theta_0 - \theta_R)}_{\text{Task Arithmetic}},
    \theta'^* = \arg \min_{\theta'} \underbrace{\mathbb{E}_{t_i \in \mathcal{T}_f} [ g(f_0, t_i) - g(f', t_i) ]}_{\text{knowledge deletion of }\mathcal{T}_f} + \underbrace{\mathbb{E}_{t_m \in \mathcal{T}_r} [ g(f, t_m) - g(f', t_m) ]}_{\text{knowledge retention of }\mathcal{T}_r},
\end{equation} 
and once the optimized model parameters $\theta'^*$ are obtained, we apply task arithmetic to reinforce general capabilities:
\begin{equation}
    \theta'^* = \underbrace{\theta'^*}_{\text{post-optimization weights}} + \underbrace{\alpha (\theta_0 - \theta_R)}_{\text{knowledge retention of }\mathcal{T}_G},
\end{equation} 
where $\alpha$ is a hyperparameter to control the magnitude of task arithmetic. 
% The loss function $L$ depends on the specific training method. 
% Following previous works, $f'$ is initialized as $f$ to maintain maximum knowledge retention on $T_r$.
% , the specific choice of optimization method loss function depends on the training method to optimize for $\theta'^*$. 
The above formulation provides flexibility in training \method using various existing paradigms, including 
supervised fine-tuning (SFT)~\citep{alpaca}, 
direct preference optimization (DPO)~\citep{rafailov2023direct}, 
reinforcement learning from human feedback (RLHF)~\citep{ouyang2022training}, 
parameter-efficient fine-tuning (PEFT)~\citep{he2022towards,su-etal-2023-exploring}, or
quantization~\citep{8bit_quant,ma2024era} techniques. 
Below we describe two variants of \method:
% \vspace{-20pt}
\begin{itemize}
\itemsep0pt
    \item \textbf{\method-SFT} fine-tunes $f$ using language modeling loss. On forget tools $\mathcal{T}_f$, we replace the original responses $\mathcal{Y}_f$ with tool-free responses $\mathcal{Y}'_f$. The samples for $\mathcal{T}_r$ are not modified. 
    % Similar to prior SFT works, we only compute loss on the response and exclude the query part.

    \item \textbf{\method-DPO} uses direct preference optimization (DPO) to prioritize wining responses over losing responses. For $(t_i, \mathcal{Q}_i, \mathcal{Y}_i) \in \mathcal{T}_f$ to be unlearned, we prioritize the corresponding tool-free response $\mathcal{Y}'_i$ over the original response $\mathcal{Y}_i$. For $(t_j, \mathcal{Q}_j, \mathcal{Y}_j) \in \mathcal{T}_r$, the original response $\mathcal{Y}_j$ is prioritized over the tool-free response $\mathcal{Y}'_i$. \looseness-1
    % Therefore, the knowledge of the unlearned model $f'$ on $\mathcal{T}_f$ can be removed without affecting $\mathcal{T}_r$. 

\end{itemize}



\subsection{LiRA-Tool for Tool Unlearning Evaluation}

\paragraph{Challenge}
A key challenge in evaluating tool unlearning is the lack of membership inference attack (MIA) models to determine whether a tool has been truly unlearned. Existing MIA models typically evaluate individual training samples by analyzing model loss, which is insufficient for tool unlearning. 
% , i.e., in case of LLMs, the loss of responses for given queries. 
Unlike sample-level unlearning, tool unlearning focuses on removing abstract parametric knowledge of tools in $\mathcal{T}_f$, not just forgetting specific training samples. The key limitation of sample-based MIA is that the prompt-response pairs $(\mathcal{Q}_f, \mathcal{Y}_f)$ in the training set may not fully represent all aspects of a tool's functionality. As a result, sample-level MIA may ``overfit'' to a limited subset of tool related prompts and fail to holistically assess whether the tool-usage capability have been fully removed from the model's parametric knowledge.\looseness-1  

\paragraph{Solution}\label{sec:lira_tool}
To address the above limitation, we introduce ``shadow samples'', a diverse set of prompt-response pairs to probe various aspects of tool knowledge. 
We prompt GPT4 with different combinations of in-context examples to obtain a comprehensive set of prompt-response pairs with various prompt format, intention, and difficulty requirements. 
These samples will be used to stress-test the unlearned LLM $f'$ beyond the specific training prompts. This approach prevents overfitting to the original training data and provides a more reliable evaluation of whether the tool has truly been forgotten. To implement this, we extend Likelihood Ratio Attack (LiRA)~\citep{lira}, the state-of-the-art MIA approach, to tool unlearning.


\paragraph{Sample-level LiRA}
LiRA infers the membership of a sample $(x, y)$ by constructing two distributions of model losses: $\mathbb{\Tilde{Q}}_{\text{in}}$ and $\mathbb{\Tilde{Q}}_{\text{out}}$ with $(x, y)$ in and out of the model training set respectively. These distributions are approximated as Gaussians, with their parameters estimated based on ``shadow models'' trained on different subsets of the training data. The Likelihood-Ratio Test~\citep{07dc41a8-17bb-36b0-8eb8-d51fd0847411,lira} is then used to determine whether $(x, y)$ is more likely to belong to $\mathbb{\Tilde{Q}}_{\text{in}}$ or $\mathbb{\Tilde{Q}}_{\text{out}}$. For LLMs, the test statistic is given by~\citep{icul} as:
% $p(L(f(x), y) | \mathbb{\Tilde{Q}}_{\text{in/out}})$
\begin{equation}
    \Lambda = \frac{P \Bigl(l \bigl(f(x), y \bigr) | \mathbb{\Tilde{Q}}_{\text{in}}\Bigr)}{P \Bigl(l \bigl(f(x), y \bigr) | \mathbb{\Tilde{Q}}_{\text{out}}\Bigr)} = \frac{\Pi_{(x_i, y_i) \in \mathcal{D}_f} P_U \Bigl(l \bigl(f'(x_i), y_i \bigr)\Bigr)}{\Pi_{(x_i, y_i) \in \mathcal{D}_f} P_{T_r} \Bigl(l \bigl(f(x_i), y_i \bigr)\Bigr)}.
\end{equation} 
% which intuitively queries the loss of $(x, y)$ to determine if $(x, y)$ is more likely to be present in the training set or not.
This approach, however, is insufficient for tool unlearning because it only assesses membership of specific training samples rather than measuring whether the model still retains the capability to use a tool.



\paragraph{LiRA-Tool: Knowledge-level LiRA}
A major limitation of sample-level LiRA is in its reliance on training-set observations, which may not fully capture the knowledge distribution of an entire tool. Therefore, applying LiRA to tool unlearning can lead to overfitting to a specific subset of training prompts and failing to comprehensively assess whether the tool knowledge has been removed. 
%
% is in approximating the distributions of losses $\mathbb{\Tilde{Q}}_{\text{in}}$ and $\mathbb{\Tilde{Q}}_{\text{out}}$ for tools, rather than individual training samples. 
% Simply using the observed data related to a tool in the training set may overfit to specific distribution of observations, and may fail to comprehensively approximate the distribution of the target tool marked for unlearning. 
We address this issue by introducing LiRA-Tool. Instead of relying on observed training samples, we construct a ``shadow distribution'' $\mathbb{P}$ that generates tool-related query-response pairs. This allows us to sample diverse tool-specific prompts that test the model's ability to use the tool. The new likelihood-ratio test is:\looseness-1
% sample a series of ``shadow'' data (query-response pairs) that evaluates the tool using the ability to compute loss and test statistic as follows:
\begin{equation}
    \Lambda = \frac{\Pi_{t_i \in \mathcal{T}_f}\Pi_{(x, y) \in \mathbb{P}_{t_i}} P_U \Bigl(l \bigl(f'(x), y \bigr) \Bigr)}{\Pi_{t_j \in T_r}\Pi_{(x, y) \in \mathbb{P}_{t_j}} P_{\mathcal{T}_r} \Bigl(l \bigl(f(x), y \bigr) \Bigr)},
\end{equation} 
where $\mathbb{P}_{t_i}$ represents the shadow distribution for generating tool-learning samples for tool $t_i$. 
% is the distribution that controls the generation of tool learning samples for $t_i$. 
In practice, we use GPT-4 to generate diverse shadow samples by prompting it with various distinct instructions to ensure that the evaluation set captures more comprehensive aspects of tool knowledge than the training set. Appendix~\ref{sec:prompt_shadow_sample} provides more details.\looseness-1
% for approximating $\mathbb{\Tilde{Q}}_{\text{in}}$ and $\mathbb{\Tilde{Q}}_{\text{out}}$ and performing likelihood-ratio test.


\paragraph{Novelty of LiRA-Tool}
The key novelty in LiRA-Tool in the sue of ``shadow samples,'' which introduce diversity across multiple dimensions.
% , including prompt format, intent, and difficulty. 
By moving beyond limited training prompts, LiRA-Tool ensures that the model loss reflect overall tool-using ability, rather than just sample-level memorization.
% The major difference is that traditional LiRA approximates $\mathbb{\Tilde{Q}}_{\text{in}}$ and $\mathbb{\Tilde{Q}}_{\text{out}}$ with a series of shadow models by controlling which samples
%models 
% are present in training set. 
% however, unlearning a skill (tool) is prioritized 
%over individual samples  
% Consequently, using the original samples may not comprehensively approximate the distribution of $\mathbb{\Tilde{Q}}_{\text{in}}$ and $\mathbb{\Tilde{Q}}_{\text{out}}$. 
% We instead 
% . Such samples differ from each other, which encourages the losses to reflect efficacy in tool using instead of membership of individual training sample. 
% We GPT-4 as the shadow distribution $\mathbb{P}$ due to its superior tool using ability and the diversity of its generation. 
%
Our loss-ratio formulation shares similarities to previous MIAs for sample-level unlearning, such as probability distribution comparison prior- and post-unlearning~\citep{cheng2023gnndelete,cheng2023multimodal} and other adaptations of LiRA 
% which performs likelihood-ratio test over 
using shadow models~\citep{unbound,icul}. However, to the best of our knowledge, this work is the first adaptation of LiRA for detecting tool presence in tool-augmented LLMs. 




% We adapt sample-level MIA into knowledge-level MIA to infer the membership of tools for tool unlearning evaluation; and propose a new method to estimate $\mathbb{\Tilde{Q}}_{\text{in}}$ and $\mathbb{\Tilde{Q}}_{\text{out}}$. 
% based on latent variables. 
% This provides a comprehensive approximation of abstract concepts beyond observed training data. 

\paragraph{Limitations of LiRA-Tool}
Shadow samples obtained from GPT-4 may not fully represent the complexity of the original tool-learning data and can potentially lead to incomplete approximations of the true knowledge distribution.
% LiRA-Tool is still more comprehensive than existing sample-level MIA, because 
However, despite this limitation, shadow samples provide a more comprehensive and consistent evaluation of a model's tool-using abilities compared to relying merely on observed training samples, which are often limited and incomplete. Expanding the diversity and robustness of shadow sample generation is indeed an important direction for future work. % In addition, if the size of the shadow sample is large enough for each tool, it can better approximate the knowledge distribution for the tool.


\begin{table*}[t]
% \setlength{\tabcolsep}{4pt}
\caption{Tool unlearning performances when deleting 20\% of tools on ToolAlpaca. Best and second-best performances are \textbf{bold} and \underline{underlined} respectively. \textit{Original} is provided \textit{for reference only}. Results on other LLMs are shown in Appendix Table~\ref{tab:tool_llama}-\ref{tab:gorilla}.}
\label{tab:main}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ll|ccc|ccccc}
\toprule
& Method & $\mathcal{T}_t (\uparrow)$ & $\mathcal{T}_r (\uparrow)$ & $\mathcal{T}_f (\downarrow)$ & \multicolumn{5}{c}{General Capability $\mathcal{T}_G (\uparrow)$} \\
                & & & & & STEM & Reason & Ins-Follow & Fact & Avg. \\
\midrule
\rowcolor{Gray} & Original (Ref Only) 
             & 60.0 & 73.1 & 75.7 & 31.7 & 17.1 & 22.6 & 25.0 & 24.1 \\
\midrule
\multirow{4}{*}{\rotatebox{90}{General}} 
& \RET & 52.1 & 71.8 & 38.5 & 30.5 & 16.1 & 14.2 & 24.7 & 21.3 \\
& \GA  & 33.3 & 51.4 & 34.6 & 21.4 & 10.4 & 12.9 & 13.1 & 14.5 \\
& \RL  & 50.3 & 70.3 & 37.5 & 26.3 & 16.4 & 13.6 & 25.1 & 20.3 \\
& \SU  & 46.2 & 54.3 & 38.2 & 27.1 & 17.0 & 17.4 & 19.5 & 20.2 \\
\midrule
\multirow{6}{*}{\rotatebox{90}{LLM-Specific}} 
& ICUL & 49.1 & \underline{74.8} & 58.3 & 12.4 &  8.7 &  1.6 &  6.2 &  7.3 \\
& SGA  & 43.5 & 63.0 & 42.1 & 21.5 & 11.6 & 17.0 & 14.7 & 16.2 \\
& TAU  & 43.8 & 61.7 & 42.5 & 22.0 & 17.6 & 22.3 & 21.7 & 20.9 \\
& CUT  & 44.7 & 61.5 & 40.2 & 21.6 & 14.8 & 20.8 & 16.4 & 18.4 \\
& NPO  & 50.8 & 66.9 & \underline{30.1} & 20.7 & 15.3 & 21.9 & 18.9 & 19.2 \\
& \SO  & 50.4 & 68.3 & 33.8 & 31.6 & 17.2 & 21.4 & 20.8 & 22.7 \\
\midrule
\multirow{2}{*}{\rotatebox{90}{Ours}} 
& \method-SFT & \underline{52.7} & 72.1 & \underline{30.5} & 31.3 & 17.5 & 21.7 & 24.1 & \textbf{23.6} \\
& \method-DPO & \textbf{53.4} & \textbf{75.1} & \textbf{28.7} & 31.6 & 16.8 & 20.4 & 23.5 & \underline{23.1} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



% \begin{table*}[t]
% % \setlength{\tabcolsep}{4pt}
% \caption{Tool unlearning performances when deleting 20\% of tools on ToolAlpaca. Evaluation is performed with the specific metric for each tool-augmented LLM on test tools $\mathcal{T}_t$, remaining tools $\mathcal{T}_r$, and unlearned tools $\mathcal{T}_f$, as well as general benchmarks for evaluation LLMs $\mathcal{T}_G$. Best and second best performances are \textbf{bold} and \underline{underlined} respectively. \textit{Original} denotes the tool-augmented LLM prior unlearning and is provided \textit{for reference only}. Results on other LLMs are shown in Appendix Table~\ref{tab:tool_llama}-\ref{tab:gorilla}.}
% \label{tab:main}
% \centering
% \small
% \begin{tabular}{l|ccc|ccccc}
% \toprule
% \textbf{Method} & $\mathcal{T}_t (\uparrow)}$ & $\mathcal{T}_r (\uparrow)}$ & $\mathcal{T}_f (\downarrow)}$ & \multicolumn{5}{c}{\textbf{General Capability} $\mathcal{T}_G} (\uparrow)}$} \\
%                 & & & & \textbf{STEM} & \textbf{Reason} & \textbf{Ins-Follow} & \textbf{Fact} & \textbf{Avg}. \\
% \midrule
% \rowcolor{Gray}\textbf{Original (Prior Un.)} 
%              & 60.0 & 73.1 & 75.7 & 31.7 & 17.1 & 22.6 & 25.0 & 24.1 \\
% \midrule
% \multicolumn{9}{l}{General Unlearning Methods} \\
% \midrule
% \textbf{\RET} & 52.1 & 71.8 & 38.5 & 30.5 & 16.1 & 14.2 & 24.7 & 21.3 \\
% \textbf{\GA}  & 33.3 & 51.4 & 34.6 & 21.4 & 10.4 & 12.9 & 13.1 & 14.5 \\
% \textbf{\RL}  & 50.3 & 70.3 & 37.5 & 26.3 & 16.4 & 13.6 & 25.1 & 20.3 \\
% \textbf{\SU}  & 46.2 & 54.3 & 38.2 & 27.1 & 17.0 & 17.4 & 19.5 & 20.2 \\
% \midrule
% \multicolumn{9}{l}{LLM-Specific Unlearning Methods} \\
% \midrule
% \textbf{ICUL}           & 49.1 & \underline{74.8} & 58.3 & 12.4 &  8.7 &  1.6 &  6.2 &  7.3 \\
% \textbf{SGA}            & 43.5 & 63.0 & 42.1 & 21.5 & 11.6 & 17.0 & 14.7 & 16.2 \\
% \textbf{TAU}            & 43.8 & 61.7 & 42.5 & 22.0 & 17.6 & 22.3 & 21.7 & 20.9 \\
% \textbf{CUT}            & 44.7 & 61.5 & 40.2 & 21.6 & 14.8 & 20.8 & 16.4 & 18.4 \\
% \textbf{NPO}            & 50.8 & 66.9 & \underline{30.1} & 20.7 & 15.3 & 21.9 & 18.9 & 19.2 \\
% \textbf{SOUL-GradDiff}  & 50.4 & 68.3 & 33.8 & 31.6 & 17.2 & 21.4 & 20.8 & 22.7 \\
% \midrule
% \textbf{\method-SFT} & \underline{52.7} & 72.1 & \underline{30.5} & 31.3 & 17.5 & 21.7 & 24.1 & \textbf{23.6} \\
% \textbf{\method-DPO} & \textbf{53.4} & \textbf{75.1} & \textbf{28.7} & 31.6 & 16.8 & 20.4 & 23.5 & \underline{23.1} \\
% \bottomrule
% \end{tabular}
% \end{table*}




% Let $t_m \in T_r$ denote a tool to be unlearned. $g$ is a function that measures the amount of knowledge a model has on a tool. A unlearned model satisfies Knowledge Retention if

% \begin{equation}
%     \mathop\mathbb{E}_{t_m \in T_r} [ g(f', t_m) - g(f, t_m) ] = \epsilon,
%     \label{eq:prop2}
% \end{equation} where $\epsilon$ is an infinitesimal constant.


% \subsection{Knowledge Removal and Retention}

% The most critical part of knowledge removal is to reset $f'$'s knowledge on $T_f$ back to a level that is similar to $f_0$, which has never seen $T_f$. In other words, to unlearn a given tool $t_i \in T_f$, we want to match $g(f', t_i)$ to $g(f_0, t_i)$. For LLMs, researchers usually prompt them to probe their knowledge on a topic or a fact. To this end, we propose to prompt the vanilla model $f_0$ with the ground-truth queries $Q_i$ associated with $t_i$, resulting in $f_0(Q_i)$. If a model outputs contents similar to $f_0(Q_i)$ when prompted $Q_i$, we can regard the model as having no additional tool knowledge than $f_0$, therefore never sees $t_i$. Formally, to unlearn $T_f$, we obtain a knowledge purged dataset

% \begin{equation}
%     D^0_f = \{ (t_i, Q_i, f_0(Q_i)) | \forall t_i \in T_f \}.
% \end{equation}

% As of knowledge maintenance on $T_r$, the remaining dataset $D_r = \{ T_r, Q_r, Y_r \}$ has documented enough amount of knowledge to retain $f'$'s knowledge. 

% To this end, we have constructed a new dataset with $D^0_f$, the dataset with purged knowledge of $T_f$, and $D_r$, the remaining data with original tools and query-response pairs. We then fine-tune $f$ on $ D^0_f \cup D_r $ to obtain $f'$.


% Inspired by the random labeling approach~\citep{amnesiac_2021} in classification tasks, we implement \method in the random labeling \& fine-tuning paradigm, while realizing the above properties for tool learning. The original random labeling approach consists of two steps. Firstly, for each sample in the unlearned data $(x_i, y_i) \in D_f$, replacing $y_i$ with a randomly chosen label $y'_i \neq y_i$, resulting in $D'_f$. Secondly, fine-tune $f$ the corrupted data $D'_f$ and the retained data $D_r$. During training, the knowledge on the unlearned samples is corrupted by encouraging $f'$ to mis-classify $D_f$. Meanwhile, $f'$ is trained on $D_r$ to further strengthen the knowledge that we want to retain.

% For the demonstrations $\{ Q_i, Y_i \}$ associated with tool $t_i$, we first obtain the query the vanilla model $f_0$ with $Q_i$ and obtain the responses $Y'_i = f_0(Q_i)$. Since $f_0$ has never been trained on $\{ Q_i, Y_i \}$, $Y'_i$ is a set of responses with no information on the $t_i$ coming from 
% we encourage model to provide similar responses as the vanilla model prior to tool learning $f_0$, which has never seen $D_f$.  to realize \propone, we encourage

% \paragraph{Various levels of knowledge removal}
% Knowledge removal can happen in different cases for tool learning / unlearning. 
% \begin{itemize}
%     \item do not choose any tool.
%     \item choose a different tool. 
%     \item choose the right tool but wrong arguments. 
%     \item say I don't know how to solve this task based on internal knowledge / learned tools. Depends on evaluation.
% \end{itemize}

% Note that the difference between 1) and 4) is that 1) predicts that $x_i$ requires no tool to solve. While 4) encourages the LLM to answer "I don't know.".

% Most existing tool-augmented LLMs are trained in a Supervised Fine-Tuning manner (SFT), where language modeling objective is optimized query-response pairs $(q_i, y_i)$.

% \subsection{Agent Unlearning Problem Formulation}
% \paragraph{Knowledge Unlearning} aims at unlearning knowledge shared by the entire community of a subset of agents.
% \paragraph{Individual Unlearning} aims at removing $k$ agents from the community.

