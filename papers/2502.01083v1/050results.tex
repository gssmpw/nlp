\section{Experimental Setup} \label{sec:experiment}

\paragraph{Datasets \& Tool-Augmented LLMs}
% We focus on tool-augmented LLMs that are explicitly fine-tuned.
We experiment with the following datasets and their corresponding LLMs:  
\vspace{-7pt}
\begin{itemize}
\itemsep-1pt
\item \textbf{ToolAlpaca}~\citep{tang2023toolalpaca} is an agent-generated tool learning dataset consisting of 495 tools and 3975 training examples. \textbf{ToolAlpaca 7B} is fine-tuned on ToolAlpaca using Vicuna-v1.3~\citep{zheng2023judging}.
\item \textbf{ToolBench}~\citep{qin2024toolllm} consists of more than 16k real world APIs from 49 categories, where each training demonstration involves complex task solving traces. \textbf{ToolLLaMA} is fine-tuned on ToolBench using LLaMA-2 7B~\citep{touvron2023llama2}.
\item \textbf{API-Bench}~\citep{patil2023gorilla} focus on APIs that load machine learning models. \textbf{Gorilla} is fine-tuned on API-Bench from LLaMA 7B~\citep{touvron2023llama1}.
% 4) API-Bank. Lynx-7B is a model fine-tuned on API-Bank.
% 5) API-Blend is .
\end{itemize}
% We choose these datasets because they are fully open source. In addition, there exists other datasets such as API-Bank~\citep{li-etal-2023-api}~and~API-Blend~\citep{basu-etal-2024-api}, but they have not provided accessible training data or instructions for reconstruction.

% There are several types of tool-augmented LLMs:
% \begin{itemize}
%     \item Finetuning-based
%     \item Embedding-based
%     \item In-Context-based
% \end{itemize}

% Tora (\url{https://huggingface.co/llm-agents/tora-13b-v1.0})
% Liquid \url{Liquid1/llama-3-8b-Instruct-liquid-agent-calling}
% ToolLLaMA
% ToolAlpaca \url{https://arxiv.org/abs/2306.05301}
% TALM \url{https://arxiv.org/abs/2205.12255}
% Lynx-7B \url{https://aclanthology.org/2023.emnlp-main.187/}

\paragraph{Setup \& Evaluation}
We use the public checkpoints of the above tool-augmented LLMs as original models--the starting point for unlearning. Then we conduct unlearning experiments with 2--20\% tools randomly selected as $\mathcal{T}_f$.
% We conduct two types of unlearning experiments: 
% \vspace{-7pt}
% \begin{itemize}
% \itemsep-1pt
% \item \textbf{Random Tool Unlearning}, where 2--20\% tools are randomly selected as $T_f$, and 
% \item \textbf{Class-wise Tool Unlearning}, where tools from a specific category, such as all tools tagged as \texttt{Development}-related in ToolAlpaca, are chosen as $\mathcal{T}_f$. Specifically, we focus on unlearning the top 5 largest categories based on class sizes.
% \end{itemize}
We evaluate tool unlearning effectiveness, general capability of tool-unlearned LLMs, and robustness to membership inference attack (MIA). 
%
For \textbf{unlearning effectiveness}, we measure performance on test sets ($\mathcal{T}_T, \uparrow$), forget set ($\mathcal{T}_f, \downarrow$), and remaining set ($\mathcal{T}_r, \uparrow$), where ``performance'' reflects the ability to solve tasks that depend on specific tools, depending on the unique metrics in the original tool-augmented models $f$. 
% , which is different from ``memorization of training sequences'' in prior LLM unlearning works~\citep{jang-etal-2023-knowledge,kassem-etal-2023-preserving,yao-etal-2024-machine,barbulescu2024textual}.
%
For \textbf{general capabilities}, we evaluate the unlearned LLMs on a wide range of tasks: 
college STEM knowledge with MMLU~\citep{hendrycks2021measuring}, 
reasoning ability with BBH-Hard~\citep{suzgun-etal-2023-challenging}, 
instruction-following with IFEval~\citep{zhou2023instruction}, and 
factual knowledge with MMLU~\citep{hendrycks2021measuring}.
%
For \textbf{MIA}, we use the proposed LiRA-Tool; following prior work on LiRA~\citep{icul}, we train the shadow models with forget set size of \{1, 5, 10, 20\} and primarily evaluate the True Positive Rate (TPR) at low False Positive Rate (FPR) (TPR @ FPR = 0.01), where TPR means the attacker successfully detects a tool is present. Therefore, a lower TPR indicates better performance (privacy).

% & \multicolumn{4}{c}{ToolAlpaca-7B} & \multicolumn{4}{c|}{ToolAlpaca-13B} & \multicolumn{4}{c|}{Lynx-7B} & \multicolumn{4}{c}{ToolLLaMA-7B} \\
% & $D_T$ & $D_f$ & $D_r$ & Gen. & $D_T$ & $D_f$ & $D_r$ & Gen. & $D_T$ & $D_f$ & $D_r$ & Gen. \\




\paragraph{Baselines}
As there are no prior works on tool unlearning, we adapt the following unlearning methods to tool unlearning setting (see Appendix~\ref{sec:baseline} for descriptions of the baselines):
general unlearning approaches, including 
\textbf{\GA}~\citep{Golatkar2020EternalSO,yao-etal-2024-machine}, 
\textbf{\RL}~\citep{amnesiac_2021}, and 
\textbf{\SU}~\citep{fan2024salun}; 
and LLM-specific unlearning approaches, including  
\textbf{ICUL}~\citep{icul}, 
\textbf{SGA}~\citep{jang-etal-2023-knowledge,barbulescu2024textual}, 
\textbf{TAU}~\citep{barbulescu2024textual}, 
\textbf{CUT}~\citep{li2024wmdp},  
\textbf{NPO}~\citep{zhang2024negative}, and
\textbf{\SO}~\citep{jia-etal-2024-soul}.
For ICUL~\citep{icul}, we randomly select one example $(q_i, y_i)$ from $\mathcal{T}_f$ and corrupt the output $y_i$ with randomly selected tokens. Then we concatenate this corrupted sequence with other intact sequences as the in-context demonstrations. For all other baselines, we treat all data related to $\mathcal{T}_f$ as unlearning examples and all data related to $\mathcal{T}_r$ as remaining examples. Everything else remains the same for each baseline. 
% See \S\ref{sec:prem} for our discussion on why sample-level unlearning methods are inadequate for effective tool unlearning. 



\section{Results}

% \paragraph{Main results}
% Overall, on average of three datasets, \method-SFT 


\paragraph{Comparison to general unlearning methods}
Our main results in Table~\ref{tab:main} show that, compared to \RET, the best-performing baseline in the general unlearning methods category, \method-SFT outperforms \RET by 0.6, 0.3, 8.0, 2.3 absolute points on \ttest, \tr, \tf, \tg respectively. \method-DPO outperforms \RET by 1.3, 3.3, 9.8, 1.8 absolute points across the same metrics. We note that \GA can effectively unlearn \tf, but it negatively impacts its \ttest and \tr performance. Although \RL and \SU outperforms \GA, they still fall short on \tg compared to \method.


\paragraph{Comparison to LLM-specific unlearning methods}
Existing LLM unlearning methods, despite effective in sample-level unlearning, are prone to under-performing in tool unlearning. Both \method-SFT and \method-DPO outperforms ICUL, SGA, and TAU on \ttest, \tr, \tf and \tg. The only exception is ICUL, which outperforms \method-SFT on \tr by 2.7 absolute points, but is outperformed by \method-DPO on \tr by 0.3 points. The good performance of ICUL on \tr is at the cost of failing to unlearn tools in \tf, which is not desired in tool unlearning. In addition,  ICUL has limited ability of preserving test set performance, it is outperformed by \method-SFT and \method-DPO by 3.6 and 4.3 respectively. Furthremore, it is particularly limited in deletion capacity, i.e. number of unlearning samples that a method can handle. As $|D_f|$ exceeds 10, the performance of ICUL on \ttest significantly degrades. This is while \method can process much larger deletion requests efficiently. 



\begin{figure}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.47\linewidth]{figure/mia.pdf}}
% \vspace{-10pt}
\caption{Measuring tool unlearning with LiRA-Tool.}% \GA and ICUL are best-performing baselines for general and LLM-specific unlearning methods.}
% \vspace{-10pt}
\label{fig:mia}
\end{center}
\vskip -0.2in
\end{figure}

% \begin{table}[t]
% \tiny
% % \setlength{\tabcolsep}{3pt}
% \centering
% \caption{Ablation study of proposed properties on ToolAlpaca. \colorbox{lightred}{Highlighted} are metrics that degrade after removing specific parts of the model.}
% \label{tab:ablation}
%     \begin{tabular}{l|cccc}
%     \toprule
%      & $\mathbf{\mathcal{T}_T (\uparrow)}$ & $\mathbf{\mathcal{T}_r (\uparrow)}$ & $\mathbf{\mathcal{T}_f (\downarrow)}$ & $\mathbf{\mathcal{T}_G (\uparrow)}$ \\
%     \midrule
%     \multicolumn{5}{l}{\textit{\method-SFT}} \\
%     \midrule
%     Full Model & \textbf{57.7} & \textbf{72.1} & \textbf{30.5} & \textbf{23.6} \\
%     \midrule
%      - TKD & 58.1 & 72.4 & \cellcolor{lightred}{65.3} & 23.3 \\
%      - TKR & \cellcolor{lightred}{32.7} & \cellcolor{lightred}{40.2} & 23.1 & 20.1 \\
%      - GCR    & 58.0 & 72.5 & 31.1 & \cellcolor{lightred}{17.5} \\
%     \midrule
%     \multicolumn{5}{l}{\textit{\method-DPO}} \\
%     \midrule
%      Full Model & \textbf{58.4} & \textbf{73.3} & \textbf{28.7} & \textbf{23.1} \\
%      \midrule
%      - TKD & 58.6 & 73.2 & \cellcolor{lightred}{65.9} & 22.7 \\
%      - TKR & \cellcolor{lightred}{40.3} & \cellcolor{lightred}{41.8} & 39.3 & 22.1\\
%      - GCR    & 55.7 & 72.7 & 33.1 & \cellcolor{lightred}{14.3} \\
%     \bottomrule
%     \end{tabular}
% \end{table}


\paragraph{SFT vs. DPO}
DPO outperforms SFT by 0.7, 3.0, and 1.8 on \ttest, \tr, \tf respectively. On \tg, SFT is slightly better than DPO by 0.5 points. However, DPO takes slightly longer time to train, see Figure~\ref{fig:time}. Both optimization methods achieve superior performance over existing approaches. 


% \paragraph{Class-wise Tool Unlearning}
% We then investigate category-wise tool unlearning.



\paragraph{Measuring tool unlearning with MIA}
Following prior practices~\citep{lira,icul}, a lower TPR indicates an unlearned model with better privacy when FPR=0.01. \method-DPO achieves 0.14 TPR, outperforming \RET by 0.01. This advantage is obtained by explicitly prioritizing tool-free responses $f_0(\mathcal{Q})$ over original responses. In addition, \method-SFT achieves comparable performance with \RET, which indicates its effectiveness to protect privacy. Both variants of our method outperforms \GA and ICUL, the best performing %general and LLM-specific 
baselines, achieving 0.21 and 0.18 TPR. This indicates that existing sample-level unlearning approaches are not sufficient for unlearning tools, see Figure~\ref{fig:mia}.


\paragraph{Sequential unlearning}
Tool unlearning requests may arrive in sequential mini-batches. We experiment with sequential unlearning requests by incrementally unlearning 2\%, 5\%, 10\%, and 20\% of tools. \RET, ICUL by design cannot process sequential deletion requests. \method can continue training according to the current deletion request, without having to retrain a new model. When 20\% of unlearning requests arrive in batches, \method can sequentially unlearn each of them. As Figure~\ref{fig:seq} and Table~\ref{tab:main} show, compared to unlearning 20\% at once, the performance does not degrade significantly. 
% At each step, deletion request of 3\%, 5\%, 10\%, 30\% comes in, making the total unlearning ratio 2\%, 5\%, 10\%, 20\%, 50\%.



\begin{table}[t]
\setlength{\tabcolsep}{3pt}
\caption{Ablation study of proposed properties on ToolAlpaca. \colorbox{lightred}{Highlighted} are metrics that degrade after removing specific parts of the model.}
\label{tab:ablation}
\vskip 0.15in
\begin{center}
\begin{tiny}
\begin{sc}
    \begin{tabular}{l|cccc||cccc}
    \toprule
     & \multicolumn{4}{c||}{\method-SFT} & \multicolumn{4}{c}{\method-DPO} \\
     & $\mathbf{\mathcal{T}_T (\uparrow)}$ & $\mathbf{\mathcal{T}_r (\uparrow)}$ & $\mathbf{\mathcal{T}_f (\downarrow)}$ & $\mathbf{\mathcal{T}_G (\uparrow)}$ & $\mathbf{\mathcal{T}_T (\uparrow)}$ & $\mathbf{\mathcal{T}_r (\uparrow)}$ & $\mathbf{\mathcal{T}_f (\downarrow)}$ & $\mathbf{\mathcal{T}_G (\uparrow)}$\\
    \midrule
    Full & \textbf{57.7} & \textbf{72.1} & \textbf{30.5} & \textbf{23.6} & \textbf{58.4} & \textbf{73.3} & \textbf{28.7} & \textbf{23.1} \\
    \midrule
     - TKD & 58.1 & 72.4 & \cellcolor{lightred}{65.3} & 23.3 & 58.6 & 73.2 & \cellcolor{lightred}{65.9} & 22.7 \\
     - TKR & \cellcolor{lightred}{32.7} & \cellcolor{lightred}{40.2} & 23.1 & 20.1 & \cellcolor{lightred}{40.3} & \cellcolor{lightred}{41.8} & 39.3 & 22.1\\
     - GCR    & 58.0 & 72.5 & 31.1 & \cellcolor{lightred}{17.5} & 55.7 & 72.7 & 33.1 & \cellcolor{lightred}{14.3} \\
    \bottomrule
    \end{tabular}
\end{sc}
\end{tiny}
\end{center}
\vskip -0.1in
\end{table}



%     \label{fig:mia}
% \begin{table}[t]
% \setlength{\tabcolsep}{3pt}
% \begin{minipage}{.5\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figure/mia.pdf}
%     \captionof{figure}{MIA performance using LiRA-Tool. \GA and ICUL are best-performing baselines for general and LLM-specific unlearning methods.}
%     \label{fig:mia}
% \end{minipage} \hfill
% \begin{minipage}{.45\linewidth}
%     \captionof{table}{Full parameters vs. LoRA in tool unlearning performances when deleting 20\% of tools on ToolAlpaca. \colorbox{Gray}{Original} denotes the tool-augmented LLM prior unlearning and is provided \colorbox{Gray}{for reference only}.}
%     \label{tab:peft}
%     \centering
%     \small
%     \begin{tabular}{p{6em}|cccc}
%     \toprule
%      & $\mathbf{\mathcal{T}_T (\uparrow)}$ & $\mathbf{\mathcal{T}_r (\downarrow)}$ & $\mathbf{\mathcal{T}_f (\uparrow)}$ & $\mathbf{\mathcal{T}_G} \mathbf{(\uparrow)}$ \\
%     \midrule
%     \rowcolor{Gray}\textbf{Original (Prior Un.)} 
%                             & 60.0 & 73.1 & 75.7 & 24.1 \\
%     \midrule
%     \textbf{Full param} & 52.7 & 72.1 & 30.5 & 23.6 \\
%     \midrule
%     \textbf{LoRA}       & 51.5 & 71.8 & 36.1 & 19.9 \\
    
%     \bottomrule
%     \end{tabular}
% \end{minipage}
% \end{table}


\paragraph{All properties contribute to effective tool unlearning}
Ablation studies in Table~\ref{tab:ablation} show that without Tool Knowledge Removal, performance of \method-SFT and \method-DPO on \tf degrade by -34.8 and -37.2 absolute points respectively. Such significant performance drop is observed for other model properties as well. Therefore, we conclude all proposed properties are necessary for successful at tool unlearning on \ttest, \tr, \tf, and \tg. 


\paragraph{\method functions effectively without access to training data}\label{sec:no_training_data}
In certain unlearning settings, access to the original training data might be restricted, e.g., in healthcare settings or in cases where training data is no longer available due to compliance. In these cases, \method can generate pseudo-samples for tools using the ``shadow samples'' technique developed for LiRA-Tool, see~\S\ref{sec:lira_tool}. Table~\ref{tab:no_training_data} in Appendix~\ref{sec:additional_result} shows that \method can perform tool unlearning effectively, achieving comparable performances to when full access to the exact training data is available.\looseness-1


\paragraph{\method is efficient}
Efficiency is a critical aspect for unlearning. As Figure~\ref{fig:time} in Appendix~\ref{sec:additional_result} illustrates, \method is substantially more efficient than retraining a new model from scratch--saving about 74.8\% of training time on average. In addition, this efficiency gain is relatively consistent as the size of $T_f$ increases. \method-SFT is slightly faster than \method-DPO, as the latter requires a negative sample for each of its prompts.

% \begin{wrapfigure}[]{r}{7cm}
%     \vspace{-20pt}
%     \centering
%   \includegraphics[scale=.5]{figure/time.pdf}
%     \vspace{-25pt}    
%   \caption{Training time of \method, which saves 74.8\% of time on average.}
%   \label{fig:time}
%   \vspace{-20pt}
% \end{wrapfigure}


\paragraph{\method-LoRA is ultra-efficient with good unlearning performance}
We experiment if \method can achieve effective tool unlearning through LoRA~\citep{hu2022lora}, when computing resource is limited. Experiments on ToolAlpaca show that \method-LoRA can achieve 97.7\%, 99.6\%, 84.5\%, and 84.3\% of the performance of \method with full parameter on \ttest, \tr, \tf, \tg on average across SFT and DPO, see Table~\ref{tab:peft} in Appendix~\ref{sec:additional_result}. In addition, it reduces save computational cost by 81.1\% and decreases the training time by 71.3\%.


\paragraph{\method is flexible in choice of tool-free responses}
In (\ref{eq:prop1}), we obtain tool knowledge-free responses from the tool-free LLM $f_0$. However, in cases where $f_0$ is unavailable, \method can still function using any knowledge-free LLM to generate tool knowledge-free responses, such as a randomly initialized LLM $f_R$. Table~\ref{tab:tool_free} compares the performances between these two implementations. While $\theta_0$ consistently outperforms $\theta_R$, using $\theta_R$ is still effective in achieving tool unlearning.\looseness-1 


\paragraph{Why is \method effectiveness?}
We attribute the performance of \method to its three key properties:
(a): Tool Knowledge Removal enables targeted tool unlearning without over-forgetting, unlike \GA and \RET. This is achieved by prioritizing tool knowledge-free responses over tool knowledge-intense responses so that the model forgets tool functionality without excessive degradation.
% This unlearning formulation poses the right strength of forgetting over specific tools, while existing methods either over-unlearn, such as \GA, or does not unlearn sufficiently, such as \RET. 
(b): Tool Knowledge Retention reinforces the knowledge about remaining tools. In fact, re-exposing the model to the original training data can further strengthen their representation. 
(c): General Capability Retention, which maintains or even improves model's general capabilities through an efficient and effective task arithmetic operation. Therefore, precise unlearning, retention of relevant knowledge, and overall model stability are the key factors that contribute to the performance of \method.  

 
% data from $f_0$, a model that has never seen the deleted tools before, and effectively