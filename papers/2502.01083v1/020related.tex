\section{Related work}
\textbf{Unlearning for non-LLM models}: These methods include 
% methods that enforce performing as a randomly initialized model on deleted samples~\citep{Chundawat_Tarun_Mandal_Kankanhalli_2023}, 
% methods that enforce memorizing wrong labels for deleted samples~\citep{amnesiac_2021}, 
methods that focus on pruning before unlearning~\citep{jia2023model} or  
finding salient parameters~\citep{fan2024salun} and manipulating gradients~\cite{pmlr-v134-ullah21a,Hoang_2024_WACV}, 
adversarial methods~\citep{Liu_2023_ICCV,setlur2022adversarial,wei2023shared}, approximation of inverse Hessian~\citep{zhang2024towards}, and data augmentation~\citep{Choi_2024_CVPR}. Other works study unlearning under multimodal setting~\citep{cheng2023multimodal}, image-to-image models~\citep{li2024machine}, and finding the most challenging unlearning subset within a dataset~\citep{fan2024challenging}. Recently, a few works started to benchmark MU performances on unlearning fictitious user profiles~\citep{maini2024tofu}, world knowledge~\citep{jin2024rwku} and a variety of tasks~\citep{cheng2024mubench}.


\textbf{Unlearning for LLMs}: Recently, more attention has been given to LLM unlearning, where gradient ascent is a common technique~\citep{eldan2023whos,jang-etal-2023-knowledge}. \citep{yao-etal-2024-machine} evaluate several traditional unlearning methods on LLMs. KGA~\citep{wang-etal-2023-kga} formulates unlearning as achieving knowledge gap between training data and test data similar to that of training data and deleted data. \citet{yao2023large} proposed to predict if the LLM output is grammatically correct on deleted samples, such that the knowledge is not over unlearned. Other methods include second-order-optimization~\citep{jia-etal-2024-soul}, performing DPO with no positive examples~\citep{zhang2024negative}, and reinforcement learning with a negative reward model~\citep{kassem-etal-2023-preserving}. Unlearning from logits difference~\citep{ji2024reversing} first builds an assisted LLM which memorizes data to be deleted and forgets the retained data, which is later used to derive the unlearned LLM by deviating from the assisted LLM in logits. 
% \citet{liu2024large} propose that corrupting the embeddings of prompts can result in efficient LLM unlearning.


\textbf{Tool-Augmented LLMs}: TAML~\citep{parisi2022talm} used self-play to boost LLMs' performance on math and reasoning tasks. \citet{schick2023toolformer} discovered that LLMs can teach themselves how to use APIs. Recently, efforts have been devoted to building benchmarks to train and evaluate the tool-using ability of LLMs, such as agent-based data generation~\citep{tang2023toolalpaca,li-etal-2023-api}, bootstrapping training data with seed examples~\citep{patil2023gorilla}, modifying existing datasets~\citep{basu-etal-2024-api}, and dataset development with GPT-4~\citep{qin2024toolllm}.