\section{Conclusion}

We introduce Tool Unlearning--a novel machine unlearning task with the goal of unlearning previously learned tools from tool-augmented LLMs. 
% Unlike traditional unlearning, tool unlearning requires the removal of parametric knowledge associated with specific tools while preserving both the modelâ€™s ability to use other tools and its general capability. 
We develop the first tool unlearning approach, \method, that implements three key properties:
{\em tool knowledge deletion}, %for precise forgetting of tools marked for unlearning; 
{\em tool knowledge retention}, %for preserving the knowledge about the remaining tools; and 
{\em general capability retention}. %for maintaining LLM's general capabilities  on a range of general tasks such as text generation.
%
In addition, we introduce LiRA-Tool, the first membership inference attack (MIA) method for evaluating tool unlearning. LiRA-Tool largely addresses the limitations of sample-based MIAs for tool unlearning. 
%
Extensive experiments on several diverse datasets and LLMs show  that \method is an efficient, flexible, and effective tool unlearning method that supports sequential unlearning, maintains strong performance across all key properties, and operates without requiring full access to training data. 
It outperforms existing methods by removing tool knowledge without over-forgetting (as shown in ablation studies), achieving 74.8\% faster training times compared to retraining, and delivering highly effective tool unlearning even in resource-constrained settings with \method-LoRA (which reduces compute costs by 81.1\% and training time by 71.3\%). 
% In addition, \method is effective when the exact training data or the original pre-tool-augmentation LLM is unavailable
% and when we  randomly initialized model instead of the original pre-tool-augmentation model.

In future, we will investigate tool unlearning in dynamically updated LLMs (e.g. API-based LLMs like GPT-4), where we address continuous unlearning challenges. In addition, we will develop adversarial training techniques and robustness evaluation frameworks to prevent unintended tool re-learning or model exploitation.

% (b) adaptive forgetting in LoRA for efficient tool unlearning.\looseness-1


\paragraph{Limitations}
We did not conduct experiments using closed-source LLMs or API-based LLMs. 
% Consequently, our findings may not directly extend to such proprietary models, and further research is needed to investigate the applicability of tool unlearning techniques in these contexts.
%
In addition, this work did not investigate the impact of varying model scales due to the limited publicly-available tool-augmented LLMs. Our experiments were conducted on the 7B scale and the scalability of the proposed tool unlearning approach across models of different sizes and scales is an open question for future investigation.






% \begin{figure}[t]
% \centering
% \begin{minipage}{0.28\textwidth}
%   \includegraphics[width=\textwidth]{figure/time.pdf}
%   \caption{Training time of \method, which saves 74.8\% of time on average.}
%   \label{fig:time}
% \end{minipage} \hfill
% \begin{minipage}{0.68\textwidth}
%   \includegraphics[width=\textwidth]{figure/seq.pdf}
%   \caption{Performance of sequential unlearning on ToolAlpaca.}
%   \label{fig:seq}
% \end{minipage}
% \end{figure}
