\section{Related work}
\textbf{Unlearning for non-LLM models}: These methods include 
% methods that enforce performing as a randomly initialized model on deleted samples____, 
% methods that enforce memorizing wrong labels for deleted samples____, 
methods that focus on pruning before unlearning____ or  
finding salient parameters____ and manipulating gradients____, 
adversarial methods____, approximation of inverse Hessian____, and data augmentation____. Other works study unlearning under multimodal setting____, image-to-image models____, and finding the most challenging unlearning subset within a dataset____. Recently, a few works started to benchmark MU performances on unlearning fictitious user profiles____, world knowledge____ and a variety of tasks____.


\textbf{Unlearning for LLMs}: Recently, more attention has been given to LLM unlearning, where gradient ascent is a common technique____. ____ evaluate several traditional unlearning methods on LLMs. KGA____ formulates unlearning as achieving knowledge gap between training data and test data similar to that of training data and deleted data. ____ proposed to predict if the LLM output is grammatically correct on deleted samples, such that the knowledge is not over unlearned. Other methods include second-order-optimization____, performing DPO with no positive examples____, and reinforcement learning with a negative reward model____. Unlearning from logits difference____ first builds an assisted LLM which memorizes data to be deleted and forgets the retained data, which is later used to derive the unlearned LLM by deviating from the assisted LLM in logits. 
% ____ propose that corrupting the embeddings of prompts can result in efficient LLM unlearning.


\textbf{Tool-Augmented LLMs}: TAML____ used self-play to boost LLMs' performance on math and reasoning tasks. ____ discovered that LLMs can teach themselves how to use APIs. Recently, efforts have been devoted to building benchmarks to train and evaluate the tool-using ability of LLMs, such as agent-based data generation____, bootstrapping training data with seed examples____, modifying existing datasets____, and dataset development with GPT-4____.