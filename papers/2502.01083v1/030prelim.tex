\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figure/tool_unlearning.pdf}
  \vspace{-10pt}
  \caption{Tool Unlearning and the proposed \method approach. \textbf{(a)}: Illustration of tool learning and tool unlearning. Learned tools may be requested to be unlearned due to many reasons, such as tools being insecure, restricted, or deprecated. \textbf{(b)}: Differences between tool unlearning and traditional sample unlearning, in terms of objective and training data. \textbf{(c)}: Proposed method \method. We encourage the unlearned model $f'$ to follow the tool-free LLM $f_0$ which has never seen $T_f$ before. Meanwhile, we maintain its ability on $T_r$ and general tasks by matching the capabilities of tool-augmented model $f$ through task arithmetic.}
  \label{fig:model}
    \vspace{-10pt}
\end{figure*}
% https://www.flaticon.com/search?word=tool

\section{Tool Unlearning: Preliminaries}\label{sec:prem} %: A Novel Machine Unlearning Task}

To understand tool unlearning, we first introduce the concept of ``tool learning,'' see Figure~\ref{fig:model}(a). Let $\mathcal{D} = \{ \mathcal{T}, \mathcal{Q}, \mathcal{Y} \}$ be a dataset with $N$ tools $\mathcal{T}$, and $(\mathcal{Q}, \mathcal{Y})$ denotes query-output examples that demonstrate how to use the tools in $\mathcal{T}$. 
% and corresponding ``demonstrations'' $(\mathcal{Q}, \mathcal{Y})$, where $\mathcal{Q}$ is the query set and $\mathcal{Y}$ is its corresponding output set--$(\mathcal{Q}, \mathcal{Y})$ are labeled examples demonstrating how to use tools in $\mathcal{T}$. 
Each tool $t_i \in \mathcal{T}$ may have one or more demonstrations $\{\mathcal{Q}_i, \mathcal{Y}_i\}$, $|\mathcal{Q}_i| = |\mathcal{Y}_i| \geq 1$. 
Starting with an instruction-tuned LLM $f_0$, 
% which has not been trained on using tools, 
a tool learning algorithm explicitly trains $f_0$ on $\mathcal{D}$ and results in a {\em tool-augmented} model $f$ capable of using the $N$ tools in $\mathcal{T}$. We note that prior to explicit tool learning, the LLM $f_0$ may already have some tool-using capabilities such as performing basic arithmetic operations. 

% An example of tool-augmented models is WebGPT~\citep{webgpt}, which mimics human behavior in answering open-ended questions using a text-based web browser to retrieve information and improve its responses.


\paragraph{Problem Definition:} Tool unlearning aims to remove specific tools from tool-augmented LLMs. Let $\mathcal{D}_f = \{ \mathcal{T}_f, \mathcal{Q}_f, \mathcal{Y}_f \}$ denotes $k < N$ tools and their corresponding demonstrations to be unlearned from the tool-augmented model $f$, and $\mathcal{D}_r = \mathcal{D} \backslash \mathcal{D}_f = \{ \mathcal{T}_r, \mathcal{Q}_r, \mathcal{Y}_r \}$ denotes the remaining tools and their demonstrations to retain. The goal is to obtain an unlearned model $f'$ that has limited knowledge on using $\mathcal{T}_f$ tools--can no longer perform tasks involving $\mathcal{T}_f$ tools--while preserving $f$'s ability to use $\mathcal{T}_r$ tools as before.
% , i.e. on how to solve tasks depending on $T_r$ as prior to unlearning. 

\paragraph{Use Cases of Tool Unlearning}\label{sec:app}
The ability to forget learned tools is essential in real-world applications. For example, 
addressing the insecure tools from untrustworthy developers that could be exploited by adversarial attackers;
% . Examples include cyber attack tools or unaligned AI models that generate harmful content. Adversarial attackers may exploit such tools to compromise the security of models and privacy of users. 
%
removing tools restricted by their providers due to copyright or privacy concerns, such as APIs that start allowing unauthorized downloads of book chapters or releasing publications that users did not author; 
%
unlearning broken or deprecated tool that lead to failed operations or corrupted outputs;
%
unlearning tools that may no longer be needed; 
%
and managing limited model capacity, where new versions of tools necessitate replacing outdated ones. More examples of parameter-level tool unlearning are provided in Appendix~\ref{sec:example}.

% : The tools a model needs to master may change over time due to capacity limitations or evolving requirements.


% \begin{itemize}
%     \item Insecure Tools: Many tools are not created, maintained, or published by well-established, trustworthy developers. Examples include cyber attack tools or unaligned AI models that generate harmful content. Adversarial attackers may exploit such malicious tools, compromising the security of models and users.
    
%     \item Restricted Tools: Tools or their associated usage examples may become unavailable due to restrictions imposed by data providers, including copyright issues or the Right to Be Forgotten (RTBF). Examples include APIs that download unauthorized book chapters or announce publications that users did not author.
    
%     \item Broken Tools/Dependencies: Tools may become broken, deprecated, or fall out of maintenance. Continuing to rely on these tools can result in undesired behavior, such as failed tool calls and corrupted outputs.
    
%     \item Useless Tools: The requirement for certain tools may be temporary. In other words, some tools may no longer be needed at a specific point in time, leading to unlearning requests in this scenario.
    
%     \item Limited Model Capacity: The tools a model needs to master may change over time due to capacity limitations or evolving requirements.
% \end{itemize}


\paragraph{Difference to Standard Unlearning Tasks} \label{sec:diff}
Tool unlearning is different from sample-level unlearning as it focuses on removing ``skills'' rather than individual training samples. 
%
\textbf{Objective}: sample-level unlearning aims to reduce the memorization likelihood or extraction probabilities of specific data samples $(q_i, y_i)$~\citep{jang-etal-2023-knowledge}, which is useful for removing copyrighted or private information. In contrast, tool unlearning targets the ``ability'' to solve tasks using tools marked for unlearning ($T_f$). For example, generating $f'(q_i)$ that is superficially different from $y_i$ (while preserving the semantics) is considered successful for sample-level unlearning. However, for tool unlearning, preserving skills and semantics indicate maintained knowledge on $T_f$, which makes unlearning a failure. Figure~\ref{fig:model}b shows successful tool unlearning, where the ability to use the API is forgotten, despite the high lexical memorization between output of the unlearned model and the training data.
% overlap. 
% This difference also lead to different evaluation methods, which we detail in \S\ref{sec:experiment}. 
In addition, selectively removing knowledge from tool-augmented models is a challenging tasks because changes to one tool may unexpectedly affect the model's ability to use other tools--referred to as {\em ripple effect} in fact editing literature~\citep{ripple_effect,gu2024model}. Furthermore, LLMs are general models that can conduct a wide range of tasks beyond tool using, and this ability must be retained. 
%
\textbf{Evaluation}: metrics like sequence extraction likelihood and perplexity are standard in sample-level unlearning. For tool unlearning, success is measured by the ability to forget or retain tool-related skills, which is more appropriate.
%
\textbf{Data}: sample-level unlearning require access to all individual samples marked for unlearning, while tool unlearning does not. This aligns with ``concept erasure'' in diffusion models~\citep{gandikota2023erasing,kumari2023conceptablation} and zero-shot unlearning~\citep{chundawat2022zero} but differs from traditional LLM unlearning~\citep{yao-etal-2024-machine}. Later we demonstrate this in \S~\ref{sec:no_training_data}.
% As long as the target tools are unlearned, we can use any dataset or choose not to use any data. 


\paragraph{Importance of Parameter-Level Tool Unlearning}
We observe that one can naively block tools at the prompt-level or remove tools from the tool set without updating the LLM. However, these shortcut solutions are insufficient to remove tool knowledge. 
\emph{Firstly}, the knowledge on $\mathcal{T}_f$ persists in the parameters of $f'$, leaving the LLM still under threat. Adversarial agents / attackers can exploit this knowledge, which also bypasses prompt-level restrictions. Since existing LLMs do not guarantee 100\% adherence to instructions or contextual information~\citep{zhou2023instruction,zeng2024evaluating}, they may ignore the tool set provided in the prompt and answer queries with their parametric knowledge~\citep{goyal-etal-2023-factual}. 
In addition, tool unlearning at prompt level can create conflicts between the model's parametric knowledge and contextual information. This may lead to misinformation, hallucination, and other unpredictable behavior~\citep{xu2024knowledge}. Finally, we show in the experiments that prompt-level tool unlearning is indeed insufficient, see Table~\ref{tab:main} (ICLU model), which aligns with existing works on LLM unlearning, where parameter update is required~\citep{jia-etal-2024-soul,zhang2024negative}.


% \paragraph{Retraining: An Impractical Solution}
% A straightforward solution is to delete $\mathcal{D}_f$ from $\mathcal{D}$ and retrain a new model only on $\mathcal{D}_r$. However, this is often infeasible due to the high cost and potential unavailability of the original training data~\citep{NEURIPS2023_299a08ee,ilharco2023editing,Gandikota_2023_ICCV}. In addition, unlearning should not be evaluated \emph{solely} based on similarity to retraining as the potential solution space is highly complex and multidimensional. Specifically, prior work has shown that relying on similarity to retraining has several drawbacks, such as poor auditability~\citep{thudi2021necessity} and ineffective deletion~\citep{cheng2023gnndelete,cheng2023multimodal}.
% % Major drawback of merely comparing to retraining include lack of auditability~\citep{thudi2021necessity}, poor practical deletion performance~\citep{cheng2023gnndelete,cheng2023multimodal}, and high dimensionality of possible solutions. 
% % mere resemblance to retraining is not sufficient nor necessary for evaluating unlearning and deciding optimal performances in practice. 
% % Resemblance to retraining is not sufficient nor necessary for optimal unlearning performances in practice. 
% % such as lack of auditability~\citep{thudi2021necessity}
% Therefore there is a need for designing specialized and efficient unlearning methods for tool-augmented models.


% Ideally, an effective tool unlearning algorithm should consider the following aspects
% \begin{itemize}
%     \item Unlearning should not affect LLM's general utility in tasks unrelated to the tool usage, i.e. $f'$ can be used as a general aligned LLM to solve tasks that $f$ is good at, such as generating text or answering general questions; 
%     % , drafting emails
%     % or math problems.
%     % \item Unlearning should not affect mode switching ability of LLMs (switching from reasoning to tool using).
%     \item Unlearning should not affect LLM's ability on $T_r$, i.e. $f'$ should maintain $f$'s capabilities in using the remaining tools that are not expected to be unlearned.

%     \item x
% \end{itemize}


% things still unclear. 
% \section{\method: Effective Tool Unlearning for LLMs}




