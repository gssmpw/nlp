\section{Introduction}
% Large Language Models (LLMs) serve as the foundation for a wide range of tasks. 
% Recently researchers have developed methods to equip large language models (LLMs) with external tools, 

Tool-augmented Large Language Models (LLMs) can use external tools such as calculators~\citep{schick2023toolformer}, 
Python interpretors~\citep{pal}, 
APIs~\citep{tang2023toolalpaca}, or 
AI models~\citep{patil2023gorilla} to complement the parametric knowledge of vanilla LLMs and enable them to solve more complex tasks~\citep{schick2023toolformer,patil2023gorilla}. They are often trained on query-response pairs, which embed the ability to use tools {\em directly} into parameters.\looseness-1
% For example, WebGPT~\citep{webgpt} extends GPT-3~\citep{gpt3} to use search engines, especially useful for events that occurred {\em after} GPT-3 was trained.
% and retrieve up-to-date information to improve GPT-3's performance in question answering, 


Despite the growing adoption of tool-augmented LLMs, the ability to selectively unlearn tools has not been investigated. In real-world applications, tool unlearning is essential for addressing critical concerns such as security, privacy, and model reliability. 
For example, consider a tool-augmented LLM deployed in a healthcare system and trained to use APIs for handling patient data. If one of the APIs is later flagged as insecure due to a vulnerability that could expose sensitive information and violate regulations like HIPAA, tool unlearning is necessary to ensure that the LLM can no longer invoke the insecure API. Similarly, when tools undergo major updates, such as the Python transformers package moving from version 3 to version 4, tool unlearning becomes essential to prevent the LLM from generating outdated or erroneous code.
% For example, if a tool-augmented LLM retains knowledge of making insecure HTTP requests, it will cause significant security risks and can become vulnerable to attacks.\footnote{\url{https://datatracker.ietf.org/doc/html/rfc7807}}
The goal of this work is to address this gap by investigating tool unlearning and providing a solution for this overlooked yet essential task.

% Additional scenarios are discussed in \S~\ref{sec:app}. In the aforementioned case, it is necessary for a tool-augmented model to forget its acquired knowledge of using certain tools--an area that has not yet been explored by existing research.
% Consider the following practical scenarios: 1) \emph{Insecure Tools}, where non-trustworthy tools need to be deleted, 2) \emph{Restricted Tools}, where tools may become unavailable due to copyright issues; 3) \emph{Broken Tools/Dependencies}, where tools may become broken, deprecated, or fall out of maintenance; 4) \emph{Unnecessary Tools}, where the requirement for certain tools may no longer be needed; and 5) \emph{Limited Model Capacity}, where the tool-augmented LLM meets capacity limitations. 


% \paragraph{A new task}
We introduce and formalize the new task of \textbf{Tool Unlearning}, which aims to remove the ability of using specific tools from a tool-augmented LLM while preserving its ability to use other tools and perform general tasks of LLMs such as coherent text generation. 
% This is essential for complying with tool deletion requests that often target a small subset of tool. 
Ideally, an effective tool unlearning model should behave as if it had never learned the tools marked for unlearning. 
% When tool deletion requests are received, a successful tool unlearning algorithm should effectively remove knowledge of the targeted tools, as if the model had never encountered them. At the same time, the model’s knowledge of remaining tools and its ability to perform other tasks should be preserved to the greatest extent possible. This is crucial because deletion requests typically focus on a specific subset of tools, which is usually much smaller than the entire tool set.
% \paragraph{Difference to sample unlearning}
Tool unlearning fundamentally differs from traditional sample-level unlearning as it focuses on removing ``skills'' or the ability to use specific tools, rather than removing individual data samples from a model. In addition, success in tool unlearning should be measured by the model’s ability to forget or retain tool-related skills, which differs from traditional metrics such as measuring likelihood of extracting training data in sample-level unlearning.
% While sample-level unlearning focuses on reducing the likelihood of extracting training data, tool unlearning aims to forget the capability to solve tasks that rely on the tools tagged for unlearning, which can be seen as knowledge-level unlearning. (2) Evaluation: Sample unlearning typically uses perplexity or extraction probability as evaluation metrics. In contrast, tool unlearning prioritizes the success rate of using specific tools, ensuring that the model can no longer effectively use the tools targeted for unlearning. (3) Data: Sample unlearning typically requires access to the exact training data, which may not be available in tool unlearning, especially when dealing with closed-source LLM training data. 
These differences are discussed in detail in~\S\ref{sec:diff}.


% \paragraph{Challenge of tool unlearning}
% as opposed to individual data samples, which makes it fun
% and existing unlearning methods are not fundamentally designed for tool removal; 
% 
% similar to sample-level unlearning, in tool unlearning, 
Removing skills requires  
modifying the parameters of LLMs, a process that is computationally expensive and can lead to unforeseen behaviors~\citep{ripple_effect,gu2024model}. In addition, existing membership inference attack (MIA) techniques, a common evaluation method in machine unlearning to determine whether specific data samples were part of training data, are inadequate for evaluating tool unlearning, as they focus on sample-level data rather than tool-based knowledge. 
% and practically difficult due to potential unforeseeable side effects on other tasks when updating LLM's parameters~\citep{ripple_effect,gu2024model}. 
% Additionally, there is no prior Membership Inference Attack (MIA) models, a desired evaluation of unlearning, designed to detect if a tool is present in training set.  


To address these challenges, we propose \method, the first tool unlearning algorithm for tool-augmented LLMs, which satisfies three key properties for effective tool unlearning: 
{\em tool knowledge removal}, which focuses on removing any knowledge gained on tools marked for unlearning; 
{\em tool knowledge retention}, which focuses on preserving the knowledge gained on other remaining tools; and 
{\em general capability retention}, which maintains LLM's general capability on a range of general tasks such as text and code generation using ideas from task arithmetic~\citep{ilharco2023editing,barbulescu2024textual}.
%
In addition, we develop LiRA-Tool, an adaptation of the Likelihood Ratio Attack (LiRA)~\citep{lira} to tool unlearning, to assess whether tool-related knowledge has been successfully unlearned. Our contributions are: 

% When receiving deletion requests, a successful tool unlearning algorithm should remove the knowledge of the tools marked for unlearning, as if the model has never seen such tools before. Meanwhile, the model's knowledge on the remaining tools as well as other tasks should be preserved to the maximum extent. This is important since the deletion request are targeted at specific subset of tools, usually much smaller than the entire tool set, and practically difficult due to potential unforeseeable side effects.

\vspace{-10pt}
\begin{itemize}
\itemsep-1pt
    \item introducing and conceptualizing tool unlearning for tool-augmented LLMs,
    \item \method, which implements three key properties for effective tool unlearning;
    \item LiRA-Tool, which is the first membership inference attack (MIA) for tool unlearning.
\end{itemize}


Extensive experiments on multiple datasets and tool-augmented LLMs show that \method outperforms existing general and LLM-specific unlearning algorithms by $+$ in accuracy on forget tools and retain tools.  In addition, it can save 74.8\% of training time compared to retraining, handle sequential unlearning requests, and retain 95+\% performance in low resource settings.\looseness-1