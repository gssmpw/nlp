@misc{ADAS,
	title = {Automated {Design} of {Agentic} {Systems}},
	url = {http://arxiv.org/abs/2408.08435},
	language = {en-US},
	urldate = {2024-10-26},
	publisher = {arXiv},
	author = {Hu, Shengran and Lu, Cong and Clune, Jeff},
	month = aug,
	year = {2024},
	note = {arXiv:2408.08435},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Full Text PDF:/Users/lby/Zotero/storage/REG7MKB6/Hu 等 - 2024 - Automated Design of Agentic Systems.pdf:application/pdf;Snapshot:/Users/lby/Zotero/storage/29C8JNDY/2408.html:text/html},
}

@inproceedings{NAACL2024_Agent-Self-Collaboration,
  author       = {Zhenhailong Wang and 
                  Shaoguang Mao and 
                  Wenshan Wu and 
                  Tao Ge and 
                  Furu Wei and 
                  Heng Ji},
  title        = {Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration},
  booktitle    = {{NAACL}},
  publisher    = {Association for Computational Linguistics},
  year         = {2024}
}

@article{arXiv2023_Dynamic-LLM-Agent,
  author       = {Zijun Liu and
                  Yanzhe Zhang and
                  Peng Li and
                  Yang Liu and
                  Diyi Yang},
  title        = {Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization},
  journal      = {CoRR},
  volume       = {abs/2310.02170},
  year         = {2023}
}

@ARTICLE{chateval,
       author = {{Chan}, Chi-Min and {Chen}, Weize and {Su}, Yusheng and {Yu}, Jianxuan and {Xue}, Wei and {Zhang}, Shanghang and {Fu}, Jie and {Liu}, Zhiyuan},
        title = "{ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = aug,
       eprint = {2308.07201}
}

@misc{chen2023frugalgptuselargelanguage,
      title={FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance}, 
      author={Lingjiao Chen and Matei Zaharia and James Zou},
      year={2023},
      eprint={2305.05176},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.05176}, 
}

@article{chen2024routerdc,
  title={RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models},
  author={Chen, Shuhao and Jiang, Weisen and Lin, Baijiong and Kwok, James T and Zhang, Yu},
  journal={arXiv preprint arXiv:2409.19886},
  year={2024}
}

@misc{dai2024costeffectiveonlinemultillmselection,
      title={Cost-Effective Online Multi-LLM Selection with Versatile Reward Models}, 
      author={Xiangxiang Dai and Jin Li and Xutong Liu and Anqi Yu and John C. S. Lui},
      year={2024},
      eprint={2405.16587},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16587}, 
}

@misc{debate2-thu,
   author = {Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Tu, Zhaopeng and Shi, Shuming},
   title = {Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate},
   pages = {arXiv:2305.19118},
   month = {May 01, 2023},
   note = {Work in progress},
   abstract = {Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of "tit for tat" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate},
   keywords = {Computer Science - Computation and Language},
   year = {2023},
   type = {Electronic Article}
}

@misc{ding2024hybridllmcostefficientqualityaware,
      title={Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing}, 
      author={Dujian Ding and Ankur Mallick and Chi Wang and Robert Sim and Subhabrata Mukherjee and Victor Ruhle and Laks V. S. Lakshmanan and Ahmed Hassan Awadallah},
      year={2024},
      eprint={2404.14618},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.14618}, 
}

@misc{eot,
      title={Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication}, 
      author={Zhangyue Yin and Qiushi Sun and Cheng Chang and Qipeng Guo and Junqi Dai and Xuanjing Huang and Xipeng Qiu},
      year={2023},
      eprint={2312.01823},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.01823}, 
}

@misc{feng2024graphroutergraphbasedrouterllm,
      title={GraphRouter: A Graph-based Router for LLM Selections}, 
      author={Tao Feng and Yanzhen Shen and Jiaxuan You},
      year={2024},
      eprint={2410.03834},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.03834}, 
}

@misc{mohammadshahi2024routoolearningroutelarge,
      title={Routoo: Learning to Route to Large Language Models Effectively}, 
      author={Alireza Mohammadshahi and Arshad Rafiq Shaikh and Majid Yazdani},
      year={2024},
      eprint={2401.13979},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.13979}, 
}

@misc{ong2024routellmlearningroutellms,
      title={RouteLLM: Learning to Route LLMs with Preference Data}, 
      author={Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
      year={2024},
      eprint={2406.18665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.18665}, 
}

@article{qian2024scaling,
  title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
  author={Qian, Chen and Xie, Zihao and Wang, Yifei and Liu, Wei and Dang, Yufan and Du, Zhuoyun and Chen, Weize and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2406.07155},
  year={2024}
}

@misc{shang2024agentsquareautomaticllmagent,
      title={AgentSquare: Automatic LLM Agent Search in Modular Design Space}, 
      author={Yu Shang and Yu Li and Keyu Zhao and Likai Ma and Jiahe Liu and Fengli Xu and Yong Li},
      year={2024},
      eprint={2410.06153},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.06153}, 
}

@misc{zhang2024cutcrapeconomicalcommunication,
      title={Cut the Crap: An Economical Communication Pipeline for LLM-based Multi-Agent Systems}, 
      author={Guibin Zhang and Yanwei Yue and Zhixun Li and Sukwon Yun and Guancheng Wan and Kun Wang and Dawei Cheng and Jeffrey Xu Yu and Tianlong Chen},
      year={2024},
      eprint={2410.02506},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2410.02506}, 
}

@article{zhang2025evoflow,
  title={EvoFlow: Evolving Diverse Agentic Workflows On The Fly},
  author={Zhang, Guibin and Chen, Kaijie and Wan, Guancheng and Chang, Heng and Cheng, Hong and Wang, Kun and Hu, Shuyue and Bai, Lei},
  journal={arXiv preprint arXiv:2502.07373},
  year={2025}
}

@misc{zhang_aflow_2024,
	title = {{AFlow}: {Automating} {Agentic} {Workflow} {Generation}},
	shorttitle = {{AFlow}},
	url = {http://arxiv.org/abs/2410.10762},
	language = {en-US},
	urldate = {2024-10-26},
	publisher = {arXiv},
	author = {Zhang, Jiayi and Xiang, Jinyu and Yu, Zhaoyang and Teng, Fengwei and Chen, Xionghui and Chen, Jiaqi and Zhuge, Mingchen and Cheng, Xin and Hong, Sirui and Wang, Jinlin and Zheng, Bingnan and Liu, Bang and Luo, Yuyu and Wu, Chenglin},
	month = oct,
	year = {2024},
	note = {arXiv:2410.10762},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {Full Text PDF:/Users/lby/Zotero/storage/5BXEZMXB/Zhang 等 - 2024 - AFlow Automating Agentic Workflow Generation.pdf:application/pdf;Snapshot:/Users/lby/Zotero/storage/QHK9G5T7/2410.html:text/html},
}

@article{zhao2023competeai,
  title={Competeai: Understanding the competition behaviors in large language model-based agents},
  author={Zhao, Qinlin and Wang, Jindong and Zhang, Yixuan and Jin, Yiqiao and Zhu, Kaijie and Chen, Hao and Xie, Xing},
  journal={arXiv preprint arXiv:2310.17512},
  year={2023}
}

@inproceedings{zhuge2024gptswarm,
  title={GPTSwarm: Language Agents as Optimizable Graphs},
  author={Zhuge, Mingchen and Wang, Wenyi and Kirsch, Louis and Faccio, Francesco and Khizbullin, Dmitrii and Schmidhuber, J{\"u}rgen},
  booktitle={Forty-first International Conference on Machine Learning},
year={2024}
}

