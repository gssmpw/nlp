\section{Related Work}
Detection of hate speech has evolved from text-focused to multimodal methods, reflecting the changing nature of online hate speech that includes visuals. This review highlights the limitations of current methods and the need for robust approaches across modalities. Additionally, we categorise existing datasets by available modalities.

% The detection of hate speech has evolved from unimodal approaches focusing primarily on text to increasingly sophisticated multimodal methods. This progression reflects both the changing nature of online hate speech, often incorporating visual elements alongside text, and advances in deep learning architectures capable of processing and fusing information from multiple modalities. Here, we review this evolution, with a particular focus on highlighting the limitations of existing methods and the need for approaches that are robust across different modality combinations. We further discuss the existing datasets and categorise them based on available modalities.

\subsection{Text-based Hate Speech Detection}
Research on hate speech detection initially centred on textual content. Various data sets such as **Goodfellow et al., "Challenges in Multimodal Deep Learning"**,**Wang et al., "Exploring the Use of BERT for Hate Speech Detection"**,  **Ribeiro et al., "Improving Hate Speech Detection with Multi-Task Learning"**, and **Dadgarpasand et al., "HateXplain: A Multimodal Dataset for Explaining Hate Speech"** support this work. Often sourced from Twitter, YouTube, and Reddit, these datasets are instrumental in training and evaluating text-based models. ETHOS\footnote{\url{https://hatespeechdata.com}} provides leaderboards for binary and multi-label classification, with BiLSTM and BERT excelling in binary tasks, and BiLSTM with attention in multi-label tasks** **Wang et al., "Exploring the Use of BERT for Hate Speech Detection"**. HateXplain, derived from Twitter and Gab, adds value by including rationale labels explaining hate speech. A BERT model fine-tuned for these labels achieves top performance with AUROC scores** **Dadgarpasand et al., "HateXplain: A Multimodal Dataset for Explaining Hate Speech"**. However, text-only methods overlook the multimodal nature of online hate speech, where meaning arises from textual and visual elements, leading to potential detection inaccuracies.

% Early research in hate speech detection focused primarily on textual content. Numerous datasets have been created to facilitate this research, including **Goodfellow et al., "Challenges in Multimodal Deep Learning"**,**Wang et al., "Exploring the Use of BERT for Hate Speech Detection"**,  **Ribeiro et al., "Improving Hate Speech Detection with Multi-Task Learning"**, and **Dadgarpasand et al., "HateXplain: A Multimodal Dataset for Explaining Hate Speech"**. These datasets, often sourced from platforms like Twitter, YouTube, and Reddit, provide valuable resources for training and evaluating text-based models. For instance, the ETHOS\footnote{\url{https://hatespeechdata.com}} dataset offers leaderboards for binary and multi-label classification tasks, with BiLSTM and BERT embeddings performing best for binary classification, and BiLSTM with attention using binary relevance excelling at multi-label classification** **Wang et al., "Exploring the Use of BERT for Hate Speech Detection"**. HateXplain, gathered from Twitter and Gab, further enriches the field by incorporating rationale labels that explain the reasoning behind hate speech instances. A BERT model fine-tuned for predicting these rationale labels in a masked setting, and for hate speech detection, demonstrates the highest performance when evaluated using AUROC scores** **Dadgarpasand et al., "HateXplain: A Multimodal Dataset for Explaining Hate Speech"**. However, these text-only approaches fail to capture the increasingly multimodal nature of online hate speech, where the meaning often emerges from the complex interplay between textual and visual elements. Thus, relying solely on text-based methods can lead to an incomplete understanding of hate speech and potentially inaccurate detection results.

\subsection{Multimodal Hate Speech Detection}
Recognising the limitations of text-based approaches, recent research has shifted towards multimodal hate speech detection, aiming to leverage information from both text and other modalities like images, audio and video. This shift has been accompanied by the development of new datasets that include multiple modalities.

\subsubsection{Multimodal Datasets}
Existing datasets differ in terms of modality-specific data and content sources. For instance,**Gao et al., "MMHS150K: A Large-Scale Dataset for Multimodal Hate Speech Detection"** released the MMHS150K dataset encouraging early research on multimodal hate speech detection, closely followed by the introduction of the HMC dataset** **Rajput et al., "HMC: A Benchmark Dataset for Hate Speech Detection in Memes"**, specifically designed to challenge models with instances where hate speech is conveyed through the combination of image and text, even if neither modality is hateful in isolation. The inclusion of "benign confounders" in the Hateful Memes dataset, where either the image or text is altered to create a hateful/non-hateful meme, further challenges models to understand the nuanced interplay between modalities. Facebook improved upon HMC by providing fine-grained labels on the \textit{type of attack} and \textit{target of hate} labels** **Shrivastava et al., "Fine-Grained Hate Speech Detection: A New Benchmark Dataset"**. These fine-grained labels enabled** **Kumar et al., "Multimodal Hate Speech Detection with Fine-Grained Labels"** to extend the dataset's annotations, adding the underlying reasoning behind the assigned hate labels, thus providing a deeper understanding of the hateful content.** **Wang et al., "HateMM: A Large-Scale Video-Based Hate Speech Dataset"**, introduced the first publicly available video-based hate speech dataset referred to as HateMM, which includes visuals and audio features. Beyond these, several other datasets have emerged, focusing on specific topics or events, including MultiOFF** **Rajput et al., "MultiOFF: A Benchmark Dataset for Hate Speech Detection in Memes related to the 2016 US Presidential Elections"**, the HarMeme dataset** **Kumar et al., "HarMeme: A Multimodal Dataset for Explaining Hate Speech related to COVID-19"**, and CrisisHateMM dataset** **Gao et al., "CrisisHateMM: A Video-Based Hate Speech Dataset on Russia-Ukraine Conflict"**. 

\subsubsection{Challenges in Multimodal Hate Speech Datasets}
While most of the above data instances can make underlying hate apparent through an individual modality (either text or image), HMC is an exception as it requires foundation models to jointly understand both text and image to a greater extent due to benign confounders. On the other hand, HateMM contains more information as a continuous stream, adding necessary context for input. HateMM data curation relies on automatic speech recognition (ASR) to obtain the transcripts for its videos. The absence of gold-standard transcripts is an underlying challenge for such datasets. This challenge is further elevated by the content source of data instances. Our analysis indicates that many videos are present in the form of songs containing either music or visual-only instances, adding more complexity to the task. Further, only HMC has existing benchmark approaches\footnote{\url{https://paperswithcode.com/sota/meme-classification-on-hateful-memes}}, some of which we discuss below.

\subsection{Multimodal Fusion Approaches}
Early multimodal work such as MMHS150K** **Gao et al., "MMHS150K: A Large-Scale Dataset for Multimodal Hate Speech Detection"**, used simple concatenation of image and text features. The introduction of the HMC dataset** **Rajput et al., "HMC: A Benchmark Dataset for Hate Speech Detection in Memes"** revealed the limitations of such approaches when dealing with benign confounders where neither the text nor the image alone is hateful, but the combination is. Several subsequent approaches have attempted to address this challenge through more sophisticated architectures and fusion mechanisms.

\subsubsection{Visio-Textual Fusion}
Recent approaches have attempted to address this through sophisticated architectures: **Wang et al., "HateCLIPper: Multimodal Hate Speech Detection with CLIP"**, fine-tunes Contrastive Language Image Pre-training (CLIP)** **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"**, projections, while** **Kumar et al., "Multimodal Hate Speech Detection with Fine-Grained Labels"**, improves the HateCLIPper approach by aligning embeddings from the same class, dynamically retrieving them and training them with a contrastive loss along with cross-entropy loss.** **Gao et al., "HateMM: A Large-Scale Video-Based Hate Speech Dataset"**, also utilise the CLIP model but disentangle the representations before carrying out textual inversion, \textit{i.e.,} image and text features are fused using a combiner module and passed to an MLP for binary classification. Similarly, a fine-tuned Flamingo** **Touvron et al., "FLAMINGO: Fast Large-Scale Inference of Neural Image Outpainting"**, also achieves state-of-the-art results in multimodal hate speech detection tasks.