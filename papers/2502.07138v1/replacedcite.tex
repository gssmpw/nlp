\section{Related Work}
Detection of hate speech has evolved from text-focused to multimodal methods, reflecting the changing nature of online hate speech that includes visuals. This review highlights the limitations of current methods and the need for robust approaches across modalities. Additionally, we categorise existing datasets by available modalities.

% The detection of hate speech has evolved from unimodal approaches focusing primarily on text to increasingly sophisticated multimodal methods. This progression reflects both the changing nature of online hate speech, often incorporating visual elements alongside text, and advances in deep learning architectures capable of processing and fusing information from multiple modalities. Here, we review this evolution, with a particular focus on highlighting the limitations of existing methods and the need for approaches that are robust across different modality combinations. We further discuss the existing datasets and categorise them based on available modalities.

\subsection{Text-based Hate Speech Detection}
Research on hate speech detection initially centred on textual content. Various data sets such as Hate Speech____, ETHOS____, Twitter Hate Speech____, and HateXplain____ support this work. Often sourced from Twitter, YouTube, and Reddit, these datasets are instrumental in training and evaluating text-based models. ETHOS\footnote{\url{https://hatespeechdata.com}} provides leaderboards for binary and multi-label classification, with BiLSTM and BERT excelling in binary tasks, and BiLSTM with attention in multi-label tasks____. HateXplain, derived from Twitter and Gab, adds value by including rationale labels explaining hate speech. A BERT model fine-tuned for these labels achieves top performance with AUROC scores____. However, text-only methods overlook the multimodal nature of online hate speech, where meaning arises from textual and visual elements, leading to potential detection inaccuracies.

% Early research in hate speech detection focused primarily on textual content. Numerous datasets have been created to facilitate this research, including Hate Speech____, ETHOS____, Twitter Hate Speech____, and HateXplain____. These datasets, often sourced from platforms like Twitter, YouTube, and Reddit, provide valuable resources for training and evaluating text-based models. For instance, the ETHOS\footnote{\url{https://hatespeechdata.com}} dataset offers leaderboards for binary and multi-label classification tasks, with BiLSTM and BERT embeddings performing best for binary classification, and BiLSTM with attention using binary relevance excelling at multi-label classification____. HateXplain, gathered from Twitter and Gab, further enriches the field by incorporating rationale labels that explain the reasoning behind hate speech instances. A BERT model fine-tuned for predicting these rationale labels in a masked setting, and for hate speech detection, demonstrates the highest performance when evaluated using AUROC scores____. However, these text-only approaches fail to capture the increasingly multimodal nature of online hate speech, where the meaning often emerges from the complex interplay between textual and visual elements. Thus, relying solely on text-based methods can lead to an incomplete understanding of hate speech and potentially inaccurate detection results.

\subsection{Multimodal Hate Speech Detection}
Recognising the limitations of text-based approaches, recent research has shifted towards multimodal hate speech detection, aiming to leverage information from both text and other modalities like images, audio and video. This shift has been accompanied by the development of new datasets that include multiple modalities.

\subsubsection{Multimodal Datasets}
Existing datasets differ in terms of modality-specific data and content sources. For instance,____ released the MMHS150K dataset encouraging early research on multimodal hate speech detection, closely followed by the introduction of the HMC dataset____, specifically designed to challenge models with instances where hate speech is conveyed through the combination of image and text, even if neither modality is hateful in isolation. The inclusion of "benign confounders" in the Hateful Memes dataset, where either the image or text is altered to create a hateful/non-hateful meme, further challenges models to understand the nuanced interplay between modalities. Facebook improved upon HMC by providing fine-grained labels on the \textit{type of attack} and \textit{target of hate} labels____. These fine-grained labels enabled____ to extend the dataset's annotations, adding the underlying reasoning behind the assigned hate labels, thus providing a deeper understanding of the hateful content.____ introduced the first publicly available video-based hate speech dataset referred to as HateMM, which includes visuals and audio features. Beyond these, several other datasets have emerged, focusing on specific topics or events, including MultiOFF____ related to the $2016$ US Presidential Elections, the HarMeme dataset____ related to COVID-19, and CrisisHateMM dataset____ on Russia-Ukraine conflict. 

\subsubsection{Challenges in Multimodal Hate Speech Datasets}
While most of the above data instances can make underlying hate apparent through an individual modality (either text or image), HMC is an exception as it requires foundation models to jointly understand both text and image to a greater extent due to benign confounders. On the other hand, HateMM contains more information as a continuous stream, adding necessary context for input. HateMM data curation relies on automatic speech recognition (ASR) to obtain the transcripts for its videos. The absence of gold-standard transcripts is an underlying challenge for such datasets. This challenge is further elevated by the content source of data instances. Our analysis indicates that many videos are present in the form of songs containing either music or visual-only instances, adding more complexity to the task. Further, only HMC has existing benchmark approaches\footnote{\url{https://paperswithcode.com/sota/meme-classification-on-hateful-memes}}, some of which we discuss below.

\subsection{Multimodal Fusion Approaches}
Early multimodal work such as MMHS150K____ used simple concatenation of image and text features. The introduction of the HMC dataset____ revealed the limitations of such approaches when dealing with benign confounders where neither the text nor the image alone is hateful, but the combination is. Several subsequent approaches have attempted to address this challenge through more sophisticated architectures and fusion mechanisms.

\subsubsection{Visio-Textual Fusion}
Recent approaches have attempted to address this through sophisticated architectures: HateCLIPper____ fine-tunes Contrastive Language Image Pre-training (CLIP)____ projections, while ____ improves the HateCLIPper approach by aligning embeddings from the same class, dynamically retrieving them and training them with a contrastive loss along with cross-entropy loss.____ also utilise the CLIP model but disentangle the representations before carrying out textual inversion, \textit{i.e.,} image and text features are fused using a combiner module and passed to an MLP for binary classification. Similarly, a fine-tuned Flamingo____ also performs well on the HMC dataset. Modularised networks have been employed for few-shot learning, improving detection performance on smaller datasets____. Multi-scale visual kernels combined with knowledge distillation architecture have also been utilised to enhance robustness and accuracy____. PALI-X-VPD____ employs a $55B$ parameter language model with access to code generation and external tools such as object detection, visual question answering, optical character recognition, etc, eventually aggregating this information with the help of chain-of-thought. While these approaches achieve strong performance ($AUROC > 0.85$), they require significant computational resources and may not generalise to other modality combinations. More recently, zero-shot evaluation of large multimodal models (LMMs) such as LLaVA-1.5, BLIP-2, Evolver, etc. has been performed on this dataset however they still fall short of the SoTA approaches____.

\subsubsection{Video-based Fusion}
Video hate detection presents additional challenges due to temporal dependencies and the need to integrate audio features. The HateMM dataset____ pioneered work in this direction, employing pre-trained encoders for text, image and audio and obtaining sequential (temporal) information with LSTM____ and aggregating them with simple embedding fusion. MultiHateClip____ extended this to multilingual content but maintained similar fusion strategies. Notably, approaches that succeed on video content often fail on static image-text pairs, suggesting fundamental limitations in current fusion methods. While most of the work discussed above focuses primarily on performance improvement, none discuss a common framework to combat modality-agnostic hate content. Our work aims to bridge this gap by conducting a comparative analysis of fusion approaches across video and image-based content, shedding light on their strengths and weaknesses in different multimodal contexts.

% While various fusion techniques have been proposed, their effectiveness appears highly dependent on the modality combination. Early fusion through concatenation shows strong results on video content but struggles with complex image-text relationships. Late fusion strategies attempt to address this through learned weights but introduce additional complexity without consistently better results. This modality-dependent performance variance remains poorly understood and motivates our systematic analysis. Our review reveals a critical gap in current research: while individual approaches show promise for specific modality combinations, we lack a systematic understanding of why certain fusion techniques succeed or fail across different contexts. This gap motivates our comparative analysis of fusion approaches across video and image-based content.