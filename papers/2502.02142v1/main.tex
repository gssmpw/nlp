\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

% *** PACKAGES ***
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

% *** EXTRA PACKAGES ***
\usepackage{mathtools,amsmath}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx} % add this line in the preamble
\usepackage{amsmath}  % For advanced math typesetting
\usepackage{graphicx} % For including graphics
\usepackage{textcmds} % For quote style
\usepackage{tikz}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{array, makecell}%
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,fill,inner sep=2pt] (char) {\textcolor{white}{#1}};}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\title{Towards Efficient LUT-based PIM: A Scalable and Low-Power Approach for Modern Workloads}

\author{\IEEEauthorblockN{Bahareh Khabbazan, Marc Riera, Antonio Gonz√°lez }
\IEEEauthorblockA{\textit{dept. of Computer Architecture } \\
\textit{Universitat Polit\`{e}cnica de Catalunya (UPC)}\\
Barcelona, Spain\\
\{bahareh.khabbazan, marc.riera.villanueva, antonio.gonzalez\}@upc.edu}
}

%%%%%%%%%%%---Make Space-----%%%%%%%%%%%%%
% shrink horizontally (letterspace=-25 suggested by julian)
%\usepackage[letterspace=-27,tracking=true,final]{microtype}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% Paper title
%\title{Lama: A Lightweight Mechanism towards LUT-based PIM Approach }
\maketitle



\begin{abstract}
Data movement in memory-intensive workloads, such as deep learning, incurs energy costs that are over three orders of magnitude higher than the cost of computation. Since these workloads involve frequent data transfers between memory and processing units, addressing data movement overheads is crucial for improving performance. Processing-using-memory (PuM) offers an effective solution by enabling in-memory computation, thereby minimizing data transfers. In this paper we propose \textbf{Lama}, a LUT-based PuM architecture designed to efficiently execute SIMD operations by supporting independent column accesses within each mat of a DRAM subarray. Lama exploits DRAM's mat-level parallelism and open-page policy to significantly reduce the number of energy-intensive memory activation (ACT) commands, which are the primary source of overhead in most PuM architectures. Unlike prior PuM solutions, Lama supports up to 8-bit operand precision without decomposing computations, while incurring only a 2.47\% area overhead. Our evaluation shows Lama achieves an average performance improvement of 8.5$\times$ over state-of-the-art PuM architectures and a 3.8$\times$ improvement over CPU, along with energy efficiency gains of 6.9$\times/$8$\times$, respectively, for bulk 8-bit multiplication.

We also introduce \textbf{LamaAccel}, an HBM-based PuM accelerator that utilizes Lama to accelerate the inference of attention-based models. LamaAccel employs exponential quantization to optimize product/accumulation in dot-product operations, transforming them into simpler tasks like addition and counting. LamaAccel delivers up to 9.3$\times/$19.2$\times$ reduction in energy and 4.8$\times/$9.8$\times$ speedup over TPU/GPU, along with up to 5.8$\times$ energy reduction and 2.1$\times$ speedup over a state-of-the-art PuM baseline.

% ##Longer version for LamaAccel##
% Additionally, we introduce LamaAccel, an HBM-based PuM accelerator designed to enhance the inference of attention-based models. LamaAccel utilizes Lama's in-memory computing capabilities and utilizes a data mapping approach to allocate encoders/decoders block of LLMs across multiple pseudo-channels of HBM and pipelines the processing of different layers within each block. Further, LamaAccel employs exponential quantization to simplify dot-product accumulation into memory-friendly addition and counting operations. Our evaluation demonstrates that LamaAccel delivers a 6.9$\times/$12.1$\times$ reduction in energy and a 4.4$\times/$7.2$\times$ speedup over TPU/GPU, along with a 3.7$\times$ energy reduction and 1.8$\times$ speedup over a state-of-the-art PuM baseline.
\end{abstract}

% Keywords
%\begin{IEEEkeywords}
%Processing in-memory; Large-language models; SIMD; Domain-specific accelerator.
%\end{IEEEkeywords}

%%%%%% -- PAPER CONTENT STARTS-- %%%%%%%%
\input{sections/1-introduction}
\input{sections/2-background}
\input{sections/3-lama_overview}
\input{sections/4-case_study1}
\input{sections/5-case_study2}
\input{sections/6-conclusion}
\input{sections/7-acknowledgement}
%%%%%%% -- PAPER CONTENT ENDS -- %%%%%%%%

%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{IEEEtranS}
\bibliography{references}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
