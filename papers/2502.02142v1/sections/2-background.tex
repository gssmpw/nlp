%%% START GUIDELINE %%%

% 1- This section describes first HBM micro organization and an overview of previous PIM proposals, then we review a general description on how attention models working and finally we discuss on log-based 2 and more specifically exponential quantization and how this approach can offer more opportunities to support in-memory operations. + convolution formula in exponential domain.

% 2- Figure1 related to HBM organization. showing different subsets in a bank (SAs, MATs), showing higher overview of BGs, psuedochannels, col counter? can be shown in this figure?

% 3- pLUTo figure 2, its description + TransPIM and REDLut and SIMDRAM.

% 4- Figure3 for attention based models. showing different layers of attention, similar to TransPIM fig1 + description on input stationary terminology.

%%% END GUIDELINE %%%

\section{Background \& Related Work}
In this section, we review key terminology and concepts that are essential for understanding the content of this paper. First, we introduce the structure of HBM and its hierarchical micro-organization. Next, we provide an overview of attention-based models, focusing on the exponential quantization technique described in \cite{DNA-TEQ}. Finally, we examine prior PuM techniques~\cite{pluto,red-LUT}, discussing their main strengths and challenges.

\subsection{HBM Micro Organization}\label{HBM_organization}
The baseline memory architecture used in this work is based on micron's high-bandwidth memory (HBM2) as shown in Figure~\ref{fig:HBM_organization}a. An HBM stack consists of multiple DRAM slices stacked atop a base die, interconnected through numerous through-silicon vias (TSVs). This configuration delivers significantly higher bandwidth and lower access latency compared to conventional DRAM solutions.

\begin{figure}[t!]
\centering
\includegraphics[width=0.75\columnwidth]{figures/HBM_organization.pdf}
%\vskip -0.15in
\caption{HBM2 Organization: (a) High-level, (b) Channel-level, and (c) Bank-level hierarchy. The bank-level organization is adapted from \cite{fine-grained}.}
\label{fig:HBM_organization}
\vskip -0.15in
\end{figure}

Each DRAM die of HBM2 supports two independent channels and each 128-bit channel can be split into two separate 64-bit pseudo-channels~\cite{psch_mode}. In HBM2's pseudo-channel mode, the 64-bit I/O interface operates at a frequency of 1 GHz, offering a bandwidth of 16 GB/s over the DDR interface. The DRAM atom size, which is the smallest unit of data that can be accessed in a single request, is 32 bytes. This amount of data is transferred using a burst length of four over the 64-bit DDR I/O interface, ensuring efficient data throughput and reduced latency.

As shown in Figure~\ref{fig:HBM_organization}b, according to the JEDEC standard, a pseudo channel comprises two bank groups, with each group containing four banks. When operating in pseudo-channel mode, the two pseudo channels function semi-independently: they share the same channel’s row and column command bus as well as clock inputs, but decode and execute commands individually~\cite{jedec}. This mode enhances command bandwidth, reduces latency, and increases effective data bandwidth~\cite{demystifyingHBM}. Consequently, throughout this paper, we consider an HBM2 stack to be operating in pseudo-channel mode, with sixteen 64-bit wide channels per stack, rather than in legacy mode with eight 128-bit wide channels.

As shown in Figure~\ref{fig:HBM_organization}c, each DRAM bank is equipped with its own set of row and column decoders, along with a column-address counter/latch module~\cite{micron_ddr4}. The column-address counter/latch is responsible for selecting the specific column address within an activated row. Once a row is activated using an ACT command, the entire row's contents are loaded into the local row-buffer which serves as temporary storage. This allows subsequent read or write operations to access the data without needing to reactivate the row. According to \cite{fine-grained}, each read/write command must retrieve a 32-byte DRAM atom, accomplished through two \textit{internal column accesses}, with each access retrieving 16 bytes of data from 16 mats. For the first internal column access, the column counter/latch selects the column based on the address specified in the read/write command, while for the second internal column access, the column counter/latch increments the address to access the next set of data.

% To perform a read operation, the column address counter identifies the precise column location within the module ensures that the selected column address remains stable throughout the read/write process. This stability guarantees consistent access to the data associated with that column, preventing any changes during the operation.

% Column address counter/latch manages column address selection and supports sequential accesses in burst mode, enabling efficient read and write operations within the bank.

% Bank Organization
A modern DRAM bank is organized into a hierarchical structure consisting of \textit{subarrays} and \textit{mats}. Each subarray within a DRAM bank contains a subset of the \textit{rows} and is equipped with a set of sense amplifiers that form the local row buffer for that subarray. In an HBM2 Micron memory configuration, a bank of 32KB is organized into 64 subarrays, each comprising 512 rows of DRAM cells. To manage the long bitlines and wordlines effectively, each subarray is further divided into multiple quadratic units called \textit{mats}, each sized at $512\times512$. When a row is activated in a subarray via an ACT command, each mat within the subarray activates a segment of the row into its local set of sense amplifiers. The subarray performs this operation by first driving a Master Wordline (MWL) in a high-level metal across the entire subarray, which then activates the Local Wordlines (LWLs) in each individual mat. Subsequently, when a DRAM atom is read, every mat of the subarray outputs a few bits of the DRAM atom~\cite{fineDRAM}.

In HBM2, a row with a width of 1KB, referred to as the \textit{page size}, is distributed across 16 mats, each 512-bit wide. During a read operation, each mat provides 8-bit per internal cycle. Over two internal cycles, all mats collectively deliver the 32-byte DRAM atom~\cite{fineDRAM}. The read command activates the Column-selection Logic (CSL) in each mat, which functions as a multiplexer, directing data from the target sense amplifiers to the master data lines (MDLs) through the Local Data Lines (LDLs)~\cite{drambook, understandingDRAM}. The bank I/O logic, or global sense amplifiers, connects to all mats via the MDLs, facilitating the aggregation and transmission of data from the individual mats to the external data bus.

\subsection{Attention-based models}
Attention-based models, also known as Transformers, are composed of a stack of encoders, decoders, or a combination of both. As shown in Figure~\ref{fig:Transformers}, each encoder includes a self-attention layer followed by a feed-forward network (FFN).

For an input sequence with \textit{L} tokens, the encoder attention block generates a query vector $Q$, a key vector $K$, and a value vector $V$ per token. This is done by passing each input token through three fully-connected (FC) layers, where each layer multiplies the input by learned weight matrices $W^Q, W^K, W^V$, respectively. Afterward, the query matrix and the key matrix are multiplied together to compute the attention scores, which indicate how well each query vector correlates with the corresponding key vectors. Softmax is then applied to the resulting matrix to normalize the scores, converting them into probabilities that sum to one. This step is often followed by a dropout to prevent overfitting, yielding to the self-attention matrix $S$. The attention matrix $S$ is then multiplied by the value matrix $V$ to produce the final output of the attention mechanism. The self-attention layer is usually split into multiple heads, processed in parallel, where each head pays attention to a different region of the input sequence. The results from each attention head are then concatenated into a single matrix and merged through a hidden fully-connected layer. Finally, the add and norm layer applies a residual connection followed by layer normalization. The residual connection adds the input of the attention layer to its output, while the normalization layer ensures that the output values remain within a specific range. At the end of the encoder block, the attention output is passed through a FFN, consisting of two fully-connected layers that generate the block’s final output.

%..., which projects the combined results into the output of the self-attention layer.

The produced output can then serve as input to the next block (either an encoder or decoder) or be directed to a task-specific output layer, such as for classification. In text generation tasks decoder blocks follow a similar structure and flow as encoder blocks but with some key differences. Unlike encoder blocks, which handle sequences of tokens, the input and output of decoder blocks typically consist of only one or a few tokens at a time. Additionally, decoder blocks include a masked self-attention layer that ensures predictions are based only on previously generated tokens, preserving the autoregressive property essential for tasks like language generation. In text generation tasks, there is one extra layer called encoder-decoder attention which allows the decoder to focus on specific parts of the encoded input sequence while generating the output. It uses the decoder's query and the encoder's key-value pairs to compute attention scores. Overall, attention-based models involve numerous memory-intensive and matrix multiplication operations~\cite{transpim, A3, mnnfast, interstellar}, making them well-suited for in-memory and near-memory acceleration.

\begin{figure}[t!]
\centering
\includegraphics[width=0.9\columnwidth]{figures/Transformers.pdf}
%\vskip -0.15in
\caption{Encoder and Decoder blocks in Attention-based models.}
\label{fig:Transformers}
\vskip -0.15in
\end{figure}
\vspace{-2.5mm}

\subsection{Exponential Quantization}\label{Exponential_quant}
Researchers have proposed various optimizations for deep neural networks (DNNs), including pruning, clustering, and quantization~\cite{permdnn}. These techniques aim to reduce the memory footprint and computational complexity of DNNs, facilitating their deployment in real-time applications. Uniform quantization~\cite{ReDY} has been widely utilized to reduce the numerical precision (usually to 8-16 bits) and computational cost, typically with minimal impact on accuracy. To further decrease the model size and computational overhead, a recent study \cite{mokey} introduced techniques that assign varying bitwidths to weights and/or activations based on their sensitivity to quantization error. This method analyzes the distribution of each tensor, using it as a criterion to determine the optimal numerical precision for each DNN layer.

Similarly, DNA-TEQ~\cite{DNA-TEQ} employs a mixed-precision scheme for each layer, but with a non-uniform quantization representation that minimizes accuracy loss by reducing quantization error. DNA-TEQ leverages exponential quantization, which exploits the property $b^{a} \cdot b^{w} = b^{a + w}$. This approach not only reduces memory consumption but also simplifies the hardware implementation by replacing expensive multiplication operations with cost-effective addition operations. DNA-TEQ quantizes values into the form $S_{W}(\alpha_{W} b^{int_{W}} + \beta_{W})$ for weights $W$ and $S_{A}(\alpha_{A} b^{int_{A}} + \beta_{A})$ for activations $A$, where $S_{W}$, $S_{A}$ represent the sign, $int_{W}$, $int_{A}$ are signed $n$-bit integer exponents, $\alpha_{W}$, $\alpha_{A}$ are scale factors and $\beta_{W}$, $\beta_{A}$ are offsets for the respective weight and activation tensors.

As shown in Equation~\ref{eqn:conv_extended}, each output activation is computed as a dot-product of activations and weights, which can be expanded into a sum of four simpler terms:

%\vskip -0.15in
\begin{equation}
\begin{scriptsize}
\begin{aligned}
\label{eqn:conv_extended}
\sum_{i=1}^{m} A_{i} \cdot W_{i} = \sum_{i=1}^{m}S_{A_{i}} (\alpha_{A} b^{int_{A_{i}}} + \beta_{A}) \cdot S_{W_{i}} (\alpha_{W} b^{int_{W_{i}}} + \beta_{W}) =\\
\underbrace{\alpha_{A}\alpha_{W}\sum_{i=1}^{m} (S_{A_{i}} S_{W_{i}})b^{int_{A_{i}} + int_{W_{i}}}}_\text{1} + \underbrace{\alpha_{W}\beta_{A}\sum_{i=1}^{m}(S_{A_{i}} S_{W_{i}})b^{int_{W_{i}}}}_\text{2} \\
+ \underbrace{\alpha_{A}\beta_{W}\sum_{i=1}^{m}( S_{A_{i}} S_{W_{i}}) b^{int_{A_{i}}}}_\text{3} + \underbrace{\beta_{A}\beta_{W}\sum_{i=1}^{m}S_{A_{i}} S_{W_{i}}}_\text{4}
\end{aligned}
\end{scriptsize}
\end{equation}

The first term requires the addition of exponents, denoted as $b^{int_{A_{i}} + int_{W_{i}}}$, and can be computed by counting the number of times that each addition of exponents occurs. The second term involves the summation of base $b$ to the power of weight exponents, adjusted by the sign of both the weight and activation. Similarly, the third term focuses on the summation of $b$ to the power of activation exponents, adjusted by their corresponding signs. Both the second and third terms can be computed by counting the occurrences of the exponents. Finally, the fourth term accumulates the sign products, which can be derived from any of the previous terms by summing the total number of occurrences.

After counting the occurrences associated with each exponent, these counts are multiplied by their corresponding values $b^{int}$. The resulting products are then accumulated into a single value, which is subsequently multiplied by the constant coefficients, and all terms are summed together to produce the final output activation.

\subsection{LUT-based PuM Techniques}\label{pluto_background}
The authors of pLUTo \cite{pluto} proposed a novel DRAM-based PuM approach that employs LUTs to perform complex operations efficiently. As illustrated in Figure~\ref{fig:pLUTo}, pLUTo can be used to perform a 4-bit bulk multiplication task with LUTs.

In this example, the LUT is designed to store pre-computed multiplication results for all possible 4-bit operand combinations. When performing the multiplication of two 4-bit operands, $a$ and $b$, where $a = \overline{a_{3}a_{2}a_{1}a_{0}}$ and $b = \overline{b_{3}b_{2}b_{1}b_{0}}$, the result $c$, is an 8-bit value. To compute $c = a\times b$ using pLUTo, the LUT query input vector is formed by concatenating the binary representations of $a$ and $b$. The input bit vector $q_{x}=[a,b]$ is then used to query the LUT, which returns the results of four multiplication queries $q_{0}$ to $q_{3}$ in a single operation. Although the LUT can accommodate up to one whole row of queries, for simplicity, only four queries are depicted in the figure. This allows for efficient multiplication directly within the memory.

To perform four LUT queries simultaneously, pLUTo stores and replicates the LUT four times within a single DRAM subarray. As shown in Figure~\ref{fig:pLUTo}, each row $i$ in the subarray contains repeated copies of the element corresponding to the $i$-th index of the LUT. The memory controller first loads the query input vector into the source row buffer. Then, it initiates a \textit{pLUTo Row Sweep} operation, which sequentially activates all rows in the pLUTo-enabled subarray that hold the LUT elements. The activation follows an orderly pattern, such as activating rows $\#0$, $\#1$, and so on, up to row $\#255$.

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\columnwidth]{figures/pLUTo.pdf}
%\vskip -0.15in
\caption{Example of pLUTo for 4-bit multiplications.}
\label{fig:pLUTo}
\vskip -0.15in
\end{figure}

During each row activation, the \textit{pLUTo Match Logic} identifies matches between the current row index in the subarray and the elements in the LUT query input vector. This matching process is essential for selecting the correct elements from the LUT that correspond to the query. Once a match is detected, the relevant elements in the pLUTo-enabled row buffer are copied into the \textit{Flip-Flop buffer}, which stores the final output for the whole LUT query operation.

pLUTo faces several challenges when performing calculations using LUTs within DRAM subarrays. First, the \textit{pLUTo Row Sweep} operation, which activates rows sequentially, results in long latency and high energy consumption due to repeated ACT commands. Second, pLUTo's ability to perform parallel LUT queries within a bank is limited by DRAM’s tFAW timing constraints. In addition, the technique introduces a 16\% area overhead due to the modifications to the row decoder and the inclusion of the match logic and extra buffer. Finally, scaling pLUTo for operations with more than 4-bit precision is costly, as it supports a maximum of 8-bit LUT query inputs. To handle higher precisions, operations must be divided into smaller segments and processed across multiple subarrays, adding complexity. Lama tackles all these challenges and provides important energy savings over pLUTo as we will show later in this paper.
