%%% START GUIDELINE %%%

% 1- caracterizing workloads into low-computational and high-computational parts. cost of moving the data compared with the computation part with one example.

% 2- PIM paradigm as a solution for low-computational workloads, breaking it into PnM and PuM mentioning their difference.

% 3- Describe different PuM techniques and their differences.

% 4- Challenges of previous PuM proposals: PIM techniques although seem so interesting, have main challenges that need to be solved. First, the proposed schemes should not touch commodity DRAM structures, lowest area overhead to reduce the fabrication cost, and be compatible with memory timings and its regulation to be practical. Previous PIM proposals have important limitations in order to use them for operations related to LLMs: 1) Not suitable for complex operations-> proposals like pLUTo tried to support this by proposing LUT-based computing but still their work is not scalable for more than 4b precision, to be scalable, the complexity, size, latency grows. 2) Not compatible with commodity memory timings or need to modify the memory controller by adding new commands or changing timing. 3) Huge area overhead 4) lead to significant energy consumption due to the successive ACT commands. 5) Need vertical data layout... We still think in-memory processing systems have a potential to improve the efficiency of executing LLMs, owing to the fact that these applications are increasingly characterized by their high data movement (from memory to the host).

% 5- First Explanation of our scheme: This work explores the design method for supporting various integer precision multiplications and an approach to support massively parallel matrix-multiplications that does not require numerous ACT commands by benefiting from the open page policy for HBMs via read memory operations. our goal is to keep existing features of commodity DRAM to support MMs in a way to be energy efficient and scalable for various precisions (up to 8b).

% 6- Main contributions: Our scheme is a LUT-based scheme that uses the intrinsic features of the memory such as open-page policy and MAT-Level parallelism inside each memory bank that is much more energy efficient than existing PIM schems due to no need for parallel or successive ACT commands or any changes to the memory timing. the main differences between our scheme with the previous works are: 1) The parallelism is not restricted by the number of parallel ACT commands and complies with FAW timing. 2) More energy efficient operations due to row access locality and due to the significant reduction in the number of ACT commands and relying on mostly read commands to handle operations in the memory.. 3) a scheme to support various precisions. in multiplications, it shows ....% faster compared with the existing baseline, pLUTo. and TPU. 4) an approach to support  MMs in DNNs, using an exponential quantization approach. 5) tested on various recent LLM models. 6) low area overhead.

%%% END GUIDELINE %%%

\section{Introduction}
Modern workloads, such as big data analytics, deep learning, and graph processing, are characterized by high data parallelism and low computational intensity~\cite{PnM_fpga}. In conventional systems, like GPUs and CPUs, these applications require frequent data transfers between memory and the processing units, significantly degrading both energy efficiency and performance. The cost of moving data can substantially outweigh the cost of performing computations. This imbalance has been highlighted in previous research~\cite{eie}, which reveals that retrieving a 32-bit word from off-chip DRAM incurs an energy cost of 6400 times higher than performing a simple ADD operation in 45 nm technology. As these applications span various domains and become more prevalent, addressing the inefficiencies and performance limitations caused by data movement is crucial for enhancing the overall system efficiency.

Processing-in-Memory (PIM) aims to alleviate the data transfer bottleneck by integrating computational capabilities directly within the memory units~\cite{ghoseprocessing} itself, thereby reducing data movement and improving overall performance~\cite{mutluprocessing}. PIM-based accelerators are classified into two main approaches: 1)~\textbf{Processing-near-Memory (PnM)} where computations take place in processing elements near the memory arrays. The processing elements can be in the same die as the memory array~\cite{pimornot,newton} or in the logic layer of 3D-stacked memory~\cite{DNA-TEQ, qeihan, neurocube}. Although PnM solutions are limited by memory bandwidth, they are well-suited for offloading complex computations. 2)~\textbf{Processing-using-Memory (PuM)} embeds computational capabilities directly within memory cells. This technique allows data to remain in the memory cells while computations are performed in place, eliminating the need for data movement out of memory. However, PuM techniques lack flexibility to support complex operations due to the intrinsic limitations of the memory. Despite the challenges in supporting a wide range of operations, PuM offers significant throughput due to the large parallelism inherent in the memory. While PnM and PuM each offer distinct advantages, a hybrid solution that leverages the strengths of both approaches can optimize performance by balancing the computational complexity with memory bandwidth. This makes it a compelling choice for a wide range of modern workloads.

Numerous PIM designs have been implemented across a diverse array of memory technologies, including static random access memory (SRAM)~\cite{neuralcache}, dynamic random access memory (DRAM)~\cite{drisa, lacc}, and emerging non-volatile memory technologies such as Resistive RAM (ReRAM)~\cite{ReDY}. PuM in the context of commodity DRAM has drawn more attention compared to SRAM. This increased focus is due to the DRAM's larger memory capacity, and the critical need to reduce costly off-chip data transfers to the host. In addition, compared to NVM, DRAM avoids the drawbacks of limited lifespan~\cite{renew} and the high energy costs associated with memory cell writes. PuM in DRAM platforms typically leverages either \textbf{charge-sharing-based}~\cite{ambit} or \textbf{lookup table (LUT)-based}~\cite{pluto} techniques to perform logic and arithmetic operations inside the memory chip.

\begin{figure}[t!]
\centering
\includegraphics[width=0.8\columnwidth]{figures/Ambit_LUT.pdf}
\caption{Different PuM techniques: (a) charge-sharing (b) LUT-based approach.}
\label{fig:charge_shared_vs_lut}
\end{figure}

Charge-sharing-based techniques, as illustrated in Figure~\ref{fig:charge_shared_vs_lut}a, draw inspiration from Ambitâ€™s Triple Row Activation (TRA) mechanism~\cite{ambit}, offering high compute throughput for bulk bitwise operations. While this approach is effective for simple bitwise tasks, it struggles to efficiently execute more complex arithmetic operations such as multiplication and addition. This inefficiency arises because each arithmetic operation must be decomposed using multiple bitwise operations, requiring numerous memory subarray accesses and costing an extensive amount of cycles and significant energy consumption~\cite{fulcrum}. Additionally, many of these techniques are not fully compatible with commodity DRAM structures, as they demand substantial modifications to the DRAM subarrays~\cite{drisa, dracc} or do not comply with the standard memory command timings~\cite{computedram}.

On the other hand, LUT-based techniques offer a powerful approach for efficiently retrieving results of complex arithmetic operations by leveraging pre-stored data. As depicted in Figure~\ref{fig:charge_shared_vs_lut}b, a LUT essentially functions as a large pre-stored array where each entry corresponds to the result of a specific operation. The operands, often concatenated into a single input value, are used as a unique index or address to quickly access the corresponding result in the LUT. This method provides a more energy-efficient and lower-latency alternative to traditional charge-sharing-based schemes for executing arithmetic operations.

However, simultaneous access to multiple LUT entries, while increasing throughput, can significantly raise energy consumption. This is especially critical in DRAM systems, where energy efficiency is paramount. To effectively enable parallel LUT accesses, it is essential to manage the interleaving of LUT data across different levels of the memory hierarchy. Inefficient mapping of LUT data can lead to sub-optimal access patterns, reducing the system's ability to fully exploit parallel processing. For instance, accessing non-contiguous memory locations across different hierarchy levels (e.g., subarrays, banks) can hinder effective parallelism. To mitigate these challenges, it is crucial to optimize the data layout and employ smart interleaving placement strategies. By strategically distributing LUT entries across the memory hierarchy, one can minimize contention and maximize parallel access capabilities. This ensures that the system can handle multiple LUT queries in parallel, improving overall performance while maintaining energy efficiency.

Several previous works~\cite{pluto, red-LUT, ppim, reconfigurable_pim, lacc, sal-pim} have proposed the design of efficient LUT-based PuM architectures that supports complex bulk arithmetic operations in memory. However, some critical limitations still hinder their practical applicability. The main challenges are:

\textbf{\textit{Area Overhead}}: Executing bulk operations often requires multiple LUT accesses. To achieve this, previous approaches replicate LUTs across different subarrays within a DRAM bank, known as subarray-level parallelism~\cite{salp}. Implementing subarray-level parallelism requires significant modifications to the DRAM bank architecture. Typically, current memory designs allow only one activation (ACT) command at a time. To address this, many proposals replicate row decoders for each subarray, enabling simultaneous ACT commands to different rows within the same bank~\cite{red-LUT, sal-pim}. Additionally, they may introduce extra buffers in some subarrays with sizes equivalent to the row buffer (e.g., 1kB) to store computational results during bulk execution~\cite{pluto}. While these modifications support parallelism, they incur substantial area overheads ranging from 10.2\% to 23.1\%~\cite{pluto}. This added overhead consumes valuable silicon space that could otherwise be allocated for data storage, thereby reducing the memory capacity.

% However, when a LUT row is opened, underutilization of the open page can lead to wasted opportunities to perform additional operations by accessing different columns within the same page.

\textbf{\textit{Performance and Energy Inefficiency}}: To handle bulk arithmetic operations, existing LUT-based techniques~\cite{pluto,red-LUT} rely on successive ACT commands across different rows in a DRAM bank, performing a search to retrieve the correct results of the operations. These ACT commands are highly energy-intensive. As an example, in HBM2, the energy required to access a row constitutes 80\% of the total energy consumed for reading one memory block (8 bytes)~\cite{fine-grained}. In addition, these methods are constrained by the Four Activate Window (tFAW) timing parameter of DRAM. This parameter restricts the number of successive ACT commands that can be issued within a given time frame due to DRAM power budget constraints, thereby limiting the parallelism and overall performance of PuM techniques.

% Each LUT access involves using input operands to select the appropriate entry within the LUT, which incurs the cost of one ACT command to activate a row in the DRAM. To handle bulk operations, existing LUT-based techniques either perform successive ACT commands across different rows within the same subarray~\cite{pluto,ppim,reconfigurable_pim} or across the same row in different subarrays~\cite{red-LUT}.

\textbf{\textit{Handling Various Arithmetic Precisions}}: Limited support for high-precision arithmetic operations poses significant challenges in LUT-based schemes, especially when handling input operands that exceed 4-bit numerical precision. Many existing LUT-based techniques support only up to 4-bit operand pairs or single 8-bit operands~\cite{reconfigurable_pim}. For higher operand precision, some schemes~\cite{ppim, pluto, lacc} decompose high-precision computations into smaller, lower-precision segments, which can be processed independently across different subarrays. However, this approach adds considerable complexity due to the need for frequent inter-subarray data movement. For example, performing an 8-bit multiplication requires splitting it into four 4-bit multiplications, followed by an 8-stage accumulation process~\cite{ppim}. This accumulation process demands multiple ACT commands and internal data transfers, leading to increased latency and energy consumption. Thus, there is a lack of support for efficient low-cost high-precision arithmetic with PuM techniques.

\textbf{\textit{Exploiting the DRAM hierarchy}}: Each DRAM bank is composed of multiple subarrays and these are further divided into smaller units known as mats~\cite{processingDRAMparadigm}. Mats are constrained by the DRAM circuit design, which requires the mats to transfer consecutive column positions (e.g., 8-bit chunks) via the column select logic (CSL) to the global row buffer~\cite{fulcrum}. Many previous approaches~\cite{red-LUT,reconfigurable_pim, pluto} do not take into account the internal mat structure when deciding their data placement and layout, resulting in reduced parallelism and inefficiencies during LUT accesses.

\textbf{Our goal} in this paper is to overcome the limitations of existing PuM schemes by introducing \textit{Lama}, a \underline{L}ightweight ~\underline{a}daptive~\underline{m}emory~\underline{a}ccess mechanism for executing bulk complex arithmetic operations using a LUT-based PuM approach. To demonstrate Lama's applicability for data-intensive applications, we also propose~\textit{LamaAccel}, an HBM-based PIM accelerator based on Lama to enhance the efficiency of Large Language Models (LLMs) for Machine Learning (ML). Lama redefines current LUT-based PuM designs by enhancing energy efficiency and scalability while maintaining high performance, without requiring changes to DRAM timing parameters or incurring significant area overheads.

Lama introduces an advanced PuM LUT mechanism optimized for arithmetic operations with two operands. To reduce the overhead of successive ACT commands during bulk operations, Lama leverages two key features of commodity DRAM: \textbf{mat-level parallelism} and the \textbf{open-page policy}.

%...whose first operand value is employed to indicate the row address and the second operand as the column address in the DRAM subarray.
% In conventional form of performing an operation with existing LUT-base schemes, every LUT access to perform one operation, requires one ACT command to access one row of a subarray and one read command to obtain the operation result which resides in specific columns.

Our approach consists of a lightweight mechanism that enables parallel column-independent accesses within each mat of a subarray. In SIMD operations where one operand is a scalar and thus it is the same for all the vector elements, Lama optimizes performance by requiring only a single ACT command for multiple LUT accesses. In this step, the scalar operand indexes the row page, while the vector elements enable simultaneous independent column accesses across different mats. This design is particularly effective for bulk multiplications involving a fixed operand, as commonly seen in vector-matrix and matrix-matrix multiplications, where the same input value is used in multiple operations. As illustrated in figure~\ref{fig:vector_matrix_mult}, a vector-matrix multiplication is broken down into several scalar-vector multiplication steps. A naive mechanism would require one LUT access for each individual multiplication, involving both an ACT command and a memory read command per operation. However, by grouping these operations into batches that share a scalar operand, all multiplications within the batch can be executed with a single ACT command followed by multiple memory read commands. This means that only the first LUT access requires an ACT command, which is then reused for the remaining LUT accesses within the batch. This approach reduces the need for total memory commands by $19.4x$ compared to a state-of-the-art work named pLUTo~\cite{pluto} for INT4 multiplications, thereby lowering overall latency and energy consumption.

% Therefore, exploiting mat-level parallelism helps to reuse the ACT commands for maximum \#mats operations. In the case of the number of SIMD operations being more than \#mats, we break the operations into groups with sizes of \#mats and execute each group sequentially. Since all groups have a common operand, the ACT commands indicate the same row.

\begin{figure}[t!]
\centering
\includegraphics[width=0.75\columnwidth]{figures/vector_matrix_mult.pdf}

\caption{Decomposing a vector-matrix multiplication into batches of scalar-vector operations, where each batch has a single scalar operand ($v_{1}, v_{2}, v_{3}$).}
\label{fig:vector_matrix_mult}
\vskip -0.15in
\end{figure}

Moreover, Lama draws inspiration from the mask logic circuit, found in commodity DRAM~\cite{micron_ddr4}, to support higher precision operations of up to 8-bit multiplications. Lama integrates easily with the existing commodity DRAM internal structure and organization, requiring minimal modifications with negligible (2.47\%) area overhead.

Finally, we introduce LamaAccel, an HBM-based PIM accelerator designed to enhance LLM inference by implementing our Lama PuM mechanism. LamaAccel utilizes a strategic data-mapping approach to allocate encoder-decoder blocks of LLMs across pseudo-channels within the HBM. This setup pipelines the processing of different layers within each encoder-decoder block, maximizing efficiency. LamaAccel integrates the DNA-TEQ~\cite{DNA-TEQ} scheme, an adaptive exponential quantization technique that simplifies complex multiply-and-add (MAC) operations into more efficient addition and counting operations (as detailed in Section~\ref{Exponential_quant}). This integration allows LamaAccel to support per-layer numerical precision of up to 8-bit, all while keeping accuracy loss below 1\%. This demonstrates the flexibility of the Lama technique in adapting to different arithmetic precisions, particularly in handling varying precision levels required by LLMs.

This paper makes the following key contributions:

\begin{itemize}

\item We present Lama, a novel lightweight memory access mechanism that enhances the efficiency of executing complex bulk arithmetic operations through a LUT-based PuM approach, exploiting the intrinsic mat-level parallelism and open-page policy of DRAM.

% \item We propose a lightweight handle mechanism to enable column-independent access to each mat within a subarray, thus using mat-interleaving for bulk arithmetic operations. Moreover, by inspiring from the mask logic unit proposed in commodity DRAM~\cite{micron_ddr4} organization, we implemented a simple mask logic unit inside each DRAM bank to select valid results for various operands' precision. Lama adds a small area cost to a state-of-the-art DRAM chip (1.x\%).

\item Lama supports multiplication operations with operand pairs of up to 8-bit precision and remains compatible with the standard DRAM timing parameters, introducing only a minimal area overhead of 2.47\%. We experimentally demonstrate that Lama significantly outperforms a PuM-based state-of-the-art approach~\cite{pluto} and a high-end CPU by $3.5\times$ and $3.8\times$ in performance and $8.3\times$ and $8\times$ in energy efficiency for bulk 8-bit multiplication tasks.

\item We propose LamaAccel, an HBM-based accelerator for attention-based models that implements our Lama PuM technique to perform addition and counting operations that replace traditional MAC operations. LamaAccel efficiently maps the encoder and decoder blocks of LLMs to different pseudo-channels within HBM, enabling parallel inference execution. We evaluate LamaAccel against a GPU, a TPU, and a state-of-the-art PuM-based technique~\cite{pluto} across various widely-used LLMs. The results show an average speedup of $4.1\times$$/7.2\times$ over TPU/GPU baselines and $1.7\times$ speedup compared to the state-of-the-art PuM technique. In terms of energy efficiency, LamaAccel achieves a $7.1\times$$/12\times$ energy reduction over TPU/GPU baselines, and $4\times$ energy savings compared to pLUTo.

%\item we explore the potentiality of Lama to accelerate attention layers in recent Large Language Models (LLMs). We employ DNA-TEQ~\cite{DNA-TEQ} approach, which is an adaptive exponential quantization technique for DNN tensors, to reduce the complexity of costly multiply-and-add (MAC) operations by converting them into simpler addition and counting operations. Since DNA-TEQ, assigns different bit-width precision to the layers of DNNs (3-bit to 7-bit) Lama can show its potential on how to handle scalability issues with operating with different precisions.

%In Lama, addition operations are implemented using LUTs, while counting operations are efficiently managed by reusing the existing collect-select logic. Lama includes two distinct execution flows for handling dot-product computation: First, the add-and-count operations, are performed in memory using the PuM approach, and the post-processing operations, which is handled by the implemented computational logic in the logic die of HBM. Hence, Lama is well-suited for efficiently handling vector-matrix multiplications, which are a core component of DNN operations.

%\item We experimentally demonstrate that Lama significantly outperforms a PuM-based state-of-the-art~\cite{pluto} proposal and a high-end CPU by x\% and y\% in performance and z\% and w\% in energy efficiency for bulk 8-bit multiplication tasks. Additionally, we evaluate LamaAccel and compare against a GPU, TPU, and a state-of-the-art PuM-based technique~\cite{pluto} across various widely-used LLMs, demonstrating an average speedup of 4.2x/?x and average energy savings of 7.0x/?x compared to TPU/GPU baselines.

\end{itemize}

% Paper Organization
