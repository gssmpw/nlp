\section{Conclusions}
In this paper, we proposed Lama, a lightweight adaptive memory access mechanism for executing bulk arithmetic operations in-memory for SIMD workloads. Lama is a LUT-based Processing-in-Memory (PuM) approach that enables parallel, column-independent accesses within each mat of a DRAM subarray. It supports operand precision of up to 8 bits while optimizing latency and energy efficiency by leveraging intrinsic DRAM features like mat-level parallelism and the open-page policy. Lama integrates seamlessly into existing DRAM architectures, incurring only a minimal area overhead of 2.47\%. In addition, Lama reduces the number of required commands for bulk operations by fivefold compared to state-of-the-art techniques like pLUTo. 

We also introduced LamaAccel, an HBM-based accelerator designed for large language models (LLMs) that provides efficient execution without requiring modifications to the DRAM timing parameters. LamaAccel tackles the challenge of accumulation in dot-product operations within memory by utilizing exponential quantization to simplify computations of activations and weights. Evaluations across various workloads demonstrate that LamaAccel significantly outperforms alternative platforms, including TPU, GPU, and pLUTo, in terms of both performance and energy efficiency.
