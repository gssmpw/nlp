Introduction:

Large Language Models (LLMs) have revolutionized the field of artificial intelligence, particularly in natural language processing~\cite{lipton}. These models such as GPT, BERT, BART, and T5 have demonstrated remarkable capabilities in understanding and generating human language, leading to advancements in applications ranging from machine translation and summarization to sentiment analysis and question-answering. The recent progress in LLMs is due to their complex architecture, which leverages attention mechanisms to process and generate text with high coherence and contextual relevance~\cite{attention}.

The attention mechanism has been widely used in recent LLMs as a powerful tool to capture contextual relationships in sequential data~\cite{attention}. It involves extensive matrix multiplications and memory-intensive operations, which account for 65-85\% of all computations~\cite{analysis_mms}. This necessitates substantial data movement between memory and computational units, resulting in significant latency and energy consumption. As LLMs scale up in size and complexity, these challenges become more pronounced, posing a bottleneck to their efficient deployment and performance. Despite these issues, LLMs have gained immense popularity due to their ability to perform a wide range of language-related tasks with high accuracy and minimal human intervention. However, their high memory bandwidth nature and the associated memory-intensive operations remain critical challenges that must be addressed to further enhance their efficiency and scalability. %add for statistic huawei, is it better to add a circle graph showing the percentage of each type of operations in LLMs?

~\textbf{Our goal} in this paper is to overcome the limitations of existing PuM schemes by introducing~\textit{Lama}, a \underline{L}ightweight ~\underline{a}daptive~\underline{m}emory~\underline{a}ccess mechanism for executing bulk complex arithmetic operations using a LUT-based PuM approach. To demonstrate Lama's applicability for data-intensive applications, we also propose~\textit{LamaAccel}, an HBM-based PIM accelerator that integrates with Lama to enhance the efficiency of Large Language Models (LLMs) for Machine Learning (ML). Lama redefines current LUT-based PuM designs by enhancing energy efficiency and scalability while maintaining high performance, without requiring changes to DRAM timing parameters or incurring significant area overheads.

Lama introduces an advanced PuM LUT mechanism optimized for arithmetic operations with two operands. To reduce the overhead of successive ACT commands during bulk operations, Lama leverages two key features of commodity DRAM:~\textbf{mat-level parallelism} and the~\textbf{open-page policy}.
%...whose first operand value is employed to indicate the row address and the second operand as the column address in the DRAM subarray. %In conventional form of performing an operation with existing LUT-base schemes, every LUT access to perform one operation, requires one ACT command to access one row of a subarray and one read command to obtain the operation result which resides in specific columns.

Our approach consists of a lightweight mechanism that enables parallel column-independent accesses within each mat of a subarray. In SIMD-like operations where one operand is common, Lama optimizes performance by requiring only a single LUT access. In this step, the common operand indexes the row page, while differing operands enable simultaneous independent column accesses across different mats. This design is particularly effective for bulk multiplications involving a fixed operand, as commonly seen in matrix-vector and matrix-matrix multiplications, where the same input value is used in multiple operations.
As illustrated in figure~\ref{fig:vector_matrix_mult}, matrix-vector multiplication is broken down into several scalar-vector multiplication steps, each with a fixed operand. By reusing the same ACT command for multiple LUT accesses, Lama efficiently handles these operations. This approach reduces the need for total memory commands by 5x, thereby lowering overall latency and energy consumption. 

%%% Extra Comments %%%
% Proposing a novel approach that enables parallel operations with higher precision operands without relying on successive ACT commands. This approach aims to enhance energy efficiency while maintaining high throughput. %In current memory designs, a DRAM subarray is partitioned into multiple 2D arrays of 512* 512 called a DRAM MAT.

% We introduce Lama, an energy-efficient LUT-based PIM scheme tailored for attention models. Lama takes advantage of inherent memory features such as MAT-level parallelism within subarrays and utilizes the open-page policy to support various precision levels while preserving throughput. Unlike earlier methods, Lama alleviates significantly the burden on the memory controller by reducing the number of required memory ACT commands significantly for bulk arithmetic operations compared with the previous PuM proposals.

% We aim to overcome these challenges by proposing Lama LUT-based technique that 1) keeps the existing memory timings for commodity DRAMs untouched 2) Scalable to different precisions.

% Ppim paper has a good numerical result compared with other previous PIM works (like drac or drisa) which are pim but not LUT-based. Since they reported exact numbers, we can easily add comparison with them. please consider it to be added in the evaluation for case 2 proposed method.

% Therefore, exploiting mat-level parallelism helps to reuse the ACT commands for maximum \#mats operations. In the case of the number of SIMD operations being more than \#mats, we break the operations into groups with sizes of \#mats and execute each group sequentially. Since all groups have a common operand, the ACT commands indicate the same row.

Case study1:

%To perform LUT-based computing, Lama defines LUT access by two consecutive memory commands. The first command activates a memory row according to the common operand and, the second command reads the batch of LUT values according to a subset of the uncommon operands in different mats. The batch size is determined by the operation precision and number of mats within a subarray. To have independent column selection in each mat, Lama replicates the Column Counter with the number of mats. Since Lama follows the commodity memory process, only one read/write command can issue at a time for a bank. Therefore, having #mats Column Counter per bank is enough. 

%To complete all operations in a group of operations with common operand, may require more than oe LUT access. For next LUT access, Lama only issues second command to perform the new batch of operations. Since the first command rely on the common operand which is same across all operations within a group, for the new batch of operations, Lama reuse the already opened row. Therefore, Lama only requires one ACT command per group of operations.

%optimizing LUT access patterns, leveraging mat-level parallelism within the DRAM hierarchy, and reusing previously accessed memory by exploiting already opened pages to perform additional operations via different columns within the same page. Lama can support not only 4-bit arithmetic operations supported by previous studies, but also until 8-bit arithmetic operations like multiplication without changing the structure of DRAM and using the already existing components inside memory banks. Lama only adds less than 2\% area overhead while maintaining the original structure of commodity DRAMs. 