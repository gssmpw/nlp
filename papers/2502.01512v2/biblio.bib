@misc{Aristimunha_Mother_of_all_2023,
  author  = {Aristimunha, Bruno and Carrara, Igor and Guetschel, Pierre and Sedlar, Sara and Rodrigues, Pedro and Sosulski, Jan and Narayanan, Divyesh and Bjareholt, Erik and Quentin, Barthelemy and Schirrmeister, Robin Tibor and Kalunga, Emmanuel and Darmet, Ludovic and Gregoire, Cattan and Abdul Hussain, Ali and Gatti, Ramiro and Goncharenko, Vladislav and Thielen, Jordy and Moreau, Thomas and Roy, Yannick and Jayaram, Vinay and Barachant, Alexandre and Chevallier, Sylvain},
  doi     = {10.5281/zenodo.10034223},
  title   = {{Mother of all BCI Benchmarks}},
  url     = {https://github.com/NeuroTechX/moabb},
  version = {1.0.0},
  year    = {2023}
}


@article{arsignyGeometricMeansNovel2006,
  title    = {Geometric {{Means}} in a {{Novel Vector Space Structure}} on {{Sysmetric Positive-Definite Matrices}}},
  author   = {Arsigny, Vincent and Fillard, Pierre and Pennec, Xavier and Ayache, Nicholas},
  year     = {2006},
  month    = jan,
  journal  = {SIAM J. Matrix Analysis Applications},
  volume   = {29},
  pages    = {328--347},
  doi      = {10.1137/050637996},
  abstract = {In this work we present a new generalization of the geometric mean of positive numbers on symmetric positive-definite matrices, called Log-Euclidean. The approach is based on two novel algebraic structures on symmetric positive-definite matrices: first, a lie group structure which is compatible with the usual algebraic properties of this matrix space; second, a new scalar multiplication that smoothly extends the Lie group structure into a vector space structure. From bi-invariant metrics on the Lie group structure, we define the Log-Euclidean mean from a Riemannian point of view. This notion coincides with the usual Euclidean mean associated with the novel vector space structure. Furthermore, this means corresponds to an arithmetic mean in the domain of matrix logarithms. We detail the invariance properties of this novel geometric mean and compare it to the recently introduced affine-invariant mean. The two means have the same determinant and are equal in a number of cases, yet they are not identical in general. Indeed, the Log-Euclidean mean has a larger trace whenever they are not equal. Last but not least, the Log-Euclidean mean is much easier to compute.},
  file     = {/Users/thibault/Zotero/storage/SSUIML7I/Arsigny et al. - 2006 - Geometric Means in a Novel Vector Space Structure .pdf}
}

@article{barachantMulticlassBrainComputer2012,
  title    = {Multiclass {{Brain}}--{{Computer Interface Classification}} by {{Riemannian Geometry}}},
  author   = {Barachant, A. and Bonnet, S. and Congedo, M. and Jutten, C.},
  year     = {2012},
  month    = apr,
  journal  = {IEEE Transactions on Biomedical Engineering},
  volume   = {59},
  number   = {4},
  pages    = {920--928},
  issn     = {0018-9294, 1558-2531},
  doi      = {10.1109/TBME.2011.2172210},
  urldate  = {2023-06-07},
  abstract = {This paper presents a new classification framework for Brain Computer Interface (BCI) based on motor imagery. This framework involves the concept of Riemannian geometry in the manifold of covariance matrices. The main idea is to use spatial covariance matrices as EEG signal descriptors and to rely on Riemannian geometry to directly classify these matrices using the topology of the manifold of Symmetric and Positive Definite (SPD) matrices. This framework allows to extract the spatial information contained in EEG signals without using spatial filtering. Two methods are proposed and compared with a reference method (multi-class Common Spatial Pattern (CSP) and Linear Discriminant Analysis (LDA)) on the multi-class dataset IIa from the BCI competition IV. The first method, named Minimum Distance to Riemanian Mean (MDRM), is an implementation of the Minimum Distance to Mean (MDM) classification algorithm using Riemannian distance and Riemannian mean. This simple method shows comparable results with the reference method. The second method, named Tangent Space LDA (TSLDA), maps the covariance matrices onto the Riemannian tangent space where matrices can be vectorized and treated as Euclidean objects. Then, a variable selection procedure is applied in order to decrease dimensionality and a classification by LDA is performed. This latter method outperforms the reference method increasing the mean classification accuracy from 65.1\% to 70.2\%.},
  langid   = {english}
}


@inproceedings{barachantRiemannianGeometryApplied2010,
  title     = {Riemannian Geometry Applied to {{BCI}} Classification},
  booktitle = {{{LVA}}/{{ICA}} 2010 - 9th {{International Conference}} on {{Latent Variable Analysis}} and {{Signal Separation}}},
  author    = {Barachant, Alexandre and Bonnet, Stephane and Congedo, Marco and Jutten, Christian},
  year      = {2010},
  month     = sep,
  volume    = {6365},
  pages     = {629},
  publisher = {Springer},
  doi       = {10.1007/978-3-642-15995-4_78},
  urldate   = {2023-05-25},
  abstract  = {In brain-computer interfaces based on motor imagery, covariance matrices are widely used through spatial filters computation and other signal processing methods. Covariance matrices lie in the space of Symmetric Positives-Definite (SPD) matrices and therefore, fall within the Riemannian geometry domain. Using a differential geometry framework, we propose different algorithms in order to classify covariance matrices in their native space.},
  langid    = {english},
  file      = {/Users/thibault/Zotero/storage/DCUIJGTI/Barachant et al_2010_Riemannian geometry applied to BCI classiﬁcation.pdf}
}


@book{bhatiaPositiveDefiniteMatrices2007,
  title      = {Positive Definite Matrices},
  author     = {Bhatia, Rajendra},
  year       = {2007},
  series     = {Princeton Series in Applied Mathematics},
  publisher  = {Princeton University Press},
  address    = {Princeton, N.J},
  isbn       = {978-0-691-12918-1},
  langid     = {english},
  lccn       = {QA188 .B488 2007},
  keywords   = {Matrices},
  annotation = {OCLC: ocm70668921},
  file       = {/Users/thibault/Zotero/storage/PSLRCDHV/Bhatia - 2007 - Positive definite matrices.pdf}
}

@book{bishop2007,
  added-at    = {2009-06-02T09:46:22.000+0200},
  asin        = {0387310738},
  author      = {Bishop, Christopher M.},
  biburl      = {https://www.bibsonomy.org/bibtex/2d21de30a3a67c0f9f3c96bd6eec3267a/midtiby},
  description = {Amazon.com: Pattern Recognition and Machine Learning (Information Science and Statistics): Christopher M. Bishop: Books},
  dewey       = {006.4},
  ean         = {9780387310732},
  edition     = 1,
  interhash   = {f6fec2ccd82dec0dcd63825e301662cf},
  intrahash   = {d21de30a3a67c0f9f3c96bd6eec3267a},
  isbn        = {0387310738},
  keywords    = {algorithms machinelearning patternrecognition statistics},
  publisher   = {Springer},
  timestamp   = {2009-06-02T15:22:29.000+0200},
  title       = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  year        = 2007
}


@article{BNCI2014004,
  author   = {Leeb, Robert and Lee, Felix and Keinrath, Claudia and Scherer, Reinhold and Bischof, Horst and Pfurtscheller, Gert},
  journal  = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  title    = {Brain–Computer Communication: Motivation, Aim, and Impact of Exploring a Virtual Apartment},
  year     = {2007},
  volume   = {15},
  number   = {4},
  pages    = {473-482},
  keywords = {Electroencephalography;Neurofeedback;Synchronous motors;Virtual reality;Laboratories;Navigation;Virtual environment;Image analysis;Brain;Communication system control;Brain–computer interface (BCI);electroencephalogram (EEG);motivation;motor imagery;navigation;neutral cue;virtual environment (VE);virtual reality (VR);Brain-Computer Interface (BCI);electroen-cephalogram (EEG);motor imagery;navigation;virtual reality (VR);virtual environment (VE);neutral cue;motivation},
  doi      = {10.1109/TNSRE.2007.906956}
}

@book{bogachevMeasureTheory2007,
  title     = {Measure {{Theory}}},
  author    = {Bogachev, Vladimir I.},
  year      = {2007},
  publisher = {Springer},
  address   = {Berlin, Heidelberg},
  doi       = {10.1007/978-3-540-34514-5},
  urldate   = {2024-07-24},
  copyright = {http://www.springer.com/tdm},
  isbn      = {978-3-540-34513-8 978-3-540-34514-5},
  langid    = {english},
  keywords  = {convergence of measures,Derivative,differential equation,Lebesgue integral,linear optimization,measure,Measure theory,Transformation,transformation of mesures}
}

@article{bouchard2024random,
  title   = {Random matrix theory improved Fr$\backslash$'echet mean of symmetric positive definite matrices},
  author  = {Bouchard, Florent and Mian, Ammar and Tiomoko, Malik and Ginolhac, Guillaume and Pascal, Fr{\'e}d{\'e}ric},
  journal = {arXiv preprint arXiv:2405.06558},
  year    = {2024}
}

@book{boumalIntroductionOptimizationSmooth2023,
  title     = {An {{Introduction}} to {{Optimization}} on {{Smooth Manifolds}}},
  author    = {Boumal, Nicolas},
  year      = {2023},
  month     = mar,
  edition   = {1},
  publisher = {Cambridge University Press},
  doi       = {10.1017/9781009166164},
  urldate   = {2023-04-11},
  abstract  = {Optimization on Riemannian manifolds-the result of smooth geometry and optimization merging into one elegant modern framework-spans many areas of science and engineering, including machine learning, computer vision, signal processing, dynamical systems and scientific computing. This text introduces the differential geometry and Riemannian geometry concepts that will help students and researchers in applied mathematics, computer science and engineering gain a firm mathematical grounding to use these tools confidently in their research. Its charts-last approach will prove more intuitive from an optimizer's viewpoint, and all definitions and theorems are motivated to build time-tested optimization algorithms. Starting from first principles, the text goes on to cover current research on topics including worst-case complexity and geodesic convexity. Readers will appreciate the tricks of the trade for conducting research and for numerical implementations sprinkled throughout the book.},
  isbn      = {978-1-00-916616-4 978-1-00-916617-1 978-1-00-916615-7}
}

@article{breizhcrops2020,
  title   = {BreizhCrops: A Time Series Dataset for Crop Type Mapping},
  author  = {Ru{\ss}wurm, Marc and Pelletier, Charlotte and Zollner, Maximilian and Lef{\`e}vre, S{\'e}bastien and K{\"o}rner, Marco},
  journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences ISPRS (2020)},
  year    = {2020}
}

@inproceedings{brooksRiemannianBatchNormalization2019,
  author    = {Brooks, Daniel and Schwander, Olivier and Barbaresco, Frederic and Schneider, Jean-Yves and Cord, Matthieu},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Riemannian batch normalization for SPD neural networks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6e69ebbfad976d4637bb4b39de261bf7-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@book{CaseBerg,
  added-at             = {2009-10-28T04:42:52.000+0100},
  author               = {Casella, George and Berger, Roger},
  biburl               = {https://www.bibsonomy.org/bibtex/21597678f36e23439610affbf46adec1c/jwbowers},
  citeulike-article-id = {105644},
  date-added           = {2007-09-03 22:45:16 -0500},
  date-modified        = {2007-09-03 22:45:16 -0500},
  howpublished         = {{Textbook Binding}},
  interhash            = {2dd8caad6c0b6fb80e6334986a231a05},
  intrahash            = {1597678f36e23439610affbf46adec1c},
  isbn                 = {0534243126},
  keywords             = {methodology probability statistics},
  month                = {June},
  opturl               = {http://www.amazon.fr/exec/obidos/ASIN/0534243126/citeulike04-21},
  publisher            = {{Duxbury Resource Center}},
  timestamp            = {2009-10-28T04:42:57.000+0100},
  title                = {Statistical Inference},
  year                 = 2001
}

@article{Chen2010,
  added-at  = {2020-03-10T00:00:00.000+0100},
  author    = {Chen, Yilun and Wiesel, Ami and Eldar, Yonina C. and III, Alfred O. Hero},
  biburl    = {https://www.bibsonomy.org/bibtex/20eea755813e9432517e8e484443aae08/dblp},
  ee        = {https://doi.org/10.1109/TSP.2010.2053029},
  interhash = {6b7dd603a9226f012ee9b78c3b4c137a},
  intrahash = {0eea755813e9432517e8e484443aae08},
  journal   = {IEEE Trans. Signal Process.},
  keywords  = {dblp},
  number    = 10,
  pages     = {5016-5029},
  timestamp = {2020-03-11T12:04:14.000+0100},
  title     = {Shrinkage algorithms for MMSE covariance estimation.},
  url       = {http://dblp.uni-trier.de/db/journals/tsp/tsp58.html#ChenWEH10},
  volume    = 58,
  year      = 2010
}


@inproceedings{chenRiemannianMultinomialLogistics,
  title     = {Riemannian Multinomial Logistics Regression for {SPD} Neural Networks},
  author    = {Ziheng Chen and Yue Song and Gaowen Liu and Ramana Rao Kompella and Xiaojun Wu and Nicu Sebe},
  booktitle = {Conference on Computer Vision and Pattern Recognition 2024},
  year      = {2024}
}

@article{chevallierBiInvariantStatisticalModel2020,
  title     = {A {{Bi-Invariant Statistical Model Parametrized}} by {{Mean}} and {{Covariance}} on {{Rigid Motions}}},
  author    = {Chevallier, Emmanuel and Guigui, Nicolas},
  year      = {2020},
  month     = apr,
  journal   = {Entropy},
  volume    = {22},
  number    = {4},
  pages     = {432},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn      = {1099-4300},
  doi       = {10.3390/e22040432},
  urldate   = {2024-03-06},
  abstract  = {This paper aims to describe a statistical model of wrapped densities for bi-invariant statistics on the group of rigid motions of a Euclidean space. Probability distributions on the group are constructed from distributions on tangent spaces and pushed to the group by the exponential map. We provide an expression of the Jacobian determinant of the exponential map of     S E ( n )     which enables the obtaining of explicit expressions of the densities on the group. Besides having explicit expressions, the strengths of this statistical model are that densities are parametrized by their moments and are easy to sample from. Unfortunately, we are not able to provide convergence rates for density estimation. We provide instead a numerical comparison between the moment-matching estimators on     S E ( 2 )     and     R 3    , which shows similar behaviors.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid    = {english},
  keywords  = {density estimation,differential of the exponential,Euclidean groups,moment-matching estimator,rigid motions,sampling,wrapped distributions}
}

@article{chevallierExponentialWrappedDistributionsSymmetric2022,
  title   = {Exponential-{{Wrapped Distributions}} on {{Symmetric Spaces}}},
  author  = {Chevallier, Emmanuel and Li, Didong and Lu, Yulong and Dunson, David},
  year    = {2022},
  month   = dec,
  journal = {SIAM Journal on Mathematics of Data Science},
  volume  = {4},
  number  = {4},
  pages   = {1347--1368},
  issn    = {2577-0187},
  doi     = {10.1137/21M1461551},
  urldate = {2024-02-06},
  langid  = {english}
}
@inproceedings{chevallierWrappedStatisticalModels2020,
  title      = {Wrapped Statistical Models on Manifolds: Motivations, the Case {{SE}}(n), and Generalization to Symmetric Spaces},
  shorttitle = {Wrapped Statistical Models on Manifolds},
  booktitle  = {Joint {{Structures}} and {{Common Foundations}} of {{Statistical Physics}}, {{Information Geometry}} and {{Inference}} for {{Learning}}},
  author     = {Chevallier, Emmanuel and Guigui, Nicolas},
  year       = {2020},
  month      = jul,
  address    = {Les Houches, France},
  urldate    = {2024-02-06},
  abstract   = {We address here the construction of wrapped probability densities on Lie groups and quotient of Lie groups using the exponential map. The paper starts by briefly reviewing the different approaches to build densities on a manifold and shows the interest of wrapped distributions. We then construct wrapped densities on SE(n) and discuss their statistical estimation. We conclude by an opening to the case of symmetric spaces.},
  keywords   = {exponential map,moment matching estimator,Non-Euclidean statistics,wrapped distributions}
}


@inproceedings{
choRotatedHyperbolicWrapped2022,
title={A Rotated Hyperbolic Wrapped Normal Distribution for Hierarchical Representation Learning},
author={Seunghyuk Cho and Juyong Lee and Jaesik Park and Dongwoo Kim},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=rHnbVaqzXne}
}

@article{collas2021,
  author   = {Collas, Antoine and Bouchard, Florent and Breloy, Arnaud and Ginolhac, Guillaume and Ren, Chengfang and Ovarlez, Jean-Philippe},
  journal  = {IEEE Transactions on Signal Processing},
  title    = {Probabilistic PCA From Heteroscedastic Signals: Geometric Framework and Application to Clustering},
  year     = {2021},
  volume   = {69},
  number   = {},
  pages    = {6546-6560},
  keywords = {Geometry;Estimation;Signal processing algorithms;Measurement;Manifolds;Principal component analysis;Optimization;Covariance matrices;probabilistic pca;heteroscedastic data;robust estimation;Riemannian optimization;clustering},
  doi      = {10.1109/TSP.2021.3130997}
}

@article{collettDiscriminatingMisesWrapped1981,
  title    = {Discriminating {{Between}} the {{Von Mises}} and {{Wrapped Normal Distributions}}},
  author   = {Collett, D. and Lewis, T.},
  year     = {1981},
  journal  = {Australian Journal of Statistics},
  volume   = {23},
  number   = {1},
  pages    = {73--79},
  issn     = {1467-842X},
  doi      = {10.1111/j.1467-842X.1981.tb00763.x},
  urldate  = {2024-10-10},
  abstract = {In the analysis of circular data, we encounter two models which lay claim to the title of normal distribution: the von Mises and wrapped normal distributions. In this paper, we consider whether it is possible to distinguish between them in a practical situation. More specifically, given data from a circular distribution which may be either von Mises or wrapped normal, we enquire as to how large a sample is required in order to tell them apart.},
  langid   = {english}
}

@article{criscitielloAcceleratedFirstorderMethod2021,
  title = {An {{Accelerated First-Order Method}} for {{Non-convex Optimization}} on {{Manifolds}}},
  author = {Criscitiello, Christopher and Boumal, Nicolas},
  year = {2023},
  month = aug,
  journal = {Foundations of Computational Mathematics},
  volume = {23},
  number = {4},
  pages = {1433--1509},
  issn = {1615-3383},
  doi = {10.1007/s10208-022-09573-9},
  abstract = {We describe the first gradient methods on Riemannian manifolds to achieve accelerated rates in the non-convex case. Under Lipschitz assumptions on the Riemannian gradient and Hessian of the cost function, these methods find approximate first-order critical points faster than regular gradient descent. A randomized version also finds approximate second-order critical points. Both the algorithms and their analyses build extensively on existing work in the Euclidean case. The basic operation consists in running the Euclidean accelerated gradient descent method (appropriately safe-guarded against non-convexity) in the current tangent space, then moving back to the manifold and repeating. This requires lifting the cost function from the manifold to the tangent space, which can be done for example through the Riemannian exponential map. For this approach to succeed, the lifted cost function (called the pullback) must retain certain Lipschitz properties. As a contribution of independent interest, we prove precise claims to that effect, with explicit constants. Those claims are affected by the Riemannian curvature of the manifold, which in turn affects the worst-case complexity bounds for our optimization algorithms.}
}


@article{daletskii1965integration,
  title   = {Integration and differentiation of functions of Hermitian operators and applications to the theory of perturbations},
  author  = {Daletskii, Ju L and Krein, Selim Grigorievich},
  journal = {AMS Translations (2)},
  volume  = {47},
  number  = {1-30},
  pages   = {10--1090},
  year    = {1965}
}


@article{delmas2024elliptically,
  title     = {Elliptically symmetric distributions in signal processing and machine learning},
  author    = {Delmas, Jean-Pierre and El Korso, Mohammed Nabil and Pascal, F and Fortunati, S},
  journal   = {Springer Nature},
  year      = {2024},
  publisher = {Springer}
}

@incollection{delmasBackgroundRealComplex2023,
  title     = {Background on Real and Complex Elliptically Symmetric Distributions},
  booktitle = {Elliptically Symmetric Distributions in {{Signal Processing}}},
  author    = {Delmas, Jean-Pierre},
  editor    = {{Springer}},
  year      = {2023},
  urldate   = {2024-10-29},
  abstract  = {This chapter presents a short overview of real elliptically symmetric (RES) distributions, complemented by circular complex elliptically symmetric (C-CES) and noncircular CES (NC-CES) distributions as complex representations of RES distributions. These distributions are both an extension of the multivariate Gaussian distribution and a multivariate extension of univariate symmetric distributions. They are equivalently defined through their characteristic functions and their stochastic representations, which naturally follow from the spherically symmetric distributions after affine transformations. Particular attention is paid to the absolutely continuous case and to the subclass of compound Gaussian distributions. Results related to moments, affine transformations, marginal and conditional distributions, and summation stability are also presented. Some well-known instances of RES distributions are provided with their main properties. Finally, the estimation of the symmetry center and scatter matrix is briefly discussed through the sample mean (SM), sample covariance matrix (SCM) estimate, maximum estimate (ML), M-estimators, and Tyler's M-estimators. Particular attention will be paid to the asymptotic Gaussianity of the M-estimators of the scatter matrix. To conclude, some hints about the Slepian-Bangs formula are provided.},
  keywords  = {Background,Circular complex system,Complex elliptically symmetric CES distribution,Sample covariance matrices,Sample mean,Symmetric distribution},
  file      = {/Users/thibault/Zotero/storage/TQK3V8EU/Delmas - 2023 - Background on real and complex elliptically symmet.pdf}
}

@article{dingDeepGenerativeModel2021,
  title     = {Deep Generative Model Embedding of Single-Cell {{RNA-Seq}} Profiles on Hyperspheres and Hyperbolic Spaces},
  author    = {Ding, Jiarui and Regev, Aviv},
  year      = {2021},
  month     = may,
  journal   = {Nature Communications},
  volume    = {12},
  number    = {1},
  pages     = {2554},
  publisher = {Nature Publishing Group},
  issn      = {2041-1723},
  doi       = {10.1038/s41467-021-22851-4},
  urldate   = {2024-12-06},
  abstract  = {Single-cell RNA-Seq (scRNA-seq) is invaluable for studying biological systems. Dimensionality reduction is a crucial step in interpreting the relation between cells in scRNA-seq data. However, current dimensionality reduction methods are often confounded by multiple simultaneous technical and biological variability, result in ``crowding'' of cells in the center of the latent space, or inadequately capture temporal relationships. Here, we introduce scPhere, a scalable deep generative model to embed cells into low-dimensional hyperspherical or hyperbolic spaces to accurately represent scRNA-seq data. ScPhere addresses multi-level, complex batch factors, facilitates the interactive visualization of large datasets, resolves cell crowding, and uncovers temporal trajectories. We demonstrate scPhere on nine large datasets in complex tissue from human patients or animal development. Our results show how scPhere facilitates the interpretation of scRNA-seq data by generating batch-invariant embeddings to map data from new individuals, identifies cell types affected by biological variables, infers cells' spatial positions in pre-defined biological specimens, and highlights complex cellular relations.},
  copyright = {2021 The Author(s)},
  langid    = {english},
  keywords  = {Computational models,Machine learning},
  file      = {/Users/thibault/Zotero/storage/G9I48AR3/Ding et Regev - 2021 - Deep generative model embedding of single-cell RNA.pdf}
}

@article{dudoitComparisonDiscriminationMethods2002,
  author    = {Sandrine Dudoit, Jane Fridlyand and Terence P Speed},
  title     = {Comparison of Discrimination Methods for the Classification of Tumors Using Gene Expression Data},
  journal   = {Journal of the American Statistical Association},
  volume    = {97},
  number    = {457},
  pages     = {77--87},
  year      = {2002},
  publisher = {ASA Website},
  doi       = {10.1198/016214502753479248},
  url       = { 
               
               https://doi.org/10.1198/016214502753479248},
  eprint    = { https://doi.org/10.1198/016214502753479248}
}

@article{galaz-garciaWrappedDistributionsHomogeneous2022,
  title={Wrapped distributions on homogeneous Riemannian manifolds},
  author={Galaz-Garcia, Fernando and Papamichalis, Marios and Turnbull, Kathryn and Lunagomez, Simon and Airoldi, Edoardo},
  journal={arXiv preprint arXiv:2204.09790},
  year={2022}
}

@book{hastieElementsStatisticalLearning2009,
  title     = {The {{Elements}} of {{Statistical Learning}}},
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year      = {2009},
  series    = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer},
  address   = {New York, NY},
  doi       = {10.1007/978-0-387-84858-7},
  urldate   = {2023-08-17},
  isbn      = {978-0-387-84857-0 978-0-387-84858-7},
  keywords  = {Averaging,Boosting,classification,clustering,data mining,machine learning,Projection pursuit,Random Forest,supervised learning,Support Vector Machine,unsupervised learning}
}


@inproceedings{haubergDirectionalStatisticsSpherical2018,
  title     = {Directional {{Statistics}} with the {{Spherical Normal Distribution}}},
  booktitle = {2018 21st {{International Conference}} on {{Information Fusion}} ({{FUSION}})},
  author    = {Hauberg, S{\o}ren},
  year      = {2018},
  month     = jul,
  pages     = {704--711},
  doi       = {10.23919/ICIF.2018.8455242},
  urldate   = {2023-11-27}
}

@article{huaCompetingPM2NO22021,
  title    = {Competing {{PM2}}.5 and {{NO2}} Holiday Effects in the {{Beijing}} Area Vary Locally Due to Differences in Residential Coal Burning and Traffic Patterns},
  author   = {Hua, Jinxi and Zhang, Yuanxun and {de Foy}, Benjamin and Mei, Xiaodong and Shang, Jing and Feng, Chuan},
  year     = {2021},
  month    = jan,
  journal  = {Science of The Total Environment},
  volume   = {750},
  pages    = {141575},
  issn     = {0048-9697},
  doi      = {10.1016/j.scitotenv.2020.141575},
  urldate  = {2023-12-01},
  abstract = {The holiday effect is a useful tool to estimate the impact on air pollution due to changes in human activities. In this study, we assessed the variations in concentrations of fine particulate matter (PM2.5) and nitrogen dioxide (NO2) during the holidays in the heating season from 2014 to 2018 based on daily surface air quality monitoring measurements in Beijing. A Generalized Additive Model (GAM) is used to analyze pollutant concentrations for 34 sites by comprehensively accounting for annual, monthly, and weekly cycles as well as the nonlinear impacts of meteorological factors. A Saturday effect was found in the downtown area, with about 4\% decrease in PM2.5 and 3\% decrease in NO2 relative to weekdays. On Sundays, the PM2.5 concentrations increased by about 5\% whereas there were no clear changes for NO2. In contrast to the small effect of the weekend, there was a strong holiday effect throughout the region with average increases of about 22\% in PM2.5 and average reductions of about 11\% in NO2 concentrations. There was a clear geographical pattern in the strength of the holiday effect. In rural areas the increase in PM2.5 is related to the proportion of coal and biomass consumption for household heating. In the suburban areas between the Fifth Ring Road and Sixth Ring Road there were larger reductions in NO2 than downtown which might be due to decreased traffic as many people return to their hometown for the holidays. This study provides insights into the pattern of changes in air pollution due to human activities. By quantifying the changes, it also provides insights for improvements in air quality due to control policies implemented in Beijing during the heating season.},
  keywords = {GAM analysis,Holiday effects,Residential coal burning,Spatial variations,Traffic emissions},
  file     = {/Users/thibault/Zotero/storage/T69R77N5/Hua et al_2021_Competing PM2.pdf;/Users/thibault/Zotero/storage/XSUFLZNW/S0048969720351044.html}
}

@inproceedings{huang2017riemannian,
  title     = {A riemannian network for spd matrix learning},
  author    = {Huang, Zhiwu and Van Gool, Luc},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {31},
  year      = {2017}
}

@book{johnson1987multivariate,
  title     = {Multivariate Statistical Simulation: A Guide to Selecting and Generating Continuous Multivariate Distributions},
  author    = {Johnson, M.E.},
  year      = {1987},
  series    = {Wiley Series in Probability and Statistics},
  publisher = {Wiley},
  isbn      = {978-0-471-82290-5},
  lccn      = {lc86022469}
}

@article{jona-lasinioSpatialAnalysisWave2012,
  title         = {Spatial Analysis of Wave Direction Data Using Wrapped {{Gaussian}} Processes},
  author        = {{Jona-Lasinio}, Giovanna and Gelfand, Alan and {Jona-Lasinio}, Mattia},
  year          = {2012},
  month         = dec,
  journal       = {The Annals of Applied Statistics},
  volume        = {6},
  number        = {4},
  eprint        = {1301.1446},
  primaryclass  = {stat},
  issn          = {1932-6157},
  doi           = {10.1214/12-AOAS576},
  urldate       = {2024-12-06},
  abstract      = {Directional data arise in various contexts such as oceanography (wave directions) and meteorology (wind directions), as well as with measurements on a periodic scale (weekdays, hours, etc.). Our contribution is to introduce a model-based approach to handle periodic data in the case of measurements taken at spatial locations, anticipating structured dependence between these measurements. We formulate a wrapped Gaussian spatial process model for this setting, induced from a customary linear Gaussian process. We build a hierarchical model to handle this situation and show that the fitting of such a model is possible using standard Markov chain Monte Carlo methods. Our approach enables spatial interpolation (and can accommodate measurement error). We illustrate with a set of wave direction data from the Adriatic coast of Italy, generated through a complex computer model.},
  archiveprefix = {arXiv},
  langid        = {english},
  keywords      = {Statistics - Applications},
  file          = {/Users/thibault/Zotero/storage/B5H6NXIX/Jona-Lasinio et al. - 2012 - Spatial analysis of wave direction data using wrap.pdf}
}

@book{kagan1973characterization,
  title     = {Characterization Problems in Mathematical Statistics},
  author    = {Kagan, A.M. and Linnik, I.U.V. and Rao, C.R.},
  year      = {1973},
  series    = {Probability and Statistics Series},
  publisher = {Wiley},
  isbn      = {978-0-471-45421-2},
  lccn      = {73009643}
}

@article{Kingma2013AutoEncodingVB,
  title   = {Auto-Encoding Variational Bayes},
  author  = {Diederik P. Kingma and Max Welling},
  journal = {CoRR},
  year    = {2013},
  volume  = {abs/1312.6114},
  url     = {https://api.semanticscholar.org/CorpusID:216078090}
}

@article{lebrigantApproximationDensitiesRiemannian2019,
  title     = {Approximation of {{Densities}} on {{Riemannian Manifolds}}},
  author    = {Le Brigant, Alice and Puechmorel, St{\'e}phane},
  year      = {2019},
  month     = jan,
  journal   = {Entropy},
  volume    = {21},
  number    = {1},
  pages     = {43},
  publisher = {MDPI},
  doi       = {10.3390/e21010043},
  urldate   = {2024-07-11},
  abstract  = {Finding an approximate probability distribution best representing a sample on a measure space is one of the most basic operations in statistics. Many procedures were designed for that purpose when the underlying space is a finite dimensional Euclidean space. In applications, however, such a simple setting may not be adapted and one has to consider data living on a Riemannian manifold. The lack of unique generalizations of the classical distributions, along with theoretical and numerical obstructions require several options to be considered. The present work surveys some possible extensions of well known families of densities to the Riemannian setting, both for parametric and non-parametric estimation.},
  keywords  = {directional densities,exponential family,group invariance,quantization,Riemannian manifold},
  file      = {/Users/thibault/Zotero/storage/H8PPL6UI/Le Brigant et Puechmorel - 2019 - Approximation of Densities on Riemannian Manifolds.pdf}
}

@article{ledoitWellconditionedEstimatorLargedimensional2004,
  title    = {A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices},
  author   = {Ledoit, Olivier and Wolf, Michael},
  year     = {2004},
  month    = feb,
  journal  = {Journal of Multivariate Analysis},
  volume   = {88},
  number   = {2},
  pages    = {365--411},
  issn     = {0047259X},
  doi      = {10.1016/S0047-259X(03)00096-4},
  urldate  = {2023-06-28},
  abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For largedimensional covariance matrices, the usual estimator{\textemdash}the sample covariance matrix{\textemdash}is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte-Carlo confirm that the asymptotic results tend to hold well in finite sample.},
  langid   = {english}
}

@book{leeIntroductionRiemannianManifolds2018,
  title     = {Introduction to {{Riemannian Manifolds}}},
  author    = {Lee, John M.},
  year      = {2018},
  series    = {Graduate {{Texts}} in {{Mathematics}}},
  volume    = {176},
  publisher = {Springer International Publishing},
  address   = {Cham},
  doi       = {10.1007/978-3-319-91755-9},
  urldate   = {2024-03-14},
  isbn      = {978-3-319-91754-2 978-3-319-91755-9},
  keywords  = {comparison theory,curvature,curvature and topology,differential geometry textbook,Gauss-Bonnet theorem,geodesics,graduate mathematics textbook,Jacobi fields,Levi-Cevita connection,manifold,Riemannian geometry,Riemannian geometry course textbook,Riemannian metrics,Riemannian submanifolds,tensor}
}

@book{LehmCase98,
  title     = {Theory of Point Estimation},
  author    = {Erich L. Lehmann and George Casella},
  publisher = {Springer-Verlag},
  year      = {1998},
  address   = {New York, NY, USA},
  edition   = {Second}
}

@article{liuWrappedGaussianProcess2024,
  title={Wrapped Gaussian Process Functional Regression Model for Batch Data on Riemannian Manifolds},
  author={Liu, Jinzhao and Liu, Chao and Shi, Jian Qing and Nye, Tom},
  journal={arXiv preprint arXiv:2409.03181},
  year={2024}
}
@article{lopezcustodio2024cheatsheetprobabilitydistributions,
  title={A cheat sheet for probability distributions of orientational data},
  author={Lopez-Custodio, PC},
  journal={arXiv preprint arXiv:2412.08934},
  year={2024}
}
@article{lotteReviewClassificationAlgorithms2018,
  title      = {A Review of Classification Algorithms for {{EEG-based}} Brain--Computer Interfaces: A 10 Year Update},
  shorttitle = {A Review of Classification Algorithms for {{EEG-based}} Brain--Computer Interfaces},
  author     = {Lotte, F. and Bougrain, L. and Cichocki, A. and Clerc, M. and Congedo, M. and Rakotomamonjy, A. and Yger, F.},
  year       = {2018},
  month      = apr,
  journal    = {Journal of Neural Engineering},
  volume     = {15},
  number     = {3},
  pages      = {031005},
  publisher  = {IOP Publishing},
  issn       = {1741-2552},
  doi        = {10.1088/1741-2552/aab2f2},
  urldate    = {2023-04-13},
  abstract   = {Objective. Most current electroencephalography (EEG)-based brain--computer interfaces (BCIs) are based on machine learning algorithms. There is a large diversity of classifier types that are used in this field, as described in our 2007 review paper. Now, approximately ten years after this review publication, many new algorithms have been developed and tested to classify EEG signals in BCIs. The time is therefore ripe for an updated review of EEG classification algorithms for BCIs. Approach. We surveyed the BCI and machine learning literature from 2007 to 2017 to identify the new classification approaches that have been investigated to design BCIs. We synthesize these studies in order to present such algorithms, to report how they were used for BCIs, what were the outcomes, and to identify their pros and cons. Main results. We found that the recently designed classification algorithms for EEG-based BCIs can be divided into four main categories: adaptive classifiers, matrix and tensor classifiers, transfer learning and deep learning, plus a few other miscellaneous classifiers. Among these, adaptive classifiers were demonstrated to be generally superior to static ones, even with unsupervised adaptation. Transfer learning can also prove useful although the benefits of transfer learning remain unpredictable. Riemannian geometry-based methods have reached state-of-the-art performances on multiple BCI problems and deserve to be explored more thoroughly, along with tensor-based methods. Shrinkage linear discriminant analysis and random forests also appear particularly useful for small training samples settings. On the other hand, deep learning methods have not yet shown convincing improvement over state-of-the-art BCI methods. Significance. This paper provides a comprehensive overview of the modern classification algorithms used in EEG-based BCIs, presents the principles of these methods and guidelines on when and how to use them. It also identifies a number of challenges to further advance EEG classification in BCI.},
  langid     = {english},
  file       = {/Users/thibault/Zotero/storage/TUHB9VSE/Lotte et al. - 2018 - A review of classification algorithms for EEG-base.pdf}
}


@inproceedings{mallastoWrappedGaussianProcess2018,
  title     = {Wrapped {{Gaussian Process Regression}} on {{Riemannian Manifolds}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author    = {Mallasto, Anton and Feragen, Aasa},
  year      = {2018},
  month     = jun,
  pages     = {5580--5588},
  publisher = {IEEE},
  address   = {Salt Lake City, UT},
  doi       = {10.1109/CVPR.2018.00585},
  urldate   = {2024-12-06},
  abstract  = {Gaussian process (GP) regression is a powerful tool in non-parametric regression providing uncertainty estimates. However, it is limited to data in vector spaces. In fields such as shape analysis and diffusion tensor imaging, the data often lies on a manifold, making GP regression nonviable, as the resulting predictive distribution does not live in the correct geometric space. We tackle the problem by defining wrapped Gaussian processes (WGPs) on Riemannian manifolds, using the probabilistic setting to generalize GP regression to the context of manifold-valued targets. The method is validated empirically on diffusion weighted imaging (DWI) data, directional data on the sphere and in the Kendall shape space, endorsing WGP regression as an efficient and flexible tool for manifold-valued regression.},
  isbn      = {978-1-5386-6420-9},
  langid    = {english},
  file      = {/Users/thibault/Zotero/storage/5XRZIGEZ/Mallasto et Feragen - 2018 - Wrapped Gaussian Process Regression on Riemannian .pdf}
}

@book{mardiaDirectionalStatistics2000,
  title     = {Directional Statistics},
  author    = {Mardia, Kantilal V. and Jupp, Peter E.},
  year      = {2000},
  series    = {Wiley Series in Probability and Statistics},
  edition   = {New ed.},
  publisher = {Wiley},
  address   = {Chichester},
  isbn      = {978-0-471-95333-3},
  langid    = {english}
}

@inproceedings{mathieuContinuousHierarchicalRepresentations2019,
  title     = {Continuous {{Hierarchical Representations}} with {{Poincar{\'e} Variational Auto-Encoders}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author    = {Mathieu, Emile and Le Lan, Charline and Maddison, Chris J. and Tomioka, Ryota and Teh, Yee Whye},
  year      = {2019},
  volume    = {32},
  publisher = {Curran Associates, Inc.},
  urldate   = {2024-10-30},
  abstract  = {The Variational Auto-Encoder (VAE)  is a popular method for learning a generative model and embeddings of the data. Many real datasets are hierarchically structured. However, traditional VAEs map data in a Euclidean latent space which cannot efficiently embed tree-like structures. Hyperbolic spaces with negative curvature can. We therefore endow VAEs with a Poincar{\'e} ball model of hyperbolic geometry as a latent space and rigorously derive the necessary methods to work with two main Gaussian generalisations on that space. We empirically show better generalisation to unseen data than the Euclidean counterpart, and can qualitatively and quantitatively better recover hierarchical structures.}
}

@article{moakherDifferentialGeometricApproach2005,
  title    = {A {{Differential Geometric Approach}} to the {{Geometric Mean}} of {{Symmetric Positive-Definite Matrices}}},
  author   = {Moakher, Maher},
  year     = {2005},
  month    = jan,
  journal  = {SIAM Journal on Matrix Analysis and Applications},
  volume   = {26},
  number   = {3},
  pages    = {735--747},
  issn     = {0895-4798, 1095-7162},
  doi      = {10.1137/S0895479803436937},
  urldate  = {2023-04-19},
  abstract = {In this paper we introduce metric-based means for the space of positive-definite matrices. The mean associated with the Euclidean metric of the ambient space is the usual arithmetic mean. The mean associated with the Riemannian metric corresponds to the geometric mean. We discuss some invariance properties of the Riemannian mean and we use differential geometric tools to give a characterization of this mean.},
  langid   = {english}
}

@article{motor_imagery,
  author   = {Pfurtscheller, G. and Neuper, C.},
  journal  = {Proceedings of the IEEE},
  title    = {Motor imagery and direct brain-computer communication},
  year     = {2001},
  volume   = {89},
  number   = {7},
  pages    = {1123-1134},
  keywords = {Electroencephalography;Sensor arrays;Brain computer interfaces;Foot;Adaptive arrays;Electrodes;Biological neural networks;Neurofeedback;State feedback;Prototypes},
  doi      = {10.1109/5.939829}
}


@article{MVTec2021,
  author     = {Bergmann, Paul and Batzner, Kilian and Fauser, Michael and Sattlegger, David and Steger, Carsten},
  title      = {The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection},
  year       = {2021},
  issue_date = {Apr 2021},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {129},
  number     = {4},
  issn       = {0920-5691},
  url        = {https://doi.org/10.1007/s11263-020-01400-4},
  doi        = {10.1007/s11263-020-01400-4},
  journal    = {Int. J. Comput. Vision},
  month      = apr,
  pages      = {1038–1059},
  numpages   = {22},
  keywords   = {Anomaly detection, Novelty detection, Datasets, Unsupervised learning, Defect segmentation}
}


@inproceedings{naganoWrappedNormalDistribution2019,
  title={A wrapped normal distribution on hyperbolic space for gradient-based learning},
  author={Nagano, Yoshihiro and Yamaguchi, Shoichiro and Fujita, Yasuhiro and Koyama, Masanori},
  booktitle={International Conference on Machine Learning},
  pages={4693--4702},
  year={2019},
  organization={PMLR}
}

@inproceedings{nguyenGeomNet,
  title       = {{GeomNet: A Neural Network Based on Riemannian Geometries of SPD Matrix Space and Cholesky Space for 3D Skeleton-Based Interaction Recognition}},
  author      = {Nguyen, Xuan Son},
  url         = {https://hal.science/hal-03720244},
  booktitle   = {{2021 IEEE/CVF International Conference on Computer Vision (ICCV)}},
  address     = {Montreal, Canada},
  year        = {2021},
  month       = Oct,
  doi         = {10.1109/ICCV48922.2021.01313},
  pdf         = {https://hal.science/hal-03720244v1/file/08523.pdf},
  hal_id      = {hal-03720244},
  hal_version = {v1}
}

@article{pennecCurvatureEffectsEmpirical2019,
  title={Curvature effects on the empirical mean in Riemannian and affine Manifolds: a non-asymptotic high concentration expansion in the small-sample regime},
  author={Pennec, Xavier},
  journal={arXiv preprint arXiv:1906.07418},
  year={2019}
}

@article{pennecIntrinsicStatisticsRiemannian2006,
  title      = {Intrinsic {{Statistics}} on {{Riemannian Manifolds}}: {{Basic Tools}} for {{Geometric Measurements}}},
  shorttitle = {Intrinsic {{Statistics}} on {{Riemannian Manifolds}}},
  author     = {Pennec, Xavier},
  year       = {2006},
  month      = jul,
  journal    = {Journal of Mathematical Imaging and Vision},
  volume     = {25},
  number     = {1},
  pages      = {127--154},
  issn       = {0924-9907, 1573-7683},
  doi        = {10.1007/s10851-006-6228-4},
  urldate    = {2023-11-27},
  langid     = {english}
}

@incollection{pennecManifoldvaluedImageProcessing2020,
  title     = {{{Manifold-valued}} Image Processing with {{SPD}} Matrices},
  booktitle = {Riemannian {{Geometric Statistics}} in {{Medical Image Analysis}}},
  author    = {Pennec, Xavier},
  editor    = {Pennec, Xavier and Sommer, Stefan and Fletcher, Tom},
  year      = {2020},
  month     = jan,
  pages     = {75--134},
  publisher = {Academic Press},
  doi       = {10.1016/B978-0-12-814725-2.00010-8},
  urldate   = {2024-01-25},
  abstract  = {Symmetric positive definite (SPD) matrices are geometric data that appear in many applications. They are used in particular in Diffusion Tensor Imaging (DTI) as a simple model of the anisotropic diffusion of water in the tissues. This chapter extends the Riemannian computing statistical estimation framework of chapters 1 and 2 to manifold-valued images with the example of SPD matrices. SPD matrices constitute a smooth but incomplete manifold with the classical Euclidean metric on matrices. This creates important computational problems for image processing since we easily pass the boundaries to end-up with negative eigenvalues. This chapter describes how we can give this space more interesting properties thanks to invariance: asking the Riemannian metric on covariance matrices to be invariant by affine changes of coordinates of the space leads to a one-parameter family of affine-invariant metrics that share the same affine connection. The geodesics are thus the same for all these metrics, even if the distance in the direction of scaling is different. With this structure, SPD matrices with null eigenvalues are at an infinite distance of any SPD matrix, so that they cannot be reached in finite time. The space obtained is a typical example of a Hadamard space of nonpositive curvature. It is also a symmetric Riemannian space of nonconstant curvature. This can be seen thanks to the explicit computation of the Riemannian, sectional and Ricci curvatures. Thanks to the relative simplicity of the geodesic equation, we can work out in detail the algorithms for statistical estimation, for instance, the Fr{\'e}chet mean computation. Moreover, the knowledge of the Ricci curvature allows us to see clearly what is controlling the difference between tangent PCA and Principal Geodesic Analysis (PGA). Building on the reformulation of the mean value in manifolds as the minimization of an intrinsic functional, we also generalize many important image processing algorithms such as interpolation, filtering, diffusion and restoration of missing data to manifold-valued images. We illustrate this framework on diffusion tensor image processing and on the modeling of the brain variability from a dataset of lines on the cerebral cortex. We also discuss alternative choices to the affine-invariant metrics. Log-Euclidean metrics, for instance, provide a very fast approximation when the data are concentrated with respect to the curvature. Other metrics give to the SPD space a positive curvature with a boundary at finite distance for rank-deficient matrices. Since the metric determines the main properties of the space, we should carefully review the assumptions that we can do on our data. Once the metric is chosen, the implementation of the geodesics (Exp and Log) provides once again the basis for all our manifold-valued image processing algorithms.},
  isbn      = {978-0-12-814725-2},
  keywords  = {affine-invariant distance,Cholesky metric,Diffusion Tensor Imaging,invariant Riemannian metric,log-Euclidean metric,power-Euclidean metric,Procrustes metric,Symmetric positive definite matrix}
}

@misc{peyreComputationalOptimalTransport2020,
  title         = {Computational {{Optimal Transport}}},
  author        = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year          = {2020},
  month         = mar,
  number        = {arXiv:1803.00567},
  eprint        = {1803.00567},
  primaryclass  = {stat},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1803.00567},
  urldate       = {2023-05-16},
  abstract      = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archiveprefix = {arXiv},
  keywords      = {Statistics - Machine Learning}
}


@article{pymanopt,
  author  = {James Townsend and Niklas Koep and Sebastian Weichwald},
  journal = {Journal of Machine Learning Research},
  number  = {137},
  pages   = {1–5},
  title   = {Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation},
  url     = {http://jmlr.org/papers/v17/16-177.html},
  volume  = {17},
  year    = {2016}
}


@misc{pyriemann,
  author    = {Alexandre Barachant and
               Quentin Barthélemy and
               Jean-Rémi King and
               Alexandre Gramfort and
               Sylvain Chevallier and
               Pedro L. C. Rodrigues and
               Emanuele Olivetti and
               Vladislav Goncharenko and
               Gabriel Wagner vom Berg and
               Ghiles Reguig and
               Arthur Lebeurrier and
               Erik Bjäreholt and
               Maria Sayu Yamamoto and
               Pierre Clisson and
               Marie-Constance Corsi and
               Igor Carrara and
               Apolline Mellot and
               Bruna Junqueira Lopes and
               Brent Gaisford and
               Ammar Mian and
               Anton Andreev and
               Gregoire Cattan and
               Arthur Lebeurrier},
  title     = {pyRiemann},
  month     = oct,
  year      = 2024,
  version   = {v0.7},
  publisher = {Zenodo},
  doi       = {10.5281/zenodo.593816},
  url       = {https://doi.org/10.5281/zenodo.593816}
}

@article{saidGaussianDistributionsRiemannian2018,
  title      = {Gaussian {{Distributions}} on {{Riemannian Symmetric Spaces}}: {{Statistical Learning With Structured Covariance Matrices}}},
  shorttitle = {Gaussian {{Distributions}} on {{Riemannian Symmetric Spaces}}},
  author     = {Said, Salem and Hajri, Hatem and Bombrun, Lionel and Vemuri, Baba C.},
  year       = {2018},
  month      = feb,
  journal    = {IEEE Transactions on Information Theory},
  volume     = {64},
  number     = {2},
  pages      = {752--772},
  issn       = {1557-9654},
  doi        = {10.1109/TIT.2017.2713829},
  keywords   = {Covariance matrices,Estimation,Extraterrestrial measurements,Gaussian distribution,Gaussian mixture model,Riemannian barycentre,Riemannian symmetric space,Statistical learning,Structured covariance matrix,Symmetric matrices}
}

      

@article{sanborn2024beyond,
  title   = {Beyond euclid: An illustrated guide to modern machine learning with geometric, topological, and algebraic structures},
  author  = {Sanborn, Sophia and Mathe, Johan and Papillon, Mathilde and Buracas, Domas and Lillemark, Hansen J and Shewmake, Christian and Bertics, Abby and Pennec, Xavier and Miolane, Nina},
  journal = {arXiv preprint arXiv:2407.09468},
  year    = {2024}
}

@article{saracenoRobustEstimationMultivariate2021,
  title    = {Robust Estimation for Multivariate Wrapped Models},
  author   = {Saraceno, Giovanni and Agostinelli, Claudio and Greco, Luca},
  year     = {2021},
  month    = aug,
  journal  = {METRON},
  volume   = {79},
  number   = {2},
  pages    = {225--240},
  issn     = {2281-695X},
  doi      = {10.1007/s40300-021-00214-9},
  urldate  = {2024-10-10},
  langid   = {english},
  keywords = {CEM algorithm,Multivariate wrapped distributions,Pearson residuals,Robust estimators,Torus,Weighted likelihood}
}


@article{scikit-learn,
  title   = {Scikit-learn: Machine Learning in {P}ython},
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
             and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
             and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
             Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011}
}

@incollection{shigaHadamardManifolds1984,
  title     = {Hadamard {{Manifolds}}},
  booktitle = {Geometry of {{Geodesics}} and {{Related Topics}}},
  author    = {Shiga, Kiyoshi},
  year      = {1984},
  month     = jan,
  volume    = {3},
  pages     = {239--282},
  publisher = {Mathematical Society of Japan},
  doi       = {10.2969/aspm/00310239},
  urldate   = {2024-07-24},
  abstract  = {Advanced Studies in Pure Mathematics}
}

@article{smithDataAnalysisUsing2022,
  title={Multi-site, multi-pollutant atmospheric data analysis using Riemannian geometry},
  author={Smith, Alexander and Hua, Jinxi and de Foy, Benjamin and Schauer, James J and Zavala, Victor M},
  journal={Science of The Total Environment},
  volume={892},
  pages={164064},
  year={2023},
  publisher={Elsevier}
}

@misc{smithMultiSiteMultiPollutantAtmospheric2022,
  type     = {{{SSRN Scholarly Paper}}},
  title    = {Multi-{{Site}}, {{Multi-Pollutant Atmospheric Data Analysis Using Riemannian Geometry}}},
  author   = {Smith, Alexander and Hua, Jinxi and {de Foy}, Benjamin and Schauer, James and Zavala, Victor},
  year     = {2022},
  month    = dec,
  number   = {4309521},
  address  = {Rochester, NY},
  doi      = {10.2139/ssrn.4309521},
  urldate  = {2023-11-28},
  abstract = {We demonstrate the benefits of using Riemannian geometry in the analysis of multi-site, multi-pollutant atmospheric monitoring data. Our approach uses covariance matrices to encode spatio-temporal variability and correlations of multiple pollutants at different sites and times. A key property of covariance matrices is that they lie on a Riemannian manifold and one can exploit this property to facilitate dimensionality reduction, outlier detection, and spatial interpolation. Specifically, the transformation of data using Reimannian geometry provides a better data surface for interpolation and assessment of outliers compared to traditional data analysis tools that assume Euclidean geometry. We demonstrate the utility of using Riemannian geometry by analyzing a full year of atmospheric monitoring data collected from 34 monitoring stations in Beijing, China.},
  langid   = {english},
  keywords = {Atmospheric monitoring,Data science,Dimensionality reduction,Multivariate analysis,Outlier detection,Spatial interpolation.},
  file     = {/Users/thibault/Zotero/storage/PCPL32LM/Smith et al_2022_Multi-Site, Multi-Pollutant Atmospheric Data Analysis Using Riemannian Geometry.pdf}
}

@article{sraConicGeometricOptimisation2015,
  title         = {Conic Geometric Optimisation on the Manifold of Positive Definite Matrices},
  author        = {Sra, Suvrit and Hosseini, Reshad},
  year          = {2015},
  month         = jan,
  journal       = {SIAM Journal on Optimization},
  volume        = {25},
  number        = {1},
  eprint        = {1312.1039},
  primaryclass  = {math},
  pages         = {713--739},
  issn          = {1052-6234, 1095-7189},
  doi           = {10.1137/140978168},
  urldate       = {2023-10-26},
  abstract      = {We develop {\textbackslash}emph\{geometric optimisation\} on the manifold of Hermitian positive definite (HPD) matrices. In particular, we consider optimising two types of cost functions: (i) geodesically convex (g-convex); and (ii) log-nonexpansive (LN). G-convex functions are nonconvex in the usual euclidean sense, but convex along the manifold and thus allow global optimisation. LN functions may fail to be even g-convex, but still remain globally optimisable due to their special structure. We develop theoretical tools to recognise and generate g-convex functions as well as cone theoretic fixed-point optimisation algorithms. We illustrate our techniques by applying them to maximum-likelihood parameter estimation for elliptically contoured distributions (a rich class that substantially generalises the multivariate normal distribution). We compare our fixed-point algorithms with sophisticated manifold optimisation methods and obtain notable speedups.},
  archiveprefix = {arXiv},
  keywords      = {Mathematics - Functional Analysis},
  file          = {/Users/thibault/Zotero/storage/LAJWSUB8/Sra_Hosseini_2015_Conic geometric optimisation on the manifold of positive definite matrices.pdf;/Users/thibault/Zotero/storage/8V7N3UC4/1312.html}
}

@book{terrasHarmonicAnalysisSymmetric1988,
  title     = {Harmonic {{Analysis}} on {{Symmetric Spaces}} and {{Applications II}}},
  author    = {Terras, Audrey},
  year      = {1988},
  publisher = {Springer},
  address   = {New York, NY},
  doi       = {10.1007/978-1-4612-3820-1},
  urldate   = {2024-03-11},
  isbn      = {978-0-387-96663-2 978-1-4612-3820-1},
  langid    = {english},
  keywords  = {automorphic forms,boundary element method,differential operator,distribution,function,functions,geometry,integration,matrices,metric space,modular form,multivariate statistics,statistics,theorem,Theta function}
}

@article{troshin2023wrapped,
  title   = {Wrapped \${\textbackslash}beta\$-Gaussians with compact support for exact probabilistic modeling on manifolds},
  author  = {Sergey Troshin and Vlad Niculae},
  journal = {Transactions on Machine Learning Research},
  issn    = {2835-8856},
  year    = {2023},
  url     = {https://openreview.net/forum?id=KrequDpWzt},
  note    = {}
}

@article{turagaStatisticalComputationsGrassmann2011,
  title     = {Statistical {{Computations}} on {{Grassmann}} and {{Stiefel Manifolds}} for {{Image}} and {{Video-Based Recognition}}},
  author    = {Turaga, P. and Veeraraghavan, A. and Srivastava, A. and Chellappa, R.},
  year      = {2011},
  month     = nov,
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {33},
  number    = {11},
  pages     = {2273--2286},
  issn      = {0162-8828, 2160-9292},
  doi       = {10.1109/TPAMI.2011.52},
  urldate   = {2024-12-06},
  abstract  = {In this paper, we examine image and video-based recognition applications where the underlying models have a special structure---the linear subspace structure. We discuss how commonly used parametric models for videos and image sets can be described using the unified framework of Grassmann and Stiefel manifolds. We first show that the parameters of linear dynamic models are finite-dimensional linear subspaces of appropriate dimensions. Unordered image sets as samples from a finite-dimensional linear subspace naturally fall under this framework. We show that an inference over subspaces can be naturally cast as an inference problem on the Grassmann manifold. To perform recognition using subspace-based models, we need tools from the Riemannian geometry of the Grassmann manifold. This involves a study of the geometric properties of the space, appropriate definitions of Riemannian metrics, and definition of geodesics. Further, we derive statistical modeling of inter and intraclass variations that respect the geometry of the space. We apply techniques such as intrinsic and extrinsic statistics to enable maximum-likelihood classification. We also provide algorithms for unsupervised clustering derived from the geometry of the manifold. Finally, we demonstrate the improved performance of these methods in a wide variety of vision applications such as activity recognition, video-based face recognition, object recognition from image sets, and activity-based video clustering.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid    = {english},
  file      = {/Users/thibault/Zotero/storage/9JDDA9BF/Turaga et al. - 2011 - Statistical Computations on Grassmann and Stiefel .pdf}
}

@article{Tuzel2008,
  author   = {Tuzel, Oncel and Porikli, Fatih and Meer, Peter},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Pedestrian Detection via Classification on Riemannian Manifolds},
  year     = {2008},
  volume   = {30},
  number   = {10},
  pages    = {1713-1727},
  keywords = {Object detection;Manifolds;Feature extraction;Covariance matrix;Boosting;Computer vision;Detectors;Shape;Principal component analysis;Dictionaries;Object recognition;Scene Analysis;Image Processing and Computer Vision;Computing Methodologies;Vision and Scene Understanding;Machine learning;Object recognition;Scene Analysis;Image Processing and Computer Vision;Computing Methodologies;Vision and Scene Understanding;Machine learning},
  doi      = {10.1109/TPAMI.2008.75}
}

@book{wassermanAllStatisticsConcise2004,
  title      = {All of {{Statistics}}: {{A Concise Course}} in {{Statistical Inference}}},
  shorttitle = {All of {{Statistics}}},
  author     = {Wasserman, Larry},
  year       = {2004},
  series     = {Springer {{Texts}} in {{Statistics}}},
  publisher  = {Springer},
  address    = {New York, NY},
  doi        = {10.1007/978-0-387-21736-9},
  urldate    = {2023-09-11},
  isbn       = {978-1-4419-2322-6 978-0-387-21736-9},
  langid     = {english},
  keywords   = {Bootstrapping,classification,data mining,machine learning,Mathematica,mathematical statistics,Random variable,ROOT,STATISTICA}
}
@article{willjuiceiruthayarajanCovarianceMatrixAdaptation2010,
  title    = {Covariance Matrix Adaptation Evolution Strategy Based Design of Centralized {{PID}} Controller},
  author   = {Willjuice Iruthayarajan, M. and Baskar, S.},
  year     = {2010},
  month    = aug,
  journal  = {Expert Systems with Applications},
  volume   = {37},
  number   = {8},
  pages    = {5775--5781},
  issn     = {0957-4174},
  doi      = {10.1016/j.eswa.2010.02.031},
  urldate  = {2024-01-25},
  abstract = {In this paper, design of centralized PID controller using Covariance Matrix Adaptation Evolution Strategy (CMAES) is presented. Binary distillation column plant described by Wood and Berry (WB) having two inputs and two outputs and by Ogunnike and Ray (OR) having three inputs and three outputs are considered for the design of multivariable PID controller. Optimal centralized PID controller is designed by minimizing IAE for servo response with unit step change. Simulations are carried out using SIMULINK-MATLAB software. The statistical performances of the designed controllers such as best, mean, standard deviations of IAE and average functional evaluations for 20 independent trials. For the purpose of comparison, recent version of real coded Genetic Algorithm (RGA) with simulated binary crossover (SBX) and conventional BLT method are used. In order to validate the performance of optimal PID controller for robustness against load disturbance rejection, load regulation experiment with step load disturbance is conducted. Also, to determine the performance of optimal PID controller for robustness against model uncertainty, servo and load response with +20\% variations in gains and dead times is conducted. Simulation results reveal that for both OR and WB systems, CMAES designed centralized PID controller is better than other methods and also it is more robust against model uncertainty and load disturbance.},
  keywords = {CMAES,Controller tuning,MIMO system,PID control,Real coded Genetic Algorithm},
  file     = {/Users/thibault/Zotero/storage/AAUJY7F7/S0957417410000709.html}
}

@article{Zhou2016,
  doi       = {10.1371/journal.pone.0162657},
  author    = {Zhou, Bangyan AND Wu, Xiaopei AND Lv, Zhao AND Zhang, Lei AND Guo, Xiaojin},
  journal   = {PLOS ONE},
  publisher = {Public Library of Science},
  title     = {A Fully Automated Trial Selection Method for Optimization of Motor Imagery Based Brain-Computer Interface},
  year      = {2016},
  month     = {09},
  volume    = {11},
  url       = {https://doi.org/10.1371/journal.pone.0162657},
  pages     = {1-20},
  abstract  = {Independent component analysis (ICA) as a promising spatial filtering method can separate motor-related independent components (MRICs) from the multichannel electroencephalogram (EEG) signals. However, the unpredictable burst interferences may significantly degrade the performance of ICA-based brain-computer interface (BCI) system. In this study, we proposed a new algorithm frame to address this issue by combining the single-trial-based ICA filter with zero-training classifier. We developed a two-round data selection method to identify automatically the badly corrupted EEG trials in the training set. The “high quality” training trials were utilized to optimize the ICA filter. In addition, we proposed an accuracy-matrix method to locate the artifact data segments within a single trial and investigated which types of artifacts can influence the performance of the ICA-based MIBCIs. Twenty-six EEG datasets of three-class motor imagery were used to validate the proposed methods, and the classification accuracies were compared with that obtained by frequently used common spatial pattern (CSP) spatial filtering algorithm. The experimental results demonstrated that the proposed optimizing strategy could effectively improve the stability, practicality and classification performance of ICA-based MIBCI. The study revealed that rational use of ICA method may be crucial in building a practical ICA-based MIBCI system.},
  number    = {9}
}

@misc{InidianaPines,
	title = {220 Band AVIRIS Hyperspectral Image Data Set: June 12, 1992 Indian Pine Test Site 3},
	month = {Sep},
	url = {https://purr.purdue.edu/publications/1947/1},
	year = {2015},
	doi = {doi:/10.4231/R7RX991C},
	author = {Marion F. Baumgardner and Larry L. Biehl and David A. Landgrebe }
}

@inproceedings{jo2024riemannian,
  author    = {Jaehyeong Jo and
               Sung Ju Hwang},
  title     = {Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes},
  booktitle = {International Conference on Machine Learning},
  year      = {2024},
}