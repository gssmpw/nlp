\section{Related work}
Other works have already tried to extend the Gaussian distribution to a Riemannian manifold. \citet{saidGaussianDistributionsRiemannian2018} propose an isotropic Gaussian on a Riemannian Symmetric Space $\mathcal{M}$ defined using a center of mass $\bar{y} \in \mathcal{M}$ and a scaling factor $\sigma > 0$. 
In our work, we are looking for a more complex model, requiring a non-isotropic distribution with some preferred directions.  
A non-isotropic Gaussian distribution on a manifold has been proposed in \citet{pennecIntrinsicStatisticsRiemannian2006}, in which the authors use the characterization of the Gaussian distribution as the distribution that maximizes entropy given a mean and a covariance matrix (theorem 13.2.2 of \citealt{kagan1973characterization}). However, sampling this distribution leads to computational difficulties, the normalizing constant cannot be computed explicitly and in the case of a full covariance matrix, the estimator of the parameters becomes problematic.  

Wrapped distributions have first been studied in directional statistics \citep{mardiaDirectionalStatistics2000}, on a circle \citep{collettDiscriminatingMisesWrapped1981} or on a sphere \citep{haubergDirectionalStatisticsSpherical2018}. Wrapped Gaussians have also been instantiated on hyperbolic spaces, first by \citet{naganoWrappedNormalDistribution2019} and then by \citet{mathieuContinuousHierarchicalRepresentations2019} and \citet{choRotatedHyperbolicWrapped2022}. They mainly use it as the distribution of the latent space of a Variational Autoencoder \citep{Kingma2013AutoEncodingVB} which is trained to learn the distribution. Apart from the manifold, another difference with our approach is that they wrap the Gaussian using a composition of the exponential map with parallel transport where we will only use the exponential map.
Wrapped distributions have also been studied on a more general Riemannian manifold. For example, \citet{galaz-garciaWrappedDistributionsHomogeneous2022} define wrapped distributions on homogeneous Riemannian manifolds. A major difference with our work is that they use a volume preserving map to push-forward the density from the tangent space to the manifold, leading to a simpler expression of the density, without any volume change term. In \citet{chevallierExponentialWrappedDistributionsSymmetric2022} and \citet{chevallierWrappedStatisticalModels2020}, the authors work on symmetric spaces. They mainly study the Jacobian determinant of the exponential map, first in a general setting and then on different examples (Grassmannians, pseudohyperboloids and special Euclidean group). Unlike our work, they consider the distribution on the tangent space to always be centered, where we consider a more general setting by allowing $\mu \neq 0$. To estimate the parameters of their wrapped Gaussians, they use moment estimation.
Finally, in \citet{troshin2023wrapped}, they define a more general wrapped Gaussian, the $\beta-$Gaussian that has a compact support. %This is useful when the manifold is not complete.

In the sequel, we propose a  wrapped Gaussian on the manifold of SPD matrices that is not centered on the tangent space. After deriving some theoretical properties, we show that our wrapped Gaussian can be used in practice, showcasing the estimation of the parameters from a finite sample. Finally, we use our wrapped Gaussian to build a framework that unifies and generalizes classification on SPD matrices, and propose new classifiers. This application shows the potential of our wrapped Gaussian to become a generic, flexible and powerful tool for manifold-based data analysis.