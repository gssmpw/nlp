\documentclass[reqno,11pt]{amsart}
\usepackage{amsaddr}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
%\usepackage[left=3cm,right=3cm,top=2.5cm,bottom=2cm]{geometry}
\usepackage[top=1in, bottom=1in, left=1.25in, right=1.25in]{geometry}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{natbib}
\usepackage{xpatch}
\xpatchcmd{\paragraph}{\normalfont}{{\normalfont\bfseries}}{}{}

\usepackage{hyperref}



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{nicefrac}
\usepackage{bm}
\usepackage{relsize}
%\usepackage{authblk}
\usepackage{fancyhdr} % Pour personnaliser les en-têtes et pieds de page


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


\pagestyle{fancy} 
\fancyhf{} % Supprime les en-têtes et pieds de page par défaut
\fancyfoot[C]{\thepage} % Place le numéro de page en bas au centre
%\fancyhead[C]{\tiny{\MakeUppercase{\shorttitle}}}
\renewcommand{\headrulewidth}{0pt} % Supprime la ligne horizontale en haut
\renewcommand{\footrulewidth}{0pt} % Supprime la ligne horizontale en bas

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\d}{\mathrm{d}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Log}{Log}
\DeclareMathOperator{\Vect}{Vect}
\DeclareMathOperator{\WG}{WG}
\DeclareMathOperator{\WEC}{WEC}
\DeclareMathOperator{\EC}{EC}
\DeclareMathOperator{\Vlog}{VLog}

\newcommand{\bigodotp}{%
  \mathop{\mathlarger{\mathlarger{~ \bigodot}}{}_{p}}
}

\title[Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices]{Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices}

\author{Thibault \MakeLowercase{de} Surrel}
\address{LAMSADE, CNRS, PSL University Paris-Dauphine, France}
\email{thibault.de-surrel@lamsade.dauphine.fr}
\author{Fabien Lotte}
\address{Inria center at the University of Bordeaux / LaBRI, France}
\author{Sylvain Chevallier}
\address{TAU, LISN, University Paris-Saclay, France}
\author{Florian Yger}
\address{LITIS, INSA de Rouen-Normandy, France}



\begin{document}

\begin{abstract}

Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks.
A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution.
In this work, we tackle those issue by focusing on the manifold of symmetric positive definite matrices, a key focus in information geometry.
We introduced a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model.
Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis.
This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data. 

\end{abstract}
%\keywords{Symmetric Positive Definite matrices, Riemannian geometry, Wrapped Gaussian, Maximum Likelihood Estimation, Classification}
\maketitle

\section{Introduction}
When dealing with complex data, modeling them as lying on a manifold often provides a powerful solution \citep{sanborn2024beyond, jo2024riemannian}. However, classical Euclidean probability distributions fail to capture the intrinsic geometry of the underlying manifold. This limitation necessitates adapting the choice of probability distributions to respect the manifold’s structure. In this work, we propose a solution by leveraging the exponential map to wrap probability distributions defined in the Euclidean tangent space onto the manifold itself. This approach yields a \emph{wrapped distribution}, intrinsically aligned with the manifold’s geometry. Wrapped distributions have found applications across many domains, such as embedding single-cell RNA data \citep{dingDeepGenerativeModel2021}, analyzing wave patterns \citep{jona-lasinioSpatialAnalysisWave2012}, recognizing video and image features \citep{turagaStatisticalComputationsGrassmann2011}, modeling Gaussian processes on manifolds \citep{mallastoWrappedGaussianProcess2018, liuWrappedGaussianProcess2024} or doing statistics on measurements of orientations \citep{lopezcustodio2024cheatsheetprobabilitydistributions}. 

Here, we focus on wrapping Gaussian distributions on the Riemannian manifold of \emph{Symmetric Positive Definite} (SPD) matrices. Gaussian distributions are a cornerstone of machine learning and statistics (see p.102 of \citealt{CaseBerg}) due to their ubiquity and theoretical underpinnings, such as the Central Limit Theorem (CLT) (Section 5.4 of \citealt{wassermanAllStatisticsConcise2004}), which ensures that Gaussian distributions naturally arise in many scenarios. In this work, we extend the CLT to wrapped Gaussians, providing a theoretical justification for their study. 

We chose to focus on SPD matrices, which form the Riemannian manifold $\P_d$, as they are pivotal in numerous applications, including \emph{Diffusion Tensor Imaging} \citep{pennecManifoldvaluedImageProcessing2020}, \emph{Brain-Computer Interfaces} (BCI) \citep{lotteReviewClassificationAlgorithms2018}, \emph{process control} \citep{willjuiceiruthayarajanCovarianceMatrixAdaptation2010}, and \emph{video processing} \citep{Tuzel2008}. The inherent Riemannian geometry of $\P_d$ necessitates adopting methods that respect its manifold structure when analyzing SPD data.

This paper is organized as follows: in \cref{sec:SPD_matrices}, we introduce the Riemannian geometry of $\P_d$. Next, we define wrapped Gaussians on $\P_d$ and explore their theoretical properties in \cref{sec:wrapped_gaussains}. %We address the non-identifiability of wrapped Gaussian parameters and propose a resolution in \cref{sec:equiv_gaussian}. 
In \cref{sec:estimation}, we develop a Maximum Likelihood Estimator for parameter estimation and validate it with synthetic experiments. Building on this foundation, \cref{sec:classification} revisits existing classifiers on $\P_d$ through a probabilistic lens and introduces novel classifiers based on wrapped Gaussians. Finally, we evaluate these classifiers with synthetic and real-world datasets in \cref{sec:classif_expe}.
\section{Related work}

Other works have already tried to extend the Gaussian distribution to a Riemannian manifold. \citet{saidGaussianDistributionsRiemannian2018} propose an isotropic Gaussian on a Riemannian Symmetric Space $\mathcal{M}$ defined using a center of mass $\bar{y} \in \mathcal{M}$ and a scaling factor $\sigma > 0$. 
In our work, we are looking for a more complex model, requiring a non-isotropic distribution with some preferred directions.  
A non-isotropic Gaussian distribution on a manifold has been proposed in \citet{pennecIntrinsicStatisticsRiemannian2006}, in which the authors use the characterization of the Gaussian distribution as the distribution that maximizes entropy given a mean and a covariance matrix (theorem 13.2.2 of \citealt{kagan1973characterization}). However, sampling this distribution leads to computational difficulties, the normalizing constant cannot be computed explicitly and in the case of a full covariance matrix, the estimator of the parameters becomes problematic.  

Wrapped distributions have first been studied in directional statistics \citep{mardiaDirectionalStatistics2000}, on a circle \citep{collettDiscriminatingMisesWrapped1981} or on a sphere \citep{haubergDirectionalStatisticsSpherical2018}. Wrapped Gaussians have also been instantiated on hyperbolic spaces, first by \citet{naganoWrappedNormalDistribution2019} and then by \citet{mathieuContinuousHierarchicalRepresentations2019} and \citet{choRotatedHyperbolicWrapped2022}. They mainly use it as the distribution of the latent space of a Variational Autoencoder \citep{Kingma2013AutoEncodingVB} which is trained to learn the distribution. Apart from the manifold, another difference with our approach is that they wrap the Gaussian using a composition of the exponential map with parallel transport where we will only use the exponential map.
Wrapped distributions have also been studied on a more general Riemannian manifold. For example, \citet{galaz-garciaWrappedDistributionsHomogeneous2022} define wrapped distributions on homogeneous Riemannian manifolds. A major difference with our work is that they use a volume preserving map to push-forward the density from the tangent space to the manifold, leading to a simpler expression of the density, without any volume change term. In \citet{chevallierExponentialWrappedDistributionsSymmetric2022} and \citet{chevallierWrappedStatisticalModels2020}, the authors work on symmetric spaces. They mainly study the Jacobian determinant of the exponential map, first in a general setting and then on different examples (Grassmannians, pseudohyperboloids and special Euclidean group). Unlike our work, they consider the distribution on the tangent space to always be centered, where we consider a more general setting by allowing $\mu \neq 0$. To estimate the parameters of their wrapped Gaussians, they use moment estimation.
Finally, in \citet{troshin2023wrapped}, they define a more general wrapped Gaussian, the $\beta-$Gaussian that has a compact support. %This is useful when the manifold is not complete.

In the sequel, we propose a  wrapped Gaussian on the manifold of SPD matrices that is not centered on the tangent space. After deriving some theoretical properties, we show that our wrapped Gaussian can be used in practice, showcasing the estimation of the parameters from a finite sample. Finally, we use our wrapped Gaussian to build a framework that unifies and generalizes classification on SPD matrices, and propose new classifiers. This application shows the potential of our wrapped Gaussian to become a generic, flexible and powerful tool for manifold-based data analysis.
\section{How to deal with SPD matrices ?}
\label{sec:SPD_matrices}
\subsection{The Riemannian geometry of SPD matrices}
\label{subsec:riem_geom_SPD}
The set of $d \times d$ \emph{symmetric, positive definite} (SPD) matrices, denoted $\P_d$ is defined as follows:
\begin{equation*}
    \P_d = \{p \in \R^{d \times d} \mid p^\top = p \text{ and } \forall x \in \R^d \setminus \{0\}, x^\top p x > 0 \}.
\end{equation*}
This set is convex and open in the set of $d\times d$ symmetric matrices $\S_d$ and thus, it is a \emph{manifold} of dimension $\nicefrac{d(d+1)}{2}$. For all $p \in \P_d$, the tangent space $T_p\P_d$ at $p$ can be identified with $\S_d$. Moreover, for $p \in \P_d$ one can define an inner product on the tangent space $T_p \P_d$ at $p$ by: 
    \begin{equation}
        \label{def:AIRM_metric}
        \forall u,v \in T_p \P_d,~ \langle u, v \rangle_p = \tr(p^{-1}up^{-1}v).
    \end{equation}
This inner product is called the \emph{Affine Invariant Riemannian Metric} (AIRM) \citep{pennecManifoldvaluedImageProcessing2020} as, if $a$ is an invertible matrix, one has $\langle aua^\top, ava^\top \rangle_{apa^\top} = \langle u, v \rangle_p$. Once endowed with this metric, $\P_d$ is a complete connected Riemannian manifold of non-positive curvature (see Appendix I of \citealt{criscitielloAcceleratedFirstorderMethod2021}). It is therefore a \emph{Hadamard manifold} \citep{shigaHadamardManifolds1984}. Using the Cartan–Hadamard theorem (theorem 12.8 of \citealt{leeIntroductionRiemannianManifolds2018}) one has that $\P_d$ is diffeomorphic to $\R^{\nicefrac{d(d+1)}{2}}$ through the exponential map $\Exp_p \colon T_p \P_d \simeq \R^{\nicefrac{d(d+1)}{2}} \rightarrow \P_d$.
Another consequence of the completeness of $\P_d$ is that each pair of points $(p,q) \in \P_d^2$ can be connected by a unique minimizing geodesic whose length defines a distance on $\P_d$. This AIRM distance, between $p$ and $q$ is given by:
\begin{equation}
    \label{eq:distance_AIRM}
    \delta(p,q) = \|\log(p^{-1/2}qp^{-1/2})\|_F
\end{equation}
where $\log$ is the matrix logarithm and $\| \cdot \|_F$ the Frobenius norm. Other useful tools of Riemannian geometry that will be used in the following are the exponential map $\Exp_p \colon T_p \P_d  \rightarrow \P_d$ and its inverse, the logarithm map $\Log_p \colon \P_d \rightarrow T_p \P_d$. For $p,q \in \P_d$ and $u \in T_p \P_d$, those maps are given by the following expressions (see chapter 6 of \citep{bhatiaPositiveDefiniteMatrices2007}):
\begin{equation}
    \label{eq:exp_log_riem}
    \begin{aligned}
        \Exp_p(u) &= p^{1/2}\exp(p^{-1/2}up^{-1/2})p^{1/2}, \\
        \Log_p(q) &= p^{1/2}\log(p^{-1/2}qp^{-1/2})p^{1/2}.
    \end{aligned}
\end{equation}
Finally, when $\P_d$ is equipped with the AIRM metric given at \cref{def:AIRM_metric}, the Riemannian volume element at $p = [[p_{ij}]] \in \P_d$ is given by (see Section 4.1.3 of \citealt{terrasHarmonicAnalysisSymmetric1988}):
\begin{equation}
    \label{eq:volume}
    \mathrm{d} \text{vol}(p) = \det(p)^{-(d+1)/2} \prod_{i \leq j} \mathrm{d}p_{ij}
\end{equation}
where $\mathrm{d}p_{ij}$ is the Lebesgue measure on $\R$. The volume $\mathrm{d} \text{vol}$ is also invariant under congruence by invertible matrices.
\subsection{The vectorization of the tangent spaces}
\label{subsec:vectorization}
As described in \cref{subsec:riem_geom_SPD}, the tangent space $T_p \P_d$ at point $p \in \P_d$ is identified with the space of $d\times d$ symmetric matrices $\S_d$ which is isomorphic to $\R^{\nicefrac{d(d+1)}{2}}$. We define such an isomorphism called the \emph{vectorization} as follows:
\begin{definition}[Vectorization]
    \label{def:vectorization}
    We start by defining the vectorization at identity for a symmetric matrix $u = [[u_{ij}]]$:
    \begin{equation*}
        \begin{aligned}
            \Vect_{I_d} \colon u \in T_{I_d} \P_d \mapsto &(u_{11}, \sqrt{2}u_{12}, u_{22}, \sqrt{2}u_{13}, \sqrt{2}u_{23}, u_{33} , \\
            &\dots, \sqrt{2}u_{d-1,d}, u_{dd}) \in \R^{\nicefrac{d(d+1)}{2}}
        \end{aligned}
    \end{equation*}
    Then, for $p \in \P_d$, we define the vectorization at $p$:
    $$\Vect_{p} \colon u \in T_{p} \P_d \mapsto \Vect_{I_d}(p^{-1/2}up^{-1/2}).$$
\end{definition}
One of the important property of $\Vect_p$ is that it is an isometry between $(T_p \P_d, \langle \cdot, \cdot \rangle_p)$ and $(\R{\nicefrac{d(d+1)}{2}}, \langle \cdot, \cdot \rangle_2)$. More information on this vectorization can be found in Section 3.3.3.3. of \citet{pennecManifoldvaluedImageProcessing2020} or in \cref{appendix:vectorization}. 

\section{Wrapped Gaussian on the manifold of SPD matrices}
\label{sec:wrapped_gaussains}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/wrapped_gaussian.pdf}
    \caption{An illustration of a wrapped Gaussian $\WG(p;\mu,\Sigma)$.}%
    \label{fig:wrapped_gaussian}
\end{figure}
In this work, our objective is to define a non-isotropic Gaussian on the manifold of SDP matrices $\P_d$. In this section, we define a \emph{wrapped Gaussian} through the way they will be sampled. We will also give the density of a wrapped Gaussian and, we will show that, unlike usual probability distributions, a given wrapped Gaussian can be parametrized by different sets of parameters, leading to an equivalence relation. 
In the following, we will denote $\Theta = \P_d \times \R^{\nicefrac{d(d+1)}{2}} \times \P_{\nicefrac{d(d+1)}{2}}$ the space of parameters of the wrapped Gaussian which is a product manifold.

\subsection{The definition}

To define a \emph{wrapped Gaussian}, we start with a classical Euclidean Gaussian random variable in $\R^{\nicefrac{d(d+1)}{2}}$ and push it on the manifold $\P_d$ using the exponential map as follows: %Let us give a more precise definition:
\begin{definition}[Wrapped Gaussian]
    \label{def:wrapped_gaussian}
    Let $p \in \P_d$, $\mu \in \R^{\nicefrac{d(d+1)}{2}}$ and $\Sigma \in \P_{\nicefrac{d(d+1)}{2}}$. A random variable $X$ on $\P_d$ follows a wrapped Gaussian denoted $\WG(p; \mu, \Sigma)$ if 
    $$X = \Exp_p(\Vect_p^{-1}(\mathbf{t})),~ \mathbf{t} \sim \N(\mu, \Sigma).$$
\end{definition}
A wrapped Gaussian is illustrated in \cref{fig:wrapped_gaussian}. This gives us a very simple algorithm to sample a wrapped Gaussian $\WG(p; \mu, \Sigma)$, since it is simply based on the sampling of $\mathcal{N}(\mu, \Sigma)$ in $T_p \P_d$. Moreover, we can rewrite this definition using a \emph{push-forward} (see definition 2.1 of \citet{peyreComputationalOptimalTransport2020}, Section 3.6 of \citet{bogachevMeasureTheory2007} or \cref{appendix:push_forward}):
\begin{equation}
    \label{eq:push-forward}
    \WG(p; \mu, \Sigma) = (\Exp_p \circ \Vect_p^{-1})\# \N(\mu, \Sigma)
\end{equation}
where $\#$ denotes the push-forward operator.

Let us comment on the different parameters: $p$ gives us a tangent plan from which the Gaussian is wrapped, so it locates the Gaussian on the manifold. $\mu$ is the mean of the Gaussian in the tangent space $T_p \P_d$. As $\P_d$ and $T_p \P_d$ are in bijection through the exponential map, $p$ and $\mu$ play a symmetric role that will be unveiled in \cref{sec:equiv_gaussian}. Finally, $\Sigma$ is the covariance matrix between the entries of the SPD matrices. In the special case where the SPD matrices are covariance matrices, $\Sigma$ models the covariance of the covariances, therefore, it can be seen as a fourth order moment.

\subsection{The density of a wrapped Gaussian}
The density of the wrapped Gaussian $\WG(p; \mu, \Sigma)$ can be obtained from its pus-forward given in \cref{eq:push-forward} (see \cref{theo:change_of_variable} in \cref{appendix:push_forward}).
\begin{theorem}
    The density $f_{p;\mu,\Sigma}$ of the wrapped Gaussian $\WG(p; \mu, \Sigma)$ exists and is:
    \begin{equation}
        \label{eq:density}
        \begin{aligned}
            &\forall x \in \P_d,~f_{p;\mu,\Sigma}(x) = \frac{g_{\mu,\Sigma}(\Vect_p(\Log_p(x))}{|J_p(\Log_p(x))|}
        \end{aligned}
    \end{equation}
    where $g_{\mu, \Sigma}$ is the density of the multivariate Gaussian $\mathcal{N}(\mu, \Sigma)$ and $J_p(\cdot ) = \det(\mathrm{d} \Exp_p(\cdot))$ is the Jacobian determinant of the exponential map $\Exp_p$.
    \label{theo:density}
\end{theorem}

The Jacobian determinant of the exponential map describes the volume change induced by the identification of $T_p\P_d$ and $\P_d$ by the exponential map $\Exp_p$. To compute the density explicitly, one needs to compute the Jacobian $J_p(u)$, which has a closed form expression for the manifold $\P_d$ (see Section 5.3 of \citet{chevallierWrappedStatisticalModels2020} or \cref{appendix:jac_exp}).
\begin{proposition}
    \label{prop:jac_exp}
    The Jacobian determinant of the exponential map at the identity $\Exp_{I_d}$ is:
    $$\forall u \in T_{I_d} \P_d,~ J_{I_d}(u) = 2^{\nicefrac{d(d-1)}{2}} \prod_{i < j} \frac{\sinh\left(\frac{\lambda_i(u) - \lambda_j(u)}{2}\right)}{\lambda_i(u) - \lambda_j(u)}$$
    where the $\lambda_i(u)$ are the eigenvalues of $u$.
    Then, one can use the previous formula to compute the Jacobian determinant of the exponential map at any point $p \in \P_d$:
    $$\forall u \in T_{p} \P_d,~ J_p(u) = J_{I_d}(p^{-1/2}up^{-1/2}).$$
\end{proposition}

It should be noted that, unlike the wrapped Gaussians defined in \citet{galaz-garciaWrappedDistributionsHomogeneous2022}, \citet{troshin2023wrapped} and \citet{naganoWrappedNormalDistribution2019}, we do not restrict ourselves to a centered multivariate Gaussian $\N(0, \Sigma)$ on the tangent space $T_p \P_d$. In our case, we allow the wrapped Gaussian to have an extra parameter $\mu$, thus extending the flexibility and applicability of the model. Having a non-centered distribution on the tangent space $T_p \P_d$ leads to new considerations that will be discussed in \cref{sec:equiv_gaussian}. 

\begin{remark}
    In this work, we focus on extending the multivariate Gaussian to a Riemannian setting. With no extra difficulty, one could wrap an \emph{Elliptically Contoured distribution} (EC) on $\P_d$ (chapter 6 of \citet{johnson1987multivariate} or \citealt{delmas2024elliptically}). We give more detail on this in \cref{appendix:wrapped_EC}. %The multivariate Gaussian is indeed a special case of an EC. 
\end{remark}

\subsection{Some properties of wrapped Gaussians}

Let us now give some properties of the wrapped Gaussians. We start by a rescaling property, then generalize the Central Limit Theorem to the manifold $\P_d$ and finally, we study the mean of the wrapped Gaussian in the special case of $\mu = 0$. %This property is an extension of a basic property of classical Euclidean multivariate Gaussians.
\begin{proposition}
    \label{prop:centered_reduced}
    Let $X \sim \WG(I_d; 0_{\nicefrac{d(d+1)}{2}}, I_{\nicefrac{d(d+1)}{2}})$ and let $(p, \mu, \Sigma) \in \Theta$. There exists a transformation of $X$ denoted $\Psi$ such that $$\Psi(X) \sim \WG(p; \mu, \Sigma).$$ Thus, the wrapped Gaussian $ \WG(I_d; 0_{\nicefrac{d(d+1)}{2}}, I_{\nicefrac{d(d+1)}{2}})$ is a building block of the wrapped Gaussians.
\end{proposition}
One can find a proof of this result, as well as an explicit expression for $\Psi$ in \cref{appendix:building_bloc}. 

Then, we give a wrapped version of the multivariate \emph{Central Limit Theorem} (CLT) (Theorem 5.12 of \citealt{wassermanAllStatisticsConcise2004}) for the manifold $\P_d$. For this, we define the logarithmic product introduced in \citet{arsignyGeometricMeansNovel2006}: 
\begin{definition}[Logarithmic product]
    \label{def:logarithmic_product}
    Let $p,q \in \P_d$. The logarithmic product of $p$ and $q$ is defined as:
    $$p \odot q = \exp\left(\log p + \log q \right).$$    
\end{definition}
Equipped with this logarithmic product, $(\P_d, \odot)$ forms a commutative group that is isomorphic to $(\S_d, +)$\footnote{More information on the properties of this logarithmic product can be found in \citet{arsignyGeometricMeansNovel2006}.}. One can generalize this notation to the sum of $n$ SPD matrices $p_1, \dots, p_n$ as 
$\bigodot_{i=1}^n p_i =  p_1 \odot \dots \odot p_n.$ Using this logarithmic product, we can state the following theorem:
\begin{theorem}[Wrapped CLT]
    \label{theo:wrapped_CLT}
    Let $(X_i)_{i \in \mathbb{N}^*}$ be a sequence of i.i.d. random variables on $\P_d$. We suppose that the sequence $(\Vect_{I_d}(\Log_{I_d}(X_i)))_{i \in \mathbb{N}^*}$ of random variables on $\R^{d(d+1)/2}$ admits a finite second order moment. We denote by $\mu$ the mean and by $\Sigma$ the covariance matrix of $\Vect_{I_d}(\Log_{I_d}(X_1))$. Then, 
    $$\left( \bigodot_{i = 1}^n (X_i \odot m^{-1})\right)^{\frac{1}{\sqrt{n}}} \xrightarrow[n \to \infty]{d} \WG(I_d, 0, \Sigma)$$ 
    where $\xrightarrow[n \to \infty]{d}$ denotes the convergence in distribution and where $m = \Exp_{I_d}(\Vect_{I_d}^{-1}(\mu))$.
\end{theorem}

This theorem shows the interest of wrapped Gaussians, as they naturally appear in the limit of a logarithmic product of random SPD matrices. The proof of this theorem can be found in \cref{appendix:CLT}. One can note that by generalizing the logarithmic product defined in \cref{def:logarithmic_product} to another tangent space $T_p \P_d$, one can extend the limit to $\WG(p; 0, \Sigma)$.

We can also give information on the mean of $\WG(p; \mu, \Sigma)$ in the special case of $\mu = 0$ i.e. when the distribution on the tangent space $T_p \P_d$ is centered. We recall from definition 3 of \citet{pennecCurvatureEffectsEmpirical2019} that a mean, or exponential barycenter, of a probability distribution $\alpha$ on $\P_d$ is defined as a point $\bar{p} \in \P_d$ satisfying $$\int_{\P_d} \Log_{\bar{p}}(x) \mathrm{d} \alpha(x) = 0.$$
For wrapped Gaussians, one has the following result:
\begin{proposition}
    A mean of $\WG(p; 0, \Sigma)$ is p.
    \label{prop:mean_WG}
\end{proposition}
The proof is straightforward from the definition of the wrapped Gaussian $\WG(p; 0, \Sigma)$.
\subsection{An equivalence relation between wrapped Gaussians}  
\label{sec:equiv_gaussian}
A key property of the Euclidean Gaussian $\N(\mu, \Sigma)$ is its \emph{identifiability}, meaning that the map $(\mu, \Sigma) \mapsto \N(\mu, \Sigma)$ is one-to-one (definition 1.5.2 of \citealt{LehmCase98}). The notion of identifiability is important when one wants to learn the parameters of a distribution from a finite set of samples as we will do in \cref{sec:estimation}. 
If we consider a model based on the wrapped Gaussian, we lose this property as we have the following proposition, illustrated in \cref{fig:illustration_equivalence}: 
\begin{proposition}
    \label{prop:equivalence}
    Let $(p, \mu, \Sigma) \in \Theta$ and $t \in \R$. One has that $\WG(p; \mu, \Sigma)$ and $\WG(e^{t}p;\mu - t\nu, \Sigma)$ are equal where $\nu = \Vect_p(p) = (1,\cdots,1,0,\cdots,0) \in \R^{\nicefrac{d(d+1)}{2}}$.
\end{proposition}
All the proofs of this section can be found in \cref{appendix:equivalence}. %Let us comment on this result. 
One can verify using \cref{eq:exp_log_riem}, that $t \mapsto e^t p$ is the geodesic $\gamma \colon t \mapsto \Exp_p(tp)$ in $\P_d$ starting at point $p$ and with initial velocity $p$ (as $p$ is a symmetric matrix, it also belongs to $T_p \P_d \simeq \S_d$).
Moreover, the map $t \mapsto \mu - t \nu = \mu - t \Vect_p(p)$ is also the geodesic in $\R^{\nicefrac{d(d+1)}{2}}$ with initial point $\mu$ and initial velocity $-\nu = -\Vect_p(p)$. 
Therefore, when $p$ is pushed in one ``direction`` (initial velocity $p$), $\mu$ is pushed in the opposite ``direction`` (initial velocity $-\Vect_p(p)$). 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{images/equivalent_wrapped_gaussians.pdf}
    \caption{Illustration of the equivalence between two wrapped Gaussians given by \cref{prop:equivalence}.}%
    \label{fig:illustration_equivalence}
\end{figure}

A wrapped Gaussian can thus be represented by several sets of parameters, so we define an \emph{equivalence relation} between sets of parameters that define the same wrapped Gaussian:
\begin{definition}
    Let $\theta_\alpha = (p_{\alpha}, \mu_{\alpha}, \Sigma_{\alpha}) \in \Theta$ and $\theta_\beta =(p_{\beta}, \mu_{\beta}, \Sigma_{\beta}) \in \Theta$ be two sets of parameters. Then, $\theta_\alpha$ and $\theta_\beta$ are equivalent, which we denote by $\theta_\alpha \cong \theta_\beta$, if they define the same wrapped Gaussian i.e.     
    $$\WG(p_{\alpha}; \mu_{\alpha}, \Sigma_{\alpha}) = \WG(p_{\beta}; \mu_{\beta}, \Sigma_{\beta}).$$ 
    We denote by $[\theta_\alpha]$ the equivalence class of $\theta_\alpha$:
    $$[\theta_\alpha] = \left\{\theta  = (p', \mu', \Sigma') \mid \theta \cong \theta_\alpha\right\}.$$                  
\end{definition}
Using \cref{prop:equivalence}, one has the immediate corollary:
\begin{corollary}
    Let $\theta_\alpha = (p_{\alpha}, \mu_{\alpha}, \Sigma_{\alpha}) \in \Theta$ and $\theta_\beta =(p_{\beta}, \mu_{\beta}, \Sigma_{\beta}) \in \Theta$. If there exists $t \in \R$ such that 
    \begin{equation*}
            p_{\beta} = e^{t}p_{\alpha}, \mu_{\beta} = \mu_{\alpha} + t\Vect_{p_\alpha}(p_{\alpha}) \text{ and } \Sigma_{\beta} = \Sigma_{\alpha},
    \end{equation*}
    then $\theta_\alpha \cong \theta_\beta$.      
\end{corollary}

\begin{remark}
    All equivalence classes do not contain a wrapped Gaussian that is centered on the tangent space ($\mu = 0$). Therefore, allowing $\mu \neq 0$ increases the expressiveness of the model.
\end{remark}

Once we have defined an equivalence class $[\theta]$ of parameters that define the same wrapped Gaussian, it is natural to define a representative of $[\theta]$. We define it as follows:
\begin{definition}[Representative of an equivalence class]
    \label{def:representative}
    We choose as representative of the class $[\theta]$, the tuple of parameters $\theta^\text{min} = (p^\text{min}, \mu^\text{min}, \Sigma^\text{min})$ such that $\mu^\text{min}$ is minimal in the sens of $\| \cdot \|_2$. We call it the \emph{minimal representative}.
\end{definition}
One is able, given a tuple of parameters $\theta = (p, \mu, \Sigma)$, to compute the minimal representative of the class $[\theta]$ of equivalent tuples of parameters using the following proposition:
\begin{proposition}
    \label{prop:param_to_min}
    Let $\theta = (p, \mu, \Sigma) \in \Theta$ be parameters. Then, the minimal representative of the class $[\theta]$  as defined at \cref{def:representative} is $\theta^\text{min} = (p^\text{min}, \mu^\text{min}, \Sigma^{\text{min}})$ where 
    \begin{equation*}
        p^{\text{min}} = e^{\frac{1}{d}\sum_{i=1}^d \mu_i}p, \quad \mu^{\text{min}} = \mu -  \frac{1}{d}\sum_{i=1}^d \mu_i \nu, \quad \Sigma^\text{min} = \Sigma
    \end{equation*}
    where we recall that $\nu = (1,\cdots,1,0,\cdots,0) \in \R^{\nicefrac{d(d+1)}{2}}$.
\end{proposition}

This minimal representative will be used in the following. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/estimation_vs_number_mu_unknown_minimal.pdf}%
    \caption{Results of the synthetic experiment on the estimation of parameters of a wrapped Gaussian using an MLE.}
    \label{fig:res_expe_MLE}
\end{figure*}


\section{Estimation of the parameters of a wrapped Gaussian distribution} 
\label{sec:estimation}
In this section, we tackle the parameter estimation problem of a wrapped Gaussian given samples. After introducing the Maximum Likelihood Estimator we will use, we lead some synthetic experiments to assess its performance. 
\subsection{A Maximum Likelihood Estimator}
Let $x_1,...,x_N$ be $N$ SPD matrices of size $d \times d$ independently sampled from a wrapped Gaussian with unknown parameters $\theta^{\star} = (p^\star \mu^\star, \Sigma^\star)$ supposed to be minimal in the sens of \cref{def:representative}. Our goal is to estimate $\theta^{\star}$ given the samples $(x_1,...,x_N)$ using a \emph{Maximum Likelihood Estimator} (MLE) (see Section 9.3 of \citealt{wassermanAllStatisticsConcise2004}). 
Let us introduce the likelihood $\mathcal{L}_N$ of the model:

$$\mathcal{L}_N(p;\mu,\Sigma) = \prod_{i=1}^{N} f_{p;\mu,\Sigma}(x_i)$$
where $f_{p;\mu,\Sigma}$ is the density of the wrapped Gaussian $\WG(p; \mu, \Sigma)$ as given in \cref{eq:density}. We will also consider the log-likelihood $\ell_N$ defined as $\ell_N(p;\mu,\Sigma) = \log \mathcal{L}_N(p;\mu,\Sigma)$. Then, we define the classical MLE $\hat{\theta}_N = (\hat{p}_N, \hat{\mu}_N, \hat{\Sigma}_N)$ as the parameter $\theta$ that maximizes $\mathcal{L}_N$ (or equivalently $\ell_N$). 
In the Euclidean setting, one has a closed form of the MLE of $\mu$ and $\Sigma$, obtained by computing the gradient of $\ell_N$. When dealing with wrapped Gaussians on $\P_d$, we were not able to derive a closed form for the MLE $\hat{p}_N$ of the parameter $p$\footnote{Moreover, it is unlikely that there exists such a closed form, as for example, there is no closed form for the Riemannian mean on $\P_d$ \citep{moakherDifferentialGeometricApproach2005}.}. However, the MLE of $\mu$ and $\Sigma$ are analogous to the Euclidean setting but depend on $p^\star$:
\begin{proposition}
    \label{prop:MLE_mu_sigma}
    The MLE $\hat{\mu}_N$ and $\hat{\Sigma}_N$ of the parameters $\mu$ and $\Sigma$ of the wrapped Gaussian are:
    \begin{equation*}
        \begin{aligned}
            \hat{\mu}_N &= \frac{1}{N}\sum_{i=1}^{N}\Vlog_{p^\star}(x_i), \\
            \hat{\Sigma}_N &= \frac{1}{N}\sum_{i=1}^{N}\left(\Vlog_{p^\star}(x_i) - \hat{\mu}_N\right)\left(\Vlog_{p^\star}(x_i) - \hat{\mu}_N\right)^\top
        \end{aligned}
    \end{equation*}
    where  $\Vlog_{p^\star} = \Vect_{p^\star} \circ \Log_{p^\star}$.
\end{proposition}
One can note that in proposition 4.7 of \citet{galaz-garciaWrappedDistributionsHomogeneous2022}, they also have a closed from for the MLE of $\Sigma$ that depends on $p^\star$ without any closed form for $\hat{p}_N$.
In practice, we used a Riemannian Conjugate Gradient algorithm \citep{boumalIntroductionOptimizationSmooth2023} on the product manifold $\Theta = \P_d \times \R^{\nicefrac{d(d+1)}{2}} \times \P_{\nicefrac{d(d+1)}{2}}$ to compute the optimal parameters $(\hat{p}_N, \hat{\mu}_N, \hat{\Sigma}_N)$. We implemented this MLE in Python using the toolbox \texttt{Pymanopt} \citep{pymanopt}\footnote{Upon acceptation, our code will be made available on GitHub.}.

This estimation problem can become challenging as the dimension $d$ of the considered SPD matrices increases. In fact, the number of coefficients to estimate is:
$$\underbrace{\frac{d(d+1)}{2}}_{p} + \underbrace{\frac{d(d+1)}{2}}_{\mu}  + \underbrace{\frac{d^2(d+1)^2 + 2d(d+1)}{8}}_{\Sigma}$$
which grows at a rate of $O(d^4)$. For example, if $d = 10$, there are $1,650$ coefficients to estimate and if $d = 30$, the number jumps to $109,275$. One should thus make sure that the number of samples $N$ is sufficiently big for results of the MLE to make sense. To get around this issue when the number of samples is small, one can assume that the covariance matrix $\Sigma$ is diagonal, which reduces the number of coefficients to $O(d^2)$. This assumption implies independent entries of the SPD matrices and will be used in \cref{sec:classif_expe}.

In other works, such as in \citet{chevallierExponentialWrappedDistributionsSymmetric2022} or in \citet{chevallierWrappedStatisticalModels2020}, the authors use the method of moments (see Section 9.2 of \citealt{wassermanAllStatisticsConcise2004}) to estimate the parameter. In our case, we can use the method of moment only when we know \textit{a priori} that $\mu^\star = 0$. Then, as given in \cref{prop:mean_WG}, a mean of the wrapped Gaussian is $p^\star$, therefore, it can be estimated using the Riemannian mean $\hat{p}_N = \mathfrak{G}(x_1,...,x_N)$ \citep{moakherDifferentialGeometricApproach2005}, and $\Sigma$ can be estimated using \cref{prop:MLE_mu_sigma}. However, in a more general case of $\mu^\star \neq 0$, estimating $p^\star$ using the Riemannian mean does not lead to the correct estimation of the true parameters. We give more details on why in \cref{appendix:moment_estimator}.
\subsection{Synthetic experiments}

We led some synthetic experiments to evaluate the MLE's performances. For this, we sampled $N$ points from a wrapped Gaussian in $\P_d$ whose minimal parameters $(p^\star, \mu^\star, \Sigma^\star)$ are known. Then, we optimize the MLE to find $(\hat{p}_N, \hat{\mu}_N, \hat{\Sigma}_N)$, and compare the true and minimal estimated parameters: for $p$ and $\Sigma$, we compare them using the AIRM distance and for $\mu$, we use $\|\cdot\|_2$. In this experiment, we looked at how the estimation error evolves when the number of samples $N$ grows from $100$ to $10,000$. We also compare different dimensions $d \in \{2,5,10\}$. More details on the experimental setup are given in \cref{appendix:MLE_expe}. The results of this experiment can be found in \cref{fig:res_expe_MLE}. We can see that, as one would expect, as the number $N$ of points sampled grows, the estimation error decreases. Moreover, we can remark that the dimension $d$ does not affect the results of the estimation of $p$ and $\mu$, but really affects the estimation of $\Sigma$. The higher the dimension $d$, the higher the error $\delta(\hat{\Sigma}_N,\Sigma^\star)$ is. This is coherent as the number of parameters of $\Sigma$ grows as $O(d^4)$, so even a small increase of the dimension $d$ leads to an important increment in the estimation error of $\Sigma$. We also led some experiments in the case where the covariance matrix $\Sigma$ is diagonal. We show, in \cref{appendix:esimation_sigma_diag}, that in this case, one needs fewer samples to have a good estimation of $\Sigma$ when the dimension $d$ rises.

\section{Classification using wrapped Gaussians}
\label{sec:classification}

\begin{table*}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c||ccccc}
    \toprule
    Dataset       & Application           & Dimension      & \# matrices & \# classes & Reference \\ \midrule

    BNCI2014004   & BCI                   & $3 \times 3$   & $720$ x 9 subjects   & $2$ & \citep{BNCI2014004}\\ %\hline
    Zhou2016      & BCI                   & $5 \times 5$   & $320$ x 4 subjects  & $2$ & \citep{Zhou2016}\\ %\hline
    AirQuality & Atmospheric data & $6 \times 6$ & $102$ & $3$ &  \citep{smithMultiSiteMultiPollutantAtmospheric2022} \\ %\hline  
    Indiana Pines & Hyperspectral imaging & $5 \times 5$   & $14,641 $   & $12$ &  \citep{InidianaPines} \\  %\hline
    Pavia Univ. & Hyperspectral imaging & $5 \times 5$   & $185,176  $  & $6$ &  - \\ %\hline
    Salinas       & Hyperspectral imaging & $5 \times 5$   & $94,184 $   & $17$ & - \\  %\hline
    Textile       & Image Analysis     & $10 \times 10$ & $16,000 $             & $2$ & \citep{MVTec2021}\\ %\hline
    BreizhCrops & Hyperspectral imaging & $13 \times 13$   & $177,658 $  &$ 6$ & \citep{breizhcrops2020}\\  \bottomrule %\hline 
    \end{tabular}%
    }
    \caption{Summary of the datasets used for the experiments.}
    \label{table:summary_datasets}
\end{table*}

In this section, we demonstrate that widely used geometry-aware classifiers on $\P_d$ can be integrated into a probabilistic framework. We also introduce new classifiers based on the wrapped Gaussian and conduct experiments on real-world data from various applications.


\subsection{Classifiers used for SPD matrices}
\label{sec:classifers_SPD}
\paragraph{MDM}
The \emph{Minimum Distance to Mean} (MDM) algorithm described in \citet{barachantRiemannianGeometryApplied2010} is a popular classifier for SPD matrices. Given a training set of labeled SPD matrices, the MDM computes the Riemannian mean \citep{moakherDifferentialGeometricApproach2005} $\mathfrak{G}_k$ of each class $k \in \{1,...,K\}$. Then, given a new SPD matrix $p$, the predicted class $\hat{k}$ is the class for which the distance between $p$ and $\mathfrak{G}_k$ is the smallest. %This simple algorithm takes into account to geometry as the quantities computed (the mean and the distances) are all Riemannian.

\paragraph{LDA and QDA} 
In a Euclidean setting, the \emph{Linear Discriminant Analysis} (LDA) (section 4.3 of \citealt{hastieElementsStatisticalLearning2009}) is a classifier that assumes that each class $k \in \{1,...,K\}$ is modeled by a multivariate Gaussian $\mathcal{N}(\mu_k, \Sigma)$ where the covariance matrix $\Sigma$ is shared among all the classes. First, the parameters of each class are learned using an MLE on the training data. Then, to classify a new point $z$, LDA compares the log-likelihood of $z$ according to each class and chooses the class that has the highest log-likelihood. As the covariance matrix is shared among the classes, the decision boundaries are linear, which led to its name: \emph{linear} discriminant analysis. When one assumes that the covariance matrices are not equal among the classes, i.e. each class is modeled by $\mathcal{N}(\mu_k, \Sigma_k)$, the decision boundaries are quadratic. This classifier is called \emph{Quadratic Discriminant Analysis} (QDA).
One can restrict the covariance matrices to be diagonal, which leads to the \emph{Diagonal LDA} and \emph{Diagonal QDA} classifiers \citep{dudoitComparisonDiscriminationMethods2002}. Then, the Diagonal QDA classifier is equivalent to the Gaussian Naive Bayes classifier (see chapter 8 sec 3.3 of \citealt{bishop2007}).

A possible extension of LDA (or QDA) to the manifold of SPD matrices $\P_d$ is call \emph{Tangent Space LDA} (or \emph{Tangent Space QDA}) and is described in part IV B. of \citet{barachantMulticlassBrainComputer2012}. The Riemannian mean $\mathfrak{G}$ of the training set is computed, and all training points are sent to the tangent space $T_\mathfrak{G} \P_d$ \emph{via} the exponential map $\Exp_{\mathfrak{G}}$. Then, a classical LDA (or QDA) can be used in this Euclidean space.

\paragraph{Other classifiers from SPD matrices} 
Other classifiers that have been developed for SPD matrices. For example, Multinomial Logistics Regression has been extended for SPD matrices in \citet{chenRiemannianMultinomialLogistics}. They rely on metric that arex pulled back from the Euclidean space which is not the case of the AIRM metric we use in our work.  
Several deep learning approaches have been proposed to classify SPD matrices \citep{huang2017riemannian, brooksRiemannianBatchNormalization2019, nguyenGeomNet}. However, most of these approaches distort the geometry of the manifold and are out of the scope of this work, as our approach does not rely on deep learning.

\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l||ccccc}
        \toprule
        Dataset &                   Acc. MDM &                Acc. TS-LDA &                   Acc. TS-QDA &              Acc. Ho-WDA &              Acc. He-WDA \\
        \midrule
        BNCI2014004 &  $78.71~ (\pm 14.53)$ &  $\mathbf{78.73}~ (\pm 14.52)$ &          $76.07~ (\pm 13.94)$ &          $75.38~ (\pm 14.40)$ &          $74.37~ (\pm 14.73)$ \\
        Zhou2016    &   $91.18~ (\pm 5.51)$ &   $\mathbf{91.21}~ (\pm 5.50)$ &           $89.45~ (\pm 7.43)$ &           $85.92~ (\pm 9.20)$ &          $82.86~ (\pm 11.65)$ \\
        Air Quality &   $94.05~ (\pm 6.53)$ &            $94.05~ (\pm 6.53)$ &  $\mathbf{97.05}~ (\pm 4.45)$ &           $96.05~ (\pm 4.17)$ &           $\mathbf{97.00}~ (\pm 4.47)$ \\
        Indiana     &   $58.01~ (\pm 0.72)$ &            $67.07~ (\pm 0.53)$ &           $73.38~ (\pm 0.45)$ &           $73.74~ (\pm 0.51)$ &  $\mathbf{74.30}~ (\pm 0.83)$ \\
        Pavia Uni.  &   $72.32~ (\pm 0.59)$ &            $84.61~ (\pm 0.06)$ &           $87.16~ (\pm 0.09)$ &  $\mathbf{87.36}~ (\pm 0.07)$ &           $85.54~ (\pm 0.15)$ \\
        Salinas     &   $36.42~ (\pm 0.12)$ &            $46.30~ (\pm 0.17)$ &           $69.87~ (\pm 0.21)$ &  $\mathbf{71.20}~ (\pm 0.33)$ &           $62.39~ (\pm 0.22)$ \\
        Textile     &   $83.08~ (\pm 0.62)$ &            $83.12~ (\pm 0.63)$ &           $86.03~ (\pm 0.66)$ &  $\mathbf{86.26}~ (\pm 0.59)$ &           $85.93~ (\pm 0.77)$ \\
        BreizhCrops &   $45.48~ (\pm 0.25)$ &            $47.67~ (\pm 0.32)$ &           $50.72~ (\pm 0.28)$ &  $\textbf{54.66}~ (\pm 0.45)$ &           $51.33~ (\pm 0.58)$ \\
        \bottomrule
    \end{tabular}%
    }
    \caption{Accuracy of the different classifiers on the different datasets we consider.}
    \label{table:res_real_data}
    \end{table*}

\subsection{A general probabilistic framework}

Our goal is to show that the MDM, Tangent Space LDA or Tangent Space QDA can be seen as part of a probabilistic framework on the manifold of SPD matrices. More precisely, we will show that the previous classifiers can be rewritten as Maximum Likelihood based classifiers (like the classical LDA or QDA) where the different classes are modeled using distributions on the manifold $\P_d$. Let us consider $K$ classes of labeled SPD matrices and let us denote $\alpha_k$ the modeled distribution of class $k$. %Depending on our choice of distribution for $\alpha_k$, one can retrieve the previous classifiers. 
\paragraph{MDM} For the MDM classifier, we first need to recall the isotropic Gaussians on $\P_d$ introduced in \citet{saidGaussianDistributionsRiemannian2018}. Let $\bar{y} \in \P_d$ and $\sigma > 0$, then, the isotropic Gaussian denoted $G(\bar{y}, \sigma)$ is defined by the following density:
$$\forall y \in \P_d,~ f_{\bar{y}, \sigma}(y) = \frac{1}{\zeta(\sigma)}\exp\left[-\frac{\delta(y, \bar{y})^2}{2\sigma^2}\right]$$
where $\zeta(\sigma)$ is a normalizing constant. If one supposes that each class is modeled by an isotropic Gaussian with a $\sigma$ shared among all the classes i.e.
$$\alpha_k = G(\bar{y}_k, \sigma)$$
then, the MDM is equivalent to a maximum likelihood classifier. Here, the spread $\sigma$ of the isotropic Gaussians does not play any role in the classification process. So, during training, one only has to estimate $\bar{y}_k$ for each class, which is the center of mass and can be estimated using the Riemannian mean (see proposition 7 of \citealt{saidGaussianDistributionsRiemannian2018}). 
\paragraph{Tangent Space LDA or QDA}
For the Tangent Space LDA, we will leverage the Wrapped Gaussians introduced in \cref{sec:wrapped_gaussains}. Suppose that each class is modeled by a wrapped Gaussian centered at $\mathfrak G$, the Riemannian mean of the training set, and with a shared covariance matrix $\Sigma$ for the Tangent Space LDA i.e.
$$\alpha_k = \WG(\mathfrak G; \mu_k, \Sigma)$$
or with one covariance matrix $\Sigma_k$ per class for the Tangent Space QDA i.e. 
$$\alpha_k = \WG(\mathfrak G; \mu_k, \Sigma_k).$$
Then, the Tangent Space LDA (or Tangent Space QDA) is a maximum likelihood classifier based on those distributions.

\subsection{Wrapped Discriminant Analysis}
Having placed the various classifiers that are used on the manifold of SPD matrices in a probabilistic framework, we propose a new maximum likelihood classifier based on the wrapped Gaussians introduced in \cref{sec:wrapped_gaussains}. First, let us model each class by a wrapped Gaussian with a shared covariance matrix $\Sigma$ among the classes:
$$\alpha_k = \WG(p_k, \mu_k, \Sigma).$$
To learn the parameters of each class, we optimize an MLE on the whole model to find the parameters:
\begin{equation*}
    (p_1,...,p_K, \mu_1,...,\mu_K, \Sigma) \in \underset{\bm{p}, \bm{\mu}, \Sigma}{\text{argmax}} \prod_{k=1}^{K} \prod_{i=1}^{N_k} f_{p_k; \mu_k, \Sigma}(x_i^k)
\end{equation*}

where $(x_i^k)_{i = 1,..,N_k}$ are the training points of class $k$. The implementation is the same as in \cref{sec:estimation}. We call it the \emph{Homogeneous Wrapped  Discriminant Analysis} (Ho-WDA).

As for the QDA, we propose another version of this classifier where each class has its own covariance matrix $\Sigma_k$: 
$$\alpha_k = \WG(p_k, \mu_k, \Sigma_k).$$
We call this classifier the \emph{Heterogeneous Wrapped Discriminant Analysis} (He-WDA). In that case, an MLE is optimized on each class individually, as in \cref{sec:estimation}: 
\begin{equation*}
    \forall k \in \{1,...,K\},~(p_k, \mu_k, \Sigma_k)\in \underset{p, \mu, \Sigma}{\text{argmax}} \prod_{i=1}^{N_k} f_{p;\mu,\Sigma}(x_i^k).
\end{equation*}
\subsection{Experiments}
\label{sec:classif_expe}


In this section, we want to compare the Ho-WDA and He-WDA to the other classifiers (MDM, Tangent Space LDA denoted TS-LDA and Tangent Space QDA denoted TS-QDA) used on the manifold of SPD matrices $\P_d$ and detailed in \cref{sec:classifers_SPD}. For this, we lead some experiments on 8 different real datasets coming from several applications.
We give a summary of the datasets used in \cref{table:summary_datasets} and more detail on each one of them in \cref{appendix:expe_real_data}. For this experiment, we restricted ourselves to the case where the covariance matrices $\Sigma$ are diagonal. Therefore, one has fewer coefficients to estimate and the MLE needs less points to converge. 
We give the accuracy of the classifiers we study on the different datasets at \cref{table:res_real_data}.

We can see two different behaviors. First, on the datasets with a lot of matrices (Textile, Salinas, Indiana Pines, Pavia Univ., BreizhCrops), the Ho-WDA and He-WDA perform the best. The number of parameters to estimate is high, so the more samples one has, the better the estimation will be as we illustrated in the synthetic experiments at \cref{sec:estimation}. For BreizhCrops, even if the matrices are of size $13 \times 13$, we have a lot of points ($177,658$) so the MLE is able to correctly estimate all the parameters.
Secondly, on the BCI datasets (BNCI2014004 and Zhou2016), we have significantly less points (less than $1,000$) so the estimation of the parameters of the underlying wrapped Gaussians is less precise. In this case, one can see that the MDM and the TS-LDA perform the best and the Ho-WDA and He-WDA perform less well. However, on the AirQuality dataset, the Ho-WDA and He-WDA perform the best. This is interesting as the number of matrices available in the dataset is small ($102$). An explanation could be that the underlying distribution of the data is not very complex, so a few points are enough to correctly estimate them. In BCI datasets, the distribution is more complex, so one needs more points to correctly estimate the distribution. Finally, we do not observe a clear dominance of the He-WDA over the Ho-WDA. This is similar to the difference between the LDA and QDA where, often, an LDA can correctly classify data. 
\section{Conclusion}

In this work, we present a generalization of non-isotropic multivariate Gaussians on the manifold of SPD matrices: \emph{Wrapped Gaussians}, and we give some theoretical properties. We solved the non-identifiability of our model by defining an equivalence relation between the set of parameters that define the same wrapped Gaussian. We also give all the tools needed to use the distribution in practice, such as an easy-to-use sampling algorithm or an MLE that correctly estimates the parameters of a wrapped Gaussian.
Finally, we showed that the MDM, TS-LDA and TS-QDA classifiers can be seen as part of a probabilistic framework using wrapped Gaussians. We introduced two new classifiers based on the wrapped Gaussian: Ho-WDA and He-WDA. We showed that the Ho-WDA and He-WDA perform well on real data when the number of samples is sufficient.
In future work, we plan to investigate the use of wrapped Gaussians to perform data augmentation or transfer learning. Moreover, as we have developed in the paper a geometry-aware Gaussian distribution, it becomes possible to extend all the classical machine learning models that rely on Gaussian distributions to the manifold of SPD matrices. For example, one could develop Gaussian Mixture Models, Hidden Markov Models or Variational Autoencoders on $\P_d$ using wrapped Gaussians. These models could then be applied to various tasks such as clustering, sequence modeling, or anomaly detection on manifold-valued data. It could also be possible to explore the use of wrapped Gaussians in the context of Bayesian inference, which could open new avenues for probabilistic modeling and uncertainty quantification in manifold-based data analysis. 
\section*{Acknowledgments}
This work was funded by the French National Research Agency for project PROTEUS (grant ANR-22-CE33-0015-01). Part of this work was carried out while Florian Yger was a member of PSL-Dauphine University, and he acknowledges the support of the ANR as part of the ``Investissements d’avenir`` program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute).

\bibliographystyle{apalike}
\bibliography{biblio}
\onecolumn
\appendix
\section{The vectorization used}
\label{appendix:vectorization}
In \cref{subsec:vectorization}, we defined an isomorphism between the tangent space $T_p \P_d$ at point $p \in \P_d$ and $\R^{\nicefrac{d(d+1)}{2}}$. This isomorphism is called the vectorization and is denoted $\Vect_p$. This isomorphism is not the only one that exists between the two spaces, so in this section we will motivate our choice of the vectorization and give some of its properties.


Let $p \in \P_d$. We recall that the tangent space $T_p \P_d$ at point $p$ is a Euclidean space once equipped with the inner product defined at \cref{def:AIRM_metric}. Therefore, one can unveil an orthonormal basis of $T_p \P_d$. One can see that the tangent space at the identity $T_{I_d} \P_d$ is the classical Euclidean space $\mathcal{S}_d$ equipped with the Frobenius inner product. Therefore, one can easily build an orthonormal basis of $T_{I_d} \P_d$ and then, transport it to the other tangent spaces.
\begin{proposition}[Orthonormal basis of the tangent spaces]
    Let $e_{ij}$ be the $d \times d$ matrix with a 1 at position $(i,j)$ and zeros everywhere else. Then 
    \begin{itemize}
        \item An orthonormal basis of $(T_{I_d} \P_d, \langle \cdot , \cdot \rangle_{I_d})$ is $(E_{I_d,ij})_{i\leq j}$ defined as follows:
            \[
            E_{I_d, ij} = \begin{cases}
            \frac{1}{\sqrt{2}} (e_{ij} + e_{ji}) & \text{for } i < j, \\
            e_{ii} & \text{for } i = j.
            \end{cases}
            \]
        \item An orthonormal basis of $(T_p \P_d, \langle \cdot , \cdot \rangle_p)$ is $(E_{p, ij})_{i\leq j}$ where $ E_{p, ij} = p^{1/2}E_{I_d, ij}p^{1/2}$.
    \end{itemize}
    \label{prop:ortho_basis}
\end{proposition}
{ \scriptsize
\begin{proof}
    One has that $T_{I_d} \P_d \simeq \S_d$ and that $\langle \cdot, \cdot \rangle_{I_d}$ is the Frobenius inner product, so one can use the classical basis of $\S_d$ to build an orthonormal basis of $T_{I_d} \P_d$. Then, by transporting the basis of $T_{I_d} \P_d$ to $T_p \P_d$ using the isometry $x \mapsto p^{1/2}xp^{1/2}$, one has a basis of $T_p \P_d$. It is still orthonormal as $x \mapsto p^{1/2}xp^{1/2}$ is an isometry.
\end{proof}
}

Let us give another intuition on the choice of this basis for $(T_p \P_d, \langle \cdot , \cdot \rangle_p)$:
\begin{proposition}
    \label{prop:basis_parallel_transport}
    The basis $(E_{p, ij})_{i\leq j}$ of $(T_p \P_d, \langle \cdot , \cdot \rangle_p)$ given at \cref{prop:ortho_basis} is the parallel transport of the basis $(E_{I_d,ij})_{i \leq j}$ of $(T_{I_d} \P_d, \langle \cdot , \cdot \rangle_{I_d})$ from $T_{I_d} \P_d$ to $T_{p} \P_d$.
\end{proposition}
{ \scriptsize
\begin{proof}
    According to  Equation 22 of \citet{sraConicGeometricOptimisation2015}, in the case of $\P_d$, the parallel transport $\Gamma_{I_d \rightarrow p}$ from $T_{I_d} \P_d$ to $T_{p} \P_d$ is:
    $$\forall u \in T_{I_d} \P_d,~ \Gamma_{I_d \rightarrow p}(u) = p^{1/2}up^{1/2}.$$
    The result follows from the definition of $(E_{p, ij})_{i\leq j}$.
\end{proof}
}

Now that we have an orthonormal basis of the tangent space $T_p \P_d$, we give the link between this basis and the vectorization $\Vect_p$:
\begin{proposition}
    \label{prop:vecto_to_basis}
    Let $(E_{p, ij})_{i\leq j}$ be the orthonormal basis of the tangent space $T_p \P_d$ described at \cref{prop:ortho_basis}. Let $u \in T_p \P_d$. Then, 
    $$\Vect_p(u) = (\langle u, E_{p, 11} \rangle_p,\langle u, E_{p, 12} \rangle_p,\langle u, E_{p, 22} \rangle_p, \cdots, \langle u, E_{p, d-1 d} \rangle_p , \langle u, E_{p, dd} \rangle_p).$$
\end{proposition}

{ \scriptsize
\begin{proof} 
    We start with the case where $p = I_d$. 
    Let $u = [[u_{ij}]] \in T_{I_d} \P_d \simeq \S_d$. We simply need to show that, for $i \leq j$, one has
    \begin{equation*}
        \langle u, E_{I_d, ij} \rangle_{I_d} = \begin{cases}u_{ii} \quad & \text{ if } i = j, \\ \sqrt{2} u_{ij} \quad & \text{ if } i < j. \end{cases}
    \end{equation*}
    One has, when $i=j$:
    $$\langle u, E_{I_d, ii} \rangle_{I_d} = \langle u, e_{ii} \rangle_{I_d} = \tr(u e_{ii}) = u_{ii}.$$
    And when $i < j$:
    $$\langle u, E_{I_d, ij} \rangle_{I_d} = \langle u, \frac{1}{\sqrt{2}}(e_{ij} + e_{ji}) \rangle_{I_d} = \frac{1}{\sqrt{2}}(\tr(u e_{ij}) + \tr(u e_{ji})) = \sqrt{2} u_{ij}.$$
    Therefore, one has the results for $\Vect_{I_d}$.

    Now, in the general case of $p \in \P_d$, one has that, for $i \leq j$
    $$\langle u, E_{p, ij} \rangle_p = \langle u, p^{1/2}E_{I_d, ij}p^{1/2} \rangle_p = \langle p^{-1/2}u p^{-1/2}, E_{I_d, ij} \rangle_{I_d}.$$
    By using the definition of $\Vect_p(u) = \Vect_{I_n}(p^{-1/2}up^{-1/2})$ and the result for $p = I_d$, one has the result.
\end{proof}
}
The previous proposition helps us motivate the choice of the vectorization. Indeed, this vectorization is simply the coordinates of the tangent vector $u$ in the orthonormal basis $(E_{p, ij})_{i\leq j}$ of the tangent space $T_p \P_d$. Now that we are more convinced on this choice of vectorization, we give some of its important properties.

\begin{proposition}
    \label{prop:norm_vect}
    \begin{itemize}
        \item Let $u \in T_{p} \P_d$, then $$\|\Vect_{p}(u)\|_2^2 = \Vect_{p}(u)^\top \Vect_{p}(u) = \|u\|_{p}^2 := \sqrt{\langle u, u\rangle}_p.$$ Therefore, $\Vect_p$ is not only an isomorphism, it is an isometry between $(T_p \P_d, \langle \cdot, \cdot \rangle_p)$ and $(\R^{\nicefrac{d(d+1)}{2}}, \|\cdot \|_2)$.
        \item Let $u \in T_{p} \P_d$, then $$\|\Vect_{p}(\Log_p u)\|_2^2 = \Vect_{p}(\Log_p u)^\top \Vect_{p}(\Log_p u) = \delta(p,u)^2.$$
    \end{itemize}
\end{proposition}
{ \scriptsize
\begin{proof}Let us prove the two points of the proposition.
    \begin{itemize}
        \item Let $u \in T_{p} \P_d$. One has, using \cref{prop:vecto_to_basis},
    \begin{equation*}
    \begin{aligned}
        \|\Vect_{p}(u)\|_2^2 &= \Vect_{p}(u)^\top \Vect_{p}(u) = \sum_{i \leq j} \langle u, E_{p, ij} \rangle_p^2. 
    \end{aligned}
    \end{equation*}
    As $(E_{p, ij})_{i\leq j}$ is an orthonormal basis of the Euclidean space $(T_p \P_d, \langle \cdot, \cdot \rangle_p)$, one has that 
    $$\sum_{i \leq j} \langle u, E_{p, ij} \rangle_p^2  = \|x\|_p.$$
    which proves the first point.
    \item Let $u \in T_{p} \P_d$. One has, using the previous point, the definition of $\| \cdot \|_p$, the expression of the Riemannian logarithm given at \cref{eq:exp_log_riem} and the expression of the AIRM distance given at \cref{eq:distance_AIRM}:
    $$\|\Vect_{p}(\Log_p u)\|_2^2 = \|\Log_p u\|_p^2 = \|p^{-1/2}\Log_p u ~ p^{-1/2}\|_F = \|\log(p^{-1/2}up^{-1/2})\|_F = \delta(p, u)^2.$$
\end{itemize}
\end{proof}
}
Finally, let us give a consequence of the previous property on the Jacobian of the vectorization: as $\Vect_p$ is an isometry, there is no volume change \emph{via} the vectorization. 
\section{The push-forward}
\label{appendix:push_forward}
In this section, we give the definition and an important result on the push-forward measure. One can find more information on the push-forward measure in Section 3.6 of \citet{bogachevMeasureTheory2007}.
\begin{definition}[Pushforward measure]
    Given two measurable spaces $(\mathcal{X}, \Omega_{\mathcal{X}})$ and $(\mathcal{Y}, \Omega_{\mathcal{Y}})$, a measurable map $f\colon \Omega_{\mathcal{X}}\to \Omega_{\mathcal{Y}}$ and a measure $\mu\colon\Omega_{\mathcal{X}}\to[0,+\infty]$, the \emph{pushforward} of $\mu$ is defined to be the measure $f \# \mu \colon\Omega_{\mathcal{Y}}\to[0,+\infty]$ given by
    $$\forall B \in \Omega_{\mathcal{Y}},~(f\#\mu)(B) = \mu(f^{-1}[B]).$$
    where $f^{-1}[B]$ is the preimage of $B$ by $f$.
\end{definition}
We now give the most important result on pushforward measures: the change of variables
\begin{theorem}[Change of variables]
    Let $\mu$ be a non-negative measure. An $\Omega_{\mathcal{Y}}$-measurable function $g$ on $\mathcal{Y}$ is integrable with respect to the pushforward measure $f\#\mu$ if and only if the function $g \circ f$ is integrable with respect to the measure $\mu$. In this case, one has:
    $$\int_{\mathcal{Y}} g(y) \mathrm{d}(f\#\mu)(y) = \int_{\mathcal{X}} (g \circ f )(x)\mathrm{d}\mu(x).$$
    \label{theo:change_of_variable}
\end{theorem}

\section{The Jacobian determinant of the exponential map on the manifold of SPD matrices}
\label{appendix:jac_exp}
In this section, we give a proof of the result stated at \cref{prop:jac_exp}. More specifically, we want to show that the Jacobian determinant of the exponential map $\Exp_{I_d}$ at identity is:
\begin{equation}
    \label{eq:jacobian_exp}
    J_{I_d}(u) = 2^{\nicefrac{d(d-1)}{2}} \prod_{i < j} \frac{\sinh\left(\frac{\lambda_i(u) - \lambda_j(u)}{2}\right)}{\lambda_i(u) - \lambda_j(u)}
\end{equation}
where the ($\lambda_i(u))_i$ are the eigenvalues of $u$.

Let us start by recalling that, when the tangent plan of interest is at the identity $I_d$, the Riemannian exponential map $\Exp_{I_d}$ is simply the matrix exponential $\exp$:
$$\Exp_{I_d} \colon u \in T_{I_d} \P_d \mapsto \exp(u) \in \P_d.$$
One can see this result using the expression of the Riemannian exponential given in \cref{eq:exp_log_riem}.

To prove the relation \cref{eq:jacobian_exp}, we will start by the case where $u$ is a diagonal matrix. We will then extend the result to the general case.
\paragraph{ Case 1: $u \in T_{I_d} \P_d$ diagonal}
Let us consider $u \in T_{I_d} \P_d$ diagonal, $u = \text{diag}(\lambda_1,...,\lambda_d)$. In the following, we will denote by $\Psi$ the differential of the Riemannian exponential in $u$: $\Psi = \d \Exp_{I_d}(u) = \d \exp(u)$. We therefore want to compute the determinant of $\Psi$: $\det \Psi$. One has that $\Psi \colon T_{I_d} \P_d  \rightarrow T_{\exp(u)}\P_d$, where we have identified $T_u T_{I_d} \P_d$ with $T_{I_d} \P_d$. To compute the determinant, one need to choose adequate bases in both tangent spaces $T_{I_d} \P_d$ and $T_{\exp(u)} \P_d$.  By ``adequate'', we mean that the transformation between the two bases does not imply any volume change.
For this, we consider for $T_{I_d} \P_d$ the basis $(E_{I_d, ij})_{i\leq j}$ and for $T_{\exp(u)} \P_d$ the basis $(E_{\exp(u), ij})_{i\leq j}$ as defined at \cref{prop:ortho_basis}. According to \cref{prop:basis_parallel_transport}, the transformation from the first to the second basis is the parallel transport, which does dot imply any volume changes, as the parallel transport is an isometry (see Prop 10.36 of \citealt{boumalIntroductionOptimizationSmooth2023}).

Now that we have our two basis, we want to compute the matrix of $\Psi$ in those two bases. For this, we need to compute $\Psi(E_{I_d, ij})$ and express it in the basis $(E_{\exp(u), ij})_{i\leq j}$. As $u$ is diagonal, we can use the \emph{Daletskii-Krein formula} (see \citet{daletskii1965integration} or Equation 2.40 of \citealt{bhatiaPositiveDefiniteMatrices2007}) that states the following in our case: for $h \in T_p \P_d$, 
\begin{equation}
    \label{eq:DK_formula}
    \Psi(h) = \left[ \left[ \exp^{[1]}(u)_{ij} h_{ij} \right] \right]
\end{equation}
where 
\[
    \exp^{[1]}(u)_{ij} = \begin{cases}
    e^{\lambda_i} &\text{for } i = j, \\
    \frac{e^{\lambda_i} - e^{\lambda_j}}{\lambda_i - \lambda_j} & \text{for } i \neq j.
    \end{cases}
\]
Using the previous formula, one can compute $ \Psi(E_{I_d,ij})$ for $i \leq j$. Now, one needs to compute the coefficients of $\Psi(E_{I_d, ij})$ in the basis $(E_{\exp(u), kl})_{k \leq l}$. As the basis $(E_{\exp(u), kl})_{k \leq l}$ is orthonormal, one simply needs to compute the dot product between $\Psi(E_{I_d, ij})$ and one element of the basis to get the corresponding coefficient. For $k \leq l$, one has:
\begin{equation*}
    \begin{aligned}
        \langle \Psi(E_{I_d, ij}), E_{\exp(u), kl} \rangle_{\exp(u)} &= \langle \Psi(E_{I_d, ij}), \exp(u/2)E_{I_d, kl}\exp(u/2) \rangle_{\exp(u)}   &\text{\footnotesize using the definition of $E_{\exp(u), kl}$}\\
        &= \tr\left(\Psi(E_{I_d, ij}) \exp(-u/2)E_{I_d, kl}\exp(-u/2)\right) &\text{ \footnotesize using the definition of the AIRM metric} \\
        &= \langle\exp(-u/2)\Psi(E_{I_d, ij})\exp(-u/2),E_{I_d, kl}\rangle_{Id}
    \end{aligned}
\end{equation*}
Therefore, it is the coefficient $(k,l)$ of the matrix $\exp(-u/2)\Psi(E_{I_d, ij}) \exp(-u/2)$ (up to a factor $\sqrt 2$ when $k \neq l$). We now need to compute this matrix. As $u$ is diagonal, $u = \text{diag}(\lambda_1,...,\lambda_d)$, one has that $\exp(-u/2) = \text{diag}(e^{-\lambda_1/2},...,e^{-\lambda_d/2})$. Therefore, and using \cref{eq:DK_formula}, the coefficient $(k,l)$ of the matrix $\exp(-u/2)\Psi(E_{I_d, ij}) \exp(-u/2)$ is:
\begin{equation*}
    \begin{cases}
        e^{-\lambda_i/2}e^{\lambda_i}e^{-\lambda_i/2} = 1 & \text{for } i = j = k = l, \\
        \frac{1}{\sqrt{2}}e^{-\lambda_i/2}\frac{e^{\lambda_i} - e^{\lambda_j}}{\lambda_i - \lambda_j}e^{-\lambda_j/2} = \sqrt{2}\frac{\sinh\left(\frac{\lambda_i - \lambda_j}{2}\right)}{\lambda_i - \lambda_j} & \text{for } i \neq j \text{ and } (k,l) = (i,j) \\
        0 & \text{if } (k,l) \neq (i,j).
    \end{cases}
\end{equation*}
Therefore, the matrix of $\Psi$ in the bases $(E_{I_d, ij})_{i \leq j}$ and $(E_{\exp(u), kl})_{k \leq l}$ is diagonal with diagonal coefficients $1$ and $2\frac{\sinh\left(\frac{\lambda_i - \lambda_j}{2}\right)}{\lambda_i - \lambda_j}$. Thus, the determinant of $\Psi $ is the product of these coefficients, which gives the result for the diagonal case.


\paragraph{Case 2: $u \in T_{I_d} \P_d$ general}
Let us now consider the general case where $u \in T_{I_d} \P_d$ is not diagonal. One has that, as $u$ is symmetric, one can diagonalize it: $u = g d g^\top$ where $g$ is an orthogonal matrix and $d$ is diagonal. As $\exp(u) = g \exp(d) g^\top$, one has that $\d \exp(u) \cdot h = g \left( \d \exp(d) \cdot (g^\top h g)\right) g^\top$. Therefore, as we are only interested in the determinant of $\d \exp(u)$, and as $g$ are orthogonal, one has that $\det \d \exp(u) \cdot h = \det\left( \d \exp(d) \cdot (g^\top h g)\right)$. 
One thus need to compute $\d \exp(d) \cdot (g^\top h g)$, and using Dalechii-Krein formula, as in the diagonal case, one has:
\begin{equation}
    \label{eq:DK_non_diag}
    \d \exp(d) \cdot (g^\top h g) = \left[ \left[ \exp^{[1]}(u)_{ij} \tilde{h}_{ij} \right] \right]
\end{equation}
where $\exp^{[1]}(u)$ is defined as above and $g^\top h g = [[\tilde{h}_{ij}]]$.
In order to do the same proof as for the diagonal case, one needs to modify the basis used in $T_{I_d} \P_d$ as in \cref{eq:DK_non_diag}, the coefficients of $g^\top h g$ appear (rather than directly the coefficients of $h$ as in the previous case). Therefore, we choose as basis for $T_{I_d} \P_d$ the basis $(E^{(u)}_{I_d, ij})_{ij}$ where $E^{(u)}_{I_d, ij} = g E_{I_d, ij} g^\top$. One can easily check that this basis is orthonormal and does not imply any volume changes. One can now use the same proof as for the diagonal case to compute the determinant of $\d \exp(u)$, which gives the result for the general case.

\section{The building block of the wrapped Gaussians}
\label{appendix:building_bloc}

In this section, we give a proof of \cref{prop:centered_reduced}. For this, we will actually show a more precise proposition:
\begin{proposition}
    Let $(p, \mu, \Sigma) \in \Theta$ and $X \sim \WG(p; \mu, \Sigma)$. Then,

    \begin{enumerate}
        \item $p^{-1/2}Xp^{-1/2} \sim \WG(I_d; \mu, \Sigma)$,
        \item $\Exp_p(\Log_p - \Vect_{p}^{-1}(\mu)) \sim  \WG(p; 0_{d(d+1)/2}, \Sigma)$,
        \item $\Exp_p( \Vect_p^{-1}(\Sigma^{-1/2}\Vect_{p}(\Log_p X))) \sim  \WG(p; \mu, I_{d(d+1)/2}).$
    \end{enumerate}
\end{proposition}
{ \scriptsize
\begin{proof}
    In this proof, we will only show the first two points of the above definition, the third one being similar to them. 
    \begin{enumerate}
        \item Let $Y = p^{-1/2}Xp^{-1/2}$. We want to show that $Y \sim  \WG(I_d; \mu, \Sigma)$. For this, let $\varphi \colon \P_d \rightarrow \R$ be a continuously bounded function. One has
        \begin{equation*}
        \begin{aligned}
        \mathbb{E}[\varphi(Y)] &= \int_{\P_d} \varphi( p^{-1/2}xp^{-1/2})f_{p; \mu, \Sigma}(x) \mathrm{d}\text{vol}(x) \\
        &= \int_{\P_d} \varphi( p^{-1/2}xp^{-1/2}) \frac{1}{\sqrt{(2\pi)^d \det \Sigma}}\frac{\exp\left(-\frac{1}{2} (\Vect_p(\Log_{p}(x)) - \mu)^\top \Sigma^{-1}(\Vect_p(\Log_{p}(x)) - \mu)\right)}{|J_{I_d}(\log(p^{-1/2}xp^{-1/2}))|}\mathrm{d}\text{vol}(x).
        \end{aligned}
        \end{equation*}
        
        Let us now define $\psi_p \colon x \mapsto p^{-1/2}xp^{-1/2}$. $\psi_p$ is a $\mathcal{C}^1$-diffeomorphisme between $\P_d$ and $\P_d$. Moreover,  as the volume element $\mathrm{d}\text{vol}$ is invariant by congruence of $\text{GL}(d, \R)$, the transformation $\psi_p$ does not imply any volume change. Therefore, by change of variables $y = \psi_p(x)$, one has:
        \begin{equation*}
        \begin{aligned}
        \mathbb{E}[\varphi(Y)] &= \int_{\P_d} \varphi( y) \frac{1}{\sqrt{(2\pi)^d \det \Sigma}}\frac{\exp\left(-\frac{1}{2} (\Vect_p(\Log_{p}(p^{1/2}yp^{1/2})) - \mu)^\top \Sigma^{-1}(\Vect_p(\Log_{p}(p^{1/2}yp^{1/2})) - \mu)\right)}{|J_{I_d}(\log(p^{-1/2}p^{1/2}yp^{1/2}p^{-1/2}))|}\mathrm{d}\text{vol}(y) \\
        &= \int_{\P_d} \varphi( y) \frac{1}{\sqrt{(2\pi)^d \det \Sigma}}\frac{\exp\left(-\frac{1}{2} (\Vect_p(p^{1/2}\log(y)p^{1/2}) - \mu)^\top \Sigma^{-1}(\Vect_p(p^{1/2}\log(y)p^{1/2}) - \mu)\right)}{|J_{I_d}(\log(y))|}\mathrm{d}\text{vol}(y).
        \end{aligned}
        \end{equation*}
        Finally, using that $\Vect_p(p^{1/2}\log(y)p^{1/2}) = \Vect_{I_d}(p^{-1/2}p^{1/2}\log(y)p^{1/2}p^{-1/2}) = \Vect_{I_d}(\log(y))$ and $\Log_{I_d} = \log$, we have,  
        $$\mathbb{E}[\varphi(Y)] = \int_{\P_d} \varphi( y) \frac{1}{\sqrt{(2\pi)^d \det \Sigma}}\frac{\exp\left(-\frac{1}{2} (\Vect_{I_d}(\Log_{I_d}(y)) - \mu)^\top \Sigma^{-1}(\Vect_{I_d}(\Log_{I_d}(y)) - \mu)\right)}{|J_{I_d}(\log(y))|}\mathrm{d}\text{vol}(y).$$
        
        This shows us that $Y \sim \WG(I_d; \mu, \Sigma)$.
        \item Let now $Y = \Exp_p(\Log_p - \Vect_{p}^{-1}(\mu))$. We want to show that $Y \sim \WG(p; 0_{d(d+1)/2}, \Sigma)$. Let $\varphi \colon \P_d \rightarrow \R$ be a bounded continuous function. One has
        \begin{equation*}
        \begin{aligned}
        \mathbb{E}[\varphi(Y)] &= \int_{\P_d} \varphi(\Exp_p(\Log_p x - \Vect_p^{-1}(\mu)))f_{p; \mu, \Sigma}(x) \mathrm{d}\text{vol}(x) \\
        &= \int_{\P_d} \varphi( \Exp_p(\Log_p x - \Vect_p^{-1}(\mu))) \frac{1}{\sqrt{(2\pi)^d \det \Sigma}}\frac{\exp\left(-\frac{1}{2} (\Vect_p(\Log_{p}(x)) - \mu)^\top \Sigma^{-1}(\Vect_p(\Log_{p}(x)) - \mu)\right)}{|J_{p}(\Log_p(x))|}\mathrm{d}\text{vol}(x).
        \end{aligned}
        \end{equation*}

        Let us now define $\psi_p \colon x \mapsto \Exp_p(\Log_p x - \Vect_p^{-1}(\mu))$. $\psi_p$ is a $\mathcal{C}^1$-diffeomorphisme between $\P_d$ and $\P_d$ and its inverse is $\psi_p^{-1} \colon y \mapsto \Exp_p(\Log_p x + \Vect_p^{-1}(\mu))$. By change of variables $y = \psi_p(x)$, one has:
        \begin{equation*}
        \begin{aligned}
        \mathbb{E}[\varphi(Y)] &= \int_{\P_d} \varphi( y) \frac{1}{\sqrt{(2\pi)^d \det \Sigma}}\frac{\exp\left(-\frac{1}{2} \Vect_p(\Log_{p}(y))^\top \Sigma^{-1}\Vect_p(\Log_{p}(y))\right)}{|J_{p}(\Log_p(y) + \Vect_p^{-1}(\mu))|}\frac{\mathrm{d}\text{vol}(y)}{|\det \mathrm{d}\psi_p(\psi^{-1}(y))|}.
        \end{aligned}
        \end{equation*}
        We need to compute the change of volume term $\det \mathrm{d}\psi_p(\psi^{-1}(y))$. For this, let us start by saying that $\mathrm{d}\psi_p(x) = \mathrm{d}\Exp_p(\Log_p x - \Vect_p^{-1}(\mu)) \circ \mathrm{d}\Log_p x$ therefore, $\det \mathrm{d}\psi_p(x) = \det \mathrm{d}\Exp_p(\Log_p x - \Vect_p^{-1}(\mu)) \det \mathrm{d}\Log_p x$. Now using the fact that $\mathrm{d} \Log_{p}(y) = \left(\mathrm{d} \Exp_{p} (\Log_{p}(y))\right)^{-1}$ and the definition of $J_p(u) = \det \mathrm{d} \Exp_p(u)$ (see \cref{theo:density}), we have 
        $$\det \mathrm{d} \psi_p(x) = J_p(\Log_px - \Vect_p^{-1}(\mu)) \frac{1}{J_p(\Log_p x)}$$ and thus, plugging $\psi_p^{-1}(y)$ into the equation:
        $$\det \mathrm{d} \psi_p(\psi_p^{-1}(y)) = J_p(\Log_p y) \frac{1}{J_p(\Log_p y + \Vect_p^{-1}(\mu))}.$$
        Therefore, 
        $$ \mathbb{E}[\varphi(Y)] = \int_{\P_d} \varphi( y) \frac{1}{\sqrt{(2\pi)^d \det \Sigma}}\frac{\exp\left(-\frac{1}{2} \Vect_p(\Log_{p}(y))^\top \Sigma^{-1}\Vect_p(\Log_{p}(y))\right)}{|J_{p}(\Log_p(y))|}\mathrm{d}\text{vol}(y).$$
        This shows us that $Y \sim \WG(p;0_{d(d+1)/2}, \Sigma)$. 
    \item For the third point, one can prove it similarly as the two previous one, having in mind that the vectorization $\Vect_p$ is an isometry (see \cref{prop:norm_vect}) therefore, neither $\Vect_p$ nor  $\Vect_p^{-1}$ implies any volume changes. 
    \end{enumerate}
\end{proof}}

Therefore, \cref{prop:centered_reduced} is a direct corollary of the previous result: 
\begin{corollary}
    Let $X \sim \WG(I_d; 0_{\nicefrac{d(d+1)}{2}}, I_{\nicefrac{d(d+1)}{2}})$ and let $(p, \mu, \Sigma) \in \Theta$. Let us define
    $$\Psi \colon x \in \P_d \mapsto p^{1/2}\Exp_p\left(\Vect_p^{-1}\left(\Sigma^{1/2}\left(\Vect_p\circ \Log_p x + \mu\right)\right)\right)p^{1/2}.$$
    Then, $\Psi(X) \sim \WG(p; \mu, \Sigma).$
\end{corollary}

\section{The wrapped Central Limit Theorem}
\label{appendix:CLT}
In this section, we give a proof of the wrapped Central Limit Theorem stated at \cref{theo:wrapped_CLT}. For this, let $(X_i)_{i \in \mathbb{N}^*}$ be a sequence of independent and identically distributed random variables on the Riemannian manifold $\P_d$. We suppose that the sequence $(\Vect_{I_d}(\Log_{I_d}(X_i)))_{i \in \mathbb{N}^*}$ of random variables on $\R^{\nicefrac{d(d+1)}{2}}$ satisfies the classical Central Limit Theorem. We want to show that the sequence $(X_i)_{i \in \mathbb{N}^*}$ satisfies the wrapped Central Limit Theorem.

\begin{remark}
    Let us start by saying that, as the map $x \mapsto \Vect_{I_d} \circ \Log_{I_d} x$ is a diffeomorphism between $\P_d$ and $\R^{\nicefrac{d(d+1)}{2}}$, the sequence $(\Vect_{I_d}(\Log_{I_d}(X_i)))_{i \in \mathbb{N}^*}$ is also independent and identically distributed. 
\end{remark}

As the sequence $(\Vect_{I_d}(\Log_{I_d}(X_i)))_{i \in \mathbb{N}^*}$ satisfies the classical Central Limit Theorem in $\R^{\nicefrac{d(d+1)}{2}}$, one has that,
$$\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \left( \Vect_{I_d}(\Log_{I_d}(X_i)) - \mu\right) \xrightarrow[n \to \infty]{d} \N(0, \Sigma).$$
By defining $m = \Exp_{I_d}(\Vect_{I_d}^{-1}(\mu)) \in \P_d$, and using the linearity of $\Vect_p$:
$$\Vect_{I_d} \left(\frac{1}{\sqrt{n}} \sum_{i=1}^{n} (\Log_{I_d}(X_i) - \Log_{I_d}(m))\right) \xrightarrow[n \to \infty]{d} \N(0, \Sigma).$$
Thus, by applying the continuous map $\Exp_{I_d} \circ \Vect_{I_d}^{-1}$ to the previous equation, by considering the fact that the convergence in distribution is stable by continuous maps (theorem 5.5 of \citealt{wassermanAllStatisticsConcise2004}) and the definition of the wrapped Gaussian (\cref{def:wrapped_gaussian}), one has that 
$$\Exp_{I_n} \left(\frac{1}{\sqrt{n}} \sum_{i=1}^{n} (\Log_{I_d}(X_i) - \Log_{I_d}(m))\right) \xrightarrow[n \to \infty]{d} \WG(I_d; 0, \Sigma).$$

We can now simplify the left-hand side of the previous equation:

\begin{equation*}
    \begin{aligned}
        \Exp_{I_n} \left(\frac{1}{\sqrt{n}} \sum_{i=1}^{n} (\Log_{I_d}(X_i) - \Log_{I_d}(m))\right) &= \exp \left(\frac{1}{\sqrt{n}} \sum_{i=1}^{n} (\log X_i - \log m)\right)
        \quad \text{\scriptsize \parbox{0.3\linewidth}{using the expression of $\Exp_{I_d}$ and $\Log_{I_d}$ of \cref{eq:exp_log_riem}.}} \\
        &= \exp \left(\sum_{i=1}^{n} (\log X_i + \log m^{-1})\right)^{\frac{1}{\sqrt{n}}} 
        \quad \text{\scriptsize \parbox{0.25\linewidth}{using $\log m = - \log m^{-1}$ and $\exp(\alpha x) = \exp(x)^\alpha$ for $\alpha \in \R$.}} \\
        &= \exp \left(\sum_{i=1}^{n} \log( X_i \odot m^{-1})\right)^{\frac{1}{\sqrt{n}}} 
        \quad \text{\scriptsize \parbox{0.3\linewidth}{using the definition of the logarithmic product.}} \\
        &= \left(\bigodot_{i=1}^n X_i \odot m^{-1}\right)^{\frac{1}{\sqrt{n}}}.
    \end{aligned}
\end{equation*}

Therefore, one has the final result of the wrapped Central Limit Theorem:
$$\left(\bigodot_{i=1}^n X_i \odot m^{-1}\right)^{\frac{1}{\sqrt{n}}} \xrightarrow[n \to \infty]{d} \WG(I_d; 0, \Sigma).$$

\paragraph{The generalized wrapped CLT}

The previous version of the wrapped Central Limit Theorem is centered around the identity matrix $I_d$. However, one can generalize this theorem to any point $p \in \P_d$. For this, we need to introduce a generalized logarithmic product $\odot_p$ between two points $q_1, q_2 \in \P_d$:
$$q_1 \odot_p q_2 = \Exp_p(\Log_p q_1 + \Log_p q_2).$$

In the same way as before, one can show the following generalized wrapped CLT
$$\left(\!\bigodotp_{i=1}^n X_i \odot_p M^{-1}\right)^{\frac{1}{\sqrt{n}}} \xrightarrow[n \to \infty]{d} \WG(p; 0, \Sigma).$$

\section{An extension to wrapped Elliptically Contoured Distributions}
\label{appendix:wrapped_EC}
As one can see from the expression of the density of a wrapped Gaussian $\WG(p; \mu, \Sigma)$ given at \cref{theo:density}, it is intrinsically linked to the density of the multivariate Gaussian $\mathcal{N}(\mu, \Sigma)$. This suggests a possible extension to \emph{Elliptically Contoured Distributions} (chapter 6 of \citet{johnson1987multivariate} or \citealt{delmas2024elliptically}). We recall the definition of an Elliptically Contoured distribution:
\begin{definition}[Elliptically Contoured Distribution]
    A random vector $X \in \R^d$ follows an Elliptically Contoured distribution if there exists $\mu \in \R^d$, $\Sigma \in \P_d$ and a function $g$ such that $X$ has density 
    $$f_X(x) = k \det(\Sigma)^{-1/2}g\left((x-\mu)^\top \Sigma^{-1}(x - \mu)\right)$$
    where $k$ is a normalizing factor. We denote $X \sim \EC(\mu,\Sigma,g)$.
\end{definition}
For example, the multivariate Gaussian $\N(\mu, \Sigma)$ is an Elliptically Contoured distribution with $g \colon t \mapsto \exp(-t/2)$. Another example is the multivariate t-distributions for which $g \colon t \mapsto \left(1+t/\nu\right)^{\nicefrac{-d+\nu}{2}}$ (see V.B  of chapter 1 of \citealt{delmas2024elliptically}). One can then extend what has been done previously on the wrapped Gaussian to define \emph{Wrapped Elliptically Contoured Distributions} just like above:
\begin{definition}[Wrapped Elliptically Contoured]
    Let $p \in \P_d$, $\mu \in \R^{\nicefrac{d(d+1)}{2}}, \Sigma \in \P_{\nicefrac{d(d+1)}{2}}$ and $g$ be a function. Then, a random vector $X$ on $\P_d$ follows a Wrapped Elliptically Contoured denoted $\WEC(p;\mu,\Sigma,g)$ if
    \begin{equation*}
        X = \Exp_p(\Vect_p^{-1}(\mathbf{t})),~  \mathbf{t} \sim \EC(\mu, \Sigma, g).
    \end{equation*}
\end{definition}
One can then compute the density of $\WEC(p; \mu, \Sigma, g)$ similarly as in \cref{theo:density}. Moreover, all the work done on the equivalence relation for wrapped Gaussians stays valid for wrapped elliptically contoured distributions.



\section{The proofs on the equivalence relation}
\label{appendix:equivalence}
In this section, we want to give proofs of the different results of \cref{sec:equiv_gaussian}. We recall the propositions and give their proofs. 
Let us start by \cref{prop:equivalence}. 

\begin{proposition}
    Let $(p, \mu, \Sigma) \in \Theta$ and $t \in \R$. One has that $\text{WG}(p; \mu, \Sigma)$ and $\text{WG}(e^{t}p;\mu - t\nu, \Sigma)$ are equal where $\nu = \Vect_p(p) = (1,\cdots,1,0,\cdots,0) \in \R^{\nicefrac{d(d+1)}{2}}$.
\end{proposition}
{ \scriptsize
\begin{proof}
    In this following, we denote by $\gamma$ the function $\gamma \colon t \mapsto e^t p$.
    Let us denote by $\tilde{f}$ the density of $\WG(\gamma(t); \mu - t\nu, \Sigma)$ and by $f$ the density of $\WG(p; \mu, \Sigma)$. We want to show that $\tilde{f} = f$. Let $x \in \P_d$, by \cref{theo:density}, one has:
    $$\tilde{f}(x) = \frac{1}{\sqrt{(2\pi)^d \det \Sigma}} \frac{\exp\left(-\frac{1}{2}\left(\Vect_{\gamma(t)}(\Log_{\gamma(t)}(x)) - \mu + t\nu \right)^\top \Sigma^{-1}\left(\Vect_{\gamma(t)}(\Log_{\gamma(t)}(x)) - \mu + t\nu\right)\right)}{|J_{\gamma(t)}(\Log_{\gamma(t)}(x))|}.$$
    One has, that 
    \begin{equation*}
        \begin{aligned}
            \Log_{\gamma(t)}(x) &= \gamma(t)^{1/2}\log(\gamma(t)^{-1/2}x\gamma(t)^{-1/2})\gamma(t)^{1/2} \\
            &= e^{ t}p^{1/2}\log(e^{- t}p^{-1/2}x p^{-1/2})p^{1/2} \quad { \text{ using that } \gamma(t) = e^{ t}}\\
            &= e^{ t}p^{1/2}\log(e^{- t}I_d)p^{1/2} + e^{ t}\Log_p(x) \quad { \text{ using that } e^{- t}I_d \text{ and } p^{-1/2}x p^{-1/2} \text{commute}} \\
            &= - t e^{ t}p +  e^{ t}\Log_p(x).
        \end{aligned}
    \end{equation*}
    Furthermore, one has:
    \begin{equation*}
        \begin{aligned}
            \Vect_{\gamma(t)}(\Log_{\gamma(t)}(x)) &= - t e^{ t} \Vect_{\gamma(t)}( p) +  e^{ t}\Vect_{\gamma(t)}(\Log_p(x))  \quad { \text{ using the linearity of}  \Vect_{\gamma(t)}} \\
            &= - t e^{ t} \Vect_{I_d}(\gamma(t)^{-1/2} p\gamma(t)^{-1/2}) + e^{ t}\Vect_{I_d}(\gamma(t)^{-1/2}\Log_p(x)\gamma(t)^{-1/2}) \\
            &= -  t e^{ t} e^{- t} \Vect_{I_d}(p^{-1/2}  p p^{-1/2}) +  e^{ t} e^{- t}\Vect_{I_d}(p^{-1/2}\Log_p(x)p^{-1/2}) \\
            &= - t \Vect_p( p) + \Vect_p(\Log_p(x)).
        \end{aligned}
    \end{equation*}

    Therefore, the numerator of the density $\tilde{f}$ can be rewritten as:
    $$\exp\left(-\frac{1}{2}(\Vect_p(\Log_p(x)) - \mu)^\top \Sigma^{-1}(\Vect_p(\Log_p(x)) - \mu)\right).$$
    which is the same numerator as $f$.

    Let us now focus on the denominator. One has:
    \begin{equation*}
        \begin{aligned}
            J_{\gamma(t)}(\Log_{\gamma(t)}(x)) &= J_{I_d}(\gamma(t)^{-1/2}\Log_{\gamma(t)}(x)\gamma(t)^{-1/2}) \\
            &= J_{I_d}(e^{- t}p^{-1/2}\Log_{\gamma(t)}(x)p^{-1/2}) \\
            &=J_{I_d}(- t I_d + p^{-1/2}\Log_{p}(x)p^{-1/2}) \quad { \text{ using the computation of } \Log_{\gamma(t)}(x)}. \\
        \end{aligned}
    \end{equation*}

    We recall that the Jacobian determinant of the exponential map at the identity is: 
    $$J_{I_d}(u) = 2^d \prod_{i < j} \frac{\sinh\left(\frac{\lambda_i(u) - \lambda_j(u)}{2}\right)}{\lambda_i(u) - \lambda_j(u)}$$ 
    where $(\lambda_i(u))_i$ are the eigenvalues of $u$. Moreover, the eigenvalues of $u := -\alpha t I_d + p^{-1/2}\Log_{p}(x)p^{-1/2}$ are 
    $$\lambda_i(u) = -\alpha t + \lambda_i\left(p^{-1/2}\Log_{p}(x)p^{-1/2}\right).$$
    Thus, for all $i < j$, one has:
    $$\lambda_i(u) - \lambda_j(u) = \lambda_i\left(p^{-1/2}\Log_{p}(x)p^{-1/2}\right) - \lambda_j\left(p^{-1/2}\Log_{p}(x)p^{-1/2}\right)$$
    and therefore, this leads to:
    $$J_{I_d}(u) = J_{I_d}\left(p^{-1/2}\Log_{p}(x)p^{-1/2}\right) = J_p\left(\Log_p(x)\right).$$
    So the denominator of the density $\tilde{f}$ is the same as the denominator of the density $f$ and therefore, the two densities are equal.
\end{proof}
}

\begin{remark}
    The function $\gamma \colon t \mapsto e^t p$ is actually the geodesic with initial point $p$ and initial velocity $p$.
    Indeed, the expression of the geodesic $\Gamma_{q,V}(t)$ with initial point $q$ and initial velocity $V \in T_q \P_d$ is (see \citealt{pennecManifoldvaluedImageProcessing2020}):
    $$\forall t \in \R, ~ \Gamma_{q,V}(t) = q^{1/2}\exp(tq^{-1/2}Vq^{-1/2})q^{1/2}.$$
    Therefore, the geodesic $\gamma$ with initial point $p$ and initial velocity $p$ (which is a symmetric matrix, therefore an element of $T_p \P_d \simeq \S_d$) is:
    $$\forall t \in \R, ~ \Gamma_{p,p}(t) = e^{t}p = \gamma(t).$$
\end{remark}

We now want to show \cref{prop:param_to_min} that we recall underneath:
\begin{proposition}
    Let $\theta = (p; \mu, \Sigma) \in \Theta$ be a tuple of parameters. Then, the minimal representative of the class $[\theta]$  as defined at \cref{def:representative} is $\theta^\text{min} = (p^\text{min}; \mu^\text{min}, \Sigma^\text{min})$ where 
    \begin{equation*}
        \left\{
            \begin{aligned}
                p^{\text{min}} &= e^{\frac{1}{d}\sum_{i=1}^d \mu_i}p, \\ 
                \mu^{\text{min}} &= \mu -  \frac{1}{d}\sum_{i=1}^d \mu_i \nu,\\
                \Sigma^\text{min} &= \Sigma.
            \end{aligned}
        \right.    
    \end{equation*}
\end{proposition}

{ \scriptsize
\begin{proof}
    We want to find the smallest $\mu^{\text{min}}$ in the sens of $\| \cdot \|_2$ and the corresponding $p^{\text{min}}$ such that $(p; \mu, \Sigma) \cong (p^{\text{min}}; \mu^{\text{min}}, \Sigma^{\text{min}})$. As all the $\mu$ in the equivalence class of $[\theta]$ are of the form $\mu - t\nu$ for $t\in \R$, to find the smallest $\mu^{\min}$, one needs to minimize the following function:
    $$\varphi \colon t \mapsto \|\mu - t\nu\|^2_2 = \|\mu\|_2^2 - 2 t\langle \mu, \nu \rangle + t^2 \|\nu\|^2_2.$$ 
    One thus has: 
    $$\varphi'(t) = -2\langle \mu, \nu \rangle + 2t\|\nu\|^2_2.$$
    The minimum is reached at $t^{\text{min}}= \frac{\langle \mu, \nu \rangle}{\|\nu\|^2_2}$ with $\|\nu\|^2 = nd$ and $\langle \mu, \nu \rangle = \sum_{i=1}^d \mu_i$. Therefore, one has:
    \begin{equation*}
        \left\{
            \begin{aligned}
                p^{\text{min}} &= e^{\frac{1}{d}\sum_{i=1}^d \mu_i}p \\ 
                \mu^{\text{min}} &= \mu - \frac{1}{d}\sum_{i=1}^d \mu_i\nu = \left(\mu_1 - \frac{1}{d}\sum_{i=1}^d \mu_i, \cdots, \mu_d - \frac{1}{d}\sum_{i=1}^d \mu_i, \mu_{d+1}, \cdots, \mu_{d(d+1)/2}\right).
            \end{aligned}
        \right.    
    \end{equation*}
\end{proof}

}


\section{Why does estimating \texorpdfstring{$p$}{p} using the Riemannian mean fails in the general case?}
\label{appendix:moment_estimator}

We said in \cref{sec:estimation} that when $\mu^\star \neq 0$, using the Riemannian mean $\mathfrak{G}(x_1,...,x_N)$ does not work to estimate the parameters $(p^\star, \mu^\star, \Sigma^\star)$. Let us explain why. For this, we suppose in the following that $\mu^\star \neq 0$. Let $\hat{p}_N$ be the Riemannian mean: $\hat{p}_N = \mathfrak{G}(x_1,...,x_N)$. Then, we can use \cref{prop:MLE_mu_sigma} to compute the MLE of $\mu$ and $\Sigma$:
\begin{equation*}
    \begin{aligned}
        \hat{\mu}_N &= \frac{1}{N}\sum_{i=1}^{N}\Vlog_{\hat{p}_N}(x_i), \\
        \hat{\Sigma}_N &= \frac{1}{N}\sum_{i=1}^{N}\left(\Vlog_{\hat{p}_N}(x_i) - \hat{\mu}_N\right)\left(\Vlog_{\hat{p}_N}(x_i) - \hat{\mu}_N\right)^\top.
    \end{aligned}
\end{equation*}
where we recall that $\Vlog_{\hat{p}_N}$ is the vectorization at $\hat{p}_N$ of $\Log_{\hat{p}_N}$ i.e. $\Vlog_{\hat{p}_N} = \Vect_{\hat{p}_N} \circ \Log_{\hat{p}_N}$. Let us focus on $\hat{\mu}_N$. Using the linearity of $\Vect_{\hat{p}_N}$, we can write that
$$ \hat{\mu}_N = \Vect_{\hat{p}_N}\left(\frac{1}{N}\sum_{i=1}^{N}\Log_{\hat{p}_N}(x_i)\right).$$
According to proposition 3.4 of \citet{moakherDifferentialGeometricApproach2005}, as $\hat{p}_N$ is the Riemannian mean of the points $(x_1,...,x_N)$, we have the following:
$$\sum_{i=1}^{N}\Log_{\hat{p}_N}(x_i) = 0.$$
Therefore, $\hat{\mu}_N = 0$. It is therefore not a good estimator of $\mu^\star \neq 0$. That is why we do not use the Riemannian mean as an estimator of $p$ in a general setting when we do not know \textit{a priori} that $\mu^\star = 0$. 

\section{More details on the MLE experiments}
\label{appendix:MLE_expe}

In this section, we give more details on the setup of the synthetic experiments lead in \cref{sec:estimation} to assess the  quality of the estimation of parameters of a wrapped Gaussian using an MLE. Upon acceptation, we will release the code used to perform these experiments.
To obtain the results plotted at \cref{fig:res_expe_MLE}, we repeated 5 times the experiment with different true parameters $\theta^\star = (p^\star, \mu^\star, \Sigma^\star)$ randomly generated. Here are details on how we generated the true parameters:
\begin{itemize}
    \item For $p^\star$, we use the function \texttt{generate\_random\_spd\_matrix} from the library PyRiemann \citep{pyriemann}. This function generates a random SPD matrix by generating a random matrix $A$ and then computing $\exp((\bar{X} + s*A)^\top (\bar{X} + s*A))$ where $\bar{X}$ and $s$ are parameters chosen by the user. We set $\bar{X} = 0.1 I_d$ and $s = 1$.
    \item For $\mu^\star$, we generate a random vector of size $\nicefrac{d(d+1)}{2}$ with values in $[0,0.1]$.
    \item For $\Sigma^\star$, we generate a random SPD matrix using the same function as for $p^\star$ with $\bar{X} = 0.01 I_{\nicefrac{d(d+1)}{2}}$ and $s = 0.02$.
\end{itemize}
We chose relatively small values for $\bar{X}$ and $s$ because otherwise, when the dimension $d$ is large, the generated parameters are very far from identity leading to numerical instability.

\section{Estimating the parameters of a wrapped Gaussian when the covariance matrix \texorpdfstring{$\Sigma$}{Sigma} is diagonal}
\label{appendix:esimation_sigma_diag}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/estimation_vs_number_diagonal_vs_full_dim_10} 
    \caption{Comparison of the estimation of the parameters of a wrapped Gaussian when the covariance matrix $\Sigma$ is diagonal or full. The dimension of the SPD matrices in this experiment is $d = 10$.} 
    \label{fig:estimation_diagonal_vs_full}
\end{figure}

In this section, we detail the experiments on the estimation of the parameters of a wrapped Gaussian when the covariance matrix $\Sigma$ is diagonal. We used the same setup as in the previous experiments detailed in \cref{appendix:MLE_expe} except that we generated $\Sigma^\star$ as a diagonal matrix. The diagonal was uniformly sampled in $[0,1]$. We repeated the experiment $5$ times with different true parameters. 
The goal for this experiment was to assess the impact of the structure of the covariance matrix on the estimation of the parameters. We wanted to show that when the covariance matrix is diagonal, the estimation of the parameters requires fewer samples. We plotted the results in the case of dimension $d=10$ at \cref{fig:estimation_diagonal_vs_full}. One can see that, while the estimation of $p$ and $\mu$ is not affected by the structure of $\Sigma$, the estimation of $\Sigma$ is better when $\Sigma$ is diagonal. With a lot less samples, one can achieve a significantly better estimation of $\Sigma$ when it is diagonal. This is coherent as the number of coefficients to estimate is reduced when $\Sigma$ is diagonal. Therefore, this setting of $\Sigma$ diagonal can be a good choice when the number of samples is limited. 


\section{More details on the real data experiments} 
\label{appendix:expe_real_data}
Let us start by giving more details on the different datasets used in the experiments and the preprocessing we used.
\begin{itemize}
    \item \textbf{BCI Datasets}: We considered 2 datasets from Brain Computer Interfaces (BCI) for our experiments: \emph{BNCI2014001} \citep{BNCI2014004} and \emph{Zhou2016} \citep{Zhou2016}. They consist of several subjects and several sessions per subjects doing a Motor Imagery task \citep{motor_imagery}. We used the library MOABB \citep{Aristimunha_Mother_of_all_2023} to load and preprocess the data.  For each EEG, We start by applying a standard band-pass filter with range $[7; 35]$ Hz. Then, we used the Ledoit-Wolf shrunk covariance matrix  \citep{ledoitWellconditionedEstimatorLargedimensional2004} to in order to compute the covariance matrices and to avoid ill-conditioned matrices. The experiment we lead was cross-subject: each classifier was trained on all subject except one and tested on this last subject. 
    \item \textbf{AirQuality}: This dataset is from the Beijing Municipal Monitoring Center. It is a dataset of air quality monitored from 34 different sites in Beijing, China \citep{huaCompetingPM2NO22021}.  For each site, six atmospheric pollutants where recorded every hour: CO, NO$_2$, O$_3$, PM$_{10}$, PM$_{2.5}$ and SO$_2$. We used the same preprocessing as in \citet{smithMultiSiteMultiPollutantAtmospheric2022} to get a point cloud of $102$ covariance matrices of size $6 \times 6$. Each covariance matrix has a label depending on which period it represents: weekdays, weekends or holidays. 
    \item \textbf{Indiana, Pavia Uni, Salinas}: These three dataset of hyperspectral remote sensing datasets are all publicly available at \url{https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes}. Each dataset contains one hyperspectral image of a certain region containing a unique number of reflectance bands. We applied the same preprocessing that in \citet{collas2021} or \citep{bouchard2024random} that consists in four main steps. First, we normalize the data by subtracting the image global mean. Then, we apply a PCA to reduce the dimension of the data to $5$. A sliding window with no overlap is then used around each pixel for data sampling and then vectorized. In our experiments, we used a window of size $25 \times 25$. Finally, we compute the covariance matrix of each vectorized window using the Sample Covariance Matrix to get a point cloud of covariance matrices. For each covariance matrix, its class was computed by taking the majority class of the pixels in the window.
    \item \textbf{Textile}: This dataset is made of a set of real images from textile manufacturing that contain non-defective and defective woven textiles. These images come from the public MVTec Anomaly Detection dataset \citep{MVTec2021}. The same preprocessing as in \citet{smithDataAnalysisUsing2022} was applied to get the covariance matrices. The two classes correspond to defective and non-defective textiles. 
    \item \textbf{BreizhCrops}: This dataset is intended for supervised classification of field crops from satellite time series. We used the dataset \texttt{FRH01} that is composed of satellite time series from the French region \emph{Finistère}. As they are multivariate time-series, we simply converted them to covariance matrices using the  Oracle Approximating Shrinkage estimator \citep{Chen2010}. The two classes correspond to different types of crops. More details on this dataset can be found in the original paper \citep{breizhcrops2020}.   
\end{itemize}
For the non-BCI datasets (AirQuality, Indiana, Pavia Uni, Salinas, Textile and BreizhCrops), we used a 5-fold cross-validation to evaluate the performance of the classifiers. 

Let us also give some details on the implementation of the different classifiers used in the experiments.
\begin{itemize}
    \item The MDM is implemented using the library PyRiemann \citep{pyriemann}.
    \item The TS-LDA uses the \texttt{TangentSpace} class from PyRiemann \citep{pyriemann} and the LDA from Scikit-learn \citep{scikit-learn}.
    \item The TS-QDA uses the \texttt{TangentSpace} class from PyRiemann \citep{pyriemann} and the Naive Bayes classifier from Scikit-learn \citep{scikit-learn}.
    \item For the Ho-WDA and He-WDA, we implemented them using our MLE to estimate the parameters of the wrapped Gaussians. To optimize the MLE, we used in practice the Riemannian Conjugate Gradient method \citep{boumalIntroductionOptimizationSmooth2023} with a maximum of $1,000,000$ iterations and a max time set to $2$ hours.
\end{itemize}
\end{document}


