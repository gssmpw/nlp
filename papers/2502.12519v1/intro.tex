In the correlation clustering problem, we are given a complete graph where each edge is labeled as either \( "+" \) or \( "-" \). A \( "+" \) edge indicates that the two vertices are \textit{similar}, while a \( "-" \) edge indicates they are \textit{dissimilar}. For any partition of the graph, an edge is considered to be in disagreement if it is a negative edge and its endpoints belong to the same cluster, or if it is a positive edge and its endpoints belong to different clusters.

Given a clustering (partition), the disagreement \( \rho(v) \) for any node \( v \) is defined as the number of edges incident to \( v \) that are in disagreement with respect to the clustering. The goal of the correlation clustering problem is to find a clustering that minimizes an objective function capturing the disagreement of edges.

Puleo and Milenkovic~\cite{PM18} introduced the objective of minimizing the \( \ell_p \)-norm of the disagreements over the vertices, which is defined as:
\[
\bigg( \sum_{v \in V} \rho(v)^p \bigg)^{1/p}.
\]
This objective generalizes the correlation clustering problem proposed by Bansal, Blum, and Chawla~\cite{BBC04}, which corresponds to the case \( p = 1 \), where the goal is to minimize the total number of disagreements. Significant progress has been made on the \( \ell_1 \)-norm objective~\cite{CGW05,ACN08,CMSY15,CLN22,CLLN23}, Cao, Cohen-Addad, Lee,
Li, Newman, and Vogl~\cite{cao2024understanding} present a $1.437$-approximate algorithm for \( \ell_1 \)-norm objective. For the case \( p = \infty \), it corresponds to the min-max correlation clustering problem, where the goal is to minimize the maximum disagreements over each vertex. The problem remains relatively unexplored and the best approximate ratio is $4$ given by Heidrich, Irmai, and Andres~\cite{heidrich20244}.

Puleo and Milenkovic~\cite{PM18} proposed an algorithm that achieves a $48$-approximation ratio. Their approach uses the standard metric linear programming formulation followed by a rounding algorithm. Later, Charikar, Gupta, and Schwartz~\cite{CGS17} improved this result to a $7$-approximation using the same framework, and Kalhan, Makarychev, and Zhou~\cite{KMZ19} further reduced it to a $5$-approximation. Davies, Moseley, and Newman~\cite{davies2023fast} designed a combinatorial algorithm that achieves a $40$-approximation ratio with a runtime of $O(n^2 \log n)$, where $n = |V|$ is the number of vertices in the graph. The best known approximation ratio to date is $4$, achieved by Heidrich, Irmai, and Andres~\cite{heidrich20244}, who also used a combinatorial approach with a runtime of $O(n^2 + n D^2)$, where $D$ is the maximum degree of the graph.



Concerning efficiency, the ultimate goal of an efficient algorithm is to achieve a running time that is nearly linear in $m = |E^{+}|$.\footnote{In previous correlation clustering literature, it is typical to use $m = |E^{+}|$ to denote the number of positive edges and to obtain bounds in terms of $m$, as it has been pointed out by \cite{CDK14} that it is common to have a much smaller number of positive edges than negative edges in practical applications.} However, none of the aforementioned algorithms has yet achieved such a goal. Recently, Cao, Li, and Ye~\cite{cao2024simultaneously} proposed an nearly-linear time algorithm that achieves a $63.3$-approximation ratio. While their algorithm is fast, the approximation ratio is far from optimal.

%However, none of these algorithms are efficient.\footnote{We say an algorithm is efficient or nearly efficient if its running time is (nearly) linear in the number of positive edges, i.e., $O(m)$.} Recently, Cao, Li, and Ye~\cite{cao2024simultaneously} proposed an efficient algorithm that achieves a $63.3$-approximation ratio. While their algorithm is fast, the approximation ratio is far from optimal.

This naturally leads to the question:

\begin{quote}
    Can we design a nearly linear algorithm for min-max correlation clustering with a small approximation ratio?
\end{quote}

We give a nearly linear algorithm for the problem that achieves a $(3+\epsilon)$-approximation. The algorithm can be tailored into algorithms in various large-scale models, such as a $O(1)$-round algorithm for the sublinear-memory massively parallel computation (MPC) model, as well as a single-pass semi-streaming algorithm. Our main result is given as follows:



\begin{restatable}{theorem}{thmmain} \label{thm:thmmain}
Let $G = (V, E^+)$ be a min-max correlation clustering instance, $\epsilon > 0$ be a small constant, and $\OPT$ be the value of the optimal solution. In the following models, there exist randomized algorithms that output a clustering $\mathcal{C}$ with $\obj(\mathcal{C}) \leq (3+\epsilon)\cdot \OPT$ w.h.p.\footnote{{\it With high probability}, which refers to with probability at least $1-1/n^{c}$ for some sufficiently large constant $c$.} with the following attributes:
\begin{enumerate}
    \item \label{itm:main1} (Sequential model) An $O(m \log^2 n/ \epsilon^2 )$-time algorithm.
    
    % \item (Sublinear model) The running time is $\tilde{O}(n)$.
    
    \item \label{itm:main2}(MPC model) An $O(\log(1/\epsilon))$-round algorithm using $O(n^{\delta})$ memory per machine and total memory $O(m \log n / \epsilon^2)$.
    
    \item \label{itm:main3}(Semi-streaming model) A single-pass streaming algorithms that uses $O(n \log n/ \epsilon^2)$ space.
\end{enumerate}
\end{restatable}

We may also trade the running time for an exact 3-factor approximation algorithm. 
\begin{restatable}{cor}{exactthreeapprox}\label{cor:exact3approx}
There exists a deterministic sequential algorithm that runs in $O(m (D\log D) (\log n))$ time that outputs a 3-approximate solution, where $D$ is the maximum degree of $G^{+} = (V, E^{+})$. 
\end{restatable}

%In addition, for more traditional settings, such as the parallel setting and the sequential setting, we obtain the following: 
%\begin{theorem}
%Given min max correlation clustering instance $G = (V, E^+)$. Let $\OPT$ be the value of an optimal min max clustering. For any $\epsilon > 0$, there exists algorithms in the following models:
%\begin{itemize}
%\item For the work-depth model, there exists a $O(\log n)$-depth $O(m\log n/ \epsilon^2)$-work algorithm that computes a $(3+\epsilon)$-approximate solution w.h.p.

%\end{theorem}
\subsection{Additional Related Works}
%Traditional solutions for correlation clustering often struggle to handle large-scale networks efficiently. As data volumes continue to grow, the need for scalable parallel algorithms becomes increasingly critical. However, designing such algorithms presents significant challenges, as many classical approaches to graph problems rely on inherently sequential techniques, such as iterative refinements and adaptive decision-making. 

A substantial body of research has focused on parallel algorithms and streaming algorithms for the correlation clustering problem. In the MPC model, significantly more work has been done for the $\ell_1$ setting. Specifically, numerous studies have focused on improving the number of rounds and the approximation ratio~\cite{blelloch2012greedy, PPORRJ15, FischerN20, CCMU21, DBLP:conf/icml/Cohen-AddadLMNP21, assadi2021sublinear, CKLPU23, cao2024breaking}. Very recently, Cohen-Addad, Lolck, Pilipczuk, Thorup, Yan, and Zhang~\cite{cohen2024combinatorial} designed an $O(1)$-round MPC algorithm that achieves a 1.876-approximation ratio.


In contrast, there has been less work for the $\ell_{\infty}$ setting. The only known work is by Cao, Ye, and Li~\cite{cao2024simultaneously}, who proposed an algorithm that achieves a 63.3-approximation ratio in $O(\log^3 n)$ rounds and another algorithm that achieves a 360-approximation ratio in $O(1)$ rounds.

The correlation clustering problem has also been extensively studied in the streaming model for the $\ell_1$ setting. Several algorithms have achieved good approximation ratios with constant rounds~\cite{CDK14, ACGMW21, DBLP:conf/icml/Cohen-AddadLMNP21, BCMT22}. In the single-pass setting, Ahn, Cormode, Guha, McGregor, and Wirth~\cite{ACGMW21} presented the first algorithm with a space complexity of $\tilde{O}(n + m^-)$, where $m^-$ is the number of negative edges in the graph. This result has been further improved by multiple works~\cite{assadi2021sublinear, BCMT22, chakrabarty2023single}. Very recently, Cohen-Addad, Lolck, Pilipczuk, Thorup, Yan, and Zhang~\cite{cohen2024combinatorial} designed an algorithm that achieves a 1.876-approximation ratio in a single pass using $n \polylog(n)$ space.

There has been very little work explicitly addressing the $\ell_{\infty}$ case in the streaming model. To the best of our knowledge, we are the first to propose a streaming algorithm for the $\ell_{\infty}$ correlation clustering problem.



\subsection{Technical Overview} 
\paragraph{Better Approximation} Our first main technical contribution is a newly achieved approximation factor of 3. Given a guess for the optimal objective value $\phi$, if $OPT \leq \phi$, \cite{heidrich20244} observed that if the neighborhoods of $u$ and $v$ share at least $2\phi$ elements, then they must belong to the same cluster in the optimal solution. Similarly, if $u$ and $v$ differ by more than $2\phi$ elements, then they must belong to different clusters in the optimal solution. 

Furthermore, they observed that these properties can be used to determine the clusters for vertices of degrees at least $4\phi$. Specifically, if $\deg(x) \geq 4\phi$, then for every other vertex $y$, either $|N[x] \cap N[y]| \geq 2\phi$ or $|N[x] \Delta N[y]| \geq 2\phi$. Here, $N[x] = N(x) \cup \{x\}$ represents the closed neighborhood of vertex $x$. The remaining vertices can then be placed in singleton clusters, as the disagreements per vertex will be upper bounded by their degrees, $4\phi$. Therefore, a clustering of disagreements upper bounded by $4\phi$ can be constructed, resulting in a 4-approximation algorithm. 

To achieve a 3-approximation, we first observe that if two vertices $x$ and $y$ have degrees greater than $3\phi$, then it is also the case either $|N[x] \cap N[y]| \geq 2\phi$ or $|N[x] \Delta N[y]| \geq 2\phi$ holds. In other words, whether $x$ and $y$ belong to the same cluster is uniquely determined in the optimal solution. Therefore, the clustering induced on the high-degree vertices (vertices with $\deg > 3\phi$) is uniquely determined.

Now the question lies in the placement of the low-degree vertices, that is, vertices with degrees upper-bounded by $3\phi$. It is unclear whether they should be placed in singleton clusters, as it is possible that they need to be included in the same cluster with certain high-degree vertices. Otherwise, the disagreements associated with the high-degree vertices could become too large.

We show that the low-degree vertices can be placed in the high-degree clusters to achieve a maximum disagreements of $3\phi$, provided $OPT \leq \phi$. A key structural result we show is that if a low-degree vertex $w$ belongs to some cluster $C$ in an optimal solution, then no vertex $v$ outside $C$ can have similar neighborhood with $w$, i.e., $|N[v] \Delta N[w]| \leq 2\phi$. Using this structural result, we show the following algorithm constructs a clustering with maximum disagreement upper bounded by $3\phi$ (presented slightly differently here than in the main body for the sake of intuition):

1. Form clusters for high-degree vertices based on whether $|N[u] \Delta N[v]| \leq 2\phi$ for all high-degree vertices $u,v$. 2. Choose an arbitrary vertex $u$ in each cluster and have it propose to low-degree neighboring vertices with whose neighborhoods are similar to $u$. 3. For each low-degree vertex that receives at least one proposal, pick one arbitrary proposal and join the cluster containing the vertex that sent it. 4. Place all low-degree vertices that do not receive a proposal into singleton clusters.

\paragraph{Efficient Implementations} 
The remaining question is how such an algorithm can be implemented efficiently, particularly in time (and total memory) nearly linear in $|E^{+}|$. A main technical challenge lies in Step 1. To implement it within the aforementioned time bound, we can only afford to conduct similarity tests (i.e.~to test whether $|N[u] \Delta N[v]| \leq 2\phi$) for $O(|E^{+}|)$ times. However, it is possible for two vertices that are endpoints of a negative edge to have similar neighborhoods and thus need to be placed in the same cluster to achieve a good clustering.

Using the structural result, we further show that two high-degree vertices $u$ and $v$ are in the same cluster in the optimal solution if and only if there are at least $\phi+1$ disjoint paths of length 2 connecting $u$ and $v$ in $\Esim$, where $\Esim \subseteq E^{+}$ consists of all the edges in $E^{+}$ whose endpoints have similar neighborhoods. This property enables us to develop efficient algorithms for the sublinear MPC model and the sequential model.

The remaining question lies in how to find $\Esim$ efficiently. To this end, for each vertex $u$, we treat its neighborhood set $N[u]$ as a point in an $n$-dimensional space. Then, we apply the (discrete) random projection technique \cite{Achlioptas03} developed for the Johnson-Lindenstrauss transform \cite{JL84} to reduce the dimension to $O(\log n / \epsilon^2)$ while preserving the $\ell_2$ distance (up to a $(1 \pm \epsilon)$ factor) between the points. For 0/1 vectors, the $\ell_2$ distance is exactly the symmetric difference. Since the dimension is $O(\log n / \epsilon^2)$, it takes $O(\log n / \epsilon^2)$ time to compute the difference. To our knowledge, this is the first time that random projection techniques have been applied to computing efficient solutions for correlation clustering and problems alike. This may be of independent interest, as neighborhood similarity is known to be used in various tools such as {\it almost-clique decompositions} \cite{HSS18, 
CLP18, HKMT21, HKNT22, FGHKN24, ACK19, FHM23, AKM23, CLMNP21,AW22}.

\paragraph{Single-Pass Semi-Streaming} While the above techniques are sufficient for getting our  MPC and sequential algorithms, the single-pass semi-streaming algorithm introduces additional technical difficulties. The main difficulty for a single-pass semi-streaming to work here lies in Step 2, where an arbitrary chosen high-degree vertex in each cluster proposes to its neighbors who have similar neighborhoods. For convenience, we call those vertices {\it pivots} here. 

To be able to do this, we need to memorize the neighbors of all the chosen vertices using $n \polylog (n)$ space. Suppose that $\OPT \leq \phi$, it can be shown that each cluster in the optimal clustering containing at least one low-degree vertex has size of $\Theta(\phi)$. As a result, any vertex from these clusters has degree at most $O(\phi)$. Since we pick a pivot per cluster, $O(\phi \cdot (n/\phi))$ is the space we need to store the neighbors of the pivots. 

However, we do not know beforehand how the clusters of high-degree vertices look like, so the pivots cannot be chosen at the beginning of the stream. Without knowing what the pivots are beforehand, it is difficult to store their neighbors in the same pass. 

As a result, we sample each vertex (both high-degree and low-degree vertices) independently with probability $O(\log n/ d(v))$ so that w.h.p.~each cluster in the optimal clustering has $O(\log n)$ sampled vertices. Then we store the neighbors of all the sampled vertices. This poses another problem: Low-degree vertices may be chosen as pivots. However, a low-degree vertex is exempted from our structural result -- using it as a pivot may steal vertices from other clusters in an optimal clustering. 

To resolve this, we do the following. For each sampled vertex $y$, we first try to recover the high-degree portion $L$ of the cluster containing $y$ in the optimal solution. We construct a candidate set $\cand(L)$ that contains vertices that would not be added to other clusters. When using $y$ as a pivot, we restrict it to consider only the intersection with the candidate set, $N[y] \cap \cand(L)$ to ensure that it does not steal vertices from other clusters. 

Roughly speaking, the candidate set $\cand(L)$ contains all the low-degree vertices that have similar neighborhood with every vertex in $L$ but have different neighborhood with every vertex in any other cluster $L'$ (as a result, $\cand(L) \cap \cand(L')=\emptyset$). We show such a modification does not affect the approximation ratio. Furthermore, since the candidate sets are defined based on similarity of neighborhoods, they can be constructed by the aforementioned dimension reduction technique, which takes $O(n \log n /\epsilon^2)$ space. 

%This is done by modifying the condition for a low-degree vertex $w$ to join the cluster from only $|N[y] \Delta N[w]| \leq 2\phi$ to both $|N[y] \Delta N[w]| \leq 2\phi$ {\bf and} $|N[u] \Delta N[w]| \leq 2\phi$. We show even with the stricter condition, every vertex $w$ in the optimal solution will satisfies the condition and be added in our solution, so the original analysis would still work. 


%Since the clusters for high-degree vertex can only be formed at the end of the pass, 


