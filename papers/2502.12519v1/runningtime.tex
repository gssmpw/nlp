To make the algorithm efficient in the sequential and MPC settings, we need to address several challenges. First, we need to compute the graph $E'$ for high-degree nodes. Second, we need to be able to conduct approximate neighborhood similarity queries efficiently. We will address these issues one by one.

\subsection{Computing $\mathcal{L}$ for High-Degree Nodes}

In this section, we show that although $E'$ may be significantly larger than $E^{+}$, it is sufficient to make only $O(|E^{+}|) = O(m)$ neighborhood similarity queries to identify the high-degree clusters. \Cref{alg:highdegclustering} describes how to form a clustering $\mathcal{F}$ for high-degree vertices by conducting $O(m)$ similarity queries. 

\paragraph{Algorithm Description} In \Cref{alg:highdegclustering}, we first perform $O(m)$ neighborhood queries to construct $\Gsim = (V, \Esim)$, where $\Esim \subseteq E^{+}$ consists of those endpoints who have similar neighborhoods. Then we assign every vertex $u$ with a unique identifier, $\ID(u)$. By sending the ID of high-degree every vertex to its neighbors in $\Gsim$, every vertex $u$ can learn ID of the high-degree vertex with the smallest ID in its closed neighborhood in $\Gsim$ (Line \ref{ln:learnID}) and then set $\min(u)$ to be such an ID. Now, every vertex $u$ (including low-degree vertices) sends a token with value $\min(u)$ to all the neighbors of $u$ in $\Gsim$. Then, if a vertex $u$ recieved at least $\phi + 1$ tokens of the same value, it will set its cluster ID, $cluster(u)$, to be the minimum value that occurs at least $\phi + 1$ times. Finally, we assign all vertices with the same cluster ID will be in the same cluster. 

In \Cref{lem:efficienthighdeg}, we show that if $\OPT \leq \phi$, then the clustering $\mathcal{F}$ constructed is exactly the same as $\mathcal{L}$. The proof is based on showing that if two high-degree vertices $u,v$ are in the same cluster in $\mathcal{C}^{*}$, then there will be at least $\phi + 1$ disjoint paths of length two in $\Gsim$. 

%\begin{definition} A neighborhood dissimilarity query $\Delta(u,v,t)$ is a query on whether $|N(u) \Delta N(v)| \leq t$. \end{definition}



\begin{lemma}\label{lem:efficienthighdeg} Suppose that $\OPT \leq \phi$. \Cref{alg:highdegclustering} outputs a clustering $\mathcal{F}$ such that $\mathcal{F} = \mathcal{L}$. \end{lemma}

\begin{proof}
Let $\mathcal{C}^{*}$ be a clustering with $\obj(\mathcal{C}^{*})\leq \phi$.  Let $u$ be a vertex in $\Vhigh$. 
% (Case $\mathcal{F}_u \subseteq \mathcal{L}_u$:) 
Let $u_{*}$ be the vertex with the minimum ID in $\mathcal{L}_u$. We will show that 
\begin{enumerate}
    \item Every vertex $x \in \mathcal{L}_u \cap \Vhigh$ will receive at least $\phi + 1$ tokens of values $\ID(u_{*})$ in Line \ref{ln:receivemany}.
    \item In addition, if $x$ received at least $\phi + 1$ tokens of value $T$, then $T \geq \ID(u_{*})$. 
\end{enumerate} 

Once these two points are established, every vertex $x \in \mathcal{L}_u \cap \Vhigh$ will be assigned $\texttt{cluster}(x) = \ID(u_*)$. Consequently, this ensures that $\mathcal{F}_u = \mathcal{L}_u = \mathcal{C}^*_u \cap \Vhigh$.

\begin{algorithm}[ht!]
\caption{{\textsc{HighDegreeClustering}}$(G^+=(V, E^+), \phi, \eta)$\label{alg:highdegreecluster}\\
\textbf{Input}: A graph $G^{+}$, a parameter $\phi$, a parameter $0 \leq \eta < 1$. \\
\textbf{Output}: A clustering $\mathcal{F} = \{L_i\}_{i=1}^{|\mathcal{L}|}$ of $\Vhigh$ such that $\mathcal{F} = \mathcal{L}$. }
%, where $x_{uv}$ is the positive edge value and $z_e$ is the negative edge value. 
%\\
%\textbf{Auxiliary Information}: $E^-:={\binom{V}{2}}\setminus E^+$;. 
%$c = \exp(\epsilon)$, where $e$ is the Euler's number.
\label{alg:highdegclustering}
\begin{algorithmic}[1]
\Function {\textsc{HighDegreeClustering}}{$G^+=(V, E^+), \phi, \eta$} 
    \State Let $\Gsim = (V,\Esim)$, where $\Esim = \{uv \in E^{+} \mid |N[u] \Delta N[v] | \leq_{\eta} 2\phi \}$. \label{ln:2} 
    \State Assign each vertex $V$ with an unique ID, $\ID(v)$.

%    \State Every vertex $v$ sends $\ID(v)$ to $N_{\Gsim}(v)$.
    \State \label{ln:learnID}Every vertex $v \in V$ sets $\min(v) \leftarrow \min_{x \in N_{\Gsim}[v] \cap \Vhigh}\ \ID(x) $
    \State Every vertex $v \in V$ sends a token of value $\min(v)$ to the neighbor of $v$ in $\Gsim$.
    \State \label{ln:receivemany}If a vertex $v \in \Vhigh$ receives at least $\phi + 1$ tokens of the same value, set $cluster(v)$ to be the minimum such value. 
    
    \State Let $\mathcal{F}$ be a clustering where each cluster is formed by vertices with the same $cluster$ values. 
\EndFunction
\end{algorithmic}
\end{algorithm}

Now we start with the first point, since $x, u_{*} \in \mathcal{C}^{*}_u$, $|N[u_{*}] \Delta N[x]| \leq 2\phi$; for otherwise $\obj(\mathcal{C}^{*}) > \phi$ by \Cref{lem:samecluster}. This implies:
\begin{align*} 
|N[u_{*}] \cap N[x]| &=  (|N[x]| + |N[u_{*}]| - |N[u_{*}] \Delta N[x]|)/2 \\
&\geq (3\phi + 2  + 3\phi + 2 - 2\phi )/2 = 2\phi + 2
\end{align*}
%Note that $N_{\Gsim}[u_{*}] \subseteq N[u_{*}]$. Also, since by \Cref{lem:samecluster} all nodes in $N[u_{*}] \setminus N_{\Gsim}[u_{*}]$ cannot be in $\mathcal{C}^{*}_{u}$, we have:
Also, since $u_{*} \in \mathcal{C}^{*}_{u}$ and by definition of $\rho_{\mathcal{C}^{*}}(u_{*})$, we have: 
\begin{align}
\phi &\geq \rho_{\mathcal{C}^{*}}(u_{*}) \geq |N[u_{*}]| - |(\mathcal{C}^{*}_{u} \cap N[u_{*}])|\label{eqn:diff}
\end{align}
Therefore, 
\begin{align*}
|(\mathcal{C}^{*}_{u} \cap N[u_{*}]) \cap N[x]| &= |N[x] \cap N[u_{*}]| - |N[x] \cap (N[u_{*}] \setminus (\mathcal{C}^{*}_{u} \cap N[u_{*}]))| \\
&\geq |N[x] \cap N[u_{*}]| - |N[u_{*}] \setminus (\mathcal{C}^{*}_{u} \cap N[u_{*}])| \\
&\geq (2\phi + 2) - \phi && \mbox{by (\ref{eqn:diff})}\\
&\geq \phi + 2
\end{align*}
%\begin{align*}
%|N_{\Gsim}[u_{*}] \cap N[x]| &= |N[x] \cap N[u^{*}]| - |N[x] \cap (N[u_{*}] \setminus N_{\Gsim}[u_{*}])| \\
%&\geq |N[x] \cap N[u^{*}]| - |(N[u_{*}] \setminus N_{\Gsim}[u_{*}])| \\
%&\geq (2\phi + 2) - \phi && \mbox{by (\ref{eqn:diff})}\\
%&\geq \phi + 2
%\end{align*}
Hence:
$$|(\mathcal{C}^{*}_{u} \cap N[u_{*}]) \cap N(x)| \geq |(\mathcal{C}^{*}_{u} \cap N[u_{*}]) \cap N[x]| - 1 \geq \phi + 1$$
Note that all vertices in $(\mathcal{C}^{*}_{u} \cap N[u_{*}])$ must have their $\min(\cdot)$ values equal to $\ID(u_{*})$. This is because if $v \in (\mathcal{C}^{*}_{u} \cap N[u_{*}])$ and $v \in \Vhigh$, then $(N_{\Gsim}(v) \cap \Vhigh) \subseteq \mathcal{C}^{*}_{u}$.  If $v \in (\mathcal{C}^{*}_{u} \cap N[u_{*}])$ and $v \notin \Vhigh$, then it must be the case that $v \in N_{\Gsim}(u_{*})$. Moreover, $N_{\Gsim}[v] \cap \Vhigh$ cannot contain any cluster nodes outside $\mathcal{C}^{*}_{u}$ by \Cref{thm:nostealing}. Thus, $\min(v) = \ID(u_{*})$.  This implies $x$ will receive at least $\phi + 1$ tokens with value $\ID(u_{*})$.

Next we show that if there is a value $T$ such that $x$ receives from at least $\phi+1$ tokens, then $T \geq \ID(u_{*})$. Suppose to the contrary that $T < \ID(u_{*})$.  First note that if $v \in N(x)$ has $\min(v) = T$, then it cannot be the case that $v \in \Vhigh$. Otherwise, $u_{*}$, $v$, and a vertex whose ID is $T$,  would all be in $\mathcal{C}^{*}_{u}$, which contradict with the fact $\ID(u_{*})$ is the smallest ID in $\mathcal{L}_u = \mathcal{C}^{*}_{u} \cap \Vhigh$. Therefore, it must be the case that vertices $v \in N(x)$ with $\min(v) = T$ are all in $\Vlow$.  

Now note that if $v \in \mathcal{C}^{*}_u$ then by \Cref{thm:nostealing}, $v$ can only be connected to vertices in $\mathcal{C}^{*}_u$ in $\Gsim$. In this case, $\min(v) \geq \ID(u_{*})$ so $\min(v) \neq T$ by the assumption of $\ID(u_{*})$. So the only possibility for $v$ to have $\min(v) = T$ is when $v \notin \mathcal{C}^{*}_u$. However, as $\rho_{\mathcal{C}^{*}}(x) \leq \phi$, there are at most $\phi$ neighbors of $x$ not in $\mathcal{C}^{*}_{x}$. This implies $x$ will receive less than $\phi + 1$ tokens whose value equals to $T$. 


% (Case $\mathcal{L}_u \subseteq \mathcal{F}_u$:) Now we show that if $x\in \Vhigh$ and $x \notin \mathcal{L}_u$ then $x$ will receive less than $x+1$ tokens of value $\ID(u_{*})$. Similar to the reasoning of the second part of the previous case, if $v \in N(x)$ and $\min(v) = \ID(u_{*})$ then $v \notin \Vhigh$; otherwise, $x$, $u$, and $u_{*}$ would have been in the same component in $(\Vhigh, E')$, a contradiction with our assumption that $x \notin \mathcal{L}_u$. 

% Therefore, if $x$ received a token whose value equals to $\ID(u_{*})$, it must be from a low degree neighbor $v \in V_1$. However, if $v \in \mathcal{C}^{*}_{x}$, then by \Cref{thm:nostealing}, $v$ cannot be connected to vertices outside $\mathcal{C}^{*}_{x}$ in $\Gsim$, which implies $\min(v) \neq \ID(u_{*})$. Therefore, for $v$ to carry $\min(v) = \ID(u_{*})$, $v$ must be outside of $\mathcal{C}^{*}_{x}$. As $\rho_{\mathcal{C}^{*}}(x) \leq \phi$, there are at most $\phi$ neighbors of $x$ not in $\mathcal{C}^{*}_{x}$. This implies $x$ will receive less than $\phi + 1$ tokens whose value equals to $\ID(u_{*})$. 

%Note that all but $\phi$ neighbors of $x$ have to be in $\mathcal{C}^{*}_u$.


%First note that if $v \in N(x)$ has $\min(v) = T$, then it cannot be the case that $v \in \Vhigh$. Otherwise, $v$ and $u_{*}$ would be both in $$

%This would imply that $\mathcal{L}_u \subseteq \mathcal{F}_u$. 
\end{proof}

% \subsection{Efficient Computing $\Esim$}

% \begin{algorithm}[htbp]
% \caption{{\textsc{ConstructApproximateEsim}}$(G^+=(V, E^+), \phi)$\label{alg:main}\\
% \textbf{Input}: A graph $G^{+}$ and a parameter $\phi$. \\
% \textbf{Output}: A clustering $\mathcal{F} = \{L_i\}_{i=1}^{|\mathcal{L}|}$ of $\Vhigh$ such that $\mathcal{F} = \mathcal{L}$. }
% %, where $x_{uv}$ is the positive edge value and $z_e$ is the negative edge value. 
% %\\
% %\textbf{Auxiliary Information}: $E^-:={\binom{V}{2}}\setminus E^+$;. 
% %$c = \exp(\epsilon)$, where $e$ is the Euler's number.
% \label{alg:constructesim}
% \begin{algorithmic}[1]
% \Function {\textsc{ConstructApproximateEsim}}{$G^+=(V, E^+), \phi$}
%     \State $\Esim \leftarrow \{ \}$
%     \For{$i \in [\log \big(\phi \epsilon^2 \big ), \log n]$}
%     \State Sample each node from $V$ with probability $\min(\frac{\log n}{2^i}, 1)$, let the sampled set be $S(i)$
%     \EndFor
%     \For{$uv \in E^+$ such that $|d(u) - d(v)| \leq 2\phi$}
%         \State Let $j \leftarrow \lceil \log \max(d(u), d(v)) + \log \epsilon \rceil$
%         \If{$\max(d(u), d(v)) \leq \phi$}
%             \State $\Esim \leftarrow \Esim \cup \{ uv \}$
%         \ElsIf{$\max(d(u), d(v)) \leq 10\phi$ and $|S(u,  \log  \big(\phi \epsilon^2 \big )) \Delta S(v, \log  \big(\phi \epsilon^2 \big ))| \leq 2 (1 + \epsilon) \phi$}
%         \State $\Esim \leftarrow \Esim \cup \{ uv \}$
%         \ElsIf{$|S(u, j) \Delta S(v, j)| \leq 4\phi$}
%         \State $\Esim \leftarrow \Esim \cup \{ uv \}$
%         \EndIf
%     \EndFor
%     \Return $\Gsim = (V, \Esim)$
% \EndFunction
% \end{algorithmic}
% \end{algorithm}

% \begin{lemma}
% Suppose that $\OPT \leq \phi$. \Cref{alg:constructesim} outputs a clustering $\mathcal{F}$ such that $\mathcal{F} = \mathcal{L}$.
% \end{lemma}

%\subsection{Modifications Needed for Approximate Neighborhood Tests}



%\begin{definition} An $\eta$-approximate neighborhood similarity test returns: 
%$$\Delta_{\epsilon}(u,v,t) =   \begin{cases}0, &\mbox{if $|N(u) \Delta N(v)| > (1+\epsilon)t$.} \\ \mbox{$0$ or $1$}, &\mbox{if $t <  |N(u) \Delta N(v)| < (1+\epsilon)t$.} \\ 1, &\mbox{if $|N(u) \Delta N(v)| \leq t$.} %\end{cases}$$.
%\end{definition}

%For efficient implementations, we can only afford to conduct approximate neighborhood test. Here, we point out the necessary modifications in the algorithms and analysis for such tests. 

%First, low-degree nodes, $V_0$, are those with degree at most $(3+\epsilon)\phi$ and high-degree nodes, $\Vhigh$, are now those with degrees greater $(3+\epsilon)\phi$. 

%Then, in Line \ref{ln:R} of \Cref{alg:lp}, we replace $R_i(u)$ with $R_i(u) = \{ w \in V_i \cap N(u_i) \mid \Delta_{\epsilon}(u,w, 2\phi) = 1 \}$. 

%Also, in Line \ref{ln:2} of \Cref{alg:highdegclustering}, we construct $\Gsim = \{uv \in E^{+} \mid \Delta_{\epsilon}(u,v,2\phi) = 1\}$. 

%Note that the approximate similarity tests may return non-deterministic answers so the constructions above may differ every time we run the algorithm. Nevertheless, we show that it achieves the $(3+\epsilon)$ approximation with w.h.p. 


%Then, note that \Cref{thm:nostealing} can be tweaked for the approximate version as follows:
%\begin{corollary}Let $\mathcal{C}^\ast$ be a clustering with $\text{obj}(\mathcal{C}^{*}) \le \phi.$ Let $u$ and $v$ be vertices of degree greater than $(3+\epsilon) \phi$ where $\mathcal{C}^{*}_u \neq \mathcal{C}^{*}_v$, and $w$ be a vertex with $\text{deg}(w) \leq (3+\epsilon)\phi$. If $w \in \mathcal{C}^{*}_u$ then $\Delta_{\epsilon}(w,v,2\phi) = 0$ and $\Delta_{\epsilon}(w,u,2\phi) = 1$.  \end{corollary}


\subsection{Approximate Neighborhood Similarity Testing by Random Projection}
In this section, we show that for $0 < \eta < 1$, w.h.p.~the queries $\Delta_{\eta}(x,y,2\phi)$ all $xy \in E^{+}$ can be answered in $O(m\log n / \eta^2)$ time and thus $\Gsim$ can be constructed in $O(m\log n / \eta^2)$ time.

\begin{definition} Given a vertex $x$, let $\vec{N}[x] \subseteq \{0,1\}^{n}$ denote the characteristic vector of $N[x]$. \end{definition}
\begin{lemma} Given $\epsilon > 0$, let $k = C \cdot (\log n / \epsilon^2)$ for some large enough constant $C>0$. Let $A$ be a $k \times n$ matrix where each entry is drawn from $\{-1, +1\}$ uniformly at random. W.h.p.~for every two vertices $x$ and $y$, we have:
$$ (1-\epsilon) \cdot |N[x] \Delta N[y] | \leq ||A \cdot \vec{N}[x] - A \cdot \vec{N}[y]||^2_{2}/k \leq (1+\epsilon) \cdot  |N[x] \Delta N[y] |$$
\end{lemma}
\begin{proof} By the Johnson-{L}indenstrauss lemma \cite{Achlioptas03,JL84}, w.h.p.~for every $x$ and $y$, we have: 
$$(1-\epsilon) \cdot ||\vec{N}[x] - \vec{N}[y]||^2_{2} \leq ||A \cdot \vec{N}[x] - A \cdot \vec{N}[y]||^2_{2}/k \leq (1+\epsilon) \cdot ||\vec{N}[x] - \vec{N}[y]||^2_{2}$$

The lemma follows by observing that $|N[x] \Delta N[y]| = ||\vec{N}[x] - \vec{N}[y]||^2_{2}$. 
\end{proof}

\begin{lemma}\label{lem:test}
Let $\epsilon = \Theta(\eta)$ be such that $(1+\eta) = (1+\epsilon)/(1-\epsilon)$, $k = O(\log n /\epsilon^2)$,  and set:
$$\Delta_{\eta}(x,y,t) = \begin{cases} 0 & \mbox{if $||A \cdot \vec{N}[x] - A \cdot \vec{N}[y]||^2_{2}/((1+\epsilon)k) > t$} \\
1 & \mbox{otherwise} \end{cases}$$
W.h.p.~the above implementation returns a correct answer for $\Delta_{\eta}(x,y,t)$.
\end{lemma}
\begin{proof}
It suffices to show that if $|N[x] \Delta N[y]| \leq t$ then w.h.p.~we will set $\Delta_{\eta}(x,y,t)$ to be $0$ and if $|N[x] \Delta N[y]| > (1+\eta)t$, then w.h.p.~we will set $\Delta_{\eta}(x,y,t)$ to be 1.

If $|N[x] \Delta N[y]| \leq t$, then w.h.p. $$||A \cdot \vec{N}[x] - A \cdot \vec{N}[y]||^2_{2}/((1+\epsilon)k) \leq |N[x] \Delta N[y]| \leq t$$
Thus, $\Delta_{\eta}(x,y,t)$ returns 1. 

On the other hand, if $|N[x] \Delta N[y]| > (1+\eta)t$ then w.h.p.
$$||A \cdot \vec{N}[x] - A \cdot \vec{N}[y]||^2_{2}/((1+\epsilon)k) \geq \frac{1-\epsilon}{1+\epsilon}\cdot (|N[x] \Delta N[y]|) > \frac{1-\epsilon}{1+\epsilon}(1+\eta) \cdot t = t$$
Thus, $\Delta_{\eta}(x,y,t)$ returns 0. 
\end{proof}

%To answer the query of whether $|N[x] \Delta N[y]| \leq_{\eta} 2\phi$ holds, 

We have presented the key ingredients that lead to efficient sequential and MPC algorithms. The details of their implementations are presented in \Cref{sec:efficientsequential} and \Cref{sec:efficientmpc}. For the streaming algorithm, different challenges arise. The necessary modifications and details are presented in \Cref{sec:streaming}. 

\subsection{Efficient Sequential Implementation}\label{sec:efficientsequential}
\subfile{efficientsequential}

\subsection{Efficient MPC Implementation}\label{sec:efficientmpc}
\subfile{efficientmpc}