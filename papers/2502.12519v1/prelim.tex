\begin{definition}
Given $u \in G^{+} = (V,E^{+})$, let $N(u)$ denote the neighbors of $u \in G^{+}$. Define $N[u] = N(u) \cup \{u\}$. $d(u) = |N(u)|$ is the number of neighbors of $u \in G^+$. We use $N_{H}(u)$ ,$N_{H}[u]$ and $d_{H}(u)$ to denote the the corresponding quantities in graph $H$ when $H \neq G^{+}$.
\end{definition}

\begin{definition}
Let $A$ and $B$ be sets. The symmetry difference between $A$ and $B$, $A \Delta B$, is defined as $A \Delta B = (A \setminus B) \cup (B \setminus A)$.
\end{definition}
In general, we say $A$ and $B$ are similar if $|A \Delta B|$ is small.
\begin{lemma}[Triangle Inequality~\cite{halmos1960naive}]
\label{lem:triangleineq}
Let $A, B, C$ be sets. Then: $|A \Delta C| \le |A \Delta B| + |B \Delta C|$.
\end{lemma}
% \begin{proof}
% \begin{align*}
%  |A \Delta B| + |B \Delta C| &= |A \setminus B| + |B \setminus A| + |B \setminus C| + |C \setminus B| \\
%  &= (|A \setminus B|+ |B \setminus C|) + (|C \setminus B| + |B \setminus A|) \\
%  &\geq |A \setminus C| + |C\setminus A| = |A \Delta C|   \qquad \qedhere
% \end{align*}
% \end{proof}


\begin{definition}
Given any clustering $\mathcal{C}$ and any vertex $u$, $\mathcal{C}_u$ is defined to be the cluster of $\mathcal{C}$ containing $u$. 
\end{definition}

\begin{definition}
Given clustering $\mathcal{C}$, define $\rho_{\mathcal{C}}(x)$ as the number of disagreements incident to vertex $x$. More precisely:
\begin{align*}
    \rho_{\mathcal{C}}(x) = |\mathcal{C}_x \setminus N[x]| + |N[x] \setminus \mathcal{C}_x| = |N[x] \Delta \mathcal{C}_x| = |N(x)~\Delta~\mathcal{C}_x| - 1
\end{align*}
\end{definition}


\begin{definition}
Given a clustering $\mathcal{C}$, the objective function of $\mathcal{C}$, $\obj(\mathcal{C}) = \max_{u} \rho_{\mathcal{C}}(u)$, is defined as the maximum incident disagreements over every vertex. 
\end{definition}



% Consider $(A \Delta B)\Delta(B \Delta C)$. Due to the associativity of the symmetric difference, we attain:
% \begin{align*}
%     (A \Delta B)\Delta(B \Delta C) &= A \Delta (B\Delta B) \Delta C \\
%     &= A \Delta C
% \end{align*}
%Observe that:
%\begin{align*}
%    &(A \Delta B)\Delta(B \Delta C) \subseteq  (A \Delta B) \cup (B \Delta C) \\
%    &\implies |(A \Delta B)\Delta(B \Delta C)| \le |(A \Delta B) \cup (B \Delta C)|
%\end{align*}
%Thus, the proof is finished, for:
%\begin{align*}
%    |A \Delta C| \le |(A \Delta B) \cup (B \Delta C)| = |(A \Delta B)| + |(B \Delta C)|
%\end{align*}
%Let $C^*$ be the optimal solution. For any node $u$, let $C^*(u)$ be the cluster in the optimal solution containing node $u$. \\
%We also define, for the number of disagreements of vertex $x$ with respect to vertices in only $C$:
%\begin{align*}
%    \rho_{[C]}(x) = |C \backslash N^{+}(x)|
%\end{align*}
% \subsection{Computational Models.}
% % \textbf{Computational Models.} 
% We consider three computational models in our papers: the sublinear time model, the streaming model and the MPC model.
% \paragraph{Streaming model.} In this model, %the algorithm knows that 
% the input graph is presented as a stream of edges.\todo{of \pedges ?  We should clarify.} The algorithm processes each edge as it arrives and has limited memory, meaning it cannot store the entire graph. The running time is typically measured by the number of passes over the stream and the space used by the algorithm. 
\paragraph{The MPC Model}
In the MPC model, computation proceeds in synchronous parallel \emph{rounds} across multiple machines. Each machine has memory $S$. At the beginning of the computation, data is arbitrarily partitioned across the machines. During each round, machines process data locally, exchange messages with other machines, and send or receive messages of total size $S$. The efficiency of an algorithm in this model is measured by the number of rounds required for the algorithm to terminate and the size $S$ of the memory available to each machine.

In this paper, we focus on the most practical and challenging regime, also known as the \emph{strictly sublinear regime}, where each machine has $S = O(n^\delta)$ local memory. Here, $n$ represents the input size, and $\delta$ is an arbitrary constant smaller than 1. Under this assumption, the input assigned to each machine and the messages exchanged during any round are of size $O(n^\delta)$.


\paragraph{The semi-streaming model.} In the semi-streaming model, the input is a stream of edges in \( E^{+} \). We are allowed to use \( n \polylog n \) space, where the space complexity refers to the number of words used. A solution is expected to be output at the end of the stream.