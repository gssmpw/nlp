%\subsection{Efficient MPC Implementation}
In this section, we aim to present the MPC component of Theorem~\ref{thm:thmmain}. More precisely, we aim to show the following lemma,

\begin{lemma}[Item \ref{itm:main2} of \Cref{thm:thmmain}]
\label{lemma:mainresultmpc}
Given a min-max correlation clustering instance $G = (V, E)$, for any constant $\delta > 0, \epsilon$, there exists a randomized MPC algorithm that, in $O(\log(1/\epsilon))$ rounds, outputs a clustering that is a $(3 + \epsilon)$-approximation for min-max correlation clustering. This algorithm succeeds with high probability and uses $O(n^{\delta})$ memory per machine and a total memory of $O(m \log n / \epsilon^2)$.
\end{lemma}

Algorithm~\ref{alg:lp} consists of two parts: constructing clusters $\mathcal{F}$ for $\Vhigh$ (handled by Algorithm~\ref{alg:highdegclustering}) and processing each cluster in $\mathcal{F}$ to manage the low-degree nodes. For Algorithm~\ref{alg:highdegclustering}, we first sample each node with a vector $A_v$, append $A_u$ and $A_v$ to each edge $uv$, and sort all edges by their endpoints. This allows us to compute $A \cdot \vec{N}[x]$ in $O(1 /\delta)$ rounds, as sorting in the MPC model takes $O(1/\delta)$ rounds. In Lines 4--5, each node propagates its information to its neighbors, which can also be done using sorting in $O(1/\delta)$ rounds. Lines 6--7 involve assigning labels, which similarly requires $O(1/\delta)$ rounds.

Once $\mathcal{L} = \mathcal{F}$ is computed, we process all low-degree nodes in Algorithm~\ref{alg:lp}. For each $L_i \in \mathcal{L}$, we randomly choose a node $u_i$. For each edge $wu_i$, we use the $A \cdot \vec{N}[x]$ value to check whether $w \in R(u_i)$. Based on Theorem~\ref{thm:nostealing}, there will be exactly one $u_i$ that adds $w$ to its cluster. Propagating information from $u_i$ to $w$ can be done using sorting, taking $O(1/\delta)$ rounds. 

At the end of Algorithm~\ref{alg:lp}, we compute $\obj(\mathcal{C})$ by appending the cluster information to each node and edge. In summary, Algorithm~\ref{alg:lp} completes in $O(1/\delta)$ rounds and uses $O(m \log n)$ total memory.


A remaining question is how Algorithm~\ref{alg:lp} determines $\phi$, which is provided as input. We can use binary search to estimate $\phi$ close to $\OPT$. While binary search could take $O(\log n)$ rounds with a poor initial guess, Cao, Li, and Ye~\cite{cao2024simultaneously} provided an $O(1)$-round MPC algorithm achieving an $O(1)$-approximation for min-max correlation clustering. Their main result is as follows:

\begin{theorem}[Restatement of Theorem~1.5~in~\cite{cao2024simultaneously}]
There exists an MPC algorithm in the strictly sublinear regime that, given a correlation clustering instance $G = (V, E)$, outputs a clustering that is a $360$-approximation for min-max correlation clustering in $O(1)$ rounds. This algorithm succeeds with high probability and uses a total memory of $O(m \log n)$.
\end{theorem}

We use their algorithm to compute an initial solution $\phi_{s} \leq 360 \OPT$, then maintain an upper bound $\phi_{u} = \phi_{s}$ and a lower bound $\phi_{l} = \phi_{s} / 360 \geq \OPT$ for binary search. The binary search terminates whenever $\phi_{u} - \phi_{l} = O(\epsilon) \cdot \phi_{s} / 360 = O(\epsilon)\cdot OPT $. Also, we will invoke \Cref{alg:lp} with $\eta = O(\epsilon)$ for an appropriate chosen constant inside $O(\cdot)$. The final approximation ratio is therefore $(3+O(\epsilon))(1+O(\epsilon)) = 3+\epsilon$, for appropriately chosen constants inside the $O(\cdot)$ notations.

Note that in the binary search, $|\phi_{u} - \phi_{l}|$ decreases by half in each iteration. Since it starts at $O(\phi_s)$ and ends at $\Omega(\epsilon \phi_s)$, the binary search takes $O(\log(1/\epsilon))$ rounds. Combining these steps establishes Lemma~\ref{lemma:mainresultmpc}.


