\section{Methodology: Measuring LM Similarity}
\label{sec:method}
We begin by describing how we quantify model similarity.

\subsection{Background}

\textbf{Functional similarity}: Prior work on model similarity has focused on two classes of similarity measures: representational and functional similarity (see~\citet{klabunde2024similarityneuralnetworkmodels} for a recent survey). \textit{Representation similarity} metrics focus on the weights and activations of the networks~\citep{kornblith2019similarity}, comparing how features are represented internally. In contrast, \textit{functional similarity} metrics focus on the inputâ€“output behavior of the model. 
Functional similarity metrics are more broadly applicable than representation metrics as (1) they allow comparisons across model families and architectures and (2) are applicable for models behind an API (where weights are not released). Functional similarity metrics are more interpretable because they operate on data samples rather than noisy, complex internal representations~\citep{golechha2024challenges}. Despite large architectural differences across models and model families, their outputs can still be fairly similar. Moreover,~\citet{geirhos2020beyond} argue models with similar internal mechanisms make similar mistakes, and thus mistakes can proxy whether models use similar internal mechanisms. Therefore, in the present work we focus on functional similarity metrics.

\textbf{Error Consistency}: A popular similarity metric designed in the context of comparing mistakes of image-classifiers to humans is error consistency~\citep{geirhos2020beyond}. It quantifies the overlap on samples where two models make mistakes while normalizing for chance overlap due to accuracy. First, they define $\cobs$ as the ``observed error overlap'' i.e., the fraction of samples on which both models are correct or both models are wrong. This itself is used a metric in recent work on LM similarity,~\citet{dutta2024accuracy}. However, as~\citet{geirhos2020beyond} point out, $\cobs$ has a crucial shortcoming: two independent models with high accuracy will have a higher $\cobs$ by chance than two models with low accuracy (\circled{1}). An independent model here is one that is correct on a uniform random subset (size corresponding to accuracy) of samples, and wrong on the others. For instance, two independent models with 90\% accuracy will agree on at least 81\% of the samples by chance, whereas for two models with 50\% accuracy, the lower-bound on chance agreement drops to 25\%. Consequently, to account for this,~\citet{geirhos2020beyond} calculate the ``expected error overlap'' ($\cexp$) as $\cexp = \acc_1 \cdot \acc_2 + (1 - \acc_1)(1 - \acc_2)$ where $\acc_i$ is the accuracy of model $i$. Similar to Cohen's $\kappa$~\citep{cohen1960coefficient}, error consistency (Eq.~\ref{eq:error_consistecy}) is then defined as the fraction of excess agreement observed ($\cobs - \cexp$) from what is possible beyond chance ($1 - \cexp$):
\begin{align} k = \frac{\cobs - \cexp}{1 - \cexp}, \label{eq:error_consistecy} 
\end{align} 

\begin{table*}[htbp]
  \centering
  \caption{%
  \textbf{Comparison of Functional Model Similarity Metrics}. Only our metric, CAPA, satisfies all three desiderata:\\ 
    \circled{1}\ \textit{Adjusts for accuracy} -- The metric should not inflate scores for high accuracy model pairs due to lesser scope to disagree. \newline
    \circled{2}\ \textit{Distinguishes different mistakes} -- The metric should consider different wrong predictions as a disagreement. \newline
    \circled{3}\ \textit{Incorporates probabilities} -- The metric should use the probability distribution over predictions provided by the models.
  }
  %\vspace{-0.3cm}
  \label{tab:metric_comparison}
  \resizebox{0.8\linewidth}{!}{
  \begin{tabular}{lccc}
    \toprule
    \textbf{Metric} & \textbf{Adjusts for} & \textbf{Distinguishes}  & \textbf{Incorporates} \\
    & \textbf{Accuracy} & \textbf{different mistakes}  & \textbf{Probabilities} \\
    \midrule
    \%Flips $= 1 - \cobs$~\citep{dutta2024accuracy} & \xmark & \xmark & \xmark \\
    Cohen's $\kappa$, Scott's $\pi$, Fleiss $\kappa$ & \xmark & \cmark & \xmark \\
    \%Agreement~\citep{zheng2023judging} & \xmark & \cmark & \xmark \\ 
    Error Consistency~\citep{geirhos2020beyond} & \cmark & \xmark & \xmark \\
    Pearson / Matthew's Correlation of Errors & \cmark & \xmark & \xmark \\
    Divergence metrics like KL, JSD & \xmark & \cmark & \cmark \\
    \midrule
    CAPA (Ours) & \cmark & \cmark & \cmark \\
    \bottomrule
  \end{tabular}}
\end{table*}

\subsection{Our Contribution}
\label{sec:metric_ours}

We identify two key limitations of error consistency ($k)$:

\textbf{Does not distinguish differing mistakes (\circled{2})}: If two models make wrong but different predictions, error consistency still counts that as an agreement. For example, two models that are always wrong, even in different ways, have perfect error consistency ($k=1$). It thus overestimates similarity.

\textbf{Does not capture probability information (\circled{3})}: For comparison to humans, error consistency assumes a single top prediction, whereas models inherently output a probability distribution. Ignoring these probabilities can lead to incorrect conclusions about model similarity. Consider two models whose outputs are \([0.49, 0.51]\) and \([0.51, 0.49]\). Despite their small differences, binary labels would classify them as making entirely different predictions (0 and 1). Conversely, models with predictions \([0.99, 0.01]\) and \([0.51, 0.49]\) may share the same binary output (0 and 0) but differ significantly in confidence distribution.

\textbf{Novel Metric.} We redefine $\cobs$ and $\cexp$ to address the above limitations. For clarity we adjust the notation of our agreement metrics to $\cobsp$ and $\cexpp$.
To compute $\cobsp$ we directly use the model output probabilities (Eq.\ref{eq:cobsp}), thus accounting for disagreement on incorrect options and better capturing model similarity. This approach lets us calculate $\cobsp$ without sample-wise ground-truth annotations. For $\cexpp$, we take into account that the model output predictions can span over multiple options rather than only looking at sample-wise accuracy. 

\textbf{Definition.} We define $\goelpi$ in the context of Multiple Choice Questions (MCQs), which is the format of many popular benchmarks for LMs. We provide a detailed derivation in Appendix~\ref{app:goelpi_derivation}, with extensions to classification and exact match evaluations in Appendix~\ref{app:goelspi_classification}.

% \begin{enumerate}
    \textbf{Observed Agreement} ($\cobsp$): It represents the probability of agreement if the model's predictions were sampled based on the \textit{observed} likelihoods assigned over options. Formally, \vspace{-0.2cm}
    \begin{equation}
    \cobsp \;=\; \frac{1}{|D|} \sum_{x \in D} \sum_{o_i \in O(x)} p_1(o_i) \cdot p_2(o_i),
    \label{eq:cobsp}
    \end{equation}
    where $p_1(o_i)$ and $p_2(o_i)$  are the output probabilities for model 1 and 2, respectively, on a data sample $x$ for option $o_i$. $O(x)$ are the possible options: $O(x) = [o_i , \dots, o_n]$, and $|D|$ is the total number of data points.
    
    \textbf{Chance Agreement} ($\cexpp$): To account for higher accuracies inflating $\cobsp$, we normalize by the agreement expected from two \textit{independent} models. First, we define $\overline{p_j}$ as the average probability model $j$ assigns to the correct option across all samples. For a perfectly calibrated model $\overline{p_j}$ approaches accuracy, thus aligning with the motivations in error consistency. Then, we define independent models as assigning $\overline{p_j}$ probability to the correct option, and uniformly distributing the remaining $1 - \overline{p_j}$ probability over the incorrect options. The latter is necessary, as there is no coherent concept of ``classes'' for MCQ data, i.e. the options can be permuted. This prevents us from computing class marginals for the remaining options, such as in inter-annotator agreement metrics like Cohen's Kappa, Scott's Pi~\citep{scott1955reliability}, Fleiss' Kappa~\citep{fleiss1981measurement}. 
    Formally, 
\begin{equation}
\begin{split}
    c_{exp}^{p} &= \underbrace{\overline{p_1} \cdot \overline{p_2}}_{\text{chance agreement on correct option}} + \\ 
    &\quad \underbrace{(1 - \overline{p_1}) \cdot (1 - \overline{p_2}) \cdot \frac{1}{|D|} \sum_{x \in D} \frac{1}{|O(x)|-1}}_{\text{chance agreement on incorrect option}}
\end{split}
\end{equation}
    where $|O(x)|$ is the number of options in question $x$.
% \end{enumerate}

Finally, the equation for CAPA is:
\begin{equation}
\goelpi = \frac{\cobsp - \cexpp}{1 - \cexpp}
\end{equation} 

\textbf{Interpretation.} We prove $\goelpi$ is bounded between $-1$ and $1$ in Appendix~\ref{app:goelpi_bounds}. A value of 0 means the models have the same agreement as independent models given their accuracies. A negative value means the models disagree, and a positive value indicates they agree beyond independent models with their accuracy. \textit{As $\goelpi$ increases, it means models make more similar mistakes, their errors become more correlated, and they are functionally less different}. We use these interpretations interchangably. 

\textbf{Alternatives and Justification.} We summarize comparisons to existing functional similarity measures based on key desiderata (\circled{1}-\circled{3}) in Table~\ref{tab:metric_comparison}. In Appendix~\ref{sec:app_metric_alternatives} we justify design choices for CAPA, comparing it with various alternatives like using Jensen Shannon Distance (JSD), or defining $\cexpp$ using assumptions similar to Scott's $\pi$ instead of Cohen's $\kappa$. We provide plots with alternative similarity metrics for our main empirical takeaways throughout the Appendix, and find consistent trends. In each case, $\goelpi$ shows the trend most clearly, with the least noise. CAPA can also be used when probabilities are unavailable by assigning probability $1$ to the predicted option and $0$ to the others. We use this to prove $\goelpi$ is a strict generalization of error consistency, and reduces to it for binary classification (Appendix~\ref{sec:kappareduction}). Furthermore, $\goelpi$ can be extended beyond pairwise comparisons to multiple models, (Appendix~\ref{app:metric_multi}). For completeness, we present probabilistic extensions for Cohenâ€™s $\kappa$, Scott's $\pi$, Fleiss' $\kappa_F$ in Appendix~\ref{sec:probabilistic_metrics} and show comparisons to CAPA on illustrative examples and synthetic data in Appendix~\ref{sec:metric_comparison}. 

