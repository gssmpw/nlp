\section{Learning from Complementary Knowledge of LM Annotators}
\label{sec:w2s}
We now study the role of similarity in AI supervising training. This can allow scaling up training data by reducing reliance on costly and time-intensive expert human inputs. There is hope that models can learn from other models to improve further even on tasks where they surpass human capabilities~\citep{hughes2024openendednessessentialartificialsuperhuman}. Where could this improvement come from? We hypothesize that the existence of complementary knowledge or complementary capabilities between two LMs can be one mechanism for driving improvements from LM annotations, if effectively leveraged. This complement can exhibit itself in the form of differing predictions on training data, and can thus be quantified using functional similarity between the supervisor and student model. Lower $\goelpi$ is indicative of more complementary knowledge, and as an implication of our hypothesis, should inversely correlate with the performance gain of a student model when trained on the supervisor's annotations.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/kappavsgain.pdf}
    \caption{\textbf{Similarity vs Gain from Weak-to-Strong Training.} Across 12 model pairs, the strong student gains more from weak-to-strong training on tasks where it is more different from the weak supervisor ($p < 0.01$).}
    \label{fig:kappavsgain}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/new_conf_mat_accs_test.pdf}
    \caption{\textbf{Role of Complementary Knowledge and Elicitation in Weak-to-Strong Generalization}. We decompose the accuracy of the weak-to-strong trained model on four parts of the test data distribution, based on the correctness of the weak supervisor and an oracle strong elicited model which uses ground-truth annotations. Sub-rectangles represent weak, strong model pairs. Results are averaged across 15 tasks. Complementary knowledge transfer explains weak-to-strong model accuracy beyond elicitation.}
    \label{fig:conftest}
    \vspace{-0.3cm}
\end{figure}

\begin{table}[ht]
    \centering
    \caption{\textbf{Accuracy gains possible from weak-to-strong training.} We average accuracies across 15 datasets and 12 model pairs (180 training runs) and report gaps to the student model's initial accuracy. Complementary knowledge transfer can enable higher gains than the previously considered ceiling estimate from elicitation.}
    \label{tab:w2s_accuracies}
     %\resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}lc@{}}
        \toprule
        \textbf{Model} & \textbf{Accuracy Gap} \\
        \midrule
        Initial Strong Student & $75.1\%$ \\
        Weak Supervisor & \textcolor{ForestGreen}{$+4.1$} \\
        Weak to Strong Trained Student & \textcolor{ForestGreen}{$+7.4$} \\
        \midrule
        \textbf{Ceiling Estimate} & \\
        \midrule
        Ground-truth Elicitation (previous) & \textcolor{ForestGreen}{$+11.2$} \\
        Elicitation $\cup$ Complementary (ours) & \textcolor{ForestGreen}{$+14.1$} \\
        \bottomrule
    \end{tabular}
    % \vspace{-0.4cm}
   % }
\end{table}

\subsection{Experimental Setup}

\citet{burns2024weaktostrong} study training a larger student model on annotations from a small task-finetuned ``expert'' teacher. They find the student can outperform the supervisor, a phenomenon they call ``weak to strong generalization''. We study this setup as it can seem counter-intuitive when viewed from the lens of accuracies. How can training a 60\% accuracy student on a 70\% accuracy task-finetuned teacher lead to 75\% accuracy? We adopt a lens of complementary knowledge to understand weak-to-strong generalization. 

 We measure similarity between the weak supervisor and base student model on the validation set. We then perform weak-to-strong training on the student model, using the confidence-weighted finetuning objective proposed in~\citet{burns2024weaktostrong}. We investigate if similarity is an apriori predictor of performance gained on the test set. For our experiments, we study 4 weak models in the $1-3$B parameter range, and 3 strong models in the $7-9$B parameter range, for a total of 12 model pairs, and $15$ of the NLP tasks studied in~\citet{burns2024weaktostrong}, specified in Table~\ref{tab:weak_strong_datasets}. The full setup is consistent with the open-weight model reproduction by~\citep{scherlis2024w2seleuther}, and is described in Appendix~\ref{sec:w2ssetup}. 



\subsection{Results \& Discussion}


\textbf{Q1: Does Complementary Knowledge Influence Performance Gain?} Figure~\ref{fig:kappavsgain} shows that for all model combinations, similarity between the weak supervisor and initial strong student inversely correlates with the improvement obtained from weak-to-strong training ($r=-0.85)$. Even after using partial correlation analysis to control for the accuracy gap between the weak supervisor and strong student, similarity is inversely correlated with weak-to-strong gain ($r=-0.35, p < 0.01$). Thus, tasks where the supervisor and student make less correlated errors tend to yield greater improvements. This contributes towards understanding why gains from weak to strong training vary across tasks, an open question posed by \citet{burns2024weaktostrong}.

\textbf{Q2. Does Complementary Knowledge Add Beyond Elicitation?} The original explanation for performance gains from weak-to-strong generalization is that the weak supervisor ``elicits'' the latent knowledge in the superior representations of the stronger student \citep{burns2024weaktostrong}. To investigate whether complementary knowledge adds to this explanation or is subsumed within it, we first obtain the strong model with ``upper-bound'' elicitation by finetuning it on ground-truth annotations. We refer to this as the \textit{strong elicited} model. We can then separate the test data into four parts based on whether the strong elicited and weak supervisor model were correct or wrong, measuring average accuracy of the weak-to-strong model on each part to disentangle gains from different factors. The experiment setup is discussed further in Appendix~\ref{app:elicitation_complementary}.

Figure~\ref{fig:conftest} reports aggregate values across 15 tasks for 12 model pairs. Accuracy on the bottom-left quadrant (avg. 71.9\%) can only be due to successful elicitation, as here the weak supervisor was wrong. Accuracy on the top-right quadrant (avg. 42.2\%) can only be due to complementary knowledge transfer as here the upper-bound elicitation model was wrong. This confirms that elicitation plays an important role in weak-to-strong generalization, with complementary knowledge transfer from the weak supervisor also contributing to significant gains.

\textbf{Q3. Where can weak-to-strong training improve?} The strong elicited model is considered to represent upper-bound performance, but as shown in Table~\ref{tab:w2s_accuracies}, the actual ceiling is significantly higher if complementary knowledge of the weak supervisor is fully leveraged. Interestingly, on the training set, the weak-to-strong trained model shows similar accuracy on the top-left and bottom-right quadrants as shown in Figure~\ref{fig:conftrain}. Yet, when generalizing to unseen samples, it falls back more often to its initial priors. We hope this analysis guides future work on improving weak-to-strong training methodology, by highlighting leveraging complementary knowledge as a concrete avenue for improvement.
