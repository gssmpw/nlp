
\section{Related Work}

There is increasing interest in finding differences between models for applications like visual tools for comparative analytics~\citep{strobelt-etal-2021-lmdiff, kahng2024llmcomparatorvisualanalytics}, efficient human evaluation~\citep{boubdir2023promptsmakedifferencedata}, comparing learning algorithms~\citep{shah2023modeldiff}, identifying side-effects of API updates~\citep{eyuboglu2024changelist} or quantization~\citep{dutta2024accuracy}. Prior work has also looked at qualitatively describing differences between data distributions~\citep{zhong2022describingdifferencestextdistributions, zhong2023goal, dunlap2024describingdifferencesimagesets, dunlap2024vibecheckdiscoverquantifyqualitative}. Our work proposes metrics to quantify LM differences (or similarity).~\citet{huh2024platonic} used representation similarity metrics~\citep{kornblith2019similarity, bansal2021revisiting} to show convergence in visual representations and their alignment with language representations. In contrast, we show model mistakes are becoming more correlated as capabilities improve. We measure differences in input-output behaviour, which leverages sample level evaluations~\citep{burnell2023reporting} such as those available on OpenLLMLeaderboard~\citep{myrzakhan2024openllmleaderboard} and HELM~\citep{bommasani2023holistic}.~\citet{geirhos2020beyond} proposed measuring ``error consistency'' between image classifiers and humans, with~\citet{geirhos2021partial} showing an early trend of data-rich models making more similar mistakes to humans. We enrich this metric, distinguishing between different mistakes and incorporating probabilistic information.

Our results on AI judges fall in a broader line highlighting their pitfalls~\citep{zheng2024cheating}. These include biases such as favoring verbose texts or options at certain positions~\citep{koo-etal-2024-benchmarking, ye2024justiceprejudicequantifyingbiases}. Interestingly, these biases are also sometimes found in human annotators~\citep{chen-etal-2024-humans}. In fact, there is rich literature documenting biases in human judgements of other humans. One such bias is affinity bias, where recruiters prefer candidates with similar knowledge and skills as them~\citep{bagues2012recruiters}. We show LM judges also systematically favor other models that make similar mistakes, generalizing previous results that showed LMs favor their own outputs~\citep{liu-etal-2024-llms-narcissistic, panickssery2024llm}. Overall, we believe AI evaluators should be accompanied with formal checks like consistency~\citep{fluri2024evaluating}.

A second aspect of AI oversight is using another model's supervision to train better models. This is similar to training on text generated by an LM~\citep{vicuna2023} with ongoing debates about its benefits~\citep{kazdan2024collapsethriveperilspromises}, and an emerging paradigm of exploiting a gap in difficulty between solution generation and evaluation~\citep{song2024mindgapexaminingselfimprovement}. In this paper, we study the more established setup of training LMs on LM annotations, where~\citet{burns2024weaktostrong} demonstrated the phenomenon of weak to strong generalization, and it has been leveraged for other applications like image classification~\citep{guo2024visionsuperalignmentweaktostronggeneralization} and aligning models~\citep{zhu2024weaktostrongpreferenceoptimizationstealing}. Prior work has attempted to understand weak to strong generalization, notably using ``misfit error''~\citep{charikar2024quantifyinggainweaktostronggeneralization}, which shows that the student's disagreement with the weak supervisor \textit{after} weak to strong training correlates with its accuracy gap from the weak supervisor. Instead, we show similarity between the weak supervisor and strong student can \textit{apriori} predict gains from weak-to-strong training. The benefit of model diversity has previously been discussed in related settings like knowledge distillation for image classifiers~\citep{roth2024fantastic} and training chess models that outperform the humans they are trained on~\citep{zhang2024transcendence}.