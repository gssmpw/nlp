\section{Affinity Bias in LLM-as-a-Judge}
\label{sec:AI_Judges}

Evaluating free-response model generations automatically at scale is tricky~\citep{biderman2024lessonstrenchesreproducibleevaluation}. This has led to popular leaderboards like Arena-hard-auto~\citep{li2024crowdsourceddatahighqualitybenchmarks}, AlpacaEval 2.0~\citep{dubois2024length}, and AidanBench~\citep{mclaughlin2025aidanbench} adopting LLM-as-a-Judge for scoring. Recent work cautions that language models are biased towards their own outputs~\citep{liu-etal-2024-llms-narcissistic, panickssery2024llm}, and these leaderboards assume that excluding the judge model from the rankings circumvents this problem. However, it has been shown that human interviewers are biased towards candidates with similar knowledge and traits, a phenomenon called \textit{affinity bias}~\citep{bagues2012recruiters}. We study whether LMs also exhibit a bias toward similar models. This would indicate that it is not sufficient to just use a held-out LM as the judge; one must account for the confounder of similarity.

\subsection{Experimental Setup}

To study whether LM judges prefer more similar models, we evaluate a large set of judges and models on MMLU-Pro~\citep{wang2024mmlupro}, a benchmark for hard problem solving questions across 14 domains. We filter 8,707 questions that can also be answered in a free-text response style, without options, following~\citet{myrzakhan2024openllmleaderboard}. Each question is posed to every evaluated model as an MCQ and as an open-style question. The per-sample results of the former are used to compute the similarities of judge-model pairs, whereas responses to the latter are given to an LLM-as-a-judge. The judge has to make a binary decision on whether a free-text response is correct or wrong. This is done based on its own internal knowledge without access to a ground-truth solution. We call the average of binary judgments across the benchmark the model's \textit{Judgment Score} for a given judge. Using a parallel MCQ evaluation with ground-truth answers allows us to compare the judgment scores with verifiable accuracy measurements (details and comparisons to alternatives are in Appendix~\ref{app:judgescore_gt_comparison}), consistent with prior scalable oversight research~\citep{bowman2022measuringprogress}. We compute pairwise similarity with CAPA, $\goelpi$, across 9 judge and 39 model pairs. For a complete overview of models investigated, the question filtering process, the inference setup and the prompts used for judges see Appendix~\ref{app:judge}. 

\subsection{Results \& Discussion}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{fig/OPENLLMLB_MCQ_data/judgement_scores_vs_similarity.png}
    \caption{\textbf{Judgment Score relation with Model Similarity.} Each line is a regression model fit between judgment and similarity scores as computed between model and judge pairs. Each point represents a single pair, and $\diamond$ indicates that both, the model and the judge, come from the same model family. We report for each fit the corresponding Pearson correlation values, $r$. We found significant positive correlation between judgment scores and similarity across all judges, $**$ indicates $p<0.01$.}
    \label{fig:judge-sim-plot}
    \vspace{-0.3cm}
\end{figure}
\begin{table}[!h]
    % \centering
    \caption{\textbf{Partial Correlation and Multiple Regression Results.} The table reports partial correlation results between judgment scores and model similarity when controlling for accuracy (r - Pearson correlation), as well as multiple regression results between judgment scores (DV) $\sim$ similarity (IV) and accuracy (IV). $*$ and $**$ indicate significance level p$<$0.05 and p$<$0.01 respectively. Across all judges we find a significant partial correlation, which implies that after controlling for accuracy there remains a relationship between judge score and model similarity. With respect to Multiple regression, across all judges we find a significant effect of both IV on judgment scores while holding the other constant, suggesting a strong positive relationship (for details, see Appendix~\ref{app:judgestats}).}
  \begin{tabular}{cccc}
        \toprule
        Judge & {Partial Cor.} & \multicolumn{2}{c}{Multiple Reg.} \\ 
          & $r$ & sim & acc\\
        \hline 
        Qwen2.5-7B-It & 0.60$^{**}$ & 0.59$^{**}$ & 0.51$^{**}$ \\ 
        Qwen2.5-32B-It & 0.43$^{**}$ & 0.41$^{**}$ & 0.86$^{**}$ \\ 
        Qwen2.5-72B-It & 0.42$^{**}$ & 0.47$^{**}$ & 1.04$^{**}$ \\ 
        Meta-Llama-3.1-8B-It & 0.65$^{**}$ & 1.15$^{**}$ & 0.53$^{**}$ \\ 
        Meta-Llama-3.1-70B-It & 0.45$^{**}$ & 0.61$^{**}$ & 0.92$^{**}$ \\ 
        Llama-3.3-70B-It & 0.35$^{*}$ & 0.50$^{*}$ & 1.02$^{**}$ \\ 
        gemma-2-9b-It & 0.65$^{**}$ & 0.76$^{**}$ & 0.69$^{**}$ \\ 
        gemma-2-27b-It & 0.65$^{**}$ & 0.71$^{**}$ & 0.68$^{**}$ \\ 
        Ministral-8B-It-2410 & 0.60$^{**}$ & 0.82$^{**}$ & 0.43$^{**}$ \\ 
        \bottomrule
    \end{tabular}
    
    \label{tab:pc_mr}
\end{table}

\textbf{Q1: Do LM Judges favor more similar models?}
As a motivating example, \texttt{Qwen2.5-72B-Instruct} as a judge scores \texttt{Qwen2.5-7B-Instruct} as being 71\% correct, while the more capable (41\% vs 51\% MCQ accuracy) \texttt{Llama-3.1-70B-Instruct} is deemed less accurate at 67\%. In Figure~\ref{fig:judge-sim-plot} we show that model favoritism extends beyond \textit{self-} or \textit{family-} preference to \textit{all} models that are functionally similar. We find a significant ($p<0.01$) positive correlation (average Pearson r=$0.84$) between LLM-as-a-judge scores and model similarity ($\goelpi$) for all judges.

\textbf{Q2: Is this merely due to better accuracy?} Note that while $\goelpi$ adjusts for inflation in agreement of highly accurate models, we do expect models with lower accuracy to be less similar with highly capable judge models, and thus have lower $\goelpi$. To control for the accuracy of the evaluated model we perform multiple regression and partial correlation analysis (see Table~\ref{tab:pc_mr}). The multiple regression analysis shows that both, accuracy and similarity, have a significant positive effect on the judge score. The coefficient of accuracy increases for more capable judge models, consistent with prior work showing improved alignment with human judges~\citep{thakur2024judgingjudgesevaluatingalignment}. We find that especially for small models ($<$32B) the effect of similarity is greater. The partial correlation results control for accuracy and confirm that there is still a significant effect of similarity on judgment scores even for the best judge models. Altogether, the statistical analysis confirms that judgment scores are confounded by affinity bias. 