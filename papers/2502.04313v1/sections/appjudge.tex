\section{LLM-as-a-Judge}
\label{app:judge}

In this section, we extend the LLM-as-a-judge experiments introduced in Section~\ref{sec:AI_Judges}. First, we compare CAPA with the related concept of error consistency \citep{geirhos2020beyond}, demonstrating its advantages in this context. We then present additional experiments to analyze the quality and behavior of the judges, as well as the performance of the evaluated models on the open-style MMLU-Pro benchmark.

To validate our findings, we provide detailed results from the statistical tests summarized in Table~\ref{tab:judges}. Specifically, we conduct Shapiro-Wilk and Breusch-Pagan tests to confirm that the assumptions of normality and homoscedasticity required for partial correlation and multiple regression analyses are satisfied.

Additionally, we outline the experimental setup, including: (1) the filtering process for MMLU-Pro to obtain open-style questions only, (2) the methodology for free-form chain-of-thought inference on this benchmark, and (3) the design of the LLM-as-a-judge evaluation framework. To ensure full reproducibility, we include all prompts and specify the language models used as judges and evaluated models at the end of this section.

\subsection{Comparison of Judge Scores for Our Similarity vs Error Consistency}

In Figure~\ref{fig:judge_sim_err_con-plot} we compare the relationship of judgment scores on the filtered MMLU-Pro dataset using different similarity metrics. On the left, we use CAPA and on the right, we compare against the original error consistency of \citet{geirhos2020beyond}. In both cases, we can see a correlation between the judge scores and the similarity of the LLM-as-a-judge and the model being evaluated. However, the relationship for CAPA is stronger, as shown by a mean Pearson $r$ of 0.9 which is greater than the one of 0.85 if error consistency is used. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/OPENLLMLB_MCQ_data/judgement_scores_vs_ours_error_cons.png}
    \caption{\textbf{Judgment Scores vs CAPA and vs Error Consistency.} We compare the relationship of judge scores on the filtered MMLU-Pro to our improved error consistency and to the original version of \citet{geirhos2020beyond}.}
    \label{fig:judge_sim_err_con-plot}
\end{figure}

\subsection{Evaluating Judge Scores Against Ground-Truth}
\label{app:judgescore_gt_comparison}

\subsubsection{MCQ Ground-Truth}

Since we source MCQ evaluations from Huggingface OpenLLM Leaderboard 2 throughout the paper, we use their default method to obtain probabilities across MCQ options. For MMLU-Pro and BBH they report the log-likelihood of each option. We apply a softmax to normalize these to 1. We checked this leads to calibrated predictions for base models and overconfident predictions for instruct models, consistent with prior observations about uncertainty of language models~\citep{openai2024gpt4technicalreport}.

\subsubsection{Judge Ensemble with Access to Reference Answers}

Since we are evaluating model responses given in open style on filtered MMLU-Pro questions using LM-as-a-judge, it is important to investigate whether the responses of the evaluated models are reasonable. To ensure that qualitative differences between models of different sizes and families remain, we compare their performance using free-form responses to the multiple-choice accuracy on the same set of questions. This is shown in Figure~\ref{fig:ref_judge-acc-plot}. Using the same question base for free-form and MCQ evaluation draws a direct connection between functional similarity and the behavior of LLM-as-judges. Focusing on a setting where we have access to ground-truth responses is important to accurately analyze the affinity biases of different LMs when used as evaluators. 

\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{fig/OPENLLMLB_MCQ_data/reference_judgement_scores_vs_mcq_accuracy.png}
    \caption{\textbf{Accuracy of free-form responses compared with multiple-choice accuracy on MMLU-Pro.} The free-form responses were rated using an ensemble of five capable LM judges. Each judge was given access to the original MMLU-Pro reference answers and their decisions whether a given response is correct or not were aggregated using majority voting.}
    \label{fig:ref_judge-acc-plot}
\end{figure}

\textbf{Experimental Setup} Every response is evaluated using an ensemble of five capable LMs used as LLM-as-a-judge from a range of different model families. The judge is given access to the question, the model's free-form response and all MMLU-Pro reference options. For each option we indicate if it is the correct or a wrong option. Using this information, the judge has to decide whether the model's response is correct or wrong. The prompt can be seen in Prompt~\ref{prompt:judge_w_references}. For every per-sample response, we aggregate the five binary decisions using majority voting. Since there are five judges and it is a binary decision task, there are no ties. A qualitative analysis has shown the high quality of this process in determining the correctness of responses. The judges used are \texttt{gemma-2-27b-it}, \texttt{Qwen2.5-32B-Instruct}, \texttt{Qwen2.5-72B-Instruct}, \texttt{Llama-3.1-70-Instruct} and \texttt{Llama-3.3-70B-Instruct} \citep{gemmateam2024gemma2improvingopen, qwen2025qwen25technicalreport, grattafiori2024llama3herdmodels, meta2024llama33}. 

\textbf{Open-style and Multiple Choice Correlate} As we can see in Figure~\ref{fig:ref_judge-acc-plot}, there is a high alignment between the performance in MCQ style compared to free-form. For the majority of models, the ordering with MCQ accuracy and open-style accuracy is very similar. There is a consistent trend that performance on the more challenging open evaluation is approximately 5-10\% lower. The exception is the instruction-tuned models from the Qwen2.5 and Gemma-2 model families that performed particularly well when giving free-form responses. For all other model families, the instruction tuned and base models show similar performance. 

\subsubsection{Judge Score Validity Against Reference-Based Ensemble}

To evaluate the quality of different judges and to analyze their similarities and differences, we compare judge scores to the correctness assessments of the previously introduced ensemble of judges. The results are shown in Figure~\ref{fig:judge_ref_judge-plot}. As we can see, most models used as LLM-as-a-judge are able to correctly rank capable and less capable models. 

\textbf{Capability-Dependent Affinity Effects} Even if the ordinal ranking of evaluated models is mostly accurate, there is a consistent trend that too many wrong responses are judged as being right. The exact behavior varies from judge to judge. Consider the small \texttt{Llama-3.1-8B-Instruct} for instance: it has a consistent positivity bias and ranks too many wrong responses as correct, even for models of low capability. \texttt{Qwen2.5-72B-Instruct} on the other hand appears to be much more capable in identifying the wrong responses of low-capability models. However, as the evaluated LMs become stronger, it exhibits the same bias as the smaller Llama judge. This aligns with the findings of Section~\ref{sec:AI_Judges} that LLM-as-judges show an affinity bias, because more capable models are also more similar to \texttt{Qwen2.5-72B-Instruct}.

\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{fig/OPENLLMLB_MCQ_data/judgement_scores_vs_reference_judgment_scores.png}
    \caption{\textbf{Judgment Scores compared with the ensemble judgment accuracy given access to reference answers.} We compare the judgment scores of each judge using only their own knowledge and capabilities to the rating of a judge ensemble that has access to the ground-truth options. The latter is a good proxy of the real correctness of responses.}
    \label{fig:judge_ref_judge-plot}
\end{figure}

%\subsection{Merging as a Controlled Setting}
% Ignore for now

\subsection{Statistical Testing}
\label{app:judgestats}

This section provides detailed statistical validation of the affinity bias observed in Section~\ref{sec:AI_Judges}. We confirm that judge-model similarity correlates with judgment scores even after controlling for MCQ ground-truth accuracy, using partial correlation and multiple regression. Additionally, we verify the statistical assumptions (normality, homoscedasticity) required for these tests across all nine judges.

\subsubsection{Quantifying Correlation Strength Using Partial Correlation}
\label{app:pc}

We compute partial correlations between judge scores and judge-model similarity while controlling for ground-truth accuracy. All judges show statistically significant positive correlations (Table~\ref{app:pc}), with coefficients ranging from $r=0.35$ (\texttt{Llama-3.3-70B-Instruct}) to $r=0.65$ (\texttt{Llama-3.1-8B-Instruct}). The strongest correlations occur for smaller judges from the same gemma-2 family (\texttt{gemma-2-27b-it} and \texttt{gemma-2-9b-it}), while larger Qwen2.5 judges exhibit moderate correlations ($r=0.42$ and $r=0.43$). All p-values remain significant ($p<0.05$), with the most robust results for the larger gemma judge ($p=0.00001$).

\begin{center}
\begin{tabular}{lrrlr}
\toprule
\multicolumn{5}{c}{\textbf{Detailed Partial Correlation Results}} \\
\midrule
Judge & n & r & CI 95\% & p-val \\
\midrule
Qwen2.5-7B-Instruct & 38 & 0.60043 & [0.34 0.77] & 0.00009 \\
Qwen2.5-32B-Instruct & 38 & 0.43376 & [0.13 0.66] & 0.00732 \\
Qwen2.5-72B-Instruct & 38 & 0.42353 & [0.12 0.66] & 0.00900 \\
Meta-Llama-3.1-8B-Instruct & 38 & 0.65172 & [0.42 0.81] & 0.00001 \\
Meta-Llama-3.1-70B-Instruct & 38 & 0.44770 & [0.14 0.67] & 0.00546 \\
Llama-3.3-70B-Instruct & 38 & 0.34882 & [0.03 0.6 ] & 0.03435 \\
gemma-2-9b-it & 38 & 0.64639 & [0.41 0.8 ] & 0.00002 \\
gemma-2-27b-it & 38 & 0.64808 & [0.41 0.8 ] & 0.00001 \\
Ministral-8B-Instruct-2410 & 39 & 0.59745 & [0.34 0.77] & 0.00007 \\
\bottomrule
\end{tabular}
\end{center}

\subsubsection{Multiple Regression}
\label{app:mr}

We perform multiple regression analysis with judgment scores as the dependent variable, using model similarity and ground-truth accuracy from the filtered set of MCQ questions as independent variables. Key results across all judges include:

\begin{itemize}
    \item \textbf{Coefficient Significance:} Both similarity and accuracy show statistically significant effects ($p<0.05$) for all judges. The similarity coefficients range from $\beta=0.35$ for the large \texttt{Llama-3.3-70B-Instruct} to $\beta=1.15$ for the smaller \texttt{Meta-Llama-3.1-8B-Instruct}, while accuracy coefficients span $\beta=0.43$ for the small \texttt{Ministral-8B-Instruct-2410} to $\beta=1.04$ for the large \texttt{Qwen2.5-72B-Instruct}.
    
    \item \textbf{Model Fit:} All regressions achieve high explanatory power with adjusted $R^2$ values between $0.87$ (\texttt{Ministral-8B}) and $0.92$ (\texttt{gemma-2-9b-it}).
    
    \item \textbf{Assumption Verification:}
    \begin{itemize}
        \item \textit{Normality:} Residuals are normally distributed (Shapiro-Wilk $p>0.05$) for 7 of 9 judges. Exceptions: \texttt{Meta-Llama-3.1-8B-Instruct} ($p=0.002$) and \texttt{Ministral-8B} ($p=0.012$).
        \item \textit{Homoscedasticity:} All models satisfy constant variance assumptions (Breusch-Pagan $p>0.05$).
    \end{itemize}
\end{itemize}

For example, the \texttt{Qwen2.5-7B-Instruct} judge model shows:

\begin{itemize}
    \item Significant positive effects for both similarity ($\beta=0.59$, $p<0.001$) and accuracy ($\beta=0.51$, $p<0.001$)
    \item Strong model fit ($R^2=0.91$, $F(2,35)=182.9$, $p=2.95\times10^{-19}$)
    \item Normally distributed residuals (Shapiro-Wilk $p=0.690$)
\end{itemize}

Full regression outputs for all judges are provided in the Tables below. We present detailed regression results for each judge model. Each judge's statistical analysis includes three components: (1) Test summary, (2) Coefficient estimates, and (3) Diagnostic statistics. The consistent significance of similarity coefficients confirms that affinity bias persists even when controlling for actual model capability.

\begin{center}
\small
\begin{tabular}{@{}ll@{\hspace{15pt}}ll@{}}
\toprule
\multicolumn{4}{c}{\textbf{Judge:} \texttt{Qwen2.5-7B-Instruct} \citep{qwen2025qwen25technicalreport}} \\
\midrule
Model: & OLS & Adj. R-squared: & 0.908 \\
Dependent Variable: & scores & AIC: & -134.3091 \\
Date: & 2025-01-30 11:42 & BIC: & -129.3963 \\
No. Observations: & 38 & Log-Likelihood: & 70.155 \\
Df Model: & 2 & F-statistic: & 182.9 \\
Df Residuals: & 35 & Prob (F-statistic): & 2.95e-19 \\
R-squared: & 0.913 & Scale: & 0.0015837 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{1}{c}{Coef.} & \multicolumn{1}{c}{Std.Err} & \multicolumn{1}{c}{t} & \multicolumn{1}{c}{P>|t|} & \multicolumn{2}{c}{95\% CI} \\
\midrule
Intercept & 0.092 & 0.019 & 4.885 & 0.000 & 0.054 & 0.131 \\
similarity & 0.586 & 0.132 & 4.442 & 0.000 & 0.318 & 0.853 \\
accuracy & 0.506 & 0.098 & 5.185 & 0.000 & 0.308 & 0.704 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{llll}
\toprule
\midrule
Omnibus: & 2.363 & Durbin-Watson: & 2.097 \\
Prob(Omnibus): & 0.307 & Jarque-Bera (JB): & 1.400 \\
Skew: & -0.437 & Prob(JB): & 0.496 \\
Kurtosis: & 3.348 & Condition No.: & 27 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Normality \& Homoscedasticity:} Shapiro-Wilk Test for Normality: Statistic=0.979, (p-value=0.690).  
Residuals are likely normally distributed. 
Breusch-Pagan test for homoscedasticity:
Lagrange Multiplier statistic: 0.456
(p-value: 0.796), 
F-value: 0.213  
(p-value: 0.809). 
No evidence of heteroscedasticity (the residuals have a constant variance, homoscedasticity met).

\begin{center}
\small
\begin{tabular}{llll}
\toprule
\multicolumn{4}{c}{\textbf{Judge:} \texttt{Qwen2.5-32B-Instruct} \citep{qwen2025qwen25technicalreport}} \\
\midrule
Model: & OLS & Adj. R-squared: & 0.907 \\
Dependent Variable: & scores & AIC: & -114.7801 \\
Date: & 2025-01-30 11:42 & BIC: & -109.8674 \\
No. Observations: & 38 & Log-Likelihood: & 60.390 \\
Df Model: & 2 & F-statistic: & 182.2 \\
Df Residuals: & 35 & Prob (F-statistic): & 3.14e-19 \\
R-squared: & 0.912 & Scale: & 0.0026477 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{1}{c}{Coef.} & \multicolumn{1}{c}{Std.Err} & \multicolumn{1}{c}{t} & \multicolumn{1}{c}{P>|t|} & \multicolumn{2}{c}{95\% CI} \\
\midrule
Intercept & 0.045 & 0.028 & 1.620 & 0.114 & -0.011 & 0.101 \\
similarity & 0.414 & 0.145 & 2.848 & 0.007 & 0.119 & 0.709 \\
accuracy & 0.861 & 0.132 & 6.513 & 0.000 & 0.593 & 1.129 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{llll}
\toprule
\midrule
Omnibus: & 0.227 & Durbin-Watson: & 2.052 \\
Prob(Omnibus): & 0.893 & Jarque-Bera (JB): & 0.411 \\
Skew: & 0.127 & Prob(JB): & 0.814 \\
Kurtosis: & 2.558 & Condition No.: & 25 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Normality \& Homoscedasticity:} Shapiro-Wilk Test for Normality: Statistic=0.989, (p-value=0.965). 
Residuals are likely normally distributed. 
Breusch-Pagan test for homoscedasticity:
Lagrange Multiplier statistic: 3.097 
(p-value: 0.213), 
F-value: 1.553 
(p-value: 0.226). 
No evidence of heteroscedasticity (the residuals have a constant variance, homoscedasticity met).

\begin{center}
\small
\begin{tabular}{llll}
\toprule
\multicolumn{4}{c}{\textbf{Judge:} \texttt{Qwen2.5-72B-Instruct} \citep{qwen2025qwen25technicalreport}} \\
\midrule
Model: & OLS & Adj. R-squared: & 0.913 \\
Dependent Variable: & scores & AIC: & -103.8097 \\
Date: & 2025-01-30 11:42 & BIC: & -98.8969 \\
No. Observations: & 38 & Log-Likelihood: & 54.905 \\
Df Model: & 2 & F-statistic: & 195.4 \\
Df Residuals: & 35 & Prob (F-statistic): & 1.03e-19 \\
R-squared: & 0.918 & Scale: & 0.0035339 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{1}{c}{Coef.} & \multicolumn{1}{c}{Std.Err} & \multicolumn{1}{c}{t} & \multicolumn{1}{c}{P>|t|} & \multicolumn{2}{c}{95\% CI} \\
\midrule
Intercept & 0.064 & 0.032 & 2.038 & 0.049 & 0.000 & 0.128 \\
similarity & 0.474 & 0.171 & 2.766 & 0.009 & 0.126 & 0.822 \\
accuracy & 1.043 & 0.156 & 6.702 & 0.000 & 0.727 & 1.359 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{@{}ll@{\hspace{15pt}}ll@{}}
\toprule
Omnibus: & 0.139 & Durbin-Watson: & 1.776 \\
Prob(Omnibus): & 0.933 & Jarque-Bera (JB): & 0.286 \\
Skew: & -0.124 & Prob(JB): & 0.867 \\
Kurtosis: & 2.655 & Condition No.: & 25 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Normality \& Homoscedasticity:} Shapiro-Wilk Test for Normality: Statistic=0.989, (p-value=0.968). 
Residuals are likely normally distributed. 
Breusch-Pagan test for homoscedasticity: 
Lagrange Multiplier statistic: 3.562 
(p-value: 0.168), 
F-value: 1.810
(p-value: 0.179). 
No evidence of heteroscedasticity (the residuals have a constant variance, homoscedasticity met).

\begin{center}
\small
\begin{tabular}{llll}
\toprule
\multicolumn{4}{c}{\textbf{Judge:} \texttt{Meta-Llama-3.1-8B-Instruct} \citep{grattafiori2024llama3herdmodels}} \\
\midrule
Model: & OLS & Adj. R-squared: & 0.881 \\
Dependent Variable: & scores & AIC: & -114.9610 \\
Date: & 2025-01-30 11:42 & BIC: & -110.0482 \\
No. Observations: & 38 & Log-Likelihood: & 60.481 \\
Df Model: & 2 & F-statistic: & 138.6 \\
Df Residuals: & 35 & Prob (F-statistic): & 2.34e-17 \\
R-squared: & 0.888 & Scale: & 0.0026351 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{1}{c}{Coef.} & \multicolumn{1}{c}{Std.Err} & \multicolumn{1}{c}{t} & \multicolumn{1}{c}{P>|t|} & \multicolumn{2}{c}{95\% CI} \\
\midrule
Intercept & 0.327 & 0.023 & 14.309 & 0.000 & 0.281 & 0.374 \\
similarity & 1.149 & 0.226 & 5.083 & 0.000 & 0.690 & 1.608 \\
accuracy & 0.532 & 0.106 & 5.035 & 0.000 & 0.317 & 0.746 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{@{}ll@{\hspace{15pt}}ll@{}}
\toprule
Omnibus: & 23.344 & Durbin-Watson: & 2.611 \\
Prob(Omnibus): & 0.000 & Jarque-Bera (JB): & 52.336 \\
Skew: & -1.424 & Prob(JB): & 0.000 \\
Kurtosis: & 7.994 & Condition No.: & 31 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Normality \& Homoscedasticity:} Shapiro-Wilk Test for Normality: Statistic=0.892, (p-value=0.002).
Residuals are likely not normally distributed.
Breusch-Pagan test for homoscedasticity:
Lagrange Multiplier statistic: 4.186
(p-value: 0.123), 
F-value: 2.166
(p-value: 0.130). 
No evidence of heteroscedasticity (the residuals have a constant variance, homoscedasticity met).

\begin{center}
\small
\begin{tabular}{llll}
\toprule
\multicolumn{4}{c}{\textbf{Judge:} \texttt{Meta-Llama-3.1-70B-Instruct} \citep{grattafiori2024llama3herdmodels}} \\
\midrule
Model: & OLS & Adj. R-squared: & 0.903 \\
Dependent Variable: & scores & AIC: & -103.3457 \\
Date: & 2025-01-30 11:42 & BIC: & -98.4329 \\
No. Observations: & 38 & Log-Likelihood: & 54.673 \\
Df Model: & 2 & F-statistic: & 172.4 \\
Df Residuals: & 35 & Prob (F-statistic): & 7.60e-19 \\
R-squared: & 0.908 & Scale: & 0.0035773 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{1}{c}{Coef.} & \multicolumn{1}{c}{Std.Err} & \multicolumn{1}{c}{t} & \multicolumn{1}{c}{P>|t|} & \multicolumn{2}{c}{95\% CI} \\
\midrule
Intercept & 0.140 & 0.031 & 4.533 & 0.000 & 0.077 & 0.202 \\
similarity & 0.615 & 0.208 & 2.962 & 0.005 & 0.193 & 1.036 \\
accuracy & 0.917 & 0.157 & 5.827 & 0.000 & 0.598 & 1.237 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{@{}ll@{\hspace{15pt}}ll@{}}
\toprule
Omnibus: & 4.624 & Durbin-Watson: & 1.984 \\
Prob(Omnibus): & 0.099 & Jarque-Bera (JB): & 3.237 \\
Skew: & -0.616 & Prob(JB): & 0.198 \\
Kurtosis: & 3.724 & Condition No.: & 28 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Normality \& Homoscedasticity:} Shapiro-Wilk Test for Normality: Statistic=0.974, (p-value=0.502).
Residuals are likely normally distributed.
Breusch-Pagan test for homoscedasticity:
Lagrange Multiplier statistic: 2.975
(p-value: 0.226), 
F-value: 1.487
(p-value: 0.240). 
No evidence of heteroscedasticity (the residuals have a constant variance, homoscedasticity met).

\begin{center}
\small
\begin{tabular}{llll}
\toprule
\multicolumn{4}{c}{\textbf{Judge:} \texttt{Llama-3.3-70B-Instruct} \cite{meta2024llama33}} \\
\midrule
Model: & OLS & Adj. R-squared: & 0.884 \\
Dependent Variable: & scores & AIC: & -94.3204 \\
Date: & 2025-01-30 11:42 & BIC: & -89.4077 \\
No. Observations: & 38 & Log-Likelihood: & 50.160 \\
Df Model: & 2 & F-statistic: & 142.6 \\
Df Residuals: & 35 & Prob (F-statistic): & 1.49e-17 \\
R-squared: & 0.891 & Scale: & 0.0045363 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{1}{c}{Coef.} & \multicolumn{1}{c}{Std.Err} & \multicolumn{1}{c}{t} & \multicolumn{1}{c}{P>|t|} & \multicolumn{2}{c}{95\% CI} \\
\midrule
Intercept & 0.162 & 0.036 & 4.544 & 0.000 & 0.089 & 0.234 \\
similarity & 0.487 & 0.221 & 2.202 & 0.034 & 0.038 & 0.935 \\
accuracy & 1.022 & 0.177 & 5.770 & 0.000 & 0.662 & 1.381 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{@{}ll@{\hspace{15pt}}ll@{}}
\toprule
Omnibus: & 4.168 & Durbin-Watson: & 1.898 \\
Prob(Omnibus): & 0.124 & Jarque-Bera (JB): & 2.830 \\
Skew: & -0.584 & Prob(JB): & 0.243 \\
Kurtosis: & 3.652 & Condition No.: & 27 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Normality \& Homoscedasticity:} Shapiro-Wilk Test for Normality: Statistic=0.978, (p-value=0.642).
Residuals are likely normally distributed.
Breusch-Pagan test for homoscedasticity:
Lagrange Multiplier statistic: 2.500
(p-value: 0.287), 
F-value: 1.232
(p-value: 0.304). 
No evidence of heteroscedasticity (the residuals have a constant variance, homoscedasticity met).

\begin{center}
\small
\begin{tabular}{llll}
\toprule
\multicolumn{4}{c}{\textbf{Judge:} \texttt{gemma-2-9b-it} \citep{gemmateam2024gemma2improvingopen}} \\
\midrule
Model: & OLS & Adj. R-squared: & 0.917 \\
Dependent Variable: & scores & AIC: & -122.8074 \\
Date: & 2025-01-30 11:42 & BIC: & -117.8946 \\
No. Observations: & 38 & Log-Likelihood: & 64.404 \\
Df Model: & 2 & F-statistic: & 206.0 \\
Df Residuals: & 35 & Prob (F-statistic): & 4.36e-20 \\
R-squared: & 0.922 & Scale: & 0.0021435 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{1}{c}{Coef.} & \multicolumn{1}{c}{Std.Err} & \multicolumn{1}{c}{t} & \multicolumn{1}{c}{P>|t|} & \multicolumn{2}{c}{95\% CI} \\
\midrule
Intercept & 0.134 & 0.021 & 6.356 & 0.000 & 0.091 & 0.177 \\
similarity & 0.763 & 0.152 & 5.012 & 0.000 & 0.454 & 1.072 \\
accuracy & 0.688 & 0.097 & 7.129 & 0.000 & 0.492 & 0.884 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{@{}ll@{\hspace{15pt}}ll@{}}
\toprule
Omnibus: & 6.751 & Durbin-Watson: & 1.901 \\
Prob(Omnibus): & 0.034 & Jarque-Bera (JB): & 5.969 \\
Skew: & -0.621 & Prob(JB): & 0.051 \\
Kurtosis: & 4.492 & Condition No.: & 25 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Normality \& Homoscedasticity:} Shapiro-Wilk Test for Normality: Statistic=0.959, (p-value=0.179).
Residuals are likely normally distributed.
Breusch-Pagan test for homoscedasticity:
Lagrange Multiplier statistic: 2.550
(p-value: 0.279), 
F-value: 1.259
(p-value: 0.297). 
No evidence of heteroscedasticity (the residuals have a constant variance, homoscedasticity met).

\begin{center}
\small
\begin{tabular}{llll}
\toprule
\multicolumn{4}{c}{\textbf{Judge:} \texttt{gemma-2-27b-it} \citep{gemmateam2024gemma2improvingopen}} \\
\midrule
Model: & OLS & Adj. R-squared: & 0.919 \\
Dependent Variable: & scores & AIC: & -121.3075 \\
Date: & 2025-01-30 11:42 & BIC: & -116.3947 \\
No. Observations: & 38 & Log-Likelihood: & 63.654 \\
Df Model: & 2 & F-statistic: & 212.0 \\
Df Residuals: & 35 & Prob (F-statistic): & 2.76e-20 \\
R-squared: & 0.924 & Scale: & 0.0022298 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{1}{c}{Coef.} & \multicolumn{1}{c}{Std.Err} & \multicolumn{1}{c}{t} & \multicolumn{1}{c}{P>|t|} & \multicolumn{2}{c}{95\% CI} \\
\midrule
Intercept & 0.191 & 0.022 & 8.655 & 0.000 & 0.146 & 0.235 \\
similarity & 0.705 & 0.140 & 5.034 & 0.000 & 0.421 & 0.989 \\
accuracy & 0.677 & 0.106 & 6.407 & 0.000 & 0.462 & 0.892 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{@{}ll@{\hspace{15pt}}ll@{}}
\toprule
Omnibus: & 8.920 & Durbin-Watson: & 1.882 \\
Prob(Omnibus): & 0.012 & Jarque-Bera (JB): & 8.659 \\
Skew: & -0.791 & Prob(JB): & 0.013 \\
Kurtosis: & 4.722 & Condition No.: & 24 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Normality \& Homoscedasticity:} Shapiro-Wilk Test for Normality: Statistic=0.945, (p-value=0.062).
Residuals are likely normally distributed.
Breusch-Pagan test for homoscedasticity:
Lagrange Multiplier statistic: 1.645
(p-value: 0.439), 
F-value: 0.792
(p-value: 0.461). 
No evidence of heteroscedasticity (the residuals have a constant variance, homoscedasticity met).

\begin{center}
\small
\begin{tabular}{llll}
\toprule
\multicolumn{4}{c}{\textbf{Judge:} \texttt{Ministral-8B-Instruct-2410} \citep{mistral2024ministral8b}} \\
\midrule
Model: & OLS & Adj. R-squared: & 0.868 \\
Dependent Variable: & scores & AIC: & -146.8086 \\
Date: & 2025-01-30 11:42 & BIC: & -141.8179 \\
No. Observations: & 39 & Log-Likelihood: & 76.404 \\
Df Model: & 2 & F-statistic: & 125.6 \\
Df Residuals: & 36 & Prob (F-statistic): & 5.80e-17 \\
R-squared: & 0.875 & Scale: & 0.0012608 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{1}{c}{Coef.} & \multicolumn{1}{c}{Std.Err} & \multicolumn{1}{c}{t} & \multicolumn{1}{c}{P>|t|} & \multicolumn{2}{c}{95\% CI} \\
\midrule
Intercept & 0.117 & 0.016 & 7.187 & 0.000 & 0.084 & 0.150 \\
similarity & 0.825 & 0.185 & 4.470 & 0.000 & 0.451 & 1.199 \\
accuracy & 0.432 & 0.063 & 6.803 & 0.000 & 0.303 & 0.560 \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{@{}ll@{\hspace{15pt}}ll@{}}
\toprule
Omnibus: & 9.707 & Durbin-Watson: & 1.766 \\
Prob(Omnibus): & 0.008 & Jarque-Bera (JB): & 8.755 \\
Skew: & -0.999 & Prob(JB): & 0.013 \\
Kurtosis: & 4.180 & Condition No.: & 36 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Normality \& Homoscedasticity:}Shapiro-Wilk Test for Normality: Statistic=0.925, (p-value=0.012).
Residuals are likely not normally distributed.
Breusch-Pagan test for homoscedasticity:
Lagrange Multiplier statistic: 1.270
(p-value: 0.530), 
F-value: 0.606
(p-value: 0.551). 
No evidence of heteroscedasticity (the residuals have a constant variance, homoscedasticity met).

\subsection{Experimental Setup for Filtering MMLU-Pro}\label{app:judge-filter}

We evaluate or models and judges on a set of questions that can be answered as MCQ as well as in open-style without access to reference options. This benchmark is obtained by using the filtering process proposed by \citet{myrzakhan2024openllmleaderboard} on MMLU-Pro, whereas it was originally used to filter MMLU \citep{hendrycks2021measuring, wang2024mmlupro}. Every question is evaluated twice using a \texttt{Qwen-2.5-32B-Instruct} LM: first, it is judged in a binary way whether it is possible to answer the question without access to the MCQ options. In the second iteration, the judge gives a fine-grained confidence score. If either the binary decision is positive or the confidence is above a threshold, the question becomes part of our filtered benchmark. After this filtering process, 8707 of the original 12032 questions remain. The detailed prompts are described in Prompts~\ref{prompt:coarse_filter} and \ref{prompt:fine_filter}.

\subsection{Experimental Setup to Perform Free-Form Inference on Filtered MMLU-Pro}\label{app:judge-osq_inference}

To obtain the per-sample responses of every model on the filtered MMLU-Pro benchmark, we evaluate them using a custom task on the LM Evaluation Harness \citep{eval-harness}. Whereas the MCQ results from the Open LLM Leaderboard were generated using 5-shot evaluation without chain-of-thought (CoT) prompt, we included CoTs when performing free-form inference \citep{myrzakhan2024openllmleaderboard}. This was necessary to ensure sufficient instruction following and response quality even for small base models, because free-form generation is more challenging than MCQ evaluation, where access to reference answers is given. 

We modified every 5-shot CoT prompt by removing the answer options from the end of the question and replacing every reference to them in the CoT with the corresponding answer text. An example of this process is shown in Prompts~\ref{prompt:cot_mcq} and \ref{prompt:cot_osq}. Our benchmark is implemented as a task for the LM Eval Harness \citep{eval-harness}. Every CoT response is generated until a stop condition is met. The final response that is judged is extracted using regex matching. We use vLLM as the backend for the LM Eval Harness \citep{kwon2023efficient}. Even for instruction-tuned models, the options \texttt{--apply\_chat\_template} and \texttt{--fewshot\_as\_multiturn} were omitted because for the majority of LMs inspected the quality of responses decreased slightly to severely. However, we did not thoroughly investigate whether this is the case for every single model.

\subsection{Experimental Setup for LLM-as-a-Judge on Filtered MMLU-Pro}

This section describes the setup of the experiment for Figure~\ref{fig:judge-sim-plot}. On the x-axis we show the similarity between our LLM-as-a-judge and the LM that is being evaluated, whereas the y-axis shows how the given responses of that model were rated by the same judge. The list of judges is shown in Table~\ref{tab:judges} and the pool of models evaluated can be seen in Table~\ref{tab:models}.

For the computation of similarities, we use the logs of the official evaluation runs of \citet{myrzakhan2024openllmleaderboard} that are provided on \hyperlink{huggingface.co}{huggingface.co}. The set of responses is filtered to include only those questions that were rated as answerable in open-style without access to the reference options, as previously described in Section~\ref{app:judge-filter}. Using the logarithmic probabilities of the models for the answer options of this set of questions, we compute CAPA and other similarities. In addition, for each model-judge pair data samples where the ground truth option differed were excluded from the final analysis, for the final sample count per pair see from table~\ref{tab:Qwen2.5-7B-Instruct} to table~\ref{tab:Ministral-8B-Instruct-2410}.

Next, the judgment scores are obtained by prompting each LLM-as-a-judge to decide whether a given response to a question is correct or not. To mimic more common, but ungrounded settings for automatic AI evaluation, such as Arena-hard-auto or AlpacaEval 2.0, we do not provide the judge with access to ground-truth responses or MCQ answer options \citep{li2024crowdsourceddatahighqualitybenchmarks, dubois2024length}. Since ground-truth responses for each question are available, it is possible to analyze the affinity bias of different judges and determine if there is any unfair preference. The prompt given to the judge is shown in Section~\ref{prompt:judge}.  Each final decision was given as token ``0'' (incorrect) or ``1'' (correct). Instruction-following is exceptional for the models used as LLM-as-a-judge, so the amount of discarded samples due to invalid responses is negligible. Finally, the \textit{Judge Score} of an evaluated model is computed by averaging the judge decisions across the set of questions.

\subsection{List of Judges and Evaluated Language Models}

Our judge preference experiments were performed using nine high-capability, open-weight models from four different model families. The models that represent the current state-of-the-art of open-weight language models from very small up to models with 72 billion parameters. Whereas the judges are all instruction-tuned, the list of evaluated models contains base models as well.

Whenever possible, we evaluated both the base and the instruction-tuned model for every combination of size and model family. Sometimes this was not possible, because the base model's weights were not available on huggingface, evaluations on the Open LLM Leaderboard v2 were not provided or the LM consistently crashed in vLLM when performing inference \citep{myrzakhan2024openllmleaderboard, kwon2023efficient}. The list below shows all models that are part of our experiments.

\begin{table}[!htbp]
    \centering
    \caption{\textbf{LMs used as LLM-as-a-Judge}}
    \begin{tabular}{ll}
        \hline
        \textbf{Judge Model Name} \\
        \hline
        \texttt{google/gemma-2-9b-it} & \citep{gemmateam2024gemma2improvingopen} \\
        \texttt{google/gemma-2-27b-it} & \citep{gemmateam2024gemma2improvingopen} \\
        \texttt{Qwen/Qwen2.5-7B-Instruct} & \citep{qwen2025qwen25technicalreport} \\
        \texttt{Qwen/Qwen2.5-32B-Instruct} & \citep{qwen2025qwen25technicalreport} \\
        \texttt{Qwen/Qwen2.5-72B-Instruct} & \citep{qwen2025qwen25technicalreport} \\
        \texttt{meta-llama/Meta-Llama-3.1-8B-Instruct} & \citep{grattafiori2024llama3herdmodels} \\
        \texttt{meta-llama/Meta-Llama-3.1-70B-Instruct} & \citep{grattafiori2024llama3herdmodels} \\
        \texttt{meta-llama/Llama-3.3-70B-Instruct} & \citep{meta2024llama33} \\
        \texttt{mistralai/Ministral-8B-Instruct-2410} & \citep{mistral2024ministral8b} \\
        \hline
    \end{tabular}
    \label{tab:judges}
\end{table}

\begin{table}[!htbp]
    \centering
    \caption{\textbf{LMs Evaluated on the Filtered MMLU-Pro Benchmark}}
    \begin{tabular}{l|l}
        \hline
        \multicolumn{2}{c}{\textbf{Model Name}} \\
        \hline \hline
        \textbf{Base Models} & \textbf{Instruction-tuned Models} \\
        \hline
        % Google
        \multicolumn{2}{c}{Gemma-2 Family \citep{gemmateam2024gemma2improvingopen}} \\
        \texttt{google/gemma-2-2b} & \texttt{google/gemma-2-2b-it} \\
        \texttt{google/gemma-2-9b} & \texttt{google/gemma-2-9b-it} \\
        \texttt{google/gemma-2-27b} & \texttt{google/gemma-2-27b-it} \\
        \midrule
        % HuggingFaceTB
        \multicolumn{2}{c}{SmolLM2 Family \citep{allal2024smollm2}} \\
        & \texttt{HuggingFaceTB/SmolLM2-135M-Instruct} \\
        & \texttt{HuggingFaceTB/SmolLM2-360M-Instruct} \\
        \texttt{HuggingFaceTB/SmolLM2-1.7B} & \texttt{HuggingFaceTB/SmolLM2-1.7B-Instruct} \\
        \midrule
        % Meta-Llama
        \multicolumn{2}{c}{Llama 3.1/3.2/3.3 Model Family \citep{grattafiori2024llama3herdmodels, meta2024llama32, meta2024llama33}} \\
        \texttt{meta-llama/Meta-Llama-3.1-8B} & \texttt{meta-llama/Meta-Llama-3.1-8B-Instruct} \\
        \texttt{meta-llama/Meta-Llama-3.1-70B} & \texttt{meta-llama/Meta-Llama-3.1-70B-Instruct} \\
        \texttt{meta-llama/Llama-3.2-1B} & \texttt{meta-llama/Llama-3.2-1B-Instruct} \\
        \texttt{meta-llama/Llama-3.2-3B} & \texttt{meta-llama/Llama-3.2-3B-Instruct} \\
        & \texttt{meta-llama/Llama-3.3-70B-Instruct} \\
        \midrule
        % Microsoft
        \multicolumn{2}{c}{Phi-4 Family \citep{phillm2024phi4}} \\
        & \texttt{microsoft/phi-4} \\
        \midrule
        % Qwen
        \multicolumn{2}{c}{Qwen2.5 Family \citep{qwen2025qwen25technicalreport}} \\
        \texttt{Qwen/Qwen2.5-0.5B} & \texttt{Qwen/Qwen2.5-0.5B-Instruct} \\
        \texttt{Qwen/Qwen2.5-1.5B} & \texttt{Qwen/Qwen2.5-1.5B-Instruct} \\
        \texttt{Qwen/Qwen2.5-3B} & \texttt{Qwen/Qwen2.5-3B-Instruct} \\
        \texttt{Qwen/Qwen2.5-7B} & \texttt{Qwen/Qwen2.5-7B-Instruct} \\
        \texttt{Qwen/Qwen2.5-14B} & \texttt{Qwen/Qwen2.5-14B-Instruct} \\
        \texttt{Qwen/Qwen2.5-32B} & \texttt{Qwen/Qwen2.5-32B-Instruct} \\ 
        \texttt{Qwen/Qwen2.5-72B} &\texttt{Qwen/Qwen2.5-72B-Instruct} \\
        \midrule
        % TII UAE
        \multicolumn{2}{c}{Falcon-3 Model Family \citep{tii2024falcon3}} \\
        & \texttt{tiiuae/Falcon3-1B-Instruct} \\
        \texttt{tiiuae/Falcon3-7B-Base} & \texttt{tiiuae/Falcon3-7B-Instruct} \\
        \texttt{tiiuae/Falcon3-10B-Base} & \texttt{tiiuae/Falcon3-10B-Instruct} \\
        \hline
    \end{tabular}
    \label{tab:models}
\end{table}

\begin{table}[!htbp]
    \centering
    \caption{Final Sample Count (N) for Qwen2.5-7B-Instruct on Similarity Computation of Filtered MMLU-Pro}
\begin{tabular}{llr}
\toprule
judge & model & N \\
\midrule
Qwen2.5-7B-Instruct & HuggingFaceTB/SmolLM2-1.7B & 8707 \\
 & HuggingFaceTB/SmolLM2-1.7B-Instruct & 8706 \\
 & HuggingFaceTB/SmolLM2-135M-Instruct & 8707 \\
 & HuggingFaceTB/SmolLM2-360M-Instruct & 8707 \\
 & Qwen/Qwen2.5-0.5B & 8707 \\
 & Qwen/Qwen2.5-0.5B-Instruct & 8707 \\
 & Qwen/Qwen2.5-1.5B & 8707 \\
 & Qwen/Qwen2.5-1.5B-Instruct & 8707 \\
 & Qwen/Qwen2.5-14B & 8707 \\
 & Qwen/Qwen2.5-14B-Instruct & 8707 \\
 & Qwen/Qwen2.5-32B & 8707 \\
 & Qwen/Qwen2.5-32B-Instruct & 8707 \\
 & Qwen/Qwen2.5-3B & 8707 \\
 & Qwen/Qwen2.5-3B-Instruct & 8707 \\
 & Qwen/Qwen2.5-72B & 8707 \\
 & Qwen/Qwen2.5-72B-Instruct & 8707 \\
 & Qwen/Qwen2.5-7B & 8707 \\
 & google/gemma-2-27b & 8702 \\
 & google/gemma-2-27b-it & 8702 \\
 & google/gemma-2-2b & 8702 \\
 & google/gemma-2-2b-it & 8685 \\
 & google/gemma-2-9b & 8702 \\
 & google/gemma-2-9b-it & 8702 \\
 & meta-llama/Llama-3.2-1B & 8707 \\
 & meta-llama/Llama-3.2-1B-Instruct & 8707 \\
 & meta-llama/Llama-3.2-3B & 8707 \\
 & meta-llama/Llama-3.2-3B-Instruct & 8707 \\
 & meta-llama/Llama-3.3-70B-Instruct & 8706 \\
 & meta-llama/Meta-Llama-3.1-70B & 8685 \\
 & meta-llama/Meta-Llama-3.1-70B-Instruct & 8685 \\
 & meta-llama/Meta-Llama-3.1-8B & 8685 \\
 & meta-llama/Meta-Llama-3.1-8B-Instruct & 8702 \\
 & microsoft/phi-4 & 8706 \\
 & tiiuae/Falcon3-10B-Base & 8706 \\
 & tiiuae/Falcon3-10B-Instruct & 8706 \\
 & tiiuae/Falcon3-1B-Instruct & 8706 \\
 & tiiuae/Falcon3-7B-Base & 8706 \\
 & tiiuae/Falcon3-7B-Instruct & 8706 \\
\bottomrule
\end{tabular}
    \label{tab:Qwen2.5-7B-Instruct}
\end{table}

\begin{table}[]
    \centering
    \caption{Final Sample Count (N) for Qwen2.5-32B-Instruct on Similarity Computation of Filtered MMLU-Pro}
    \begin{tabular}{llr}
\toprule
judge & model & N \\
\midrule
Qwen2.5-32B-Instruct & HuggingFaceTB/SmolLM2-1.7B & 8707 \\
 & HuggingFaceTB/SmolLM2-1.7B-Instruct & 8706 \\
 & HuggingFaceTB/SmolLM2-135M-Instruct & 8707 \\
 & HuggingFaceTB/SmolLM2-360M-Instruct & 8707 \\
 & Qwen/Qwen2.5-0.5B & 8707 \\
 & Qwen/Qwen2.5-0.5B-Instruct & 8707 \\
 & Qwen/Qwen2.5-1.5B & 8707 \\
 & Qwen/Qwen2.5-1.5B-Instruct & 8707 \\
 & Qwen/Qwen2.5-14B & 8707 \\
 & Qwen/Qwen2.5-14B-Instruct & 8707 \\
 & Qwen/Qwen2.5-32B & 8707 \\
 & Qwen/Qwen2.5-3B & 8707 \\
 & Qwen/Qwen2.5-3B-Instruct & 8707 \\
 & Qwen/Qwen2.5-72B & 8707 \\
 & Qwen/Qwen2.5-72B-Instruct & 8707 \\
 & Qwen/Qwen2.5-7B & 8707 \\
 & Qwen/Qwen2.5-7B-Instruct & 8707 \\
 & google/gemma-2-27b & 8702 \\
 & google/gemma-2-27b-it & 8702 \\
 & google/gemma-2-2b & 8702 \\
 & google/gemma-2-2b-it & 8685 \\
 & google/gemma-2-9b & 8702 \\
 & google/gemma-2-9b-it & 8702 \\
 & meta-llama/Llama-3.2-1B & 8707 \\
 & meta-llama/Llama-3.2-1B-Instruct & 8707 \\
 & meta-llama/Llama-3.2-3B & 8707 \\
 & meta-llama/Llama-3.2-3B-Instruct & 8707 \\
 & meta-llama/Llama-3.3-70B-Instruct & 8706 \\
 & meta-llama/Meta-Llama-3.1-70B & 8685 \\
 & meta-llama/Meta-Llama-3.1-70B-Instruct & 8685 \\
 & meta-llama/Meta-Llama-3.1-8B & 8685 \\
 & meta-llama/Meta-Llama-3.1-8B-Instruct & 8702 \\
 & microsoft/phi-4 & 8706 \\
 & tiiuae/Falcon3-10B-Base & 8706 \\
 & tiiuae/Falcon3-10B-Instruct & 8706 \\
 & tiiuae/Falcon3-1B-Instruct & 8706 \\
 & tiiuae/Falcon3-7B-Base & 8706 \\
 & tiiuae/Falcon3-7B-Instruct & 8706 \\
\bottomrule
\end{tabular}
    \label{tab:Qwen2.5-32B-Instruct}
\end{table}

\begin{table}[]
    \centering
     \caption{Final Sample Count (N) for Qwen2.5-72B-Instruct on Similarity Computation of Filtered MMLU-Pro}
\begin{tabular}{llr}
\toprule
judge & model & N \\
\midrule
Qwen2.5-72B-Instruct & HuggingFaceTB/SmolLM2-1.7B & 8707 \\
 & HuggingFaceTB/SmolLM2-1.7B-Instruct & 8706 \\
 & HuggingFaceTB/SmolLM2-135M-Instruct & 8707 \\
 & HuggingFaceTB/SmolLM2-360M-Instruct & 8707 \\
 & Qwen/Qwen2.5-0.5B & 8707 \\
 & Qwen/Qwen2.5-0.5B-Instruct & 8707 \\
 & Qwen/Qwen2.5-1.5B & 8707 \\
 & Qwen/Qwen2.5-1.5B-Instruct & 8707 \\
 & Qwen/Qwen2.5-14B & 8707 \\
 & Qwen/Qwen2.5-14B-Instruct & 8707 \\
 & Qwen/Qwen2.5-32B & 8707 \\
 & Qwen/Qwen2.5-32B-Instruct & 8707 \\
 & Qwen/Qwen2.5-3B & 8707 \\
 & Qwen/Qwen2.5-3B-Instruct & 8707 \\
 & Qwen/Qwen2.5-72B & 8707 \\
 & Qwen/Qwen2.5-7B & 8707 \\
 & Qwen/Qwen2.5-7B-Instruct & 8707 \\
 & google/gemma-2-27b & 8702 \\
 & google/gemma-2-27b-it & 8702 \\
 & google/gemma-2-2b & 8702 \\
 & google/gemma-2-2b-it & 8685 \\
 & google/gemma-2-9b & 8702 \\
 & google/gemma-2-9b-it & 8702 \\
 & meta-llama/Llama-3.2-1B & 8707 \\
 & meta-llama/Llama-3.2-1B-Instruct & 8707 \\
 & meta-llama/Llama-3.2-3B & 8707 \\
 & meta-llama/Llama-3.2-3B-Instruct & 8707 \\
 & meta-llama/Llama-3.3-70B-Instruct & 8706 \\
 & meta-llama/Meta-Llama-3.1-70B & 8685 \\
 & meta-llama/Meta-Llama-3.1-70B-Instruct & 8685 \\
 & meta-llama/Meta-Llama-3.1-8B & 8685 \\
 & meta-llama/Meta-Llama-3.1-8B-Instruct & 8702 \\
 & microsoft/phi-4 & 8706 \\
 & tiiuae/Falcon3-10B-Base & 8706 \\
 & tiiuae/Falcon3-10B-Instruct & 8706 \\
 & tiiuae/Falcon3-1B-Instruct & 8706 \\
 & tiiuae/Falcon3-7B-Base & 8706 \\
 & tiiuae/Falcon3-7B-Instruct & 8706 \\
\bottomrule
\end{tabular}

    \label{tab:Qwen2.5-72B-Instruct}
\end{table}

\begin{table}[]
    \centering
     \caption{Final Sample Count (N) for Meta-Llama-3.1-8B-Instruct on Similarity Computation of Filtered MMLU-Pro}
\begin{tabular}{llr}
\toprule
judge & model & N \\
\midrule
Meta-Llama-3.1-8B-Instruct & HuggingFaceTB/SmolLM2-1.7B & 8702 \\
 & HuggingFaceTB/SmolLM2-1.7B-Instruct & 8701 \\
 & HuggingFaceTB/SmolLM2-135M-Instruct & 8702 \\
 & HuggingFaceTB/SmolLM2-360M-Instruct & 8702 \\
 & Qwen/Qwen2.5-0.5B & 8702 \\
 & Qwen/Qwen2.5-0.5B-Instruct & 8702 \\
 & Qwen/Qwen2.5-1.5B & 8702 \\
 & Qwen/Qwen2.5-1.5B-Instruct & 8702 \\
 & Qwen/Qwen2.5-14B & 8702 \\
 & Qwen/Qwen2.5-14B-Instruct & 8702 \\
 & Qwen/Qwen2.5-32B & 8702 \\
 & Qwen/Qwen2.5-32B-Instruct & 8702 \\
 & Qwen/Qwen2.5-3B & 8702 \\
 & Qwen/Qwen2.5-3B-Instruct & 8702 \\
 & Qwen/Qwen2.5-72B & 8702 \\
 & Qwen/Qwen2.5-72B-Instruct & 8702 \\
 & Qwen/Qwen2.5-7B & 8702 \\
 & Qwen/Qwen2.5-7B-Instruct & 8702 \\
 & google/gemma-2-27b & 8707 \\
 & google/gemma-2-27b-it & 8707 \\
 & google/gemma-2-2b & 8707 \\
 & google/gemma-2-2b-it & 8690 \\
 & google/gemma-2-9b & 8707 \\
 & google/gemma-2-9b-it & 8707 \\
 & meta-llama/Llama-3.2-1B & 8702 \\
 & meta-llama/Llama-3.2-1B-Instruct & 8702 \\
 & meta-llama/Llama-3.2-3B & 8702 \\
 & meta-llama/Llama-3.2-3B-Instruct & 8702 \\
 & meta-llama/Llama-3.3-70B-Instruct & 8701 \\
 & meta-llama/Meta-Llama-3.1-70B & 8690 \\
 & meta-llama/Meta-Llama-3.1-70B-Instruct & 8690 \\
 & meta-llama/Meta-Llama-3.1-8B & 8690 \\
 & microsoft/phi-4 & 8701 \\
 & tiiuae/Falcon3-10B-Base & 8701 \\
 & tiiuae/Falcon3-10B-Instruct & 8701 \\
 & tiiuae/Falcon3-1B-Instruct & 8701 \\
 & tiiuae/Falcon3-7B-Base & 8701 \\
 & tiiuae/Falcon3-7B-Instruct & 8701 \\
\bottomrule
\end{tabular}
    \label{tab:Meta-Llama-3.1-8B-Instruct}
\end{table}

\begin{table}[]
    \centering
     \caption{Final Sample Count (N) for Meta-Llama-3.1-70B-Instruct on Similarity Computation of Filtered MMLU-Pro}
\begin{tabular}{llr}
\toprule
judge & model & N \\
\midrule
Meta-Llama-3.1-70B-Instruct & HuggingFaceTB/SmolLM2-1.7B & 8685 \\
 & HuggingFaceTB/SmolLM2-1.7B-Instruct & 8684 \\
 & HuggingFaceTB/SmolLM2-135M-Instruct & 8685 \\
 & HuggingFaceTB/SmolLM2-360M-Instruct & 8685 \\
 & Qwen/Qwen2.5-0.5B & 8685 \\
 & Qwen/Qwen2.5-0.5B-Instruct & 8685 \\
 & Qwen/Qwen2.5-1.5B & 8685 \\
 & Qwen/Qwen2.5-1.5B-Instruct & 8685 \\
 & Qwen/Qwen2.5-14B & 8685 \\
 & Qwen/Qwen2.5-14B-Instruct & 8685 \\
 & Qwen/Qwen2.5-32B & 8685 \\
 & Qwen/Qwen2.5-32B-Instruct & 8685 \\
 & Qwen/Qwen2.5-3B & 8685 \\
 & Qwen/Qwen2.5-3B-Instruct & 8685 \\
 & Qwen/Qwen2.5-72B & 8685 \\
 & Qwen/Qwen2.5-72B-Instruct & 8685 \\
 & Qwen/Qwen2.5-7B & 8685 \\
 & Qwen/Qwen2.5-7B-Instruct & 8685 \\
 & google/gemma-2-27b & 8690 \\
 & google/gemma-2-27b-it & 8690 \\
 & google/gemma-2-2b & 8690 \\
 & google/gemma-2-2b-it & 8707 \\
 & google/gemma-2-9b & 8690 \\
 & google/gemma-2-9b-it & 8690 \\
 & meta-llama/Llama-3.2-1B & 8685 \\
 & meta-llama/Llama-3.2-1B-Instruct & 8685 \\
 & meta-llama/Llama-3.2-3B & 8685 \\
 & meta-llama/Llama-3.2-3B-Instruct & 8685 \\
 & meta-llama/Llama-3.3-70B-Instruct & 8684 \\
 & meta-llama/Meta-Llama-3.1-70B & 8707 \\
 & meta-llama/Meta-Llama-3.1-8B & 8707 \\
 & meta-llama/Meta-Llama-3.1-8B-Instruct & 8690 \\
 & microsoft/phi-4 & 8684 \\
 & tiiuae/Falcon3-10B-Base & 8684 \\
 & tiiuae/Falcon3-10B-Instruct & 8684 \\
 & tiiuae/Falcon3-1B-Instruct & 8684 \\
 & tiiuae/Falcon3-7B-Base & 8684 \\
 & tiiuae/Falcon3-7B-Instruct & 8684 \\
\bottomrule
\end{tabular}
    \label{tab:Meta-Llama-3.1-70B-Instruct}
\end{table}

\begin{table}[]
    \centering
     \caption{Final Sample Count (N) for Llama-3.3-70B-Instruct on Similarity Computation of Filtered MMLU-Pro}
\begin{tabular}{llr}
\toprule
judge & model & N \\
\midrule
Llama-3.3-70B-Instruct & HuggingFaceTB/SmolLM2-1.7B & 8706 \\
 & HuggingFaceTB/SmolLM2-1.7B-Instruct & 8707 \\
 & HuggingFaceTB/SmolLM2-135M-Instruct & 8706 \\
 & HuggingFaceTB/SmolLM2-360M-Instruct & 8706 \\
 & Qwen/Qwen2.5-0.5B & 8706 \\
 & Qwen/Qwen2.5-0.5B-Instruct & 8706 \\
 & Qwen/Qwen2.5-1.5B & 8706 \\
 & Qwen/Qwen2.5-1.5B-Instruct & 8706 \\
 & Qwen/Qwen2.5-14B & 8706 \\
 & Qwen/Qwen2.5-14B-Instruct & 8706 \\
 & Qwen/Qwen2.5-32B & 8706 \\
 & Qwen/Qwen2.5-32B-Instruct & 8706 \\
 & Qwen/Qwen2.5-3B & 8706 \\
 & Qwen/Qwen2.5-3B-Instruct & 8706 \\
 & Qwen/Qwen2.5-72B & 8706 \\
 & Qwen/Qwen2.5-72B-Instruct & 8706 \\
 & Qwen/Qwen2.5-7B & 8706 \\
 & Qwen/Qwen2.5-7B-Instruct & 8706 \\
 & google/gemma-2-27b & 8701 \\
 & google/gemma-2-27b-it & 8701 \\
 & google/gemma-2-2b & 8701 \\
 & google/gemma-2-2b-it & 8684 \\
 & google/gemma-2-9b & 8701 \\
 & google/gemma-2-9b-it & 8701 \\
 & meta-llama/Llama-3.2-1B & 8706 \\
 & meta-llama/Llama-3.2-1B-Instruct & 8706 \\
 & meta-llama/Llama-3.2-3B & 8706 \\
 & meta-llama/Llama-3.2-3B-Instruct & 8706 \\
 & meta-llama/Meta-Llama-3.1-70B & 8684 \\
 & meta-llama/Meta-Llama-3.1-70B-Instruct & 8684 \\
 & meta-llama/Meta-Llama-3.1-8B & 8684 \\
 & meta-llama/Meta-Llama-3.1-8B-Instruct & 8701 \\
 & microsoft/phi-4 & 8707 \\
 & tiiuae/Falcon3-10B-Base & 8707 \\
 & tiiuae/Falcon3-10B-Instruct & 8707 \\
 & tiiuae/Falcon3-1B-Instruct & 8707 \\
 & tiiuae/Falcon3-7B-Base & 8707 \\
 & tiiuae/Falcon3-7B-Instruct & 8707 \\
\bottomrule
\end{tabular}
    \label{tab:Llama-3.3-70B-Instruct}
\end{table}

\begin{table}[]
    \centering
     \caption{Final Sample Count (N) for gemma-2-9b-it on Similarity Computation of Filtered MMLU-Pro}
\begin{tabular}{llr}
\toprule
judge & model & N \\
\midrule
gemma-2-9b-it & HuggingFaceTB/SmolLM2-1.7B & 8702 \\
 & HuggingFaceTB/SmolLM2-1.7B-Instruct & 8701 \\
 & HuggingFaceTB/SmolLM2-135M-Instruct & 8702 \\
 & HuggingFaceTB/SmolLM2-360M-Instruct & 8702 \\
 & Qwen/Qwen2.5-0.5B & 8702 \\
 & Qwen/Qwen2.5-0.5B-Instruct & 8702 \\
 & Qwen/Qwen2.5-1.5B & 8702 \\
 & Qwen/Qwen2.5-1.5B-Instruct & 8702 \\
 & Qwen/Qwen2.5-14B & 8702 \\
 & Qwen/Qwen2.5-14B-Instruct & 8702 \\
 & Qwen/Qwen2.5-32B & 8702 \\
 & Qwen/Qwen2.5-32B-Instruct & 8702 \\
 & Qwen/Qwen2.5-3B & 8702 \\
 & Qwen/Qwen2.5-3B-Instruct & 8702 \\
 & Qwen/Qwen2.5-72B & 8702 \\
 & Qwen/Qwen2.5-72B-Instruct & 8702 \\
 & Qwen/Qwen2.5-7B & 8702 \\
 & Qwen/Qwen2.5-7B-Instruct & 8702 \\
 & google/gemma-2-27b & 8707 \\
 & google/gemma-2-27b-it & 8707 \\
 & google/gemma-2-2b & 8707 \\
 & google/gemma-2-2b-it & 8690 \\
 & google/gemma-2-9b & 8707 \\
 & meta-llama/Llama-3.2-1B & 8702 \\
 & meta-llama/Llama-3.2-1B-Instruct & 8702 \\
 & meta-llama/Llama-3.2-3B & 8702 \\
 & meta-llama/Llama-3.2-3B-Instruct & 8702 \\
 & meta-llama/Llama-3.3-70B-Instruct & 8701 \\
 & meta-llama/Meta-Llama-3.1-70B & 8690 \\
 & meta-llama/Meta-Llama-3.1-70B-Instruct & 8690 \\
 & meta-llama/Meta-Llama-3.1-8B & 8690 \\
 & meta-llama/Meta-Llama-3.1-8B-Instruct & 8707 \\
 & microsoft/phi-4 & 8701 \\
 & tiiuae/Falcon3-10B-Base & 8701 \\
 & tiiuae/Falcon3-10B-Instruct & 8701 \\
 & tiiuae/Falcon3-1B-Instruct & 8701 \\
 & tiiuae/Falcon3-7B-Base & 8701 \\
 & tiiuae/Falcon3-7B-Instruct & 8701 \\
\bottomrule
\end{tabular}
    \label{tab:gemma-2-9b-it}
\end{table}

\begin{table}[]
    \centering
     \caption{Final Sample Count (N) for gemma-2-27b-it on Similarity Computation of Filtered MMLU-Pro}
\begin{tabular}{llr}
\toprule
judge & model & N \\
\midrule
gemma-2-27b-it & HuggingFaceTB/SmolLM2-1.7B & 8702 \\
 & HuggingFaceTB/SmolLM2-1.7B-Instruct & 8701 \\
 & HuggingFaceTB/SmolLM2-135M-Instruct & 8702 \\
 & HuggingFaceTB/SmolLM2-360M-Instruct & 8702 \\
 & Qwen/Qwen2.5-0.5B & 8702 \\
 & Qwen/Qwen2.5-0.5B-Instruct & 8702 \\
 & Qwen/Qwen2.5-1.5B & 8702 \\
 & Qwen/Qwen2.5-1.5B-Instruct & 8702 \\
 & Qwen/Qwen2.5-14B & 8702 \\
 & Qwen/Qwen2.5-14B-Instruct & 8702 \\
 & Qwen/Qwen2.5-32B & 8702 \\
 & Qwen/Qwen2.5-32B-Instruct & 8702 \\
 & Qwen/Qwen2.5-3B & 8702 \\
 & Qwen/Qwen2.5-3B-Instruct & 8702 \\
 & Qwen/Qwen2.5-72B & 8702 \\
 & Qwen/Qwen2.5-72B-Instruct & 8702 \\
 & Qwen/Qwen2.5-7B & 8702 \\
 & Qwen/Qwen2.5-7B-Instruct & 8702 \\
 & google/gemma-2-27b & 8707 \\
 & google/gemma-2-2b & 8707 \\
 & google/gemma-2-2b-it & 8690 \\
 & google/gemma-2-9b & 8707 \\
 & google/gemma-2-9b-it & 8707 \\
 & meta-llama/Llama-3.2-1B & 8702 \\
 & meta-llama/Llama-3.2-1B-Instruct & 8702 \\
 & meta-llama/Llama-3.2-3B & 8702 \\
 & meta-llama/Llama-3.2-3B-Instruct & 8702 \\
 & meta-llama/Llama-3.3-70B-Instruct & 8701 \\
 & meta-llama/Meta-Llama-3.1-70B & 8690 \\
 & meta-llama/Meta-Llama-3.1-70B-Instruct & 8690 \\
 & meta-llama/Meta-Llama-3.1-8B & 8690 \\
 & meta-llama/Meta-Llama-3.1-8B-Instruct & 8707 \\
 & microsoft/phi-4 & 8701 \\
 & tiiuae/Falcon3-10B-Base & 8701 \\
 & tiiuae/Falcon3-10B-Instruct & 8701 \\
 & tiiuae/Falcon3-1B-Instruct & 8701 \\
 & tiiuae/Falcon3-7B-Base & 8701 \\
 & tiiuae/Falcon3-7B-Instruct & 8701 \\
\bottomrule
\end{tabular}
    \label{tab:gemma-2-27b-it}
\end{table}

\begin{table}[]
    \centering
     \caption{Final Sample Count (N) for Ministral-8B-Instruct-2410 on Similarity Computation of Filtered MMLU-Pro}
\begin{tabular}{llr}
\toprule
judge & model & N \\
\midrule
Ministral-8B-Instruct-2410 & HuggingFaceTB/SmolLM2-1.7B & 8706 \\
 & HuggingFaceTB/SmolLM2-1.7B-Instruct & 8707 \\
 & HuggingFaceTB/SmolLM2-135M-Instruct & 8706 \\
 & HuggingFaceTB/SmolLM2-360M-Instruct & 8706 \\
 & Qwen/Qwen2.5-0.5B & 8706 \\
 & Qwen/Qwen2.5-0.5B-Instruct & 8706 \\
 & Qwen/Qwen2.5-1.5B & 8706 \\
 & Qwen/Qwen2.5-1.5B-Instruct & 8706 \\
 & Qwen/Qwen2.5-14B & 8706 \\
 & Qwen/Qwen2.5-14B-Instruct & 8706 \\
 & Qwen/Qwen2.5-32B & 8706 \\
 & Qwen/Qwen2.5-32B-Instruct & 8706 \\
 & Qwen/Qwen2.5-3B & 8706 \\
 & Qwen/Qwen2.5-3B-Instruct & 8706 \\
 & Qwen/Qwen2.5-72B & 8706 \\
 & Qwen/Qwen2.5-72B-Instruct & 8706 \\
 & Qwen/Qwen2.5-7B & 8706 \\
 & Qwen/Qwen2.5-7B-Instruct & 8706 \\
 & google/gemma-2-27b & 8701 \\
 & google/gemma-2-27b-it & 8701 \\
 & google/gemma-2-2b & 8701 \\
 & google/gemma-2-2b-it & 8684 \\
 & google/gemma-2-9b & 8701 \\
 & google/gemma-2-9b-it & 8701 \\
 & meta-llama/Llama-3.2-1B & 8706 \\
 & meta-llama/Llama-3.2-1B-Instruct & 8706 \\
 & meta-llama/Llama-3.2-3B & 8706 \\
 & meta-llama/Llama-3.2-3B-Instruct & 8706 \\
 & meta-llama/Llama-3.3-70B-Instruct & 8707 \\
 & meta-llama/Meta-Llama-3.1-70B & 8684 \\
 & meta-llama/Meta-Llama-3.1-70B-Instruct & 8684 \\
 & meta-llama/Meta-Llama-3.1-8B & 8684 \\
 & meta-llama/Meta-Llama-3.1-8B-Instruct & 8701 \\
 & microsoft/phi-4 & 8707 \\
 & tiiuae/Falcon3-10B-Base & 8707 \\
 & tiiuae/Falcon3-10B-Instruct & 8707 \\
 & tiiuae/Falcon3-1B-Instruct & 8707 \\
 & tiiuae/Falcon3-7B-Base & 8707 \\
 & tiiuae/Falcon3-7B-Instruct & 8707 \\
\bottomrule
\end{tabular}
    \label{tab:Ministral-8B-Instruct-2410}
\end{table}


\subsection{Prompts}

\subsubsection{LM-Judge Prompt without Reference Answer}\label{prompt:judge}
\begin{tcolorbox}[colback=white, colframe=black, title=Prompt for Free-Form Evaluation]
\texttt{Your task is to judge whether the given response to a question is correct or not. You are only given a question and the response you are judging.  \\ \\
Possible judgments: \\
"0": The response is incorrect. \\
"1": The response is correct. \\ \\
Question: "[Insert the question here]" \\
Response: "[Insert the response here]" \\ \\
To the best of your knowledge: Does the provided response answer the question correctly? This is part of an automated evaluation process, therefore you must only output a single word: "0" or "1". Do not justify your decision. \\ \\
Evaluation (0/1):}
\end{tcolorbox}

\subsubsection{LM-Judge Prompt with MCQ Options}\label{prompt:judge_w_references}
\begin{tcolorbox}[colback=white, colframe=black, title=Prompt for Free-Form Evaluation with Access to MCQ Reference Options]
\texttt{Your task is to judge whether the given response to a question is correct or not. You are given a question, a ground truth response, incorrect options and the response you are judging. \\ \\
Possible judgments: \\
"0": The response is incorrect. It does not match the ground-truth answer or is more similar to any of the incorrect options than to the ground-truth answer. \\
"1": The response is correct. It matches the ground-truth. \\ \\
Question: "[Insert the question here]" \\
Ground truth: "[Insert the ground-truth option here]" \\
Incorrect option (1): "[Insert the 1st wrong option here]" \\
... \\
Incorrect option (9): "[Insert the 9th wrong option here]" \\
Response: "[Insert the response here]" \\ \\
To the best of your knowledge: Does the provided response answer the question correctly, taking the ground-truth and wrong answer options into account? This is part of an automated evaluation process, therefore you must only output a single word: "0" or "1". Do not justify your decision.\\ \\
Evaluation (0/1):}
\end{tcolorbox}

\subsubsection{Original MCQ CoT Prompt}\label{prompt:cot_mcq}

We describe how an original MCQ prompt on MMLU-Pro is transformed into an open-style prompt for free-form inference without access to the reference options. The original chain-of-thought (CoT) prompt consists of general information about the task, a few-shot list of questions-answer pairs and finally the actual question that is to be solved. 

Each question is preceded by the keyword ``Question:'', followed by the question text and the list of answer options. Every option text is marked with a letter. Next, a reference chain-of-thought is given after the key-phrase ``Answer: Let's think step by step'' to provide an in-context example on how to solve related questions. This CoT can include references to the answer options. The CoT answer ends with the key-phrase ``The answer is (X)'' where ``X'' is the letter of the correct option. The phrase nudges the evaluated LM to answer in the same way, allowing to extract the final response using regex matching. The number of in-context examples depends on the \texttt{--num\_fewshot} parameter. In our experiment, we use five examples, but for reasons of brevity, only a single one is part of the example prompt below. Finally, the phrase that starts a CoT is repeated right before the model's response.

We automatically transform these MCQ into OSQ CoT prompts. The general information is slightly adjusted to indicate the type of task. All key-phrases remain the same. We completely omit the MCQ options at the end behind the question. Any reference to an option in the chain-of-thought is replaced with the option text itself -- e.g. ``(G)'' is replaced with the corresponding ``(The second and third pharyngeal arches)''. This includes the final response: ``The answer is (XYZ).''. Our experiments have shown that even the smallest models evaluated are able to follow these instructions and provide free-form responses that can be automatically extracted in the vast majority of cases. 

\begin{tcolorbox}[colback=white, colframe=black, title=Few-shot CoT MCQ Prompt]
\texttt{The following are multiple choice questions (with answers) about health. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice. \\ \\
Question: What is the embryological origin of the hyoid bone?\\
Options:\\
A. The third and fourth pharyngeal arches\\
B. The fourth pharyngeal arch\\
C. The third pharyngeal arch\\
D. The second pharyngeal arch\\
E. The second, third and fourth pharyngeal arches\\
F. The first pharyngeal arch\\
G. The second and third pharyngeal arches\\
H. The first and third pharyngeal arches\\
I. The first, second and third pharyngeal arches\\
J. The first and second pharyngeal arches\\ \\
Answer: Let's think step by step. We refer to Wikipedia articles on anatomy for help. Let’s solve this problem step by step. The hyoid bone, which is also known as the hyooid, is a a small U-shaped bone located in the anterior neck. In its resting position, it lies between the base of the mandible and the third cervical vertebrae. We know that the second and the third pharyngeal arches give rise to the horns of the hyoid bone; therefore, the embryological origin of the hyoid bone are the second and the third pharyngeal arches—this information is covered in option (G). Therefore, we conclude that (G) must be the correct answer. The answer is (G) \\ \\
Question: \dots \\ \\
Question: Which disease do polyomaviruses predominantly cause?  \\
Options:\\
A. Tumours\\
B. Brain pathology\\
C. No disease at all\\
D. Kidney infections\\ \\
Answer: Let's think step by step.}
\end{tcolorbox}

\subsubsection{Open-style CoT Prompt}\label{prompt:cot_osq}
\begin{tcolorbox}[colback=white, colframe=black, title=Few-shot CoT OSQ Prompt]
\texttt{The following are multiple choice questions (with answers) about health. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice. \\ \\
Question: What is the embryological origin of the hyoid bone?\\ \\
Answer: Let's think step by step. We refer to Wikipedia articles on anatomy for help. Let’s solve this problem step by step. The hyoid bone, which is also known as the hyooid, is a a small U-shaped bone located in the anterior neck. In its resting position, it lies between the base of the mandible and the third cervical vertebrae. We know that the second and the third pharyngeal arches give rise to the horns of the hyoid bone; therefore, the embryological origin of the hyoid bone are the second and the third pharyngeal arches—this information is covered in option (The second and third pharyngeal arches). Therefore, we conclude that (The second and third pharyngeal arches) must be the correct answer. The answer is (The second and third pharyngeal arches) \\ \\
Question: \dots \\ \\
Question: Which disease do polyomaviruses predominantly cause?  \\ \\
Answer: Let's think step by step.}
\end{tcolorbox}

These are the two prompts used for coarse and fine-grained filtering to get the OSQ version of MMLU-Pro. They almost exactly match the original ones provided by \citet{myrzakhan2024openllmleaderboard}, but we performed minimal adjustments to make them more suitable to MMLU-Pro.

\subsubsection{Coarse Filtering Prompt}\label{prompt:coarse_filter}
\begin{tcolorbox}[colback=white, colframe=black, title=Coarse Prompt]
\texttt{Your task is to review a series of multiple-choice questions and evaluate their ability to be answered without the provided answer choices. \\ \\ 
For questions that begin with an incomplete sentence (e.g., "During swallowing, ..."), use your knowledge to attempt to complete the sentence accurately. For direct questions that ask for specific information or identification (e.g., "Which of the following structures is part of the small intestine?"), assess whether the question is formulated clearly enough that an informed answer can be given without seeing the multiple-choice options. For mathematical or analytical questions (e.g., "Find all cosets of the subgroup 4Z of 2Z"), determine if the question provides enough context and information for a solution to be formulated without additional options. \\ \\
Please follow this format for your evaluation: \\ \\
QUESTION: [Insert the question here] \\ \\
VERDICT: Respond with "YES" if the question is clear and can be directly answered based on its content alone, or "NO" if it relies on the answer choices to be understood or answered. Your response should include only the verdict without any justification or reasoning.}
\end{tcolorbox}

\subsubsection{Fine-grained Filtering Prompt}\label{prompt:fine_filter}
\begin{tcolorbox}[colback=white, colframe=black, title=Fine-grained Prompt]
\texttt{You will assign a numerical score from 1 to 10 based on how confidently it can be answered without the choices. The scoring criteria are as follows: \\ \\
1: The question is entirely dependent on its choices for an answer, making it impossible to answer without them. Example: ‘Which of the following statements is correct?’ \\ \\
10: The question can be easily and confidently answered based solely on the question stem,without any need to refer to the provided options. Example: ‘What is the first law of thermodynamics in physics?’ \\ \\
Intermediate Scores: \\ \\
2-4: The question stem gives very little information and is highly reliant on the choices forcontext. Example: ‘Which of these is a prime number?’ 'The \_\_\_\_\_\_\_\_ perspective on sustainability resulted from growth models that analysed the carrying capacity of the planet, overall concluding that the finite capacity of the earth and \_\_\_\_\_\_\_, \_\_\_\_\_\_\_\_ and \_\_\_\_\_\_\_ by current and past generations could reduce quality of life for future generations.'\\
5: The question provides some context or information, that gives a moderate possibility to answer the question. Example: ‘Which of the following best describes the structure that collects urine in the body?’ \\
6: The question provides a good amount of context or information, that gives a moderate possibility to answer the question. Example: ‘Statement 1 | A factor group of a non-Abelian group is non-Abelian. Statement 2 | If K is a normal subgroup of H and H is a normal subgroup of G, then K is a normal subgroup of G.’ \\
7: The question provides a good amount of context or information, that gives a high possibility to answer the question. Example: ‘The element (4, 2) of Z\_12 x Z\_8 has order’ \\
8-9: The question provides a good amount of context or information, that gives a high possibility to answer the question. Example: ‘A "dished face" profile is often associated with’ \\ \\
ONLY GIVE THE VALUE BETWEEN 1-10 AS YOUR ANSWER. DO NOT INCLUDE ANY OTHER INFORMATION IN YOUR RESPONSE.}
\end{tcolorbox}

%if extra time add final datasample count per model, and count of exclusions (ilze)

