\section{Metrics}
\label{app:metrics}
The following section covers design details for CAPA $\goelpi$. Firstly, we address derivation of CAPA in Section~\ref{app:goelpi_derivation} and its theoretical bounds in Section~\ref{app:goelpi_bounds}. Secondly, we explain how to extend CAPA to multi-model set-up (Section~\ref{app:metric_multi}) and how to adapt CAPA to classification and exact match settings (Section~\ref{app:goelspi_classification}). Lastly, we introduce probabilistic versions of popular agreement metrics (Section~\ref{sec:probabilistic_metrics}) and provide a comparison between them and CAPA (Section~\ref{sec:app_metric_alternatives}). 

\subsection{Derivation of CAPA}
\label{app:goelpi_derivation}
CAPA is intended to be used as a similarity metric in the context of model accuracies. As such, it extends Error consistency \cite{geirhos2020beyond}, a metric that adjusts chance agreement by taking into account model accuracy. In particular, the same formula is used to define Cohen's $\kappa$, Scott's $\pi$, Fleiss' $\kappa$, Error Consistency and  CAPA:
\begin{align}
    \frac{\textrm{observed agreement} - \textrm{chance agreement}}{\textrm{maximum possible agreement - chance agreement}},
\end{align}
 where the excess agreement is subtracted both from the numerator and denominator, essentially calculating the proportion of possible excess agreement that is observed between the two models. Across all metrics, the maximum possible agreement is $1$. Where CAPA differs from the existing metrics is how we calculate the observed and change agreement. 

We redefine \textit{error consistency} \cite{geirhos2020beyond} by incorporating probabilistic information. To achieve this we introduce a probabilistic computation of the observed agreement, $\cobs$ as $\cobsp$, and the chance agreement, $\cexp$ as $\cexpp$. The new equation becomes: 
\begin{align}
    \goelpi = \frac{\cobsp - \cexpp}{1 - \cexpp}.
\end{align}

\paragraph{Observed agreement $\cobsp$}: Given that we have the predicted output probabilities, $p_1(o_*)_x$, by a LM for all possible options, $O_x = [o_1, \dots, o_N]$,  for a data sample $x$, e.g. $p_1(o_*)_x \forall o_* \in O_x$, we can compute the relative observed overlap as:
\begin{align}
    \cobsp \;=\; \frac{1}{|D|} \sum_{x=1}^D \sum_{i=1}^O p_1(o_i)_x \cdot p_2(o_i)_x
\end{align}

where $p_1(\cdot)$ is the predicted probability by model 1 and $p_2(\cdot)$ is the predicted probability by model 2. We would like to highlight that the above calculation is performed on \textbf{sample level} to avoid confusion with the common chance agreement $p_e$ calculation in Cohen's kappa \footnote{Cohen's kappa uses the marginal probabilities across categories to estimate $p_e$. However, in MCQ there are no 'class categories' as the options can be permuted across data samples. Therefore, marginal probabilities cannot be estimated.}.  

\paragraph{Agreement by chance $\cexpp$} To estimate the model chance agreement $\cexpp$ we first start by computing the average probability that a given model is correct $\overline{p_*}$:
\begin{align}
    \overline{p_*} = \frac{1}{|D|} \sum_{x=1}^{D}\sum_{i=1}^O \mathbb{I}[o_i=\text{gt}]p_*(o_i)_x \: \text{where} \: \text{gt} = \text{ground truth}
\end{align}
Performing the above calculation per model accounts for the possibility that each model may have different marginal distributions. An assumption that is fair to assume in the context of LMs. Subsequently, given the $\overline{p_*}$ per model we can compute the probability that two models are \textbf{correct} by chance as: $\overline{p_1} \cdot \overline{p_2}$. Conversely, to account for model chance disagreement we (1) group all the remaining options as \textbf{incorrect} and (2) adjust for the number of options: $\frac{1}{|D|}\sum_{x=1}^D\frac{1}{|O_x|-1}(1-\overline{p_1})(1-\overline{p_2})$. These steps are necessary because (1) MCQ options can be permuted, therefore, class marginal probabilities cannot be computed, and (2) the chance disagreement without adjusting for the number of options overestimates the agreement by chance:
\begin{align}
    0 < \frac{1}{|D|}\sum_{x=1}^D\frac{1}{|O_x|-1}(1-\overline{p_1})(1-\overline{p_2}) \leq (1-\overline{p_1})(1-\overline{p_2})  
\end{align}
In particular, if the number of options is ignored then the underlying assumption is that both models put their 'incorrect' probability mass on the same option, following a Dirac delta $\delta(o_*)$ distribution. This is a very strong assumption, that overestimates model error agreement. Therefore, we propose to adjust this by assuming that the distribution for the incorrect options follows a uniform distribution $\mathbf{U}\{o_1,o_{n-1}\}$ as adjusted by our normalizing factor $\frac{1}{|D|}\sum_{x=1}^D\frac{1}{|O_x|-1}$, where $|O_x|$ is the total number of options for a sample $x$. As such, the overall agreement by chance probability is:
\begin{align}
    \cexpp = \underbrace{\overline{p_1} \overline{p_2}}_{\text{chance agreement correct}} + \underbrace{\frac{1}{|D|} \sum_{x=1}^{D}}_{\text{mean}} \underbrace{\frac{1}{|O_x|-1}}_{\substack{\text{uniformity assumption}}} \underbrace{(1 - \overline{p_1})(1 - \overline{p_2})}_{\text{chance agreement incorrect}} 
\end{align}

 Moreover, for perfectly calibrated models the mean correct probability $\overline{p_*}$ would approach model accuracy, $\overline{p_*} \rightarrow \hat{p_*} $ and is upper bounded by it $\overline{p_*} < \hat{p_*}$ as $\overline{p_*}$ is computed based on probabilities ($\hat{p_*}=\frac{TP+TN}{|D|}$). 

 
\paragraph{Reduction of CAPA to Error Consistency for binary classification}
\label{sec:kappareduction}
In binary classification setting when the underlying probabilities are unavailable CAPA reduces to error consistency, as (1) $\cobsp = \frac{1}{|D|}\sum_{x=1}^D \mathbb{I}[\text{arg max }p_1 =\text{arg max }p_2] = \cobs$, and (2) $\cobsp = \cobs$ as $\overline{p_*}=acc_*$, and the normalizing factor simplifies to 1. 


\subsection{Extending CAPA to more than two models}
\label{app:metric_multi}
In Section~\ref{sec:metric_ours}, we computed functional similarity between a pair of models. Here, we extend CAPA to multi-model comparisons. In the inter-annotator agreement literature, Fleiss' $\kappa$~\citep{fleiss1981measurement} is commonly used for this. However, it is ill suited to our modeling paradigm as it defines $\cexpp$ using the assumptions of Scott's $\pi$ instead of Cohen's $\kappa$ (this is problematic when measuring model similarity as discussed in the previous section). We derive CAPA for more than two models using first principles logic, similar to how Fleiss' $\kappa$ was derived.

Suppose the number of models is $M>2$. We still use the $\frac{\cobsp - \cexpp}{1 - \cexpp}$ formula, but change the definition of $\cobsp$ and $\cexpp$. For $\cobsp$, Fleiss' $\kappa$ measures the proportion of observed pairwise agreements from the total possible for each question, averaging across questions. This is equivalent to averaging the observed agreements for each pair of models when all models annotate all questions, which is true in our case\footnote{Fleiss $\kappa$ is also defined when not all annotators respond to every question, as long as the number of respondents per question is fixed.}. This gives us $\cobsp \;=\; \frac{2}{M(M-1)} \sum_{1 \leq i < j \leq M} \frac{1}{|D|} \sum_{x=1}^D \sum_{k=1}^O p_i(o_k)_x \cdot p_j(o_k)_x$. 

Second, for $\cexpp$, Fleiss' $\kappa$ measures the expected pairwise agreements if all $M$ models were independent. This can be obtained by averaging the $\cexpp$ for two models across all possible pairs of $M$ models. This gives us $$\cexpp = \frac{2}{M(M-1)}\sum_{1 \leq i < j \leq M} (\overline{p_i} \cdot \overline{p_j} + (1 - \overline{p_i}) \cdot (1 - \overline{p_j}) \cdot (\frac{1}{|D|} \sum_{x=1}^D \frac{1}{|O_x|-1}))$$.

\subsection{How to use CAPA for classification and exact match settings?}
\label{app:goelspi_classification}
In Section~\ref{sec:metric_ours} we defined CAPA for MCQs as this is used throughout the paper, and more commonly for language models. For completeness, we now define CAPA for classification settings and exact match settings, which are alternate strategies for evaluating models.

\textbf{Classification}: Unlike MCQs, in this setting are coherent classes (categories), representing nominal data. The model output now is a probability distribution over $C$ classes. Therefore, we compute $\cobsp$ across categories as follows:

\begin{equation}
    \cobsp \;=\; \frac{1}{|D|} \sum_{x \in D} \sum_{c_i \in C(x)} p_1(c_i) \cdot p_2(c_i),
\end{equation}

where $p_*(c_i)$ is the output probability for class $c_i$. 
For the computation of $\cexpp$ we follow the same definition as in the main paper, but now $\overline{p_j}$ is computed for the correct class and the chance agreement on the incorrect class is adjusted by the number of classes instead of number of options:
\begin{align}
    \cexpp &= \underbrace{\overline{p_1} \cdot \overline{p_2}}_{\text{chance agreement on correct class}} + \\ 
    &\underbrace{(1 - \overline{p_1}) \cdot (1 - \overline{p_2}) \cdot \frac{1}{|D|} \sum_{x \in D} \frac{1}{|C(x)|-1}}_{\text{chance agreement on incorrect class}}
\end{align}
In principle, the above implementation could also be adjusted to further take into account the class categories by computing the marginal probabilities per class as:
\begin{align}
    \overline{p(c_i)_*} = \frac{1}{|D|} \sum_{i=1}^{D}p_*(c_i) \: \text{where} \: c_i \neq \text{ground truth,}
\end{align}
and replacing the chance agreement on incorrect class with the product of per class 'incorrect' probabilities. 

\textbf{Exact or Fuzzy Match}: Here, models are not provided categories or options to choose between, and instead provide an answer from an unconstrained set. The model's output string is matched with a reference answer. Here, the probability of independent models agreeing by chance approaches zero due to an unconstrained set of outputs. Further computing probabilistic agreement is difficult over conditional distributions across multiple tokens. We recommend calculating the discrete version of CAPA, where $\cobs^{EM} = \frac{1}{|D|} \sum_{x=1}^D \mathbb{I}[m_1(x) == m_2(x)]$, and $\cexp^{EM} = acc_1 \cdot acc_2$, finally computing $\frac{\cobs^{EM} - \cexp^{EM}}{1 - \cexp^{EM}}$. 

\subsection{Detailed Discussion on Design Choices}
\label{sec:app_metric_alternatives}

In this section, we discuss alternative design choices we could have taken. For an overview of the equations for each metric, see table~\ref{tab:kappa_pi}.

\begin{table}
    \centering
    \begin{tabular}{l c c}
        \toprule
        \textbf{Metric} & \textbf{Formula} & \textbf{Description} \\
        \midrule
        Cohen's Kappa & $\kappa = \frac{P_o - P_e}{1 - P_e}$ & Measures inter-rater reliability $P_0$ while accounting for chance agreement $P_e$. \\[10pt]
        Scott's Pi & $\pi = \frac{P_o - P_e}{1 - P_e}$ & Similar to Kappa, but uses marginal probabilities for $P_e$. \\
        Error Consistency &  $k = \frac{\cobs - \cexp}{1 - \cexp}$ & Adjusts for accuracy via $\cexp = acc_1 \cdot acc_2 + (1-acc_1)(1-acc_2)$ \\ 
        CAPA & $\goelpi = \frac{\cobsp - \cexpp}{1 - \cexpp}$ & Accounts for sample level probabilities $\cobsp = \frac{1}{|D|}\sum^D_{x=1} \sum^{O}_{i=1}p_1(o_i)_x\cdot p_2(o_i)_x$\\ 
        & & and accounts for accuracy via $\cexpp = \overline{p}_1 \cdot \overline{p}_2 + \frac{1}{|D|}\sum^{D}_{x=1}\frac{1}{|O_x|-1}(1-\overline{p}_1)(1-\overline{p}_2)$ \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of different inter-rater metrics}
    \label{tab:kappa_pi}
\end{table}

\textbf{Why not use inter-annotator agreement metrics like Cohen’s $\kappa$?} Cohen’s $\kappa$, Scott's $\pi$, Krippendorf's $\alpha$ measure how people differ when answering survey questions, focusing on the reliability of those questions and the data~\citep{krippendorff2004reliability}. They assume nominal data and computes marginal probability distributions per category. However, MCQs do not have an inherent category `a' or `b', i.e. options can be permuted, so we cannot compute such marginal probability distributions. Moreover, measuring LM similarity requires adjusting for chance agreement due to accuracy to avoid inflating similarity for high-accuracy models~\citep{geirhos2020beyond}. For inter-annotator agreement metrics stemming from human survey studies — where there is no built-in concept of accuracy — and thus they are unsuitable for LM analysis without additional modification.

\textbf{Should $\cexpp$ be defined similarly to Cohen’s $\kappa$ or Scott’s $\pi$?} When measuring similarity between LM Judges and human annotators,~\citet{thakur2024judgingjudgesevaluatingalignment} recommend using Scott’s $\pi$ over Cohen’s $\kappa$, as it is a better metric for inter-annotator agreement studies~\citep{krippendorff2004reliability}. The two differ in how they compute $\cexp$, Scott's $\pi$ assumes that the two human raters are sampled from a common distribution, estimating it by averaging the marginal probabilities of the two raters. This is in contrast to Cohen's $\kappa$, which assumes the given different marginal distributions for the two raters. In our case, we wish to account for chance agreement due to accuracies rather than the marginal distribution over classes. To see the relative comparison of how Cohen's $\kappa$ and Scott's $\pi$ behave in our setting, we consider an example. 

Suppose we have a binary classification problem, where both models always agree when they are both wrong as there is only one incorrect option. We now consider two pairs of models. Pair 1 has accuracies $0.2, 0.8$, whereas in pair 2, both models have accuracies $0.5$. Intuitively, if both pairs were to have the same observed agreement, it would be more surprising if this happened for pair 1 than pair 2, given the vast difference in their accuracy. In other words, models in pair 2 are more similar than expected for independent models with the given accuracies than pair 1. We want this to be reflected in our similarity metric.

For pair 1, Scott's $\pi$, $\cexp$ would be computed assuming a joint accuracy of $\frac{0.2 + 0.8}{2} = 0.5$, and for pair 2 with the same joint accuracy $\frac{0.5 + 0.5}{2} = 0.5$, giving $\cexp = 0.5^2 + (1-0.5)^2 = 0.5$. Cohen's $\kappa$ of pair 2 would be computed as $0.5 \cdot 0.5 + (1-0.5) \cdot (1 - 0.5) = 0.5$ too. However, for Cohen's $\kappa$ of pair 1, $\cexp = 0.2 \cdot 0.8 + 0.8 \cdot 0.2 = 0.32$. This means for a fixed observed agreement $\cobs$, say $0.5$, $\pi = \frac{0.5 - 0.5}{1 - 0.5} = 0$ for both models, and similarly $\kappa = 0$ for pair 2. However, for pair 1, $\kappa = \frac{0.5 - 0.32}{1 - 0.32} = 0.264$. Indeed, Scott's $\pi$ would lead us to think both pairs are equally similar, whereas $\kappa$ indicates pair 2 is more similar, beyond chance agreement arising due to accuracy. Thus $\kappa$ has the more desirable behavior. 

More broadly, we do not wish to assume both models are drawn from a joint distribution, assigning them a common mean accuracy. However, Scott's $\pi$ does this, which makes sense when calculating reliability of surveys or measuring alignment between human and LLM judges. However, this does not make sense in our setting where we wish to adjust for chance agreement expected due to the two model's given accuracies. Hence, we choose to define $\cexp$ similar to Cohen's $\kappa$, where we retain the difference in the two model's accuracies when computing chance agreement.

\textbf{Why not use Matthews Correlation Coefficient}: We could take a completely different approach by computing the Pearson or Matthews Correlation Coefficient of the binary vectors of sample-wise correctness for the two models \citep{chicco2021matthews}. However, it would be difficult to incorporate probabilistic information, and that models can be incorrect and still disagree by predicting different options. In other words, it suffers from the same issues as error consistency, and we found it more difficult to extend.

\textbf{Why not use regression analysis?} We could have performed a multinomial regression using the probabilities of the first model to predict probabilities of the second model, using this predictability as a measure of similarity. However, it is unclear whether a linear model would be enough. Ideally this prediction should also be contextualized on the input sample, but for this we would need a model-based metric to obtain a representation of the input sample. We chose to stick to a more interpretable, closed-form metric.

\textbf{Why not use divergence metrics like KL or JSD?} KL-divergence or Jensen–Shannon Distance (JSD) can measure the divergence between probability distributions assigned by models to the options with lucrative information-theoretic properties. Further, JSD is a valid distance metric, and normalized between $0$ and $1$. We could use the mean JSD over all questions as a model similarity metric. However, higher-accuracy models are expected to have lower JSD simply because they have more correct answers, i.e, end up assigning more probability mass to correct options across samples. Retaining the information-theoretic properties of JSD while adjusting for chance agreement due to accuracy remains an interesting open problem.


\textbf{Why not use JSD of the two distributions instead of overlap in computing CAPA?}: We could have plugged $1 - JSD$ into $\cobsp$ in the $\goelpi$ formula. It is also possible to define $\cexpp$ by computing JSD between the two independent model distributions defined and subtracting from $1$. However, JSD instead of probabilistic overlap is not intuitively interpretable, especially when divided by the possible excess agreement as in $\goelpi$. $\cobsp$ computes the expected agreement when sampling from both models based on the probability distribution they assign to the options. Intuitively, it gives us the fraction of times the two models would agree if we kept sampling predictions from these distributions infinitely.


\subsection{Probabilistic versions of popular agreement metrics}
\label{sec:probabilistic_metrics}
We now provide probabilistic versions of Cohen's $\kappa$ , Scott's $\pi$, and Fleiss' $\kappa_F$, so that the interested reader can contrast them with CAPA.

\paragraph{Probabilistic Cohen's $\kappa$} One can obtain a probabilistic Cohen's $\kappa$ by computing $P_0$ as $\cobsp$, therefore accounting for the observed agreement based on model output probabilities. While $P_e = \sum_{i=1}^C \frac{1}{|D|}\sum_{x=1}^{D}p_1(c_i)_x \cdot \frac{1}{|D|}\sum_{x=1}^{D}p_2(c_i)_x$ where we compute the product of marginals for each class. 

\paragraph{Probabilistic Scott's $\pi$} Similarly to Cohen's $\kappa$ to compute the observed agreement probabilistically we compute the average product across probabilities for 2 models, meaning $P_0$ becomes $\cobsp$. While we adjust $P_e$ computation as follows: $P_e = \sum_{i=1}^{C}(\frac{1}{2}(\frac{1}{|D|}\sum_{x=1}^{D}p_1(c_i)_x+\frac{1}{|D|}\sum_{x=1}^{D}p_2(c_i)_x))^2$, where we now compute the sum of the marginal probabilities per class as we assume that both models have a shared marginal distribution.

\textbf{Probablistic Fleiss' Kappa ($\kappa_F$)}: It extends the $\frac{\cobs - \cexp}{1 - \cexp}$ formula to more than two models, where the observed and chance agreement is computed across pairs of two in the set of models. Like $\pi$ it assumes chance predictions are sampled from a common combined distribution. While generally Fleiss' Kappa allows a partial random subset of annotators for each question, in our work we assume all models annotate all questions. Let $M$ be the number of models, and $|C|$ be the number of classes. Let $m_{xi}$ be the number of models that put sample $x$ in class $i$. Let $P_x = \frac{1}{M(M-1)} \sum_{i \in C} m_{xi}(m_{xi}-1)$ be the proportion of observed pairwise agreements for each question. $\cobs = \frac{1}{|D|}\sum_{x \in D} P_x$. For the chance agreement, $\cexp = \sum_{i \in C} (\frac{\sum_{1 \leq j \leq M} p_j(i)}{M})^2$.

Let $M$ be the number of models, and $|C|$ be the number of classes. Let $m_{xi}$ be the number of models that put sample $x$ in class $i$. Let $P_x = \frac{1}{M(M-1)} \sum_{i \in C} m_{xi}(m_{xi}-1)$ be the proportion of observed pairwise agreements for each question. $\cobs = \frac{1}{|D|}\sum_{x \in D} P_x$. For the chance agreement, $\cexp = \sum_{i \in C} (\frac{\sum_{1 \leq j \leq M} p_j(i)}{M})^2$.

\subsection{Theoretical bounds for CAPA}
\label{app:goelpi_bounds}

\textbf{Bounds for $\cobsp$}. Compared to~\citet{geirhos2020beyond} the resulting observed agreement is strictly greater than 0, as all probabilities are positive values, and strictly smaller than 1, as the sum of probability products is strictly smaller than the sum of probabilities:
\begin{align}
    0 < \cobsp < 1 
\end{align}

Theorem:
If $0<a<1$ and $0<b<1$, and $a+b=1$, then $a^{2} + b^{2} < a+b$ 

Proof:
\begin{align*}
    a^{2} + b^{2} < a+b \\
    a^{2} -a + b^{2} -b  < 0 \\
    a(a-1) + b(b-1) <0 
\end{align*}
For $0<a<1$, $a>0$ and $a-1 <0 $, therefore, $a(a-1)<0$. For $0<b<1$, $b>0$ and $b-1<0$, therefore $b(b-1)<0$. Since $a(a-1)<0$ and $b(b-1)<0$, their sum will also be negative $a(a-1) + b(b-1) <0$,
this implies that indeed $a^{2} + b^{2} < a+b$. 

\textbf{Bounds for $\cexpp$}. The lower bound for $\cexpp$ is when the first term approaches zero and the scaling fraction approach 0, thus resulting in $\cexpp=0$. The upper bound is maximized when both terms are maximized, but as the second term is the inverse of the first times a scaling factor, the maximum upper bound is 1 (as $\overline{p_1} \cdot \overline{p_2}$ $\rightarrow$ 1, $(1 - \overline{p_1}) \cdot (1 - \overline{p_2})$ $\rightarrow$ 0), resulting in: 
\begin{align}
    0 < \cexpp < 1 
\end{align}

\textbf{Bounds for $\goelpi$}.
The upper bound for $\goelpi$ is 1. In particular, $\goelpi$ will always be strictly smaller than 1, but approaching it in the limit. 

Theorem:
Given $\goelpi=1$. Then by definition:

Proof:
\begin{align*}
      & 1 = \frac{\cobsp - \cexpp}{1-\cexpp} \\ 
     & 1 - \cexpp = \cobsp - \cexpp \\
     & 1 = \cobsp
\end{align*}

However, as $\cobsp < 1$, $\goelpi < 1$. 

Although the above implies that CAPA does not obtain 'perfect agreement' as originally defined by Cohen's $k$, we show that this is not a concern for our metric as (1) when model probability for the correct class approach 1, $\goelpi \rightarrow 1$ and (2) using probabilities allows us to capture observed agreement at a more precise level:
\begin{enumerate}
    \item  Theorem:
    Given probabilities [a,b] and [c,d], where $a,c \rightarrow 1$ and conversely $b,d \rightarrow 0$, $\goelpi \rightarrow 1$:
    
    Proof:
    \begin{align*}
        & \cobsp = a\cdot c + b \cdot d \\
        & \text{as} \: a \cdot c \rightarrow 1 \: \text{and} \:  b \cdot d \rightarrow 0 \\
        & \cobsp \rightarrow 1 
    \end{align*}
    which confirms $\goelpi \rightarrow 1$. 

    \item~\citet{geirhos2020beyond} computes $\cobs$ as $c_{obs_{i,j}} = \frac{e_{i,j}}{n}$ where $e_{i,j}$ is the number of equal responses. As such, $c_{obs_{i,j}}$ is independent of the observed output probabilities. However, for a model pair with output probabilities [0.999.. , 0.000..1] versus [0.8. 0.2] (assume the same for both models), we would like the first case to have a higher observed agreement than the second, but~\citet{geirhos2020beyond} fails to capture this, while $\cobsp$ does:
    
    Theorem:
    Given two probabilities [a,b] and [c,d] where $0 < a,b,c,d < 1$, $a+b=1$, $c+d=1$, and $a>c$, $a>d$, $c>d$, $b<d$, indicates that $a \cdot a + b \cdot b > c \cdot c + d \cdot d$

    Proof:
    \begin{align*}
        & a \cdot a + b \cdot b > c \cdot c + d \cdot d \\
        & a^2 + (1-a)^2 > c^2 + (1-c)^2 \\
        &  a^2 + (1-a)^2 - (c^2 + (1-c)^2) > 0 \\
        & 2a^2 - 2c^2 -2a + 2c > 0 \\
        & 2(a-c)(a+c-1) >0 \\
        & (a-c)(a+c-1) >0 \\
        & \text{as} \: a>c \Rightarrow (a-c)>0 \\
        & \text{as} \: a>d \: \text{and} \: c+d=1 \Rightarrow d=1-c, a>1-c \Rightarrow a+c>1, \: \text{thus}, \Rightarrow (a+c-1)>0, \\
    \end{align*}
    therefore, $a \cdot a + b \cdot b > c \cdot c + d \cdot d$. 
\end{enumerate}

The lower bound for $\goelpi$ is -1. In particular, $\goelpi$ will always be strictly greater than -1. 

Theorem: Given $\goelpi \geq -1$, and $ 0 < \cexpp < 1 $, and $ 0 < \cobsp < 1 $.

Proof: 
\begin{align*}
    & \frac{\cobsp - \cexpp}{1-\cexpp} \geq -1 \\
    & \cobsp - \cexpp \geq -(1-\cexpp) \\
    & \cobsp + 1 -2\cexpp \geq 0 \\ 
    & \cobsp \geq 2\cexpp - 1 \\
    & \text{minimal possible } \cobsp \rightarrow 0 \text{ (complete disagreement)} \\
    & 0 \geq 2\cexpp - 1 \\ 
    & 1 \geq 2\cexpp \\
    & 0.5 \geq \cexpp \\ 
\end{align*}
therefore, $\goelpi \geq -1$. Even though, the theoretical lower bound for $\goelpi=-1$, to achieve $\goelpi=-1$ in practice $\cobsp$ must be 0 (both models perfectly oppose each other), leading that $\cobsp = 2\cexpp - 1, \rightarrow \cexpp=0.5$. As $\cobsp$ is computed based on probabilities its value is $\cobs < 1$, therefore, the actual lower bound for $\goelpi > -1$. 

Altogether, the bounds for CAPA are as follows:
\begin{equation}
    -1 < \goelpi < 1
\end{equation}


\subsection{CAPA comparison with other inter-rater metrics}
\label{sec:metric_comparison}

\paragraph{Numerical Example} For a simple mathematical example consider two models with 2 data samples with the following probability distributions, model 1 = [[0.9,0.1],[0.8, 0.2]] and model 2 = [[0.7,0.3],[0.6, 0.4]]. The underlying ground truth index is [0,1]. For Cohen's $k$ and Scott's $\pi$ we treat this is example as a binary classification with option A and B, converting the probabilities to model 1= [A,A], model 2 = [A, A] (these metrics do not take accuracy into account). The accuracy for both models is 50\%. In table~\ref{tab:metrics_numerical_ex} we report the computed similarity for each metric as well specify the exact computation values. As it can be noted, all other metrics suffer from the following limitations: (1) Cohen's $\kappa$ and Scott's $\pi$ treat the problem as a classification, as such both metrics report that the similarity between models is 0.00, indicating no relationship as $P_o = P_e$, (2) Probabilistic versions of the metrics slightly deviate from 0.00 however still undermine model similarity, (3) Error consistency over estimates model similarity by ignoring model output probabilities in its $\cobs$ calculation. As such, only CAPA is able to accurately account for the observed sample level similarity across the two models. 

%add metrics table 
\begin{table}[H]
    \centering
    \caption{Numerical Example}
    \begin{tabular}{ccc}
      \toprule 
      \textbf{Metric}  & \textbf{Similarity} & \textbf{Computation} \\
      \midrule
      $\kappa$ & 0.00 & $P_o = \frac{2}{2}=1.0$, $P_e = \frac{2}{2}\cdot\frac{2}{2} + \frac{0}{2}\cdot\frac{0}{2}=1.0$ \\
      Probabilistic $\kappa$ & 0.01 & $P_o = \frac{1}{2}(0.9\cdot0.7+0.1\cdot0.3+0.8\cdot0.6+0.2\cdot0.4)=0.61$ \\
      & & $P_e=\frac{0.9+0.8}{2} \cdot \frac{0.7+0.6}{2} + \frac{0.1+0.2}{2}\cdot\frac{0.3+0.4}{2}=0.605$ \\
      $\pi$ & 0.00 & $P_o = 1.00$, $P_e = (\frac{2+2}{2\cdot2})^2 + (\frac{0+0}{2\cdot2})^2 = 1.0$ \\
      Probabilistic $\pi$ & $-0.04$ & $P_o = \frac{1}{2}(0.9\cdot0.7+0.1\cdot0.3+0.8\cdot0.6+0.2\cdot0.4)=0.61$ \\
      & & $P_e = ((\frac{0.9+0.8}{2} + \frac{0.7+0.6}{2})\frac{1}{2})^2 +((\frac{0.1+0.2}{2} + \frac{0.3+0.4}{2})\frac{1}{2})^2 = 0.625 $ \\
      error consistency & 1.00 & $\cobs = 1.00$, $\cexp = 0.5\cdot0.5 + (1-0.5)(1-0.5)=0.5$ \\
      CAPA & 0.21 & $\cobsp=\frac{1}{2}(0.9\cdot0.7+0.1\cdot0.3+0.8\cdot0.6+0.2\cdot0.4)=0.61$ \\
      & & $\overline{p}_1 = \frac{1}{2}(0.9+0.2)=0.55$, $\overline{p}_2 = \frac{1}{2}(0.7+0.4)=0.55$ \\
      & & $\cexp = 0.55\cdot 0.55 + \frac{1}{2}\frac{2}{2-1}(1-0.55)(1-0.55)=0.51 $ \\
      \bottomrule
    \end{tabular}
    \label{tab:metrics_numerical_ex}
\end{table}


\paragraph{Simulation Experiment} Furthermore, we design a simulation experiment to compare the 'behavior' of the above listed inter-rater metrics with our novel contribution CAPA. In particular, we limit the simulation to a binary classification problem as standard metrics like Cohen's $k$ and Scott's $\pi$ are ill-suited for multiple choice question settings. In total we investigate the performance of 4 metrics: Cohen's $k$ Probabilistic, Scott's $\pi$ Probabilistic, Error consistency, and CAPA. We simulate N=10000 observations for 2 models. For the first model we set it's accuracy to 90\%, it always favors the 1st option, and has a high calibration, 0.99, meaning the model is highly confident in it's predictions (e.g. single data point is [0.99, 0.01]). For the second model we iteratively increase it's accuracy by adjusting it's calibration from 0.01 to 0.99 for the first option, as such, making the models more similar artificially.

The first observation from the results reported in Fig.~\ref{fig:metric_comp} is that both standard inter-rater metrics, Cohen's $\kappa$ and Scott's $\pi$ (even when adjusted to take into account probabilities) are ill suited for the present use-case: capturing model similarity. The main issue stems from the fact that the computation of $P_e$ if simply adjusted to probabilistic setting without taking into account model accuracy, obtains a similar computational value as $P_o$ (in this case equal to $\cobsp$). $P_e^p$ computes marginal class probabilities as indicated in Section~\ref{sec:probabilistic_metrics}, which is ill suited when the model attributes all it's probability mass to a single option (always prefers option A in MCQ setting). Furthermore, whilst error consistency improves upon Cohen's $\kappa$ and Scott's $\pi$ it over estimates model similarity. In particular, when both models reach 90\% accuracy error consistency reports perfect agreement, while in reality model output probabilities differ, [0.99, 0.01] and [0.65, 0.35] respectively. As such, our metric is the only one that is able to capture model observed agreement $\cobsp$ increasing beyond model accuracy levels and reaching 1 when models are highly calibrated, e.g. [0.99, 0.01] and [0.99, 0.01]. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig/metrics/calibrated_versus_inc.png}
    \caption{Metric comparison when models tend towards agreement. We compare different metric values for two models in a binary setting. For first model we set 90\% accuracy and calibration to 0.99 (meaning the model is highly confident in its answers). For the second model, we increase it's calibration from 0.01 to 0.99 to approach the same distribution as the first model. On y-axis we are plotting metric value on x-axis we are reporting $\overline{p}$ for the second model which as the model becomes more calibrated approaches accuracy of the first model.}
    \label{fig:metric_comp}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \vspace{0pt}
        \includegraphics[width=\linewidth]{fig/metrics/calibrated_versus_inc_disagree.png}
        \caption{Metric comparison when models tend towards disagreement (Read plot from right to left). We compare different metric values for two models in a binary setting. For the first model, we set accuracy to 90\% and calibration to 0.99 (the model is highly confident in its answers). For the second model, we incrementally increase its disagreement with model one by pushing its probability mass to the second option and increasing its calibration to 0.99.}
        \label{fig:metric_dis}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \vspace{0pt}
        \includegraphics[width=\linewidth]{fig/metrics/calibrated_versus_inc_disagree_adj.png}
        \caption{Metric comparison when models tend towards disagreement with adjusted $\goelpi$. Replication of fig.~\ref{fig:metric_dis} but with adjusted $\goelpi$ as  as $\hat{\goelpi}$, computation following eq.~\ref{eq:k_p_adj}.}
        \label{fig:metric_adj}
    \end{minipage}
\end{figure}

\paragraph{Limitations of CAPA.} In addition, we investigated the 'behavior' of the above listed metrics as the models become increasingly dissimilar. In this set up we change that the second model always prefers the second option. Thus, by iteratively increasing its calibration we obtain models that maximally differ in their probability distribution, e.g. [0.99, 0.01] and [0.01, 0.99] respectively. As such, also the accuracy of the second model decreases overtime from random chance (50\%) to 0.10 \%, and we would like to obtain metric of -1. As it can be seen in Fig.~\ref{fig:metric_dis}, CAPA never reaches -1. Importantly, the same issue also can be observed for error consistency. This observation comes from the fact that both metrics use the original Cohen's $\kappa$ equation. As explained in Section~\ref{app:goelpi_bounds}, $\goelpi=-1$ $\text{ iff }$ $\cexpp=0.5$. For probabilistic Cohen's $\kappa$ we see the same observation as in Fig.~\ref{fig:metric_comp}, the marginal probability computation is not suited for the given problem. Interestingly, probabilistic Scott's $\pi$ is the only metric that approaches -1. Whilst a desired final outcome, Scott's $\pi$ overestimates model disagreement when model probabilities are independent, [0.99, 0.01] and [0.5, 0.5]. 

\paragraph{Possible solution for lower bound.} In the context of the current work, the above limitation is not an issue, as models are trained to maximize accuracy, hence, there will always be some level of agreement. However, if CAPA would be used in settings like preference judgments, we would advise to adjust the computation of $\cexpp$ as described by \citet{safak2020min}:
\begin{align}
    \goelpi = \left\{\begin{matrix}
                    \frac{\cobsp - \cexpp}{1 - \cexpp} & \cobsp \geq \cexpp  \\
                    \frac{\cobsp - \cexpp}{\cexpp - c_{\text{obs-min}}^{p}} & \cobsp < \cexpp \\
                \end{matrix}\right.
    \label{eq:k_p_adj}
\end{align}
where $c_{\text{obs-min}}^{p}$ is computed as $c_{\text{obs-min}}^{p} = \text{max}(0, \overline{p_1}+\overline{p_2}-1)$. This resolves the observed limitation of CAPA over the negative domain, see Fig.~\ref{fig:metric_adj}. Now, as the models become increasingly dissimilar $\hat{\goelpi}$ approaches -1. 