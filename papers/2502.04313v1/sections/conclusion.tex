\section{Conclusion, Limitations, Future Work}

Our paper shows the importance of measuring functional similarity for language models. We derive a novel, probabilistic metric for model similarity, CAPA ($\goelpi$). We then use it to study the implications of similarity for AI oversight -- showing affinity bias in AI judges, and the role of complementary knowledge when training on LM annotations, such as in weak-to-strong generalization. AI oversight will become more relevant as capabilities improve, so our finding that increasing capabilities could lead to more correlated errors is particularly concerning. Thus, we believe measuring and accounting for model similarity is going to be increasingly important. We now list some limitations of our work, along with avenues for future work that can help develop a better understanding of model similarity and its implications.

\textbf{Establishing Causation}: We established similarity correlates with both aspects of AI oversight -- evaluation and training supervision. To establish causality, we need methods to make a model less similar without harming capabilities, which is itself a challenging open problem.

\textbf{Extending to similarity metrics for free-text outputs}: Everyday use of generative models is based on their free-text responses. Like much work on benchmarking, we had to limit to MCQ tasks as the science of precisely evaluating free-text is still evolving~\citep{biderman2024lessonstrenchesreproducibleevaluation}. For example, both model-free~\citep{papineni-etal-2002-bleu} and model-based metrics~\citep{pillutla2021mauve} suffer from a wide range of syntax and style sensitivity issues~\citep{kocmi-etal-2021-ship, he-etal-2023-blind}. We hope the community takes up the challenge of designing similarity metrics for free-response text and reasoning. This would allow studying the role of similarity using more promising oversight setups like debate~\citep{kenton2024scalableoversightweakllms} and process supervision~\citep{lightman2023let}.
    
\textbf{Generator-Verifier gap}: AI oversight has recently shown promise for tasks where it is easier to validate a solution than generate it~\citep{song2024mindgapexaminingselfimprovement}. Similarity may continue to play a role here. (1) In evaluations, similarity in stylistic preferences of the generated solution may influence judge scores. (2) In training, the generator-verifier gap may be larger if models are more different.
    
\textbf{Safety implications}: Researchers separately develop many post-training interventions to reduce harmfulness, dual-use knowledge, dishonesty etc. In real world model deployments, all these problems have to be tackled at once, which can benefit from composing interventions~\citep{kolbeinsson2024composableinterventionslanguagemodels}. If the benefits are decorrelated, composing would lead to greater compound safety. If the side effects are correlated, composing would lead to lower accumulation. More broadly, measuring LM similarity post-intervention can help characterize decorrelation in research bets, for granters~\citep{CANTON2025105129}. For example, LM unlearning was recently found to be functionally similar to refusal training~\citep{lucki2024adversarialperspectivemachineunlearning}, even though it was proposed as a complementary safeguard~\citep{li2024wmdp}. Finally, as we transition towards language agents, similarity can help understand collective ``blind spots''~\citep{he-etal-2023-blind}, and could lead to better cooperation~\citep{lowe2017multi} but also scheming~\citep{balesni2024evaluationsbasedsafetycasesai} between multiple agents. 
    
\textbf{Qualitative analysis of model differences}: We developed quantitative methods for measuring LM similarity on a given data distribution. One exciting direction is to use these metrics to provide qualitative difference descriptors~\citep{dunlap2024vibecheckdiscoverquantifyqualitative} between models, by describing data distributions where models are least similar.  

\section*{Acknowledgments} The authors would like to thank (in alphabetical order) Arvindh Arun, Nikhil Chandak, Thomas Klein, Ankit Sonthalia, Guinan Su, Mark Tygert, Vishaal Udandarao for helpful feedback. We thank HuggingFace for the public sample-wise predictions provided in OpenLLMLeaderboard, which enabled our work. This work was supported by the TÃ¼bingen AI Center. JS thanks the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support. AP and MB acknowledge financial support by the Federal Ministry of Education and Research (BMBF), FKZ: 011524085B and Open Philanthropy Foundation funded by the Good Ventures Foundation. 



\section*{Author Contributions} Shashwat conceived the project, proposed the CAPA metric, and led the LM Annotators experiments (Section 4). Joschka led the LLM-as-a-Judge experiments (Section 3). Ilze led statistical analysis of all results, and characterized properties of CAPA (Section 2, Appendix A). Karuna analyzed trends for capabilities-similarity (Section 5). Ameya helped across sections. The manuscript was written by Shashwat, Ilze, Ameya and Joschka. Douwe, Matthias and PK provided feedback and advice throughout the project. Jonas advised the design of all experiments. 