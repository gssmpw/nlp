\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/MMLU_cleaned/size_colored_by_family.png}
    \caption{\textbf{Average Similarity ($\goelpi$) vs Model Capability}. We split 130 LMs into 5 buckets based on their accuracy percentile. For each LM we compute its mean similarity within the bucket (across models from different developers), and plot it against model accuracy. The size of the scatter points is proportional to model size. As $\goelpi$ measures overlap in mistakes, the positive correlation indicates LM mistakes are getting more correlated with increasing capabilities.}
    \label{fig:capability-similarity}
    \vspace{-0.4cm}
\end{figure}

\section{Models are making more similar mistakes as capabilities increase}
\label{sec:errors}

The previous two sections highlighted two major advantages of having access to more diverse LMs: a) it leads to less biased judges, b) it can drive more performance gains from training on LM annotations. This points to the importance of diversity, or lower model similarity, for AI oversight. As AI oversight becomes increasingly relevant with advancing capabilities, we now study similarity trends in existing LMs across different levels of capability. It has been shown model representations across modalities are converging with increasing capabilities~\citep{huh2024platonic}. Does this also lead to more similar mistakes?

\subsection{Experimental Setup}
We collect sample-wise evaluation files for 130 official models from the OpenLLM Leaderboard 2 released by HuggingFace, listed in Appendix~\ref{sec:modellist}. We use MMLU-Pro~\citep{wang2024mmlupro} and Big Bench Hard (BBH)~\citep{suzgun-etal-2023-challenging} as they measure a broad range of capabilities using MCQ, and frontier models have reasonable accuracies while not saturating these datasets. We first bucket these models into five performance percentile ranges. Then, for each model, we compute its mean similarity ($\goelpi$) with models in the same bucket from different developers, to prevent confounding from distillation or continual training. More setup details are provided in Appendix~\ref{sec:capsetup}. In Appendix~\ref{app:capdiffmetrics} we also report pairwise results, and using the extension of $\goelpi$ for sets of $M>2$ models.

\subsection{Results \& Discussion}

\textbf{Q1. Are model errors becoming more correlated with improving capabilities?} Figure~\ref{fig:capability-similarity} shows a strong positive correlation between model capabilities and $\goelpi$, which measures similarity beyond chance agreement due to accuracy. In Appendix~\ref{sec:capindivsubj} we find this also holds across individual categories in both datasets, not just in aggregate.

\textbf{Potential Implications.} If this trend continues, it could mean greater affinity bias when using LM judges, and lower potential for gains from inter-LM training in the context of our earlier results. It could undermine benefits from using LM juries by compromising independence and amplifying collective biases. Most concerningly, our results indicate that as model blind-spots get harder to detect, making us defer more to AI oversight, models also make more similar mistakes, posing safety risks from correlated failures. 

\textbf{Q2. Why are model errors becoming more correlated?} This is an interesting research direction in itself. We perform a preliminary analysis in Appendix~\ref{sec:capvarying}, summarizing key conclusions here. First, we observe only a slight increase in similarity for harder questions in our datasets, indicating difficulty is not a significant confounder for this trend. We find this trend is stronger in instruction-tuned models, and using alternative architectures like Mamba~\citep{gu2023mamba} may not be enough to increase diversity.