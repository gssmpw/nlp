\section{Similarity Trends with Increasing Capabilities}


\subsection{Setup Details}
\label{sec:capsetup}

We utilize two prominent benchmark datasets from the OpenLLM leaderboard to explore the relationship between model similarity and capability: MMLU Pro and BBH. For the BBH dataset, we aggregate 23 distinct tasks that can be studied as multiple-choice questions from the Big-Bench Hard benchmark to ensure that each model is evaluated on sufficient questions, thereby ensuring statistically significant results. The MMLU Pro dataset consists of MCQs across 14 different subjects, with varying numbers of options per question. Notably, some questions are repeated with shuffled option orders. To maintain consistency, we filter the dataset by retaining only those questions for which both the question text and the correct option index remain consistent across all models. This filtering process yields a refined dataset of 11,828 questions.

To analyze trends across model capabilities, we divide 130 models (Table \ref{tab:model_used_cap}) into five bins based on their individual accuracy percentiles. This binning strategy is followed for all experimental setups and ensures an approximately equal distribution of models per bin, maintaining a consistent sample size across bins. We select model pairs within each bin and compute their similarity and average accuracy. This approach ensures that the average accuracy of the pairs remains representative of the individual model accuracies within the bin. We do not consider model pairs from the same family to avoid confounding effects of model similarity being attributed to model family rather than the capability.

\subsection{Why are model mistakes becoming more similar? A preliminary analysis}
\label{sec:capvarying}


\subsubsection{Instruction-tuning exacerbates the trend}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/MMLU_cleaned/instruct_base_both.png}
    \caption{\textbf{LM Similarity ($\goelpi$) vs Capabilities in Instruct-tuned and Base models on MMLU pro and BBH}. After applying the same model binning stratergy and pairwise similarity, a steeper trend is observed in the instruct-tuned models compared to base models for both datasets.  }
    \label{fig:bvsi}
\end{figure}
Instruction-tuned models are base models that have been fine-tuned on instruction datasets and their corresponding outputs, enhancing their ability to follow user instructions accurately. Among the models analyzed for the capability-similarity trend, we categorize them into instruction-tuned and base models. Using the same binning strategy as discussed in the previous section, we first assign all models to bins based on their accuracy percentiles. When computing pairwise similarity and accuracy, we restrict to pairs of the same model type— base-base and instruct-instruct models. As illustrated in Fig. \ref{fig:bvsi}, instruction-tuned model pairs exhibit a stronger similarity trend with a steeper slope compared to base models. This can likely be attributed to the fact that instruction-tuned models may have been fine-tuned on similar instruction datasets, leading to a higher similarity trend among them.


\subsubsection{Is the trend confounded by question difficulties?}

To address the potential confounder that models might exhibit higher similarity as their capability increases simply due to their inherent ability to solve harder problems, we analyze the relationship between question hardness and model similarity on MMLU Pro and BBH. Question hardness is determined by the percentage of models that answer a question correctly, with harder questions being those that fewer models answer correctly. We split the data samples into percentile bins based on question hardness and compute the average similarity across all capability bins of the initial setting, as illustrated in Fig. \ref{fig:hardness}(a).

Fig. \ref{fig:hardness}(a) demonstrates that the overall average similarity remains consistent across different levels of question hardness, with only a slight increase observed for the hardest questions (100th percentile). This consistency indicates that the hardness of the questions does not significantly confound the observed trend of increasing similarity with model capability. These findings reinforce the hypothesis that the growing similarity among models is not merely a byproduct of their ability to solve harder problems but reflects a deeper trend in model behavior as their capabilities improve.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/MMLU_cleaned/combined_plots.png}
    \caption{\textbf{Role of question difficulty in similarity-capability trend.} We plot in parallel (a) Scatter plot with model pairs, illustrating the increasing trend of similarity ($\goelpi$) with model capability. (b) Average similarity ($\goelpi$) across all capability bins for different levels of question hardness. CAPA is mostly consistent across question hardness, with a slight increase on the hardest questions. This shows that question difficulty is not a significant confounder for increasing similarity in mistakes.}
    \label{fig:hardness}
\end{figure}






\subsubsection{Can changing architecture reduce model similarity?}

To study the effect of model similarities across different architectures, we analyze Falcon3 Mamba 7B base and instruct models by computing their CAPA values with Falcon3 7B transformer, Mistral 7Bv0.3, and Llama 3.1 8B base and instruct models. We ensure that the accuracies of the non-Falcon3 transformers are within ±5\% of the Mamba model to ensure comparable capabilities.

In Table \ref{table:mamab-trans-arch},  $Similarity_1$ presents the CAPA between the Falcon3 Mamba and Falcon3 Transformer, $Similarity_2$ the CAPA between Falcon3 Mamba and Llama/Mistral Transformer, and $Similarity_3$ between Falcon3 Transformer and Llama/Mistral Transformers. The results reveal that base models exhibit lower overall similarity compared to instruction-tuned models, with pairwise similarity between Falcon3 Mamba and non-Falcon Mistral/Llama Transformers showing the least similarity. Within the base model category, Falcon3 Mamba and Falcon Transformers demonstrate the highest similarity. For instruction-tuned models, Falcon3 Transformer and Mistral/Llama Transformer pairs exhibit the highest similarity, followed closely by Falcon3 Mamba and Falcon3 Transformer. 

The Falcon Mamba-Falcon Transformers maintain higher similarity overall, potentially due to their shared model family, despite differences in their underlying architectures. This observation highlights that architectural differences may play a less significant role in model similarity compared to factors such as training data and fine-tuning procedures. From the earlier section, instruction-tuned models exhibit a stronger similarity trend, similar to as observed in this setting.



\begin{table}[ht]
\centering
\caption{\textbf{Analyzing the effect of difference in architecture on CAPA $\goelpi$.} Using base and instruct variants of Falcon3 Mamba and Falcon3 Transformer of size 7B, we compare it with transformers with similar size and accuracy from a different model family- LLama and Mistral. $Similarity_1$ consistently has an overall higher similarity due to models belonging to the same family. In instruct-tuned models, $Similarity_3$ is the highest, possibly due to the instruct-tuning. }
\label{table:mamab-trans-arch}
{
\begin{tabular}{llllll}
\toprule
\textbf{Falcon Mamba} & \textbf{Falcon Transformer} & \textbf{Transformer Model} & \textbf{$Similarity_1$} & \textbf{$Similarity_2$ }& \textbf{$Similarity_3$}   \\ \midrule

\multirow{2}{*}{7B Base} & \multirow{2}{*}{7B Base} & Llama 3.1 8B & \multirow{2}{*}{0.0619} & 0.0167 & 0.0422 \\ 
                         &                          & Mistral v0.3 7B &                         & 0.0105 & 0.0235  \\ \midrule

\multirow{2}{*}{7B Instruct} & \multirow{2}{*}{7B Instruct} & Llama 3.1 8B Instruct & \multirow{2}{*}{0.1111} & 0.0665 & 0.173 \\ 
                         &                                  & Mistral v0.3 7B Instruct &                         & 0.0582 & 0.1584  \\ \bottomrule
 
\end{tabular}}
\end{table}







\subsection{Alternative Similarity Metrics}
\label{app:capdiffmetrics}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/MMLU_cleaned/side_by_side_scatter_other_metrics.png}
    \caption{\textbf{Error consistency and JSD for model similarity on BBH and MMLU Pro.} The y-axis represents the similarity computed using JSD and Error consistency. JSD exhibits high variance and a flat trend, whereas Error Consistency shows an increasing trend with model capability, similar to the trend observed in $\goelpi$.}
    \label{fig:others}
\end{figure}


As discussed in earlier sections, multiple metrics can be employed to quantify the similarity between models, each with its own strengths and limitations. In this analysis, we evaluate several alternative metrics under the same experimental setting used for CAPA, including the binning and averaging strategies, for the two benchmark datasets. Fig. \ref{fig:kappa-discrete-bins} presents the results for discrete $\goelpi$, which does not utilize logit information, while Fig. \ref{fig:fleiss} demonstrates a similar trend using $\kappa_p$ for $M>2$ ($\kappa_p$ extended for more than 2 models). Additionally, Fig. \ref{fig:others} includes results for Jensen-Shannon Divergence (JSD) and Error Consistency \cite{geirhos2020beyond}.

Discrete $\goelpi$ exhibits an increasing trend with model capability, closely mirroring the trend observed with $\goelpi$. Similarly, $\kappa_p$ for $M>2$, which leverages probabilistic information, unlike Discrete $\goelpi$, shows a strong upward trend. Unlike other metrics, $\kappa_p$ for $M>2$ provides a direct measure of similarity that quantifies agreement among all models within a bin, eliminating the need for averaging pairwise similarities. Models of same family within the same bin are retained when computing the metric. In contrast, JSD does not exhibit a clear trend and remains flat with high variance across the capability axis. Error Consistency, however, aligns with the upward trend observed in other metrics, further supporting the hypothesis that model similarity increases with capability. 



\begin{figure}
  \begin{subfigure}{0.49\textwidth}
     \includegraphics[width=\linewidth]{fig/MMLU_cleaned/fleiss_kappa_bar_side_by_side.png}
    \caption{ \textbf{LM Similarity ($\kappa_p$ for $M>2$) vs Average Accuracy of Model Pairs in each Capability bin} }
    \label{fig:kappa-discrete-bins}
  \end{subfigure}%
  \hspace*{\fill}   % maximize separation between the subfigures
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{fig/MMLU_cleaned/discrete_kappa_bar_side_by_side.png}
    \caption{\textbf{LM Similarity (Discrete $\goelpi$) vs Average Accuracy of Model Pairs in each Capability bin}}
    \label{fig:fleiss}
  \end{subfigure}%

\caption{Discrete $\goelpi$ and $\kappa_p$ for $M>2$ values computed on the MMLU Pro and BBH dataset. An increasing trend in similarity is observed across both datasets in accordance with the hypothesis. Discrete $\goelpi$ uses similar averaging idea as used in $\goelpi$ while in $\goelpi$ for $M>2$, the similarity is computed using all models in a capability bin at once. } \label{fig:kappa_variations}
\end{figure}









\subsection{Model capability vs similarity across domains}
\label{sec:domainwisecap}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/MMLU_cleaned/subjectwise_MMLU.png}
    \caption{\textbf{LM Similarity ($\goelpi$) vs Capability on MMLU pro for each subject.} The increasing trend holds for all 14 subjects in MMLU pro. The similarity trend is therefore not a consequence of a particular domain or subject in MMLU Pro.}
    \label{fig:MMLU subject}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/MMLU_cleaned/subset_BBH.png}
    \caption{\textbf{LM Similarity ($\goelpi$) vs Capability on each Big-Bench Hard task.} The increasing trend holds for most BBH tasks. Each task has atmost 250 questions, resulting in minimal data to compute similarity for the individual tasks.}
    \label{fig:BBH subset}
\end{figure}
\label{sec:capindivsubj}

The scatter plot in Fig \ref{fig:capability-similarity} shows the increasing similarity trend after aggregating across the subjects (MMLU pro) and tasks (BBH). 
Fig \ref{fig:MMLU subject} and Fig \ref{fig:BBH subset} show the observed trend within each individual subject and task for MMLU Pro and BBH respectively. 

In the MMLU Pro dataset, the trend of increasing average similarity within each bin as model capability improves is consistently observed across all individual subjects. For the BBH tasks, while the trend is not as pronounced in some tasks, it remains significant in the majority of them. This weaker trend in certain BBH tasks can be attributed to the limited number of questions per task for each model, with a maximum of 250 questions per task, which reduces the reliability of the results compared to the more robust MMLU Pro dataset. This is also visible through the high confidence interval in the BBH tasks, unlike MMLU pro subjects.







\subsection{List of models} 
\label{sec:modellist}


\begin{table}[ht]
\centering
\caption{\textbf{Models from OpenLLM leaderboard used to study the capability-similarity trend}. Models across different families, architectures, size and versions were used to ensure robustness in the experimental results. The models included are both base and fine-tuned versions where available.}
\label{tab:model_used_cap}
\begin{tabular}{llll}
\toprule
\textbf{Dev} & \textbf{Model Family} & \textbf{Size} & \textbf{Type} \\ \midrule
\multirow{2}{*}{01-ai} & Yi-1.5 & 9B, 34B & Base, Instruct \\ 
 & Yi & 34B & Base, Instruct \\ \midrule
\multirow{2}{*}{CohereForAI} & c4ai-command-r-plus & -- & Base, Instruct \\ 
 & aya-expanse & 32b & Base \\ \midrule
EleutherAI & Pythia & 160m, 410m, 2.8b, 6.9b, 12b & Base \\ \midrule
\multirow{5}{*}{Google} & Gemma & 2b, 7b & Base, Instruct \\ 
 & Gemma-1.1 & 2b, 7b & Instruct \\ 
 & Gemma-2 & 2b, 9b, 27b & Base, Instruct \\ 
 & Flan-T5 & Small, Base, Large, XL, XXL & Base \\ \midrule
\multirow{6}{*}{Meta} & Llama-2 & 7b, 13b, 70b & Base, Instruct \\ 
 & Llama-3.2 & 1B, 3B & Base, Instruct \\ 
 & Llama-3 & 8B, 70B & Base, Instruct \\ 
 & Llama-3.1 & 8B, 70B & Instruct \\ 
 & Llama-3.3 & 70B & Instruct \\ \midrule
\multirow{3}{*}{Mistral AI} & Mistral-7B & v0.1, v0.2, v0.3 & Base, Instruct \\ 
 & Mixtral-8x7B & v0.1 & Base, Instruct \\ 
 & Mistral-Large & -- & Instruct \\ \midrule
Nvidia & Mistral-NeMo-Minitron & 8B & Base, Instruct \\ \midrule
\multirow{6}{*}{Qwen} & Qwen2 & 0.5B, 1.5B, 7B, 72B & Base, Instruct \\ 
 & Qwen2.5 & 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B & Base, Instruct \\ 
 & Qwen2-Math & 7B, 72B & Base, Instruct \\ 
 & Qwen2-VL & 7B, 72B & Instruct \\ 
 & Qwen2.5-Coder & 7B & Base, Instruct \\ 
 & Qwen1.5 & 32B, 110B & Base, Chat \\ \midrule
\multirow{3}{*}{Tiiuae} & Falcon & 7b, 11B, 40b & Base, Instruct \\ 
 & Falcon3 & 7B, 10B & Base, Instruct \\ 
 & Falcon-mamba & 7b & Base \\ \midrule
Upstage & solar-pro-preview & -- & Instruct \\ \bottomrule
\end{tabular}
\end{table}
