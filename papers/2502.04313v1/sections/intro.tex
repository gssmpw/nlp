\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/main_fig.png}
    \caption{\textbf{Our Main Contributions}. We develop a novel probabilistic metric for model similarity, CAPA ($\goelpi$), which adjusts for chance agreement due to accuracy. Using this, we find (1) LLM-as-a-judge scores are biased towards more similar models controlling for the model's capability (2) Gain from training strong models on annotations of weak supervisors (weak-to-strong generalization) is higher when the two models are more different, (3) Concerningly, model errors are getting more correlated as capabilities increase.}
    \label{fig:main-fig}
    \vspace{-.2cm} % :>
\end{figure}

Machine Learning model capabilities have improved immensely over the last few years. Scaling up the amount of data used for training has played a crucial role in these improvements~\citep{kaplan2020scaling}. Initially, most of the gains in Language Model (LM) capabilities came from scaling pretraining data~\citep{grattafiori2024llama3herdmodels}. Recently, there is increasing interest in post-training, either with human preferences~\citep{ouyang2022traininglanguagemodelsfollow}, or task-specific expert annotations~\citep{lightman2023let}. Collecting human preferences or annotations is slow and expensive. Therefore, with increasing model capabilities, an attractive alternative is to use LMs to annotate training data~~\citep{gilardi2023chatgpt} and score model outputs~\citep{zheng2023judging}, to boost both training~\citep{stiennon2020learning} and evaluation~\citep{li2024crowdsourceddatahighqualitybenchmarks}. In this paper, we refer to both these techniques together as \textit{AI oversight}\footnote{The term is inspired by ``scalable oversight"~\citep{bowman2022measuringprogress}, which studies human-in-the-loop mechanisms for AI Safety.}. 

Can we rely on AI oversight going forward? This remains a topic of much debate. In this work, we study oversight from the perspective of model similarity. When assessing or teaching humans, it is well recognized that individuals have different strengths and weaknesses. Similarly, two models with 50\% accuracy may misclassify completely different samples and thus be highly dissimilar (having different `strengths'). To measure model similarity, we build on \textit{error consistency}~\citep{geirhos2020beyond}, which measures overlap in the samples where two models err beyond what is expected by chance due to the two models' accuracies. In Section~\ref{sec:method}, we extend the error consistency metric in two crucial ways -- 1) by counting differences in predictions rather than correctness for each sample, and 2) incorporating output probabilities. In this way, our novel similarity metric, \textit{Chance Adjusted Probabilistic Alignment} (CAPA), provides a novel way to quantify functional similarity between models. We use this to analyze both evaluation and training using AI oversight as depicted in Figure~\ref{fig:main-fig}:

\textbf{1. LLM-as-a-Judge.} Prior work has shown that LM judges are biased towards their own generations~\citep{liu-etal-2024-llms-narcissistic, panickssery2024llm}. It might seem possible to avoid this concern by simply using a different model as the judge. However, just like human evaluators prefer candidates with similar traits \citep{bagues2012recruiters}, could LM judges also exhibit this \textit{affinity bias}? In Section \ref{sec:AI_Judges}, we study this using CAPA, finding LM judges indeed assign higher scores to models that are more similar to themselves.

\textbf{2. Training LMs on annotations of other LMs.} Next, we study the effect of similarity on inter-LM training set\-ups, where one model annotates data used to train another model. We hypothesize that performance gained through such training leverages complementary knowledge among models, and is thus inversely proportional to CAPA. We investigate this hypothesis in Section~\ref{sec:w2s}, following the weak-to-strong generalization setup~\citep{burns2024weaktostrong}, where a strong (larger) student model is shown to outperform the weaker (smaller) supervisor whose annotations it is fine-tuned on. Indeed, we find performance gains are higher when the weak supervisor and the strong student model are more different. Moreover, our findings indicate a higher performance ceiling for weak-to-strong generalization than previously estimated, if the weak supervisor's complementary knowledge is leveraged effectively.

\textbf{3. With increasing LM capability errors are becoming more correlated.} AI oversight is gaining popularity as capabilities increase. The above results show the benefits of diverse models for AI oversight -- less similarity between models reduces bias in LLM-as-a-judge, and also leads to greater gains when training on LM annotations. Unfortunately, in Section~\ref{sec:errors} we find that as popular frontier LMs become more capable, their mistakes become more similar as captured by CAPA. This trend indicates a risk of common blind-spots and failure modes when using AI oversight, which is concerning for safety~\citep{kenton2024scalableoversightweakllms}.

Overall, our work proposes a novel probabilistic metric for model similarity, and demonstrates the risks of correlated mistakes in the emerging paradigm of AI oversight. We hope the community shifts towards releasing sample-wise model predictions alongside benchmark scores~\citep{burnell2023reporting, ghosh2024onebench}, as they enable richer analysis like measuring similarity.

% We include all code and sample-wise evaluations in the supplementary material, and will later release them publicly.