%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{authblk} % 导入authblk宏包
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{style2025}
%\usepackage[under review]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks}
\setlength{\abovedisplayskip}{3pt} % 公式前的间距
\setlength{\belowdisplayskip}{3pt} % 公式后的间距


\begin{document}
\title{X-SG$^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks}
\author[1]{Zihang Cheng}
\author[1]{Huiping Zhuang}
\author[2]{Chun Li}
\author[3]{Xin Meng}
\author[4]{Ming Li}
\author[4]{Fei Richard Yu}


\affil[ ]{\textsuperscript{1}South China University of Technology, \textsuperscript{2}ShenZhen MSU-BIT University, \textsuperscript{3}Peking University, \textsuperscript{4}Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)}
\renewcommand\Authands{ }
\date{} % 去掉日期



\twocolumn[
\begin{@twocolumnfalse}
\maketitle
\vspace{-3em}
\begin{center}
\texttt{ftcldj@mail.scut.edu.cn},  
        \texttt{ming.li@u.nus.edu},  
\texttt{hpzhuang@scut.edu.cn} 
\end{center}
\end{@twocolumnfalse}
% \icmltitle{X-SG$^2$S: Safe and Generalizable Gaussian Splatting \\ with X-dimensional Watermarks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Zihang Cheng}{sch}
% \icmlauthor{Chun Li}{sch2}
% \icmlauthor{Xin Meng}{sch3}
% \icmlauthor{Fei Richard Yu}{comp}
% \icmlauthor{Ming Li}{comp}
% \icmlauthor{Huiping Zhuang}{sch}
% % \icmlauthor{Firstname7 Lastname7}{comp}
% % %\icmlauthor{}{sch}
% % \icmlauthor{Firstname8 Lastname8}{sch}
% % \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

% % \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% % \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of South China University of Technology}
% \icmlaffiliation{sch2}{ShenZhen MSU-BIT University}
% \icmlaffiliation{sch3}{Peking University}
% \icmlaffiliation{comp}{ShenZhen GuangMing Lab}
% \icmlcorrespondingauthor{Ming Li}{ming.li@u.nus.edu}
% \icmlcorrespondingauthor{Huiping Zhuang}{hpzhuang@scut.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document


\icmlkeywords{3DGS watermark, 3D Steganography}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D generation. Training to get a 3DGS scene often takes a lot of time and resources and even valuable inspiration. The increasing amount of 3DGS digital asset have brought great challenges to the copyright protection. However, it still lacks profound exploration targeted at 3DGS. In this paper, we propose a new framework X-SG$^2$S which can simultaneously watermark 1 to 3D messages while keeping the original 3DGS scene almost unchanged. Generally, we have a X-SG$^2$S injector for adding multi-modal messages simultaneously and an extractor for extract them. Specifically, we first split the watermarks into message patches in a fixed manner and sort the 3DGS points. A self-adaption gate is used to pick out suitable location for watermarking. Then use a XD(multi-dimension)-injection heads to add multi-modal messages into sorted 3DGS points. A learnable gate can recognize the location with extra messages and XD-extraction heads can restore hidden messages from the location recommended by the learnable gate. Extensive experiments demonstrated that the proposed X-SG$^2$S can effectively conceal multi modal messages without changing pretrained 3DGS pipeline or the original form of 3DGS parameters. Meanwhile, with simple and efficient model structure and high practicality, X-SG$^2$S still shows good performance in hiding and extracting multi-modal inner structured or unstructured messages. X-SG$^2$S is the first to unify 1 to 3D watermarking model for 3DGS and the first framework to add multi-modal watermarks simultaneous in one 3DGS which pave the wave for later researches. Our project is available at: \url{https://github.com/ChengLiDuoJi/XSGS}. 
\end{abstract}
\section{Introduction}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./figures/senario.png}}
\caption{Application scenario of the proposed. \textbf{Scenario 1:} after training a 3DGS pipeline by a trainer, an X-SG$^2$S injector should be combined for joint use. In this way, when other users employ the trainer's model, they can obtain a 3DGS result with some watermarks. By providing the 3DGS file to X-SG$^2$S's extractor, the trainer can determine whether a user has generated the result by using the trainer's model. \textbf{Scenario 2:}  if a user has specific requirements to add extra information to the 3DGS result for a particular purpose, they can directly use X-SG$^2$S to achieve this goal. \textbf{For example}, you can add a text with ``ICML'', an image of the ICML logo and a small 3DGS object at the same time by using X-SG$^2$S's injector and extract them by using X-SG$^2$S's extractor.}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure}
The applications of 3D reconstruction and 3D generation technology are extremely extensive. It has already made significant contributions in fields such as healthcare, architecture and engineering, cultural heritage preservation, virtual reality and augmented reality, as well as game and film production. The existing work mainly studies how to build 3DGS models to generate 3D scenes or reconstruct 3D scenes.

However, the increasing emergence of 3D Gaussian Splatting (3DGS) models may lead to issues of information leakage and infringement. At the same time, in the near future, it is likely that people will need to add more, and even multimodal, information or watermarks to a reconstructed or generated scene which poses a challenge to existing methods. Meanwhile, using different methods to add watermarks makes the watermarking adding mechanism complicated and chaotic. 

Currently, only a small portion of research has addressed these problems. A new study GS-Hider restore image or 3D scene into another 3D scene via design a coupled secured feature. Considering the security issues of generative 3D, GaussianStego explores how to embed image watermarks into diffusion models. However, these models all have their limitations. For example, GS-Hider modifies the parameters of GS, which may increase the size of the 3D model parameters and no longer make it universal. The approach of GaussianStego rooting dino into diffusion can not well demonstrate its effect in feed forward 3D generative models. At the same time, adding extra elements in original 3DGS pipelines for protecting copyright is costly and complicate in design or training. We can see that how to effectively add additional watermarks or information without changing the GS parameters or model structure and let the method be general will be a great challenge. Meanwhile, embedding multimodal watermarks into a 3DGS simultaneously is even more of an unexplored area.

To solve the above challenges, we propose an effective, simple and flexible watermarking framework X-SG$^2$S. It aims to embed 3D objects or images or binary messages simultaneously into the original 3DGS scene, and accurately extract the hidden message via just an ultra simple way. Specifically, we use a determined sorting method to sort the GS point set, then use a self-adaption gate which can learn the adding position through the interaction between the additional information(watermarks) and the container(3DGS scenes). A XD-injection heads are used to add different modal messages respectively. Then a learnable gate is used to recognize the locations where we have added the messages. Finally, XD-extraction heads are used to restore the message added. The training for X-SG$^2$S does not need to fine tune or redesign the pipeline. This property makes this model suitable not only for the feedforward model, but also for the model like diffusion which needs multiple iterations. And if you do not need to add additional information, you can use the original model in the fastest time without further fine-tuning. However, it is also possible to fine tune with the pipeline. In a nutshell, the contributions and advantages of our X-SG$^2$S can be summarized as follows.

\begin{itemize}
  \setlength{\itemsep}{0pt}  % 减少 item 之间的垂直间距
  \setlength{\parskip}{0pt}
    \item We have pioneered the first method that can simultaneously embed watermarks of 1 to 3 dimensions into one 3DGS scene, which paves the way for future research.
    \item We have provided an effective approach for GS clouds to store additional structured or unstructured data.
    \item Our model, in an extremely concise and efficient manner, makes the process of adding watermarks to 3DGS both universal and secure.
    \item We conducted extensive experiments to demonstrate the efficiency. The experimental results were SOTA, which established the baseline method for later research.
\end{itemize}
\section{Related Works}
\subsection{Generalizable GS}
3D Gaussian Splatting (3DGS) \cite{kerbl20233d} has emerged as a powerful method for reconstructing and representing 3D scenes using millions of 3D Gaussians. There has been a significant amount of work that has achieved good results in the reconstruction of small objects \cite{szymanowicz2024splatter} \cite{boss2024sf3d} and scenes \cite{chen2025mvsplat} \cite{charatan2024pixelsplat}. Additionally, some studies have utilized diffusion models to realize text-to-3D generation \cite{li2024instant3d}, which also achieves promising outcomes.

\subsection{3D Steganography}
Steganography has been evolving over the decades. Some research has made 3D steganography achieve good results in explicit geometry like meshes and point clouds \cite{ohbuchi2002frequency} \cite{zhu2024rethinking} \cite{ferreira2020robust} or implicit geometer like nerf \cite{li2023steganerf}\cite{luo2023copyrnerf}. 3DGS is a new technology to represent explicit geometry. However, few works have been researched in steganography for 3DGS. GS-Hider \cite{zhang2024gs}design a coupled secured feature attribute to replace the original 3DGS’s spherical harmonics coefficients and then use a scene decoder and a message decoder to disentangle the original RGB scene and the hidden message. GaussianStego\cite{li2024gaussianstego} use dino to add image watermarks and use U-Net to extract them via multi rendered views. But there is not research about watermarking 1 to 3D messages to single 3DGS scene simultaneous while keeping GS parameters or model structure unchanged.

\section{Safe Generalizable GS with X-dimensional watermarks}
\subsection{Preliminaries of Gaussian Splatting}
Gaussian Splatting is a rasterization technique for 3D scene reconstruction and rendering. It works by using 3D Gaussian functions to represent points in a scene and projecting these Gaussian functions onto a 2D image plane for rendering.
For every point, it has many different parameters to represent the location, shape, size, color and opacity. 
$$\exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_i)^\top\Sigma_i^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_i)\right)$$
This function shows the location, shape and size of a GS point where $0<i<N$, $\mu_i\in R^3$ is the mean or center, $\Sigma_i\in R^{3\times3}$ is its covariance specifying its shape and size. The covariance can be decomposed into scaling factor $s\in R^3$ and rotation factor $q\in R^4$.
Each Gaussian has also an opacity $\alpha_i\in [0, 1]$ and a view-dependent color $c_i(v)\in R^{3\times(k+1)^2}$ where k is the higher level of spherical harmonics(SH).
The set of GS points G = \{($\mu_i$ $\Sigma_i$  $\alpha_i$ $c_i$), i =1,...,N\} can be used to represent a scene or a small object. Through rasterization, it can be rendered to an image where you only need to know the projective transformation P, viewing transformation W, and Jacobian of the affine approximation of P, J, respectively.
$$\hat{\mu_i}=PW\mu_i$$   $$\hat{\Sigma_i}=JW\Sigma_iJ^TW^T$$
$$\sigma_i=\alpha_i\mathrm{e}^{-\frac{1}{2}(\mathbf{p}-\hat{\boldsymbol{\mu_i}})^\top\hat{\boldsymbol{\Sigma_i}}^{-1}(\mathbf{p}-\hat{\boldsymbol{\mu_i}})}$$
$$\mathbf{C}[\mathbf{p}]=\sum_{i=1}^N\boldsymbol{c}_i\sigma_i\prod_{j=1}^{i-1}(1-\sigma_j)$$
where p represents the pixel.

\subsection{Task Settings}
\textbf{3DGS steganography: }We hope that for any 3dgs scene, we can add additional information without damaging the original 3d structure and gs parameters.

\textbf{Copyright Protection: }By adding fixed watermark into generated scenes, the designers can easily use X-SG2S to detect whether the GS art works are used by others without permission.

\textbf{Multi-modal Message Adding: }This model can not only add information of single modality, but also add information of multiple modalities without producing confusion. This can greatly bring convenience to the users. In theory, this model can add sequential structure data or non-sequential structure data, and accurately extract them when the GS point set is well preserved.

\textbf{Generalizable Modules: }This model does not require changing the structure of the GS generated model, but rather adds watermarks or additional information to the scene in the form of additional modules. 

\subsection{Deep Thinking About Watermarking 3DGS}
Gaussian sphere is a special kind of point set. You can simply see it as a kind of point clouds with many parameters. First of all, we think about why it is difficult for us to add information to GS point clouds. 
\begin{itemize}
  \setlength{\itemsep}{0pt}  % 减少 item 之间的垂直间距
  \setlength{\parskip}{0pt}  % 减少段落之间的垂直间距
    \item Point set is an unordered set which means you may treat the point set as a whole to add information (e.g. add information to the frequency domain of the entire point cloud or add the information to the entire point cloud). But this can make the waste of storage space when the amount of information is small or may make large damage to the original model.
    \item If you just use a part of points to add information, you have difficulty knowing the location and order ,especially, when you add more than one watermarks.
    \item Each point has a limited capacity to store information. How to correctly use small containers to store watermark with large amount of information is also a challenge.
\end{itemize}
After deep thinking, we can sort out such information adding ideas:
\begin{itemize}
  \setlength{\itemsep}{0pt}  % 减少 item 之间的垂直间距
  \setlength{\parskip}{0pt}
    \item Every point in the point set have a few of storage space. But when all the storage units are added up, the storage capacity is very large. If we really want to fully use all of the storage space, we should smash the information into patches in an order that we can restore them easily and add them respectively for every points. If we add too much information to one of the point, it may cause loss.
    \item If we want to know how to extract the information from the point set, we should firstly know which of them are added. This means you would better know the location of each point and the restoration order. In another words, you should make the point set an ordered set and a known order of watermark composition.
\end{itemize}
\subsection{Overview}

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{./figures/structure.png}}
\caption{This figure shows the structure of X-SG$^2$S. X-SG$^2$S is made up of a watermark injector and a watermark extractor. The injector includes a self-adaption gate to propose the locations for adding message patches and three injection heads to add 1 to 3D message patches to GS points respectively. The extractor includes a learnable gate to recognize the locations added extra message and three extraction heads to extract 1 to 3D message patches respectively. Self-adaption gate is composed by an embedding layer, an interaction block and a top-k. Embedding layer is used to embed multi modal message patches to a general space and embed GS points to ``gs-emb" space. Interaction block uses the embedded message to generate the final-score which will be a reference for adding locations. In the experiment part, we compare five different interaction blocks and choose the one shown in this figure. Via top-k, the recommended locations can be made as binary mask. Every time, only one modality's adding locations are recommended. By multiple consecutive calculations, it will recommend the locations for all the modalities. The mask of one modality may be reused to generate masks for latter modalities, which prevents overlapping of adding locations.}
\label{fig:stru}
\end{center}
\vskip -0.2in
\end{figure*}


The major target of X-SG$^2$S is to design a general framework for multi-modal message adding model which can inject and extract 1D,2D and 3D massages with one model while keeping the original Gaussian scene as unchanged as possible.

As depicted in Figure~\ref{fig:stru}, X-SG$^2$S contains mainly four components: 1) an adaptive self-adaption gate for adaptively selecting GS points to use. 2) a multi-head MLP-based message injector for integrating information into SH parameters. 3) a learnable selection gate for recognizing the location of injected GS points. 4) a multi-head MLP-based message extractor for restoring message from the injected GS points. X-SG$^2$S injector is made up of 1) and 2), extractor is made up of 3) and 4).

The training process and the inference process are slightly difference. The following part will illustrate them in detailed.
\subsection{Point Set Sorting}
As can be seen above, before injecting information and extracting information, we should use the sorting method to sort the point set. If you want every time to get the same order for a point set, you should use a deterministic sorting method. There are many methods for sorting. Here we choose Hilbert filling curve to complete this task. This is because Hilbert filling curve is a kind of deterministic curve. Hilbert curves can map high-dimensional data to a one-dimensional space to get the order of high-dimensional data. Meanwhile, when the size of the data set increases, data access time and computation time are often affected by spatial locality. Hilbert filling curve can reduce unnecessary jumps in storage and retrieval and improve the processing efficiency of large-scale data sets through space filling. At the same time, the Hilbert filling curve can be accelerated by GPU, which can effectively reduce the operation time.
\subsection{Self-Adaption Gate}
This component is to find out the points which are suitable for injecting the messages while ensuring that the original scenario is not disturbed so much. A simple idea is that let a gate to learn a score to measure the locations. The gate will give every point a comprehensive score. A higher score indicates that the location is more suitable for adding information.

The comprehensive score is calculated from two scores:``self-score” and ``cross-score”. ``Self-score” is used to measure which points can significantly impact the 3DGS scenario and which are trivial. It is obvious that if we choose the trivial points to inject messages, we will not disturb the scenario too much. But only use ``self-score” is not enough. We need to know where are more suitable for message to inject and extract. ``Cross-score” is used to manage this problem. 

After we get two scores, we simply multiply two scores together to get the final score. By using top-k, we can set the recommended k positions to 1 and others to 0 to get the selection mask.
\subsubsection{message embedding}
Different modalities belong to  different vector spaces. If we want to calculate the interactions results uniformly and efficiently, we need to map them into a unified space first. Here we simply use three different linear layer to map 1,2,3D message to a general space. And for gs cloud, we also use a linear layer to map it to another high dimension space we call ``gs-emb" space. The dimension of two space can be different. 
\subsubsection{induced set attention to produce self-score}
After we get embedded message, we can measure the interact scores. Attention mechanism is a very efficient method for interact different information. We use transformer-based model to calculate the ``self-score". But we find that if we simply use scale dot product attention, the memory will overflow because the number of GS points are large. However, to calculate “self-score” is merely want to measure the importance of individual information in the overall information instead of learning detailed information between individuals. So set attention\cite{lee2019set} is used to overcome the memory problem. At the same time, set transformer does not need for the data to be sequential. This is very suitable for point sets, because point sets are unordered. 

Induced set attention first design a set of anchor vectors $I\in R^{m\times d}$, where d means the inner dimension of the vector, m means the number of anchor vectors. Then it uses the input $X\in R^{n\times d}$ and I to do multi-head transformer to get the hidden message $H\in R^{m\times d}$ of the point set. Then use H and X to do multi-head transformer again to get the importance of every points in overall information $O_s\in R^{n\times d}$. Finally, use $O_s$ to calculate the  $self\_ score\in R^{n\times 1}$ by using a linear layer. Formally,
$$H = MA(I, X)$$
$$O_s = MA(X, H)$$
$$self\_ score = sigmoid(linear(O_s))$$
In our task, we simply set the number of heads to 1 and the number of anchor vectors to 1 so that the time complexity goes from $O(N^2)$ to $O(N)$.

\subsubsection{efficient attention to produce cross-score}
Because ``Cross-score" needs to interact the whole GS points with embedded message patches. However, if the amount of message patches is large, the attention may also face the problem of overflowing. Thus, we use efficient transformer \cite{shen2021efficient} to approximation the common transformer in order to reduce the memory and preserve the accuracy of calculating. 

Because this is a cross attention mechanism, we let $X$ to become $Q$, and the message patches $Y\in R^{l\times d}$ to become $K$,$V$. It is easy to get that $l$ is usually less then $n$. So, it is efficient to let $KV$  interact first. Formally,
$$O_c=\sigma_{row} (Q)(\sigma_{col} (K)^T V)$$
Where $\sigma$ means the softmax function. The same as above, use $O_c$ to calculate the  $cross\_ score\in R^{n\times 1}$ by using a linear layer.
$$cross\_ score = sigmoid(linear(O_c))$$
\subsubsection{Location masks}
After getting two scores, we simply multiply two scores to get a final score. For every modality, it has one final score. This final score can comprehensively evaluate whether a point is suitable to add message. According to the number of message patches for one modality, we use top-k to select the most suitable locations for adding, where k is the number of patches. If we want to generate the next modality's location mask, we should use the previous masks to avoid selecting the duplicate locations.
\subsection{Learnable Selection Gate}
This gate is to find out the locations where the information added. For above we can know that the self-adaption gate can produce a mask. Here we see the masks as the ground truth. After we get the ordered point set with added messages, we see it as the input of the learnable selection gate. The gate is a four layers MLP to predict the true location. 
\subsection{XD-Injection/Extraction Heads}
These two are used for injecting messages and extracting messages. For each modal, we use a separate MLP \cite{qi2017pointnet}. Usually, 3 to 5 layers are enough for the task. We take out the GS points of target location recommended by self-adaption gate and just use the spherical harmonic parameters of three channels. We concatenate the spherical harmonic parameters with the message patches we want to added. We treat the concatenated data as input to multi-head injector which produce the spherical harmonic parameters with extra information. 

And for extractor, it take the SH parameters of injected points found out by learnable selection gate as input and extract the messages from them. The extracted messages will be optimal by the prediction loss as well. 

\subsection{Training Process}
\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{./figures/Train.png}}
\caption{This figure shows the training process of X-SG$^2$S. Orange lines mean there exists gradient flow. Blue lines mean the gradient are truncated. Purple lines mean the GS cloud is sorted by sorting method. Black lines mean the losses used to train.}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure*}
This model can be trained directly by GS cloud files or a pretrained 3DGS pipeline. 
\begin{enumerate}
  \setlength{\itemsep}{0pt}  % 减少 item 之间的垂直间距
  \setlength{\parskip}{0pt}  % 减少段落之间的垂直间距
    \item Get the GS cloud inferred from a pipeline or read from a cloud file.
    \item Use sorting method to sort the GS cloud to get an ordered set. Use the data loader to load the goal information we want to add. And we scatter them into message patches in order.
    \item Give self-adaption gate the message patches and the GS cloud to recommend the locations. 
    \item Use XD-injection heads to add message. 
    \item Replace the original SH parameters by the injected SH parameters so that we get a GS point set with extra messages.
    \item Truncate the gradient flow of the GS point set with extra messages and take it as input to the learnable gate. It will output the location mask. This part is off-line to ensure proper flow of gradients. If we directly choose the predicted location to extract messages, the message extractor may choose some GS points without messages. This may cause the error of gradient flow. 
    \item Use the truth location to choose the injected GS points and extract messages by XD-extraction heads.
\end{enumerate}
We can see from above that, in training steps we do not need to restore the message patches to their original structure. For example in our work, we encode a graph into features using AE. We inject the feature patches into GS points. If we want to restore the graph from AE, we just need to reshape the feature patches to the features and take it as the input of the decoder of the AE without optimize the encoder and decoder. And you do not need to design or use another losses to train. The message patches will automatically converge. And specifically for 3DGS small object watermarks, we should first reuses some of the points in the object to align the number of points in order to let X-SG$^2$S perform parallel computation without causing extra loss. 

\subsection{Training Loss}
To finish the task, we should design 1) message patches loss, 2) location mask loss, 3) SH parameters loss, 4) other losses.

To ensure the feature patches can be restore exactly, we should use message patches loss to ensure the extracted message is as same as possible to the message injected. For 1D message, we use cross entropy loss; for 2D and 3D message, we use MSE loss.

To ensure the GS parameters do not change much before and after adding information, we use a SH parameter MSE loss to ensure this.

To ensure the learnable selection gate can precisely recognize the location where we inject the message, we use a mask cross entropy loss. This loss use the mask produced by self-adapted gate as the ground truth. The weight for the mask cross-entropy loss does not need to be set as its training is offline. We default this weight to 1. 

Other loss like 2D view loss and lpips loss can be choosed to use when we use pretrained 3DGS pipeline to train X-SG$^2$S. They may let X-SG$^2$S more efficient to preserve the SH parameters.

Thus, the form of loss function is like:
%\vspace{-10pt} % 减少上方间距
\begin{align*}
LOSS &= Optional + \gamma\, \text{SH-MSE} + \phi\, \text{1D-BCE} \\
&\quad + \theta\, \text{2D-MSE} + \delta\, \text{3D-MSE} + \text{Mask-BCE}
\end{align*}

\section{Experiments}
\subsection{Datasets}
We use a pretrained model MVSplat for real time inference. We use large-scale ACID datasets to let MVSplat\cite{chen2025mvsplat} generate the 3DGS scenes. For 1D dataset, we randomly generate a sequence of binary message. For each bit, it has the same probability to be 1 or 0. For 2D dataset, we use Logo-2K\cite{wang2020logo} dataset. For 3D dataset, we first download a subset of objaverse\cite{deitke2023objaverse}, and use Gamba\cite{shen2024gamba} to generate the 3DGS objects. They are then saved in .ply format as a 3D dataset.

Specifically, ACID contains nature scenes captured by aerial drones, which are split into 11,075 training scenes and 1,972 testing scenes. After reconstructing Logo-2K dataset, it has 110313 training logos and 28415 testing logos. 3D dataset made by objaverse and Gamba contains 353 training files and 94 testing files. To ensure the number of datasets is aligned, we reuse the GS cloud files while training.

\subsection{Metrics}
For original GS scenes and GS objects, we use pixel-level PSNR, patch-level SSIM, and feature-level LPIPS to measure the completeness of the original scenes and the quality of restoration of the GS objects. For a fair comparison, all the scenes are rendered to 256$\times$256 resolutions while all the objects are rendered to 512$\times$512 resolutions. For 1D data, we use precision rate to measure the extraction accuracy of binary data. For 2D data, we use pixel-level PSNR to quantization the restoration of the 2D images.

\subsection{Implementation Details}
X-SG$^2$S is implemented with PyTorch, along with an off-the-shelf 3DGS render implemented in CUDA. All models are trained on single A6000ada with the Adam optimizer. 

Its training involves multimodality, so there are many hyperparameters in the loss function that need to be set. We default to using optional loss functions unless in loss function ablation experiments. The optional loss functions include 2D rendered image MSE loss and LPIPS loss, with weights of 1 and 0.05, respectively. And other hyperparameter are $\gamma = 0.2$, $\phi = 0.005$, $\theta = 0.8$, $\delta = 1.5$. 

We choose binary message of 8bit$\times$128 as 1D watermark and 3DGS object with 10000 points as 3D watermark. For 2D watermark, we embed them via a pretrained AE encoder then split them in order to get 16$\times$32768 message patches. In the testing phase, we reconstruct the information fragments into a predetermined shape and then pass them through the AE decoder to obtain the image. And the number of SH parameters is 75 for each GS point.

\subsection{Main Results}
\subsubsection{ablation experiments}
\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./figures/gates.png}}
\caption{Interaction Blocks: a) ``Multiply": it multiplies the self-score and the cross-score to generate final-score. b) ``Add": it uses efficient cross attention to generate cross-feature and set self attention to generate self-feature. Then it concatenates them and calculates final-score. c) ``Concatenate": it concatenates cross-feature and self-feature together and calculates final-score. d) ``Normal": it just uses 3DGS original points as input and passes them through n layers of MLP gate. e) ``Random": it randomly selects k locations to add extra message. It can not be learned.}
\label{ib}
\end{center}
\vskip -0.3in
\end{figure}
In Figure~\ref{ib}, we design five different structures of interaction block and conduct ablation experiments respectively. We jointly train X-SG$^2$S with multimodal watermarks. In Table~\ref{s}, we show that the method of a) is the best.
\begin{table}[h]
\label{sample-table}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\textwidth}{!}{ % 调整宽度为文本宽度，高度按比例缩放
\begin{tabular}{lccccr}
\toprule
Method           & Multiply       & Add              & Concatenate      & Normal & Random  \\
\midrule
PSNR(org)$\uparrow$      & \textbf{25.341}& 23.81            & 24.271           & 23.857      & 24.406 \\
SSIM(org)$\uparrow$      & \textbf{0.848} & 0.829            & 0.833            & 0.833       & 0.759  \\
LPIPS(org)$\downarrow$   & \textbf{0.178} & 0.214            & 0.208            & 0.203       & 0.246  \\
presion$\uparrow$        & \textbf{1.0}   & \textbf{1.0}     & 0.814            & \textbf{1.0}& 0.564  \\
PSNR(image)$\uparrow$    & 36.0062        & 36.0426          & \textbf{36.0877} & 35.8784     & 3.9009  \\
PSNR(3D obj)$\uparrow$   & 22.1400        & \textbf{23.7939} & 22.5261          & 22.1902     & 22.9520  \\
SSIM(3D obj)$\uparrow$   & 0.8603         & \textbf{0.8762}  & 0.8653           & 0.8606      & 0.8728  \\
LPIPS(3D obj)$\downarrow$& 0.1444         & \textbf{0.1387}  & 0.1491           & 0.1555      & 0.1626  \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\caption{Multimodal joint training with different structures of interaction block.}
\label{s}
\end{table}
\begin{table}[t]
\label{sample-table}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\textwidth}{!}{ % 调整宽度为文本宽度，高度按比例缩放
\begin{tabular}{lcccr}
\toprule
Method & 1D-SG$^2$S & 2D-SG$^2$S & 3D-SG$^2$S & Origin  \\
\midrule
PSNR(org)$\uparrow$      & 27.821         & 27.4           & 26.398          & 28.25  \\
SSIM(org)$\uparrow$      & \textbf{0.871} & \textbf{0.866} & \textbf{0.858}  & 0.843  \\
LPIPS(org)$\downarrow$   & \textbf{0.125} & \textbf{0.139} & 0.153           & 0.144  \\
presion$\uparrow$        & 1.0            & /              &  /              &   /    \\
PSNR(image)$\uparrow$    & /              & 35.879         &  /              &   /    \\
PSNR(3D obj)$\uparrow$   & /              & /              & 22.28           & 28.807 \\
SSIM(3D obj)$\uparrow$   & /              & /              & 0.869           & 0.942  \\
LPIPS(3D obj)$\downarrow$& /              & /              & 0.154           & 0.067  \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\caption{Single-modality watermark injection and extraction training.}
\label{single}
\vskip -0.1in
\end{table}
\begin{table}[t]
\label{sample-table}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\textwidth}{!}{ % 调整宽度为文本宽度，高度按比例缩放
\begin{tabular}{lcccr}
\toprule
Method & X-SG$^2$S(Joint) & X-SG$^2$S(Individual)  \\
\midrule
PSNR(org)$\uparrow$      & 27.788(1D) / 26.125(2D) / 26.493(3D)         & 27.821(1D) / 27.4(2D) / 26.398(3D) \\
SSIM(org)$\uparrow$      & 0.871(1D) / 0.859(2D) / 0.860(3D)          & 0.871(1D) / 0.866(2D) / 0.858(3D)  \\
LPIPS(org)$\downarrow$   & 0.125(1D) / 0.154(2D) / 0.149(3D)          & 0.125(1D) / 0.139(2D) / 0.153(3D)  \\
presion$\uparrow$        & 1.0                & 1.0                 \\
PSNR(image)$\uparrow$    & 36.0085                  & 35.879              \\
PSNR(3D obj)$\uparrow$   & 22.2160            & 22.28               \\
SSIM(3D obj)$\uparrow$   & 0.8608             & 0.869               \\
LPIPS(3D obj)$\downarrow$& 0.1443             & 0.154               \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\caption{Comparison between adding a single watermark under joint training and individual training.}
\label{js}
\end{table}

\subsubsection{single modality injection test}
We also test X-SG$^2$S's performance on training single modality. In Table~\ref{single}, we can see that X-SG$^2$S can well adapt the task of 1 to 3D watermarking.  To our delight, we discovered that the scene with watermark outperformed the original in terms of both SSIM and LPIPS metrics. Meanwhile, in Table~\ref{js}, we compare  the differences between adding a single watermark under joint training and under individual training. We can see that two results are almost identical. This demonstrates the effectiveness of our method. The image results are all presented in the appendix.
\subsubsection{The Exploration of Loss Function}
\begin{table}[t]
\label{sample-table}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\textwidth}{!}{ % 调整宽度为文本宽度，高度按比例缩放
\begin{tabular}{lcccr}
\toprule
Method & without optional losses & with optional losses  \\
\midrule
PSNR(org)$\uparrow$      & 24.148         & \textbf{25.341}     \\
SSIM(org)$\uparrow$      & 0.840          & \textbf{0.848}      \\
LPIPS(org)$\downarrow$   & 0.182          & \textbf{0.178}      \\
presion$\uparrow$        & 1.0            & \textbf{1.0}        \\
PSNR(image)$\uparrow$    & 35.8708        & \textbf{36.0062}    \\
PSNR(3D obj)$\uparrow$   & 21.9596        & \textbf{22.1400}    \\
SSIM(3D obj)$\uparrow$   & 0.8568         & \textbf{0.8603}     \\
LPIPS(3D obj)$\downarrow$& 0.1564         & \textbf{0.1444}     \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\caption{Comparison with and without optional loss functions.}
\label{noloss}
\vskip -0.1in
\end{table}

Now, we want to know if X-SG$^2$S can be trained without optional losses which means it can trained by using just 3DGS files. Here we set the weights of optional losses to 0 and set the $\gamma = 2$. From Table~\ref{noloss}, we can see that this way is feasible! However, X-SG$^2$S trained without optional loss functions performs worse in both watermark embedding effectiveness and preserving the integrity of the original scene compared to those trained with optional loss functions. Therefore, we recommend using optional loss functions to enhance the effectiveness of the training.

\subsubsection{The Exploration of Text Watermarking}
We surprisingly found that X-SG$^2$S can achieve nearly 100\% accuracy on text watermark. So, we have delved deeper into its exploration. We conduct two experiments: 1) Investigate the impact of binary bit size on watermarking. 2) Investigate the impact of the sentence's length on watermarking. 

For 1), we test the bit sizes of 8, 16, 24, 32, 40 which are the most commonly used, with its length of 12288. From Figure~\ref{bit}, we can see that under our experimental conditions, the size of 16 can achieve the best result. And the experiment also shows that the bit sizes does not have a great impact on the original scene.

For 2), we test the lengths of 128, 4096, 8192, 12288, 16384, 20480 with its size of 8. From Figure~\ref{bit}, we can see that as the length increases, the integrity of the original scene tends to decline, which implies that the length of the message patches affects the completeness of the original scene.

\begin{figure}[h]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{./figures/bit1.jpg}}
\centerline{\includegraphics[width=\columnwidth]{./figures/len1.jpg}}
\caption{The above row of images shows the experiment 1) and the below row shows 2). It shows the original measurements(triangle) and the trend(line) when X-SG$^2$S achieves nearly 100\% accuracy. The red one is the metric of the original scene without watermark.}
\label{bit}
\end{center}
\vskip -0.1in
\end{figure}
\section{Conclusion}
In this work, we propose a multi modal watermarking framework for 3DGS , X-SG$^2$S, designed for efficient adding 1,2,3D extra messages to a GS scene while keeping the original scene almost unchanged. X-SG$^2$S successfully adds binary, graph, 3DGS object messages to a 3DGS scene simultaneously or individually. Additionally, X-SG$^2$S does not need to fine tune pretrain model (but train with pretrained models is workable) and it can even trained on only 3DGS files which depletes training time and space. Besides, X-SG$^2$S does not change the form of GS parameters so that it is suitable for most of 3DGS scenes or objects. 

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. Moreover, as this paper is the first to propose embedding multimodal watermarks in 3DGS, it encourages more scholars to explore how to incorporate multimodal watermarks into 3DGS to protect creators' copyrights. Additionally, adding extra information to 3DGS files could open up more possibilities for the metaverse, virtual reality, and other fields. For instance, if people need to access relevant knowledge about a scene, this approach would be an excellent solution. As for ethical considerations, there are no inherent ethical issues unless unethical information is added to the 3DGS.

\nocite{yoo2022deep}
\nocite{wu2024point}
\bibliographystyle{style2025}
\bibliography{./xsg2s}




\newpage
\appendix
\onecolumn
\section{Appendix}

\begin{figure}[ht]
  \centering
  \begin{minipage}[b]{0.25\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/logo.png}
    \label{fig:subfig1}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.38\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/obj.png}
    \label{fig:subfig2}
  \end{minipage}
  \label{fig:mainfig}
  \hfill
  \begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/all.jpg}
    \label{fig:subfig2}
  \end{minipage}
  \caption{This image demonstrates its capability to inject and extract watermarks across all three modalities simultaneously. And the X-SG$^2$S was jointly trained with three modalities' watermarks. The first column is the restored logo and the second is ground truth. The third one is the restore objects, the forth is the image render by dataset, the fifth column is the ground truth. The sixth column is the scene we added message and the last column is the ground truth. We can see that the results are quite impressive. And binary precision is nearly 1.0.}
  \label{fig:mainfig}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}