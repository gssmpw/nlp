\section{Related Work}
\label{sec:related_work}

%While many open-source LLM benchmarks exist today relating to the cybersecurity domain, including Capture-the-Flag (CTF) challenges ____, Hack-the-Box, Penetration Testing, and certification/accreditation exams ____, these LLM cyber tests largely evaluate a model's ability to be little more than a knowledge source of current cyber defense knowledge and cyber threat intelligence. While testing that a given LLM can reliably produce meaningful cyber domain knowledge when implemented as a resource for cyber workforce training or question and answer "copilots", these evaluations lack the comprehensiveness to methodically evaluate offensive cyber capabilities that are grounded in real-world scenarios and well quantified.

%Benchmarks also often tend to use other LLMs as the basis for evaluation of a test (CyberSecEval 1\&2\&3, etc.) which is not a clear indicator of the usefulness or risk of the LLM from a cyber operator perspective - we would rather see clearly defined metrics that are meaningful to an operator that map back to a taxonomy such as ATT\&CK i.e pinpointing exact risks of an OCO capability such as understanding of an APT’s suite of preferred tactics.

%Project Naptime most closely aligns with our evaluation ideology: to include high-fidelity simulation and emulation, where open-source tools (give example) can be utilized by the LLM with interactivity from the environment that is near equivalency to real-world expected outputs. Some open-source simulation tools like CyBench ____ incorporate structured CTF-style evaluations where the LLM acts as an operator on a terminal, however, these types of simulations lack the operational flow, courses of action and toolkit selection that real operators leverage for offensive cyber operations. Additionally, our simulation and emulation platforms provide the flexibility and fidelity for an LLM to iteratively reason to and through an environment as a human operator would.


\subsection{Capture-the-Flag (CTF) Evaluations and Benchmarks for LLMs} 

To date, the most widely used approach to evaluate LLMs for OCO, cyber-attack, red teaming, or penetration testing capabilities is the use of cyber security Capture-the-Flag (CTF) challenges or tasks. System architectures and integration details vary, but the core approach is to interface the LLM under test (through its prompt API) to CTF challenge environments and ask the LLM to complete the given challenge by interacting (via terminal or shell connection) with the challenge environment. This evaluation approach is highly attractive as CTF challenge corpuses are large and widely available, are highly codified and measurable, and have a (relatively) strong analog to real OCO when compared to other evaluation approaches. \textit{PentestGPT} ____, \textit{CyberSecEval 3} ____, Google DeepMind's LLM evaluations ____, \textit{PenHeal} ____, \textit{AutoAttacker} ____, \textit{Cybench} ____, \textit{EnIGMA} ____, \textit{InterCode-CTF} ____, \textit{3CB} ____, and others ____ ____ use CTF benchmarks as a means for LLM evaluation. Some efforts, notably \textit{CyberSecEval 3} ____, also use CTFs to evaluate OCO operator uplift, i.e., evaluating whether a human operator,  when given an LLM as a tool/resource, performs better at the CTF challenge. In our view, and as detailed in later sections, this form of ``copilot" test is a more appropriate evaluation given an attacker profile/paradigm.

It must be noted that not all the challenges from CTFs are universally accepted as accurate comparisons to real-world OCO environments. Some challenges are more realistic than others and thus have more important real-world implications for cyber security. For example, aspects of gamification, discrete state transitions, and scoped boundaries often preclude challenges from being a true analog to real environments. There have been efforts to further reduce this gap in evaluation. In ____ and ____, an explicitly scoped benchmark for Linux privilege escalation (priv-esc) vulnerabilities is established. The benchmark is relatively small (15 tasks), but each priv-esc task is the exact analog of a real-world priv-esc vulnerability. Additionally, the evaluation environment is stripped of any explicit gamification. In \textit{CyberSecEval 3} ____, a larger cyber range evaluation is created to allow for a more end-to-end ransomware emulation scenario to play out via the attacking LLM agent. End-to-end cyber-attack evaluation scenarios are generally more difficult and resource intensive, hence their lower occurrence in existing efforts.


\subsection{Multiple Choice and Free Response Tests}

In addition to the use of CTFs, one approach frequently seen in the research community is the use of benchmarks consisting of multiple-choice questions. Cyber benchmarking directly taken from general LLM benchmark approaches ____ is attractive. Not only is it measurable, but it also allows for the synthetic generation of question corpuses, providing greater scale. \textit{CyberSecEval} ____, \textit{CyberMetric} ____, \textit{SecEval} ____, \textit{SecQA} ____, and ____ all use multiple choice question benchmarks. Approaches to question generation and question subject/domains may vary among efforts. For example, \textit{CyberSecEval} ____ generates questions via templating and combinatorial expansions of pre-made cyber security question fragments and generative LLM augmentation. \textit{SecEval} ____ uses an LLM with prompts from textbooks, system documentation, technology guidelines, and industrial standards to generate multiple choice questions. However, attention must be given to the actual content and validity of the benchmark questions, whether generated or not. For example, in the \textit{CyberMetric} ____ CyberMetric-2000 benchmark, a stated cyber security benchmark, there are out-of-scope questions concerning contract law and the motivations of terrorism, as well as an incorrect question/answer pair that assert the ``ideal approach to securing an infrastructure" is ``developing a plan to address the business needs of the organization" over the clearly more correct answers of ``implementing a hierarchical strategy to identify assets and threats" and ``identifying vulnerabilities, threats, and countermeasures."

Alternatively, \textit{CyberSecEval} ____ uses free response question benchmarks where LLM responses are free form text. For evaluation of this benchmark, \textit{CyberSecEval} uses an additional LLM (not the one under test) to evaluate whether responses are effectively malicious (i.e., respond effectively to a question prompt for aiding with a malicious cyber-attack request/question). As the authors note, this metric is not perfect and is indicative of the challenge of deterministically evaluating free response benchmarks at scale.


In short, there are two major concerns with multiple choice question benchmarks. First, there is the challenge of creating multiple choice question benchmarks that contain effective questions that cannot be memorized or gamified and are related to the actual domain of evaluation. Second, with regards to evaluating LLMs for OCO capabilities, there is the debate on whether multiple choice questions serve as an appropriate proxy or indication of real offensive cyber capabilities. In later sections we detail a novel improvement for OCO multiple choice benchmarks that addresses some of these concerns.


\subsection{Vulnerability Identification and Exploitation}

Although vulnerability identification and exploitation are often included within individual challenges in Capture-The-Flag (CTF) competitions, they are increasingly emerging as a distinct benchmark category. The purpose of these evaluations is to assess whether LLMs can demonstrate performance in analyzing static code or running software for vulnerabilities. Some efforts take this evaluation further and when a vulnerability is found, proceed to challenge the LLM to then create exploit implementations for exploiting the vulnerability for effect. \textit{CyberSecEval 2} ____ and \textit{Project Naptime} ____ uses \textit{CyberSecEval 2}'s synthetic code samples, which contain vulnerabilities, for evaluating LLMs. \textit{Project Naptime} ____ augments LLM's under test with additional tooling integrations to include a code browser, Python interpreter, and debugger to evaluate for improved performance against the benchmark. Google DeepMind's frontier model evaluation effort ____ tests for classifying security patches and identifying vulnerable code/functions. \textit{eyeballvul} ____ consists of a continuously updated benchmark of a large repository of open-source software versions and known vulnerabilities, which can be used to evaluate LLM’s against. The scalability and breadth of these approaches is promising.

Alternatively to static code analysis benchmarks, \textit{LLM Agents can Autonomously Exploit One-day Vulnerabilities} ____ and \textit{LLM Agents can Autonomously Hack Websites} ____  emulate vulnerable websites and applications for targeting by LLMs under test. These benchmarks are smaller, but the test cases are live systems and applications. While not a benchmark or evaluation suite, \textit{Vulnhuntr} ____ is an LLM-enabled system that analyzes Python code (and function tracing) for vulnerabilities in open-source code bases; to date, it has found dozens of exploitable vulnerabilities. Lastly, there is the ongoing DARPA Artificial Intelligence Cyber Challenge (AIxCC) ____ competition to create LLM-enabled systems for finding and patching vulnerable code at scale. To our knowledge, the evaluation approaches that the performers (i.e., the project teams that execute the research and development) are using to test their systems have not been made public as the competition is still ongoing at time of publication.


The primary challenges to vulnerability identification and exploit benchmarks are determining whether the LLM under test is memorizing versus reasoning and generalizing when they do actually find vulnerabilities. The latter indicates a much greater risk.


\subsection{LLM/AI Systems} 

By necessity, each of the aforementioned evaluation efforts have developed an LLM-enabled system (or as more generally referred to, an AI system) to actuate and evaluate the model under test. Some of these systems are very lightweight, designed to merely support the action and observation loop between the LLM agent and the evaluation: examples include \textit{Cybench}____ and \textit{InterCode-CTF} ____. Other LLM systems maintain a moderate set of scaffolding and integrated functionality, to include \textit{Vulnhuntr} ____ and \textit{AutoAttacker} ____. And, in line with a natural progression, are heavy weight LLM systems that may have extensive technology or tool interfaces, i.e., Agent Computer Interfaces (ACIs) ____; multiple LLMs/models for different functionality purposes; larger sub-components for observation parsing \& summarizing, reasoning/planning over action selection and sequences; and ad-hoc human feedback. \textit{PentestGPT} ____, \textit{Project Naptime} ____, \textit{EnIGMA} ____, and most recently the multi-stage attack LLM interface, \textit{Incalmo} ____, are examples of these growing and larger LLM agent systems. Furthermore, \textit{Project Naptime} ____ and \textit{EnIGMA} ____ are examples of what will become the paradigm in LLM systems: having standard, effective, and tight integrations with programming interpreters, debuggers, code analyzers, web APIs and task management.


\subsection{OCCULT Goals}

In the context of existing OCO LLM evaluation efforts, the goal of this work is to:
\begin{itemize}
\item \vspace{-0.2cm} Inform on and unify the methodology for effective and scalable OCO LLM testing,  providing hard evidentiary results that ultimately lead to clear, open-source risk implications. 
\item \vspace{-0.2cm} Evolve testing towards breadth of coverage on OCO capability areas that are most concerning to the community and/or represent large remaining gaps in coverage. \
\item \vspace{-0.2cm} Improve standardization, shareability, and tooling of LLM OCO evaluation benchmarks. \
\end{itemize}

In our view, without meaningful progress on these points, OCO LLM evaluations will remain fragmented, non-comprehensive, and incapable of keeping pace with the exponential creation and proliferation of LLMs.
The OCCULT methodology aims to provide a more realistic and standardized testing, which allows for better comparisons across models, training datasets, and user approaches. Our work also strives to assess how LLMs compare not just to each other, but to the humans that currently perform in the roles those models aim to replace.