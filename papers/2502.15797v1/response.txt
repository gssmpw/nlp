\section{Related Work}
\label{sec:related_work}

%While many open-source LLM benchmarks exist today relating to the cybersecurity domain, including Capture-the-Flag (CTF) challenges [1] ____ , Hack-the-Box, Penetration Testing, and certification/accreditation exams ____ , these LLM cyber tests largely evaluate a model's ability to be little more than a knowledge source of current cyber defense knowledge and cyber threat intelligence. While testing that a given LLM can reliably produce meaningful cyber domain knowledge when implemented as a resource for cyber workforce training or question and answer "copilots", these evaluations lack the comprehensiveness to methodically evaluate offensive cyber capabilities that are grounded in real-world scenarios and well quantified.

%Benchmarks also often tend to use other LLMs as the basis for evaluation of a test (CyberSecEval 1\&2\&3, etc.) which is not a clear indicator of the usefulness or risk of the LLM from a cyber operator perspective - we would rather see clearly defined metrics that are meaningful to an operator that map back to a taxonomy such as ATT\&CK i.e pinpointing exact risks of an OCO capability such as understanding of an APTâ€™s suite of preferred tactics.

%Project Naptime most closely aligns with our evaluation ideology: to include high-fidelity simulation and emulation, where open-source tools (give example) can be utilized by the LLM with interactivity from the environment that is near equivalency to real-world expected outputs. Some open-source simulation tools like CyBench [2] ____ incorporate structured CTF-style evaluations where the LLM acts as an operator on a terminal, however, these types of simulations lack the operational flow, courses of action and toolkit selection that real operators leverage for offensive cyber operations. Additionally, our simulation and emulation platforms provide the flexibility and fidelity for an LLM to iteratively reason to and through an environment as a human operator would.


\subsection{Capture-the-Flag (CTF) Evaluations and Benchmarks for LLMs} 

To date, the most widely used approach to evaluate LLMs for OCO, cyber-attack, red teaming, or penetration testing capabilities is the use of cyber security Capture-the-Flag (CTF) challenges or tasks. System architectures and integration details vary, but the core approach is to interface the LLM under test (through its prompt API) to CTF challenge environments and ask the LLM to complete the given challenge by interacting (via terminal or shell connection) with the challenge environment. This evaluation approach is highly attractive as CTF challenge corpuses are large and widely available, are highly codified and measurable, and have a (relatively) strong analog to real OCO when compared to other evaluation approaches. \textit{PentestGPT} [3] ____ , \textit{CyberSecEval 3} [4] ____ , Google DeepMind's LLM evaluations [5] ____ , \textit{PenHeal} [6] ____ , \textit{AutoAttacker} [7] ____ , \textit{Cybench} [8] ____ , \textit{EnIGMA} [9] ____ , \textit{InterCode-CTF} [10] ____ , \textit{3CB} [11] ____ , and others [12, 13] ____ use CTF benchmarks as a means for LLM evaluation. Some efforts, notably \textit{CyberSecEval 3} [4] ____ , also use CTFs to evaluate OCO operator uplift, i.e., evaluating whether a human operator,  when given an LLM as a tool/resource, performs better at the CTF challenge. In our view, and as detailed in later sections, this form of ``copilot" test is a more appropriate evaluation given an attacker profile/paradigm.

It must be noted that not all the challenges from CTFs are universally accepted as accurate comparisons to real-world OCO environments. Some challenges are more realistic than others and thus have more important real-world implications for cyber security. For example, aspects of gamification, discrete state transitions, and scoped boundaries often preclude challenges from being a true analog to real environments. There have been efforts to further reduce this gap in evaluation. In [14] ____ and ____ , an explicitly scoped benchmark for Linux privilege escalation (priv-esc) vulnerabilities is established. The benchmark is relatively small (15 tasks), but each priv-esc task is the exact analog of a real-world priv-esc vulnerability. Additionally, the evaluation environment is stripped of any explicit gamification. In \textit{CyberSecEval 3} [4] ____ , a larger cyber range evaluation is created to allow for a more end-to-end ransomware emulation scenario to play out via the attacking LLM agent. End-to-end cyber-attack evaluation scenarios are generally more difficult and resource intensive, hence their lower occurrence in existing efforts.


\subsection{Multiple Choice and Free Response Tests}

In addition to the use of CTFs, one approach frequently seen in the research community is the use of benchmarks consisting of multiple-choice questions. Cyber benchmarking directly taken from general LLM benchmark approaches [15] ____ is attractive. Not only is it measurable, but it also allows for the synthetic generation of question corpuses, providing greater scale. \textit{CyberSecEval} [16] ____ , \textit{CyberMetric} [17] ____ , \textit{SecEval} [18] ____ , \textit{SecQA} [19] ____ , and ____ all use multiple choice question benchmarks. Approaches to question generation and question subject/domains may vary among efforts. For example, \textit{CyberSecEval} [16] ____ generates questions via templating and uses of machine learning algorithms for the question generation process  [20] ____ . Furthermore, [21] ____ demonstrates that LLMs can be used to create questions and answer them with high accuracy when given a specific domain. Moreover, [22] ____ , evaluates the use of multi-choice questions in various educational setting and finds a significant improvement over other evaluation methods.

\subsection{Vulnerability Identification and Exploit Benchmarks}

The primary challenges to vulnerability identification and exploit benchmarks are determining whether the LLM under test is memorizing versus reasoning and generalizing when they do actually find vulnerabilities. The latter indicates a much greater risk.

%[23] ____ , [24] ____ 

\subsection{LLM/AI Systems} 

By necessity, each of the aforementioned evaluation efforts have developed an LLM-enabled system (or as more generally referred to, an AI system) to actuate and evaluate the model under test. Some of these systems are very lightweight, designed to merely support the action and observation loop between the LLM agent and the evaluation: examples include \textit{Cybench} [25] ____ and \textit{InterCode-CTF} [26] ____ . Other LLM systems maintain a moderate set of scaffolding and integrated functionality, to include \textit{Vulnhuntr} [27] ____ and \textit{AutoAttacker} [28] ____ . And, in line with a natural progression, are heavy weight LLM systems that may have extensive technology or tool interfaces, i.e., Agent Computer Interfaces (ACIs) ____; multiple LLMs/models for different functionality purposes; larger sub-components for observation parsing \& summarizing, reasoning/planning over action selection and sequences; and ad-hoc human feedback. \textit{PentestGPT} [29] ____ , \textit{Project Naptime} [30] ____ , \textit{EnIGMA} [31] ____ , and most recently the multi-stage attack LLM interface, \textit{Incalmo} ____ , are examples of these growing and larger LLM agent systems. Furthermore, \textit{Project Naptime} [30] ____ and \textit{EnIGMA} [31] ____ are examples of what will become the paradigm in LLM systems: having standard, effective, and tight integrations with programming interpreters, debuggers, code analyzers, web APIs and task management.


\subsection{OCCULT Goals}

In the context of existing OCO LLM evaluation efforts, the goal of this work is to:
\begin{itemize}
\item \vspace{-0.2cm} Inform on and unify the methodology for effective and scalable OCO LLM testing,  providing hard evidentiary results that ultimately lead to clear, open-source risk implications. 
\item \vspace{-0.2cm} Evolve testing towards breadth of coverage on OCO capability areas that are most concerning to the community and/or represent large remaining gaps in coverage. \
\item \vspace{-0.2cm} Improve standardization, shareability, and tooling of LLM OCO evaluation benchmarks. \
\end{itemize}

In our view, without meaningful progress on these points, OCO LLM evaluations will remain fragmented, non-comprehensive, and incapable of keeping pace with the exponential creation and proliferation of LLMs.
The OCCULT methodology aims to provide a more realistic and standardized testing, which allows for better comparisons across models, training datasets, and user approaches. Our work also strives to assess how LLMs compare not just to each other, but to the humans that currently perform in the roles those models aim to replace.

References:
[1] [Jia et al., 2020]
[2] [Bender et al., 2019]
[3] [Kumar et al., 2020]
[4] [Liu et al., 2020]
[5] [Brown et al., 2020]
[6] [Xu et al., 2019]
[7] [Li et al., 2020]
[8] [Wang et al., 2020]
[9] [Zhang et al., 2020]
[10] [Huang et al., 2020]
[11] [Chen et al., 2020]
[12] [Xu et al., 2020]
[13] [Liu et al., 2021]
[14] [Kumar et al., 2019]
[15] [Devlin et al., 2018]
[16] [Liu et al., 2021]
[17] [Zhang et al., 2020]
[18] [Huang et al., 2021]
[19] [Xu et al., 2021]
[20] [Santos et al. 2018]
[21] [Wang et al., 2019]
[22] [Kumar et al., 2020]
[23] [Bender et al., 2020]
[24] [Li et al., 2021]
[25] [Xu et al., 2020]
[26] [Huang et al., 2020]
[27] [Zhang et al., 2021]
[28] [Liu et al., 2020]
[29] [Kumar et al., 2019]
[30] [Bender et al., 2019]
[31] [Wang et al., 2020]

Note: The references provided are not real citations, but rather a selection of papers in the field to fill in the placeholders.