\section{Related Works}
\begin{figure*}[t]
\begin{center}
\includegraphics[scale=0.5]{ilovepdf-compress_2/overview-compressed.pdf}
\caption{The production pipeline of ChineseSimpleVQA consists of automated verification, difficulty filtering, and human verification. This process generates multi-hop questions (i.e. object recognition Q\&A and merged Q\&A).}
\label{fig.method}
\end{center}
\vspace{-0.6cm}
\end{figure*}

\paragraph{VQA Benchmarks:}
The problem of Visual Question Answering (VQA) was originally conceptualized as a Visual Turing Test **Malcolm, "The Measurement of Intelligence"**. Addressing this test necessitates that computational systems replicate human faculties, including real-world visual recognition, comprehension of language, rudimentary reasoning, and factual knowledge. Certain datasets emphasize the evaluation of perceptual and linguistic understanding capabilities **Antol, et al., "VQA: Visual Question Answering"**, whereas others delve into the intricacies of complex reasoning processes **Goyal, et al., "Making the V in VQA Matter"**. Notwithstanding these advancements, assessing factuality remains a formidable challenge, an aspect that has received limited attention within the scope of VQA datasets.

\paragraph{Factuality Benchmarks:}
Several datasets also necessitate factual or commonsense knowledge **Talmet, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering"**. For instance, OK-VQA **Talmet, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering"** evaluates a model's capability to answer questions using open-domain knowledge. Building upon this, A-OKVQA **Zhang, et al., "A-OKVQA: Integrating External Knowledge in Visual Question Answering"** integrates diverse external knowledge and reasoning capabilities, while S3VQA **Yin, et al., "S3VQA: Spatio-Temporal Reasoning for Video Question Answering"** introduces a dataset featuring questions that require object detection within images. Recently, factuality benchmarks have become increasingly important for LLMs: SimpleQA introduced by OpenAI **Wang, et al., "SimpleQA: Evaluating Factuality in Short-Form Text"** assesses short-form factuality, and a Chinese SimpleQA **Zhang, et al., "Chinese SimpleQA: Evaluating Factuality in Chinese"** is proposed by Alibaba Group. 

%In the field of natural language processing (NLP), factuality benchmarks are increasingly gaining attention for testing large models as knowledge bases. Recently, OpenAI introduced SimpleQA **Wang, et al., "SimpleQA: Evaluating Factuality in Short-Form Text"** to assess short-form factuality in large language models (LLMs), with Chinese SimpleQA **Zhang, et al., "Chinese SimpleQA: Evaluating Factuality in Chinese"** focusing on evaluating factuality in Chinese. However, corresponding efforts in the multimodal domain remain scarce.

\paragraph{Comparison to other knowledge-based VQA benchmarks:}
In Table~\ref{table.1}, we present a comparative analysis of ChineseSimpleVQA alongside several mainstream knowledge-based VQA benchmarks. Our dataset represents the first Chinese multimodal evaluation set designed to comprehensively assess factual abilities, thereby addressing a significant gap in the multimodal domain.
Moreover, the multi-hop questioning in our dataset queries the image's content and related facts, effectively evaluating the model's object recognition and factual knowledge handling.
%Moreover, the multi-hop questioning pipeline employed in our dataset involves querying both the content of the image and related factual information. This approach facilitates a more thorough evaluation of the model's image recognition capabilities and its proficiency in handling factual questions, thereby enhancing assessment efficiency.
%The generative evaluation method utilized in ChineseSimpleVQA closely mimics real-world scenarios, providing a practical and rigorous test environment. Furthermore, our benchmark offers more extensive coverage compared to similar datasets, ensuring a comprehensive assessment of the model's performance across various dimensions.


% Our dataset is the first Chinese evaluation set to adopt a generative approach for comprehensively evaluating factual abilities. 
% Notably, C-Eval and CMMLU primarily use multiple-choice evaluation methods, which may introduce option bias and reduce question difficulty, making it easier for models to guess the correct answer rather than truly understand the question (detailed experimental analysis is in Appendix C). 
%This paper aims to create a dataset for evaluating short-form factuality in Chinese within the multimodal domain. We also propose a multi-hop evaluation mechanism to assess image recognition capabilities by breaking down questions.