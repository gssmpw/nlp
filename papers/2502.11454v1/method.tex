\section{\textsc{UniCBE}}
% \section{CBE with Better Scalability, Accuracy and Convergence}
% 以上的讨论揭示了为CBE带来Scalability, Accuracy and Convergence的guidelines，基于此我们在本节提出可以兼顾这些特性的CBE方法，UniCBE。
The discussions above reveal guidelines for strengthening scalability, accuracy, and convergence in CBE. Based on this, we propose \textsc{UniCBE}, a unified uniformity-driven framework that can simultaneously enhance these objectives well.

\subsection{Budget Allocation}
% 为了实现尽可能均匀的采样，从而最少地引入采样bias，我们如下构建P：
To ensure the uniformity of tuple combination sampling for minimizing the introduction of sampling bias according to \S\ref{sec:3.2}, we construct $P^{acc\text{-}l}$ 
 at iteration $l$ as follows:
\begin{equation}
    P^{acc\text{-}l}_{i,j,k} = \alpha^{-\sum_{k=1}^{N} C^l_{i,j,k}}\times \alpha^{-\sum_{i=1}^{M} C^l_{i,j,k}}\times \alpha^{-\sum_{j=1}^{M} C^l_{i,j,k}}
    \label{eq:11}
\end{equation}
where $\sum_{k=1}^{N} C^l_{i,j,k}$ denotes the times model pair $(m_i,m_j)$ has been compared, $\sum_{i=1}^{M} C^l_{i,j,k}$ and $\sum_{j=1}^{M} C^l_{i,j,k}$ denote the times model $m_i$ and $m_j$ has been tested on $s_k$ respectively. 
If certain model-model combination or model-sample combination have been sampled multiple times,~\eqref{eq:11} will reduce the probability of such combinations being selected again, thereby achieving sufficient uniformity to minimize the introduction of bias between models and samples, respectively.

To accelerate the convergence of evaluation results, we construct $P^{con\text{-}l}$ according to \S\ref{sec:3.3} as follows:
\begin{equation}
    P^{con\text{-}l}_{i,j,k} = \epsilon^l_{i,j}
    \label{eq:12}
\end{equation}
Sampling specific model pair helps reduce the uncertainty of their win rate estimation according to~\eqref{eq:10}. By sampling proportionally to the win rate uncertainty matrix, we can uniformly decrease the uncertainty for each model pair, thereby facilitating convergence.
% Based on~\eqref{eq:10} that sampling specific model pair help reduce the uncertainty of their win rate, sampling according to the win rate uncertainty matrix can ensure that the uncertainty of each pair of models is reduced simultaneously, thereby facilitating convergence.
% 
%如果模型pair或模型样本pair已经被多次采样了，（15）将减小这样的pair再次被选中的概率，从而实现足够的均匀性来减少模型间bias和样本间bias的引入。

We construct $P^{sca\text{-}l}$ to allocate more preference budget to the newly introduced model so as to improving the scalability according to \S\ref{sec:3.4} as follows:
\begin{equation}
    P^{sca\text{-}l}_{i,j,k} = \alpha^{-\sum_{k=1}^{N}\sum_{i=1}^{M} C^l_{i,j,k}}\times \alpha^{-\sum_{k=1}^{N}\sum_{j=1}^{M} C^l_{i,j,k}}
    \label{eq:13}
\end{equation}
%最后，我们集成上述三个矩阵来获得P，从而实现依据P采样时能够同时兼顾accuracy、convergence和scalability：
Finally, we integrate the matrices mentioned above to obtain $P^l$, ensuring that sampling according to $P^l$ can simultaneously balance the accuracy, convergence, and scalability of evaluation results:
\begin{equation}
\begin{aligned}
    P^{l} = \frac{P^{acc\text{-}l} \circ P^{con\text{-}l} \circ P^{sca\text{-}l}}{\sum (P^{acc\text{-}l} \circ P^{con\text{-}l} \circ P^{sca\text{-}l})}
    \label{eq:14}
\end{aligned}
\end{equation}


\subsection{Tuple Sampling}
% 在获得$P^l$后，我们要依据它采样tuple for judging. 我们考虑两种 tuple sampling策略：probabilistic sampling and greedy sampling.
After obtaining \( P^l \), we need to sample tuples for judging based on it. Two tuple sampling strategies are considered: 
\begin{itemize}[leftmargin=20pt]
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item \textbf{probabilistic sampling $f^{ts}_{p}$} means sampling tuple directly according to \( P^l \).
\item \textbf{greedy sampling $f^{ts}_{g}$} means selecting the tuple with the maximum probability in \( P^l \).
\end{itemize}
The default tuple sampling strategy of \textsc{UniCBE} is $f^{ts}_{g}$, which can avoid the suboptimal achievement of objectives due to uncertainty in the sampling process.

\subsection{Preference Aggregation}
As discussed in \S\ref{sec:2-pa}, mainstream preference aggregation strategies include averaging win rate $f^{pa}_{avg}$, Elo rating system $f^{pa}_{Elo}$ and Bradley-Terry model $f^{pa}_{BT}$. In our preliminary experiment (Figure~\ref{fig:bias3}) we observe that $f^{pa}_{BT}$ can better alleviate sampling bias, for which we choose it as our default setting.
