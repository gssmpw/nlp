\begin{abstract}
%Human preference在衡量和引导LLM更好地与人类对齐方面发挥了重要作用。然而当前的COMPARING BASED EVALUATION方法通常只关注单一的目标而无法有效地利用珍贵的preference signals。为此，我们深入分析了提升CBE的准确性、收敛性和可扩展性的关键分别在于抑制采样bias、平衡不确定性下降过程和削弱updating uncertainty。遵循这些guidelines，我们提出了UniCBE方法，其通过解耦地构建三个采样概率矩阵并进行集成来平衡优化三个核心目标。我们还消融了最优的tuple sampling和PREFERENCE AGGREGATION策略，从而实现高效的CBE。在AlpacaEval上，UniCBE saves over 17% of evaluation budgets when 达到了与ground truth超过0.995的皮尔逊一致性，展现了良好的准确性与收敛性。在有新模型持续加入的场景中，UniCBE can even save over 50% of evaluation costs，showcasing excellent scalability.
Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty.
Following the derived guidelines, we propose \textsc{UniCBE}, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE.
On the AlpacaEval benchmark, \textsc{UniCBE} saves over 17\% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, \textsc{UniCBE} can even save over 50\% of evaluation costs, highlighting its improved scalability.
\end{abstract}