% \clearpage
% \appendix
\section{Proof of Theorem in \S\ref{sec:3.2}}
\label{app:proof}
Given that $\sum_{i=1}^U X_i = 0$, we want to attain sampling set $\mathcal{S}$ that satisfies $|\mathcal{S}|=V$ and:
\begin{equation}
    \mathcal{S} = argmin_{\mathcal{S}} | \sum\limits_{i \in \mathcal{S}} X_i | = argmin_{\mathcal{S}} (\sum\limits_{i \in \mathcal{S}} X_i )^2
    \label{eq-ap:1}
\end{equation}
Firstly, it is easy to know that for any sampling set $\mathcal{S}$:
\begin{equation}
    \mathbb{E}[ \sum\limits_{i \in \mathcal{S}} X_i ]=0
    \label{eq-ap:2}
\end{equation}
Thus, 
\begin{equation}
    \mathbb{E}[(\sum\limits_{i \in \mathcal{S}} X_i)^2] = 
    \mathbb{E}[(\sum\limits_{i \in \mathcal{S}} X_i-0)^2] =
    \mathbb{E}[(\sum\limits_{i \in \mathcal{S}} X_i-\mathbb{E}[ \sum\limits_{i \in \mathcal{S}} X_i ])^2] =\mathbb{E}[  \mathrm{Var}[ \sum\limits_{i \in \mathcal{S}} X_i ] ]
    \label{eq-ap:3}
\end{equation}
Considering that:
\begin{equation}
    \mathrm{Var}[ \sum\limits_{i \in \mathcal{S}} X_i ] =  \sum\limits_{i \in \mathrm{set}(\mathcal{S})} c_i^2\mathrm{Var}(X_i)
    \label{eq-ap:4}
\end{equation}
where $c_i$ denotes the number of $X_i$ in $\mathcal{S}$. On this basis, we derive that:
\begin{equation}
\begin{aligned}
    \mathbb{E}[(\sum\limits_{i \in \mathcal{S}} X_i)^2] &= \mathbb{E}[\mathrm{Var}(X)]\sum\limits_{i \in \mathrm{set}(\mathcal{S})} c_i^2\\
&\geq \mathbb{E}[\mathrm{Var}(X)](\sum\limits_{i \in \mathrm{set}(\mathcal{S})} c_i)^2 |\mathrm{set}(\mathcal{S})|^{-1}\\
&=V^2\mathbb{E}[\mathrm{Var}(X)] |\mathrm{set}(\mathcal{S})|^{-1} \\
&\geq V^2\mathbb{E}[\mathrm{Var}(X)]\mathrm{min}(U,V)^{-1}
\end{aligned}
\label{eq-ap:5}
\end{equation}
The equality condition of the first inequality is: the number of samples taken from each category is equal. The equality condition of the second inequality is: the number of sampled categories equals to $\mathrm{min}(U,V)$. These two conditions imply that a completely uniform sampling strategy is optimal.

\section{Introduction of Elo Rating System and Bradley-Terry Model}
\subsection{Elo Rating System}
\label{app:elo}
The Elo rating system \citep{elo} is widely used to rank participants based on their relative performance in competitive settings. Given two models, $A$ and $B$, with initial ratings $R_A$ and $R_B$, the expected score of model $A$ in a pairwise comparison is calculated as:
\[
E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}
\]
Similarly, the expected score for model $B$ is:
\[
E_B = \frac{1}{1 + 10^{(R_A - R_B)/400}}
\]
After the comparison, the actual results are used to update the ratings. If model $A$ wins, its new rating $R_A'$ is updated as:
\[
R_A' = R_A + K(S_A - E_A)
\]
where $S_A$ is the actual result of the match (1 for a win, 0 for a loss, and 0.5 for a draw), and $K$ is a constant that controls the sensitivity of the rating adjustment. Model $B$'s rating is updated in a similar way:
\[
R_B' = R_B + K(S_B - E_B)
\]
where $S_B$ is the actual result for model $B$.

When extending the Elo rating system to multiple models, we consider a set of $n$ models. Pairwise comparisons between the models are conducted, resulting in $\binom{n}{2}$ unique pairs:
\[
\binom{n}{2} = \frac{n(n-1)}{2}
\]
Each pair is evaluated using the Elo score update rules, and the results are iteratively applied to adjust the ratings, ensuring that each model’s rating reflects its relative performance within the set.

The extension to multiple models leverages the transitive property. For any three models $i, j, k \in \{1, 2, \dots, n\}$, if $R_i > R_j$ and $R_j > R_k$, the transitivity implies:
\[
R_i > R_j \quad \text{and} \quad R_j > R_k \quad \implies \quad R_i > R_k
\]
This property ensures consistency in the rankings, even when individual match outcomes vary. By iterating over all $\binom{n}{2}$ comparisons, the Elo scores converge to reflect the overall capabilities of the models, with higher scores indicating stronger performance.




\subsection{Bradley-Terry Model}
\label{app:bt}
The Bradley-Terry model \citep{BT} estimates the probability that one model outperforms another in pairwise comparisons. For two models $M_i$ and $M_j$ with strength parameters $\xi_i$ and $\xi_j$, the probability that model $i$ beats model $j$ is modeled as:
\begin{equation}
\label{eq:bt}
P(M_i>M_j) = \frac{1}{1 + e^{\xi_j - \xi_i}}    
\end{equation}
where $\xi$ is an $|M|$-length vector of Bradley-Terry coefficients. Given a set of comparing results $\mathcal{S}=\{(M_i^t,M_j^t,R^t)\}_{t=1}^{T}$ where $R^t$ represents the degree $M_i^t$ wins over $M_j^t$. We set $\rm{mean}(\xi) = 0$.
After attaining the BT scores using $f^{pa}$, we calculate the estimated win rate matrix with~\eqref{eq:bt}.
% The Bradley-Terry coefficients are estimated by minimizing the binary cross-entropy loss:
% \[
% s(P) = \arg \min_{\xi} \mathbb{E}_{\mathcal{S}} \left[ \ell \left( R, \frac{1}{1 + e^{\xi_{i} - \xi_{j}}} \right) \right]
% \]
% where $\ell(h, p) = -(h \log(p) + (1 - h) \log(1 - p))$. This ensures robust estimation of model rankings.

% The standard Bradley-Terry model assumes transitivity, which can struggle with non-transitive preferences or misspecified logistic assumptions. To address this, the Nonparametric Bradley-Terry model relaxes transitivity by evaluating paths of model pairings leading to a specific model $m$. The score for model $m$ is:
% \[
% s(\theta)_m = \frac{1}{|G(m)|} \sum_{g \in G(m)} \left( \log \frac{\theta'(g_1)}{1 - \theta'(g_1)} + \sum_{a \in g} \log \frac{\theta'(a)}{1 - \theta'(a)} \right)
% \]
% where $G(m)$ represents all paths to $m$, and $\theta'(a)$ captures the win probability along each path.

% In the original Bradley-Terry model, the probability that model $m$ beats model $m'$ is:
% \[
% \theta((m', m)) = \frac{e^{\xi_m}}{e^{\xi_m} + e^{\xi_{m'}}}
% \]
% where $\xi_m$ and $\xi_{m'}$ are the Bradley-Terry coefficients. The goal is to estimate these coefficients from outcomes.

% To recover the Bradley-Terry coefficients in a nonparametric setting, the log odds for each comparison along the path $g \in G(m)$ are computed as:
% \[
% \log \frac{\theta((1, m'))}{1 - \theta((1, m'))} + \log \frac{\theta((m', m))}{1 - \theta((m', m))}
% \]
% Summing over all paths leading to model $m$, the Bradley-Terry coefficient $\xi_m$ is recovered as:
% \[
% \xi_m = \xi_1 + \sum_{g \in G(m)} (\xi_{m'} - \xi_{m''})
% \]
% where $m'$ and $m''$ are successive models in the comparison path. This method allows the recovery of the Bradley-Terry coefficients when the parametric model is well-specified.










\section{More Experimental Analyses}
\begin{figure}[h]
    \centering
    \subfigure{
    \label{fig:351m}
        \includegraphics[width=0.31\textwidth]{figs/performance-add_new-4o-pre_wins_theta_dyn.pdf}}
    \hfill
    \subfigure{
    \label{fig:355m}
        \includegraphics[width=0.31\textwidth]{figs/performance-add_new-4o-s_dyn.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:356m}
        \includegraphics[width=0.31\textwidth]{figs/performance-add_new-4o-p_dyn.pdf}
    } \vspace{-0.2cm}\\
    \subfigure{
    \label{fig:mt1m}
        \includegraphics[width=0.31\textwidth]{figs/save-add-4o-pre_wins_theta_dyn.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt5m}
        \includegraphics[width=0.31\textwidth]{figs/save-add-4o-s_dyn.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt6m}
        \includegraphics[width=0.31\textwidth]{figs/save-add-4o-p_dyn.pdf}
    }
    \vspace{-0.2cm}
    \caption{Results of compared CBE methods in a scenario where models and samples are dynamically added or removed at a random frequency. }
    \vspace{-0.3cm}
    \label{fig:dynamic}
\end{figure}
\subsection{Performance in Scenarios Close to Reality}
We think that conducting experiments in settings that are closer to real-world scenarios (highly dynamic and requiring real-time evaluation) can help us more comprehensively assess \textsc{UniCBE} and the baseline methods. To this end, we perform the following experiments: Starting with a sample size of $N=600$ and model number of $M=12$, we execute a random operation at each time step. The operations included: adding one model to be evaluated with a probability of 0.01, removing one model with a probability of 0.01, adding one potential sample with a probability of 0.01, randomly deleting one sample with a probability of 0.01, and taking no action with a probability of 0.96. Based on the experimental results shown in Figure~\ref{fig:dynamic}, we have the following observations:
\begin{itemize}[leftmargin=20pt]
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item The convergence speed of all baseline methods significantly slowed down compared to Figure~\ref{fig:main-4o}. None of the baseline methods achieve a Spearman correlation coefficient of 0.96 or a Pearson correlation coefficient of 0.97 by $T=2000$, highlighting the difficulty of model evaluation in this setting. In contrast, \textsc{UniCBE} achieve rapid convergence, reaching a Spearman coefficient of approximately 0.97 and a Pearson coefficient exceeding 0.98 by $T=2000$.
\item Over the long term, as $T$ increases, \textsc{UniCBE} consistently demonstrates over 10\% savings in preference budget across all metrics, even under this challenging setting, showcasing its strong practicality.
\item An interesting observation is that \textsc{AlpacaEval} exhibits better convergence in the early stages compared to \textsc{Random} and \textsc{Arena}, supporting our previous conclusions in Table~\ref{tb:intro}. However, as $T$ increases, \textsc{AlpacaEval}'s lack of accuracy optimization objective leads to its performance being surpassed by \textsc{Random} and \textsc{Arena}.
\end{itemize} 

\subsection{Ablation Study of Uniformity Constraints}
Based on our analysis in \S\ref{sec:3}, the degree to which uniformity is achieved is positively correlated with performance in terms of accuracy, convergence, and scalability. To explore the empirical relationship between the degree of uniformity constraints and the final outcomes, we draw inspiration from the concept of temperature-based control in random sampling. By adjusting the temperature $T$ in the following formula for sampling $f^{ts}_T$, we regulate the extent of uniformity constraints according to $P^l$ in ~\eqref{eq:14}:
\begin{equation}
    f^{ts}_T(i,j,k) =\frac{(P^l_{i,j,k})^{-T}}{\sum (P^{l})^{-T}}
\end{equation}
As \(T\) increases, the uniformity constraints become more relaxed. When \(T=0\), it corresponds to greedy sampling \(f^{ts}_g\), which imposes the strictest uniformity constraints. When \(T=1\), it corresponds to probabilistic sampling \(f^{ts}_p\), which imposes general uniformity constraints. When \(T=+\infty\), it corresponds to random sampling, where no uniformity constraints are applied. 
Our experimental results are shown in Figure~\ref{fig:T}. 
As \(T\) increases from 0 to \(T=+\infty\), the evaluation results progressively deteriorate. This indicates that adopting greedy sampling to impose the strictest uniformity constraints yields the optimal evaluation performance. This observation also validates the correctness of our conclusions in \S\ref{sec:3}.

\begin{figure}[h]
    \centering
    \subfigure{
    \label{fig:351m}
        \includegraphics[width=0.31\textwidth]{figs/performance-T-4o-pre_wins_theta.pdf}}
    \hfill
    \subfigure{
    \label{fig:355m}
        \includegraphics[width=0.31\textwidth]{figs/performance-T-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:356m}
        \includegraphics[width=0.31\textwidth]{figs/performance-T-4o-p.pdf}
    } \vspace{-0.2cm}\\
    \subfigure{
    \label{fig:mt1m}
        \includegraphics[width=0.31\textwidth]{figs/save-T-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt5m}
        \includegraphics[width=0.31\textwidth]{figs/save-T-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt6m}
        \includegraphics[width=0.31\textwidth]{figs/save-T-4o-p.pdf}
    }
    \vspace{-0.2cm}
    \caption{Results of \textsc{UniCBE} with different sampling temperatures. }
    \vspace{-0.4cm}
    \label{fig:T}
\end{figure}

\subsection{Adjusting the Weights of Optimization Objectives}
In ~\eqref{eq:14}, we integrate sampling matrices targeting different optimization objectives with equal weights. In practice, when faced with varying requirements, it is straightforward to prioritize a specific objective by adjusting the weights \(\theta_{acc}\), \(\theta_{con}\), and \(\theta_{sca}\) for these matrices, as shown in ~\eqref{eq:theta}. 
\begin{equation}
    P^{l} = \frac{(P^{acc\text{-}l})^{\theta_{acc}} \circ (P^{con\text{-}l})^{\theta_{con}}  \circ (P^{sca\text{-}l})^{\theta_{sca}} }{\sum ((P^{acc\text{-}l})^{\theta_{acc}} \circ (P^{con\text{-}l})^{\theta_{con}}  \circ (P^{sca\text{-}l})^{\theta_{sca}})}
    \label{eq:theta}
\end{equation}
As demonstrated in Table~\ref{tab:beta_theta}, we set different settings and calculate the degree of achievement \(\beta\) for each optimization objective following the procedure described in \S\ref{app:beta}. Compared to equal-weight integration, users can increase the corresponding \(\beta\) (e.g., \(\beta_{acc}\)) by assigning a larger weight to a specific optimization objective (\(\theta_{acc}\)), thereby better meeting their practical needs (accuracy).
We also observe that enhancing a specific optimization objective often comes with a slight decrease in the achievement of other objectives. In Figure~\ref{fig:theta}, we illustrate an example of improving accuracy, where \(\theta_{acc}\) is increased from 1 to 2. We find that the increased focus on accuracy objective slightly slows down the convergence speed. As a result, when \(T\) is relatively small, the performance of \(\theta_{acc} = 2\) lags behind that of \(\theta_{acc} = 1\). However, in the later stages, after convergence, the enhanced accuracy objective enables \(\theta_{acc} = 2\) to outperform \(\theta_{acc} = 1\), resulting in greater savings in the preference budget.
\begin{table}[h]
\renewcommand\arraystretch{1.4}
\centering
\setlength{\tabcolsep}{0.6em} 
\vspace{-0.1cm}
\caption{The measurement results of the achievement of objectives in \S\ref{sec:3} for \textsc{UniCBE} with varied hyperparameters.}
\begin{tabular}{lcccc}
\toprule
\multirow{3}{*}{Settings}&$\theta_{acc} = 2$&$\theta_{acc} = 1$&$\theta_{acc} = 1$&$\theta_{acc} = 1$\\
\multirow{3}{*}{}&$\theta_{con} = 1$&$\theta_{con} = 2$&$\theta_{con} = 1$&$\theta_{con} = 1$\\
\multirow{3}{*}{}&$\theta_{sca} = 1$&$\theta_{sca} = 1$&$\theta_{sca} = 2$&$\theta_{sca} = 1$\\
\midrule
$\beta_{acc}$&.7380\tiny \textcolor{mygreen}{(+.0016)}&.7355\tiny \textcolor{red}{(-.0009)}&.7351\tiny \textcolor{red}{(-.0013)}&\textbf{.7364}\\
$\beta_{con}$&.9221\tiny \textcolor{red}{(-.0007)}&.9235\tiny \textcolor{mygreen}{(+.0007)}&.9217\tiny \textcolor{red}{(-.0011)}&\textbf{.9228}\\
$\beta_{sca}$&.9996\tiny \textcolor{red}{(-.0001)}&.9997\tiny \textcolor{gray}{(.0000)}&.9998\tiny \textcolor{mygreen}{(+.0001)}&\textbf{.9997}\\
\bottomrule
\end{tabular}
\vspace{-0.4cm}
\label{tab:beta_theta}
\end{table}
\begin{figure}[h]
    \centering
    \subfigure{
    \label{fig:351m}
        \includegraphics[width=0.31\textwidth]{figs/save-4o-pre_wins_theta_theta.pdf}}
    \hfill
    \subfigure{
    \label{fig:355m}
        \includegraphics[width=0.31\textwidth]{figs/save-4o-s_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:356m}
        \includegraphics[width=0.31\textwidth]{figs/save-4o-p_theta.pdf}
    }
    \vspace{-0.2cm}
    \caption{Results of \textsc{UniCBE} with different $\theta_{acc}$. }
    \vspace{-0.4cm}
    \label{fig:theta}
\end{figure}




\section{Prompt for Having an LLM Act as Judge}
\label{app:prompt}
We follow AlpacaEval\footnote{\url{https://github.com/tatsu-lab/alpaca_eval}} to instruct the LLMs act as judge with the following prompt:

\begin{quote}
{\itshape
You are a helpful assistant, that ranks models by the quality of their answers.
\\
\\
I want you to create a leaderboard of different of large-language models. To do so, I will give you the instructions (prompts) given to the models, and the responses of two models. Please give the winner model based on which responses would be preferred by humans. All inputs and outputs should be python dictionaries.
\\
\\
Here is the prompt:
\\
\{
\\
    "instruction": """\{instruction\}""",
\\
\}
\\
\\
Here are the outputs of the models:
\\
    \{
\\
        "model": "$model_1$",
\\
        "answer": """\{$output_1$\}"""
\\
    \},
\\
    \{
\\
        "model": "$model_2$",
\\
        "answer": """\{$output_2$\}"""
\\
    \}
\\
\\
Now please give the winner model according to the quality of their answers, so that the winner model has the best output. Then return the winner model in the following format if $model_x$ is the winner:
winner: $model_x$
\\
\\
You need to strictly follow the format above. Please provide the ranking that the majority of humans would give.
}
\end{quote}

\section{The Calculation Process of $\beta$ in Table~\ref{tab:beta}}
\label{app:beta}
%我们计算各CBE方法的$\beta$值来衡量其在多大程度上贴合了第三节我们所分析出的优化目标：ensures uniformity across tuples, uniformity across model pairs in win-rate uncertainty, and uniformity across model。
We calculate the $\beta$ values for each CBE method to measure how well they align with the optimization objectives we analyzed in \S\ref{sec:3}: ensuring uniformity across tuples, uniformity across model pairs in win-rate uncertainty, and uniformity across models.
Specifically, we first construct $U^{acc1}$, $U^{acc2}$, $U^{con}$ and $U^{sca}$ as follows:
\begin{equation}
\begin{gathered}
    U^{acc1}_{i,j} = \begin{cases} 0, \ \ \rm{if}\ \ i=j\\
    \frac{1}{M(M-1)}, \ \ \rm{else}
    \end{cases}\\
    U^{acc2}_{i,k} = \frac{1}{MN}
\end{gathered}
\end{equation}
\begin{equation}
    U^{con}_{i,j} = \begin{cases} 0, \ \ \rm{if}\ \ i=j\\
    \frac{1}{M(M-1)}, \ \ \rm{else}
    \end{cases}
\end{equation}
\begin{equation}
    U^{sca}_{i} = \frac{1}{M}
\end{equation}
On this basis, we calculate $\beta_{acc}$, $\beta_{con}$ and $\beta_{sca}$ as follows:
\begin{equation}
    \beta_{acc} = \rm{CosineSim}(U^{acc1},C.\rm{mean}(\rm{dim}=-1))\times \rm{CosineSim}(U^{acc2},C.\rm{mean}(\rm{dim}=0))
\end{equation}
\begin{equation}
    \beta_{con} = \rm{CosineSim}(U^{con},\epsilon)
\end{equation}
\begin{equation}
    \beta_{sca} = \rm{CosineSim}(U^{sca},C.\rm{mean}(\rm{dim}=-1).\rm{mean}(\rm{dim}=-1))
\end{equation}

\section{Further Discussions}
\subsection{Affinity of the Three Optimization Objectives}
\label{app:dis-aff}
We have discussed in \S\ref{sec:3} that the keys to strengthen the accuracy, convergence and scalability of CBE are: ensuring uniformity across tuples, uniformity across model pairs in win-rate uncertainty, and uniformity across models.
%我们以下来讨论他们的兼容性关系。首先，ensuring uniformity across models 可以视作是 ensuring uniformity across tuples的一个子目标，二者有着良好的亲和度。前者可以视作对于后者在model uniformity方面的一个加强。
Below we discuss their compatibility. Firstly, ensuring uniformity across models can be seen as a sub-goal of ensuring uniformity across tuples, therefore they exhibit a strong affinity. The former can be considered a refinement of the latter, specifically focusing on model uniformity. 
%此外，从公式2可以看出，模型间胜率的不确定度和模型间的比较次数是呈反比的，因此提升uniformity across model pairs in win-rate uncertainty也将有助于提升模型间的比较次数的均匀性。这进一步意味着确保uniformity across model pairs in win-rate uncertainty和ensuring uniformity across tuples you着兼容的目标和良好的亲和性。
Moreover, as shown in\eqref{eq:9}, the uncertainty in win-rate between models $\epsilon_{i,j}$ is inversely proportional to the number of comparisons between models. Therefore, improving uniformity across model pairs in win-rate uncertainty $\epsilon$ will also contribute to a more uniform distribution of comparisons between models. This further implies that ensuring uniformity across model pairs in win-rate uncertainty and ensuring uniformity across tuples are compatible goals with a strong affinity.
%此外从Table2我们可以看出相比于基线SaAcCon的\beta都得到了提升，这也从实验角度验证了优化三个目标可以相互增益的事实。基于以上分析，我们可推断改变$\alpha$其实是在改变对不同目标的优化力度。但由于三者是相互促进的，因此改变$\alpha$所带来的效果就会很小。
Furthermore, as shown in Table~\ref{tab:beta}, all $\beta$ values of \textsc{UniCBE} are improved compared to the baselines, experimentally validating the fact that optimizing the three objectives can be mutually beneficial. Based on this analysis, we can infer that changing $\alpha$ essentially adjusts the optimization emphasis on different objectives. However, since the three objectives are mutually reinforcing, the effect of changing $\alpha$ will be relatively small.

\subsection{Discussion on Sampling Bias in Incomplete Sampling Scenarios}

Previous studies have discussed the risks of introducing sampling bias in incomplete sampling scenarios. Specifically, \citet{samplebias1} demonstrated through simulation experiments that K-fold cross-validation (K-fold CV) can produce significant performance estimation bias when dealing with small sample sizes. This bias persists even when the sample size reaches 1000. In contrast, methods like nested cross-validation (Nested CV) and train/test split have been shown to provide robust and unbiased performance estimates regardless of sample size. \citet{samplebias2} introduced a weighting scheme, as described in \citep{samplebias3}, to mitigate sampling bias in active testing scenarios. \citet{AP} proposed leveraging information obtained from source models to select representative samples from the test set, thereby reducing sampling bias. Additionally, \citet{tiny} employed Item Response Theory \citep{irt} to correct sample bias in addressing this issue.

These studies inspired us to investigate the bias problem in the CBE scenario. Unlike the aforementioned studies, we found that in CBE scenario, not only does sample bias exist, but model bias also plays a role, and the two are coupled. This coupling poses greater challenges for analyzing and mitigating these biases. To address this, based on the analyses outlined in \S\ref{sec:3}, we propose the \textsc{UniCBE} method, which effectively alleviates biases in this scenario.




\begin{figure}[htbp]
    \centering
    \subfigure{
    \label{fig:351}
        \includegraphics[width=0.31\textwidth]{figs/performance-full-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:352}
        \includegraphics[width=0.31\textwidth]{figs/performance-full-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:353}
        \includegraphics[width=0.31\textwidth]{figs/performance-full-p.pdf}
    } \\
    \subfigure{
    \label{fig:354}
        \includegraphics[width=0.31\textwidth]{figs/save-full-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:355}
        \includegraphics[width=0.31\textwidth]{figs/save-full-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:356}
        \includegraphics[width=0.31\textwidth]{figs/save-full-p.pdf}
    }
    \caption{Results of compared CBE methods with GPT-3.5-turbo as the judge on AlpacaEval. }
    \label{fig:main-35}
\end{figure}

\begin{figure}[htbp]
    \centering
    \subfigure{
    \label{fig:mt1}
        \includegraphics[width=0.31\textwidth]{figs/performance-mt-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt2}
        \includegraphics[width=0.31\textwidth]{figs/performance-mt-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt3}
        \includegraphics[width=0.31\textwidth]{figs/performance-mt-p.pdf}
    } \\
    \subfigure{
    \label{fig:mt4}
        \includegraphics[width=0.31\textwidth]{figs/save-mt-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt5}
        \includegraphics[width=0.31\textwidth]{figs/save-mt-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt6}
        \includegraphics[width=0.31\textwidth]{figs/save-mt-p.pdf}
    }
    \caption{Results of compared CBE methods with human as the judge on MT-Bench.}
    \label{fig:main-mt}
\end{figure}


\begin{figure}[h]
    \centering
    \subfigure{
    \label{fig:351m}
        \includegraphics[width=0.31\textwidth]{figs/performance-qwen_plus-pre_wins_theta.pdf}}
    \hfill
    \subfigure{
    \label{fig:355m}
        \includegraphics[width=0.31\textwidth]{figs/performance-qwen_plus-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:356m}
        \includegraphics[width=0.31\textwidth]{figs/performance-qwen_plus-p.pdf}
    } \vspace{-0.2cm}\\
    \subfigure{
    \label{fig:mt1m}
        \includegraphics[width=0.31\textwidth]{figs/save-qwen_plus-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt5m}
        \includegraphics[width=0.31\textwidth]{figs/save-qwen_plus-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt6m}
        \includegraphics[width=0.31\textwidth]{figs/save-qwen_plus-p.pdf}
    }
    \vspace{-0.2cm}
    \caption{Results of compared CBE methods with Qwen-Plus as the judge on AlpacaEval. }
    \vspace{-0.3cm}
    \label{fig:qwen}
\end{figure}







