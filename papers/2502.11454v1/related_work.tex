%\cite{all}考虑直接利用所有模型在所有samples上的pair-wise比较结果来比较模型能力，这也是理论上能得到的最准确评测结果。 
% \cite{arena}以模型间胜率关系观测值的方差下降梯度为模型对的采样概率，试图在每次调用judger时最大程度地减少评估结果的不确定性。
% \cite{alpacaeval}通过选择固定的reference model并调用judger比较待评测模型与reference model来将\cite{all}所需的O(N^2)的评估开销缩减到O(N)。

\section{Related Work}
% 我们在本节讨论现有的CBE方法和Preference Aggregation方法
% 由于能够区分质量差异微小的responses，基于对比的preference信号长期被用来进行模型训练和测试。
% Due to the ability of distinguishing between subtle difference in response quality, 
Comparative preference signals have long been used for model training \citep{insgpt,llama2} and evaluation \citep{arena,batcheval}.
Centered around comparing-based evaluation, we will discuss existing budget allocation strategies and preference aggregation methods below.
\paragraph{Budget Allocation}
\label{sec:2.1}
% 最naive的Allocation方法是每次随机选择(models,sample)组合，并由judge提供preference，直到达到预设的Preference Budget. according to our 推导 in section3，该方法在期望上保证了对于不同样本组合的均匀采样从而保证了评测结果准确性。
% Arena 考虑在每一步依据胜率方差梯度对模型对采样，旨在通过greedy的方式减小观测胜率矩阵的不确定性，加速evaluation的收敛过程。
% AlpacaEval 通过比较待评测模型与固定的reference model之间的胜负关系来衡量模型性能。不同的待评测模型被要求在相同的set上进行评测，这意味着在动态场景下当新模型被加入时会优先分配preference给新模型使对其的能力估计快速稳定，从而实现良好的可扩展性。
% 尽管这些方法都能在期望的aspect保障良好的表现，但却不能同时兼顾准确性、收敛性和可扩展性，这对探索能平衡多方面性质的Preference Budget Allocation方法提出了迫切需求。
Many efforts have been made to explore preference budget allocation approaches.
The most naive allocation method is to randomly select (models, sample) tuple for judging each time until the preset preference budget is reached \citep{allpair}. 
This method ensures a relatively uniform sampling across different tuple combinations in expectation, thereby guaranteeing the accuracy of evaluation results according to our derivation in \S\ref{sec:3.2}.
Arena \citep{arena} aims to sample model pairs proportionally to the variance gradient of win rate at each step, seeking to accelerating the convergence of evaluation by reducing the uncertainty of the observed win rate matrix in a greedy manner. 
AlpacaEval \citep{alpacaeval} measures model performance by comparing the models under evaluation with a fixed reference model. Different models under evaluation are expected to be assessed on the same set. Thus, when new models are introduced, preference budget is prioritized for them to stabilize the estimation of their capabilities, thereby achieving good scalability.
Despite these methods performing well in their intended objectives, they cannot achieve a balance among accuracy, convergence, and scalability. This makes it imperative to explore better preference budget allocation strategy that can effectively reconcile all these attributes.
\paragraph{Preference Aggregation}
\label{sec:2-pa}
% 由于同一组模型可能在不同的samples上呈现不同的胜负关系，因此我们需要find the optimal preference ranking 在收集一定量的non-transitive comparison preference results among models under evaluation后。
Due to the possibility that the same group of models may exhibit different ranking relationships across different samples, it is essential to estimate the global model capability scores to better fit these non-transitive preference results.
% \citet{alpacaeval,judging} 直接用每个模型的平均胜率作为该模型能力的度量。\citet{elo2,elo1} 采用经典的Elo rating system（见附录 for 细致的计算过程）将评测过程视为序列化的模型battle来获得模型score。\citet{BTapp,arena}采用Bradley-Terry model \citep{BT} 估计模型score来最大似然模型间的比较结果。我们将在实验中系统比较这些Preference Aggregation方法在efficient CBE中的效果。
\citet{alpacaeval,judging} directly use the average pair-wise win rate of each model as a measure of its capability. 
\citet{elo2,elo1} apply the classical Elo rating system \citep{elo} (see the Appendix~\ref{app:elo} for detailed introduction) by treating the evaluation process as a sequence of model battles in order to derive model scores. 
\citet{BTapp,arena} employ the Bradley-Terry model \citep{BT} (see Appendix~\ref{app:bt} for detailed introduction) to estimate model scores by maximizing the likelihood of the comparison results between models. We will systematically compare the effectiveness of these preference aggregation methods in \S\ref{sec:exp-2}.
