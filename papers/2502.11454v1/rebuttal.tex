# Reviewer 1 (Rating: 5)

Weaknesses:
1. The framework assumes preference signals (particularly from automated judges like GPT-4) are consistent with human judgment, a potentially risky simplification given known limitations in automated preference evaluations.
该框架假设偏好信号（特别是来自自动化判断工具如GPT-4的偏好信号）与人类判断一致，这种简化存在潜在的风险，因为自动偏好评估存在已知的局限性。

A：
我们认同来自模型和来自人类的偏好信号之间存在bias，我们在多数实验中采用LLM作为judge，少数实验中采用human作为judge进行实验的原因有两点：一个是在sample和model层面提供对齐的人类监督信号的高质量benchmark相对有限，因此我们重点在MT-Bench上检验了UniCBE在人类preference signal下的有效性；一个是现在越来越多的研究开始使用LLM作为judge开展研究，因此验证UniCBE在model preference signal下的有效性也十分关键。
实验结果表明，分别采用GPT-4o、GPT-3.5-turbo和human作为judge，UniCBE都展现了更好的performance，证实了UniCBE对于preference signal来源的鲁棒性。

2. The formulation of multi-dimensional sampling matrices and their interaction in optimizing accuracy, convergence, and scalability may be overly complex for practical implementations and difficult to interpret for further tuning or adjustment.
多维采样矩阵的构建及其在优化精度、收敛性和可扩展性方面的相互作用，可能对于实际应用而言过于复杂，且难以进行进一步的调优或调整。
4. Could the authors elaborate on how UNICBE would handle scenarios with dynamic preference priorities, where, for example, accuracy is weighted more heavily than convergence?
作者能否进一步阐述UNICBE在动态偏好优先级场景下的应对方式？例如，当精度的权重高于收敛性时。

A：
多维采样矩阵的构建和计算仅依赖于快速的CPU运算，事实上在实际运算过程中是易于实现的，我们将在未来集成并发布一个工具来方便user高效地调用UniCBE. 
在公式13中，为了不引入过多的超参数，我们事实上将三个矩阵以相等weight进行了集成（公式），并在三方面都取得了良好的效果。在实际应用中，如果user对于某个特性有更高的需求（比如accuracy is weighted more heavily than convergence），他们可以提升$\theta_{acc}$来简单地进行实现。
$$P^{l} = \frac{(P^{acc\text{-}l})^{\theta_{acc}} \circ (P^{con\text{-}l})^{\theta_{con}}  \circ (P^{sca\text{-}l})^{\theta_{sca}} }{\sum ((P^{acc\text{-}l})^{\theta_{acc}} \circ (P^{con\text{-}l})^{\theta_{con}}  \circ (P^{sca\text{-}l})^{\theta_{sca}})}$$
以下是我们的实验效果：


Questions:
3. How does UNICBE perform when preference signals are less reliable, as is often the case with models lower than GPT-4 or inconsistent human annotations?
当偏好信号不够可靠时，例如在使用比GPT-4更低级别的模型或人类标注不一致的情况下，UNICBE的表现如何？

A：如Figure 6所示，除了GPT-4o和human，我们还以GPT-3.5-turbo作为judge进行了实验，whose preference signals are less reliable. 实验证明（line 462-line 466），there is a certain decline in the performance of all methods （特别是Arena方法和随机采样基本持平）, which is likely due to the increased noise in the preferences provided by GPT-3.5-turbo, leading to slower convergence. 横向比较而言，UniCBE相比于随机采样仍能实现超过15\%的preference budget节省。



5. To what extent could the uniformity constraints in the sampling matrices be relaxed while maintaining cost-effectiveness?
在保持成本效益的前提下，采样矩阵中的一致性约束能在多大程度上放宽？

A：依据我们在section 3中的分析，uniformity 的实现程度和评测结果在ACCURACY、CONVERGENCE、SCALABILITY的表现是正相关的。为了探究To what extent could the uniformity constraints in the sampling matrices be relaxed while maintaining cost-effectiveness，我们借鉴随机采样中通过调控温度来控制采样约束的想法，通过调控下式中的温度T进行采样$f^{ts}_T$来控制uniformity constraints程度：
$$    f^{ts}_T(i,j,k) =\frac{(P^l_{i,j,k})^\frac{1}{T}}{\sum (P^{l})^\frac{1}{T}}$$
当T=0时，相当于greedy sampling $f^{ts}_g$，也即施加最严格的uniformity constraints；当T=1，相当于probabilistic sampling $f^{ts}_p$，也即施加general程度的uniformity constraints；当T=正无穷，相当于random sampling，也即不施加uniformity constraints。
我们的实验结果如下所示：







# Reviewer 2 (Rating: 6)

Weaknesses:
1. The motivation is not enough clear. Why the previous method cannot perform well in both accuracy, convergence, and scalability?
动机不够清晰。为何之前的方法无法在精度、收敛性和可扩展性方面表现良好？

A：之前的方法往往关注单一的优化目标（line 64-65， Table 1）,而忽视了对其他属性的优化。我们在related work部分（line 107-118）具体说明了Random、Arena、AlpacaEval方法分别只关注了对于accuracy、convergence、scalability的优化。我们进一步在Table 2通过实验证明了这三种方法因为没有同时考虑三个优化目标而导致了在多个维度上采样相比于UniCBE不均衡，进而导致了他们无法在三个属性上同时表现良好。



2. The runtime of the previous CBE method (O(NMM)) is one of the major limitations, and the author starts from this limitation as one of the motivations for the proposed method. However, they lack the runtime analysis for the UniCBE but only an approximate number for saving time when compared to the previous method.
之前的CBE方法的运行时间复杂度（O(NMM)）是其主要局限之一，作者以此作为提出新方法的动机之一。然而，他们仅提供了与之前方法相比的大致节省时间数据，而缺少对UniCBE的运行时间分析。

A：我们十分认同reviewer关于引入对UniCBE的运行时间分析的建议。为此，我们统计了当我们需求win rate error $\Delta$小于0.02时，需要preference budget $T$=2800 (Random需要$T$=3200)；当我们需求 $\Delta$小于0.01时，需要$T$=9400 (Random需要$T$=11300)，而O(NMM)=N*M*(M-1)/2=84525。因此在一定的评测精度需求下，采用UniCBE能极大的节省preference budget。

3. While UniCBE shows promising results for scenarios with periodically introduced new models, it may be less efficient in highly dynamic, real-time evaluation settings where new models or samples are constantly introduced at high frequencies.
尽管UniCBE在周期性引入新模型的场景中显示出良好的效果，但在高度动态、实时评估的设置中（即频繁引入新模型或样本的情形），其效率可能较低。

A：我们认为在高度动态、实时评估的设置中测试UniCBE能帮助我们更全面的评估UniCBE。为此，我们进行了如下实验：我们以sample数量$N$=600，模型数量$M$=12做为起始点，每隔一个随机时间步（按照时间步均值为100的泊松分布采样）执行一次随机操作（包括增加1个sample、增加一个模型、减少一个模型、无操作）。实验结果如下图所示





# Reviewer 3 (Rating: 6)
Weaknesses:
1. The paper can be improved with more performance comparison with existing work and state-of-the-art performance.
该论文可以通过增加与现有工作和最新技术性能的比较来进一步改进。

A：我们认同您的说法，在实验中我们选择了目前最广泛使用的先进方法Arena和AlpacaEval进行比较来检验UniCBE的性能。事实上，尽管Comparing-based evaluation在大模型时代有着重要的研究意义，但目前相关的研究工作比较有限，Arena和AlpacaEval是我们能找到的最相关的工作。我们呼吁未来有更多的学者在这一领域进行研究，探究如何更高效地利用珍贵的preference signals.



2. Expect more statistical and experimental conclusions with the proposed CBE method for the scenario of large-scale preference learning.
希望针对大规模偏好学习场景，提供更多关于所提出的CBE方法的统计和实验结论。

A：我们认同您的建议，为此我们尝试在一种更贴近真实场景的、高度动态、实时评估的CBE场景下进行实验。具体来说：


# Reviewer 4 (Rating: 8)

Weaknesses:
1. The novelty of balancing accuracy, convergence, and scalability needs further justification, as similar uniform sampling strategies have been discussed in prior works that highlight the uniformity, such as Vabalas et al. (2019) for sampling biases, which could diminish its uniqueness.
在准确性、收敛性和可扩展性之间的平衡的新颖性需要进一步的论证，因为在之前的研究中已经讨论了类似的均匀采样策略，这些研究强调了采样的均匀性，例如Vabalas等人（2019）关于采样偏差的研究，这可能会削弱其独特性。

A：我们认同您的说法，并围绕我们的工作对涉及采样均匀性的相关工作进行讨论：

我们将把这一部分纳入到我们的修订版本中，以增强其质量。


2. Although the experiment of MT-Bench is based on human evaluator, larger portion of the evaluation is relied on AlpacaEval, as larger number of models and samples are used for the evaluation with AlpacaEval. The reliance on GPT-4 and GPT-3.5-turbo as evaluators, while useful, could benefit from validation against human judgments or additional LLMs, such as Claude, to establish greater reliability and generalizability across evaluator types.
尽管MT-Bench实验基于人工评估，但更大一部分的评估依赖于AlpacaEval，因为更多的模型和样本是通过AlpacaEval进行评估的。虽然将GPT-4和GPT-3.5-turbo作为评估者具有一定的实用性，但若能通过人工判断或增加其他大型语言模型（如Claude）进行验证，或许可以提高评估的可靠性和在不同评估类型间的泛化性。

A：是的，我们在实验中分别测试了用human、GPT-4和GPT-3.5-turbo作为评估者时UniCBE的性能。我们十分认可采用更多类型的测试者能帮助我们提升实验结论的鲁棒性和泛化性。为此，我们尝试了以Qwen-Plus和Gemini作为judge时的结果，如下所示：

3. Minor details, but the readability of all figures could be enhanced by widening the lines in each plot, which would improve clarity and interpretation for readers.
一些小细节方面，为了提高图表的可读性，可以加宽图中每条线的粗细，这将有助于提升清晰度并增强读者的理解。

感谢您提供的宝贵建议，我们已经在修订版本中增加了图中线的宽度以增强可读性。



Questions:
4. As the UniCBE is based on three matrix, 
, each targeting different goal of accuracy, convergence, scaliability, can user steer between those by adding hyperparameter for each matrix? Would it be also possible to quantify it through experiment?
由于UniCBE基于三个矩阵，每个矩阵针对不同的目标——准确性、收敛性和可扩展性，用户是否可以通过为每个矩阵添加超参数来进行调整？是否也可以通过实验对其进行量化？

A：我们十分认同您关于为每个矩阵添加超参数来实现对于不同优化目标可控性的建议。事实上，我们可以简单的添加超参数$\theta_{acc}$来实现这一点：
$$P^{l} = \frac{(P^{acc\text{-}l})^{\theta_{acc}} \circ (P^{con\text{-}l})^{\theta_{con}}  \circ (P^{sca\text{-}l})^{\theta_{sca}} }{\sum ((P^{acc\text{-}l})^{\theta_{acc}} \circ (P^{con\text{-}l})^{\theta_{con}}  \circ (P^{sca\text{-}l})^{\theta_{sca}})}$$
在我们的默认设置中，$\theta_{acc}=\theta_{sca}=\theta_{con}=1$。我们尝试分别改变$\theta$来检验对应的效果，如下所示：


5. While scalability is addressed by sequentially adding models, the paper could enhance this section by incorporating real-world scenarios, where models enter and exit dynamically, further proving UNICBE’s robustness in evolving benchmarks.
虽然可扩展性通过顺序添加模型得以体现，但论文可以通过加入模型动态进出场景的真实案例来增强这一部分内容，从而进一步证明UniCBE在不断变化的基准测试中的稳健性。

A：我们认同您的提议，在高度动态、实时评估的设置中测试UniCBE能帮助我们更全面的评估UniCBE。为此，我们进行了如下实验：我们以sample数量$N$=600，模型数量$M$=11做为起始点，每隔一个随机时间步（按照时间步均值为100的泊松分布采样）执行一次随机操作（包括增加1个sample、增加一个模型、减少一个模型、无操作）。实验结果如下图所示


6. The given choice of greedy sampling over probabilistic sampling and Bradley-Terry model over Elo rating system appears significant to the framework’s success. Could the authors conduct a small experiment to demonstrate that UniCBE maintains its effectiveness across different sampling and aggregation settings?
选择贪心采样而非概率采样，以及选择Bradley-Terry模型而非Elo评分系统，似乎对框架的成功具有重要影响。作者能否进行一个小规模的实验，来展示UniCBE在不同采样和聚合设置下依然能保持其有效性？

A：我们认可您宝贵的提议，事实上，在Figure 5中，我们尝试了UniCBE与按概率采样、Elo rating、average win rate的组合效果，并与默认组合进行了比较。我们发现：1. UniCBE在与各种设置结合时的效果都显著优于baselines。 2. 在默认的设置下（greedy sampling、 BT model），UniCBE能发挥最优的性能，我们认为这是由于greedy sampling能最大程度上实现采样的均匀性以及BT model能够在样本不对齐的情况下better alleviate sampling bias。

# Reviewer 5 (Rating: 8)
Weaknesses:
1. I don't see any major weakness of the paper, just the presentation can be improved, especially lack in explaining why the method is better than others in an intuitive and easy to follow way.
我没有发现该论文有明显的弱点，只是展示方面可以改进，特别是在以直观且易于理解的方式解释为什么该方法优于其他方法上存在不足。

A：

Questions:
2. The authors argue that to avoid bias, the budget should be allocated uniformly, if so how could this method be more sample efficient than random? I guess the reason is if model A is much better than B and model B is much better than C, then it's not necessary to compare A and C a lot. But if thats the reason why this method is more sample-efficient, it would be contradictory to the uniform assumption. Could the authors provide more insight into this?
作者认为，为了避免偏差，预算应均匀分配。如果是这样，那么该方法如何能比随机采样更高效？我猜测原因在于，如果模型A比模型B好很多，且模型B比模型C好很多，那么就不需要频繁比较A和C。但如果这是该方法更高效的原因，那将与均匀假设相矛盾。作者能否提供更多的见解？

A：我认为您的问题具有重要的讨论意义，我们从两方面进行回复：
1. 首先，您提到的利用模型性能之间的传递性来减小preference budget，在优化目标是模型之间的性能排序关系、或者是找到最优模型时是可行的（事实上UCB算法体现了这一思想）。然而，我们的目标在于精准评测每个模型的真实模型能力值，而非模型间的秩序关系或最优模型选择。比如，对于模型A、B、C，我们期望算出他们的能力值关系到底是[0.9,0.41,0.4]还是[0.9,0.89,0.4]，而不只是得到A>B>C。这在实际应用中很重要，因为我们在选择模型进行部署时要综合考虑模型性能和cost，如果A的api价格显著高于B同时我们能精确知道A与B的能力值差距并不大（0.9和0.89），那我们可能会更倾向于选择模型B。在仅求得秩序关系时，我们无法做出这样的决断。
2. 因此，当我们的优化目标是确切的模型能力值时，均匀采样就易于理解了：首先，由于题目有难易的区别，同一模型在面对不同的模型时表现出的能力也有区别（例如我们在line 213提到的，在有些情况下模型性能的传递性可能不成立），因此对他们的组合进行均匀采样有助于消除这些bias提升准确率；其次，由于我们需要所有模型的能力值，因此对它们进行尽量均衡的采样能够避免对某些模型能力估计值的不确定性过大。