\section{Experiments}
Centered around \textsc{UniCBE}, we will empirically compare its performance with baselines and validate its scalability in \S\ref{sec:exp-1}, explore the optimal variants in \S\ref{sec:exp-2}, and demonstrate its generalizability under different settings in \S\ref{sec:exp-3}.

\subsection{Experimental Settings}
\paragraph{Benchmarks.} We choose AlpacaEval \citep{alpacaeval} and MT-Bench \citep{judging} benchmarks for our experiments. For AlpacaEval, we use its default version which includes 805 high-quality human annotated instructions and corresponding responses from multiple LLMs. We randomly choose 20 LLMs (listed in Figure~\ref{fig:bias3}) for experiments, with GPT-4o and GPT-3.5-turbo as judges (see Appendix~\ref{app:prompt} for the prompt). For MT-Bench, we use the released responses from the all 6 LLMs and corresponding human preferences for experiments.
\paragraph{Baselines.} We choose widely applied \footnote{\url{https://tatsu-lab.github.io/alpaca_eval/}, \url{https://lmarena.ai/}} methods \textsc{Random}, \textsc{Arena} and \textsc{AlpacalEval} as baselines, which have been discussed in \S\ref{sec:2.1} and listed in Table~\ref{tb:intro}.
\paragraph{Metrics.} 
% 为了衡量CBE方法的有效性，我们分别检验预估的胜率和模型score的准确性。我们计算预估的胜率与对应Ground truth（$T=\hat{T}$时的预估值）之间的平均误差一范数
To assess the effectiveness of the CBE methods, we evaluate the accuracy of both the estimated model pair-wise win rates and the model scores. We calculate the average absolute error between the estimated win rates and corresponding ground truth (the estimates when $T = \hat{T}$).
% \begin{equation}
%     \Delta = \rm{mean}(|\Phi^{T}-\Phi^{\hat{T}}|)
%     \label{eq:15}
% \end{equation}
%我们计算预估的模型分数和对应Ground truth之间的斯皮尔曼系数和皮尔逊系数来分别检验预估的模型能力的秩序关系和线性关系的准确性
We calculate the Spearman correlation coefficient $r_s$ between the predicted model scores and the corresponding ground truth to evaluate the accuracy of the model's rank-order relationship, and the Pearson correlation coefficient $r_p$ to assess the accuracy of the linear relationship.
\paragraph{Details.} 
To ensure the reliability of the experimental results, for each setting, we randomly select $M$ (default to 15 for AlpacaEval and 5 for MT-Bench) models and $N$ (default to 805 for AlpacaEval and 700 for MT-Bench) samples, and report the average results across 10,000 random seeds. We don't observe obvious performance difference in preliminary experiments when varying $\alpha$ within the range of $[1.5,3]$ (we conduct a detailed discussion about this in Appendix~\ref{app:dis-aff}), thus we set the default value of $\alpha$ as 2 in our experiments. 
\begin{figure}[h]
    \centering
    \subfigure{
    \label{fig:4o1}
        \includegraphics[width=0.31\textwidth]{figs/performance-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4o2}
        \includegraphics[width=0.31\textwidth]{figs/performance-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4o3}
        \includegraphics[width=0.31\textwidth]{figs/performance-4o-p.pdf}
    } \\
    \subfigure{
    \label{fig:4o4}
        \includegraphics[width=0.31\textwidth]{figs/save-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4o5}
        \includegraphics[width=0.31\textwidth]{figs/save-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4o6}
        \includegraphics[width=0.31\textwidth]{figs/save-4o-p.pdf}
    }
    \vspace{-0.4cm}
    \caption{Results of compared CBE methods with GPT-4o as the judge on AlpacaEval benchmark. The X-axis (applicable to all plots below) represents the preference budget ($k$). $\mathbf{\Delta}$ denotes the mean absolute error of the estimated win rate. $\mathbf{r_s}$ and $\mathbf{r_p}$ denote the Spearman and Pearson correlations between the the estimated model scores and the ground truth respectively.}
    \vspace{-0.4cm}
    \label{fig:main-4o}
\end{figure}
%为了确保实验结果的可靠性，对于每组实验，我们随机选择M个模型和N条数据并汇报10000个随机种子下的平均结果。


\subsection{Main Results}
\label{sec:exp-1}
\paragraph{Accuracy and Convergence.}
The results of compared CBE methods on AlpacaEval benchmark with GPT4-turbo as the judge are shown in Figure~\ref{fig:main-4o}. 
%为了更好地呈现，在第二行我们计算了在达到与Random baseline相同的性能时各方法所节省的preference budget比例。
To better illustrate the results, we also calculate the percentage of preference budget saved by each method compared to \textsc{Random} baseline when achieving the same performance. 
In terms of performance, \textsc{AlpacaEval} $<<$ \textsc{Random} $<$ \textsc{Arena} $<$ \textsc{UniCBE}.
%从表现而言，\textsc{AlpacaEval}<<\textsc{Random}<\textsc{Arena}<\textsc{UniCBE}.
To understand the differences in the performance of each method, we quantitatively analyze them based on the guidelines summarized in \S~\ref{sec:3}. 
% Specifically, we calculate the extent to which these methods satisfy the objectives for suppressing sampling bias, balancing the uncertainty descent process, and mitigating updating uncertainty, denoted as $\beta_{acc}$, $\beta_{con}$, and $\beta_{sca}$, respectively (See Appendix for detailed equations). 
To achieve accuracy, convergence, and scalability, it is supposed  to allocate the preference budget in a way that ensures uniformity across tuples, uniformity across model pairs in win-rate uncertainty, and uniformity across models. 
We calculate the cosine similarity between the allocation results of these methods and the corresponding expected uniform vectors for each objective as a measure, denoted as $\beta_{acc}$, $\beta_{con}$, and $\beta_{sca}$, respectively (see Appendix~\ref{app:beta} for calculating process).
%由于实现准确性、收敛性和可扩展性分别要求preference budget的分配满足tuple间的均匀性、胜率不确定度的均匀性、模型间的均匀性，我们计算了这些方法与对应期望的均匀向量之间的余弦相似度作为衡量，并分别记作$\beta_{acc}$,$\beta_{con}$,\beta_{sca}.
% 为了理解各方法表现的差异性，我们从第三节所总结的三条guidelines来量化分析它们。具体来说我们分别计算了这些方法对于suppressing sampling bias, balancing the uncertainty descent process, and mitigating updating uncertainty的满足程度，并分别记作$\beta_{acc}$,$\beta_{con}$,\beta_{sca}.
As shown in Table~\ref{tab:beta}, the fixed inclusion of the reference model in the tuple selection of \textsc{AlpacaEval} compromises uniformity across multiple aspects, thereby resulting in lower $\beta$ values and significantly poorer performance. 
%由于\textsc{AlpacaEval} 的tuple选择中固定包含reference model，因此在多方面都无法保证较好的均匀性，从而导致了较低的$\beta$值和明显较差的性能。
\textsc{Arena} and \textsc{Random} respectively improve the balance of uncertainty and suppression of sampling bias, resulting in higher $\beta_{con}$ and $\beta_{acc}$ values.
%
Following our guidelines, \textsc{UniCBE} improves $\beta_{con}$, $\beta_{acc}$, and $\beta_{sca}$ simultaneously and save over 17\% of the preference budget compared to \textsc{Random} with a $\Delta$ close to 0.01, showcasing improved accuracy and convergence.
\begin{table}[h]
\renewcommand\arraystretch{1.4}
\centering
\setlength{\tabcolsep}{0.6em} 
\vspace{-0.4cm}
\caption{The measurement results of the achievement of objectives in \S\ref{sec:3} for the compared methods.}
\begin{tabular}{lcccc}
\toprule
Methods&\textsc{Random}&\textsc{Arena}&\textsc{AlpacaEval}&\textsc{UniCBE}\\
\midrule
$\beta_{acc}$&.5803&.5725&.0925&.7364\\
$\beta_{con}$&.9081&.9172&.3515&.9228\\
$\beta_{sca}$&.9972&.9945&.4987&.9997\\
\bottomrule
\end{tabular}
\vspace{-0.4cm}
\label{tab:beta}
\end{table}

% \textsc{UniCBE}，通过遵循guidelines同时提升了$\beta_{con}$， $\beta_{acc}$ 和 $\beta_{sca}$，从而在达到相同的性能时(接近0.01的$\Delta$)相比于\textsc{Random}节省了超过17%的preference budget，展现了良好的accuracy和convergence
\paragraph{Scalability.}
%To analyze the scalability, we 设置了一个场景where开始有11个待评测模型，然后每采样2000次就有一个新模型加入。
To analyze scalability, we establish a scenario where we initially have 11 models awaiting assessment, and new models are sequentially added every 2000 samplings.
As shown in Figure~\ref{fig:main-4o-add}, 
%在每次有新模型被引入时，UniCBE都能通过preference budget的自适应倾斜实现性能的快速稳定，相比于\textsc{Random} baseline甚至能节省超过50%的budget。相比之下arena和random因为没有考虑相应的优化目标展现出较差的scalability. 尽管分配给reference model的budget显著多于其他模型导致AlpacaEval的\beta_{sca}较低，但新模型被引入时会自动分配budget的策略也给它带来了较好的可扩展性。
Whenever a new model is introduced, \textsc{UniCBE} can rapidly stabilize the performance through adaptive preference allocation skewing for the new model, saving over 50\% of the budget compared to the \textsc{Random} baseline. In contrast, \textsc{Arena} and \textsc{Random} exhibit poorer scalability since they do not consider scalability as optimization objective. Although the budget allocated to the reference model is significantly more than that for other models, resulting in a lower \(\beta_{sca}\) for \textsc{AlpacaEval}, the strategy of automatically allocating the budget to the new introduced models also provides it with good scalability.

\begin{figure}[h]
    \centering
    \subfigure{
    \label{fig:4oadd1}
        \includegraphics[width=0.31\textwidth]{figs/performance-add-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4oadd2}
        \includegraphics[width=0.31\textwidth]{figs/performance-add-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4oadd3}
        \includegraphics[width=0.31\textwidth]{figs/performance-add-4o-p.pdf}
    } \\
    \vspace{-0.2cm}
    \subfigure{
    \label{fig:4oadd4}
        \includegraphics[width=0.31\textwidth]{figs/save-add-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4oadd5}
        \includegraphics[width=0.31\textwidth]{figs/save-add-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4oadd6}
        \includegraphics[width=0.31\textwidth]{figs/save-add-4o-p.pdf}
    }
    \vspace{-0.2cm}
    \caption{Results of compared CBE methods in the scenario where new model are consistently introduced every 2000 iterations. }
    \vspace{-0.2cm}
    \label{fig:main-4o-add}
\end{figure}

\subsection{Variants Ablations}
\label{sec:exp-2}
\paragraph{Budget Allocation Objectives.}
We test the impact of different optimization objectives by removing \(P^{acc}\), \(P^{con}\), and \(P^{sca}\) from~\eqref{eq:14} separately. As shown in Figure~\ref{fig:main-4o-ab}, the significant performance degradation observed when removing \( P^{acc} \) from \textsc{UniCBE} indicates that mitigating sampling bias to improve accuracy is the most critical factor in achieving efficient CBE. Furthermore, we find that \( P^{con} \) has a considerable impact on \( r_s \). We hypothesize that this is because balancing the uncertainty among different models helps prevent any one model from having a significant ranking bias due to its larger uncertainty. The performance drop when removing \( P^{sca} \) also suggests that ensuring uniformity in sampling across models not only enhances scalability but also further reduces sampling bias, thereby improving accuracy.
%\textsc{UniCBE}在去掉P^{acc}后出现了显著的表现下滑，这说明通过抑制采样偏差来提升准确性是实现高效CBE的最核心要素。此外，我们发现\(P^{con}\)对$r_s$的影响很大，我们猜想这是因为平衡各个模型之间的不确定度能够避免某个模型因较大的不确定度而出现明显的秩序偏差。去掉$P^{sca}$也会导致\textsc{UniCBE}下滑说明平衡模型间的采样均匀度除了能提升可扩展性，还可以进一步降低采样偏差从而提升准确性。
\begin{figure}[t]
    \centering
    \subfigure{
    \label{fig:4oa1}
        \includegraphics[width=0.31\textwidth]{figs/performance-ablation-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4oa2}
        \includegraphics[width=0.31\textwidth]{figs/performance-ablation-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4oa3}
        \includegraphics[width=0.31\textwidth]{figs/performance-ablation-4o-p.pdf}
    } \\
    \subfigure{
    \label{fig:4oa4}
        \includegraphics[width=0.31\textwidth]{figs/save-ablation-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4oa5}
        \includegraphics[width=0.31\textwidth]{figs/save-ablation-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4oa6}
        \includegraphics[width=0.31\textwidth]{figs/save-ablation-4o-p.pdf}
    }
    \vspace{-0.2cm}
    \caption{Ablation studies of \textsc{UniCBE} with GPT-4o as the judge on AlpacaEval benchmark. }
    \vspace{-0.4cm}
    \label{fig:main-4o-ab}
\end{figure}

\paragraph{Tuple Sampling and Preference Aggregation Strategies.} 
%如图所示，如果将greedy sampling替换为按照概率采样，性能将会出现显著的下降。这很可能是因为按概率采样引入的随机性会阻碍多个优化目标的实现程度。在preference aggregation 策略方面，Elo rating由于具有较强的不稳定性导致了性能相比BT model有略微下降。而直接使用胜率平均值的策略则因没有考虑不同模型所面临对手能力的不同而引入了额外的bias，导致了性能的下降。
As shown in Figure~\ref{fig:main-4o-ab}, replacing greedy sampling with probabilistic sampling $f^{ts}_p$ results in a significant performance drop. This is likely because the randomness introduced by $f^{ts}_p$ hinders the achievement of multiple optimization objectives. In terms of preference aggregation strategies, the Elo rating system $f^{pa}_{Elo}$ shows a slight performance decline compared to the BT model due to its higher instability \citep{eloun}. Moreover, the strategy of directly using the average win rate $f^{pa}_{avg}$ may introduce additional bias, as it fails to consider the varying strengths of the opponents faced by different models, leading to a performance decrease.


\subsection{Generalizability under Different Settings}
\label{sec:exp-3}
\paragraph{Different Judges.} Apart from GPT-4o, we also experiment with GPT-3.5-turbo and Qwen-Plus as judge on AlpacaEval, and human as judge on MT-Bench. As shown in the above part of Figure~\ref{fig:main-diffjudge}, 
the overall conclusion with GPT-3.5-turbo is similar to GPT-4o, except for: (1) The performance of \textsc{Arena} no longer shows advantage over \textsc{Random}. (2) There is a certain decline in the performance of all methods, which is likely due to the increased noise in the preferences provided by GPT-3.5-turbo, leading to slower convergence. 
Similar trends are observed with Qwen-Plus (See Figure~\ref{fig:qwen}).
%在MT-Bench上的实验结果如图所示。
Results on MT-Bench are shown in the below part of Figure~\ref{fig:main-diffjudge}, where \textsc{UniCBE} also demonstrates better performance compared to other methods. However, due to the limited preference data included in MT-Bench, the experimental results show relatively larger fluctuations.
The results above demonstrate the good generalizability of \textsc{UniCBE} across different judges and the data domain.
\begin{figure}[h]
    \centering
    \subfigure{
    \label{fig:351m}
        \includegraphics[width=0.31\textwidth]{figs/performance-full-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:355m}
        \includegraphics[width=0.31\textwidth]{figs/save-full-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:356m}
        \includegraphics[width=0.31\textwidth]{figs/save-full-p.pdf}
    } \\
    \subfigure{
    \label{fig:mt1m}
        \includegraphics[width=0.31\textwidth]{figs/performance-mt-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt5m}
        \includegraphics[width=0.31\textwidth]{figs/save-mt-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:mt6m}
        \includegraphics[width=0.31\textwidth]{figs/save-mt-p.pdf}
    }
    \vspace{-0.2cm}
    \caption{Results of compared CBE methods with GPT-3.5-turbo as the judge on AlpacaEval (above) and human as the judge on MT-Bench (below). }
    \vspace{-0.4cm}
    \label{fig:main-diffjudge}
\end{figure}
% \textsc{UniCBE} 相比于其他方法同样呈现了更好的表现。不过由于MT-Bench所包含的preference数据较少，实验结果的波动也相对较大。
\paragraph{Varied Number of Models and Samples.}
%最后，如图所示，我们改变了模型数量$M$和sample 数量$N$进行实验. 可以看到\textsc{UniCBE}在这些settings下相比于各类baselines都取得了显著更好的效果，特别是在$M$和$N$较大时。
Finally, as shown in Figure~\ref{fig:main-4o-hyper}, we conducte experiments by varying the number of models \(M\) and samples \(N\). It can be observed that \textsc{UniCBE} achieves significantly better results compared to all the baselines under these settings, especially when \(M\) and \(N\) are large.
\paragraph{List-wise Preference.} 
\label{sec:5.4-2}
%\textsc{UniCBE}同样可以适用于list-wise的preference。假设judge每次给出$K$个模型的preference排序结果，我们可以类似~\eqref{eq:14}计算$K+1$维的P，并采样得到tuple((m^{l\text{-}1},...,m^{l\text{-}K},s^l)). 我们再由judge对该tuple的排序结果得到K(K-1)/2个pair-wise的preference。
%图3展示了K=3时各方法的结果，可以看到\textsc{UniCBE}在该setting下相对于\textsc{Random}的节省比例甚至超过30%。
\textsc{UniCBE} can also be applied to list-wise preference. Suppose the judge provides a preference ranking for $K$ models each time. We can compute a $K+1$ dimensional $P$, similar to equation~\eqref{eq:14}, and sample to obtain a tuple \((m^{l\text{-}1},...,m^{l\text{-}K},s^l)\). From the judge's ranking of this tuple, we derive \(\frac{K(K-1)}{2}\) pair-wise preferences.
Figure~\ref{fig:main-4o-3} shows the results for the case where \(K=3\). It can be seen that \textsc{UniCBE} achieves a savings compared to \textsc{Random} of over 30\% in this setting.
%这可能是因为List-wise Preference会使得\(\frac{K(K-1)}{2}\) pair-wise preferences集中在K个模型之间，加剧了采样bias，更需要\textsc{UniCBE}方法来抑制
This may be due to the fact that list-wise preference results in \(\frac{K(K-1)}{2}\) pair-wise preferences concentrated among the $K$ models, exacerbating the sampling bias. Therefore, \textsc{UniCBE} is even more needed to suppress this effect with list-wise preference.
%


%整体的结论和在GPT4-turbo作为judge时相似，除了：（1）Arena的表现相比Random不再有显著的优势。（2）各类方法的performance都有一定的下滑，这可能主要是因为GPT-3.5-turbo提供的preference中有更多的噪声导致了较慢的收敛。

\begin{figure}[htbp]
    \centering
    \vspace{-0.2cm}
    \subfigure[$M=15,N=805$]{
    \label{fig:4ohy1}
        \includegraphics[width=0.31\textwidth]{figs/performance-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure[$M=12,N=805$]{
    \label{fig:4ohy2}
        \includegraphics[width=0.31\textwidth]{figs/performance-4o-pre_wins_theta-12_805.pdf}
    }
    \hfill
    \subfigure[$M=15,N=600$]{
    \label{fig:4ohy3}
        \includegraphics[width=0.31\textwidth]{figs/performance-4o-pre_wins_theta-15_600.pdf}
    } \\
    \subfigure[$M=15,N=805$]{
    \label{fig:4ohy4}
        \includegraphics[width=0.31\textwidth]{figs/save-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure[$M=12,N=805$]{
    \label{fig:4ohy5}
        \includegraphics[width=0.31\textwidth]{figs/save-4o-pre_wins_theta-12_805.pdf}
    }
    \hfill
    \subfigure[$M=15,N=600$]{
    \label{fig:4ohy6}
        \includegraphics[width=0.31\textwidth]{figs/save-4o-pre_wins_theta-15_600.pdf}
    }
    \vspace{-0.2cm}
    \caption{Effects of number of models under evaluation $M$ and number of samples $N$. The results are obtained with GPT-4o as the judge on AlpacaEval.}
    \vspace{-0.4cm}
    \label{fig:main-4o-hyper}
\end{figure}

\begin{figure}[htbp]
    \centering
    \subfigure{
    \label{fig:4o31}
        \includegraphics[width=0.31\textwidth]{figs/performance-3-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4o32}
        \includegraphics[width=0.31\textwidth]{figs/performance-3-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4o33}
        \includegraphics[width=0.31\textwidth]{figs/performance-3-4o-p.pdf}
    } \\
    \subfigure{
    \label{fig:4o34}
        \includegraphics[width=0.31\textwidth]{figs/save-3-4o-pre_wins_theta.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4o35}
        \includegraphics[width=0.31\textwidth]{figs/save-3-4o-s.pdf}
    }
    \hfill
    \subfigure{
    \label{fig:4o36}
        \includegraphics[width=0.31\textwidth]{figs/save-3-4o-p.pdf}
    }
    \vspace{-0.2cm}
    \caption{Performance of CBE methods with list-wise preference of GPT-4o on AlpacaEval.}
    \vspace{-0.4cm}
    \label{fig:main-4o-3}
\end{figure}



