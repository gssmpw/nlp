\section{Introduction}


% arena chatbot
% RLHF
% 基于概率


% 语言模型能力的持续进化使得评估它们与人类期望之间的对齐性变得愈发重要而困难。一方面，人类提供的评估信号是正确衡量并引导模型通往安全、可靠的AGI的核心依据；另一方面，模型在训练和应用场景的快速迭代催生了大量的评估需求，这使得获取足够的劳动密集型的人类preference变得困难。因此，高效地利用珍贵的人类preference来评估模型是一项重要的挑战。
The ongoing evolution of large language models (LLMs) has made it increasingly important to assess their alignment with human preferences \citep{alpacaeval,judging}. 
The preference signals provided by humans are crucial for accurately assessing and guiding models toward safe and reliable AGI \citep{safe,survey}. 
However, the rapid iteration of LLMs in training and application scenarios has created a substantial demand for evaluation, complicating the acquisition of sufficient labor-intensive human preferences \citep{arena,ultra}. 
Therefore, exploring the use of precious preference signals for efficient model alignment evaluation is of great significance and requires long-term research.



% scoring-based 方法和comparing-based 方法是当前两类主流评估模型能力的范式。其中前者需要judger每次判断单个模型response的质量而后者要求judger每次给出多个候选模型responses间的质量比较结果。
% 由于能够直接对不同模型生成的内容进行细粒度的对比，\citet{judging, role} 证实在judger的评估能力一定的情况下，CBE能够更准确地衡量模型能力。
% However，当有M条数据和N个待评估模型时，O(MN^2)的评测开销限制了CBE的实用性\cite{all}。为此，一些研究\cite{arena，alpacaeva, speeding}探索了基于特定优化目标的[待比较模型，样本]组合采样策略。
% 它们在各自优化目标的指导下根据已有的观测结果来选择(待评测模型，样本)组合，并记录judge在该组合上的preference结果，并重复该过程。
% 模型能力的估值可以通过特定的preference aggregation方式作用于preference的观测结果来获得
% 以期在更少的评测开销下实现高效的CBE。（selector）
% Nevertheless, 我们分析发现这些方法所设定的优化目标往往是单一的，无法兼顾所获得评测结果的准确性、稳定性和可扩展性，并通过实验进行了证实。

\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.9\textwidth]{figs/intro.pdf}
\end{center}
\vspace{-0.2cm}
\caption{Flowchart of the process for comparing-based evaluation. }
\vspace{-0.2cm}
\label{fig:intro}
\end{figure}

Current mainstream model evaluation paradigms include scoring-based evaluation (SBE) \citep{geval,pointwise} and comparing-based evaluation (CBE) \citep{arena,alpacaeval}. The former requires the judge to offer preference scores for individual responses, while the latter needs the judge to establish a preference order among multiple candidate model responses. 
By directly comparing the responses of different models, \citet{judging,role} confirm that CBE can more accurately assess model performance.
However, the $O(NM^2)$ evaluation overhead limits the practicality of CBE when there are $M$ models to evaluate on $N$ samples \citep{allpair}. 
To achieve efficient CBE, various methods have been explored \cite{arena,speeding,alpacaeval}. 
As shown in Figure~\ref{fig:intro}, based on existing observational results, these methods iteratively allocate preference budget to the next (models, sample) tuple according to respective optimization objectives. 
Specific preference aggregation methods (e.g., ELO rating \citep{elo}) are then applied to predict the model capability scores based on these preference results. 
Nevertheless, as shown in Table~\ref{tb:intro}, the optimization objectives of these methods are often singular, failing to simultaneously achieve the accuracy, convergence, and scalability well. We will discuss this in detail in \S\ref{sec:2.1} and conduct experimental validation in \S\ref{sec:exp-1}.


% \begin{wraptable}{r}{0.6\textwidth}
%     \vspace{-22pt}
%     \centering
%     \small
%     \caption{Optimization Objectives of Different Methods.}
%     \begin{tabular}{l c c c}
%     \toprule
%        Methods & Accuracy & Convergence & Scalability \\ \toprule
%       \citet{allpair}  & +  & - & - \\
%       \citet{arena}   & - & + & -\\
%       \citet{alpacaeval} & - & - &++\\ 
%       \textsc{UniCBE} & ++ & ++  &++\\
%         \bottomrule
%     \end{tabular}
%     \vspace{-22pt}
%     \label{tb:intro}
% \end{wraptable}
% \begin{wraptable}{r}{0.6\textwidth}
\begin{table}[ht]
    \vspace{-8pt}
    \centering
    \small
    \caption{Optimization Objectives of widely applied CBE Methods. The number of '+' indicates the degree of optimization for the objective, which is discussed in \S\ref{sec:2.1} and measured in Table~\ref{tab:beta}.}
    \begin{tabular}{l c c c c}
    \toprule
       \multirow{2}{*}{Methods} & \citet{allpair} & \citet{arena} & \citet{alpacaeval} & Ours \\
&\textsc{Random}&\textsc{Arena}&\textsc{AlpacaEval}&\textsc{UniCBE} \\ \midrule
      Accuracy  & +  & - & - & ++ \\
      Convergence   & - & + & - &++\\
      Scalability & - & - &++&++\\ 
        \bottomrule
    \end{tabular}
    % \vspace{-22pt}
    \label{tb:intro}
\end{table}
% \end{wraptable}
% 为了设计能快速收敛并准确评测模型性能、当新的待评测模型加入时实现良好可扩展性的采样方法，我们从理论上分析总结了以下insights：提升评测结果准确性的关键在于均匀化采样元素组合，从而削弱潜在的观测偏差；提升评测结果稳定性的关键在于均匀化胜率矩阵不确定性的梯度下降过程，从而在全局层面削弱观测方差；而提升可扩展性的关键则在于对新加入的待评测模型倾斜足够的评估开销，从而加速削弱更新偏差、实现评测结果稳态化的进程。基于这些insights的指导，我们提出了\textsc{EfficientArena}方法。我们首先建立采样过程中的状态变量分别与观测偏差、观测方差、更新偏差间的映射关系；然后我们联立这些解耦的映射关系来获取兼顾准确性、稳定性与可扩展性的采样概率矩阵，并据此进行采样。

To develop a method that can accurately assess model performance, quickly converge evaluation results, and ensure good scalability when new models are introduced, we theoretically analyze and summarize the following guidelines:
\begin{itemize}[leftmargin=20pt]
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
% \item The key to improving the \textbf{accuracy} of evaluation results depends on the uniform sampling of item (model and sample) tuples, which helps mitigate the sampling bias.
% \item The key to accelerating the \textbf{convergence} process lies in balancing the gradient descent process of the win rate uncertainty matrix, thereby globally reducing observation variance.
% \item The key to strengthening the \textbf{scalability} is to allocate sufficient evaluation budgets to newly introduced models under assessment, thereby accelerating the reduction of updating bias.
\item Improving the \textit{\textbf{accuracy}} of evaluation results relies on completely \textbf{uniform} sampling of tuple combinations, so as to mitigate sampling bias.
\item Accelerating the \textit{\textbf{convergence}} process involves ensuring the \textbf{uniformity} of the win rate uncertainty matrix during its descending process to reduce observation variance.
\item Enhancing \textit{\textbf{scalability}} requires sufficient budgets being allocated to new added models to ensure the \textbf{uniform} allocation among models, which helps reduce updating uncertainty.
\end{itemize}
Based on these insights, we propose \textsc{UniCBE}, a unified uniformity-driven framework that can achieve CBE with better accuracy, convergence and scalability. 
In each iteration of the evaluation process, we first establish sampling probability matrices under different optimization objectives respectively based on real-time preference results.
Afterwards, we integrate these matrices to obtain a global sampling probability matrix. 
Furthermore, we explore various tuple sampling strategies and preference aggregation methods to achieve optimal evaluation results.

% 为了全面地验证所提出方法的有效性和泛化性，我们设置了多组实验，其中包括不同的judger（GPT-3.5-turbo, GPT-4o, human），不同的数据集（AlpacaEval、MTBench），不同的待评测模型组合（随机地从20个主流LLM中进行选择），不同的场景（包括静态的和不断有新的待评测模型加入的场景），不同的评测指标（模型能力预测值与真实值之间的一致性系数、模型间胜率预测值与真实值之间的MAE距离）。Main results显示，相比于随机采样的基线，在达到相同的评测精度时（超过0.995的斯皮尔曼系数），\textsc{EfficientArena}能够节省超过20%的评测开销，展现了显著高于其他各基线的良好稳定性和准确性。此外在有新的待评测模型加入的场景下，\textsc{EfficientArena}能够显著更快地完成对其能力的准确评测，相较于随机采样能够节省超过50%的评测开销，展现了优异的可扩展性。

To comprehensively validate the effectiveness and generalizability of \textsc{UniCBE}, we conduct multiple experiments involving various types of judges (LLMs and humans), different benchmarks, varied model sets to be evaluated, diverse scenarios (static and dynamic), and multiple evaluation metrics. 
The main results indicate that, compared to the random sampling baseline, \textsc{UniCBE} saves over 17\% of evaluation budgets when achieving the same assessment accuracy (with a Pearson coefficient exceeding 0.995 with the ground truth), demonstrating significantly better convergence and accuracy than other baselines. Furthermore, in scenarios where new models are continuously introduced, \textsc{UniCBE} can even save over 50\% of evaluation costs compared to random sampling, showcasing excellent scalability.




