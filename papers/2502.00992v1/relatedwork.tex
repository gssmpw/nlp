\section{Related Work}
\label{sec:bg}
	
	Outfit generation is a multifaceted research area that has been investigated in several studies. In this section, we present a brief review of previous work on outfit generation and multimodal I2I translation. Finally, we highlight the positioning of our work. 
	
	\textbf{Outfit Generation:} In general, outfit generation attempts to synthesize complementary fashion items that match given fashion items. In previous studies, pair-wise \cite{liu2019toward,liu2019collocating,yu2019personalized} and set-wise \cite{zhou2022learning,zhou2022coutfitgan} outfit generation methods have been explored. For instance, an Attribute-GAN \cite{liu2019toward} framework was proposed to carry out the outfit generation in a pair-wise way with a new attribute loss. This model adopted upper and lower clothing as the domains of translation. Liu \textit{et al.} \cite{liu2019collocating} improved Attribute-GAN by introducing a multi-discriminator framework to further enhance the quality of collocated clothing generation. In contrast, Yu \textit{et al.} \cite{yu2019personalized} proposed a new task in the field of outfit generation, personalized fashion design, in which users were recommended collocated clothing synthesized by their model. These generative models only worked on upper and lower clothing domains, meaning that they needed to learn a mapping only between these two domains. To overcome this issue, Zhou \textit{et al.} \cite{zhou2022learning} originally proposed a new framework called OutfitGAN to learn a one-to-many mapping for outfit generation. OutfitGAN adopted one fashion item as input and produced multiple items from different categories as output. Zhou \textit{et al.} \cite{zhou2022coutfitgan} then extended their framework to a many-to-many mapping that was capable of synthesizing complementary fashion items based on an arbitrary set of items as input. These set-wise frameworks concentrated on the generation of outfits consisting of several fashion items.
	
	\textbf{Multimodal I2I Translation:} In this approach, a model learns a conditional distribution that can produce diverse images in a target domain, given an input image in the source domain as input. Huang \textit{et al.} \cite{huang2018munit} proposed a model called MUNIT to disentangle images into style and content codes. Once a well-trained disentanglement network had been achieved, content code from the source domain could be aggregated with the style code sampled from a Gaussian distribution and decoded into an image in the target domain. In the same period, Lee \textit{et al.} \cite{DRIT} proposed a scheme called DRIT to disentangle images into attribute and content space. In this approach, the source and target domains shared the same content domain. In a later study, Lee \textit{et al.} \cite{DRIT_plus} extended DRIT to create the DRIT++ framework, which facilitated translation across multiple domains. More recently, Mao \textit{et al.} \cite{mao2022continuous} utilized signed attribute vectors to translate an image from the source domain to the target domain with diverse possibilities alongside continuous translation through interpolation.
	
	\textbf{Positioning of Our Work:} The aim of our research is to synthesize collocated and diversified sets of fashion items based on a given set of items, which constitutes a novel task in the field of fashion intelligence. COutfitGAN \cite{zhou2022coutfitgan} is the most relevant existing scheme in this context; however, the lack of diversity of the generated outfits and the need for additional guiding information in the form of silhouette masks during the translation process imposes certain limitations on this approach. Multimodal I2I translation methods \cite{huang2018munit, DRIT, DRIT_plus, mao2022continuous} could be explored as an alternative approach to outfit generation, but they are not directly applicable to our task due to their dependence on an assumption of spatial alignment between the input and output. Furthermore, these methods can only use unsupervised learning to create mappings. In contrast, our task provides explicit supervision by using outfits put together by fashion experts.