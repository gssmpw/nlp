% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{titlesec}
\usepackage{multirow}
\usepackage{graphicx} % for \rotatebox
\usepackage{array}
% \usepackage{savetrees}
\usepackage{adjustbox}
\usepackage{natbib}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{bbding}
\usepackage{utfsym}
\usepackage{hyperref}
\usepackage{enumitem}
% \usepackage{ulem}
\usepackage{setspace}
\usepackage{courier} % light font weight
\usepackage{url}            % simple URL typesetting
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[edges]{forest}
% ==============================================

\definecolor{hidden-red}{RGB}{205, 44, 36}
\definecolor{hidden-blue}{RGB}{194,232,247}
\definecolor{hidden-orange}{RGB}{243,202,120}
\definecolor{hidden-green}{RGB}{34,139,34}
\definecolor{hidden-pink}{RGB}{255,245,247}
\definecolor{hidden-black}{RGB}{20,68,106}

\newcommand{\cmark}{\ding{52}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\eg}{E.g.,}
\newcommand{\xot}[0]{\textrm{XoT}}
\newcommand{\github}[0]{\url{https://github.com/}}






% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% Standard package includes
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs} % For better table formatting
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Ask in Any Modality:\\ A Comprehensive Survey on Multimodal Retrieval-Augmented Generation}

\author{
\small
  \textbf{Mohammad Mahdi Abootorabi\textsuperscript{\textdagger}},
  \textbf{Amirhosein Zobeiri\textsuperscript{$\diamond$}},
  \textbf{Mahdi Dehghani\textsuperscript{\textparagraph}},
  \textbf{Mohammadali Mohammadkhani\textsuperscript{\textsection}},
\\
  \small
  \textbf{Bardia Mohammadi\textsuperscript{\textsection}},
  \textbf{Omid Ghahroodi\textsuperscript{\textdagger}},
  \textbf{Mahdieh Soleymani Baghshah\textsuperscript{\textsection, \textasteriskcentered}},
  \textbf{Ehsaneddin Asgari\textsuperscript{\textdagger, \textasteriskcentered}}
  \\[0.6em]
  % \\ % Ensure a new line starts after the spacing
  \small
  \textsuperscript{\textsection}Computer Engineering Department, Sharif University of Technology, Tehran, Iran,\\
  \small
  \textsuperscript{$\diamond$}College of Interdisciplinary Science and Technology, University of Tehran, Tehran, Iran,\\
  \small
  \textsuperscript{\textparagraph}Computer Engineering Department, K.N. Toosi University of Technology, Tehran, Iran,\\
  \small
  \textsuperscript{\textdagger}Qatar Computing Research Institute, Doha, Qatar
\\
  \small{
    \textbf{Correspondence:} \href{mailto:mahdi.abootorabi2@gmail.com}{mahdi.abootorabi2@gmail.com}, \href{mailto:soleymani@sharif.edu}{soleymani@sharif.edu}, 
    \href{mailto:easgari@hbku.edu.qa}{easgari@hbku.edu.qa}
  }
}

\titlespacing*{\paragraph}{0pt}{0.3ex plus 0.1ex minus 0.2ex}{0.5em}
\titleformat{\paragraph}[runin]{\normalfont\normalsize\bfseries}{\theparagraph}{0.3em}{}[]

% \titlespacing{\section}{0pt}{1.2ex}{1.0ex}
% \titlespacing{\subsection}{0pt}{1.0ex}{1.0ex}

\begin{document}
\maketitle
% {\makeatletter\acl@finalcopytrue
%   \maketitle
% }
\begin{abstract}
Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG.
This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. 
Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at \url{https://github.com/llm-lab-org/Multimodal-RAG-Survey}.
\end{abstract}

\newcommand{\defaultfootnote}{\thefootnote}
\renewcommand{\thefootnote}{\textasteriskcentered}
\footnotetext{These authors contributed equally.}
\renewcommand{\thefootnote}{\defaultfootnote}

\section{Introduction \& Background}

\noindent
In recent years, significant breakthroughs have been achieved in language models, driven primarily by the advent of transformers \cite{10.5555/3295222.3295349}, enhanced computational capabilities, and the availability of large-scale training data \cite{naveed2024comprehensiveoverviewlargelanguage}. The emergence of foundational Large Language Models (LLMs) 
\cite{NEURIPS2022_b1efde53, grattafiori2024llama3herdmodels, touvron2023llama2openfoundation, qwen2025qwen25technicalreport, anil2023palm2technicalreport} 
% \cite{NEURIPS2022_b1efde53, openai2024gpt4technicalreport, grattafiori2024llama3herdmodels, touvron2023llama2openfoundation, qwen2025qwen25technicalreport, 10.5555/3648699.3648939, anil2023palm2technicalreport}
has revolutionized natural language processing (NLP), demonstrating unprecedented capabilities in a wide range of tasks including instruction following \cite{qin-etal-2024-infobench}, sophisticated reasoning \cite{10.5555/3600270.3602070}, In-context Learning \cite{NEURIPS2020_1457c0d6}, and multilingual machine translation \cite{zhu-etal-2024-multilingual}. These advancements have elevated the performance of various NLP tasks, opening new avenues for research and application. 
Despite their remarkable achievements, LLMs face significant challenges, including hallucination, outdated internal knowledge, and a lack of verifiable reasoning \cite{10.1145/3703155, xu2024hallucinationinevitableinnatelimitation}. Their reliance on parametric memory restricts their ability to access up-to-date knowledge, making them less effective for knowledge-intensive tasks compared to task-specific architectures. Moreover, providing provenance for their decisions and updating their world knowledge remain critical open problems \cite{lewis2020retrieval}.

% \vspace{-0.2ex}

\paragraph{Retrieval-Augmented Generation (RAG)}
RAG \cite{lewis2020retrieval} has emerged as a promising solution to these limitations by enabling LLMs to retrieve and incorporate external knowledge, improving factual accuracy and reducing hallucinations \cite{shuster-etal-2021-retrieval-augmentation, ding2024retrieveneedsadaptiveretrieval}. By dynamically accessing vast external knowledge repositories, RAG systems enhance knowledge-intensive tasks while ensuring responses remain grounded in verifiable sources \cite{Gao2023RetrievalAugmentedGF}.

\noindent
In practice, RAG systems operate through a retriever-generator pipeline. The retriever leverages embedding models \cite{chen-etal-2024-m3, rau2024contextembeddingsefficientanswer} to identify relevant passages from external knowledge bases and optionally applies re-ranking techniques to improve retrieval precision \cite{dong2024dontforgetconnectimproving}. These retrieved passages are then passed to the generator, incorporating this external context to produce informed responses.
Recent advancements in RAG frameworks, such as planning-guided retrieval \cite{lee-etal-2024-planrag}, agentic RAG \cite{an2024goldenretrieverhighfidelityagenticretrieval}, and feedback-driven iterative refinement \cite{liu-etal-2024-ra, asai2023selfrag}, further enhance both retrieval and generation stages.
However, traditional RAG architectures are primarily designed for textual information, limiting their ability to address multimodal challenges that require integrating diverse data formats.

\paragraph{Multimodal Learning}
Parallel to these developments, significant advances in multimodal learning have reshaped artificial intelligence by enabling systems to integrate and analyze heterogeneous data sources for a holistic representation of information. The introduction of CLIP (Contrastive Language-Image Pretraining) \cite{radford2021learning} marked a pivotal moment in connecting visual and textual information through contrastive learning, inspiring numerous subsequent models and applications \cite{10.5555/3600270.3601993, wang2023one, pramanick2023volta}. 

\noindent
These breakthroughs have driven progress in various domains, including sentiment analysis \cite{10.1145/3586075} and cutting-edge biomedical research \cite{hemker2024healnet}, demonstrating the value of multimodal approaches. By enabling systems to process and understand diverse data types such as text, images, audio, and video, multimodal learning plays a key role in advancing artificial general intelligence (AGI) \cite{song2025bridge}.

\begin{figure*}[t]
\centering
  \includegraphics[width=\textwidth]{MM-RAG-500.png}
\caption{\label{fig:overview}
Overview of the multimodal retrieval-augmented generation (RAG) pipeline, highlighting the advancements and techniques employed at each stage. The flow begins with query preprocessing, where user queries are refined and then encoded into a shared embedding space alongside a multimodal database. Retrieval strategies, such as modality-centric retrieval, similarity search, and re-ranking, enhance document selection, while fusion mechanisms align and integrate data from multiple modalities using score fusion or attention-based methods. Augmentation techniques such as iterative retrieval with feedback mechanisms, further refine the retrieved documents for the multimodal LLM. The generation stage incorporates innovations like Chain-of-Thought reasoning and source attribution for better outputs, with loss functions combining alignment loss and generation loss to optimize both retrieval and generation components. Noise management techniques are also applied to improve training stability and robustness.
}
\end{figure*}

\paragraph{Multimodal RAG}
The extension of LLMs to multimodal LLMs (MLLMs) has further expanded their capabilities, allowing them to process, reason, and generate outputs across diverse modalities \cite{liu2023llava, geminiteam2024geminifamilyhighlycapable, 10.5555/3618408.3619222}. For example, GPT-4 \cite{openai2024gpt4technicalreport} demonstrates human-level performance in various benchmarks by processing both text and images, marking a significant milestone in multimodal perception and interaction. Building on this foundation, multimodal RAG systems extend traditional RAG frameworks by incorporating multimodal knowledge sources, such as images and audio, to provide enriched context for generation \cite{Hu_2023_CVPR, chen-etal-2022-murag}. This integration enhances the precision of generated outputs while leveraging multimodal cues to improve the reasoning capabilities of MLLMs.
However, these multimodal systems also present unique challenges, including determining which modalities to retrieve, effectively fusing diverse data types, and addressing the complexities of cross-modal relevance \cite{zhao-etal-2023-retrieving}.
\autoref{fig:overview} illustrates the general pipeline of these systems.

\paragraph{Task Formulation}
A mathematical formulation of the general task for multimodal RAG is presented in this section. These systems generate a multimodal response, denoted as $r$, in response to a multimodal query $q$.

Let $D = \{d_1, d_2, ..., d_n\}$ be a multimodal corpus. Each document $d_i \in D$ is associated with a modality $M_{d_i}$ and processed by a modality-specific encoder, yielding:
\begin{equation}
    z_i = Enc_{M_{d_i}}(d_i)
\end{equation}
The set of all encoded representations is denoted by $Z = \{z_1, z_2, ..., z_n\}$.
Modality-specific encoders map different modalities into a shared semantic space for cross-modal alignment.
A retrieval model $R$ assesses the relevance of each encoded document representation $z$ with respect to the query $q$, represented as $R(q, z)$.
To construct the retrieval-augmented multimodal context, the retrieval model selects the most relevant documents based on a modality-specific threshold:
\begin{equation}
X = \{d_i \mid s(e_q, z_i) \ge \tau_{M_{d_i}}\}
\end{equation}
where $\tau_{M_{d_i}}$ is a relevancy threshold for the modality of $M_{d_i}$, $e_q$ is the encoded representation of $q$ in the shared semantic space, and $s$ is a scoring function that measures the relevance between the encoded query and document representations. 
The generative model $G$ produces the final multimodal response, given the user query $q$ and the retrieved documents $X$ as context, denoted as $r = G(q, X)$.


\paragraph{Related Works}
As the field of multimodal RAGs is newly introduced and evolving rapidly, especially in recent years, there is a pressing need for a comprehensive survey that explores the current innovations and frontiers of these systems. While more than ten surveys have been published on RAG-related topics such as Agentic RAG \cite{singh2025agenticretrievalaugmentedgenerationsurvey}, none provide a detailed and comprehensive overview of advancements in multimodal RAGs. The only related survey to date \cite{zhao-etal-2023-retrieving} categorizes multimodal RAGs by grouping relevant papers based on their applications and modalities. However, our survey provides a more detailed and innovation-driven perspective, offering a detailed taxonomy and exploring emerging trends and challenges in depth. Furthermore, significant advancements have been made in the field since its publication, and interest in this topic has grown substantially within the research community. 
In this survey, we review over 100 papers on multimodal RAGs published in recent years, primarily from the ACL Anthology and other repositories such as the ACM Digital Library.

\paragraph{Contributions}
In this work, \textbf{(i)} we provide a comprehensive review of multimodal RAG, covering task formulation, datasets, benchmarks, tasks and domain-specific applications, evaluation, and key innovations in retrieval, fusion, augmentation, generation, training strategies, and loss functions. \textbf{(ii)} We introduce a precise structured taxonomy (\autoref{fig:taxonomy_full}) that categorizes state-of-the-art models based on their primary contributions, highlighting methodological advancements and emerging frontiers. \textbf{(iii)} To support further research, we make resources, including datasets, benchmarks, and key innovations, publicly available. \textbf{(iv)} 
We identify current research trends and knowledge gaps, providing insights and recommendations to guide future advancements in this evolving field.

\section{Datasets and Benchmarks}
\noindent
Multimodal RAG research employs diverse datasets and benchmarks to evaluate retrieval, integration, and generation across heterogeneous sources. Image–text tasks, including captioning and retrieval, commonly use MS-COCO \cite{lin2014microsoft}, Flickr30K \cite{young2014image}, and LAION-400M \cite{laion400m}, while visual question answering with external knowledge is supported by OK-VQA \cite{marino2019okvqa} and WebQA \cite{Chang_2022_CVPR}. For complex multimodal reasoning, MultimodalQA \cite{talmor2021multimodalqa} integrates text, images, and tables, whereas video-text tasks leverage ActivityNet \cite{caba2015activitynet} and YouCook2 \cite{zhou2018youcook2}. 

In the medical domain, MIMIC-CXR \cite{johnson2019mimic} and CheXpert \cite{irvin2019chexpert} facilitate tasks such as medical report generation. It is noteworthy that a number of these datasets are unimodal (e.g., solely text-based or image-based). Unimodal datasets are frequently employed to represent a specific modality and are subsequently integrated with complementary datasets from other modalities. This modular approach allows each dataset to contribute its domain-specific strengths, thereby enhancing the overall performance of the multimodal retrieval and generation processes. 

Benchmarks assess multimodal RAG systems on visual reasoning, external knowledge integration, and dynamic retrieval. The $M^{2}RAG$ \cite{ma2024multimodalretrievalaugmentedmultimodal} benchmark provides a unified evaluation framework that combines fine-grained text-modal and multimodal metrics to jointly assess both the quality of generated language and the effective integration of visual elements Vision-focused evaluations, including MRAG-Bench \cite{hu2024mragbench}, VQAv2 \cite{balanced_vqa_v2} and VisDoMBench \cite{suri2024visdommultidocumentqavisually}, test models on complex visual tasks. Dyn-VQA \cite{li2024benchmarking}, MMBench \cite{liu2025mmbench}, and ScienceQA \cite{saikh2022scienceqa} evaluate dynamic retrieval and multi-hop reasoning across textual, visual, and diagrammatic inputs. 

Knowledge-intensive benchmarks, such as TriviaQA \cite{joshi2017triviaqa} and Natural Questions \cite{kwiatkowski2019natural}, together with document-oriented evaluations such as OmniDocBench \cite{ouyang2024omnidocbenchbenchmarkingdiversepdf}, measure integration of unstructured and structured data. Advanced retrieval benchmarks such as RAG-Check \cite{mortaheb2025ragcheckevaluatingmultimodalretrieval} evaluate retrieval relevance and system reliability, while specialized assessments such as Counterfactual VQA \cite{niu2021counterfactual} test robustness against adversarial inputs. Additionally, OCR impact studies such as OHRBench \cite{zhang2024ocr} examine the cascading effects of errors on RAG systems.
Additional details about datasets, benchmarks, and their categorization are presented in Table~\ref{tab:categorized_datasets} and Table~\ref{tab:benchmark_datasets} in Appendix~(\S\ref{sec:app_dataset}).

\begin{center}
    \input{taxonomy_whole}
\end{center}

\section{Evaluation}
\label{sec: evalmetrics}
\noindent

% Assessing multimodal RAG models presents significant challenges due to their diverse modalities, intricate architectures, and the diverse evaluation dimensions involved. Given the broad spectrum of modalities and system components, evaluation requires a combination of metrics drawn from vision-language models (VLMs), generative AI, and retrieval systems. These metrics enable the assessment of various aspects of multimodal RAG, such as text and image generation or information retrieval, either independently or in combination. By analyzing prior works, we identified approximately 60 distinct evaluation metrics used in multimodal RAG studies. 




\noindent
Evaluating multimodal RAG models is complex due to their varied input types and complex structure. The evaluation combines metrics from VLMs, generative AI, and retrieval systems to assess capabilities like text/image generation and information retrieval. Our review found about 60 different metrics used in the field. More details, including the formulas for the RAG evaluation metrics, can be found in Appendix~(\S\ref{sec:app_metrics}). In the following paragraphs, we will examine the most important and widely used metrics for evaluating multimodal RAG.

% The Appendix~(\S\ref{sec:app_metrics}) contains detailed metric formulas.


\noindent
\paragraph{Retrieval Evaluation} Retrieval performance is measured through accuracy, recall, and precision metrics, with an F1 score combining recall and precision.
Recall@K, which examines relevant items in top K results, is preferred over standard recall. Mean Reciprocal Rank (MRR) serves as another key metric for evaluation, which is utilized by \cite{omar-etal-2024-multi, nguyen2024multimodallearnedsparseretrieval}.



\paragraph{Modality Evaluation} Modality-based evaluations primarily focus on text and image, assessing their alignment, text fluency, and image caption quality. For text evaluation, metrics include Exact Match (EM), BLEU \cite{papineni-etal-2002-bleu}, ROUGE \cite{lin-2004-rouge}, and METEOR \cite{banerjee-lavie-2005-meteor}. MultiRAGen \cite{shohan2024xlheadtagsleveragingmultimodalretrieval} uses Multilingual ROUGE for multilingual settings. \\
For image captioning, CIDEr (Consensus-Based Image Description Evaluation) \cite{vedantam2015cider} measures caption quality using TF-IDF and cosine similarity \cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling, zhao-etal-2024-unifashion, luo2024doestextualinformationaffect, yuan2024ragdrivergeneralisabledrivingexplanations, Sharifymoghaddam2024UniRAGUR, Hu_2023_CVPR, rao2024ravenmultitaskretrievalaugmented,xu2024retrievalaugmentedegocentricvideocaptioning,kim2024you,zhang2024c3net}, while SPICE (Semantic Propositional Image Caption Evaluation) \cite{anderson2016spicesemanticpropositionalimage} focuses on semantics. SPIDEr \cite{liu2017improved}, used in \cite{zhang2024c3net}, combines both metrics.
For semantic alignment, BERTScore \cite{zhang2020bertscoreevaluatingtextgeneration} compares BERT embeddings \cite{sun2024factawaremultimodalretrievalaugmentation, shohan2024xlheadtagsleveragingmultimodalretrieval}, and evaluates fluency \cite{chen-etal-2022-murag,10535103,ma2024multimodalretrievalaugmentedmultimodal}. 
CLIP Score \cite{hessel-etal-2021-clipscore}, used in \cite{Sharifymoghaddam2024UniRAGUR, zhang2024c3net}, measures image-text similarity using CLIP \cite{radford2021learning}.
For image quality, FID (Fréchet Inception Distance) \cite{heusel2017gans} compares feature distributions \cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling, zhao-etal-2024-unifashion, Sharifymoghaddam2024UniRAGUR, zhang2024c3net}, while KID (Kernel Inception Distance) \cite{bińkowski2018demystifying} provides an unbiased alternative. 
Inception Score (IS) evaluates image diversity and quality through classification probabilities \cite{10535103}.
For audio evaluation, \cite{zhang2024c3net} uses human annotators to assess sound quality (OVL) and text relevance (REL), while also employing Fréchet Audio Distance (FAD) \cite{kilgour2019frechetaudiodistancemetric}, an audio-specific variant of FID.

System efficiency is measured through FLOPs, execution time, response time, and retrieval time per query \cite{nguyen2024multimodallearnedsparseretrieval,strand2024soccerragmultimodalsoccerinformation,dang2024multi, Zhou_2024}. Domain-specific metrics include geodesic distance for geographical accuracy \cite{zhou2024img2loc}, and Clinical Relevance for medical applications \cite{lahiri2024alzheimerragmultimodalretrievalaugmented}.

% \noindent
% \paragraph{Retrieval Evaluation} Evaluates how well a system retrieves relevant information from a database or knowledge source. Accuracy, recall, and precision are used to evaluate retrieval and downstream task performance, the F1 score also being used to assess both Recall and Precision simultaneously. A more commonly used metric than recall is Recall@K, which measures the proportion of relevant items appearing within the top K retrieved results. Another important metric Mean Reciprocal Rank (MRR), which is utilized by \cite{omar-etal-2024-multi, nguyen2024multimodallearnedsparseretrieval}.

% \noindent
% \paragraph{Modality Evaluation} These evaluations focus on criteria that rely on modalities. Text and image are the most commonly used modalities in multimodal RAG. This category of evaluations considers various aspects of these modalities, including the alignment between generated text and images, the fluency of the text, the quality of image captions, and other related factors. 
% In text similarity and overlap metrics, Exact Match (EM), BLEU \cite{papineni-etal-2002-bleu}, ROUGE \cite{lin-2004-rouge} and METEOR \cite{banerjee-lavie-2005-meteor} are used. MultiRAGen \cite{shohan2024xlheadtagsleveragingmultimodalretrieval} uses Multilingual ROUGE for multilingual settings.
% For image captioning evaluation, CIDEr (Consensus-Based Image Description Evaluation) \cite{vedantam2015cider} is a widely used metric for evaluating image captioning quality. It quantifies the alignment between a generated caption and a set of human-written reference captions by leveraging the term frequency-inverse document frequency (TF-IDF) and cosine similarity. \cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling, zhao-etal-2024-unifashion, luo2024doestextualinformationaffect, yuan2024ragdrivergeneralisabledrivingexplanations, Sharifymoghaddam2024UniRAGUR, Hu_2023_CVPR, rao2024ravenmultitaskretrievalaugmented,xu2024retrievalaugmentedegocentricvideocaptioning,kim2024you,zhang2024c3net} use CIDEr for evaluation. SPICE (Semantic Propositional Image Caption Evaluation) \cite{anderson2016spicesemanticpropositionalimage} is a metric that emphasizes semantic aspects, in contrast to CIDEr. SPIDEr \cite{liu2017improved}, utilized in \cite{zhang2024c3net}, is a linear combination of both SPICE and CIDEr.
% In semantic alignment evaluations, BERTScore \cite{zhang2020bertscoreevaluatingtextgeneration}, employed in \cite{sun2024factawaremultimodalretrievalaugmentation, shohan2024xlheadtagsleveragingmultimodalretrieval}, measures semantic similarity by comparing BERT-generated embeddings. \cite{chen-etal-2022-murag,10535103,ma2024multimodalretrievalaugmentedmultimodal}  evaluate Fluency by employing BERTScore. CLIP Score \cite{hessel-etal-2021-clipscore}, used in \cite{Sharifymoghaddam2024UniRAGUR, zhang2024c3net}, measures image-text similarity using the CLIP model \cite{radford2021learning}. FID (Fréchet Inception Distance) \cite{heusel2017gans} quantifies image quality by computing the distributional difference between the generated and original image features. \cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling, zhao-etal-2024-unifashion, Sharifymoghaddam2024UniRAGUR, zhang2024c3net} use FID in their works. \cite{zhao-etal-2024-unifashion} use Kernel Inception Distance (KID) \cite{bińkowski2018demystifying} for evaluation. KID is another metric used to evaluate the quality of generated images. It is similar to FID but offers an unbiased estimator. \cite{10535103} employs the Inception Score (IS) to assess the diversity and quality of generated images based on classification probabilities obtained from the Inception network.
% For the evaluation of audio synthesis, \cite{zhang2024c3net} assesses overall sound quality (OVL) and relevance to the text (REL) with the assistance of human annotators. Additionally, this study employs the Fréchet Audio Distance (FAD) \cite{kilgour2019frechetaudiodistancemetric} as an evaluation metric. FAD serves a similar purpose to the FID but is specifically designed for the audio domain.

% FLOPs, execution time, response time, and average retrieval time per query are utilized to assess efficiency and computational performance \cite{nguyen2024multimodallearnedsparseretrieval,strand2024soccerragmultimodalsoccerinformation,dang2024multi,Zhou_2024}.
% \cite{10.1145/3626772.3657929} employs Spearman's Rank Correlation (SRC), which calculates the Pearson correlation between the rank values of two variables. Several evaluation metrics are domain- and problem-specific. Geodesic distance measurement quantifies the model's accuracy by calculating the geodesic distance between predicted and actual geographical coordinates \cite{zhou2024img2loc}. \cite{lahiri2024alzheimerragmultimodalretrievalaugmented} utilizes Clinical Relevance (CR) for evaluation.

% \noindent
% \subsection{Retrieval Performance} Metrics  This category includes metrics such as Accuracy, Recall, Precision, and Exact Match, with the F1 score also being used to assess both Recall and Precision simultaneously. A more commonly applied metric than Recall is Recall@K, which differs in that it measures the proportion of relevant items that appear within the top K retrieved items. Another significant metric in this category is Mean Reciprocal Rank (MRR), which, unlike Recall, also considers the rank of the retrieved document. MRR is the average of the reciprocal ranks of the first relevant document retrieved for each query. This metric can be adapted for retrieval evaluation according to the modality in question. For instance, \cite{zhou2024img2loc} utilizes Geodesic Distance Measurement, which calculates the distance between two geographical points.

% Metrics evaluating how well a system retrieves relevant information from a database or knowledge source. These include Accuracy, Recall, Precision, Exact Match, and the F1 score, which balances Recall and Precision. A widely used variant is Recall@K, which measures the proportion of relevant items among the top K retrieved results. Another key metric is Mean Reciprocal Rank (MRR), which accounts for the rank of the first relevant document retrieved. MRR can be adapted for different modalities; for instance, \cite{zhou2024img2loc} employs Geodesic Distance Measurement to calculate distances between geographic points.
% \noindent
% \subsection{Fluency and Readability}
% Metrics assessing the naturalness, grammatical correctness, and coherence of generated text. Additional metrics include Fluency (FL) and Readability.



% \noindent
% \subsection{Text Similarity and Overlap} 
% Metrics quantifying similarity between generated and reference texts via n-gram overlap, semantic similarity, or text matching. 
% This category includes the metrics BLEU and ROUGE. BLEU focuses on n-gram precision with a brevity penalty, while ROUGE emphasizes recall through overlapping n-grams.

% \noindent
% \subsection{Statistical Metrics} Quantitative measures derived from statistical analysis.
% One metric in this category is Spearman's Rank Correlation (SRC), which calculates the Pearson correlation between the rank values of two variables.

% \noindent
% \subsection{Efficiency and Computational Performance}
% Metrics measuring system performance, including processing speed and resource consumption. For instance, FLOPs quantify computational cost by counting the number of floating-point operations. \cite{Zhou_2024} employs Average Retrieval Time Per Query to assess document retrieval speed.

% \noindent
% \subsection{User-focused Metrics} 
% Metrics emphasizing user-centric evaluation, including satisfaction \cite{dang2024multi}, response time \cite{dang2024multi}, and perceived usefulness \cite{zhu2024murarsimpleeffectivemultimodal}.

% \noindent
% \subsection{Clinical and Specialized Metrics} Domain-specific metrics for fields like healthcare, focusing on clinical relevance, reliability, and precision.

\section{Key Innovations and Methodologies}
\subsection{Retrieval Strategy}
\label{sec_retrieval_strategies}
\subsubsection{Efficient Search and Similarity Retrieval} \label{sec_similarity_search}
Modern multimodal RAG systems encode diverse input modalities into a unified embedding space to enable direct cross-modal retrieval. Recent advancements in CLIP-based \cite{radford2021learning} and BLIP-inspired \cite{li2022blip} approaches have driven the evolution of contrastive learning strategies through novel multimodal retrieval architectures and training methodologies \cite{zhou2024marvelunlockingmultimodalcapability, 10.1007/978-3-031-73021-4_23, zhang2024gmeimprovinguniversalmultimodal}. As these multi-encoder models project different modalities into a shared latent space, multimodal RAGs rely on efficient search strategies to retrieve relevant external knowledge. 

Maximum inner product search (MIPS) variants are widely used for fast and direct similarity comparisons \cite{tiwari2024faster, wang2023musteffectivescalableframework, 10.14778/3579075.3579084}. Systems such as MuRAG \cite{chen-etal-2022-murag} and RA-CM3 \cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling} employ approximate MIPS to efficiently retrieve the top-k candidates by maximizing the inner product between the query vector and a large collection of image–text embeddings. Large-scale implementations leverage distributed MIPS techniques, such as TPU-KNN \cite{chern2022tpuknnknearestneighbor}, for high-speed retrieval. Additionally, ScaNN (Scalable Nearest Neighbors) \cite{guo2020acceleratinglargescaleinferenceanisotropic}, MAXSIM score \cite{chan2008maxsim, cho2024m3docragmultimodalretrievalneed}, and approximate KNN methods \cite{caffagni2024wiki} have been adopted for efficient similarity computation.

Recent advancements in MIPS optimization focus on reducing retrieval latency and improving accuracy. These include adaptive quantization \cite{Zhang_Lian_Zhang_Wang_Chen_2023, li2024adaptivedatasetquantization}, hybrid sparse-dense representations \cite{nguyen2024multimodallearnedsparseretrieval, zhang2024efficienteffectiveretrievaldensesparse}, and learned index structures \cite{10.1145/3580305.3599897, basnet2024deeperimpactoptimizingsparselearned}. These techniques help optimize retrieval performance by balancing efficiency and precision in multimodal RAG systems.

\subsubsection{Modality-Based Retrieval} \label{sec_modality_based}
Modality-aware retrieval techniques optimize retrieval efficiency by leveraging the unique characteristics of each modality.

\paragraph{Text-Centric Retrieval} This remains foundational in multimodal RAG systems, with both traditional methods like BM25 \cite{INR-019}, and dense retrievers such as MiniLM \cite{wang2020minilmdeepselfattentiondistillation} and BGE-M3 \cite{chen2024bgem3embeddingmultilingualmultifunctionality} dominate text-based evidence retrieval \cite{chen2022reimagenretrievalaugmentedtexttoimagegenerator, suri2024visdommultidocumentqavisually, nan-etal-2024-omg}. Novel approaches also address the need for fine-grained semantic matching and domain specificity: For instance, ColBERT \cite{10.1145/3397271.3401075} and PreFLMR \cite{lin-etal-2024-preflmr} employ token-level interaction mechanisms that preserve nuanced textual details to improve precision for multimodal queries, while RAFT \cite{zhang2024raft} and CRAG \cite{yan2024corrective} enhance retrieval performance by ensuring precise citation of text spans. 

\paragraph{Vision-Centric Retrieval} This focuses on directly leveraging image representations for knowledge extraction \cite{kumar2024improvingmedicalmultimodalcontrastive, yuan2023rammretrievalaugmentedbiomedicalvisual}. Systems such as EchoSight \cite{Yan_2024} and ImgRet \cite{shohan2024xlheadtagsleveragingmultimodalretrieval} retrieve visually similar content by using reference images as queries. In addition, composed Image Retrieval (CMI) models \cite{feng2023vqa4cirboostingcomposedimage, zhao-etal-2024-unifashion, jang2024visual} enhance retrieval by integrating multiple image features into a unified query representation. Similarly, Pic2word \cite{saito2023pic2wordmappingpictureswords} maps visual content to textual descriptions, enabling zero-shot image retrieval.

\paragraph{Video-Centric Retrieval} These methods extend vision-based techniques by incorporating temporal dynamics and large video-language models (LVLMs), driven by novel frameworks like iRAG \cite{Arefeen_2024}, which introduces incremental retrieval for sequential video understanding, and MV-Adapter \cite{jin2024mv}, optimizing multimodal transfer learning for video-text retrieval. Recent breakthroughs focus on long-context processing: 
Video-RAG \cite{luo2024videoragvisuallyalignedretrievalaugmentedlong} leverages visually aligned auxiliary texts (OCR/ASR) to enhance retrieval without proprietary models, T-Mass \cite{wang2024textmassmodelingstochastic} models text as a stochastic embedding to enhance text-video retrieval, and VideoRAG \cite{ren2025videoragretrievalaugmentedgenerationextreme} -employs dual-channel architectures with graph-based knowledge grounding for extreme-length videos. For temporal reasoning, CTCH \cite{ijcai2024p136} uses contrastive transformer hashing to model long-term dependencies, while RTime \cite{10.1145/3664647.3680731} introduces reversed-video hard negatives to rigorously benchmark temporal causality. Meanwhile, for complex video understanding, OmAgent \cite{zhang-etal-2024-omagent} adopts a divide-and-conquer framework, and DRVideo \cite{ma2024drvideodocumentretrievalbased} addresses long-video understanding with a document-based retrieval approach.

\paragraph{Document Retrieval and Layout Understanding} 
Recent research has advanced beyond traditional uni-modal retrieval methods toward models that directly process entire document pages. These approaches integrate textual, visual, and structural elements like tables, font styles, and page layouts to enhance retrieval performance in complex documents.
ColPali \cite{faysse2024colpaliefficientdocumentretrieval} pioneers end-to-end document image retrieval by embedding page patches with a vision-language backbone, bypassing OCR entirely. Models like ColQwen2 \cite{Qwen2VL, 10.1145/3397271.3401075} and M3DocVQA \cite{cho2024m3docragmultimodalretrievalneed} extend this paradigm with dynamic resolution handling and holistic multi-page reasoning. 

Newer frameworks refine efficiency and layout understanding: ViTLP \cite{mao-etal-2024-visually} and DocLLM \cite{wang-etal-2024-docllm} pre-train generative models to align spatial layouts with text, while CREAM \cite{10.1145/3664647.3680750} employs coarse-to-fine retrieval with multimodal efficient tuning to balance accuracy and computation costs. Finally, mPLUG-DocOwl 1.5 \cite{hu-etal-2024-mplug} and 2 \cite{hu2024mplugdocowl2highresolutioncompressingocrfree} unify structure learning across formats (e.g., invoices, forms) without OCR dependencies.

\subsubsection{Re-ranking and Selection Strategies} \label{sec_reranking_strategies} Effective retrieval in multimodal RAG systems requires not only identifying relevant information but also prioritizing retrieved candidates. Re-ranking and selection strategies enhance retrieval quality by optimizing example selection, refining relevance scoring, and applying filtering mechanisms.

\paragraph{Optimized Example Selection} These techniques often employ multi-step retrieval, integrating both supervised and unsupervised selection strategies \cite{luo2024doestextualinformationaffect, yuan2023rammretrievalaugmentedbiomedicalvisual}. For instance, \cite{su2024hybrid} enhance multimodal inputs using probabilistic control keywords to improve credibility, RULE \cite{xia-etal-2024-rule} calibrates retrieved context selection via statistical methods like the Bonferroni correction to mitigate factuality risks, and clustering-based key-frame selection ensures diversity in video-based retrieval \cite{10.1145/3626772.3657833}.

\paragraph{Relevance Score Evaluation} Several methods employ advanced scoring mechanisms to improve retrieval relevance \cite{mortaheb2025rerankingcontextmultimodalretrieval,mortaheb2025ragcheckevaluatingmultimodalretrieval, 10535103}. Multimodal similarity measures, including structural similarity index measure (SSIM) \cite{wang2020deeplearningimagesuperresolution}, normalized cross-correlation (NCC), and BERTScore \cite{zhang2020bertscoreevaluatingtextgeneration}, aid in re-ranking retrieved documents. Cross-encoders trained on sequence classification tasks refine relevance scoring within retrieval pipelines \cite{10535103}, while hierarchical post-processing techniques integrate retrieval, passage-level, and answer confidence scores for improved ranking \cite{zhang2024mr2agmultimodalretrievalreflectionaugmentedgeneration, Yan_2024, xu2024retrievalaugmentedegocentricvideocaptioning}. LDRE \cite{10.1145/3626772.3657740} employs semantic ensemble methods to adaptively weigh multiple caption features, whereas RAGTrans \cite{10.1145/3637528.3672041} and OMG-QA \cite{nan-etal-2024-omg} incorporate traditional ranking functions like BM25 \cite{INR-019}.

\paragraph{Filtering Mechasnism} This ensures high-quality retrieval by eliminating irrelevant data. Hard negative mining, as used in GME \cite{zhang2024gmeimprovinguniversalmultimodal} and MM-Embed \cite{lin2024mmembeduniversalmultimodalretrieval}, mitigates modality bias through modality-aware sampling and synthesized negatives. Similarly, consensus-based filtering approaches, seen in MuRAR \cite{zhu2024murarsimpleeffectivemultimodal} and ColPali \cite{faysse2024colpaliefficientdocumentretrieval}, employs source attribution and multi-vector mapping to filter out low-similarity candidates. Moreover, dynamic modality filtering methods, such as RAFT \cite{zhang2024raft}, Img2Loc \cite{zhou2024img2loc}, and MAIN-RAG \cite{chang2024mainragmultiagentfilteringretrievalaugmented}, train retrievers to disregard confusing data, thereby enhancing the discriminative capacity and overall robustness of multi-modal retrieval systems.

% \cite{Yan_2024}, combine visual and textual similarity metrics for improved relevance scoring \cite{xu2024retrievalaugmentedegocentricvideocaptioning}, and leverage transformer-based cross-attention mechanisms to refine selection \cite{mortaheb2025rerankingcontextmultimodalretrieval}.

\subsection{Fusion Mechanisms}
\label{sec_fusion_mechanisms}
\subsubsection{Score Fusion and Alignment} \label{sec_socre_alignment}
Models in this category utilize distinct strategies to align multimodal representations. \citet{10535103} convert text, tables, and images into a single textual format using a cross-encoder trained for relevance scoring. \citet{Sharifymoghaddam2024UniRAGUR} introduce interleaved image–text pairs that vertically merge multiple few-shot images (as in LLaVA \cite{liu2023llava}), while aligning modalities via CLIP score fusion \cite{hessel-etal-2021-clipscore} and BLIP feature fusion \cite{li2022blip}. \citet{riedler2024textoptimizingragmultimodal}, Wiki-LLaVA \cite{caffagni2024wiki}, C3Net \cite{zhang2024c3net}, and MegaPairs \cite{zhou2024megapairsmassivedatasynthesis}, embed images and queries into a shared CLIP space. 

VISA \cite{ma2024visaretrievalaugmentedgeneration} employs the Document Screenshot Embedding (DSE) model to align textual queries with visual document representations by encoding both into a shared embedding space. REVEAL \cite{Hu_2023_CVPR} injects retrieval scores into attention layers to minimize L2-norm differences between query and knowledge embeddings, and MA-LMM \cite{he2024ma} aligns video-text embeddings via a BLIP-inspired Query Transformer \cite{li2022blip}. LLM-RA \cite{jian-etal-2024-large} concatenates text and visual embeddings into joint queries to reduce retrieval noise, while RA-BLIP \cite{ding2024rablipmultimodaladaptiveretrievalaugmented} employs a 3-layer BERT-based adaptive fusion module to unify visual–textual semantics. \citet{xue2024enhancedmultimodalragllmaccurate} use a prototype-based embedding network \cite{10204116} to map object-predicate pairs into a shared semantic space, aligning visual features with textual prototypes.
Re-IMAGEN \cite{chen2022reimagenretrievalaugmentedtexttoimagegenerator} balances creativity and entity fidelity in text-to-image synthesis via interleaved classifier-free guidance during diffusion sampling.
To improve multimodal alignment, VISRAG \cite{yu2024visragvisionbasedretrievalaugmentedgeneration} enhances alignment with position-weighted mean pooling on VLM hidden states, prioritizing later tokens for relevance, and RAG-Driver \cite{yuan2024ragdrivergeneralisabledrivingexplanations} aligns visual–language embeddings using visual instruction tuning and an MLP projector.

\subsubsection{Attention-Based Mechanisms} \label{sec_attention_based}
Attention-based methods dynamically weight cross-modal interactions to support task-specific reasoning. EMERGE \cite{zhu2024emergeintegratingragimproved}, MORE \cite{cui2024moremultimodalretrievalaugmented}, and AlzheimerRAG \cite{lahiri2024alzheimerragmultimodalretrievalaugmented} integrate heterogeneous data via cross-attention. 
RAMM \cite{yuan2023rammretrievalaugmentedbiomedicalvisual} employs a dual-stream co-attention transformer, combining self-attention and cross-attention to fuse retrieved biomedical images/texts with input data. RAGTrans \cite{10.1145/3637528.3672041} applies user-aware attention to social media features. 

For video-text alignment, MV-Adapter \cite{jin2024mv} leverages Cross Modality Tying to align embeddings, and M2-RAAP \cite{10.1145/3626772.3657833} enhances fusion through an auxiliary caption-guided strategy that re-weights frames and text captions based on intra-modal similarity. A mutual-guided alignment head then filters misaligned features using dot-product similarity and frame-to-token attention, generating refined frame-specific text representations. \citet{xu2024retrievalaugmentedegocentricvideocaptioning} condition text generation on visual features using gated cross-attention, and Mu-RAG \cite{chen-etal-2022-murag} employs intermediate cross-attention for open-domain QA. \citet{kim2024you} leverage cross-modal memory retrieval with pre-trained CLIP ViT-L/14 to map video-text pairs into a shared space, enabling dense captioning through the attention-based fusion of retrieved memories.

\subsubsection{Unified Frameworks and Projections}\label{sec_unified_frameworks_projections}
Unified frameworks and projection methods consolidate multimodal inputs into coherent representations. \citet{su2024hybrid} employ hierarchical cross-chains and late fusion for healthcare data, while IRAMIG \cite{Liu_2024} iteratively integrates multimodal results into unified knowledge representations. M3DocRAG \cite{cho2024m3docragmultimodalretrievalneed} flattens multi-page documents into a single embedding tensor, and PDF-MVQA \cite{ding2024pdfmvqadatasetmultimodalinformation} fuses Region-of-Interest (RoI)-based and patch-based (CLIP) vision-language models \cite{ijcai2022p773}. 

DQU-CIR \cite{Wen_2024} unifies raw data by converting images into text captions for complex queries and overlaying text onto images for simple ones, then fusing embeddings via MLP-learned weights. SAM-RAG \cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration} aligns image-text modalities by generating captions for images, converting the multimodal input into unimodal text for subsequent processing. UFineBench \cite{zuo2024ufinebench} utilizes a shared granularity decoder for ultra-fine text–person retrieval. \citet{nguyen2024multimodallearnedsparseretrieval} introduce Dense2Sparse projection, converting dense embeddings from models like BLIP/ALBEF \cite{li2022blip} into sparse lexical vectors using layer normalization and probabilistic expansion control to optimize storage and interpretability.


\subsection{Augmentation Techniques} 
\label{sec_augmentation}
\noindent
Basic RAG systems typically follow a single retrieval step and pass retrieved content directly to the generation phase without further refinement, which can lead to inefficiencies and suboptimal outputs. 
To address this, augmentation techniques have been introduced to refine retrieved data beforehand, improving multimodal interpretation, structuring, and integration \cite{Gao2023RetrievalAugmentedGF}.

\subsubsection{Context Enrichment} \label{sec_context_enrichment}
This focuses on enhancing the relevance of retrieved knowledge by refining or expanding retrieved data. General approaches incorporate additional contextual elements (e.g., text chunks, image tokens, structured data) to provide a richer grounding for generation \cite{caffagni2024wiki, xue2024enhancedmultimodalragllmaccurate}.
EMERGE \cite{zhu2024emergeintegratingragimproved} enriches context by integrating entity relationships and semantic descriptions.
MiRAG \cite{omar-etal-2024-multi} expands initial queries through entity retrieval and reformulation, enhancing subsequent stages for the visual question-answering.
Video-RAG \cite{luo2024videoragvisuallyalignedretrievalaugmentedlong} enhances long-video understanding through Query Decoupling, which reformulates user queries into structured retrieval requests to extract auxiliary multimodal context.
Img2Loc \cite{zhou2024img2loc} enhances accuracy by including both the most similar and the most dissimilar points from the database in the prompt, allowing the model to rule out implausible locations for its predictions.

\subsubsection{Adaptive and Iterative Retrieval} \label{sec_adaptive} 
For more complex queries, dynamic retrieval mechanisms have proven effective. Adaptive retrieval approaches optimize relevance by adjusting retrieval dynamically. SKURG \cite{SKURG} determines the number of retrieval hops based on query complexity. SAM-RAG \cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration} and mR$^2$AG \cite{Zhang2024mR2AGMR} dynamically assess the need for external knowledge and filter irrelevant content using MLLMs to retain only task-critical information. MMed-RAG \cite{xia2024mmedragversatilemultimodalrag} further improves retrieval precision by discarding low-relevance results, while OmniSearch \cite{li2024benchmarking} introduces a self-adaptive retrieval agent that dynamically decomposes complex multimodal questions into sub-question chains and plans retrieval actions in real-time based on retrieved content.

Iterative approaches refine results over multiple steps by incorporating feedback from prior iterations. IRAMIG \cite{Liu_2024} improves multimodal retrieval by dynamically updating queries based on retrieved content. Similarly, OMG-QA \cite{nan-etal-2024-omg} introduces a multi-round retrieval strategy, where each retrieval step incorporates episodic memory to refine subsequent queries. An evaluation module assesses retrieval effectiveness at each step, guiding the refinement of subsequent retrieval efforts through feedback. RAGAR \cite{khaliq-etal-2024-ragar} further enhances contextual consistency by iteratively adjusting retrieval based on prior responses and multimodal analysis.


\subsection{Generation Techniques}
\label{sec_generation}
\noindent
Recent advancements in multimodal RAG generation focus on robustness, cross-modal coherence, and task-specific adaptability. These innovations can be broadly categorized into these sections:

\subsubsection{In-Context Learning} \label{sec_incontext_learning}
In-context learning (ICL) with retrieval augmentation enhances reasoning in multimodal RAGs by leveraging retrieved content as few-shot examples without requiring retraining. Models such as RMR \cite{tan2024retrievalmeetsreasoninghighschool}, \citet{Sharifymoghaddam2024UniRAGUR}, and RA-CM3 \cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling}, extend this paradigm to multimodal RAG settings. 

RAG-Driver \cite{yuan2024ragdrivergeneralisabledrivingexplanations} refines ICL by retrieving relevant driving experiences from a memory database, ensuring scenario-specific contextual alignment. MSIER \cite{luo2024doestextualinformationaffect} further enhances example selection through a Multimodal Supervised In-Context Examples Retrieval framework, leveraging a foundation MLLM scorer to evaluate both textual and visual relevance. Meanwhile, Raven \cite{rao2024ravenmultitaskretrievalaugmented} introduces Fusion-in-Context Learning, a novel approach that enriches ICL by incorporating a more diverse set of in-context examples, leading to better performance than standard ICL.

\subsubsection{Reasoning} \label{sec_reasoning}
Structured reasoning techniques, such as chain-of-thought (CoT), decompose complex reasoning into smaller sequential steps, enhancing coherence and robustness in multimodal RAG systems. RAGAR \cite{khaliq-etal-2024-ragar} introduces Chain of RAG and Tree of RAG to iteratively refine fact-checking queries and explore branching reasoning paths for more robust evidence generation. 

VisDoM \cite{suri2024visdommultidocumentqavisually} integrates CoT with evidence curation to ensure logical and contextual accuracy, while SAM-RAG \cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration} employs reasoning chains alongside multi-stage answer verification to enhance the relevance, utility, and support of generated responses. Meanwhile, LDRE \cite{10.1145/3626772.3657740} leverages LLMs for divergent compositional reasoning, generating refined captions by incorporating dense captions and modification text.

\subsubsection{Instruction Tuning} \label{sec_instruction_tuning} Several works have fine-tuned or instruct-tuned generation components for specific applications. RA-BLIP \cite{ding2024rablipmultimodaladaptiveretrievalaugmented} leverages the Q-Former architecture from InstructBLIP \cite{dai2023instructblipgeneralpurposevisionlanguagemodels} to extract visual features based on question instructions, while RAGPT \cite{lang2025retrieval} employs a context-aware prompter to generate dynamic prompts from relevant instances. mR$^2$AG \cite{Zhang2024mR2AGMR} uses instruction tuning with the mR2AG-IT dataset to train MLLMs to adaptively invoke retrieval, identify relevant evidence, and generate accurate answers for knowledge-based VQA tasks. 

RagVL \cite{chen2024mllm} employs instruction tuning to enhance the ranking capability of MLLMs, serving them as a re-ranker for filtering the top-k retrieved images. \citet{jang2024visual} focus on distinguishing image differences to generate descriptive textual responses. MMed-RAG \cite{xia2024mmedragversatilemultimodalrag} applies preference fine-tuning to help models balance retrieved knowledge with internal reasoning. 

To improve generation quality, MegaPairs \cite{zhou2024megapairsmassivedatasynthesis} and Surf \cite{sun2024surfteachinglargevisionlanguage} construct multimodal instruction-tuning datasets from prior LLM errors, while Rule \cite{xia-etal-2024-rule} refines Med-LVLM through direct preference optimization to mitigate overreliance on retrieved contexts.

\subsubsection{Source Attribution and Evidence Transparency} \label{sec_source_attrib} Ensuring source attribution in multimodal RAG systems is a key focus of recent research. MuRAR \cite{zhu2024murarsimpleeffectivemultimodal} integrates multimodal data, fetched by a source-based retriever, to refine LLM's initial response, ensuring informativeness. VISA \cite{ma2024visaretrievalaugmentedgeneration} uses large vision-language models to generate answers with visual source attribution by identifying and highlighting supporting evidence in retrieved document screenshots. Similarly, OMG-QA \cite{nan-etal-2024-omg} ensures transparency by prompting the LLM to explicitly cite evidence in generated responses.

\subsection{Training Strategies}
\label{sec_training_strategies}
\noindent
Training multimodal RAG models involves a multi-stage process to effectively handle cross-modal interactions \cite{chen-etal-2022-murag}. Pretraining establishes the foundation using large paired datasets to learn cross-modal relationships while fine-tuning adapts models to specific tasks by leveraging cross-modal attention \cite{ye2019cross}.
For instance, REVEAL \cite{Hu_2023_CVPR} integrates multiple training objectives. Its pretraining phase optimizes Prefix Language Modeling Loss (\(L_{\text{PrefixLM}}\)), where the model predicts text continuations from a prefix and an image. Supporting losses include Contrastive Loss (\(L_{\text{contra}}\)) for aligning queries with pseudo-ground-truth knowledge, Disentangled Regularization Loss (\(L_{\text{decor}}\)) to improve embedding expressiveness, and Alignment Regularization Loss (\(L_{\text{align}}\)) to align query and knowledge embeddings. During fine-tuning, a cross-entropy objective trains the model for tasks like VQA and image captioning. Details of formulas for widely used RAG loss functions can be found in Appendix~(\S\ref{sec:app_loss}).

\subsubsection{Alignment} \label{sec_alignment}
Contrastive learning improves representation quality by pulling positive pairs closer and pushing negative pairs apart in the embedding space. A common objective is the InfoNCE loss \cite{oord2019representationlearningcontrastivepredictive}, which maximizes the mutual information between positive pairs while minimizing similarity to negatives. Several multimodal RAG models, such as VISRAG \cite{yu2024visragvisionbasedretrievalaugmentedgeneration}, MegaPairs \cite{zhou2024megapairsmassivedatasynthesis} and SAM-RAG \cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration} utilize InfoNCE loss to improve retrieval-augmented generation. 
Furthermore, EchoSight \cite{Yan_2024} enhances retrieval accuracy by selecting visually similar yet contextually distinct negatives, while HACL \cite{jiang2024hallucination} improves generation by introducing hallucinative captions as distractors. Similarly, UniRaG \cite{10535103} strengthens retrieval by incorporating hard negative documents, helping the model distinguish relevant contexts from noise.

The eCLIP loss \cite{kumar2024improvingmedicalmultimodalcontrastive} extends contrastive training by integrating expert-annotated data and an auxiliary Mean Squared Error (MSE) loss to refine embedding quality. Mixup strategies further augment training by generating synthetic positive pairs, improving generalization in contrastive learning \cite{kumar2024improvingmedicalmultimodalcontrastive}.
Dense2Sparse \cite{nguyen2024multimodallearnedsparseretrieval} incorporates two unidirectional losses: an image-to-caption loss \(\ell(I \to C)\) and a caption-to-image loss \(\ell(C \to I)\).  It enforces sparsity through L1 regularization, optimizing retrieval precision by balancing dense and sparse representations. 

\subsubsection{Generation} A key aspect of multimodal RAG is the generation ability. Autoregressive language models are typically trained using Cross-Entropy Loss \cite{NEURIPS2020_1457c0d6}. For image generation, widely used approaches include Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} and Diffusion Models \cite{ho2020denoising}. GANs employ various loss functions, such as Binary Cross-Entropy Loss, Minimax Loss, Wasserstein Loss (WGAN), and Hinge Loss. Diffusion Models utilize Mean Squared Error (MSE) Loss for noise prediction, a common approach in Denoising Diffusion Probabilistic Models (DDPMs) \cite{ho2020denoising}.

\subsubsection{Robustness and Noise Management} \label{sec:robustness}
Multimodal training faces challenges such as noise and modality-specific biases \citet{buettner-kovashka-2024-quantifying}. Managing noisy retrieval inputs is critical for maintaining model performance. MORE \cite{cui2024moremultimodalretrievalaugmented} injects irrelevant results during training to enhance focus on relevant inputs. AlzheimerRAG \cite{lahiri2024alzheimerragmultimodalretrievalaugmented} uses progressive knowledge distillation to reduce noise while maintaining multimodal alignment. RAGTrans \cite{10.1145/3637528.3672041} leverages hypergraph-based knowledge aggregation to refine multimodal representations, ensuring more effective propagation of relevant information. RA-BLIP \cite{ding2024rablipmultimodaladaptiveretrievalaugmented} introduces the Adaptive Selection Knowledge Generation (ASKG) strategy, which leverages the implicit capabilities of LLMs to filter relevant knowledge for generation through a denoising-enhanced loss term, eliminating the need for fine-tuning. This approach achieves strong performance compared to baselines while significantly reducing computational overhead by minimizing trainable parameters. 

RagVL \cite{chen2024mllm} improves robustness through noise-injected training by adding hard negative samples at the data level and applying Gaussian noise with loss reweighting at the token level, enhancing the model’s resilience to multimodal noise. 
Finally, RA-CM3 \cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling} enhances generalization using Query Dropout, which randomly removes query tokens during retrieval, serving as a regularization method that improves generator performance.
% AlzheimerRAG \cite{lahiri2024alzheimerragmultimodalretrievalaugmented} employs a distillation loss function that integrates the Cross-Entropy loss with the KL divergence between the softened predictions of the teacher and student models. 
% Re-IMAGEN \cite{chen2022reimagenretrievalaugmentedtexttoimagegenerator} employs a time-reweighted squared error loss.


% \subsection{Loss Function}

% \subsection{Contrastive Learning Losses} 
% Contrastive loss is a metric learning loss function as it calculates the distance between vector pairs. A widely used loss function in this category is the CLIP or InfoNCE loss, which aims to minimize the embedding distance between positive pairs while maximizing the distance between negative pairs. The eCLIP \cite{kumar2024improvingmedicalmultimodalcontrastive} loss enhances this approach by incorporating expert-annotated data. The mixup strategy generates extra positive pairs during the computation of the CLIP InfoNCE loss. Furthermore, eCLIP integrates an auxiliary mean-squared error (MSE) loss into its overall loss function. VISRAG \cite{yu2024visragvisionbasedretrievalaugmentedgeneration} and MegaPairs \cite{zhou2024megapairsmassivedatasynthesis} employ the InfoNCE loss function. EchoSight \cite{Yan_2024} uses hard negative sampling in contrastive learning. Specifically, the negative samples are specifically selected from examples that are visually similar yet contextually distinct



% These are a class of metric learning loss functions that optimize representation learning by maximizing similarity between positive pairs while minimizing similarity between negative pairs in an embedding space. A widely used loss in this category is the InfoNCE loss. It employs a softmax-based formulation to maximize the alignment between paired embeddings while pushing apart unpaired samples. The eCLIP loss \cite{kumar2024improvingmedicalmultimodalcontrastive} extends this approach by incorporating expert-annotated data and an auxiliary mean-squared error (MSE) loss to refine embedding quality. Additionally, mixup strategies augment training by generating synthetic positive pairs, improving generalization in contrastive training. Methods such as VISRAG \cite{yu2024visragvisionbasedretrievalaugmentedgeneration} and MegaPairs \cite{zhou2024megapairsmassivedatasynthesis} leverage InfoNCE loss for multimodal retrieval-augmented generation (RAG). 
% Furthermore, EchoSight \cite{Yan_2024} employs hard negative sampling, selecting negative samples that are visually similar yet contextually distinct to improve retrieval accuracy and robustness.

% \textbf{Cross-Entropy Based Losses}:
% \noindent
% \subsection{Embedding Alignment Losses}
% Dense2Sparse \cite{nguyen2024multimodallearnedsparseretrieval} incorporates two unidirectional losses: an image-to-caption loss \(\ell(I \to C)\) and a caption-to-image loss \(\ell(C \to I)\). To enforce sparsity, it utilizes the Sparse Regularization Parameter, which employs L1 regularization.
% The REVEAL \cite{Hu_2023_CVPR} loss function integrates multiple components for pre-training and fine-tuning. The main pre-training objective is the Prefix Language Modeling Loss (\(L_{\text{PrefixLM}}\)), where the model predicts text continuations from a prefix and an image. Supporting losses include Contrastive Loss (\(L_{\text{contra}}\)) for aligning query embeddings with pseudo ground-truth knowledge, Disentangled Regularization Loss (\(L_{\text{decor}}\)) to enhance embedding expressiveness, and Alignment Regularization Loss (\(L_{\text{align}}\)) to align query and knowledge embeddings. During fine-tuning, a cross-entropy objective trains the model for tasks like VQA and image captioning. In pre-training, \(L_{\text{PrefixLM}}\) is the primary focus.


% \textbf{Preference Optimization Losses}:

% \textbf{Multimodal Matching Losses}:
% \noindent
% \subsection{Specialized Losses}
% AlzheimerRAG \cite{lahiri2024alzheimerragmultimodalretrievalaugmented} employs a distillation loss function that integrates the Cross-Entropy loss with the KL divergence between the softened predictions of the teacher and student models. Re-IMAGEN \cite{chen2022reimagenretrievalaugmentedtexttoimagegenerator} employs a time-reweighted squared error loss.


\section{Tasks Addressed by Multimodal RAGs}
\noindent
Multimodal RAG systems extend RAG beyond unimodal settings to tasks requiring cross-modal integration, enhancing performance across modalities such as text, images, and audio. In content generation, these models enhance image captioning \cite{10535103, Hu_2023_CVPR, rao2024ravenmultitaskretrievalaugmented} and text-to-image synthesis \cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling, chen2022reimagenretrievalaugmentedtexttoimagegenerator} by retrieving relevant contextual information. They also improve coherence in visual storytelling and ensure factual alignment in multimodal summarization \cite{tonmoy2024comprehensive}. In knowledge-intensive applications, multimodal RAG supports open-domain and knowledge-seeking question answering \cite{chen2024mllm, ding2024rablipmultimodaladaptiveretrievalaugmented, yuan2023rammretrievalaugmentedbiomedicalvisual}, video-based QA \cite{luo2024videoragvisuallyalignedretrievalaugmentedlong}, and fact verification \cite{khaliq-etal-2024-ragar}, grounding responses in retrieved knowledge and thereby mitigating hallucinations. 

Cross-modal retrieval further advances zero-shot image–text retrieval \cite{10.1145/3626772.3657740, 10.1145/3626772.3657833}. Additionally, the recent incorporation of chain-of-thought reasoning \cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration, khaliq-etal-2024-ragar} has enhanced its ability to support complex problem solving and logical inference. Finally, integrating multimodal RAG into interactive agents and AI assistants such as Gemini \cite{geminiteam2024geminifamilyhighlycapable} enables natural language-driven visual search, document understanding, and multimodal reasoning. 
The taxonomy of application domains can be seen in Figure~\ref{fig:app_taxonomy}.
The following sections explore domain-specific adaptations of these techniques in greater depth.


\noindent
\paragraph{Healthcare and Medicine} \label{sec_healthcare}
Multimodal RAG enhances clinical decision-making through integrated analysis of medical imaging, electronic health records, and biomedical literature. Systems like MMED-RAG \cite{xia2024mmedragversatilemultimodalrag} address diagnostic uncertainty in medical visual question answering by aligning radiology images with contextual patient data. RULE \cite{xia-etal-2024-rule} mitigates hallucinations in automated report generation through dynamic retrieval of clinically similar cases. AsthmaBot \cite{bahaj2024asthmabot} introduces a multimodal RAG-based approach for supporting asthma patients across multiple languages, enabling structured, language-specific semantic searches. Predictive frameworks such as Realm \cite{zhu2024realmragdrivenenhancementmultimodal} demonstrate robust risk assessment by fusing heterogeneous patient data streams, while Hybrid RAG \cite{su2024hybrid} advances privacy-preserving architectures for federated clinical data integration. FactMM-RAG \cite{sun2024factawaremultimodalretrievalaugmentation} automates radiology report drafting by retrieving biomarker correlations from medical ontologies, exemplifying the paradigm’s capacity to operationalize expert knowledge at scale.

\noindent
\paragraph{Software Engineering}\label{sec_Software_Engineering}
Code generation systems leverage multimodal RAG to synthesize context-aware solutions from technical documentation and version histories. DocPrompting \cite{zhou2023docprompting} improves semantic coherence in code completion by retrieving API specifications and debugging patterns. Commit message generation models like RACE \cite{shi-etal-2022-race} contextualize code diffs against historical repository activity, while CEDAR \cite{CEDAR} optimizes few-shot learning through retrieval-based prompt engineering. REDCODER \cite{parvez2021retrieval} enhances code summarization via semantic search across open-source repositories, preserving syntactic conventions across programming paradigms.

\noindent
\paragraph{Fashion and E-Commerce} \label{sec_Fashion}
Cross-modal alignment drives advancements in product discovery and design automation. UniFashion \cite{zhao-etal-2024-unifashion} enables style-aware retrieval by jointly embedding garment images and textual descriptors, while \citet{dang2024multi} reduces search friction through multimodal query expansion. LLM4DESIGN \cite{chen2024llm4design} demonstrates architectural design automation by retrieving compliance constraints and environmental impact assessments, underscoring RAG’s adaptability to creative domains.

\noindent
\paragraph{Entertainment and Social Computing} \label{sec_Entertainment} Multimedia analytics benefit from RAG’s capacity to correlate heterogeneous signals. SoccerRAG \cite{strand2024soccerragmultimodalsoccerinformation} derives tactical insights by linking match footage with player statistics. MMRA \cite{10.1145/3626772.3657929} predicts content virality through joint modeling of visual aesthetics and linguistic engagement patterns.

\noindent
\paragraph{Emerging Applications}\label{sec_Emerging_Applications} Autonomous systems adopt multimodal RAG for explainable decision-making, as seen in RAG-Driver’s \cite{yuan2024ragdrivergeneralisabledrivingexplanations} real-time retrieval of traffic scenarios during navigation. ENWAR \cite{nazar2024enwar} enhances wireless network resilience through multi-sensor fusion, while \citet{riedler2024textoptimizingragmultimodal} streamline equipment maintenance by retrieving schematics during fault diagnosis. Geospatial systems such as Img2Loc \cite{zhou2024img2loc} advance image geolocalization through cross-modal landmark correlation.

\section{Open Problems and Future Directions}
\noindent
Despite rapid advancements in multimodal RAG systems, fundamental challenges remain in achieving robust, efficient, and human-like reasoning across modalities. 
% Key limitations arise from the complexity of cross-modal alignment, the dynamic nature of real-world knowledge, and the need for scalable, trustworthy systems.

\subsection{Generalization, Explainability, and Robustness}
Multimodal RAG systems often struggle with domain adaptation and exhibit modality biases, frequently over-relying on text for both retrieval and generation \cite{winterbottom2020modalitybiastvqadataset}.
Explainability remains a major challenge, as these systems typically fail to attribute answers to precise sources.
Current methods often cite entire documents or large visual regions as source attribution rather than identifying the exact part of an image, speech, or other modality that led to the answer \cite{ma2024visaretrievalaugmentedgeneration, Hu_2023_CVPR}. 

In addition, the interplay between modalities affects the quality of outcomes produced by these models; for example, answers derived solely from text sources may differ in quality compared to those requiring a combination of text and image inputs \cite{10.1109/TPAMI.2018.2798607}.
They are also vulnerable to adversarial perturbations, such as misleading images influencing textual outputs, and their performance can degrade when relying on low-quality or outdated sources \cite{chen2022reimagenretrievalaugmentedtexttoimagegenerator}.
While the trustworthiness of unimodal RAGs has been studied \cite{zhou2024TrustworthyRAG}, enhancing the robustness of multimodal RAGs remains an open challenge and a promising research direction.


\subsection{Reasoning, Alignment, and Retrieval Enhancement}
Multimodal RAGs struggle with compositional reasoning, where information from different modalities must be logically integrated to generate coherent and contextually rich outputs. While cross-modal techniques such as Multimodal-CoT \cite{zhang2023multicot} have been proposed, further innovations are needed to improve the coherence and contextual relevance of multimodal outputs. Enhancing modality alignment and retrieval strategies, particularly for entity-aware retrieval, is essential. Moreover, despite the potential of knowledge graphs to enrich cross-modal reasoning, they remain largely underexplored in multimodal RAGs compared to text-based RAGs \cite{Zhang2024mR2AGMR, peng2024graphragsurvey}. 

Retrieval biases such as position sensitivity \cite{hu2024mragbench}, redundant retrieval \cite{nan-etal-2024-omg}, and biases propagated from training data or retrieved content \cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration}, pose significant challenges that require further attention. Another promising direction is developing a unified embedding space for all modalities, enabling direct multimodal search without intermediary conversion models (e.g., ASRs).
Despite some progress, mapping multimodal knowledge into a unified space remains an open challenge with significant potential.

\subsection{Agent-Based and Self-Guided Systems}
Recent trends indicate a shift towards agent-based multimodal RAGs that integrate retrieval, reasoning, and generation across diverse domains. Unlike static RAG systems, future multimodal RAGs should integrate interactive feedback and self-guided decision-making to iteratively refine outputs. Existing feedback mechanisms often fail to accurately determine whether errors stem from retrieval, generation, or other stages \cite{10.1145/3626772.3657833}.
The incorporation of reinforcement learning and end-to-end human-aligned feedback into multimodal RAGs remains largely unexplored but holds significant potential for enhancing these systems. These methods could enable multimodal RAGs to assess whether retrieval is necessary, evaluate the relevance of retrieved content, and dynamically determine the most suitable modalities for response generation. Achieving robust support for any-to-any modality is essential for adaptability in open-ended tasks \cite{wu24next}. 

Future multimodal RAGs should incorporate data from diverse real-world sources, such as environmental sensors, alongside traditional modalities like text and images, to enhance situational awareness.
This progression aligns with the trend toward embodied AI, where models integrate knowledge with physical interaction, enabling applications in robotics, navigation, and physics-informed reasoning.
Bridging retrieval-based reasoning with real-world agency brings these systems closer to AGI.

\subsection{Long-Context Processing, Efficiency, Scalability, and Personalization}
High computational costs in video frame sampling and memory bottlenecks in processing multi-page documents with images remain key challenges in long-context processing. Fixed extraction rates struggle to capture relevant frames, requiring adaptive selection based on content complexity and movement \cite{kandhare2024empiricalcomparisonvideoframe}. Additionally, retrieval speed-accuracy trade-offs in edge deployments and redundant computations in cross-modal fusion layers emphasize the need for efficient, scalable architectures. Personalization mechanisms, such as adapting retrieval to user-specific contexts like medical history, remain underexplored. As personalization mechanisms evolve, ensuring privacy and mitigating the risks of sensitive data leakage in multimodal outputs remain critical challenges. Lastly, the lack of datasets with complex reasoning tasks and multimodal adversarial examples limits robust evaluation.


\section{Conclusion}
\noindent
This study provides a comprehensive review of multimodal Retrieval-Augmented Generation (Multimodal RAG) literature. Specifically, we explore and categorize key advancements across different aspects of multimodal RAG systems, including retrieval, multimodal fusion, augmentation, generation, and training strategies. Additionally, we examine the tasks these systems address, their domain-specific applications, and the datasets, benchmarks, and evaluation methods. We also discuss open challenges and limitations in current approaches, along with promising future directions.
We hope this work inspires future research, particularly in enhancing cross-modal reasoning and retrieval, developing agent-based interactive systems, and advancing unified multimodal embedding spaces.


\section{Limitations}
\noindent
This study offers a comprehensive examination of multimodal RAG systems. Extended discussions, details of datasets and benchmarks, and additional relevant work are available in the Appendices. While we have made our maximum effort; however, some limits may persist. First, due to space constraints, our descriptions of individual methodologies are necessarily concise. Second, although we curate studies from major venues (e.g., ACL, EMNLP, NeurIPS, CVPR, ICLR, ICML, ACM Multimedia) and arXiv, our selection may inadvertently overlook emerging or domain-specific research, with a primary focus on recent advancements. Additionally, this work does not include a comparative performance evaluation of the various models, as task definitions, evaluation metrics, and implementation details vary significantly across studies, and executing these models requires substantial computational resources.

Furthermore, multimodal RAG is a rapidly evolving field with many open questions, such as optimizing fusion strategies for diverse modalities and addressing scalability challenges. As new paradigms emerge, our taxonomy and conclusions will inevitably evolve. To address these gaps, we plan to continuously monitor developments and update this survey and the corresponding repository to incorporate overlooked contributions and refine our perspectives.

\section{Ethical Statement}
\noindent
This survey provides a comprehensive review of research on multimodal RAG systems, offering insights that we believe will be valuable to researchers in this evolving field. All the studies, datasets, and benchmarks analyzed in this work are publicly available, with only a very small number of papers requiring institutional access. Additionally, this survey does not involve personal data or user interactions, and we adhere to ethical guidelines throughout.

Since this work is purely a survey of existing literature and does not introduce new models, datasets, or experimental methodologies, it presents no potential risks. However, we acknowledge that multimodal RAG systems inherently raise ethical concerns, including bias, misinformation, privacy, and intellectual property issues. Bias can emerge from both retrieval and generation processes, potentially leading to skewed or unfair outputs. Additionally, these models may hallucinate or propagate misinformation, particularly when retrieval mechanisms fail or rely on unreliable sources. The handling of sensitive multimodal data also poses privacy risks, while content generation raises concerns about proper attribution and copyright compliance. Addressing these challenges requires careful dataset curation, bias mitigation strategies, and transparent evaluation of retrieval and generation mechanisms.

\bibliography{acl_latex}

\newpage
\appendix

\input{applications}


\section{Taxonomy}
\label{sec:appendix}
\noindent
In this section, we provide more details regarding the taxonomy of multimodal RAG systems, previously mentioned in \autoref{fig:taxonomy_full}. Additionally, we present a classification of multimodal RAG application domains in \autoref{fig:app_taxonomy}.

\textbf{Figure~\ref{fig:taxonomy_full} }provides an overview of recent advances in multimodal retrieval-augmented generation (RAG) systems. The taxonomy is organized into several key categories.
\begin{itemize} \item \textbf{Retrieval strategies} cover efficient search and similarity retrieval methods (including maximum inner product search (MIPS) variants and different multi-modal encoders) and modality-centric techniques that distinguish between text-, vision-, and video-centric as well as document retrieval models. Re-ranking strategies further refine these methods via optimized example selection, relevance scoring, and filtering.
\item \textbf{Fusion mechanisms} are implemented through score fusion and alignment, attention-based techniques, and unified frameworks that project multimodal information into common representations. \item \textbf{Augmentation techniques} address context enrichment as well as adaptive and iterative retrieval. \item \textbf{Generation methods} include in-context learning, reasoning, instruction tuning, and source attribution. \item \textbf{training strategies} are characterized by approaches to alignment and robustness.
\end{itemize}
Detailed discussions of these categories are provided in the corresponding sections.

\noindent
\autoref{fig:app_taxonomy} presents the taxonomy of application domains for multimodal RAG systems. The identified domains include \emph{healthcare and medicine}, \emph{software engineering}, \emph{fashion and e-commerce}, \emph{entertainment and social computing}, and \emph{emerging applications}. This classification offers a concise overview of the diverse applications and serves as a framework for the more detailed analyses that follow.


\section{Dataset and Benchmark}
\label{sec:app_dataset}
\noindent
\autoref{tab:categorized_datasets} and ~\autoref{tab:benchmark_datasets} present a comprehensive overview of datasets and benchmarks commonly employed in multimodal RAG research. The table is organized into five columns:
\begin{itemize} \item \textbf{Category:} This column categorizes each dataset or benchmark based on its primary domain or modality. The datasets are grouped into eight categories: \emph{Image–Text General}, \emph{Video–Text}, \emph{Audio–Text}, \emph{Medical}, \emph{Fashion}, \emph{3D}, \emph{Knowledge \& QA}, and \emph{Other}. The benchmarks are grouped into two categories: \emph{Cross-Modal Understanding} and \emph{Text-Focused}.
This classification facilitates a clearer understanding of each dataset or benchmark’s role within a multimodal framework. \item \textbf{Name:} The official name of the dataset or benchmarks is provided along with a citation for reference. \item \textbf{Statistics and Description:} This column summarizes key details such as dataset size, the nature of the content (e.g., image–text pairs, video captions, QA pairs), and the specific tasks or applications for which the dataset or benchmarks are used. These descriptions are intended to convey the dataset’s scope and its relevance to various multimodal RAG tasks. \item \textbf{Modalities:} The modalities covered by each dataset or benchmark are indicated (e.g., Image, Text, Video, Audio, or 3D). Notably, several datasets are unimodal; however, within multimodal RAG systems, these are combined with others to represent distinct aspects of a broader multimodal context. \item \textbf{Link:} A hyperlink is provided to direct readers to the official repository or additional resources for the dataset or benchmark, thereby facilitating further exploration of its properties and applications. \end{itemize}


\begin{table*}[h]
\centering
\caption{Overview of Popular Datasets in Multimodal RAG Research.}
\label{tab:categorized_datasets}
\small
\renewcommand{\arraystretch}{1.5}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c c l c l}
\toprule
\rotatebox[origin=c]{90}{\textbf{Category}} & \textbf{Name} & \multicolumn{1}{c}{\textbf{Statistics and Description}} & \textbf{Modalities} & \multicolumn{1}{c}{\textbf{Link}} \\ \midrule
\multirow{10}{*}{\rotatebox[origin=c]{90}{Image-Text General}} 
 & LAION-400M \cite{laion400m} & 200M image–text pairs; used for pre-training multimodal models. & Image, Text & \href{https://laion.ai/projects/laion-400-mil-open-dataset/}{LAION-400M} \\
 & Conceptual-Captions (CC) \cite{Sharma2018ConceptualCA} & 15M image–caption pairs; multilingual English–German image descriptions. & Image, Text & \href{https://github.com/google-research-datasets/conceptual-captions}{Conceptual Captions} \\
 & CIRR \cite{Liu_2021_ICCV} & 36,554 triplets from 21,552 images; focuses on natural image relationships. & Image, Text & \href{https://github.com/Cuberick-Orion/CIRR}{CIRR} \\
 & MS-COCO \cite{lin2014microsoft} & 330K images with captions; used for caption–to–image and image–to–caption generation. & Image, Text & \href{https://cocodataset.org/}{MS-COCO} \\
 & Flickr30K \cite{young2014image} & 31K images annotated with five English captions per image. & Image, Text & \href{https://shannon.cs.illinois.edu/DenotationGraph/}{Flickr30K} \\
 & Multi30K \cite{elliott2016multi30k} & 30k German captions from native speakers and human–translated captions. & Image, Text & \href{https://github.com/multi30k/dataset}{Multi30K} \\
 & NoCaps \cite{nocaps} & For zero–shot image captioning evaluation; 15K images. & Image, Text & \href{https://nocaps.org/}{NoCaps} \\
 & Laion-5B \cite{schuhmann2022laion} & 5B image–text pairs used as external memory for retrieval. & Image, Text & \href{https://laion.ai/blog/laion-5b/}{LAION-5B} \\
 & COCO-CN \cite{cococn} & 20,341 images for cross-lingual tagging and captioning with Chinese sentences. & Image, Text & \href{https://github.com/li-xirong/coco-cn}{COCO-CN} \\
 & CIRCO \cite{circo} & 1,020 queries with an average of 4.53 ground truths per query; for composed image retrieval. & Image, Text & \href{https://github.com/miccunifi/CIRCO}{CIRCO} \\ \midrule
\multirow{22}{*}{\rotatebox[origin=c]{90}{Video-Text}} 
 & BDD-X \cite{bddx} & 77 hours of driving videos with expert textual explanations; for explainable driving behavior. & Video, Text & \href{https://github.com/JinkyuKimUCB/BDD-X-dataset}{BDD-X} \\
 & YouCook2 \cite{zhou2018youcook2} & 2,000 cooking videos with aligned descriptions; focused on video–text tasks. & Video, Text & \href{https://youcook2.eecs.umich.edu/}{YouCook2} \\
 & ActivityNet \cite{caba2015activitynet} & 20,000 videos with multiple captions; used for video understanding and captioning. & Video, Text & \href{http://activity-net.org/}{ActivityNet} \\
 & SoccerNet \cite{giancola2018soccernet} & Videos and metadata for 550 soccer games; includes transcribed commentary and key event annotations. & Video, Text & \href{https://www.soccer-net.org/}{SoccerNet} \\
 & MSR-VTT \cite{xu2016msrvtt} & 10,000 videos with 20 captions each; a large video description dataset. & Video, Text & \href{https://ms-multimedia-challenge.com/2016/dataset}{MSR-VTT} \\
 & MSVD \cite{chen2011collecting} & 1,970 videos with approximately 40 captions per video. & Video, Text & \href{https://www.cs.utexas.edu/~ml/clamp/videoDescription/}{MSVD} \\
 & LSMDC \cite{rohrbach2015lsmdc} & 118,081 video–text pairs from 202 movies; a movie description dataset. & Video, Text & \href{https://sites.google.com/site/describingmovies/}{LSMDC} \\
 & DiDemo \cite{Hendricks_2017_ICCV} & 10,000 videos with four concatenated captions per video; with temporal localization of events. & Video, Text & \href{https://github.com/LisaAnne/TemporalLanguageRelease}{DiDemo} \\
 & Breakfast \cite{Kuehne_2014_CVPR} & 1,712 videos of breakfast preparation; one of the largest fully annotated video datasets. & Video, Text & \href{https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/}{Breakfast} \\
 & COIN \cite{coin} & 11,827 instructional YouTube videos across 180 tasks; for comprehensive instructional video analysis. & Video, Text & \href{https://coin-dataset.github.io/}{COIN} \\
 & MSRVTT-QA \cite{Xu2017VideoQA} & Video question answering benchmark. & Video, Text & \href{https://github.com/xudejing/video-question-answering}{MSRVTT-QA} \\
 & MSVD-QA \cite{Xu2017VideoQA} & 1,970 video clips with approximately 50.5K QA pairs; video QA dataset. & Video, Text & \href{https://github.com/xudejing/video-question-answering}{MSVD-QA} \\
 & ActivityNet-QA \cite{yu2019activitynet} & 58,000 human–annotated QA pairs on 5,800 videos; benchmark for video QA models. & Video, Text & \href{https://github.com/MILVLG/activitynet-qa}{ActivityNet-QA} \\
 & EpicKitchens-100 \cite{dima2020rescaling} & 700 videos (100 hours of cooking activities) for online action prediction; egocentric vision dataset. & Video, Text & \href{https://epic-kitchens.github.io/2021/}{EPIC-KITCHENS-100} \\
 & Ego4D \cite{grauman2022ego4d} & 4.3M video–text pairs for egocentric videos; massive–scale egocentric video dataset. & Video, Text & \href{https://ego4d-data.org/}{Ego4D} \\
 & HowTo100M \cite{Miech_2019_ICCV} & 136M video clips with captions from 1.2M YouTube videos; for learning text–video embeddings. & Video, Text & \href{https://www.di.ens.fr/willow/research/howto100m/}{HowTo100M} \\
 & CharadesEgo \cite{charadesego} & 68,536 activity instances from ego–exo videos; used for evaluation. & Video, Text & \href{https://prior.allenai.org/projects/charades-ego}{Charades-Ego} \\
 & ActivityNet Captions \cite{Krishna_2017_ICCV} & 20K videos with 3.7 temporally localized sentences per video; dense–captioning events in videos. & Video, Text & \href{https://cs.stanford.edu/people/ranjaykrishna/densevid/}{ActivityNet Captions} \\
 & VATEX \cite{wang2019vatex} & 34,991 videos, each with multiple captions; a multilingual video–and–language dataset. & Video, Text & \href{https://eric-xw.github.io/vatex-website/}{VATEX} \\
 & Charades \cite{sigurdsson2016charades} & 9,848 video clips with textual descriptions; a multimodal research dataset. & Video, Text & \href{https://allenai.org/plato/charades/}{Charades} \\
 & WebVid \cite{bain2021frozen} & 10M video–text pairs (refined to WebVid-Refined-1M). & Video, Text & \href{https://github.com/m-bain/webvid}{WebVid} \\
 & Youku-mPLUG \cite{xu2023youkumplug10millionlargescale} & Chinese dataset with 10M video–text pairs (refined to Youku-Refined-1M). & Video, Text & \href{https://github.com/X-PLUG/Youku-mPLUG}{Youku-mPLUG} \\\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{Audio-Text}} 
 & LibriSpeech \cite{panayotov2015librispeech} & 1,000 hours of read English speech with corresponding text; ASR corpus based on audiobooks. & Audio, Text & \href{https://www.openslr.org/12}{LibriSpeech} \\
 & SpeechBrown \cite{abootorabi2024claspcontrastivelanguagespeechpretraining} & 55K paired speech-text samples; 15 categories covering diverse topics from religion to fiction. & Audio, Text & \href{https://huggingface.co/datasets/llm-lab/SpeechBrown}{SpeechBrown} \\
 & AudioCap \cite{kim-etal-2019-audiocaps} & 46K audio clips paired with human-written text captions. & Audio, Text & \href{https://audiocaps.github.io/}{AudioCaps} \\
 & AudioSet \cite{gemmeke2017audioset} & 2,084,320 human–labeled 10–second sound clips from YouTube; 632 audio event classes. & Audio, Text & \href{https://research.google.com/audioset/}{AudioSet} \\ \midrule
\multirow{5}{*}{\rotatebox[origin=c]{90}{Medical}} 
 & MIMIC-CXR \cite{johnson2019mimic} & 125,417 training pairs of chest X–rays and reports. & Image, Text & \href{https://physionet.org/content/mimic-cxr/2.0.0/}{MIMIC-CXR} \\
 & CheXpert \cite{irvin2019chexpert} & 224,316 chest radiographs of 65,240 patients; focused on medical analysis. & Image, Text & \href{https://stanfordmlgroup.github.io/competitions/chexpert/}{CheXpert} \\
 & MIMIC-III \cite{johnson2016mimic} & Health-related data from over 40K patients (text data). & Text & \href{https://mimic.physionet.org/}{MIMIC-III} \\
 & IU-Xray \cite{pavlopoulos-etal-2019-survey} & 7,470 pairs of chest X–rays and corresponding diagnostic reports. & Image, Text & \href{https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university}{IU X-ray} \\
 & PubLayNet \cite{zhong2019publaynetlargestdatasetdocument} & 100,000 training samples and 2,160 test samples built from PubLayNet (tailored for the medical domain). & Image, Text & \href{https://github.com/ibm-aur-nlp/PubLayNet}{PubLayNet} \\ \midrule
\multirow{5}{*}{\rotatebox[origin=c]{90}{Fashion}} 
 & Fashion-IQ \cite{wu2019fashioniq} & 77,684 images across three categories; evaluated with Recall@10 and Recall@50. & Image, Text & \href{https://github.com/XiaoxiaoGuo/fashion-iq}{Fashion IQ} \\
 & FashionGen \cite{fashiongen} & 260.5K image–text pairs of fashion images and item descriptions. & Image, Text & \href{https://www.elementai.com/datasets/fashiongen}{Fashion-Gen} \\
 & VITON-HD \cite{vitonhd} & 83K images for virtual try–on; high–resolution clothing items. & Image, Text & \href{https://github.com/shadow2496/VITON-HD}{VITON-HD} \\
 & Fashionpedia \cite{fashionpedia} & 48,000 fashion images annotated with segmentation masks and fine-grained attributes. & Image, Text & \href{https://fashionpedia.ai/}{Fashionpedia} \\
 & DeepFashion \cite{Liu_2016_CVPR} & Approximately 800K diverse fashion images for pseudo triplet generation. & Image, Text & \href{https://github.com/zalandoresearch/fashion-mnist}{DeepFashion} \\ \midrule
\multirow{1}{*}{\rotatebox[origin=c]{90}{3D}} 
 & ShapeNet \cite{chang2015shapenet} & 7,500 text–3D data pairs; repository for 3D CAD models. & Text, 3D & \href{https://shapenet.org/}{ShapeNet} \\ \midrule
\multirow{15}{*}{\rotatebox[origin=c]{90}{Knowledge \& QA}} 
 & VQA \cite{antol2015vqa} & 400K QA pairs with images for visual question answering. & Image, Text & \href{https://visualqa.org/}{VQA} \\
 & PAQ \cite{lewis2021paq} & 65M text–based QA pairs; a large–scale dataset. & Text & \href{https://github.com/facebookresearch/PAQ}{PAQ} \\
 & ELI5 \cite{fan2019eli5} & 270K complex and diverse questions augmented with web pages and images. & Text & \href{https://facebookresearch.github.io/ELI5/}{ELI5} \\
 & ViQuAE \cite{viquae} & 11.8M passages from Wikipedia covering 2,397 unique entities; knowledge–intensive QA. & Text & \href{https://github.com/PaulLerner/ViQuAE}{ViQuAE} \\
 & OK-VQA \cite{marino2019okvqa} & 14K questions requiring external knowledge for VQA. & Image, Text & \href{https://okvqa.allenai.org/}{OK-VQA} \\
 & WebQA \cite{webqa} & 46K queries that require reasoning across text and images. & Text, Image & \href{https://webqna.github.io/}{WebQA} \\
 & Infoseek \cite{infoseek} & Fine-grained visual knowledge retrieval using a Wikipedia–based knowledge base (~6M passages). & Image, Text & \href{https://open-vision-language.github.io/infoseek/}{Infoseek} \\
 & ClueWeb22 \cite{clueweb22} & 10 billion web pages organized into three subsets; a large–scale web corpus. & Text & \href{https://lemurproject.org/clueweb22/}{ClueWeb22} \\
 & MOCHEG \cite{yao2023end} & 15,601 claims annotated with truthfulness labels and accompanied by textual and image evidence. & Text, Image & \href{https://github.com/VT-NLP/Mocheg}{MOCHEG} \\
 & VQA v2 \cite{goyal2017vqa} & 1.1M questions (augmented with VG–QA questions) for fine-tuning VQA models. & Image, Text & \href{https://visualqa.org/}{VQA v2} \\
 & A-OKVQA \cite{schwenk2022aokvqabenchmarkvisualquestion} & Benchmark for visual question answering using world knowledge; around 25K questions. & Image, Text & \href{https://github.com/allenai/aokvqa}{A-OKVQA} \\
 & XL-HeadTags \cite{shohan2024xlheadtagsleveragingmultimodalretrieval} & 415K news headline-article pairs consist of 20 languages across six diverse language families. & Text & \href{https://huggingface.co/datasets/faisaltareque/XL-HeadTags}{XL-HeadTags} \\
 & SEED-Bench \cite{li2023seedbenchbenchmarkingmultimodalllms} & 19K multiple–choice questions with accurate human annotations across 12 evaluation dimensions. & Text & \href{https://github.com/AILab-CVC/SEED-Bench}{SEED-Bench} \\ \midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{Other}} 
 & ImageNet \cite{deng2009imagenet} & 14,197,122 images for perspective understanding; a hierarchical image database. & Image & \href{http://www.image-net.org/}{ImageNet} \\
 & Oxford Flowers102 \cite{nilsback2008automated} & 102 flower categories with five examples per category; image classification dataset. & Image & \href{https://www.robots.ox.ac.uk/~vgg/data/flowers/102/}{Oxford Flowers102} \\
 & Stanford Cars \cite{krause20133d} & Images of different car models (five examples per model); for fine-grained categorization. & Image & \href{https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset}{Stanford Cars} \\
 & GeoDE \cite{geode} & 61,940 images from 40 classes across 6 world regions; emphasizes geographic diversity in object recognition. & Image & \href{https://github.com/AliRamazani/GeoDE}{GeoDE} \\ \midrule
\end{tabular}%
}
\end{table*}

% add benchmark table



\begin{table*}[t]
\centering
\caption{Overview of Popular Benchmarks in Multimodal RAG Research.}
\label{tab:benchmark_datasets}
\large
\renewcommand{\arraystretch}{3.8}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c c l c l}
\toprule
\rotatebox[origin=c]{90}{\textbf{Category}} & \textbf{Name} & \multicolumn{1}{c}{\textbf{Statistics and Description}} & \textbf{Modalities} & \multicolumn{1}{c}{\textbf{Link}} \\ \midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{Cross-Modal Understanding}} 
 & MRAG-Bench \cite{hu2024mragbench} & Evaluates visual retrieval, integration, and robustness to irrelevant visual information. & Images & \href{https://mragbench.github.io/}{MRAG-Bench} \\
 & M2RAG \cite{ma2024multimodalretrievalaugmentedmultimodal} & Benchmarks multimodal RAG; evaluates retrieval, multi-hop reasoning, and integration. & Images + Text & \href{https://arxiv.org/abs/2411.16365}{M2RAG} \\ 
 & Dyn-VQA \cite{li2024benchmarking} & Focuses on dynamic retrieval, multi-hop reasoning, and robustness to changing information. & Images + Text & \href{https://openreview.net/forum?id=VvDEuyVXkG}{Dyn-VQA} \\ 
 & MMBench \cite{liu2025mmbench} & Covers VQA, captioning, retrieval; evaluates cross-modal understanding across vision, text, and audio. & Images + Text + Audio & \href{https://github.com/open-compass/MMBench}{MMBench} \\ 
 & ScienceQA \cite{saikh2022scienceqa} & Contains 21,208 questions; tests scientific reasoning with text, diagrams, and images. & Images + Diagrams + Text & \href{https://scienceqa.github.io/}{ScienceQA} \\ 
 & SK-VQA \cite{su2024sk} & Offers 2 million question-answer pairs; focuses on synthetic knowledge, multimodal reasoning, and external knowledge integration. & Images + Text & \href{https://arxiv.org/abs/2406.19593}{SK-VQA} \\ 
 & SMMQG \cite{wu-etal-2024-synthetic} & Includes 1,024 question-answer pairs; focuses on synthetic multimodal data and controlled question generation. & Images + Text & \href{https://arxiv.org/abs/2407.02233}{SMMQG} \\ \midrule
\multirow{2}{*}{\rotatebox[origin=c]{90}{Text-Focused}} 
 & TriviaQA \cite{joshi2017triviaqa} & Provides 650K question-answer pairs; reading comprehension dataset, adaptable for multimodal RAG. & Text & \href{https://nlp.cs.washington.edu/triviaqa/}{TriviaQA} \\
 & Natural Questions \cite{kwiatkowski2019natural} & Contains 307,373 training examples; real-world search queries, adaptable with visual contexts. & Text & \href{https://paperswithcode.com/dataset/natural-questions}{Natural Questions} \\ \midrule
\end{tabular}%
}
\end{table*}

\section{Metrics}
\label{sec:app_metrics}
\noindent
\textbf{Accuracy}: 
Accuracy is typically defined as the ratio of correctly predicted instances to the total instances.
In retrieval-based tasks, Top-K Accuracy is defined as:
% $$\text{Top-K Accuracy}(y, \hat{f}) = \frac{1}{n} \sum_{i=0}^{n-1} \sum_{j=1}^{k} 1(\hat{f}_{i,j} = y_i)$$
\begin{align}
\text{Top-K Accuracy}(y, \hat{f}) &= \frac{1}{n} \sum_{i=0}^{n-1} \sum_{j=1}^{k} \mathbb{1}(\hat{f}_{i,j} = y_i)
\end{align}

\noindent
\textbf{FID (Fréchet inception distance)}: FID is a metric used to assess the quality of images created by a generative model. The formula for FID is:

\begin{align}
\text{FID} &= \|\mu_r - \mu_g\|^2 + tr(\Sigma_r + \Sigma_g - 2\sqrt{\Sigma_r \Sigma_g}) 
\end{align}
\noindent
where $\mu_r$ and $\Sigma_r$ are the mean and covariance of real images' feature representations, respectively. $\mu_g$ and $\Sigma_g$ are the mean and covariance of generated images' feature representations, respectively. To extract features, InceptionV3 is typically used. \\


\noindent
\textbf{ROUGE-N (N-gram Recall)}: The ROUGE metric is commonly used to evaluate text summarization and generation. 
ROUGE-N measures the overlap of N-grams between the generated and reference text. The formula for ROUGE-N is:

\begin{align}
\text{ROUGE-N} &= \frac{\sum_{\text{gram}_N \in \text{Ref}} \text{Count}_{\text{match}}(\text{gram}_N)}{\sum_{\text{gram}_N \in \text{Ref}} \text{Count}(\text{gram}_N)}
\end{align}

\noindent
ROUGE-L measures the longest common subsequence (LCS) between generated and reference text. The formula for ROUGE-L is:
\begin{align}
\text{ROUGE-L} &= \frac{LCS(X, Y)}{|Y|}
\end{align}
\textbf{BLEU}: BLEU is another metric used for assessing text generation. The formula for calculating BLEU is:

\begin{align}
\text{BLEU}(p_n, \text{BP}) = \text{BP} \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right)
\end{align}

\noindent
Here, $p_n$ represents the precision of n-grams, $w_n$ denotes the weight assigned to the n-gram precision, and the Brevity Penalty (BP) is defined as:

\begin{align}
\text{BP} = 
\begin{cases}
1 & \text{length} > \text{$rl$} \\
\exp\left( 1 - \frac{\text{$rl$}}{\text{$cl$}} \right) & \text{length} \leq \text{$rl$}
\end{cases}
\end{align}

\noindent
Here, $rl$ represents the reference length and $cl$ represents the candidate length.
\\
% \begin{align}
% p_n = \frac{\text{Count of n-grams in the candidate translation that appear in the references}}{\text{Total number of n-grams in the candidate translation}}
% \end{align}


\noindent
\textbf{BERTScore}: BERTScore is a metric for evaluating the quality of text generation, based on the similarity between the contextual embeddings of words in the candidate and reference texts. The formula for calculating BERTScore is:

\begin{align}
\text{BERTScore}(c, r) = \frac{1}{|c|} \sum_{i=1}^{|c|} \max_{j=1}^{|r|} \cos(\mathbf{e}_i, \mathbf{e}_j)
\end{align}

\noindent
$c$ is the candidate sentence, and $r$ is the reference sentence. $e_i$ and $e_j$ are the embeddings (e.g., from BERT) for words $c_i$ and $r_j$ in the candidate and reference sentences, respectively. \\


\noindent
\textbf{CLIPScore}: CLIPScore is a metric that evaluates the similarity between the text and an image by using the CLIP model. The formula for calculating CLIPScore is:

\begin{align}
\text{CLIPScore} = \frac{\cos(\mathbf{t}, \mathbf{i})}{\| \mathbf{t} \| \cdot \| \mathbf{i} \|}
\end{align}

\noindent
where t and i are text and image embedding respectively. \\

\noindent
\textbf{Mean Reciprocal Rank (MRR)}: MRR is a metric used to evaluate the performance of systems that return a list of results, such as search engines or recommendation systems. MRR measures the rank position of the first relevant result in the returned list. The formula for calculating MRR is:

\begin{align}
\text{MRR} = \frac{1}{Q} \sum_{q=1}^{Q} \frac{1}{\text{rank}_q}
\end{align}

\noindent
where $Q$ is the total number of queries.
$rank_q$ is the rank of the first relevant result for query $q$.

\section{Loss Function}
\label{sec:app_loss}
\noindent
\textbf{InfoNCE (Information Noise Contrastive Estimation)}: The InfoNCE loss is commonly used in self-supervised learning, especially in contrastive learning methods. The formula for InfoNCE loss is:
\begin{align}
\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{K} \exp(\text{sim}(z_i, z_k)/\tau)}
\end{align}
where $z_i$ and $z_j$ are the embeddings of a positive pair and $\tau$ is a temperature parameter. \\

\noindent
\textbf{GAN (Generative Adversarial Network)}: The GAN loss consists of two parts: the discriminator loss and the generator loss.
The discriminator loss formula is:
\begin{align}
\scriptstyle \mathcal{L}_D = - \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] - \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
\end{align}
where $x$ is a real sample from the data distribution.
$G(z)$ is the generated sample from the generator, where 
$z$ is a noise vector.
$D(x)$ is the probability that $x$ is real.

\noindent
The Generator loss formula is:
\begin{align}
% \mathcal{L}_G = - \mathbb{E}_{z \sim p_z(z)}[\log D(G(z))]
\mathcal{L}_G = \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
\end{align}

\noindent
\textbf{Triplet Loss}: Triplet Loss is used in metric learning to ensure that similar data points are closer together while dissimilar ones are farther apart in the embedding space. The Triplet loss formula is:

\begin{align}
\scriptstyle \mathcal{L} = \sum_{i=1}^{N} \max(0, \| f(x_a^i) - f(x_p^i) \|^2 - \| f(x_a^i) - f(x_n^i) \|^2 + \alpha)
\end{align}

\noindent
where $x_a^i$ is the anchor sample. $x_p^i$ and $x_n^i$ are the positive and negative samples respectively.
$f(x)$ is the neural network.

\newpage


\end{document}
