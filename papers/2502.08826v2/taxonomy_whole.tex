% \clearpage
% \thispagestyle{empty}

\begin{figure*}[t!]
    % \vspace{-1cm}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                child anchor=west,
                parent anchor=east,
                grow'=east,
                anchor=west,
                base=left,
                font=\normalsize,
                rectangle,
                draw=hidden-black,
                rounded corners,
                minimum height=2em,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                % inner xsep=2pt,
                % inner ysep=3pt,
                inner xsep=0.4em,
                inner ysep=0.6em,
                line width=0.8pt,
                text width=8.5em,
                where level=1{ % Target second layer nodes
                text width=5em,
                inner xsep=0.3em,
                inner ysep=0.5em,
                font=\small,
                }{},
                where level=2{ % Apply to all subsequent layers
                text width=10em,
                }{},
                where level=3{ % Apply to all subsequent layers
                text width=7em,
                }{},
                ver/.style={
                    fill=white!50, % Optional: Give them a light background color
                    rotate=90,
                    child anchor=north,
                    parent anchor=south,
                    anchor=center,
                    text width=8em
                },
                leaf/.style={
                    text opacity=1,
                    inner sep=2pt,
                    fill opacity=.5,
                    % line width=0.8pt,
                    % minimum height=1.5em,
                    fill=green!20, 
                    text=black,
                    % align=left,
                    text width=44.5em
                    font=\normalsize,
                    % inner xsep=2pt,
                    % inner ysep=4pt,
                    inner xsep=0.4em,
                    inner ysep=0.6em,
                    draw,
                    % line width=0.8pt,
                }, 
            },
            % where level=1{text width=7em}{},
            % where level=2{text width=8.5em}{},
            % where level=3{text width=8.5em}{},
            % where level=4{text width=12em}{},
            [Multimodal RAG, ver
                [
                    Retrieval \\ Strategies\\~(\S\ref{sec_retrieval_strategies}), fill=yellow!20
                    [
                        Efficient-Search and \\ Similarity Retrieval \\~(\S\ref{sec_similarity_search}), fill=red!15
                        [
                            Maximum Inner Product Search (MIPS), fill=blue!15
                            [   
                                TPU-KNN~\cite{chern2022tpuknnknearestneighbor}{,} ScaNN~\cite{guo2020acceleratinglargescaleinferenceanisotropic}{,}
                                MAXSIM score~\cite{chan2008maxsim}{,}
                                ADQ~\cite{li2024adaptivedatasetquantization}{,}
                                \citet{Zhang_Lian_Zhang_Wang_Chen_2023}{,}
                                BanditMIPS~\cite{tiwari2024faster}{,} MUST~\cite{wang2023musteffectivescalableframework}{,}
                                FARGO~\cite{10.14778/3579075.3579084}{,}
                                MuRAG~\cite{chen-etal-2022-murag}{,}
                                RA-CM3~\cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling}{,}
                                \citet{nguyen2024multimodallearnedsparseretrieval}{,}
                                Graph-based ANNs~\cite{zhang2024efficienteffectiveretrievaldensesparse}{,}
                                \citet{10.1145/3580305.3599897}{,} Deeperimpact~\cite{basnet2024deeperimpactoptimizingsparselearned}{,} RetrievalAttention~\cite{liu2024retrievalattentionacceleratinglongcontextllm}{,}
                                FactMM-RAG~\cite{sun2024factawaremultimodalretrievalaugmentation}
                                , leaf, text width=53em
                            ]
                        ]
                        [
                            Multimodal \\ Encoders, fill=blue!15
                            [
                                CLIP~\cite{radford2021learning}{,}
                                BLIP~\cite{li2022blip}{,}  MARVEL~\cite{zhou2024marvelunlockingmultimodalcapability}{,}
                                ALIGN~\cite{pmlr-v139-jia21b}{,}
                                FLAVA~\cite{singh2022flava}{,}
                                UniVL-DR~\cite{liu2023universal}{,}
                                UniIR~\cite{wei2023uniirtrainingbenchmarkinguniversal}{,} GME~\cite{zhang2024gmeimprovinguniversalmultimodal}{,}
                                VISTA~\cite{zhou-etal-2024-vista}{,} ColPali~\cite{faysse2024colpaliefficientdocumentretrieval}{,} InternVideo~\cite{wang2022internvideogeneralvideofoundation}{,} Ovis~\cite{lu2024ovisstructuralembeddingalignment}
                                Mi-RAG~\cite{omar-etal-2024-multi}
                                , leaf, text width=53em
                            ]
                        ]
                    ]
                    [
                        Modality-Centric \\ Retrieval~(\S\ref{sec_modality_based}), fill=red!15
                        [
                            Text-Centric, fill=blue!15
                            [
                                Contriever~\cite{izacard2022unsuperviseddenseinformationretrieval}{,} GTE~\cite{li2023generaltextembeddingsmultistage}{,}
                                Re-Imagen~\cite{chen2022reimagenretrievalaugmentedtexttoimagegenerator}{,}
                                BM25~\cite{INR-019}{,}
                                MiniLM~\cite{wang2020minilmdeepselfattentiondistillation}{,}
                                BGE-M3~\cite{chen2024bgem3embeddingmultilingualmultifunctionality}{,} CapRet~\cite{shohan2024xlheadtagsleveragingmultimodalretrieval}{,} 
                                OMG-QA~\cite{nan-etal-2024-omg}{,}
                                ColBERT~\cite{10.1145/3397271.3401075}{,} PreFLMR~\cite{lin-etal-2024-preflmr}{,}
                                RAFT~\cite{zhang2024raft}{,}
                                CRAG~\cite{yan2024corrective}{,}
                                M2-RAG~\cite{ma2024multimodalretrievalaugmentedmultimodal}
                                , leaf, text width=53em
                            ]
                        ]
                        [
                            Vision-Centric, fill=blue!15
                            [
                                VQA4CIR~\cite{feng2023vqa4cirboostingcomposedimage}{,}
                                Unifashion~\cite{zhao-etal-2024-unifashion}{,}
                                Jang et al.~\cite{jang2024visual}{,} Pic2word~\cite{saito2023pic2wordmappingpictureswords}{,} eClip~\cite{kumar2024improvingmedicalmultimodalcontrastive}{,} RAMM~\cite{yuan2023rammretrievalaugmentedbiomedicalvisual}{,}
                                Joshi et al.~\cite{joshi2024robust}{,} VISA~\cite{ma2024visaretrievalaugmentedgeneration}{,} ImgRet~\cite{shohan2024xlheadtagsleveragingmultimodalretrieval}{,}
                                EchoSight~\cite{Yan_2024}{,}
                                Xue et al.~\cite{xue2024enhancedmultimodalragllmaccurate}
                                , leaf, text width=53em
                            ]
                        ]
                        [
                            Video-Centric, fill=blue!15
                            [
                                iRAG~\cite{Arefeen_2024}{,} VideoRAG~\cite{ren2025videoragretrievalaugmentedgenerationextreme}{,} VideoRAG~\cite{jeong2025videoragretrievalaugmentedgenerationvideo}{,}
                                T-Mass~\cite{wang2024textmassmodelingstochastic}{,}
                                MV-Adapter~\cite{jin2024mv}{,} OmAgent~\cite{zhang-etal-2024-omagent}{,}
                                CM2~\cite{kim2024you}{,}
                                Video-RAG~\cite{luo2024videoragvisuallyalignedretrievalaugmentedlong}{,}
                                CTCH~\cite{ijcai2024p136}{,}
                                RTime~\cite{10.1145/3664647.3680731}{,} VideoMAE~\cite{tong2022videomaemaskedautoencodersdataefficient}{,}  DrVideo~\cite{ma2024drvideodocumentretrievalbased}
                                , leaf, text width=53em
                            ]
                        ]
                            [
                            Document Retrieval, fill=blue!15
                            [ 
                                ColPali~\cite{faysse2024colpaliefficientdocumentretrieval}{,}
                                ColQwen2~\cite{Qwen2VL}{,} M3DocVQA~\cite{cho2024m3docragmultimodalretrievalneed}{,}
                                ViTLP~\cite{mao-etal-2024-visually}{,}
                                DocLLM~\cite{wang-etal-2024-docllm}{,}
                                CREAM~\cite{10.1145/3664647.3680750}{,}
                                mPLUG-DocOwl 1.5~\cite{hu-etal-2024-mplug}{,}
                                mPLUG-DocOwl 2~\cite{hu2024mplugdocowl2highresolutioncompressingocrfree}{,} VisDom~\cite{suri2024visdommultidocumentqavisually}{,} DSE~\cite{ma2024unifyingmultimodalretrievaldocument}
                                , leaf, text width=53em
                            ]
                        ]
                    ]
                    [
                        Re-ranking \\ Strategies~(\S\ref{sec_reranking_strategies}), fill=red!15
                        [
                            Optimized Example Selection, fill=blue!15
                            [ 
                            MSIER~\cite{luo2024doestextualinformationaffect}{,}
                                Hybrid RAG~\cite{su2024hybrid}{,}
                                RULE~\cite{xia-etal-2024-rule}{,} RAMM~\cite{yuan2023rammretrievalaugmentedbiomedicalvisual}{,} M2RAAP~\cite{10.1145/3626772.3657833}
                                , leaf, text width=53em
                            ]
                        ]
                        [
                            Relevance Score \\ Evaluation, fill=blue!15
                            [
                                RAG-Check~\cite{mortaheb2025ragcheckevaluatingmultimodalretrieval, mortaheb2025rerankingcontextmultimodalretrieval}{,}
                                UniRaG~\cite{10535103}{,}        MR2AG~\cite{zhang2024mr2agmultimodalretrievalreflectionaugmentedgeneration}{,}
                                LDRE~\cite{10.1145/3626772.3657740}{,} BM25~\cite{INR-019}{,} RAGTrans~\cite{10.1145/3637528.3672041}{,} OMG-QA~\cite{nan-etal-2024-omg}{,} EchoSight~\cite{Yan_2024}{,} EgoInstructor~\cite{xu2024retrievalaugmentedegocentricvideocaptioning}
                                , leaf, text width=53em
                            ]
                        ]
                        [
                            Filtering \\ Mechanisms, fill=blue!15
                            [
                                MAIN-RAG~\cite{chang2024mainragmultiagentfilteringretrievalaugmented}{,}
                                MM-Embed~\cite{lin2024mmembeduniversalmultimodalretrieval}{,} GME~\cite{zhang2024gmeimprovinguniversalmultimodal}{,}
                                MuRAR~\cite{zhu2024murarsimpleeffectivemultimodal}
                                RAFT~\cite{zhang2024raft}
                                , leaf, text width=53em
                            ]
                        ]
                    ]
                ]
                [
                    Fusion \\ Mechanisms\\~(\S\ref{sec_fusion_mechanisms}), fill=yellow!20
                    [
                        Score Fusion \\ and Alignment\\~(\S\ref{sec_socre_alignment}), fill=red!15
                        [
                            M3~\cite{cai2025matryoshka}
                            ~\citet{10535103}{,} ~\citet{Sharifymoghaddam2024UniRAGUR}{,} REVEAL~\cite{Hu_2023_CVPR}{,}
                            RAG-Driver~\cite{yuan2024ragdrivergeneralisabledrivingexplanations}{,}
                            C3Net~\cite{zhang2024c3net}{,}
                            LLM-RA~\cite{jian-etal-2024-large}{,}
                            ~\citet{riedler2024textoptimizingragmultimodal}{,} VISA~\cite{ma2024visaretrievalaugmentedgeneration}{,}
                            MA-LMM~\cite{he2024ma}{,}
                            ~\citet{xue2024enhancedmultimodalragllmaccurate}{,}
                            RA-BLIP~\cite{ding2024rablipmultimodaladaptiveretrievalaugmented}{,}
                            Re-IMAGEN~\cite{chen2022reimagenretrievalaugmentedtexttoimagegenerator}{,} MegaPairs~\cite{zhou2024megapairsmassivedatasynthesis}{,}
                            Wiki-LLaVA~\cite{caffagni2024wiki}{,} VISRAG~\cite{yu2024visragvisionbasedretrievalaugmentedgeneration}
                            , leaf, text width=62.0em
                        ]
                    ]
                    [
                        Attention-Based \\ Mechanisms~(\S\ref{sec_attention_based}), fill=red!15
                        [
                            RAMM~\cite{yuan2023rammretrievalaugmentedbiomedicalvisual}{,} EMERGE~\cite{zhu2024emergeintegratingragimproved}{,} MORE~\cite{cui2024moremultimodalretrievalaugmented}{,} RAGTrans~\cite{10.1145/3637528.3672041}{,} AlzheimerRAG~\cite{lahiri2024alzheimerragmultimodalretrievalaugmented}{,}
                            MV-Adapter~\cite{jin2024mv}{,} ~\citet{xu2024retrievalaugmentedegocentricvideocaptioning}{,}
                            ~\citet{kim2024you}{,}
                            M2-RAAP~\cite{10.1145/3626772.3657833}{,}
                            Mu-RAG~\cite{chen-etal-2022-murag}{,}
                            Ou et al.~\cite{10.1007/s00530-024-01649-6}{,}  CADMR~\cite{khalafaoui2024cadmrcrossattentiondisentangledlearning}  
                            , leaf, text width=62.0em
                        ]
                    ]
                    [
                        Unified Frameworks \\and Projections~(\S\ref{sec_unified_frameworks_projections}), fill=red!15
                        [
                            Hybrid-RAG~\cite{su2024hybrid}{,} Dense2Sparse~\cite{nguyen2024multimodallearnedsparseretrieval}{,}
                            IRAMIG~\cite{Liu_2024}{,} M3DocRAG~\cite{cho2024m3docragmultimodalretrievalneed}{,}
                            DQU-CIR~\cite{Wen_2024}{,}
                            PDF-MVQA~\cite{ding2024pdfmvqadatasetmultimodalinformation}{,}
                            SAM-RAG~\cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration}{,} UFineBench~\cite{zuo2024ufinebench}{,}
                            ~\citet{li2022blip}
                            , leaf, text width=62.0em
                        ]
                    ]
                ]
                [
                    Augmentation \\ Techniques\\~(\S\ref{sec_augmentation}), fill=yellow!20
                    [
                        Context-Enrichment~(\S\ref{sec_context_enrichment}), fill=red!15
                        [
                            EMERGE~\cite{zhu2024emergeintegratingragimproved}{,}
                            MiRAG~\cite{omar-etal-2024-multi}{,}
                            Wiki-LLaVA~\cite{caffagni2024wiki}{,}
                            Video-RAG~\cite{luo2024videoragvisuallyalignedretrievalaugmentedlong}{,}
                            Img2Loc~\cite{zhou2024img2loc}{,}
                            ~\citet{xue2024enhancedmultimodalragllmaccurate}
                            , leaf, text width=62.0em
                        ]
                    ]
                    [
                        Adaptive and \\ Iterative \\ Retrieval~(\S\ref{sec_adaptive}), fill=red!15
                        [
                              SKURG~\cite{SKURG}{,}
                              IRAMIG~\cite{Liu_2024}{,}
                              OMG-QA~\cite{nan-etal-2024-omg}{,}
                              SAM-RAG~\cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration}{,}
                              MMed-RAG~\cite{xia2024mmedragversatilemultimodalrag}{,}
                              OmniSearch~\cite{li2024benchmarking}{,}
                              mR$^2$AG~\cite{Zhang2024mR2AGMR}{,}
                              RAGAR~\cite{khaliq-etal-2024-ragar}
                            , leaf, text width=62.0em
                        ]
                    ]                 
                ]
                [
                    Generation \\ Technique\\s~(\S\ref{sec_generation}), fill=yellow!20 
                    [
                        In-Context \\ Learning~(\S\ref{sec_incontext_learning}), fill=red!15
                        [
                            RMR~\cite{tan2024retrievalmeetsreasoninghighschool}{,}
                            ~\citet{Sharifymoghaddam2024UniRAGUR}{,}
                            RA-CM3~\cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling}{,}
                            RAG-Driver~\cite{yuan2024ragdrivergeneralisabledrivingexplanations}{,}
                            MSIER~\cite{luo2024doestextualinformationaffect}{,}
                            Raven~\cite{rao2024ravenmultitaskretrievalaugmented}
                            , leaf, text width=62.0em
                        ]
                    ]
                    [
                        Reasoning~(\S\ref{sec_reasoning}), fill=red!15
                        [
                            RAGAR~\cite{khaliq-etal-2024-ragar}{,}
                            VisDoM~\cite{suri2024visdommultidocumentqavisually}{,}
                            SAM-RAG~\cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration}{,}
                            LDRE~\cite{10.1145/3626772.3657740}
                            , leaf, text width=62.0em
                        ]
                    ]
                    [
                        Instruction \\ Tuning~(\S\ref{sec_instruction_tuning}), fill=red!15
                        [
                            RA-BLIP~\cite{ding2024rablipmultimodaladaptiveretrievalaugmented}{,}
                            RAGPT~\cite{lang2025retrieval}{,}
                            mR$^2$AG~\cite{Zhang2024mR2AGMR}{,}
                            RagVL~\cite{chen2024mllm}{,}
                            \citet{jang2024visual}{,}
                            MMed-RAG~\cite{xia2024mmedragversatilemultimodalrag}{,}
                            MegaPairs~\cite{zhou2024megapairsmassivedatasynthesis}{,}
                            Surf~\cite{sun2024surfteachinglargevisionlanguage}{,}
                            Rule~\cite{xia-etal-2024-rule}
                            , leaf, text width=62.0em
                        ]
                    ]
                    [
                        Source \\ Attribution~(\S\ref{sec_source_attrib}), fill=red!15
                        [
                            MuRAR~\cite{zhu2024murarsimpleeffectivemultimodal}{,}
                            VISA~\cite{ma2024visaretrievalaugmentedgeneration}{,}
                            OMG-QA~\cite{nan-etal-2024-omg}                          
                            , leaf, text width=62.0em
                        ]
                    ]
                ]
                [
                    Training \\ Strategies\\~(\S\ref{sec_training_strategies}), fill=yellow!20
                    [
                        Alignment~(\S\ref{sec_alignment}), fill=red!15
                        [
                            VISRAG~\cite{yu2024visragvisionbasedretrievalaugmentedgeneration}{,}
                            MegaPairs~\cite{zhou2024megapairsmassivedatasynthesis}{,}
                            SAM-RAG~\cite{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration}{,}
                            EchoSight~\cite{Yan_2024}{,}
                            HACL~\cite{jiang2024hallucination}{,}
                            ~\citet{10535103}{,}
                            ~\citet{kumar2024improvingmedicalmultimodalcontrastive}{,}
                            Dense2Sparse~\cite{nguyen2024multimodallearnedsparseretrieval}
                            , leaf, text width=62.0em
                        ]
                    ]     
                    [
                        Robustness~(\S\ref{sec:robustness}), fill=red!15
                        [
                            ~\citet{buettner-kovashka-2024-quantifying}{,}
                            MORE~\cite{cui2024moremultimodalretrievalaugmented}{,}
                          AlzheimerRAG~\cite{lahiri2024alzheimerragmultimodalretrievalaugmented}{,}
                          RAGTrans~\cite{10.1145/3637528.3672041}{,}
                          RA-BLIP~\cite{ding2024rablipmultimodaladaptiveretrievalaugmented}{,}
                          RagVL~\cite{chen2024mllm}{,}
                          RA-CM3~\cite{yasunaga2023retrievalaugmentedmultimodallanguagemodeling}
                            , leaf, text width=62.0em
                        ]
                    ]            
                    ]
                ]
            ]
        \end{forest}   
    }
    % \vspace{-6mm}
    % \caption{Taxonomy of recent advances and enhancements in multimodal retrieval-augmented generation research. For more details regarding this taxonomy, refer to Appendix ~\ref{sec:appendix}.}
    \caption{Taxonomy of recent advances in Multimodal RAG. Refer to Appendix~(\S\ref{sec:appendix}) for further details.}
    \label{fig:taxonomy_full}
    \vspace{-5mm}
\end{figure*}