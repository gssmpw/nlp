\section{Evaluation Setup}

\noindent\textbf{Implementation.} We implemented \method in Python 3.11, using custom log parsers for data extraction. The deviation analyzer employs scikit-learn's LOF model, and the graph analyzer uses NetworkX's\cite{networkx} MultiDiGraph with the Louvain algorithm.\footnote{\url{https://github.com/taynaud/python-louvain}}
We developed a custom filtering mechanism to filter suspicious file operations, network communications, and process-file operations. For the LLM, we deployed Qwen 2.5 (32B) in 8-bit quantized mode on an NVIDIA RTX 6000 GPU (32GB VRAM). \\

%\subsection{Datasets}
\noindent\textbf{Datasets}
Our evaluation leverages three system-level audit log datasets (THEIA, CADETS, Public Arena) and a high-level application log dataset (BlindEagle) in diverse environments (in Table~\ref{tab:train_test} we provide information on the datasets): 

\begin{itemize}[leftmargin=*,noitemsep]
    \item \textit{DARPA eng.3 THEIA:} A comprehensive Ubuntu Linux dataset comprising 106 million system audit logs (40GB) collected over 12 days. 
    It contains two stages of a successful APT attack instance and one failed attempt, with the successful attack spanning three daysâ€”(ideal for evaluating long-duration, multi-stage APT detection capabilities).

    \item \textit{DARPA eng.3 CADETS:} A FreeBSD-based dataset containing 42 million audit logs (25GB) captured over 12 days, featuring four APT attack instances (three successful and one failed). This dataset complements THEIA by providing attack patterns in a different Unix-like environment.

    \item \textit{Public Arena:} A Windows-focused dataset collected from two hosts in a simulated public cloud environment over six days. It contains one instance of an APT attack. With 16 million audit records (7GB), it enables cross-platform validation of our framework. 

    \item \textit{High-Level Application Log:}
To validate \method's applicability in enterprise environments, we developed an in-house testbed emulating the Blind Eagle APT campaign. 
The experiment ran for 60 minutes, generating 6,100 Splunk logs across three Windows workstations running Windows 10 and Windows 11. 
We simulated the attack by compromising two out of the workstations while incorporating realistic background user activities, such as web browsing, document editing, and email usage, in our controlled environment. 
This setup enabled us to evaluate \method's efficacy in detecting APT patterns and distinguishing them from benign user behaviors.
\end{itemize}

To further ensure that our evaluation reflects real-world scenarios, we split each dataset into training and testing windows. 
For training, we used data from earlier logs, prior to the occurrence of the first attack event, minimizing data leakage and simulating the challenges of deploying \method in a real-time environment (see Table~\ref{tab:train_test} for the time windows). 
For training, we used 28\% of the CADETS logs, 35\% of the THEIA logs, 29\% of the Public Arena logs and 33\% of the Blind Eagle APT logs, respectively. 
ensures both consistency across datasets and that there is sufficient historical context for extended inference periods during testing.


DARPA released the ground truths for CADETS and THEIA datasets containing IoCs but not specific attack events. 
To find the relevant attack events, we developed a customized script to retrieve logs containing these IoCs by cross-referencing log timestamps with the attack times delineated in the ground truth. 
We then manually annotated each attack event from these logs. 
For the Public Arena and Blind Eagle datasets we already had the ground truths with the identified attack events.
Table~\ref{tab:train_test} summarizes the characteristics the datasets, including key provenance statistics (e.g., number of edges, nodes, malicious entities), and overall dataset size. 
These datasets and their ground truth mappings served as the foundation for our comprehensive evaluation methodology. 


\begin{table}[!ht]
\centering
\caption{Train and test windows with detailed provenance statistics.}

\begin{adjustbox}{width=1.0\textwidth,center=\textwidth}
\begin{tabular}{|l|l|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Split Time}} & \multicolumn{2}{c|}{\textbf{Train}} & \multicolumn{5}{c|}{\textbf{Test}} \\
\cline{3-9}
 &  & \textbf{Duration} & \textbf{\# Logs} & \textbf{Duration} & \textbf{\# Logs} & \textbf{\# Attacks} & \multicolumn{2}{c|}{\textbf{Provenance Statistics}} \\
\cline{8-9}
 & & & & & & & \textbf{Benign (N, E, KE)} & \textbf{Malicious (N, E, KE)} \\
\hline
CADETS & 2018-04-06 11:00:00 & 6D8H & 4,239,474 & 3D4H & 10,949,668 & 3 & 263,775, 10,947,794, 1,293,534 & 33, 2,037, 60 \\

THEIA & 2018-04-09 22:15:12 & 3D13H & 9,654,772 & 7D11H & 17,827,942 & 1 & 492,556, 17,827,833, 1,823,963 & 16, 170, 31 \\

Public Arena & 2022-05-13 00:00:00 & 3D9H & 2,593,769 & 7D9H & 6,468,573 & 1 & 25,527, 6,093,093, 106,285 & 6, 375,480, 23 \\

Blind Eagle & 2024-11-06 16:18:00 & 20M & 700 & 40M & 6149 & 2 & 651, 5886, 4354 & 37, 263, 114 \\
\hline
\end{tabular}
\end{adjustbox}
\textit{N - nodes, E - edges, KE - key edges;} \\
\textit{*High due to repetitive communication between the malicious nodes.}
%\vspace{0.3cm}

\label{tab:train_test}
\end{table}

\noindent\textbf{Evaluation Metrics.}
To evaluate \method's performance, we employed a two-phase analysis process: coarse-grained (time-window level) detection and fine-grained (event-level) detection. 
This dual-layer approach enables comprehensive evaluation of the system's detection capabilities.
We used standard performance metrics: precision, recall, and the F1-score. 
At the event level, we focused on measuring the alert quality and assess \method's effectiveness in reducing alert fatigue, a critical consideration for practical deployment in security operations.
We compare our framework's performance to that of state-of-the-art APT detection systems, including UNICORN~\cite{han2020unicorn}, DeepLog~\cite{du2017deeplog}, and KAIROS~\cite{cheng2023kairospracticalintrusiondetection}
For fairness, we used the original implementations and hyperparameters provided by the respective authors. 
UNICORN and KAIROS were adapted to our sliding window mechanism, ensuring consistent evaluation across methods while preserving their core functionality.