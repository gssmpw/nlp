\documentclass[onecolumn,journal]{IEEEtran}
\usepackage{amsmath, amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{cite}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{adjustbox}
\usepackage{makecell}
% \usepackage{multirow}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}

\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning, decorations.pathreplacing}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\begin{document}


\title{Task Offloading in Vehicular Edge Computing using Deep Reinforcement Learning: A Survey}

\author{Ashab Uddin,  Ning Zhang and  Ahmed Hamdi Sakr
  %
  \thanks{A. Uddin,  N. Zhang, and A. H. Sakr are with the Department of Electrical and Computer Engineering, University of Windsor, Canada. Email: \{uddin81,  ning.zhang, ahmed.sakr\}@uwindsor.ca
}}

% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle
% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\begin{abstract}
The increasing demand for Intelligent Transportation Systems (ITS) has introduced significant challenges in managing the complex, computation-intensive tasks generated by modern vehicles while offloading tasks to external computing infrastructures such as edge computing (EC), nearby vehicular  , and UAVs has become influential solution to these challenges. However, traditional computational offloading strategies often struggle to adapt to the dynamic and heterogeneous nature of vehicular environments. In this study, we explored the potential of Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) frameworks to optimize computational offloading through adaptive, real-time decision-making, and we have thoroughly investigated the Markov Decision Process (MDP) approaches on the existing literature. The paper focuses on key aspects such as standardized learning models, optimized reward structures, and collaborative multi-agent systems, aiming to advance the understanding and application of DRL in vehicular networks. Our findings offer insights into enhancing the efficiency, scalability, and robustness of ITS, setting the stage for future innovations in this rapidly evolving field.

\end{abstract}

\begin{IEEEkeywords} edge computing, reinforcement learning, vehicular networks, multi-agent learning.
\end{IEEEkeywords}

\section{Introduction}
In recent years, the automotive industry has undergone a significant transformation with the integration of various embedded sensors, processing units, and wireless communication modules to enhance the overall driving experience by making it safer, more efficient, and more comfortable \cite{b201}. Consequently, the degree of data dependency, the workload associated with data preprocessing, as well as the computational intensity and delay sensitivity, have increased across various areas of vehicular technology, including autonomous driving, real-time traffic management, and advanced driver-assistance systems (ADAS). Moreover, due to the dynamic nature of vehicular environments, significant challenges to the effective offloading and processing of tasks are exacerbated by factors such as high mobility, variable network conditions, and stringent latency requirements \cite{b2021}. 

To address the challenges of onboard vehicle computational limitations , latency sensitivity and energy scarcity ,  computation offloading enables vehicles to delegate resource-intensive tasks to more capable external computing infrastructures, such as centralized cloud computing (CC) , Mobile Edge Computing (MEC) and FOG \cite{b202,b203, ba07,ba06,b204}. MEC brings computation closer to the user by deploying resources at the network edge, including Roadside Units, onboard vehicle systems, and other infrastructure like parking and gas stations \cite{b207}. The integration of MEC into the vehicular ecosystem is becoming increasingly critical, as it aligns with the low-latency, high-reliability requirements essential for the emerging 5G and future 6G networks \cite{b208}, \cite{b2081} while Fog computing, on the other hand, enables cooperation among edge devices creating an intermediate bridge between edge device and cloud \cite{b209}.

In order to facilitate communication between these computing resources, vehicles rely on Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication protocols. These protocols leverage advanced radio access technologies (RATs) such as dedicated short-range communications (DSRC) and cellular networks, including LTE and the emerging new radio (NR) standards, to ensure reliable and efficient data exchange in vehicular networks \cite{b210}. Moreover, As vehicles and mobile users move across different geographic areas, the ongoing services hosted on nearby edge servers can experience significant performance degradation, a drop in Quality of Service (QoS), and even interruptions if they remain tied to a server that the user has moved away from. Service migration \cite{b211} addresses these challenges by dynamically relocating services to the most appropriate edge server, ensuring that the services remain close to the user and continue to function smoothly. This process is crucial to maintain seamless service continuity, minimizing latency, optimizing network performance,and balancing the migration costs.

Due to the massive interconnected and heterogeneous nature of vehicular networks, efficient resource management  \cite{b214}, \cite{b101} is essential to ensure service quality and reliability and optimal usage of computational resources, meeting the stringent latency and reliability requirements. Advanced techniques, such as predictive analytics and machine learning, can be employed to anticipate resource demands and optimize task distribution across available computing infrastructures, including edge servers,  fog servers, cloud and nearby users. By dynamically balancing the load and prioritizing critical tasks, these approaches help prevent resource bottlenecks, reduce processing delays, and enhance overall system performance.

%\section{Background, Motivation and Contribution}
In recent years, edge computing and computational offloading have emerged as essential solutions to address challenges in modern vehicular, mobile, and IoT networks. Researchers have explored various architectures such as MEC, ad hoc systems, UAV networks, and cloudlets to improve performance, reduce latency, and optimize resource management. Surveys such as those by Abbas et al.  \cite{b2071} and Shi et al. \cite{b152} emphasized MEC's role in latency reduction, resource allocation, and privacy enhancement, with applications in smart homes, augmented reality, and healthcare. Comparative studies like Nouhas et al  \cite{b2072}  highlighted the strengths of hybrid Cloud-Edge models, while others, like Deepak et al. \cite{b1072}, Liu et al. \cite{b151}, and Wang et al.  \cite{b211}, stressed the need for advancements in resource allocation, Edge Intelligence, and blockchain integration for security. Studies on vehicular networks, including the work of Hou et al. \cite{b204}, examined AI and RL for dynamic task optimization and the potential of Digital Twin Edge Networks (DITEN), and Mao et al. \cite{b202} studied the feasibility of server deployment. Furthermore, Vehicular fog computing (VFC) and heterogeneous Vehicular Networks (HetVNETs) have been explored to leverage vehicular resources and improve connectivity while the integration of MEC with SDN and NFV, has been discussed by Haibeh et al. \cite{b1071}  followed by Filali et al. \cite{b207}, further highlighted advances in network performance and scalability, particularly in 5G environments.

Traditional optimization techniques such as convex optimization \cite{b83}, evolutionary algorithms \cite{b85}, and game-theory \cite{b88} approaches face significant challenges in solving the problem of mixed integer nonlinearty, along with with the increasing size and complexity of vehicular networks. These methods often struggle with scalability, adaptability to dynamic environments, and computational efficiency, especially in systems with large state-action spaces and non-convex dynamics. In contrast, deep reinforcement learning (DRL) offers a promising solution by simultaneously learning state transitions and optimizing decision-making policies in real time. DRL's ability to handle both centralized and multi-agent frameworks enables it to adapt to diverse vehicular scenarios, making it well-suited for dynamic and uncertain environments. 


Considering the solution perspective through DRL, Nguyen et al. \cite{b2073} highlighted DRL's potential for MEC-enabled AANs but emphasized challenges with RIS and NOMA integration while Chen et al. \cite{b2074} noted MDP limitations in IoT, advocating robust frameworks for decentralized and multi-agent systems. Meanwhile, Liu et al. \cite{b203} identified scalability and coordination gaps in vehicular networks, suggesting hierarchical DRL models for efficient offloading. Focusing on One of the key challenges that arise in vehicular networks, such as high mobility and the non-stationary nature of the environment, Althamary et al. in  \cite{b175} detailed how MARL frameworks can provide robust solutions, offering examples of MARL’s application to V2V, V2I, and V2X communications. Another key challenge i.e edge caching, has been examined by  Zhu et al. in \cite{b176} illustrating how DRL can improve caching strategies in mobile networks by making intelligent, data-driven decisions based on user behavior and network conditions.  Finally, Li et al. \cite{b174} presented a comprehensive survey of Multi-Agent Reinforcement Learning (MARL) in future Internet technologies, focusing on challenges like dynamic network access, transmit power control, and computation offloading.  

Studies on DRL  highlights not only potential solutions for mixed integer nonlinear problems, but also demonstrates a greater ability to adapt and cope with varying conditions and large networks. However,  A key challenge that remains under-explored in current DRL research is the examination of MDP formulations with learning methods that fully account for the complexities of dynamic environments while meeting system-wide goals. Effective coordination of MDP formulations with state-action transitions and reward structures tailored to specific DRL approaches is essential for optimizing performance. This includes designing reward functions that not only guide agents toward local objectives but also ensure overall system efficiency. In multi-agent systems, uncoordinated individual decisions often lead to suboptimal outcomes, such as resource contention and inefficiency. Furthermore, aggregating individual POMDPs into a coherent global MDP is complex, requiring an understanding of how these MDPs interact, synchronize, or merge to form a valid and accurate representation of the whole environment. Studying this aggregation process is crucial for optimizing multi-agent systems' performance. Mechanisms that foster collaboration among agents are vital to prevent locally optimal decisions from leading to globally suboptimal results.

DRL studies show promise in solving mixed integer nonlinear problems and adapting to dynamic, large-scale networks. However, challenges remain in formulating MDPs that address environmental complexities and system-wide goals., and at the same time effective reward structures are crucial for balancing local objectives with overall efficiency. Morover, in multi-agent systems, uncoordinated decisions often cause resource contention and inefficiency while aggregations of individual POMDPs into a coherent global MDP is particularly complex, requiring synchronization and interaction analysis to optimize performance. Furthermore, uniform learning among agents is a potential bottleneck that need to be addressed and analyzed with performance dynamics.

Our research objectives are to investigate how MDP formulations can be optimized to enhance task offloading and decision-making under uncertainty. Critical challenges such as value function or quality function approximation, policy convergence, exploration and exploitation trade off, central and decentral learning approach for MARL, uniform learning accross agents and reward function design are analyzed within a multi-agent context. By addressing these issues, we aim to improve the overall performance of MDP-based DRL methods, offering new solutions for both local and global optimization in dynamic, multi-agent environments. 



The key contributions of this work are summarized as follows:

\begin{itemize}
    \item We have investigated Markov Decision Process (MDP) formulations across different studies, underlining the gap and addressing simplified assumptions. 
    \item Our study reveals that the variability in reward functions remains an unresolved research question in many works, i.e. the trade-offs between latency and energy that fail to optimize both metrics simultaneously and fairness among multiagents.  
    \item  We analyze value function approximation techniques for value iteration and policy gradient DRL models in vehicular networks, examining the impact of discrete vs. continuous action spaces and stochastic vs. deterministic policies. Our findings highlight their influence on scalability and stability in single and multi-agent offloading, addressing real-time challenges in autonomous driving and intelligent transportation systems.
    \item Our study provides a comprehensive context for collaborative decision-making among multiple agents in vehicular networks, addressing learning architectures such as centralized and decentralized models. We investigate the impact of dynamic environmental changes on learning and coordination, enabling agents to adapt effectively to fluctuating conditions such as network topology, user mobility, and resource availability.
    \item Our work offers valuable insights for future research on MDP objectives, collaborative and uniform learning in vehicular task offloading scenarios for both single and multi agent scenerios. 
    
    
\end{itemize}

The rest of the paper is organized as follows: Section III discusses the Computing Paradigms for Vehicular Networks,Section IV delves into the type connectivity of computational resources , and section V introduces 
the Basics of Deep Reinforcement Learning (DRL), offering a foundational understanding of RL approaches. Consequently  Section VI presents a Comprehensive Review of the Literature, and finally, Section VIII provides the Conclusion, summarizing the paper's contributions and the potential impact of DRL on enhancing vehicular network performance.

\section {Computing paradigm for Vehicular Network}
The evolution of vehicular networks has introduced smart vehicles supported by advanced information and communication technologies, enabling significant innovations in communication, computing, and caching. Despite these advancements, vehicles often face constraints such as limited computational resources and finite onboard battery life. To address these challenges, powerful computing paradigms like Centralized Cloud (CC),  Edge Cloud (MEC), and Vehicular Cloudlet (VC),UAV, Sattelite have been developed, significantly enhancing the performance of vehicular applications through computation offloading.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{comp_paradhim.png}
    \caption{Different Computing Paradigm}
    \label{fig:computing paradigm}
\end{figure}


\subsection{Centralized Cloud (CC)} Cloud computing is a transformative model that enables ubiquitous, convenient, and on-demand network access to a shared pool of configurable computing resources. These resources include networks, servers, storage, applications, and services, which can be rapidly provisioned and released with minimal management effort or interaction with service providers. One of the key advantages of cloud computing is its ability to enhance computing performance by leveraging abundant , innovative, cost-effective, and scalable storage and computeing solutions, facilitating a wide range of applications \cite{b101}. Despite its numerous benefits, the physical distance between cloud servers and users can introduce delays, which may degrade the performance of latency-sensitive applications. Additionally, the transmission of large amounts of data can overburden limited network bandwidth, leading to potential congestion and higher transmission costs \cite{b102}.

 
\subsection{ Edge Cloud (EC)} Edge cloud computing is an extension of cloud computing that brings computational resources closer to the data source or end-users. Edge cloud computing is particularly beneficial for applications requiring real-time data processing, such as autonomous driving, augmented reality, and smart grids.
Edge cloud can be referred as:
\subsubsection{Mobile Edge Computing (MEC):}
Mobile Edge computing \cite{b104} refers to the processing of data near the source of data generation, such as IoT devices, sensors, and smart gadgets. This proximity ensures low latency, real-time data processing, and reduced bandwidth usage and enables cloud computing capabilities and an IT service environment at the edge of the cellular network i.e placed in road side unit, a nearest coputational source from vehicles \ref{fig:computing paradigm}. MEC allows network operators to open their radio access networks (RAN) to authorized third parties, such as application developers and content providers. This is particularly beneficial for latency-sensitive applications and services that require rapid data processing close to the mobile users.

\subsubsection{Fog Computing(FC):}
Fog computing  \cite{b103}, on the other hand, acts as an intermediary layer between the edge devices and the central cloud. It extends cloud computing capabilities to the edge of the network, providing additional computational power, storage, and networking services closer to the data sources but little i.e placed in parking station far from MEC .It involves the use of decentralized computing infrastructure in which data, compute, storage, and applications are distributed in the most logical, efficient place between the data source and the cloud. Fog computing is particularly useful for applications that require low latency and are bandwidth-sensitive, providing computation, storage, and networking services between end devices and traditional cloud data centers.



\subsubsection{On Board Vehicle Server:}
A vehicular cloud or On Board Vehicle Server \cite{b105} is a mobility-enhanced, small-scale cloud datacenter mounted onboard vehicles, designed to support resource-intensive and interactive mobile applications. Vehicles with underutilized resources create a distributed network by leveraging their computational, storage, and communication capacities as mobile nodes. This setup enables data processing to occur closer to the point of need, reducing reliance on distant data centers. The decentralized structure of VCC enhances real-time applications in transportation, such as traffic management and autonomous driving, by improving network efficiency and reducing latency.




\subsubsection{Unmanned Aerial Vehicles (UAV)}
Unmanned Aerial Vehicles (UAVs) or drones \cite{b1061}, have become a critical asset in enhancing vehicular networks and Intelligent Transportation Systems (ITS) by providing dynamic, aerial platforms for data collection, communication, and environmental monitoring. UAVs can be rapidly deployed in areas lacking ground-based infrastructure, such as rural or disaster-stricken regions, to monitor traffic conditions, detect accidents, and relay real-time information to traffic management centers or directly to vehicles. Additionally, UAVs serve as mobile communication relays, augmenting traditional networks in high-density urban areas or large-scale events, thereby reducing latency and improving data throughput. Despite their potential, challenges such as regulatory constraints, safety concerns, and limited flight endurance must be addressed to ensure their effective integration into vehicular networks.


\section {Network Topology for Vehicular Network}
Vehicular networks are rapidly evolving, driven by the increasing demands for low-latency communication, real-time data processing, and efficient resource management. With the rise of connected and autonomous vehicles along with multiple types of computing paradigm, different connectivity topologies, such as central, distributed, hierarchical, ad-hoc, and heterogeneous, enable efficient data handling across various layers of the network. Each of these topologies comes with its own strengths and weaknesses, depending on factors like latency requirements, the density of vehicles, and the availability of infrastructure. This section delves into these diverse topologies, exploring how they can be optimized for vehicular networks to enhance overall system performance .



\subsection{Central MEC }

A single central MEC \cite{b149} system offers strong centralized control, efficiently coordinating resources like radio bandwidth and computational power. Deploying the MEC server at a base station (centre of the network) allows for task processing close to the user, reducing latency compared to traditional cloud computing. Centralized management simplifies resource allocation, enabling the central MEC server to use channel state information (CSI) and computation requests to optimize. However, this setup faces challenges in scalability and resource limitations, as multiple users share the MEC server's resources, potentially leading to bottlenecks, especially in high-traffic situations. Distributed MEC systems, with multiple edge servers, address these issues by distributing the load, enhancing scalability, and reducing latency.


\subsection{Distributed MEC}

Distributed MEC \cite{b150} offers a solution to challenges in vehicular networks by dispersing computational resources across multiple edge nodes, such as vehicles and roadside units (RSUs), instead of relying on a single centralized server. This setup significantly reduces latency by enabling data processing closer to its source, which is essential for real-time applications like autonomous driving and traffic management. By distributing tasks across nodes, the system better adapts to the mobility and resource variability in vehicular networks, improving scalability and resilience. Security and privacy are also enhanced, as data can be processed locally, minimizing transmissions to distant servers. 

\subsection{Hierarchical MEC Network}

Distributed edge computing systems face challenges \cite{b152} such as resource fragmentation, lack of centralized control, and issues with latency and data consistency, especially in environments with numerous edge nodes. Hierarchical edge computing \cite{b151} addresses these issues by organizing resources across layered levels, from local edge devices to centralized cloud systems. This structure optimizes resource management by processing simpler tasks locally and offloading complex ones to higher tiers, reducing latency, network congestion, and energy use. Technologies like Software-Defined Networking (SDN), Network Function Virtualization (NFV), and network slicing enhance control and flexibility, improving traffic flow, task scheduling, and efficiency. The hierarchical model also supports load balancing and fault tolerance, making it particularly suited for large-scale settings like smart cities.  Hierarchical network may include:



\subsubsection{Ad-hoc Networks in MEC}
In MEC, ad-hoc networks formed by VANETs (Vehicular Ad-hoc Networks) \cite{b153},\cite{b154} and UAVs (Unmanned Aerial Vehicles) \cite{b155},\cite{b156} are essential for enabling decentralized, real-time data processing closer to the source. These networks operate independently of centralized infrastructure, offering flexibility and adaptability in dynamic environments. Edge servers on vehicles or UAVs forming ad-hoc nature allows rapid, direct communication between vehicles (V2V), vehicles and UAVs (V2U), or UAVs (U2U), forming a dynamic and resilient network. This setup enhances robustness and scalability, enabling task offloading to nearby nodes even in areas with limited fixed infrastructure like disaster zones, rural areas, and dense urban spaces.

\subsubsection{Heterogeneous MEC}

Heterogeneous MEC refers to a topology that integrates different types of network elements, including vehicles, RSUs, satellites, and cloud servers. This approach ensures that vehicular networks can support a variety of communication protocols and device capabilities, allowing for more robust and flexible architectures. Heterogeneous MEC is particularly useful for long-range communication and applications that require global coverage, such as integrating satellite links for vehicular communication in remote areas \cite{b205}. By combining diverse technologies, this topology provides seamless connectivity, even in highly variable and dynamic network conditions.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{hierchial_net.png}
    \caption{Hierarchical Network with adhoc}
    \label{fig:hierarchical Offloading}
\end{figure}

\section{DRL Basics}

\subsection{Markob Decision Process}

In reinforcement learning Fig. \ref{fig: Reinforcement Learning -MDP}, the agent interacts with the environment by observing states, taking actions, and receiving rewards. The goal is to maximize long-term expected rewards. This process is modeled as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \gamma)$ \cite{b131}:

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{mdp_presentation.png}
    \caption{Reinforcement Learning -MDP}
    \label{fig: Reinforcement Learning -MDP}
\end{figure}

$S$: State space
$A$: Action space
$P$: Transition probabilities
$R$: Rewards
$\gamma$: Discount factor
The transition probability $p(s', r \mid s, a)$ depends only on the current state and action, not past states a fundamental property of Markov Process. Policies can be deterministic, a direct measure ($a = \pi(s)$) or stochastic, a probability distribution  ($\pi(a \mid s)$). 
The state value function, $V_\pi(s)$, represents how good a state is when following a policy $\pi$. It is defined as:
\begin{align}
V_\pi(s) = \sum_a \pi(a \mid s) \sum_{s', r} p(s' \mid s, a) (r + \gamma V_\pi(s'))
\end{align}
This can be simplified to:
\begin{align}
V_\pi(s)  = \sum_a \pi(a \mid s) Q_\pi(s, a)
\end{align}


The action value function, $Q_\pi(s, a)$, represents how good a state-action pair is when following a policy $\pi$. It is defined as:
\[
Q_\pi(s, a) = \sum_{s', r} p(s' \mid s, a) (r + \gamma V_\pi(s'))
\]
This can be simplified to:
\[
Q_\pi(s, a) = \sum_{s', r} p(s' \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') Q_\pi(s', a') \right]
\]

The Bellman optimality equation \cite{b132} for the value function is given by:

\begin{align}
    V_{\pi^*}(s) &= \max_a \left[ r + \gamma \sum_{s'} p(s' \mid s, a) V_{\pi^*}(s') \right] \quad 
\end{align}

Similarly, the Bellman optimality equation for the action-value function is:

\begin{align}
    Q_{\pi^*}(s, a) &= \sum_{s', r} p(s' \mid s, a) \left[ r + \gamma \max_{a'} Q_{\pi^*}(s', a') \right] \quad 
\end{align}



\subsection{Value Function Approximation:Value Iteration}

Value Function Approximation (VFA) \cite{b134} uses supervised learning to map states (or state-action pairs) to value functions. The parameters $\theta$ are updated based on observed data.

The loss function for VFA is:
\begin{align}
L(\theta) = \mathbb{E}_{(s, r, s')} \left[ \left( r + \gamma \hat{V}(s'; \theta) - \hat{V}(s; \theta) \right)^2 \right]
\end{align}
For action-value function approximation:
\begin{align}
L(\theta) = \mathbb{E}_{(s, a, r, s')} \left[ \left( r + \gamma \max_{a'} \hat{Q}(s', a'; \theta) - \hat{Q}(s, a; \theta) \right)^2 \right]
\end{align}


\subsubsection{Deep Q-Network (DQN) with Variant}

The Deep Q-Network (DQN) \cite{b135} is an off-policy reinforcement learning algorithm, meaning that it learns the optimal policy from the experience buffer, collected following old policy. In DQN, a neural network is used to estimate current state action-values (Q-values). To stabilize training, a target network is updated less frequently and used to calculate target Q- values, reducing the instability caused by correlations in the training data. Double DQN \cite{b136} addresses the overestimation bias by using the main network to select actions and the target network to evaluate the selected actions, further refining the Q-value estimates while Dueling DQN improves the model by separately estimating the state value and advantage functions, which are then combined to produce more robust action-value predictions, enhancing performance and generalization.


\subsection{Value Function Approximation: Policy Gradient}
The gradient of the value function is the expectation of the trajectory return times the accumulated score function \cite{b131}:
\begin{align}
\nabla_{\theta} V^\pi(s_0) &\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_{\theta} \pi(a \mid s) \notag \\
&= \sum_{s \in \mathcal{S}} \left[ d^\pi(s) \sum_{a \in \mathcal{A}} \pi(a \mid s) Q^\pi(s, a) \nabla_{\theta} \log \pi(a \mid s) \right] \notag \\
&= \mathbb{E}_{\pi} \left[ Q^\pi(S_t, A_t) \nabla_{\theta} \log \pi_{\theta}(A_t \mid S_t) \right]
\end{align}

\subsubsection{Actor-Critic Method}
The Actor-Critic \cite{b139} method method involves two networks: the Actor, which generates actions(policy) based on the current state, and the Critic, which estimates the expected return for that state following maximum quality function $Q(s,a)$. The Critic updates its value estimates by minimizing the Temporal Difference error for bellman equation, which accounts for the immediate reward and the next state's value. The Actor's policy is refined using the gradient of the objective function, guided by the Critic's feedback. This interaction helps the agent learn an optimal policy to maximize rewards.

\subsubsection{Trust Region Policy Optimization (TRPO)}
A variant of the Actor-Critic method , Trust Region Policy Optimization (TRPO) \cite{b142}, is often called on policy method because of policy updates with the experienece collected by current policy, and utilize restricted policy updates during each iteration to ensure stability. It does so by applying a constraint on the change in policy, measured by the KL divergence between the new and old policies. The objective is to optimize the policy while keeping the policy changes within a specified limit to prevent large, unstable updates. This approach helps in maintaining more reliable and consistent improvements during training.


\subsubsection{Proximal Policy Optimization (PPO)}
Another variant of the Actor-Critic method (on policy), Proximal Policy Optimization (PPO) \cite{b143}, restricts updates to ensure stability. The objective function balances policy improvement by taking the minimum of the advantage function and a clipped version to prevent excessive changes. Additionally, to encourage exploration, the objective is refined by incorporating an entropy term, which promotes a diverse set of actions. This approach helps maintain a balance between exploration and exploitation, leading to more stable and efficient learning.



\subsubsection{Soft Actor-Critic (SAC)}
The Soft Actor-Critic (SAC)  \cite{b140}, an off-policy algorithm uses three key components: a policy network, Q-networks, and a value network. The policy network generates actions based on the current state, optimized by maximizing expected returns and incorporating an entropy term to encourage exploration. The Q-networks estimate expected returns for state-action pairs and are updated using the soft Bellman residual, which considers immediate rewards and future values. The value network predicts the overall value of a state, stabilizing training by minimizing the Temporal Difference (TD) error. These components work together to balance exploration and exploitation, enabling the learning of effective policies.


\subsubsection{DDPG}
The Deep Deterministic Policy Gradient (DDPG) \cite{b141} is an off-policy algorithm designed for environments with continuous action spaces. It involves an Actor network that generates deterministic actions based on the current state, meaning the actions are not probabilistic but directly selected according to the policy. The Critic network evaluates these actions by estimating the expected returns, and it updates its estimates by minimizing the Temporal Difference error. The Actor's policy is refined using gradients that drive the actions toward those that maximize the Q-value. To ensure stable learning, both the Actor and Critic networks have corresponding target networks that are updated gradually. This framework allows DDPG to learn effective and stable policies in environments requiring deterministic decisions and continuous actions.


\subsubsection{TD3} The Twin Delayed Deep Deterministic Policy Gradient (TD3) \cite{b157} algorithm improves upon the Deep Deterministic Policy Gradient (DDPG) by addressing overestimation bias and instability in continuous action spaces. TD3 uses two Critic networks, selecting the smaller Q-value to reduce overestimation. It delays policy updates, updating the Actor less frequently than the Critics to avoid unstable updates. Additionally, TD3 adds noise to target actions (Target Policy Smoothing) to prevent overfitting. These enhancements make TD3 more stable and reliable, leading to better performance in continuous action environments.







\subsection{MultiAgent DRL }

In Vehicular Edge Computing (VEC), objectives such as latency reduction, energy efficiency, load balancing, scalability, reliability, and data security can be achieved through a single-agent approach with a fully observable MDP or via a multi-agent system using a partially observable Markov decision process (POMDP). Single Agent RL centrally optimizes latency, energy, and task allocation for multiple vehicles or servers while multiagent Reinforcement Learning (MARL) \cite{b146}, \cite{b147} involves multiple agents interacting within a shared environment, each optimizing the set objectives:

\begin{itemize}
    \item \textbf{Collaborative MARL}: Agents cooperate to achieve a shared/common goal, e.g., optimizing system latency and energy without regard for individual interests  \cite{b162}. 
    
    \item \textbf{Cooperative MARL}: Agents act independently to achieve own objective, but also consider cooperation to each other e.g., optimizing overall system utility with cooperative channel sensing mechanism \cite{b163}.
    
    \item \textbf{Competitive MARL}: Agents compete with each other solely to maximize their own objectives, e.g., competing for channel or resource access \cite{b164}.
\end{itemize}

\subsubsection{Key Challenges of MARL}
In multi-agent environments, agents often operate under Partial Observability, making decisions based on local information, modeled by Partially Observable Markov Decision Processes (POMDPs). This leads to the non-stationarity  \cite{b165}  issue, where agents learn and update their policies concurrently. As one agent adapts, others are doing the same, causing the environment to change continuously. This violates the assumption that state transitions and rewards are going to be stationary, making it difficult for any agent to achieve stable learning.  In addition to non-stationarity, scalability \cite{b166} is a critical challenge in multi-agent systems. As the number of agents increases, the joint action space expands exponentially, making it computationally expensive to calculate optimal policies. While Deep Neural Networks (DNNs) are used in Multi-Agent Reinforcement Learning (MARL) to approximate large action spaces and enhance scalability, they introduce challenges in terms of convergence due to the complexity of deep learning theory. As the system size grows, ensuring efficient and stable learning remains a major concern for MARL algorithms, requiring advanced methods to handle the trade-offs between complexity and performance. The multi agent approach can be referred as the followings:
\subsubsection{\textbf{Centralized Training Decentralized Execution (CTDE)}} In
CTDE ,\cite{b167} a centralized critic uses global information to optimize the policies of all agents during training, ensuring that the system can handle large-scale environments efficiently. However, during execution, agents act independently, which reduces communication overhead and enables decentralized decision-making.
Moroever,  by using global information during centralized training, CTDE helps mitigate the non-stationarity problem, as agents learn while considering the actions and policies of others. This results in more stable learning because the centralized critic can account for the evolving environment and ensure policy convergence, even when agents interact dynamically. Following are two CTDE method:
\begin{itemize}
     \item \textbf{MultiAgent DDPG:}  MADDPG (\cite {b144})is a multi-agent reinforcement learning algorithm based on the Deep Deterministic Policy Gradient (DDPG) framework. It follows the Centralized Training and Decentralized Execution (CTDE) paradigm, where each agent has a critic that has access to global state and action information from all agents during training. This access helps improve the evaluation of actions in complex multi-agent environments, where both cooperation and competition may occur. During execution, however, each agent acts independently using only its actor, which makes decisions based solely on local observations, enabling decentralized and autonomous behavior in real-time environments. While MADDPG effectively handles complex multi-agent dynamics, its reliance on global state information during training introduces scalability issues, especially as the number of agents increases. Additionally, it lacks explicit mechanisms for value decomposition or reward sharing among agents, which limits its ability to compute individual agent contributions to the overall team performance.

    \item \textbf{Counterfactual Multi-Agent Policy Gradient (COMA):} 
     As the central critic in MADDPG evaluates $Q$ based on the overall state and joint actions, it can be difficult for an agent to determine how much its individual action contributed to the overall team reward. COMA (Counterfactual Multi-Agent Policy Gradients) (\cite{b169}) is an actor-critic algorithm designed to address the credit assignment problem in cooperative multi-agent environments. Unike MADDPG, it uses a centralized critic to evaluate joint actions, but with the added feature of calculating a counterfactual baseline to determine each agent's contribution to the total reward. This method compares an agent's action to what would have happened if the agent had taken a different action, thus improving credit assignment. COMA is efficient but is best suited for cooperative scenarios as it relies on knowing the global state, which can be impractical in large-scale systems.
     
     \item \textbf{Value Decomposition Network (VDN):}  VDN \cite{b172} is designed specifically for cooperative multi-agent systems by simplifying the training process through value decomposition. In VDN, the global Q-value is estimated as the sum (decomposed) of individual agents' Q-values, which allows each agent to focus on optimizing its individual task while still contributing to a collective goal. This decomposition facilitates decentralized decision-making while ensuring that agents remain aligned towards maximizing the overall team reward. VDN is particularly useful in settings where agents must work together, as it simplifies coordination by breaking down the global objective into smaller, independent tasks. However, this method assumes that the global value function can be accurately represented as a simple sum of individual Q-values, which may not capture the full complexity of certain interactions between agents in highly interdependent environments.

     
%VDN is updated by VDN target (TD error) as same DQN, MADDPG and COMA also has central Critic Traget to update using bellman equation.









  
    
\end{itemize}


\subsubsection{ \textbf{Decentralized Learning with Networked Agents}}
 Decentralized learning \cite{b168}  excels at handling scalability since agents only rely on their own local observations and, in some cases, information from nearby agents. This significantly reduces the need for global information exchange, making it suitable for large-scale systems like IoT or vehicular networks. It also minimizes the computational and communication burden, enabling the system to scale efficiently.  However, the reduced information sharing in decentralized learning can compromise stationarity. Agents make decisions based on limited information, and as other agents update their policies independently, this can lead to instability in the environment. The lack of global oversight may cause agents to adapt to a constantly changing environment, making it difficult for any single agent to converge to an optimal policy.
 \begin{itemize}
     \item \textbf{Multi-Agent Deep Q-Network(MADQN):} MADQN (\cite{b170}) is a decentralized reinforcement learning algorithm designed for multi-agent environments. Each agent independently learns its Q-function based on local observations, allowing for autonomous decision-making. While agents do not communicate directly, they share a global reward, promoting cooperation in optimizing shared resources. MADQN is particularly effective in dynamic, resource-constrained environments like V2X communications, where agents need to manage heterogeneous traffic. By reducing the action space through techniques like using virtual agents, MADQN enhances learning efficiency and improves performance in scenarios with varying resource demands.
     \item \textbf{Multi-Agent Actor Critic (MAAC):}  The MAAC \cite{b171} algorithm is a multi-agent reinforcement learning approach where each agent uses a local actor and critic to make decisions based on local observations. The critic provides feedback on action quality, while the actor updates the agent's policy accordingly. A key feature of MAAC is its decentralized approach, where agents communicate with neighbors to share estimates of the global action-value function, enabling them to refine policies with limited global information. This method promotes collaboration and scalability, making it suitable for complex environments with multiple agents working toward common goals.
     \item \textbf{Multi-Agent Soft Actor Critic (MASAC):} MASAC \cite{b59} framework, each agent is responsible for optimizing its own actions to achieve a balance between exploration and exploitation using soft actor-critic (SAC)  techniques. The SAC algorithm is a stochastic policy method that incorporates entropy maximization, which encourages the agent to explore a diverse range of strategies while learning an optimal policy. By adopting a decentralized approach, MASAC allows for distributed decision-making, making it particularly useful in scenarios where direct communication between agents is limited or infeasible. 
 \end{itemize}

\subsubsection{\textbf{Game Theory integrated MARL}}
Game theory and Multi-Agent Reinforcement Learning (MARL) \cite{b179}, \cite{b180} converge to address complex decision-making in dynamic multi-agent environments, where multiple agents must adapt their strategies based on the actions of others. Game theory offers strategic frameworks like Nash equilibrium and Stackelberg equilibrium to model the interactions between rational agents, helping define stable solutions where no agent benefits from deviating unilaterally. In contrast, MARL focuses on how agents learn optimal strategies through trial and error, aiming to maximize cumulative rewards in an evolving environment. Integrating game-theoretic concepts into MARL allows for more structured coordination, enabling agents to converge toward equilibrium solutions, manage cooperation and competition, and enhance the learning process in decentralized, multi-agent systems.

\begin{table*}[t]
\centering
\tiny
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|p{1.0cm}|p{1.75cm}|p{1.5cm}|p{1.2cm}|p{1.2cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.8cm}|}
\hline
 & \textbf{DQN} & \textbf{AC} & \textbf{A2C} & \textbf{A3C} & \textbf{SAC} & \textbf{DDPG} & \textbf{TRPO} & \textbf{PPO} \\
\hline
\textbf{Policy Type} & Off Policy & On Policy & On Policy & On Policy & Off Policy & Off Policy & On Policy & On Policy \\
\hline
\textbf{Method} & Value Iteration & Policy Iteration & Policy Iteration & Policy Iteration & Hybrid (Value + Policy) & Hybrid (Value + Policy) & Policy Iteration & Policy Iteration \\
\hline
\textbf{Policy Evaluation} & \text{Optimize:}  $V_{\pi}(s)/Q_{\pi}(s, a)$ & $V_{\pi}(s)/Q_{\pi}(s, a)$ & $A_{\pi}(s, a)$ & $A_{\pi}(s, a)$ & \text{Optimize:  Entropy} + $V_{\pi}(s)/Q_{\pi}(s, a)$ & \text{Optimize} $V_{\pi}(s)/Q_{\pi}(s, a)$ & \scalebox{0.8}{$\frac{\pi_{\text{new}}(a|s)}{\pi_{\text{old}}(a|s)} A_{\pi}(s, a)$} & \scalebox{0.8} {\textbf{Clip({$\frac{\pi_{\text{new}}(a|s)}{\pi_{\text{old}}(a|s)} A_{\pi}(s, a)$})}} \\
\hline
\textbf{Features} & Learn from buffer, increase sample efficiency & Reduce Variance in Policy Gradient & Synchronous Learning for Multiagent environment & Prevent Correlation and variance in policy for Multiagent environment & Soft Objective with more randomness and exploration & Efficient in Continuous Control & Prevent unexpected parameter updates & Prevent unexpected parameter updates \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Comparison of Reinforcement Learning Methods}
\end{table*}







\section{DRL Literature Review in Vehicular Computational Offloading}
 

In Vehicular Edge Computing (VEC), the efficiency of task handling, latency reduction, and energy optimization is significantly influenced by the underlying topology, which can be categorized into centralized, hierarchical, and distributed models, each with distinct advantages and challenges.
Centralized models rely on a single computing server for resource allocation, simplifying coordination but facing issues with scalability and bottlenecks. In contrast, distributed topologies offer greater flexibility by enabling the selection of optimal resource parameters, such as computation frequencies, server allocation, and migration paths, to address these challenges. Meanwhile, hierarchical architectures, which utilize hybrid computing resources, facilitate multi-tier control by combining protocol-based and ad-hoc advantages. This approach enhances scalability and supports a more robust and adaptable system.

\subsection{Centralized Offloading: A central Server for Computation}
. 

Centralized offloading refers to a system architecture where a single central server or base station handles all the computational offloading requests from multiple user devices. In this model, the central server is responsible for task execution, and resource allocation. User devices, such as smartphones, IoT devices, or vehicles in a network, offload their tasks to the central server for processing, as they may have limited computational capabilities or power constraints. This architecture, as implemented in \cite{b3}, \cite{b27}, \cite{b32}, \cite{b33}, \cite{b35}, \cite{b36}, \cite{b42}, and \cite{b45}, represents a simplified structure by centralizing offloading decisions and resource management, eliminating the need for complex interactions between user devices.
\subsubsection{\textbf{Key Techniques in Offloading }}
A wide range of techniques such as Zhao et al. \cite{b3} introduced contract theory as an innovative approach to establishing optimal resource allocation agreements between vehicles and edge servers, striking a balance between energy efficiency and computational offloading. Complementing this, Ju et al. \cite{b27} proposed a spectrum-sharing mechanism that facilitates more efficient bandwidth utilization by allowing Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communications to coexist seamlessly. In a different approach, Khan et al. \cite{b32} developed an adaptive mode selection strategy that dynamically shifts offloading tasks between edge and cloud servers based on real-time network conditions and available resources, ensuring flexibility in offloading decisions. %Along the same lines, Feng et al. \cite{b33} enhanced decision-making capabilities under uncertainty by introducing probabilistic action generation within a Deep Q-Network (DQN) framework, providing a more robust response to dynamic environments.

Following strategic development on learning, Zhao et al. \cite{b35} refined offloading strategies further by employing a hierarchical learning framework that integrates convex optimization and DRL, breaking down complex offloading problems into manageable subproblems. Zhang et al. \cite{b36} addressed spectrum efficiency through a cooperative spectrum sensing mechanism, improving the accuracy of spectrum detection and enabling more effective resource sharing. Meanwhile, Guo et al. \cite{b42} focused on optimizing resource allocation by utilizing Quality of Service (QoS) flow estimation, tailoring the allocation based on the specific QoS requirements of different applications. In a more interactive approach, Gao et al. \cite{b45} combined competitive game theory with cooperative multi-agent reinforcement learning (RL), enabling users to engage in both competitive and cooperative interactions to achieve the optimal task offloading decisions. Finally, Zheng et al. \cite{b76} explored Directed Acyclic Graph (DAG)-based task offloading determining the task dependency for terminal devices, employing a single UAV as a computational resource, thus broadening the application of offloading strategies in dynamic environments.




\subsubsection{\textbf{Reinforcement Learning Approaches}}
Different reinforcement learning (RL) paradigms have been employed across various studies to optimize task offloading decisions, each tailored to the unique requirements of their environments. Studies such as \cite{b3}, \cite{b32}, \cite{b33}, and \cite{b42} utilized value iteration techniques, leveraging a central agent and Deep Q-Networks (DQN) to facilitate individual decision-making by agents operating in isolated environments. These approaches are motivated by the effectiveness of DQN in discrete action spaces, allowing agents to learn optimal policies through iterative updates. In contrast, actor-critic methods like Deep Deterministic Policy Gradient (DDPG) were applied in \cite{b35} to manage continuous action spaces and dynamic decision-making, particularly in complex environments where actions require fine-tuned adjustments.

Multi-agent RL approaches have also gained attention, offering solutions for more interactive and cooperative decision-making. For instance, Ju et al. \cite{b27} and Zhang et al. \cite{b36} explored stable value function approximations using Double DQN to enhance performance in multi-agent scenarios. Meanwhile folllowing centralized learning approach,  Gao et al. \cite{b45} implemented multi-agent DDPG to enable cooperative decision-making among multiple agents, particularly in vehicular networks, where coordination is key to optimizing task offloading. Furthermore, Zheng et al. \cite{b76} introduced a multi-agent hierarchical system, employing three agents to address multiple objectives for multi-user terminal devices with a single UAV serving as the computational resource.
\subsubsection{ \textbf{Optimization Objectives}}
Several optimization objectives were pursued across these studies, depending on the specific system requirements:

\paragraph{\textbf{Energy Optimization}}
Reducing energy consumption was a common goal addressed by Zhao et al. \cite{b3}, Khan et al. \cite{b32}, Feng et al. \cite{b33}, Zhao et al. \cite{b35}, and Zhang et al. \cite{b36}. These studies employed techniques such as contract theory, adaptive mode selection, and probabilistic actions to optimize energy usage. Significant improvements in energy efficiency were demonstrated, especially in scenarios with limited power budgets and high computational demands.
\paragraph{\textbf{Latency Optimization}}
Another key objective was minimizing latency, prioritized by Ju et al. \cite{b27}, Zheng et al. \cite{b76}, Guo et al. \cite{b42}, and Gao et al. \cite{b45}. Techniques such as spectrum sharing, QoS-based resource allocation, and cooperative multi-agent RL were implemented to reduce delays in offloading tasks, which proved particularly effective in time-sensitive applications. Additionally, Zhang et al. \cite{b36} focused on optimizing task scheduling to reduce processing queue congestion, which further enhanced system responsiveness by minimizing task delays.
\paragraph{\textbf{Resource Utilization}}
Improving resource utilization was emphasized by Guo et al. \cite{b42} through QoS flow estimation, ensuring that resources were allocated according to the specific needs of each application. This approach was effective in managing diverse workloads with varying resource demands. Similarly, Feng et al. \cite{b33} addressed data rate optimization alongside energy consumption, balancing transmission rates with resource allocation to ensure efficient use of bandwidth without compromising energy efficiency.



\subsubsection{\textbf{Challenges and Limitations}}
Despite the significant contributions made by these studies, several challenges and limitations remain unresolved.
\paragraph{\textbf{MDP Presentation}}
 Khan et al. \cite{b32} overlooked the energy cost associated with the transmission cost i.e terminal device to MEC transmission during cloud offloading in their reward function, which results in an incomplete assessment of energy consumption. Similarly, Feng et al. \cite{b33} did not incorporate task and resource dynamics into their state representation, limiting the model’s ability to adapt to real-time changes in system conditions. Zhao et al. \cite{b3} also oversimplified user-specific and location-specific conditions, neglecting the influence of dynamic environmental factors on offloading decisions. This abstraction could reduce the model’s accuracy in real-world scenarios, where users and edge servers experience constantly changing conditions. Likewise, Gao et al. \cite{b45} failed to model channel and server dynamics, which restricted the model’s ability to accurately reflect network conditions. Proper representation of these dynamics is essential for optimizing task offloading decisions in highly dynamic vehicular networks.
\paragraph{\textbf{Baseline Comparison}}
Several studies faced challenges related to baseline comparisons. Zhang et al. \cite{b36} demonstrated uniform convergence during the learning process, raising concerns about how the model behaves deterministically during initial learning which is expected to be in random exploration through actions. Similarly, Zheng et al. \cite{b76} observed variations in convergence times across their three-agent model, suggesting a need to consider different time scales and exploration trade-offs for each agent when updating and learning their individual objectives. Ju et al. \cite{b27} incorporated fixed network parameters, such as a predefined exploration rate, which led to a more deterministic transition, failing to capture the variability of real-world networks. This limited the adaptability of their solution in dynamic environments. Zhao et al. \cite{b3} did not include baseline comparisons with standard DRL algorithms, which could have provided further insights into the effectiveness of their approach. Additionally, Zhao et al. \cite{b3} omitted detailed evaluations of QoS metrics, only visualizing utility or reward. This lack of focus on latency, throughput, and other QoS factors makes it challenging to fully understand the practical performance trade-offs.
\subsubsection{\textbf{Conclusion}}
Centralized offloading is an effective approach for managing task execution and resource allocation, particularly in systems with limited edge computing power. Various techniques, such as contract theory, spectrum sharing, and QoS flow estimation, have improved energy efficiency, reduced latency, and enhanced overall performance. However, challenges like incomplete MDP representations, lack of baseline comparisons, and limited exploration might weaken the adaptability of the applications to dynamic real-world environments, where task, resource, and channel dynamics are crucial.







% \onecolumn 
\begin{longtable} {|p{0.8cm}|p{0.5cm}|p{1.5cm}|p{1.8cm}|p{3.5cm}|p{2cm}|p{3.7cm}|}
\hline
\multicolumn{1}{|c|}{\makecell{\textbf{Agent} \\ \textbf{Type}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Cite.}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{DRL} \\ \textbf{Method}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Optimization} \\ \textbf{Obj.}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Key} \\ \textbf{Techniques}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Comp.} \\ \textbf{Source}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Remark}}} \\ \hline

\endhead

\multirow{7}{0.1cm}{\textbf{Single Agent}l} & \cite{b3} & \multirow{4} {0.1cm}{DQN} & Energy and Latency & Contract Theory & Vehicle and Fog & more detailed performance metrics and a solid baseline could be emphasized. \\ \cline{2-2} \cline{4-7} 
 & \cite{b32} &  & Energy & Adaptive Offloading Mode selection for MTC IOT device & Edge and Cloud & Energy consumed during the MTCD-to-MEC transmission is overlooked. \\ \cline{2-2} \cline{4-7} 
 & \cite{b33} &  & Energy and data rate & Probabilistic Action generation & Local and edge & Articulate MDP presentation could be highlighted. \\ \cline{2-2} \cline{4-7} 
 & \cite{b42} &  & Delay and Resource Utilization & Application /QoS Based Resource allocation & Edge & Fluctuation in convergence suggests room for MDP and learning improvement. \\ \cline{2-7} 
 & \cite{b35} & DDPG & Energy & Hierarchical Learning (Problem divided in subproblem: optimization and DDPG) & Local and edge & The trade-off between convex optimization and DRL could be elaborated. \\ \cline{2-7} 
 & \cite{b52} & TD3 & Comput. rate & Joint optimization of RIS Phase Shifts and Mobile device Energy Partitioning & Local and edge(BS) & Optimal mode of offloading (direct or via IRS) could be investigated. \\ \cline{2-7} 
 & \cite{b7} & SAC & Latency and Cost & Cooperative Vehicle Selection with Link Duration Estimation & Nearby Vehicles & Analysis for resource starvation of low priority could be conducted. \\ \cline{1-7} 
\multirow{5}{0cm}{Multi-Agent} & \cite{b45} & Game MADDPG (CTDE) & Delay & Competitive Game and Cooperative MARL & Local and Edge & MDP presentation and fairness could be focused. \\  \cline{2-7} 
 & \cite{b78} & Game MAAC (DTDE) & Latency and Computational resource & Differential Neural Computer (DNC) for memorizing past learning  & Edge & Without sharing any learning knowledge and information, how NE was achieved could be well justified with detailed analytical explanation. \\  \cline{2-7} 
 & \cite{b27} & MADDQN (DTDE) & Latency & Spectrum Sharing from V2V, for V2I & Edge (BS) & Training parameter (Exploration rate) in state implies improper state transition.  \\  \cline{2-7} 
 & \cite{b76} & MATD3 (DTDE) & Latency & DAG for UAV & UAV and Local & The work resembles hierarchical distributed deep learning with multiple agents with different objectives.  \\ \cline{2-7} 
 & \cite{b36} & MADQN (DTDE) & Energy and Queue Length & Cooperative Spectrum Sensing Reward & Local and Edge & Uniform convergence of DQN infers insufficient exploration. \\ \hline

\end{longtable} 
% \twocolumn


\subsection{Distributed Offloading: Collaborative Servers for Computation}
Unlike the centralized offloading approach, where a single server controls all task management , in distributed system computational offloading relies on a network of interconnected servers or edge nodes that share the computational burden and coordinate their actions to optimize the system's performance as a whole. This collaborative nature means that servers exchange information about network conditions, available resources, and task requirements to collectively decide how to best handle offloading requests. Each server in a distributed system contributes to a global or local decision-making process, often through centralized single-agent or cooperative learning mechanisms such as distributed optimization, federated learning, or multi-agent reinforcement learning (MARL) i.e the works described in \cite{b4},
\cite{b5}, \cite{b6}, \cite{b7}, \cite{b10}, \cite{b11}, \cite{b12}, \cite{b15}, \cite{b16}, \cite{b19}, \cite{b20}, \cite{b23}, \cite{b24}, \cite{b29}, \cite{b31}, \cite{b41} and \cite{b43}.   

\subsubsection{\textbf{Key Techniques in Offloading}} 
A variety of advanced techniques have been explored to optimize task offloading and resource allocation in distributed environments, with a focus on effective task scheduling and resource server selection. Cooperative vehicle selection, one of key edge resources has been added in resource services by Lv et al. \cite{b5} using historical trajectory data and LightGBM, while Liu et al. \cite{b6} extended this utilizing XGBoost for more robust decision-making and accurate vehicle selections. Consequently, Shi et al. \cite{b7} incorporated link duration estimation to ensure stable communication links during offloading, further improving the process. Beyond improving resource reliability,  Offloading strategies based on task characteristics have been explored in many studies i.e, Tang et al. \cite{b11} proposed dynamic framing for subtask offloading based on real-time conditions, improving allocation efficiency while Uddin et al. \cite{b82} augmented the adaptation of task priority following prioritized task selection strategy to offload the task successfully according to environment state dynamics. 

Task migration has been emphasized in various works, including Li et al. \cite{b15} and Liu et al. \cite{b65}, where the authors considered the dynamicity of task allocation and rescheduling between vehicles through optimization. Following the same path, Yuan et al. \cite{b23} focused on seamless handover and migration within a distributed edge environment to maintain offloading efficiency while Li et al. \cite{b15} combined task partitioning and migration with DRL-based optimization, and Huang et al. \cite{b31} developed a joint model that accounts for task types and vehicle speeds to meet delay constraints.

%Multi-agent learning coordination is another crucial approach to improve offloading i.e. Zhang et al. \cite{b24} utilized graph structures to improve agent collaboration. Similarly, Huang et al. \cite{b29} employed federated MADDPG with interference coordination to optimize decision-making and reduce interference across the network, whereas Wang et al. \cite{b63} implemented a multi-critic and attention mechanism using a VDN network for UAV-assisted offloading. 
In addition to these efforts, learning network structures have been extensively studied for instance Zhang et al. \cite{b62} presented an attention-based bidirectional LSTM network to manage temporal dependencies in low earth orbit (LEO) networks. Gao et al. \cite{b73} applied game-based hierarchical DRL for service assignment and trajectory planning of UAVs, while Yu et al. \cite{b74} introduced action branching mechanisms with separate networks to reduce the dimensionality of the action space. Apart from learning collaborations, user coordination has also been addressed in numerous works. Zhang et al. \cite{b16} applied social-aware content caching to leverage social interactions for more efficient task offloading. Zhao et al. \cite{b19} focused on mobility-aware offloading by dynamically adjusting strategies based on vehicle movement patterns. 

Additionally, communication-assisted decentralized trajectory planning for UAVs was discussed in \cite{b64}. Furthermore, Hou et al. \cite{b61} explored service vehicle clustering in fog zones to utilize vehicle computational resources for V2V and V2R communication in distributed offloading. Various network topologies have also been proposed to enhance system efficiency. For example, Cheng et al. \cite{b68} utilized ultra-dense networks for the joint optimization of task partitioning and resource allocation. Lu et al. \cite{b71} integrated HAP-supported networks for power harnessing and task offloading, whereas Li et al. \cite{b72} explored knowledge-based methods for multi-agent systems in UAV-assisted networks



\subsubsection{\textbf{Reinforcement Learning Agent Approaches}}
In single-agent systems, DQN was widely applied for decision-making, where agents learned optimal policies by updating Q-values based on their interactions with the environment \cite{b4}, \cite{b5}. In \cite{b82}, a variation of DQN with adaptive priority was adopted to emphasize strict latency. However, for better training stability, DDQN was often used to mitigate overestimation issues, leading to more accurate decision-making \cite{b10}, \cite{b11}, and \cite{b12}. Building on this foundation, Dueling Double DQN (D3QN) was implemented to further enhance stability and address reward fluctuations by separating the value and advantage streams within the network architecture \cite{b6}. Moving beyond value-based methods, policy gradient approaches such as SAC (Soft Actor-Critic) were utilized to handle more complex environments that required continuous action spaces, such as cooperative vehicle selection with link duration estimation \cite{b7}. 

In multi-agent systems, decentralized training and decentralized execution-oriented MADQN had been implemented in \cite{b23}, incorporating a migration and handover mechanism. Beyond decentralized and isolated learning, a centralized approach, for example, MADDPG (CTDE), had been implemented in many works. For instance, \cite{b29} focused on federated learning and interference coordination, whereas \cite{b31} applied a joint task type and vehicle speed-aware delay constraint model. Similarly, \cite{b68} and \cite{b73} implemented the same centralized approach with a global state-action sharing strategy for ultra-dense networks. Following the same objective, \cite{b24} employed a coordinated graph-based MADDPG system for distributed agent collaboration, optimizing communication and coordination in decentralized environments.

Moreover, \cite{b41} used MAAC (CTDE), incorporating a cost-revenue model for task completion with agent-specific contracts and user significance as key decision factors in the centralized training phase. The Counterfactual Multi-Agent (COMA) approach, under the centralized training, decentralized execution (CTDE) framework, had been adopted in \cite{b61} for vehicular networks, in \cite{b62} for satellite networks, and in \cite{b65} for edge user networks. Additionally, the value decomposition network strategy (CTDE), such as AC mix and MA2DDPG, had been implemented in \cite{b63} for UAV bandwidth and trajectory control, while actor-critic based on value-decomposition networks (VDN) had been applied, following an encoding and intention module in \cite{b64}. Similarly, multi-agent QMix, a centralized learning approach of value decomposition, had been used in \cite{b74}. Finally, some decentralized approaches (DTDE) had been explored, i.e., \cite{b66}, leveraging the concept of observing the POMDP as an MDP, the integration of DDPG and SAC for multi-agent DRL in \cite{b71}, and MASAC in knowledge-guided exploration in \cite{b72}.


\subsubsection{\textbf{Optimization Objectives}}


\paragraph{\textbf{Latency}}
Digital Twin technology minimized delays in local and edge environments by creating real-time virtual models, which enhanced system responsiveness \cite{b4}. Subtask offloading, implemented with dynamic framing, addressed delays by flexibly redistributing tasks across local, edge, and base station (BS) environments, improving task execution efficiency \cite{b11}. Additionally, handover-enabled migration techniques enhanced task migration efficiency and reduced delays while optimizing cost management in distributed edge systems \cite{b23}, \cite{b65}. Furthermore, service vehicle operations were optimized in \cite{b61} by employing regional and cross-regional vehicles to minimize delays in offloading scenarios.

Similarly, \cite{b66} utilized non-cooperative multi-agent DRL to optimize latency and aggregated costs. Meanwhile, inverse latency optimization \cite{b73} and latency difference optimization \cite{b74} were applied in UAV-based trajectory control and service offloading, respectively. Cooperative vehicle selection using XGBoost represented as an effective technique in optimizing resource allocation for nearby vehicles, resulting in smoother communication and task execution \cite{b6}. Additionally, link duration estimation ensured both latency and cost efficiency, particularly in vehicle-to-vehicle communication scenarios, where timely decision-making was crucial \cite{b7}. Mobility-aware dependent task offloading further optimized latency and energy consumption, especially in local and Roadside Unit (RSU) environments, by dynamically adapting to changing mobility conditions \cite{b19}.

\paragraph{\textbf{Duel Objective: Energy and Latency Optimization}}

Shared results with an ID pool balance latency, energy, and cost across local, edge, and base station (BS) systems, providing a more efficient distribution of tasks and resources \cite{b10}. Similarly, handover-enabled strategies enhance both energy and latency optimization, particularly in vehicle and RSU systems, ensuring smooth task migration while managing these metrics effectively \cite{b12}. Federated Multi-Agent Deep Deterministic Policy Gradient (MADDPG), coupled with interference coordination, also plays a critical role in optimizing energy and latency, especially in RSU setups, where agent coordination is vital \cite{b29}. Additionally, DDPG and SAC are integrated to jointly optimize energy and latency in \cite{b71} while \cite{b82} integrated prioritized reward with DQN to optmize latency and energy focusing on task completion ratio. 

Moreover, a joint task type and vehicle speed-aware delay constraint model enhances energy and latency management across local and nearby vehicles, improving system performance in dynamic environments \cite{b31}. Similarly, \cite{b62} addresses delay-constrained energy optimization, while \cite{b68} focuses on joint latency and energy optimization with an emphasis on computation maximization.Link duration estimation serves a dual purpose in vehicle-to-vehicle communications by optimizing both cost and latency, ensuring that tasks are processed efficiently with minimal expense \cite{b7}. Additionally, delay-constrained migration techniques further enhance cost optimization by minimizing transformation efforts and streamlining resource usage in local and RSU systems, making them highly effective for real-time edge computing scenarios \cite{b23}.


\paragraph{\textbf{QoS and QoE Improvement}}
To improve Quality of Service (QoS), user significance, cost, and revenue models are implemented in edge environments, ensuring that user demands are met while maintaining system profitability \cite{b41}. For Quality of Experience (QoE), heterogeneous task offloading strategies cater to users by optimizing offloading decisions based on task types, providing more tailored and responsive system performance \cite{b43}. Additionally, data rate optimization has been addressed in \cite{b63} as a QoS metric, while fairness concerns have been tackled in \cite{b64}. Similarly, \cite{b72} approaches latency-constrained fairness optimization objectives for multi-agent UAV offloading and trajectory control, enhancing system performance under constrained conditions.



\subsubsection{\textbf{Challenges and Limitations}}

\paragraph{\textbf{MDP representation}}

Xu et al. \cite{b4} focused on data size, latitude, and longitude but overlooked critical communication dynamics like transmission speed, channel gain, and path loss, limiting the robustness of their state representation. This is similar to \cite{b62}, where the state considered the visibility matrix of the satellite network but neglected channel dynamics. Likewise, Hou et al. \cite{b61} illustrated the selection of service vehicles in the action space, but state details about the service vehicles were not presented, creating ambiguity in learning, particularly regarding the zone the service vehicle belongs to and the required communication service (e.g., V2V, V2R) based on coverage.

Similarly, Xu et al. \cite{b4} and Liu et al. \cite{b65} did not account for essential factors such as transmission speed and channel conditions, which are critical for capturing the full scope of user communication state transitions. Moreover, Peng et al. \cite{b10} experienced instability due to fluctuating rewards, suggesting the need for a more observable Markov Decision Process (MDP). Zhao et al. \cite{b19} also faced challenges where task priorities were determined only after offloading decisions, leading to inefficient resource allocation due to the lack of task urgency in the initial decision-making process. Wang et al. \cite{b63} highlighted that fixed and very strict penalties, such as those for SNR, boundary, and collision violations, can disproportionately dominate the reward. Given the logarithmic growth of communication capacity, this led to unbalanced decision-making, as these penalties became overly influential in the learning process.

  
\paragraph{\textbf{Algorithmic fairness and result comparison}}
Lv et al. \cite{b5} compared DQN with SARSA in a way that appeared biased, as SARSA was implemented using a tabular method rather than a deep reinforcement learning (DRL) framework, skewing the results and rendering the comparison inadequate. Similarly, Tang et al. \cite{b11} noted a performance gap between DDQN and Dueling DQN but failed to provide a sufficient explanation for why Dueling DQN did not experience the same level of performance improvement, leaving this disparity unexplained. Liu et al. \cite{b6} did not offer a detailed comparison of the performance differences between PATO and other schemes, such as D3DQN, making it difficult to identify the specific factors contributing to performance gains, particularly why PATO outperforms D3QN. Shi et al. \cite{b7} omitted key metrics such as energy consumption and the number of tasks handled, limiting the evaluation of system efficiency. Similarly, Peng et al. \cite{b10} encountered fluctuations in reward curves, indicating unstable learning, while Huang et al. \cite{b29, b31} faced inconsistencies in their results due to varying priority levels, which led to unclear performance analysis.

Shi et al. \cite{b7} also found that their dynamic pricing model performed similarly to baseline greedy approaches when local computational resources were sufficient, casting doubt on the value of their offloading scheme. Zhao et al. \cite{b19} struggled with agents making short-sighted decisions based on immediate rewards, which could result in suboptimal long-term outcomes. These challenges highlight the need for more robust methods in state representation, reward design, fairness, communication management, and energy efficiency in task offloading frameworks for vehicular networks. Li et al. \cite{b72} illustrated a promising knowledge-guided exploration technique for the MASAC system, but where expert knowledge is unavailable, the solution could benefit from further investigation with more advanced approaches. Gao et al. \cite{b73} proposed game-based service allocation and DRL-based trajectory planning, but both approaches utilized the same communication model, overlooking the impact of DRL actions on UAV positioning. Additionally, Yu et al. \cite{b74} presented action branching in QMix, but convergence curves revealed insufficient exploration of certain baselines, indicating the need for more in-depth analysis.

\paragraph{\textbf{Deterministic and Stochastic Policy Tradeoff}}

Many works, such as \cite {b15}, \cite {b29} and \cite {b31}, based on deterministic policy approaches, fail to address the challenges associated with discrete action spaces. One critical issue is the proper discretization of actions in a deterministic manner, which remains a significant challenge. Moreover, the added noise in deterministic policies, aimed at obtaining discrete actions and ensuring exploration, requires greater emphasis. A more focused exploration mechanism could strengthen the applicability and reliability of these works in discrete action spaces 


\paragraph{\textbf{Synchronization and  Coordination }}
 
Maleki et al. \cite{b12} encountered difficulties with task synchronization during partial offloading, raising concerns about the coordination and reliability of offloading decisions. Similarly, Zhou et al. \cite{b43} faced challenges managing multiple users selecting the same server, which is a critical issue in distributed environments and can lead to resource contention. Deng et al. \cite{b20} investigated LSTM feature selection by including all state data but overlooked targeted (relevant) features, which could lead to inefficiencies in state representation. Tan et al. \cite{b64} illustrated that while information sharing can improve decision-making by providing additional context, it does not guarantee that agents will make perfectly cooperative decisions. In a different approach, Heydari et al. \cite{b66} investigated non-cooperative mechanisms by transforming POMDP into MDP. However, the assumption of a finite state space for channels and waiting time might be problematic in capturing the actual transition dynamics and the stationary distribution of the environment.

For central coordination, Cheng et al. \cite{b68} used DDPG to share global state and actions between agents, effectively coordinating decisions. However, the study's baseline was also DDPG, which may require a clearer presentation of how single-agent MDPs are compared with multi-agent MDPs. While Lu et al. \cite{b71} integrated DDPG and SAC for energy harnessing and latency optimization in task offloading, the final convergence performance exhibited inefficiencies, suggesting the need for further investigation into the learning strategies and MDP formulation.








\subsubsection{\textbf{Conclusion}}
In summary, distributed offloading strategies in vehicular networks and edge computing environments offer significant potential for enhancing system efficiency, reducing latency, and optimizing resource allocation. By leveraging collaborative frameworks, such as multi-agent systems and federated learning, these approaches distribute the computational load and decision-making processes across interconnected servers, addressing bottlenecks and central node failures. Despite these advancements, several challenges remain such as incomplete state representations limits the robustness of decision-making processes, particularly when critical factors like communication dynamics, transmission speed, and channel gain are overlooked. Reward function design often presents issues, particularly when scaling metrics like task priority and latency, leading to unstable learning and skewed outcomes. Additionally, algorithmic fairness and adequate comparisons between different reinforcement learning methods are frequently lacking, leaving gaps in performance evaluation. Moreover, resource contention and synchronization difficulties persist, especially in distributed multi-agent environments, where multiple users compete for the same resources. 


% \onecolumn 
\begin{longtable} {|p{0.8cm}|p{0.5cm}|p{1.5cm}|p{1.8cm}|p{3.5cm}|p{2cm}|p{3.7cm}|}
\hline
\multicolumn{1}{|c|}{\makecell{\textbf{Agent} \\ \textbf{Type}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Cite.}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{DRL} \\ \textbf{Method}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Optimization} \\ \textbf{Obj.}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Key} \\ \textbf{Techniques}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Computational} \\ \textbf{Source}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Remark}}} \\ \hline

\endhead

\multirow{13}{0.1cm}{\centering Single Agent} & \cite{b4} & \multirow{2}{0.01cm}{DQN} & Delay & Digital Twin & Local   and edge & Communication state could be enhanced in MDP  \\ \cline{2-2} \cline{4-7} 
 & \cite{b5} &  & Data   Rate and resource & Cooperative Vehicle selection with historic trajectory and LightGBM & Nearby Vehicles & Standard Baseline comparison could be emphasized, i.e., DQN compared with SARSA. \\ \cline{2-2} \cline{4-7} 
 & \cite{b82} &  & Latency, Energy, and Completion Ratio & Prioritized Task Selection for Offloading  & Edge & User Fairness and Quality of Satisfaction/Experience could be investigated through MARL. \\ \cline{2-7} 

 & \cite{b10} & \multirow{2}{0.01cm}{DDQN}  & Latency, Energy, and Cost & Shared Result with ID pool & Local, Edge, and BS & Fluctuation in final episode implies insufficient learning and improper stationary distribution of state. \\ \cline{2-2} \cline{4-7} 
 & \cite{b11} &  & Delay & Subtask Offloading with Dynamic Framing & Local, Edge, and BS & Elaborated explanation might help to understand the performance between two closely related RL methods, i.e., D3QN and DDQN. \\ \cline{2-2} \cline{4-7}  
 & \cite{b12} &  & Energy and Latency & Handover enabled & Local, Nearby Vehicles, and Edge & Task dependencies and task segmentation could be investigated. \\ \cline{2-7} 
 & \cite{b6} & \multirow{2}{0.01cm}{D3QN} & Latency and Resource & Cooperative Vehicle Selection with XGBoost & Nearby Vehicles & Deeper analytical comparison could enhance performance advantage. \\ \cline{2-2} \cline{4-7} 
 & \cite{b43} &  & QoE & Heterogeneous Task Offloading (task type) & Local and edge & Performance evaluation could be observed in an episodic manner, as time-slot-based evaluation does not show convergence. \\ \cline{2-7} 

 & \cite{b15} & \multirow{4}{0.01cm}{DDPG} & Latency & Task Partitioning and Migration through Optimization, Decision DRL & Edge & Explanation of DDPG action, i.e., discrete to continuous action conversion, could be emphasized. \\ \cline{2-2} \cline{4-7} 
 & \cite{b16} &  & Latency & Social-Aware Content Caching & Vehicle and Edge & MDP could be underscored, reflecting actual communication and computation dynamics. \\ \cline{2-2} \cline{4-7} 
 & \cite{b19} &  & Energy and Latency & Mobility-Aware Dependent Task Offloading & Local and RSU & Relative improvements in reward and priority after taking action may direct toward a suboptimal solution. \\ \cline{2-2} \cline{4-7} 
 & \cite{b20} &  & Energy and Latency & CNN, GNN, LSTM used to calculate Vehicle Trust value to avoid DDoS attack & Local and Edge & Could be investigated with targeted (relevant) features using LSTM, ignoring all features of states. \\ \cline{2-7} 
 & \cite{b53} & TD3 & Energy and Latency & Cooperative RSU Migration and Load Transfer & Local and edge & Edge load information has been overlooked in MDP state. \\ \cline{1-7} 

\multirow{8}{0.01cm}{Multi Agent} & \cite{b23} & MADQN (DTDE) & Delay-Constrained Migration Cost and Transformation Cost & Handover/Migration enabled & Local and Edge & Route efficiency could be focused along with successful trip time. \\ \cline{2-7} 
  
 & \cite{b29} & \multirow{3}{0.01cm}{MADDPG (CTDE)} & Energy and Latency & Federated MADDPG and Interference Coordination & Local and Edge & Could focus on SBS-based performance metrics to validate uniform performance. \\ \cline{2-2} \cline{4-7} 
 & \cite{b31} &  & Energy and Latency & Joint Task Type and Vehicle Speed-Aware Delay Constraint Model & Local, Nearby Vehicles, Edge & Performance evaluation might require justification with consistent notation to ensure clarity and accuracy in the results. \\ \cline{2-2} \cline{4-7} 
 & \cite{b68} &  & Computation Bit, Energy, and Latency & Joint optimization of task partitioning and resource allocation & Local and Edge & Performance baseline DDPG needs explanation of the MDP as a single agent compared to MADDPG. \\ \cline{2-7} 

 & \cite{b24} &  & Latency-Constrained Cost & Coordinated Graph from Multi-Agent Coordination & Nearby Vehicles & Collaboration among aggregated groups could be illustrated. \\ \cline{2-7} 
 & \cite{b73} & \multirow{3}{0.01cm}{Game MADDPG (CTDE)} & Latency and Fairness (bits) & Game approach hierarchical RL & Local and UAV & Use of the same communication model for game and DRL model might not reflect the position changes of UAV by DRL. \\ \cline{2-7} 

 & \cite{b41} & MAAC (CTDE) & QoS & User Significance, Cost, and Revenue Agent & Local and Edge & Elaboration of conflicting actions might be focused. \\ \cline{2-7} 

 & \cite{b71} & MA-DDPG + SAC (CTDE) & Latency and Energy & Integrated MADDPG and SAC & Local and Edge (HAP) & Significant fluctuation in the final episode suggests the requirement of MDP and learning method improvement. \\ \cline{2-7} 

 & \cite{b72} & MASAC (CTDE) & Latency and Fairness & Knowledge-driven and Exploration Guidance & Local and UAV & Knowledge-driven exploration-based (KMASAC) has more directional search of solutions than more dynamic MADDPG. \\ \cline{2-7} 

 & \cite{b66} & MAA2C (CTDE) & Latency & Non-cooperative environment transformed from POMDP to MDP & Local and Edge & The assumption of a finite state for channel and waiting time may not accurately capture the true transition dynamics of the environment. \\ \cline{2-7} 

 & \cite{b61} & \multirow{3}{0.01cm}{COMA (CTDE)} & Latency & Service Vehicle Clustering and Cross-Regional Offloading & Service Vehicle & Improper MDP such as action ambiguity and lack of clustering details. \\ \cline{2-2} \cline{4-7} 
 & \cite{b65} &  & Latency & Task Migration & Edge & Only task data in the state suggests the model cannot fully capture the broader system dynamics, such as network conditions and resource contention. \\ \cline{2-2} \cline{4-7} 
 & \cite{b62} &  & Energy & Attention-based bidirectional long short-term memory network & Satellite & Improper MDP such as overlooking of link channel state. \\ \hline

\end{longtable}

% \twocolumn


\subsection{\textbf{Hiererchial Offloading: Multier, and Heterogeneous Servers}}

Distributed edge computing, though flexible due to decentralized decision-making, faces challenges as networks scale. Increased device numbers and task complexity lead to coordination issues, higher latency, and inefficiencies, especially in dynamic environments. Each node's independent operation results in communication overhead and redundancy. Hierarchical edge computing offers a more efficient and flexible solution by leveraging heterogeneous servers at different levels. Heterogenous servers like intercommunicating nearby vehicles, and fog nodes collaborate, allowing tasks to be offloaded locally for low-latency needs or to remote or nearby servers for resource-heavy operations. This layered approach optimizes resource use, improves system performance, and reduces the burden on individual devices i.e \cite{b1},\cite{b2},\cite{b8},\cite{b13},\cite{b14},\cite{b17},\cite{b18},\cite{b21},\cite{b22},\cite{b25},\cite{b28},\cite{b30},\cite{b34}, \cite{b44} and \cite{b46}.


\subsubsection{\textbf{Key techniques in Offloading }}
Various strategies have been developed to enhance task offloading and resource optimization in hierarchical, heterogeneous environments, with a strong focus on improving task scheduling and server selection. Learning coordination plays a critical role in these strategies. For example, DDPG combined with FCNN and Convex Optimization in \cite{b34} had been used to optimize energy efficiency and task offloading, while centralized training with distributed action approaches in \cite{b30} and \cite{b22} aimed to improve task distribution across systems. 

In IoT environments, strategies like the Independent IoT Device Agent in \cite{b38} optimize task allocation based on energy requirements and task volume, ensuring that IoT devices operate efficiently without system overload. To improve security, Trust Value for Vehicles was calculated in \cite{b2}, incorporating social awareness to enable safe and reliable offloading decisions. Similarly, the Improved Clustering Algorithm based on social relations in \cite{b14} enhanced resource allocation by grouping tasks according to social interactions, leading to more effective computational resource utilization. In dynamic environments, predicting communication states is critical for effective offloading. Techniques like LSTM and BRNN in \cite{b25} predict future communication states, improving the accuracy of offloading decisions, while Seq2Seq-based BiGRU in \cite{b28} managed sequential sub-tasks and extracts environmental and task-specific features for better decision-making. Cooperative Edge Offloading in \cite{b18} leveraged cooperation among nearby edge nodes to optimize the offloading process, while joint action learning was implemented in \cite{b67} for offloading tasks from cloud centers to the edge.

Following the fairness perspective,  Comparison Model in \cite{b46} promoted equity by minimizing disparities in task completion times across vehicles or users. Meanwhile, the use of Directed Acyclic Graphs (DAG) and Experience Trajectory Sharing in \cite{b44} improved task scheduling efficiency by sharing task trajectories among agents, enhancing task execution in multi-agent systems. Knowledge-driven approaches like model sharing with new users and historical knowledge-driven learning in \cite{b69} provided a foundation for improving offloading efficiency, while \cite{b70} scales rewards based on task priority, ensuring higher rewards for higher-priority tasks. To reduce unnecessary overhead, Adaptive Video Quality Enhancement in \cite{b13} minimized video quality adjustments, conserving bandwidth and computational resources. Moreover, a multi-objective formulation had been adopted for UAV networks in \cite{b75}, balancing energy, latency, and other objectives effectively.

\subsubsection{\textbf{Reinforcement Learning Approaches}}
In single-agent systems, DQN and its variants are widely employed to enhance offloading and resource allocation. For instance, Distributed DQN \cite{b8}, \cite{b22} focused on learning from distributed edge and UAV resources, while Double DQN \cite{b2} integrated trust values into offloading decisions, emphasizing security. DDPG, when combined with enhancements like FCNN and Convex Optimization \cite{b34}, achieved a balance in energy efficiency. Additionally, adaptive video quality transmission \cite{b13} and improved clustering algorithms based on social interactions \cite{b14} leveraged environmental factors to improve offloading performance, with cooperative edge processing \cite{b18} further optimizing the process. Moreover, the application of the Ornstein-Uhlenbeck Noise Vector to DDPG \cite{b17} enhanced action exploration, while Prime Dual DDPG \cite{b21} efficiently allocated tasks between local and remote servers. A3C \cite{b46} ensured fairness in task scheduling by minimizing delays and balancing offloading across tasks, while Distributed PPO \cite{b44} shared experience trajectories among agents to enhance overall performance, and similarly PPO \cite{b28} was applied to manage sequential tasks, ensuring smooth coordination between agents.

In multi-agent systems, federated DQN (DTDE) \cite{b1} allows distributed agents to learn local models while sharing insights for global optimization. Although this approach explores federated learning in decentralized networks, a more uniform MDP could enhance consistency in learning. Independent IoT device agents \cite{b38} manage task and energy distribution, with agents making independent decisions but coordinating to resolve task conflicts. Double DQN (DTDE) \cite{b2} incorporates trust value for security, combining latency and energy optimization with security protocols, adding a trust-based decision layer to the traditional DRL approach. Following the motivation for distributed learning, joint action Q-learning is implemented in \cite{b67}, while SAC (DTDE) \cite{b59} incorporates fairness mechanisms into decision-making, ensuring equitable resource distribution and balancing user success rates, with further exploration of user fairness potential.

On the other hand, DDPG (CTDE) \cite{b25} combined advanced techniques like LSTM and BRNN for communication state prediction, leveraging sequential models to enhance state prediction and incorporating priority-based decision-making. Similarly, \cite{b30} and \cite{b70} applied centralized training and distributed execution (CTDE) MADDPG with a focus on latency reduction, using a coefficient-based latency and priority reward model to dynamically adjust actions during task execution. Lastly, decentralized learning through A3C \cite{b69} had been implemented for multi-agent learning, where a central critic and individual agent critics share network weights without sharing MDP information, similar decentralized Multi-agent actor-critic methods had also been adopted in \cite{b75}, further exploring decentralized learning strategies.

\subsubsection{\textbf{Optimization Objectives:}}

\paragraph{\textbf{Latency}}

Strategies such as cooperative edge offloading \cite{b18} and Prime Dual DDPG \cite{b21} reduced processing delays by streamlining task distribution between edge and cloud resources. Additionally, various approaches, including \cite{b14}, \cite{b28}, \cite{b30}, \cite{b44}, \cite{b67}, \cite{b69}, and \cite{b46}, focused on minimizing latency through optimized task scheduling and resource allocation within hierarchical architectures, ensuring that tasks are executed efficiently and promptly.

\paragraph{\textbf{Duel Objective: Latency, and Energy }} A number of methods focused on optimizing both energy efficiency and latency to improve system performance. For instance, works such as \cite{b1}, \cite{b2}, \cite{b25}, and \cite{b8} implemented energy-conscious offloading strategies that showcased lower energy consumption while simultaneously reducing task completion times. Additionally, \cite{b38} addressed energy and task management in IoT devices by minimizing energy usage and ensuring timely task execution. Similarly, \cite{b34} prioritized energy optimization as a primary objective, while \cite{b75} optimized both energy and latency with a focus on completing tasks efficiently.

\paragraph{\textbf{ QoS and QoE Improvement }} Moreover, the joint optimization of latency, data rate, and bandwidth were a key goal in several  for assuring QoS and QoE. For example, \cite{b17} addressed the trade-offs between these factors, particularly in dynamic edge environments. Similarly, \cite{b22} focuses on minimizing latency and optimizing data rates to ensure efficient task processing and smooth communication in high-traffic environments. Techniques like adaptive video quality transmission also had been presented in  \cite{b13} to balance latency and bandwidth usage, facilitating seamless video streaming in edge computing setups. On the other hand, channel access was utilized for latency optimization in \cite{b70}.

\subsubsection{\textbf{Challenges and Limitations:}}

\paragraph{\textbf{MDP Presentation}} Several studies have struggled with incomplete or insufficient state representations, which impacted decision-making and overall performance. For example, Khayyat et al. \cite{b8} did not account for transmission and computation dynamics in their state data, leading to ambiguity in state transitions. Similarly, Xue et al. \cite{b18} omitted temporal features in their edge caching decisions, which limited the long-term efficiency of task processing. Zhang et al. \cite{b21} overlooked important transmission dynamics, such as fading channel gain, reducing the precision of their task offloading decisions. Additionally, Li et al. \cite{b34} failed to incorporate task dynamics, resource dynamics, and channel dynamics, all of which are essential for ensuring adaptability and optimization in real-world environments.

\paragraph{\textbf{Synchronization and Coordination}}  Synchronization among agents or networks posed significant challenges in several studies. Khayyat et al. \cite{b8} did not provide a clear mechanism for synchronizing parallel policy networks, which led to potential inconsistencies in policy learning across agents. Chouikhi et al. \cite{b38} lacked a coordination mechanism among IIoT devices, raising concerns about system efficiency and scalability as the number of devices and server conflicts increased. Goudarzi et al. \cite{b44} did not propose a resource contention management strategy, such as TDMA or FDMA, to handle simultaneous offloading requests, leading to bottlenecks when multiple users selected the same server. Similarly, Zhang et al. \cite{b67} approached joint action Q-learning but raised concerns about fair learning among agents, as simulation results depicted selfish behavior in individual agents rather than coordination. Likewise, Qi et al. \cite{b69} presented A3C for multi-agent learning, but their approach focused on weight sharing rather than sharing actual information, which is a significant bottleneck in achieving effective cooperative multi-agent learning.

\paragraph{\textbf{Reward Function and Optimization Trade-offs:}} Reward function design presented another common issue. Ke et al. \cite{b17} raised concerns about squaring energy and latency values in their reward function, which diminished their impact when the values were less than one. Gao et al. \cite{b25} struggled with appropriately scaling task priority and latency metrics in their reward function, which risked skewing the learning process. Similarly, the priority weight in the reward function by Cao et al. \cite{b70} led to reward scaling, but it did not show a significant impact on performance, as the results for latency, energy, and agent-specific outcomes were not adequately presented.

 

\paragraph{\textbf{Lack of Clarity}} Some studies failed to address scalability and real-world constraints. For instance, Chouikhi et al. \cite{b38} did not account for bandwidth limitations or communication delays between IIoT devices and edge servers, which are critical considerations in large-scale deployments. Similarly, Li et al. \cite{b34} neglected task arrival rates and waiting times at the server, both of which are essential factors in real-world task offloading systems. Zheng et al. \cite{b2} lacked clarity in their Double DQN implementation regarding the incorporation of transmission dynamics, which could have compromised the accuracy of offloading decisions. Similarly, Gao et al. \cite{b25} encountered a lack of clarity in scaling task priority and latency in their reward function, which affected the robustness of the state representation. Xu et al. \cite{b13} faced difficulties adapting DDPG for discrete actions in mobile VR services, and their lack of explanation for state transitions further impacted decision-making accuracy. Zhao et al. \cite{b14} left unresolved questions about the validity period of clustering aggregation and whether DDPG updated its state after each aggregation.

Moreover, Dai et al. \cite{b28} did not decouple dependent and independent subtasks, potentially limiting the flexibility and scalability of their task offloading model. Mishra et al. \cite{b1} failed to explain the necessity of uniform state representation in each fog zone, despite the varying number of fog servers and RSUs. For Federated Learning, achieving aggregation from all zones required a uniform network structure, but this was not clearly discussed. Additionally, \cite{b75} raised the need for a clearer explanation and analytical review regarding the UAV height increase, which depicted higher task completion. This aspect should be explored in greater depth, as increasing UAV height typically leads to higher path loss, potentially impacting task completion efficiency.

\subsubsection{\textbf{Conclusion}}
Hierarchical offloading presents a scalable solution to the limitations of distributed systems by coordinating tasks across multiple layers of devices,cooperated vehicles , fog nodes, and cloud servers. It balances low-latency offloading with the computational demands of resource-intensive tasks by efficiently distributing workloads across heterogeneous environments. While various DRL optimization strategies  models and cooperative learning techniques, have enhanced system performance, challenges remain. Issues such as incomplete MDP representations, synchronization, reward function design, and real-world scalability constraints must be addressed to fully realize the potential of hierarchical offloading. 


% \onecolumn
\begin{longtable} {|p{0.8cm}|p{0.5cm}|p{1.5cm}|p{1.8cm}|p{3.5cm}|p{2cm}|p{3.7cm}|}
\hline
\multicolumn{1}{|c|}{\makecell{\textbf{Agent} \\ \textbf{Type}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Cite.}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{DRL} \\ \textbf{Method}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Optimization} \\ \textbf{Obj.}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Key} \\ \textbf{Techniques}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Comp.} \\ \textbf{Source}}} & 
\multicolumn{1}{c|}{\makecell{\textbf{Remark}}} \\ \hline

\endhead

\multirow{14}{0.01cm}{Single Agent} & \cite{b8} & DQN & Energy and Latency & Distributed RL & Local, Edge, and Cloud & The MDP and parallel policy learning could be elaborated to fully capture transmission and computation dynamics. \\ \cline{2-2} \cline{4-7} 
 & \cite{b22} &  & Latency and Data Rate & Centralized and Distributed Frameworks of DRL & Local, UAV, Cloud & Modeling data rate without real-time channel state information might raise concerns. \\ \cline{2-7} 
 & \cite{b34} & \multirow{6}{0.01cm}{DDPG} & Energy & Hybrid DRL (DDPG, FCNN, and Convex Optimization) & Local, Edge, and Cloud & Incomplete MDP, i.e., task dynamics, resource dynamics, and channel dynamics are not included in MDP. \\ \cline{2-2} \cline{4-7} 
 & \cite{b13} &  & Latency and Bandwidth & Adaptive Video Quality Transmission & Local, Edge, Cloud & Explanation of DDPG action, i.e., discrete-to-continuous action conversion, could be emphasized. \\ \cline{2-2} \cline{4-7} 
 & \cite{b14} &  & Latency & Improved Clustering Algorithm Based on Social Relation & Local, Edge, Cloud, and Vehicles & Clarification could be included about the validity period of clustering aggregation and whether DDPG updated its state after each aggregation. \\ \cline{2-2} \cline{4-7} 
 & \cite{b17} &  & Energy, Bandwidth, and Latency & Ornstein-Uhlenbeck Noise Vector in Action for DDPG & Local, Edge, and Cloud & Squaring elements in the reward function may lead to disproportionate penalization. \\ \cline{2-2} \cline{4-7} 
 & \cite{b18} &  & Latency & Cooperative Edge & Local, Edge, and Cloud & Lack of temporal features in the MDP framework for effective caching may lead to suboptimal long-term performance. \\ \cline{2-2} \cline{4-7} 
 & \cite{b21} &  & Latency & Prime Dual DDPG & Local, Edge, and Cloud & Results and MDP could be improved with more details. \\ \cline{2-7} 
 & \cite{b51} & \multirow{4}{0.01cm}{TD3} & Latency Optimization & Edge-Assisted Learning Approach & Local, Edge, and Cloud & Fixed set of tasks suggests limited load and state dynamics. \\ \cline{2-2} \cline{4-7} 
 & \cite{b54} &  & Peak Age of Information (PAoI) & UAV Cloud and Optimization of Age of Information & Local, Edge, UAV & Continuous action conversion demonstrates ambiguity, i.e., overlapping of two spaces. \\ \cline{2-2} \cline{4-7} 
 & \cite{b55} &  & Resource Utilization Efficiency & Hierarchical DRL for Service Function Chain Offloading & Local, Edge, and Cloud & The trade-off of convergence speed between heuristics and TD3 could be investigated. \\ \cline{2-2} \cline{4-7} 
 & \cite{b56} &  & Joint Optimization of Delay and Energy & Mobility-Aware with Dynamic Location & Local and Edge & MDP state representation demonstrates lack of linkage between tasks and task vehicles. \\ \cline{2-7} 
 & \cite{b46} & A3C & Latency & Fairness with Relative Delay Comparison & Local, Edge, Cloud & Exclusion of channel parameters in state suggests improper MDP. \\ \cline{2-7} 
 & \cite{b44} & \multirow{2}{0.01cm}{PPO} & Latency & Distributed DRL and Experience Trajectory & Local, Edge, Cloud & Multi-user conflict resolution mechanism could be addressed. \\ \cline{2-2} \cline{4-7}
  & \cite{b28} &  & Latency & Seq2Seq-based BiGRU for Sequential Features & Local, Edge, Cloud & Decoupling of dependent and independent tasks in learning could be assessed. \\ \cline{2-7} 
 & \cite{b58} & \multirow{2}{0.01cm}{SAC} & Latency and Energy & Entropy Normalization SAC & Local and UAV & Exclusion of channel parameters in state suggests improper MDP. \\ \cline{2-2} \cline{4-7} 
 & \cite{b57} &  & Service Latency & Centralized and Decentralized Agent & Local, Edge, Cloud & Imbalanced performance in some distributed agents suggests unfair learning. \\ \cline{1-7} 

\multirow{9}{0.01cm}{Multi Agent} & \cite{b1} & \multirow{3}{0.01cm}{MADQN (DTDE)} & Energy and Latency & Federated DQN & Local, Edge, Cloud & Uniform MDP presentation could be investigated for federated learning. \\ \cline{2-2} \cline{4-7} 
 & \cite{b38} &  & Energy and Number of Tasks & Independent IoT Device Agent & Local, Edge, Cloud & Model might improve multi-agent coordination and conflict resolution. \\  \cline{2-7} 
 & \cite{b2} & MADDQN (DTDE) & Energy and Latency & Trust Value for Security & Local, Edge, Cloud & Clarification of Double DQN could be enhanced for better illustration of performance. \\ \cline{2-7}  
 & \cite{b67} & MA Joint Action Q-Learning (DTDE) & Latency & Joint Action Q-Learning & Local, Edge, Access Point & Simulation results of excessive latency of non-agent users (cloud center) raise concerns about fair learning across agents. \\ \cline{2-7} 
 & \cite{b25} & \multirow{2}{0.01cm}{MADDPG (CTDE)} & Latency and Energy & LSTM and BRNN for Communication State Prediction & Local, Edge, Cloud & The need for scaling the reward function could be explored. \\ \cline{2-2} \cline{4-7} 
 & \cite{b30} &  & Latency & Centralized Training and Distributed Action & Local, Edge, Cloud & Performance metrics, including latency, could be examined. \\ \cline{2-2} \cline{4-7} 
 & \cite{b70} &  & Latency and Channel Access & Prioritized Reward & Local, Edge, Cloud & Scaling of reward suggests more resources for high priority, which could cause unbalanced resource allocation. \\ \cline{2-7} 

 & \cite{b59} & MASAC (DTDE) & Latency and Success Rate & Fairness & Local, Edge & Inter-fairness among users could be studied. \\ \cline{2-7} 

 & \cite{b69} & MAA3C (DTDE) & Delay & Knowledge-Driven Model & Local, Edge, and BS & The MARL section described only decentralized A3C, suggesting an explanation about multi-agent coordination. \\ \cline{2-7} 
 & \cite{b75} & MAAC (CTDE) & Latency and Energy & Multi-Objective Formulation & UAV and Edge & UAV height increase depicts higher task completion; this needs to be explored as an increase in height suggests more path loss. \\  \hline

\end{longtable}

% \twocolumn




\section{Learned Lesson, Open Issues and Future Research Aspects}



\subsection{\textbf{Fully Informed MDP and Multiagent POMDP:}}
A comprehensive state representation is crucial for effective decision-making in dynamic environments like edge computing and vehicular networks. Incomplete MDP formulations that neglect key dynamics such as transmission rates, task processing times, resource availability, and temporal dependencies, particularly in caching mechanisms, lead to ambiguous state transitions and suboptimal decisions. Multi-agent systems face additional challenges due to partial observability, where agents lack full state-space awareness, leading to suboptimal decisions and conflicting actions . The exponential growth of the state space (curse of dimensionality) complicates state representation, making optimal decision-making computationally expensive . Overlapping observations between agents can lead to redundant information, further complicating coordination. To  mitigate those challenges in POMDP and accessibility dificulties to states , proper estimation of belief states with Bayesian inference could enhance the actual state presentation even with reduced communication overhead. 

\subsection{\textbf{ Game theory Approach challenges:}} 

Game theory has been widely applied to offloading problems, often leveraging Nash Equilibrium (NE) and Stackelberg Equilibrium (SE) for optimal decision-making. However, a key question remains underexplored: how to effectively integrate MDP-based DRL within game-theoretic frameworks, particularly in scenarios where agents must determine the optimal schedule for updating their policies while accounting for the fixed strategies of opponents. Additionally, the impact of agent deviations from equilibrium in result demonstartion is often overlooked, raising concerns about system stability and efficiency. Some studies have employed hierarchical approaches that combine game theory and DRL to address subproblems, but a major challenge lies in synchronizing their learning processes, as game-theoretic methods typically converge faster than DRL. Addressing these challenges could serve as a strong foundation for future research in game-theoretic DRL approaches.




\subsection {\textbf{Uniform State Action Representation in Federated and Multi-Agent Learning:}}
Establishing a uniform state size representation and action space in both federated and multi-agent learning environments is significantly important \cite{b1}. This uniformity is essential for effective aggregation and coordination, ensuring scalability and generalization across different zones or agents. The absence of such uniformity can lead to asymmetrical learning, where discrepancies in state and action representations result in inconsistent model updates and inefficiencies in learning processes. This misalignment can prevent models from different zones or agents from properly integrating their learning experiences, thereby degrading overall system performance. Future research should investigate methods to standardize inputs and outputs or employ meta-learning approaches to address heterogeneity, thereby enhancing robustness and practical applicability. 




\subsection{\textbf{Handling  DAG, Task and Data Dependencies:}}
In the context of vehicular networks and task offloading, the application of the Directed Acyclic Graph (DAG) approach to map task dependencies presents an important area of study, particularly in differentiating dependent and independent subtasks . Dependent tasks, relying on others’ outputs, could be addressed through sequential learning methods like Seq2Seq models, while independent subtasks may benefit from parallel processing. Exploring how these distinctions impact computational efficiency and resource allocation could optimize task offloading strategies by balancing sequential and parallel processing for adaptability and scalability in dynamic environments. Additionally, recognizing the distinction between temporal (e.g., location changes, speed) and non-temporal features (e.g., task size, wireless channel state) can improve learning efficiency. Temporal features, best captured using methods like LSTMs, provide insights over time, while non-temporal features contribute differently to decision-making. Decoupling these types of features allows for more focused learning, enhancing both the capture of temporal dynamics and the overall system's adaptability. This approach ensures a comprehensive understanding of system behavior, improving decision-making in dynamic environments.

\subsection {\textbf{Reliable Baseline Comparisons and Results}}
Providing comprehensive baseline comparisons is essential for conducting fair and reliable evaluations in reinforcement learning research. Benchmarking new algorithms against established methods allows for a clear understanding of their effectiveness across diverse scenarios, ensuring practical applicability and robustness. However, numerous studies face challenges with inconsistent or incomplete baseline comparisons, which can skew results and obscure the true impact of new methods. In multi-agent reinforcement learning (MARL), the absence of dedicated multi-agent baselines further complicates evaluations, as the complexities and dynamics inherent to multi-agent systems are not fully captured, leading to biased assessments . Additionally, the omission of essential performance metrics like energy consumption, task efficiency, and system throughput limits the ability to assess models holistically, reducing the depth of their evaluations . Incomplete data prevents a thorough analysis of how models perform across varying conditions, impacting the ability to generalize findings.

Furthermore, unstable learning processes, characterized by reward fluctuations and inconsistent prioritization, reduce the reliability of models . Inadequate exploration strategies, such as fixed exploration rates or uniform convergence that fail to adapt to dynamic environments, weaken the adaptability and robustness of the models in real-world applications . These challenges highlight the importance of incorporating thorough baseline comparisons, flexible exploration strategies, and comprehensive performance metrics to ensure more accurate, scalable, and reliable reinforcement learning evaluations in dynamic environments.


\subsection {\textbf{Reward Function}}
Reward function scaling concerning different performances, such as latency and energy, is crucial for effective learning . For instance, squaring the energy or latency in the reward function can lead to disproportionate reward changes, which can undermine learning effectiveness. Proper scaling ensures balanced reward representation, supporting better learning outcomes. Moreover, the inclusion of energy and latency terms without considering any coefficient leads to an imbalance in reward contribution, which undermines the joint objective of optimizing latency and energy. 

Moreover, a subject of investigation is the impact of relative reward improvements in consecutive time steps that could risk agents focusing on short-term gains at the expense of long-term outcomes, particularly in environments with fluctuating conditions, such as varying traffic and intermittent communication. To mitigate this, enhancing reward signals with more frequent and informative feedback could help agents distinguish between transient conditions and stable trends. The reliance on relative gains may reinforce short-term improvements, while absolute rewards based on overarching system goals, such as minimizing latency or maximizing throughput, would better prevent agents from becoming stuck in local optima. Investigating the incorporation more measurable techniques i.e moving averages into reward functions is necessary to balance short-term fluctuations and long-term optimization. This adjustment could lead to more stable, efficient performance in dynamic environments, ensuring both immediate and sustained improvements.



\subsection {\textbf{Synchronization and Coordination among Multi Agent}}
One of the key lessons learned from the reviewed studies is the critical importance of synchronization and coordination in multi-agent systems and distributed networks. When synchronization mechanisms are absent or poorly defined, inefficiencies and scalability challenges often arise, particularly as system complexity increases. Agents that fail to align their actions effectively can create bottlenecks, especially in environments where multiple agents or devices attempt to offload tasks or access shared resources simultaneously, leading to suboptimal system performance. Another crucial takeaway is the need for effective resource contention management. In scenarios where multiple users offload tasks to the same server or share communication channels, the absence of strategies like TDMA or FDMA can result in conflicts, causing delays and reducing task execution efficiency. Proper mechanisms for managing shared resources are essential to prevent such issues and ensure smooth system operation. Finally, central coordination strategies that rely on global state information can enhance decision-making by aligning actions across the system. However, these methods must be carefully balanced to address scalability challenges as the number of agents might increase in multi-agent systems. To mitigate these issues, adaptive coordination frameworks, dynamic resource allocation techniques, and decentralized learning strategies can be explored to enhance efficiency and scalability.

\subsection {\textbf{Handling Continuous and Discrete Action Spaces:}}
Deterministic policy grdaient method like DDPG, PPO , and TRPO  provide a deterministic policy, producing a single action rather than a probability distribution, making it well-suited for continuous action spaces. However, applying DDPG to discrete action spaces presents challenges, as it inherently generates continuous outputs and relies on adding noise (e.g., Ornstein-Uhlenbeck or Gaussian noise) for exploration. This noise can lead to frequent switching between discrete actions, potentially unbalalnce exportation , destabilizing learning and reducing convergence efficiency. A key concern in such implementations is ensuring balanced exploration across all possible actions, as improper exploration can result in biased action selection, limiting the model's ability to learn an optimal policy. Moreover, the misalignment between discrete and continuous action spaces can lead to suboptimal performance, requiring careful action space design. Focusing on this specific concern could be a potential strt to work in Deterministic policy gradient implementation for discrete action space. 

\subsection {\textbf{Multi Objective Multi agent Learning:}}
In a multi-objective multi-agent system, decision dependencies between agents require careful coordination within a unified global time step to ensure efficient task execution. Actions taken by one agent, such as a vehicle offloading a task to a base station (BS), directly impact subsequent decisions by other agents, such as the BS determining whether to process the task locally or offload it further. Without synchronized decision-making, agents may operate asynchronously, leading to inefficiencies, increased latency, or resource misallocation. For instance, if a BS offloads a task to a UAV, but the UAV's learning process is unsynchronized, the task may remain idle, causing delays and disrupting the overall workflow.

To mitigate these challenges, implementing a global time step can help synchronize decision-making, ensuring agents interact efficiently and maintain system stability. Additionally, alternative solutions such as event-triggered coordination, where agents make decisions based on state changes rather than fixed time steps, can enhance responsiveness. Asynchronous learning with adaptive delays can also be explored, allowing agents to adjust their update frequency based on network conditions or computational load. These approaches help balance synchronization while maintaining flexibility, ultimately improving multi-agent coordination in dynamic environments.



\section{Conclusion}
In conclusion, this research underscores the importance of several key factors in enhancing learning and decision-making in dynamic, multi-agent, and federated environments. Uniform state representation and action spaces are essential for scalability and coordination, while comprehensive baseline comparisons and well-informed state representations ensure robust evaluation and decision-making. The precise handling of reward functions, especially in balancing short-term and long-term objectives, is crucial. Additionally, integrating real-time data, promoting reward sharing in multi-agent systems, and ensuring fair comparisons with appropriate baselines are vital for improving system performance. Addressing these challenges paves the way for more effective and scalable learning frameworks in complex, dynamic environments.


\begin{thebibliography}{00}

\bibitem{b201} I. W. Damaj, J. K. Yousafzai and H. T. Mouftah, "Future Trends in Connected and Autonomous Vehicles: Enabling Communications and Processing Technologies," in IEEE Access, vol. 10, pp. 42334-42345, 2022, doi: 10.1109/ACCESS.2022.3168320.


\bibitem{b2021} K. Zheng, Q. Zheng, P. Chatzimisios, W. Xiang and Y. Zhou, "Heterogeneous Vehicular Networking: A Survey on Architecture, Challenges, and Solutions," in IEEE Communications Surveys \& Tutorials, vol. 17, no. 4, pp. 2377-2396, Fourthquarter 2015, doi: 10.1109/COMST.2015.2440103.

\bibitem{b202} Y. Mao, C. You, J. Zhang, K. Huang and K. B. Letaief, "A Survey on Mobile Edge Computing: The Communication Perspective," in IEEE Communications Surveys \& Tutorials, vol. 19, no. 4, pp. 2322-2358, Fourthquarter 2017, doi: 10.1109/COMST.2017.2745201.



\bibitem{b203} J. Liu et al., "RL/DRL Meets Vehicular Task Offloading Using Edge and Vehicular Cloudlet: A Survey," in IEEE Internet of Things Journal, vol. 9, no. 11, pp. 8315-8338, 1 June1, 2022, doi: 10.1109/JIOT.2022.3155667.
\bibitem{ba07} P. Yang, N. Zhang, S. Zhang, L. Yu, J. Zhang, X. Shen, "Content popularity prediction towards location-aware mobile edge caching", IEEE Transactions on Multimedia, vol.21, no.4, pp.915-929, 2018. 

\bibitem{ba06} L. Ale, N. Zhang, H. Wu, D. Chen, T. Han, "Online proactive caching in mobile edge computing using bidirectional deep recurrent neural network", IEEE Internet of Things Journal, vol.6, no.3, pp.5520--5530, 2019. 

\bibitem{b204} L. Hou, M. A. Gregory and S. Li, "A Survey of Multi-Access Edge Computing and Vehicular Networking," in IEEE Access, vol. 10, pp. 123436-123451, 2022, doi: 10.1109/ACCESS.2022.3224032.
\bibitem{b1071} L. A. Haibeh, M. C. E. Yagoub and A. Jarray, "A Survey on Mobile Edge Computing Infrastructure: Design, Resource Management, and Optimization Approaches," in IEEE Access, vol. 10, pp. 27591-27610, 2022, doi: 10.1109/ACCESS.2022.3152787.

\bibitem{b207} A. Filali, A. Abouaomar, S. Cherkaoui, A. Kobbane and M. Guizani, "Multi-Access Edge Computing: A Survey," in IEEE Access, vol. 8, pp. 197017-197046, 2020, doi: 10.1109/ACCESS.2020.3034136.

\bibitem{b208} Y. Yu, "Mobile edge computing towards 5G: Vision, recent progress, and open challenges," in China Communications, vol. 13, no. Supplement2, pp. 89-99, 2016, doi: 10.1109/CC.2016.7833463.

\bibitem{b2081} A. Alawadhi, A. Almogahed and E. Azrag, "Towards Edge Computing for 6G Internet of Everything: Challenges and Opportunities," 2023 1st International Conference on Advanced Innovations in Smart Cities (ICAISC), Jeddah, Saudi Arabia, 2023, pp. 1-6, doi: 10.1109/ICAISC56366.2023.10085007. 

\bibitem{b209} K. Jurczenia and J. Rak, "A Survey of Vehicular Network Systems for Road Traffic Management," in IEEE Access, vol. 10, pp. 42365-42385, 2022, doi: 10.1109/ACCESS.2022.3168354.


\bibitem{b210} E. Moradi-Pari, D. Tian, M. Bahramgiri, S. Rajab and S. Bai, "DSRC Versus LTE-V2X: Empirical Performance Analysis of Direct Vehicular Communication Technologies," in IEEE Transactions on Intelligent Transportation Systems, vol. 24, no. 5, pp. 4889-4903, May 2023, doi: 10.1109/TITS.2023.3247339.

\bibitem{b211} S. Wang, J. Xu, N. Zhang and Y. Liu, "A Survey on Service Migration in Mobile Edge Computing," in IEEE Access, vol. 6, pp. 23511-23528, 2018, doi: 10.1109/ACCESS.2018.2828102.

\bibitem{b214} Y. Xu, W. Liang, and L. Wang, "Deep Reinforcement Learning for Resource Management in Vehicular Networks," IEEE Wireless Communications, vol. 28, no. 2, pp. 120-126, April 2021.

\bibitem{b101} W. M. Danquah and D. T. Altilar, "Vehicular Cloud Resource Management, Issues and Challenges: A Survey," in IEEE Access, vol. 8, pp. 180587-180607, 2020, doi: 10.1109/ACCESS.2020.3027637.

\bibitem{b2071} N. Abbas, Y. Zhang, A. Taherkordi and T. Skeie, "Mobile Edge Computing: A Survey," in IEEE Internet of Things Journal, vol. 5, no. 1, pp. 450-465, Feb. 2018, doi: 10.1109/JIOT.2017.2750180. 

\bibitem{b152} W. Shi, J. Cao, Q. Zhang, Y. Li and L. Xu, "Edge Computing: Vision and Challenges," in IEEE Internet of Things Journal, vol. 3, no. 5, pp. 637-646, Oct. 2016, doi: 10.1109/JIOT.2016.2579198.
\bibitem{b2072} H. Nouhas, A. Belangour and M. Nassar, "Cloud and Edge Computing Architectures: A Survey," 2023 IEEE 11th Conference on Systems, Process \& Control (ICSPC), Malacca, Malaysia, 2023, pp. 210-215, doi: 10.1109/ICSPC59664.2023.10420123.

\bibitem{b1072} 1.	Deepak, M. K. Upadhyay and M. Alam, "Edge Computing: Architecture, Application, Opportunities, and Challenges," 2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS), Tashkent, Uzbekistan, 2023, pp. 695-702, doi: 10.1109/ICTACS59847.2023.10390171. 

\bibitem{b151} B. Liu, Z. Luo, H. Chen and C. Li, "A Survey of State-of-the-art on Edge Computing: Theoretical Models, Technologies, Directions, and Development Paths," in IEEE Access, vol. 10, pp. 54038-54063, 2022, doi: 10.1109/ACCESS.2022.3176106.

\bibitem{b83} S. Singh and D. Ho Kim, ``Profit optimization for mobile edge computing using genetic algorithm," \emph{IEEE Region 10 Symposium (TENSYMP)}, 2021.

\bibitem{b85} T. Zhou, Y. Yue, D. Qin, X. Nie, X. Li, and C. Li, ``Mobile device association and resource allocation in HCNs with mobile edge computing and caching," \emph{IEEE Systems Journal}, vol. 17, no. 1, pp. 976-987, March 2023.

\bibitem{b88} M. Zakarya et al., ``epcAware: A game-based, energy, performance and cost-efficient resource management technique for multi-access edge computing," \emph{IEEE Transactions on Services Computing}, vol. 15, no. 3, pp. 1634-1648, 1 May-June 2022.

\bibitem{b2073} T. -H. Nguyen and L. Park, "A Survey on Deep Reinforcement Learning-driven Task Offloading in Aerial Access Networks," 2022 13th International Conference on Information and Communication Technology Convergence (ICTC), Jeju Island, Korea, Republic of, 2022, pp. 822-827, doi: 10.1109/ICTC55196.2022.9952687.

\bibitem{b2074} W. Chen, X. Qiu, T. Cai, H. -N. Dai, Z. Zheng and Y. Zhang, "Deep Reinforcement Learning for Internet of Things: A Comprehensive Survey," in IEEE Communications Surveys \& Tutorials, vol. 23, no. 3, pp. 1659-1692, thirdquarter 2021, doi: 10.1109/COMST.2021.3073036.



\bibitem{b175} I. Althamary, C. -W. Huang and P. Lin, "A Survey on Multi-Agent Reinforcement Learning Methods for Vehicular Networks," 2019 15th International Wireless Communications \& Mobile Computing Conference (IWCMC), Tangier, Morocco, 2019, pp. 1154-1159, doi: 10.1109/IWCMC.2019.8766739. 
\bibitem{b176} H. Zhu, Y. Cao, W. Wang, T. Jiang and S. Jin, "Deep Reinforcement Learning for Mobile Edge Caching: Review, New Features, and Open Issues," in IEEE Network, vol. 32, no. 6, pp. 50-57, November/December 2018, doi: 10.1109/MNET.2018.1800109.

\bibitem{b174} T. Li et al., "Applications of Multi-Agent Reinforcement Learning in Future Internet: A Comprehensive Survey," in IEEE Communications Surveys \& Tutorials, vol. 24, no. 2, pp. 1240-1279, Secondquarter 2022, doi: 10.1109/COMST.2022.3160697.

\bibitem{b102} L. Liu, C. Chen, Q. Pei, S. Maharjan, and Y. L. Zhang, “Vehicular edge computing and networking: A survey,” Mobile Netw. Appl., vol. 26, no. 3, pp. 1145–1168, 2021.

\bibitem{b104} P. Mach and Z. Becvar, "Mobile Edge Computing: A Survey on Architecture and Computation Offloading," in IEEE Communications Surveys $\&$ Tutorials, vol. 19, no. 3, pp. 1628-1656, thirdquarter 2017, doi: 10.1109/COMST.2017.2682318.
\bibitem{b103} M. Muniswamaiah, T. Agerwala and C. C. Tappert, "Fog Computing and the Internet of Things (IoT): A Review," 2021 8th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2021 7th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom), Washington, DC, USA, 2021, pp. 10-12, doi: 10.1109/CSCloud-EdgeCom52276.2021.00012.


\bibitem{b105} X. Huang, R. Yu, J. Liu and L. Shu, "Parked Vehicle Edge Computing: Exploiting Opportunistic Resources for Distributed Mobile Applications," in IEEE Access, vol. 6, pp. 66649-66663, 2018, doi: 10.1109/ACCESS.2018.2879578.

\bibitem{b1061} Y. Bai, H. Zhao, X. Zhang, Z. Chang, R. Jäntti and K. Yang, "Toward Autonomous Multi-UAV Wireless Network: A Survey of Reinforcement Learning-Based Approaches," in IEEE Communications Surveys \& Tutorials, vol. 25, no. 4, pp. 3038-3067, Fourthquarter 2023, doi: 10.1109/COMST.2023.3323344.

\bibitem{b149} Y. Mao, C. You, J. Zhang, K. Huang and K. B. Letaief, "A Survey on Mobile Edge Computing: The Communication Perspective," in IEEE Communications Surveys \& Tutorials, vol. 19, no. 4, pp. 2322-2358, Fourthquarter 2017, doi: 10.1109/COMST.2017.2745201.

\bibitem{b150} X. Hou, Y. Li, M. Chen, D. Wu, D. Jin and S. Chen, "Vehicular Fog Computing: A Viewpoint of Vehicles as the Infrastructures," in IEEE Transactions on Vehicular Technology, vol. 65, no. 6, pp. 3860-3873, June 2016, doi: 10.1109/TVT.2016.2532863.



\bibitem{b153} J. Cui, L. Wei, H. Zhong, J. Zhang, Y. Xu and L. Liu, "Edge Computing in VANETs-An Efficient and Privacy-Preserving Cooperative Downloading Scheme," in IEEE Journal on Selected Areas in Communications, vol. 38, no. 6, pp. 1191-1204, June 2020, doi: 10.1109/JSAC.2020.2986617.



\bibitem{b154} M. A. Al-shareeda, M. A. Alazzawi, M. Anbar, S. Manickam and A. K. Al-Ani, "A Comprehensive Survey on Vehicular Ad Hoc Networks (VANETs)," 2021 International Conference on Advanced Computer Applications (ACA), Maysan, Iraq, 2021, pp. 156-160, doi: 10.1109/ACA52198.2021.9626779. 


\bibitem{b155} L. N. T. Huynh and E. -N. Huh, "UAV-Enhanced Edge Intelligence: A Survey," 2022 6th International Conference on Computing Methodologies and Communication (ICCMC), Erode, India, 2022, pp. 42-47, doi: 10.1109/ICCMC53470.2022.9753892. 

\bibitem{b156} C. Lin, G. Han, S. B. H. Shah, Y. Zou and L. Gou, "Integrating Mobile Edge Computing Into Unmanned Aerial Vehicle Networks: An Sdn-Enabled Architecture," in IEEE Internet of Things Magazine, vol. 4, no. 4, pp. 18-23, December 2021, doi: 10.1109/IOTM.001.2100070.



\bibitem{b205}  Y. Xu, G. Gui, H. Gacanin and F. Adachi, "A Survey on Resource Allocation for 5G Heterogeneous Networks: Current Research, Future Trends, and Challenges," in IEEE Communications Surveys \& Tutorials, vol. 23, no. 2, pp. 668-695, Secondquarter 2021, doi: 10.1109/COMST.2021.3059896.


\bibitem{b131}  R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018.
\bibitem{b132} R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957.
\bibitem{b134} M. Riedmiller, "Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method," in Proc. 16th European Conf. Machine Learning (ECML 2005), pp. 317-328, 2005.

\bibitem{b135} V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, "Playing Atari with Deep Reinforcement Learning," arXiv preprint arXiv:1312.5602, 2013.

\bibitem{b136} H. van Hasselt, A. Guez, and D. Silver, "Deep Reinforcement Learning with Double Q-learning," arXiv preprint arXiv:1509.06461, 2015.
\bibitem{b139}  V. R. Konda and J. N. Tsitsiklis, "Actor-Critic Algorithms," in Advances in Neural Information Processing Systems (NIPS), 1999.
\bibitem{b142} J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel, "Trust Region Policy Optimization," in Proceedings of the 32nd International Conference on Machine Learning (ICML 2015), Lille, France, Jul. 2015.
\bibitem{b143} J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal Policy Optimization Algorithms," arXiv preprint arXiv:1707.06347, 2017.

\bibitem{b140} T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor," in Proceedings of the 35th International Conference on Machine Learning (ICML 2018), Stockholm, Sweden, Jul. 2018.
\bibitem{b141} T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforcement learning," arXiv preprint arXiv:1509.02971, 2015.
\bibitem{b157} S. Fujimoto, H. van Hoof, and D. Meger, "Addressing function approximation error in actor-critic methods," in Proc. 35th Int. Conf. Mach. Learn. (ICML), Stockholm, Sweden, Jul. 2018, pp. 1392–1401. [Online]. Available: arXiv:1802.09477.
\bibitem{b146} P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, ... and D. Silver, "Value-Decomposition Networks For Cooperative Multi-Agent Learning," in Proc. 17th Int. Conf. Autonomous Agents and MultiAgent Systems, 2018, pp. 2085-2087.

\bibitem{b147}  R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments," in Advances in Neural Information Processing Systems, 2017, pp. 6379-6390.


\bibitem{b162} A. M. Seid, G. O. Boateng, S. Anokye, T. Kwantwi, G. Sun and G. Liu, "Collaborative Computation Offloading and Resource Allocation in Multi-UAV-Assisted IoT Networks: A Deep Reinforcement Learning Approach," in IEEE Internet of Things Journal, vol. 8, no. 15, pp. 12203-12218, 1 Aug.1, 2021, doi: 10.1109/JIOT.2021.3063188.


\bibitem{b163} T. Cai et al., "Cooperative Data Sensing and Computation Offloading in UAV-Assisted Crowdsensing With Multi-Agent Deep Reinforcement Learning," in IEEE Transactions on Network Science and Engineering, vol. 9, no. 5, pp. 3197-3211, 1 Sept.-Oct. 2022, doi: 10.1109/TNSE.2021.3121690.

\bibitem{b164}  J. Cui, Y. Liu, and A. Nallanathan, “Multi-agent reinforcement learning-based resource allocation for UAV networks,” IEEE Trans. Wireless Commun., vol. 19, no. 2, pp. 729–743, Feb. 2020.

%nonstationary
\bibitem{b165} P. Hernandez-Leal, M. Kaisers, T. Baarslag, and E. Munoz de Cote, "A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity," arXiv preprint arXiv:1707.09183, Jul. 2017. [Online]. Available: https://arxiv.org/abs/1707.09183

%Scalibility
%\bibitem{b166} K. Gogineni, P. Wei, T. Lan, and G. Venkataramani, "Scalability Bottlenecks in Multi-Agent Reinforcement Learning Systems," arXiv preprint arXiv:2302.05007, Feb. 2023. [Online]. Available: https://arxiv.org/abs/2302.05007


\bibitem{b166} K. Gogineni, P. Wei, T. Lan, and G. Venkataramani, “Scalability Bottlenecks in Multi-Agent Reinforcement Learning Systems,” arXiv
preprint arXiv:2302.05007, 2023..

\bibitem{b167} S. V. Macua, J. Chen, S. Zazo, and A. H. Sayed, “Distributed policy evaluation under multiple behavior strategies,” IEEE Trans. Autom. Control, vol. 60, no. 5, pp. 1260–1274, May 2015.
\bibitem{b144} R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-agent actor-critic for mixed cooperative-competitive environments,” 2017.
\bibitem{b169} J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual multi-agent policy gradients,” in Proc. AAAI Conf. Artif. Intell., vol. 32, 2018, pp. 1–10.

\bibitem{b172} P. Sunehag et al., “Value-decomposition networks for cooperative multi-agent learning,” 2017, arXiv:1706.05296.

\bibitem{b168} K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized multi-agent reinforcement learning with networked agents,” in Proc. Int. Conf. Mach. Learn., 2018, pp. 5872–5881.

\bibitem{b170} I. Lee and D. K. Kim, "Decentralized Multi-Agent DQN-Based Resource Allocation for Heterogeneous Traffic in V2X Communications," in IEEE Access, vol. 12, pp. 3070-3084, 2024, doi: 10.1109/ACCESS.2023.3349350.

\bibitem{b171} K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized multi-agent reinforcement learning with networked agents,” in Proc. Int. Conf. Mach. Learn., 2018, pp. 5872–5881.

\bibitem{b179} G. Jain, A. Kumar and S. A. Bhat, "Recent Developments of Game Theory and Reinforcement Learning Approaches: A Systematic Review," in IEEE Access, vol. 12, pp. 9999-10011, 2024, doi: 10.1109/ACCESS.2024.3352749.
\bibitem{b180} Z. Yan, H. Zhao, L. Li and S. Galland, "Evolutionary game theory combined with reinforcement learning synthesis - A comprehensive survey," 2024 36th Chinese Control and Decision Conference (CCDC), Xi'an, China, 2024, pp. 1077-1082, doi: 10.1109/CCDC62350.2024.10587977. 

%Single Server
\bibitem{b3} J. Zhao, M. Kong, Q. Li and X. Sun, "Contract-Based Computing Resource Management via Deep Reinforcement Learning in Vehicular Fog Computing," in IEEE Access, vol. 8, pp. 3319-3329, 2020, doi: 10.1109/ACCESS.2019.2963051.

\bibitem{b27} Y. Ju et al., "Joint Secure Offloading and Resource Allocation for Vehicular Edge Computing Network: A Multi-Agent Deep Reinforcement Learning Approach," in IEEE Transactions on Intelligent Transportation Systems, vol. 24, no. 5, pp. 5555-5569, May 2023, doi: 10.1109/TITS.2023.3242997.

\bibitem{b32}  I. Khan, X. Tao, G. M. S. Rahman, W. U. Rehman and T. Salam, "Advanced Energy-Efficient Computation Offloading Using Deep Reinforcement Learning in MTC Edge Computing," in IEEE Access, vol. 8, pp. 82867-82875, 2020, doi: 10.1109/ACCESS.2020.2991057.
\bibitem{b33} C. Wang, W. Lu, S. Peng, Y. Qu, G. Wang and S. Yu, "Modeling on Energy-Efficiency Computation Offloading Using Probabilistic Action Generating," in IEEE Internet of Things Journal, vol. 9, no. 20, pp. 20681-20692, 15 Oct.15, 2022, doi: 10.1109/JIOT.2022.3175760.
\bibitem{b35} S. Zhao, Y. Liu, S. Gong, B. Gu, R. Fan and B. Lyu, "Computation Offloading and Beamforming Optimization for Energy Minimization in Wireless-Powered IRS-Assisted MEC," in IEEE Internet of Things Journal, vol. 10, no. 22, pp. 19466-19478, 15 Nov.15, 2023, doi: 10.1109/JIOT.2023.3265011.

\bibitem{b36} X. Zhang, A. Pal and S. Debroy, "Deep Reinforcement Learning Based Energy-efficient Task Offloading for Secondary Mobile Edge Systems," 2020 IEEE 45th LCN Symposium on Emerging Topics in Networking (LCN Symposium), Sydney, Australia, 2020, pp. 48-59, doi:10.1109/LCNSymposium50271.2020.9363256.
\bibitem{b42} B. Guo, X. Zhang, Y. Wang and H. Yang, "Deep-Q-Network-Based Multimedia Multi-Service QoS Optimization for Mobile Edge Computing Systems," in IEEE Access, vol. 7, pp. 160961-160972, 2019, doi: 10.1109/ACCESS.2019.2951219.
\bibitem{b45} A. Gao, S. Zhang, Y. Hu, W. Liang and S. X. Ng, "Game-Combined Multi-Agent DRL for Tasks Offloading in Wireless Powered MEC Networks," in IEEE Transactions on Vehicular Technology, vol. 72, no. 7, pp. 9131-9144, July 2023, doi: 10.1109/TVT.2023.3250274.
\bibitem{b52} J. Xu, B. Ai, L. Wu, Y. Zhang, W. Wang and H. Li, "Deep Reinforcement Learning for Computation Rate Maximization in RIS-Enabled Mobile Edge Computing," in IEEE Transactions on Vehicular Technology, vol. 73, no. 7, pp. 10862-10866, July 2024, doi: 10.1109/TVT.2024.3387759.

\bibitem{b76} C. Zheng et al., "Multi-Agent Collaborative Optimization of UAV Trajectory and Latency-Aware DAG Task Offloading in UAV-Assisted MEC," in IEEE Access, vol. 12, pp. 42521-42534, 2024, doi: 10.1109/ACCESS.2024.3378512.
\bibitem{b78} Y. Zhan, S. Guo, P. Li and J. Zhang, "A Deep Reinforcement Learning Based Offloading Game in Edge Computing," in IEEE Transactions on Computers, vol. 69, no. 6, pp. 883-893, 1 June 2020, doi: 10.1109/TC.2020.2969148.

 

%Distribute Server


\bibitem{b5} B. Lv, C. Yang, X. Chen, Z. Yao and J. Yang, "Task Offloading and Serving Handover of Vehicular Edge Computing Networks Based on Trajectory Prediction," in IEEE Access, vol. 9, pp. 130793-130804, 2021, doi: 10.1109/ACCESS.2021.3112077.

\bibitem{b6} J. Liu, N. Liu, L. Liu, S. Li, H. Zhu and P. Zhang, "A Proactive Stable Scheme for Vehicular Collaborative Edge Computing," in IEEE Transactions on Vehicular Technology, vol. 72, no. 8, pp. 10724-10736, Aug. 2023, doi: 10.1109/TVT.2023.3255213.

\bibitem{b7} J. Shi, J. Du, J. Wang, J. Wang, and J. Yuan, “Priority-aware task offloading in vehicular fog computing based on deep reinforcement learning,” IEEE Trans. Veh. Technol., vol. 69, no. 12, pp. 16067–16081, Dec. 2020. 
\bibitem{b11} H. Tang, H. Wu, G. Qu and R. Li, "Double Deep Q-Network Based Dynamic Framing Offloading in Vehicular Edge Computing," in IEEE Transactions on Network Science and Engineering, vol. 10, no. 3, pp. 1297-1310, 1 May-June 2023, doi: 10.1109/TNSE.2022.3172794.
\bibitem{b82} A. Uddin, A. H. Sakr and N. Zhang, "Prioritized Task Offloading in Vehicular Edge Computing Using Deep Reinforcement Learning," 2024 IEEE 99th Vehicular Technology Conference (VTC2024-Spring), Singapore, Singapore, 2024, pp. 1-6, doi: 10.1109/VTC2024-Spring62846.2024.10683050. 


\bibitem{b4} X. Xu et al., "Service Offloading With Deep Q-Network for Digital Twinning-Empowered Internet of Vehicles in Edge Computing," in IEEE Transactions on Industrial Informatics, vol. 18, no. 2, pp. 1414-1423, Feb. 2022, doi: 10.1109/TII.2020.3040180.

\bibitem{b10} X. Peng et al., "Deep Reinforcement Learning for Shared Offloading Strategy in Vehicle Edge Computing," in IEEE Systems Journal, vol. 17, no. 2, pp. 2089-2100, June 2023, doi: 10.1109/JSYST.2022.3190926.


\bibitem{b12} H. Maleki, M. Başaran and L. Durak-Ata, "Handover-Enabled Dynamic Computation Offloading for Vehicular Edge Computing Networks," in IEEE Transactions on Vehicular Technology, vol. 72, no. 7, pp. 9394-9405, July 2023, doi: 10.1109/TVT.2023.3247889.

\bibitem{b15} M. Li, J. Gao, L. Zhao and X. Shen, "Deep Reinforcement Learning for Collaborative Edge Computing in Vehicular Networks," in IEEE Transactions on Cognitive Communications and Networking, vol. 6, no. 4, pp. 1122-1135, Dec. 2020, doi: 10.1109/TCCN.2020.3003036.
\bibitem{b16} K. Zhang, J. Cao, H. Liu, S. Maharjan and Y. Zhang, "Deep Reinforcement Learning for Social-Aware Edge Computing and Caching in Urban Informatics," in IEEE Transactions on Industrial Informatics, vol. 16, no. 8, pp. 5467-5477, Aug. 2020, doi: 10.1109/TII.2019.2953189.

\bibitem{b19} L. Zhao et al., "MESON: A Mobility-Aware Dependent Task Offloading Scheme for Urban Vehicular Edge Computing," in IEEE Transactions on Mobile Computing, vol. 23, no. 5, pp. 4259-4272, May 2024, doi: 10.1109/TMC.2023.3289611.
\bibitem{b20} Y. Deng et al., "Resource Provisioning for Mitigating Edge DDoS Attacks in MEC-Enabled SDVN," in IEEE Internet of Things Journal, vol. 9, no. 23, pp. 24264-24280, 1 Dec.1, 2022, doi: 10.1109/JIOT.2022.3189975.
\bibitem{b23} Q. Yuan, J. Li, H. Zhou, T. Lin, G. Luo and X. Shen, "A Joint Service Migration and Mobility Optimization Approach for Vehicular Edge Computing," in IEEE Transactions on Vehicular Technology, vol. 69, no. 8, pp. 9041-9052, Aug. 2020, doi: 10.1109/TVT.2020.2999617.
\bibitem{b24} K. Zhang, J. Cao and Y. Zhang, "Adaptive Digital Twin and Multiagent Deep Reinforcement Learning for Vehicular Edge Computing and Networks," in IEEE Transactions on Industrial Informatics, vol. 18, no. 2, pp. 1405-1413, Feb. 2022, doi: 10.1109/TII.2021.3088407.
\bibitem{b29} X. Huang, S. Leng, S. Maharjan and Y. Zhang, "Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks," in IEEE Transactions on Vehicular Technology, vol. 70, no. 9, pp. 9282-9293, Sept. 2021, doi: 10.1109/TVT.2021.3096928.
\bibitem{b31} X. Huang, L. He, X. Chen, L. Wang and F. Li, "Revenue and Energy Efficiency-Driven Delay-Constrained Computing Task Offloading and Resource Allocation in a Vehicular Edge Computing Network: A Deep Reinforcement Learning Approach," in IEEE Internet of Things Journal, vol. 9, no. 11, pp. 8852-8868, 1 June1, 2022, doi: 10.1109/JIOT.2021.3116108.
\bibitem{b41} K. Li, X. Wang, Q. He, B. Yi, A. Morichetta and M. Huang, "Cooperative Multiagent Deep Reinforcement Learning for Computation Offloading: A Mobile Network Operator Perspective," in IEEE Internet of Things Journal, vol. 9, no. 23, pp. 24161-24173, 1 Dec.1, 2022, doi: 10.1109/JIOT.2022.3189445.
\bibitem{b43} Z. Zhou, Y. Wu and J. Hou, "QoE-Guaranteed Heterogeneous Task Offloading with Deep Reinforcement Learning in Edge Computing," 2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS), Chengdu, China, 2022, pp. 558-564, doi: 10.1109/CCIS57298.2022.10016367.
\bibitem{b53} W. Fan, Y. Zhang, G. Zhou and Y. Liu, "Deep Reinforcement Learning-Based Task Offloading for Vehicular Edge Computing With Flexible RSU-RSU Cooperation," in IEEE Transactions on Intelligent Transportation Systems, vol. 25, no. 7, pp. 7712-7725, July 2024, doi: 10.1109/TITS.2024.3349546.
%%Check little details

\bibitem{b65} C. Liu, F. Tang, Y. Hu, K. Li, Z. Tang and K. Li, "Distributed Task Migration Optimization in MEC by Extending Multi-Agent Deep Reinforcement Learning Approach," in IEEE Transactions on Parallel and Distributed Systems, vol. 32, no. 7, pp. 1603-1614, 1 July 2021, doi: 10.1109/TPDS.2020.3046737.
\bibitem{b62} H. Zhang, H. Zhao, R. Liu, A. Kaushik, X. Gao and S. Xu, "Collaborative Task Offloading Optimization for Satellite Mobile Edge Computing Using Multi-Agent Deep Reinforcement Learning," in IEEE Transactions on Vehicular Technology, doi: 10.1109/TVT.2024.3405642.
\bibitem{b73} A. Gao, Q. Wang, W. Liang and Z. Ding, "Game Combined Multi-Agent Reinforcement Learning Approach for UAV Assisted Offloading," in IEEE Transactions on Vehicular Technology, vol. 70, no. 12, pp. 12888-12901, Dec. 2021, doi: 10.1109/TVT.2021.3121281.
\bibitem{b74} H. Yu, S. Leng and F. Wu, "Joint Cooperative Computation Offloading and Trajectory Optimization in Heterogeneous UAV-Swarm-Enabled Aerial Edge Computing Networks," in IEEE Internet of Things Journal, vol. 11, no. 10, pp. 17700-17711, 15 May15, 2024, doi: 10.1109/JIOT.2024.3362321.
%%Check Ref b74 %% Not included in table
\bibitem{b64} S. Tan, B. Chen, D. Liu, J. Zhang and L. Hanzo, "Communication-Assisted Multi-Agent Reinforcement Learning Improves Task-Offloading in UAV-Aided Edge-Computing Networks," in IEEE Wireless Communications Letters, vol. 12, no. 12, pp. 2233-2237, Dec. 2023, doi: 10.1109/LWC.2023.3316794.
%%Check Ref b64 %% Not included in table

\bibitem{b61} Y. Hou, Z. Wei, R. Zhang, X. Cheng and L. Yang, "Hierarchical Task Offloading for Vehicular Fog Computing Based on Multi-Agent Deep Reinforcement Learning," in IEEE Transactions on Wireless Communications, vol. 23, no. 4, pp. 3074-3085, April 2024, doi: 10.1109/TWC.2023.3305321.
\bibitem{b68} Z. Cheng, M. Min, Z. Gao and L. Huang, "Joint Task Offloading and Resource Allocation for Mobile Edge Computing in Ultra-Dense Network," GLOBECOM 2020 - 2020 IEEE Global Communications Conference, Taipei, Taiwan, 2020, pp. 1-6, doi: 10.1109/GLOBECOM42002.2020.9322099. 
\bibitem{b71} H. Lu, C. Gu, F. Luo, W. Ding, S. Zheng and Y. Shen, "Optimization of Task Offloading Strategy for Mobile Edge Computing Based on Multi-Agent Deep Reinforcement Learning," in IEEE Access, vol. 8, pp. 202573-202584, 2020, doi: 10.1109/ACCESS.2020.3036416.
\bibitem{b72} X. Li, Y. Qin, J. Huo and W. Huangfu, "Computation Offloading and Trajectory Planning of Multi-UAV-Enabled MEC: A Knowledge-Assisted Multiagent Reinforcement Learning Approach," in IEEE Transactions on Vehicular Technology, vol. 73, no. 5, pp. 7077-7088, May 2024, doi: 10.1109/TVT.2023.3338612.
\bibitem{b63} J. Wang, X. Zhang, X. He and Y. Sun, "Bandwidth Allocation and Trajectory Control in UAV-Assisted IoV Edge Computing Using Multiagent Reinforcement Learning," in IEEE Transactions on Reliability, vol. 72, no. 2, pp. 599-608, June 2023, doi: 10.1109/TR.2022.3192020.
%%Check Ref b63 %% Not included in table
\bibitem{b66} J. Heydari, V. Ganapathy and M. Shah, "Dynamic Task Offloading in Multi-Agent Mobile Edge Computing Networks," 2019 IEEE Global Communications Conference (GLOBECOM), Waikoloa, HI, USA, 2019, pp. 1-6, doi: 10.1109/GLOBECOM38437.2019.9013115.

%Hetergenous
\bibitem{b30} X. Zhu, Y. Luo, A. Liu, M. Z. A. Bhuiyan and S. Zhang, "Multiagent Deep Reinforcement Learning for Vehicular Computation Offloading in IoT," in IEEE Internet of Things Journal, vol. 8, no. 12, pp. 9763-9773, 15 June15, 2021, doi: 10.1109/JIOT.2020.3040768.

\bibitem{b34}  Y. Li et al., "Two-Tier Multi-Access Partial Computation Offloading via NOMA: A Hybrid Deep Learning Approach for Energy Minimization," 2022 31st Wireless and Optical Communications Conference (WOCC), Shenzhen, China, 2022, pp. 138-143, doi: 10.1109/WOCC55104.2022.9880599. 
\bibitem{b22} Y. Liu, S. Xie and Y. Zhang, "Cooperative Offloading and Resource Management for UAV-Enabled Mobile Edge Computing in Power IoT System," in IEEE Transactions on Vehicular Technology, vol. 69, no. 10, pp. 12229-12239, Oct. 2020, doi: 10.1109/TVT.2020.3016840.
\bibitem{b2} X. Zheng, M. Li, Y. Chen, J. Guo, M. Alam and W. Hu, "Blockchain-Based Secure Computation Offloading in Vehicular Networks," in IEEE Transactions on Intelligent Transportation Systems, vol. 22, no. 7, pp. 4073-4087, July 2021, doi: 10.1109/TITS.2020.3014229.
\bibitem{b14} L. Zhao et al., "A Digital Twin-Assisted Intelligent Partial Offloading Approach for Vehicular Edge Computing," in IEEE Journal on Selected Areas in Communications, vol. 41, no. 11, pp. 3386-3400, Nov. 2023, doi: 10.1109/JSAC.2023.3310062.
\bibitem{b38}  S. Chouikhi, M. Esseghir and L. Merghem-Boulahia, "Energy-Efficient Computation Offloading Based on Multiagent Deep Reinforcement Learning for Industrial Internet of Things Systems," in IEEE Internet of Things Journal, vol. 11, no. 7, pp. 12228-12239, 1 April1, 2024, doi: 10.1109/JIOT.2023.3333044.
\bibitem{b18}  Z. Xue, C. Liu, C. Liao, G. Han and Z. Sheng, "Joint Service Caching and Computation Offloading Scheme Based on Deep Reinforcement Learning in Vehicular Edge Computing Systems," in IEEE Transactions on Vehicular Technology, vol. 72, no. 5, pp. 6709-6722, May 2023, doi: 10.1109/TVT.2023.3234336.
\bibitem{b25} H. Gao, X. Wang, W. Wei, A. Al-Dulaimi and Y. Xu, "Com-DDPG: Task Offloading Based on Multiagent Reinforcement Learning for Information-Communication-Enhanced Mobile Edge Computing in the Internet of Vehicles," in IEEE Transactions on Vehicular Technology, vol. 73, no. 1, pp. 348-361, Jan. 2024, doi: 10.1109/TVT.2023.3309321.
\bibitem{b28} P. Dai, Y. Huang, K. Hu, X. Wu, H. Xing and Z. Yu, "Meta Reinforcement Learning for Multi-Task Offloading in Vehicular Edge Computing," in IEEE Transactions on Mobile Computing, vol. 23, no. 3, pp. 2123-2138, March 2024, doi: 10.1109/TMC.2023.3247579.
\bibitem{b67} Y. Zhang, B. Di, Z. Zheng, J. Lin and L. Song, "Joint Data Offloading and Resource Allocation for Multi-Cloud Heterogeneous Mobile Edge Computing Using Multi-Agent Reinforcement Learning," 2019 IEEE Global Communications Conference (GLOBECOM), Waikoloa, HI, USA, 2019, pp. 1-6, doi: 10.1109/GLOBECOM38437.2019.9013596. 
\bibitem{b13} X. Xu and Y. Song, "A Deep Reinforcement Learning-Based Optimal Computation Offloading Scheme for VR Video Transmission in Mobile Edge Networks," in IEEE Access, vol. 11, pp. 122772-122781, 2023, doi: 10.1109/ACCESS.2023.3327921.
\bibitem{b44} M. Goudarzi, M. A. Rodriguez, M. Sarvi and R. Buyya, "$\mu$ DDRL: A QoS-Aware Distributed Deep Reinforcement Learning Technique for Service Offloading in Fog Computing Environments," in IEEE Transactions on Services Computing, vol. 17, no. 1, pp. 47-59, Jan.-Feb. 2024, doi: 10.1109/TSC.2023.3332308.
\bibitem{b46} C. Chen, H. Li, H. Li, R. Fu, Y. Liu and S. Wan, "Efficiency and Fairness Oriented Dynamic Task Offloading in Internet of Vehicles," in IEEE Transactions on Green Communications and Networking, vol. 6, no. 3, pp. 1481-1493, Sept. 2022, doi: 10.1109/TGCN.2022.3167643.
\bibitem{b69} Q. Qi et al., "Knowledge-Driven Service Offloading Decision for Vehicular Edge Computing: A Deep Reinforcement Learning Approach," in IEEE Transactions on Vehicular Technology, vol. 68, no. 5, pp. 4192-4203, May 2019, doi: 10.1109/TVT.2019.2894437.

\bibitem{b70} Z. Cao, P. Zhou, R. Li, S. Huang and D. Wu, "Multiagent Deep Reinforcement Learning for Joint Multichannel Access and Task Offloading of Mobile-Edge Computing in Industry 4.0," in IEEE Internet of Things Journal, vol. 7, no. 7, pp. 6201-6213, July 2020, doi: 10.1109/JIOT.2020.2968951.

\bibitem{b75} Z. Gao, J. Fu, Z. Jing, Y. Dai and L. Yang, "MOIPC-MAAC: Communication-Assisted Multiobjective MARL for Trajectory Planning and Task Offloading in Multi-UAV-Assisted MEC," in IEEE Internet of Things Journal, vol. 11, no. 10, pp. 18483-18502, 15 May15, 2024, doi: 10.1109/JIOT.2024.3362988.
\bibitem{b8} M. Khayyat, I. A. Elgendy, A. Muthanna, A. S. Alshahrani, S. Alharbi and A. Koucheryavy, "Advanced Deep Learning-Based Computational Offloading for Multilevel Vehicular Edge-Cloud Computing Networks," in IEEE Access, vol. 8, pp. 137052-137062, 2020, doi: 10.1109/ACCESS.2020.3011705.
\bibitem{b17} H. Ke, J. Wang, L. Deng, Y. Ge and H. Wang, "Deep Reinforcement Learning-Based Adaptive Computation Offloading for MEC in Heterogeneous Vehicular Networks," in IEEE Transactions on Vehicular Technology, vol. 69, no. 7, pp. 7916-7929, July 2020, doi: 10.1109/TVT.2020.2993849.
\bibitem{b1} K. Mishra, G. N. V. Rajareddy, U. Ghugar, G. S. Chhabra and A. H. Gandomi, "A Collaborative Computation and Offloading for Compute-Intensive and Latency-Sensitive Dependency-Aware Tasks in Dew-Enabled Vehicular Fog Computing: A Federated Deep Q-Learning Approach," in IEEE Transactions on Network and Service Management, vol. 20, no. 4, pp. 4600-4614, Dec. 2023, doi: 10.1109/TNSM.2023.3282795.

\bibitem{b21}  H. Zhang, L. Feng, X. Liu, K. Long and G. K. Karagiannidis, "User Scheduling and Task Offloading in Multi-Tier Computing 6G Vehicular Network," in IEEE Journal on Selected Areas in Communications, vol. 41, no. 2, pp. 446-456, Feb. 2023, doi: 10.1109/JSAC.2022.3227097.
\bibitem{b59} X. Li, Y. Qin, J. Huo and W. Huangfu, "Heuristically Assisted Multiagent RL-Based Framework for Computation Offloading and Resource Allocation of Mobile-Edge Computing," in IEEE Internet of Things Journal, vol. 10, no. 17, pp. 15477-15487, 1 Sept.1, 2023, doi: 10.1109/JIOT.2023.3264253.
\bibitem{b51}H. Huang, Q. Ye and Y. Zhou, "6G-Empowered Offloading for Realtime Applications in Multi-Access Edge Computing," in IEEE Transactions on Network Science and Engineering, vol. 10, no. 3, pp. 1311-1325, 1 May-June 2023, doi: 10.1109/TNSE.2022.3188921.

\bibitem{b54} J. Yan, X. Zhao and Z. Li, "Deep-Reinforcement-Learning-Based Computation Offloading in UAV-Assisted Vehicular Edge Computing Networks," in IEEE Internet of Things Journal, vol. 11, no. 11, pp. 19882-19897, 1 June1, 2024, doi: 10.1109/JIOT.2024.3370553.
%%Check ref 54 %% Not details included
\bibitem{b55} W. Fan, F. Yang, P. Wang, M. Miao, P. Zhao and T. Huang, "DRL-Based Service Function Chain Edge-to-Edge and Edge-to-Cloud Joint Offloading in Edge-Cloud Network," in IEEE Transactions on Network and Service Management, vol. 20, no. 4, pp. 4478-4493, Dec. 2023, doi: 10.1109/TNSM.2023.3271769.
%%Check ref 55  %% Not details included
\bibitem{b56}  L. Yao, X. Xu, M. Bilal and H. Wang, "Dynamic Edge Computation Offloading for Internet of Vehicles With Deep Reinforcement Learning," in IEEE Transactions on Intelligent Transportation Systems, vol. 24, no. 11, pp. 12991-12999, Nov. 2023, doi: 10.1109/TITS.2022.3178759.
%%Check ref 56 %% Not details included
\bibitem{b57} C. Sun, X. Wu, X. Li, Q. Fan, J. Wen and V. C. M. Leung, "Cooperative Computation Offloading for Multi-Access Edge Computing in 6G Mobile Networks via Soft Actor Critic," in IEEE Transactions on Network Science and Engineering, doi: 10.1109/TNSE.2021.3076795.
%%Check ref 57  %% Not details included
\bibitem{b58} T. Deng et al., "Entropy Normalization SAC-Based Task Offloading for UAV-Assisted Mobile-Edge Computing," in IEEE Internet of Things Journal, vol. 11, no. 15, pp. 26220-26233, 1 Aug.1, 2024, doi: 10.1109/JIOT.2024.3395276.
%%Check ref 58 %% Not details included

\end{thebibliography}









\end{document}
