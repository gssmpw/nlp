\section{Related Works}
\label{section 2}
\subsection{Normalizing Flows in Robotics}

Normalizing flows have demonstrated their versatility in modeling complex distributions in robotics.
Chang et al. introduced IL-flOw____, they applied normalizing flows to imitation learning from observations, addressig the challenges of trajectory reconstruction in multimodal settings by modeling expert state transitions directly.
This approach provided robust performance while smoothing reward signals and avoiding instability in adversarial training.
On the other hand, Chisari et al. introduced Point Flow Match (PFM)____, a method that applies conditional Flow Matching (cFM) to robotic manipulation policy learning. This approach utilizes conditional flows to model policies directly from point cloud data, enabling the generation of long-horizon, multi-step trajectories adaptable to various task scenarios.
Moreover, Fadel et al. proposed a Contextual Movement Model (CMM)____ using conditional Normalizing Flows (cNFs) to analyze spatiotemporal data, integrating contextual variables such as speed and positional relationships for efficient, real-time predictions. These advancements collectively highlight the potential of normalizing flows in handling multimodality, nonlinear dynamics, and local discontinuities in robotic policies.

While IL-flOw, PFM, and CMM leverage normalizing flows for specific tasks, they lack the efficiency required for general policy modeling. IL-flOw decouples reward learning but struggles with scalability to complex policies. PFM excels in manipulation tasks but is constrained by reliance on point clouds. The CMM is efficient for spatiotemporal data but fails to generalize to varying, multimodal policies. 



\subsection{Gaussian Processes in Robotic Policy}
GPs have been widely utilized in robotic control due to their ability to probabilistically perform regression on functions that describe relationships within data while avoiding overfitting, even with limited data.
The Probabilistic Inference for Learning Control (PILCO)____ was introduced using a GP model to enhance data efficiency in autonomous learning systems, particularly in robotics and control.
This method significantly reduces the extensive interaction typically required in model-based reinforcement learning, making it practical for real-time applications and accelerating learning.
Another application____ presents the use of sparse GPs, which select a representative subset of the available training data as a pseudo dataset to provide more efficient modeling. This paper demonstrates the capability of the sparse GP policy model in real-time robot control tasks. 
Furthermore, another work described a multimodal policy search algorithm based on a policy of a sparse OMGP____. Since policy search tasks often require high computational efficiency, this method applies a sparse technique to OMGPs to reduce their computational cost. The sparse OMGP policies allow the policy search algorithm to learn multimodal policies, although it still has limitations in modeling local discontinuities and requires a known mixture number of $M$.


These works collectively focus on data efficiency or reducing computation time, both of which are crucial for advancing real-time robotic systems.
However, since their algorithms are typically suited for simpler policies and environments, these methods are incapable of learning complex multimodal policies or policies with local discontinuities caused by the environment.



\subsection{Deep Generative Models in Robotic Policy}
The development of models for robotic policy learning has seen significant advancements with the integration of deep learning techniques. Recent research has focused on multimodal imitation learning frameworks. For instance, Imitation Generative Adversarial Nets (Imitation GAN)____ is a framework based on Generative Adversarial Nets (GAN)____. Imitation GAN uses a latent intention distributed by a categorical or uniform distribution to select a specific mode of the multimodal policy.
Simulations in a previous work____ show that the ability to capture latent intention of an Imitation GAN outperforms GAN, but it cannot learn all the modes for complex multimodal tasks, such as a four-modal reaching task.
Another more advanced approach is Implicit Behavioral Cloning (IBC)____, which runs optimization on an implicit energy function instead of directly on the multimodal policy model. 
Simulations and experiments show that IBC has great performance on contact rich tasks and can learn bimodal policies. 
However, its training process can take over 15 hours even for simple bimodal tasks.
Another work____ proves that Diffusion Policy has a better success rate than IBC on complex multimodal tasks; unfortunately, it needs over 12 hours to learn each task.


The above studies emphasize the advancements and challenges in deep learning for robotic policy learning. The high computational cost of these models can lead to substantial time and resource expenditure when adapting them to different tasks, limiting their practicality in dynamic environments. To address this issue, we propose an approach that integrates OMGPs with a deep generative model. This method strikes a balance between cost and performance, providing a practical solution for real-world robotics tasks.