\section{Re-Considering Evaluation for Multi-Agent Retrieval Settings}
\label{sec:evalConsider}
There is a long history of work on evaluation of ad hoc retrieval methods. Perhaps the most fundamental paradigm is Cranfield's \cite{cranfield}: using a test collection with documents, queries and relevance judgments. The Cranfield paradigm was the basis for foundational evaluation initiatives: TREC, CLEF, NTCIR, INEX, etc. 

The question of whether retrieval performance patterns observed for one test collection will transfer to others is of utmost importance. For example, in the relatively early days of Web search, studies showed that classical retrieval methods which were highly effective on TREC's newswire collections failed over Web collections \cite{Singhal+Kaszkiel:01a}. This demonstrates, among others, the effect of document agents --- humans, back in these days. From a user perspective, recent work shows that the relative performance patterns of ranking functions can significantly depend on the quality of the query selected to represent the information need \cite{shane:22a,Rashidi+al:24a}. 

In Section \ref{sec:evaluation} we demonstrate the substantial effect of the interplay between ranker, document, and query agents on retrieval performance. A case in point, using for evaluation a corpus composed only of human-generated documents can lead to completely different findings about the relative effectiveness of ranker agents with respect to a corpus composed of both human and LLM-based generated documents. We also show that the alignment, or lack thereof, of the types of query and document (or ranker) agents has considerable impact. At the same time, creating document and query agents (e.g., using different LLM prompts) is quite an easy task. 

The reality just described is a call for arms to re-consider evaluation in multi-agent retrieval settings. The main question is how should test collections be constructed in terms of agents generating documents and queries, and to what extent can findings for one collection transfer to another given the extreme variability and fast-paced changes in agent technology. 

Another important aspect is evaluation in competitive search settings \cite{kurland_competitive_2022} where document authors modify their documents so as to improve their future ranking (search engine optimization). As noted above, there is an increasing body of work on document agents that modify documents for rank promotion while preserving content quality \cite{Greg-bot,Niv}. Static collections cannot be used to evaluate new retrieval methods, for example, as document agents respond to specific rankings. This state-of-affairs calls for simulation-based evaluation in multi-agent retrieval settings (cf., \cite{Lemss,kurland_competitive_2022}) where designers of new retrieval methods (ranker agents) can evaluate them in an online manner. While there has been work on simulating users for interactive retrieval \cite{Balog+Zhai:24a}, integrating document and ranker agents is an important venue for future work.


