\section{The Document Agent}
\label{sec:docAgent}

The document agent is responsible for generating or modifying documents. In the past, humans generated the documents, and agents' roles were limited to applying document modifications. However, LLMs are not only capable of performing document modifications but also of creating original content. Although the texts produced by LLMs may closely resemble human writing, there is still a ground difference in the way LLMs understand, interpret, and generate language~\cite{llm-parrots, how-llms-understand-language, tortured-phrases}.

\myparagraph{Non-Strategic Document Modifications}
In the competitive search ecosystem, document modifications are sometimes of strategic nature~\cite{Greg-bot, kurland_competitive_2022}.
Such modifications include, for example, keyword stuffing and link spamming~\cite{Gyongyi+Molina:05a}. 
These modifications are applied post-document generation and therefore require additional effort from the publisher. 
In contrast, LLM agents often facilitate the writing process.
Publishers therefore leverage these models for purposes beyond rank promotion, which can blure the distinction between strategic and non-strategic modifications~\cite{gpt-papers}.

How to differentiate between strategic and non-strategic modifications is already a challenging endeavor, even when considering only pure human-generated documents. Publishers could perform document modifications for a variety of reasons, such as reflecting temporal changes~\cite{temporal-web-dynamics}. The key distinction, however, lies in the impact on rankings: successful strategic modifications correlate with improved rankings, whereas non-strategic modifications need not necessarily result in improved ranking.
Future research should therefore go beyond classifying modifications as strategic or non-strategic. A possible direction could be to design mechanisms that actively de-incentivize strategic modifications without penalizing non-strategic updates. 

\subsection{Strategic Document Manipulations}

\myparagraph{Mimicking the Winner}
A well-documented phenomenon in competitive search environments is the tendency of less successful publishers to mimic the strategies of top-ranking competitors~\cite{Nimrod,Greg-bot,Greg-Herding,MultQueries,Lemss}.
This behavior, often referred to as "mimicking the winner," has been observed in ranking competitions among humans~\cite{Nimrod}, ranking competitions of humans with document agents~\cite{Greg-bot,Niv,MultQueries} and between document agents themselves~\cite{Lemss}. \citet{Nimrod} demonstrated that when publishers observe a dominant strategy leading to high rankings, they are incentivized to adopt similar modifications. This behavior leads to homogenization of documents and reduction in the diversity of the corpus~\cite{Greg-Herding}.

\myparagraph{Adapting to Ranker Biases}
It was recently shown \cite{neural-llm-bias, dai2024cocktail} that LLM rankers are biased towards LLM-generated documents.
In a competitive environment, publishers may find themselves in a challenging position where human-generated content could be deprioritized in favor of LLM-generated alternatives.

Suppose that this very paper were to be submitted for review at a conference.
If we suspect that LLMs influence the review process~\cite{conference-llm-reviews}, whether
through automated initial assessments or reviewers using LLM
tools for comprehension, then, as authors, we are incentivized to
optimize our writing for these models. This creates a paradox: the
very tools designed to enhance evaluation may instead undermine its integrity by encouraging content to cater to algorithmic biases rather than relevance.

\myparagraph{From query attacks to query-agent attacks}
Many existing attacks on ranking functions, in the form of document modifiications, are query-specific~\cite{Wang+al:22a, Greg-Herding,Liu+al:22a,Song+al:22a,Wang+al:22a,Wu+al:22a,Chen+al:23a,Chen+al:23b}. By generating multiple queries and performing aggregation, etc., query agents could perhaps be able to generate queries that can mitigate the impact of these specific attacks. 
Hence, attack strategies of document agents may shift toward targeting query agents~\cite{GEO}.
This further strengthens our call for future research that explores methods to mitigate the bias of LLM rankers in favor of LLM-generated content, as discussed in Section~\ref{sec:devising-ranker-agents}.
The study of such potential attacks and the means to counteract them is an important research direction.
 


