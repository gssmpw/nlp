
\section{Introduction}
There is a growing body of work and discussion on the potential impact
of large language models (LLMs) on the information retrieval (IR)
field \cite{Chen+al:21a,White:25a,Zhai:24a,Zhu+al:24a}. 
A prominent theme is using LLM-based agents to assist the user in information-seeking tasks~\cite{Chen+al:21a,Shah+White:24a,White:25a,Zhai:24a,Zhang+al:24a}. 
For example, these agents, henceforth referred to as {\em query agents}, can help formulate queries representing usersâ€™ information needs~\cite{Chen+al:21a,Shah+White:24a,White:25a,Zhai:24a}.

There are two additional types of agents which together with query
agents play a crucial role in modern retrieval
settings. The first are {\em document agents}: agents whose goal is to
assist in document generation and editing. Indeed, the proliferation
of LLM-based generated content is ever growing
\cite{ai-generated-social-media}. The second type of agents is that
used to induce ranking, specifically for ad hoc (query-based) retrieval which
is our focus in this paper. Herein, we refer to this type of agent as
{\em ranker agent}. Accordingly, in the ad hoc retrieval setting we
consider, documents in the corpus can be generated by humans or
document agents, queries representing users' information needs can be
generated by the users themselves or agents acting on their behalf\footnote{Agents can also have information needs, but this 
  is outside the scope of this paper.}, and the ranking mechanism can be an agent (e.g., LLM-based rankers
\cite{liang2022holistic,qin2024pairwise,zhuang2024setwise}).

We argue in this paper
that the mutual effects among the three types of agents just discussed --- query, document and ranker --- call for
re-consideration of
some classical retrieval paradigms and frameworks. Furthermore, there are far
reaching implications on evaluation of ad hoc retrieval. As we discuss below, we do not confine our call for arms for LLM-based agents only.

Consider, for example, the generative theory for relevance
\cite{Lavrenko+Croft:01a,lavrenko-rm-book}. The relevance assumption is
that there is a relevance language model\footnote{The specific language
  models were unigram.} that generates terms in the query and in
documents relevant to the information need it represents. Since
document and query agents need not be aligned --- e.g., they can
utilize completely different language models --- this generative
assumption does not necessarily hold. In fact, the potential
misalignment between document and query agents is conceptually
reminiscent of the state-of-affairs in cross-lingual retrieval
\cite{Lavrenko+al:02a}. The misalignment between the ranker agent and document agent can also have significant effect. A case in point, recent work \cite{neural-llm-bias,dai2024cocktail}
shows that LLM-based rankers are biased in favor of LLM-based
generated content.

Another important aspect of multi-agent retrieval settings is ranking incentives.
In competitive search
settings \cite{kurland_competitive_2022}, some document authors are
incentivized to have their documents highly ranked for specific
queries (e.g., queries of commercial intent). They often modify documents in response to induced ranking so as to improve their future ranking.
There is recent work on devising document agents that automatically modify documents
for rank promotion, while preserving document quality
\cite{Greg-bot,Niv}. Commonly used modification strategies were shown
to lead to herding effects not only of human authors
\cite{Nimrod,Greg-Herding}, but also of LLM-based document agents
\cite{Lemss}. The resultant corpus effects (e.g., reduced topical
diversity) are unwarranted~\cite{Nimrod}. Devising ranking functions (i.e., ranker
agents) that account for both search effectiveness and long-term
corpus effects is an open challenge
\cite{kurland_competitive_2022}. Devising query agents that operate in 
the competitive search setting is an additional important research
question we discuss.

The foundational Cranfield evaluation paradigm \cite{cranfield}, which
is the underpinning of many evaluation practices in IR (e.g., those
used in TREC \cite{Harman+Voorhees:06a}), is based on using a test
collection that includes documents, information needs (and queries
representing them) and relevance judgments. Evaluating search
effectiveness in the multi-agent search setting we discuss poses
several evaluation challenges. First, creating a test collection
composed of documents generated by a representative variety of
document agents is a challenge given the rapid emergence of new LLM
types and technologies. Such collections can become obsolete very
quickly. Second, a similar challenge emerges due to query
agents which generate queries. Different agent technologies can result
in completely different types of queries. Additional complexity is introduced by the need to account for
competitive search conditions \cite{kurland_competitive_2022} as those
mentioned above. Static collections cannot support evaluation of corpus effects driven by document agents responding to induced rankings \cite{kurland_competitive_2022}. Accounting for all these considerations together, we argue for the increasing importance of simulation-based evaluation in multi-agent retrieval settings (cf., \cite{Lemss}).

Our call for arms in this paper, about re-considering the fundamentals
of ad hoc retrieval in multi-agent settings, is composed of two
parts. 
In the first part, we present our perspectives about the setting and potential future research directions it gives rise to.
In the
second part, we present an in-depth empirical illustration of the
multi-agent retrieval setting and its potential consequences. We use
three different types of approaches to devise document, ranker and
query agents: lexical (\tfidf-based), semantic (embedding-based), and
LLM-based. Some of these agent types are novel to this study.  Our
empirical findings clearly support our call for arms. For example,
when the query agent and ranker agent are of different types (e.g.,
one is semantic and the other is LLM-based), retrieval effectiveness
degrades with respect to having both of the same type. Additional example is the result of misalignment between the types of document and ranker agents. If both are LLM-based, but implemented with different LLMs, then the ability of the document agent to promote its document in rankings is reduced with respect to a setting where the same LLM is used for both the document and the ranker agents.


The paper is structured as follows. In Sections \ref{sec:rankAgent},
\ref{sec:query-agent} and \ref{sec:docAgent} we present our perspectives about
the ranker (agent), query (agent) and document (agent), respectively, in the multi-agent setting.
In
Section \ref{sec:evalConsider}, we discuss the evaluation implications of the multi-agent retrieval setting. Sections
\ref{sec:experimental-settings} 
and \ref{sec:evaluation} present an
empirical exploration of various multi-agent retrieval settings.



