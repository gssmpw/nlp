\section{The Ranker Agent}
\label{sec:rankAgent}
The ranker agent has to induce a ranking in response to a query as is
standard in ad hoc retrieval. We use the term ``ranker agent'' for
alignment with the terminology used for the document and query agents discussed below,
and to indicate that the ranking mechanism can be quite evolved: it can include a query understanding (a.k.a.,
query intent identification) module \cite{Jansen+al:07a}, utilize
pseudo relevance feedback \cite{Xu+Croft:96a}, or fuse lists retrieved
by different ranking functions \cite{Kurland+Culpepper:18a}.



Regardless of who generated the query --- i.e., the user
directly or a query agent --- the goal is to satisfy the
information need it represents. Herein we assume that the information
need is that of the user. Treatment of cases where agents have their
own information needs is outside the scope of this paper. Accordingly,
the probability ranking principle (PRP) \cite{Robertson:77a} remains
optimal in the multi-agent retrieval setting under the conditions
specified by Robertson \cite{Robertson:77a}: the relevance of
different documents is independent and users have the same utility
function. That is, the ranking should be based on the probability a
document is relevant to the query (or more precisely, the information
need it represents) where the probability is estimated using all
information available to the search system. The fundamental difference
between retrieval methods is the relevance estimate: it could be a
probability-based estimate (e.g., as in Okapi BM25
\cite{Robertson:93a}), or a proxy thereof (e.g., cosine between the
embedding vectors of the query and the document).

The different
frameworks and paradigms for relevance estimation can be significantly affected by
the fact that query and document agents might have generated the
query and the document, respectively. We next discuss a few examples of foundational relevance estimation paradigms that should be re-considered.

%\subsubsection{Re-Considering Existing Relevance Estimation Paradigms and Frameworks}


\myparagraph{The relevance model} The generative relevance assumption
is that terms in the query and in documents relevant to the query are
generated by the same language model \cite{Lavrenko+Croft:01a,lavrenko-rm-book}. The assumption, as well as the  relevance-model estimation approach, were based on unigram language models. Conceptually, the assumption should hold for more evolved language models.

However, since documents and queries can be generated by different types of agents, the original generative assumption, and its conceptual generalization just proposed, do not hold. This state-of-affairs is reminiscent of that in cross-lingual retrieval where queries and documents are written in different languages \cite{Lavrenko+al:02a,crossling}. Accordingly, re-visiting the generative theory to relevance is an interesting future direction to explore in multi-agent retrieval settings.



\myparagraph{The risk minimization framework} Lafferty and Zhai
\cite{Lafferty+Zhai:01a} describe the document authorship and
retrieval processes as follows. A document author selects a (language)
model from which she samples terms for the document she writes. The
user of the search system also selects a (language) model representing
the information need. Terms are sampled from the user model to
generate the query. To estimate document relevance, the query and
document models are compared. Since these are unigram language models \cite{Lafferty+Zhai:01a}, KL divergence and cross entropy are often used to compare the models.

Lafferty and Zhai's framework \cite{Lafferty+Zhai:01a} is conceptually
aligned at its most basic level with the multi-agent retrieval setting
we address here. The models selected by the document author and user
are now replaced with agents. The agents can use language models or
any other means to generate documents and queries. More importantly, due to the potential divergence
between the document and query agent, basic model comparison as that in
Lafferty and Zhai's framework \cite{Lafferty+Zhai:01a} does not
necessarily work. Extending the risk minimization framework to account for document and query agent type misalignment is an interesting future direction to explore.


\myparagraph{The axiomatic framework} The axiomatic framework for
retrieval \cite{Fang+Zhai:05a} has evolved to
also handle modern neural retrieval methods \cite{Cheng+Fang:20a}. The idea is to use a set of axioms to analyze existing
retrieval methods and to devise new ones.

In competitive search settings \cite{kurland_competitive_2022},
document authors might modify their documents so as to improve their
future ranking. This practice is often referred to as search engine
optimization (SEO) \cite{Gyongyi+Molina:05a}. There are recent
examples of document agents devised to modify document content for
improved ranking while maintaining document quality
\cite{Greg-bot,Niv}. As noted elsewhere
\cite{kurland_competitive_2022}, this reality undermines some of the
basic axioms for retrieval \cite{Fang+Zhai:05a}; e.g., that increased
query term occurence in a document should result in increased
retrieval score. Indeed, when document agents are ranking
incentivized, query term occurrence need not necessarily reflect pure
authorship considerations. An interesting future direction is to
revise the axiomatic approach to reflect that documents and queries can be generated by agents and these agents can be of completely different types (e.g., lexical vs. semantic), as we present in Section~\ref{sec:agents-implementation}.
  
\subsection{Devising Ranker Agents}
\label{sec:devising-ranker-agents}
The multi-agent retrieval settings brings about new challenges as
those mentioned above. At the same time, it gives rise to a plethora
of new opportunities, a few of which we now turn to discuss.

\myparagraph{Addressing the query agent} Commercial search engines often employ a query
intent identification method \cite{Jansen+al:07a} to improve reasoning
about the information need underlying a query. In a setting where a
query agent generates the query, agent (type) identification can
potentially also be of much merit. A case in point, suppose that the query
agent is based on a specific LLM. If the ranker agent is able to
identify which LLM that is, then a few opportunities emerge. To begin
with, automatic query reformulation can be performed using the exact same
LLM \cite{Jagerman+al:23a}. Such practice can potentially decrease the chances for query drift.
Identifying the query agent (type) can also help in selecting a ranking
function~\cite{Balasubramanian+Allan:10a}, especially if the document agent is also identified, as we further discuss below. Indeed, we show in Section \ref{sec:evaluation} that the alignment, or lack thereof, between the query, document and ranker agent can have considerable impact on search effectiveness.

\myparagraph{Addressing the document agent} Comparing document and query representations is a
fundamental relevance estimation paradigm. Document representation can
be induced using tf.idf, stochastic language models
\cite{Lafferty+Zhai:01a,Lavrenko+Croft:01a}, modern embedding
approaches \cite{e5,chen2024bge} and other approaches.  There has
also been a large body of work on enriching document representations
using cluster-based and topic-based information
\cite{Kurland+Lee:04a,Liu+Croft:04a,Wei+Croft:06a,Efron+al:12a}
and using queries automatically generated from a document
\cite{Nogueira+al:19a}.

In the multi-agent retrieval setting the ranker agent has to rank
documents, some of which were generated using document
agents. The potential ability to identify the document agent type can be of
much merit. Specifically, given our findings in Section \ref{sec:effectiveness-experiment}, and those in some recent work \cite{neural-llm-bias,dai2024cocktail}, alignment of the ranker agent and document agent can have significant impact on search effectiveness \cite{neural-llm-bias,dai2024cocktail}. Hence, an intriguing research direction is estimating document relevance based on the type of agent that generated the specific document.

Another direction worth exploration is the ``translation'' between
agent types. Such translation can be performed by the ranker agent
online during retrieval time or offline. Consider the following
example. Suppose that a query agent generates a query using some LLM
applied to a natural language description of an information need. On
the other hand, say that the document agent used a BERT-based
embedding approach~\cite{bert-transformer} to modify a human-authored document. (We devise and
evaluate such query and document agents in Sections \ref{sec:agents-implementation} and~\ref{sec:evaluation}.) We show in Section
\ref{sec:evaluation} that this misalignment has considerable impact on retrieval. Now, if the ranker agent knows how to ``translate''
the generated query, the document or both to a shared ground (e.g.,
LLM-based or embedding-based space), then presumably the retrieval would
be more effective. Conceptually similar considerations are common
in work on cross-lingual retrieval \cite{crossling}.

As noted above, in competitive search settings, document authors might be 
incentivized to modify their documents to improve their ranking for
queries of interest (i.e., search engine optimization). LLMs provide an effective grounds for such modifications, as recently shown \cite{Niv}. This reality brings about the need to devise ranker agents that account for the ranking incentives of document agents. For example, distilling content from the document which is due to pure authorship considerations rather than ranking incentives (e.g., keyword stuffing) is an important research direction (cf., \cite{Ziv-Ranker}). Kurland and Tennenholtz \cite{kurland_competitive_2022} present an elaborated discussion about the challenges of devising ranking functions in the face of ranking-incentivized document modifications.
