\section{Empirical Exploration}
\label{sec:experimental-settings}

The main goal of the experiments presented below is to demonstrate the effects on the retrieval setting of interactions between different query, document, and ranker agents.
To this end, we conduct a series of three experiments. 
In the first experiment, we evaluate the effectiveness of different rankers
with various query and document agents (Section~\ref{sec:effectiveness-experiment}).
In the second experiment, we explore the interplay between the document and ranker agent, specifically when document agents compete against human-authored documents for rank promotion (Section~\ref{sec:offline-experiment}).
In the third experiment, we
study how ranker and query agents influence the competitive dynamics among different document agents (Section~\ref{sec:online-experiment}).

We now turn to describe our methodology. First, we detail the datasets used for evaluation (Section~\ref{sec:data}). Then, in Section~\ref{sec:agents-implementation}, we introduce the implementations of the ranker, query, and document agents, each categorized into three types: lexical, semantic, and LLM-based. Additionally, for query and document agents, we also consider human agents, i.e., queries and documents generated by humans.
Several of the agents we present are novel to this study.

% overview
\subsection{Data}
\label{sec:data}

We utilize datasets from ranking competitions in which human participants competed against automated agents ~\cite{Greg-bot, Niv, MultQueries}. These competitions were structured to simulate realistic retrieval environments where documents evolved across multiple ranking rounds due to ranking incentives. The datasets consist of documents that were iteratively modified by human participants attempting to improve their ranking, alongside automated agents that applied ranking-incentivized modifications.

% details of competitions
We use four publicly available datasets~\cite{Greg-bot, Niv, MultQueries} with recordings of content-based ranking competitions held in IR courses where students acted as publishers. The students were given an initial document and were tasked with promoting it in rankings induced by an undisclosed ranking function over several rounds. At the
beginning of the competition, all students received the same example of a relevant document for each topic. After each round, they were presented with a ranking of their documents.
Their objective was to modify their documents in an effort to improve their ranking in the subsequent round. All documents were plaintext and limited to a maximum of $150$ terms. The competitions were incentivized through performance-based course-grade bonuses and were conducted with the approval of ethics committees.


\input{material/datasets}

% the datasets
% greg data
The competitions conducted by~\citet{Greg-bot} employed a \lambdamart
ranker \cite{wu2010adapting} with $25$ features: $24$ lexical
features, and a single document quality score.  The competitions were held for
$15$ queries (topic titles) from TREC's ClueWeb09 dataset.

% niv dataset 
The competitions held by~\citet{Niv} employed the \efive-based ~\cite{e5} ranking function\footnote{intfloat/e5-large-unsupervised}. Two competitions were held for each of the $15$ same queries as Goren et al.'s~\cite{Greg-bot} competition.

% mult dataset
As discussed in Section~\ref{sec:query-agent}, the query agent might consider different query variations, generated by either one or a variety of query agent types. 
To explore the effects of query variations on the competition dynamics, we utilize the dataset from \citet{MultQueries}. In these competitions students were requested to improve their rankings for three different query variations per topic. We use two of their competitions: the ones where students were not allowed to use \llm tools.
In the first competition, the ranker was a \bert~\cite{BERT} model fine-tuned on the MS-MARCO dataset.
The second competition employed LambdaMART trained specifically for competitive setings~\cite{Ziv-Ranker}.
The features were all lexical except for one semantic feature based on the score of the same \bert model used in the first competition.  
Both competitions were conducted for $30$ topics from TREC's ClueWeb12 collection, with query variations from the dataset reported in~\cite{uqv100}.

% single vs. multi
We use \firstmention{\single} to refer to competitions where students competed for a single query and \firstmention{\multi} 
to refer to competitions with multiple query variations. Similarly, \firstmention{\ltr} denotes competitions using a LambdaMART,
while \firstmention{\neu} refers to those using a neural ranker (\bert / \efive). 

All competitions featured a total of five participants per topic. However, not all of them were students, as some of the competing documents were planted. 
In addition, each competition included different \botagents that competed alongside human participants. The students were unaware that they were competing against automated agents. The \botagent in \GregDataset utilized both lexical and semantic features, whereas in the other competitions, the \botagent was entirely \llm-based.
Table~\ref{tab:dataset} summarizes the details of all four datasets used in our experiments.


\subsection{Agents Implementation}
\label{sec:agents-implementation}
% \input{perspectives/mediator/agent}
\myparagraph{Ranker}
We use
three types of agents (lexical, semantic and LLM) each with two rankers.
\subsubsection*{\lexicalagent}
% TF-IDF weights 
We use \bm and \tfidf\footnote{The retrieval score is the sum of \tfidf weights of query terms in the document.}  with Indri's default hyperparameter values\footnote{\url{https://www.lemurproject.org/indri}}.
Collection statistics are based on ClueWeb09 for \GregDataset and \NivDataset and ClueWeb12 for \MultiB and \MultiD.

\subsubsection*{\semanticagent}
We use \contriever~\cite{contriever} and \efive~\cite{e5}, both fine-tuned on the MS-MARCO~\cite{msmarco} dataset\footnote{intfloat/e5-base-v2, nthakur/contriever-base-msmarco}. The document retrieval score is the cosine between the query and document embedding vectors.

\subsubsection*{LLM} 
We adopt a pointwise relevance generation model~\cite{liang2022holistic} for an LLM ranker.
We experiment with two lightweight ($<10$B parameters) instruction-tuned open-source LLMs: \llamaWithVersion~\cite{dubey_llama_2024} and \gemmaWithVersion~\cite{gemma_team_gemma_nodate} from the Hugging Face repository\footnote{meta-llama/Meta-Llama-3.1-8B-Instruct, google/gemma-2-9b-it}. 

All the pre-trained models described above are used consistently across all experiments with their Hugging Face default hyperparameter values, unless specified otherwise.

\myparagraph{Query}
To
study the impact of different query formulation techniques, we implemented several query agents that generate queries based on the backstories provided in the UQV100 dataset~\cite{uqv100}.


%In our experiments,
Herein, we treat each query variation independently as a query, i.e., without directly considering the combined effect of multiple queries simultaneously generated by the query agent.
Previous work~\cite{MultQueries} explored the dynamics when humans and document agents compete for multiple (human) queries simultaneously. 
%multiple queries --- all human generated.  
The exploration of the dynamics when document agents compete for multiple queries generated by different query agents is beyond the scope of this paper and remains an interesting direction for future work. 


\subsubsection*{Human} 
We utilize the human-generated query variations from the UQV100 dataset, which was created via crowdsourcing~\cite{uqv100}. For each topic, we selected the five most frequent query variations. 

\subsubsection*{Lexical}
We applied the YAKE keyword extraction method~\cite{YAKE} to identify key phrases in the backstories. Each extracted phrase was considered a potential query and was ranked w.r.t. the backstory using BM25. The five highest-scoring phrases per topic were selected.

\subsubsection*{Semantic} 
We adopt the doc2query method~\cite{doc2query} to generate for each topic a pool of $1,000$ query variations from the backstory.
We use two representations: \efive and \contriever.
For each of them, the final five variations for each topic were selected based on the cosine similarity of the embedding vector of the variation and that of the backstory. 


\subsubsection*{LLM}
We utilized an existing dataset containing query variations generated by \gptWithVersion~\cite{llm-query-variations} and replicated its methodology to generate additional variations with \llama~\cite{dubey_llama_2024} and \gemma~\cite{gemma_team_gemma_nodate}, all set with a temperature of $1$.
To ensure well-formed outputs, we added to the prompt an instruction to return plain lists of queries. For each topic, the first five variations generated by each LLM were selected.





\myparagraph{Document}
% How we defined the agents for the experiment
We implement different document agents that apply ranking-incentivized modifications to documents. 
The \botagents operate as follows: first, they observe the ranking for a given query. Then, they apply modifications to a document to improve its ranking in the next round.

Two
ranking-incentivized \botagents for document modifications were recently presented.
~Gorent et al.'s \cite{Greg-bot} \botagent replaces a sentence \SrcSentence from the
document with a candidate sentence \TargetSentence. 
The \botagent selects the pair $(\SrcSentenceMath, \TargetSentenceMath)$ using a learning-to-rank approach with a small set of lexical and semantic features. 
Our goal is to compare \textit{lexical} and \textit{semantic} agents, making it unsuitable to adopt their method directly.
Instead, we develop a \botagent inspired by their approach.
Bardas et al. ~\cite{Niv} used LLM-based agents for document modification. We adopt their approach to develop our LLM \botagent.


\subsubsection*{Lexical}

Inspired by~\citet{Greg-bot}, the lexical \botagent modifies documents
by replacing a sentence from the original document, \SrcSentence (source), with
a candidate sentence \TargetSentence (target).  Candidate sentences are
extracted from documents published by other publishers in the round.
The \botagent developed by \citet{Greg-bot} relies on a set of lexical and semantic features.  In particular, it employs two lexical
features, assuming that higher feature values indicate increased
retrieval score assigned by the undisclosed ranking function:
\firstmention{\QryTerm}, which is the fraction of query term occurrences in the sentence, and \firstmention{\SimTop}, which represents the cosine similarity between the sentence's \tfidf vector and the centroid of the top \numhighestranked ranked documents in the current ranking.

Each feature is computed for both the source (\QryTermSrc, \SimSrcTop) and target (\QryTermTarget, \SimTargetTop) sentences. The final score is:
\begin{equation*}
    score_{lex}(\SrcSentenceMath, \TargetSentenceMath) = \interpolationMath \cdot (\QryTermTargetMath - \QryTermSrcMath) + (1-\interpolationMath) \cdot (\SimTargetTopMath-\SimSrcTopMath).
\end{equation*}
The highest scoring sentence pair is selected to perform the replacement.
To ensure that modifications do not significantly alter
the document, we only consider a pair $(\SrcSentenceMath, \TargetSentenceMath)$ if the \tfidf-based cosine similarity between \SrcSentence and \TargetSentence exceeds a predefined threshold \nlithreshold.
The parameter values are: $\interpolationMath \in [0, 0.1, \ldots,1]$, $\numhighestrankedMath \in \{2, 3, 4\}$ and $\nlithresholdMath \in [0,0.1,\ldots,0,5]$.

\subsubsection*{Semantic}
The semantic document agent, novel to this study, modifies documents in a similar manner to the lexical document agent, but relies on semantic (embedding) representations rather than lexical features. 
We
use three strategies to select the candidate target sentences to use
for replacement in the original document.
The first strategy, \firstmention{\all}, considers sentences from all
other documents as candidates.  The \firstmention{\better} strategy
selects only sentences from documents ranked higher than the original
document.  The \firstmention{\best} strategy picks sentences
exclusively from the highest-ranked document.  For each pair of
sentences $(\SrcSentenceMath, \TargetSentenceMath)$ and query $q$, we
compute the cosine similarity between the embedding vectors
representing \SrcSentence and \TargetSentence and the similarity between \TargetSentence and $q$: 
\begin{equation*}
    score_{sem}(\SrcSentenceMath, \TargetSentenceMath) = \interpolationMath \cdot cos(\SrcSentenceMath, \TargetSentenceMath) + (1-\interpolationMath) \cdot cos(\SrcSentenceMath, q)
\end{equation*}
We use this method to balance the presumed relevance of the candidate (target)
sentences, and accordingly their contribution to the retrieval score, and their
contextual fit within the document.  To preserve document coherence,
we use an NLI model~\cite{NLI} to ensure that the new sentence is entailed
by the candidate sentence. A candidate sentence is considered only if
it entails the original sentence, which is considered to be the case
if the probability assigned by the NLI model is higher
than \firstmention{\nlithreshold}.  In our experiments, we use two
dense pretrained text embedding models before
fine-tuning: \efive~\cite{e5}
and \contriever~\cite{contriever}\footnote{intfloat/e5-base-unsupervised,
facebook/contriever}.
The value of \interpolation was selected from $[0, 0.1,\ldots,1]$ and the value of \nlithreshold was selected from $[0,0.1,\ldots,0.5]$. 

\subsubsection*{LLM}

The \textit{LLM-based} \botagent introduced by \citet{Niv} modifies documents using different prompting strategies.
%that guide the agent's behavior. 
Document agents are defined by a specific LLM model and the prompting strategy.
The prompts include the rules and constraints for the document format, the original document, and the assigned query.
Additionally, they include information about past documents and their rankings. 
We use two prompting strategies: \feedbackPair, which includes two randomly selected documents and their rankings, and \feedbackAll, which provides the full ranked list. 
To mitigate copying behavior observed in human-driven ranking competitions~\cite{Nimrod, Greg-Herding}, the \nocopy\xspace flag ensures that the \botagent is also explicitly warned against directly copying content from other documents. 
Both \llama and \gemma were used with $top_p=0.95$ and $temperature=0.7$.

Unless otherwise specified, the free-parameter values of all document agents were set per ranker to maximize \firstmention{scaled rank promotion}: the raw rank change of a document, normalized by the maximum possible promotion or demotion based on its position. Higher values indicate increased improvement.
