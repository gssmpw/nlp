
Our online evaluation consists of a simulation of a ranking competition between different \botagents. 
We use the documents from the last round of the \MultiB and \MultiD datasets as a starting point for the simulation. 
The simulation was performed using the CSP framework~\cite{Lemss}. 
We employ two semantic \botagents: \efive and \contriever with the candidate sentence selection strategy \textit{\better}, 
$\nlithresholdMath=0.5$ and $\interpolationMath=0$.
Two \llm \botagents are also included in the simulation: \gemma and \llama, both with \feedbackPair \xspace and \withoutnocopy. (See Section \ref{sec:agents-implementation} for details about the 
agents\footnote{Hyperparameter values were set to maximize rank promotion performance w.r.t. the ranker agent for which the promotion was minimal in the offline evaluation.}.)
Additionally, we use as a reference comparison a static agent that does not modify her initial document throughout the rounds. 
The agents are randomly paired with an initial document from the last round of the competition.
We ran the simulation for four rounds for each of the query variations, which results in $30$ 
different 
competitions for each set of documents.  
In total, we ran
$1800$ simulations. 
Our research question is \textbf{RQ7:} How do different query agent types impact the dynamics between different document agents that compete against each other for document rankings? 
The effectiveness of \botagents is measured by the average rank position of their documents. 