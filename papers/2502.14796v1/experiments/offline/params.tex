We perform the offline evaluation with the two \single\xspace datasets: \GregDataset and \NivDataset. We cannot use the \multi datasets since our goal here is to contrast the effectiveness of document agents with respect to humans in promoting documents in rankings. The \botagents are designed to promote the document for a single query, while the students in the \multi competitions modified their documents to promote them for multiple queries. 

The goal of this experiment is not to develop the most effective \botagent but rather to examine how different document agents interact with ranker agents. 
Our research questions are:
\textbf{RQ4}: Can the zero-shot lexical and semantic document agents we developed successfully compete for rankings against human publishers?
\textbf{RQ5}: Does the misalignment between the \botagent and the ranker agent types affect the \botagents' effectiveness in promoting their documents? 
\textbf{RQ6}: How well can document agents perform when knowing only the ranker agent type (i.e., lexical, semantic, or \llm)?
