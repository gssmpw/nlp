In this experiment, we assess the effectiveness of different ranker agents when queries and documents are generated by
query and document agents, respectively. 
A \firstmention{\agentcorpus} is a set of documents that are generated by a collective of document agents.
We use three
corpora: (i) \firstmention{\purehumancorpus}, which contains only human-generated documents; (ii) \firstmention{\purellmcorpus}, which consists solely of documents generated by the \llm document agent; and (iii) \firstmention{\mixedcorpus}, which includes a combination of both human and \llm-generated documents. 
The query agents generate query variations based on the backstory corresponding to each topic. We consider both query variations generated by humans and variations of queries generated by different types of query agents.

For evaluation, we use the two \multi datasets, \MultiB and \MultiD,  with competitions for multiple queries.
These are the only datasets with a corresponding backstory, which is required by the query agent.
We evaluate the effectiveness of ranker agents on rounds $2$-$10$, since the \llmagent documents were not modified in the first round. 

A total of five documents are ranked for each query, with $2$-$3$ authored by humans and the remaining generated by the LLM agent. 
Hence, we measure the effectiveness of different rankers using the nDCG@1 metric. Statistical significance is determined based on a two-tailed paired t-test with $p < 0.05$ and Bonferroni correction for multiple comparisons.


We address three research questions:
\textbf{RQ1}: How does retrieval effectiveness change when rankers operate on corpora composed of documents created by different document agents? 
\textbf{RQ2}: Do ranker agents perform differently depending on the query agent?
\textbf{RQ3}: How is ranker agent effectiveness
affected by the document and query agent types (i.e., human, lexical, semantic and LLM)?










