

% mixed < human | llm
\input{experiments/effectiveness/tables/ndcg1}

% RQ1
Table~\ref{tab:ndcg1_results} reports for each agent type the average nDCG@1 over the two ranker agents of the same type. 
We see that \highlight{across all rankers and query agent types,  retrieval performance is significantly lower on a mixed corpus (i.e., corpus composed of documents created by human and LLM agents)
%by different types of document agents)
compared to a corpus with documents generated by a single agent (human or LLM)} (\textbf{RQ1}). For human-generated queries, the rankers consistently performed better on a \purehumancorpus corpus than on a \purellmcorpus corpus. However, for LLM-generated queries, performance was similar across the \purehumancorpus and \purellmcorpus corpora. 



% new llms
\input{experiments/effectiveness/tables/contriever_query_generator}

The results in Table~\ref{tab:ndcg1_contriever} reveal that retrieval effectiveness varies not only between human and LLM-generated queries but also significantly among different query agents of the same type. 
For example, Table~\ref{tab:ndcg1_contriever} shows that the \contriever ranker agent achieves $NDCG@1$ of $92.167$ when queries are generated by a \contriever query agent on the \purehumancorpus corpus, but it drops to $90.531$ when the query agent is \llama. 
However, it drops only to $91.438$ when the queries are generated by a different LLM: \gpt. 
The results in Table~\ref{tab:ndcg1_contriever} demonstrate that ranker agents exhibit not only a difference between human and LLM-generated queries (that is, different query agent types), but also substantial variance across different LLMs (i.e., different query agents of the same type) (\textbf{RQ2}).
This suggests that ranking agents are not uniformly effective across different query agents and document agents. Moreover, we see in Table~\ref{tab:ndcg1_contriever} that the performance of the ranker agent is quite the same for different query agents when the documents are all generated by the same \llm document agent (\textbf{RQ3}).
%Moreover,
Table~\ref{tab:ndcg1_contriever} also shows that the highest effectiveness for both human and mixed corpora for the \contriever ranker agent is when the queries are generated by the matching query agent (\contriever), which attests to the importance of alignment between the query agent and the ranker agent.

The findings presented above underscore the
%necessity
importance of using corpora with documents generated by both LLMs and
humans for evaluation. Corpora composed only of human-authored
documents do not represent the real world anymore, and the resultant
evaluation can be biased. Moreover, evaluation should also be based on a variety of query agents.
