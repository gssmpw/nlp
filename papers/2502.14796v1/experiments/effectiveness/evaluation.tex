
A total of five documents are ranked for each query, with $2$-$3$ authored by humans and the remaining generated by the LLM agent. 
Hence, we measure the effectiveness of different rankers using the nDCG@1 metric. 
The NDCG values are quite high, as in past work~\cite{Ziv-Ranker} as most documents in the competitions were relevant~\cite{Nimrod, MultQueries}.
Statistical significance is determined based on a two-tailed paired t-test with $p < 0.05$ and Bonferroni correction for multiple comparisons.

