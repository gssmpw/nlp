The rise of large language models (LLMs) has introduced a new era in information retrieval (IR), where queries and documents that were once assumed to be generated exclusively by humans can now also be created by automated agents. These agents can formulate queries, generate documents, and perform ranking. This shift challenges some long-standing IR paradigms and calls for a reassessment of both theoretical frameworks and practical methodologies.
We advocate for a multi-agent perspective to better capture the complex interactions between query agents, document agents, and ranker agents. 
Through empirical exploration of various multi-agent retrieval settings, we reveal the significant impact of these interactions on system performance.
Our findings underscore the need to revisit classical IR paradigms and develop new frameworks for more effective modeling and evaluation of modern retrieval systems.