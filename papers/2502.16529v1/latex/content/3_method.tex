\section{Proposed Methodology}
In this section, we present a two-stage training strategy to further improve the accuracy of VPL generation. Figure~\ref{fig:main_figure} illustrates an overview of our proposed methodology.

\subsection{Stage 1: Retrieval-augmented Fine-Tuning for Visual Program Generation (RAFT-V)}
Visual programming languages often contain recurring subroutines and modules that are reused in similar contexts~\cite{Terra_Neves_2021}. To leverage this property, we retrieve related examples from a training data pool and use them to guide the generation process. Inspired by \citet{zhang2024raft}, which incorporates relevant documents as context during fine-tuning to improve domain-specific question answering, we extend this idea to the generation of VPL code. Specifically, we retrieve relevant prompt-code pairs as context to enhance the modelâ€™s performance.
Let $\mathcal{D}$ be a training dataset consisting of \(N\) prompt-code pairs, where $p$ is a \textit{prompt} and $c$ 
is a corresponding VLP \textit{code}. Given a new input prompt \(q\), we aim to generate the target code \(c_q\). We augment the input to the model with additional prompt-code pairs that are most similar to \(q\). This process is driven by a \textit{retriever} \(\mathcal{R}\), which identifies similar prompts from \(\mathcal{D}\).  \(\mathcal{R}\) ranks all prompts in \(\mathcal{D}\) based on their similarity to \(q\). We then select the top-\(k\) most similar prompts:
\begin{equation}
    \{(p_{i}^r, c_{i}^r)\}_{i=1}^{k} = \mathcal{R}(q, \mathcal{D})
\end{equation}
where \(p_i^r\) and \(c_i^r\) denote the retrieved prompt and code, respectively. We then concatenate the original prompt \(q\) with each retrieved pair \((p_r^i, c_r^i)\) to form the augmented input $q_a$ for the model:
\begin{equation}
    q_a =\bigl(q, [(p_1^r, c_1^r), (p_2^r, c_2^r), \dots, (p_k^r, c_k^r)]\bigr)
\end{equation}
During fine-tuning, the model takes the $q_a$ and learns to generate the target code \(c_q\) from $q_a$ via SFT. At inference, the trained model predicts the code \(\hat{c_q}\) from $q_a$.


\subsection{Stage 2: Preference Optimization}
Preference optimization techniques, such as DPO~\citep{rafailov2023direct}, are widely used as a post-training step following supervised learning in reasoning tasks like code generation~\cite{hui2024qwen25codertechnicalreport, 10.5555/3600270.3601819, weyssow2024codeultrafeedbackllmasajudgedatasetaligning}. These methods rely on high-quality labels to construct preference pairs~\citep{pace2024westofnsyntheticpreferencesselfimproving}. However, in domain-specific applications, particularly in underexplored areas such as VPLs, the available preference pairs are virtually nonexistent. To mitigate this limitation, we propose a graph editing approach to construct preference pairs.

\input{latex/table/graph_augmentation}
\paragraph{Overall graph editing process}
As described in \S\ref{sec:conversion}, we transform visually structured programs into NetworkX graphs to facilitate access to nodes and edges. Based on this graphical representation, we introduce a preference pair collection method. Furthermore, we utilize Graph Edit Distance (GED)~\cite{6313167, abu2015exact} to enable the extraction of hard negative samples from the edited graphs. Algorithm~\ref{alg:graph_augmentation} provides the detailed process for sampling negative graphs, where we use 10 seeds in our experiments. Based on our empirical observations, we set the deletion ratio $\tau$ to 0.1, modifying approximately 10\% of the graph structure. Accordingly, no nodes are deleted for graphs with fewer than 10 nodes. Instead, as shown in line~\ref{alg:add}, the graph is augmented by duplicating and adding node-edge pairs. The impact of varying $\tau$ is discussed in \S\ref{sec:tau}.

\paragraph{Hard negative selection}
Preference optimization using hard negative samples is beneficial for learning subtle differences that supervised learning alone may not capture~\cite{zhu2024selfsupervised, chen2024on}.
This suggests that the difference in connectivity between hub nodes and leaf nodes is substantial, and when nodes are randomly deleted, the quality of negative data varies considerably.
Therefore, we select hard negative samples from the negative set collected via graph editing (Algorithm~\ref{alg:graph_augmentation}). We employ the GED-based metric for selection:
\begin{equation}
    \text{GED}(G,G')=\min_{T\in\mathcal{T}(G,G')}\sum\limits_{e \in T}\text{cost}(e)
\end{equation}
where $\mathcal{T}(G,G')$ is the set of all possible edit operation that transform $G'$ into $G$. Given $\mathcal{T}(G,G')$ and the cost, the GED is the minimum associative cost of each edit operation across all possible edit paths. In this study, the cost of an edit path is measured using the Levenshtein distance~\citep{black1998dictionary}. To construct preference pairs, we select the graph with the lowest edit distance to the reference graph from $\mathcal{G}=\{G'_1\dots G'_s\}$ collected through multiple seeds. We designate the graph with the lowest score as the hard negative $G_{\text{Hard}}$. Finally, we construct preference pair by pairing ground-truth graph $G$ with $G_{\text{Hard}}$. 

\paragraph{Direct preference optimization}
Preference learning is conducted using the selected preference pairs, with the retrieved prompt and its corresponding VPL code augmented as input. Implementation details are provided in Appendix~\ref{sec:implementation_details}.