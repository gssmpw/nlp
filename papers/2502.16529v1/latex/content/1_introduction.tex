\section{Introduction}
Recent advances in large language models (LLMs) have significantly improved their capabilities in code generation. Moreover, LLMs such as GPT-4~\cite{achiam2023gpt}, StarCoder~\cite{li2023starcodersourceyou}, and DeepSeek-Coder~\cite{guo2024deepseekcoderlargelanguagemodel} are able to automate large parts of programming tasks and substantially improve programmers' efficiency. Yet, despite these achievements, most of the previous research has focused on text-based programming languages (TPLs) such as Python or Java, which leaves visual programming languages (VPLs) relatively unexplored.

VPLs typically represent programs as node graphs (Figure~\ref{fig:format}), which allow users with limited programming backgrounds to create and modify programs by graphical manipulation~\cite{delozier2023using}. As such, due to their easier accessibility, VPLs have been widely adopted across various domains, from Ladder Diagram (LD) in industrial control systems~\cite{IEC61131-3} to Unreal Engineâ€™s Blueprints in game development~\cite{epicgames_blueprints}. Although VPLs lower the barrier to programming, creating such visual programs from scratch can still be cumbersome. Consequently, recent studies~\cite{zhang2024benchmarking,xue2024comfybenchbenchmarkingllmbasedagents,52868} have explored generating visual programs in text formats (e.g., JSON) from user instructions using prompting-based approaches.

While these studies have shown promising results in VPL generation, the sole reliance on prompting-based methods can be less effective for industrial VPLs like LD. This is because these languages are widely used in industrial automation, which follow domain-specific configurations (e.g., address mapping) that vary drastically by environments~\cite{alphonsus2016review}. Given the vast number of these configurations, it is challenging to include all such information in a single prompt. In contrast, training-based approaches can implicitly learn these configurations during fine-tuning. Therefore, we argue that training-based methods are necessary for this setting.

To validate this, we select LD as a test language and compare supervised fine-tuning (SFT) with retrieval-augmented generation (RAG), where the latter is a commonly used prompting-based method for VPL generation~\cite{zhang2024benchmarking,xue2024comfybenchbenchmarkingllmbasedagents}. Experimental results show that SFT outperforms RAG even with a smaller LLM backbone, which highlights the advantage of training-based approaches for industrial VPL generation.

Motivated by these findings, we propose a two-stage training approach to enhance VPL generation. First, we adopt retrieval-augmented fine-tuning, as VPL often reuses \textit{subroutines} in similar contexts~\cite{Terra_Neves_2021}. During both training and inference, we retrieve similar examples from a corpus and incorporate them into the model input to improve generation quality. Second, we apply offline direct preference optimization (DPO)~\cite{rafailov2023direct} to further guide the model toward accurate outputs.
Since VPLs are represented as node-based graphs, we construct preference pairs by systematically applying graph editing operations to transform ground-truth (preferred) VPL code into unpreferred variants. By learning from these pairs, the model captures the preference relationship between the preferred and unpreferred code, thereby enhancing its fine-grained understanding of VPL's structural properties.


To evaluate our approach, we collect LD data from an actual production environment and conduct extensive evaluations across various text formats and models. Results demonstrate a consistent improvement in generation performance. Notably, our approach improves exact-match accuracy at the program level by more than 10\% compared to SFT. The advantages of our methodology are substantiated by further analyses and ablation studies.

Our contributions are as follows:
(1) We present a pioneering study on training-based VPL generation and demonstrate its effectiveness for industrial VPL.
(2) We propose a two-stage training strategy that combines retrieval-based fine-tuning and preference optimization via graph editing, which yields significant performance improvements.
(3) To the best of our knowledge, this is the first work to effectively generate Ladder Diagrams using LLMs, underscoring its potential to enhance industrial automation.