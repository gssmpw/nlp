\section{\label{sec:result}Results \& Discussion}

\subsection{\label{sec:overall_performance}Main Results}
\paragraph{Stage 1 (RAFT-V) improves upon SFT} Table~\ref{tab:main_result} compares the performance of SFT, RAFT-V, and our two-stage method across different output formats (XML, JSON, metaprogram) with two backbone models. By incorporating retrieval augmentation, RAFT-V consistently outperforms SFT and significantly enhances VPL generation. For example, in the JSON format, RAFT-V raises the Program EM from 52.6\% to 63.2\%, demonstrating more accurate VPL generation. Similar improvements are observed for other metrics.

\paragraph{Preference optimization (Stage 2) yields further gains} Building on Stage 1's improvements, Stage 2 further boosts correctness through preference optimization, particularly in terms of exact match (EM) scores. As shown in Table~\ref{tab:main_result}, our two-stage approach improves Program EM over RAFT-V by 3.4\% in the metaprogram format and achieves an overall 13.2\% gain compared to SFT. Because EM only assigns a score when the entire graph exactly matches, even minor generation failures can significantly impact the metric. These results show that preference optimization reduces such minor generation errors and ensures more precise outputs. Notably, these gains are consistent across all output formats and base models, demonstrating the robustness of our approach.\footnote{See Appendix~\ref{sec:human_eval} for human evaluation details.}

\subsection{\label{sec:retrieved_examples}Impact of Number of Retrieved Examples}
\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
\( k \) & Node F1 & Edge F1 & Node EM & Edge EM & Program EM \\ \midrule
\rowcolor[HTML]{EFEFEF}
1 (Ours)      & 89.9    & 82.4    & 64.6    & 63.8    & 63.8     \\
2       & 90.6    & 83.5    & \textbf{66.4} & \textbf{65.6} & \textbf{65.6} \\
3       & \textbf{90.9} & \textbf{84.0} & 65.4    & 64.6    & 64.6     \\ \bottomrule
\end{tabular}%
}
\caption{Performance of RAFT-V across different numbers of retrieved examples $k$. We use the Llama3.1-8B-Instruct model with metaprogram format.}
\label{tab:retrieval_quantity_performance}
\end{table}
To assess whether the number of retrieved examples affects performance, we evaluate RAFT-V trained with different numbers of retrieval examples \( k \in \{1, 2, 3\} \) (Table~\ref{tab:retrieval_quantity_performance}). Although increasing retrieval examples \(k\) generally improves generation quality, the gains are marginal, which indicates that even a few examples are able to capture functional patterns in visual programming languages. Based on this observation, we set \( k = 1 \) for our main experiments.


\subsection{\label{sec:tau}Variation in Deletion Ratio}
\input{latex/table/dpo_tau}
Furthermore, we report the performance variation depending on the deletion ratio $\tau$. We train the model using preference pairs with different $\tau$ values, where $\tau \in \{0, 0.1, 0.5, 0.9\}$. Table~\ref{tab:dpo_tau} shows that F1 scores remain stable across $\tau$ values, while EM scores decrease as $\tau$ increases. In particular, when $\tau$ is 0.9, the negative samples during preference training are graphs with 90\% of the original graph removed. As a result, the model can easily distinguish them, making the training less effective. Based on these results, we set $\tau$ to 0.1.

\input{latex/table/dpo_aug}
\subsection{\label{sec:data_augmentation}Analysis of Graph Editing}
We validate our graph editing approach for collecting preference pairs by comparing it with Best-of-N (BoN) sampling\footnote{For BoN sampling, we use nucleus decoding~\citep{Holtzman2020The} with temperature=1.0, top\_p=1.0.}~\citep{10.5555/3495724.3495977, snell2025scaling}, which selects the best generation among $N$ candidates.
We generate $N=10$ outputs from the RAFT-V model and compare them with our negative candidate set $\mathcal{G}=\{G'_1\dots G'_{10}\}$, which is derived from graph editing (algorithm~\ref{alg:graph_augmentation}).

\paragraph{Preference learning is effective}
To address the concern that the gains in the 2-stage approach may result from the additional data used for preference learning, we compare it with a 1-stage approach, where the model is trained on all the data without a second stage, and is referred to as \underline{RAFT-V (100\%)}. As shown in Table~\ref{tab:dpo_aug}, RAFT-V improves performance as the dataset size increases. However, RAFT-V (100\%) shows only marginal improvement in EM scores compared to preference-learned models. Although RAFT-V (100\%) demonstrates effectiveness in improving F1 scores when compared to BoN-based preference learning baselines, it is insufficient to minimize minor errors (EM).

\paragraph{Editing-based negative selection is efficient}
We introduce four baselines for comparison with graph editing: \underline{BoN (Random)}, where $\mathcal{G}$ is sampled from BoN, and $G_{\text{Hard}}$ is randomly selected from BoN-sampled $\mathcal{G}$; \underline{BoN (Unstrict)}, which selects preference pairs based on GED; and \underline{BoN (Strict)}, which considers preference pairs for training only when the sampled output includes an exact match with the correct answer (i.e., GED = 0).  Finally, in the case of \underline{Ours (Random)}, $\mathcal{G}$ is sampled from graph editing, but $G_{\text{Hard}}$ is randomly selected instead of using GED.

As shown in Table~\ref{tab:dpo_aug}, BoN (Strict) constructs preference pairs based on exact matches and achieves the highest accuracy among BoN-based methods despite utilizing only 30\% of the data. This result demonstrates that dataset quality has a more significant impact on preference learning than dataset size~\cite{hou2024doesrlhfscaleexploring, kim2024aligninglargelanguagemodels}. However, BoN sampling often fails to generate challenging cases consistently, as negative samples are selected from the sampled outputs. In contrast, our editing-based negative selection provides a more systematic approach to generating negative samples, which consistently enables the generation of hard negative pairs with higher efficiency. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\columnwidth,keepaspectratio]{latex/figure/total_exact_match_accuracy_by_complexity.pdf}
    \caption{Program EM score across different complexities. We use the metaprogram format with the Llama3.1-8B-Instruct model.}
    \label{fig:total_EM_trend_complexity}
\end{figure}
\subsection{\label{sec:difficulty}Performance Across Program Complexity}
To evaluate our methodology's performance across varying program complexities, we convert each program (VPL code) in the test split into NetworkX graphs. We define complexity as the total number of nodes and edges in the graph. We then sort programs by complexity and split them into five percentile ranges: 0–20\%, 20–40\%, 40–60\%, 60–80\%, and 80–100\%, labeling them from 1 to 5. Figure~\ref{fig:total_EM_trend_complexity} shows the average Program EM across these categories. Our approach consistently outperforms the SFT baseline, widening the gap at higher complexities (+18.3\% in 4, +16.3\% in 5). These findings highlight our method's robustness, with benefits increasing in challenging scenarios.