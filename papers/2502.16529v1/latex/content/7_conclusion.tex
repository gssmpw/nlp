\section{Conclusion}
In this paper, we have demonstrated the importance of fine-tuning for VPL generation and introduced a novel two-stage training strategy. By combining retrieval augmentation leveraging repetitive subroutines in VPLs with preference learning through graph editing, we achieved significant improvements in LD generation. Our work marks a crucial step toward LLM-based LD generation, and given that the method leverages VPL characteristics, it holds potential for broader applicability. 
