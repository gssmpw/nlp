\input{latex/table/effect}

\section{Experimental Settings}\label{sec:exp_setup}
\paragraph{Dataset} Due to the lack of publicly available datasets, we created our own by annotating ladder diagrams from actual production environments. Using XG5000~\cite{XG5000Manual}, we exported these diagrams as XML files and divided them into functional units, where each unit consists of one or more interconnected rungs designed to perform a specific function. An experienced PLC programmer then annotated natural language instructions for each unit. The dataset was randomly split into training, validation, and test sets of 13,124, 500, and 500 samples. 80\% of the training data was used for SFT, while the remaining 20\% was used for preference learning. Appendix~\ref{sec:data_samples} shows an example of the dataset utilized.

\paragraph{Implementation details} We utilize Llama-3.1-8B-Instruct~\cite{dubey2024llama} as the main backbone model and also use Qwen2.5-7B-Instruct~\cite{yang2024qwen2} to assess the generalizability of our method. To facilitate task understanding, we provide a detailed task description and explanations of the visual elements used in ladder diagrams via the system prompt. For models using retrieval, we utilize BM25~\cite{robertson2009probabilistic}, which is a widely used lexical matching-based method. Using BM25, we augment the input to these retrieval-based models with a top-1 retrieved prompt-code pair from the training dataset\footnote{System prompts are in Appendix~\ref{sec:system_prompt}, and further implementation details are in Appendix~\ref{sec:implementation_details}.}.

\paragraph{Evaluation metrics}
Text-based programming languages like Python are typically evaluated for correctness by unit tests~\cite{chen2021evaluating,austin2021programsynthesislargelanguage, chen2023codet}. In contrast, industrial visual programming languages often rely on separate simulators for evaluation, which limit consistent automated assessment across program variations and diverse environments~\cite{ray2017survey, ren2024infiniteworldunifiedscalablesimulation}.

To address this, we introduce graph-based automatic evaluation by transforming visually structured programs into NetworkX, as introduced in \S\ref{sec:conversion}. Specifically, we measure partial correctness in the graph representation by comparing it with the ground-truth graph using Node F1 and Edge F1 at the node and edge levels. For exact matches, we use Node EM and Edge EM to assess complete accuracy at the node and edge levels. Finally, Program EM evaluates whether the entire graph matches the ground-truth graph precisely in terms of both nodes and edges. Detailed explanations are provided in Appendix~\ref{sec:metric}.