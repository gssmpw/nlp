\section{Further Implementation Details}\label{sec:implementation_details}
\paragraph{Supervised fine-tuning (including RAFT-V)}
We fine-tuned models using SFT with 10 epochs, a batch size of 8, and a learning rate of $5 \times 10^{-5}$. We applied LoRA~\cite{hulora} with a rank of 256, $\alpha = 256$, and a dropout rate of 0.05. LoRA adaptation was applied to the following target modules: q\_proj, v\_proj, k\_proj, o\_proj. Using the AdamW~\cite{loshchilov2018decoupled} optimizer, we minimized the cross-entropy loss $\mathcal{L}_{\text{CE}}$ between ground-truth code $c$ and the predicted code $\hat{c}$ as follows:
\begin{equation}
\nonumber
    \mathcal{L}_{\text{CE}} = - \sum_{t=1}^{T} \sum_{v \in V} c_t(v) \log \hat{c}_t(v)
\end{equation}
where $T$ is the sequence length and $V$ is a vocabulary size. The SFT process took 6 hours using 4 A100-80GB GPUs, while the SFT process for RAFT-V took 8 hours using the same hardware configuration.

\paragraph{Preference learning} For preference learning, we utilized Direct Preference Optimization (DPO)~\cite{rafailov2023direct}, and trained models for 5 epochs with a batch size of 64. The learning rate was set to $1 \times 10^{-7}$, with a warmup ratio of 0.03, a weight decay of 0.01, and $\beta=0.1$. As with SFT, {LoRA} was applied with the same rank, $\alpha$, dropout rate, and target modules.

Given a pair of responses, a preferred response $y^+$ and a dispreferred response $y^-$ for a given input $x$, we minimize the following DPO loss:

\begin{equation}
\nonumber
    \mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x,y^+, y^-) \sim \mathcal{D}} \left[ 
    \log \sigma \left( \beta \left( \log \frac{\pi_\theta(y^+ \mid x,\mathcal{R}(x))}{\pi_{\text{ref}}(y^+ \mid x,\mathcal{R}(x))} - \log \frac{\pi_\theta(y^- \mid x,\mathcal{R}(x))}{\pi_{\text{ref}}(y^- \mid x,\mathcal{R}(x))} \right) \right) 
    \right]
\end{equation}
where $\sigma(z)$ is the sigmoid function, $\beta$ is a scaling factor controlling the strength of preference optimization, $\mathcal{R}(x)=(p^r, c^r)$ represents a retrieved prompt and its corresponding VPL code obtained from a retriever $\mathcal{R}$, and $\mathcal{D}$ represents the dataset of preference-labeled samples. The reference model $\pi_{\text{ref}}$ serves as a baseline to prevent reward overoptimization, ensuring stable preference learning. The preference training stage took 6 hours on 4 A100-80GB GPUs.
We measured the loss on the validation set at each epoch in both training stages and applied early stopping based on this criterion, with a patience value of 2. 
\paragraph{Sampling parameters} We employed beam search decoding with a beam size of 4 during inference. 
