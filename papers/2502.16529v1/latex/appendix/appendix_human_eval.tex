\section{\label{sec:human_eval}Human Evaluation}

\begin{table*}[!htbp]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
Method        & Functional Score ($\uparrow$) & Kappa Score ($\uparrow$) \\ \midrule
SFT-only      & 4.34            &       0.62      \\
RAFT-V        & 4.60            &         0.58    \\
Ours & \textbf{4.62} & \textbf{0.66}   \\ \bottomrule
\end{tabular}
\caption{Average of human evaluation results. We used the metaprogram format for evaluation.}
\label{tab:human_evaluation}
\end{table*}

To further evaluate the effectiveness of our methods, we conduct human evaluations on a randomly selected 50 test set examples. Due to the dataset's security sensitivity, human evaluations were conducted exclusively by PLC engineers who had authorized access to confidential information. 
Three experienced LD programmers evaluated the generated code based on functionality, and assigned scores on a scale of 1 to 5.
The ratings were based on their professional experience and the practical applicability of the code in real industrial settings. The results are shown in Table~\ref{tab:human_evaluation}. Compared to SFT-only, RAFT-V showed an improvement of 0.26 points, while our method outperformed RAFT-V by 0.02 points. Furthermore, we report Fleiss' kappa coefficient~\citep{fleiss1971measuring} to statistically evaluate the level of agreement among human evaluators. The results indicate that our proposed methodology demonstrates the highest degree of inter-rater consistency.
