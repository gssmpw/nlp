\section{Related Work}
Although various studies have introduced methods for video action segmentation **Kitani, "Action Bank: A Large-Scale Database for Human Action Recognition"**, **Sadanand, "Action bank: A visual sensor network approach to understanding human action in 3D space-time"** and **Liu, "Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video"** these methods typically rely on direct use of video data, often from extensive benchmark datasets. Additionally, since these methods are designed to infer the final labels directly from the video, they face challenges in providing intermediate process information to therapists. Furthermore, if the criteria for segmentation change, the entire dataset must be retrained, posing a significant drawback.

Moreover, the datasets used in these studies typically involve labeling based on the overall assessment of human actions, rather than distinguishing fine hand movements. This makes it challenging to apply these methods to our real-world data. Furthermore, in the ASAR dataset, learning relationships between entire frames is inefficient since areas of interest are predefined, whereas models trained on benchmark datasets typically consider the entire frame, including backgrounds. To address this, we divide the task into 2D object detection and temporal segmentation. For hand landmark detection, we use Google's MediaPipe **Zhou, "MediaPipe: A Framework for Efficient Hand Tracking"**, and for object detection, among various CNN-based models **Lin, "Focal Loss for Dense Object Detection"** and transformer-based models **Carion, "End-to-End Object Detection with Transformers"**, we choose TridentNet **Liu, "TridentNet: A Pyramid Set of Attentions Applied on Task-Aware Aggregated Representations"** for its computational efficiency achieved through weight-sharing mechanisms.

For temporal segmentation, statistical methods like cusum **Page, "A Test for a Change in a Parameter Estimating the Binomial Distribution"** and Bayesian change point detection **Fearnhead, "Online Changepoint Detection with Parametric Models"** are available but they are complex and sensitive to noise. In contrast, deep-learning models **Kumar, "Temporal Convolutional Networks for Action Recognition"** offer superior flexibility, robustness, and scalability. We focus on transformers for their ability to capture the overall context, applying them in our research. In the previous work **Zhao, "Action Segmentation with Attention-Based Temporal Context Modeling"**, a fusion model was developed that combined data-driven models (MSTCN++, **Zhang, "Multiscale Temporal Convolutional Network for Action Recognition"**) and prior knowledge-driven models (HMM, **Rabiner, "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition"**) to learn the complex bi-directional inter-state relationship between the segments. This work demonstrated the efficacy of action segmentation for automated movement quality assessment. However, the number of states is greater in the previous home-based setup compared to the ASAR dataset, and the state relationships are bi-directional, making the overall structure more complex.

 \begin{figure}[]
\begin{center}
\centerline{\includegraphics[width=1.1\columnwidth]{figures/model_architecture2.png}}
\caption{Overview of the three-phase process for temporal keypoint-based video action segmentation using a transformer model. This includes the detection of keypoints (such as object centers and hand landmarks), the refinement of the detected outcomes, and the use of the refined time-series data to train a transformer-based model for accurate timestamp prediction.}
\label{fig:Overview}
\end{center}
\end{figure}

\begin{figure*}[]
  \centering
  \includegraphics[width=2\columnwidth]{figures/hand_obj_res.png}
  \caption{The results of object and hand landmark detection on a sample frame extracted from a video. The left image shows the input frame, the middle image displays the object detection results obtained by TridentNet, and the right image visualizes the 21 hand keypoints extracted using MediaPipe.}
\label{fig:hand_object_detection}
\end{figure*}