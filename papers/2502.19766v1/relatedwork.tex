\section{Related Work}
Although various studies have introduced methods for video action segmentation \cite{JiPAMI2010, Wang2016ECCV, Neimark2021, Jiang2023}, these methods typically rely on direct use of video data, often from extensive benchmark datasets. Additionally, since these methods are designed to infer the final labels directly from the video, they face challenges in providing intermediate process information to therapists. Furthermore, if the criteria for segmentation change, the entire dataset must be retrained, posing a significant drawback.

Moreover, the datasets used in these studies typically involve labeling based on the overall assessment of human actions, rather than distinguishing fine hand movements. This makes it challenging to apply these methods to our real-world data. Furthermore, in the ASAR dataset, learning relationships between entire frames is inefficient since areas of interest are predefined, whereas models trained on benchmark datasets typically consider the entire frame, including backgrounds. To address this, we divide the task into 2D object detection and temporal segmentation. For hand landmark detection, we use Google's MediaPipe \cite{Lugaresi2019}, and for object detection, among various CNN-based models \cite{LinCVPR2017, LiICCV2019} and transformer-based models \cite{CarionECCV2020}, we choose TridentNet \cite{LiICCV2019} for its computational efficiency achieved through weight-sharing mechanisms.

For temporal segmentation, statistical methods like cusum \cite{Page1954} and Bayesian change point detection \cite{adams2007bayesian} are available but they are complex and sensitive to noise. In contrast, deep-learning models 
 \cite{Hochreiter1997, LeaCVPR2017, Cho-etal-2014-learning, VaswaniNeurIPS2017} offer superior flexibility, robustness, and scalability. We focus on transformers for their ability to capture the overall context, applying them in our research. In the previous work \cite{AhmedFrontiers2021}, a fusion model was developed that combined data-driven models (MSTCN++ \cite{Li2023}, Transformer \cite{VaswaniNeurIPS2017}) and prior knowledge-driven models (HMM \cite{Rabiner1989}, rule-based decision tree) to learn the complex bi-directional inter-state relationship between the segments. This work demonstrated the efficacy of action segmentation for automated movement quality assessment. However, the number of states is greater in the previous home-based setup compared to the ASAR dataset, and the state relationships are bi-directional, making the overall structure more complex.

 \begin{figure}[]
\begin{center}
\centerline{\includegraphics[width=1.1\columnwidth]{figures/model_architecture2.png}}
\caption{Overview of the three-phase process for temporal keypoint-based video action segmentation using a transformer model. This includes the detection of keypoints (such as object centers and hand landmarks), the refinement of the detected outcomes, and the use of the refined time-series data to train a transformer-based model for accurate timestamp prediction.}
\label{fig:Overview}
\end{center}
\end{figure}

\begin{figure*}[]
  \centering
  \includegraphics[width=2\columnwidth]{figures/hand_obj_res.png}
  \caption{The results of object and hand landmark detection on a sample frame extracted from a video. The left image shows the input frame, the middle image displays the object detection results obtained by TridentNet, and the right image visualizes the 21 hand keypoints extracted using MediaPipe.}
\label{fig:hand_object_detection}
\end{figure*}