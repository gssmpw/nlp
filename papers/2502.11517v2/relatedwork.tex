\section{Related Work}
After discussing parallel decoding approaches in \Cref{sec:taxonomy}, we now turn to other relevant research areas.




\textbf{Agent Planning/Tool Use.}
Our work is related to the idea of agent planning and tool use \citep{yao2023react, schick2023toolformer, shen2023hugginggpt, liang2023taskmatrixai, lu2023chameleon}.
Prior studies show that LLM-based agents can solve complex tasks by planning and using tools such as web search and external APIs.
Our work extends the suite of tools available to LLMs with the \lang{} language and interpreter for improving their own decoding efficiency.

\textbf{Approximate Parallelization.}
Our work extends the idea of approximate parallelization \citep{10.1145/1993498.1993555, 10.1145/2414729.2414738}.
\citet{10.1145/1993498.1993555} proposed a framework that enables programmers to annotate breakable data dependencies in a program and 
developed a compiler and runtime that exploits these annotations to automatically parallelize otherwise sequential regions of code.
\citet{10.1145/2414729.2414738} opportunistically relaxes synchronization primitives in a parallel program to improve parallelism, to program outputs that are acceptably close to the original one.
Similarly, we break the sequential decoding process of LLMs into approximately parallelizable and independent components and exploit the parallelism to improve decoding efficiency.
Our work differs in that instead of relying on end-user annotation or compiler analysis, we teach LLMs to autonomously express parallelism in their own decoding process using the \lang{} annotation language.


\vspace{-.5em}