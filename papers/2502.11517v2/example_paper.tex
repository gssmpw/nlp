
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage{subcaption}
\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\ayazdan}[1]{\textcolor{purple}{}}


\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{hyperref}
\usepackage{url}
\usepackage{ulem}
\normalem
\usepackage[hide=true]{marginalia}
\usepackage{xcolor} %
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{circledsteps}
\input{commands}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}
\usepackage[most]{tcolorbox}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}



\lstdefinestyle{mystyle}{
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    showspaces=false, 
    breakindent=0pt,
}

\lstset{style=mystyle}

\newtcolorbox{pillbox}[2][]{colback=blue!10!white, colframe=blue!50!black,
    fonttitle=\bfseries, title=#2, sharp corners=south, 
    rounded corners=north, #1}


\newcommand{\ns}[1]{{\color{orange}[NS: #1]}}
\newcommand{\nsedit}[1]{{\color{blue}#1}}


\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Learning to Keep a Promise}
\begin{document}
\twocolumn[
\icmltitle{Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with \\
Learned Asynchronous Decoding}



\icmlsetsymbol{equal}{*}
\icmlsetsymbol{intern}{\textdagger}

\begin{icmlauthorlist}
\icmlauthor{Tian Jin}{equal,mit,intern}
\icmlauthor{Ellie Y. Cheng}{equal,mit}
\icmlauthor{Zack Ankner}{mit}
\icmlauthor{Nikunj Saunshi}{gr}
\icmlauthor{Blake M. Elias}{google}
\icmlauthor{Amir Yazdanbakhsh}{gdm}
\icmlauthor{Jonathan Ragan-Kelley}{mit}
\icmlauthor{Suvinay Subramanian}{google}
\icmlauthor{Michael Carbin}{mit}
\end{icmlauthorlist}

\icmlaffiliation{mit}{MIT CSAIL, Cambridge, USA}
\icmlaffiliation{google}{Google, Mountain View, USA}
\icmlaffiliation{gr}{Google Research, New York, USA}
\icmlaffiliation{gdm}{Google DeepMind, Mountain View, USA}


\icmlcorrespondingauthor{Tian Jin}{tianjin@csail.mit.edu}
\icmlcorrespondingauthor{Michael Carbin}{mcarbin@csail.mit.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution,\icmlInternship} %

\begin{abstract}
 

Decoding with autoregressive large language models (LLMs) traditionally occurs sequentially, generating one token after another.
An emerging line of work explored parallel decoding by identifying and simultaneously generating semantically independent chunks of LLM responses. 
However, they rely on hand-crafted heuristics tied to syntactic structures like lists and paragraphs, making them rigid and imprecise. 
We present \pasta{}, a learning-based system that teaches LLMs to identify semantic independence and express parallel decoding opportunities in their own responses.
At its core are \lang{} and its interpreter: \lang{} is an annotation language that enables LLMs to express semantic independence in their own responses; the language interpreter acts on these annotations to orchestrate parallel decoding on-the-fly at inference time. 
Through a two-stage finetuning process, we train LLMs to generate \lang{} annotations that optimize both response quality and decoding speed.
Evaluation on AlpacaEval, an instruction following benchmark, shows that our approach Pareto-dominates existing methods in terms of decoding speed and response quality; our results demonstrate geometric mean speedups ranging from 1.21× to 1.93× with corresponding quality changes of +2.2\% to -7.1\%, measured by length-controlled win rates against sequential decoding baseline.

\end{abstract}

\section{Introduction}



Autoregressive decoding is a fundamental efficiency bottleneck in large language model (LLM) inference.
Contemporary LLMs routinely require multiple seconds or even minutes of decoding time to complete user requests \citep{jiang2024mixtralexperts,touvron2023llamaopenefficientfoundation, openai2024openaio1card, deepseekai2025deepseekr1incentivizingreasoningcapability}.
This latency stems from the sequential nature of autoregressive decoding, which leads to inefficient hardware utilization during inference.
While training achieves 40-60\% Model Flops Utilization (MFU) \citep{korthikanti2022reducingactivationrecomputationlarge}, inference typically achieves less than 20\% MFU \citep{pope2022efficientlyscalingtransformerinference}.



\textbf{Semantic Independence.}
Recent works like Skeleton-of-Thought (SoT) and APAR leverage semantic independence in LLM responses as a source of parallelism, decoding independent chunks of tokens in parallel.
Namely, these methods decode semantically independent \emph{chunks} (contiguous sequences of tokens) of tokens in the response in parallel.
Given a request, SoT first produces a bullet-point outline, then applies regular-expression-based syntactic pattern matching to extract points that are then expanded in parallel.
APAR, in contrast, applies regular-expression-based syntactic pattern matching on training data to identify structures like lists and paragraphs, and finetunes an LLM to decode in parallel the item descriptions given list items and the paragraph bodies given the first sentences.




\begin{figure*}
\vspace{-0.7em}
    \centering
    \includegraphics[
  width=0.95\linewidth,
  trim=25 610 20 25,
  clip
]{assets/main/pasta-intro.pdf}
    \vspace{-1em}
\caption{
Example response from a \pasta{} model executed by the \lang{} interpreter. 
The interpreter begins with only the main thread. 
It first decodes \CircledText{1}, and it creates an asynchronous decoding thread, which decodes \CircledText{4} in red.
In parallel, the main thread decodes \CircledText{2}. 
It creates another asynchronous decoding thread, which contains both the \promise{} tag on coordinates extraction and the \promise{} tag on length formula in its prefix, and decodes \CircledText{5} in green.
The main thread continues decoding in parallel to both threads to get \CircledText{3}.
It wait at this point until all other threads complete. 
The interpreter then inserts each asynchronous content after their corresponding \promise{} tags.
Finally, the interpreter decodes \CircledText{6}, with both of the asynchronously decoded content in the prefix.
}
\label{fig:example}
\vspace{-1em}
\end{figure*}


While semantic independence broadly exists in LLM responses, relying purely on syntactic heuristics -- manifested as hand-crafted regular expressions -- to identify them has limitations.
First, these heuristics lack scalability, requiring manual engineering to capture more semantic independence, even as more training compute becomes available. 
Second, they lack robustness, failing to detect semantic independence when responses deviate from expected patterns, even by a missing punctuation mark.
These limitations motivate a learning-based approach to optimize LLMs' ability to identify semantic independence, enabling LLMs to find parallelization opportunities beyond fixed patterns.







\textbf{Learned Parallel Decoding.} 
We present \pasta{}, a system that teaches LLMs to identify and annotate parallelization opportunities in their own responses.
Our system consists of a set of annotations that extend the model's vocabulary for asynchronous decoding, an interpreter that acts on these annotations to orchestrate parallel decoding, and a finetuning procedure that optimizes LLMs' ability to identify and express parallelization opportunities.
Through this system, LLMs develop and execute their own asynchronous decoding strategies.
In \Cref{fig:example}, we show how these components implement asynchronous decoding.

\textbf{Annotations.}
Our annotation language, \lang{} (\underline{PA}rallel \underline{ST}ructure \underline{A}nnotation \underline{LANG}uage), enables LLMs to express semantic independence in their responses.
In \Cref{fig:example}, we show a \lang{}-annotated response. 
The \promise{} tags serve as placeholders for content chunks that are semantically independent to each other, such as  extracting coordinates (Tag \CircledText{1}) and recalling the line segment length formula (Tag \CircledText{2}). 
Each \promise{} tag includes a \topic{} attribute that concisely describes the chunk.
When further decoding steps require conditioning on tokens that are still being asynchronously decoded, the LLM issues an \sync{} tag to indicate so, as shown at \CircledText{3} in \Cref{fig:example}.

\textbf{Interpreter.}
We develop the \lang{} interpreter, which acts on \lang{} annotations to orchestrate asynchronous decoding during inference.
It launches parallel decoding threads for semantically independent contents marked with \promise{} tags and synchronizes them at \sync{} tags. 
The interpreter simultaneously decodes multiple non-contiguous token chunks from the LLM, improving overall decoding latency.















\textbf{Finetuning.}
Training an LLM to generate \lang{} annotations starts with two manual inputs: seven human-crafted demonstrations and a description of the \lang{} annotation language.
Prompting the Gemini 1.5 Flash model~\citep{geminiteam2024geminifamilyhighlycapable} with these manual inputs, the \pasta{} system initiates an automated two-stage finetuning process.
In the first stage, \pasta{} uses the prompted Gemini model to create the \pastasft{} dataset by annotating the SlimOrca instruction-finetuning dataset~\citep{SlimOrca} with \lang{} annotations that identify semantically independent chunks compatible with asynchronous decoding.
\pasta{} then finetunes an LLM on this dataset to produce a model that generates \lang{} annotations.

In the second stage, \pasta{} creates another dataset by sampling the finetuned LLM and scoring each output based on its quality and latency.
Unlike traditional uses of preference optimization to improve response quality ~\citep{gui2024bonbonalignmentlargelanguage, rafailov2023direct}, we adapt one such algorithm for \pasta{} to optimize for both output quality and latency.
\pasta{} applies preference optimization to the finetuned LLM on this dataset to produce a model with improved output quality and latency.
This second stage of finetuning features a quality weight hyperparameter that controls the trade-off between quality and speedup.
Through repeated iterations of the second stage, \pasta{} creates models that respond with increasingly better quality and lower latency.

\textbf{Results.}
Varying the quality weight hyperparameter, \pasta{} produces a suite of models with different quality-latency trade-offs.
We evaluate these models on 805 representative instruction-following prompts from AlpacaEval~\cite{alpaca_eval, dubois2024length}.
After one iteration of preference optimization, these models Pareto-dominate all existing asynchronous decoding methods.
Additional iterations of preference optimization further improve the speedup-quality Pareto frontier, showing no signs of saturation even after two iterations.
Our results demonstrate geometric mean speedups ranging from 1.21x to 1.93x\footnote{\label{note1}Geometric mean should be used to compute normalized values~\citep{fleming1986not}. 
However, the prevailing practice in parallel decoding literature uses arithmetic averaging when reporting speedup, which would show this result as 1.57-2.6$\times$.} with corresponding quality changes of +2.2\% to -7.1\% respectively, measured as in length-controlled win rates












\textbf{Contribution.} We present a collection of contributions:
\begin{itemize}[leftmargin=*, topsep=0px, itemsep=0px]
    \item We design \lang{} to be an annotation language that enables LLMs to annotate semantically independent chunks of tokens in their own responses.
    \item  We implement a \lang{} interpreter that efficiently orchestrates asynchronous decoding based on \lang{} annotations at inference time.
    \item We develop a two-stage finetuning technique that trains LLMs to identify diverse patterns of semantic independence in their output and express them through \lang{} annotations, while directly optimizing for both response quality and inference speedup.
    \item We evaluate our method on AlpacaEval~\citep{alpaca_eval, dubois2024length}, a suite of 805 representative instruction-following prompts, and find our method Pareto-dominate all existing asynchronous decoding methods in terms of quality and speedup.
\end{itemize}
    
    

\textbf{Implication.}
\pasta{} demonstrates the utility of incorporating latency objectives into the standard preference optimization step during LLM post-training. 
The effectiveness and scalability of this approach makes it a practical prescription for reducing LLM decoding latency.


\section{Asynchronous Decoding}
\label{sec:taxonomy}

To provide context for how \lang{} relates to other parallel decoding techniques for accelerating LLM decoding, we present a dichotomy of parallel decoding techniques.
Specifically, a given parallel decoding strategy can be categorized as either performing \emph{synchronous} or \emph{asynchronous} decoding.
In synchronous decoding, only a single chunk is decoded in parallel while the rest of the generation is halted.
In contrast, during asynchronous decoding, multiple chunks of of the language model's output are decoded independently in parallel.


We consider speculative decoding~\citep{leviathan2023fast,chen2023accelerating, stern2018blockwise,cai2024medusa, ankner2024hydra, he2023rest,fu2024break,spector2023accelerating,santilli2023accelerating} as a prototypical example of synchronous decoding.
It decodes multiple tokens within a single chunk in parallel, but must complete that chunk before moving on to any subsequent tokens.

In contrast, works such as SoT~\citep{ning2023skeleton} and APAR~\citep{liu2024apar} implement asynchronous decoding techniques.
Both methods enable decoding to jump ahead in the output sequence and generate tokens before previous positions are filled, resulting in multiple chunks of the output being decoded in parallel.
While our \pasta{} system also implements asynchronous decoding, we improve upon previous works by employing a learning-based system to identify parallelization opportunities, instead of relying on human-defined heuristics.
By training an LLM to identify and exploit parallelization opportunities, \pasta{} achieves Pareto-optimal trade-off between speedup and quality as compared to previous asynchronous decoding techniques.
























\section{Language and Interpreter Design}
\label{sec:language-design}
\lang{} is an XML-like annotation language designed for a language model to annotate semantic independence in its own response.
We present the syntax of the language and the operations of the interpreter in this section.

\textbf{Syntax.}
\lang{} defines three tags: \async{} tags which appear in pairs to wrap around blocks of content, and two standalone tags \promise{} and \sync{}. A \promise{} tag requires two attributes: a string attribute \topic{} and an integer attribute \tokens{}, and must appear before the content block it refers to.






\begin{figure*}[]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=10px 140px 10px 5px, clip]{assets/main/forking.html.pdf}
        \vspace*{-4mm}
        \caption{\lang{} interpreter orchestrates parallel decoding. 
        \CircledText{A} shows the user prompt.
        \CircledText{B} shows the \promise{} tag which initiates the first asynchronous decoding thread named ``Fork\#1''.
        \CircledText{C} indicates where the interpreter appends an \async{} tag to the prefix of Fork\#1, signaling Fork\#1 should complete the promised content with topic ``coordinates''.
        \CircledText{D} denotes the asynchronous generation by Fork\#1. 
        \CircledText{E} shows the \sync{} tag where the interpreter pauses to wait for all asynchronous generations.
        \CircledText{F} shows the main thread decodes the remaining content with both asynchronous generations in its prefix.
        }
        \label{fig:interpreter-detail}
    \end{subfigure}
    \vspace{-0.6em}
    
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=.95\linewidth, clip, trim={20px 0px 20px 0px}]{assets/main/parallel-decoding.html.pdf}
        \vspace*{-2mm}
        \caption{
        Decoding parallelism and attention patterns at each timestamp. At each timestamp, we show the tokens decoded in parallel in that timestamp. The directed edges between tokens show the attention relationships: each edge connects a token to the very next token that may attend to it. A token can then attend to any tokens that can be reached by following these edges back through the graph. \CircledText{A} shows the last few tokens of the user query. \CircledText{B} shows when the interpreter decodes a \promise{} token, after which it immediately appends an \async{} token for Fork\#1 at \CircledText{C}. Subsequently at \CircledText{D}, Fork\#1 begins asynchronous decoding, while in parallel, the interpreter creates another decoding thread (Fork\#2). At \CircledText{E}, when encountering the \sync{}, the interpreter pauses the main thread until all asynchronous threads complete. Finally at \CircledText{F}, the main thread resumes decoding with both asynchronously decoded content in its prefix.}
        \label{fig:parallelism}
    \end{subfigure}

    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth, clip, trim={20px 510px 0px 0px}]{assets/main/kv-cache.html.pdf}
        \vspace*{-4mm}
        \caption{
        KV-cache layout throughout parallel decoding. At \CircledText{A}, the last few tokens of the user prompt, the KV-cache is laid out in typical contiguous manner. Starting at \CircledText{B}, the KV-cache begins to interleave between threads, while inserting the corresponding \async{} token for the new thread at \CircledText{C}. \CircledText{D} shows parallel decoding in progress, with tokens from Fork\#1 being generated while the main thread continues decoding. At \CircledText{E}, the \sync{} token (decoded by the main thread at T=3) is inserted into the KV-cache. After synchronization, the KV-cache returns to a contiguous layout at \CircledText{F}.
        }
        \vspace{-0.5em}
        \label{fig:kv-cache-management}
    \end{subfigure}
    
    \caption{Details for efficient \lang{} interpreter implementation. 
    Color shows the identity of the decoding thread (purple=main, red=Fork\#1, green=Fork\#2); orange denotes interpreter-inserted tokens.}
    \vspace{-0.5em}
\end{figure*}


\textbf{Interpreter.}
A \lang{}-equipped language model initiates asynchronous decoding by generating the \lang{} tags that the \lang{} interpreter executes to implement asynchronous decoding. We describe here the functionality of each tag and how they are used by the interpreter. 
\Cref{fig:interpreter-detail} shows how the interpreter orchestrates asynchronous decoding.


With the user query \CircledText{A} as the prefix,
the interpreter decodes sequentially until encountering a \promise{} tag at tag \CircledText{B}.
The \topic{} attribute indicates the topic of the chunk that will be decoded asynchronously, and the \tokens{} attribute estimates the number of tokens in multiples of 10 in the \async{} tag. 
These attributes provide context for decoding future tokens.
The interpreter then initiates a new asynchronous decoding thread named "Fork\#1". 
The main thread continues decoding in parallel, while the new thread first appends an \async{} tag to its prefix at \CircledText{C} and then decodes content matching the specified \topic{} until reaching a \casync{} tag at \CircledText{D}.
The main thread proceeds without conditioning on any asynchronously decoded content. 
When encountering a \sync{} tag at \CircledText{E}, the interpreter pauses to wait for all asynchronous decoding threads to complete, 
and enables the language model to condition on asynchronously decoded content for decoding subsequent tokens in \CircledText{F}.

\textbf{Efficiency.}
Our interpreter implementation addresses several key challenges in efficient asynchronous decoding, with KV-cache management being the core issue. 
Since ML compilers often requires static tensor shapes for effective compilation \citep{50530,paszke2019pytorchimperativestylehighperformance}, we assume a fixed batch size and sequence length \footnote{We use a batch size of 1 and max sequence length of 2048 as in \citet{gpt-fast}.}
Implementing asynchronous decoding naively as batched decoding faces two suboptimal options: 1) allocate differently sized KV-cache pools and switch between them upon thread creation and termination, which wastes precious accelerator memory, or 2) pre-allocating a fixed number of decoding threads, leading to wasted memory and computation due to inactive threads.
We further discuss the drawbacks of naive option 2) in \Cref{sec:naive-interpreter}.
Instead, we store KV-cache from all asynchronous decoding threads in a single contiguous pre-allocated memory pool with an interleaved layout.

\Cref{fig:parallelism,fig:kv-cache-management} illustrates our approach.
We denote decoding timestamp with T and index in the KV-cache pool with i. 
When only one thread is active at T=0 (\CircledText{A}), it appends new KV-cache sequentially to the pool at i=19-20.
When the main thread (purple) decodes a \promise{} token at T=1 (\CircledText{B}), the interpreter immediately appends an \async{} token to signal the start of a new thread Fork\#1 (\CircledText{C}). 
At T=2 (\CircledText{D}), Fork\#1 begins asynchronous decoding, while the interpreter initiates another asynchronous decoding thread Fork\#2.
At T=3, (\CircledText{E}), the interpreter decodes 3 tokens in parallel from 3 active threads, where the \sync{} token signals pausing to wait for other threads to complete and therefore this token does not enter into the KV-cache pool until later.
Where as the other two tokens do enter the KV-cache pool in neighboring positions (i=26-27).
The two threads (green/red) continue to decode in parallel, alternating as they append tokens to the KV-cache pool (T=3-22).

To prevent cross-thread interference with this interleaved KV-cache layout, we use attention masks to ensure threads cannot attend to each other's tokens before synchronization. In \Cref{fig:parallelism}, each directed edge connects a token to the very next token that may attend to it. A token can attend to any ancestor token reachable by following these edges backward through the graph.
A token in an asynchronous thread can only attend to tokens within its own thread and tokens from the main thread that existed before the thread was spawned. For example, the token \texttt{find} (at T=3, red) attends to \texttt{To} (at T=2, red) and \texttt{segment?} (at T=0, purple), but cannot attend to the second \async{} token (at T=2, orange), the second \promise{} token (at T=2, purple), or the \sync{} token (at T=3, purple).


Once the interpreter decodes the \sync{} token in the main thread at T=3 (\CircledText{E}), it pauses the main thread to synchronize, waiting for both forks to complete: Fork\#1 and Fork\#2 decode their \casync{} tokens at T=34 and T=22 respectively. After the wait is over at T=34, the interpreter inserts the \sync{} token into the KV-cache pool at i=80, and the main thread resumes decoding (\CircledText{F}) while conditioning on both forks' asynchronous generations.






While prior works like radix attention \citep{zheng2023efficiently} enables multiple decoding threads to share attention to a common prefix, our design has to additionally address the challenge of enabling a single decoding thread to attend to multiple asynchronously decoded threads.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{assets/pasta-flow2.pdf}
    \vspace{-0.5em}
    \caption{\lang{} dataset creation and model training.}
    \label{fig:pasta-flow}
    \vspace{-1.5em}
\end{figure}


\section{Training \lang{} Capable Models}
\label{sec:training}
We present a two-stage finetuning process that trains an LLM to annotate semantic independence in their own responses using \lang{}. 
\Cref{fig:pasta-flow} illustrates the \pasta{} system pipeline for dataset construction and model training to produce a \lang{}-equipped model.

\textbf{Building a \pastasft{} Dataset.}
The \pasta{} system pipeline begins by constructing an initial finetuning dataset, which we refer to as the \pastasft{} dataset.
It prompts the Gemini 1.5 Flash~\citep{geminiteam2024geminifamilyhighlycapable} to add \lang{} annotations to responses from an instruction-following dataset
We provide Gemini 1.5 Flash with 7 human annotated examples and a description of the syntax and semantics of \lang{} and have it label a 100K response subset of the SlimOrca dataset~\citep{lian2023mistralslimorca1}\footnote{Due to API limitations, we only successfully annotated 87K/100K instruction-response pairs as the \pastasft{} dataset.}.
We provide the annotation prompt used in~\Cref{app:dataset-prompts}.

\textbf{Training a \pastasft{} Model.}
\pasta{} then finetunes the base LLM on the \pastasft{} dataset, producing what we call the \pastasft{} model. 
Since the model must decode content after \promise{} tags without access to the corresponding \async{} content, we implement three key modifications to the standard next-token prediction finetuning algorithm.

First, the attention mask prevents the tokens after a \promise{} from attending to content within its \async{} tags until a \sync{} tag is reached. 
This enforces the semantic independence constraint between asynchronous blocks.
Second, during inference, the position IDs of tokens after a \promise{} cannot be known until fully decoding the corresponding \async{} block.
To handle this, we train the model to predict the length of each \async{} block in the \tokens{} attribute of its \promise{} tag. 
This prediction enables us to assign estimated position IDs to tokens that follow the \promise{} tag, even before fully decoding the \async{} block.
Finally, to enable the model to continue decoding past \promise{} tags, we set the next-token prediction target at each \promise{} to be the first token after its corresponding \async{} block. 
This enables the model to skip past placeholder \promise{} tags.



\textbf{\lang{} Preference Optimization.}
\label{para:pref-pairs}
\pasta{} further improves the \pastasft{}  model with a second stage of training that directly optimizes for quality and speedup achieved by the \lang{} annotations.

First, \pasta{} trains a baseline sequential model \baseline{} by fine-tuning the same base model on \pastasft{} dataset without the annotations.
Then, for each prompt in the \pastasft{} dataset, \pasta{} samples $N$ different \lang{} annotated responses from the \pastasft{} model using a temperature $T.$
It then scores each of the $N$ sampled responses. 
We set $N = 10$ and temperature $T=1.$
The score for ranking the sampled responses is a combination of:
\begin{enumerate}[leftmargin=*, topsep=0pt, itemsep=0pt]
    \item Response \emph{theoretical speedup} -- the ratio between (1) the total number of tokens in \baseline{}'s response, and (2) the length of the longest sequence that must be decoded sequentially for \pastasft{}'s response. This measures the maximum achievable speedup of \pastasft{} over \baseline{}.
     \item Response \emph{quality} -- 
     for each sampled response, \pasta{} computes a confidence-weighted win-loss ratio by comparing it against the \baseline{}'s response and the original SlimOrca response. 
     Each comparison appears in both orders and is judged by Gemini 1.5 Pro, which provides a preference and a probability for that preference between 0 and 1, interpreted as confidence. 
     The final ratio is the confidence-weighted sum of wins divided by the confidence-weighted sum of losses.
\end{enumerate}

The score for each sampled response has the following definition:
$\text{score} = \text{speedup} + \lambda \times \text{quality}$, where $\lambda$ is the \emph{quality weight} hyperparameter. 
For each prompt, \pasta{} selects the highest-scoring response as the preferred example and the lowest-scoring one as the un-preferred example.

While our methodology is compatible with any LLM preference optimization algorithm, in this work we use BoNBoN optimization~\citep{gui2024bonbonalignmentlargelanguage} for it was the state-of-the-art algorithm at the time of writing this paper.
The BonBon algorithm trains a model to approximate the best-of-N response distribution by combining an SFT loss on the preferred example with an IPO preference loss~\citep{azar2024general} between the best and worst response.
 Specifically, the BoNBoN objective is:
\begin{align*}
\mathcal{L}_{\rm BoNBoN}(\theta; D, \theta_\text{init}) =\; &\mathbb{E}_{x, y^+, y^- \sim D} [-\alpha\log p_\theta( y^+ | x)
\\&+(1 - \alpha)(
\log{\frac{p_\theta( y^+ | x)}{p_\theta( y^- | x)}} 
\\&- \log{\frac{p_{\theta_\text{init}}( y^+ | x)}{p_{\theta_\text{init}}( y^- | x)}} -
\frac{1}{\beta}
)^2]
\end{align*}
where $\theta$ are the model weights being trained, $\theta_{\text{init}}$ are the initial model weights, $x$ is a prompt, $y^+, y^-$ are the best and worst-of-N \lang{}-annotated responses respectively, and $\alpha$ is a hyperparameter to weight the SFT and IPO loss contributions.
We set $\theta_\text{init}$ to be the \pastasft{} model.
We use a learning rate of 5E-7 and set $\alpha$ to $0.005$ as recommended by \citep{gui2024bonbonalignmentlargelanguage}.




























\section{Experiment}
\label{sec:results}
We evaluated the performance of our \lang{}-equipped model in terms of speedup, parallelism, and response quality and show that it is Pareto-optimal compared to other asynchronous decoding techniques. 

\subsection{Experimental Setup}


\textbf{Baselines.}
We present the following baselines: 
standard autoregressive decoding from \baseline{}, APAR decoding~\citep{liu2024apar} and SoT decoding~\citep{ning2023skeleton}.
APAR and SoT are examples of asynchronous decoding techniques relying on hand-crafted syntactic heuristics.
We evaluate sequential autoregressive decoding using the \baseline{} model finetuned on the the \pastasft{} dataset without \lang{} annotations. 
To evaluate APAR decoding, we train an APAR model again on the same \pastasft{} dataset, except preprocessed following the official APAR methodology. 
Using regex and filters, we recreated the APAR heuristics for extracting structured data as described in their work~\citep{liu2024apar}.

\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=15px 0px 15px 0, clip]{assets/main/pasta-rt-readout-decode-time.pdf}
        \label{fig:quality-parallelism-rs}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=15px 0px 15px 0, clip]{assets/main/pasta-rt-readout-latency.pdf}
        \label{fig:pasta-theoretical-speedup}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=15px 0px 15px 0, clip]{assets/main/pasta-rt-readout-self.pdf}
        \label{fig:pasta-parallelism}
    \end{minipage}
    \vspace{-1.5em}
    \caption{Left (Realized Speedup). \pasta{} models achieved Pareto-optimal quality-speedup trade-off than asynchronous decoding strategies with hand-crafted heuristics. 
    Middle (Theoretical Speedup). The realized speedup using \lang{} interpreter is close to the theoretical speedup.
    Right (Theoretical Parallelism).\pasta{} responses show high degree of parallelism.}
    \label{fig:comparison-plots}
\end{figure*}

\textbf{Models and Hyperparameters.}
For all experiments, we use Gemma 7B \citep{gemmateam2024gemmaopenmodelsbased} as the base model.  
We finetune all models using a batch size of 8, a learning rate that decays linearly from 1e-5 to 0, and train for a total of 4 epochs.
We chose these hyperparameters as they maximized the quality of the baseline model.
\footnote{For reference, our baseline model achieves a 38\% length controlled win rate against Gemma-7B-it, Google's officially released instruction-tuned Gemma-7b model. 
This comparison demonstrate that despite our smaller-scale experimental setup, our baseline model performs competitively against state-of-the-art model.}
We provide further details on hyperparamter selection in~\Cref{app:hparams}.

There are two \pasta{} specific hyperparamters: $\lambda$, the quality weight used for building preference pairs (\Cref{para:pref-pairs}); and $r$, the number of BoNBoN preference optimization iterations.
We train multiple \pasta{} models, with $\lambda=1,2,4,8$ and set $r=2$. 
We refer to each model as \pastabon{$r$}{$\lambda$}.
We also train \pasta{} models while optimizing exclusively for quality, denoted as \pastabon{$r$}{$\infty$}.

\textbf{Hardware and Software.}
We evaluate decoding performance using PyTorch with torch.compile optimization set to maximum auto-tuning mode  \citep{paszke2019pytorchimperativestylehighperformance}.
All experiments run on H100 GPUs using greedy decoding.
We use a batch size of 1 as is common with parallel decoding literature \citep{leviathan2023fast}. 
To avoid measuring compilation overhead, we decode each request twice and only take the timing of the second of two decoding runs.

\textbf{Evaluation.}
We evaluate all models on the AlpacaEval benchmark~\citep{dubois2024length,alpaca_eval}, an open ended suite of 805 representative prompts.
For each decoding method we evaluate, we compute the following metrics:
\begin{enumerate}[leftmargin=*, topsep=0pt, itemsep=0pt]
    \item We measure the \textit{realized speedup} against the baseline as the ratio between the wall-clock decoding times: baseline model time divided by test model time, both using the \lang{} interpreter.
    \item We calculate the \textit{theoretical speedup} against the baseline as the ratio between (1) the total number of tokens in the baseline response and (2) the length of the longest sequence that must be decoded sequentially in the test model's response, as described in~\Cref{sec:training}.
    \item We measure the \textit{theoretical parallelism} in the model output: the ratio of (1) the total number of non-control tokens to (2) the length of the longest sequence of tokens that must be decoded sequentially.
    \item We measure the \textit{quality} as the length-controlled, LLM-as-a-judge win-rate using AlpacaEval benchmark~\citep{alpaca_eval,dubois2024length} when compared to the baseline model.
    We use Gemini 1.5 Pro~\citep{geminiteam2024geminifamilyhighlycapable} as the judge model for development and GPT4 as the judge for evaluation, to prevent reward hacking.
\end{enumerate}
We aggregate the speedup and parallelism over each prompt in the dataset using the geometric mean\footnote{The arithmetic mean of ratios can lead to inconsistent results depending on the baseline used to compute the ratio and is therefore not appropriate for our use~\citep{fleming1986not}. However, the prevailing practice in parallel decoding literature uses arithmetic mean when reporting speedup. We provide the speedup computed with arithmetic mean for reference in \Cref{app:arithmean}.}.











\subsection{Results}




The left, middle and right plots in \Cref{fig:comparison-plots} show how response quality trades off against realized speedup, theoretical speedup, and parallelism respectively across \pasta{} and baseline models.
We mark the 
baseline (50\% win rate with no speedups) with an X.
An ideal asynchronous decoding strategy should match this baseline in quality while surpassing it in speedup; 
the closer to the top right corner of the plot, the better the technique.

\textbf{Pareto-Optimality.}
The left plot in \Cref{fig:comparison-plots} shows that \pasta{} models achieve Pareto-optimal trade-off between quality and realized speedup. 
The best-quality model 
(\pastabon{2}{$\infty$})
achieved 52.3\% win rate at 1.21x speedup, whereas the best-speedup model 
(\pastabon{2}{1})
achieved 42.9\% win rate at 1.93x speedup.
The \pastabon{2}{2} model 
Pareto-dominate all prior asynchronous decoding techniques,
achieving superior quality-speedup trade-off.

The APAR model and \pastabon{2}{2} achieved similar speedups (1.6x vs 1.62x), but \pastabon{2}{2} obtained a 4.3\% higher win rate. 
This result stems from
APAR's reliance on syntactic heuristics for identifying semantic independence, which can lead to false positives. 
Similarly, while APAR and \pastabon{2}{1} showed comparable win rates (41.2\% vs 42.9\%), \pastabon{2}{1} delivered a 20.6\% higher speedup. 
This superior speedup stems from the flexibility of \lang{}'s annotation, which enables asynchronous decoding at any position in the output.



Notably, we do not observe any speedup by SoT \citep{ning2023skeleton}, when applied to \baseline{}.
We believe that SoT, as a prompt-based method, requires the base model to have strong instruction-following ability to perform well, and validated this hypothesis by applying SoT to the stronger official instruction-finetuned Gemma-IT model from Google.
With this generous implementation, SoT achieves a 1.61x speedup while dropping its win rate by 12\%.
In contrast, our \pastabon{2}{2} achieves 1.62x speedup with only a 5\% drop to win rate.

\textbf{Role of $\lambda.$}
We observe that the quality weight $\lambda$ serves as an effective control knob for the trade-off between quality and speedup. A lower weight results in more aggressive optimization for speedup at the cost of reduced quality.

\textbf{Theoretical speedup.}
Comparing the left (realized speedup) and middle (theoretical speedup) plot in \Cref{fig:comparison-plots} shows that the combination of \lang{} interpreter and \pasta{} models deliver realized speedup close to theoretical optimal. 

\textbf{Parallelism.}
The right plot in \Cref{fig:comparison-plots} shows that \pasta{} models achieve a high degree of theoretical parallelism. 
Notably, while the \pastasft{} model starts with high theoretical parallelism, this does not translate into significant decoding speedup. 
This disconnect occurs because the induced parallelism is not effective - the model learns to generate redundant content, which hurts both quality and speedup. 
Preference optimization effectively mitigates this pathology, as the \emph{Pasta-BoN} models deliver speedups commensurate with their  theoretical parallelism.

\textbf{Conclusion.}
\pasta{} is an effective technique for enabling asynchronous decoding, producing Pareto-dominant performance. 
\pasta{} also enables flexible trade-off between speedup improvements and response quality. 


        

    

\vspace{-.8em}
\section{Sensitivity Analysis.}
In this section, we investigate how three key design choices of the \pasta{} system impact model quality and latency:
a) the number of preference optimization iterations,
b) the configurations of positional embeddings, and
c) the scoring method for \lang{} annotated responses.

\subsection{Number of Iterations}
\label{sec:num-iterations}


We examine how the number of preference optimization iterations affects the quality-speedup trade-off in \pasta{} models;
we observe continuous improvements as training compute increases and find distinct optimization dynamics at different preference optimization iterations.

\textbf{Methodology.}
We analyzed the impact of preference optimization on the speedup-quality trade-off by \pasta{} models at 5 different stages: 
initial (aka \pastasft{}), 10\% into Round 1, and 100\% into Round 1, 10\% into Round 2, and 60\% into Round 2\footnote{We stopped at 60\% of Round 2 due to time constraints.}.
The initial stage is represented by a single star, while results from later stages contain four points each, corresponding to quality weights ($\lambda$) of 1, 2, 4, and 8.
We visualize each stage's data using distinct colors in \Cref{fig:sensitivity-analysis-plots} (Left).
To illustrate the difference in quality-speedup trade-off between stages, we computed linear fits for each stage after \pastasft{} and plotted the linear fit using the same color as each stage.

\textbf{Results.}
\Cref{fig:sensitivity-analysis-plots} (Left) demonstrates the scalability of preference optimization, as increased training compute continuously improves the Pareto frontier toward better quality-speedup trade-offs (i.e., top right corner).
As is common when scaling neural networks with more training compute~\citep{kaplan2020scalinglawsneurallanguage}, we observe diminishing returns, though we do not observe saturation after two rounds of preference optimization.

Observing the linear fits of quality-speedup trade-off within each stage reveals distinct optimization dynamics.
The initial 10\% of Round 1 preference optimization aggressively optimizes for speedup, moving the group to the right of the plot.
However, from 10\% Round 2 to 60\% Round 2, we observe an emphasis on quality improvements, shifting the group upward.
Overall, preference optimization effectively navigates the trade-off space by exploring both quality improvement and speedup improvement.

\textbf{Conclusion.}
Preference optimization is a scalable technique that 
improves the speedup-quality trade-off with increased training compute. 
Unlike prior asynchronous decoding techniques that require hand-crafted syntactic heuristics, our technique directly converts computational resources into better quality and speedup trade-off, offering a more scalable path to improving decoding speed.


\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=15px 0px 15px 15px, clip]{assets/main/scalability.pdf}
        \label{fig:pasta-scalability}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=0px 0px 0px 15px, clip]{assets/main/positional-encoding-comparison.pdf}
        \label{fig:positional-encoding}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=0px 0px 0px 15px, clip]{assets/main/optimization-objective-comparison.pdf}
        \label{fig:optimization-objective}
    \end{minipage}
    \vspace{-2.5em}
    \caption{Left (Scalability). As we continue investing training compute by increasing the number of rounds of preference optimization, we see the quality-latency trade-off continuously improve.
    Middle (Positional Embedding). Analysis of different methods for computing position IDs during decoding. LLM based prediction of the position IDs in multiples of ten (Pred-10x) achieves the highest quality without significantly sacrificing speedup.
    Right (Preference Score). Analysis of different metrics for decoding efficiency used in calculating the \lang{} preference scores. Optimizing for the theoretical speedup achieves both high theoretical speedup and LC win rate.}
    \label{fig:sensitivity-analysis-plots}
    \vspace{-0.8em}
\end{figure*}


\subsection{Positional Embedding}

Asynchronous decoding methods introduce uncertainty over the true position of tokens as they are being generated as the output is generated in a non-sequential manner.
Namely, the main decoding thread is unaware of the true length of of any previously occurring \async{} blocks that have not been synchronized.
Thus, the main thread must estimate the number of tokens in an \async{} block before continuing generation.
If the predicted number of tokens for the \async{} block does not match the true number of tokens generated, this can lead to errors as the position IDs after synchronization will either not increase monotonically (predicted too few) or contain a gap (predicted too many).

\textbf{Methodology.}
To minimize the error between the true and predicted position IDs, we compare three different approaches for position ID assignment:

\textit{Fixed-length}: We make the assumption that each \async{} block has a fixed length.
In our experiments, we chose this length to be forty tokens as this is slightly larger than the median \async{} block length in our training data.

\textit{Length Prediction}: We train the model to predict the length of each \async{} block, and then use the model's predictions during decoding.
We evaluate two variants that predict \async{} block lengths at different granularities:
1) Pred-1X: Predict the \async{} token length exactly; and
2) Pred-10X: Predict the \async{} token length as a multiple of ten for coarser granularity.


\textit{Oracle}:
We use the ground truth length of each \async{} block (i.e. the position IDs of the tokens in the block if the chunk was decoded sequentially) to assign position IDs. 
While the oracle position IDs are infeasible to obtain during deployment, we evaluate the performance of oracle ID decoding to serve as a reference point for the performance of decoding with no error in the position ID calculations.
We evaluate two different granularity for the oracle position IDs:
1) Oracle-1X: Using the true \async{} block length; and
2) Oracle-10X: Using the true \async{} block length rounded to the nearest multiple of ten.
We consider one final oracle baseline, which we refer to as Oracle, where the exact \async{} block lengths are used to offset the position IDs but the length is not included in the \promise{} tag.


\textbf{Results.}
\Cref{fig:sensitivity-analysis-plots} (Middle) presents both the response quality and speedup for models finetuned on the \pastasft{} dataset using each of the different position ID estimation techniques.
Length prediction performs the best, achieving quality and speedup metrics matching (or even slightly exceeding) the Oracle. 
The Pred-10X variant offers a flexible solution to the position ID assignment problem that is admissible to preference optimization. 
Based on these results, we adopted Pred-10X as our position ID assignment strategy.

\subsection{\lang{} Preference Score}




When performing \lang{} preference optimization, we compute a preference score for each \lang{} annotated response as a weighted combination of a quality term and a \fTBD{ZA: IDK if other people like this}{decoding efficiency} term, which is meant to reflect the improvement in decoding speed for a response.
While our ultimate goal is to increase decoding speed, it is not obvious \emph{a priori} that directly optimizing for the speedup will produce the desired behavior. Such an objective could lead to degenerate solutions such as producing short responses.


\textbf{Methodology.}
To determine the most performant decoding efficiency term, we investigate four separate metrics for the decoding efficiency of a response:
(a) the theoretical speedup of the response only
(b) the harmonic mean of the theoretical speedup and theoretical parallelism of the response
(c) the arithmetic mean of the theoretical speedup and theoretical parallelism of the response
(d) the theoretical parallelism of the response only.

We re-compute the preference score for each training response using each of the efficiency metrics we present above.
We then perform a single round of BoNBoN training on each of the different preference labeled datasets.

\textbf{Results.}
\Cref{fig:sensitivity-analysis-plots} (Right) presents the response quality and speedup for each model trained to optimize a different decoding efficiency metric.
Intuitively, one might expect the harmonic mean to be the most effective since it encourages balanced optimization by inducing a larger weight on the weaker metric.
However, we find that optimizing for harmonic mean performs similarly to optimizing directly for speedup. We hypothesize this is because our LLM-based quality evaluation naturally favors longer, more detailed responses, preventing the model from artificially increasing speedup through response truncation. As expected, optimizing solely for theoretical parallelism leads to poor speedup, demonstrating the importance of including speedup in the objective.
Based on these results, we adopted theoretical speedup only as our efficiency metric.





































































    


    



















\vspace{-.5em}
\section{Related Work}



After discussing parallel decoding approaches in \Cref{sec:taxonomy}, we now turn to other relevant research areas.




\textbf{Agent Planning/Tool Use.}
Our work is related to the idea of agent planning and tool use \citep{yao2023react, schick2023toolformer, shen2023hugginggpt, liang2023taskmatrixai, lu2023chameleon}.
Prior studies show that LLM-based agents can solve complex tasks by planning and using tools such as web search and external APIs.
Our work extends the suite of tools available to LLMs with the \lang{} language and interpreter for improving their own decoding efficiency.

\textbf{Approximate Parallelization.}
Our work extends the idea of approximate parallelization \citep{10.1145/1993498.1993555, 10.1145/2414729.2414738}.
\citet{10.1145/1993498.1993555} proposed a framework that enables programmers to annotate breakable data dependencies in a program and 
developed a compiler and runtime that exploits these annotations to automatically parallelize otherwise sequential regions of code.
\citet{10.1145/2414729.2414738} opportunistically relaxes synchronization primitives in a parallel program to improve parallelism, to program outputs that are acceptably close to the original one.
Similarly, we break the sequential decoding process of LLMs into approximately parallelizable and independent components and exploit the parallelism to improve decoding efficiency.
Our work differs in that instead of relying on end-user annotation or compiler analysis, we teach LLMs to autonomously express parallelism in their own decoding process using the \lang{} annotation language.


\vspace{-.5em}
\section{Conclusion}
We present \pasta{}, a system that teaches LLMs to identify semantically independent chunks in their own responses and annotate them for parallel decoding opportunities.
Through a set of annotations, an interpreter and a finetuning procedure, our method enables learned asynchronous decoding.


\textbf{Conclusion.}
\pasta{} is an effective and scalable system which enables learned asynchronous decoding. Evaluation on AlpacaEval demonstrates that our approach Pareto-dominates existing asynchronous decoding methods in terms of quality and speedup. The improvements continue with additional training compute, showing no signs of saturation during our experimental evaluation.








\textbf{Impact statement.}
This work improve the efficiency of LLM inference, which could lead to reduced computational resource requirements and improved accessibility to LLMs.

\textbf{Acknowledgement.}
This work was supported in-part by the Sloan Foundation and SRC JUMP 2.0 (CoCoSys). 
We are deeply grateful to William Brandon, Michael Brenner, Gintare Karolina Dziugaite, Xin Dong, Han Guo, Zhun Liu, Jesse Michel, Aniruddha `Ani' Nrusimha, Ankit Singh Rawat, Narges Shahidi, Ananda Theertha Suresh, Xin Wang, Logan Weber, Cambridge Yang, and Xiaofan Zhang for their thoughtful feedback and detailed review of this manuscript.
This work benefited significantly from the support of the Google CoreML Performance Team. We also thank the extended team at Google Research and Google DeepMind for their valuable contributions to this research.



\bibliography{example_paper}
\bibliographystyle{icml2025}

\newpage

\appendix


















\section{Dataset Annotation Prompt}
\label{app:dataset-prompts}
Below is the prompt used to annotate our \lang{} seed dataset. Additional in-context examples are omitted for presentation.


\begin{lstlisting}
As a highly-paid expert annotator, you will be given a chatbot response, your job is to decide whether and how this response may be generated asynchronously in parallel by a large language model. Parallel text generation allows the model to generate segments of text simultaneously rather than sequentially, significantly improving efficiency and response times.

You will first identify whether the given chatbot response may be generated in parallel. You are to then annotate the chatbot response using specific tags that highlight segments suitable for parallel generation.

Use <async> tags to denote segments of text that may be generated asynchronously in parallel with respect to the text that follows. Thus apply <async> tags only to sentences that do not serve as necessary context for subsequent sentences. Sentences that are crucial for understanding or generating following text are not suitable for parallel asynchronous generation. For each <async> tag, include a very concise topic description of the text surrounded within the <async> tags. The topic description will be accessible to text generation after the closing async tag to ensure continuity and coherence.

Use the singleton <sync/> tag for synchronization. All content generated before <sync/>, including text marked by <async> is accessible to subsequent text generation after the <sync/> tag, ensuring continuity and coherence.

Detailed Instructions:
- Tagging Rules:
 - Use <async> tag in pairs.
 - Ensure that each <async> tag encompasses at least five words.
 - Refrain from altering the content of the response during annotation.
 - Use a maximum of 3 words in the topic description.
 - Use <sync/> sparingly as it introduces significant slowdown.

Input conversation:
```json
[
  {
    "from": "human",
    "value": "Article: In March 1968, CBS and Sony formed CBS/Sony Records, a Japanese business joint venture. With Sony being one of the developers behind the compact disc digital music media, a compact disc production plant was constructed in Japan under the joint venture, allowing CBS to begin supplying some of the first compact disc releases for the American market in 1983.\n\nNow answer this question: In what year did CDs come to the American market?"
  },
  {
    "from": "chatbot",
    "value": "CDs (Compact Discs) came to the American market in 1983. This was made possible through a joint venture between CBS and Sony, named CBS/Sony Records, which was formed in March 1968. As Sony was one of the developers behind the compact disc digital music media, they built a compact disc production plant in Japan under this joint collaboration. This plant enabled CBS to supply some of the first compact disc releases for the American market in 1983. The introduction of CDs revolutionized the music industry as they provided a new and improved way to store and listen to music, providing higher sound quality and durability compared to the earlier vinyl records and cassette tapes."
  }
]
```
Annotated chatbot response:
```json
{
  "from": "chatbot",
  "value": "CDs (Compact Discs) came to the American market in 1983. <async topic='joint venture'>This was made possible through a joint venture between CBS and Sony, named CBS/Sony Records, which was formed in March 1968.</async> <async topic='japan production plant'>As Sony was one of the developers behind the compact disc digital music media, they built a compact disc production plant in Japan under this joint collaboration. This plant enabled CBS to supply some of the first compact disc releases for the American market in 1983.</async> <async topic='impact'> The introduction of CDs revolutionized the music industry as they provided a new and improved way to store and listen to music, providing higher sound quality and durability compared to the earlier vinyl records and cassette tapes.</async>"
}
```

...
\end{lstlisting}













































\section{Naive Interpreter Implementation.}
\label{sec:naive-interpreter}
In this section, we illustrate the KV-cache layout for naive implementation before and after synchronization in \Cref{fig:naive-before} and \Cref{fig:naive-after}.
We repeat the full model output in \Cref{fig:interpreter-detail-refresh}.
Here we illustrate the inefficiencies associated with implementing asynchronous decoding as batched decoding, where we pre-allocate the KV-cache pool assuming a fixed number of decoding threads of 4.

In \Cref{fig:naive-before}, we show the KV-cache content before synchronization at \CircledText{E}. The naive batched implementation requires duplicating the prefix for each asynchronous thread, which runs as an independent batch item. Since the KV-cache pool is sized for the maximum possible number of parallel threads, many rows often remain unused. This wastes both accelerator memory and computation, as the KV-cache pool's shape determines the shape of attention computation.

In \Cref{fig:naive-after}, we show the KV-cache content after synchronization, two decoding steps into \CircledText{F}. The naive interpreter must copy and insert Fork\#1 and Fork\#2's asynchronous generations after their corresponding \promise{} tags in the main thread's KV-cache row, then mark the rows of terminated threads as uninitialized. This naive batched implementation thus suffers from wasted accelerator memory during memory allocation, wasted computation from the oversized attention operations, and additional overhead from KV-cache movement during synchronization.

\begin{figure*}
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=10px 140px 10px 5px, clip]{assets/main/forking.html.pdf}
        \vspace*{-4mm}
        \caption{\lang{} interpreter orchestrates parallel decoding. 
        \CircledText{A} shows the user query.
        \CircledText{B} shows the \promise{} tag which initiates the first asynchronous decoding thread named ``Fork\#1''.
        \CircledText{C} indicates where the interpreter appends an \async{} tag to the prefix of Fork\#1, signaling Fork\#1 should complete the promised content with topic ``coordinates''.
        \CircledText{D} denotes the asynchronous generation by Fork\#1. 
        \CircledText{E} shows the \sync{} tag where the interpreter pauses to wait for all asynchronous generations.
        \CircledText{F} shows the main thread decodes the remaining content with both asynchronous generations in its prefix.
        }
        \label{fig:interpreter-detail-refresh}
    \end{subfigure}
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=5 540 225 5, clip]{assets/main/naive-before-sync.html.pdf}
        \subcaption{Naive KV-cache layout before synchronization. The naive approach implements asynchronous decoding through batched decoding, with a pre-allocated fixed-size KV-cache pool supporting a maximum number of parallel threads, which is set of 4 in this figure. 
        Each decoding thread operates as an independent batch item, with shared prefix duplicated for each thread. 
        The KV-cache pool is stored in row-major layout with shape (max batch size × sequence length), shown here before synchronization.
        The figure show KV-cache pool contents immediately after decoding \sync{} at \CircledText{E}.}
        \label{fig:naive-before}
    \end{subfigure}
    \vspace*{3em}
    \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=\linewidth, trim=5 520 110 -10, clip]{assets/main/naive-after.html.pdf}
    \subcaption{Naive KV-cache layout after synchronization. For asynchronous content to be available to subsequent decoding steps, the naive interpreter must copy and insert each thread's KV-cache (shown in red and green) after their corresponding \promise{} tokens in the main thread's KV-cache row. After insertion, the \sync{} token is added and subsequent decoding can attend to the asynchronous content. The interpreter must then mark rows corresponding to terminated threads (Fork \#1, Fork \#2) as uninitialized to free memory. The figure shows the KV-cache pool contents two steps into \CircledText{F}.}
    \label{fig:naive-after}
    \end{subfigure} 
    \vspace{-3em}
    \caption{Example Naive Interpreter Implementation.}
\end{figure*}

\clearpage


\section{Hyperparamater Selection}
\label{app:hparams}
We selected the learning rate and epochs that we used to train all models based on which combination of hyperparameters led to the highest quality \baseline{-SFT} model as measured by length-controlled win-rate on AlpacaEval.
We performed a grid search over all combinations of learning rates in $[1e-4, 1e-5, 1e-6]$ and epochs in $[1, 4, 8]$.
We found that training with a learning rate of $1e-5$ for $4$ epochs resulting in the highest win-rate model.


\section{Evaluation}
\label{app:arithmean}




The prevailing practice in parallel decoding literature uses arithmetic mean to compute average speedup. 
However, geometric mean should when averaging normalized values such as speedup against a baseline~\citep{fleming1986not}. 
As such, we report geometric mean in \Cref{sec:results} but include here in \Cref{fig:quality-parallelism-rs-arith,fig:pasta-theoretical-speedup-arith,fig:pasta-parallelism-arith} the results computed using arithmetic mean as reference.
Notably, by definition, arithmetic mean is larger than or equal to geometric mean, therefore  \Cref{fig:quality-parallelism-rs-arith,fig:pasta-theoretical-speedup-arith,fig:pasta-parallelism-arith} show notably higher speedup and parallelism than \Cref{sec:results}.
Furthermore,
\pasta{} models still Pareto-dominate using arithmetic mean.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/main/pasta-rt-readout-decode-time-arithmean.pdf}
    \caption{Realized Speedup. \pasta{} models achieved a Pareto-optimal quality-speedup trade-off compared to asynchronous decoding strategies with hand-crafted heuristics.}
    \label{fig:quality-parallelism-rs-arith}
\end{figure}


\begin{figure}[t]
\vspace{-5em}
    \centering
    \includegraphics[width=\linewidth]{assets/main/pasta-rt-readout-latency-arithmean.pdf}
    \caption{Theoretical Speedup. The realized speedup using \lang{} interpreter is close to the theoretical speedup.}
    \label{fig:pasta-theoretical-speedup-arith}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=\linewidth]{assets/main/pasta-rt-readout-self-arithmean.pdf}
    \caption{Theoretical Parallelism. \pasta{} responses show a high degree of parallelism.}
    \label{fig:pasta-parallelism-arith}
\end{figure}






















































































\end{document}
