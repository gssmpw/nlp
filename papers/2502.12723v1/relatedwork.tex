\section{Related Works}
\subsubsection{Open Source Driver Gaze Datasets} 
While many studies rely on simulators for data collection \cite{fisher2007empirical8,deng2019prediction9}, advancements in wearable eye trackers and vision processing algorithms have enabled real-world eye-tracking. This technology has diverse applications, such as investigating distractions from mobile usage \cite{ojstersek2019eye10}, the effects of stress on driving behaviour \cite{wang2023eye11}, and assessing the efficacy of driver assistance systems \cite{said2018real12}.

Most driver eye-tracking research focuses on four-wheelers, with limited attention to two-wheelers \cite{distasi2011behavioral13,papakostopoulos2020semantic14,hosking2010visual15}, outside Western countries. Developing robust datasets is crucial for refining prediction algorithms derived from ground-truth gaze estimation. This section reviews notable datasets to emphasise the unique contributions of the \textit{myEye2Wheeler} dataset.
\subsubsection{Gaze Zone Predictor datasets}
The DG-UNICAMP dataset,  by Ribeiro et al. \cite{ribeiro16}, integrates RGB, IR, and depth cameras, but its stationary vehicle setup limits capturing dynamic driving scenarios.
The Driver Gaze in the Wild (DGW) Dataset, introduced by Ghosh et al. \cite{ghosh17}, improves representation with a large subject pool but retains the limitations of a stationary setup.
The Driver Monitoring Dataset (DMD) \cite{ortega18} focuses on driver behaviour and gaze zone classification of interior components of the car and, similar to previous datasets, does not include gaze dynamics to the external traffic or road.

\subsubsection{Traffic Driver Gaze Datasets}
The \textit{DADA-2000} dataset \cite{dada19} is a benchmark for driver attention prediction in accident scenarios, comprising 2000 video sequences with detailed annotations covering diverse driving conditions and accident types. It provides fixation maps, saccade scan paths, accident categories, and spatial crash object locations, offering insights into the relationship between driver attention and accidents.

The \textit{DR(eye)VE }dataset \cite{alletto2016dr6} collected on European roads focuses on driver gaze research in varied lighting conditions. It includes 74 videos captured from a wearable eye tracker and a roof-mounted camera, providing synchronised gaze coordinates for each frame.

The \textit{Look Both Ways} (LBW) Dataset \cite{Isaac7} offers gaze data from a wearable eye tracker and dashboard-mounted camera images. It provides insights into gaze patterns and analysis derived from the face images of drivers.

The \textit{DGAZE} dataset \cite{Dua20} captures traffic gaze data in Indian traffic conditions, featuring face and traffic images with gaze annotations, albeit collected in a lab setting mimicking 4-wheeler driving conditions.