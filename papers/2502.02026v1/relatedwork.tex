\section{Related Work}
\subsection{Crystal Property Prediction}
Modern machine learning schemes to predict material properties were first developed to accelerate molecular dynamics simulation~\cite{PhysRevLett.98.146401}.
For this purpose, the radial and angular distribution functions were convoluted with artificial neural networks inside each atom, and then the total energy was output.
The GNN is also employed to construct a universal model including various kinds of elements~\cite{CGCNN,SchNet,MEGNet,PotNet}.
This GNN model can handle periodicity and rotation, permutation, and translation invariance and be extended to the hypergraph to treat the bond angle directly~\cite{ALIGNN}.
Recently, a more accurate graph-transformer-based model~\cite{Matformer} and infinitely fully connected neural network for crystal systems~\cite{Crystalformer} have been proposed.
These models can be assumed as an extension of the GNN for crystals by using the attention mechanism.
This study uses these GNN models to describe the logarithmic probability because that function is expected to behave as the total energy, and we can easily modify that model to guarantee the continuity of the lattice deformation.

\subsection{CSP and Crystal Generation}
CSP and crystal generation using machine learning have traditionally developed under the predict-optimize paradigm \cite{Predict_Optimize}, based on formation energy predictors and optimization methods. On the other hand, in recent years, significant research efforts have been dedicated to exploring the potential of generative models. FTCP \cite{FTCP}, proposed alongside the implementation of a crystal generative model using VAE, serves as a reversible feature representation for crystals. Among crystal generative models utilizing GANs is CrystalGAN \cite{CrystalGAN}, which targets crystals with specific compositions and enables efficient crystal generation by partitioning the search space. Furthermore, inspired by the success of diffusion models in the field of computer vision, numerous studies have reported advances using diffusion models. One advantage of diffusion models lies in their ability to guarantee the invariance of probability density functions by adopting equivariant score predictors for translations, rotations, and permutations of atomic orders. CDVAE \cite{CDVAE}, a pioneering work in this approach, explicitly handles atomic coordinates. DiffCSP \cite{DiffCSP} employs fractional coordinates and Fourier-transformed features, while SyMat \cite{SyMat} utilizes interatomic distances. EquiCSP \cite{EquiCSP} explores the invariance concerning the permutation of lattice vector orders. DiffCSP++ \cite{DiffCSP++} further extends the DiffCSP method by incorporating space group considerations. However, we believe that, although these approaches partially ensure certain invariances, they still fall short of fully satisfying all the necessary properties that should be met.

\subsection{Continuity of Machine Learning Models}
In the field of computer vision, the continuity of machine learning models has been a subject of discussion. Specifically, it has been pointed out that using classical representations such as quaternions or Euler angles for 3D rotations in point clouds or joints results in a lack of the continuity \cite{Continuity_Rotation}. In general, for 3D rotations, representations with 4 or less dimensions are insufficient from the perspective of the continuity, and it has been shown that representations of 5 or higher dimensions should be used, along with practical representation example. Additionally, self-selecting ensembles \cite{Self_Selecting_Ensambles} provide an approach to address the topological complexity arising from the rotational symmetry of the target.

\subsection{Energy-Based Models}
Energy-based models (EBMs), which trace their origins back to the Boltzmann machine \cite{Boltzman_Machine}, continue to be actively explored across various domains. For example, \cite{EBM_OpenAI} demonstrates performance comparable to then state-of-the-art methods in tasks such as image generation and corrupted data restoration. Also, Generative PointNet \cite{Generative_PointNet} leverages EBMs for point cloud generation, achieving high performance while ensuring invariance. In the realm of physics applications, protein conformation prediction stands out as a prominent example \cite{EBM_Meta}. Moreover, EBMs form the theoretical foundation of diffusion models, and their indirect impact in this area is immeasurable.