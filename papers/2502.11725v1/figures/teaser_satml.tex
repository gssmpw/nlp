% \begin{figure*}[t!]
% \centering
% %\small
% % Row 1
% % \begin{subfigure}
%     \subfloat[We evaluate the clean and robust performance of several models %regarding their effectiveness
%     as a perceptual metric on the 2AFC task from NIGHTS dataset. Our \rclip models (zero-shot and fine-tuned on NIGHTS) outperform all other baselines, except DreamSim, in terms of clean performance while having significantly higher robustness.]{
%     \includegraphics[width=0.8\columnwidth]{assets/teaser_bar.pdf}
%     }
% % \end{subfigure}
% \vspace{1mm}%
% % \begin{subfigure}[b]{0.5\columnwidth} % bar plot
% %     \caption{Bar plot}
% % \end{subfigure}%
% %\hline
% \par\noindent\rule{\columnwidth}{0.6pt}
% \par
% \vspace{1mm}%
% % % \hspace{2em}%
% % \begin{subfigure}[b]{0.4\columnwidth} % text inversion
%   \textbf{Text Inversion: Checking for Visual Concepts captured by (Robust) CLIP}\par\medskip
% \vspace{-2mm}
% \subfloat[Starting from a grey image we maximize the cosine similarity to the text embedding of a text query once for CLIP and once for \rclip (Left: ``Yoshua Bengio'', Right: ``A penguin at the beach''). Our robust CLIP model generates images which show that both concepts are captured/memorized, whereas the clean CLIP model produces only adversarial noise.]{
% \centering
% \begin{subfigure}[b]{0.24\columnwidth}
%     \caption*{\footnotesize \clip}
%     \includegraphics[width=0.96\columnwidth]{assets/text-inversion/big-step/clean-bengio.png}
% \end{subfigure}%
% \hspace{0.5mm}%
% \begin{subfigure}[b]{0.24\columnwidth}
%     \caption*{\footnotesize \rclip}
%     \includegraphics[width=0.96\columnwidth]{assets/text-inversion/big-step/bengio.png}
% \end{subfigure}%
% \hspace{2mm}%
% \begin{subfigure}[b]{0.24\columnwidth}
%     \caption*{\footnotesize \clip}
%     \includegraphics[width=0.96\columnwidth]{assets/text-inversion/big-step/clean-penguin-beach.png}
% \end{subfigure}%
% \hspace{0.5mm}%
% \begin{subfigure}[b]{0.24\columnwidth}
%     \caption*{\footnotesize \rclip}
%     \includegraphics[width=0.96\columnwidth]{assets/text-inversion/big-step/penguin.png}
% \end{subfigure}%
% }
% % \vspace{0.5mm}

% \caption{\textbf{The benefits of robust \clip as a perceptual metric.}
% }
% % Visualizing the benefits of a robust \clip for perceptual metric evaluation and interpretable image generation.\chs{maybe just "The benefits of robust \clip as a perceptual metric" ?}}}
% % or "The benefits of robust \clip as a perceptual model"
% % \hspace{1mm}%
% \label{fig:teaser}
% % \vspace{-1em}
% \end{figure*}

\begin{figure*}[t]
\centering

\tcbset{colframe=white, left=0pt, right=0pt, top=2pt, bottom=2pt, boxrule=0pt, arc=5pt}

\begin{minipage}[c]{1.28\columnwidth}
\centering \small
%
\begin{tcolorbox}[colback=blue!10!white, width=\columnwidth]
\centering \small
\textbf{(Clean and robust) SOTA zero-shot 2AFC performance}\par\medskip

\begin{subfigure}{.96\columnwidth}
\centering \footnotesize
\figwidth=.9\columnwidth
\includegraphics[width=\figwidth, trim=1mm 1mm 0mm 1mm, clip]{figures/perceptual_Scatter.pdf}
% {figures/teaser-bar-updated.pdf}

    \caption{%\textbf{NIGHTS dataset.} We evaluate the clean and robust performance of several models as perceptual metrics on the 2AFC task from the NIGHTS dataset \cite{fu2023learning}. Our \rclipf model outperforms all other zero-shot baselines and is close to the  DreamSim-Ensemble (fine-tuned on NIGHTS) in terms of clean performance while having significantly higher robustness.
    %
    \rev{We report the clean and robust performance of several perceptual metrics on %the 2AFC task from 
    the NIGHTS dataset \cite{fu2023learning}. The adversarially trained  \rclipf  (ConvNeXt-B) and \rdinof (ViT-B/16) models achieve higher both clean and robust accuracy than their non-robust counterparts (CLIP, DINO), and have SOTA zero-shot performance.}
     }
     \label{fig:teaser_2afc}
\end{subfigure}
\end{tcolorbox}



%\vspace{3mm}

 % Set background and frame color

\begin{tcolorbox}[colback=green!10!white, width=\columnwidth]
\centering \small

\textbf{Robust detection of NSFW content}\par\medskip

\begin{subfigure}{.96\columnwidth}
\centering
\figwidth=.9\columnwidth
\includegraphics[width=\figwidth, trim=1mm 1mm 0mm 1mm, clip]{figures/nsfw_scatter.pdf}

% {figures/retrieval-nsfw-
% bar.pdf}

    \caption{%textbf{Robust detection.} Perceptual metrics can be used to detect ``Not Safe for Work'' (NSFW) images via image-to-image retrieval. Our \rclipf achieves clean accuracy close to that of \clip and DreamSim ($\sim$90\%) while being robust against $\ell_\infty$-attacks ($\epsilon_\infty=8/255$) with the goal of turning unsafe ($\mathcal{U})$ into safe ($\mathcal{S})$ images (75.0\% robust accuracy versus $<$20\% of competitors).
    %
    \rev{We test perceptual metrics to detect ``Not Safe for Work'' (NSFW) images via image-to-image retrieval. \rclipf achieves clean accuracy close to that of \clip and DreamSim ($>$90\%) while being robust against $\ell_\infty$-attacks ($\epsilon_\infty=8/255$) which aim to turn unsafe into safe images (75\% robust accuracy vs $<$40\% of competitors).}
    }
    \label{fig:teaser_nsfw}
\end{subfigure}
\end{tcolorbox}

\end{minipage}
%
\hfill
%
%
\begin{minipage}[c]{.73\columnwidth}
\centering \small

\begin{tcolorbox}[colback=orange!10!white, width=\columnwidth]
\centering \small
\textbf{Interpretability of perceptual metrics} %\\ (visual concepts captured by CLIP)
\par\medskip
\begin{subfigure}{.96\columnwidth}
\figwidth=.48\columnwidth
\tabcolsep=2pt
\centering \footnotesize
\begin{tabular}{cc}
% CLIP & \rclip (ours) \\
% \addlinespace[0.1cm]
\multicolumn{2}{c}{Original}\\
\includegraphics[width=0.9\figwidth]{assets/img-inv-teaser/batch55-img0-clean.png} & \includegraphics[width=0.9\figwidth]{assets/img-inv-teaser/batch39-img0-clean.png} \\[1.2mm]
\multicolumn{2}{c}{Reconstructed with CLIP}\\
\includegraphics[width=0.9\figwidth]{assets/img-inv-teaser/original-target=-batch55-img0-2024-09-25_07-19-48-adv.png} & \includegraphics[width=0.9\figwidth]{assets/img-inv-teaser/original-target=-batch39-img0-2024-09-24_20-35-37-adv.png} \\[1.2mm]
\multicolumn{2}{c}{Reconstructed with \rclipf (ours)}\\
\includegraphics[width=0.9\figwidth]{assets/img-inv-teaser/convnext_base_w-fare4-target=-batch55-img3-2024-09-25_07-42-01-adv.png} &
\includegraphics[width=0.9\figwidth]{assets/img-inv-teaser/convnext_base_w-fare4-target=-batch39-img5-2024-09-24_21-05-38-adv.png}
\end{tabular}
\caption{\textbf{Feature inversion.} Starting from a grey image we maximize the cosine similarity to the embedding of an image  (original), once for CLIP and once for our \rclipf. With \mbox{\rclipf}, we get semantically correct reconstructions, whereas the clean CLIP model produces only adversarial noise.
}
\label{fig:teaser_feature_inversion}
\end{subfigure}
\end{tcolorbox}
\end{minipage}

%\end{mdframed}
\caption{Our perceptual metric \rclipf performs similar to DreamSim across tasks and is by far the most robust one.}
\label{fig:teaser}
\end{figure*}
