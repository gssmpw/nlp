\section{Related Work}
\label{sec:related}

\textbf{Perceptual metrics.}
Low-level pixel-based $\ell_p$-metrics and structural similarity SSIM \cite{wang2004image} do not capture well higher-order semantic similarity.
These are outperformed by metrics based on features extracted from neural networks trained on ImageNet, such as
LPIPS \cite{zhang2018unreasonable}, 
PIE-APP \cite{prashnani2018pieapp} and DISTS \cite{Ding_2020}. 
More recently, it has been shown that metrics induced by the features extracted by models trained on larger datasets and via self-supervised training, like CLIP \cite{radford2021clip}, DINO \cite{caron2021emerging} or MAE \cite{he2021masked}, are well aligned with human perception regarding semantic similarity \cite{fu2023learning}. 
DreamSim \cite{fu2023learning} is a recent fine-tuned ensemble of three of these models on the 2AFC dataset NIGHTS which shows the best alignment with human preferences on NIGHTS.
\rev{In our work we show how to improve the performance of CLIP- and DINO-based perceptual metrics.}
\\

\textbf{Adversarial robustness of perceptual metrics.}
Virtually all vision tasks tackled via neural networks are vulnerable to adversarial examples \cite{Szegedy2014AdvExamples}, 
and attacks in several threat models exist \cite{Carlini2017DetectionBypassing,Croce2020Autoattack,Laidlaw2019ThreatModelPerceptibility}.
The main empirical defense which works across different vision tasks is adversarial training \cite{Madry2018AT}. However, the price of having a robust model is typically a drop in performance.
Not surprisingly, perceptual metrics, including LPIPS and DreamSim, are also not robust to adversarial perturbations \cite{ghazanfari2023rlpips,ghazanfari2023lipsim,ghildyal2023attacking}.
In order to get a robust version R-LPIPS of the popular LPIPS metric, Ghazanfari et al. \cite{ghazanfari2023rlpips} perform adversarial training on the 2AFC 
fine-tuning task of the Berkeley-Adobe Perceptual Patch Similarity dataset (BAPPS) \cite{zhang2018unreasonable}.
LipSim \cite{ghazanfari2023lipsim} distills from DreamSim \cite{fu2023learning} a 1-Lipschitz network, and then fine-tunes it on the NIGHTS dataset \cite{fu2023learning}
to achieve certified adversarial robustness.
\rev{Conversely, we will leverage recent techniques for robust zero-shot classification with CLIP to obtain perceptual metrics with SOTA adversarial robustness.}
\\

\textbf{
\rev{Interpretability} of adversarially robust models.}
Feature inversion, i.e. finding an image which matches given features at the output layer, can be used to understand the inner workings of a network. However, it often yields highly distorted images without much semantic content \cite{mahendran2014understanding}.
Notably, adversarially robust models suffer significantly less from this problem,
and can be used to generate semantically meaningful images when maximizing the probability of a specific class \cite{santurkar2019imagesynthesis}.
This 
can be even exploited to generate visual counterfactuals (instance-specific explanations) for modern image classifiers~\cite{augustin2020ratio, boreiko2022sparsevisual}.
\rev{Similarly, robust classifiers are known to have more interpretable gradients than standard ones \cite{santurkar2019imagesynthesis}}.
\rev{Our work illustrates that such properties yield more interpretable perceptual metrics.}