\documentclass{article}
\usepackage{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
\usepackage{tablefootnote}  % for table footnotes
\usepackage{xcolor}     
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{fullpage}
\usepackage{wrapfig}
\usepackage{makecell}
% \usepackage[square,numbers]{natbib}
% \bibliographystyle{abbrvnat}
\renewcommand{\baselinestretch}{1.06}
\usepackage{fullpage}
\usepackage[protrusion=true,expansion=true]{microtype}

% colors
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{cleveref}
\newcommand{\proj}{\mathrm{\Pi}}
\usepackage{amsthm}

\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}


\usepackage{colortbl}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{subfigure}
\usepackage{times}

\usepackage{bm}
\usepackage{pifont}% 
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\title{ReGNet: Reciprocal Space-Aware Long-Range Modeling and Multi-Property Prediction for Crystals}

\author{%
    Jianan Nie\thanks{Equal contribution.} \thanks{Jianan Nie and Peng Gao are with Department of Computer Science, Virginia Tech, Blacksburg, VA, United States (e-mail: \href{mailto:jianan@vt.edu}{jianan@vt.edu}, \href{mailto:penggao@vt.edu}{penggao@vt.edu}). } 
  % examples of more authors
   \quad
   Peiyao Xiao\footnotemark[1] \thanks{Peiyao Xiao and Kaiyi Ji are with Department of Computer Science and Engineering, University at Buffalo, Buffalo, NY, United States (e-mail: \href{mailto:peiyaoxi@buffalo.edu}{peiyaoxi@buffalo.edu}, \href{mailto:kaiyiji@buffalo.edu}{kaiyiji@buffalo.edu}).} 
   \quad
 Kaiyi Ji\footnotemark[3] 
   \quad
   Peng Gao \footnotemark[2]
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}

\maketitle

\begin{abstract}
Predicting properties of crystals from their structures is a fundamental yet challenging task in materials science. Unlike molecules, crystal structures exhibit infinite periodic arrangements of atoms, requiring methods capable of capturing both local and global information effectively. However, most current works fall short of capturing long-range interactions within periodic structures. To address this limitation, we leverage \emph{reciprocal space} to efficiently encode long-range interactions with learnable filters within Fourier transforms. We introduce Reciprocal Geometry Network (ReGNet), a novel architecture that integrates geometric GNNs and reciprocal blocks to model short-range and long-range interactions, respectively. Additionally, we introduce ReGNet-MT, a multi-task extension that employs mixture of experts (MoE) for multi-property prediction. Experimental results on the JARVIS and Materials Project benchmarks demonstrate that ReGNet achieves significant performance improvements. Moreover, ReGNet-MT attains state-of-the-art results on two bandgap properties due to positive transfer, while maintaining high computational efficiency. These findings highlight the potential of our model as a scalable and accurate solution for crystal property prediction. The code will be released upon paper acceptance.

\end{abstract}

\section{Introduction}
\label{submission}

AI-driven methods for accelerated material property prediction~\citep{choudhary2022recent} have achieved notable success in materials sciences.
As the demand for novel materials grows, traditional methods like wet lab experiments are costly and time-intensive. 
Computational methods such as density functional theory (DFT) offer advances but remain limited by high computational costs, hindering scalable exploration of the vast materials space \citep{jones2015density}.
To overcome these challenges, recent studies have adopted machine learning (ML) models in material property prediction and accelerating discovery \citep{chen2019graph, yan2022periodic, zeni2023mattergen, lin2023efficient, yan2024complete}.

Crystals are solid materials extensively utilized across various scientific and industrial domains, including semiconductors, photonics, catalysis, and pharmaceuticals \citep{ashcroft1976solid}. Unlike molecules, crystal structures possess a unique and essential structural feature called \emph{periodicity}, in which atoms are arranged in a highly ordered, infinitely repeating pattern that extends in three-dimensional (3D) space, as illustrated in \Cref{fig:crystalstructure}. This periodic arrangement, referred to as the crystal lattice, fundamentally shapes the material's distinct physical and chemical properties. 
Recently, graph neural network (GNN)-based methods have been developed to predict these properties by constructing graphs connecting atoms within a \emph{small}, pre-defined distance threshold~\citep{xie2018crystal, schutt2018schnet, chen2019graph, louis2020graph, choudhary2021atomistic}.
However, these methods fail to explicitly capture long-range interactions between distant atoms.

A key challenge in predicting crystal properties is \emph{effectively encoding long-range atomic interactions within periodic structures}.
The previous GNN-based method, PotNet~\citep{lin2023efficient}, uses fixed-form physical equations to compute infinite potential summations as edge features. However, this method incurs an additional $\mathcal{O}(n^2)$ computational cost, making it inefficient for complex material systems with large $n$, where $n$ is the number of atoms in the unit cell. Furthermore, material interactions are inherently complicated and cannot be adequately captured by a direct summation of a few predefined potentials.


In this work, we introduce, \emph{reciprocal space}, to efficiently encode long-range interactions using \emph{learnable} continuous filters within Fourier transforms. Reciprocal space is fundamental to determining crystal properties, particularly electronic properties, as described by Blochâ€™s theorem~\citep{bloch1929quantenmechanik}.
Notably, while long-range interactions decay slowly with distance in real space, their Fourier transform decays rapidly, enabling efficient computations. Moreover, our learnable continuous filters, parameterized by neural networks, enable more precise representations of long-range interactions in complex crystals. Previous work, EwaldMP~\citep{kosmala2023ewald}, encodes molecular long-range interactions using Fourier transforms, but relies on a predefined supercell grid size as hyperparameters, which can disrupt unit cell symmetry. 
In contrast, our method leverages reciprocal lattice vectors and atomic fractional coordinates, thus eliminating dependence on predefined supercell grids and enhancing symmetry preservation for more accurate modeling.



While our novel usage of reciprocal space representations effectively captures long-range interactions and global periodicity, it still adheres to the traditional approach of predicting properties individually \citep{schutt2017schnet, choudhary2021atomistic, yan2024complete}. 
This approach becomes computationally and memory inefficient when repeatedly applied to multi-property predictions. To address this limitation, Multi-Task Learning (MTL) offers a promising solution by enabling the prediction of multiple properties within a single model~\citep{evgeniou2004regularized}. Additionally, MTL can leverage positive transfer, where knowledge and representations learned from one task can benefit other tasks. This advantage is particularly relevant for materials, as similar properties (e.g., OPT bandgap and MBJ bandgap) are often highly correlated and share similar structural features. However, to our knowledge, few studies have specifically investigated multi-property prediction for materials. 

In brief, this paper makes the following contributions.

\begin{itemize}
    \vspace{-0.15cm}
    \item \textbf{Architecture design.} We introduce a new model, \emph{Reciprocal Geometry Network (\textbf{ReGNet})}. It consists of multiple message-passing blocks, where each block comprises a geometric GNN for short-range interactions and a novel reciprocal block for long-range interactions. The reciprocal block integrates reciprocal space representations by updating trainable filters within Fourier transforms. The extracted features are then processed by decoders for property prediction.
    
    \item \textbf{Multi-property prediction.}
    We further incorporate a mixture of experts (MoE)~\citep{shazeer2017outrageously} approach in ReGNet, resulting in a new model (\textbf{ReGNet-MT}) that can predict multiple properties in a single run. The short- and long-range structural information are passed to MoE layers such that different tasks receive customized features tailored for specific property predictions. This design allows similar properties to benefit from overlapping expert selections, which promotes positive transfer and enhances prediction accuracy.

    \item \textbf{Superior empirical performance.}
    Extensive experiments show that ReGNet achieves state-of-the-art performance compared to various methods across common material property prediction datasets, JARVIS and the Material Project. ReGNet-MT achieves the best results on OPT bandgap and MBJ bandgap prediction, setting new benchmarks.
    ReGNet-MT also excels in simultaneously predicting multiple properties while maintaining low memory and computational overhead.  
    
\end{itemize}

\section{Background}\label{sec:preliminary}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{material.pdf}
    

    \vspace{-4ex}

    \caption{Illustration of the relationship between real space (left) and reciprocal space (right) in crystals. Long-range interactions can be commutated through Fourier series expansion. More details can be found in \Cref{sec:prelim_reciprocal}.}


    \label{fig:crystalstructure}
\end{figure}

\subsection{Periodic Crystal Structure Representation}
\label{sec:prelim_coordinate}
A crystalline material can be represented as the infinite periodic arrangement of atoms in 3D space, with the smallest repeating structural unit referred to as the unit cell. The unit cell defines the lattice symmetry and dictates the overall periodicity and physical properties of the material.
The structure of crystals is commonly represented using two coordinate systems: the Cartesian coordinate system and the fractional coordinate system.

\paragraph{Cartesian coordinate.}
Mathematically, a material can be represented as $\mathbf{M} = (\mathbf{A}, \mathbf{P}, \mathbf{L})$. 
$\mathbf{A} = [\boldsymbol{a}_1, \boldsymbol{a}_2, \cdots, \boldsymbol{a}_n]^\top \in \mathbb{R}^{n \times h}$ denotes the atom feature matrix for $n$ atoms within a unit cell, where $\boldsymbol{a}_i \in \mathbb{R}^{h}$ represents the $h$-dimensional features of atom $i$ in the unit cell. $\textbf{P} = [\boldsymbol{p}_1, \boldsymbol{p}_2, \cdots, \boldsymbol{p}_n]^\top \in \mathbb{R}^{n \times 3}$ is the position matrix, where $\boldsymbol{p}_i \in \mathbb{R}^{3}$ is the Cartesian coordinate of atom $i$ inside the unit cell. 
In crystallography, a lattice is a periodic arrangement of points that defines a crystalâ€™s translational symmetry. As shown in \Cref{fig:crystalstructure}, the lattice matrix
$\textbf{L} = [\boldsymbol{\ell}_1, \boldsymbol{\ell}_2, \boldsymbol{\ell}_3]^\top \in \mathbb{R}^{3\times 3}$ consists of three translation lattice vectors that determine the shape and periodicity of the unit cell.
The periodic repetition of the unit cell is described by integer multiples of the lattice vectors, ensuring translational invariance. Specifically, the infinite crystal structure can be expressed as:
\begin{equation}
\begin{aligned}
    \hat{\mathbf{P}} =  \{&\hat{\boldsymbol{p}_i} | \hat{\boldsymbol{p}_i} = \boldsymbol{p}_i + n_1\boldsymbol{\ell}_1 + n_2\boldsymbol{\ell}_2 + n_3\boldsymbol{\ell}_3,\\ ~& n_1, n_2, n_3 \in \mathbb{Z}, i \in \mathbb{Z}, 1 \le i \le n \}\nonumber
\end{aligned}
\end{equation}
where the integers $n_1, n_2, n_3$ define a 3D translation with $\boldsymbol{\ell}_1, \boldsymbol{\ell}_2, \boldsymbol{\ell}_3$.
$\hat{\mathbf{A}} = \{\hat{\boldsymbol{a}_i} | \hat{\boldsymbol{a}_i} = \boldsymbol{a}_i, i \in \mathbb{Z}, 1 \le i \le n \} $ is the atom feature in repeated unit cells, which remains unchanged under periodic translation.

\paragraph{Fractional coordinate.} Fractional coordinates capture the relative positions of atoms within the unit cell and offer significant advantages for periodic materials compared to Cartesian coordinates. Specifically, the fractional coordinate uses the lattice matrix $\textbf{L}\in \mathbb{R}^{3\times 3}$ as the basis, where atomic positions are described by fractional coordinate vectors $\boldsymbol{f_{i}}=[f_{1},f_{2},f_{3}]^\top\in[0,1)^{3}$. The corresponding Cartesian coordinates are obtained as $\boldsymbol{p}_{i}=\boldsymbol{L}\boldsymbol{f_{i}}$, where $\boldsymbol{p}_{i}=f_{1}\boldsymbol{\ell}_1 +  f_{2}\boldsymbol{\ell}_2 + f_{3}\boldsymbol{\ell}_3$.
The representation of crystal $\mathbf{M}$ generalizes to $\mathbf{M} = (\mathbf{A}, \mathbf{F}, \mathbf{L})$, where $\mathbf{F}=[\boldsymbol{f}_1,\cdots,\boldsymbol{f}_n]^\top\in[0,1)^{n\times 3}$ denotes the fractional coordinates of all atoms in the unit cell. Fractional coordinates inherently align with lattice periodicity, ensuring that calculations respect periodic boundary conditions. Moreover, they remain invariant under space group operations, such as translations and rotations, thereby preserving structural relationships. These properties make fractional coordinates a more efficient and symmetry-aware representation for periodic materials compared to Cartesian coordinates.


\begin{figure}
    \centering
    % \vspace{-0.5cm}
    \includegraphics[width=0.7\linewidth]{architecture.pdf}
    
    \caption{Overall framework architecture and details of each block, consisting of three main components: (a) Model architecture, (b) ReGNet block, and (c) STL/MTL Decoder. The Model Architecture utilizes multiple ReGNet blocks. Each ReGNet block (b) combines local features through graph message passing and global features via reciprocal space convolution to update node embeddings. The STL/MTL Decoder (c) outputs material property predictions, supporting both single-task (STL) and multi-task (MTL) learning setups.}
    \label{fig:model_architecture}
\end{figure}
% -----
\subsection{Reciprocal Space in Crystalline Materials}\label{sec:prelim_reciprocal}


Reciprocal space is essential for understanding important natures of materials such as electronic, structural, and vibrational properties \citep{ashcroft1976solid, gross2014festkorperphysik}. Integrating its representations can enhance predictive accuracy by capturing global periodicity and long-range interactions. Reciprocal space consists of wave vectors $\boldsymbol{k}$ spanned by reciprocal lattice vectors $\boldsymbol{b_1}, \boldsymbol{b_2}, \boldsymbol{b_3}$, which are derived from lattice vectors in real space $\boldsymbol{\ell}_1, \boldsymbol{\ell}_2, \boldsymbol{\ell}_3$. Each vector in reciprocal space represents a spatial frequency that encodes crystal periodicity along specific directions. More details can be found in \Cref{app:reciprocal}.


To establish the reciprocal space representation, we employ the Fourier transform, which expresses periodic function $f$ via Fourier series expansion:
\begin{equation}
    f(\boldsymbol{p}_j) = \sum_{\boldsymbol{k}} \hat{f}(\boldsymbol{k}) \exp(i \boldsymbol{k} \cdot \boldsymbol{p}_j),
\end{equation}
where \( \hat{f}(\boldsymbol{k}) \) represents the Fourier coefficients, encoding the contributions of wave vectors \( \boldsymbol{k} \) at atom $j$. The corresponding inverse transform, which computes \( \hat{f}(\boldsymbol{k}) \) from the real-space representation \( f(\boldsymbol{p}_j) \), is given by:
\begin{equation}\label{eqn:fouriercoefficient}
    \hat{f}(\boldsymbol{k}) = \frac{1}{\Omega} \sum_{j \in S} h_j \exp(-i \boldsymbol{k} \cdot \boldsymbol{p}_j),
\end{equation}
where \( S \) denotes the set of atomic positions in the crystal lattice, \( h_j \) represents function values at site \( \boldsymbol{p}_j \) such as nuclear embedding, and \( \Omega \) is the system volume. 


\subsection{MTL and MoE}\label{sec:moe} Multi-Task Learning (MTL) typically employs a shared backbone network to extract common features, followed by task-specific output heads (decoders) that specialize in individual predictions \citep{xiao2024direction}. In this context, the Mixture of Experts (MoE) framework offers a flexible and scalable approach to capture both shared and task-specific features by leveraging task-relevant experts. MoE has gained significant attention in multi-task scenarios across computer vision and natural language processing due to its adaptability and efficiency \citep{riquelme2021scaling}. An MoE layer consists of a set of expert networks, denoted as $E_i, \forall i\in[1, N]$, where $N$ is the number of experts along with a routing network $R$. The output of an MoE layer
is the weighted sum of the output $E_n(x)$ from every expert, where weights $G_n(x)$ are calculated by the routing network $R$ and $x$ represent the model input. Formally, the output of a MoE layer is given by $$y = \sum_{i=1}^NG_n(x)E_n(x).$$
The routing network utilizes a noisy Top-$K$ selection mechanism \citep{shazeer2017outrageously} to model the probabilities of each expert and select the top $K$ candidates. This process is defined as:
$$G(x)=\text{TopK}(\text{Softmax}(x W+\mathcal{N}(0,1)\text{Softplus}(x W_{\text{noise}}))),$$
where $W$ and $W_{\text{noise}}$ are router model parameters, $\mathcal{N}(0,1)$ is a standard normal distribution, and Softmax($\cdot$) and Softplus($\cdot$) are activation functions. Besides, the TopK($\cdot$) function retains only the $K$ largest values, setting all others to zero.


\section{Method}
This work focuses on predicting the thermodynamic, electronic, and mechanical properties of crystalline materials through a novel approach that explicitly incorporates long-range interactions from reciprocal space. The architectures of ReGNet and its multi-task variant, ReGNet-MT, are illustrated in \Cref{fig:model_architecture}. Unlike \cite{lin2023efficient} which employs fixed physical potentials as physics-informed edge features, our approach employs continuous filters with Fourier transforms, utilizing learnable parameters to encode reciprocal space representations while preserving the periodic symmetry of crystalline systems. 
Inspired by the molecular graph framework in \cite{kosmala2023ewald}, we propose the \textit{ReciprocalBlock}, a specialized module designed to capture long-range interaction in crystalline materials. This module leverages reciprocal lattice vectors and atomic fractional coordinates, eliminating reliance on predefined supercell grid sizes. Therefore, our approach enhances symmetry and periodicity preservation in long-range representations. Besides, we incorporate GNN with our \textit{ReciprocalBlock} to model short- and long-range information in materials. Lastly, we enable multi-property prediction by leveraging MoE.

\subsection{Representation and Embedding}
\paragraph{Invariant crystal graph and embedding.}
Firstly, we construct a radius-based graph, where nodes represent atoms and edges encode atomic interactions within a predefined cutoff radius $r_\text{cut}$. Formally, the edge set is defined as:  
\begin{equation*}  
    \mathcal{E} = \{ e_{ij} : \|\boldsymbol{p}_i- \boldsymbol{p}_j\|_2 \leq r_\text{cut}, \ \forall i, j \in V \}, 
\end{equation*}  
where $V$ is the set of nodes, and distances between nodes are computed based on their 3D Cartesian coordinates $\boldsymbol{p}_i$ and $\boldsymbol{p}_j$. 

Once the crystal graph is established, we can proceed to obtain embeddings for nodes and edges. 
Node feature $\boldsymbol{a}_i$ for node $i$ is first mapped using CGCNN \citep{xie2018crystal} embeddings, followed by a linear transformation to initialize the short-range node features $h_{i,\text{local}}^0$. The initial global atom representation is derived through a linear transformation of ${h}^0_\text{local}$, as shown below: 
\begin{equation*}  
    h^0_\text{global} = \sigma(W_r h^0_\text{local}),  
\end{equation*}
where $W_r$ is a learnable weight matrix. We use ${h}^0_\text{global}$ as initial long-range node features.
The invariant edge feature $||\mathbf{p}_i - \mathbf{p}_j||_2$ is defined by the Euclidean distance between node pair $i$ and $j$, are scaled by \( c / ||\mathbf{p}_i - \mathbf{p}_j||_2 \), where c is a chosen constant to mimic the pairwise potential in \cite{lin2023efficient}. These values are then embedded using radial basis function (RBF) kernels to get the initial edge features ${v}_{ij}^e$. More details are shown in \Cref{app:emmbeddingsetting}


\paragraph{Fractional coordinates and reciprocal lattice vectors.}
\label{sec:fractional_coordinates}
To effectively extract features from long-range information in reciprocal space, we propose using fractional coordinates $\boldsymbol{f}$ and basis reciprocal lattice vectors $\boldsymbol{k}_m$ with continuous filters convolution.
Together, they enable a precise and symmetry-consistent representation of periodic structures (discussed in \Cref{sec:preliminary}).
Our approach differs from previous methods \citep{kosmala2023ewald} for the following reasons.
First, for crystalline materials, Cartesian coordinates can introduce ambiguities, as identical crystal structures may be expressed differently due to periodic transformations, such as translations or rotations. For example, the lattice matrices $\textbf{L}=[\boldsymbol{\ell}_1, \boldsymbol{\ell}_2, \boldsymbol{\ell}_3]^\top$ and $\textbf{L}'=[\boldsymbol{\ell}_1+\boldsymbol{\ell}_2, \boldsymbol{\ell}_2, \boldsymbol{\ell}_3]^\top$ describe identical periodic patterns but differ in their Cartesian representations. 
In contrast, fractional coordinates normalize atomic positions relative to the unit cell, ensuring invariance under periodic transformations and eliminating redundancies in Cartesian representations.
Second, it relies on a predefined grid size as hyperparameters to cover all relevant frequencies as $\boldsymbol{k}$, which could disrupt the unit cell symmetry and substantially increase computational costs, particularly for large unit cells and complex systems. Instead, our method leverages the natural periodicity of the lattice in materials, avoiding redundant grid parameterization and ensuring a symmetry-consistent representation of long-range interactions. Therefore, by combining fractional coordinates $\boldsymbol{f}$ and basis reciprocal lattice vectors $\boldsymbol{k}_m$, we achieve an effective framework for feature extraction in reciprocal space, accurately capturing the long-range information of crystal materials.

With the aforementioned representations, our model enables both short-range and long-range message passing to effectively capture comprehensive material structural features.

\subsection{Short-Range Message Passing}
We employ the short-range block in \Cref{fig:model_architecture}(b)
to update the atomic representations using an invariant graph neural network with multiple message-passing layers. Each node aggregates information only from its geometric neighbors within the cutoff radius $r_{\text{cut}}$. Therefore, we naturally interpret this as capturing the short-range information to material representations.
The computational process at the \(\ell\)-th message-passing layer for a node \(i\) is expressed as:
\begin{equation}
\label{eqn:mpnn}
    h_{i,\text{local}}^{\ell+1} \leftarrow g\left(h_{i,\text{local}}^{\ell}, \sum_{j \in V} \phi\left(h_{i,\text{local}}^{\ell}, h_{j,\text{local}}^{\ell}, v_{ij}^e\right)\right),
\end{equation}
where \(h_{i,\text{local}}^{\ell}\) represents the embedding features of node \(i\) at the \(\ell\)-th layer. Here, \(g\) and \(\phi\) are trainable layers within the GNN. Specifically, \(\phi\) computes interactions between node embeddings and edge features \(v_{ij}^e\), while \(g\) aggregates these messages to update the node embeddings. Further details on the architecture of the local geometric GNN are provided in Appendix \ref{app:exp}.



\subsection{Long-Range Message Passing}
To complement the short-range modeling, we employ the long-range block in \Cref{fig:model_architecture}(b) to globally update the atomic representations using the \textit{ReciprocalBlock}. Specifically, this block leverages Fourier transform-based continuous filters with neural networks to encode periodic structures from reciprocal space. Besides, as discussed in \Cref{sec:fractional_coordinates}, we utilize atomic fractional coordinates $\boldsymbol{f}$ and basis reciprocal lattice vectors $\boldsymbol{k}_m$ to extract structural information from reciprocal space and update the node features in the model block. The global embeddings are computed iteratively:
\begin{equation*}
h^{\ell+1}_\text{global} = \text{ReciprocalBlock}({h}_\text{global}^{\ell}, \boldsymbol{f}, \boldsymbol{k}_m),
\end{equation*}
where \( {h}^{\ell}_\text{global} \) represents the input embeddings from the $\ell$-th layer.


\paragraph{Reciprocal block.}  
Here, we explain the reciprocal block in detail. The reciprocal block incorporates long-range interactions into the node embeddings using continuous filters on features derived from Fourier transforms.
Each material $m$ has reciprocal lattice vectors $\boldsymbol{k}_m$ based on the lattice vector in real space. The set of atoms in the material is denoted as \( \mathcal{I}_m \), where \( j \in \mathcal{I}_m \) represents all atoms within material $m$. This block aggregates the contributions of \emph{all} atoms in each material to compute a global representation in the reciprocal domain. For each material $m$, the reciprocal embedding ${r}_{_m}$ is computed as:
\begin{equation*}
{r}_{m} = \sum_{j \in \mathcal{I}_m} {h}_{j,\text{global}}^{\ell} \cdot \exp(-i \boldsymbol{k}_m^\top \boldsymbol{f}_j),
\end{equation*}
where ${h}_{j,\text{global}}^{\ell}$ is the global embedding of atom $j$ from $\ell$-th layer, \( \boldsymbol{f}_j \) is its fractional coordinate, and $i$ is the imaginary unit. This operation aggregates atomic embeddings into a single reciprocal representation for materials, capturing global periodic interactions.

Subsequently, the reciprocal embedding ${r}_m$ 
undergoes an inverse Fourier transform to be mapped back into real space for the atoms in material $m$. This process can be expressed as below:
\begin{equation*}
 \tilde{h}_{ \text{global}}^{\ell}= \sum_{j\in\mathcal{I}_m}\exp(i \boldsymbol{k}_m^\top \boldsymbol{f}_j) \cdot {r}_{m} \cdot \mathbf{W}_\text{filter},
\end{equation*}
where \( \exp(i \boldsymbol{k}_m^\top\boldsymbol{f}_j) \) performs the inverse Fourier transform to map the reciprocal representation back to the real domain.  
\( \mathbf{W}_\text{filter} \) is a \emph{trainable filter} refining the reciprocal embedding, selectively emphasizing important contributions from the reciprocal space features to model long-range interactions. 
In addition, the prefactor \( \frac{1}{\Omega} \) in \cref{eqn:fouriercoefficient} is incorporated into the learned filter \( \mathbf{W}_\text{filter} \), without requiring explicit normalization by the system volume.
In a word, this operation effectively incorporates long-range interactions and periodicity from the reciprocal domain into the real-space representation of each atom \( j \in \mathcal{I}_m \). 

Finally, the global embeddings $\tilde{{h}}^{\ell}_\text{global}$ are updated using residual connections to get ${h}^{\ell+1}_\text{global}$, as shown below.
\begin{equation*}
{h}^{\ell+1}_\text{global} = {h}^{\ell}_\text{global} + \tilde{{h}}^{\ell}_\text{global}.
\end{equation*}
At this point, the complete process of a Reciprocal Block has been fully described.

\subsection{Hierarchical Embedding Integration}

Our model employs multiple ReGNet blocks to integrate local and global information at each layer. Within each block, node embeddings are updated by combining contributions from short-range graph message passing and long-range reciprocal-based message passing. The resulting integrated embeddings are passed to the next block, enabling the model to propagate and refine hierarchical representations of material structures. This approach ensures the effective capture of both local atomic-scale features and global lattice-scale features throughout the network.


\subsection{Decoder Block}

\paragraph{Single-property prediction.}  
After completing the iterative message-passing steps, node features are aggregated within each graph using mean pooling, which captures the overall structural information of the material. The resulting graph-level representation is then processed through fully connected layers to predict the target material property.

\paragraph{Multi-property prediction.}
When doing multi-property prediction, we employ an MTL decoder, consisting of MoE layers and task-specific heads (fully connected layers in our case). We assume that the aggregated node features from the final ReGNet block encapsulate both short-range and long-range information of materials, that can be shared among all property prediction tasks. However, different properties require customized features before individual task heads. Therefore, experts are expected to learn distinct features, while the routing network can determine the optimal expert combination. Unlike the post-processing of the predicted property values after decoders with physical laws \citep{ren2024physical}, this approach offers more flexibility. Meanwhile, similar properties could have a large expert selection overlap such that the knowledge can be shared among them. Notably, this expert-sharing mechanism facilitates positive transfer.



\begin{table}[t!]
\centering
\caption{Comparison of test MAE on The Materials Project dataset. The best results are highlighted in \textbf{bold}, while the second-best results are indicated with \underline{underlines}. The symbol $\boldsymbol{|}$ separates results from two distinct 2-property prediction experiments, where the four properties are divided into two groups and trained separately.}
\label{mp-table}
\small
\vspace{0.5em}
% \resizebox{0.7\columnwidth}{!}{%

\begin{tabular}{lcccc}
    \toprule
    & Formation Energy & Band Gap & Bulk Moduli & Shear Moduli \\
    Method & meV/atom  &  eV &   log(GPa) & log(GPa)  \\
    \midrule
    CGCNN & 31 & 0.292  & 0.047 & 0.077 \\
    SchNet & 33 & 0.345 & 0.066 & 0.099 \\
    MEGNET & 30 & 0.307 & 0.060 & 0.099 \\
    GATGNN & 33 & 0.280 & 0.045 & 0.075 \\
    ALIGNN & 22 & 0.218 & 0.051 & 0.078 \\
    Matformer & 21.0 & 0.211 & 0.043 & 0.073 \\
    PotNet & 18.8 & 0.204 & 0.040 & 0.0650 \\
    Crystalformer & 18.6 & 0.198 & \underline{0.0377} & 0.0689 \\
    eComFormer & \underline{18.16} & 0.202 & 0.0417 & 0.0729 \\
    iComFormer & 18.26 & \underline{0.193} & 0.0380 & \underline{0.0637} \\
    \midrule
    ReGNet & \textbf{17.07} & \textbf{0.189} & \textbf{0.0328} & \textbf{0.0628}\\
    ReGNet-MT & 24.50 & 0.218 & \multicolumn{1}{|c}{0.0391} & 0.0668 \\
    
    \bottomrule
\end{tabular}%
% }
\end{table}
\section{Related Work}
\paragraph{Crystal property prediction.} 
Crystal property prediction has been widely studied using both physics-based and deep-learning approaches. Traditional physics-based methods, such as Coulomb matrices \citep{rupp2012fast, elton2018applying}, effectively model ionic and metallic materials but lack generalization due to constraints like permutation invariance. Deep learning has introduced more flexible predictions by representing crystals as chemical formulas and using sequence models \citep{jha2018elemnet, goodall2020predicting, wang2021compositionally}. More recent methods leverage 3D geometric structures, modeling crystals as 3D graphs with graph neural networks (GNNs) \citep{schutt2017schnet, gasteiger2020directional}. Key advances include CGCNN \citep{xie2018crystal}, which utilized multi-edge graphs to model periodic invariance, inspiring methods such as ALIGNN \citep{choudhary2021atomistic}, which incorporated angle-based features, and Matformer \citep{yan2022periodic}, which captured periodic patterns using self-connecting edges. PotNet \citep{lin2023efficient} proposed physics-informed edge features that embed the infinite summation of pre-defined interatomic potentials, while Crystalformer \citep{taniai2024crystalformer} encoded periodic structures with infinitely connected attention. \cite{yan2024complete} introduced iComFormer with invariant descriptors and eComFormer with equivariant vectors. Despite significant advancements, many approaches still face limitations in accurately capturing periodic patterns and long-range interactions while maintaining computational efficiency and reliable generalization.


\paragraph{Long-range interaction modeling.}
Modeling long-range interactions remains a significant challenge. Early approaches augmented physical equations with hand-crafted terms \citep{staacke2021role} or predicted atomic charges \citep{unke2021spookynet}, while recent methods focus on data-driven learning. 
\cite{yu2022capturing} computed scalar-valued structure factors and integrates them with GNN outputs only in the final layer, limiting interaction between reciprocal space and local features. In contrast, \cite{kosmala2023ewald} integrated structure factor embeddings directly into GNN message passing, enabling feedback to node embeddings. However, it requires extensive hyperparameter tuning for grid sizes ($N_x, N_y, N_z$) to define the supercell, which can disrupt the symmetry of the unit cell, alter its space group, and substantially increase computational costs, particularly for systems with complex symmetries.
\cite{lin2023efficient} considered the infinite summations of pairwise atomic interactions with additional computational cost $\mathcal{O}(n^2)$, relied on precomputed, non-trainable physical potentials as edge features, which limited adaptability to diverse material systems and further increased computational overhead.


\paragraph{Multi-property prediction.} While multi-property prediction in materials remains largely unexplored, we summarize several advances in molecular domains. \cite{liu2022structured} introduced the MTL concept in molecular property prediction in the early stage. Their approach involves constructing a sparse dataset and relying on prior knowledge of property relations before training. \cite{christofidellis2023unifying} proposed a language model that bridges natural and chemical languages to perform multi-task learning, such as chemical reaction prediction and retrosynthesis. However, their work is not specifically focused on multi-property prediction. Additionally, \cite{beaini2023towards} developed a foundation model and a large-scale molecular dataset tailored for multi-task learning. Nevertheless, there is currently no comparably large material dataset to support a similar effort. In another approach, \cite{ren2024physical} leveraged physical laws after task decoders to address data heterogeneity across different properties, improving prediction consistency. 
In contrast, we integrate the Mixture of Experts (MoE) between encoders and decoders, allowing each task to obtain customized structural features before property prediction.


\section{Experimental Studies}

\begin{table*}[t!]
 \centering
 \caption{Comparison of test MAE on the JARVIS dataset. The best results are highlighted in \textbf{bold}, and the second-best results are indicated with \underline{underlines}. Lower test MAE indicates better results.}
 \label{jarvis-table}
 \small % Change font size for the table
 \vspace{0.5em}
 \begin{tabular}{lccccc}
    \toprule
    & Form. Energy & Bandgap(OPT) & $E_{\text{total}}$ & Bandgap(MBJ) & $E_{\text{hull}}$ \\
    \cmidrule(r){2-6}
    Method & meV/atom  &  eV & meV/atom & eV & meV  \\
    \midrule
    CGCNN  & 63 &   0.20 & 78 & 0.41 & 170 \\
    SchNet  & 45 &   0.19 & 47 & 0.43 & 140 \\
    MEGNET  & 47 &   0.145 & 58 & 0.34 & 84 \\
    GATGNN  & 47 &   0.170 & 56 & 0.51 & 120 \\
    ALIGNN & 33.1 &  0.142 & 37 & 0.31 & 76  \\
    Matformer & 32.5 & 0.137 & 35 & 0.30 & 64  \\
    PotNet & 29.4 & 0.127 & 32 & 0.27 & 55   \\
    Crystalformer & 30.6 & 0.128 & 32 & 0.27 & \underline{46}   \\
    eComFormer & 28.4 & 0.124 & 32  & 0.28 & \textbf{44} \\
    iComFormer & \underline{27.2} & \textbf{0.122} & \underline{28.8}  & 0.26 & 47 \\
    \midrule
    ReGNet & \textbf{27.0} & 0.127 & \textbf{26.8} & \underline{0.24} & 48 \\
    ReGNet-MT &  35.0 & \textbf{0.122} & 36.0 & \textbf{0.21} & 69.2 \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Experimental Setup}


We conduct a series of experiments using two widely recognized datasets in materials science. Materials Project \citep{chen2019graph} is a collection of 69239 crystals from the Materials Project database retrieved by \cite{chen2019graph}, with regression tasks of formation energy, bandgap, bulk modulus, and shear modulus. JARVIS-DFT \citep{choudhary2020joint} is a collection of 55723 crystals, with regression tasks of formation energy, total energy, bandgap (OPT), bandgap (MBJ), and energy above hull ($E_{\text{hull}}$). These datasets cover a range of scales, including 69239 crystals for large-scale tasks, 18171 crystals for medium-scale tasks, and 5450 crystals for small-scale tasks.

To ensure consistency, we adopt experimental settings aligned with prior works, including MatFormer \citep{yan2022periodic} and PotNet \citep{lin2023efficient}. 
We benchmark against several baseline methods, including CGCNN \citep{xie2018crystal}, SchNet \citep{schutt2017schnet}, MEGNet \citep{chen2019graph}, GATGNN \citep{louis2020graph}, ALIGNN \citep{choudhary2021atomistic}, MatFormer \citep{yan2022periodic}, PotNet \citep{lin2023efficient}, Crystalformer \citep{taniai2024crystalformer}, and ComFormer \citep{yan2024complete}. All experiments are conducted on NVIDIA A100 GPUs. Additional details on the experimental configurations and ReGNet's settings are provided in Appendix \ref{app:exp}.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth, keepaspectratio, height=8cm]{expert_prob_sim.pdf}
\vspace{-0.8cm} % Adjust vertical space if needed
\caption{MoE experts frequencies for each property on the JARVIS dataset and property similarities. The expert selections in OPT bandgap and MBJ bandgap show the highest similarity and positive transfer happens between these two predictions.}
\vspace{-0.3cm}
\label{fig:probandsimilarity}
\end{figure*}

\subsection{Experimental Results} 
\emph{ReGNet achieves state-of-the-art performance across various crystal property prediction tasks.} On the Materials Project dataset, ReGNet consistently outperforms previous works across all four tasks, achieving the best results with significant margins as shown in \Cref{mp-table}. For predicting two energy properties and two mechanical properties, we apply two ReGNet-MT models separately, achieving comparable performance.
For the JARVIS dataset, as shown in \Cref{jarvis-table}, ReGNet surpasses baseline models in three out of five tasks: formation energy, $E_{\text{total}}$, and band gap (MBJ), while achieving competitive results on $E_{\text{hull}}$ and band gap (OPT). Notably, it consistently outperforms PotNet across all tasks. ReGNet-MT on all 5-property prediction delivers the best results for band gap (OPT) and band gap (MBJ), outperforming all single-property prediction models, while maintaining comparable performance on the remaining tasks.


The superior performance of ReGNet and ReGNet-MT is attributed to several key factors. ReGNet leverages trainable Fourier filters from reciprocal space to capture long-range interactions while preserving the periodic symmetry of crystal structures. In contrast, prior methods rely on fixed physical potentials or predefined grid sizes that require tuning. Besides, by integrating reciprocal-space-based global information with GNN-driven local modeling, ReGNet provides a comprehensive representation of crystalline materials. Additionally, ReGNet-MT benefits from positive task transfer, achieving superior performance on band gap (OPT) and band gap (MBJ) in the 5-property prediction. 


\begin{table}[!ht] 
 % \vspace{-0.3cm}
 \caption{Ablation studies for the effects of reciprocal space and number of blocks.}
 \label{tb:ablation_small}
 \small % Change font size for the table
 \vspace{0.5em}
 \centering
\begin{tabular}{lc}
    \toprule
    \multirow{2}{*}{Method}  & JARVIS Formation Energy \\
    \cmidrule(lr){2-2}
    & meV/atom \\
    \midrule
    ReGNet w/o Reciprocal(3) & 29.6 \\
    ReGNet(3)  & 27.8 \\
    ReGNet(4) & \textbf{27.0} \\
    \bottomrule
\end{tabular}

\vspace{-0.3cm}
\end{table}


\subsection{Model Efficiency Comparison}
As shown in \Cref{tb:efficiency}, we evaluate the computational efficiency of ReGNet and ReGNet-MT by comparing their model parameters and training time per epoch with ALIGNN, Matformer, PotNet, and ComFormer on the JARVIS formation energy task. 
We use 3 and 4 to indicate the number of blocks in ReGNet, which covers all experimental settings of ReGNet.
ReGNet demonstrates superior computational efficiency, with ReGNet(3) achieving the fastest training time per epoch among all models.
Notably, in the multi-task setting, ReGNet-MT predicts five properties simultaneously. The training time does not even double that of single-task ReGNet, and the model's parameter size is smaller than that of single-task eComFormer. It is also important to note that PotNet reduces its training time by precomputing infinite potential summations for the training data, which is not employed in ReGNet. This highlights the efficiency of ReGNet in handling both single-task and multi-task scenarios.

\begin{table}[t]
\vskip -0.1in
\caption{Comparison of model efficiency on JARVIS formation energy prediction. We include training time per epoch, 1-task model parameter size, and 5-task model parameter size. N/A denotes that ReGNet-MT is not designed for single-property prediction.}
\vspace{-1ex}
\begin{center}
\vskip -0.05in
\label{tb:efficiency}
\small
% \resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccc}
\toprule
Method   & Time/Epoch & 1-task Model Para. & 5-task Model Para. \\ 
\midrule
ALIGNN & 261 s & 4.0 M & 20.0 M \\
Matformer & 51 s & 2.9 M & 14.5 M\\
PotNet & 34 s & 1.8 M & 9.0 M\\
iComFormer & 62 s & 5.0 M & 25.0 M\\
eComFormer & 92 s & 12.4 M & 62.0 M\\
\midrule
ReGNet(3)  & 31 s & 3.3 M & 16.5 M \\
ReGNet(4)  & 38 s & 4.3 M & 21.5 M \\
ReGNet-MT  & 60 s & N/A & 9.5 M \\
\bottomrule
\end{tabular}
% }
\end{center}
% \vspace{-2ex}
\vskip -0.25in
\end{table}



\subsection{Ablation Studies}
In this section, we highlight the significance of incorporating reciprocal space information and analyze the impact of the number of ReGNet blocks. As shown in
\Cref{tb:ablation_small}, the inclusion of reciprocal space information enhances performance.
Additionally, increasing the number of ReGNet blocks further improves performance. However, to balance accuracy and computational efficiency, we limit the maximum number of blocks to 4 in all results, as additional blocks introduce higher computational overhead.
Detailed results of the ablation study on the JARVIS dataset are presented in \Cref{tb:ablation_jarvis_recp} within \Cref{app:ablation}.


\subsection{Understanding ReGNet-MT}
In the experimental evaluation of ReGNet-MT on the JARVIS dataset for jointly predicting five properties, the model achieves the best performance for two bandgaps due to positive transfer though some degradation happens. Notably, bandgap (MBJ) surpasses the state-of-the-art (SOTA) results of single-task models, while bandgap (OPT) matches the SOTA performance. To evaluate this improvement, we track the selection frequencies of the 15 experts and visualize the expert selection probabilities for each task (see \Cref{fig:probandsimilarity}, left). We also compute the cosine similarity among properties via expert selection probabilities (see \Cref{fig:probandsimilarity}, right). Among the five tasks, the two bandgap properties exhibit the highest similarity score of 0.85, indicating a big overlap in the expert selection, which facilitates the positive transfer. This finding explains why ReGNet-MT outperforms single-task training for two bandgaps.


\section{Limitation}
One limitation of our study lies in using geometric GNNs for short-range modeling to capture local features. While effective, recent advancements in Transformer-based models \citep{yan2022periodic, yan2024complete} present opportunities for further improvement by incorporating reciprocal space information to enhance the structural representation of materials. Besides, multi-property prediction remains an area worthy of further exploration, as current efficient multi-task learning methods can be explored. Despite these limitations, our study provides a new perspective on modeling crystal materials by leveraging reciprocal-space-based long-range information alongside graph-based short-range information. 

\section{Conclusion}
In this paper, we introduce a novel model, ReGNet, designed to capture the reciprocal space-based long-range information and geometric GNNs short-range information, in crystal property prediction.
Besides, its multi-task variant, ReGNet-MT with the help of MoE, explores the multi-property prediction. Empirically, ReGNet achieves significant performance improvement on prevalent benchmarks and ReGNet-MT achieves the best results in 2 bandgap properties efficiently. We hope that this efficient model provides a valuable perspective in terms of both ML and materials science to promote further interdisciplinary research and encourage its applications in various materials systems.


\bibliography{ref}
\bibliographystyle{ref_style}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\onecolumn
\appendix
\section{Reciprocal Space Concepts}\label{app:reciprocal}
\subsection{Relations Between Real and Reciprocal Lattice Vectors}\label{app:reciprocallattice}
In materials science, crystals exhibit translational symmetry, where atoms are periodically arranged in 3D space, forming a direct lattice in real space. Each lattice point corresponds to a repeating unit in the crystal structure, described by primitive lattice vectors $\boldsymbol{\ell}_1,\boldsymbol{\ell}_2,\boldsymbol{\ell}_3$. 
Reciprocal space, on the other hand, represents the spatial frequencies associated with this periodic arrangement. It forms a dual lattice, mathematically related to the real lattice through reciprocal lattice vectors $\boldsymbol{b}_1,\boldsymbol{b}_2,\boldsymbol{b}_3$. $V$ is the volume of the parallelepiped spanned by the three primitive
translation vectors of the original Bravais lattice. It remains invariant under periodic permutations of the indices \citep{economou2010physics}. 

Volume and vectors are defined as:
\begin{align}
V &={\boldsymbol{\ell}_1 \cdot (\boldsymbol{\ell}_2 \times \boldsymbol{\ell}_3)}\\
\boldsymbol{b}_1 &= 2\pi \cdot \frac{\boldsymbol{\ell}_2 \times \boldsymbol{\ell}_3}{V}  \\
\boldsymbol{b}_2 &= 2\pi \cdot \frac{\boldsymbol{\ell}_3 \times \boldsymbol{\ell}_1}{V}  \\
\boldsymbol{b}_3 &= 2\pi \cdot \frac{\boldsymbol{\ell}_1 \times \boldsymbol{\ell}_2}{V} 
\end{align}
These vectors satisfy the reciprocal relationship:
\begin{equation}
    \boldsymbol{\ell}_i \cdot \boldsymbol{b}_j = 2\pi \delta_{ij}.
\end{equation}
The symbol \( \delta_{ij} \) represents Kronecker delta, which is a mathematical function defined as:

\begin{equation}
\delta_{ij} =
\begin{cases}
1, & \text{if } i = j \\
0, & \text{if } i \neq j
\end{cases}
\end{equation}

This allows us to define a periodic function naturally in reciprocal space, where vectors 
$\boldsymbol{k} \in \Lambda$ can be expanded in terms of these reciprocal basis vectors \citep{ashcroft1976solid}. 

The 3D spatial frequencies must be integer combinations of \emph{three} spatial basis frequencies 
$\boldsymbol{b}_1,\boldsymbol{b}_2,\boldsymbol{b}_3 \in \mathbb{R}^{3\times 3}$ spanning the \emph{reciprocal lattice}:
\begin{equation}
    \Lambda = \{ n_1'\boldsymbol{b}_1 + n_2'\boldsymbol{b}_2 + n_3'\boldsymbol{b}_3 | n_1', n_2', n_3' \in \mathbb{Z} \}.
\end{equation}

\subsection{Diffraction Methods}\label{app:reciprocaldiffraction}

Reciprocal space is fundamental to diffraction techniques such as X-ray diffraction (XRD) and electron diffraction.  
XRD is a widely used technique in crystallography that examines the constructive interference of X-rays scattered by periodic atomic planes, providing detailed insights into crystal structures \citep{ladd1977structure}.  
Similarly, in electron diffraction, the elastic scattering of electrons by atoms produces diffraction patterns that reveal structural information \citep{egerton2005physical}.
The scattering intensity is modeled using the structure factor:
\begin{equation}
S(\mathbf{G}) = \sum_{j} f_j e^{-i \mathbf{G} \cdot \mathbf{r}_j}
\end{equation}
where $\mathbf{G}$ is the reciprocal lattice vector. $f_j$ represents the atomic form factor in X-ray diffraction, and the electron scattering factor in electron diffraction separately.
Diffraction patterns are governed by Bragg's law:
\begin{equation}
n\lambda = 2d \sin\theta,
\end{equation}
relating lattice spacings ($d$) to diffraction angles ($\theta$) and wavelength ($\lambda$) \citep{cullity1957elements}. Notably, this condition applies to both XRD and ED, although electron diffraction often requires consideration of dynamical scattering effects due to stronger interactions between electrons and matter \citep{james1963dynamical}.

Electron diffraction is widely used in transmission electron microscopy (TEM) to analyze materials. For instance, single crystals produce discrete spot patterns, polycrystals form concentric rings, and amorphous materials show diffuse rings \citep{egerton2005physical}. These patterns enable crystallinity and orientation analysis, with applications in semiconductors, metals, and oxides. \Cref{fig:diffraction} illustrates a typical diffraction pattern. By satisfying the Bragg condition, elastically scattered electrons produce high-intensity spots, with the angular relationship between transmitted and diffracted beams revealing the crystal structure \citep{poeppelmeier2023comprehensive}.


\begin{figure} 
    \centering
    \includegraphics[width=1.0\linewidth]{xray_diffraction.pdf}
    \caption{Example of electron diffraction patterns from  \cite{poeppelmeier2023comprehensive}.}
    \label{fig:diffraction}
\end{figure}



\subsection{Electronic Band}
Reciprocal space plays a crucial role in determining electronic and vibrational properties. Specifically, Bloch's theorem relates the electron wavefunction \(\psi(\mathbf{r})\) to the wave vector $\boldsymbol{k}$ as:
\begin{equation}
\psi(\mathbf{r}) = e^{i \boldsymbol{k} \cdot \mathbf{r}} u(\mathbf{r})
\end{equation}
where $\psi(\mathbf{r})$ is the electronic wavefunction, $\boldsymbol{k}$ is the wave vector in reciprocal space. This theorem facilitates the study of electronic band structures, enabling the prediction of properties like band gaps and carrier mobility \citep{ashcroft1978solid}.

$u(\mathbf{r})$ is a function periodic with the lattice, and r denotes position:
\begin{equation}
u(\mathbf{r} + \mathbf{R}) = u(\mathbf{r})
\end{equation}
with $\mathbf{R}$ being a lattice translation vector. This decomposition separates the plane wave component, representing long-range propagation, and the periodic modulation, capturing local interactions within the unit cell.

The energy eigenvalues, plotted as a function of \( \boldsymbol{k} \) within the first Brillouin zone \citep{ashcroft1976solid}, form a band structure that determines key material properties, including band gaps which distinguish insulators, semiconductors, and metals, effective mass and carrier mobility which influence electronic conductivity, and optical absorption spectra and dielectric constants which govern photon-electron interactions and optoelectronic performance \citep{ziman2001electrons}.


\section{Experimental Details}\label{app:exp}

\subsection{Dataset}
We provide more details for the JARVIS and Materials Project datasets in this section.
\begin{table}[ht!]
\centering
\caption{Dataset sizes vary for different properties.}
\label{tab:dataset_sizes}
\begin{tabular}{llr}
\toprule
\textbf{Dataset} & \textbf{Task} & \textbf{Total} \\
\midrule
\multirow{5}{*}{JARVIS-DFT} 
 & Formation Energy & 55722 \\
 & Bandgap (OPT) & 55722 \\
 & Total Energy & 55722 \\
 & Bandgap (MBJ) & 18171 \\
 & $E_\text{hull}$ & 55370 \\
\midrule
\multirow{4}{*}{Materials Project} 
 & Formation Energy & 69239 \\
 & Band Gap & 69239 \\
 & Bulk Moduli & 5450 \\
 & Shear Moduli & 5450 \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{The Materials Project Dataset}
The Materials Project dataset, introduced by \cite{chen2019graph}, is a comprehensive collection of 69239 materials, widely utilized in crystal property prediction studies. We adopt the data splits in \cite{yan2024complete} to ensure consistency and fair comparisons with prior methods. Specifically, for formation energy and bandgap prediction tasks, the dataset comprises 60000, 5000, and 4239 crystals for training, validation, and testing, respectively. For bulk modulus and shear modulus predictions, the splits include 4664, 393, and 393 crystals. Notably, one validation sample in the shear modulus task is excluded due to a negative GPa value, reflecting an underlying unstable or metastable crystal structure. Among the included crystals, 30084 have been experimentally observed, further highlighting the dataset's reliability for studying material properties.

\paragraph{The JARVIS Dataset}
The JARVIS dataset, proposed by \cite{choudhary2020joint}, contains 55723 materials and serves as a benchmark for crystal property prediction. Following the experimental protocols of \cite{yan2024complete}, we evaluate our methods on five regression tasks: formation energy, total energy, bandgap (using both OPT and MBJ methods), and energy above the convex hull (Ehull). The training, validation, and testing splits for formation energy, total energy, and OPT bandgap consist of 44578, 5572, and 5572 crystals, respectively. The splits for Ehull include 44296, 5537, and 5537 crystals, while MBJ bandgap tasks consist of 14537, 1817, and 1817 crystals. The MBJ functional is considered more accurate for bandgap calculations compared to OPT, and both are utilized in this study. Notably, 18865 of the datasetâ€™s crystal structures have been experimentally observed, adding robustness to its use in predictive modeling.



\subsection{Settings of Model Embeddings} \label{app:emmbeddingsetting}
Node feature is embedded into a CGCNN \citep{xie2018crystal} embedding vector of length 92 based on atomic number, and then mapped to a 256-dimensional initial short-range node feature ${h}^0_\text{local}$ by a linear transformation. The initial global node feature ${h}^0_\text{global}$ is obtained by linear transformation on ${h}^0_\text{local}$ and follows an activation function.
The invariant edge feature $||\mathbf{p}_{i}- \mathbf{p}_{j}||_2$ is inversely scaled to $-0.75/||\mathbf{p}_{i}- \mathbf{p}_{j}||_2$ based on \cite{lin2023efficient}, and then expanded into a 256-dimensional vector by RBF kernels with 256 center values from -4.0 to 4.0; after that, we transfer the 256-dimensional vector to initial edge feature ${v}_{ij}^e$ through a linear transformation followed by a SoftPlus activation.  

\subsection{Geometric GNN Information}  

Building on PotNet \citep{lin2023efficient}, we adopt its base model as the local graph component, without using its additional computed infinite summation of interatomic potentials. The local module performs iterative message passing over the constructed graph using a sequence of graph convolutional layers. 
Each layer updates node features $h$ by aggregating information from neighboring nodes and edge attributes. At layer $l$, messages are constructed by concatenating features of neighboring nodes and edge attributes:  
\begin{equation}  
    \mathbf{z}_{ij}^{(l)} = [h_i^{(l)} \| h_j^{(l)} \| {v}_{ij}^e] \in \mathbb{R}^{3d},  
\end{equation}  
where $h_i^{(l)}$ and $h_j^{(l)}$ are node embeddings of nodes $i$ and $j$, respectively, $d_{ij}$ is the edge attribute, and $\|$ denotes concatenation. 
These concatenated features are processed by a multi-layer perceptron to compute attention coefficients that regulate the influence of neighboring nodes:  
\begin{equation}  
    \beta_{ij}^{(l)} = \text{Sigmoid}\left(\text{BN} \left(\text{MLP} \left( \mathbf{z}_{ij}^{(l)} \right) \right) \right) \in \mathbb{R}^d,  
\end{equation}  
where $\beta_{ij}^{(l)}$ denotes the local attention coefficient. Messages are then aggregated from neighbors using a weighted sum:  
\begin{equation}  
    \tilde{h}_i^{(l)} = \text{Aggregation}\left(\beta_{ij}^{(l)} \odot \text{MLP} \left( \mathbf{z}_{ij}^{(l)} \right) \right) \in \mathbb{R}^d,  
\end{equation}  
with $\odot$ as the Hadamard product and the aggregation operator performing sum or mean pooling. The updated embeddings are computed via residual connections with batch normalization and activation:  
\begin{equation}  
    h_i^{(l+1)} = \text{ReLU}\left(h_i^{(l)} + \text{BN} \left( \tilde{h}_i^{(l)} \right)\right) \in \mathbb{R}^d.  
\end{equation}  

\subsection{Hyperparameter Settings of ReGNet on Different Tasks}

In this subsection, we share the detailed hyperparameter settings of ReGNet and ReGNet-MT for different tasks in JARVIS and the Materials Project crystal datasets. We use a different number of blocks for different tasks, and higher performance is expected if hyperparameters are tuned specifically for each task.

\begin{table}[!ht]
\tiny
  \caption{Model settings of ReGNet for the JARVIS dataset.}
  \label{tb:regnet-jarvis-setting}
  \centering
  \small
  \begin{tabular}{l|c|c|c|c}
    \toprule
     & Num. blocks & Total epochs & Learning rate & Num. neighbors  \\
    \midrule
    formation energy & 4 & 500 & 0.0008 & 16 \\
    band gap (OPT) & 3 & 500 & 0.0006 & 16 \\
    band gap (MBJ) & 3 & 500 & 0.001 & 16 \\
    total energy & 4 & 500 & 0.0008 & 16 \\
    Ehull & 4 & 500 & 0.0006 & 16 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[!ht]
\tiny
  \caption{Model settings of ReGNet for the Materials Project dataset.}
  \label{tb:regnet-mp-setting}
  \centering
  \small
  \begin{tabular}{l|c|c|c|c}
    \toprule
     &  Num. blocks & Total epochs & Learning rate & Num. neighbors  \\
    \midrule
    formation energy & 3 & 500 & 0.0008 & 16 \\
    band gap  & 3 & 500 & 0.0008 & 16 \\
    bulk moduli & 3 & 500 & 0.001 & 16 \\
    shear moduli & 3 & 300 & 0.001 & 16 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h!]
\tiny
  \caption{Model settings of ReGNet-MT for JARVIS and the Materials Project datasets.}
  \label{tb:regnet-mt-setting}
  \centering
  \small
  \begin{tabular}{l|c|c|c|c}
    \toprule
     &  Num. blocks & Total epochs & Learning rate & Num. neighbors  \\
    \midrule
    JARVIS & 3 & 500 & 0.0008 & 16 \\
    Materials Project-(formation energy+band gap) & 3 & 500 & 0.0008 & 16 \\
    Materials Project-(bulk moduli+shear moduli)  & 3 & 300 & 0.001 & 16 \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{JARVIS}. 
We show the model settings of ReGNet and ReGNet-MT on the JARVIS dataset in \Cref{tb:regnet-jarvis-setting} and \Cref{tb:regnet-mt-setting}. We train both models using the Adam \citep{kingma2014adam} optimizer with weight decay of 1eâ€“5, Onecycle scheduler \citep{smith2019super}, and for a duration of 500 training epochs. The batch size is standardized at 64, and the models are trained using L1 loss function. The effectiveness of the model is quantitatively measured using the mean absolute error (MAE). The number of neighbors indicates the $k$-th nearest distance we use as the radius for node $i$. 

\textbf{The Materials Project}. 
We show the model settings of ReGNet and ReGNet-MT on the Materials Project dataset in \Cref{tb:regnet-mp-setting} and \Cref{tb:regnet-mt-setting}. We train both models using the Adam \citep{kingma2014adam} optimizer with weight decay of 1eâ€“5, and Onecycle scheduler \citep{smith2019super}. The batch size is standardized at 64, and the models are trained using L1 loss function. The effectiveness of the model is quantitatively measured using the mean absolute error (MAE). The number of training epochs for shear moduli on ReGNet is 300, while on other three properties and ReGNet-MT is 500.

\begin{figure}[h]
    \centering
    % \vspace{-0.5cm}
    \includegraphics[width=0.65\linewidth]{efficiency.pdf}
    \vspace{-0.8cm}
    \caption{Efficiency comparison of training time per epoch, with each method's training time scaled relative to ReGNet(3). 3 and 4 indicate the number of ReGNet blocks. }
    \label{fig:EfficiencyAnalysis}
\end{figure}

\subsection{Full Ablation on JARVIS Dataset} \label{app:ablation}

\begin{table*}[h]
\caption{Ablation on our method with/without reciprocal space information and model depth in terms of test MAE on JARVIS dataset. 3 and 4 indicate the number of ReGNet blocks. The best results are shown in \textbf{bold}.}
\small % Change font size for the table
\label{tb:ablation_jarvis_recp}
\begin{center}
\begin{tabular}{l|ccccc}
\toprule
% & Formation Energy & Bandgap(OPT) & Total energy & Bandgap(MBJ) & Ehull\\
& Form. Energy & Bandgap(OPT) & $E_{total}$ & Bandgap(MBJ) & $E_{hull}$\\

\cmidrule(r){2-6}
Method & meV/atom  &  eV & meV/atom & eV & meV   \\
\midrule
ReGNet w/o Reciprocal(3) & 29.6 & 0.135 & 32.5 & 0.295 & 68.8\\
ReGNet(3)  & 27.8 & 0.127 & 27.2 & 0.242 & 52.9\\
ReGNet(4) & \textbf{27.0} & \textbf{0.127} & \textbf{26.8} & \textbf{0.239} & \textbf{48.0} \\
\bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\end{table*}


In addition to the primary results, we conduct an ablation study to assess the impact of incorporating reciprocal space information in ReGNet using the JARVIS dataset. As shown in \Cref{tb:ablation_jarvis_recp}, integrating reciprocal space information consistently improves performance across all evaluated properties. Notably, ReGNet with reciprocal space information outperforms its counterpart without it, achieving significant gains in predictive accuracy for total energy and Ehull. The model with four ReGNet blocks achieves better result than the one with three blocks, highlighting the importance of both reciprocal space features and model depth in improving prediction accuracy.



\subsection{Training Time Scale Efficiency Analysis}

Beyond the superior modeling capacity for crystalline materials, our ReGNet is faster and more efficient than previous works. To demonstrate the efficiency of ReGNet, we compare ReGNet and its multi-task variant ReGNet-MT with iComFormer, eComFormer, PotNet, Matformer, and ALIGNN. Evaluation is in terms of training time per epoch on the task of JARVIS formation energy prediction. From \Cref{fig:EfficiencyAnalysis}, ReGNet demonstrates superior computational efficiency. Specifically, ReGNet(3) outperforms iComFormer by 1.77$\times$, eComFormer by 2.96$\times$, PotNet by 1.08$\times$, Matformer by 1.65$\times$, and ALIGNN by 8.41$\times$ speedup. Finally, the time for ReGNet-MT predicting 5 properties does not even double.
These results highlight the effectiveness of ReGNet in reducing computational overhead while maintaining performance in prediction as previously mentioned.

\end{document}
