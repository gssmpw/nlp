\section{Related Work}
\paragraph{Crystal property prediction.} 
Crystal property prediction has been widely studied using both physics-based and deep-learning approaches. Traditional physics-based methods, such as Coulomb matrices \citep{rupp2012fast, elton2018applying}, effectively model ionic and metallic materials but lack generalization due to constraints like permutation invariance. Deep learning has introduced more flexible predictions by representing crystals as chemical formulas and using sequence models \citep{jha2018elemnet, goodall2020predicting, wang2021compositionally}. More recent methods leverage 3D geometric structures, modeling crystals as 3D graphs with graph neural networks (GNNs) \citep{schutt2017schnet, gasteiger2020directional}. Key advances include CGCNN \citep{xie2018crystal}, which utilized multi-edge graphs to model periodic invariance, inspiring methods such as ALIGNN \citep{choudhary2021atomistic}, which incorporated angle-based features, and Matformer \citep{yan2022periodic}, which captured periodic patterns using self-connecting edges. PotNet \citep{lin2023efficient} proposed physics-informed edge features that embed the infinite summation of pre-defined interatomic potentials, while Crystalformer \citep{taniai2024crystalformer} encoded periodic structures with infinitely connected attention. \cite{yan2024complete} introduced iComFormer with invariant descriptors and eComFormer with equivariant vectors. Despite significant advancements, many approaches still face limitations in accurately capturing periodic patterns and long-range interactions while maintaining computational efficiency and reliable generalization.


\paragraph{Long-range interaction modeling.}
Modeling long-range interactions remains a significant challenge. Early approaches augmented physical equations with hand-crafted terms \citep{staacke2021role} or predicted atomic charges \citep{unke2021spookynet}, while recent methods focus on data-driven learning. 
\cite{yu2022capturing} computed scalar-valued structure factors and integrates them with GNN outputs only in the final layer, limiting interaction between reciprocal space and local features. In contrast, \cite{kosmala2023ewald} integrated structure factor embeddings directly into GNN message passing, enabling feedback to node embeddings. However, it requires extensive hyperparameter tuning for grid sizes ($N_x, N_y, N_z$) to define the supercell, which can disrupt the symmetry of the unit cell, alter its space group, and substantially increase computational costs, particularly for systems with complex symmetries.
\cite{lin2023efficient} considered the infinite summations of pairwise atomic interactions with additional computational cost $\mathcal{O}(n^2)$, relied on precomputed, non-trainable physical potentials as edge features, which limited adaptability to diverse material systems and further increased computational overhead.


\paragraph{Multi-property prediction.} While multi-property prediction in materials remains largely unexplored, we summarize several advances in molecular domains. \cite{liu2022structured} introduced the MTL concept in molecular property prediction in the early stage. Their approach involves constructing a sparse dataset and relying on prior knowledge of property relations before training. \cite{christofidellis2023unifying} proposed a language model that bridges natural and chemical languages to perform multi-task learning, such as chemical reaction prediction and retrosynthesis. However, their work is not specifically focused on multi-property prediction. Additionally, \cite{beaini2023towards} developed a foundation model and a large-scale molecular dataset tailored for multi-task learning. Nevertheless, there is currently no comparably large material dataset to support a similar effort. In another approach, \cite{ren2024physical} leveraged physical laws after task decoders to address data heterogeneity across different properties, improving prediction consistency. 
In contrast, we integrate the Mixture of Experts (MoE) between encoders and decoders, allowing each task to obtain customized structural features before property prediction.