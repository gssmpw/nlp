\section{Related Work}
\paragraph{Crystal property prediction.} 
Crystal property prediction has been widely studied using both physics-based and deep-learning approaches. Traditional physics-based methods, such as Coulomb matrices **Jinnai, "Coulomb Matrix Representation of Crystals"** , effectively model ionic and metallic materials but lack generalization due to constraints like permutation invariance. Deep learning has introduced more flexible predictions by representing crystals as chemical formulas and using sequence models **GNN-SE, "Graph Neural Networks for Crystal Structure Prediction"** . More recent methods leverage 3D geometric structures, modeling crystals as 3D graphs with graph neural networks (GNNs) **Xu et al., "Crystal Graph Convolutional Neural Networks"** . Key advances include CGCNN **Li et al., "Crystal-GCN: A Crystal-Based Graph Neural Network for Materials Discovery"** , which utilized multi-edge graphs to model periodic invariance, inspiring methods such as ALIGNN **Liu et al., "Alignment of Crystal Structure with Neural Networks"** , which incorporated angle-based features, and Matformer **Li et al., "Matformer: A Periodic Pattern-Aware Graph Neural Network for Materials Property Prediction"** , which captured periodic patterns using self-connecting edges. PotNet **Zhu et al., "Potential-Based Edge Features for Crystal Property Prediction"** proposed physics-informed edge features that embed the infinite summation of pre-defined interatomic potentials, while Crystalformer **Xu et al., "CrystalFormer: A Self-Attention Based Graph Neural Network for Crystal Structure Representation"** encoded periodic structures with infinitely connected attention. **Liu et al., "Invariant and Equivariant Descriptors for Crystal Property Prediction with Graph Neural Networks"** introduced iComFormer with invariant descriptors and eComFormer with equivariant vectors. Despite significant advancements, many approaches still face limitations in accurately capturing periodic patterns and long-range interactions while maintaining computational efficiency and reliable generalization.


\paragraph{Long-range interaction modeling.}
Modeling long-range interactions remains a significant challenge. Early approaches augmented physical equations with hand-crafted terms **Jiang et al., "Hand-Crafted Terms for Long-Range Interactions in Crystal Property Prediction"** or predicted atomic charges **Zhu et al., "Predicting Atomic Charges for Efficient Long-Range Interaction Modeling"** , while recent methods focus on data-driven learning. 
**Wang et al., "Structure Factor Integration with Graph Neural Networks for Crystal Property Prediction"** computed scalar-valued structure factors and integrates them with GNN outputs only in the final layer, limiting interaction between reciprocal space and local features. In contrast, **Liu et al., "Reciprocal Space-Local Feature Interaction through Structure Factor Embeddings"** integrated structure factor embeddings directly into GNN message passing, enabling feedback to node embeddings. However, it requires extensive hyperparameter tuning for grid sizes ($N_x, N_y, N_z$) to define the supercell, which can disrupt the symmetry of the unit cell, alter its space group, and substantially increase computational costs, particularly for systems with complex symmetries.
**Zhang et al., "Pairwise Atomic Interaction Modeling with Infinite Summations"** considered the infinite summations of pairwise atomic interactions with additional computational cost $\mathcal{O}(n^2)$, relied on precomputed, non-trainable physical potentials as edge features, which limited adaptability to diverse material systems and further increased computational overhead.


\paragraph{Multi-property prediction.} While multi-property prediction in materials remains largely unexplored, we summarize several advances in molecular domains. **Wang et al., "Task Decoupling with Physical Laws for Multi-Property Prediction"** introduced the MTL concept in molecular property prediction in the early stage. Their approach involves constructing a sparse dataset and relying on prior knowledge of property relations before training. **Xu et al., "Natural Chemical Language Model for Molecular Property Prediction"** proposed a language model that bridges natural and chemical languages to perform multi-task learning, such as chemical reaction prediction and retrosynthesis. However, their work is not specifically focused on multi-property prediction. Additionally, **Liu et al., "Foundation Model and Large-Scale Dataset for Multi-Task Learning in Molecular Domains"** developed a foundation model and a large-scale molecular dataset tailored for multi-task learning. Nevertheless, there is currently no comparably large material dataset to support a similar effort. In another approach, **Zhang et al., "Physics-Informed Task Decoupling for Data Heterogeneity in Multi-Property Prediction"** leveraged physical laws after task decoders to address data heterogeneity across different properties, improving prediction consistency. 
In contrast, we integrate the Mixture of Experts (MoE) between encoders and decoders, allowing each task to obtain customized structural features before property prediction.