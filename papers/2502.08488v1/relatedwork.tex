\section{Prior Work}
\subsection{One-Shot Federated Learning}
% One-shot federated learning (OSFL) ~\cite{jhunjhunwala2024fedfisher, dai2024enhancing, heinbaugh2023data, yang2024exploring} aims to learn a global model in a single communication round between the clients and the server. 
Existing OSFL approaches can be divided into two categories based on their methodology. The first category utilizes knowledge distillation to learn a global model through either data distillation~\cite{zhou2020distilled} or model distillation~\cite{li2020practical}. In distilled one-shot federated learning (DOSFL) ~\cite{zhou2020distilled}, the clients share distilled synthetic data with the server, which is utilized for the global model training. FedKT ~\cite{li2020practical} utilizes a public auxiliary dataset and student-teacher models trained at clients to learn a global student model. The second category of methods uses auxiliary data generation at the server based on intermediary information shared by the clients. DENSE ~\cite{zhang2022dense} trains a generator model on local classifiers, later used to generate auxiliary data for global model training. In FedCVAE ~\cite{heinbaugh2023data}, the server aggregates the decoder part of the conditional variational encoders (CVAE) trained at each client and generates auxiliary data for the global model. FedDiff ~\cite{mendieta2024navigating} aggregates locally trained diffusion models for forming a global diffusion model for data generation. FedCADO ~\cite{yang2023one} utilizes classifiers trained at each client to generate data for global model training via classifier-guided pre-trained diffusion models (DMs). FedDISC ~\cite{yang2024exploring} utilizes data features for data generation via pre-trained DMs. 

\subsection{Federated Learning with Foundation Models}

The emergence of foundation models (FMs), both large language models (LLMs) ~\cite{achiam2023gpt} and vision language models (VLMs) ~\cite{li2023blip}, has impacted the landscape of machine learning. The application of these FMs, however, has been understudied in FL. Yu et al., ~\cite{yu2024federated} and Charles et al., ~\cite{charles2024towards} explore training FMs in FL setups. PromptFL ~\cite{guo2023promptfl} investigates prompt learning for FMs under data scarcity in FL settings. FedDAT ~\cite{chen2024feddat} proposes a federated fine-tuning approach for multi-modal FMs. FedPCL ~\cite{tan2022federated} integrates FMs into the traditional FL process to act as class-wise prototype extractors. While FMs have the potential to mitigate data heterogeneity and communication load in FL, their full potential has not been utilized in FL settings.