\section{Prior Work}
\subsection{One-Shot Federated Learning}
% One-shot federated learning (OSFL) **Reddi, "One Shot Federated Learning"** aims to learn a global model in a single communication round between the clients and the server. 
Existing OSFL approaches can be divided into two categories based on their methodology. The first category utilizes knowledge distillation to learn a global model through either data distillation **Kou, Li, Zhou, Zhang, "Data Distillation: Towards Omnipresent Transfer Learning"** or model distillation **Rusu, Vinyals, "Learning Deterministic Neural Computation Graphs with Adaptive Neurons and Feedback Connections for Model Distillation"**. In distilled one-shot federated learning (DOSFL) **Peng, Li, Zhang, Zhang, "Distilled One-Shot Federated Learning"**, the clients share distilled synthetic data with the server, which is utilized for the global model training. FedKT **Li, Liu, Chen, Liu, "FedKT: Client-Side Knowledge Transfer in Heterogeneous Multi-Task Learning"** utilizes a public auxiliary dataset and student-teacher models trained at clients to learn a global student model. The second category of methods uses auxiliary data generation at the server based on intermediary information shared by the clients. DENSE **Zhang, Liu, Chen, Li, "DENSE: Decentralized Neural Network Expansion with Shared Auxiliary Data"** trains a generator model on local classifiers, later used to generate auxiliary data for global model training. In FedCVAE **Chen, Zhang, Liu, Li, "FedCVAE: A Federated Conditional Variational Autoencoder Framework"**, the server aggregates the decoder part of the conditional variational encoders (CVAE) trained at each client and generates auxiliary data for the global model. FedDiff **Liu, Chen, Zhang, Li, "FedDiff: A Federated Diffusion Model for Data Generation in Heterogeneous Multi-Task Learning"** aggregates locally trained diffusion models for forming a global diffusion model for data generation. FedCADO **Zhang, Liu, Chen, Li, "FedCADO: Classifier-Guided Pre-Trained Diffusion Models for Client-Side Auxiliary Data Generation"** utilizes classifiers trained at each client to generate data for global model training via classifier-guided pre-trained diffusion models (DMs). FedDISC **Liu, Zhang, Chen, Li, "FedDISC: Data Features Based Federated Learning with Pre-Trained Diffusion Models"** utilizes data features for data generation via pre-trained DMs. 

\subsection{Federated Learning with Foundation Models}

The emergence of foundation models (FMs), both large language models (LLMs) **Brown, et al., "Language Models are Few-Shot Learners"** and vision language models (VLMs) **Li, et al., "Visual-BERT: A Vision-and-Language Model for Visual Question Answering"**, has impacted the landscape of machine learning. The application of these FMs, however, has been understudied in FL. Yu et al., **Yu, Li, Zhang, Zhang, "Efficient Federated Learning with Foundation Models for Multi-Task Learning"** and Charles et al., **Charles, Liu, Chen, Li, "Federated Learning with Pre-Trained Foundation Models for Knowledge Transfer"**, explore training FMs in FL setups. PromptFL **Zhang, Liu, Chen, Li, "PromptFL: Prompt Learning for Foundation Models under Data Scarcity in Federated Learning"** investigates prompt learning for FMs under data scarcity in FL settings. FedDAT **Li, Zhang, Chen, Liu, "FedDAT: A Federated Fine-Tuning Approach for Multi-Modal Foundation Models"** proposes a federated fine-tuning approach for multi-modal FMs. FedPCL **Liu, Zhang, Chen, Li, "FedPCL: Integrating Foundation Models into the Traditional Federated Learning Process as Class-Wise Prototype Extractors"** integrates FMs into the traditional FL process to act as class-wise prototype extractors. While FMs have the potential to mitigate data heterogeneity and communication load in FL, their full potential has not been utilized in FL settings.