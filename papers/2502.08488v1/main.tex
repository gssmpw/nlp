\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\bibliographystyle{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{balance}
\usepackage[table, dvipsnames]{xcolor}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{graphicx}% colors
\newcommand{\oz}[1]{\textcolor{red}{OZ: #1}}
\newcommand{\sj}[1]{\textcolor{blue}{sj: #1}}

\usepackage{tcolorbox}
\definecolor{lightgray}{gray}{0.9}
\newcommand{\mb}[2][yellow]{
  \begin{tcolorbox}[colback=#1, colframe=white, boxrule=0pt, width=\linewidth, left=2pt, right=2pt, top=2pt, bottom=2pt]
    MB: #2
  \end{tcolorbox}
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\DeclareRobustCommand*{\IEEEauthorrefmark}[1]{%
  \raisebox{0pt}[0pt][0pt]{\textsuperscript{\footnotesize #1}}%
}


\title{One-Shot Federated Learning with Classifier-Free Diffusion Models\\}
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
\author{\IEEEauthorblockN{Obaidullah Zaland\IEEEauthorrefmark{1}\textsuperscript{*},
Shutong Jin\IEEEauthorrefmark{2}\textsuperscript{*},
Florian T. Pokorny\IEEEauthorrefmark{2} and  
Monowar Bhuyan\IEEEauthorrefmark{1}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Computing Science,
        Ume\r{a} Unviersity, Ume\r{a}, SE-90781, Sweden\\
Email: \{ozaland, monowar\}@cs.umu.se}

\IEEEauthorblockA{\IEEEauthorrefmark{2}KTH Royal Institute of Technology, Stockholm, SE-10044, Sweden\\
Email: \{shutong, fpokorny\}@kth.se}

\thanks{This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation via the WASP NEST project “Intelligent Cloud Robotics for Real-Time Manipulation at Scale.” The computations and data handling essential to our research were enabled by the supercomputing resource Berzelius provided by the National Supercomputer Centre at Linköping University and the gracious support of the Knut and Alice Wallenberg Foundation.}% <-this % stops a space
% \thanks{Identify applicable funding agency here. If none, delete this.}
}


% \author{\IEEEauthorblockN{Anonymous submission}}
%\IEEEauthorblockA{\textit{Department} \\
%\textit{Affiliation}\\
%Address \\
%email address}
%}
% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }

\maketitle
% Abstract to be reduced to 150 words
\begin{abstract}
% Abstract V1 - Currently 215 words
% Federated learning (FL) enables collaborative learning across clients without gathering data in a central location. Traditional FL algorithms, however, require multiple communication rounds between the server and participating parties to learn a global model that can capture the heterogeneous data distribution, leading to a significant communication overhead. One-shot federated learning (OSFL) lessens the communication load by enabling global model formation with a single communication round between the participating parties and the server, usually through global dataset generation or model distillation. With the increasing popularity of diffusion models (DMs), these models have been employed to generate auxiliary datasets at the server for global model training. However, existing DM-assisted OSFL approaches require auxiliary model training at participating parties, resulting in additional computation overhead at each party. This work proposes OSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a novel OSFL approach that efficiently learns a global model without requiring any auxiliary models. OSCAR employs a vision language foundation model to devise category-specific data representations at each participating party. These representations integrate seamlessly into the classifier-free diffusion model pipeline, enabling auxiliary data generation at the server to obtain the fine-tuned global model. OSCAR is a simple yet cost-effective OSFL approach that outperforms the state-of-the-art on the NICO++ benchmarking dataset while reducing the communication load by at least $\mathbf{99}\%$\footnote{\url{https://anonymous.4open.science/r/oscar-FE04/.}}.


Federated learning (FL) enables collaborative learning without data centralization but introduces significant communication costs due to multiple communication rounds between clients and the server. One-shot federated learning (OSFL) addresses this by forming a global model with a single communication round, often relying on the server's model distillation or auxiliary dataset generation - often through pre-trained diffusion models (DMs). Existing DM-assisted OSFL methods, however, typically employ classifier-guided DMs, which require training auxiliary classifier models at each client, introducing additional computation overhead. This work introduces OSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a novel OSFL approach that eliminates the need for auxiliary models. OSCAR uses foundation models to devise category-specific data representations at each client, seamlessly integrated into a classifier-free diffusion model pipeline for server-side data generation. OSCAR is a simple yet cost-effective OSFL approach that outperforms the state-of-the-art on four benchmarking datasets while reducing the communication load by at least $\mathbf{99}\%$\footnote{\url{https://anonymous.4open.science/r/oscar-FE04/.}}.







\end{abstract}

\begin{IEEEkeywords}
Federated Learning, One-Shot Federated Learning, Diffusion Models
\end{IEEEkeywords}


\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contributions.}
\section{Introduction}


Federated Learning (FL) ~\cite{mcmahan2017communication} is a decentralized machine learning (ML) training methodology that enables multiple clients to collaboratively train a global model without moving the data to a central location, addressing the concerns on data privacy and ownership in the age of growing data privacy regulations. This has led to the application of FL to various domains, including autonomous vehicles ~\cite{xue2024spatial}, the Internet of Things (IoT) ~\cite{wang2022blockchain}, and healthcare ~\cite{banerjee2020multi}. However, since the participating clients usually own nonindependent and identically distributed (non-IID) data, an \textit{optimal} global model is learned through multiple communication rounds between the clients and the server, causing high communication overhead ~\cite{kairouz2021advances}. Several strategies, including client selection ~\cite{singhal2024greedy}, update compression ~\cite{lan2023improved}, and update dropping ~\cite{zhou2024accelerating, buyukates2021timely}, have been proposed to reduce communication load in each communication round. However, these strategies still require client synchronization and suffer under non-IID data distribution across clients, as local client models can drift from the global model at each communication round ~\cite{shi2022optimization}. 
 
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/model_acc.png}
    \caption{The number of uploaded parameters by each client and accuracy for various algorithms on OpenImage dataset and ResNet-18.}
    \label{fig:communicaiton}
\end{figure}


One-shot federated learning (OSFL) ~\cite{guha2019one, tangfusefl} offers an alternative, where the global model is learned through a single communication round between the clients and the server. OSFL can also reduce the impact of heterogeneous data distribution as the global model is not directly formed from the local models. The single communication round can further benefit in scenarios where FL suffers from client dropout or stragglers (i.e., slow-communicating clients) ~\cite{yang2023one}.  Existing OSFL approaches ~\cite{zhang2022dense, zhou2020distilled, yang2024feddeo} rely on auxiliary dataset generation or knowledge distillation to form the global model. Knowledge distillation approaches usually require an auxiliary public dataset as the knowledge transfer medium ~\cite{zhou2020distilled}. On the other hand, dataset generation methods employ pre-trained generative models (e.g., diffusion models) to generate \textit{new} data for training the global model. DMs are pre-trained with vast amounts of data, and \textit{with proper guidance}, these DMs can generate realistic images that resemble a desired distribution. DMs can have an immense impact on FL, as new data resembling the clients' distribution can be generated without access to the raw dataset of the participating clients.
% reducing the impact of data heterogeneity and potentially overperforming the ceiling performance by eliminating the data barrier.  


Current studies incorporating diffusion models in OSFL ~\cite{yang2023one, mendieta2024navigating, yang2024feddeo, zhang2023federated} utilize classifier-guided DMs. Employing classifier-guided DMs has two potential disadvantages: they require auxiliary classifier training at each client, introducing computational and communication burdens at each client. Furthermore, in some cases, the diffusion model needs to be downloaded to the clients~\cite{yang2024feddeo}. Classifier-free DMs ~\cite{ho2022classifier} solve these challenges by integrating the conditioning directly into the model, which is also adopted by most of the prevalent image generative models~\cite{betker2023improving, rombach2022high}. FMs ~\cite{li2023blip, achiam2023gpt} can be employed for the encoding generation without training or fine-tuning. Replacing the classifier models with FMs significantly reduces the client upload size compared to classifier-guided DM-based OSFL approaches, as shown in \textit{Fig} \ref{fig:communicaiton}. The seamless integration of pre-trained FMs and DMs in OSFL simplifies the overall framework, reduces communication load, and enhances scalability and efficiency across heterogeneous client datasets.



In this work, we present \textbf{OSCAR}, \textbf{O}ne-\textbf{S}hot federated learning with \textbf{C}l\textbf{A}ssifier-F\textbf{R}ee diffusion models. OSCAR leverages the strengths of FMs and a classifier-free diffusion model to train a global FL model in a single communication round between the clients and the server. OSCAR relies on each participating party's category-specific encodings to generate data through classifier-free DMs. The generated data is then used to train the global model on the server. By removing the need for classifier training at each client, OSCAR reduces the client upload size by 99\% compared to current state-of-the-art (SOTA) DM-assisted OSFL approaches. In addition to reducing the communication overhead at each client, OSCAR outperforms existing SOTA on four different benchmarking datasets. 



\section{Prior Work}

\subsection{One-Shot Federated Learning}
% One-shot federated learning (OSFL) ~\cite{jhunjhunwala2024fedfisher, dai2024enhancing, heinbaugh2023data, yang2024exploring} aims to learn a global model in a single communication round between the clients and the server. 
Existing OSFL approaches can be divided into two categories based on their methodology. The first category utilizes knowledge distillation to learn a global model through either data distillation~\cite{zhou2020distilled} or model distillation~\cite{li2020practical}. In distilled one-shot federated learning (DOSFL) ~\cite{zhou2020distilled}, the clients share distilled synthetic data with the server, which is utilized for the global model training. FedKT ~\cite{li2020practical} utilizes a public auxiliary dataset and student-teacher models trained at clients to learn a global student model. The second category of methods uses auxiliary data generation at the server based on intermediary information shared by the clients. DENSE ~\cite{zhang2022dense} trains a generator model on local classifiers, later used to generate auxiliary data for global model training. In FedCVAE ~\cite{heinbaugh2023data}, the server aggregates the decoder part of the conditional variational encoders (CVAE) trained at each client and generates auxiliary data for the global model. FedDiff ~\cite{mendieta2024navigating} aggregates locally trained diffusion models for forming a global diffusion model for data generation. FedCADO ~\cite{yang2023one} utilizes classifiers trained at each client to generate data for global model training via classifier-guided pre-trained diffusion models (DMs). FedDISC ~\cite{yang2024exploring} utilizes data features for data generation via pre-trained DMs. 

\subsection{Federated Learning with Foundation Models}

The emergence of foundation models (FMs), both large language models (LLMs) ~\cite{achiam2023gpt} and vision language models (VLMs) ~\cite{li2023blip}, has impacted the landscape of machine learning. The application of these FMs, however, has been understudied in FL. Yu et al., ~\cite{yu2024federated} and Charles et al., ~\cite{charles2024towards} explore training FMs in FL setups. PromptFL ~\cite{guo2023promptfl} investigates prompt learning for FMs under data scarcity in FL settings. FedDAT ~\cite{chen2024feddat} proposes a federated fine-tuning approach for multi-modal FMs. FedPCL ~\cite{tan2022federated} integrates FMs into the traditional FL process to act as class-wise prototype extractors. While FMs have the potential to mitigate data heterogeneity and communication load in FL, their full potential has not been utilized in FL settings. 

\section{Preliminaries}


% \subsection{Federated Learning}

% A centralized FL system contains a server responsible for the model aggregation and distribution alongside $n$ participating devices $\mathcal{R}$, where each device owns a local dataset $\mathcal{D}_{r\in\mathcal{R}}$, and the cumulative dataset can be represented as $\mathcal{D} = \cup_{r\in\mathcal{R}}{\mathcal{\mathcal{D}}_{r}}$. In every training iteration $i$, the clients train the local model $\mathbf{w}_k^i$ and communicate it to the server. The server creates a global model based on an aggregation function (e.g., FedAvg). 


% \begin{equation}\label{eq:FedAvg}
%     \mathbf{w}^{i+1} = \sum_{r\in\mathcal{R}}{\frac{|\mathcal{D}_r|}{{|\mathcal{D}|}} \mathbf{w}^{i}_r}
% \end{equation}
% \
\subsection{Diffusion Models}
Denoising Diffusion Probabilistic Models (DDPMs)~\cite{ho2020denoising} employ a U-Net architecture~\cite{ronneberger2015u}, denoted as $\epsilon_{\theta}$, to model data distribution $x \sim q(x_0)$. For any given timestamp $t \in \{0, \ldots, T\}$
, during the \textit{forward process}, Gaussian noise $\mathbf{I}$ is progressively added according to:
\begin{equation}
q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1},\; \beta_t \mathbf{I}),
\end{equation}

with $\beta_t$ as a learned variance scheduler. In the \textit{reverse process}, $x_0$ is sampled from:

\begin{equation}
p_{\theta}(x_{t-1} | x_t) \sim \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t),\; \Sigma_{\theta}(x_t,\;t)),
\end{equation}

where $\mu_{\theta}(x_t, t)$ is derived from $\epsilon_\theta (x_t, t)$, and $\Sigma_{\theta}(x_t,t)$ is a time-dependent constant. 

When a conditioning signal $y$, such as text, is added, the network is trained to minimize:
\begin{equation}
    \mathcal{L}_{t}(\theta) = \mathbb{E}_{z_0\sim q(\mathcal{E}(x_{0})), \;\epsilon\sim\mathcal{N}(0,\;\mathbf{I}),\;t}\left[ \left\|\epsilon - \hat{\epsilon}_t\right\|^{2}\right], \;
    \hat{\epsilon}_t = \epsilon_\theta (x_t,\; t,\; y),
\end{equation}
where $\hat{\epsilon}_t$ provides an estimate of the score function used for data generation during the reverse process.


\textbf{Classifier-guided Models}~\cite{dhariwal2021diffusion} generate conditional samples by combining the diffusion model's score estimate with the input gradient of a classifier's log probability:
\begin{equation}
    \hat{\epsilon}_t = \epsilon_\theta(x_t, t, y) - s \sigma_t \nabla_{x_t} \log p(y|x_t),
\end{equation}
where \( p(y \mid x_t)\) is the probability of class \(y\) with respect to the input \(x_t\), obtained from the classifier. \(\sigma_t\) denotes the noise scale at timestep \(t\), while $s$ controls the influence of the classifier's guidance on the classifier's output.

\textbf{Classifier-Free Models}~\cite{ho2022classifier}, in contrast, combine the score estimate from a conditional diffusion model with that of a jointly trained unconditional model, guiding the generation by their difference:
\begin{equation}
    \hat{\epsilon}_t = (1 + s)\epsilon_\theta(x_t, t, y) - s\epsilon_\theta(x_t, t, \emptyset),
\end{equation}
where \(\epsilon_\theta(x_t, t, \emptyset)\) represents prediction without conditioning.

In conclusion, classifier-guided models rely on a trained classifier to guide the generative process by predicting the likelihood that the generated output matches a given text description. Classifier-free Models integrate text conditioning directly into the generative process, eliminating the need for an external classifier. This approach has driven recent trends in training text-driven generative models, such as DALL·E~\cite{betker2023improving} and Stable Diffusion~\cite{rombach2022high}.
% \mb{You can save some space from bottom part of the figure. Also you can improve the figure, check this paper from ACM MM'24: https://dl.acm.org/doi/pdf/10.1145/3664647.3681490}
\section{OSCAR: One Shot Federated Learning with Classifier-Free Diffusion Models}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{Figures/OSCAR.png}
    \caption{An illustration of the proposed OSCAR pipeline, where BLIP~\cite{li2022blip}, CLIP~\cite{radford2021learning} Text Encoder, and Stable Diffusion~\cite{rombach2022high} are all used with frozen weights and in a zero-shot manner.}
    \label{structure}
\end{figure*}
Traditional FL setup consists of a central server and a set of clients $\mathcal{R}$, where each client $r\in\mathcal{R}$ trains a local model $\mathbf{w}_r$ on its local dataset $\mathcal{D}_r$ in each iteration and communicates it to the server. The server is responsible for forming a global model $\mathbf{w}$ from the local client models through an aggregation function like federated averaging (FedAvg). Considering the intrinsic data heterogeneity in FL, traditional FL algorithms require multiple communication rounds between the clients and the server to form a global model, leading to significant communication overhead. 

To reduce the communication load in the FL setup, we propose OSCAR, a novel one-shot federate learning (OSFL) approach. OSCAR integrates foundation and pre-trained classifier-free generative models, specifically Stable Diffusion~\cite{rombach2022high}, and learns a global model from clients' category-specific representations. OSCAR facilitates global model learning within a single communication round under non-IID data distribution among the clients. As illustrated in \textit{Fig.}~\ref{structure}, the OSCAR pipeline can be divided into four steps: \textbf{(1)} generating descriptions of the client's client-specific data, \textbf{(2)} encoding features from the generated descriptions, and transmitting them to the server \textbf{(3)} transmitting the category-specific representations to the server, and \textbf{(4)} generating data on the server to facilitate final model training. 
% sj: need to doublecheck whether the FL technical details in my statement are correct or not and add related citation}
\paragraph{Description Generation and Text Encoding} Unlike existing DM-assisted OSFL approaches, OSCAR eliminates classifier training and utilizes the clients' category-specific data features as conditioning for the diffusion model (DM). Each client follows a two-step approach to generate category-specific encodings. First, the client uses a vision-language model (VLM), specifically BLIP~\cite{li2022blip}, to generate textual descriptions for all their images. Then, the client uses a text encoder, specifically CLIP~\cite{radford2021learning} to generate category-specific text-encodings from the textual descriptions, as shown in \textit{Eq.}~\ref{blipclip}. A classifier-free diffusion model can utilize the CLIP encodings directly as text conditioning to generate new data. 

 

% \begin{equation}
%     \{y_i\}_{i=1}^{n} = \{\textsc{CLIP}_{\text{Text}}(\textsc{BLIP}(x_i))\}_{i=1}^{n},
%     \label{blipclip}
% \end{equation}
% where $\{x_i\}_{i=1}^{n}$ represents the set of $n$ category-specific images at the party and $\{y_i\}_{i=1}^{n}$ represents the set of encoded texts, each corresponding to an input $x_i$.
% \begin{equation}
%     y_{cn} = \textsc{CLIP}_{\text{Text}} \left( \textsc{BLIP}(x_{cn}) \right), \quad \text{for } c = 1, \dots, C \text{ and } n = 1, \dots, N
%     \label{blipclip}
% \end{equation}
\begin{equation}
\begin{split}
    y_{cn} &= \textsc{CLIP}_{\text{Text}} \left( \textsc{BLIP}(x_{cn}) \right), \\
    &\quad \text{for } c = 1, \dots, C \text{ and } n = 1, \dots, N
\end{split}
\label{blipclip}
\end{equation}
where $C$ represents the number of categories for the client, $N$ is the number of category-specific images at the party, and $y_{cn}$ denotes the encoded text corresponding to the input $x_{cn}$.


% \oz{For Shutong: The symbol $y_i$ does not provide any information about the category when the client sends $\bar{y}$ to the server. I think it should be a set of them, something like $\{\bar{y}^c\}_{c=1}^{m}$?}
\paragraph{Client Representation and Server Data Synthesis} 
Each client averages the category-specific encodings to form a unified representation for the specific category. Despite its simplicity, averaging the category-specific text encodings aligns well with the classifier-free approach:
% \begin{equation}
%     \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i
% \end{equation}
\begin{equation}
    \bar{y}_c = \frac{1}{N} \sum_{n=1}^{N} y_{cn}
\end{equation}
The averaged feature \(\bar{y}_c\) for each category $c$ is then sent directly to the server to initiate classifier-free sampling:
% \begin{equation}
%     \hat{\epsilon}_T(\bar{y}) = (1 + s) \epsilon_\theta(x_T, T, \bar{y}) - s \epsilon_\theta(x_T, T, \emptyset)
% \end{equation}
\begin{equation}
    \hat{\epsilon}_T(\bar{y}_c) = (1 + s) \epsilon_\theta(x_T, T, \bar{y}_c) - s \epsilon_\theta(x_T, T, \emptyset)
\end{equation}
where the guidance scale \(s\) is fixed at 7.5. In this process, \(x_T\) (with \(T\) set to 50) is randomly sampled from the noise space, and the subsequent sampling is carried out according to:
\begin{equation}
    x_{T-1} = \frac{1}{\sqrt{\alpha_T}} \left( \sqrt{\alpha_T} x_T - \hat{\epsilon}_T(\bar{y}_c) \right) + \sigma_T \mathcal{N}(0, \mathbf{I}), 
\end{equation}
Starting from random noise $x_T$, the model iteratively refines the image through the sequence$ x_T \rightarrow x_{T-1} \rightarrow \dots \rightarrow x_0$, guided by the category-specific encoding $\bar{y}_c$, to produce the final output image $x_0$ within the client's distribution. Replacing a classifier model with the clients' category-specific representations reduces the upload size for each client by at least 99\%, compared to SOTA models. The server generates \textbf{ten} images for each category-specific client representation to form a global synthesized dataset $\mathcal{D}_{syn}$, with $10 \times |\mathcal{R}| \times C$ new images, where $|\mathcal{R}|$ is the number of clients and $C$ is the number of categories. As the global dataset $\mathcal{D}_{syn}$ is constructed based on the category-specific representations from individual clients, it effectively captures the heterogeneous data distribution present at each client.


% \sj{add the number here, specify how many is used for the following model training and maybe how many clients}.
\paragraph{Model Training} After generating the global synthesized dataset $\mathcal{D}_{syn}$, the server trains a centralized model $\mathbf{w}$, specifically a ResNet-18 classifier, on the synthesized dataset. This approach not only reduces the dependency on client synchronization and availability at each training and communication round but also ensures that the model can generalize well to non-IID data across clients. The server communicates the global model $\mathbf{w}$ to all clients after formation, to be later used for inference locally.

\section{Experimental Setup}
\paragraph{Datasets}

\begin{itemize}
    \item \textbf{NICO++} ~\cite{zhang2023nico++} The dataset contains images of size 224 $\times$ 224 from 60 different categories, where each category contains images from different domains. The dataset is designed to evaluate models under heterogeneous environment settings. The dataset has two settings. In \textit{Common NICO++}, all the categories share the same six domains: [autumn, dim, grass, outdoor, rock, and water]. On the other hand, the \textit{Unique NICO++} contains different domains for each category. 

    \item \textbf{DomainNet} ~\cite{peng2019moment} The DomainNet dataset contains images of 345 categories over six domains. This work uses a subset of the DomainNet dataset with 90 categories. 

    \item \textbf{OpenImage} ~\cite{kuznetsova2020open} OpenImage is a multi-task dataset with over 1.7 million images across 600 categories. This work uses a subset of 120 categories from the dataset following the pre-processing in ~\cite{yang2023one}.
 \end{itemize}


\paragraph{Data Division} The data has been divided among all parties in a nonindependent and identically (non-IID) manner. Specifically, the data is \textbf{feature distribution skewed}, as each client owns data about a single domain from each category in the NICO++ and DomainNet datasets. For Openimage, the classes have been divided into six similar subgroups, where each client owns a single category from each subgroup. The number of clients is fixed to \textbf{six}, aligning with the number of domains in all the datasets. 

\paragraph{Baselines} We have considered local learning, federated learning, and state-of-the-art DM-assisted OSFL approaches for comparison against OSCAR. Each client trains a local standalone model on its data in local training. FedAvg~\cite{mcmahan2017communication}, FedProx~\cite{li2020federated}, and FedDyn~\cite{acar2021federated} consider traditional FL setups with minor variations in the local objective and aggregation functions. FedDISC~\cite{yang2024exploring} and FedCADO~\cite{yang2023one} are DM-assisted OSFL approaches and train auxiliary models for image generation at the server.  

% \begin{itemize}
%     \item Local training:


%     \item Traditional FL aggregation: 

%     \item One-Shot FL approaches:
    

    
% \end{itemize}



\section{Results and Analysis}


\paragraph{Main Results} In this section, experimental results are provided to compare OSCAR against baselines. The main comparisons are carried out on the four benchmarking datasets. In local and traditional FL settings, original images are used for training client models, while in FedCADO, FedDISC, and OSCAR, the global models are trained with synthetic data. The test set images, however,  are the actual dataset test images in all the experiments. We have compared OSCAR against two SOTA DM-assisted approaches, FedCADO~\cite{yang2023one} and FedDISC~\cite{yang2024exploring}, alongside traditional FL algorithms and local training. The experiments consider accuracy as the performance measure, calculated as the number of images classified correctly by the trained model divided by the number of images in the test set. Specifically, we only consider \textit{top-1} accuracy.  OSCAR performs better than the baselines on all the considered datasets, as shown in \textit{Table}. \ref{tab:results}. Aside from superior \textit{average} accuracy on the overall test set, OSCAR also performs better than the SOTA on domain-specific test sets. As we have assigned each domain to a single client, we consider the domain-specific test sets as client-specific test sets. All the experiments were carried out with the ResNet-18 classifier network, and the number of images per category for each client was set to 30. 

Like other DM-assisted OSFL approaches, OSCAR performs better on datasets that consist of real images (i.e., NICO++ and OpenImage). The difference is evident in the two domains (sketch and clipart) corresponding to client2 and client3 in the DomaiNnet dataset. While OSCAR performs lower than average in these domains, FL algorithms struggle similarly. 

\begin{table*}[!htb]
    \centering
    \caption{Accuracy (in \%) on the test set for the baselines and OSCAR on four benchmarking datasets. The best results are in bold.}
    \begin{tabularx}{\textwidth}{m{1.2cm}*{7}{>{\centering\arraybackslash}X}|m{1.2cm}*{7}{>{\centering\arraybackslash}X}}
    \hline
         \textbf{Model} &  \multicolumn{7}{c|}{\textbf{Client Test Set Accuracy}} & \textbf{Model} &  \multicolumn{7}{c}{\textbf{Client Test Set Accuracy}} \\
         \hline
         
         \hline
         
         & client1 & client2 & client3 & client4  & client5 &  client6 & avg & & client1 &   client2 & client3 & client4  & client5 &  client6 & avg\\
         \hline

            \multicolumn{8}{c|}{\textbf{DomainNet}} & \multicolumn{8}{c}{\textbf{OpenImage}}\\
         \hline
         Local & 22.22 & 8.54 & 7.67 & 28.95 & 19.16  & 16.10  & 17.64 & Local & 37.72 & 39.95 & 49.01 & 47.41 & 49.20& 41.13 & 43.97 \\
         FedAvg & 35.27 & 11.99 & 5.68& 36.99 & 22.97 & 22.33 &21.88 & FedAvg & 51.84& \textbf{52.63}& 62.70& 58.53 & 63.08 & 54.86 & 57.14 \\
         FedProx & 42.10& 11.73 & 6.29 & 42.61 & 27.53 & 25.60 & 25.33 & FedProx & 54.08& 51.30 &\textbf{63.96} & 60.53 & 63.11 & 51.19 & 57.20 \\
         FedDyn & 37.62 & 13.92 & 6.71 & 40.21 & 26.09 & 23.87 & 23.24 & FedDyn & 51.60 & 49.08& 62.75& 56.07& 59.55& 53.06& 55.22 \\
         FedCADO & 57.31 & 17.51 & 9.43  & 44.25 & \textbf{38.74}  & 38.44  & 34.28 & FedCADO & 51.66 & 48.99 & 62.41 & 55.59 & 58.86 &  52.80 & 55.05 
           \\
          %              
         FedDISC & 56.19 & 14.84 & 8.35  & 43.89 &  38.38 & 36.82 & 33.07 & FedDISC & 49.65 & 47.42 & 54.73 & 53.41 & 60.74 & 52.81 & 53.12  \\
         % FedDEO & &  &  &  & &  &   & FedDEO & &  &  &  & &  & \\
         \rowcolor{lightgray}
         OSCAR & \textbf{66.95} &\textbf{23.25} & \textbf{10.02} & \textbf{44.54} & 34.14 & \textbf{38.97} & \textbf{37.60}& OSCAR &\textbf{55.42} & 51.14 & 63.42 & \textbf{61.12} & \textbf{68.55} & \textbf{58.11} & \textbf{59.49} \\
         % Centralized & 79.26 & 82.93 & 76.68 & 82.53 & 79.12 & 78.96 & 79.74 \\
         % FedAvg & & & & & & & \\
         % FedProx & & & & & & & \\

         % FedDEO & 68.53 & 71.03 & 58.02 & 73.33 & 68.16 & 63.04 & 67.01 \\


        \hline
                    \multicolumn{8}{c|}{\textbf{NICO++ Common}} & \multicolumn{8}{c}{\textbf{NICO++ Unique}}\\
         \hline
         % Centralized & 93.41 & 92.97 & 92.95 & 92.80 & 93.37 & 92.83 & 93.05 \\
         % FedAvg & & & & & & & \\
         % FedProx & & & & & & & \\
         Local & 54.10 & 53.95 & 42.49 & 56.68 & 53.86 & 46.14 & 51.29 & Local & 49.19 & 54.77  & 56.48 & 50.62 & 56.06 & 56.13 & 53.89 \\
         FedAvg & 58.57 & 55.36 & 44.60 & 58.63 & 55.90 & 50.27 & 54.17 & FedAvg & 69.16 &71.34 &74.22 &67.58 &\textbf{79.59} &77.14 & 73.15 \\
         FedProx & 58.63 & 52.12 & 44.96 &58.12 &54.68 & 50.43 & 53.66 & FedProx & 69.48&71.75 & 74.43 & 67.68 & 78.73 & 76.61 & 73.09 \\
         FedDyn & 62.13 & 56.62 & 48.08 & 61.76 & 57.61 & 51.60 & 56.67 & FedDyn & 66.60& 72.46& 74.84& 66.84  &77.66& 78.62 & 72.83  \\
         FedCADO & 49.21 & 58.13 & 54.63 & 54.75 & 54.64 & 47.03 & 53.06 & FedCADO & 75.13 & 70.31 & 73.60 & 68.88 &73.30 & 72.51 & 72.28 \\
         FedDISC & 51.43 & \textbf{59.45} & \textbf{56.17} & 56.82 & 52.32 & 45.64 & 53.64 &FedDISC & 74.32 & 71.25 & \textbf{75.28} & 66.79 &73.47 & 70.06  & 71.86 \\
         % FedDEO & &  &  &  & &  &   & FedDEO & &  &  &  & &  & \\

         \rowcolor{lightgray}
          OSCAR & \textbf{59.11} & 59.32 & 52.96  & \textbf{64.04} & \textbf{62.18} &\textbf{51.70} & \textbf{58.19} & OSCAR & \textbf{75.95} & \textbf{71.32} & 75.13 & \textbf{70.14} & 75.00 & \textbf{73.93} & \textbf{73.62}\\

        \hline
    \end{tabularx}
    \label{tab:results}
\end{table*}



% \paragraph{Label-bassed non-IID resutls:}




% \begin{table*}[!htb]
%     \centering
%     \caption{Label-based non-IID results}
%     \begin{tabularx}{\textwidth}{m{1.2cm}*{7}{>{\centering\arraybackslash}X}|m{1.2cm}*{7}{>{\centering\arraybackslash}X}}
%     \hline
%          \textbf{Model} &  \multicolumn{7}{c|}{\textbf{Client Test Set Accuracy}} & \textbf{Model} &  \multicolumn{7}{c}{\textbf{Client Test Set Accuracy}} \\
%          \hline
         
%          \hline
         
%          & client1 & client2 & client3 & client4  & client5 &  client6 & avg & & client1 &   client2 & client3 & client4  & client5 &  client6 & avg\\
%          \hline
%             \multicolumn{8}{c|}{\textbf{NICO++ Unique}} & \multicolumn{8}{c}{\textbf{NICO++ Common}}\\
%          \hline
%          % Centralized & 93.41 & 92.97 & 92.95 & 92.80 & 93.37 & 92.83 & 93.05 \\
%          % FedAvg & & & & & & & \\
%          % FedProx & & & & & & & \\
%          Local & & & & & & & & Local & & & & & & & \\
%          FedAvg & & & & & & & & FedAvg & & & & & & & \\
%          FedProx & & & & & & & & FedProx & & & & & & & \\
%          SCAFFOLD & & & & & & & & SCAFFOLD & & & & & & & \\
%          FedCADO &  &  &  & & &  & & FedCADO &  &  &  & & &  &\\
%          FedDISC & & & & & & & & FedDISC & & & & & & & \\
%          FedDEO & &  &  &  & &  &   & FedDEO & &  &  &  & &  & \\
%          OSCAR & &  &  &  & &  &   & OSCAR & &  &  &  & &  & \\

%          \hline
%     \end{tabularx}
%     \label{tab:label-based}
% \end{table*}
% \mb{In the Table, "Resnet" should be "ResNet", check other occurrences as well}
\paragraph{Classifier Networks} To facilitate direct comparison with existing baseline approaches, the classifier network in the main results reported is a ResNet-18. However, the synthesized data has more potential that advanced backbones can utilize. The results for NICO++ Unique and NICO++ Common datasets with different classifier networks are reported in \textit{Table} \ref{tab:backbone}. These results indicate that the generated data can potentially improve the global model's optimality with an improved model architecture and may even improve more as the number of images per category increases. In the ResNet family, ResNet-101 performs the best, while the base version of the vision transformer (ViT B-16) has the best overall performance. The number of images per category for each client in these experiments is set to 10. 







\begin{table}[!htb]
    \centering
    \caption{Accuracy (in \%) on the test set for OSCAR with different classifier networks at the server.}
    \begin{tabularx}{0.5\textwidth}{m{1.3cm}*{7}{>{\centering\arraybackslash}X}}
    \hline
         \textbf{Model} &  \multicolumn{7}{c}{\textbf{Client Test Set Accuracy}} \\
         \hline
         
         \hline
         
         & client1 &   client2 & client3 & client4  & client5 &  client6 & avg\\
         \hline
            \multicolumn{8}{c}{\textbf{NICO++ Unique}}\\
         \hline
         ResNet-18 & 65.42 & 71.14 & 74.02 & 68.52 & 71.00 & 70.69 & 70.15\\
         VGG-16 & 75.53 & 69.14 & 71.96  & 67.13 & 74.71 & 73.51 & 72.06\\
         ResNet-50 & 80.72 & 73.50 & 76.40  & 77.34 & 79.61 & 78.52 & 77.73\\
         ResNet-101 & 80.61 & 75.03 & 79.05  & 76.48 & 80.49 & 81.86 & 78.97\\
         DenseNet-121 & 80.51 & 77.10 & 76.93  & 75.94 & 79.61 & 78.73 & 78.17\\
         VIT B-16 & 84.53 & 77.86 & 83.49  & 79.91 & 81.08 & 82.48 & 81.58\\

         \hline

         \multicolumn{8}{c}{\textbf{NICO++ Common}}\\
         \hline

         ResNet-18 & 57.46 & 58.55 & 60.12 & 61.83 & 60.05 & 50.85 & 56.43\\
         VGG-16 & 60.31 & 62.45 & 51.68  & 63.11 & 62.04 & 54.39 & 58.95\\
         ResNet-50 & 62.19 & 65.70 & 54.44  & 67.07 & 67.78 & 55.19 & 61.76\\
         ResNet-101 & 64.04 & 69.82 & 58.01  & 69.01 & 69.32 & 57.46 & 64.16\\
         DenseNet-121 & 60.28 & 65.86 & 55.00  & 64.93 & 66.61 & 55.53 & 60.93\\
         VIT B-16 & 65.19 & 70.97 & 58.17  & 71.86 & 70.85 & 58.60 & 65.60\\
         
         \hline

        \hline
    \end{tabularx}
    \label{tab:backbone}
\end{table}



\paragraph{Number of generated images}
This section examines the impact of generated dataset size on the global model performance in OSCAR. Traditionally, the increase in dataset size impacts the performance positively. In this case, while in the initial increase in synthesized dataset size boosts the model performance, the performance remains constant, or in some cases decrease, after a certain threshold. This can also mean that diffusion model synthesized data can act as auxiliary for training OSFL approaches rather than a replacement. \textit{Table} \ref{tab:sample-count} shows the result for OSCAR with different number of samples synthesized per category of each client. 


\begin{table}[!htb]
    \centering
    \caption{Impact of sample count per category on OSCAR.}
    \begin{tabularx}{0.5\textwidth}{m{1cm}*{7}{>{\centering\arraybackslash}X}}
    \hline
         \textbf{Samples} &  \multicolumn{7}{c}{\textbf{Client Test Set Accuracy}} \\
         \hline
         
         \hline
         
         & client1 &   client2 & client3 & client4  & client5 &  client6 & avg\\
         \hline
            \multicolumn{8}{c}{\textbf{NICO++ Unique}}\\
         \hline
         10 & 65.42 & 71.14 & 74.02 & 68.52 & 71.00 & 70.69 & 70.15\\
         20 & 67.62 & 71.14& 74.95 & 68.94 & 74.11 & 70.90 &71.19 \\
         30 & 75.95 & 71.32 & 75.13 & 70.14 & 75.00 & 73.93 & 73.62\\
         40 & 69.26 & 72.56 &75.26 & 70.72 & 73.47 & 72.70 & 72.34 \\
         50 & 68.52 &71.44 &74.95  & 70.72 & 73.36 & 70.69 & 71.62\\

         \hline

         \multicolumn{8}{c}{\textbf{NICO++ Common}}\\
         \hline

         10 & 57.46 & 58.55 & 60.12 & 61.83 & 60.05 & 50.85 & 56.43\\
         20 & 58.03 & 57.78 & 49.52 & 62.42  & 62.13 & 52.17 & 57.06 \\
         30 & 59.11 & 59.32 & 52.96  & 64.04 & 62.18 &51.70 & 58.19\\
         40 & 58.57 & 59.10 & 61.68 & 61.90 & 61.14 & 52.17 & 57.36\\
         50 & 59.41 & 57.07& 52.08 & 62.26 & 61.42 & 53.38 & 57.93\\
         
         \hline

        \hline
    \end{tabularx}
    \label{tab:sample-count}
\end{table}




% \begin{figure}[h!]
%     \centering
%     % First row
%     \subfigure[Common NICO++ ResNet-18 Loss]{
%         \includegraphics[width=0.22\textwidth]{Figures/loss/resnet18_nico_dg_1.png}
%     }
%     \subfigure[Unique NICO++ ResNet-18 Loss]{
%         \includegraphics[width=0.22\textwidth]{Figures/loss/resnet18_nico_u_1.png}
%     } \\
%     % Second row
%     \subfigure[Common NICO++ ResNet-101 Loss]{
%         \includegraphics[width=0.22\textwidth]{Figures/loss/resnet101_nico_dg_1.png}
%     }
%     \subfigure[Unique NICO++ ResNet-101 Loss]{
%         \includegraphics[width=0.22\textwidth]{Figures/loss/resnet18_nico_u_1.png}
%     }
%     \caption{Classifier training epoch loss on synthetic data. }
%     \label{fig:loss}
% \end{figure}


% \paragraph{Synthetic Data} We visually present samples from different domains of the actual dataset and generated dataset in \textit{Table} \ref{tab:example}. The table displays real images alongside those generated by stable diffusion, using client-provided encodings for two categories—goose and helicopter—across all six domains of the Common NICO++ dataset. The generated images closely resemble the real ones within each domain, demonstrating the effectiveness and potential of the category-specific encodings.


% \begin{table*}[h!]
% \centering
% \caption{Comparison of the actual images and generated images for the category Goose}
% \begin{tabularx}{\textwidth}{p{1.35cm}p{1.35cm}|*{6}{X}}
% \hline
% \textbf{Category} & & \multicolumn{6}{c}{\textbf{Domains}} \\
%  &  & Autumn & Dim & Grass & Outdoor & Rock & Water \\ 
% \hline
% \multirow{2}{*}{Goose} & Actual & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/goose_autumn.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/goose_dim.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/goose_grass.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/goose_outdoor.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/goose_rock.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/goose_water.jpg}} \\ 
% \cline{2-8}
%  & Generated & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/goose_automn.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/goose_dim.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/goose_grass.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/goose_outdoor.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/goose_rock.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/goose_water.png}} \\ 
 
%  \hline
% % \multirow{2}{*}{Helicopter} & Actual & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/helicopter_autumn.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/helicopter_dim.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/helicopter_grass.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/helicopter_outdoor.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/helicopter_rock.jpg}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/actual_images/helicopter_water.jpg}} \\ 
% % \cline{2-8}
% %  & Generated & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/helicopter_autumn.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/helicopter_dim.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/helicopter_grass.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/helicopter_outdoor.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/helicopter_rock.png}} & \adjustbox{\includegraphics[width=1.5cm]{Figures/generated_images/helicopter_water.png}} \\ 
 
%  \hline
% \end{tabularx}

% \label{tab:example}
% \end{table*}

\paragraph{Communication Analysis} As OSCAR only uploads the data encodings, it uploads the least parameters from each client. OSCAR uploads less than 1\% of the number of parameters compared to the SOTA models. OSCAR achieves this by eliminating the classifier training, and hence classifier uploading, and each client only communicates 512 parameters for each category. \textit{Table} \ref{tab:parameters} shows the number of parameters each client uploads. FedCADO trains a classifier model; hence, each client uploads a model with 11.69 million parameters. While FedDISC reduces the communication size by more than 60\%  compared to FedCADO at each client, the upload size from each client is still 100 times higher than OSCAR. The difference becomes more prominent as the number of clients increases. 
% \mb{looks good, but can be improved the aspects such as: 1) ablation study (missing), 2) good have an algorithm even in supplementary materials, I know you're not allowed to upload the supplementary materials, but it is always possible to provide an anonymous link to the materials, which is good because reviewers can look into it for better understanding. 3) Where are non-iid scenarios? For example, feature or label skewed, 4) How about the effect of training description levels?} 
% \oz{The impact of number of images and backbone can be counted as ablation studies, although I have not written it explicitly. For label-skewed, I want to add another table, but due to space constraints, I am still thinking how to do it. And as we are not training descriptions, we don't need to look into that Our descriptions are generated by a frozen model. }


% The explanation is not provided currently, due to lack of understanding of the models. Should be provided in the next draft.
%\mb{The explanation of results lacks why OSCAR improves performance. Why are some cases of FedDISC better than OSCAR? It could be better to explain the reasons.}

\begin{table}[!h]
    \centering
    \caption{Total number of parameters (in millions) uploaded by each client.}
    % \begin{tabularx}{0.23\textwidth}{X|X}
    \begin{tabularx}{0.47\textwidth}{m{1.2cm}*{5}{>{\centering\arraybackslash}X}}
    
    % Model & Parameters\\
    % \hline
    % Local & - \\
    % FedAvg & 234 \\
    % FedProx & 234 \\
    % FedDyn & 234 \\
    % FedCADO & 11.69 \\
    % FedDISC & 4.23 \\
    % OSCAR & 0.03 \\
    \hline
    Model & Local & FedAvg & FedCADO & FedDISC & OSCAR \\
    \hline
    Parameters & - & 234 & 11.69 & 4.23 &  0.03 \\
    \hline
    \end{tabularx}
    \label{tab:parameters}
\end{table}


% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figures/uploaded_parameters.pdf}
%     \caption{Number of parameters uploaded by clients as the number of clients increases in OSCAR and other OSFL approaches}
%     \label{fig:parameters}
% \end{figure}




% \section{Discussion and Limitations}

% \paragraph{Data Privacy Consideration:}
% The main motivation for implementing federated learning (FL) is that it keeps clients' data local and hence improves data privacy. In current DM-assisted OSFL approaches, the clients share auxiliary information with the server, which can potentially harm clients' privacy. While in OSCAR, the category-specific encodings are \textit{averaged}, and do not represent individual data points, they might still carry information about client data that can harm data privacy. Differential privacy (DP) can be a possible solution for improving data privacy and reducing privacy leakage through client representations. Investigating data privacy under existing OSFL approaches is crucial, but it should not deteriorate synthetic data quality, which will harm the overall classifier accuracy. 

% \paragraph{Scaling Law:} The number of generated images per category improves the model performance ~\cite{yang2024exploring}; however, current studies fail to pinpoint the saturation point. It needs to be investigated if increasing the number of generated images can potentially improve the performance of the OSFL approaches over centralized methods. Currently, the number of generated image sizes, set to \textbf{ten} for each category at each client, is magnitudes more minor than the actual dataset. 

% \paragraph{Label Distribution Skew Consideration:} In current experiments, we only consider feature distribution skewed datasets, where each client owns a single domain but across all categories. In real-world FL scenarios, the distribution might differ, and every client might not have data from all categories (i.e., label distribution skew). Investigating OSCAR under label distribution skewed versions of current and new datasets will provide a better look into OSCAR's real-world performance. Additionally, experiments on a mixture of both label and feature distribution skewed data, where clients own a subset of categories across more than one domain, can simulate close-to-real-world scenarios in FL.



\section{Conclusion}
In this work, we propose OSCAR, a novel one-shot federated learning approach that utilizes pre-trained vision language models and classifier-free diffusion models (DMs) to train a global model in a single communication round in FL settings. OSCAR eliminates the need for training a classifier model at each client by replacing the classifier-guided DM with a classifier-free DM in the image synthesis phase. OSCAR generates category-specific data representations for each client through BLIP and CLIP foundation models, which are communicated to the server. The server generates \textit{new} data samples and trains a global model on the generated \textit{data}. OSCAR reduces the communication load by reducing the client upload size by 100X, compared to state-of-the-art DM-assisted OSFL approaches, while exhibiting superior performance on four benchmarking datasets. 

In future work, we want to extend OSCAR to fuse the knowledge of auxiliary generated datasets with existing learned knowledge at each client. Existing DM-assisted OSFL approaches neglect the local models after data synthesis at the server, as it leads to computational overhead for the clients. A fine balance between both can lead to a more efficient global model.
% In future work, we want to test OSCAR on more datasets (e.g., DomainNet) under closer to real-world FL scenarios, both label and feature distribution skewed data. Furthermore, we want to explore the data privacy aspect of OSCAR and improve the overall data privacy of the pipeline.

% \section*{Acknowledgment}


\balance
\bibliography{main}


\end{document}
