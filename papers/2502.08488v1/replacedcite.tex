\section{Prior Work}
\subsection{One-Shot Federated Learning}
% One-shot federated learning (OSFL) ____ aims to learn a global model in a single communication round between the clients and the server. 
Existing OSFL approaches can be divided into two categories based on their methodology. The first category utilizes knowledge distillation to learn a global model through either data distillation____ or model distillation____. In distilled one-shot federated learning (DOSFL) ____, the clients share distilled synthetic data with the server, which is utilized for the global model training. FedKT ____ utilizes a public auxiliary dataset and student-teacher models trained at clients to learn a global student model. The second category of methods uses auxiliary data generation at the server based on intermediary information shared by the clients. DENSE ____ trains a generator model on local classifiers, later used to generate auxiliary data for global model training. In FedCVAE ____, the server aggregates the decoder part of the conditional variational encoders (CVAE) trained at each client and generates auxiliary data for the global model. FedDiff ____ aggregates locally trained diffusion models for forming a global diffusion model for data generation. FedCADO ____ utilizes classifiers trained at each client to generate data for global model training via classifier-guided pre-trained diffusion models (DMs). FedDISC ____ utilizes data features for data generation via pre-trained DMs. 

\subsection{Federated Learning with Foundation Models}

The emergence of foundation models (FMs), both large language models (LLMs) ____ and vision language models (VLMs) ____, has impacted the landscape of machine learning. The application of these FMs, however, has been understudied in FL. Yu et al., ____ and Charles et al., ____ explore training FMs in FL setups. PromptFL ____ investigates prompt learning for FMs under data scarcity in FL settings. FedDAT ____ proposes a federated fine-tuning approach for multi-modal FMs. FedPCL ____ integrates FMs into the traditional FL process to act as class-wise prototype extractors. While FMs have the potential to mitigate data heterogeneity and communication load in FL, their full potential has not been utilized in FL settings.