%%%% ijcai25.tex

\typeout{Multimodal Learning for Low-Resource Languages: A Survey}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage[switch]{lineno}
\usepackage{pax}
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

\def\citea#1{\citeauthor{#1}~[{\citeyear{#1}}]}
\newcommand{\todo}[1]{{\color{red}TODO: #1}}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Large Multimodal Models for Low-Resource Languages: A Survey\vspace{-0.5cm}}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\author{
Ana-Cristina Rogoz$^{1,*}$\and
Marian Lupa\c{s}cu$^{1,*}$\and
Mihai Sorin Stupariu$^1$\And
Radu Tudor Ionescu$^{1,\diamond}$\\
\affiliations
$^1$Department of Computer Science, University of Bucharest, Romania\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation\\
\emails
$^{*}$Equal contribution. $^{\diamond}$\texttt{raducu.ionescu@gmail.com}}
\setcounter{secnumdepth}{5} % seting level of numbering (default for "report" is 3). With ''-1'' you have non number also for chapters

\begin{document}

\maketitle

\begin{abstract}
In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 106 studies across 75 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. We aim to provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: \url{https://github.com/marianlupascu/LMM4LRL-Survey}.
% Because at least one of the authors must be a recognized expert on the topic, submissions to the survey track are *not* anonymous and should include the names and affiliations of all authors. 
\end{abstract}

\vspace{-0.2cm}
\section{Introduction}

% \todo{
% DONE
% - introduce abbreviations at first occurrence in text, e.g., LSTM, GCN, LLM, GPT, BERT, BiLSTM, BiGRU, CNN etc. (or add table with abbreviations?)

% - add references, where possible

% DONE
% - refer to figures 1 and 3. Perhaps adding some text to explain the figures and relate to the existing text is required. If these figures are referred later (in other sections), they should be moved to the respective section.
% }

% - Background (current state), 

% - define high-resource vs.~low-resource / understudied languages

% - Motivation, challenges for low resource languages 

% - related work in introducere

% - number of papers and covered period

Recent advancements in large multimodal models (LMMs) showcased remarkable capabilities in processing and understanding diverse types of data, including text, images, audio and video. Models like GPT-4V, KOSMOS-1 \cite{NEURIPS2023_e425b75b} and PaLM-E \cite{driess2023palm} achieved impressive performance levels across various multimodal tasks through their ability to simultaneously process and reason about multiple modalities. However, these developments have primarily focused on high-resource languages, particularly English, leaving a significant gap in supporting the world's many low-resource languages.
The distinction between high-resource (HR) and low-resource (LR) languages is primarily determined by the availability of digital resources and training data. High-resource languages, such as English, Mandarin, and Spanish, benefit from extensive digital corpora, parallel texts, and annotated datasets. In contrast, low-resource or understudied languages, which constitute the majority of the world's languages, lack sufficient digital resources, standardized datasets, and computational tools. This disparity is particularly pronounced in multimodal contexts, where the scarcity of paired data across modalities (e.g.~image-text pairs, audio-text alignments) poses additional challenges.

\begin{figure}
     \centering
    \includegraphics[width=1.0\linewidth]{venn_6.pdf}
    \vspace{-0.5cm}
    \caption{A Venn diagram with the distribution of papers across different modality combinations used by LMMs for low-resource languages. Text+image is the dominant modality pair, while more complex video-inclusive combinations are less common. A selection of representative papers is included for each  modality combination. References are clickable links to papers.}
    \label{fig:venn_chart}
     \vspace{-0.4cm}
\end{figure}

The motivation for developing multimodal capabilities for LR languages is compelling. First, multimodal processing better reflects how humans naturally communicate and understand information through multiple sensory channels. Second, visual and audio cues can provide crucial contextual information that helps to overcome the limitations of scarce textual data. Third, many LR languages are primarily spoken rather than written, making multimodal approaches particularly relevant for their digital preservation and processing.
However, developing multimodal systems for LR languages faces several significant challenges, including: (1) the scarcity of high-quality multimodal datasets in these languages, (2) the lack of standardized evaluation benchmarks, (3) the computational cost of training large-scale models with limited resources, and (4) the complexity of handling different writing systems, dialects, and cultural contexts. Moreover, the problem of catastrophic forgetting when adapting pre-trained models to new languages and the challenge of maintaining performance across different modalities pose significant technical hurdles.

\begin{figure}[t!]
     \centering
    \includegraphics[width=1.0\linewidth]{pie3.pdf}
    \vspace{-0.5cm}
    \caption{Distribution of papers across 75 low-resource languages, representing 106 papers. Hindi leads with 26 studies, followed by Arabic (18), Bengali and Malayalam (16 each), and Tamil (11). 
    %Less represented languages include: Fongbe, Javanese, Manipuri, Romanian, Uyghur, and Vietnamese with 5 papers each; Kazakh, Korean, and Uzbek with 4 papers each. 
    The remaining languages have less than 5 papers each.
    % and Bulgarian, Igbo, Lao, Mongolian, Persian, Sinhala, and Turkish with 3 papers each;  Buginese, Hebrew, Indonesian, Marathi, Sanskrit, Sundanese, Swahili, Tagalog, and Ukrainian have 2 papers each. The remaining 40 languages have only 1 paper each. %, such as Armenian, Azerbaijani, Belarusian, Bemba, Bosnian, Cantonese, Cebuano, Czech, Darija, Egyptian, Egyptian Arabic, Georgian, Greek, Haitian Creole, Halh Mongolian, Irish, Kamba, Kannada, Latin Cuengh, Luo, Malagasy, Malay, Maori, Mauritanian, Mongolic, Moroccan, Norwegian, Occitan, Palestinian, Polish, SEA languages, Saudi, Serbian, Swedish, Tajik, Umbundu, Waray-Waray, Xhosa, Yemeni, and Zulu. 
    The data highlights the disparity in research focus among LR languages, with a few languages receiving more focus, while many others remain understudied in the context of multimodal learning. Some papers simultaneously address multiple languages, contributing to the individual language counts. HR languages such as English, Chinese, Mandarin and Spanish are excluded from this chart.}
    \label{fig:pie_chart}
    \vspace{-0.3cm}
\end{figure}

% \begin{figure}[!thbp]
%      \centering
%     \includegraphics[width=1.0\linewidth]{bar4.pdf}
%     \caption{Distribution of papers across different modality combinations in multimodal low-resource language research. Text+Image combinations dominate the field (67 papers, 63\%), reflecting a primary focus on visual-linguistic tasks. This is followed by Text+Audio (14 papers) and Text+Audio+Image (11 papers) combinations. More complex combinations incorporating multiple modalities, such as Text+Audio+Video, are less common but present in the literature. The data indicates that while vision-language pairs are well-studied, there remains significant room for exploration of other modality combinations, particularly those involving video and audio streams simultaneously.}
%     \label{fig:bar_chart_modalities}
% \end{figure}


In this context, we survey research articles from 2018 to the end of 2024 that specifically study LMMs for LR languages. We focus on works that go beyond simple cross-lingual transfer or translation, examining techniques that leverage multiple modalities to improve model performance. Our analysis reveals several interesting patterns in how researchers approached multimodal learning for LR languages. As shown in Figure \ref{fig:venn_chart}, text-image combinations dominate the research landscape, appearing in 67 papers (63\% of surveyed works), while more complex combinations incorporating audio and video remain less explored. In addition, the distribution of research focus across languages is notably uneven, as illustrated in Figure \ref{fig:pie_chart}, with Hindi (26 papers), Arabic (18 papers) and Bengali (16 papers) receiving significant focus, whereas 40 other languages are each represented by a single study. This disparity highlights the need for a broader coverage of understudied languages in multimodal research.

\begin{figure*}[th!]
    \centering
    \includegraphics[width=1.0\textwidth]{CompressedTaxonomy5.pdf}
    \vspace{-0.7cm}
    \caption{High-level taxonomy of LMMs for low-resource languages. We depict six main categories (inside boxes with green background), which are further divided into subcategories, exemplified via a few representative studies. References are clickable links to papers.}
    \label{fig:Taxonomy}
    \vspace{-0.3cm}
\end{figure*}

Some recent surveys have explored various aspects of multimodal language models. \citea{2023-yin} provide a comprehensive overview of LMM architectures, training strategies, and applications, while \citea{2023-Wang-survey} focus on pre-training techniques and model architectures. However, these works primarily focus on high-resource languages, and do not specifically address the unique challenges and solutions for LR languages in multimodal contexts. To the best of our knowledge, our survey is the first to address multimodal learning for understudied languages.

In summary, our contribution is fourfold:
\begin{itemize}
\item \vspace{-0.1cm} We provide the first comprehensive analysis of LMMs specifically focused on LR languages, examining 106 studies across 75 languages.
\item \vspace{-0.1cm} We develop a novel taxonomy (see Figure \ref{fig:Taxonomy}) that categorizes existing approaches into six main categories: data creation and engineering, fusion techniques, visual enhancement, cross-modal transfer, synthetic data generation, and architectural innovations.
\item \vspace{-0.1cm} We systematically organize the literature to enable a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR languages.
\item \vspace{-0.1cm} We provide an open-source repository that includes implementation details, datasets, and benchmarks to facilitate future research in this emerging field.
\end{itemize}

\vspace{-0.2cm}
\section{Taxonomy}
% \todo{discuss the taxonomy here}

In Figure \ref{fig:Taxonomy}, we systematically organize LMMs for LR languages into six main categories, reflecting both the current state of the field and the primary research strategies for addressing challenges in the context of under-represented languages. The first two categories focus on constructing high-quality resources. While the first category discusses multimodal data creation either from scratch or via extending existing datasets, the second approach centers on synthetic data generation, which automatically expands available resources via back-translation and image-based generation. Building upon this work, we present several multimodal fusion techniques and provide various strategies for effectively combining this information, ranging from early and late fusion to more complex hybrid approaches. In the fourth category, we illustrate visual enhancement techniques that harness visual information through image-guided translation and visual disambiguation methods, highlighting their importance for improving translation quality and resolving ambiguities. Expanding from the single-modality solutions, the next category focuses on cross-modal transfer learning approaches that can facilitate knowledge sharing based on both modality transfer and language transfer. Finally, our last category comprises architectural innovations specifically tailored for multimodal tasks in the context of LR languages. We structure the remainder of this article according to our novel taxonomy shown in Figure \ref{fig:Taxonomy}.

\vspace{-0.2cm}
\section{Multimodal Data Creation}

There are two main approaches to create multimodal datasets for LR languages. The first is based on multimodal dataset creation from scratch, while the second is based on using an existing resource as a starting point. We next discuss papers introducing novel datasets based on the two alternatives.

% \subsubsection{Dataset Creation from Scratch}
% 1) Multimodal Sentiment Analysis of Arabic Videos \cite{2018-najadat}\\
% 2) MemoSen: A Multimodal Dataset for Sentiment Analysiƒƒs of Memes \cite{2022-hossain} \\
% 3) MUTE: A Multimodal Dataset for Detecting Hateful Memes \cite{2022-hossain-mute} \\
% BIG-C: a Multimodal Multi-Purpose Dataset for Bemba \cite{2023-sikasote}\\
% 5) Towards Arabic Multimodal Dataset for Sentiment Analysis \cite{2023-haouhat} \\
% 6) ArabSign: A Multi-modality Dataset and Benchmark for Continuous Arabic Sign Language Recognition \cite{2022-hamzah} \\
% 7) Multimodal Sentiment Analysis for the Malay Language: New Corpus using CNN-based Framework \cite{2024-taylor} \\
% 8) RoMemes: A multimodal meme corpus for the Romanian language \cite{2024-pais} \\
% 10) Dravidianmultimodality: A dataset for multi-modal sentiment analysis in Tamil and Malayalam \cite{2021-Chakravarthi} \\ 

\noindent
\textbf{Dataset creation from scratch.}
Dataset creation from scratch has emerged as a crucial approach for enabling multimodal research in LR languages, particularly for sentiment analysis and specific language tasks. Multiple research teams have focused on creating specialized datasets through direct data collection and annotation, such as collecting Arabic videos with multimodal features for sentiment analysis \cite{2018-najadat}, building comprehensive Tamil and Malayalam video review datasets \cite{2021-Chakravarthi}, and developing new corpora for languages such as Malay \cite{2024-taylor}. A significant trend has been the creation of meme-based datasets, with efforts focused on Bengali, through MemoSen and MUTE \cite{2022-hossain,2022-hossain-mute}, and Romanian, through RoMemes \cite{2024-pais}, all incorporating multiple levels of annotation.

These dataset creation efforts have expanded beyond sentiment analysis to encompass other crucial applications, such as sign language recognition with ArabSign \cite{2022-hamzah} and multi-purpose datasets like BIG-C for Bemba \cite{2023-sikasote} and Arabic multimodal sentiment collections \cite{2023-haouhat}. These projects typically involve careful quality control by using multiple annotators, standardized recording environments, and expert validation, demonstrating a shift toward building comprehensive resources specifically designed for LR languages, rather than relying on translation or transfer from high-resource languages.

\noindent
\textbf{Dataset extension.}
In addition to building data from scratch in the context of LR language understanding, there have been several efforts for leveraging existing datasets of rich-resource languages and building upon them. \citea{2022-sen} introduced the Bengali Visual Genome (BVG) dataset, which extends the Visual Genome dataset \cite{krishna2017visual} with Bengali translations and annotations, enabling the development and evaluation of multimodal models for Bengali-English machine translation (MT) and image captioning. Similarly, \citea{2022-abdulmumin} created the Hausa Visual Genome (HaVG) dataset by translating a subset of the Visual Genome dataset into Hausa, providing a valuable resource for English-to-Hausa multimodal MT. Building upon prior work and continuing the focus on the Hausa language, \citea{2023-parida} introduced the Hausa Visual Question Answering (HaVQA) dataset, which adapts question-answer pairs from the Visual Genome dataset to the Hausa language through manual translation, creating the first visual question-answering (VQA) dataset for Hausa.


Apart from the focus on African languages, \citea{2023-saichyshyna} extended the Multi30K dataset \cite{elliott2016multi30k} to include Ukrainian translations and captions, facilitating integrated vision and language research in Ukrainian. More recently, \citea{2024-lovenia} presented SEACrowd, a comprehensive multilingual and multimodal data hub and benchmark suite for Southeast Asian languages, which covers 13 tasks across three modalities (text, image, and audio) and 38 Southeast Asian indigenous languages. These tasks include MT, image captioning, VQA, and speech recognition. To build SEACrowd, the authors leveraged already existing datasets, but they also created new data through crowd-sourcing and manual annotation.

\vspace{-0.2cm}
\section{Synthetic Data Generation}
Another approach to efficiently create multimodal datasets for under-represented languages relies on synthetic data generation. While the previous section captures efforts comprising significant human involvement, we next cover synthetic data generation, which relies on existing resources and automated techniques to generate new data.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.845\textwidth]{fig_fusion_2.pdf}
    \vspace{-0.3cm}
    \caption{An overview of various fusion strategies employed in LMMs, categorized into early fusion, late fusion, and architectural fusion approaches. Early fusion combines features from different modalities (text, audio, and visual) using feature extractors and fusion techniques, before passing them to a classifier for the final output. Concatenation fusion directly concatenates features from different modalities, while gated fusion employs a gate controller network to regulate information flow between modalities. 
    Late fusion processes each modality using separate models, then combines their predictions using decision-level fusion methods, such majority voting or weighted averaging. %Key characteristics of late fusion include its focus on combining final model outputs and emphasis on how features are integrated. 
    Architectural fusion approaches, such as attention fusion and encoder-decoder fusion, provide more sophisticated methods for multimodal integration. Attention fusion leverages self-attention layers and learned attention weights to selectively focus on relevant features across modalities.}
    \label{fig:Fusion}
    \vspace{-0.3cm}
\end{figure*}

\noindent
\textbf{Back-translation.}
A common approach for synthetic data generation relies on the usage of back-translation, which has proven to be an effective technique to enhance the data for multilingual MT (MMT) in LR language pairs. \citea{2018-dutta} demonstrated the effectiveness of this technique for training a neural MMT system in the context of LR language pairs by leveraging the Flickr30k dataset \cite{young2014image} and translating the source language (English) captions to the target LR language (Hindi). In the WMT24 English-to-Low-Resource Multi-Modal Translation task, \citea{2024-haq} showcased the effectiveness of back-translation for Hindi. Another use case of back-translation was shown by \citea{2024-Alwajih-dallah}, who, starting from English-based image-text pairs, employed translation to Arabic, as well as back-translation. This was necessary for evaluating the quality of the translation, before passing the data to humans for Arabic dialect translation and training a dialect-aware LMM, named Dallah. However, the consistency of back-translated synthetic data has been a concern. 
To address this issue, \citea{2023-wang-adapting} proposed a framework to improve the robustness of models when adapting grounded VQA models to LR languages, aiming to improve the performance without relying on machine-translated data.

\noindent
\textbf{Image-based generation.}
Another mainstream approach for synthetic data generation uses images as a starting point. In the case of Indic language multimodal MT \cite{2023-dash}, synthetic images generated by diffusion models were deemed beneficial, their main goal being that of capturing the complexity of the target domain, and augmenting the existing image dataset. Similarly, \citea{2024-haq} created exhaustive image descriptions in addition to the already existing short region-based descriptions. \citea{2024-doan} utilized image-based generation for several purposes, such as description generation and relevant information extraction, to develop Vintern-1B, an efficient LMM for Vietnamese.
Collectively, these studies demonstrate the versatility and effectiveness of synthetic data for tackling a diverse set of multimodal tasks in the context of LR languages.

% 1) BITS-P at WAT 2023: Improving Indic Language Multimodal Translation by Image Augmentation using Diffusion Models \cite{2023-dash} \\
% 2) Haq si aici -- an image caption model
% 3) Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese \cite{2024-doan} \\
% 4) ELAICHI: Enhancing LR TTS by Addressing Infrequent and Low-frequency Character Bigrams \cite{2024-anand}

\vspace{-0.2cm}
\section{Multimodal Fusion Techniques}

We identified three distinct types of fusion approaches  employed in multimodal learning, categorized into early fusion, late fusion, and architectural fusion approaches. An overview of the different fusion strategies is provided in Figure \ref{fig:Fusion}. The diagram depicts the various ways in which textual, visual and auditory features can be combined at different stages to enable effective integration of multimodal information.

%old
% 1. Enhanced Video Analytics for Sentiment Analysis (2020, Arabic): \cite{2020-azani}
% 2. Urdu sentiment analysis via multimodal data mining (2021): \cite{2021-Sehar}
% 3. Multimodal Hate Speech Detection from Bengali Memes (2022): \cite{2022-karim}
% 4. Multi-modal Sentiment Analysis of Mongolian (2024):  \cite{2024-yang}


%new
% 1. A Novel Context-Aware Multimodal Framework for Persian Sentiment Analysis: \cite{2021-dashtipour}
% 2. Findings of the Shared Task on Multimodal Abusive Language Detection and Sentiment Analysis in Tamil and Malayalam: \cite{2023-premjith}
% 3. A Novel Deep Learning Multi-Modal Sentiment Analysis Model for English and Egyptian Arabic Dialects Using Audio and Text  : \cite{2023-alalem}
% 4. English-to-LR Translation: A Multimodal Approachfor Hindi, Malayalam, Bengali, and Hausa: \cite{2024-Hatami}
% 5. Detecting Hate Speech in Amharic Using Multimodal Analysis of Social Media Memes \cite{2024-Jigar}

\noindent
\textbf{Early fusion.} Early fusion, also known as feature-level fusion, involves combining features from different modalities at the input level before passing them through a unified model. In Persian sentiment analysis, \citea{2021-dashtipour} demonstrated the effectiveness of early fusion by combining acoustic, visual, and textual features through a context-aware framework. Their approach showed that integrating features at an early stage allowed the model to capture cross-modal interactions more effectively, achieving 91.39\% accuracy compared to unimodal approaches. Similarly, the shared task on Tamil and Malayalam multimodal sentiment analysis \cite{2023-premjith} revealed that early fusion techniques were particularly effective for handling code-mixed content and cultural nuances specific to these languages.

For Amharic hate speech detection in memes \cite{2024-Jigar}, the authors employed concatenation, directly combining visual features from memes with textual features, demonstrating the effectiveness of this straightforward approach for LR languages. The integration of multimodal features through gating mechanisms has shown particular promise in LR scenarios, as demonstrated in English-to-Low-Resource translation tasks for Hindi, Malayalam, Bengali, and Hausa, where \cite{2024-Hatami} used gated fusion to selectively combine visual and textual information. This approach was further validated by \citea{2023-alalem} in their Audio-Text Fusion (ATFusion) model for English and Egyptian Arabic, where they employed Group Gated Fusion (GGF) to dynamically control the flow of information between modalities, achieving superior performance over traditional fusion methods with accuracy improvements of up to 76.21\% for English and 70.79\% for Egyptian Arabic.

% \textbf{Early fusion.}
% Early fusion in LR language sentiment analysis typically involves combining features from different modalities at the feature level, before classification. In a work on Arabic video emotion analysis \cite{2020-azani}, the authors concatenated textual features from transcripts, prosodic/spectral audio features from open-source Speech and Music Interpretation by Large-space Extraction(openSMILE) \cite{eyben2010opensmile}, and visual features from facial expressions to create unified feature vectors for sentiment prediction. Similarly, for Urdu sentiment analysis \cite{2021-Sehar}, the authors extracted bidirectional long short-term Memory(BiLSTM)-based text features, openSMILE audio features, and 3D-convolutional neural network(CNN) visual features, which were concatenated before being passed to classifiers, achieving an accuracy improvement from 84.32\% (unimodal) to 95.35\% (multimodal).

% Studies on Bengali \cite{2022-karim} and Mongolian \cite{2024-yang} further advanced early fusion techniques for LR scenarios. For Bengali hate speech detection, \citea{2022-karim} employed feature-level fusion by combining XLM-RoBERTa \cite{conneau-etal-2020-unsupervised} text embeddings and DenseNet \cite{huang2017densely} visual features through concatenation, demonstrating that joint modeling of text and image features improved hate speech detection accuracy. For Mongolian, \citea{2024-yang} introduced a more sophisticated early fusion strategy, using a multi-head attention mechanism to dynamically weight and combine features from different modalities, specifically fusing graph convolutional networks (GCN)-based text features, bidirectional gate recurrent unit (BiGRU) audio features, and high-resolution net (HRNet) visual features. This attention-based early fusion helped capture complementary information between modalities, while accounting for their relative importance in sentiment prediction.

%old
% % 1. Enhanced Video Analytics for Sentiment Analysis Based on Fusing Textual, Auditory and Visual Information (2020): \cite{2020-azani}
% % 2. Multimodal Arabic Rumors Detection (2023): \cite{2023-Albalawi}
% % 3. SADTech@DravidianLangTech: Multimodal Sentiment Analysis of Tamil and Malayalam (2023) : \cite{2023-patil} 
% % 4. Advanced Multimodal Emotion Recognition for Javanese Language Using Deep Learning (2024):  \cite{2024-arifin}

%new
% 1. A Novel Context-Aware Multimodal Framework for Persian Sentiment Analysis: \cite{2021-dashtipour}
% 2. : Multimodal Sensing for Depression Risk Detection: Integrating Audio, Video, and Text Data\cite{zhang-2024}
% 3. : Advanced Multimodal Emotion Recognition for Javanese Language Using Deep Learning\cite{2024-arifin}
% 4. : Multimodal Abusive Language Detection in Tamil based on Integrated Approach of Machine Learning and Deep Learning Techniques \cite{2024-rahman}
% 5. : Multimodal Hate Speech Detection from Bengali Memes and Texts\cite{2022-karim}

\noindent
\textbf{Late fusion.}
Late fusion, also known as decision-level fusion, combines predictions from separate modality-specific models at the decision stage rather than fusing features early in the pipeline. In this approach, individual models are trained independently for each modality (e.g., text, audio, and video) and their outputs are merged to produce the final prediction \cite{zhang-2024,2024-arifin}. This allows each modality to be processed optimally according to its characteristics before integration. The independence between modality-specific models also provides flexibility, as individual components can be modified without affecting the others, and the system can still function if one modality is missing.

Two popular late fusion strategies are weighted averaging and majority voting. In weighted averaging fusion, the predictions from different modalities are combined using weighted sums, with weights determining the contribution of each modality to the final decision \cite{2024-rahman,2022-karim}. The weights can be uniform or learned to optimize performance. Majority voting fusion employs gating mechanisms to control information flow between modalities and determine which modality should be emphasized. For example, \citea{2021-dashtipour} used gating networks to adaptively combine predictions from audio, visual and textual models based on their estimated reliability for Persian sentiment analysis. Their results showed that intelligent fusion using gates improved performance compared to simple averaging, highlighting the benefits of selective information integration in multimodal systems.

% Late fusion, also known as decision-level fusion, combines predictions from different modalities after individual classifiers make their decisions. In low-resource language scenarios, this approach allows each modality to be processed independently with specialized models before integration. For example, \citea{2020-azani} employed late fusion by training separate models for audio, visual and text modalities for Arabic sentiment analysis, then combining their predictions through voting mechanisms. Similarly, \citea{2023-albalawi} developed a late fusion approach for Arabic rumor detection where decisions from text and image classifiers were concatenated before a final sigmoid layer made the ultimate prediction.

% The late fusion approaches in these works demonstrated varying levels of effectiveness. For Tamil and Malayalam sentiment analysis, \citea{2023-patil} found that late fusion of audio and text modalities through averaging classifier outputs achieved competitive performance. Similarly, \citea{2024-arifin} successfully applied late fusion for Javanese emotion recognition by independently processing video and audio features before combining their recognition outcomes. More recently, \citea{2024-rahman} achieved state-of-the-art results for Tamil abusive language detection using weighted late fusion that combined predictions from ConvLSTM (video), BiLSTM (audio), and MNB (text) models with uniform weights. A common theme across these papers is that late fusion allows the model for each modality to be optimized independently, which can be especially beneficial in low-resource scenarios where one modality may have significantly more or higher quality training data than others. However, this approach may miss potential interactions between modalities during the feature learning stage.

%old
% % 1. An Efficient Fusion Mechanism for Multimodal Low-resource Setting (2022): \cite{2022-Chauhan}
% % 2. English-to-low-resource translation: A multimodal approach for hindi, malayalam, bengali, and hausa (2024): \cite{2024-Hatami}

%new
% 1. Multilingual Multimodal Machine Translation for Dravidian Languages using Phonetic Transcription: \cite{2019-chakravarthi}
% 2. Urdu Sentiment Analysis via Multimodal Data Mining Based on Deep Learning Algorithms: \cite{2021-sehar}
% 3. Exploiting multiple correlated modalities can enhance low-resource machine translation quality: \cite{2023-meetei}
% 4. Multi-modal Deep Learning Approach to Improve Sentence level Sinhala Sign Language Recognition: \cite{haputhanthri-2023}
% 5. Multi-modal Sentiment Analysis of Mongolian Language based on Pre-trained Models and High-resolution Networks: \cite{2024-yang}
% 6. Multimodal Sensing for Depression Risk Detection: Integrating Audio, Video, and Text Data: \cite{zhang-2024}


\noindent
\textbf{Architectural fusion.} 
Encoder-decoder fusion architectures have emerged as an effective approach for multimodal integration across different languages and tasks. Several studies have demonstrated success with this approach. For example, \citea{2019-chakravarthi} employed an encoder-decoder framework with phonetic transcription to improve machine translation between Dravidian languages, while \citea{2021-sehar} utilized an encoder-decoder architecture to fuse audio, video and text features for Urdu sentiment analysis. Similarly, \citea{2023-meetei} showed that encoder-decoder fusion of correlated modalities can enhance translation quality for LR languages. The key advantage of encoder-decoder architectures is their ability to first encode input features from different modalities into a shared representation space before decoding them into the target output.

Attention-based fusion has also proven to be highly effective for multimodal integration \cite{ristea2023cascaded}. As shown by \citea{haputhanthri-2023} for Sinhala sign language recognition, attention mechanisms allow the model to dynamically focus on the most relevant features across modalities. \citea{2024-yang} successfully employed attention fusion for Mongolian sentiment analysis by combining features from audio, text and visual inputs. \citea{zhang-2024} demonstrated that attention-based fusion of multimodal data improves depression risk detection by allowing the model to attend to salient information across audio, video and text modalities. The ability of attention mechanisms to learn dynamic weights between modalities makes them particularly suitable for tasks requiring adaptive integration of complementary information sources.


% Hybrid fusion techniques aim to combine the benefits of both early and late fusion for multimodal learning in low-resource settings. \citea{2022-chauhan} proposed a multi-representative fusion mechanism that generates diverse representations for each modality using convolution filters. Their method combines modality representations in a pairwise fashion to obtain multiple fusions. An attention mechanism is applied to select the most informative fusions, while ignoring noisy ones. \citea{2024-hatami} employed a gated fusion approach to integrate textual and visual information in the encoder for low-resource MT. CNN features are extracted from images and combined with textual representations via a gating matrix that controls the amount of visual information to be used, based on its relevance. In general, we observe that hybrid strategies enable dynamic integration of complementary cross-modal information.


\noindent
\textbf{Comparative analysis of fusion techniques.}
Each fusion approach presents distinct advantages and challenges in the context of LR languages. Early fusion enables deep interaction between modalities from the start, but can be computationally expensive and may suffer when one modality is noisy. Late fusion offers flexibility and robustness when modalities are missing, but may miss important cross-modal interactions. Architectural fusion approaches show promise in capturing complex relationships between modalities, but require careful tuning and substantial computational resources. A notable innovation in this space is the Multi-Representative Fusion (MRF) mechanism \cite{2022-chauhan}, which generates diverse representations for each modality and selectively chooses the best fusion via attention. This approach has shown particular promise in handling noisy inputs, achieving state-of-the-art performance on several LR sentiment analysis benchmarks.

% \todo{

% - advantages / disadvantages that we can point out?
% - write about An Efficient Fusion Mechanism for Multimodal
% Low-resource Setting  (a novel hybrid fusion technique called "Multi-Representative Fusion")
% - recommendations that we can make?
% }

\vspace{-0.2cm}
\section{Visual Enhancement Techniques}

Visual enhancement techniques aim to improve MT quality by leveraging visual information to provide additional context and resolve ambiguities in the source text. These techniques broadly fall into two main categories: image-guided translation, which uses visual features to enhance the overall translation process, and visual disambiguation, which specifically focuses on resolving ambiguous words or phrases using visual context. 
% \todo{some brief intro of the section} -- done 

\noindent\textbf{Image-guided translation.}
A promising direction for improving translation quality for LR languages is the use of image-guided translation approaches. \citea{2018-dutta} showed that augmenting neural MT systems with visual features extracted from a pre-trained CNN and integrated into an encoder-decoder architecture can improve translation quality, achieving a bilingual evaluation understudy (BLEU) score of 24.2 for Hindi to English translation. Building upon these ideas, \citea{2020-laskar} developed a multimodal neural MT system with a bidirectional RNN encoder and doubly-attentive decoder for English-Hindi translation. Their system, which combines visual and textual features and employs pre-trained word embeddings from monolingual data, outperforms a text-only baseline, achieving a BLEU score of 33.57 versus 27.75 on the test set.

Subsequent studies \cite{2022-shi,2023-meetei-cues,2023-meetei-hindi,2024-haq} have demonstrated the effective use of visual information for improving MT in LR settings, particularly for the English-Hindi language pair. \citea{2023-meetei-cues} proposed a video-guided multimodal MT framework that incorporates spatio-temporal video features, showing improvements of up to +4.2 BLEU over text-only baselines for English to Hindi translation, while \citea{2023-meetei-hindi} explored multimodal translation for news domain data, showing that ResNet-based image features outperform VGG-based features and improve BLEU scores by +1.8 points. Additionally, \citea{2022-shi} explored different approaches for extracting and integrating image features using VGG and ResNet models, achieving a +3 BLEU improvement over text-only translation. 
More recently, \citea{2024-haq} presented a context-aware transformer model that integrates visual features via BERT encoding, demonstrating consistent improvements over text-only baselines. % in both evaluation and challenge sets for the WMT-WAT 2024 shared task. 
Across all studies, qualitative analyses confirmed that visual cues are particularly beneficial for handling rare words and domain-specific terms, with both image and video modalities helping to resolve ambiguity and improve translation quality in LR scenarios.

\noindent
\textbf{Visual disambiguation.}
While image-guided translation aims to enhance overall translation quality by integrating visual context, the visual disambiguation techniques focus on task-specific ambiguities by grounding them in visual information. In this regard, studies revolving around the creation of Visual Genome datasets for LR languages, such as Hindi \cite{2019-parida}, Bengali \cite{2022-sen} and Hausa \cite{2022-abdulmumin}, have played a pivotal role in advancing visual disambiguation techniques. 
% This paragraph is not important / does not say much: While the HVG dataset consists of 31,525 English-Hindi sentence pairs aligned with corresponding images, the BVG dataset comprises 32,923 Bengali-English sentence pairs with associated images. Similarly, HaVG proposes a new dataset comprised 32,923 samples as well, each containing the description of an image or a section within the image in Hausa and its equivalent in English. %ch

Another work highlighting the benefits of using visual features for disambiguation is presented by \citea{2021-jain}. Their model, called MURAL, shows strong performance on text-to-image retrieval tasks, where it manages to retrieve relevant images for ambiguous queries. This finding is also supported by the qualitative examples, where MURAL successfully disambiguates word senses based on visual context. In addition, \citea{2024-kovath} proposed a co-attention mechanism for Malayalam VQA that allows the model to jointly learn attention over both textual and visual inputs, demonstrating improved performance over baseline approaches that use only textual features.

\noindent
\textbf{Comparative analysis of visual enhancement techniques.}
Image-guided translation consistently demonstrates performance improvements over text-only baselines for LR languages, though effectiveness varies with language pair, dataset size, and translation direction. These approaches excel at handling semantic ambiguities and culturally-specific concepts, but their success depends heavily on the quality of extracted visual features. A key limitation is the reliance on high-quality image-text pairs, which are often scarce for LR languages. While these techniques improve translation quality, they also introduce computational overheads. Future work should focus on developing more efficient visual feature extraction methods and better approaches for leveraging visual information with limited paired data.
% # DONE 
% \todo{ 

% - advantages / disadvantages that we can point out?

% - recommendations that we can make?
% }

\vspace{-0.2cm}
\section{Cross-Modal Transfer Learning}
% 1. A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation (2022): \cite{2023-chen}
% 2) A multilingual training strategy for low resource Text to Speech (2024): \cite{2024-amalas} \\
% 3) Visual Speech Recognition for Languages with Limited Labeled Data using Automatic Labels from Whisper \cite{2024-yeo} \\
% 4) Qalam: A Multimodal LLM for Arabic Optical Character and Handwriting Recognition \cite{bhatia2024qalammultimodalllm} \\ 
% 5) LaVy: Vietnamese Multimodal Large Language Model \cite{2024-tran-lavy}
\textbf{Modality transfer.}
Modality transfer addresses the challenge of transferring knowledge between different modalities to improve performance on LR tasks. A diversity of approaches have been used to achieve modality transfer. \citea{2023-chen} proposed a progressive transfer learning strategy that leverages both general pre-training (Kinetics-400 for visual and CC25 for language) and domain-specific pre-training (sign-to-gloss translation) to bridge modalities for sign language translation. \citea{2024-amalas} introduced a data-driven approach for selecting source languages and demonstrated that multilingual pre-training outperforms monolingual pre-training for text-to-speech systems. \citea{2024-yeo} tackled LR visual speech recognition by using Whisper's automatic transcriptions to generate training labels from unlabeled multilingual audio-visual data. For Arabic handwriting recognition, \citea{bhatia2024qalammultimodalllm} employed modality transfer through an architecture combining SwinV2 for visual encoding and RoBERTa for text decoding, while \citea{2024-tran-lavy} demonstrated successful modality transfer for Vietnamese through extensive pre-training of both vision and language components, combined with automated data curation methods. Notably, \citea{2024-onuoha} challenged the assumptions about multimodal integration through their study of Igbo minimal pairs. Their findings show that native Igbo speakers can accurately distinguish minimal pairs through audio alone, suggesting that the benefits of cross-modal integration may be more relevant for non-native speakers than fluent ones.

% 1. Adapting Grounded Visual Question Answering Models to Low Resource Languages (2023): \cite{2023-wang} \\
% 2. Lip Reading for Low-resource Languages (2023): \cite{2023-kim} \\
% 3. CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages \cite{2023-santos} \\
% 4. Visually Grounded Few-Shot Word Learning (2024): \cite{2024-nortje}  \\
\noindent\textbf{Language transfer.}
Language transfer is an approach for leveraging knowledge from resource-rich languages to improve model performance on LR languages. Recent work demonstrates several effective strategies. For example, \citea{2023-wang-adapting} adapted MDETR to new languages by using adapters and code-switching without relying on MT data. \citea{2023-kim} focused on learning general speech knowledge from English for lip reading, and combining it with language-specific audio features. \citea{2023-santos} proposed to use data augmentation and contrastive learning to improve multilingual CLIP models for LR languages.  \citea{2024-nortje} showed that initializing a Yorùbá few-shot word learning model with weights from an English speech-image model substantially improves performance. These approaches share the common theme of transferring learned representations and knowledge from high-resource languages (typically English), while developing techniques to efficiently adapt and fine-tune models for target LR languages.

\vspace{-0.2cm}
\section{Architectural Innovations}
% 1. LowCLIP: Adapting the CLIP Model Architecture for Low-Resource Languages in Multimodal Image Retrieval Task \cite{2023-Asgarov}\\
% 2. XtremeCLIP: Extremely Parameter-efficient Tuning for Low-resource Vision Language Understanding \cite{2023-tang-xtremeclip}\\
% 3. Improving Captioning for Low-Resource Languages by Cycle Consistency \cite{2019-wu} \\ 
% 4. A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models 
Some recent architectural innovations in the context of LR languages have focused on adapting the CLIP architecture, originally introduced by \citea{radford2021learning}. One such example is the LowCLIP \cite{2023-Asgarov} model, which replaces the original text encoder trained primarily on English text with a multilingual BERT (mBERT). The authors evaluated various lightweight image encoders, such as EfficientNet-B0 and Tiny Swin Transformer, for a more computationally efficient approach, while also targeting LR languages like Azerbaijani. To compensate for the lighter architecture and the scarcity of image-text pairs in Azerbaijani, LowCLIP leverages synthetic data generation via MT for text features, and image augmentation techniques, such as crop and rotation, for image features. In contrast, XtremeCLIP \cite{2023-tang-xtremeclip} took a different approach, where the authors introduced a parameter-efficient method that only tunes a small prototype matrix, while keeping the visual and text encoders frozen. Their model also leverages contrastive learning to provide additional supervised signals in LR settings. Collectively, these efforts extend the applicability of CLIP to multimodal image retrieval tasks.

% by focusing on architectural modifications and training strategies in the context of LR languages. 

Another approach for multimodality in the context of LR languages is introduced by \citea{2019-wu}. The approach combines two existing methods, a translation-based one and an alignment-based one, into a unified architecture for improving image captioning. The framework employs a model that first generates high-quality English captions, which are then used together with the images to produce captions in the LR language. The model achieves a fine-grained alignment between visual elements and captions in both languages via a cycle-consistency constraint, outperforming existing methods on standard metrics. 

% \todo{paragraph on FEWVLM is too long, should be shorter}

More recently, \citea{jin2022good} introduced FEWVLM, showing that careful prompt engineering and efficient architectural design can achieve strong performance in the context of LMM usage with either little data or computational needs. They managed to develop a moderate-size vision-language model that combines the sequence-to-sequence transformer architecture with prefix language modeling and masked language modeling,
introducing effective prompt engineering approaches for visual-language tasks in the LR setting. Notably, FEWVLM outperforms Frozen (a model 31$\times$ larger) and achieves comparable results to PICa (246$\times$ larger), demonstrating that an efficient architecture can compensate for model size. 

Together, the architectural innovations introduced by LowCLIP, XtremeCLIP, FEWVLM and similar frameworks demonstrate various viable alternatives for making multimodal models more accessible for LR languages.

% From LowCLIP's lightweight architectures and multilingual encoders, to XtremeCLIP's parameter-efficient tuning, to cycle consistency's clever use of high-resource languages, to FEWVLM's effective prompt engineering.

\vspace{-0.2cm}
\section{Conclusion and Future Work}

\noindent
\textbf{Conclusion.}
Our survey has provided a comprehensive analysis of LMM-based approaches for LR languages, comprising 106 studies across 75 languages. We noted that vision-language combinations dominate the current research landscape (63\% of surveyed works), but there is also an increasing trend toward incorporating video and speech in recent works.
% , particularly in sentiment analysis and emotion recognition tasks. 
Additionally, we observed a particular concentration of research in South Asian languages (including Hindi, Bengali, Malayalam) and Southeast Asian languages (including Vietnamese, Javanese, Malay), followed by contributions in Middle Eastern languages (Persian, Arabic) and African languages (Hausa, Amharic), leaving 40 other languages each represented by a single study. 
However, the landscape of LMMs for LR languages has shown remarkable progress across multiple dimensions, from data creation and engineering to different fusion techniques and architectural innovations. The emergence of projects like HVG, SEACrowd, and BVG highlights the growing attention to creating high-quality multimodal resources for traditionally understudied languages, from simple parallel corpus creation to synthetic data generation. Both fusion techniques and visual enhancement approaches have matured significantly, with advances ranging from early fusion architectures to advanced visual disambiguation methods. Recent successes with models such as Qalam, LaVy, and Amharic LLaVA demonstrate that carefully designed multimodal strategies can effectively leverage limited resources, while adapting large-scale architectures for LR contexts.

\noindent
\textbf{Future work.}
We identify several promising directions for future research in the area of LMMs for LR languages. While current work has established a foundation in vision-language applications, there is a clear need to explore additional modality combinations, particularly incorporating audio and video modalities for LR languages, which could enable more robust and realistic language learning. 
Additionally, the development of comprehensive multimodal datasets remains a challenge, particularly for languages outside major language families. To address this limitation, research should focus on two areas: advancing synthetic data generation techniques, as demonstrated by recent works such as HVG, ELAICHI and Vintern-1B, and improving cross-lingual transfer methodologies, building upon frameworks such as XtremeCLIP and LowCLIP. These approaches show promise in overcoming the persistent challenge of limited training data availability for LR languages.

%\todo{Add late fusion / stacking as future work}
We also identify several advanced methods that have not been extensively explored in the context of multimodal learning for LR languages. These include: late fusion based on stacking, which learns to optimally combine outputs of individual models; tensor fusion, which captures complex cross-modal interactions through outer product and decomposition operations; kernel-based fusion, which maps data to high-dimensional spaces for non-linear integration of heterogeneous modalities; and graphical fusion, which leverages graph-based representations and neural architectures for multimodal data. Investigating the adaptation of these techniques to LR settings, while addressing challenges of data scarcity and computational efficiency, presents promising avenues for future research. 
We further recommend several strategies for improving fusion techniques in LR settings. First, researchers should consider adaptive fusion mechanisms that can dynamically adjust the contribution of each modality based on input quality and task requirements. Second, more efficient architectural designs are needed to make sophisticated fusion techniques accessible in resource-constrained environments. Third, the development of robust evaluation metrics specifically for multimodal fusion in LR contexts would help better understand the effectiveness of different approaches. Finally, taking inspiration from the success of MRF \cite{2022-chauhan}, future work should explore more hybrid approaches that combine the strengths of different fusion strategies, while maintaining computational efficiency. These recommendations aim to address the unique challenges of multimodal fusion in LR languages, while maximizing the benefits of complementary information from different modalities.

% \todo{}

% Retete din approachurile existente
% Idei noi care nu au fost abordate pana acum
% Concluzii: ce merge si ce nu merge din ce zicem in survey

% Noi lacune in literatura si sublinierea a ceea ce lipseste din literatura. De dat cateva directii cu destul detalii (cam 3 directii, un paragraf fiecare directie)


% citea -- citare subiect/noun (nu poate lipsi din text), cite altfel
% timpul trecut 

% \appendix

% \section*{Ethical Statement}
% There are no ethical issues.

% \section*{Acknowledgments}

\vspace{-0.2cm}
%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

