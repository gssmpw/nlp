\section{Related Works}
% knowledge distillation ____
% 
There has been extensive research into what types of data yield the best training outcomes for large language models (LLMs). Previous studies have identified several factors that positively influence model training, such as adding complexity ____, adding details ____, adding diversity ____, augmenting ground-truth answers in a step-by-step manner ____, and ensuring correctness ____. However, in practice, these metrics are challenging to measure for a given dataset, making it difficult to determine the quality of training data based on these criteria. ____ found that familiarity, measured by perplexity, significantly impacts model training. They argue that language models are more familiar with content generated by other language models, which explains why synthetic data is often more effective for training compared to human-generated data for educating LLMs. 

Perplexity has been widely used for different purpose in prior research. Perplexity has been used to select prompts ____, showing that prompts with lower perplexity generally lead to better performance in question-answering tasks. It has also been used for selecting pretraining datasets ____, detecting AI-generated content ____, and selecting instruction-tuning data from a database ____. ____ modify the perplexity score and propose “IDF” (Instruction Following Difficulty), which is used to select a small pool of challenging data from the original dataset for efficient training. Researchers hypothesized that higher perplexity reflects more challenging data, which can be beneficial for teaching LLMs new knowledge. Unlike these studies, our focus is on finding the best strategy to generate the target responses (y) for a given x.  

Two recent works have explored optimal strategies for generating responses, but their approaches diverge from ours. In ____, the authors introduce the Compatibility-Adjusted Reward (CAR) to rank training datasets; however, they limit their experiments to two instruction-following datasets and do not consider multiple meta-prompts. Similarly, ____ identifies the key factors influencing training outcomes and employs principal component analysis (PCA) to fit a combination of these factors in order to predict the training performance for the same datasets. In contrast, our work presents several key distinctions. First, unlike the previous work, we evaluate only a small subset of generated data rather than complete datasets, enabling more efficient analysis. Second, while previous studies rely on composite metrics that require hyperparameter tuning—potentially risking overfitting on limited data diversity—we employ a single, robust metric validated across diverse tasks. Third, instead of ranking teacher models using a single meta-prompt, we rank models paired with diverse meta-prompts. Lastly, rather than solely demonstrating ranking effectiveness, our method also enhances overall accuracy.