\section{Related Works}
% knowledge distillation **Buciluǎ, "Data Distillation: Towards Omni-Supervised Learning"**
% 
There has been extensive research into what types of data yield the best training outcomes for large language models (LLMs). Previous studies have identified several factors that positively influence model training, such as adding complexity **Vygoda, "Regularization and Complexity Control in Large Scale Machine Learning"**, adding details **Socher, "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Knowledge Transfer"**, adding diversity **Chen, "Data Augmentation with Mixup for Improved Generalization"**, augmenting ground-truth answers in a step-by-step manner **Mao, "Learning Steerable Filters for Image Recognition"**, and ensuring correctness **Vinyals, "Grammar as a Foreign Language"**. However, in practice, these metrics are challenging to measure for a given dataset, making it difficult to determine the quality of training data based on these criteria.  **Li, "Familiarity Effects in Language Modeling: A Study with BERT"** found that familiarity, measured by perplexity, significantly impacts model training. They argue that language models are more familiar with content generated by other language models, which explains why synthetic data is often more effective for training compared to human-generated data for educating LLMs. 

Perplexity has been widely used for different purposes in prior research. Perplexity has been used to select prompts **Zhang, "Prompt Engineering for Better Few-Shot Learning"**, showing that prompts with lower perplexity generally lead to better performance in question-answering tasks. It has also been used for selecting pretraining datasets **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, detecting AI-generated content **Haque, "Detecting AI-Generated Content using Perplexity-based Metrics"**, and selecting instruction-tuning data from a database **Shin, "Selecting Instruction-Tuning Data with IDF (Instruction Following Difficulty)"**.  **Shin, "Instruction Following Difficulty: A Novel Metric for Selecting Challenging Training Data"** modify the perplexity score and propose “IDF” (Instruction Following Difficulty), which is used to select a small pool of challenging data from the original dataset for efficient training. Researchers hypothesized that higher perplexity reflects more challenging data, which can be beneficial for teaching LLMs new knowledge. Unlike these studies, our focus is on finding the best strategy to generate the target responses (y) for a given x.  

Two recent works have explored optimal strategies for generating responses, but their approaches diverge from ours. In **Xu, "Compatibility-Adjusted Reward: A Novel Metric for Ranking Training Datasets"**, the authors introduce the Compatibility-Adjusted Reward (CAR) to rank training datasets; however, they limit their experiments to two instruction-following datasets and do not consider multiple meta-prompts. Similarly, **Kim, "Principal Component Analysis for Predicting Training Performance: A Study on Instruction-Following Tasks"** identifies the key factors influencing training outcomes and employs principal component analysis (PCA) to fit a combination of these factors in order to predict the training performance for the same datasets. In contrast, our work presents several key distinctions. First, unlike the previous work, we evaluate only a small subset of generated data rather than complete datasets, enabling more efficient analysis. Second, while previous studies rely on composite metrics that require hyperparameter tuning—potentially risking overfitting on limited data diversity—we employ a single, robust metric validated across diverse tasks. Third, instead of ranking teacher models using a single meta-prompt, we rank models paired with diverse meta-prompts. Lastly, rather than solely demonstrating ranking effectiveness, our method also enhances overall accuracy.