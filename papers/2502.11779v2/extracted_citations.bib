@article{de2022bertin,
  title={Bertin: Efficient pre-training of a spanish language model using perplexity sampling},
  author={De la Rosa, Javier and Ponferrada, Eduardo G and Villegas, Paulo and Salas, Pablo Gonzalez de Prado and Romero, Manu and Grandury, Mar{\i}a},
  journal={arXiv preprint arXiv:2207.06814},
  year={2022}
}

@inproceedings{fu2023specializing,
  title={Specializing smaller language models towards multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar},
  booktitle={International Conference on Machine Learning},
  pages={10421--10430},
  year={2023},
  organization={PMLR}
}

@article{gonen2022demystifying,
  title={Demystifying prompts in language models via perplexity estimation},
  author={Gonen, Hila and Iyer, Srini and Blevins, Terra and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2212.04037},
  year={2022}
}

@misc{hinton2015distillingknowledgeneuralnetwork,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1503.02531}, 
}

@article{ho2022large,
  title={Large language models are reasoning teachers},
  author={Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  journal={arXiv preprint arXiv:2212.10071},
  year={2022}
}

@misc{hsieh2023distilling,
      title={Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes}, 
      author={Cheng-Yu Hsieh and Chun-Liang Li and Chih-Kuan Yeh and Hootan Nakhost and Yasuhisa Fujii and Alexander Ratner and Ranjay Krishna and Chen-Yu Lee and Tomas Pfister},
      year={2023},
      eprint={2305.02301},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hu2020systematic,
  title={A systematic assessment of syntactic generalization in neural language models},
  author={Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P},
  journal={arXiv preprint arXiv:2005.03692},
  year={2020}
}

@misc{kang2023knowledgeaugmented,
      title={Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks}, 
      author={Minki Kang and Seanie Lee and Jinheon Baek and Kenji Kawaguchi and Sung Ju Hwang},
      year={2023},
      eprint={2305.18395},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kim2024evaluatinglanguagemodelssynthetic,
      title={Evaluating Language Models as Synthetic Data Generators}, 
      author={Seungone Kim and Juyoung Suk and Xiang Yue and Vijay Viswanathan and Seongyun Lee and Yizhong Wang and Kiril Gashteovski and Carolin Lawrence and Sean Welleck and Graham Neubig},
      year={2024},
      eprint={2412.03679},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.03679}, 
}

@inproceedings{li-etal-2024-quantity,
    title = "From Quantity to Quality: Boosting {LLM} Performance with Self-Guided Data Selection for Instruction Tuning",
    author = "Li, Ming  and
      Zhang, Yong  and
      Li, Zhitao  and
      Chen, Jiuhai  and
      Chen, Lichang  and
      Cheng, Ning  and
      Wang, Jianzong  and
      Zhou, Tianyi  and
      Xiao, Jing",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.421/",
    doi = "10.18653/v1/2024.naacl-long.421",
    pages = "7602--7635",
    abstract = "In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model`s expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere 10{\%} of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available."
}

@misc{li2022explanations,
      title={Explanations from Large Language Models Make Small Reasoners Better}, 
      author={Shiyang Li and Jianshu Chen and Yelong Shen and Zhiyu Chen and Xinlu Zhang and Zekun Li and Hong Wang and Jing Qian and Baolin Peng and Yi Mao and Wenhu Chen and Xifeng Yan},
      year={2022},
      eprint={2210.06726},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{luo2023wizardcoderempoweringcodelarge,
      title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct}, 
      author={Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},
      year={2023},
      eprint={2306.08568},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.08568}, 
}

@inproceedings{magister-etal-2023-teaching,
    title = "Teaching Small Language Models to Reason",
    author = "Magister, Lucie Charlotte  and
      Mallinson, Jonathan  and
      Adamek, Jakub  and
      Malmi, Eric  and
      Severyn, Aliaksei",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.151",
    doi = "10.18653/v1/2023.acl-short.151",
    pages = "1773--1781",
    abstract = "Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11{\%} to 21.99{\%} and 18.42{\%} when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.",
}

@article{mekala2024smaller,
  title={Smaller language models are capable of selecting instruction-tuning training data for larger language models},
  author={Mekala, Dheeraj and Nguyen, Alex and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.10430},
  year={2024}
}

@inproceedings{ranaldi-freitas-2024-aligning,
    title = "Aligning Large and Small Language Models via Chain-of-Thought Reasoning",
    author = "Ranaldi, Leonardo  and
      Freitas, Andre",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.109",
    pages = "1812--1827",
    abstract = "Chain-of-Thought (CoT) prompting empowersthe reasoning abilities of Large Language Models (LLMs), eliciting them to solve complexreasoning tasks in a step-wise manner. However, these capabilities appear only in models with billions of parameters, which represent an entry barrier for many users who are constrained to operate on a smaller model scale, i.e., Small Language Models (SLMs). Although many companies are releasing LLMs of the same family with fewer parameters, these models tend not to preserve all the reasoning capabilities of the original models, including CoT reasoning.In this paper, we propose a method for aligning and transferring reasoning abilities between larger to smaller Language Models. By using an Instruction-tuning-CoT method, that is, an Instruction-tuning designed around CoT-Demonstrations, we enable the SLMs to generate multi-step controlled reasoned answers when they are elicited with the CoT mechanism. Hence, we instruct a smaller Language Model using outputs generated by more robust models belonging to the same family or not, evaluating the impact across different types of models. Results obtained on question-answering and mathematical reasoning benchmarks show that LMs instructed via the Instruction-tuning CoT method produced by LLMs outperform baselines within both in-domain and out-domain scenarios.",
}

@inproceedings{ren2024learn,
    title = "{I} Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with {LLM}-Generated Responses",
    author = "Ren, Xuan  and
      Wu, Biao  and
      Liu, Lingqiao",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.571/",
    doi = "10.18653/v1/2024.emnlp-main.571",
    pages = "10225--10245",
    abstract = "This paper explores an intriguing observation: fine-tuning a large language model (LLM) with responses generated by a LLM often yields better results than using responses generated by humans, particularly in reasoning tasks. We conduct an in-depth investigation to understand why this occurs. Contrary to the common belief that these instances is due to the more detailed nature of LLM-generated content, our study identifies another contributing factor: an LLM is inherently more {\textquotedblleft}familiar{\textquotedblright} with LLM generated responses. This familiarity is evidenced by lower perplexity before fine-tuning. We design a series of experiments to understand the impact of the {\textquotedblleft}familiarity{\textquotedblright} and our conclusion reveals that this {\textquotedblleft}familiarity{\textquotedblright} significantly impacts learning performance. Training with LLM-generated responses not only enhances performance but also helps maintain the model`s capabilities in other reasoning tasks after fine-tuning on a specific task."
}

@article{trinh2024solving,
  title={Solving olympiad geometry without human demonstrations},
  author={Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
  journal={Nature},
  volume={625},
  number={7995},
  pages={476--482},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{xu2024detecting,
  title={Detecting AI-Generated Code Assignments Using Perplexity of Large Language Models},
  author={Xu, Zhenyu and Sheng, Victor S},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={21},
  pages={23155--23162},
  year={2024}
}

@misc{xu2024strongermodelsstrongerteachers,
      title={Stronger Models are NOT Stronger Teachers for Instruction Tuning}, 
      author={Zhangchen Xu and Fengqing Jiang and Luyao Niu and Bill Yuchen Lin and Radha Poovendran},
      year={2024},
      eprint={2411.07133},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.07133}, 
}

@inproceedings{zhang2024distillation,
  title={Distillation with Explanations from Large Language Models},
  author={Zhang, Hanyu and Wang, Xiting and Ao, Xiang and He, Qing},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={5018--5028},
  year={2024}
}

