

\documentclass{article} % For LaTeX2e
% \usepackage[preprint]{colm2025_conference}
\usepackage[preprint]{neurips_2025}
\usepackage{colortbl}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{tablefootnote}
\usepackage{listings}

\usepackage{tikz} 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}

\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}}
\def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}}
\def\cf{\emph{c.f.}} \def\Cf{\emph{C.f.}}
\def\etc{\emph{etc}} \def\vs{\emph{vs.}}
\def\etal{\emph{et al}}
\def\wrt{\emph{w.r.t.}} \def\dof{d.o.f}

\usepackage{lineno}


\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}


\def\qi{\textcolor{blue}}
\def\lingqiao{\textcolor{red}}

\title{Efficient Response Generation Strategy Selection for Fine-Tuning Large Language Models Through Self-Aligned Perplexity}


% \title{Efficient Strategy Selection for Response Generation in Fine-Tuning Large Language Models Through Self-Aligned Perplexity}



% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% \final
% Non-anonymous submissions will be rejected without review.
\author{
  Xuan Ren\thanks{Equal contribution.} \\
  University of Adelaide \\
  \texttt{xuan.ren@adelaide.edu.au} \\
  \And
  Qi Chen\textsuperscript{*} \\
  University of Adelaide \\
  % \texttt{qi.chen04@adelaide.edu.au} \\
  \And
  Lingqiao Liu\textsuperscript{1}\thanks{Corresponding author.} \\
  University of Adelaide \\
  \texttt{lingqiao.liu@adelaide.edu.au} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



\begin{document}

% \ifcolmsubmission
% \linenumbers
% \fi

\maketitle

\begin{abstract}

Fine-tuning large language models (LLMs) typically relies on producing large sets of input-output pairs. Yet for a given question, there can be many valid outputs. In practice, these outputs are often derived by distilling knowledge from teacher models, and they can vary depending on the specific teacher model or prompting strategy employed.
Recent findings show that \emph{how} these training outputs are generated can significantly affect the performance of the fine-tuned model, raising an important question: how do we pick the best \emph{data generation method} from among numerous possibilities? Rather than exhaustively training and evaluating on each candidate, this paper proposes a scalable approximate method that assesses a \emph{small} subset of generated data to estimate its suitability for a specific target LLM. Our central idea is that effective outputs should be \emph{familiar} to the target LLM. While previous work measures familiarity with perplexity, we find that perplexity might be suboptimal in characterizing ``familiarity'' through theoretical analysis and practical observations. To address this, we introduce \emph{self-aligned perplexity}, a novel metric capturing how closely candidate outputs adhere to the target LLM’s own style and reasoning patterns. In this way, we can identify the most effective generation strategy on a small sample, then apply it to produce the complete training set. We demonstrate that training on data generated by the chosen method yields significant improvements across diverse reasoning-focused benchmarks.



\end{abstract}

\section{Introduction}

When instruction-tuning an LLM, training data consists of question-response pairs, where multiple valid responses can be generated for the same input. 
Previous studies ~\citep{ren2024learn} show that datasets with identical input questions but different responses can lead to varied learning outcomes, even when responses contain similar levels of detail.
This raises a key question: \textit{how can we construct responses that are most effective for the target LLM?}

Prior research has explored improving responses by adding details or rationales, such as structuring ground truth step by step~\citep{hsieh2023distilling, ranaldi-freitas-2024-aligning}, incorporating rationales, or enriching responses with additional information~\citep{zhang2024distillation, kang2023knowledgeaugmented, li2022explanations}. However, recent studies~\citep{ren2024learn, yang-etal-2024-self} suggest that more details or converting responses to step by step style do not always improve performance and that alignment with the LLM’s linguistic style is crucial.

From our experiment, we observe that, no single response generation strategy works universally across tasks. Thus, we need to creating a method to find out the most effective way to generate response for each task, rather than use a single method for all of the tasks.

Some works \citep{xu2024strongermodelsstrongerteachers, kim2024evaluatinglanguagemodelssynthetic} attempt to predict the effectiveness of response generation methods by evaluating the entire training dataset. They generate full training datasets using each method and then estimate training effectiveness based on scores computed via algorithms or reward models. However, these approaches are computationally expensive and not scalable.

However, can we predict the effectiveness of each data generation methods efficiently? We observe an interesting phenomenon that each response generation method produces responses with a consistent style, meaning that a small subset of generated examples can effectively represent the entire dataset. Based on this assumption, we propose an efficient ranking pipeline that evaluates a limited number of samples (e.g., 10) to assess the performance of each response generation strategy. This approach uses an alignment estimation function to assign scores to each strategy, enabling us to identify the best-performing method without the need for a full-dataset evaluation. 

Previous research \citep{ren2024learn} used perplexity to measure a model’s familiarity with candidate question-answer pairs, proposing that lower-perplexity responses for the same input tend to yield better training performance. However, we found several cases where perplexity-based filtering was ineffective. For instance, responses structured in a step-by-step or redundant language style often have low perplexity but do not necessarily improve training outcomes.When examining the initial response from the target LLM, we note that on some tasks, the probability of the model producing a step-by-step or redundant response in its initial prediction is very low, even though these responses have low perplexity. These findings suggest that perplexity can be easily "hacked" by response style. Consequently, traditional perplexity alone is insufficient for selecting the best response generation strategy.



To address this, we propose self-aligned perplexity, a refined metric for measuring a model’s familiarity with target responses. The key idea is that a model is most familiar with the data it generates itself. Leveraging this, we modify perplexity computation by incorporating model-generated responses as in-context examples. Specifically, we first have the model produce an initial response, which is then appended to the question as an in-context example. A prompt enforce the model to pay attention to this example when computing perplexity, thereby altering the probability estimation of the candidate response. If the target response deviates significantly from the model’s own generated response—the one it is most familiar with—the model assigns it a lower probability, increasing its perplexity. Our experiments show that self-aligned perplexity outperforms traditional perplexity in selecting effective data generation strategies.

In our experiments, we observe a strong correlation between the proposed indicator and the ranking of training dataset performance. Furthermore, we construct a pool of answer generation strategies and demonstrate that applying our selection criterion leads to significant performance gains compared to the baselines.



\section{Related Works}
% knowledge distillation \citep{hinton2015distillingknowledgeneuralnetwork}
% 
There has been extensive research into what types of data yield the best training outcomes for large language models (LLMs). Previous studies have identified several factors that positively influence model training, such as adding complexity \citep{xu2023wizardlm}, adding details \citep{zhang2024distillation, kang2023knowledgeaugmented, li2022explanations}, adding diversity \citep{luo2023wizardcoderempoweringcodelarge}, augmenting ground-truth answers in a step-by-step manner \citep{hsieh2023distilling, ho2022large, magister-etal-2023-teaching, fu2023specializing, ranaldi-freitas-2024-aligning}, and ensuring correctness \citep{trinh2024solving, ranaldi-freitas-2024-aligning}. However, in practice, these metrics are challenging to measure for a given dataset, making it difficult to determine the quality of training data based on these criteria. \citet{ren2024learn} found that familiarity, measured by perplexity, significantly impacts model training. They argue that language models are more familiar with content generated by other language models, which explains why synthetic data is often more effective for training compared to human-generated data for educating LLMs. 

Perplexity has been widely used for different purpose in prior research. Perplexity has been used to select prompts \citep{gonen2022demystifying}, showing that prompts with lower perplexity generally lead to better performance in question-answering tasks. It has also been used for selecting pretraining datasets \citep{de2022bertin}, detecting AI-generated content \citep{xu2024detecting, hu2020systematic}, and selecting instruction-tuning data from a database \citep{mekala2024smaller}. \citet{li-etal-2024-quantity} modify the perplexity score and propose “IDF” (Instruction Following Difficulty), which is used to select a small pool of challenging data from the original dataset for efficient training. Researchers hypothesized that higher perplexity reflects more challenging data, which can be beneficial for teaching LLMs new knowledge. Unlike these studies, our focus is on finding the best strategy to generate the target responses (y) for a given x.  

Two recent works have explored optimal strategies for generating responses, but their approaches diverge from ours. In \citep{xu2024strongermodelsstrongerteachers}, the authors introduce the Compatibility-Adjusted Reward (CAR) to rank training datasets; however, they limit their experiments to two instruction-following datasets and do not consider multiple meta-prompts. Similarly, \citep{kim2024evaluatinglanguagemodelssynthetic} identifies the key factors influencing training outcomes and employs principal component analysis (PCA) to fit a combination of these factors in order to predict the training performance for the same datasets. In contrast, our work presents several key distinctions. First, unlike the previous work, we evaluate only a small subset of generated data rather than complete datasets, enabling more efficient analysis. Second, while previous studies rely on composite metrics that require hyperparameter tuning—potentially risking overfitting on limited data diversity—we employ a single, robust metric validated across diverse tasks. Third, instead of ranking teacher models using a single meta-prompt, we rank models paired with diverse meta-prompts. Lastly, rather than solely demonstrating ranking effectiveness, our method also enhances overall accuracy.


\section{Method}
This paper aims to efficiently select the most effective answer generation strategy for fine-tuning a target LLM. In what follows, we first present the problem setup, then detail our proposed \emph{self-aligned perplexity} metric for scoring the outputs from each candidate strategy.

% \subsection{Problem Definition}\label{sec:problem}
% Let \(\mathcal{S} = \{ S_1, \dots, S_n \}\) be a set of candidate answer-generation strategies, where each strategy \(S_k\) produces a candidate response \(\hat{y}^k = S_k(x)\) for an input \(x\). Our goal is to select a strategy \(S_\iota\) that yields the most effective training data \(\mathcal{D} = \{(x, \hat{y}^k)\}\) for fine-tuning a target model \(M\). Since generating the full dataset via the API for every strategy is costly, we evaluate a small subset \(\mathcal{D}_s\) of size \(K\) (\(K \ll |\mathcal{D}|\)) to estimate how well each strategy’s outputs align with \(M\).

\subsection{Problem Definition}\label{sec:problem}

Let \(\mathcal{S} = \{ S_1, \dots, S_n \}\) be a set of candidate answer-generation strategies, where each strategy \(S_k\) produces a response \(\hat{y}^k = S_k(x)\) for an input \(x\). Our goal is to select the strategy \(S_\iota\) that yields the most effective training data \(\mathcal{D} = \{(x, \hat{y}^k)\}\) for fine-tuning a target model \(M\). Since generating the full dataset via the API for every strategy is costly, we evaluate a small subset \(\mathcal{D}_s\) of size \(K\) (\(K \ll |\mathcal{D}|\)) to estimate how well each strategy’s outputs align with \(M\).



\subsection{The familiarity hypothesis}
The work in \cite{ren2024learn} suggests that if the model is more ``familiar'' with a given response, then the model can learn better with the given response. In \cite{ren2024learn}, perplexity, which is equivalent to the likelihood of generating a response with the model, is used to measure this familiarity score. In our study, we argue that perplexity might be sub-optimal to measure the familiarity. We suggest that the familiarity can be more precisely (at least theoretically) measured by the following equation:
\begin{align}
    F(\hat{y}) = \mathbb{E}_y \left[s(y,\hat{y})\right] = \int s(y,\hat{y}) P_M(y) dy,
\end{align}where $s(y,\hat{y})$ is a semantic similarity measure between $\hat{y}$ and a sample response $y$ drawn from the model $M$. In plain language, it quantifies how similar a candidate response is to the range of answers that the model might generate. It is straightforward to demonstrate that when $s(y,\hat{y}) = \delta(y,\hat{y})$, i.e., when $\delta(y,\hat{y}) = 1$ only if $y$ is exactly identical to $\hat{y}$, the function $F$ becomes equivalent to the likelihood $P_M(\hat{y})$, and hence equivalent to perplexity. Theoretically, using perplexity as a surrogate to measure familiarity fails to account for the variety of responses that may be semantically equivalent to a candidate response, thereby underestimating the familiarity. In practice, this results in assigning an excessively high perplexity to a good candidate response that the model might actually be familiar with, as evidenced by our empirical study in section~\ref{sec:EmpiricalStudyOfWhyDoesSelfAlignedPerplexityWorkBetterThanTraditionalPerplexity} and examples in Table~\ref{tab:Example_of_self_aligned_perplexity}.


\begin{table}[h]
\centering
 \small 
\begin{tabular}{l|l|c|c|c|c|c}
\hline
 Model & Task& Prompt &  Accuracy & Perplexity  & $\texttt{Self-algined}$ \texttt{PPL} \\ \hline
Mistral7B   & GSM8K & Step by Step & \textbf{0.627}  &  2.09      &  \textbf{2.26}  \\ 
     &    &  Claud Answer Directly & 0.592 & \textbf{1.96}& 2.45\\ 
\hline
% Llama3   & MATH & GPT-4o Answer Directly & \textbf{0.56}  &  1.90      &  \textbf{1.85}  \\ 
%      &    &  Claud Answer Directly & 0.495 & \textbf{1.83}&1.90 \\ 
Llama3   & MATH & Step by Step & \textbf{0.56}  &  1.88      &  \textbf{1.91}  \\ 
     &    &  Claud Answer Directly & 0.495 & \textbf{1.84} &2.02 \\ 
\hline
Qwen2.5  &    API-Bank & Step by Step   & 0.41  &  \textbf{3.02}   & 2.98  \\ 
     &    &Claud Answer Directly  &\textbf{0.461} & 3.14  & \textbf{2.90} \\ 
\hline
\end{tabular}
\caption{Example of the cases where self-aligned perplexity is more effective than perplexity.}
\label{tab:Example_of_self_aligned_perplexity}
\end{table}




\subsection{Self-Aligned Perplexity}
\label{sec:self_aligned_perplexity}
To address the issue described above, we introduce an alternative surrogate for measuring familiarity by leveraging the concept of in-context learning. Specifically, we use the model's initial prediction, \(y = M(x)\), as an \emph{in-context example of style} and then evaluate the likelihood of a candidate response conditioned on this example in the prompt. This approach encourages the model to assign a higher probability to a response if its style closely aligns with that of the model's generated response.


\paragraph{Traditional Perplexity.}  
For a candidate response \(\hat{y}\) of length \(|N|\), traditional perplexity is defined as:
\[
  \varphi_{\texttt{PPL}}(M, x, \hat{y}) 
  = \exp\Bigl\{-\frac{1}{|N|}\sum_{t=1}^{|N|} \log P_M\bigl(\hat{y}_t \mid x, \hat{y}_{<t}\bigr)\Bigr\}.
\]
% Although lower perplexity suggests familiarity, it can misjudge responses that differ stylistically from what \(M\) typically produces (see Table~\ref{tab:Example_of_self_aligned_perplexity}).

\paragraph{Self-Aligned Perplexity.}  
To address this, we use \(M\)'s initial prediction, \(y=M(x)\), as an in-context style example. For each input \(x_i\) in a subset \(\mathcal{D}_s\):
\begin{enumerate}
    \item Generate \(y_i = M(x_i)\). \footnote{\(M(x_i)\) may generate incorrect responses, which can sometimes cause \(\mathrm{Prompt}(y_{i+1})\) to mislead the in-context perplexity calculation. To mitigate this, we employ a filtering mechanism to remove incorrect \(M(x_i)\). According to our experimental results, since  \(y_{i+1}\) is primarily used to guide style, its correctness is less critical. Hence, the correctness filter is optional. For more details, please refer to Appendix~\ref{tab:correctness} and~\ref{sec:select_correct_examples}.} 
    \item For candidate \(\hat{y}_i\), include \(y_{i+1}\) (or \(y_1\) when \(i=K\)) in the prompt to provide style guidance.
    \item Compute the perplexity of \(\hat{y}_i\) conditioned on this prompt.
\end{enumerate}
Formally,
\[
  \varphi_{\texttt{ICPPL}}(M, x_i, \hat{y}_i) 
  = \begin{cases}
    \varphi_{\texttt{PPL}}(M, \mathrm{Prompt}(y_{i+1}), \hat{y}_i), & \text{if } i < K, \\
    \varphi_{\texttt{PPL}}(M, \mathrm{Prompt}(y_1), \hat{y}_i), & \text{if } i = K.
  \end{cases}
\]
This self-aligned measure penalizes responses that deviate from \(M\)'s own style. For more details, please read Appendix~\ref{sec:self_aligned_perplexity_details}

\paragraph{Selection Criterion.}  
For each strategy \(S_k\), we calculate:
\[
  \pi_{\texttt{ICPPL}}(S_k) 
  = \frac{1}{K}\sum_{i=1}^{K} \varphi_{\texttt{ICPPL}}(M, x_i, \hat{y}_i^k),
\]
and select:
\[
  S_{\iota} = \arg\min_{k}\, \pi_{\texttt{ICPPL}}(S_k).
\]








Table~\ref{tab:Example_of_self_aligned_perplexity} shows that self-aligned perplexity more reliably selects the optimal strategy across three tasks (GSM8K, MATH, and API-Bank). For example, on GSM8K, Mistral7B achieves its best accuracy with the step-by-step strategy—correctly favored by self-aligned perplexity but not by standard perplexity.


% \subsection{Self-Aligned Perplexity}
% \label{sec:self_aligned_perplexity}
% \paragraph{Perplexity} 
% A straightforward way to gauge familiarity is via perplexity, which is typically defined as the exponentiation of the average negative log-likelihood per token. That is, for a candidate response \(\hat{y}\) consisting of \({N}\) tokens, we define:
% {\small
% \[
%   \varphi_{\texttt{PPL}}(M, x, \hat{y}) =
%   \exp\left\{-\frac{1}{|{N}|}\sum_{t=1}^{|{N}|}\log P_M\left(\hat{y}_t \,\big\vert\, x, \hat{y}_{<t}\right)\right\}.
% \]
% }



% \paragraph{In-Context Perplexity}
% To address this, we propose \emph{self-aligned perplexity}, which incorporates \(M\)'s own initial prediction \(y = M(x)\) as an \emph{in-context example of style}. Concretely:
% \begin{enumerate}
%     \item Generate \(K\) responses, \(y_1, y_2, \dots, y_K = M(x_1), M(x_2), \dots, M(x_K)\), where each response is generated from the corresponding input \(x_1, x_2, \dots, x_K\).\footnote{\(M(x)\) may generate incorrect responses, which can sometimes cause \(\mathrm{Prompt}(y)\) to mislead the perplexity calculation. To mitigate this, we employ a filtering mechanism to remove incorrect \(M(x)\). According to our experimental results, since \(y\) is primarily used to guide style, its correctness is less critical. Hence, the correctness filter is optional. For more details, please refer to Appendix~\ref{tab:correctness}.} 


%     \item For each candidate response \(\hat{y}_i\), we assign in-context examples in a cyclic manner: the in-context example for the first input \(x_1\) is based on the second input \(x_2\)'s generated response, the in-context example for \(x_2\) is based on \(x_3\)'s response, and so on, until the in-context example for the last input \(x_K\) is based on \(x_1\)'s response. This way, the model focuses on imitating the inference style rather than directly copying the answers.

%     \item Construct a prompt, \(\mathrm{Prompt}(y_i)\), containing both \(x_i\) and a directive such as:
%     \begin{quote}
%         \itshape
%         ``Here is an example of how to reason and respond. Please strictly follow this style. If you drift, you must correct yourself mid-inference.''
%     \end{quote}
%     The detailed prompt can be found in Appendix~\ref{in_context_prompt}.
    
%     \item Compute perplexity for a candidate \(\hat{y}_i\) with the model forced to follow the self-demonstrated style in \(y_{i+1}\) or \(y_1\).

% \end{enumerate}

% Formally, define:
% {\small\[
%   \varphi_{\texttt{ICPPL}}(M, x_i, \hat{y}_i) = \varphi_{\texttt{PPL}}(M, \mathrm{Prompt}(M(x_{i+1})), \hat{y}_i) \quad \text{if} \, i < K
% \]
% \[
%   = \varphi_{\texttt{PPL}}(M, \mathrm{Prompt}(M(x_1)), \hat{y}_i) \quad \text{if} \, i = K
% \]
% }
% In this way, if \(\hat{y}_i\) is fluent but diverges from \(M\)'s reasoning style, the perplexity will be higher. This makes self-aligned perplexity a more reliable measure of style mismatch compared to plain token-level perplexity.

% \paragraph{Selection Criterion.}
% For each strategy \(S_k\), we obtain \(\{\hat{y}_i^k\}\) from the small subset \(\mathcal{D}_s\) and average:
% \[
%   \pi_{\texttt{ICPPL}}(S_k)
%   \;=\;
%   \frac{1}{K}\sum_{i=1}^{K}
%     \varphi_{\texttt{ICPPL}}(M, x_i, \hat{y}_i^k).
% \]
% We then select:
% \[
%     S_{\iota} 
%     \;=\; 
%     \arg\min_{k} 
%     \;\pi_{\texttt{ICPPL}}(S_k).
% \]



% Table~\ref{tab:Example_of_self_aligned_perplexity} presents examples where self-aligned perplexity selects a better strategy \(S_{\iota}\) than standard perplexity across three tasks (GSM8K, MATH, and API-Bank). In each case, standard perplexity ranks suboptimal strategies higher, whereas self-aligned perplexity favors the strategy that yields higher accuracy. For instance, on GSM8K, Mistral7B achieves its best accuracy when using training data generated by the step-by-step strategy, which is correctly favored by self-aligned perplexity but not by standard perplexity.

\section{Benchmark Construction}

In the section, we shows how do we use different strategies (distinct prompts and teacher LLMs) in generating the high-quality responses with different styles.

\subsection{Target LLMs and APIs}

We use  Mistral-7B-instruct-V2 \citep{jiang2023mistral}, Llama3-instruct \citep{dubey2024llama3herdmodels} and  Qwen-2.5-7B-Instruct\citep{qwen2025qwen25technicalreport} as the target language models $M$. In this paper, we refer to Llama3-instruct, Mistral-7B-instruct-V2, and Qwen-2.5-7B-Instruct as Mistral7B, Llama3, and Qwen2.5, respectively. We use GPT-4o, MiniGPT-4o, and Claude 3.5 APIs as teacher models for response generation. Specifically, we use gpt-4o-mini-2024-07-18 and gpt-4o-2024-08-06 \citep{openai_gpt4_api} from OpenAI, and claude-3-5-sonnet-20240620 \citep{anthropic_claude_api} from Anthropic.

% \subsection{Datasets}
% We select reasoning related datasets from all the training (if included) and evaluation datasets referenced in the technical reports of LLaMA3\citep{dubey2024llama3herdmodels}, Mistral \citep{jiang2023mistral}, and Qwen-2.5\citep{qwen2025qwen25technicalreport}, which are also the three target models \( M \) used in our experiments. Specifically, we select datasets that are relevant to reasoning tasks(English only), contain more than 650 data points, and can be evaluated using accuracy metrics. If the dataset lacks training data, we reconstruct the testing data to ensure it contains at least 400 training data points, 50 validation data and 200 testing data points.

% For datasets with hierarchical subcategories (e.g., MATH, MMLU,  MMLU\_PRO, API\_BANK, AGIEVAL), we select data from the most challenging subcategory operationally defined as the one with the lowest reported accuracy in the original publication. Specifically, we include moral scenarios from MMLU, Professional Law from MMLU\_PRO, Level 3 problems from API\_BANK, geometry from MATH, and LogicQA from AGIEVAL. Following the experimental protocol of \citep{ren2024learn}, we additionally incorporate the Algebra subcategory from MATH to ensure comparability with prior work.

% We follow the settings from~\citep{ren2024learn} that for each dataset, we train and evaluate models or strategies on the first 1,000 training and testing examples. For each task, we generate up to 1,000 training examples per data generation strategy. For instance, if we apply seven different data generation strategies (Section~\ref{sec:data_generation_strategy} in Appendix), the GSM8K task will yield 7,000 target responses (7 × 1,000).

% In total, we select the following datasets: \textbf{Mathmatics:} GSM8K \citep{cobbe2021training}, MATH (Algebra) and MATH (Geometry)\citep{hendrycks2021measuring}. \textbf{Commonsense reasoning:} PIQA \citep{bisk2020piqa}, WinoGrande \citep{sakaguchi2021winogrande}, Hellaswag \citep{zellers-etal-2019-hellaswag} and ECQA \citep{aggarwal2021explanations}. \textbf{Reading comprehension:} BoolQ \citep{clark2019boolq} and SQuAD \citep{rajpurkar-etal-2016-squad}. \textbf{Aggregated benchmarks:} MMLU (Moral Senarios)\citep{hendrycks2020measuring}, MMLU\_PRO (Professional Law) \citep{wang2024mmlu}, and AGIEval (LogicQA) \citep{zhong2023agieval}. \textbf{Coding:} MBPP \citep{austin2021program}. \textbf{Reasoning:} DROP \citep{dua2019dropreadingcomprehensionbenchmark} and ARC-Challenge\citep{clark2018think}. \textbf{Tool-using:} API-BANK (Lv 3 problems)\citep{li2023api}

% For more details, please refer to Table~\ref{tab:why_each_data_is_chosen} in Appendix.

\subsection{Datasets}
We use English reasoning datasets referenced in the technical reports of LLaMA3 \citep{dubey2024llama3herdmodels}, Mistral \citep{jiang2023mistral}, and Qwen-2.5 \citep{qwen2025qwen25technicalreport} (the three target models \( M \) in our experiments). We select datasets with at least 650 examples that can be evaluated via accuracy. If a dataset lacks sufficient training data, we reconstruct it to contain at least 400 training, 50 validation, and 200 testing examples.

For datasets with subcategories (e.g., MATH, MMLU, MMLU\_PRO, API\_BANK, AGIEVAL), we choose the most challenging subcategory (i.e., with the lowest reported accuracy). For example, we include moral scenarios from MMLU, Professional Law from MMLU\_PRO, Level 3 problems from API\_BANK, geometry from MATH, and LogicQA from AGIEVAL; we also incorporate the Algebra subcategory from MATH as in \cite{ren2024learn}.

Following \cite{ren2024learn}, we train and evaluate the first 1,000 training and testing examples, generating up to 1,000 training examples per data generation strategy.

In total, our datasets include: \textbf{Mathematics:} GSM8K \citep{cobbe2021training}, MATH (Algebra) and MATH (Geometry) \citep{hendrycks2021measuring}; \textbf{Commonsense reasoning:} PIQA \citep{bisk2020piqa}, WinoGrande \citep{sakaguchi2021winogrande}, Hellaswag \citep{zellers-etal-2019-hellaswag}, and ECQA \citep{aggarwal2021explanations}; \textbf{Reading comprehension:} BoolQ \citep{clark2019boolq} and SQuAD \citep{rajpurkar-etal-2016-squad}; \textbf{Aggregated benchmarks:} MMLU (Moral Scenarios) \citep{hendrycks2020measuring}, MMLU\_PRO (Professional Law) \citep{wang2024mmlu}, and AGIEval (LogicQA) \citep{zhong2023agieval}; \textbf{Coding:} MBPP \citep{austin2021program}; \textbf{Reasoning:} DROP \citep{dua2019dropreadingcomprehensionbenchmark} and ARC-Challenge \citep{clark2018think}; and \textbf{Tool-using:} API-BANK (Lv 3 problems) \citep{li2023api}.
For further details, please refer to Table~\ref{tab:why_each_data_is_chosen} in the Appendix.
\subsection{Data Generation Strategies}
\label{sec:data_generation_strategy}
Given 1,000 samples, we use different strategies to generate target responses.
% 
For a fair comparison, we use the same prompts from \citep{ren2024learn} to generate responses, including \textbf{\hyperref[prompt answer directly]{GPT-4o Answer Directly}}, \textbf{\hyperref[prompt answer directly]{Claud Answer Directly}}, \textbf{\hyperref[prompt answer directly]{MiniGPT-4o Answer Directly}}, \textbf{\hyperref[prompt step by step]{Step-by-Step}} and 
\textbf{\hyperref[prompt rewrite groundtruth]{Rewrite Ground Truth}}. Besides, we design two new prompts named \textbf{\hyperref[prompt gpt4o example]{GPT-4o Examples}} and \textbf{\hyperref[prompt human written example]{Human Examples}} on our own. Please refer to Appendix~\ref{appendix:Response Construction Details} for details on each response construction method.

We provide ground truth to the teacher models and allow up to three attempts for data generation. If the first result is incorrect, we regenerate; otherwise, we stop. The same evaluation script used during testing is applied to check correctness.





\section{Experiment}





\begin{figure*}[t]
  \centering
  \includegraphics[width=0.7\textwidth]{all_avg_scores_bar_plot_50.png}
  \caption{Our method($K=50$, average of three subsets) Vs. baselines(each data generation method).}
  \label{figure:ours_vs_baseline_unfiltered}
\end{figure*}



\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{accuracy_ranking_vs_pgr.png}
  \caption{The benefits of our method ($K=50$, average of three subsets) improve upon the average performance of the baselines(each data generation method) as the PGR increases.}
  \label{accuracy_ranking_vs_pgr}
\end{figure*}


\begin{figure}[ht]
\centering
\includegraphics[width=0.7\columnwidth]{Diversity_vs_Improved_Percentage.png}
  \caption{This figure demonstrates that the higher PGR, the more diversity of the training effectiveness between using the data generated by different data generation methods.}
\label{Diversity_vs_Improved_Percentage_2_plots}
\end{figure}


\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.9\textwidth]{ranking_method_comparison_50.png}
    \caption{Comparison between our method ($K=50$) and other data selection methods ($K=50$, average of three subsets).}
    \label{fig:ours vs other metircs}
\end{figure*}




% In this section, we treat each generation strategy from \hyperref[sec:data_generation_strategy]{Section~4.3}, as well as other response-selection methods, as baselines. We then compare the average training outcomes of our method against each baseline across all tasks.

% When training the target model using a given data generation strategy, we run experiments with random seeds 0, 1, and 2, and report the average performance in Table~\ref{tab:ntrain_1000_lr_2e-05_seedaverage_of_seed_0,1,2}. For our method, we also repeat the process three times, each time selecting a different subset of size \(K=10\) from training dataset, and report the average performance across these three runs. For example, we may calculate the self-aligned perplexity of the first, second and third 10 subsets and report the average.



In this section, we treat each generation strategy from \hyperref[sec:data_generation_strategy]{Section~4.3}, along with other response-selection methods mentioned in the related work section, as baselines. We then compare the average training outcomes of our method against these baselines across all tasks.

When training the target model using a particular data generation strategy, we run experiments with random seeds 0, 1, and 2, and report their average performance in Table~\ref{tab:ntrain_1000_lr_2e-05_seedaverage_of_seed_0,1,2}. For our method, we also repeat the process three times, each time selecting a different subset of size \(K=50\) from the training data, and then report the average of these three runs. For instance, one run may use the first 50 samples, another the second 50, and so on, and we combine their results to obtain a final average.

\subsection{Hyperparameters}
We utilize the identical hyperparameter settings as referenced in \citep{ren2024learn}. Specifically, for model fine-tuning, a learning rate of 2e-5, a batch size of 32, and a warm-up phase encompassing 10\% of the total training iterations are applied. A cosine annealing schedule is implemented for the learning rate, and only the Q and V matrices of the LoRA parameters are fine-tuned with a rank of 8. All models undergo training and evaluation using half-precision arithmetic. 


\subsection{Our Method vs. Baselines (Each Data Generation Methods)}
\label{experiment_all_data}
Figure~\ref{figure:ours_vs_baseline_unfiltered} compares our method against baselines. The \textbf{\hyperref[prompt rewrite groundtruth]{Rewrite Ground Truth}} method is omitted from the Figure~\ref{figure:ours_vs_baseline_unfiltered} and Figure~\ref{accuracy_ranking_vs_pgr} since it applies only to datasets with CoT groundtruth. Other results represent task averages across all datasets, while \textbf{\hyperref[prompt rewrite groundtruth]{Rewrite Ground Truth}} averages a subset of tasks. Despite its exclusion from the figure, all metrics and tables incorporate this method's results. 

As shown in Figure~\ref{figure:ours_vs_baseline_unfiltered}, our method outperforms every data generation strategy. On average, it surpasses the best data generation strategy (Claude) by 0.66\%. The average performance of all methods is 62.97\%, which is 1.22\% lower than ours (64.19\%).

% ``IDF'' (instruction following difficulty) is a method from \citep{li-etal-2024-quantity}, and

% Figure~\ref{comparison_between_weighted_ranking_methods_unfiltered} shows that, on average, our method achieves 66.87\%. Other methods only reach up to 66.05\%, 0.82\% lower than ours.  Our method also show good spearman correlation compared to the other methods. On average, our method achieves 0.2408, which is much higher than other methods except perplexity, but we are better in terms of selecting the most effect data generation method.

% Since the performance diversity for different tasks are very different, we use Spearman correlation to account for the importance of ranking in each task.

% Our method achieves a 0.4169 weighted Spearman correlation, significantly outperforming the second-best method, ``perplexity'', which achieves only 0.2617. This demonstrates that our method ranks response generation strategies more accurately than other methods.


% Since the performance diversity for different tasks are very different, we use weighted Spearman correlation to account for the importance of ranking in each task. 
% Specifically, we first calculate the diversity \(D_i\) of performance variants for each method after training, and then normalize the diversity values to compute the weight \(W_i = \frac{D_i}{\sum_{i=1}^{n} D_i}\).
% Finally, we compute the weighted Spearman correlation as
% \[
% \rho_{\text{weighted}} = \sum_{i=1}^{n} W_i \cdot \rho_i,
% \]
% where \( \rho_i \) is the Spearman correlation for the \(i\)-th method and \( W_i \) is the normalized weight based on the diversity of the methods. 



\subsection{Choosing Data Generation Methods Becomes More Important When PGR Increases}
\label{filter_out_unlearnable}

When analyzing the detailed performance record (n = 1000) (Table~\ref{tab:ntrain_1000_lr_2e-05_seedaverage_of_seed_0,1,2} in Appendix), we notice that there is substantial performance variation between methods for some tasks, while for others, the variation is minimal. For example, when training the Qwen model on API-BANK, using training data generated by \textbf{\hyperref[prompt answer directly]{Claud Answer Directly}} strategy results in an evaluation accuracy of 45.7\%, whereas training with data generated by \textbf{\hyperref[prompt gpt4o example]{GPT-4o Examples}} strategy yields an evaluation accuracy of 33.9\%. In contrast, for tasks like GSM8K and MATH Algebra, the performance gap between the best and worst methods is under 2\%.

We observe that the performance variation between methods is minimal when training with 1000 samples (data from Table~\ref{tab:ntrain_1000_lr_2e-05_seedaverage_of_seed_0,1,2} in Appendix) yields only negligible improvement over training with 100 samples 
(data from Table~\ref{tab:ntrain_100_lr_2e-05_seed0} in Appendix).


To investigate this relationship, we created Figure~\ref{Diversity_vs_Improved_Percentage_2_plots}, where the X-axis represents the performance gain ratio (PGR) on a logarithmic scale, and the Y-axis measures the standard diviation in training effectiveness across methods. The PGR is defined as:
\[
\text{PGR} = \left( \frac{\text{Acc}_{1000}}{\text{Acc}_{100}} - 1\\\ \right) 
\]
This metric quantifies the improvement in accuracy when increasing the training data from 100 to 1000 examples.


While the right plot demonstrates that when the PGR exceeds 2\%, the diversity across generation methods increases significantly, the left plot shows a strong positive correlation between \(\log(\text{PGR})\) and \(\log(\text{Standard Diviation})\). Notably, when PGR is below 2\%, the diversity is nearly 0.

These two subplots show that the variation in training effectiveness increases as PGR increases. The greater the divergence between the training outcomes, the more important it becomes to select the best response generation strategy.


% \subsubsection{Our method is More Effective When PGR Increases} \label{filter_out_unlearnable_1}
% In this section, we aim to investigate whether our method demonstrates enhanced effectiveness as PGR increases. Figure~\ref{accuracy_ranking_vs_pgr} compares our approach against baseline methods on datasets with varying PGR values. The results show that as the minimum PGR requirement increases -INF(All data) to 0.3, the performance advantage of our method over the average baseline gradually increases from 0.014 to 0.039. This trend confirms that our method achieves better results on datasets with higher PGR.





% \subsubsection{Our Method Vs Other Data Selection Metrics} \label{sec:Our Method Vs Other Data Selection Metrics}
% Figure~\ref{fig:ours vs other metircs} shows that, on average, our method achieves 66.14\%, while other methods only reach up to 65.57\%, 0.57\% lower than ours. As PGR increases, the advantage of our method also increases, gradually rising from about 0.6\% to about 1.8\% when PGR \textgreater 0.3.







\subsubsection{Our Method Performs Better as PGR Increases}  
\label{filter_out_unlearnable_1}

We investigate whether our method becomes more effective as the dataset is filtered using increasingly higher PGR thresholds. Figure~\ref{accuracy_ranking_vs_pgr} compares our method with baselines on subsets where the minimum PGR ranges from \(-\infty\) (all data) to 0.3. As the PGR threshold increases, the performance gap between our method and the average baseline grows from 1.3\% to 3.5\%, indicating that our method performs especially well on datasets with higher PGR values.


\subsubsection{Comparison with Other Data Selection Metrics}  
\label{sec:Our Method Vs Other Data Selection Metrics}

Figure~\ref{fig:ours vs other metircs} shows that our method consistently outperform other methods in terms of accuracy and weighted spearman correlation. As the PGR threshold increases, this gap widens.





\section{Ablation Study}


\subsection{Why Self-Aligned Perplexity Outperforms Traditional Perplexity}
\label{sec:EmpiricalStudyOfWhyDoesSelfAlignedPerplexityWorkBetterThanTraditionalPerplexity}
\begin{table}[h]
\centering
 \small 
\begin{tabular}{l|l|c|c|c|c|c}
\hline
Target Response style & Model & Task  & \texttt{PPL} &  \texttt{IC}$_{\text{sbs}}$\texttt{PPL}    & \texttt{IC}$_{\text{cad}}$\texttt{PPL}  & \texttt{IC}$_{\text{r}}$\texttt{PPL}  \\ \hline


Step by Step(sbs) &  Mistral7B & ECQA     & \textbf{4.476}  & \textbf{3.695} & 4.85    &  4.329    \\ 
GPT4 Answer Directly(cad)   &        &  & 5.551  & 4.116 & \textbf{4.768}    &   4.456  \\ 
Redundant(r)    &      & & 4.944 & \textcolor{red}{4.334} &  \textcolor{red}{5.615}   &   \textbf{4.326}   \\  
\hline
Step by Step(sbs) &  Mistral7B &     PIQA    &    \textbf{4.290}    &  \textbf{3.816}  & 5.968 & 4.028\\ 
GPT4 Answer Directly(cad)  &      &    & 6.277 & 4.053 &  \textbf{5.962} & 4.250\\ 
Redundant(r)   & &   &  4.547  & 3.919 & \textcolor{red}{6.724} &  \textbf{4.027} \\  
\hline
\end{tabular}
\caption{Examples showing that in-context perplexity favors responses matching the style of the in-context example. \texttt{PPL} denotes traditional perplexity, while \texttt{IC}$_{\text{sbs}}$\texttt{PPL}, \texttt{IC}$_{\text{cad}}$\texttt{PPL}, and \texttt{IC}$_{\text{r}}$\texttt{PPL} are computed using step-by-step, GPT-4o Answer Directly, and redundant responses as in-context examples, respectively.}
\label{tab:self-aligned perplexity push perplexity to the right direction}
\end{table}



\textbf{Failure example of perplexity:} As shown in Table~\ref{tab:self-aligned perplexity push perplexity to the right direction}, on the ECQA and PIQA tasks, responses written in a highly redundant style (see Appendix~\ref{appendix:redundant_prompt}) receive lower perplexity scores—even though the target model is unlikely to generate such responses—while high-quality \textbf{\hyperref[prompt answer directly]{GPT-4o Answer Directly}} responses receive higher perplexity.

Here’s a slightly more concise version with one word removed for brevity:

\textbf{Effectiveness of in-context example:} When we incorporate a style-specific in-context example (e.g., a step-by-step response), in-context perplexity adjusts the model’s probability estimation to favor responses that match the provided style. Our method uses the target model's own prediction as the in-context example, allowing self-aligned perplexity to better reflect the alignment between the model's style and the response candidates.

\begin{table}[t]
\centering
  \small 
% \resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|l|c|c}
\hline
Weighted \textbf{Method}         & \textbf{Model} & \textbf{Accuracy} & \textbf{Spearman's $\rho$} \\ \hline
Ours                     & Mistral7B           &     0.582        &         0.411                \\
Train-then-test lr = 2e-5      &   &   0.578        &   0.249                         \\
Train-then-test lr = 2e-4     &           &  0.579      &  0.440           \\ \hline
Ours                       &     Llama3          &     0.650    &  0.344              \\
Train-then-test lr = 2e-5         &               &     0.626    &         0.257          \\
Train-then-test lr = 2e-4         &              &     0.638    &       0.286          \\ \hline

Ours                &  Qwen2.5                  &    0.723          &   0.494            \\
Train-then-test lr = 2e-5          &    &    0.710         &          0.160       \\ 
Train-then-test lr = 2e-4          &                 &  0.726       &       0.345       \\ \hline

Ours                    & Average                &   0.652            &         0.416               \\
Train-then-test lr = 2e-5      &    &    0.638             &           0.222      \\ 
Train-then-test lr = 2e-4     &     &         0.648        &              0.357         \\ \hline

\end{tabular}
% }
\caption{Our Method ($K=50$) vs. Train-then-test ($K=100$, 1 seed, lr = 2e-5 or 2e-4) on tasks with PGR \textgreater 2\%}
\label{tab:ours_vs_train_then_test}
\end{table}





% All data & Mistral7B &  &  &  &  &  \\
%          & Llama3    &  &  &  &  &  \\
%          & Qwen2.5   &  &  &  &  &  \\ \hline
% PGR $>$ 2\% & Mistral7B &  &  &  &  &  \\
%             & Llama3    &  &  &  &  &  \\
%             & Qwen2.5   &  &  &  &  &  \\ \hline

\begin{table}[t]
\centering
 \small 
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
\hline
 Model & \multicolumn{5}{c}{Accuracy} & \multicolumn{5}{c}{Weighted Spearman’s $\rho$}  \\ \hline 
 & 0--10 & 10--20 & 20--30 & 0--50 & 0--100 & 0--10 & 10--20 & 20--30 & 0--50 & 0--100\\ \hline
% All data & Mistral7B &0.583  & 0.585 & \textcolor{red}{0.570} & 0.583 & 0.584 \\
%          & Llama3    &  0.631&0.635  & \textcolor{red}{0.628} & 0.636 & 0.637 \\
%          & Qwen2.5   &0.704  &0.703  &0.704  & 0.707  & 0.708 \\ \hline
 Mistral7B & 0.583 & 0.584 & 0.570 & 0.583 & 0.583 &  0.396 & 0.310 & 0.257 &0.412 & 0.411\\
 Llama3    &  0.644 & 0.650 & 0.642 & 0.650 & 0.652  &  0.331 & 0.393 & 0.077 &0.344 & 0.314\\
 Qwen2.5   & 0.714 & 0.711 & 0.718 & 0.723 & 0.726  & 0.363 & 0.472 & 0.327 & 0.494 & 0.401\\ \hline
\end{tabular}

\caption{Performance stability of our methods when PGR \textgreater 2\%. The subsets 0-10, 10-20, and 20-30 each contain 10 samples, while 0-50 and 0-100 represent the first 50 and 100 samples.}
\label{tab:stability_our_method}
\end{table}



\subsection{Our Method vs. Train-Then-Select}
One natural—but computationally expensive—approach to selecting the optimal response generation strategy is to adopt a "Train-Then-Select" procedure. In this approach, we first generate a small dataset (e.g., 100 samples) using each candidate strategy. For each dataset, we train the target model and evaluate its performance. We then rank the strategies based on these results and choose the best-performing one to generate the remainder of the dataset. 

When evaluating "Train-Then-Select", we train the target model on 100 samples under two settings:  

  
\textbf{1. Standard Training:} A learning rate of 2e-5 for 20 epochs (matching our main setup). The performance accuracies for each strategy under this settings is reported in the Table~\ref{tab:ntrain_100_lr_2e-05_seed0}.

\textbf{2. Intense Training:} A learning rate of 2e-4 for 40 epochs. The performance accuracies for each strategy under this settings is reported in the Table~\ref{tab:ntrain_100_lr_0.0002_seed0}.



After ranking the strategies using Train-Then-Select, we compare its performance with our method. As shown in Table~\ref{tab:ours_vs_train_then_test}, despite using less data and requiring no training, validation, or testing computations for strategy selection, our method achieves better accuracy and higher Spearman correlation.


\subsection{Stability of Our Methods}

As shown in Table~\ref{tab:stability_our_method}, our method exhibits improved accuracy as the subset size increases. Generally speaking, the performance of our method is consistent. Some of the small subsets might causes degradtion of the accuracy(highlighted in red), thus we recommend to choose a relatively larger size of subsets. In the table, "0–10", "10–20", and "20–30" indicate that we use the first, second, and third groups of 10 training samples, respectively, to compute the ranking. "0–50" and "0–100" represent subsets containing the first 50 and 100 samples. According to the table, when the subset is large (e.g., K = 50 or K = 100), the performance is better in terms of accuracy and Spearman correlation. However, when the subset is small (K = 10), the performance of each subset becomes less stable.


\subsection{Groundtruth vs. Synthetic Data}
\label{sec:gt_vs_synthetic_data}
As shown in Table~\ref{tab:ntrain_1000_lr_2e-05_seedaverage_of_seed_0,1,2} of Appendix, when ground truth is provided in natural language (e.g., GSM8K, MATH, ECQA, MBPP), training on ground truth is less effective than training on synthetic data. This is because LLMs are more familiar with LLM-generated data, as demonstrated by \citet{ren2024learn}. However, when the ground truth is written as a gold label without a CoT inference process, training on gold label can sometimes outperform training on CoT synthetic data within the same domain. However, as shown in Table~\ref{table:cross_domain_performance_recording} of Appendix, training on gold labels harms cross-domain performance more than training on synthetic data. Additionally, in real-life scenarios, training on natural language data is crucial, as users expect to see the rationale behind the final prediction made by LLMs.










\section{Conclusions}

In this paper, we present a novel and scalable approach for selecting the optimal response generation strategy to train large language models. We introduce a new metric, self-aligned perplexity, which more effectively evaluates the alignment between the target model and the response options compared to traditional perplexity. Our experiments show that as the PGR increases, selecting the best response generation method becomes more crucial. We demonstrate that choosing the optimal generation strategy based on self-aligned perplexity can lead to substantial improvements in model performance, particularly on tasks with higher PGR. We hope our work will inspire researchers who use perplexity as a downstream metric and those seeking to generate the most effective responses from a set of inputs for training large language models.


% \section{Limitations}
% While our proposed method for training data optimization offers significant improvements, it does have some limitations. 

% Firstly, due to cost constraints, the training datasets in most of our experiments are limited to a size of up to 1000. Whether our method is applicable to larger datasets, such as those with 10,000 examples, remains to be validated. 

% Secondly, some companies have recently released open-source models capable of generating O1-type chain-of-thought (COT) small models. However, whether our method is applicable to O1-type small models still needs to be verified. We believe that the principles behind LLM training are transferable, so our approach and alignment score should be applicable to O1-type small models. However, since these models were only recently released and our experiments are large in scale, with the associated high costs of O1 models, we have not yet conducted experiments in this area.

% % The last limitation of this work is that, our method are not helpful for comparing CoT responses and single label ground-truth, such as True/False classification groundtruth or multiple choice groundtruth. The basic assumption of our method is ``a good output should closely resemble the output generated by the target LLM'', It works when the output contains a paragraph, e.g., a reasoning step. It is not applicable for the case when the answer only contains a single word. For example, how do we compute the semantic similarity between a CoT initial prediction from a target LLM and a `True/False' label? However, as discussed in the section~\ref{sec:gt_vs_synthetic_data}, training on CoT are desired in many senarios because training only on gold label can damage the cross-domain performance and sometimes people wish to see the rationale.




\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}


\appendix

\section{Appendix}


\subsection{The Details of Strategy Selection Through Self-Aligned Perplexity}
\label{sec:self_aligned_perplexity_details}
\paragraph{Perplexity} 
A straightforward way to gauge familiarity is via perplexity, which is typically defined as the exponentiation of the average negative log-likelihood per token. That is, for a candidate response \(\hat{y}\) consisting of \({N}\) tokens, we define:
{\small
\[
  \varphi_{\texttt{PPL}}(M, x, \hat{y}) =
  \exp\left\{-\frac{1}{|{N}|}\sum_{t=1}^{|{N}|}\log P_M\left(\hat{y}_t \,\big\vert\, x, \hat{y}_{<t}\right)\right\}.
\]
}


%\lingqiao{\textbf{Why In-context perplexity?} Perplexity is computed based on the likelihood of generating a specific $y$. More precisely, the probability calculated using the chain rule measures the likelihood of generating \textit{exactly the same output}. However, what we truly seek to assess is how similar a candidate response is to the range of answers that the model might generate. The conventional perplexity of a response does not account for the likelihood of responses that are semantically equivalent. In-context perplexity overcomes this limitation by using in-context examples to increase the probability of answers that are similar to those generated by the model.  }

%A response that uses very common tokens or has redundant language might seem ``fluent'' (resulting in low perplexity) yet fail to reflect the reasoning style of \(M\). In other words, vanilla perplexity alone can be misled by trivial or generic responses.

\paragraph{In-Context Perplexity}
To address this, we propose \emph{self-aligned perplexity}, which incorporates \(M\)'s own initial prediction \(y = M(x)\) as an \emph{in-context example of style}. Concretely:
\begin{enumerate}
    \item Generate \(K\) responses, \(y_1, y_2, \dots, y_K = M(x_1), M(x_2), \dots, M(x_K)\), where each response is generated from the corresponding input \(x_1, x_2, \dots, x_K\). \(M(x_i)\) may generate incorrect responses, which can sometimes cause \(\mathrm{Prompt}(y_{i+1})\) to mislead the in-context perplexity calculation. To mitigate this, we employ a filtering mechanism to remove incorrect \(M(x_i)\). According to our experimental results, since  \(y_{i+1}\) is primarily used to guide style, its correctness is less critical. Hence, the correctness filter is optional. For more details, please refer to Appendix~\ref{tab:correctness}.



    \item For each candidate response \(\hat{y}_i\), we assign in-context examples in a cyclic manner: the in-context example for the first input \(x_1\) is based on the second input \(x_2\)'s generated response, the in-context example for \(x_2\) is based on \(x_3\)'s response, and so on, until the in-context example for the last input \(x_K\) is based on \(x_1\)'s response. This way, the model focuses on imitating the inference style rather than directly copying the answers.

    \item Construct a prompt, \(\mathrm{Prompt}(y_i)\), containing both \(x_i\) and a directive such as:
    \begin{quote}
        \itshape
        ``Here is an example of how to reason and respond. Please strictly follow this style. If you drift, you must correct yourself mid-inference.''
    \end{quote}
    The detailed prompt can be found in Appendix~\ref{in_context_prompt}.
    
    \item Compute perplexity for a candidate \(\hat{y}_i\) with the model forced to follow the self-demonstrated style in \(y_{i+1}\) or \(y_1\).

\end{enumerate}

Formally, define:
{\small\[
  \varphi_{\texttt{ICPPL}}(M, x_i, \hat{y}_i) = \varphi_{\texttt{PPL}}(M, \mathrm{Prompt}(M(x_{i+1})), \hat{y}_i) \quad \text{if} \, i < K
\]
\[
  = \varphi_{\texttt{PPL}}(M, \mathrm{Prompt}(M(x_1)), \hat{y}_i) \quad \text{if} \, i = K
\]
}
In this way, if \(\hat{y}_i\) is fluent but diverges from \(M\)'s reasoning style, the perplexity will be higher. This makes self-aligned perplexity a more reliable measure of style mismatch compared to plain token-level perplexity.

\paragraph{Selection Criterion.}
For each strategy \(S_k\), we obtain \(\{\hat{y}_i^k\}\) from the small subset \(\mathcal{D}_s\) and average:
\[
  \pi_{\texttt{ICPPL}}(S_k)
  \;=\;
  \frac{1}{K}\sum_{i=1}^{K}
    \varphi_{\texttt{ICPPL}}(M, x_i, \hat{y}_i^k).
\]
We then select:
\[
    S_{\iota} 
    \;=\; 
    \arg\min_{k} 
    \;\pi_{\texttt{ICPPL}}(S_k).
\]

% Finally, we use the selected S_{\iota} to generate the entire 
% By aligning perplexity with the target model’s reasoning style, this criterion penalizes responses that may appear fluent but are not aligned with \(M\)'s actual reasoning style.




% Table~\ref{tab:Example_of_self_aligned_perplexity} presents examples where self-aligned perplexity selects a better strategy \(S_{\iota}\) than standard perplexity across three tasks (GSM8K, MATH, and API-Bank). In each case, standard perplexity ranks suboptimal strategies higher, whereas self-aligned perplexity favors the strategy that yields higher accuracy. For instance, on GSM8K, Mistral7B achieves its best accuracy when using training data generated by the step-by-step strategy, which is correctly favored by self-aligned perplexity but not by standard perplexity.


% \textbf{Limitations of Perplexity:} As shown in Table~\ref{tab:self-aligned perplexity push perplexity to the right direction}, when measuring the perplexity of responses on the Mistral7B model for the ECQA task, high-quality \textbf{\hyperref[prompt answer directly]{GPT-4o Answer Directly}} responses have a perplexity of 5.551. In contrast, responses constructed in a redundant style (see Appendix~\ref{appendix:redundant_prompt} for details) yield a significantly lower perplexity score of 4.944, even though this style of response is unlikely to be generated by the target LLM. 

% \textbf{In-Context Examples Modify Perplexity Score:} We show how in-context example can modify the perplexity score to better reflect how closely the style of the response aligns with the in-context example. 

% Table~\ref{tab:self-aligned perplexity push perplexity to the right direction} shows that if we add a style of response as in-context example, the example will modify the probability estimation of the target model such that \texttt{ICPPL} in favor of the target response with the same style, regardless of what the perplexity initially favor. 


% the original perplexity of GPT-4 Answer Directly (4.944) is higher than that of Step by Step (4.476). However, when we attach the question to the GPT-4 Answer Directly example while keeping the target response the same, the in-context perplexity of the step-by-step response becomes 4.85, which is higher than that of GPT-4 Answer Directly (4.768). This demonstrates that even though the initial perplexity of GPT-4 Answer Directly is higher than that of Step by Step, the in-context example modifies the perplexity such that when the target response adopts the same style as the in-context example, it achieves a lower perplexity. 

% Additionally, if we rank the two examples from Table~\ref{tab:self-aligned perplexity push perplexity to the right direction} using perplexity, the order would be Step by Step \textgreater Redundant Data \textgreater GPT-4 Answer Directly. However, Redundant Data is clearly the worst response option. If we add Step by Step or GPT-4 Answer Directly as in-context examples, as shown in the \texttt{IC}$_{\text{sbs}}$\texttt{PPL} and \texttt{IC}$_{\text{cad}}$\texttt{PPL} columns of Table~\ref{tab:self-aligned perplexity push perplexity to the right direction}, we observe that these in-context examples significantly increase the perplexity of the redundant response, making its perplexity much higher than the other two styles (the examples are highlighted in red in the table). The target LLM may try to imitate the normal GPT-4 or step-by-step style, which causes it to assign higher perplexity to the redundant response.








\subsection{Can we get performance gain if we simply put all of the response variants together?}



\begin{table}[t]
\centering
\small
% \resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|l|c|c|c}
\hline
\textbf{Method} & \textbf{Model} & \textbf{DROP} & \textbf{Hellaswag} &\textbf{API-Bank}  \\ \hline
Best $n_{\text{train}} = 1000$ & Mistral7B &0.743 & 0.675 & 0.559\\ 
Avg $n_{\text{train}} = 1000$ &  & 0.726 & 0.646 & 0.446 \\ 
Total $n_{\text{train}} = 6000$ & &0.740 & 0.738 & 0.555 \\ 
Mixture of good $n_{\text{train}} = 3000$ & &0.770 &  0.731& 0.555\\ 
Mixture of good $n_{\text{train}} = 1000$ & & 0.744& 0.686&0.535 \\
Average of all $n_{\text{train}} = 1000$ & & 0.711&0.686 &  0.433\\  
\hline
Best $n_{\text{train}} = 1000$& Llama3 &0.805  & 0.718 & 0.547\\
Avg $n_{\text{train}} = 1000$ &  & 0.778& 0.711 &0.392 \\ 
Total $n_{\text{train}} = 6000$ &  & 0.810 &0.738 & 0.490 \\ 
Mixture of good $n_{\text{train}} = 3000$ & &0.812 & 0.745 & 0.527\\ 
Mixture of good $n_{\text{train}} = 1000$ & & 0.804& 0.728& 0.490 \\  
Average of all $n_{\text{train}} = 1000$ & & 0.771 & 0.705& 0.457\\ 
% Mixture of best\&worst  $n_{\text{train}} = 2000$ & &  0.824& 0.742&  0.535\\
\hline

\hline
Best $n_{\text{train}} = 1000$& Qwen2.5 & 0.814  & 0.739 & 0.461\\  
Avg $n_{\text{train}} = 1000$ &  &0.804 & 0.719 & 0.413 \\ 
Total $n_{\text{train}} = 6000$ &  & 0.798 & 0.748& 0.584\\ 
Mixture of good $n_{\text{train}} = 3000$ & & 0.824& 0.738 & 0.584\\ 
Mixture of good $n_{\text{train}} = 1000$ & & 0.818& 0.742& 0.490 \\ 
Average of all $n_{\text{train}} = 1000$ & &  0.778& 0.712& 0.412\\  
% Mixture of best\&worst  $n_{\text{train}} = 2000$ & &  0.792&0.728 & 0.514\\
\hline

\end{tabular}
% }
\caption{\textbf{Best} represents the best data generation strategy for the task with the target model. \textbf{Total} combines all strategies, yielding $n_{\text{train}} = 6000$. \textbf{Mixture of good} ($n_{\text{train}} = 3000$) includes the top three strategies with 1000 samples each, while \textbf{Mixture of good} ($n_{\text{train}} = 1000$) has about 333 samples per strategy.}
\label{tab:best_data_vs_all_data}
\end{table}


Selecting the optimal data generation strategy remains essential, even when resources or funding are unlimited. As shown in Table~\ref{tab:best_data_vs_all_data}, simply combining six types of synthetic data (Total $n_{\text{train}} = 6000$) does not guarantee a performance gain over selecting the best synthetic training data. For example, after training the Llama3 model on API-Bank using all six types of synthetic data, the evaluation accuracy is only 49\%, much lower than when selecting the Claude Answer Directly data (54.7\%). Indeed, according to Table~\ref{tab:best_data_vs_all_data}, if we combine the mixture of the top three data generation strategies (Mixture of good $n_{\text{train}} = 3000$), the performance is almost always better than if we simply combine all of the data together (Total $n_{\text{train}} = 6000$). This underscores the importance of selecting data generation strategies, even if we can afford large-scale synthetic data generation and training.



% \subsection{Appendices}
\subsection{Spearman's Rank Correlation Coefficient}
\label{tab:appendix_spearman}
Spearman’s rank correlation coefficient (Spearman’s $\rho$) is a non-parametric measure of rank correlation. Unlike Pearson’s correlation, which assesses the strength of a linear relationship between two continuous variables, Spearman’s $\rho$ focuses on the monotonic ordering of items. This makes it robust to outliers and suitable for evaluating ranking tasks where exact values matter less than the relative order in which items appear.

Formally, given two sets of ranks \( R_1 \) and \( R_2 \) for the same set of \( n \) items, Spearman’s $\rho$ is defined as:

\[
\rho = 1 - \frac{6 \sum_{i=1}^n (R_{1,i} - R_{2,i})^2}{n(n^2 - 1)}
\]

Here, \( R_{1,i} \) and \( R_{2,i} \) represent the rank of the \( i \)-th item in each respective ranking. The numerator captures the sum of squared rank differences. If \( \rho \) approaches 1, the two rankings are highly positively correlated; if \( \rho \) nears 0, there is no monotonic relationship; and if \( \rho \) approaches -1, the two rankings are inversely related.

In our experiments, we use the gold-standard orderings provided by the datasets as \( R_1 \) and the predicted rankings generated by our model and the train-then-test baseline as \( R_2 \). By comparing these rankings via Spearman’s $\rho$, we directly evaluate how faithfully each method captures the underlying ordering of items, providing a valuable complement to other accuracy-based metrics.


\subsection{The Effectiveness of the Correctness Filter for Initial Predictions from the Target LLM}
\label{tab:correctness}
Before computing the semantic similarity between the initial prediction of the target LLM and the response from the teacher model, we apply a correctness filter(made by Qwen2.5. For more details, you can read the Appendix~\ref{sec:select_correct_examples}) to remove incorrect predictions from the target LLM. In other words, we only compare the semantic similarity between correct initial predictions from the target LLM and responses from the teacher model. For details about how we make the 

How effective is the correctness filter?

According to Table~\ref{tab:effectiveness_of_correctness_filter}, the correctness filter helps to improve both accuracy and weighted spearman correlation.




\begin{table}[t]
\centering
  \small 
% \resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|l|c|c}
\hline
Weighted \textbf{Method}         & \textbf{Model} & \textbf{Accuracy} & \textbf{Weighted Spearman's $\rho$} \\ \hline
Ours with filter                      & Mistral7B           &     0.582        &         0.411              \\
Ours without filter       &   &          0.574     &             0.364         \\ \hline

Ours with filter                       &     Llama3          &     0.650    &  0.344              \\
Ours without filter         &   &       0.651       &       0.303    \\ \hline

Ours with filter                   & Qwen2.5                  &    0.723          &   0.494                 \\
Ours without filter      &    &         0.714      &  0.432          \\ \hline

\end{tabular}
% }
\caption{Our Method with correctness filter($K=50$) vs. Our Method without correctness filter($K=50$) on tasks with PGR \textgreater 2\%}
\label{tab:effectiveness_of_correctness_filter}
\end{table}



\subsection{\textbf{The Impact of Accuracy of the Synthetic Data on Training Outcomes}}

In our experiment, we aim to ensure the correctness of generated answers by validating them against ground truth answers. Our research seeks to identify the best strategy for generating the optimal version of an answer. In other words, we can adjust data generation strategies to ensure correctness.

In our experiments, we use ground truth answers to guide the generated answers for nearly all datasets, with the only exceptions being mathematical problems. This follows the setting of the paper to maintain consistency with previous work \citep{ren2024learn}. This approach might be acceptable since closed-source APIs tend to generate accurate answers. For GSM8K and Math Algebra, GPT-4o, Claude, and MiniGPT-4o achieve accuracies of 90\% or above.

To evaluate the impact of accuracy on training outcomes, we conducted the following experiment. As shown in Table~\ref{tab:impact_accuracy}, we tested three approaches: training on the full dataset, using only correct predictions, and replacing incorrect predictions with rewritten ground truth. These approaches showed less than a 2\% improvement overall. Note that in this experiment, GPT-4 refers to the gpt-4-1106-preview API, rather than the gpt-4o-2024-08-06 API, which was used in all other experiments in the paper. The mathematical capabilities of GPT-4o, GPT-4-Mini, and Claude are similar on Math Algebra tasks. Therefore, we used the gpt-4-1106-preview API, which has a weaker ability to solve Math Algebra problems. The benifit of using it is that it makes more mistakes on GSM8K so that we can better evaluate the influence of accuracy. We used this API once to generate the data and train the model from there.

According to the table, the overall benefit of replacing incorrect examples with rewritten ground truth or removing incorrect examples has minimal impact on the overall training outcomes.



\subsection{\textbf{Data Selection Rationale for the Benchmark}}

The datasets included in our benchmark, drawn from the Mistral, Llama, and Qwen benchmarks, were selected according to a specific set of rules designed to ensure relevance and suitability.  These rules are as follows:

1.  Sufficient Dataset Size: We only included datasets where the combined size of the training, validation, and testing sets exceeded 650 samples. This threshold was chosen to ensure sufficient data for robust model evaluation.

2.  Accuracy as Evaluation Metric: A key requirement was that the dataset could be evaluated using accuracy as the primary metric. This allows for a clear and quantifiable assessment of model performance.

3.  English Question-Answering Format:  All selected datasets are in an English question-and-answer format to maintain consistency and focus on English language reasoning abilities.

4.  Focus on Reasoning Tasks: The underlying task presented by each dataset must involve reasoning skills.  This ensures that the benchmark effectively assesses the models' ability to reason and infer.

A detailed justification for the inclusion or exclusion of each dataset can be found in Table~\ref{tab:why_each_data_is_chosen}.





\subsection{Correctness Filter}\label{sec:select_correct_examples}

Without supervised fine-tuning (SFT), $M(x)$ may generate incorrect responses, making cosine similarity calculations between $M(x)$and $\hat{y}$ unreliable.
To alleviate this, we introduce a filtering mechanism to filter out the incorrect $M(x)$. We notice that for mathematical problems, the correct final answer typically appears as the last number in $M(x)$. Therefore, for Math-related tasks, we use regular expressions (regex) to extract the last number from the prediction and compare it directly with the ground truth. For other types of problem, we use the Qwen2.5-Instruct 7b model to extract the predicted label from the model output. We then compare this extracted label with the true gold label; if they match, we consider the prediction correct by default. The code we used to extract labels are detailed in Appendix~\ref{sect:filter_prompt}





\subsubsection*{Response Construction Details}
\label{appendix:Response Construction Details}

\textbf{Ground Truth}: This strategy uses the original ground-truth responses from the datasets as target outputs. Since our focus is on selecting effective chain-of-thought (CoT) target responses, we apply this method to datasets that include human-annotated CoT reasoning steps, such as GSM8K, MATH, ECQA, MBPP. When human-annotated CoT is unavailable, we use the gold label as ground truth.

\textbf{\hyperref[prompt answer directly]{GPT-4o Answer Directly}}, \textbf{\hyperref[prompt answer directly]{Claud Answer Directly}}, and \textbf{\hyperref[prompt answer directly]{MiniGPT-4o Answer Directly}} generate responses based on questions and the ground truth using GPT-4o, Claude 3.5 and Mini-GPT4, respectively. \textbf{\hyperref[prompt rewrite groundtruth]{Rewrite Ground Truth:}} Direct GPT-4o to restyle the ground truth in its own language. This method is only applicable to GSM8K, MATH Algebra, ECQA. The other tasks's ground truth consists of target labels without any human-annotated chain-of-thought (CoT) reasoning, making rewriting infeasible. \textbf{\hyperref[prompt step by step]{Step-by-Step:}} instructs GPT-4o to generate step-by-step responses based on questions and ground truth. \textbf{\hyperref[prompt gpt4o example]{GPT-4o Examples:}} To facilitate problem-solving, we provide GPT-4o with two high-quality, expert-selected in-context examples of its own responses. GPT-4o is then tasked with generating new responses based on these examples. \textbf{\hyperref[prompt human written example]{Human Examples:}} To aid GPT-4o in understanding problem-solving for these datasets, we provide two carefully chosen human-written examples as context. GPT-4o then uses these examples to generate new responses. We put more details in Section~\ref{sect:gpt4_varient_appendix} in Appendix.



\subsubsection*{Redundant Prompt}
\label{appendix:redundant_prompt}
We construct redundant prompts to demonstrate that the perplexity of the redundant target responses is lower than that of GPT-4's answers. Perplexity primarily reflects how fluent the language is and how well the language style aligns with the model, but it places less emphasis on semantic meaning.

\begin{lstlisting}[breaklines=true]
f"""We have the question and the groundtruth. Given on the groundtruth, please reformat the groundtruth so that it answer the question in a step by step redundant manner. Be as repetitive and step by step and redundant as possible.


Question: {question}
Groundtruth: {groundtruth}


1. We wish you to reformat a new groundtruth. The new groundtruth are reformated a new groundtruth which solve the problem as steo by step and redundant as possible.
2. You will pretend as you do not know the groundtruth, because we will use your step by step redundant answer as target responses to train our model.
3. (important format) You must generate the groundtruth with the step by step redundant inference process directly. Please not saying anything like 'sure I can help you with' or 'sure, i will not mention the gold label'
4. (important format) You will inference first then put the Final Answer: {gold_label}

at the end like this

INFERENCE HERE
Final Answer: {gold_label}
"""
\end{lstlisting}


\subsubsection*{Declaration of Independence}
\label{appendix:declaration_of_independence}
This is the part of Declaration of Independence that we use in the experiment in Table 4.
\begin{lstlisting}[breaklines=true]
f"""The Unanimous Declaration of the Thirteen United States of America . When, in the course of human events, it becomes necessary for one people to dissolve the political bonds which have connected them with another, and to assume among the powers of the earth, the separate and equal station to which the laws of nature and of nature\''s God entitle them, a decent respect to the opinions of mankind requires that they should declare the causes which impel them to the separation 

We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable rights, that among these are life, liberty and the pursuit of happiness. That to secure these rights, governments are instituted among men, deriving their just powers from the consent of the governed. That whenever any form of government becomes destructive to these ends, it is the right of the people to alter or to abolish it, and to institute new government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their safety and happiness."""
\end{lstlisting}





\textbf{Self-Aligned In-Context Prompt for Perplexity Calculation}\label{in_context_prompt}
This prompt shows how we add self - generated initial predictions from other questions as in - context examples for perplexity calculation.
\begin{lstlisting}[breaklines=true]
in_context_question = \
f"""Question: {original_question}

We have an inference example below to show you how to solve the problem. please follow the inference style and solve the problem

inference example: {initial_prediction_of_another_question}


now, according to the inference example, please solve the problem. 

IMPORTANT FORMAT REQUIREMENT: When you solve the problem, you need to make the problem solving process and language as similar to the inference example above as possible. If the inference process does not follow at the prediction before, you have to correct your style at anytime when you notice the style is not following the inference example. this is the most important requirement. please follow it.

"""
\end{lstlisting}




\subsubsection{Data Generation Strategies}\label{sect:gpt4_varient_appendix}
We instruct GPT-4, Claude 3.5, Mini-GPT4 to generate different of target responses using different target reponse generation strategies.

\textbf{GPT-4/Claude 3.5/Mini-GPT4 Answer Directly:}\label{prompt answer directly} This prompt is from \citep{ren2024learn}. For tasks involving mathematics and coding, we submit the problems from our training dataset directly to GPT-4 or Claude 3.5 to obtain their solutions. In the case of classification tasks, we provide these models with the input questions alongside the correct labels (excluding any human-generated explanations) and utilize their outputs. These generated answers are then paired with the original questions to form the GPT-4/Claude 3.5 Direct Answer Training Dataset.

To ensure that the models develop their own problem-solving and analytical capabilities, we deliberately exclude any solutions or rationales related to math, coding, or classification tasks. This approach prevents the models from simply mimicking the ground truth processes, which could otherwise result in some of GPT-4’s predictions lacking its unique reasoning style. Such mimicry would undermine the reliability of our perplexity measurements, which are designed to evaluate how effectively a language model handles outputs from other models.

The prompt below is designed to guide GPT-4/Claude 3.5 in generating responses without relying on the ground truth solutions:

\begin{lstlisting}[breaklines=true]
"""We have the {question} 


1. We wish you to answer the question.
2. You must answer the question (with inference process) directly without say anything else. Please not saying anything 'like sure I can help you with' or 'sure, i will not mention the gold label'
3. You will inference first then put the Final Answer (NUMBER_HERE) at the end of the prediction like this

INFERENCE HERE
Final Answer: NUMBER_HERE""" 
\end{lstlisting}

\textbf{Rewrite Ground Truth:}\label{prompt rewrite groundtruth} This prompt is from \citep{ren2024learn}. In this approach, we provide GPT-4 and Claude 3.5 with the ground truth data, which includes human-annotated rationales and detailed problem-solving steps. The goal is to have GPT-4 and Claude 3.5 rephrase the ground truth content using their own linguistic styles.

The subsequent prompt guides GPT-4 and Claude 3.5 to generate the GPT-4/Claude 3.5 Response (Rewrite GT) output.


\begin{lstlisting}[breaklines=true]
"""Given the question: {question} 
and the groundtruth: {groundtruth}

Please states the prediction in your own words. The groundtruth is 100% correct. You should not change the problem solving logic of the groundtruth. just restates it in your own words.

1. You will pretend as you do not know the groundtruth, because we will use your prediction as target labels to train our model.
2. (important format) You must generate the groundtruth directly. Please not saying anything like 'sure I can help you with' or 'sure, i will not mention the gold label'
3. (important format) Please make sure the Final Answer: {gold_label} is placed at the end of the modified prediction."""
\end{lstlisting}

\textbf{Step-by-step:}\label{prompt step by step} This prompt is from \citep{ren2024learn}. We instruct GPT-4 and Claude 3.5 to methodically address each problem by breaking it down into sequential steps. For tasks involving mathematics and coding, we present the problems directly from our training dataset to these models to obtain their solutions. In classification tasks, we provide GPT-4 and Claude 3.5 with the correct labels (excluding any human-generated explanations) along with the input questions, and then utilize their detailed, step-by-step responses. These generated answers are subsequently paired with the original questions to form the GPT-4/Claude 3.5 Step-by-Step Response (No GT) Dataset.

To ensure that the models develop their own unique problem-solving and analytical approaches, we intentionally exclude the solutions or rationales for the mathematics, coding, or classification tasks. This prevents the models from simply mimicking the problem-solving and analytical methods found in the ground truth data. Including such processes could result in some of GPT-4’s and Claude 3.5’s outputs not reflecting their inherent reasoning styles, thereby compromising the accuracy of our perplexity measurements. These measurements are designed to assess how effectively a language model can handle outputs generated by other language models.

The following prompt directs GPT-4 and Claude 3.5 to generate the GPT-4/Claude 3.5 Step-by-Step Response (No GT) responses.
\begin{lstlisting}[breaklines=true]
"""
We have the question and the groundtruth. Please reformat the groundtruth in step by step manner with details.

Question: {question}
Groundtruth: {groundtruth}

1. We wish you to regenerate a new groundtruth. The new groundtruth solve the problem step by step. If you believe the groundtruth is not detail enough, you could add details.
2. You will pretend as you do not know the groundtruth, because we will use your prediction as target labels to train our model.
3. (important format) You must generate the groundtruth with the step by step inference process directly. Please not saying anything like 'sure I can help you with' or 'sure, i will not mention the gold label'
4. (important format) You will inference first then put the Final Answer: {gold_label}

at the end like this

INFERENCE HERE
Final Answer: {gold_label}
"""
\end{lstlisting}










\textbf{GPT-4o with GPT-4o Examples:} \label{prompt gpt4o example} We developed this prompt specifically for the API-Bank and Plan-Bench datasets. This prompt utilizes GPT-4's own accurate generations as examples to help GPT-4 not only better understand the task but also demonstrate how to solve the problems effectively. The prompt below is an example that we used to generate target responses for the API-Bank dataset.

\begin{lstlisting}[breaklines=true]
"""
We have the {question} and the groundtruth {gold_label}


1. We wish you to answer the question. We will use your answer to train our model, thus you will answer and pretend as not knowing the gold_label.
2. You must answer the question (with inference process) directly without say anything else. Please not saying anything 'like sure I can help you with' or 'sure, i will not mention the gold label'
3. You will inference first then put the Final Answer ({gold_label}) at the end of the prediction like this

INFERENCE HERE
Final Answer: {gold_label}

Example 1:

Question : {q1}

groundtruth: API-Request: [ToolSearcher(keywords='healthcare provider appointment availability checker')]

Inference: The user is requesting to find a healthcare provider (specifically a cardiologist) in Los Angeles for a check-up appointment. The available API description indicates that the ToolSearcher API can be used to search for relevant tools based on the provided keywords. Therefore, the first step is to search for a tool that can help find a healthcare provider appointment availability checker.

Final Answer: API-Request: [ToolSearcher(keywords='healthcare provider appointment availability checker')]


Example 2:

question: {q2}

groundtruth: API-Request: [HealthcareProviderAppointmentChecker(specialty='cardiologist', location='Los Angeles')]

Inference: The first API request was successfully made to find a tool for checking healthcare provider appointment availability. The HealthcareProviderAppointmentChecker API was identified, which requires specialty and location as input parameters to search for available appointment slots. Based on the user's request to find a cardiologist in Los Angeles for a check-up appointment, the next API call should use this information.

Final Answer: API-Request: [HealthcareProviderAppointmentChecker(specialty='cardiologist', location='Los Angeles')]


Example 3:

question: {q3}

groundtruth: API-Request: [ToolSearcher(keywords='healthcare provider appointment scheduler')]

Inference: The user initially searched for an availability checker and found available appointment slots for a cardiologist in Los Angeles. Now, the user needs to schedule an appointment, so the next step is to find a tool for scheduling healthcare provider appointments using the ToolSearcher API with relevant keywords.

Final Answer: API-Request: [ToolSearcher(keywords='healthcare provider appointment scheduler')]

We have the {question} and the groundtruth {gold_label}


1. We wish you to answer the question. We will use your answer to train our model, thus you will answer and pretend as not knowing the gold_label.
2. You must answer the question (with inference process) directly without say anything else. Please not saying anything 'like sure I can help you with' or 'sure, i will not mention the gold label'
3. You will inference first then put the Final Answer ({gold_label}) at the end of the prediction like this

INFERENCE HERE
Final Answer: {gold_label}

"""
\end{lstlisting}








\textbf{GPT-4 with Human Written Examples:}\label{prompt human written example} We developed this prompt specifically for the API-Bank and Plan-Bench datasets. This prompt utilizes human written examples to help GPT-4 not only better understand the task but also demonstrate how to solve the problems effectively. The prompt below is an example that we used to generate target responses for the API-Bank dataset.


\begin{lstlisting}[breaklines=true]
"""
We have the {question} and the groundtruth {gold_label}


1. We wish you to answer the question. We will use your answer to train our model, thus you will answer and pretend as not knowing the gold_label.
2. You must answer the question (with inference process) directly without say anything else. Please not saying anything 'like sure I can help you with' or 'sure, i will not mention the gold label'
3. You will inference first then put the Final Answer ({gold_label}) at the end of the prediction like this

INFERENCE HERE
Final Answer: {gold_label}

Example 1:

Question : {q1}

groundtruth: API-Request: [ToolSearcher(keywords='healthcare provider appointment availability checker')]

Inference: The user is requesting to find a healthcare provider (specifically a cardiologist) in Los Angeles for a check-up appointment. The first step should be to search for a tool that can help find a healthcare provider appointment availability checker. To accomplish this, we choose the ToolSearcher API from the available APIs. The ToolSearcher API is used to search for relevant tools based on the provided keywords according to the description. We need to fill out the keywords according to the description. The keywords could be 'healthcare provider appointment availability checker.' Therefore, the next step (which is also the first step) is:

Final Answer: API-Request: [ToolSearcher(keywords='healthcare provider appointment availability checker')]



Example 2:

question: {q2}

groundtruth: API-Request: [HealthcareProviderAppointmentChecker(specialty='cardiologist', location='Los Angeles')]

Inference: According to the API call history, the user has called the ToolSearcher API and found the HealthcareProviderAppointmentChecker API. The next step is to fill out the input parameters for HealthcareProviderAppointmentChecker and use it to find healthcare provider appointment availability. The input parameters are specialty and location. The user wants to find a cardiologist in Los Angeles for a check-up appointment. Therefore, the next API request should be:

Final Answer: API-Request: [HealthcareProviderAppointmentChecker(specialty='cardiologist', location='Los Angeles')]


Example 3:

question: {q3}

groundtruth: API-Request: [ToolSearcher(keywords='healthcare provider appointment scheduler')]

Inference: The user previously called the HealthcareProviderAppointmentChecker API and found three appointment times, which are '2034-04-18 14:30:00', '2034-04-19 11:00:00', and '2034-04-20 09:45:00'. The next step is to find the scheduler for the appointment. Since there is no available tool, the user needs to search for a tool that can schedule healthcare provider appointments. The ToolSearcher API can be used to search for relevant tools based on the keywords according to the description. The keywords should be 'healthcare provider appointment scheduler'. Therefore, the answer is:

Final Answer: API-Request: [ToolSearcher(keywords='healthcare provider appointment scheduler')]




We have a question and a groundtruth 

question: {question}

groundtruth: {gold_label}


1. We wish you to answer the question. We will use your answer to train our model, thus you will answer and pretend as not knowing the gold_label.
2. You must answer the question (with inference process) directly without say anything else. Please not saying anything 'like sure I can help you with' or 'sure, i will not mention the gold label'
3. You will inference first then put the Final Answer ({gold_label}) at the end of the prediction like this

INFERENCE HERE
Final Answer: {gold_label}

"""
\end{lstlisting}




\subsection{Prompt Used to Extract Labels from Predictions}\label{sect:filter_prompt}
The code below shows how we use Qwen-2.5-Instruct to extract the predicted labels from the predictions.

\begin{lstlisting}[breaklines=true]
if 'arc_challenge' in task_name or 'mmlu' in task_name or 'agieval' in task_name:
        gold_label_type = 'A/B/C/D'
    elif 'piqa' in task_name or 'winogrande' in task_name:
        gold_label_type = '1/2'
    elif 'squad' in task_name:
        gold_label_type = 'text_span'
    elif 'gsm8k' in task_name or 'math' in task_name:
        gold_label_type = 'number'
    elif 'ecqa' in task_name:
        gold_label_type = '1/2/3/4/5'
    elif 'esnli' in task_name:
        gold_label_type = 'Entailment/Neutral/Contradiction'
    elif 'boolq' in task_name:
        gold_label_type = 'True/False'
    elif 'mmlu_pro' in task_name:
        gold_label_type = 'A/B/C/D/E/F/G/H/I/J'
    elif 'hellaswag' in task_name:
        gold_label_type = '1/2/3/4'
    elif 'drop' in task_name:
        gold_label_type = 'number_or_text_span'
    elif 'api_bank' in task_name:
        gold_label_type = 'API-request'
    elif 'plan_bench' in task_name:
        gold_label_type = '[PLAN]SOME_PLAN_HERE[PLAN END]'
    else:
        a = 1

    for i in range(len(question_list)):
        question = \
f"""Given the prediction, what is the final answer by the prediction? 

The prediction is "{predict_list[i]}"

Directly output {gold_label_type} without saying anything else.
"""
\end{lstlisting}






\subsection{AI writing assistant}
We use GPT-4o for writing assistant.









\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l|c|c|c|c|c|c|c|c|}
\hline

Method & Model Type & training task& GSM8K & Math Algebra  & ECQA &SQUAD & DROP & WINOGRANDE \\ \hline

Gold Label & Mistral& ECQA& 0.383 &0.181  &  \cellcolor{gray!50}0.722 & 0.251 & 0.084&  0.562 \\
GPT-4o Answer Directly &  &  & 0.484          & 0.218 & \cellcolor{gray!50} 0.707 & 0.175& 0.016 & 0.638  \\
\hline
Gold Label & Mistral& SQUAD& 0.082 & 0.0931 &  0.633 & \cellcolor{gray!50} 0.74 &0.208 & 0.566 \\
GPT-4o Answer Directly& & &0.512  & 0.234 &  0.594 & \cellcolor{gray!50}0.748 & 0.268&  0.628 \\
\hline
Gold Label & Mistral& DROP& 0.076 &0.097 & 0.621  &  0.561&\cellcolor{gray!50} 0.628 & 0.578 \\
GPT-4o Answer Directly &      &       & 0.542 & 0.241 &0.602  & 0.546 & \cellcolor{gray!50} 0.736 & 0.638 \\
\hline
Gold Label & Mistral& WINOGRANDE& 0.381 & 0.172 & 0.625  & 0.166 & 0.042 &\cellcolor{gray!50} 0.742 \\
GPT-4o Answer Directly &      &        & 0.477 & 0.219 & 0.569  & 0.106 & 0.016& \cellcolor{gray!50}0.713 \\

\hline
\hline
\hline

Gold Label & LLAMA3& ECQA& 0.798 & 0.416&  \cellcolor{gray!50}0.734 & 0.193 &0.1 &  0.637\\
GPT-4o Answer Directly & &  &  0.778           &0.469  & \cellcolor{gray!50} 0.723 &0.389  & 0.284 &0.638 \\
\hline
Gold Label & LLAMA3& SQUAD& 0.584 & 0.366 &  0.712 & \cellcolor{gray!50} 0.758& 0.49&0.639  \\
GPT-4o Answer Directly &      &         & 0.791 & 0.457 &  0.726 & \cellcolor{gray!50}0.759 & 0.368& 0.651 \\
\hline
Gold Label & LLAMA3& DROP& 0.144 & 0.169 &  0.674 &  0.574&\cellcolor{gray!50} 0.738 & 0.582  \\
GPT-4o Answer Directly  &     &        &  0.776& 0.507 & 0.703 & 0.555 & \cellcolor{gray!50} 0.786 & 0.626 \\
\hline
Gold Label & LLAMA3& WINOGRANDE&  0.776& 0.445 &  0.717 &  0.226& 0.162&\cellcolor{gray!50} 0.766 \\
GPT-4o Answer Directly &        &      & 0.775 & 0.485 &  0.721 & 0.305 & 0.238 & \cellcolor{gray!50} 0.695 \\


\hline
\hline
\hline

Gold Label & Qwen& ECQA&0.914  & 0.903 &  \cellcolor{gray!50}0.814 &   0.662& 0.008 & 0.675  \\
GPT-4o Answer Directly& &  &    0.903         &  0.888& \cellcolor{gray!50} 0.793 & 0.668 & 0.016& 0.716 \\
\hline
Gold Label & Qwen& SQUAD & 0.899 &  0.892 &  0.784& \cellcolor{gray!50} 0.768 & 0.056 & 0.693  \\
GPT-4o Answer Directly &     &          &0.896  &  0.911&  0.789 & \cellcolor{gray!50} 0.756 & 0.074&0.712   \\
\hline
Gold Label & Qwen& DROP& 0.788 &0.904 & 0.799  &  0.701&\cellcolor{gray!50}0.664  & 0.711  \\
GPT-4o Answer Directly &     &        & 0.911 & 0.903 & 0.792 & 0.741 & \cellcolor{gray!50} 0.806 & 0.701 \\
\hline
Gold Label & Qwen& WINOGRANDE & 0.893 & 0.904  & 0.78 & 0.651 & 0.004&\cellcolor{gray!50} 0.725 \\
GPT-4o Answer Directly&       &       & 0.902 & 0.896 & 0.798  &  0.68&0.022 & \cellcolor{gray!50} 0.721 \\
\hline

\hline




\end{tabular}

}
\caption{The training data size is 1000. This table compares the in-domain and cross-domain performance when training on gold-label vs. GPT-4 generated synthetic data. As can be seen from the table, the in-domain performance of the model is typically higher when training with gold-label data. However, the cross-domain performance when training on GPT-4 generated data is significantly higher than when training with only gold-label data. The grey area represents the in-domain performance.}

\label{table:cross_domain_performance_recording}

\end{table*}










\begin{table*}[t!]
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|l|c|c|c}
\hline
Dataset&\textbf{Method} & \textbf{Accuracy and N train} & \textbf{Mistral} &\textbf{Llama3-8B-Chat} \\ \hline 
\textbf{MATH Algebra} &GPT4 preview&  82.5\%, 1000&   0.301& 0.504\\ 
 &GPT4 only correct & 100\%, 825&0.293 &  0.501\\ 
 &GPT4 only correct + rewritten ground truth &    100\%, 1000 & 0.293 & 0.500\\ 
\hline
\textbf{MATH Algebra} &Claude& 90.1\%, 1000&  0.265&  0.508\\ 
 &Claude only correct &  100\%, 901 &  0.277&  0.487\\ 
 &Claude only correct + rewritten ground truth &     100\%, 1000& 0.286& 0.492\\ 
\hline
\textbf{MATH Algebra} &Mini GPT4& 91.6\%  , 1000   & 0.313 &0.523\\ 
 &Mini GPT4 only correct &   100\%, 916 & 0.311 & o.523\\ 
 &Mini GPT4 only correct + rewritten ground truth &   100\%, 1000 & 0.326& 0.539\\ 
\hline

\hline
 
\textbf{GSM8K} &GPT4 preview&  92.1\%, 1000&   0.597& 0.799\\ 
 &GPT4 only correct & 100\%, 921& 0.587& 0.791 \\ 
 &GPT4 only correct + rewritten ground truth &    100\%, 1000 & 0.588& 0.808\\ 
\hline
\textbf{GSM8K} &Claude& 95.6\%, 1000& 0.578 & 0.796 \\ 
&Claude only correct &  100\%, 956 & 0.580 & 0.797  \\ 
 &Claude only correct + rewritten ground truth &     100\%, 1000& 0.588&0.798 \\ 
\hline
\textbf{GSM8K} &Mini GPT4& 89.8\%  , 1000   & 0.623 & 0.795\\ 
 &Mini GPT4 only correct &   100\%, 898  & 0.606 & 0.793\\ 
 &Mini GPT4 only correct + rewritten ground truth &   100\%, 1000 &0.607 &0.790 \\ 
\hline
\end{tabular}
}

\caption{The table shows that the accuracy of the generated data has a marginal effect on the training outcome. In this table, we use the API with different math abilities. The rank of their math problem-solving abilities is: Claude \textgreater MiniGPT-4 \textgreater GPT-4 preview. GPT-4 preview represents the data generated using the GPT-4 preview model, rather than the GPT-4o model.}
\label{tab:impact_accuracy}
\end{table*}




\begin{table*}[t!]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
Data Generation Strategy & Model Type & gsm8k & math algebra & math geometry & ecqa & boolq & winogrande & piqa & agieval & squad & arc challenge & drop & mbpp & api bank & hellaswag & mmlu pro law & mmlu moral scenarios \\ \hline
gold label & mistral &  &  &  & 0.717 & 0.997 & 0.736 & 0.854 & 0.44 & 0.741 & 0.747 & 0.645 &  & 0.452 & 0.772 & 0.263 & 0.679\\
groundtruth &  & 0.442 & 0.194 & 0.125 & 0.684 &  &  &  &  &  &  &  & 0.325 &  &  &  & \\
gpt4 &  & 0.62 & 0.324 & 0.146 & 0.703 & 0.87 & 0.717 & 0.864 & 0.41 & 0.732 & 0.631 & 0.723 & 0.362 & 0.515 & 0.659 & 0.238 & 0.691\\
claude &  & 0.582 & 0.278 & 0.136 & 0.735 & 0.885 & 0.724 & 0.848 & 0.445 & 0.736 & 0.753 & 0.729 & 0.379 & 0.579 & 0.553 & 0.248 & 0.751\\
mini gpt4 &  & 0.619 & 0.306 & 0.151 & 0.708 & 0.882 & 0.695 & 0.868 & 0.427 & 0.732 & 0.772 & 0.735 & 0.348 & 0.43 & 0.663 & 0.205 & 0.659\\
step by step &  & 0.626 & 0.314 & 0.137 & 0.706 & 0.874 & 0.693 & 0.862 & 0.445 & 0.749 & 0.71 & 0.696 & 0.333 & 0.377 & 0.644 & 0.249 & 0.714\\
openai human written examples &  & 0.621 & 0.303 & 0.163 & 0.708 & 0.891 & 0.721 & 0.859 & 0.413 & 0.76 & 0.692 & 0.741 & 0.345 & 0.411 & 0.674 & 0.233 & 0.71\\
gpt4 style in context examples &  & 0.61 & 0.254 & 0.158 & 0.726 & 0.884 & 0.727 & 0.868 & 0.44 & 0.761 & 0.697 & 0.735 & 0.378 & 0.416 & 0.672 & 0.225 & 0.728\\
rewrite groundtruth in own words &  & 0.502 & 0.238 & 0.127 & 0.703 &  &  &  &  &  &  &  & 0.306 &  &  &  & \\
\hline\hline
gold label & llama 3 instruct &  &  &  & 0.737 & 0.979 & 0.761 & 0.852 & 0.432 & 0.756 & 0.766 & 0.742 &  & 0.507 & 0.772 & 0.332 & 0.639\\
groundtruth &  & 0.678 & 0.404 & 0.239 & 0.701 &  &  &  &  &  &  &  & 0.445 &  &  &  & \\
gpt4 &  & 0.816 & 0.559 & 0.301 & 0.74 & 0.87 & 0.697 & 0.866 & 0.448 & 0.759 & 0.806 & 0.793 & 0.482 & 0.477 & 0.712 & 0.247 & 0.659\\
claude &  & 0.803 & 0.5 & 0.254 & 0.756 & 0.865 & 0.72 & 0.86 & 0.445 & 0.765 & 0.801 & 0.757 & 0.471 & 0.547 & 0.709 & 0.259 & 0.737\\
mini gpt4 &  & 0.805 & 0.551 & 0.28 & 0.721 & 0.864 & 0.677 & 0.868 & 0.437 & 0.747 & 0.816 & 0.783 & 0.491 & 0.384 & 0.719 & 0.225 & 0.645\\
step by step &  & 0.797 & 0.562 & 0.26 & 0.731 & 0.869 & 0.72 & 0.853 & 0.433 & 0.779 & 0.792 & 0.78 & 0.455 & 0.227 & 0.71 & 0.242 & 0.684\\
openai human written examples &  & 0.81 & 0.547 & 0.283 & 0.735 & 0.893 & 0.717 & 0.867 & 0.44 & 0.766 & 0.804 & 0.807 & 0.477 & 0.347 & 0.706 & 0.229 & 0.667\\
gpt4 style in context examples &  & 0.796 & 0.494 & 0.285 & 0.736 & 0.885 & 0.719 & 0.87 & 0.447 & 0.752 & 0.811 & 0.794 & 0.47 & 0.368 & 0.721 & 0.259 & 0.681\\
rewrite groundtruth in own words &  & 0.742 & 0.444 & 0.241 & 0.727 &  &  &  &  &  &  &  & 0.437 &  &  &  & \\
\hline\hline
gold label & qwen &  &  &  & 0.816 & 0.892 & 0.732 & 0.867 & 0.48 & 0.77 & 0.855 & 0.663 &  & 0.515 & 0.74 & 0.303 & 0.605\\
groundtruth &  & 0.899 & 0.894 & 0.667 & 0.793 &  &  &  &  &  &  &  & 0.59 &  &  &  & \\
gpt4 &  & 0.897 & 0.916 & 0.679 & 0.794 & 0.858 & 0.709 & 0.878 & 0.552 & 0.76 & 0.886 & 0.798 & 0.591 & 0.436 & 0.722 & 0.3 & 0.656\\
claude &  & 0.895 & 0.904 & 0.648 & 0.788 & 0.862 & 0.72 & 0.88 & 0.553 & 0.766 & 0.874 & 0.793 & 0.607 & 0.462 & 0.72 & 0.309 & 0.66\\
mini gpt4 &  & 0.904 & 0.904 & 0.654 & 0.787 & 0.87 & 0.712 & 0.882 & 0.555 & 0.763 & 0.891 & 0.821 & 0.642 & 0.379 & 0.701 & 0.308 & 0.664\\
step by step &  & 0.899 & 0.907 & 0.642 & 0.792 & 0.859 & 0.716 & 0.88 & 0.548 & 0.767 & 0.881 & 0.806 & 0.623 & 0.417 & 0.713 & 0.287 & 0.662\\
openai human written examples &  & 0.905 & 0.909 & 0.647 & 0.787 & 0.871 & 0.697 & 0.883 & 0.547 & 0.794 & 0.883 & 0.82 & 0.628 & 0.458 & 0.724 & 0.283 & 0.608\\
gpt4 style in context examples &  & 0.899 & 0.903 & 0.657 & 0.803 & 0.88 & 0.731 & 0.878 & 0.57 & 0.785 & 0.868 & 0.809 & 0.631 & 0.309 & 0.742 & 0.3 & 0.642\\
rewrite groundtruth in own words &  & 0.902 & 0.904 & 0.654 & 0.787 &  &  &  &  &  &  &  & 0.589 &  &  &  & \\
\hline
\end{tabular}}
\caption{average of seed 0,1,2 train datasize 1000 lr 2e-05 epoch num 20}
\label{tab:ntrain_1000_lr_2e-05_seedaverage_of_seed_0,1,2}
\end{table*}


\begin{table*}[t!]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
Data Generation Strategy & Model Type & gsm8k & math algebra & math geometry & ecqa & boolq & winogrande & piqa & agieval & squad & arc challenge & drop & mbpp & api bank & hellaswag & mmlu pro law & mmlu moral scenarios \\ \hline
gold label & mistral &  &  &  & 0.722 & 0.996 & 0.742 & 0.852 & 0.440 & 0.748 & 0.759 & 0.628 &  & 0.465 & 0.771 & 0.252 & 0.650\\
groundtruth &  & 0.440 & 0.201 & 0.110 & 0.672 &  &  &  &  &  &  &  & 0.370 &  &  &  & \\
gpt4 &  & 0.625 & 0.319 & 0.177 & 0.700 & 0.867 & 0.713 & 0.869 & 0.400 & 0.732 & 0.611 & 0.746 & 0.347 & 0.510 & 0.654 & 0.229 & 0.713\\
claude &  & 0.583 & 0.279 & 0.160 & 0.720 & 0.886 & 0.709 & 0.849 & 0.425 & 0.728 & 0.732 & 0.726 & 0.403 & 0.584 & 0.549 & 0.219 & 0.760\\
mini gpt4 &  & 0.627 & 0.291 & 0.148 & 0.710 & 0.873 & 0.688 & 0.877 & 0.420 & 0.740 & 0.775 & 0.726 & 0.363 & 0.433 & 0.663 & 0.183 & 0.643\\
step by step &  & 0.639 & 0.323 & 0.127 & 0.705 & 0.885 & 0.687 & 0.861 & 0.445 & 0.752 & 0.708 & 0.676 & 0.340 & 0.478 & 0.639 & 0.196 & 0.723\\
openai human written examples &  & 0.604 & 0.306 & 0.160 & 0.709 & 0.897 & 0.718 & 0.869 & 0.420 & 0.756 & 0.685 & 0.742 & 0.350 & 0.400 & 0.664 & 0.196 & 0.717\\
gpt4 style in context examples &  & 0.619 & 0.231 & 0.169 & 0.725 & 0.887 & 0.732 & 0.879 & 0.430 & 0.764 & 0.678 & 0.732 & 0.373 & 0.433 & 0.687 & 0.223 & 0.710\\
rewrite groundtruth in own words &  & 0.511 & 0.231 & 0.127 & 0.709 &  &  &  &  &  &  &  & 0.323 &  &  &  & \\
\hline\hline
gold label & llama 3 instruct &  &  &  & 0.734 & 0.978 & 0.766 & 0.855 & 0.435 & 0.761 & 0.764 & 0.738 &  & 0.502 & 0.777 & 0.312 & 0.630\\
groundtruth &  & 0.681 & 0.396 & 0.215 & 0.691 &  &  &  &  &  &  &  & 0.450 &  &  &  & \\
gpt4 &  & 0.814 & 0.562 & 0.278 & 0.723 & 0.880 & 0.695 & 0.865 & 0.435 & 0.752 & 0.801 & 0.796 & 0.480 & 0.494 & 0.722 & 0.276 & 0.677\\
claude &  & 0.816 & 0.493 & 0.253 & 0.748 & 0.879 & 0.728 & 0.864 & 0.455 & 0.763 & 0.808 & 0.746 & 0.500 & 0.547 & 0.710 & 0.286 & 0.757\\
mini gpt4 &  & 0.795 & 0.557 & 0.278 & 0.725 & 0.867 & 0.702 & 0.863 & 0.450 & 0.739 & 0.826 & 0.730 & 0.500 & 0.384 & 0.703 & 0.223 & 0.670\\
step by step &  & 0.798 & 0.564 & 0.308 & 0.728 & 0.874 & 0.718 & 0.866 & 0.460 & 0.783 & 0.792 & 0.780 & 0.450 & 0.216 & 0.715 & 0.229 & 0.657\\
openai human written examples &  & 0.811 & 0.547 & 0.266 & 0.736 & 0.891 & 0.719 & 0.864 & 0.450 & 0.770 & 0.809 & 0.808 & 0.457 & 0.355 & 0.699 & 0.269 & 0.640\\
gpt4 style in context examples &  & 0.792 & 0.515 & 0.274 & 0.742 & 0.875 & 0.717 & 0.854 & 0.460 & 0.755 & 0.809 & 0.798 & 0.483 & 0.273 & 0.718 & 0.219 & 0.683\\
rewrite groundtruth in own words &  & 0.729 & 0.443 & 0.241 & 0.715 &  &  &  &  &  &  &  & 0.417 &  &  &  & \\
\hline\hline
gold label & qwen &  &  &  & 0.814 & 0.880 & 0.725 & 0.868 & 0.500 & 0.769 & 0.856 & 0.652 &  & 0.518 & 0.747 & 0.296 & 0.590\\
groundtruth &  & 0.906 & 0.898 & 0.675 & 0.784 &  &  &  &  &  &  &  & 0.610 &  &  &  & \\
gpt4 &  & 0.889 & 0.916 & 0.658 & 0.793 & 0.865 & 0.721 & 0.879 & 0.545 & 0.762 & 0.890 & 0.794 & 0.607 & 0.433 & 0.706 & 0.276 & 0.633\\
claude &  & 0.884 & 0.906 & 0.662 & 0.796 & 0.873 & 0.716 & 0.885 & 0.550 & 0.767 & 0.867 & 0.798 & 0.600 & 0.457 & 0.717 & 0.322 & 0.667\\
mini gpt4 &  & 0.905 & 0.904 & 0.654 & 0.782 & 0.865 & 0.704 & 0.881 & 0.535 & 0.760 & 0.891 & 0.818 & 0.633 & 0.396 & 0.704 & 0.299 & 0.657\\
step by step &  & 0.899 & 0.908 & 0.624 & 0.795 & 0.846 & 0.703 & 0.874 & 0.545 & 0.752 & 0.882 & 0.766 & 0.630 & 0.412 & 0.717 & 0.276 & 0.653\\
openai human written examples &  & 0.907 & 0.910 & 0.658 & 0.790 & 0.876 & 0.699 & 0.884 & 0.540 & 0.808 & 0.881 & 0.816 & 0.617 & 0.445 & 0.731 & 0.286 & 0.583\\
gpt4 style in context examples &  & 0.896 & 0.902 & 0.654 & 0.799 & 0.883 & 0.734 & 0.871 & 0.540 & 0.782 & 0.863 & 0.800 & 0.607 & 0.339 & 0.742 & 0.302 & 0.653\\
rewrite groundtruth in own words &  & 0.911 & 0.899 & 0.654 & 0.791 &  &  &  &  &  &  &  & 0.587 &  &  &  & \\
\hline
\end{tabular}}
\caption{seed 0 train datasize 1000 lr 2e-05 epoch num 20}
\label{tab:ntrain_1000_lr_2e-05_seed0}
\end{table*}



\begin{table*}[t!]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
Data Generation Strategy & Model Type & gsm8k & math algebra & math geometry & ecqa & boolq & winogrande & piqa & agieval & squad & arc challenge & drop & mbpp & api bank & hellaswag & mmlu pro law & mmlu moral scenarios \\ \hline
gold label & mistral &  &  &  & 0.714 & 0.997 & 0.733 & 0.855 &  & 0.738 & 0.741 & 0.654 &  & 0.445 & 0.772 & 0.269 & 0.693\\
groundtruth &  & 0.443 & 0.191 & 0.131 & 0.690 &  &  &  &  &  &  &  & 0.303 &  &  &  & \\
gpt4 &  & 0.617 & 0.327 & 0.148 & 0.704 & 0.872 & 0.719 & 0.861 & 0.415 & 0.732 & 0.641 & 0.712 & 0.370 & 0.518 & 0.662 & 0.243 & 0.680\\
claude &  & 0.581 & 0.277 & 0.143 & 0.742 & 0.885 & 0.731 & 0.847 & 0.455 & 0.740 & 0.764 & 0.730 & 0.367 & 0.576 & 0.555 & 0.262 & 0.747\\
mini gpt4 &  & 0.615 & 0.314 & 0.148 & 0.707 & 0.886 & 0.698 & 0.863 & 0.430 & 0.728 & 0.771 & 0.740 & 0.340 & 0.429 & 0.663 & 0.216 & 0.667\\
step by step &  & 0.619 & 0.309 & 0.131 & 0.707 & 0.868 & 0.696 & 0.862 & 0.445 & 0.748 & 0.711 & 0.706 & 0.330 & 0.327 & 0.646 & 0.276 & 0.710\\
openai human written examples &  & 0.630 & 0.302 & 0.165 & 0.707 & 0.888 & 0.723 & 0.854 & 0.410 & 0.762 & 0.695 & 0.740 & 0.343 & 0.416 & 0.679 & 0.252 & 0.707\\
gpt4 style in context examples &  & 0.605 & 0.265 & 0.152 & 0.726 & 0.882 & 0.724 & 0.862 & 0.445 & 0.760 & 0.706 & 0.736 & 0.380 & 0.408 & 0.665 & 0.226 & 0.737\\
rewrite groundtruth in own words &  & 0.497 & 0.241 & 0.139 & 0.700 &  &  &  &  &  &  &  & 0.297 &  &  &  & \\
\hline\hline
gold label & llama 3 instruct &  &  &  & 0.738 & 0.979 & 0.759 & 0.850 & 0.430 & 0.754 & 0.767 & 0.744 &  & 0.510 & 0.769 & 0.342 & 0.643\\
groundtruth &  & 0.677 & 0.408 & 0.241 & 0.706 &  &  &  &  &  &  &  & 0.443 &  &  &  & \\
gpt4 &  & 0.817 & 0.557 & 0.312 & 0.748 & 0.865 & 0.698 & 0.866 & 0.455 & 0.762 & 0.808 & 0.792 & 0.483 & 0.469 & 0.707 & 0.233 & 0.650\\
claude &  & 0.796 & 0.504 & 0.253 & 0.760 & 0.858 & 0.716 & 0.858 & 0.440 & 0.766 & 0.797 & 0.762 & 0.457 & 0.547 & 0.709 & 0.246 & 0.727\\
mini gpt4 &  & 0.810 & 0.548 & 0.274 & 0.719 & 0.863 & 0.664 & 0.871 & 0.430 & 0.751 & 0.811 & 0.810 & 0.487 & 0.384 & 0.727 & 0.226 & 0.633\\
step by step &  & 0.796 & 0.561 & 0.266 & 0.733 & 0.867 & 0.721 & 0.846 & 0.420 & 0.777 & 0.792 & 0.780 & 0.457 & 0.233 & 0.708 & 0.249 & 0.697\\
openai human written examples &  & 0.809 & 0.547 & 0.291 & 0.735 & 0.894 & 0.716 & 0.868 & 0.435 & 0.764 & 0.801 & 0.806 & 0.487 & 0.343 & 0.709 & 0.209 & 0.680\\
gpt4 style in context examples &  & 0.798 & 0.484 & 0.291 & 0.733 & 0.890 & 0.720 & 0.878 & 0.440 & 0.751 & 0.812 & 0.792 & 0.463 & 0.416 & 0.723 & 0.279 & 0.680\\
rewrite groundtruth in own words &  & 0.749 & 0.445 & 0.253 & 0.733 &  &  &  &  &  &  &  & 0.447 &  &  &  & \\
\hline\hline
gold label & qwen &  &  &  & 0.817 & 0.898 & 0.735 & 0.867 & 0.470 & 0.771 & 0.854 & 0.668 &  & 0.514 & 0.737 & 0.306 & 0.613\\
groundtruth &  & 0.896 & 0.892 & 0.658 & 0.798 &  &  &  &  &  &  &  & 0.580 &  &  &  & \\
gpt4 &  & 0.901 & 0.904 & 0.692 & 0.794 & 0.855 & 0.703 & 0.878 & 0.555 & 0.759 & 0.884 & 0.800 & 0.583 & 0.437 & 0.730 & 0.312 & 0.667\\
claude &  & 0.901 & 0.903 & 0.654 & 0.784 & 0.857 & 0.722 & 0.878 & 0.555 & 0.765 & 0.877 & 0.790 & 0.610 & 0.465 & 0.721 & 0.302 & 0.657\\
mini gpt4 &  & 0.903 & 0.904 & 0.662 & 0.789 & 0.872 & 0.716 & 0.882 & 0.565 & 0.765 & 0.891 & 0.822 & 0.647 & 0.371 & 0.700 & 0.312 & 0.667\\
step by step &  & 0.899 & 0.907 & 0.646 & 0.790 & 0.866 & 0.723 & 0.883 & 0.550 & 0.775 & 0.881 & 0.826 & 0.620 & 0.420 & 0.711 & 0.292 & 0.667\\
openai human written examples &  & 0.904 & 0.908 & 0.641 & 0.786 & 0.868 & 0.696 & 0.883 & 0.550 & 0.787 & 0.884 & 0.822 & 0.633 & 0.465 & 0.720 & 0.282 & 0.620\\
gpt4 style in context examples &  & 0.900 & 0.903 & 0.658 & 0.805 & 0.878 & 0.730 & 0.882 & 0.585 & 0.787 & 0.870 & 0.814 & 0.643 & 0.294 & 0.742 & 0.299 & 0.637\\
rewrite groundtruth in own words &  & 0.897 & 0.907 & 0.692 & 0.785 &  &  &  &  &  &  &  & 0.590 &  &  &  & \\
\hline
\end{tabular}}
\caption{seed 1 train datasize 1000 lr 2e-05 epoch num 20}
\label{tab:ntrain_1000_lr_2e-05_seed1}
\end{table*}





\begin{table*}[t!]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
Data Generation Strategy & Model Type & gsm8k & math algebra & math geometry & ecqa & boolq & winogrande & piqa & agieval & squad & arc challenge & drop & mbpp & api bank & hellaswag & mmlu pro law & mmlu moral scenarios \\ \hline
gold label & mistral &  &  &  & 0.681 & 0.996 & 0.743 & 0.838 & 0.450 & 0.741 & 0.734 & 0.656 &  & 0.449 & 0.776 & 0.269 & 0.663\\
groundtruth &  & 0.441 & 0.211 & 0.101 & 0.679 &  &  &  &  &  &  &  & 0.350 &  &  &  & \\
gpt4 &  & 0.617 & 0.315 & 0.169 & 0.708 & 0.868 & 0.700 & 0.870 & 0.415 & 0.739 & 0.661 & 0.720 & 0.343 & 0.482 & 0.641 & 0.276 & 0.703\\
claude &  & 0.612 & 0.277 & 0.148 & 0.742 & 0.883 & 0.716 & 0.856 & 0.410 & 0.744 & 0.743 & 0.726 & 0.367 & 0.445 & 0.570 & 0.246 & 0.713\\
mini gpt4 &  & 0.622 & 0.320 & 0.177 & 0.703 & 0.865 & 0.688 & 0.855 & 0.435 & 0.740 & 0.768 & 0.708 & 0.353 & 0.429 & 0.670 & 0.219 & 0.697\\
step by step &  & 0.622 & 0.322 & 0.139 & 0.709 & 0.866 & 0.697 & 0.843 & 0.430 & 0.763 & 0.700 & 0.714 & 0.360 & 0.298 & 0.661 & 0.219 & 0.720\\
openai human written examples &  & 0.614 & 0.323 & 0.156 & 0.701 & 0.900 & 0.718 & 0.855 & 0.405 & 0.754 & 0.663 & 0.748 & 0.363 & 0.408 & 0.679 & 0.246 & 0.720\\
gpt4 style in context examples &  & 0.606 & 0.251 & 0.165 & 0.712 & 0.884 & 0.724 & 0.860 & 0.420 & 0.771 & 0.711 & 0.748 & 0.373 & 0.449 & 0.673 & 0.266 & 0.737\\
rewrite groundtruth in own words &  & 0.506 & 0.222 & 0.135 & 0.703 &  &  &  &  &  &  &  & 0.327 &  &  &  & \\
\hline\hline
gold label & llama 3 instruct &  &  &  & 0.735 & 0.980 & 0.760 & 0.865 & 0.445 & 0.757 & 0.762 & 0.740 &  & 0.465 & 0.784 & 0.329 & 0.663\\
groundtruth &  & 0.696 & 0.415 & 0.228 & 0.690 &  &  &  &  &  &  &  & 0.413 &  &  &  & \\
gpt4 &  & 0.806 & 0.553 & 0.278 & 0.733 & 0.864 & 0.697 & 0.865 & 0.450 & 0.742 & 0.824 & 0.748 & 0.487 & 0.445 & 0.725 & 0.233 & 0.683\\
claude &  & 0.789 & 0.489 & 0.257 & 0.734 & 0.866 & 0.685 & 0.846 & 0.450 & 0.759 & 0.800 & 0.770 & 0.507 & 0.547 & 0.716 & 0.243 & 0.743\\
mini gpt4 &  & 0.795 & 0.536 & 0.287 & 0.733 & 0.866 & 0.690 & 0.869 & 0.450 & 0.754 & 0.796 & 0.686 & 0.467 & 0.367 & 0.709 & 0.246 & 0.640\\
step by step &  & 0.800 & 0.551 & 0.245 & 0.719 & 0.884 & 0.707 & 0.865 & 0.460 & 0.767 & 0.782 & 0.792 & 0.467 & 0.245 & 0.697 & 0.269 & 0.653\\
openai human written examples &  & 0.796 & 0.529 & 0.287 & 0.736 & 0.884 & 0.714 & 0.863 & 0.450 & 0.757 & 0.808 & 0.800 & 0.460 & 0.367 & 0.689 & 0.223 & 0.680\\
gpt4 style in context examples &  & 0.800 & 0.500 & 0.283 & 0.729 & 0.876 & 0.709 & 0.856 & 0.440 & 0.767 & 0.809 & 0.816 & 0.480 & 0.433 & 0.708 & 0.252 & 0.683\\
rewrite groundtruth in own words &  & 0.754 & 0.431 & 0.291 & 0.715 &  &  &  &  &  &  &  & 0.457 &  &  &  & \\
\hline\hline
gold label & qwen &  &  &  & 0.818 & 0.887 & 0.724 & 0.867 & 0.495 & 0.774 & 0.861 & 0.652 &  & 0.539 & 0.740 & 0.302 & 0.590\\
groundtruth &  & 0.901 & 0.910 & 0.675 & 0.758 &  &  &  &  &  &  &  & 0.623 &  &  &  & \\
gpt4 &  & 0.897 & 0.892 & 0.654 & 0.791 & 0.858 & 0.710 & 0.883 & 0.540 & 0.777 & 0.882 & 0.788 & 0.603 & 0.433 & 0.703 & 0.306 & 0.607\\
claude &  & 0.881 & 0.916 & 0.641 & 0.785 & 0.859 & 0.735 & 0.877 & 0.540 & 0.762 & 0.872 & 0.798 & 0.597 & 0.461 & 0.732 & 0.292 & 0.690\\
mini gpt4 &  & 0.902 & 0.904 & 0.658 & 0.778 & 0.875 & 0.711 & 0.880 & 0.555 & 0.760 & 0.890 & 0.798 & 0.613 & 0.396 & 0.696 & 0.309 & 0.677\\
step by step &  & 0.886 & 0.907 & 0.679 & 0.770 & 0.859 & 0.715 & 0.869 & 0.560 & 0.767 & 0.867 & 0.792 & 0.597 & 0.404 & 0.711 & 0.332 & 0.677\\
openai human written examples &  & 0.900 & 0.887 & 0.646 & 0.794 & 0.881 & 0.707 & 0.872 & 0.540 & 0.802 & 0.895 & 0.804 & 0.603 & 0.449 & 0.724 & 0.306 & 0.640\\
gpt4 style in context examples &  & 0.911 & 0.908 & 0.650 & 0.792 & 0.875 & 0.728 & 0.891 & 0.535 & 0.790 & 0.866 & 0.824 & 0.577 & 0.318 & 0.734 & 0.316 & 0.643\\
rewrite groundtruth in own words &  & 0.905 & 0.899 & 0.637 & 0.799 &  &  &  &  &  &  &  & 0.600 &  &  &  & \\
\hline
\end{tabular}}
\caption{seed 2 train datasize 1000 lr 2e-05 epoch num 20}
\label{tab:ntrain_1000_lr_2e-05_seed2}
\end{table*}




\begin{table*}[t!]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
Data Generation Strategy & Model Type & gsm8k & math algebra & math geometry & ecqa & boolq & winogrande & piqa & agieval & squad & arc challenge & drop & mbpp & api bank & hellaswag & mmlu pro law & mmlu moral scenarios \\ \hline
gold label & mistral &  &  &  & 0.627 & 0.869 & 0.608 & 0.814 & 0.430 & 0.582 & 0.704 & 0.482 &  & 0.220 & 0.625 & 0.153 & 0.420\\
groundtruth &  & 0.420 & 0.205 & 0.101 & 0.591 &  &  &  &  &  &  &  & 0.267 &  &  &  & \\
gpt4 &  & 0.513 & 0.231 & 0.101 & 0.596 & 0.837 & 0.636 & 0.790 & 0.345 & 0.333 & 0.624 & 0.244 & 0.317 & 0.249 & 0.269 & 0.166 & 0.380\\
claude &  & 0.505 & 0.215 & 0.110 & 0.634 & 0.837 & 0.627 & 0.804 & 0.400 & 0.290 & 0.630 & 0.250 & 0.340 & 0.257 & 0.284 & 0.179 & 0.413\\
mini gpt4 &  & 0.511 & 0.223 & 0.097 & 0.619 & 0.845 & 0.644 & 0.782 & 0.360 & 0.404 & 0.633 & 0.210 & 0.337 & 0.253 & 0.223 & 0.183 & 0.343\\
step by step &  & 0.494 & 0.247 & 0.080 & 0.593 & 0.845 & 0.636 & 0.765 & 0.355 & 0.314 & 0.618 & 0.092 & 0.317 & 0.265 & 0.254 & 0.183 & 0.403\\
openai human written examples &  & 0.504 & 0.230 & 0.118 & 0.611 & 0.853 & 0.639 & 0.811 & 0.355 & 0.467 & 0.578 & 0.280 & 0.317 & 0.257 & 0.316 & 0.166 & 0.517\\
gpt4 style in context examples &  & 0.500 & 0.245 & 0.114 & 0.560 & 0.845 & 0.649 & 0.789 & 0.340 & 0.312 & 0.611 & 0.124 & 0.337 & 0.208 & 0.295 & 0.183 & 0.423\\
rewrite groundtruth in own words &  & 0.450 & 0.214 & 0.110 & 0.603 &  &  &  &  &  &  &  & 0.317 &  &  &  & \\
\hline\hline
gold label & llama 3 instruct &  &  &  & 0.710 & 0.852 & 0.636 & 0.789 & 0.395 & 0.680 & 0.764 & 0.620 &  & 0.082 & 0.610 & 0.196 & 0.200\\
groundtruth &  & 0.794 & 0.460 & 0.249 & 0.691 &  &  &  &  &  &  &  & 0.407 &  &  &  & \\
gpt4 &  & 0.791 & 0.491 & 0.266 & 0.686 & 0.802 & 0.634 & 0.801 & 0.430 & 0.504 & 0.760 & 0.410 & 0.480 & 0.082 & 0.592 & 0.223 & 0.387\\
claude &  & 0.804 & 0.492 & 0.266 & 0.699 & 0.806 & 0.640 & 0.821 & 0.450 & 0.495 & 0.739 & 0.420 & 0.483 & 0.082 & 0.608 & 0.233 & 0.430\\
mini gpt4 &  & 0.797 & 0.477 & 0.257 & 0.710 & 0.800 & 0.621 & 0.823 & 0.425 & 0.509 & 0.751 & 0.400 & 0.497 & 0.086 & 0.589 & 0.229 & 0.373\\
step by step &  & 0.808 & 0.496 & 0.219 & 0.702 & 0.818 & 0.626 & 0.799 & 0.435 & 0.565 & 0.743 & 0.488 & 0.477 & 0.110 & 0.608 & 0.199 & 0.407\\
openai human written examples &  & 0.809 & 0.472 & 0.257 & 0.720 & 0.810 & 0.630 & 0.815 & 0.440 & 0.564 & 0.747 & 0.414 & 0.500 & 0.078 & 0.600 & 0.236 & 0.387\\
gpt4 style in context examples &  & 0.800 & 0.434 & 0.266 & 0.695 & 0.793 & 0.638 & 0.808 & 0.455 & 0.429 & 0.762 & 0.344 & 0.497 & 0.090 & 0.582 & 0.229 & 0.407\\
rewrite groundtruth in own words &  & 0.813 & 0.480 & 0.262 & 0.718 &  &  &  &  &  &  &  & 0.447 &  &  &  & \\
\hline\hline
gold label & qwen &  &  &  & 0.791 & 0.843 & 0.677 & 0.875 & 0.465 & 0.703 & 0.877 & 0.334 &  & 0.220 & 0.702 & 0.306 & 0.387\\
groundtruth &  & 0.913 & 0.918 & 0.679 & 0.792 &  &  &  &  &  &  &  & 0.603 &  &  &  & \\
gpt4 &  & 0.908 & 0.898 & 0.692 & 0.802 & 0.831 & 0.711 & 0.863 & 0.560 & 0.661 & 0.891 & 0.092 & 0.637 & 0.237 & 0.697 & 0.326 & 0.580\\
claude &  & 0.911 & 0.912 & 0.679 & 0.788 & 0.837 & 0.718 & 0.876 & 0.550 & 0.652 & 0.894 & 0.114 & 0.640 & 0.237 & 0.691 & 0.282 & 0.577\\
mini gpt4 &  & 0.902 & 0.914 & 0.667 & 0.791 & 0.852 & 0.720 & 0.884 & 0.550 & 0.660 & 0.891 & 0.068 & 0.600 & 0.237 & 0.702 & 0.296 & 0.583\\
step by step &  & 0.909 & 0.919 & 0.599 & 0.802 & 0.848 & 0.708 & 0.870 & 0.545 & 0.682 & 0.879 & 0.062 & 0.617 & 0.224 & 0.690 & 0.309 & 0.563\\
openai human written examples &  & 0.900 & 0.918 & 0.671 & 0.789 & 0.842 & 0.718 & 0.864 & 0.545 & 0.681 & 0.887 & 0.090 & 0.597 & 0.237 & 0.706 & 0.322 & 0.563\\
gpt4 style in context examples &  & 0.918 & 0.914 & 0.679 & 0.798 & 0.836 & 0.710 & 0.865 & 0.535 & 0.678 & 0.888 & 0.050 & 0.627 & 0.196 & 0.696 & 0.326 & 0.567\\
rewrite groundtruth in own words &  & 0.910 & 0.916 & 0.667 & 0.782 &  &  &  &  &  &  &  & 0.590 &  &  &  & \\
\hline
\end{tabular}}
\caption{seed 0 train datasize 100 lr 2e-05 epoch num 20}
\label{tab:ntrain_100_lr_2e-05_seed0}
\end{table*}






\begin{table*}[t!]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
Data Generation Strategy & Model Type & gsm8k & math algebra & math geometry & ecqa & boolq & winogrande & piqa & agieval & squad & arc challenge & drop & mbpp & api bank & hellaswag & mmlu pro law & mmlu moral scenarios \\ \hline
gold label & mistral &  &  &  & 0.681 & 0.870 & 0.694 & 0.830 & 0.420 & 0.730 & 0.726 & 0.620 &  & 0.486 & 0.737 & 0.236 & 0.677\\
groundtruth &  & 0.409 & 0.186 & 0.093 & 0.638 &  &  &  &  &  &  &  & 0.293 &  &  &  & \\
gpt4 &  & 0.586 & 0.270 & 0.152 & 0.672 & 0.864 & 0.686 & 0.821 & 0.455 & 0.649 & 0.736 & 0.670 & 0.340 & 0.404 & 0.623 & 0.233 & 0.687\\
claude &  & 0.554 & 0.237 & 0.122 & 0.663 & 0.858 & 0.701 & 0.855 & 0.405 & 0.690 & 0.760 & 0.662 & 0.360 & 0.400 & 0.619 & 0.243 & 0.710\\
mini gpt4 &  & 0.514 & 0.266 & 0.152 & 0.705 & 0.850 & 0.674 & 0.847 & 0.425 & 0.670 & 0.739 & 0.666 & 0.357 & 0.359 & 0.651 & 0.233 & 0.623\\
step by step &  & 0.575 & 0.235 & 0.131 & 0.662 & 0.853 & 0.667 & 0.842 & 0.415 & 0.691 & 0.746 & 0.646 & 0.327 & 0.286 & 0.575 & 0.233 & 0.593\\
openai human written examples &  & 0.536 & 0.278 & 0.156 & 0.674 & 0.874 & 0.665 & 0.850 & 0.435 & 0.700 & 0.764 & 0.698 & 0.340 & 0.302 & 0.628 & 0.229 & 0.677\\
gpt4 style in context examples &  & 0.548 & 0.222 & 0.156 & 0.658 & 0.879 & 0.681 & 0.864 & 0.430 & 0.676 & 0.741 & 0.650 & 0.333 & 0.343 & 0.628 & 0.203 & 0.687\\
rewrite groundtruth in own words &  & 0.443 & 0.202 & 0.101 &  &  &  &  &  &  &  &  & 0.330 &  &  &  & \\
\hline\hline
gold label & llama 3 instruct &  &  &  & 0.705 & 0.866 & 0.675 & 0.847 & 0.430 & 0.727 & 0.773 & 0.684 &  & 0.494 & 0.682 & 0.299 & 0.633\\
groundtruth &  & 0.683 & 0.404 & 0.211 & 0.679 &  &  &  &  &  &  &  & 0.430 &  &  &  & \\
gpt4 &  & 0.798 & 0.529 & 0.257 & 0.731 & 0.864 & 0.679 & 0.845 & 0.440 & 0.729 & 0.815 & 0.734 & 0.470 & 0.424 & 0.711 & 0.246 & 0.683\\
claude &  & 0.805 & 0.495 & 0.224 & 0.712 & 0.834 & 0.694 & 0.857 & 0.420 & 0.744 & 0.789 & 0.742 & 0.467 &  & 0.677 & 0.226 & 0.693\\
mini gpt4 &  & 0.807 & 0.504 & 0.278 & 0.719 & 0.852 & 0.674 & 0.858 & 0.445 & 0.746 & 0.795 & 0.744 & 0.473 & 0.335 & 0.676 & 0.266 & 0.630\\
step by step &  & 0.779 & 0.528 & 0.198 & 0.690 & 0.874 & 0.683 & 0.863 & 0.435 & 0.736 & 0.797 & 0.708 & 0.457 & 0.253 & 0.688 & 0.233 & 0.620\\
openai human written examples &  & 0.772 & 0.483 & 0.249 & 0.712 & 0.873 & 0.678 & 0.853 & 0.405 & 0.726 & 0.789 & 0.772 & 0.473 & 0.302 & 0.674 & 0.262 & 0.640\\
gpt4 style in context examples &  & 0.794 & 0.488 & 0.283 & 0.712 & 0.861 & 0.690 & 0.859 & 0.440 & 0.729 & 0.770 & 0.754 & 0.473 & 0.380 & 0.702 & 0.259 & 0.693\\
rewrite groundtruth in own words &  & 0.693 & 0.415 & 0.232 &  &  &  &  &  &  &  &  & 0.430 &  &  &  & \\
\hline\hline
gold label & qwen &  &  &  & 0.820 & 0.883 & 0.704 & 0.858 & 0.480 & 0.747 & 0.849 & 0.642 &  & 0.457 & 0.725 & 0.339 & 0.563\\
groundtruth &  & 0.867 & 0.896 & 0.637 & 0.823 &  &  &  &  &  &  &  & 0.523 &  &  &  & \\
gpt4 &  & 0.897 & 0.890 & 0.620 & 0.787 & 0.859 & 0.709 & 0.881 & 0.545 & 0.743 & 0.882 & 0.808 & 0.617 & 0.388 & 0.687 & 0.362 & 0.690\\
claude &  & 0.882 & 0.890 & 0.616 & 0.790 & 0.869 & 0.738 & 0.867 & 0.555 & 0.766 & 0.871 & 0.810 & 0.603 & 0.527 & 0.702 & 0.316 & 0.750\\
mini gpt4 &  & 0.889 & 0.912 & 0.624 & 0.794 & 0.867 & 0.719 & 0.887 & 0.530 & 0.750 & 0.891 & 0.772 & 0.580 & 0.429 & 0.707 & 0.289 & 0.640\\
step by step &  & 0.902 & 0.899 & 0.586 & 0.788 & 0.868 & 0.731 & 0.878 & 0.545 & 0.737 & 0.881 & 0.788 & 0.630 & 0.339 & 0.715 & 0.309 & 0.677\\
openai human written examples &  & 0.892 & 0.899 & 0.616 & 0.783 & 0.874 & 0.727 & 0.883 & 0.565 & 0.776 & 0.875 & 0.824 & 0.590 & 0.392 & 0.694 & 0.233 & 0.643\\
gpt4 style in context examples &  & 0.896 & 0.899 & 0.637 & 0.782 & 0.864 & 0.720 & 0.881 & 0.550 & 0.764 & 0.868 & 0.832 & 0.620 & 0.335 & 0.752 & 0.302 & 0.677\\
rewrite groundtruth in own words &  & 0.899 & 0.892 & 0.646 &  &  &  &  &  &  &  &  & 0.583 &  &  &  & \\
\hline
\end{tabular}}
\caption{seed 0 train datasize 100 lr 0.0002 epoch num 40}
\label{tab:ntrain_100_lr_0.0002_seed0}
\end{table*}




\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|c|c}
\hline
Benchmark Name & Data Name & Chosen/Not Chosen & Why not chosen \\ \hline
Mistral 7B & Winogrande & $\checkmark$ & \\
 & PIQA & $\checkmark$& \\
 &GSM8K &$\checkmark$ & \\
 & MATH& $\checkmark$& \\
  & MBPP& $\checkmark$& \\
   & MMLU& $\checkmark$& \\
        & AGIEVAL&$\checkmark$ & \\
& ARC Challenge & $\checkmark$& \\
 & BoolQ& $\checkmark$& \\
 & Hellaswag &$\checkmark$ &  \\
  & CommonsenseQA &$\times$ & not a reasoning task\\
    & BBH&  $\times$&  In github, it says this dataset can never used in training.\\
    & SIQA & $\times$ &  not a reasoning task\\
 & OpenbookQA &$\times$ & not a reasoning task \\
 & ARC Easy & $\times$& We already choose ARC Challenge \\
 
 & NaturalQuestions & $\times$& It evaluates world knowledge instead of reasoning ability\\
 & TriviaQA &$\times$ & It evaluates world knowledge instead of reasoning ability \\
 & QuAC&  $\times$& this is a multiturn, muti context qa dataset. evaluation is too hard\\
\hline
Llama 3 & MMLU & $\checkmark$& \\
 & MMLU\_Pro & $\checkmark$ & \\
 & GSM8K & $\checkmark$& \\
 & MATH & $\checkmark$& \\
 & AGIEVAL & $\checkmark$& \\
 & ARC CHALLENGE &$\checkmark$ & \\
 & DROP & $\checkmark$& \\
 & API-BANK & $\checkmark$& \\
& IFEval & $\times$ & less than 650 data \\
 & HumanEval+& $\times$& less than 650 data\\
 & BFCL& $\times$&  (subcategory) less than 650 data\\
 & Nexus& $\times$&  Unable to find the dataset\\
  % & MGSM& $\times$&  Not English Dataset\\
 & GPQA &$\times$ & less than 650 data \\
 & HumanEval &$\times$ & less than 650 data \\
&ZeroSCROLLS/QuALITY&$\times$ & This dataset evaluating model's long context QA ability. The input is too long thus is hard to train. \\
&InfiniteBench/En.MC&$\times$ & This dataset evaluating model's long context QA ability. The input is too long thus is hard to train. \\

&NIH/Multi-needle&$\times$ & Long context QA task. The input is too long thus is hard to train. Llama already achieves 98.8\% accuracy with zero-shot setting.\\



 
\hline
Qwen2.5 & MMLU & $\checkmark$ & \\
 &MMLU Pro &$\checkmark$ & \\
  & MBPP& $\checkmark$& \\
 & ARC CHALLENGE&$\checkmark$ & \\
  & GSM8K&$\checkmark$ & \\
 &MATH &$\checkmark$ & \\
 & WindoGrande&$\checkmark$ & \\
  & HellaSwag& $\checkmark$&  \\
 &MMLU stem &$\times$ & (subcategory) less than 650 data  \\
 &TruthfulQA &$\times$ & not reasoning task\\
 &GPQA &$\times$ &  less than 650 data\\
  &TheoremQA & $\times$& the data set is tooooo challenging for GPT-4o. it does not have the ability to be a teacher for this task. \\
 &HumanEval & $\times$& less than 650 data \\ 
 & HumanEval+& $\times$& less than 650 data\\
 & MMLU redux&$\times$ & (subcategory) less than 650 data \\
 &BBH & $\times$& In github, it says this dataset can never used in training. \\
 &MBPP+ &$\times$ & less than 650 data \\
 & MultiPL-E& $\times$&  (subcategory) less than 650 data\\

\hline
\end{tabular}
}
\caption{This table explains which data from the Mistral, LLaMA3, and Qwen benchmarks were chosen and why some data were not selected. Multi-lingual dataset is not listed in this Table since our experiment only covers English-only datasets. API-BANK is in Table 16 from Llama 3 technical report.}
\label{tab:why_each_data_is_chosen}
\end{table*}









\end{document}


