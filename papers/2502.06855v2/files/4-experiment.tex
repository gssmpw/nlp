\section{Experiment}
\label{sec:exp}

\subsection{Experimental Setup}

\begin{table*}[t!]
\caption{Comparison of performance between conventional prompt methods and prompt generated by prompt optimization methods in five benchmarks. All methods are executed with GPT-4o-mini on the divided test set, with results averaged over three runs. \ours and \ours$^*$ use Claude-3.5-Sonnet and GPT-4o as their optimization models, respectively. The Avg. Cost refers to the averaged optimization cost.}

\label{tab:mainres}

% \renewcommand\tabcolsep{3.2pt}
% \renewcommand\arraystretch{1.2}
\small
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{-0.2cm}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccccccc}
\hline

\hline

\hline

\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c}{\textbf{Method Analysis}}  \\
& GPQA & AGIEval-MATH& LIAR& WSC & BBH-Navigate & Avg. Perf. & Avg. Cost(\$)\\
\hline

\hline
IO  & 38.9& 42.1& 63.5& 72.4& 91.3& 61.6& - \\
CoT~\cite{wei2022COT} & 41.6& 44.5& 65.4& 77.8& 89.7& 63.8& - \\
Rephrase~\cite{deng2023rephrase} & 40.2& 42.1& 50.5& 79.1& 93.5& 61.1& - \\
Step-back~\cite{hua2024step} & 42.4& \textbf{47.5}& 62.8& 78.7& 93.5& 65.0& - \\ 
\hline
APE~\cite{yong2023ape} & 41.1 & 44.4 & 65.9 & 80.2 & 92.5 & 64.8 & 9.07 \\
OPRO~\cite{yang2023opro} & \underline{43.3} & \underline{46.1} & \textbf{67.6} & 80.2 & 95.8 & \underline{66.6} & 4.51  \\
PromptAgent~\cite{xin2024pa} & 41.3& 41.4& 64.1& \textbf{82.7}& 95.7& 65.0 & 2.71  \\
PromptBreeder~\cite{chris2024pb} & 40.9& 45.9& 63.2& 76.7& \underline{96.3}& 64.5 & 4.82 \\
TextGrad ~\cite{mert2024textgrad} & 40.2 & 44.4 & 65.7 & 78.0 & 91.3 & 63.9 & 13.14 \\
\rowcolor[gray]{.8}
\textbf{SPO} (ours) & \textbf{43.6}& \underline{46.1}& \underline{67.1}& \underline{82.0}& \textbf{97.2}& \textbf{66.9} & \underline{0.15}\\
\rowcolor[gray]{.8}
\textbf{SPO}$^*$ (ours) & 41.8& 45.3& 66.9 & 81.1 & \underline{96.3} & 66.3 & \textbf{0.12} \\
\hline

\hline

\hline

\hline
\end{tabular}
}
\end{table*}


\paragraph{Datasets}
We evaluated \ours on a diverse set of tasks, including both \textit{\textbf{closed tasks}} and \textbf{\textit{open-ended tasks}}, to comprehensively assess its effectiveness. 


For \textit{closed tasks}, we utilized five established benchmarks: GPQA~\cite{david2023gpqa}, AGIEval-MATH~\cite{wan2024agieval}, LIAR~\cite{william2017liar}, WSC~\cite{hector2012wsc}, and BBH-navigate~\cite{mirac2023bbh}. For WSC, LIAR, and BBH-Navigate, we sampled portions from their original datasets as test sets following~\citet{cilin2024erm}. 
For GPQA, we used the more challenging GPQA-Diamond subset as the test set, while for AGIEval-Math, we used Level 5 problems as the test set. For \textit{open-ended tasks}, we selected \textit{writing}, \textit{roleplay}, and \textit{humanities} tasks from MT-Bench~\cite{lianmin2023mtbench}. Given the limited size of the dataset, we manually constructed three validation sets for these tasks. Detailed descriptions of the datasets and the construction procedures for validation and test sets are provided in Appendix~\ref{appendix:exp}.


\paragraph{Baselines}

We evaluate \ours against two categories of methods on \textit{closed tasks}:
(1) conventional prompting approaches, comprising io (direct llm invocation), chain-of-thought~\cite{wei2022COT}, rephrase~\cite{deng2023rephrase}, and step-back abstract~\cite{hua2024step}; and 
(2) automated prompt optimization methods, including APE~\cite{yong2023ape}, OPRO~\cite{yang2023opro}, PromptAgent~\cite{xin2024pa}, PromptBreeder~\cite{chris2024pb} and TextGrad~\cite{mert2024textgrad}, with their evaluation framework setting detailed in Table \ref{tab:features}.

For \textit{open-ended tasks} in MT-Bench~\cite{lianmin2023mtbench}, we use GPT-4o to compare outputs generated by \ours against those directly generated by the model. 

\begin{table}[htbp]
\caption{Comparison of evaluation frameworks across different prompt optimization methods. OvG denotes evaluation against ground truth, while OvO represents output-vs-output comparison. Methods are categorized by their evaluation source and method.}
% Evaluation
\label{tab:features}
\renewcommand\tabcolsep{3.2pt}
\renewcommand\arraystretch{1.2}
\small
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{-0.2cm}
\centering
% \resizebox{\textwidth}{!}{
\begin{tabular}{l|cc}
\hline

\hline

\hline

\hline
{\textbf{Method}} & \textbf{Evaluation Source} & \textbf{Evaluation Method}   \\
\hline

\hline
APE & OvG & Benchmark  \\
OPRO & OvG & Benchmark  \\
PromptAgent & OvG & Benchmark \\
PromptBreeder & OvG & Benchmark \\
TextGrad & OvG & LLM as a judge \\
\hline
\rowcolor[gray]{.8}
\ours & \textbf{OvO} & LLM as a judge \\

\hline

\hline

\hline

\hline
\end{tabular}
% }
\end{table}


\paragraph{Implementation Details}
\ours employs different models for optimization, evaluation, and execution. In the main experiments, we use Claude-3.5-Sonnet-20240620 and GPT-4o-0806 (temperature = 0.7) as optimization models, while GPT-4o-mini-0718 (temperature = 0.3 for evaluation, 0 for execution) is used for both evaluation and execution. The optimization process runs for 10 iterations with three samples per iteration, while detailed baseline implementation settings are provided in Appendix~\ref{appendix:exp}.


\paragraph{Metrics}
We evaluate performance using F1 scores for GPQA, LIAR, and BBH-Navigate datasets, and accuracy metrics for AGIEval-MATH and WSC, following~\citet{cilin2024erm, jon2024archon, david2023gpqa}. For MT-Bench, we report win rates of \ours compared to other methods. To assess cost-efficiency, we also measure optimization costs.


\subsection{Experimental Results and Analysis}

\paragraph{Main Result of Closed Tasks}


As shown in Table \ref{tab:mainres}, prompts optimized by \ours outperform all conventional prompting methods on average, exceeding the best baseline by \textbf{1.9}. Meanwhile, it achieves comparable performance to ground truth-dependent prompt optimization methods across most datasets, and reaches optimal results on GPQA and BBH-navigate datasets. Specifically, \ours's superior average performance over other optimization methods demonstrates that its pairwise evaluation approach can generate more effective optimization signals compared to the tested methods relying on external reference. Furthermore, to verify the effectiveness of our method across different optimization models, we conducted experiments using GPT-4o as the optimization model, achieving an average performance of \textbf{66.3}. While slightly lower than results using Claude-3-5-Sonnet as the optimization model, this still ranks third among all compared methods.

\paragraph{Cost Analysis}


We present a comprehensive comparison of optimization costs and performance between SPO (using Claude-3.5-Sonnet and GPT-4o as optimization models) and other optimization methods in Table~\ref{tab:mainres}. While maintaining comparable performance with other ground truth-dependent prompt optimization methods, \ours requires only \textbf{1.1\%} to \textbf{5.6\%} of their optimization costs, with an average optimization cost of \textbf{0.15 \$} per dataset. This significant reduction in computational overhead, combined with its independence from ground truth, makes \ours highly attractive for real-world applications.


\paragraph{Ablation Study}

\begin{table}[htbp]
\caption{Performance comparison on BBH-navigate: prompting methods (IO and COT) and \ours with different \textbf{evaluation models} (rows) and execution models (columns). The optimization model is set to Claude-3.5-Sonnet.}
\label{tab:ablation-eval}
\renewcommand\tabcolsep{3.2pt}
\renewcommand\arraystretch{1.2}
\small
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{-0.2cm}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|ccc}
\hline

\hline

\hline

\hline
{\textbf{}} & \textbf{GPT-4o-mini} & \textbf{Llama3-70B} & \textbf{Claude-3-Haiku}  \\
\hline

\hline
IO & 91.3 & 82.7 & 62.2 \\
COT & 89.7 & 86.2 & 68 \\
\hline
Claude-3.5-Sonnet & 95 & 86.8 & 68.2 \\
Llama3-70B & 94.5 & 94.2 & 82.0 \\
GPT-4o-mini & \textbf{97.8}& 90.7 & 82.0 \\

\hline

\hline

\hline

\hline
\end{tabular}
}
\end{table}



\begin{table}[htbp]
\caption{Performance comparison across different \textbf{optimization models} (rows) and execution models (columns) on BBH-navigate. The evaluation model is set to GPT-4o-mini.}
\label{tab:ablation}
\renewcommand\tabcolsep{3.2pt}
\renewcommand\arraystretch{1.2}
\small
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{-0.2cm}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|ccc}
\hline

\hline

\hline

\hline
{\textbf{}} & \textbf{GPT-4o-mini} & \textbf{Llama3-70B} & \textbf{Claude-3-Haiku}  \\
\hline

\hline
Claude-3.5-Sonnet & 97.2 & 86.7 & 89.7 \\
GPT-4o & 96.3 & 85.5 & 73.0 \\
GPT-4o-mini & \textbf{97.8}& 90.7 & 82.0 \\
DeepSeek-V3 & 94.7 & 83.7 & 77.2  \\

\hline

\hline

\hline

\hline
\end{tabular}
}
\end{table}

To evaluate the transferability of \ours across different optimization, evaluation, and execution models, we conducted ablation experiments on the BBH-Navigate dataset. The experimental results in Table \ref{tab:ablation-eval}, \ref{tab:ablation} demonstrate that \ours exhibits robust performance across different models. Notably, the best performance (\textbf{97.8}) was achieved when GPT-4o-mini was used as the optimization, execution, and evaluation model.  In terms of execution, \ours effectively improves the performance of weaker models, elevating Claude-3-Haiku from \textbf{62.2} to \textbf{89.7}, demonstrating \ours's applicability to weaker models and further expanding its potential for real-world applications.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{images/figure-sample-curve.pdf}
  \vspace{-2em}
  \caption{Impact of sample number on performance across different optimization models on BBH-Navigate dataset. We evaluate three optimization models: GPT-4o-mini, GPT-4o, and Claude-3.5-Sonnet. The results demonstrate an inverted U-shaped relationship between sample number and performance.}
  \label{fig:sample}
\end{figure}

We conduct an ablation study to investigate the impact of sample number on \ours's performance using the BBH-Navigate dataset, as shown in Figure~\ref{fig:sample}. The performance curves of all three optimization models exhibit similar patterns: performance initially improves with increased sample number but eventually  converges or decline. This phenomenon can be attributed to two factors: insufficient samples lead to overfitting in prompt optimization, while excessive samples not only increase computational costs but also result in longer context for the evaluation model, potentially degrading assessment quality. Based on extensive experiments, we determine that a sample size of 3 achieves the optimal balance between cost-efficiency and performance.

\paragraph{Main Result of Open-ended Tasks}

To validate \ours's capability in open-ended tasks, we selected three categories from MT-Bench: ``Writing'', ``Roleplay'', and ``Humanities'' for evaluation. We use Claude-3.5-Sonnet as the optimization model, Gpt-4o-mini as the evaluation model, and selected Claude-3.5-Sonnet, DeepSeek-V3, and GPT-4o-mini as execution models, conducting five iterations. Subsequently, following the evaluation methodology in~\cite{lianmin2023mtbench}, we employed GPT-4o to perform pairwise comparisons between model A and model B's output in Figure\ref{fig:mt-bench}. The experimental results shown in \ref{fig:mt-bench} demonstrate that \ours significantly improves model performance across all model configurations. 
Notably, smaller models (such as GPT-4o-mini) using optimized prompts frequently outperformed larger models in most scenarios.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{images/figure-mt-bench.pdf}
  \vspace{-2em}
  \caption{Win rates comparison between different LLMs and \ours across Writing, Roleplay, and Humanities tasks. The heatmap shows pairwise win rates (\%) where each cell represents the row model's win rate against the column model. Models tested include Claude-3.5-Sonnet, DeepSeek-V3, and GPT-4o-mini. Models are evaluated both in IO (top three rows) and after SPO optimization (bottom three rows). Win rates range from 0\% to 100\%, with higher percentages indicating better performance.}
  \label{fig:mt-bench}
\end{figure}

\subsection{Case Study}

We present optimization results on additional open-ended tasks without datasets and \ours's optimization trajectories in the Appendix \ref{appendix:case-study}. We also provide optimal prompt across five closed tasks discoverd by \ours in the supplementary material.
Given that real-world applications often face challenges with limited datasets, we evaluate \ours's performance on tasks that lack conventional benchmarks. The experimental results, coupled with \ours's cost efficiency, demonstrate its practical value in real-world scenarios. Specifically, we demonstrate the optimization results after 10 iterations using Claude-3.5-Sonnet as the optimization model, GPT-4o-mini as the evaluation model, and Llama-3-8B as the execution model across four tasks: Advertising Design, Social Media Content, Modern Poetry Writing, and Concept Interpretation in Appendix \ref{appendix:open-ended}. Moreover, we provide a comprehensive analysis of \ours's optimization trajectory on the BBH-navigate dataset in Appendix \ref{appendix:trajectory}, presenting both successful and unsuccessful examples to offer deeper insights into the optimization process.
