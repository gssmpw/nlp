\section{Preliminary}
\label{sec:pre}

\subsection{Problem Definition} 
\label{sec:problem-formulation}

\textbf{Prompt Optimization} aims to automatically enhance the effectiveness of a prompt for a given task. Formally, let $T = (Q, G_t)$ represent a task, where $Q$ denotes the input question and $G_t$ is the optional ground truth.
The goal is to generate a task-specific prompt $P_t^*$ that maximizes performance on task $T$. This optimization objective can be formally expressed as:

\begin{equation}
P_t^* = \argmax_{P_t \in \mathcal{P}} \mathbb{E}_{T \sim D}[\phi_{eval}(\phi_{exe}(Q, P_t))],
\end{equation}


where $\mathcal{P}$ represents the space of all possible prompts. As illustrated in Figure~\ref{fig:contrast}, this optimization process typically involves three fundamental functions:
(1) Optimization function ($\phi_{opt}$): generates a revised prompt based on the candidate prompt; 
(2) Execution function ($\phi_{exe}$): applies the revised prompt with an LLM to produce outputs $O$, consisting of a reasoning path and a final answer; 
(3) Evaluation function ($\phi_{eval}$):  assesses the quality of $O$ and provides feedback $F$ to guide further optimization, refining the candidate prompts iteratively.

Among these functions, the evaluation function plays a pivotal role as its output (feedback $F$) guides the assessment and improvement of prompts.
We will discuss the evaluation framework for prompt optimization in Section~\ref{sec:feedback}.



\subsection{Evaluation Framework in  Prompt Optimization}
\label{sec:feedback}

This section outlines our evaluation framework for prompt optimization, covering three key components: evaluation sources, evaluation methods, and feedback types, as shown in Figure~\ref{fig:components}. We conclude by introducing our selected evaluation framework for \ours.


\textbf{Evaluation Sources}
As shown in Figure~\ref{fig:components}(a), two primary sources can be used for evaluation: LLM-generated outputs and task-specific ground truth. These sources provide the basis for assessing prompt performance.


\textbf{Evaluation Methods}
The evaluation method defines how the evaluation sources are assessed and the associated costs. Three common methods are used:
(1) \textit{Benchmark} relies on predefined metrics~\cite{mirac2023bbh, david2023gpqa} or rules~\cite{yong2024promst}. 
(2) \textit{LLM-as-a-judge}~\cite{lianmin2023mtbench} leverage LLMs capability to understand and assess outputs based on task requirements. 
(3) \textit{Human Feedback}~\cite{lin2024apohf} provides the most comprehensive evaluation through direct human assessment of outputs.

While Human Feedback offers the most thorough evaluation by capturing human preferences and task-specific needs, it incurs substantially higher costs than Benchmark or LLM-as-a-judge methods, creating a trade-off between evaluation quality and feasibility.


\textbf{Feedback Types}
Feedback produced by evaluation methods typically take three forms:
(1) \textit{Numerical Feedback} provides quantitative performance measures across the dataset. However, it requires substantial samples for stable evaluation and may overlook instance-specific details~\cite{zhang2024aflow}. 
(2) \textit{Textual Feedback} offers rich, instance-specific guidance through analysis and suggestions, directly generating optimization signals~\cite{mert2024textgrad}.
(3) \textit{Ranking or Selection Feedback}~\cite{yin2024pair} establishes relative quality ordering among outputs through either complete ranking or pairwise comparisons, providing clear optimization direction without requiring absolute quality measures.

\begin{figure}[t!]
	\centering
	\includegraphics[width=\linewidth]{images/figure-components.pdf}
        \vspace{-2em}
	\caption{Components of the Evaluation Framework for Prompt Optimization. (a) Evaluation Sources: Compares different outputs, including ground truth and model-generated outputs, to assess quality.
(b) Evaluation Methods: Showcases various evaluation techniques, including benchmark comparisons, LLM-as-a-Judge, and human feedback.
(c) Feedback Types: Showcases a range of feedback.
The \textit{\textcolor{bleudefrance}{\textbf{blue}}} in (a), (b), and (c) indicate the specific evaluation approach selected for \ours.
}
	\label{fig:components}
\end{figure}

\paragraph{Evaluation Framework}
Building on the previous discussion on evaluation's sources, methods, and feedback types, the evaluation framework determines how sources are compared and assessed within the context of prompt optimization. Specifically, we derive two evaluation frameworks to generate feedback $F$ for prompt optimization:


(1) \textbf{Output \textit{vs.} Ground Truth (OvG):} 
    Feedback is generated by comparing outputs $O$ with ground truth $G_T$:
    
    \begin{equation}
    \small
        f_{OvG}(O_i, G_i) = \phi_{eval}(\phi_{exe}(Q_i, T_{p_i}), G_i)
    \end{equation}
    
    Although this approach allows for a direct quality assessment through an external reference, it requires well-defined ground truth, making it unsuitable for open-ended tasks where ground truth may not always be available or practical to define.
    
(2) \textbf{Output \textit{vs.} Output (OvO):}
    When ground truth is unavailable, we turn to direct output comparison. The core idea behind OvO is that even in the absence of perfect ground truth, comparing outputs generated by different prompts can offer valuable signals about their relative quality. This method removes the dependency on external references and is particularly useful for open-ended tasks where multiple answers may be valid. It can be formally expressed as:
    
    \begin{equation}\small
        f_{OvO}(O_1, ..., O_k) = \phi_{eval}(\{\phi_{exe}(Q_i, P_{t_i})\}_{i=1}^k)
    \end{equation}

After introducing the \textbf{OvG} and \textbf{OvO} evaluation frameworks, we emphasize that \textbf{OvO} serves as the core method in Self-Supervised Prompt Optimization (\ours). By comparing outputs generated by different prompts, \textbf{OvO} provides valuable feedback on their relative quality without relying on external reference. This approach aligns with our objective of generating feedback directly from the outputs themselves, thus facilitating iterative optimization in both closed and open-ended tasks.
