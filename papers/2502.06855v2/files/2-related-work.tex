\section{Related Work}

\subsection{Prompt Engineering}
Research on effective prompting methods for large language models has primarily evolved along two main directions. The first focuses on task-agnostic prompting techniques that enhance LLMs' general capabilities. Notable examples include the chain-of-thought \cite{wei2022COT, take2022zerocot} which improved reasoning across various tasks, techniques for enhancing single-shot reasoning \cite{deng2023rephrase, hua2024step, wang2024negativeprompt}, and methods for output format specification \cite{zhang2024aflow, he2024doseformat, zhi2024freeformat}. These techniques, developed through human insights and extensive experimentation, provide essential optimization seeds for automated prompt optimization research.

The second direction addresses domain-specific prompting, where researchers have developed specialized techniques for tasks in code generation~\cite{sirui2024meta, tal2024alpha,10720675}, data analysis~\cite{hong2024data, liu2024surveynl2sqllargelanguage,  DBLP:journals/pvldb/LiLCLT24}, question answering~\cite{DBLP:conf/emnlp/WuYSW0L24,DBLP:journals/corr/abs-2406-07815,yang2024askchartuniversalchartunderstanding}, decision-makings~\cite{zhang2024mobileexperts, guan2024voyager}, and other domains~\cite{guo2024largelanguagemodelbase,DBLP:journals/corr/abs-2404-18144,shen2024askhumansaiexploring}. However, as applications of LLMs expand to increasingly complex real-world scenarios, manually crafting effective prompts for each domain becomes impractical~\cite{zhang2024aflow}. This challenge has motivated research in prompt optimization, which aims to systematically develop effective domain-specific prompts rather than discovering general prompting principles.

\subsection{Prompt Optimization}

The design of evaluation frameworks is crucial in Prompt Optimization (PO), as it determines both optimization effectiveness and computational efficiency. The evolution of evaluation mechanisms in PO has progressed from simple evaluation feedback collection to sophisticated optimization signal generation~\cite{kai2024posurvey}.
Existing PO methods can be categorized based on their evaluation sources and mechanisms. 

The most common approach relies on ground truth as the evaluation source, utilizing benchmark-based numerical assessments \cite{yong2023ape, qing2024evoprompt, yang2023opro, chris2024pb, xin2024pa, omar2024dspy}. While these methods have demonstrated success in specific tasks, they typically require substantial iterations and samples to ensure evaluation stability, leading to significant computational overhead.

To reduce sample requirements, several methods \cite{cilin2024erm, mert2024textgrad, yu2024stargo, wang2024semantic, reid2023protegi, ya2025ttpo} use LLM-as-a-judge~\cite{lianmin2023mtbench} to generate detailed textual feedback. Although this approach provides richer evaluation signals with fewer samples, it still depends on ground truth data, limiting its applicability in open-ended tasks where reference answers may not exist.

Alternative approaches focus on human preferences, either through manually designed evaluation rules or direct human feedback \cite{yong2024promst, lin2024apohf}. While these methods can handle open-ended tasks effectively, their reliance on extensive human involvement contradicts the goal of automated prompt optimization. Meanwhile, some researchers explore different evaluation criteria, such as \citet{xuan2024glape}'s proposal to evaluate prompt effectiveness through output consistency. However, this approach faces a fundamental challenge: the non-linear relationship between consistency and effectiveness often leads to suboptimal evaluation signals.

In contrast to these approaches, \ours introduces a novel evaluation paradigm that eliminates dependency on external references while maintaining efficiency. By leveraging only model outputs through pairwise comparisons, \ours achieves robust evaluation without requiring ground truth, human feedback, or extensive sampling, making it particularly suitable for real-world applications.