\section{Conclusion}



This paper addresses a fundamental challenge in prompt optimization: reliance on external references that limits real-world applications. We introduce Self-Supervised Prompt Optimization (\ours), a framework that overcomes this reliance while achieving remarkable cost-efficiency at only \textbf{\$0.15} per dataset. Drawing inspiration from self-supervised learning, \ours innovatively constructs evaluation and optimization signals through pairwise comparisons of model outputs, enabling reference-free optimization without compromising effectiveness. 

Our comprehensive evaluation demonstrates \ours's superior performance across both closed and open-ended tasks, achieving state-of-the-art results while requiring only \textbf{1.1\%-5.6\%} of existing methods' costs. The success on both standard benchmarks and diverse real-world applications validates \ours's effectiveness and generalization capabilities. By dramatically reducing both resource requirements and operational complexity, \ours represents a significant advancement in making prompt optimization accessible and practical for real-world applications, potentially accelerating the adoption of LLM technologies across various domains.
