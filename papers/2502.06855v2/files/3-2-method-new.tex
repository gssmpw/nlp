\section{Self-Supervised Prompt Optimization}
\label{sec:method}

In this section, we first overview our method (Section~\ref{sub:overview}) and then analyze its effectiveness (Section~\ref{sec:method_essence}).

\begin{figure*}[t!]
  \centering
\includegraphics[width=\linewidth]{images/figure-method.pdf}
  \vspace{-2em}
  \caption{A Running Example of \ours Framework: Through pairwise evaluation on \highlight{output}{output}, \ours extract labels indicate which \highlight{prompt}{prompt} is \highlight{response}{better} and guide optimization. Furthermore, using a case from MT-bench, we show the complete process of \ours's $\phi_{opt}$, $\phi_{exe}$, and $\phi_{eval}$ and corresponding \highlight{metaprompt}{prompt}.}
  \label{fig:main_method}
  \vspace{-1em}
\end{figure*}

\subsection{An Overview of SPO}
\label{sub:overview}

A core challenge in reference-free prompt optimization is how to construct effective evaluation and optimization signals. We propose Self-Supervised Prompt Optimization (\ours), a simple yet effective framework that retains the basic Optimize-Execute-Evaluate loop while enabling reference-free optimization by leveraging only model outputs as both evaluation sources and optimization guidance.

As shown in Algorithm~\ref{alg:concise-algo-pipo}, \ours operates through three key components and the corresponding prompts are shown in Appendix \ref{appendix:prompt}: 

\begin{itemize}
\item Optimization function ($\phi_{opt}$): Generates new prompts by analyzing the current best prompt and its corresponding outputs.
\item Execution function ($\phi_{exe}$): Applies the generated prompts to obtain outputs.
\item Evaluation function ($\phi_{eval}$): Uses an LLM to compare outputs and determine the superior prompt through pairwise comparisons.
\end{itemize}

This iterative process begins with a basic prompt template (\eg Chain-of-Thought~\cite{wei2022COT}) and a small question set
sampled from the dataset. In each iteration, \ours generates new prompts, executes them, and performs pairwise evaluations of outputs to assess their adherence to task requirements.

The prompt associated with the superior output is selected as the best candidate for the next iteration. The process continues until a predefined maximum number of iterations is reached.

\textbf{A Running Example}
As illustrated in Figure~\ref{fig:main_method}, \ours achieves high efficiency, requiring only 8 LLM calls per iteration with three samples, significantly lower than existing methods~\cite{xin2024pa, chris2024pb, mert2024textgrad, 10720675, yong2023ape}. Despite its simplicity, \ours demonstrates superior performance across a range of tasks, as detailed in Section~\ref{sec:exp}. In the following section, we analyze the theoretical foundations of its effectiveness.


\begin{algorithm}[t!]
\small
\caption{An Overview of \ours.}
\label{alg:concise-algo-pipo}
\begin{algorithmic}[1]
\REQUIRE Dataset $D$
\ENSURE Optimized Prompt $P^*$
\STATE Initialize $P_0$; Sample 3 Questions $Q$ from $D$
\STATE $\text{Best Prompt } P^* \gets P_0$
\STATE $\text{Best Answer } A^* \gets \phi_{exe}(Q, P^*)$
\FOR{$iteration \gets 1$ to $N_{max}$}
    \STATE $P' \gets \phi_{opt}(P^*, A^*)$
    \STATE $A' \gets \phi_{exe}(Q, P')$
    \STATE $optimizationSuccess \gets \phi_{eval}(Q, A', A^*)$
    \IF{$optimizationSuccess$}
        \STATE $P^* \gets P'$
        \STATE $A^* \gets A'$
    \ENDIF
\ENDFOR
\STATE \textbf{return} $P^*$
\end{algorithmic}
\end{algorithm}


\subsection{Understanding the Effectiveness of \ours}
\label{sec:method_essence}


The theoretical foundation of \ours is built upon two key observations:

First, the outputs of LLMs inherently contain rich quality information that directly reflects prompt effectiveness, as evidenced by how step-by-step reasoning paths demonstrate the success of Chain-of-thought prompting~\cite{wei2022COT}. Second, LLMs exhibit human-like task comprehension, enabling them to assess answer quality and identify superior solutions based on task requirements. These complementary capabilities allow SPO to perform prompt evaluation and optimization without external references. These two aspects of utilizing model outputs work together to enable effective prompt optimization:

\textbf{Output as Optimization Guidance}
In terms of $\phi_{opt}$ design, unlike other methods that introduce explicit optimization signals \cite{chris2024pb, mert2024textgrad, reid2023protegi}, $\phi_{opt}$ optimizes directly based on the prompt and its corresponding outputs. The optimization signal stems from the LLMs' inherent ability to assess output quality, while the optimization behavior is guided by its understanding of what constitutes superior solutions. Therefore, even without explicit optimization signals, \ours's optimization essentially guides prompts toward the LLM's optimal understanding of the task.

\textbf{Output as Pairwise Evaluation Reference} 
Regarding $\phi_{eval}$ design, by employing the evaluation model to perform pairwise selection, we are effectively leveraging the evaluation model's inherent preference understanding of tasks. This internal signal can be obtained through simple pairwise comparisons of outputs, avoiding the need for large sample sizes to ensure scoring stability, which is typically required in score-based feedback methods.

 While we mitigate potential biases through four rounds of randomized evaluation, these biases cannot be completely eliminated~\cite{zhou2024zepo}. However, these biases do not affect the overall optimization trend because eval's feedback merely serves as a reference for the next round of optimization. The overall optimization process naturally aligns with the optimization model's task understanding, with the eval mechanism serving to validate the effectiveness of each iteration.



