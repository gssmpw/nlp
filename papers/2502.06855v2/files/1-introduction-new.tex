\section{Introduction}

\begin{figure}[t!]
	\centering
\includegraphics[width=\linewidth]{images/figure-compare.pdf}
        \vspace{-1em}
        \caption{\textbf{Comparison of Prompt Optimization Methods.} (a) illustrates the traditional prompt optimization process with external reference, where feedback from the ground truth of humans is used to iteratively improve the best prompt. (b) presents our proposed self-supervised prompt optimization, which utilizes pairwise comparisons of LLM's own outputs to optimize prompts without relying on external reference.}
	\label{fig:contrast}
\end{figure}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=\linewidth]{images/figure-performance.pdf}
        \vspace{-1em}
	\caption{\textbf{Comparison of Performance ($y$-axis) and Optimization Costs in Dollars ($x$-axis) across Six Prompt Optimization Methods.}
     \ours demonstrates competitive performance, consistently ranking among the top two methods while maintaining significantly lower costs (ranging from 1.1\% to 5.6\% of the costs incurred by other methods) across all datasets.
     }
    \label{fig:performance-cost}

\end{figure*}


As large language models (LLMs) continue to advance, well-designed prompts have become critical for maximizing their reasoning capabilities~\cite{wei2022COT, hua2024step, deng2023rephrase} and ensuring alignment with diverse task requirements~\cite{sirui2024meta, liu2024surveynl2sqllargelanguage, zhang2024mobileexperts, hong2024data}.
However, creating effective prompts often requires substantial trial-and-error experimentation and deep task-specific knowledge.


To address this challenge, researchers have explored Prompt Optimization (PO) methods that use LLMs' own capabilities to automatically improve prompts. PO advances beyond traditional prompt engineering, by providing a more systematic and efficient approach to prompt design. As shown in Figure~\ref{fig:contrast}(a), these methods typically involve an iterative process of prompt optimization, execution, and evaluation. The design choices for these components significantly influence optimization effectiveness and efficiency.
Existing approaches have demonstrated success with both numerical evaluation mechanisms~\cite{xin2024pa, yang2023opro, chris2024pb} and textual ``gradient'' optimization strategies~\cite{wang2024semantic, mert2024textgrad}. Through these innovations, PO methods have shown promise in reducing manual effort while enhancing task performance~\cite{reid2023protegi, zhang2024aflow, zhou2024zepo}.


Despite their potential, existing PO methods face significant challenges in real-world scenarios, as discussed below. First, current methods often depend heavily on external references for evaluation. Methods using ground truth for evaluation~\cite{yang2023opro, chris2024pb, mert2024textgrad,reid2023protegi} require large amounts of annotated data to assess prompt quality, yet such standard answers are often unavailable in many practical applications, especially for open-ended tasks. Similarly, methods relying on human~\cite{yong2024promst, lin2024apohf} require manual evaluations or human-designed rules to generate feedback, which is time-consuming and contradicts the goal of automation.
Second, existing methods typically require evaluating prompts on numerous samples to obtain reliable feedback, leading to substantial computational overhead~\cite{xin2024pa, chris2024pb}.


At the core of these challenges lies the absence of reliable and efficient reference-free methods for assessing prompt quality. Analysis of LLM behavior reveals two key insights that inform our approach. First, prompt quality inherently manifests in model outputs, as evidenced by how different prompting strategies significantly influence both reasoning paths~\cite{wei2022COT, deng2023rephrase} and response features~\cite{lei2024character, schmidgall2025agentlaboratory}. Second, extensive studies on LLM-as-a-judge have demonstrated their effectiveness in evaluating output adherence to task requirements~\cite{lianmin2023mtbench, dawei2024laajsurvey}. These observations suggest that by leveraging LLMs' inherent ability to assess outputs that naturally reflect prompt quality, reference-free prompt optimization becomes feasible.



Motivated by these insights, we propose a cost-efficient framework that generates evaluation and optimization signals purely from LLM outputs, similar to how self-supervised learning derives training signals from data. We term this approach \textbf{S}elf-Supervised \textbf{P}rompt \textbf{O}ptimization (\ours). As shown in Figure \ref{fig:contrast}, \ours builds upon the fundamental Optimize-Execute-Evaluate loop while introducing several innovative mechanisms: 

(1) \textbf{\textit{Output as Pairwise Evaluation Reference}}: At its core, \ours employs a pairwise comparison approach that assesses the relative quality of outputs from different prompts. This evaluation mechanism leverages LLM's inherent capability to understand task requirements, validating optimization effectiveness without external references.

(2) \textbf{\textit{Output as Optimization Guidance}}: \ours optimizes prompts through LLM's understanding of better solutions for the current best output. Rather than relying on explicit optimization signals, this process naturally aligns prompt modifications with the model's comprehension of optimal task solutions.

\textbf{Contributions.}
Our main contributions are as follows:

(1) \textbf{Self-Supervised Prompt Optimization Framework.} We introduce \ours, a novel framework that leverages pairwise comparisons of LLM's outputs to guide prompt optimization without requiring external reference.

(2) \textbf{Cost-effective Optimization.} 
\ours optimizes prompts with minimal computational overhead (\$0.15 per dataset) and sample requirements (3 samples), significantly reducing resource demands.

(3) \textbf{Extensive Evaluation.} As shown in Figure~\ref{fig:performance-cost}, \ours requires only \textbf{1.1\% to 5.6\%} of the cost of state-of-the-art methods while maintaining superior performance across both closed and open-ended tasks.



