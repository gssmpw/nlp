@inbook{sauro2016quantifying,
  title={Quantifying the User Experience: Practical Statistics for User Research},
  author={Sauro, J. and Lewis, J.R.},
  isbn={9780128025482},
  url={https://books.google.com/books?id=USPfCQAAQBAJ},
  year={2016},
  pages={190-197},
  publisher={Elsevier Science}
}

@incollection{soloway_programcomprehensionreview_1988,
  title={Knowledge and processes in the comprehension of computer programs},
  author={Soloway, Elliot and Adelson, Beth and Ehrlich, Kate},
  booktitle={The nature of expertise},
  pages={129--152},
  year={1988},
  publisher={Psychology Press}
}

@inproceedings{choiVIVIDHumanAICollaborative2024a,
  title = {{{VIVID}}: {{Human-AI Collaborative Authoring}} of {{Vicarious Dialogues}} from {{Lecture Videos}}},
  shorttitle = {{{VIVID}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Choi, Seulgi and Lee, Hyewon and Lee, Yoonjoo and Kim, Juho},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--26},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642867},
  urldate = {2024-09-10},
  abstract = {The lengthy monologue-style online lectures cause learners to lose engagement easily. Designing lectures in a ``vicarious dialogue'' format can foster learners' cognitive activities more than monologue-style. However, designing online lectures in a dialogue style catered to the diverse needs of learners is laborious for instructors. We conducted a design workshop with eight educational experts and seven instructors to present key guidelines and the potential use of large language models (LLM) to transform a monologue lecture script into pedagogically meaningful dialogue. Applying these design guidelines, we created VIVID which allows instructors to collaborate with LLMs to design, evaluate, and modify pedagogical dialogues. In a within-subjects study with instructors (N=12), we show that VIVID helped instructors select and revise dialogues efficiently, thereby supporting the authoring of quality dialogues. Our findings demonstrate the potential of LLMs to assist instructors with creating high-quality educational dialogues across various learning stages.},
  isbn = {9798400703300},
  file = {/Users/mehmetarifdemirtas/Zotero/storage/2STK8BNH/Choi et al. - 2024 - VIVID Human-AI Collaborative Authoring of Vicario.pdf}
}

@inproceedings{ferdowsiValidatingAIGeneratedCode2024,
  title = {Validating {{AI-Generated Code}} with {{Live Programming}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Ferdowsi, Kasra and Huang, Ruanqianqian (Lisa) and James, Michael B. and Polikarpova, Nadia and Lerner, Sorin},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642495},
  urldate = {2024-09-10},
  abstract = {AI-powered programming assistants are increasingly gaining popularity, with GitHub Copilot alone used by over a million developers worldwide. These tools are far from perfect, however, producing code suggestions that may be incorrect in subtle ways. As a result, developers face a new challenge: validating AI's suggestions. This paper explores whether Live Programming (LP), a continuous display of a program's runtime values, can help address this challenge. To answer this question, we built a Python editor that combines an AI-powered programming assistant with an existing LP environment. Using this environment in a between-subjects study (N = 17), we found that by lowering the cost of validation by execution, LP can mitigate over- and under-reliance on AI-generated programs and reduce the cognitive load of validation for certain types of\&nbsp;tasks.},
  isbn = {9798400703300},
  file = {/Users/mehmetarifdemirtas/Zotero/storage/QWI5P5P7/Ferdowsi et al. - 2024 - Validating AI-Generated Code with Live Programming.pdf}
}

@inproceedings{jinTeachAIHow2024,
  title = {Teach {{AI How}} to {{Code}}: {{Using Large Language Models}} as {{Teachable Agents}} for {{Programming Education}}},
  shorttitle = {Teach {{AI How}} to {{Code}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Jin, Hyoungwook and Lee, Seonghee and Shin, Hyungyu and Kim, Juho},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--28},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642349},
  urldate = {2024-09-10},
  abstract = {This work investigates large language models (LLMs) as teachable agents for learning by teaching (LBT). LBT with teachable agents helps learners identify knowledge gaps and discover new knowledge. However, teachable agents require expensive programming of subject-specific knowledge. While LLMs as teachable agents can reduce the cost, LLMs' expansive knowledge as tutees discourages learners from teaching. We propose a prompting pipeline that restrains LLMs' knowledge and makes them initiate ``why'' and ``how'' questions for effective knowledge-building. We combined these techniques into TeachYou, an LBT environment for algorithm learning, and AlgoBo, an LLM-based tutee chatbot that can simulate misconceptions and unawareness prescribed in its knowledge state. Our technical evaluation confirmed that our prompting pipeline can effectively configure AlgoBo's problem-solving performance. Through a between-subject study with 40 algorithm novices, we also observed that AlgoBo's questions led to knowledge-dense conversations (effect size=0.71). Lastly, we discuss design implications, cost-efficiency, and personalization of LLM-based teachable agents.},
  isbn = {9798400703300},
  file = {/Users/mehmetarifdemirtas/Zotero/storage/VFQ27DF2/Jin et al. - 2024 - Teach AI How to Code Using Large Language Models .pdf}
}

@inproceedings{kazemitabaarCodeAidEvaluatingClassroom2024,
  title = {{{CodeAid}}: {{Evaluating}} a {{Classroom Deployment}} of an {{LLM-based Programming Assistant}} That {{Balances Student}} and {{Educator Needs}}},
  shorttitle = {{{CodeAid}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kazemitabaar, Majeed and Ye, Runlong and Wang, Xiaoning and Henley, Austin Zachary and Denny, Paul and Craig, Michelle and Grossman, Tovi},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--20},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642773},
  urldate = {2024-09-10},
  abstract = {Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student's incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI's unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.},
  isbn = {9798400703300},
  file = {/Users/mehmetarifdemirtas/Zotero/storage/QGZHZJHS/Kazemitabaar et al. - 2024 - CodeAid Evaluating a Classroom Deployment of an L.pdf}
}

@inproceedings{logachevaEvaluatingContextuallyPersonalized2024,
  title = {Evaluating {{Contextually Personalized Programming Exercises Created}} with {{Generative AI}}},
  booktitle = {Proceedings of the 2024 {{ACM Conference}} on {{International Computing Education Research}} - {{Volume}} 1},
  author = {Logacheva, Evanfiya and Hellas, Arto and Prather, James and Sarsa, Sami and Leinonen, Juho},
  year = {2024},
  month = aug,
  series = {{{ICER}} '24},
  volume = {1},
  pages = {95--113},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3632620.3671103},
  urldate = {2024-09-10},
  abstract = {Programming skills are typically developed through completing various hands-on exercises. Such programming problems can be contextualized to students' interests and cultural backgrounds. Prior research in educational psychology has demonstrated that context personalization of exercises stimulates learners' situational interests and positively affects their engagement. However, creating a varied and comprehensive set of programming exercises for students to practice on is a time-consuming and laborious task for computer science educators. Previous studies have shown that large language models can generate conceptually and contextually relevant programming exercises. Thus, they offer a possibility to automatically produce personalized programming problems to fit students' interests and needs. This article reports on a user study conducted in an elective introductory programming course that included contextually personalized programming exercises created with GPT-4. The quality of the exercises was evaluated by both the students and the authors. Additionally, this work investigated student attitudes towards the created exercises and their engagement with the system. The results demonstrate that the quality of exercises generated with GPT-4 was generally high. What is more, the course participants found them engaging and useful. This suggests that AI-generated programming problems can be a worthwhile addition to introductory programming courses, as they provide students with a practically unlimited pool of practice material tailored to their personal interests and educational needs.},
  isbn = {9798400704758},
  file = {/Users/mehmetarifdemirtas/Zotero/storage/CMGDWHV7/Logacheva et al. - 2024 - Evaluating Contextually Personalized Programming E.pdf}
}

@inproceedings{yangDebuggingAITutor2024,
  title = {Debugging with an {{AI Tutor}}: {{Investigating Novice Help-seeking Behaviors}} and {{Perceived Learning}}},
  shorttitle = {Debugging with an {{AI Tutor}}},
  booktitle = {Proceedings of the 2024 {{ACM Conference}} on {{International Computing Education Research}} - {{Volume}} 1},
  author = {Yang, Stephanie and Zhao, Hanzhang and Xu, Yudian and Brennan, Karen and Schneider, Bertrand},
  year = {2024},
  month = aug,
  series = {{{ICER}} '24},
  volume = {1},
  pages = {84--94},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3632620.3671092},
  urldate = {2024-09-10},
  abstract = {Debugging is a crucial skill for programmers, yet it can be challenging for novices to learn. The introduction of large language models (LLMs) has opened up new possibilities for providing personalized debugging support to students. However, concerns have been raised about potential student over-reliance on LLM-based tools. This mixed-methods study investigates how a pedagogically-designed LLM-based chatbot supports students' debugging efforts in an introductory programming course. We conducted interviews and debugging think-aloud tasks with 20 students at three points throughout the semester. We specifically focused on characterizing when students initiate help from the chatbot during debugging, how they engage with the chatbot's responses, and how they describe their learning experiences with the chatbot. By analyzing data from the debugging tasks, we identified varying help-seeking behaviors and levels of engagement with the chatbot's responses, depending on students' familiarity with the suggested strategies. Interviews revealed that students appreciated the content and experiential knowledge provided by the chatbot, but did not view it as a primary source for learning debugging strategies. Additionally, students self-identified certain chatbot usage behaviors as negative, ``non-ideal'' engagement and others as positive, ``learning-oriented'' usage. Based on our findings, we discuss pedagogical implications and future directions for designing pedagogical chatbots to support debugging.},
  isbn = {9798400704758},
  file = {/Users/mehmetarifdemirtas/Zotero/storage/7XVLERMJ/Yang et al. - 2024 - Debugging with an AI Tutor Investigating Novice H.pdf}
}

@inproceedings{yanIvieLightweightAnchored2024,
  title = {Ivie: {{Lightweight Anchored Explanations}} of {{Just-Generated Code}}},
  shorttitle = {Ivie},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Yan, Litao and Hwang, Alyssa and Wu, Zhiyuan and Head, Andrew},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642239},
  urldate = {2024-09-10},
  abstract = {Programming assistants have reshaped the experience of programming into one where programmers spend less time writing and more time critically examining code. In this paper, we explore how programming assistants can be extended to accelerate the inspection of generated code. We introduce an extension to the programming assistant called Ivie, or instantly visible in-situ explanations. When using Ivie, a programmer's generated code is instantly accompanied by explanations positioned just adjacent to the code. Our design was optimized for low-cost invocation and dismissal. Explanations are compact and informative. They describe meaningful expressions, from individual variables to entire blocks of code. We present an implementation of Ivie that forks VS Code, applying a modern LLM for timely segmentation and explanation of generated code. In a lab study, we compared Ivie to a contemporary baseline tool for code understanding. Ivie improved understanding of generated code, and was received by programmers as a highly useful, low distraction complement to the programming assistant.},
  isbn = {9798400703300},
  file = {/Users/mehmetarifdemirtas/Zotero/storage/UZ8RJRVP/Yan et al. - 2024 - Ivie Lightweight Anchored Explanations of Just-Ge.pdf}
}
@inproceedings{pratherRobotsAreHere2023,
author = {Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir},
title = {The Robots Are Here: Navigating the Generative AI Revolution in Computing Education},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633499},
doi = {10.1145/3623762.3633499},
abstract = {Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80\% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving.There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {108–159},
numpages = {52},
keywords = {ai, artificial intelligence, chatgpt, code generation, codex, computer programming, copilot, cs1, curriculum, generative ai, github, gpt, gpt-3, gpt-4, large language models, llm, llms, novice programming, openai, pedagogical practices, programming},
location = {Turku, Finland},
series = {ITiCSE-WGR '23}
}