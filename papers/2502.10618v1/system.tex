
%PlanGREEN

%GEN-Plan

%G- generate
%R-refine
%E- edit

%% GREEN-plan

%% PURPLE
\begin{figure*}
    \centering
    \Description{PLAID's system architecture diagram. Top part shows the database (a), and bottom part shows the interface (b). The system starts from bottom right as an instructor is interested in a programming domain, then the pipeline described in the text produces reference materials at different levels of granularity, and these are presented in the interface.}
    \includegraphics[width=\textwidth]{img/system-architecture-subgoals.png}
    \caption{PLAID's reference content is generated through an LLM pipeline
    %inspired by the practices of instructors who have successfully identified programming plans. 
    that produces output on three levels.
    First, a wide variety of use cases are generated to create example programs that focus on code's applications. Next, using LLM's explanatory comments that represent subgoals within the code, the examples are segmented into meaningful code snippets. The LLM is queried to generate other plan components for each code snippet. Finally, the code snippets are clustered to identify the most common patterns, representing plan candidates. The full programs are presented in `Programs' views of PLAID interface, whereas snippets are presented in clusters in the `Plan Creation' view.}
    \label{fig:system-pipeline}
\end{figure*}
\section{PLAID: A System for Supporting Plan Identification}
\label{sec:system-design}

Following the design goals devised from the design workshop, we refined our early prototype into PLAID: a
%LLM-powered
tool to assist instructors in their plan identification process.
PLAID synthesizes the capabilities of LLMs in code generation with interactions enabling plan identification practices observed in our studies with instructors.
As we noted in the findings of our design workshop, the LLM-generated candidate plans are not ready to be used as is in instruction, but instructors can readily adapt and correct them (\cref{sec:workshop-findings-condition2}).
PLAID enables collaboration between instructors and LLMs, enhancing the plan identification process by automating its time-intensive information-gathering tasks and facilitating instructors' ability to refine LLM-generated candidate plans based on their knowledge about pedagogy and the programming domain. 



\input{example-scenario}

\begin{figure*}[h]
        \Description{An annotated screenshot of PLAID's `Programs' view. On the left, a list of use cases such as `Renaming columns in a Frame' and `Plotting a histogram of a column' is shown, with a scrollable list and a search bar. The latter one is selected, and on the right, we see the contents of the program in a monospaced font, with four buttons explained in the caption.}
        \includegraphics[width=\textwidth]{img/system-diagram-1-fixed.png}
        \caption{Plan Identification using PLAID: (a) list of example programs for instructors organized by natural language descriptions, (b) list of full programs of code, (c) search bar enabling easy navigation of given content to find code for specific ideas, (d) button to create a plan using the selected part of the code, (e) button to create a plan using the complete example program, (f) button to view an explanation for a selected code snippet, and (g) button for executing the selected code.}
        \label{fig:system-diagram-1}
\end{figure*}

\subsection{System Design}

At a high level, PLAID\footnote{The code for PLAID can be found at: https://github.com/yosheejain/plaid-interface.} operates on two subsystems: (1) a database of LLM-generated reference materials created through a pipeline that uses \edit{OpenAI's GPT-4o\footnote{https://openai.com/index/hello-gpt-4o/}~\cite{achiam2023gpt}}, inspired by instructors' best practices for identifying programming plans (see ~\cref{fig:system-pipeline})
%LLM for identifying plans in application-focused domains 
and (2) an interface that allows instructors to browse reference materials for relevant code snippets 
% and other plan components to achieve a goal that meets their needs. Then, they refine the candidates to mine plans 
and refine suggested content into programming plans
(see Figures~\ref{fig:system-diagram-1} and~\ref{fig:system-diagram-2}).
% In this section, we describe the implementation of the pipeline generating the reference materials and the key interface features of PLAID.



\subsubsection{Database of Reference Materials for Application-Focused Domains}

PLAID extracts information from reference materials at three levels of granularity to support each instructor's unique workflow: complete programs that address a particular use case, annotated program snippets that include goals and changeable areas, and plan candidates that cluster relevant program snippets together.

\textbf{Generating complete example programs.}
The content at the lowest level of granularity in the PLAID database are the complete programs. 
%These candidate plans were generated using a pipeline to generate \textit{plan-ful examples}, which we define as examples of programming plans in use, with all plan components identified (see Section~\ref{sec:components}). This implementation had three stages: (1) generating in-domain programs, (2) segmenting programs into plan-ful examples, and (3) clustering plan-ful examples into plans. 
\label{sec:llm-pipeline}
% \begin{figure}
% \centering
% % \includegraphics[width=0.5\textwidth]{img/pipeline-new.png}
% \includegraphics[width=\textwidth]{img/new-plan-pipeline.png}
% \caption{The three stage process for generating example programs, segmenting them with plan components, and clustering these plan-ful examples.
% %collecting and processing responses from ChatGPT into plan-ful examples}
% %\caption{The pipeline for LLM plan generation.}
% }
% \label{fig:llm-methods}
% \end{figure}
% \subsubsection{Generating In-Domain Programs}
% Informed by the insights identified in our interview study, we generated programming plans relevant to an application-focused domain: web scraping via BeautifulSoup. We utilized an LLM-based approach to generate these plans with the GPT-4 model from OpenAI using its publicly available API in an iterative workflow. 
% Our participants examined example programs and conducted literature reviews (Section \ref{sec:viewing-programs}) as key parts of their plan identification process. 
As these examples should capture a variance of use cases in the real world, we utilized an LLM trained on a large corpus of computer programs and natural language descriptions~\cite{liu2023isyourcode}.
% Inspired by this, we used Open AI's GPT-4, a state-of-the-art large language model for code generation that is trained on a large corpus of computer programs~\cite{liu2023isyourcode},
% to generate candidate programs along with its respective plan components in the programs.
We prompted\footnote{Full prompts can be found in \cref{sec:appendix-pipeline}.} the model to generate \texttt{specific use cases of <application-focused library>}, defining use case as \texttt{a task you can achieve 
with the given library} (see \cref{sec:use_case_prompt}). Subsequently, we prompted the model to \texttt{write code to do the following: <use case>}, producing a set of 100 example programs with associated tasks (see \cref{sec:code_prompt}). By generating the use cases first and generating the solution later, we avoided the problems with context windows of LLMs where the earlier input might get `forgotten', resulting in the model producing the same output repeatedly. For practical purposes, we generated 100 programs per domain. \edit{To test for potential ``hallucinations'' where the LLM generates plausible yet incorrect code~\cite{Ji_2023_hallucination}, we checked the syntactic validity of the generated programs before developing the rest of our pipeline. No more than one out of 100 generated programs included syntax errors in each of our domains, i.e., Pandas, Django, and PyTorch. Thus, we concluded that hallucinations are not a major threat to the code generation aspect of PLAID.}
%while hallucinations in LLMs are a pressing concern for systems that utilize these models,
% This collection of example programs (which we refer to as 
%dataset 
% $\mathcal{D}$) was used as our primary dataset for further analysis.

\textbf{Generating annotated program snippets.}
% \subsubsection{Segmenting Programs Into Plan-ful Examples}
% We then proceed to compile these examples with each of the plan components generated using ChatGPT. We construct a new dataset with these components, Dataset \((\mathcal{D}^{\textit{Comp}})\).
The second level of granularity in PLAID consists of small program snippets and a goal, with changeable areas annotated. 
We used the generated programs from
% \mathcal{D}$
the prior step as the input to the LLM to add subgoal labels, where we prompted the LLM to annotate subgoals (see \cref{sec:subgoals_prompt}) as comments that describe \texttt{small chunks of code that achieve a task that can be explained in natural language}. These subgoal labels were used to break the full program into shorter snippets. Each snippet was fed back to the model to generate changeable areas (see \cref{sec:ca_prompt}), defined in the prompt as \texttt{parts of the idiom that would change when it is used in different scenarios}. The subgoal label that explained a code snippet corresponded to its goal in the plan view and the list of elements assigned as changeable was used for annotations.
% (see Stage 2 in Figure~\ref{fig:llm-methods}),
%We fragmented these generated programs into smaller code pieces by generating \textit{subgoals} in the program. Then, each goal (Section \ref{sec:goal}) and the accompanying code solution (Section \ref{sec:solution}) were added as a single unit of data in our plan-ful example dataset of components, \(\mathcal{D}^{\textit{Plan-ful}}\). For each of these datapoints, we prompted the model to identify \textit{changeable areas} (Section \ref{sec:changeable}). %The name (Section \ref{sec:name}) was determined later in the pipeline (Stage 2 in Figure \ref{fig:llm-methods}).


% From the results of our qualitative study, we now know about the parts of a programming plan. In order to extract these plans automatically, we used ChatGPT. We accessed it using its publicly available API and we used the GPT-4 model. We selected 3 domains that are interesting for non-majors. This included . 

% For each of these domains, we first asked the LLM to generate 100 use cases. We then re-prompted it with the use cases it generated and asked it to generate code that would be written to accomplish that use case.
% potential for another table?
% add code metrics from stackoverflow github work for chatgpt
% With all these code pieces collected, we then asked ChatGPT to generate each of the plan parts one-by-one.

% \subsubsection*{Extracting Goals and Solutions}Generated programs 
% in \(\mathcal{D}\) 
% typically included a comment before each line, which described that line's functionality. However, these comments did not capture the high-level purpose of the code, as required by a plan goal. To generate more abstract goals for a piece of code, we defined subgoals as \texttt{short descriptions of small pieces of code that do something meaningful} in a prompt and asked the LLM to \texttt{highlight subgoals as comments in the code.} %In our query, we also added the way we define subgoals to provide the relevant context to the model. Specifically, we wrote that 
% The output from this prompt was a modified version of each program
% from \(\mathcal{D}\), 
% where blocks of code are preceded by a comment describing the goal of that block. % of code. % instead of restating functionality. 

% We split each complete program into multiple segments based on these new comments. Thus, the subgoal comments from each complete program I
% n the modified \(\mathcal{D}\) 
% became a plan goal, and the code following that comment became the associated solution. %, collected in \(\mathcal{D}^{\textit{Plan-ful}}\). % After it returned the annotated code piece, we extracted the comment and the following lines of code before the next comment. This pair acted as a subgoal-code piece. We collected all such pairs across all use cases from \(\mathcal{D}\) and added them to \(\mathcal{D}^{\textit{Plan-ful}}\).
% Each goal 
% %(Section \ref{sec:goal}) 
% and solution pair
% %(Section \ref{sec:solution}) 
% was added as a single unit of data in our plan-ful example dataset.
% , \(\mathcal{D}^{\textit{Plan-ful}}\).

% \subsubsection*{Extracting Changeable Areas}To annotate the changeable areas for a plan, we defined changeable areas as \texttt{parts of the plan that would change when it is used in a different context} in our prompt and asked the model to \texttt{return the exact part of the code from the line that would change} for all code pieces from the dataset with plan-ful examples.
% from \(\mathcal{D}^{\textit{Plan-ful}}\). 
% This data was added to \(\mathcal{D}^{\textit{Plan-ful}}\).

% to-do
% \subsubsection{Clustering Plan-ful Examples into Plans}
\textbf{Generating clustered plan candidates.}
\label{sec:clustering}
% We perform k-means clustering on the plans \(\mathcal{D}^{\textit{Plan-ful}}\) to identify clusters of similar code pieces and thus, programming plans.
The highest level of granularity provided in PLAID
%presents users with 
are
plan candidates, in the form of clusters of annotated program snippets. To compare the similarity of program snippets, we used CodeBERT embeddings following prior work~\cite{codebert} and applied Principal Component Analysis (PCA) \cite{PCAanalysis} to reduce the dimensionality of the embedding while preserving 90\% of the variance. The snippets were clustered using the K-means algorithm~\cite{kmeansclustering}, using the mean silhouette coefficient for determining optimal K~\cite{silhouettecoeff}. Each cluster is treated as a plan candidate, with the goal, code, and changeable areas from each program snippet in the cluster presented as a suggested value for the plan attributes.
% We used a clustering algorithm to group similar program snippets 
% plan-ful examples together as a programming plan. For clustering the code pieces, we used the CodeBERT model from Microsoft \cite{codebert} to obtain embeddings for each code piece in our dataset of plan-ful examples
% % in \(\mathcal{D}^{\textit{Plan-ful}}\) 
% and applied Principal Component Analysis (PCA) \cite{PCAanalysis} to reduce the dimensionality of the embedding vectors while preserving 90\% of the variance. These embeddings were clustered using the K-means algorithm~\cite{kmeansclustering}. The optimal number of clusters \(\mathcal{K}\) was determined by assessing all possible \(\mathcal{K}\) values 
% % (where \(\mathcal{K} \in [2, \texttt{length}(\mathcal{D}^{\textit{Plan-ful}})]\))
% using the mean silhouette coefficient \cite{silhouettecoeff}. We assigned each example 
% % in \(\mathcal{D}^{\textit{Plan-ful}}\) 
% to a cluster of similar code pieces. 
% \subsubsection*{Extracting Names}
For each plan candidate, a name (see \cref{sec:name_prompt}) that summarizes all snippets in the cluster was generated by prompting an LLM with the contents of the snippets and stating that it should generate \texttt{a name that reflects the code's purpose} and it should focus on \texttt{what the code is achieving and not the context}. 
% Then, all code snippets from each cluster of examples were provided as input to the LLM along with a prompt asking it to \texttt{devise a name for that cluster of plans}.

% \subsection{Interface for Refining Candidate Plans}

% %nd the back-end server relied on routes written in Flask. The domain-specific candidate plans suggested to the user are queried from the database of candidate plans generated using the LLM. Each participant was required to log in to the web page using their unique credentials, which allowed us to record their activity for analysis. While the complete details of our implementation of the web-based application are out of scope for this paper, we describe its main features in Section~\ref{sec:implementation_of_webinterface}.

% \subsubsection{\edit{Preliminary Technical Evaluation of Generated Content}}

% \edit{syntactic validity and standard code complexity metrics to determine
% their suitability for novices}


\begin{figure*}[h]
    \Description{An annotated screenshot of PLAID's Plan Creation view with three panes, with plans shown as boxes on the left. A plan is highlighted, and we see its components on the middle pane. On the rightmost pane, we see suggested values for the selected component.}
    \includegraphics[width=\textwidth]{img/system-diagram-2-new.png}        
    \caption{Plan Identification using PLAID: (h) button that suggests a domain-specific candidate plan from the system database, (i) pane enabling viewing of similar values for the selected plan component, (j) button to view the solution code as part of a complete program, (k) pane with a structured template for plan design with editable fields to refine plan components, (l) button to copy a selected plan, (m) button to mark snippets of code from the plan solution as changeable areas, and (n) a button to group plans together into a category and add a name.}
    \label{fig:system-diagram-2}
\end{figure*}

% \subsubsection{Key Characteristics}
% PLAID supports the process of plan identification in data processing with Pandas, machine learning with Pytorch, web development using Django, and web scraping using BeautifulSoup. 

\subsubsection{Interface for Designing Programming Plans}
Building on the 
%characteristics addressed in the artifact (Section~\ref{sec:design-artifact}) and 
design goals identified in the design workshop (\cref{sec:design-goals}), PLAID enables a set of key interactions to assist instructors in refining candidates to design plans for their instruction. 



\textbf{Interactions for Initial Plan Identification.}
% Initial Plan Identification with Quick Exploration of Many Authentic Programs
While instructors valued the availability of code examples in the design workshop (Section~\ref{sec:design-workshop-findings}), we observed many opportunities for scaffolding their interaction with the reference material. To this end, PLAID presents example programs in two different views \textbf{(DG1)}. 
% We saw instructors scanning examples, selecting desired code pieces, and copying them over into their plan templates in all conditions in the study. 
The ``Programs (Organized by Use Case)'' (\cref{fig:system-diagram-1}a) tab includes a list of use cases where instructors can click on an item to expand the program for that use case.
The ``Programs (Full Text)''  tab (\cref{fig:system-diagram-1}b) lists all the programs and enables instructors to scroll or search through (\cref{fig:system-diagram-1}c) all the code at once.
% presents the contents of all the programs expanded viewing a list of complete code examples, allowing instructors to look at materials they would typically search for when designing plans.
% equipping instructors with full-code programs organized in a list of short natural language descriptions of common use cases in their domain of expertise. 
Both views support directly creating a plan from the whole example (\cref{fig:system-diagram-1}e), or a selected part of it (\cref{fig:system-diagram-1}d), by copying the solution and the goal of the program into an empty plan template
% < Highlight code in full code and code pane in tab1 and make a plan (D1)
% < Add a button to add full program as a plan too (D1)
further supporting efficient use of the material \textbf{(DG3)}.
% This interaction copies over the selected code and its respective use case into the solution and name fields, respectively. 
% < Code explanation plugin for strange syntax (GPT) (D2)

To facilitate understanding unfamiliar code and syntax, we implemented a ``View Explanation'' button (\textbf{DG2}) that generates a short description of the selected line(s) of code by prompting an LLM (\cref{fig:system-diagram-1}f). 
% In this case, participants hesitated to use the suggested syntax in their plans because its functionality was unclear to them. PLAID supports a button named ``View Explanation'' where the user can select a method, function, or line of code that is unclear and click on it to understand its working \textbf{(D2)}. 
Participants also looked for code execution to validate and understand a program. However, since the code snippets instructors work with are often incomplete in this task, we implemented a ``Run Code'' feature (\textbf{DG2}) that predicts the output of a selected code snippet by prompting an LLM to walk through the code \texttt{step by step}, using Chain-of-Thought prompting~\cite{wei2022chain} (\cref{fig:system-diagram-1}g). Only the predicted output for the code is presented, ignoring other output from the LLM.

% to examine the code behavior and thus mitigate the challenge of being faced with unfamiliar syntax. Thus, using PLAID, instructors are able to run complete programs to view their output \textbf{(D2)}.
% < Search in the use cases (and full progs) (D3)
% Frequently, instructors relied on their expertise and experience to formulate ideas about goals for which they wanted to create plans. While interacting with condition C in the design workshop, interviewees suggested including a mechanism to search for specific keywords within code and  its natural language description. To facilitate the instructor-LLM collaboration, allowing users to find examples implementing their ideas, PLAID includes a search bar that helps users navigate the given use cases, complete programs, and effectively find specific examples they may be looking for \textbf{(D3)}.

\textbf{Interactions for Plan Refinement.}
% Support Plan Refinement with Comparisons of content
% Participants indicated difficulty mining plans from code examples (Section~\ref{sec:challenges_practice}). 
To provide suggestions for code patterns common enough to be potential programming plans,
%To alleviate challenges in identifying content common enough for designing plans, 
we utilize the clustered program snippets from our database. In the ``Plan Creation'' view of PLAID, instructors can ask for suggestions (\cref{fig:system-diagram-2}h) to see a candidate plan to refine (\textbf{DG3}).  \edit{This functionality allows instructors to draw on their experience to recognize common code snippets and decide if they are valuable to teach students.}
% If instructors want to demonstrate their plan as part of a complete code example, they can review these examples reducing the effort that they would need to put in to recall syntax and construct a complete example. 
\edit{This promotes recognition over recall \cite{recognition_over_recall}, thus helping reduce the cognitive effort that instructors may have to put in while designing programming plans traditionally.}
To allow instructors to better understand the context of a plan under refinement, PLAID 
also includes a button for searching for the current solution within the entire set of full programs
%, showing the code snippet in context 
%as part of a complete example
(\textbf{DG3}, \cref{fig:system-diagram-2}j).
% < Keyword search/embedding filter for potential values (D1)

As instructors edit the components of a plan, they are shown similar values from the corresponding component in that cluster (\cref{fig:system-diagram-2}i). By clicking on any suggested value, instructors can replace a plan component with a suggestion that better captures that aspect of the plan \textbf{(DG1)}. \edit{By allowing instructors to view the plan they are working on along with other related code pieces in a split screen view, we promote instructor efficiency by reducing the split-attention effect \cite{tarmizi1988guidance}. In the current plan creation process, even when using LLMs from their chat interface, instructors would have to switch between windows with code examples and their text editor which may increase the load on the instructors' working memory \cite{clark2023learning}. In PLAID, instructors can edit their plans and view similar code pieces at the same time.}

% \edit{By enabling these interactions and thus organizing ``knowledge in the world'' effectively, PLAID reduces the need for instructors to store and retrieve the ``knowledge in their head'' \cite{Norman_DOET}. Thus, PLAID optimizes the plan creation process by allowing efficient search within the ``knowledge in the world'' and reducing the cognitive load while storing and retrieving ``knowledge in the head'', minimizing the total effort required \cite{} by instructors.}
% after searching its code corpus for similar examples using a keyword search \textbf{(DG1)}.
% < Show use case button in solution (add highlighting) (D2)
% To help instructors easily consider the context of a plan as they refine it, PLAID 
% In the design workshop, few instructors emphasized the importance of presenting worked and contextualized examples to students. 

% ‘go to a use case’ button that redirects the user to the tab with full code programs and highlights the plan as part of a complete example \textbf{(D2)}.

\textbf{Interactions for Building Robust and Shareable Plan Descriptions.}
% Support robust/sharable plan descriptions
% From Section~\ref{sec:process_intro_plan_design}, instructors indicated drawing on their experience in the application-specific domain and instructional expertise to think about how to best solve a problem. 
PLAID encourages instructors to design plans in a structured template (\cref{fig:system-diagram-2}k). Moreover, PLAID reinforces the plan template by providing a dedicated method for annotating changeable areas by highlighting any part of the code (\textbf{DG3}, \cref{fig:system-diagram-2}m). Instructors can further explain the changeable areas by adding comments as text.
% \edit{The structured template view of the plan encourages instructors to articulate their mental models of how the plan would generalize to other problems, allowing the transfer of ``knowledge in the head'' to ``knowledge in the world''.}

Our design workshop showed that participants would create a plan and copy it to emphasize alternatives or modifications to the underlying idea. To support this workflow,
% In our design workshop, participants created copies of their plans to display alternative solutions to achieve the same goal, emphasizing that multiple possible solutions in code could accomplish the same goal.
% < Duplicating plans (D3)
% To accelerate this process of teaching a variety of possible solutions, 
PLAID allows users to ``duplicate'' plans on the canvas and further edit them to present alternative solutions for the same plan \textbf{(DG3}, \cref{fig:system-diagram-2}l).
% Highlight text from solution to change it to changeable areas (highlighting code itself) (D4)

% In conditions A and B, instructors highlighted the changeable areas in the code itself.
% To allow participants to emphasize the changeable areas in code in PLAID, we implemented the ``add to changeable areas'' button. After selecting the changeable piece of code, clicking on this button highlights the text in a different color and adds it to the box of changeable areas to complete the templated plan design (\textbf{D4}).
% Grouping plans into categories (D4)
% < Multiple selection of the boxes (D4)
% < Naming groups of boxes (D4)
To encourage instructors to think about organizing plans in ways that they would present them to students, PLAID provides an open canvas view for instructors that allows them to arrange plans as they prefer. In addition, PLAID supports a ``grouping'' feature (\cref{fig:system-diagram-2}n), which allows instructors to combine plans with similar goals together into one category (\textbf{DG4}).

% A handful of users postulated each plan as an example question that can be used on assessments. They intended to create multiple variants of the same question for students. They suggested that being able to visualize the different categories would be helpful. Using PLAID, users can select multiple patterns together, add them to a group, and name the group \textbf{(D4)}.  % :(

\subsubsection{System Architecture}
The pipeline to create reference materials is implemented in Python, using the state-of-the-art large language model GPT-4o (Model Version: 2024-05-13). The interface for PLAID is implemented as a web application in Python as a Flask webserver, with an SQLite database. The user-facing interface is implemented using HTML, CSS, and JavaScript, with the canvas interactions realized with the library `\textit{interact.js}'. 


