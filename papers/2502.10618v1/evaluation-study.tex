\section{Evaluation of PLAID}
\label{sec:user-study}

To evaluate PLAID, we aimed to determine if computing instructors were able to use PLAID to identify plans in an application-specific programming domain more efficiently and with a more positive user experience than the current state-of-the-art. 
Specifically, we performed a within-subjects user study to gather insight into (1) instructors' productivity in the plan identification process, (2) the task load for using the system, and (3) the overall usability of PLAID. 
%With a within-subjects user study with 12 participants, we evaluate PLAID's performance in enabling instructors to create programming plans in application-focused domains. In this section, we detail our study design and report the key findings from all phases of the study.



\subsection{Study Design}
\subsubsection{Participants}
\edit{The target end-users for PLAID are computing instructors who intend to create instructional content to teach an application-specific computing course. So, we} recruited four instructors and eight graduate teaching assistants with at least a year of experience in teaching \edit{a programming course (see Table~\ref{tab:participants-evaluation}) at the undergraduate level whether in introductory or upper-level programming courses}. In addition to teaching experience, our inclusion criteria required participants to indicate expertise in at least one application-focused programming domain: data analysis with Pandas (six participants), machine learning using Pytorch (four participants), and web programming with Django (two participants). None of our participants had prior experience identifying plans for instruction. Each participant engaged in a 60-minute design session and was compensated with a \$50 Amazon gift card.

\subsubsection{Procedure}
We conducted a within-subjects study, where each instructor performed plan identification with a baseline condition representing the current state-of-the-art and with PLAID. The study was counterbalanced, with half of our participants seeing the baseline condition first and the other half seeing PLAID first.

Each session began with a description of what a programming plan is, using an example from introductory programming. Then, participants were given 15 minutes in each condition to identify programming plans in their application-focused domain. We prompted them to create these plans as if they will be used in lectures that teach important concepts to students with no experience in their domain. To encourage instructors to undertake a significant amount of plan identification, they were given a suggested target of four to five plans. They were encouraged to continue if they reached this goal before their time was over. %In each condition, participants were tasked with creating five plans or as many as possible within 15 minutes.
After each condition, participants completed the NASA-TLX questionnaire~\cite{cao2009nasa} to indicate their workload while performing the task. % across six dimensions.

\textit{Baseline condition.} Participants worked on an empty Google document. They were given one example programming plan from introductory programming, which they could refer to to understand the expected plan structure. Participants were allowed access to external resources, including web searches, ChatGPT or other AI tools, or their own code and content. 


\textit{PLAID condition.} Participants were given access to the PLAID web interface after a short demonstration of fundamental interactions supported by the system by the interviewer. Like the baseline condition, they were allowed to access any external resources besides the content suggested by PLAID.




\subsubsection{Post-task reflection.}
% In addition to collecting qualitative data in the think-aloud settings of both tasks, we utilized two surveys to quantitatively evaluate (1) the workload of participants under each condition and (2) the usability of PLAID.
 
The sessions ended with a short interview, asking participants for feedback on the system and their opinions on plan-based pedagogy.
Participants also evaluated PLAID with the PSSUQ Version 3 usability survey \cite{pssuq_usability, sauro2016quantifying}. 

\begin{table}
\caption{Demographics of the Participants in our User Study.}
    \centering
    \footnotesize
    \label{tab:participants-evaluation}
    \begin{tabular}{l|cccccc}
    \toprule
            & Domain & Academic Title & \shortstack{Teaching \\ Experience \\ in CS} & \shortstack{Teaching \\ Experience \\ in Domain} & \shortstack{Used \\ Plans in \\ Instruction?}
    \\\midrule
        E1 & Django & Instructor & 20+ & 1-5 & No \\
        E2 & Pandas & Instructor & 1-5 & 1-5 & No \\
        E3 & Django & Instructor & 11-15 & 6-10 & No \\
        E4 & Pytorch & Graduate TA & <1 & <1 & No \\
        E5 & Pandas & Instructor & 1-5 & 1-5 & No \\
        E6 & Pytorch & Graduate TA & 1-5 & <1 & No \\
        E7 & Pandas & Graduate TA & 1-5 & <1 & No \\
        E8 & Pytorch & Graduate TA & 1-5 & <1 & No \\
        E9 & Pandas & Graduate TA & 1-5 & <1 & No \\
        E10 & Pytorch & Graduate TA & 1-5 & <1 & No \\
        E11 & Pandas & Graduate TA & 1-5 & 1-5 & No \\
        E12 & Pandas &  Instructor & 20+ & 6-10 & No \\
    %\\\bottomrule
    \end{tabular}%
\end{table}

\begin{figure}[h]
    \centering
    \Description{A box plot with seven pairs of horizontal bars. Each bar corresponds to one of the measures on NASA TLX, with the top bar being the overall score. Median value for PLAID is better than baseline for all measures, and the difference is significant for overall score, physical demand, and mental demand.}
    \includegraphics[width=\linewidth]{img/cog-load-new.png}
    \caption{Participants' responses on the NASA Task Load Index survey administered after both the baseline condition and PLAID condition. For all items, lower values are preferred. The chart also indicates the results of the Wilcoxon signed rank test between the baseline and PLAID conditions. $**$, $*$, $ns$ indicate $p < 0.01$, $p < 0.05$, and $p > 0.5$ respectively.}
    \label{fig:cognitive-load}
\end{figure}

\begin{figure*}[h]
    \Description{Bar plots showing the distribution of responses for each item across system usability (SYSUSE), information quality (INFOQUAL), and interface quality (INTERQUAL) on the PSSUQ survey. For most items, The majority of items are rated above then the median option (Neither Agree or Disagree).}
    \includegraphics[width=\textwidth]{img/pssuq-chi-25.pdf}
    % \centering
    % \begin{subfigure}{0.48\textwidth}
    %     \centering
    % \includegraphics[width=\textwidth]{img/legend-usability.png}
    % \end{subfigure}
    % \newline
    % \newline
    % \begin{subfigure}{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth, trim=0 4 0 0, clip]{img/sysuse.png}
    %     \caption{SYSUSE}
    %     \label{fig:subfig-a}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth, trim=0 4 0 0, clip]{img/infoqual.png}
    %     \caption{INFOQUAL}
    %     \label{fig:subfig-b}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth, trim=0 4 0 0, clip]{img/interqual.png}
    %     \caption{INTERQUAL}
    %     \label{fig:subfig-c}
    % \end{subfigure}
    
    \caption{Self-reported reflections of participants on the usability of PLAID using the PSSUQ survey. The graph encapsulates their responses for each question across each category on a 7-point Likert Scale.}
    \label{fig:three-horizontal}
\end{figure*}

\subsection{Findings}

% \subsubsection{Does the system work?}

% \subsubsection{Task efficiency}
\subsubsection{PLAID enables instructors to identify plans more efficiently.}
Participants created more plans when using PLAID (4.75 plans on average) compared to the baseline condition (3.92 plans on average).
Seven participants were able to reach the target of identifying five plans in the PLAID condition, whereas only three of twelve participants were able to identify five plans in the baseline condition.
% Most participants created more plans using PLAID than the baseline condition (7 participants).
% Seven participants in the PLAID condition were able to identify five or more plans, whereas only three participants managed to achieve this goal in the baseline condition.
% Participants also spent less time identifying each plan: on average, they spent 8 minutes and 53 seconds per plan in PLAID, compared to the average of 9 minutes and 38 seconds per plan in the baseline. 


%Participants who started identifying plans in the baseline condition were found to identify programming plans the fastest. 

However, there was a wide variety in participants' ability to identify plans, which could be impacted by many factors, such as the individual instructor's content knowledge, the particular domain they are working in, the condition they are in, the condition they started with, and how much time they spent doing plan identification so far.

To understand how other experimental factors affect the time instructors take to identify plans, we built a linear mixed-effects model with the time spent per plan as the outcome variable. Fixed effects were the experimental condition (baseline or PLAID), session order (started with baseline or PLAID), how far into the task the instructor is (number of plans they have identified before this plan), and their domain (Pandas, Django, or PyTorch). The participants were modeled as random effects to control for differences in their expertise and other individual values. Even with a small sample size of 12 instructors and 104 identified plans, we observed a marginally significant coefficient for the experimental condition ($b = -57.2 (sec), t=-1.77, p = .079$) when controlling for these other factors, indicating that instructors were faster in identifying plans using PLAID by almost one minute per plan compared to the baseline. We also observed a statistically significant difference between the specific plans within a task and the time taken to design each plan ($b=129.2 (sec), t=12.85, p < .001$), indicating that instructors spent more time designing each plan for the later plans they suggested, potentially due to starting with easier concepts and moving to more complex ones.

% Although the difference observed was not statistically significant, it could be attributed to the small study sample.

% \subsubsection{Did the system make the process easier?}
% \subsubsection{NASA-TLX measures}

% \begin{figure*}[h]
%     \centering
%     \Description{Bar plot showing the distribution of responses for each item on the PSSUQ survey. For most items, The majority of items are rated above then the median option (Neither Agree or Disagree).}
%     \includegraphics[width=\textwidth]{img/usability-textsize-fixed.png}
%     \caption{Self-reported reflections of participants on the usability of PLAID using the PSSUQ survey. The graph encapsulates their responses for each question on a 7-point Likert Scale.}
%     \label{fig:usability}
% \end{figure*}

\subsubsection{PLAID decreases cognitive demands and overall task load during plan identification.}

% Participants reported a lower overall workload across all measures on NASA-TLX in the PLAID condition (\cref{fig:cognitive-load}). 
We find that the average task load for instructors\footnote{Computed across E2 to E12. E1 was excluded due to procedural error.} was significantly lower with PLAID (\cref{fig:cognitive-load}), indicated by Wilcoxon signed-rank test ~\cite{wilcoxon1992individual} (PLAID: $M = 2.83$ , $SD = 1.40$, Baseline: $M = 3.94$, $SD = 1.57$, $p = .04$). In addition, differences in two sub-measures were statistically significant: mental demand (PLAID: $M = 3.09$, $SD = 1.04$, Baseline: $M = 5.18$, $SD = 1.32$, $p = .008$), and physical demand (PLAID: $M = 1.63$, $SD = 0.92$, Baseline: $M = 2.54$, $SD = 1.75$, $p = .047$).

% \subsubsection{PSSUQ usability survey}
\subsubsection{PLAID provides instructors with a satisfactory experience.}
Participants responded positively to the PLAID user experience as indicated by responses to the PSSUQ survey\footnote{Option 1 indicated strong agreement and Option 7 indicated strong disagreement.} items (see Figure~\ref{fig:three-horizontal}). The responses aggregated into an overall mean of $M = 2.73$ ($SD= 1.49$), $M = 2.42$ for System Usefulness (SYSUSE, $SD= 1.46$), $M = 2.99$ for Information Quality (INFOQUAL, $SD = 1.57$), and $M = 2.81$ for Interface Quality (INTERQUAL, $SD = 1.44$).
%containing 16 items on a 7-point Likert scale .
% feels redundant to say it explicitly here because we report descriptions of each category in the results
% Responses to PSSUQ items are used to compute scores in three subscales, measuring system usability, information quality, and interface quality, in addition to the overall score.


% \subsubsection{How the system made the process easier?}
% \subsubsection{Participant interactions}

% Each participant's system interaction trace is visualized in \cref{fig:trace-diagram}. % We see that provided reference materials and the plan structure were highly utilized...

\begin{figure*}[t!]
    \centering
    \Description{A visualization of participant actions in the system. Each participant is represented as a horizontal sequence of square markers, and the color of the marker corresponds to one of these actions: Create Empty Plan, Create Candidate Plan, Delete Plan, Edit Plan, Annotate Changeable Area, Browse Reference Material, Browse External Material. We can see most instructors use browse reference materials, edit plans, and annotate changeable areas, but there are many individual differences between participants.}
    \includegraphics[width=\textwidth]{img/trace-diagram-vert.pdf}
    \caption{Trace diagram depicting participant interactions with PLAID. Each participant is shown as a horizontal line consisting of a series of actions.}
    \label{fig:trace-diagram}
\end{figure*}

\subsubsection{PLAID scaffolds instructors at multiple stages of the plan design process.}

Using our think-aloud data, post-task interviews, and trace diagrams (see \cref{fig:trace-diagram}), we noted instructors using PLAID to effectively and efficiently identify plans. 
% a variety of authentic instructor behaviors that were supported by PLAID. %In this section, we report the findings from our qualitative analysis and visualize the key interactions supported by PLAID using logged data in .

\textbf{PLAID accelerates plan identification for instructors by providing easily navigable reference material.}
Almost all participants appreciated the example programs included as part of PLAID.
While only some participants utilized the automated suggestions based on clusters of similar code snippets (orange in \cref{fig:trace-diagram}), all participants except E12 primarily interacted with the given reference content by browsing the example programs 
and reading their short descriptions 
%as part of the use cases 
% interaction with the reference material was browsing the example programs and use cases 
(pink in \cref{fig:trace-diagram}). 
 % utilized the program view, and most participants used it repeatedly. 
Participants indicated that developing initial ideas for designing plans was the most challenging stage of plan identification. E10 said it is easier to ``\textit{derive from an existing codebase...because the sample code is the key part}'', clarifying that they believed they were more efficient when using PLAID. 
E6 appreciated the inclusion of ``\textit{readily available code snippets}'' and E11 valued the ``\textit{condensed view}'', expressing that it felt like ``\textit{going through an email inbox}''. 
% Participants not only found the provided use cases advantageous, but also satisfying to explore.
% We also observed that the program view was not just useful, but also satisfying to explore.
Participants found it captivating to browse the list of use cases and search for key concepts. 
% mining fundamental patterns to create plans.
% Participants liked going through the list and finding important patterns to create plans from, even after they were done with the task. 
After they completed their timed task, E2 added ``\textit{I could keep going...I almost just want to read the list at this point.}'' 

While participants were allowed access to alternative reference content, they indicated that PLAID's technique of presenting examples was more suited to their needs. For example, multiple interviewees used ChatGPT to design plans in the baseline condition; however, they still found the process tedious. E9 communicated that the output was verbose and that it was ``\textit{quite an effort to ask even ChatGPT [for ideas]}''. E6 reported that ChatGPT split code into snippets at a different granularity than they would prefer. E5 prompted ChatGPT for ``\textit{things students struggle with when using Pandas}'' but did not find the output appropriate for beginners. ``\textit{I don't even know if I fully understand [this concept]}'', said E5.
While the queries that instructors used to prompt ChatGPT were not so different from the prompts employed as part of PLAID's pipeline,
PLAID prioritizes goal-focused information. More precisely, PLAID shows a brief natural language description of the relevant use case or candidate plan before displaying any code. Without this guidance in ChatGPT, reviewing output might be overwhelming.
% and code snippets are always shown with associated LLM-generated goals.
% the goal-oriented structure of the programs presented in PLAID improved instructors' perceptions about the generated content.
Arguably, a chat interface as the only mode of interaction is challenging, as important information is very spread out and interleaved with verbose explanatory text; participants like E6 spent a long time combining code to abstract high-level ideas from multiple responses given by ChatGPT to design plans.
% deriving high-level ideas from multiple answers generated by ChatGPT.
% <The prompts that instructors used to query ChatGPT in the baseline condition were actually rather similar to the early stages of our content generation pipeline. However, our additional processing of the candidate content seemed to yield more benefits for instructors. ChatGPT's default includes comments on nearly every line -- this doesn't helpful for encouraging abstraction....  >


Participants noted weaknesses in other external reference resources as well. 
% For example, web searches were utilized when both working in PLAID and the baseline condition. 
% However, it was not ideal: 
E10 found it tedious and challenging to compare inconsistent examples from multiple webpages and to find differences between these variable implementations. E8 stated, ``\textit{I know the material for this on the Internet isn't especially good}'' before they transitioned on to reference the code that they authored in the past. Even with their own code, we observed that participants needed to substantially modify their programs to meet the needs of their students. 
For example, E8 copied a snippet from code they wrote for a project and edited it, saying that ``\textit{This isn't necessarily optimal, but it's simple. That should be good for teaching material}''. E12 explained that they added structures they would otherwise not use in a complete program to help students understand (\textit{I'll do it one time, but I won't do it repeatedly}'', said E12). In contrast to other external resources, instructors referenced the documentation, often for reviewing the syntax they wanted to use in their plans.

 

% Participants appreciated the changeable areas, and found it useful for learning
\textbf{PLAID helps instructors create learner-friendly material by providing a structured template.}
Participants valued having a structure for designing plans. E10 found that stating explicit goals was useful for students to ``\textit{get more motivated that [they] know the purpose of learning}'' about the code. 
According to most instructors, the most advantageous part of the plan template were the changeable areas. Instructors perceived these annotations as a strategy of providing support to students. For example, E9 stated: \begin{quote}
``If I'm creating exercises in it, I'm specifying [to students] very clearly that `This is the overall intuition of the coding flow, and these are the areas that you can play with.' It kind of helps me direct the attention of the student towards the exact problem that we should be thinking about.''
\end{quote}
Most instructors used the annotation tool in PLAID to mark changeable areas in their solutions (yellow in \cref{fig:trace-diagram}). We also observed that participants who started with PLAID looked for a similar annotation mechanism in the baseline condition, pointing that there is no ``\textit{intuitive}'' (E9) way to achieve it.  

\textbf{PLAID supports the diverse iterative workflows of instructors.} We observed that instructors preferred to build high-level narratives with their plans, such as designing multiple plans that are all part of a complete program. PLAID's canvas, which shows all the in-progress plans at once, supports this behavior. E10 explained how this view was more helpful compared to the baseline: \begin{quote}
    ``The visual aspect of it [viewing boxes with only plan names], as opposed to seeing the whole thing [written-out plans in the baseline document], made it more modular, I like that abstraction. So I could focus on higher level takeaway of what I want the class to be about, instead of getting fixated of details of each [program].''
\end{quote}
Some participants imitated this process in the baseline by creating a list of initial ideas and then elaborating on each idea with other details. However, we observed that PLAID encouraged participants to keep refining and iterating at various granularities. For example, E8 designed one plan, started exploring the reference material for another, then found a concept that fit the previous plan better, and quickly went back to the previous plan to modify it as well. Similarly, E4 copied a code snippet from the reference material and made some changes, including adding a name and a goal. Then, instead of going back to the reference material or creating an empty plan, they copied the same plan and created another variation on it with small modifications.

\textbf{PLAID offers promise in introducing plan-based pedagogy to application-specific courses.}
Even though instructors did not have prior exposure to plan-focused instruction, instructors had overwhelmingly positive responses when asked about incorporating plan-based pedagogy in their instruction and using PLAID for designing plans. E9 described plan-based instruction as a ``\textit{step-by-step walkthrough of fundamental concepts}''. E11 indicated that learning about programming plans would help students retain common and important tasks that ``\textit{you can never remember the code for}''. Without any prompting, E2 and E3 even requested access to PLAID to design their upcoming courses.
%without any cues.
However, a few participants expressed concerns about using plan-based pedagogies for instruction. For E8, plans were useful for teaching ``\textit{many small individual things}'', but they were uncertain about their usefulness when it came to combining these smaller tasks into larger projects. E1 and E5 found programming plans valuable for conceptual understanding but were hesitant to design their existing course around these structures from scratch. E12 stated that plans could be useful for some learners, but also explained that they would prefer to include executable, full programs in lecture instead.


% For in the course, in a practice oriented course, doing a bunch of compound tasks that consist of.

% 00:47:10.000 --> 00:47:14.000
% Many small individual things that

% 00:47:14.000 --> 00:47:17.000
% You learn in the 1st month or so.

% 00:47:17.000 --> 00:47:18.000
% But

% 00:47:18.000 --> 00:47:26.000
% In increasingly challenging ways. So there are limitations to that structure. I think.

% E8 e


% Most participants were on board with the idea, but few opposed

% Breaking down into parts is hard, suggestions were useful

% Overall, participants found it satisfying.
