\section{LLM Plan Generation Evaluation}

To evaluate the plan-ful examples created by generative AI tools, we performed a mixed methods evaluation 
%using quantitative metrics and qualitative analysis based on 
guided by
the characteristics instructors use to judge good programming plans (see Section ~\ref{sec:judging}).
% commonality, usability, and appropriateness for learners.
%to determine the feasibility of using LLMs in programming plan generation.

% Cunningham et al. \cite{Cunningham_PurposeFirstProgramming_CHI-2021} devised a set of programming plans for BeautifulSoup, a Python library, to assist non-majors learn web scraping 

We assessed our programming plans in reference to a set of programming plans identified and used by Cunningham et al. \cite{Cunningham_PurposeFirstProgramming_CHI-2021} to teach web scraping to undergraduate conversational programmers. These plans were designed by researchers with programming plan expertise as well as instructional experience in the domain, and were also validated with web scraping experts~\cite{Cunningham_PurposeFirstProgramming_CHI-2021}. 
To obtain a control set, we extracted the same set of plan components (name, goal, solution, and changeable areas) from the publicly available curriculum\footnote{runestone.academy/ns/books/published/PurposeFirstWebScraping/index.html} and created clusters of plan-ful examples (denoted as \(\mathcal{D}^{\textit{Control}}\)). %in future. 

We also subsampled the generated plan-ful examples in \(\mathcal{D}^{\textit{Plan-ful}}\) to have an equivalent number of examples as the control set. To achieve this, we chose the 10 largest clusters with the most data points and calculated the centroid of each cluster using the embeddings. We then selected the four closest plans to this centroid as the most representative examples in each cluster (compiled together as \(\mathcal{D}^{\textit{Plan-ful*}}\)). 
% Thus, both \(\mathcal{D}^{\textit{Control}}\) and \(\mathcal{D}^{\textit{Plan-ful*}}\) had the same number of examples.

% Our quantitative analysis focuses on analysis of the plan-ful example solutions, in order to understand their validity across the entire datasets and to compare them with non-LLM generated code. Our qualitative analysis was necessary to gain insight about the non-code components of the plan-ful examples, particularly the qualities of goals and plan names.

\subsection{Quantitative Analysis}

\subsubsection{Syntactic Validity}
\label{sec:quant_accuracy}

Before comparing \(\mathcal{D}^{\textit{Plan-ful*}}\) to \(\mathcal{D}^{\textit{Control}}\), we tested the syntactic validity of the generated programs from our original dataset \(\mathcal{D}\).
%, which included 100 complete Python programs. 
We note that from our set of 100 complete Python programs, all but one were syntactically valid. That program included a syntax error and could not be parsed nor executed. Thus, we conclude that the raw code generated by the LLM is mostly accurate and reliable, at least in our target domain. 

% Summary statistics of these errors and their numbers are presented in ~\ref{table:errors}. On closer examination, we find that most of the compilation errors can be accounted to parsing and data cleaning. ChatGPT does not follow a uniform structure while generating its responses. Thus, it was unlikely to ...

%     \begin{table}
%         \caption{Errors in Non-Compiling ChatGPT Code}
%         \centering
%         \label{tab:errors}
%         \begin{tabular}{|p{4.5cm}|p{3cm}|}
%         \toprule
%                 Error Type & Number of Occurences
%         \\\midrule
%             Unterminated String Literal & 8 \\
%             Invalid Syntax & 9 \\
%             Unterminated f-string Literal & 5 \\
%             Unindent Does Not Match Any Outer Indentation Level & 1 \\
            
%         \end{tabular}
%     \end{table}

% This evidence suggests that while there may be some inaccuracies in data generated using generative AI tools, a lot of them can be controlled using a more systematic data cleaning approach.

\subsubsection{Appropriateness for Learners}
\label{sec:quant_learners_appropriateness}

Our instructors emphasized the importance of plans being suitable for their learner audience (Section~\ref{sec:judging}). Thus, we compared \(\mathcal{D}^{\textit{Plan-ful}}\) to \(\mathcal{D}^{\textit{Control}}\) with standard code complexity metrics
%examined the examples in \(\mathcal{D}^{\textit{Plan-ful}}\) in contrast to \(\mathcal{D}^{\textit{Control}}\) using quantitative metrics on four metrics: 
%average lines of code, cyclomatic complexity~\cite{cyclomatic_complexity_mccabe}, Halstead's volume~\cite{halstead_metrics}, and cognitive complexity~\cite{cognitive_complexity_Campbell} 
to determine their suitability for novices: non-comment lines of code, cyclomatic complexity~\cite{cyclomatic_complexity_mccabe}, Halstead volume~\cite{halstead_metrics}, and cognitive complexity~\cite{cognitive_complexity_Campbell}.


%Cyclomatic complexity~\cite{cyclomatic_complexity_mccabe} and cognitive complexity~\cite{cognitive_complexity_Campbell} are measures of the understandability and modification scope of code, whereas
%,  wherein a lower score indicates that the code is easy to understand and modify. We estimate the cyclomatic complexity of each plan and report the mean score in table~\ref{tab:metrics_appropriateness} across each dataset \(\mathcal{D}^{\textit{Plan-ful}}\) and \(\mathcal{D}^{\textit{Control}}\). 
%Halstead volume~\cite{halstead_metrics} is representative of the size of a program in terms of its operands and operators and hence also reflective of the code's complexity.
%Analogous to cyclomatic complexity, a lower score is indicative of a more straightforward code snippet. Similar to cyclomatic complexity, the mean Halstead volume score for each dataset in reported in table~\ref{tab:metrics_appropriateness}.
%Cognitive complexity is another metric that estimates the understandability of code proposed by Campbell~\cite{cognitive_complexity_Campbell}.

Table \ref{tab:metrics_appropriateness} shows the mean value for all metrics across the datasets. For all metrics, a lower value indicates a simpler program that is more appropriate for beginning learners. We also conducted a two-sided non-parametric Mann-Whitney U-test for each complexity metric.
%with rank biserial correlation as the effect size. 

While \(\mathcal{D}^{\textit{Control}}\) is marginally less complex compared to generated code according to the metrics, we did not find any statistically significant trends (p $>$ 0.05 for all comparisons). Thus, it is reasonable to claim that the examples generated using ChatGPT can be used in instruction for novices.

% 


\begin{table}
\caption{Mean Code Complexity Metrics}
    \centering
    \label{tab:metrics_appropriateness}
    \begin{tabular}{cccc}
    \toprule
        Metric & \thead{\(\mathcal{D}^{\textit{Plan-ful}}\) \\ (n = 781)} & \thead{\(\mathcal{D}^{\textit{Plan-ful*}}\) \\ (n = 40)} & \thead{\(\mathcal{D}^{\textit{Control}}\) \\ (n = 43)}
    \\\midrule
        % Number of Plan-ful Examples & 781 & 40 & 43 \\
        Lines of Code & 2.30 & 3.10 &  2.72 \\
        Cyclomatic Complexity & 2.43 & 2.21 & 2.40 \\
        Halstead Volume & 173.69 & 178.91 & 114.02 \\
        Cognitive Complexity & 0.217 & 0.375 & 0.233 \\
        % Starsinic Readability & -0.362 & \\
    \end{tabular}
\end{table}

% \[
% \begin{matrix}
%   & \(\mathcal{D}^{\textit{Plan-ful*}}\ & \(\mathcal{D}^{\textit{Plan-ful}}\ \\
%   \(\mathcal{D}^{\textit{Control}}\ & & d 
% \end{matrix}
% \]

\subsubsection{Usability}
\label{sec:quant_usability}

Aside from generating code that is accurate and appropriate for learners, it is also important that programming plans be representative of key functionalities in the domain.
%to provide its users with the flexibility and scaffolding to perform a variety of tasks. 
To this end, we compared the number of distinct method calls used in \(\mathcal{D}^{\textit{Plan-ful}}\) and \(\mathcal{D}^{\textit{Control}}\). Having examples on more distinct methods may indicate a set of examples that can be employed to solve a larger number of problems.% (See % (See Figure~\ref{fig:methods-vs-clusters}).

%We found that 
\(\mathcal{D}^{\textit{Control}}\) included four distinct method calls (\texttt{append, find, find\_all, get}), which were also included in \(\mathcal{D}^{\textit{Plan-ful*}}\), the plan-ful examples from the 10 largest clusters generated by the LLM, such as \texttt{select} and \texttt{select\_one}. Moreover, these largest clusters also included five additional methods not included in \(\mathcal{D}^{\textit{Control}}\).
%showing that our clustering algorithm did not cause a loss in method variance. 
This shows that our pipeline generates plans with similar functionality to those designed by an instructor. 



% To examine this behavior more closely, we investigated the number of distinct method calls introduced by each cluster of examples, starting from the largest cluster.
% to illustrate the relationship between the number of distinct methods being used in the clusters as the number of clusters rises. 
% As evident from Figure~\ref{fig:methods-vs-clusters}, 
% By selecting the 10 largest clusters, a majority of distinct methods in the generated set is captured. Moreover, the variance of methods in generated code increases steeply compared to the previously proposed plans.
% almost more of all unique methods present in the set of all generated examples \(\mathcal{D}^{\textit{Plan-ful}}\) are also present in the subset \(\mathcal{D}^{\textit{Plan-ful*}}\). Moreover, 
% follows an upward trend suggesting that more clusters form a larger coverage over the domain's features.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\textwidth]{img/graph-new-pfp-and-gpt.png}
%     \caption{Comparative Analysis of Methods Used As the Number of Clusters Grow in the Plan-ful Examples from ChatGPT}
%     \label{fig:methods-vs-clusters}
% \end{figure}

% We also drew a comparison between the distinct types of methods used in \(\mathcal{D}^{\textit{Control}}\) with \(\mathcal{D}^{\textit{Plan-ful}}\). We find that \(\mathcal{D}^{\textit{Control}}\) encompasses 4 distinct methods including \texttt{['append', 'find', 'find\_all', 'get']} in 6 clusters. The code plans in \(\mathcal{D}^{\textit{Plan-ful}}\) need 7 clusters for addressing these 4 distinct methods. This demonstrates that the ChatGPT generated examples are as varied as the researcher created plans and thus it is unlikely that there will be a loss of variability if examples are generated using AI tools.

% the most common in the D plan-ful have the same methods as D control


\subsubsection{Commonality}
\label{sec:quant_commonality}

A significant motive for using programming plans in instruction is to equip novices with the necessary technical skills to contribute to real-world code problems. Thus, it is essential that plans used in instruction are representative of actual practice.
%to provide learners with the appropriate scaffolding. 
To obtain an estimate of how the LLM-generated plan-ful examples compare to actual practice, we compared \(\mathcal{D}^{\textit{Plan-ful}}\) and \(\mathcal{D}^{\textit{Control}}\) to web scraping files from GitHub. We created a new dataset \(\mathcal{D}^{\textit{GitHub}}\) by collecting Python files from public repositories via GitHub's API that met the following criteria: contained a BeautifulSoup import statement, included the BeautifulSoup contructor, and was not a test file. 
%that were written in Python and included the string \textit{BeautifulSoup}
%in any files in any public repository using the publicly available GitHub REST API. This resulted in a collection of 1016 files. We filtered this dataset to include files that had an import statement (\textit{import BeautifulSoup, import bs4, from BeautifulSoup import BeautifulSoup} or \textit{from bs4 import BeautifulSoup} and a constructor initialization (\textit{= BeautifulSoup(}). Finally, we removed test files by filtering any of the following statements: \textit{import unittest, (unittest.TestCase), from bs4.testing import}. 
This resulted in the final dataset with 733 files. Then, we generated the embeddings for these programs using CodeBERT in a similar manner to Section~\ref{sec:clustering} to compare the sets \(\mathcal{D}^{\textit{GitHub}}\) and \(\mathcal{D}^{\textit{Control}}\) as well as \(\mathcal{D}^{\textit{GitHub}}\) and \(\mathcal{D}^{\textit{Plan-ful*}}\).

% To compare the commonality of generated plans \(\mathcal{D}^{\textit{Plan-ful*}}\) to \(\mathcal{D}^{\textit{Control}}\), we compute the distance between embeddings of the code in each dataset to \(\mathcal{D}^{\textit{GitHub}}\). The embeddings for the new datasets are computed with CodeBERT as similar to the prior Section~\ref{sec:clustering}. %, denoted as \(\mathcal{D}^{\textit{GitHub}}\).

%We now compare \(\mathcal{D}^{\textit{Plan-ful}}\), \(\mathcal{D}^{\textit{Control}}\), and \(\mathcal{D}^{\textit{GitHub}}\) where \(\mathcal{D}^{\textit{GitHub}}\) is our baseline, \(\mathcal{D}^{\textit{Plan-ful}}\) is our test set, \(\mathcal{D}^{\textit{Control}}\) is our control set. 
% Using a number of quantitative metrics, we perform analysis that yields information about how the plans compare to real-world code. We first convert the code pieces in \(\mathcal{D}^{\textit{Control}}\) and \(\mathcal{D}^{\textit{GitHub}}\) to embeddings using CodeBERT like before. For \(\mathcal{D}^{\textit{Plan-ful}}\), we use the previously collected embeddings from \(\mathcal{D}^{\textit{Emb}}\). 

% For our first experiment, we compute the average embedding per dataset using a mean of values across each dimension. This yields one multi-dimensional vector per dataset representative of the average embedding of that dataset. Then we perform a cosine distance analysis using functions from the \texttt{sklearn} library in Python on these pairs: \(\mathcal{D}^{\textit{Control}}\) and \(\mathcal{D}^{\textit{GitHub}}\) and \(\mathcal{D}^{\textit{Plan-ful}}\) and \(\mathcal{D}^{\textit{GitHub}}\) to gain insight into which dataset is more similar to the baseline. Cosine distance is measured on a scale of 0 to 2 where a value closer to 0 indicates high similarity and a value nearer to 2 represents high dissimilarity. We observe a score of 0.188 between \(\mathcal{D}^{\textit{Plan-ful}}\) and \(\mathcal{D}^{\textit{GitHub}}\) and 0.213 between \(\mathcal{D}^{\textit{Control}}\) and \(\mathcal{D}^{\textit{GitHub}}\). Accordingly we infer that the example plans generated using ChatGPT are closer to real-world practice than the purpose-first programming plans. This can be accounted by the fact that LLMs like ChatGPT are trained on public data sources \cite{} whereas the purpose-first programming plans were created by instructors and researchers crafted for novices. We also note the similarity between \(\mathcal{D}^{\textit{Plan-ful}}\) and \(\mathcal{D}^{\textit{Control}}\) which is 0.012 which indicates that the LLM generated plans and the purpose-first programming plans are highly similar.

To evaluate the similarity between sets, we computed Hausdorff distance~\cite{tahaEfficientAlgorithmCalculating2015} and Wasserstein distance~\cite{ramdasWassersteinTwoSample2015}, which are common metrics for comparing generated content to reference sets~\cite{pmlr-v70-arjovsky17a, weilihausdorff}. For both Hausdorff (\(\mathcal{D}^{\textit{Plan-ful}}=13.66\), \(\mathcal{D}^{\textit{Control}}=14.92\)) and Wasserstein (\(\mathcal{D}^{\textit{Plan-ful}}=12.97\), \(\mathcal{D}^{\textit{Control}}=13.97\)) distances, the set of generated examples had smaller distance to code from GitHub in comparison to the control set of previously proposed plans, conveying that the ChatGPT can generate plan-ful code that is more representative of real-world examples compared to instructor code.


% We compute Hausdorff distance between sets of embeddings, which is a measure of the maximum distance from a random point in a set to a reference set~\cite{tahaEfficientAlgorithmCalculating2015}. A smaller Hausdorff distance indicates that points for all points in the set have a close corresponding point in the reference set. We found that the distance between \(\mathcal{D}^{\textit{Plan-ful}}\) and \(\mathcal{D}^{\textit{GitHub}}\) as 13.66 and the distance between \(\mathcal{D}^{\textit{Control}}\) and \(\mathcal{D}^{\textit{GitHub}}\) as 14.92, suggesting that the ChatGPT generated code pieces are more similar to GitHub examples compared to the plan-ful examples from the purpose-first programming study.

% We also compute the Wasserstein Distance, which is a measure of the similarity of the probability distributions of two sets~\cite{ramdasWassersteinTwoSample2015}. % cost of an optimal transport plan to convert one distribution into the other \cite{}, using in-built functions in the \texttt{scipy} library in Python, between the pairs of the datasets. Analogous to the prior result, 
% Analogous to the previous result, the distance between \(\mathcal{D}^{\textit{Plan-ful}}\) and \(\mathcal{D}^{\textit{GitHub}}\) (12.97) is lesser than the distance between \(\mathcal{D}^{\textit{Control}}\) and \(\mathcal{D}^{\textit{GitHub}}\) (13.97), 
%We also measure the Hausdorff Distance, measure of the discrepancy between two sets of points \cite{}, using the same Python library, among the datasets which also yielded similar results with the score between 

%These findings suggest that examples generated using AI tools are more likely to be used in common code practice, over examples generated by instructors from scratch.

\subsection{Qualitative Evaluation}

To obtain a richer picture of the strengths and weaknesses of plan generation with LLMs, we conducted a qualitative evaluation of the generated plan-ful examples, inspired by thematic analysis approaches in prior work on code generation~\cite{kazemitabaarHowNovicesUse2023}.

We started our analysis with a free-form discussion on both generated plans and previously proposed plans from \(\mathcal{D}^{\textit{Control}}\) to familiarize ourselves with the data. One member of the research team prepared an initial codebook, with codes organized under two main dimensions reflecting the \textit{components} and \textit{characteristics} highlighted in Section \ref{sec:interview_results}.
Two researchers coded a subset of examples (10\% of the data) and obtained inter-rater reliability of 0.76 using percentage agreement\cite{miles1994qualitative}. The codebook was refined through discussion, and two researchers achieved an IRR of 0.89 after the second subset. One member of the team coded the rest of the data according to the refined codebook\footnote{The codebook is available online: https://tinyurl.com/fk6pzat8}.

\subsubsection{Components}
\label{sec:qual_components}

The generated plan-ful examples were `mostly accurate' (90\%, n=36). Only four examples in \(\mathcal{D}^{\textit{Plan-ful*}}\) had `mostly inaccurate' code, indicating that LLMs can generate the solution component of a plan reliably.

Changeable areas of the examples were also somewhat successfully generated: there was only a single case where an unalterable part of the code was annotated as a changeable area. Yet, 22.5\% of examples were missing changeable areas (n=9), and another 22.5\% had changeable areas that were considered `improbable' (n=9). For example, some default arguments of the commonly used functions were annotated as changeable. While technically correct, these areas are not likely to be modified in simpler examples and were not included in previously proposed plans from \(\mathcal{D}^{\textit{Control}}\).

The generation of goals and names was less satisfactory. On the example level, more than half of the generated goals were `descriptive' (55\%, n=22), but 17.5\% of examples were missing a goal label (n=7), and 12.5\% of examples had an `insufficient' or `too general' goal (n=5). On the cluster level, only 40\% of generated names were `descriptive' (n=4), with other names either being `insufficient' to understand when to use a plan (n=2) or `overstating' what the plan actually does (n=4). For example, a cluster that accesses multiple attributes of an object was named ``Data Extraction and Database Management'', even though it does not have any database interaction.

\subsubsection{Characteristics}
\label{sec:qual_characteristics}

The most consistent characteristic in generated examples was commonality: 80\% of examples had `common syntax' with plans placed in the same cluster (n=32) and 67.5\% of them had `common goals' with the plans in the cluster (n=27). Another 12.5\% of examples shared `vague commonalities' (n=5), where it was hard to find the overall goal of the cluster due to great differences in syntax and structure. Moreover, some code statements were repeated in multiple plans (30\%, n=12), and the code for shorter plans, such as importing libraries or calling the BeautifulSoup constructor, was also included within some of the larger plans.

From a usability perspective, most plans were `cohesive' examples of a given use case (67.5\%, n=27), and they were `generalizable' to new contexts (57.5\%, n=23). Moreover, some of the shorter plans did not require customization but could still be useful to students, e.g. ``Importing Libraries''.
% However, some of the other plans were so simple that they would not be beneficial to solve a new problem.
% For example, one of the plans was ``Importing Libraries'' and it just included a set of import statements.

Finally, the appropriateness of the generated content for beginners was questionable: while there were similarities to the ones defined in \(\mathcal{D}^{\textit{Control}}\), 42.5\% of plans used `technical jargon' in the name and goals (n=17). These included revealing some web technologies that were abstracted away in the previously proposed plans, such as GET requests and HTML structure, as seen in Figure \ref{fig:contrasting-cases}. Furthermore, some plans included `advanced concepts' in Python (15\%, n=6) such as list comprehensions or exception handling. 

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{img/contrasting-cases.pdf}
    \caption{Two LLM-generated plan-ful examples from the same cluster, with an example almost identical to an instructor-generated plan from prior work (top), and an example that includes technical jargon and improbable changeable areas, making it potentially confusing for novices (bottom).}
    \label{fig:contrasting-cases}
\end{figure}

% Abstractness


%%%Accuracy

% 36/40 mostly accurate

%%%Quality of Generated Components

%% names
% 4 descriptive
% 2 insufficient
% 4 overstated


%% subgoals
%22 of 40 descriptive
%7 missing
%4 insufficient or 1 too general 

%% changeable areas
%9 missing
%9 improbable
%1 inaccurate

%%%characteristics
% 32 common syntax
% 27 common goal
% 5 vague commonality
% 12 common across

% 27 cohesive and 23 generalizable

% 17 technical jargon
% 6 advanced, 7 confusing


%Appropriateness to Beginners


% \subsection{Results}


%  Accuracy

% A. Are the clusters cohesive?
% B. Are the changeable areas, goal, name, solution accurate?


% Qual


% A. Look at each of the parts
% 	- what do we see



% Quant 

% A. Basic metrics comparison (usability for beginners)
% - lines of code
% - cyclomatic complexity

% B. Embedding distance comparison (commonality)
% - 1. Comparison to GitHub (using prior method)
%     - Which average is closer
%     - How many clusters are closer
%   - # of anomalites