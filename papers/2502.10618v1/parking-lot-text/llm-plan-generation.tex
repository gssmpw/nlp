% \begin{figure}
% \centering
% % \includegraphics[width=0.5\textwidth]{img/pipeline-new.png}
% \includegraphics[width=\textwidth]{img/new-plan-pipeline.png}
% \caption{The three stage process for generating example programs, segmenting them with plan components, and clustering these plan-ful examples.
% %collecting and processing responses from ChatGPT into plan-ful examples}
% %\caption{The pipeline for LLM plan generation.}
% }
% \label{fig:llm-methods}
% \end{figure}

\subsubsection{Generating In-Domain Programs}
% Informed by the insights identified in our interview study, we generated programming plans relevant to an application-focused domain: web scraping via BeautifulSoup. We utilized an LLM-based approach to generate these plans with the GPT-4 model from OpenAI using its publicly available API in an iterative workflow. 

Our participants examined example programs and conducted literature reviews (Section \ref{sec:viewing-programs}) as key parts of their plan identification process. Inspired by this, we used Open AI's GPT-4, a state-of-the-art large language model for code generation that is trained on a large corpus of computer programs~\cite{liu2023isyourcode}, to generate candidate programs along with its respective plan components in the programs. First, we prompted the model to generate \texttt{100 use cases of using BeautifulSoup}. Subsequently, we asked the model to \texttt{write pieces of code that use BeautifulSoup to achieve <use case>}. 
% This collection of example programs (which we refer to as 
%dataset 
% $\mathcal{D}$) was used as our primary dataset for further analysis.


\subsubsection{Segmenting Programs Into Plan-ful Examples}
% We then proceed to compile these examples with each of the plan components generated using ChatGPT. We construct a new dataset with these components, Dataset \((\mathcal{D}^{\textit{Comp}})\).
We used the generated programs from
% \mathcal{D}$
the prior step as the input in a set of prompts (see Stage 2 in Figure~\ref{fig:llm-methods}), where each prompt was used to generate one of the plan components identified in our interview study (see Section \ref{sec:components}). 
%We fragmented these generated programs into smaller code pieces by generating \textit{subgoals} in the program. Then, each goal (Section \ref{sec:goal}) and the accompanying code solution (Section \ref{sec:solution}) were added as a single unit of data in our plan-ful example dataset of components, \(\mathcal{D}^{\textit{Plan-ful}}\). For each of these datapoints, we prompted the model to identify \textit{changeable areas} (Section \ref{sec:changeable}). %The name (Section \ref{sec:name}) was determined later in the pipeline (Stage 2 in Figure \ref{fig:llm-methods}).


% From the results of our qualitative study, we now know about the parts of a programming plan. In order to extract these plans automatically, we used ChatGPT. We accessed it using its publicly available API and we used the GPT-4 model. We selected 3 domains that are interesting for non-majors. This included . 

% For each of these domains, we first asked the LLM to generate 100 use cases. We then re-prompted it with the use cases it generated and asked it to generate code that would be written to accomplish that use case.
% potential for another table?
% add code metrics from stackoverflow github work for chatgpt
% With all these code pieces collected, we then asked ChatGPT to generate each of the plan parts one-by-one.

\subsubsection*{Extracting Goals and Solutions}Generated programs 
% in \(\mathcal{D}\) 
typically included a comment before each line, which described that line's functionality. However, these comments did not capture the high-level purpose of the code, as required by a plan goal. To generate more abstract goals for a piece of code, we defined subgoals as \texttt{short descriptions of small pieces of code that do something meaningful} in a prompt and asked the LLM to \texttt{highlight subgoals as comments in the code.} %In our query, we also added the way we define subgoals to provide the relevant context to the model. Specifically, we wrote that 
The output from this prompt was a modified version of each program
% from \(\mathcal{D}\), 
where blocks of code are preceded by a comment describing the goal of that block. % of code. % instead of restating functionality. 

We split each complete program into multiple segments based on these new comments. Thus, the subgoal comments from each complete program I
% n the modified \(\mathcal{D}\) 
became a plan goal, and the code following that comment became the associated solution. %, collected in \(\mathcal{D}^{\textit{Plan-ful}}\). % After it returned the annotated code piece, we extracted the comment and the following lines of code before the next comment. This pair acted as a subgoal-code piece. We collected all such pairs across all use cases from \(\mathcal{D}\) and added them to \(\mathcal{D}^{\textit{Plan-ful}}\).
Each goal 
%(Section \ref{sec:goal}) 
and solution pair
%(Section \ref{sec:solution}) 
was added as a single unit of data in our plan-ful example dataset.
% , \(\mathcal{D}^{\textit{Plan-ful}}\).

\subsubsection*{Extracting Changeable Areas}To annotate the changeable areas for a plan, we defined changeable areas as \texttt{parts of the plan that would change when it is used in a different context} in our prompt and asked the model to \texttt{return the exact part of the code from the line that would change} for all code pieces from the dataset with plan-ful examples.
% from \(\mathcal{D}^{\textit{Plan-ful}}\). 
% This data was added to \(\mathcal{D}^{\textit{Plan-ful}}\).

% to-do
\subsubsection{Clustering Plan-ful Examples into Plans}
\label{sec:clustering}
% We perform k-means clustering on the plans \(\mathcal{D}^{\textit{Plan-ful}}\) to identify clusters of similar code pieces and thus, programming plans.

We used a clustering algorithm to group similar plan-ful examples together as a programming plan. For clustering the code pieces, we used the CodeBERT model from Microsoft \cite{codebert} to obtain embeddings for each code piece in our dataset of plan-ful examples
% in \(\mathcal{D}^{\textit{Plan-ful}}\) 
and applied Principal Component Analysis (PCA) \cite{PCAanalysis} to reduce the dimensionality of the embedding vectors while preserving 90\% of the variance. These embeddings were clustered using the K-means algorithm~\cite{kmeansclustering}. The optimal number of clusters \(\mathcal{K}\) was determined by assessing all possible \(\mathcal{K}\) values 
% (where \(\mathcal{K} \in [2, \texttt{length}(\mathcal{D}^{\textit{Plan-ful}})]\))
using the mean silhouette coefficient \cite{silhouettecoeff}. We assigned each example 
% in \(\mathcal{D}^{\textit{Plan-ful}}\) 
to a cluster of similar code pieces. 

\subsubsection*{Extracting Names}
To generate names for the plan-ful examples, we first defined the properties for a name in the prompt by expressing that \texttt{a name reflects the code's purpose} and it should focus \texttt{what the code is achieving and not the context.} Then, all code snippets from each cluster of examples were provided as input to the LLM along with a prompt asking it to \texttt{devise a name for that cluster of plans}.