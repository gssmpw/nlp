\section{Related Work}
\textbf{Instability of Training Large Language Models.} 
The instability of large language model (LLM) training, which are marked by loss spikes and catastrophic divergence\citep{chowdhery2023palm,molybog2023theory}, has driven extensive research into stabilization techniques. These methods generally fall into three main categories: (1) gradient preprocessing, (2) architectural modifications, and (3) initialization strategies. Gradient preprocessing typically involves scaling and clipping gradients at the start of the optimization process to stabilize the training. A well-known example is gradient clipping \citep{goodfellow2016deep}, which globally rescales  the gradient norm to a fixed value. Later, Adafactor \citep{shazeer2018adafactor} introduced capping the norm of the parameter updates instead of the raw gradients. More recently, SPAM \citep{huang2025spam} proposed detecting and clipping anomalous gradients based on historical gradient statistics. However, a common drawback of these methods is that they require manually setting a predefined threshold. Architecturally, \citet{xiong2020layer} showed that Post-LayerNorm (Post-LN) amplifies gradients, causing instability with large learning rates, while Pre-LayerNorm (Pre-LN) preserves gradient norms for stable training. Embed LayerNorm (Embed LN) normalizes embeddings\citep{dettmers20218}, though it may impact performance\citep{scao2022language}, while Embed Detach\citep{ding2021cogview,zeng2022glm} reduces loss spikes by truncating gradients. DeepNorm\citep{wang2024deepnet} scales residual connections to stabilize ultra-deep models, and $\alpha$Reparam\citep{zhai2023stabilizing} prevents attention entropy collapse via spectral-normalized parameterization. Initialization strategies offer complementary stability benefits. Scaled Embed\citep{takase2023spike} stabilizes LayerNorm gradients, while Scaled Initialization\citep{nguyen2019transformers} reduces variance using $\mathcal{N}(0, \sqrt{2/(5d)}/\sqrt{2N})$. Fixup\citep{zhang2019fixup,huang2020improving} eliminates LayerNorm entirely, inspiring norm-free architectures. Though ongoing advancements refine these approaches, training stability remains a key challenge in LLM development.



\textbf{Low-precision LLM Training.} Low-precision training~\cite{wang2018training,lin2022device,xi2024coat,xi2024jetfire,wortsman2023stable} has emerged as a promising approach to improve both computational and memory efficiency during training. Among these methods, FP16~\cite{micikevicius2017mixed} and BF16~\cite{kalamkar2019study} are the most widely adopted precision formats. To push the efficiency further, 8-bit training has garnered increasing attention. For instance, LM-FP8~\cite{peng2023fp8} enables training with FP8 precision While \cite{fishman2024scaling} demonstrates that as training scales up (larger than 250B tokens), the issue of activation outliers becomes more pronounced, posing challenges to the representation range of low-bit data formats. To address this challenge, \cite{fishman2024scaling} proposes a smoothing strategy, while \cite{ashkboos2025halo} leverages Hadamard transformations to mitigate the impact of activation outliers. Furthermore, the choice of data format significantly influences training performance. The INT8 format is the most widely supported low-precision format, whereas FP8, available in NVIDIAâ€™s Hopper GPU architecture, provides specialized support. Additionally, the MX format~\cite{rouhani2023microscaling} demonstrates superior representational capability, though it is rarely supported by current hardware. In this work, we investigate the training instability associated with low-precision training and propose enhancements through the design of optimizers. Our approach is compatible with existing techniques, providing a complementary solution to improve the stability of low-precision training.