\section{Related Work}
\textbf{Instability of Training Large Language Models.} 
The instability of large language model (LLM) training, which are marked by loss spikes and catastrophic divergence____, has driven extensive research into stabilization techniques. These methods generally fall into three main categories: (1) gradient preprocessing, (2) architectural modifications, and (3) initialization strategies. Gradient preprocessing typically involves scaling and clipping gradients at the start of the optimization process to stabilize the training. A well-known example is gradient clipping ____, which globally rescales  the gradient norm to a fixed value. Later, Adafactor ____ introduced capping the norm of the parameter updates instead of the raw gradients. More recently, SPAM ____ proposed detecting and clipping anomalous gradients based on historical gradient statistics. However, a common drawback of these methods is that they require manually setting a predefined threshold. Architecturally, ____ showed that Post-LayerNorm (Post-LN) amplifies gradients, causing instability with large learning rates, while Pre-LayerNorm (Pre-LN) preserves gradient norms for stable training. Embed LayerNorm (Embed LN) normalizes embeddings____, though it may impact performance____, while Embed Detach____ reduces loss spikes by truncating gradients. DeepNorm____ scales residual connections to stabilize ultra-deep models, and $\alpha$Reparam____ prevents attention entropy collapse via spectral-normalized parameterization. Initialization strategies offer complementary stability benefits. Scaled Embed____ stabilizes LayerNorm gradients, while Scaled Initialization____ reduces variance using $\mathcal{N}(0, \sqrt{2/(5d)}/\sqrt{2N})$. Fixup____ eliminates LayerNorm entirely, inspiring norm-free architectures. Though ongoing advancements refine these approaches, training stability remains a key challenge in LLM development.



\textbf{Low-precision LLM Training.} Low-precision training____ has emerged as a promising approach to improve both computational and memory efficiency during training. Among these methods, FP16____ and BF16____ are the most widely adopted precision formats. To push the efficiency further, 8-bit training has garnered increasing attention. For instance, LM-FP8____ enables training with FP8 precision While ____ demonstrates that as training scales up (larger than 250B tokens), the issue of activation outliers becomes more pronounced, posing challenges to the representation range of low-bit data formats. To address this challenge, ____ proposes a smoothing strategy, while ____ leverages Hadamard transformations to mitigate the impact of activation outliers. Furthermore, the choice of data format significantly influences training performance. The INT8 format is the most widely supported low-precision format, whereas FP8, available in NVIDIAâ€™s Hopper GPU architecture, provides specialized support. Additionally, the MX format____ demonstrates superior representational capability, though it is rarely supported by current hardware. In this work, we investigate the training instability associated with low-precision training and propose enhancements through the design of optimizers. Our approach is compatible with existing techniques, providing a complementary solution to improve the stability of low-precision training.