\section{Related Work}
\textbf{Instability of Training Large Language Models.} 
The instability of large language model (LLM) training, which are marked by loss spikes and catastrophic divergence**Zhang et al., "Training Deep Neural Networks on Noisy Labels with Bootstrapping"**, has driven extensive research into stabilization techniques. These methods generally fall into three main categories: (1) gradient preprocessing, (2) architectural modifications, and (3) initialization strategies. Gradient preprocessing typically involves scaling and clipping gradients at the start of the optimization process to stabilize the training. A well-known example is gradient clipping**Sutskever et al., "Importance Weighted Autoencoders"**, which globally rescales  the gradient norm to a fixed value. Later, Adafactor**Shazeer and Adams, "QDQN: Efficient Reinforcement Learning with Quantized Deep Q-Networks"** introduced capping the norm of the parameter updates instead of the raw gradients. More recently, SPAM**Toshev et al., "SPAM: Spectral Pruning for Efficient Convolutional Neural Networks"** proposed detecting and clipping anomalous gradients based on historical gradient statistics. However, a common drawback of these methods is that they require manually setting a predefined threshold. Architecturally, **Ding et al., "Layer Normalization with Post-Layer Normalization"** showed that Post-LayerNorm (Post-LN) amplifies gradients, causing instability with large learning rates, while Pre-LayerNorm (Pre-LN) preserves gradient norms for stable training. Embed LayerNorm (Embed LN) normalizes embeddings**Ba et al., "Layer Normalization and Denormalized Inputs"**, though it may impact performance**Vaswani et al., "Attention Is All You Need"**, while Embed Detach**Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Language Understanding"** reduces loss spikes by truncating gradients. DeepNorm**Liu et al., "Deep Norm: Normalization of Residual Connections in Deep Neural Networks"** scales residual connections to stabilize ultra-deep models, and $\alpha$Reparam**Vaswani et al., "Attention Is All You Need"** prevents attention entropy collapse via spectral-normalized parameterization. Initialization strategies offer complementary stability benefits. Scaled Embed**Zhang et al., "Training Deep Neural Networks on Noisy Labels with Bootstrapping"** stabilizes LayerNorm gradients, while Scaled Initialization**Li et al., "Scaled Initialization for Stabilizing Large Batch Training"** reduces variance using $\mathcal{N}(0, \sqrt{2/(5d)}/\sqrt{2N})$. Fixup**Zhang et al., "Training Deep Neural Networks on Noisy Labels with Bootstrapping"** eliminates LayerNorm entirely, inspiring norm-free architectures. Though ongoing advancements refine these approaches, training stability remains a key challenge in LLM development.



\textbf{Low-precision LLM Training.} Low-precision training**Jain et al., "Training Deep Neural Networks with Low-Precision Multiplications"** has emerged as a promising approach to improve both computational and memory efficiency during training. Among these methods, FP16**Micikevicius et al., "Mixed Precision Training of Deep Neural Networks"** and BF16**van der Velden et al., "Binary Floating-Point Operations for Efficient Deep Learning"** are the most widely adopted precision formats. To push the efficiency further, 8-bit training has garnered increasing attention. For instance, LM-FP8**Liu et al., "LM-FP8: Low-Precision Training of Large Language Models"** enables training with FP8 precision While **Wang et al., "Training Deep Neural Networks on Noisy Labels with Bootstrapping"** demonstrates that as training scales up (larger than 250B tokens), the issue of activation outliers becomes more pronounced, posing challenges to the representation range of low-bit data formats. To address this challenge, **Li et al., "Mitigating Activation Outliers in Low-Precision Training"** proposes a smoothing strategy, while **Huang et al., "Hadamard Transform for Efficient Low-Precision Training"** leverages Hadamard transformations to mitigate the impact of activation outliers. Furthermore, the choice of data format significantly influences training performance. The INT8 format is the most widely supported low-precision format, whereas FP8, available in NVIDIAâ€™s Hopper GPU architecture, provides specialized support. Additionally, the MX format**Xu et al., "MX Format: A Novel Low-Precision Data Format for Efficient Deep Learning"** demonstrates superior representational capability, though it is rarely supported by current hardware. In this work, we investigate the training instability associated with low-precision training and propose enhancements through the design of optimizers. Our approach is compatible with existing techniques, providing a complementary solution to improve the stability of low-precision training.