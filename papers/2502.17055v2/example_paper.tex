%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{caption}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{algorithm2e}
\usepackage{pifont} 
\usepackage{bm}
\usepackage{enumitem}
\usepackage{multirow}
% \usepackage{itemize}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:


% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Packages for the algorithm
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e} % algorithm2e for the algorithm formatting
\usepackage[accepted]{icml2025}
% Math packages
% \usepackage{amsmath} % for equations
% \usepackage{amssymb} % for symbols like \mathbb{R}
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% For colored text
\usepackage{xcolor}

% Optional: For adjusting page layout
% \usepackage[margin=1in]{geometry}

\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.90}
\newcolumntype{g}{>{\columncolor{Gray}}c}
\definecolor{ffe1da}{RGB}{255,225,218}
\definecolor{F7E0D5}{RGB}{247,224,213}
\definecolor{darkF7E0D5}{RGB}{209,154,128}
\usepackage{anyfontsize}
% \usepackage{lmodern}
\usepackage{xcolor}
\newcommand{\shiwei}[1]{\textcolor{orange}{Shiwei:#1}}
\newcommand{\HTJ}[1]{\textcolor{blue}{HTJ:#1}}
\newcommand{\XL}[1]{\textcolor{yellow}{(XL:#1)}}
\newcommand{\GJ}[1]{\textcolor{green}{(GJ:#1)}}

\newcommand{\bdelta}{\boldsymbol\delta}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\btheta}{\boldsymbol\theta}
\newcommand{\bmm}{\mathbf{m}}
\newcommand{\bc}{\mathbf{c}}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{\node[shape=circle,draw,inner sep=0.4pt] (char) {#1};}}

\newcommand{\gr}{\rowcolor[gray]{.95}}
\newcommand{\go}{\rowcolor[gray]{2}{.95}}
\newcommand{\ours}{\texttt{Stable-SPAM}\xspace}

% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \usepackage{amsmath,amssymb}
\usepackage{xcolor}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
% \usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

% \input{macro}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\texttt{Stable-SPAM}:  How to Train in 4-Bit More Stably than 16-Bit Adam}

\begin{document}

\twocolumn[
\icmltitle{\texttt{Stable-SPAM}:  How to Train in 4-Bit More Stably than 16-Bit Adam}

\icmlsetsymbol{equal}{*}
\icmlsetsymbol{corr}{\dagger}

\begin{icmlauthorlist}
\icmlauthor{Tianjin Huang*}{1,2}
\icmlauthor{Haotian Hu*}{3}
\icmlauthor{Zhenyu Zhang*}{4}
\icmlauthor{Gaojie Jin}{1}
\icmlauthor{Xiang Li}{5}\\
\icmlauthor{Li Shen}{6}
\icmlauthor{Tianlong Chen}{7}
\icmlauthor{Lu Liu}{1}
\icmlauthor{Qingsong Wen}{8}
\icmlauthor{Zhangyang Wang}{4}
%\icmlauthor{}{sch}
\icmlauthor{Shiwei Liu}{2,9}

%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{1}{Department of Computer Science, University of Exeter}
\icmlaffiliation{2}{Department of Mathematics and Computer Science, Eindhoven University of Technology}
\icmlaffiliation{3}{School of the Gifted Young, University of Science and Technology of China}
\icmlaffiliation{4}{Department of Electrical and Computer Engineering, University of Texas at Austin}
\icmlaffiliation{5}{Department of Computer Science, University of Reading}
\icmlaffiliation{6}{School of Cyber Science and Technology, Sun Yat-sen University}
\icmlaffiliation{7}{Department of Computer Science, The University of North Carolina at Chapel Hill}
\icmlaffiliation{8}{Squirrel Ai Learning}
\icmlaffiliation{9}{Mathematical Institute, University of Oxford}




\icmlcorrespondingauthor{Tianjin Huang}{t.huang2@exeter.ac.uk}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

% 4-bit training is a promising solution to reduce the memory and computational cost of large language models (LLMs). 
This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featuring momentum reset and spike-aware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. 
To address these limitations, we propose \texttt{Stable-SPAM}, which incorporates enhanced gradient normalization and clipping techniques. In particular, \texttt{Stable-SPAM} $(1)$ adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; $(2)$ normalizes the entire gradient matrix based on its historical $l_2$-norm statistics; and $(3)$ inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that \texttt{Stable-SPAM} effectively stabilizes gradient norms in 4-bit LLM training, delivering superior performance compared to Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with \texttt{Stable-SPAM} outperforms the BF16 LLaMA-1B trained with Adam by up to $2$ perplexity. Furthermore, when both models are trained in 4-bit, \texttt{Stable-SPAM} achieves the same loss as Adam while requiring only about half the training steps. Code is available at \url{https://github.com/TianjinYellow/StableSPAM.git}.

% outperforms SPAM and Adam by XXX and XXX in BF16 training, and by XXX and XXX in FP4 training, enabling stable 4-bit training even with a learning rate of XXX.


% Low-bit training suffers from training instability more than FP16. We evaluated various popular optimizers on low-bit training and found most of them do not outperform Adam and suffer from 
% training instability, except for a recent spike-aware SPAM, which can outperform Adam or Adafactor with the optimal learning rate, but still suffers from LR training instability. 

% We delve into the gradient norm of low-bit training and discover that the gradient norm of low-bit tends to be larger, occasionally occurring spikes, than FP16 training, causing loss spikes during training.  To this end, we propose Stable Adam.  



\end{abstract}

\section{Introduction}

\looseness=-1 Recently, several advanced optimizers have been proposed, claiming to either outperform the widely used Adam optimizer or achieve comparable performance at reduced costs in the context of Large Language Models (LLMs). Given the massive size of LLMs, reducing the memory footprint of Adam has become a key objective in this line of research \citep{shazeer2018adafactor,chen2024symbolic,zhang2024adam,zhao2024galore,zhang2024q,ma2024swan}. Another area of focus is addressing the challenges of instability in LLM training. For instance, \citet{huang2025spam} proposed SPAM which incorporates momentum reset and spike-aware gradient clip (SpikeClip) to mitigate the adverse effects of loss spikes. 
\citet{zhao2024deconstructing} studied the stability of various optimizers to hyperparameters with BF16. These optimizers are predominantly evaluated using the standard BF16 precision, which is a practical option for real-world LLM training \citep{touvron2023llama,li2023colossal}. 
With the growing shift toward low-bit precisions such as FP8 and FP4 in LLMs due to their significant cost-saving potential \citep{liu2024deepseek,lee2024fp8,peng2023fp8,xi2023training}, it is crucial to investigate whether their effectiveness persists under lower-bit precisions. For the newly proposed optimizers to be economical, their training with low-bit precisions should be similarly robust to hyperparameter choice as trained using higher precision. 

\begin{figure*}[thb]
    \centering \includegraphics[width=0.93\textwidth]{Figures/Llama_4bitsTraining1.pdf}
    \vspace{-1em}
    \caption{\textbf{Performance of 4-bit LLM training.} Experiments are conducted with LLaMA-130M/350M/1B models on C4 Dataset. \textbf{Adam-BF16} denotes that the model is trained with BF16 by Adam. Perplexity on validation set is reported.}
    \label{fig:4bitsperformence}
\end{figure*}

This paper provides a comprehensive evaluation of the effectiveness and robustness of learning rate choices across various recent optimizers, including Adam \citep{kingma2014adam}, Adafactor \citep{shazeer2018adafactor}, Adam-mini \citep{zhang2024adam}, and SPAM \citep{huang2025spam}, when training with 4-bit weights and activations. Our study reveals several key observations:

\begin{itemize}[leftmargin=*]
    \item[$\star$]  All evaluated optimizers exhibit increased sensitivity to learning rate choices during 4-bit training, often diverging quickly when larger learning rates are used as shown in Figure \ref{fig:inStability}.


    \item[$\star$] SPAM consistently achieves the lowest evaluation loss across various bit levels but requires careful learning rate tuning. Adafactor is surprisingly robust to learning rate choices, even outperforming Adam in this regard.

    \item[$\star$] Our analysis of the training dynamics in Figure 
    \ref{fig:loss} reveals that 4-bit training often exhibits extremely unstable gradient norms, often accompanied by spikes, compared to BF16. This behavior can result in loss spikes and, in some cases, even training divergence with relatively larger learning rates.

    \item[$\star$] While SpikeClip introduced in SPAM mitigates the unstable gradient norms caused by 4-bit training to a certain extent, it falls short of fully preventing training divergence, as shown in Figure \ref{fig:spikeclip}.
    
    % While spike-aware gradient clipping proposed in SPAM relieves the unstable gradient norm caused by 4-bit training, it is not enough to completely prevent training divergence. 
    
    % While spike-aware clipping, as proposed in SPAM, effectively mitigates loss spikes, it falls short in preventing training divergence and fails to address the occurrence of gradient norm spikes.
    
\end{itemize}


\begin{figure*}[thb]
    \centering
 \includegraphics[width=0.95\textwidth]{Figures/inStability.pdf}
    \vspace{-1em}
    \caption{\textbf{Final validation loss when training LLaMA-130M on C4, sweeping across learning rates (LR).} The vertical dotted line indicates that the model cannot be trained further as increasing the learning rate, i.e. Training loss becomes NaN. Red dashed horizontal lines indicate the best performance achieved.}
    \label{fig:inStability}
\end{figure*}
Despite its sensitivity to learning rate selection, SPAM consistently achieves the lowest evaluation loss across various bit levels, making it an ideal foundation for improvement. Building on this, we introduce \texttt{Stable-SPAM} to address the instability challenges associated with low-precision training of LLMs. Stable-SPAM retains the superior performance of SPAM\footnote{Nevertheless, results in Table~\ref{tab:Otheroptimizers} show that our proposed techniques also improve the performance of other optimizers.} while improving stability, offering a significant advancement in low-precision optimization.


Specifically, beyond the original momentum reset operation in SPAM, \texttt{Stable-SPAM} introduces two key techniques: Adaptive Spike-Aware Clipping (\texttt{AdaClip}), which enables adaptive clipping of spiked gradients, followed by Adaptive Gradient Norm (\texttt{AdaGN}), which normalizes the entire gradient matrix based on its historical $l_2$ norm statistics. Our analysis demonstrates that these enhancements effectively stabilize the gradient norm of 4-bit training, achieving better performance than Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with \texttt{Stable-SPAM} outperforms the BF16 LLaMA-1B trained with Adam. Furthermore, when both models are trained in 4-bit, \texttt{Stable-SPAM} achieves the same loss as Adam while requiring only about half the training steps.


% Specifically, apart from the original momentum reset operation of SPAM, Stable-SPAM introduces Adaptive Spike-Aware Clipping (AdaClip) allowing more smoothly clipping of spkied gradients, followed by Adaptive Gradient Norm (AdaGN) to normalize the whole gradient matrix by the gradient norm's historical value. Our analysis demonstrates that both techniques significantly stabilizes the training of SPAM with better performance across various bit levels. Notably, Stable-SPAM outperforms SPAM and Adam by xxx and xxx with FP16 training, XXX and XXX with FP4. 



\begin{figure}[th]
    \centering
 \includegraphics[width=0.5\textwidth]{Figures/Spike.pdf}
    \vspace{-1em}
    \caption{\textbf{Effect of SpikeClip~\cite{huang2025spam} on stabilizing training.} Left: gradient norms before and after performing gradient spike clip. Right: training loss with and without gradient spike clip. Models are trained by Adam optimizer based on LLaMA-130M and C4. }
    \label{fig:spikeclip}
\end{figure}


\section{4-bit Training Stability Investigation}

Recent studies~\cite{zhao2024deconstructing,wortsman2023small,huang2025spam,takase2023spike,wortsman2023small} have investigated stability challenges in large language model (LLM) training, including issues such as learning rate instability, gradient spikes, and loss spikes. In this section, we extend the evaluation by analyzing the stability of various optimization algorithms under a 4-bit LLM training setting. Following the experimental setup outlined in~\cite{wortsman2023small, zhao2024deconstructing}, we evaluate the final performance using a range of learning rates from 1e-4 to 3e-3. This evaluation includes two widely used optimizers, Adam~\cite{kingma2014adam} and Adafactor~\cite{shazeer2018adafactor}, as well as two recently proposed methods, Adam-mini~\cite{zhang2024adam} and SPAM~\cite{huang2025spam}. Additionally, we monitor both the global gradient norm and training loss throughout the 4-bit LLM training process. The global gradient norm is defined as follows:
$\sqrt{\sum_{i=0}^{N} {\|g_i\|_2^{2}}}$
where $N$ is the number of layers in model and $g_i$ denotes the gradient of $i$-th layer. The experiments are conducted on the LLaMA-130M/350M models using the C4 dataset and showed in Figure~\ref{fig:inStability} and Figure~\ref{fig:loss}. We observe: 

\ding{172} \textbf{Lower-bit training exhibits reduced learning rate stability.} As illustrated in Figure~\ref{fig:inStability}, the final evaluation loss for 4-bit training increases significantly with larger learning rates, whereas BF16 training exhibits a more stable performance across different learning rates. This indicates that 4-bit training is more sensitive and less stable in terms of learning rate.

\ding{173}  \textbf{Lower-bit training suffers more loss spikes and gradient norm spikes.} Figure~\ref{fig:loss} illustrates this phenomenon by comparing the training loss and gradient norm curves of LLaMA-130M and LLaMA-350M trained under BF16 and FP4 (E1M2) precision, using various learning rates.  We observe that BF16 training remains stable, but FP4 training exhibits significant loss spikes, which occur on both model sizes. Furthermore, these loss spikes are consistently accompanied by gradient norm explosions.

\ding{174} \textbf{SPAM performs the best in 4-bit training but needs careful learning rate tuning.} As shown in Figure~\ref{fig:inStability}, SPAM achieves the lowest eval loss among various optimizers in INT4 or FP4 with the optimal learning rate. However, its validation loss either diverges to NaN or sharply increases as the learning rate rises. Additionally, we monitored the training loss and gradient norm after applying the spike clipping technique (SpikeClip) proposed in SPAM. SpikeClip detects and mitigates gradient outliers by leveraging the second moment of gradients. Specifically, it follows the expression: \( g_i = \mathrm{sign}(g_i) \cdot \sqrt{\theta V_i} \) under the condition \(\frac{g_i^2}{V_i} >\theta \) where $g_i, V_i,\theta$ are the gradient, second moment and pre-defined threshold ($5000$ used by default in their paper) respectively.  We found that merely SpikeClip can mitigate the loss spike to some extent but can not prevent the training divergence completely. One possible explanation is that SpikeClip operates on an element-wise basis and may use a threshold that is too high. If all gradient components increase simultaneously, SpikeClip may still allow a large overall gradient norm, as it focuses solely on clipping individual outliers and does not effectively handle uniformly large gradients.  This is supported by the observation in Figure~\ref{fig:spikeclip} that the gradient norm remains high even after SpikeClip is applied.




\begin{figure*}[th]
    \centering
 \includegraphics[width=1.0\textwidth]{Figures/gradient_loss.pdf}
    \vspace{-1em}
    \caption{\textbf{Training loss and gradient norm of Adam using various learning rates with BF16 and FP4 precision.} Experiments are conducted under the same training configuration with LLaMA-130M/350M. }
    \label{fig:loss}
\end{figure*}

\section{\texttt{Stable-SPAM}}
To address the training instability in 4-bit LLM training, we propose \texttt{Stable-SPAM}, a stabilized spike-aware Adam optimizer. Apart from the momentum reset inherited from the original SPAM, \texttt{Stable-SPAM} introduces two techniques: Adaptive Gradient Norm (\texttt{AdaGN}) and Adaptive Spike-Aware Clipping (\texttt{AdaClip}), which we will explain in detail. The pseudocode is provided in Appendix~\ref{Pseudocode}.

% Additionally, similar to SPAM~\cite{huang2025spam}, \texttt{Stable-SPAM} retains the momentum reset operation to mitigate the residual effects of anomalously large gradients on the optimizer's state.

\textbf{Adaptive Gradient Norm} (\texttt{AdaGN}). As we can observe in Figures~\ref{fig:loss} and~\ref{fig:spikeclip}, spikes in training loss and instances of training divergence usually align with abrupt surges in the gradient norm, consistent with findings in~\cite{takase2023spike,huang2025spam}.
To address these training instabilities, we propose \texttt{AdaGN}, a method that stabilizes gradients by adaptively scaling them based on their historical $l_2$ norm statistics. To better track the dynamics of the gradient norm during training, we leverage the idea of Adam by maintaining moving averages of both the first and second moments of the gradient norm. 
% \texttt{AdaGN} can smooth out transient spikes in a manner analogous to how Adam adjusts learning rates. 
Concretely, we compute and update the moving averages of the gradient norm ($m_{norm},v_{norm}$), then use them to derive a normalized gradient:
\begin{align}
    g_{\mathrm{norm}} &= \|g_t\|_2, \\
    m_{\mathrm{norm}} &= \gamma_1 \cdot m_{\mathrm{norm}} 
    + \bigl(1 - \gamma_1\bigr) \cdot g_{\mathrm{norm}}, \\
    v_{\mathrm{norm}} &= \gamma_2 \cdot v_{\mathrm{norm}} 
    + \bigl(1 - \gamma_2\bigr) \cdot g_{\mathrm{norm}}^2, \\
    \hat{g}_t &= \frac{g_t}{g_{\mathrm{norm}}} \cdot 
    \frac{m_{\mathrm{norm}}}{\sqrt{v_{\mathrm{norm}}} + \epsilon}.
\end{align}
where $\hat{g_t}$ is the normalized gradient, $\gamma_1$ and $\gamma_2$ are momentum coefficients and $\epsilon$ is small constant for numerical stability. By rescaling $g_t$ with a ratio of its historical mean norm $m_{norm}$ to the square root of its historical second moment $\sqrt{v_{norm}}$, \texttt{AdaGN} mitigates abrupt gradient norm spikes. Note that as the gradient norm $g_{norm}$
is essentially a scalar for an entire layer, the additional parameter overhead introduced by \texttt{AdaGN} is negligible, i.e., two extra parameters per layer. 

% \textbf{Adaptive Spike Gradient Scaling (ASGS).} 
\textbf{Adaptive Spike-Aware Clipping (\texttt{AdaClip}).} 
Different from the spike gradient clipping technique in~\cite{huang2025spam}, which sets a fixed clipping threshold, we propose an adaptive clipping approach, i.e., \texttt{AdaClip}. The core idea is to dynamically adjust the clipping threshold by tracking the maximum gradient magnitude observed over time, rather than relying on a pre-defined fixed value.  Concretely, let $g_t$ be the gradient at time step $t$. We first compute $g_{\max}$, the maximum absolute gradient value across all parameters. %\GJ{I guess $g_t$ is a vector or vectorization of a matrix. Maybe using $g_{max}=\max_i(|g_t[i]|)$ is better. In addition, $g_t$ is a vector, $g_{max}$ is a scalar, may confuse readers.} 
Then, we update the threshold $T_{\text{threshold}}$ with an exponential moving average that incorporates $g_{\max}$. Finally, any entries of $g_t$ that exceed $T_{\text{threshold}}$ are rescaled to maintain stability. The procedure is formally expressed as follows:
\begin{align}
    g_{\max} &= \max_i(|g_t[i]|), \\
    T_{\text{threshold}} &= \gamma_3 \cdot T_{\text{threshold}} + (1 - \gamma_3) \cdot g_{\max}, \\
    \mathrm{Mask}_{\text{spikes}} &= (g_t > T_{\text{threshold}}), \\
    g_t[\mathrm{Mask}_{\text{spikes}}] &= \frac{g_t[\mathrm{Mask}_{\text{spikes}}]}{g_{\max}} \times T_{\text{threshold}},
\end{align}


where $\gamma_3 \in [0,1]$ controls the weight of the moving average. When $\gamma_3$ is large, $T_{\text{threshold}}$ responds more slowly to new gradient maxima, leading to more stable updates. When $\gamma_3$ is small, it adapts more quickly to sharp changes in gradient magnitude.


\begin{figure*}[ht]
    \centering
    \begin{minipage}[t]{0.47\textwidth}
        \centering
        % Table
        \vspace{-34mm}
        \captionof{table}{\textbf{Comparison of various optimizers of INT4 and FP4 training of LLaMA models on C$4$.} Perplexity is reported.}
        \vspace{1mm}
        \label{tab:4bits}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{lcccccc}
            \toprule
             \multirow{2}{*}{}& \multicolumn{3}{c}{\textbf{INT4 Training}} & \multicolumn{3}{c}{\textbf{FP4 Training}} \\
            \cmidrule(lr){2-4} \cmidrule(lr){5-7}
             & \textbf{130M} & \textbf{350M} & \textbf{1B} & \textbf{130M} & \textbf{350M} & \textbf{1B} \\
            \midrule
            Adam & 26.4  & 24.14  & 21.59  & 28.9  & 24.59  &22.01 \\
            Adam+GradClip & 26.30 & 21.64& 19.74& 28.27 &20.84 & 20.25 \\
            Adafactor & 25.11  & 20.45 & 20.65 & 26.89  & 20.53  & 20.03 \\
            SPAM & 25.03  &20.19  & 19.98 & 26.78 & 20.35  & 19.74 \\
            \rowcolor{gray!20}
            \texttt{Stable-SPAM} & \textbf{24.33} & \textbf{17.76} & \textbf{17.42} & \textbf{26.31} & \textbf{19.49} & \textbf{18.48} \\
            \midrule
            Adam (BF16) & 24.53 & 21.38 & 19.73 & 24.53 & 21.38 & 19.73 \\
            \midrule
            Training Tokens & \multicolumn{6}{c}{2.2B } \\
            \bottomrule
        \end{tabular}}
    \end{minipage}%
    \hspace{1.8em} % Space between the table and figure
    \begin{minipage}[t]{0.47\textwidth}
        \centering
        % Figure
        \includegraphics[width=0.8\linewidth]{Figures/exetremely.pdf}
        \vspace{-1em}
        \caption{\textbf{StableSPAM under Extremely Low-Precision Training.} Experiments are conducted with 350M models on C4 Dataset. \textbf{BF16-Adam} denotes that the model is trained with BF16 by Adam. The final loss on validation set is reported.}
        \label{fig:2bitsperformence}
    \end{minipage}
\end{figure*}


\textbf{Momentum Reset (\texttt{MoRet}).} Following \citet{huang2025spam}, we adopt momentum reset (\texttt{MoRet}) to periodically reset the accumulated first and second moments in Adam. The effectiveness of \texttt{MoRet} lies in addressing the negative effects of gradient spikes, which can inflate the first and second moments of Adam. Since Adam uses exponential moving averages to track their historical information, these inflated values caused by spiked gradients can have prolonged detrimental effects \citep{huang2025spam} on moments. By resetting the momentum terms at fixed intervals ($\Delta T$), \texttt{MoRet} mitigates the lasting influence of unusually large gradients, enabling more stable and consistent optimization.




% of past gradients to update parameters effectively. However, when a gradient spike occurs, it can disproportionately inflate these moments, causing the spike’s effects to persist over multiple iterations.


\section{Experiments}


\label{submission}
To demonstrate the efficacy of the proposed \texttt{Stable-SPAM}, we conduct extensive experiments with various sizes of the LLaMA model on the C4 dataset. 

\textbf{Baselines.} We adopt five popular optimizers as our baselines including Adam~\cite{kingma2014adam}, Adafactor~\cite{shazeer2018adafactor}, Lion~\cite{chen2024symbolic}, Adam-mini~\cite{zhang2024adam} and SPAM~\cite{huang2025spam}.  Among these, Adam and Adafactor are well-established and widely used, while Adam-mini and SPAM have been introduced more recently. Besides, we also include gradient clipping~\cite{goodfellow2016deep} (GradClip) in conjunction with Adam as an additional baseline.

\textbf{Experimental Setup.}   Following \cite{lialin2023relora, zhao2024galore}, we train LLaMA-based architectures ranging from 60M to 1B parameters. Each architecture is configured with RMSNorm \cite{shazeer2020glu} and SwiGLU activations \cite{zhang2019root}. For every model size, we keep the same set of hyperparameters across methods and vary only the learning rate. Specifically, we sweep over learning rates from $1\times 10^{-4}$ to $1\times 10^{-3}$ , incrementing by $2\times 10^{-4}$  for each optimizer. Following the settings in~\cite{takase2023spike,huang2025spam}, we set the threshold to 1 for the GradClip baseline. For Adafactor, we adopt the hyperparameters from the original paper~\cite{shazeer2018adafactor}, where $\epsilon_1=10^{-30}$, $\epsilon_2=10^{-3}$, and $d=1.0$. The hyperparameters for SPAM are configured based on the settings in~\cite{huang2025spam}, with reset intervals set to 500, learning rate warmup steps to 150, and the GSS threshold to 5000. For \texttt{Stable-SPAM}, we set $\gamma_1=0.7$, $\gamma_2=0.9$ and $\theta=0.999$ for 4-bit LLM training and $\gamma_1=0.85$, $\gamma_2=0.9999$ and $\gamma_3=0.999$ for BF16 training. Detailed descriptions of our task setups and hyperparameters are provided in the Appendix~\ref{hyperparameterdetials}.

\subsection{Performence of 4-bit LLM Training}
To evaluate the performance of \ours in 4-bit LLM training, we conduct experiments using both FP4 ( E1M2: 1-bit exponent, 2-bit mantissa) and INT4 (4-bit integer) quantization-aware training strategies. The training curves of various LLaMA models on the C4 dataset are presented in Figure~\ref{fig:4bitsperformence}, and the final perplexity results are summarized in Table~\ref{tab:4bits}.

We observe that \ding{182} 4-bit training leads to a significant performance drop compared to BF16 training. As shown in Table \ref{tab:4bits}, the perplexity gap between BF16 (Adam) and INT4/FP4 (Adam) exceeds 1.5 across all model sizes, highlighting the challenges of reduced precision. \ding{183} Figure \ref{fig:4bitsperformence} shows that \ours consistently outperforms Adam by a significant margin in 4-bit scenarios, even surpassing the performance of 16-bit Adam. Table \ref{tab:4bits} further demonstrates that \ours outperforms other advanced optimizers, such as Adafactor and SPAM. Among the baselines, incorporating GradClip reduces perplexity, while Adafactor and SPAM both outperform the simple application of GradClip.
\ding{184} \ours is able to match Adam's performance with half the tokens in 4-bit training. As illustrated in Figure \ref{fig:4bitsperformence}, \ours achieves the same perplexity as Adam in approximately half the training steps.
\ding{185} Notably, \ours performs particularly well with larger models, such as LLaMA-350M and LLaMA-1B, showcasing its strong potential for large-scale training. This is likely because large-scale, low-precision training is more susceptible to instability issues \cite{fishman2024scaling}, making stabilized training approaches like \ours especially beneficial.

\begin{figure*}[th]
    \centering
 \includegraphics[width=0.98\textwidth]{Figures/Llama_standardTraining1.pdf}
    \vspace{-1em}
    \caption{\textbf{Performance of BF16 training with various model sizes.} Experiments are based on LLaMA models trained on C4 Dataset. }
    \label{fig:BF16performence}
        % \vspace{-1 em}
\end{figure*}

\subsection{Performence of Extremely Low-Precision Training}
To evaluate the performance of \ours under extremely low-precision training, we conducted experiments on LLaMA-350M using A2W2 (INT2), A3W3 (INT3), and A4W4 (INT4) configurations. The final validation loss is presented in Figure~\ref{fig:2bitsperformence}. The results indicate that \ours consistently outperforms Adam across all low-precision settings and even matches the performance of BF16-Adam under INT3 training.

% \vspace{-0.7em}
\subsection{Performence of BF16 LLM Training}
To further evaluate the efficacy of \ours, we conducted experiments on various LLaMA model sizes using standard BF16 training. The experiments are based on C4 dataset. The training curves and final perplexity values are presented in Figure~\ref{fig:BF16performence} and Table~\ref{tab:BF16Performence}, respectively. Table~\ref{tab:BF16Performence} highlights that \ours consistently delivers superior performance across different model sizes, surpassing the second-best optimizer with significant improvements. Furthermore, Figure~\ref{fig:BF16performence} illustrates that \ours achieves the same performance as Adam in only half the training steps or even fewer for LLaMA-350M and LLaMA-1B, validating its ability to match Adam's performance while requiring significantly fewer tokens under BF16 LLM training. The above results demonstrate that the promise of \ours not only holds for low-precision LLM training but also holds for the standard BF16 training. 


\begin{table}[htb]
    \centering
    % \vspace{-0.2em}
    \caption{\textbf{Comparison among various optimizers on BF16 training.} Perplexity is reported.}
    \vspace{1mm}
    \label{tab:BF16Performence}
        \resizebox{1.0\linewidth}{!}{%
    \begin{tabular}{lcccc}
        \toprule
        \bf Optimizer & \textbf{60M} & \textbf{130M} & \textbf{350M} & \textbf{1B} \\
        \midrule
        Adam-mini & 34.10  & 24.85  & 19.05  & 16.07  \\
        Adam & 34.09  & 24.91  & 18.77  & 16.13  \\
        Adam + GradClip & 33.33  & 24.88  & 18.51  & 15.22  \\
        Adafactor & 32.57  & 23.98  & 17.74  & 15.19  \\
        SPAM & 30.46 & 23.36  & 17.42  & 14.66  \\
        \gr
        \texttt{Stable-SPAM} & \textbf{28.84} & \textbf{22.21}  & \textbf{16.85}  & \textbf{13.90}  \\
        % Stable SPAM No Step Reset for $m/v\_norm\_t$ & 29.65 & 22.82  & 16.92  & 13.90  \\
        \midrule
        Training Tokens & 1.1B & 2.2B & 6.4B & 11.6B \\ 
        \bottomrule
    \end{tabular}}
   \vspace{-0.5em}
\end{table}


\begin{figure*}[thb]
    \centering
 \includegraphics[width=1.0\textwidth]{Figures/stableffect1.pdf}
    \vspace{-1em}
    \caption{\textbf{Effect of \texttt{AdaGN} and \texttt{AdaClip} on stabilizing FP4 LLM training.} The left two figures use LLaMA-130M (LR = 3e-3), and the right two figures use LLaMA-60M.}
    \label{fig:stableeffect}
     % \vspace{-0.3em}
\end{figure*}


\subsection{Integration with Other Optimizers}
Although \texttt{AdaGN} and \texttt{AdaClip} are proposed specifically for \ours, one may wonder, ``Can \texttt{AdaGN} and \texttt{AdaClip} also be compatible with other optimizers?" To answer this question, we applied \texttt{AdaGN} and \texttt{AdaClip} to two recently published optimizers: Lion~\cite{chen2024symbolic} and Adam-mini~\cite{zhang2024adam}. We conducted comparative experiments using Lion and Adam-mini alone, as well as in combination with \texttt{AdaGN} and \texttt{AdaClip}, under a 4-bit training setting. These experiments were performed on LLaMA-60M/130M models with the C4 dataset.

The results in Table~\ref{tab:Otheroptimizers} show that \texttt{AdaGN} and \texttt{AdaClip} consistently enhance the performance of both Lion and Adam-mini under FP4 and INT4 training settings, across LLaMA-60M and LLaMA-130M model sizes. Notably, on LLaMA-130M with INT4 training, Lion achieves a perplexity improvement of up to 5.88, and Adam-mini on LLaMA-60M under FP4 training sees an improvement of 1.72. These improvements underscore the broad applicability and effectiveness of the proposed \texttt{AdaGN} and \texttt{AdaClip} methods.  

\begin{table}[htb]
    \centering
    % \vspace{-0.2em}
    \caption{\textbf{Performence of \texttt{AdaGN} and \texttt{AdaClip} on Lion and Adam-mini optimizers.} Experiments are based on LLaMA-60M/130M with 4-Bit training.}
    \vspace{1mm}
    \label{tab:Otheroptimizers}
        \resizebox{1.0\linewidth}{!}{%
    \begin{tabular}{lcccc}
        \toprule
         \multirow{2}{*}{Optimizers}& \multicolumn{2}{c}{\textbf{INT4 Training}} & \multicolumn{2}{c}{\textbf{FP4 Training}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
         & \textbf{60M} & \textbf{130M} & \textbf{60M} & \textbf{130M}  \\
        \midrule
        Lion& 39.36 & 35.28 & 39.89 & 34.20 \\
       \gr  Lion+\texttt{AdaGN}+\texttt{AdaClip} & \textbf{38.49} &\textbf{ 29.40} &\textbf{ 36.75} &\textbf{ 31.63} \\
        Adam-mini & 34.84 & 29.79 & 36.37 & 32.95 \\ 
        \gr Adam-mini+\texttt{AdaGN}+\texttt{AdaClip}  & \textbf{34.61} & \textbf{29.65} &\textbf{ 34.65} & \textbf{32.39} \\        
        \midrule
        Training Tokens & \multicolumn{4}{c}{1.1B } \\ 
        \bottomrule
    \end{tabular}}
\end{table}

% \subsection{Additional Analysis}

\subsection{Effect on Stabilizing Training} 
To validate the effectiveness of our proposed \texttt{AdaGN} and \texttt{AdaClip} techniques in stabilizing the LLM training process, \underline{Firstly}, we compared the training loss and gradient norm curves across three settings: using Adam alone, using Adam with \texttt{AdaGN}, and using Adam with both \texttt{AdaGN} and \texttt{AdaClip}. Our experiments employed LLaMA-130M with a learning rate of 3e-3 under an FP4 training setting. As shown in Figure~\ref{fig:stableeffect}, training solely with Adam leads to divergence in the training loss and frequent spikes in the gradient norm. However, once \texttt{AdaGN} is introduced, the training loss converges, and the gradient norm is noticeably reduced. Adding \texttt{AdaClip} on top of \texttt{AdaGN} further decreases the gradient norm and yields a smoother training loss curve. \underline{Secondly}, we present the final performance across a range of learning rates, from $5 \times 10^{-4}$ to $5 \times 10^{-3}$, evaluated on LLaMA-60M under both FP4 and INT4 training settings. The results in Figure~\ref{fig:stableeffect} show that \texttt{Stable-SPAM} produces a significantly flatter curve, highlighting its stability across varying learning rates.  These results demonstrate the effectiveness of the proposed \texttt{AdaGN} and \texttt{AdaClip} techniques in achieving a more stable and consistent training process.





% \textbf{Ablation study.} We conduct ablations for validating the effectiveness of the three component \texttt{MoRet}, \texttt{AdaGN} and \texttt{AdaClip} in \ours. Specifically, on the one way, we iteratively apply $\texttt{MoRet}$, \texttt{AdaGN} and \texttt{AdaClip} upon Adam to monitor their improvements on both FP4 and BF16 training setting. On the other way, we replace \texttt{AdaClip} with SpikeClip~\cite{huang2025spam} and \texttt{GradGN} with GradClip~\cite{goodfellow2016deep} to further validate their effectiveness. The results are shown in Table~\ref{tab:Ablations}. We observe that \ding{182} \texttt{MoRet} improves performance consistently on both FP4 and BF16 setting. \ding{183} Purely add \texttt{AdaGN} does not yield benefits under FP4 training setting but the combination of \texttt{AdaGN} and \texttt{AdaClip} reduces the final perplexity a lot. This behavior differs in BF16 training setting where purely apply \texttt{AdaGN} improves the performance a lot but add \texttt{AdaClip} on top of that do not yield further improvements. One plausible explanation for this different behavior could be that there is more extremely large element-wise gradient spikes occurs in FP4 training setting where \texttt{AdaClip} are needed to correct this biased updating directions.  Additionally, we find that replace \texttt{AdaClip} with SpikeClip~\cite{huang2025spam} and \texttt{GradGN} with GradClip~\cite{goodfellow2016deep} increase the final perplexity, further validating the effectiveness of the proposed \texttt{AdaGN} and \texttt{AdaClip}.
\subsection{Ablation Study}
To validate the effectiveness of the three components, \texttt{MoRet}, \texttt{AdaGN}, and \texttt{AdaClip}, in \ours, we conduct a comprehensive ablation study. Specifically, we take two approaches: $\textbf{(1)}$ We iteratively incorporate \texttt{MoRet}, \texttt{AdaGN}, and \texttt{AdaClip} into the Adam optimizer to measure their individual and combined improvements under both FP4 and BF16 training settings. $\textbf{(2)}$ We replace \texttt{AdaClip} with SpikeClip~\cite{huang2025spam} and \texttt{AdaGN} with GradClip~\cite{goodfellow2016deep} to further assess the unique contributions of our proposed components.  The results, summarized in Table~\ref{tab:Ablations}, reveal the following observations: \ding{182} \texttt{MoRet} consistently improves performance across both FP4 and BF16 settings. \ding{183} Under both FP4 training, \texttt{AdaGN} alone shows limited improvement. However, when combined with \texttt{AdaClip}, it substantially reduces final perplexity. \ding{184} Conversely, in the BF16 setting, \texttt{AdaGN} alone yields considerable performance gains, but adding \texttt{AdaClip} offers limited improvement.  This discrepancy may stem from the higher frequency of extreme element-wise gradient spikes in this FP4 training experiments, which necessitates \texttt{AdaClip} to correct biased update directions effectively. Finally, replacing \texttt{AdaClip} with SpikeClip~\cite{huang2025spam} and \texttt{AdaGN} with GradClip~\cite{goodfellow2016deep} results in increased perplexity, further validating the efficacy of our proposed \texttt{AdaGN} and \texttt{AdaClip}.

%However, when \texttt{AdaGN} is combined with \texttt{AdaClip}, the final perplexity is substantially reduced. A plausible explanation is that \texttt{AdaGN} alone cannot correct update directions biased by extreme element-wise gradient spikes, whereas integrating it with \texttt{AdaClip} more effectively mitigates these spikes and leads to better updates. 


%\ding{184} Conversely, in the BF16 setting, \texttt{AdaGN} alone yields considerable performance gains, but adding \texttt{AdaClip} offers no further improvement. 
%This discrepancy may stem from the higher frequency of extreme element-wise gradient spikes in this INT4 training experiments, which necessitates \texttt{AdaClip} to correct biased update directions effectively. 

% Finally, replacing \texttt{AdaClip} with SpikeClip~\cite{huang2025spam} and \texttt{AdaGN} with GradClip~\cite{goodfellow2016deep} results in increased perplexity, further validating the efficacy of our proposed \texttt{AdaGN} and \texttt{AdaClip}.


\begin{table}[htb]
    \centering
    \vspace{-1em}
    \caption{\textbf{Ablations on \texttt{Stable-SPAM}.} Experiments are based on LLaMA-60M and C4.}
\vspace{1mm}
    \label{tab:Ablations}
        \resizebox{1.0\linewidth}{!}{%
    \begin{tabular}{lcccc}
        \toprule
        \bf Optimizer & FP4 &BF16 \\
     
        \midrule
      Adam &35.47  &34.09 \\
        Adam + \texttt{MoRet} &  32.4 &31.47\\
                Adam + \texttt{MoRet} + \texttt{AdaClip} &31.97 & 30.29\\
        Adam + \texttt{MoRet} + \texttt{AdaGN} & 32.26 &28.96 \\
      \gr  Adam + \texttt{MoRet} + \texttt{AdaGN} + \texttt{AdaClip} (\textbf{\ours}) & \textbf{31.40} & \textbf{28.84} \\
      \midrule
    \midrule
        Adam + \texttt{MoRet}+\texttt{AdaGN}+\textcolor{blue}{SpikeClip}~\cite{huang2025spam} &32.01& 28.90 \\
        Adam + \texttt{MoRet}+ \textcolor{blue}{GradClip}~\cite{goodfellow2016deep}+\texttt{AdaClip} &31.95  & 29.87\\
             \gr  Adam + \texttt{MoRet}+\texttt{AdaGN}+\texttt{AdaClip} (\textbf{\ours}) & \textbf{31.40} & \textbf{28.84} \\
      \midrule
        Training Tokens &\multicolumn{2}{c}{ 1.1B} \\ 
        \bottomrule
    \end{tabular}}
\end{table}
\begin{figure*}[th]
    \centering
 \includegraphics[width=0.94\textwidth]{Figures/Hypeparameters.pdf}
    \vspace{-1em}
    \caption{ \textbf{Hyper-parameter Analysis.} Experiments are conducted with FP4 training on LLaMA-60M and C4 with 1.1B tokens. }
    \label{fig:hyperparameters}
        \vspace{-0.5em}
\end{figure*}


\subsection{Hyper-Parameter Analysis}

\ours introduces four hyperparameters: $\gamma_1$, $\gamma_2$, $\gamma_3$, and $\Delta T$, which extend the functionality of Adam. Among these, $\gamma_1$ and $\gamma_2$ serve a similar purpose to $\beta_1$ and $\beta_2$ in Adam, controlling the smoothness of updates to the first moment $m_{norm}$ and the second moment $v_{norm}$. Larger values of $\gamma_1$ and $\gamma_2$ result in smoother updates, placing greater emphasis on historical gradient norm statistics when adapting the current gradient norm. Similarly, $\gamma_3$ plays a role in determining the threshold for identifying gradient spikes. A larger $\gamma_3$ leads to a smoother and more conservative threshold, resulting in a higher proportion of gradients being classified as spike gradients. To investigate the impact of these hyperparameters, we plot the final perplexity curve while varying $\gamma_1$ from 0.5 to 0.9, $\gamma_2$ from 0.8 to 0.999, $\gamma_3$ from 0.9 to 0.999, and $\Delta T$ from 250 to 5000. The experiments are conducted using LLaMA-60M, trained on 1.1B C4 tokens under the FP4 training setting. The results in Figure~\ref{fig:hyperparameters} demonstrate that overly small or excessively large values of these hyperparameters can degrade performance. However, the intuitive interpretations of these hyperparameters make them straightforward to tune, and they typically require minimal adjustments. In this paper, we adopt the optimal values $\gamma_1=0.7$, $\gamma_2=0.9$, $\gamma_3=0.999$, and $\Delta T=1000$, which work effectively for all 4-bit training scenarios.


\section{Related Work}

\textbf{Instability of Training Large Language Models.} 
The instability of large language model (LLM) training, which are marked by loss spikes and catastrophic divergence\citep{chowdhery2023palm,molybog2023theory}, has driven extensive research into stabilization techniques. These methods generally fall into three main categories: (1) gradient preprocessing, (2) architectural modifications, and (3) initialization strategies. Gradient preprocessing typically involves scaling and clipping gradients at the start of the optimization process to stabilize the training. A well-known example is gradient clipping \citep{goodfellow2016deep}, which globally rescales  the gradient norm to a fixed value. Later, Adafactor \citep{shazeer2018adafactor} introduced capping the norm of the parameter updates instead of the raw gradients. More recently, SPAM \citep{huang2025spam} proposed detecting and clipping anomalous gradients based on historical gradient statistics. However, a common drawback of these methods is that they require manually setting a predefined threshold. Architecturally, \citet{xiong2020layer} showed that Post-LayerNorm (Post-LN) amplifies gradients, causing instability with large learning rates, while Pre-LayerNorm (Pre-LN) preserves gradient norms for stable training. Embed LayerNorm (Embed LN) normalizes embeddings\citep{dettmers20218}, though it may impact performance\citep{scao2022language}, while Embed Detach\citep{ding2021cogview,zeng2022glm} reduces loss spikes by truncating gradients. DeepNorm\citep{wang2024deepnet} scales residual connections to stabilize ultra-deep models, and $\alpha$Reparam\citep{zhai2023stabilizing} prevents attention entropy collapse via spectral-normalized parameterization. Initialization strategies offer complementary stability benefits. Scaled Embed\citep{takase2023spike} stabilizes LayerNorm gradients, while Scaled Initialization\citep{nguyen2019transformers} reduces variance using $\mathcal{N}(0, \sqrt{2/(5d)}/\sqrt{2N})$. Fixup\citep{zhang2019fixup,huang2020improving} eliminates LayerNorm entirely, inspiring norm-free architectures. Though ongoing advancements refine these approaches, training stability remains a key challenge in LLM development.



\textbf{Low-precision LLM Training.} Low-precision training~\cite{wang2018training,lin2022device,xi2024coat,xi2024jetfire,wortsman2023stable} has emerged as a promising approach to improve both computational and memory efficiency during training. Among these methods, FP16~\cite{micikevicius2017mixed} and BF16~\cite{kalamkar2019study} are the most widely adopted precision formats. To push the efficiency further, 8-bit training has garnered increasing attention. For instance, LM-FP8~\cite{peng2023fp8} enables training with FP8 precision While \cite{fishman2024scaling} demonstrates that as training scales up (larger than 250B tokens), the issue of activation outliers becomes more pronounced, posing challenges to the representation range of low-bit data formats. To address this challenge, \cite{fishman2024scaling} proposes a smoothing strategy, while \cite{ashkboos2025halo} leverages Hadamard transformations to mitigate the impact of activation outliers. Furthermore, the choice of data format significantly influences training performance. The INT8 format is the most widely supported low-precision format, whereas FP8, available in NVIDIA’s Hopper GPU architecture, provides specialized support. Additionally, the MX format~\cite{rouhani2023microscaling} demonstrates superior representational capability, though it is rarely supported by current hardware. In this work, we investigate the training instability associated with low-precision training and propose enhancements through the design of optimizers. Our approach is compatible with existing techniques, providing a complementary solution to improve the stability of low-precision training.

\section{Conclusion}

This paper presents a comprehensive study on the training instability challenges of 4-bit quantization in large language models. We find that while low-precision training significantly reduces memory and computational costs, it also amplifies the sensitivity to learning rates, and increases the likelihood of gradient and loss spikes. To address these issues, we propose \texttt{Stable-SPAM}, an optimizer that combines three key techniques: \texttt{AdaClip}, \texttt{AdaGN}, and \texttt{MoRet}. Empirical results on LLaMA models of various sizes demonstrate that \texttt{Stable-SPAM} not only stabilizes 4-bit training but also achieves better performance compared to existing optimizers, sometimes even surpassing BF16 performance. We additionally show that these stabilization strategies are broadly applicable, benefiting other optimizers like Lion and Adam-mini.
%
\section*{Acknowledgments}
This work used the Dutch national e-infrastructure with the support of the
SURF Cooperative using the funding of the projects EINF-12538 and EINF-10925.
\section*{Impact Statement}
This paper advances the field of large language model (LLM) training by proposing a stable optimizer that enables more stable and efficient optimization at low-precision (4-bit) arithmetic. By reducing computational and memory overhead, our approach has the potential to lower energy consumption and lessen the environmental footprint of training large-scale models. There are many
potential societal consequences of our work, none of which
we feel must be specifically highlighted here.
%At the same time, making LLMs more accessible could amplify risks common to these technologies, including the spread of misinformation or biases if such models are deployed without proper safeguards. Consequently, while this work primarily aims to improve algorithmic efficiency and stability in LLM training, it underscores the importance of responsible deployment and continued scrutiny of ethical considerations in training LLMs.
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Architecture and Hyperparameters} \label{hyperparameterdetials}

We introduce details of the LLaMA architecture and hyperparameters used for 4-bit and BF16 pre-training, following \citet{lialin2023relora,zhao2024galore}. 
Table~\ref{tab:llama_hyperparameters} shows the most hyperparameters of LLaMA models across model sizes. 
We use a max sequence length of $256$ for all models, with a batch size of $512$, with a batch size of $131$K tokens.
For all experiments, we adopt learning rate warmup of $2000$ training steps, and use cosine annealing for the learning rate schedule, decaying to $10$\% of the initial learning rate.


\begin{table*}[htb]
    \small
    \caption{Configurations of LLaMA models used in this paper.}
    \label{tab:llama_hyperparameters}
    \vskip 0.15in
    \begin{center}
    \begin{tabular}{cccccccc}
    \toprule
    Params & Hidden & Intermediate & Heads & Layers  \\
    \midrule
    $60$M & $512$ & $1376$ & $8$ & $8$   &  \\
    $130$M & $768$ & $2048$ & $12$ & $12$    \\
    $350$M & $1024$ & $2736$ & $16$ & $24$   \\
    $1 \mathrm{~B}$ & $2048$ & $5461$ & $24$ & $32$  \\
    \bottomrule
    \end{tabular}
    \end{center}
    \vskip -0.1in
\end{table*}

For all methods across each model size (from $60$M to $1$B), we tune the learning rates from $1e-4$ to $1e-3$ with an increasing step of $2 \times 10^{-4}$ for pre-training tasks, and the best learning rate is selected based on the validation perplexity. The detailed hyperparameter of \ours on 4-bit training and BF16 training are reported in Table~\ref{tab:hyperparameters_setup} and Table~\ref{tab:hyperparameters_setup1}.

% For all models, xxx use the same hyperparameters, including the learning rate of $0.01$, scale factor $\alpha$ of $0.25$, and the subspace change frequency of $T$ of $200$. We note that since $\alpha$ can be viewed as a fractional learning rate, most of the modules (e.g., multi-head attention and feed-forward layers) in LLaMA models have the actual learning rate of $0.0025$.
% This is, still, a relatively large stable learning rate compared to the full-rank baseline, which usually uses a learning rate $\leq 0.001$ to avoid spikes in the training loss.

\begin{table*}[htb]
\small
    \caption{Hyperparameters of \ours for 4-bit pre-training experiments in this paper.}
    \label{tab:hyperparameters_setup}
    \vskip 0.15in
    \begin{center}
    \begin{tabular}{cccc}
    \toprule
    Hyper-Parameters & LLaMA-$130$M & LLaMA-$350$M & LLaMA-$1$B  \\
    \midrule
    LR & $1e-3$   & $4e-4$ & $2e-4$  \\
    $\Delta T$& $1000$ & $1000$ & $1000$ \\
    $\gamma_1$& $0.7$ & $0.7$ & $0.7$   \\
    $\gamma_2$ & $0.9$ & $0.9$ & $0.9$ \\
    $\gamma_3$ & $0.999$ & $0.999$ & $0.999$ \\
    \bottomrule
    \end{tabular}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[htb]
\small
    \caption{Hyperparameters of \ours for BF6 pre-training experiments in this paper.}
    \label{tab:hyperparameters_setup1}
    \vskip 0.15in
    \begin{center}
    \begin{tabular}{cccccccc}
    \toprule
    Hyper-Parameters & LLaMA-$60$M & LLaMA-$130$M & LLaMA-$350$M & LLaMA-$1$B  \\
    \midrule
    &\multicolumn{4}{c}{Standard Pretraining} \\
    \midrule
    LR & $1e-3$ &$8e-4$  & $4e-4$ & $2e-4$  \\
    $\Delta T$& $1000$ & $1000$ & $1000$ & $1000$   \\
    $\gamma_1$& $0.85$ & $0.85$ & $0.85$ & $0.85$   \\
    $\gamma_2$ & $0.99999$ & $0.99999$ & $0.99999$ & $0.99999$ \\
    $\gamma_3$ & $0.999$ & $0.999$ & $0.999$ & $0.999$ \\
    \bottomrule
    \end{tabular}
    \end{center}
    \vskip -0.1in
\end{table*}


\section{Time Series Forescasting Task}
We conducted additional experiments on time-series prediction tasks. In these experiments, we intentionally introduced anomalous data with a probability A=10\%  to simulate gradient anomalies.  Experiments are conducted with 10 repeated runs on Weather time series data\footnote{\url{https://www.bgc-jena.mpg.de/wetter/}} using PatchTST~\citep{nie2022time} model. The results are presented in Figure~\ref{fig:Forestcasting}.


The findings demonstrate that as the severity ($S$) of anomalous data increases, \texttt{Stable-SPAM}'s performance advantage over Adam becomes more pronounced. Besides, \texttt{Stable-SPAM} consistently surpasses SPAM across all settings. These results further highlight the  effectiveness of the proposed \ours. 

\begin{figure*}[thb]
    \centering
 \includegraphics[width=0.95\textwidth]{Figures/Test_loss_PatchTST.pdf}
    \vspace{-1em}
    \caption{\textbf{Test Loss during Training Process on Weather Time-series Data.} Anomalous data is generated by adding Gaussian noise to 10\% of randomly selected input values. Specifically, the anomalies data are conducted with $X=X+\texttt{Gaussin}(0,\texttt{Severity}*\texttt{Max}(X))$ where $X$ is the inputs and $S$ is the severity.}
    \label{fig:Forestcasting}
\end{figure*}

\section{Pseudocode} \label{Pseudocode}
The pseudocode is presented in Alogrithm~\ref{algo:stablespam}.

% \begin{algorithm*}[h]
% \SetKwInput{KwInput}{Input}                % Set the Input
% \SetKwInput{KwOutput}{Output} 
% \small
% \caption{\texttt{StableSPAM}}
% \label{algo:adampro}
% \DontPrintSemicolon
%   \KwInput{ A layer weight matrix $w \in \mathbb{R}^{m \times n}$, learning rate $\alpha$, decay rates $\beta_1=0.9, \beta_2=0.999 $, initial parameters $w_0$, randomly initialize mask $\bm{\mathrm{M}}$ with $d$ density for each layer, the first moment $m$, the second moment $v$, threshold $\theta$ for GSS, momentum rerest interval $\Delta T$, warmup scale total steps $N$, small constant $\epsilon=1 \times 10^{-6}$. $T$ is total training steps.  }
% %   \KwInit{\cross{Use $\text{T}_\text{magnitude}$ to initialize $W$ according to sparsity ratio $s$.}}%; set initial $p$ value.}

%   \KwOutput{optimized parameters $w_T$.}

%   \While{ $t < \text{T}$ }
%   {     // \textcolor{blue}{Generate Gradients}
  
%         Get $g_t \in \mathbb{R}^{m \times n} \gets - \nabla_W \phi_t(w_t)$ 


%         // \textcolor{blue}{Spike Gradients Scaling}
        
%         max\_grad=\texttt{Max}(\texttt{abs}($g_t$))
        
%         EMA\_theshold=EMA\_theshold*\theta+(1-\theta)*\text{max\_grad}
        
%         EMA\_theshold\_hat=EMA\_theshold/(1-\theta^t) 
        
%         Spike\_M=\texttt{abs}($g_t$)$>$\text{EMA\_theshold\_hat}
        
%         \If{\texttt{sum}(\text{Spike\_M}) $>$ 0}
%         {
%             $g_t$[Spike\_M]=$g_t$[Spike\_M]/max\_grad*EMA\_theshold\_hat
%             }
        
%          // \textcolor{blue}{Adaptive Norm for Gradient}
         
%         grad\_norm=$\|g_t\|_2$

%         m\_norm\_t=\gamma_1*\text{m\_norm\_t}+(1-\gamma_1)*\text{grad\_norm}

%         v\_norm\_t=\gamma_2*\text{v\_norm\_t}+(1-\gamma_2)*\text{grad\_norm}^2

%         m\_norm\_hat\_t=m\_norm\_t/(1-\gamma_1^t)

%         v\_norm\_hat\_t=v\_norm\_t/(1-\gamma_2^t)

%         adaptive\_norm=m\_norm\_hat\_t/(\sqrt{\text{v\_norm\_hat\_t}}+\epsilon)
        
%         $g_t$=$g_t$/\text{grad\_norm}*\text{adaptive\_norm}
        
%         //\textcolor{blue}{Momentum Reset}     
        
%         \If{Mod $(t,\Delta T)=0$} 
%         {        
%         \ \  $\bm{\mathrm{m}} \leftarrow \texttt{zeros\_like}(\theta)$ 
        
%         \ \  $\bm{\mathrm{v}} \leftarrow \texttt{zeros\_like}(\theta)$ 
%         }
        
%         //\textcolor{blue}{Adam Updates} 
        
%         \ \ $\bm{\mathrm{m}}_t = \beta_1 \bm{\mathrm{m}}_{t-1} + (1 - \beta_1) g_t$
        
%         \ \ $\bm{\mathrm{v}}_t = \beta_2 \bm{\mathrm{v}}_{t-1} + (1 - \beta_2) g_t^2$
        
%         \ \ $\hat{\bm{\mathrm{m}}}_t = \frac{\bm{\mathrm{m}}_t}{1 - \beta_1^t}$

%         \ \ $\hat{\bm{\mathrm{v}}}_t = \frac{\bm{\mathrm{v}}_t}{1 - \beta_2^t}$
        
%         \ \ $w_t = w_{t-1} - \alpha* \frac{\hat{\bm{\mathrm{m}}}_t}{\sqrt{\hat{\bm{\mathrm{v}}}_t} + \epsilon}$
        
%         \ \ t=t+1
%   }
%     \textbf{Return: } optimized parameters $w_T$
% \end{algorithm*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% \begin{algorithm*}[h]
% \SetKwInput{KwInput}{Input}                % Set the Input
% \SetKwInput{KwOutput}{Output}
% \small
% \caption{\texttt{Stable-SPAM}}
% \label{algo:stablespam}
% \DontPrintSemicolon

% \KwInput{%
% A layer weight matrix $w \in \mathbb{R}^{m \times n}$, 
% learning rate $\alpha$, 
% decay rates $\beta_1 = 0.9$, $\beta_2 = 0.999$, 
% initial parameters $w_0$, $\gamma_1=0.7, \gamma_2=0.9$ for \texttt{AdaGN} and $\gamma_3=0.999$ for \texttt{AdaClip}, 
% momentum reset interval $\Delta T$, small constant $\epsilon = 1 \times 10^{-6}$, 
% and total training steps $T$.}

% \KwOutput{Optimized parameters $w_T$.}

% \While{$t < T$}{

%     % --------------------------------------------------
%     % 1) Generate Gradients
%     % --------------------------------------------------
%     // \textcolor{blue}{Generate Gradients}
    
%     \quad $g_t \in \mathbb{R}^{m \times n} \gets - \nabla_{w} \phi_t(w_t)$
%     \tcp*{Gradient of the objective at step $t$.}

%     % --------------------------------------------------
%     % 2) Adaptive Spike Gradient Scaling (AdaClip)
%     % --------------------------------------------------
%     // \textcolor{blue}{AdaClip}
    
%     \quad $g_{\max} = \texttt{Max}(\texttt{abs}(g_t))$ \\
%     \quad $T_{\mathrm{threshold}} = T_{\mathrm{threshold}} \cdot \theta + (1 - \theta)\, g_{\max}$ \\
%     \quad $\widehat{T}_{\mathrm{threshold}} = \dfrac{T_{\mathrm{threshold}}}{1 - \theta^t}$
%     \tcp*{Bias correction for threshold} \\
%     \quad $\text{Mask}_{\mathrm{spikes}} = \bigl(\texttt{abs}(g_t) > \widehat{T}_{\mathrm{threshold}}\bigr)$ \\
%     \quad \If{$\texttt{sum}\bigl(\text{Mask}_{\mathrm{spikes}}\bigr) > 0$}{
%         \quad\quad $g_t[\text{Mask}_{\mathrm{spikes}}] = 
%         \dfrac{g_t[\text{Mask}_{\mathrm{spikes}}]}{g_{\max}}
%         \;\times\; \widehat{T}_{\mathrm{threshold}}$ \\
%     }

%     % --------------------------------------------------
%     % 3) Adaptive Gradient Normalization (AdaGN)
%     % --------------------------------------------------
%     // \textcolor{blue}{AdaGN}
    
%     \quad $g_{\mathrm{norm}} = \|g_t\|_2$ \\
%     \quad $m_{\mathrm{norm}} = \gamma_1 \, m_{\mathrm{norm}} 
%             + (1 - \gamma_1)\, g_{\mathrm{norm}}$ \\
%     \quad $v_{\mathrm{norm}} = \gamma_2 \, v_{\mathrm{norm}} 
%             + (1 - \gamma_2)\, g_{\mathrm{norm}}^2$ \\
%     \quad $\widehat{m}_{\mathrm{norm}} = \dfrac{m_{\mathrm{norm}}}{1 - \gamma_1^t}, 
%            \quad
%            \widehat{v}_{\mathrm{norm}} = \dfrac{v_{\mathrm{norm}}}{1 - \gamma_2^t}$
%     \tcp*{Bias-corrected norm estimates} \\
%     \quad $\mathrm{adaptive\_norm} = 
%             \dfrac{\widehat{m}_{\mathrm{norm}}}{
%                 \sqrt{\widehat{v}_{\mathrm{norm}}} + \epsilon
%             }$ \\
%     \quad $g_t = \dfrac{g_t}{g_{\mathrm{norm}}} \times \mathrm{adaptive\_norm}$

%     % --------------------------------------------------
%     % 4) Momentum Reset 
%     % --------------------------------------------------
%     // \textcolor{blue}{Momentum Reset}
    
%     \quad \If{(\texttt{Mod}(t, $\Delta T$) = 0)}{
%         \quad\quad $\bm{m} \gets \texttt{zeros\_like}(\bm{m})$ \\
%         \quad\quad $\bm{v} \gets \texttt{zeros\_like}(\bm{v})$
%     }

%     % --------------------------------------------------
%     % 5) Adam Updates
%     % --------------------------------------------------
%     // \textcolor{blue}{Adam Updates}
    
%     \quad $\bm{m}_t = \beta_1 \,\bm{m}_{t-1} + (1 - \beta_1)\, g_t$ \\
%     \quad $\bm{v}_t = \beta_2 \,\bm{v}_{t-1} + (1 - \beta_2)\, g_t^2$ \\
%     \quad $\widehat{\bm{m}}_t = \dfrac{\bm{m}_t}{1 - \beta_1^t} 
%          \quad // \text{bias correction}$ \\
%     \quad $\widehat{\bm{v}}_t = \dfrac{\bm{v}_t}{1 - \beta_2^t} 
%          \quad // \text{bias correction}$ \\
%     \quad $w_t = w_{t-1} - \alpha 
%             \;\dfrac{\widehat{\bm{m}}_t}{
%                 \sqrt{\widehat{\bm{v}}_t} + \epsilon
%             }$ \\
%     \quad $t = t + 1$
% }
%  \textbf{Return:} Optimized parameters $w_T$.
% \end{algorithm*}


\begin{algorithm*}[h]
\SetKwInput{KwInput}{Input}                
\SetKwInput{KwOutput}{Output}
\small
\caption{\texttt{Stable-SPAM}}
\label{algo:stablespam}
\DontPrintSemicolon

\KwInput{%
A layer weight matrix $w \in \mathbb{R}^{m \times n}$, 
learning rate $\alpha$, 
decay rates $\beta_1 = 0.9$, $\beta_2 = 0.999$, 
initial parameters $w_0$, $\gamma_1=0.7, \gamma_2=0.9$ for \texttt{AdaGN} and $\gamma_3=0.999$ for \texttt{AdaClip}, 
momentum reset interval $\Delta T$, small constant $\epsilon = 1 \times 10^{-6}$, 
and total training steps $T$.}
\KwOutput{Optimized parameters $w_T$.}

\While{$t < T$}{
    \quad $g_t \in \mathbb{R}^{m \times n} \gets - \nabla_{w} \phi_t(w_t)$\tcp*[f]{Gradient of the objective at step $t$.}
    
    \quad $g_{\max} \gets \texttt{Max}(\texttt{abs}(g_t))$\;

    \quad $T_{\mathrm{threshold}} \gets T_{\mathrm{threshold}} \cdot \theta + (1 - \theta)\, g_{\max}$\;
    
    \quad $\widehat{T}_{\mathrm{threshold}} \gets \dfrac{T_{\mathrm{threshold}}}{1 - \theta^t}$\tcp*[f]{Bias correction for threshold}
    
    \quad $\text{Mask}_{\mathrm{spikes}} \gets \bigl(\texttt{abs}(g_t) > \widehat{T}_{\mathrm{threshold}}\bigr)$\;
    
    \quad \If{$\texttt{sum}\bigl(\text{Mask}_{\mathrm{spikes}}\bigr) > 0$}{
        \quad\quad $g_t[\text{Mask}_{\mathrm{spikes}}] \gets 
        \dfrac{g_t[\text{Mask}_{\mathrm{spikes}}]}{g_{\max}}
        \times \widehat{T}_{\mathrm{threshold}}$\;
        
    }
    
    \quad $g_{\mathrm{norm}} \gets \|g_t\|_2$\;
    
    \quad $m_{\mathrm{norm}} \gets \gamma_1\, m_{\mathrm{norm}} + (1 - \gamma_1)\, g_{\mathrm{norm}}$\;
    
    \quad $v_{\mathrm{norm}} \gets \gamma_2\, v_{\mathrm{norm}} + (1 - \gamma_2)\, g_{\mathrm{norm}}^2$\;
    
    \quad $\widehat{m}_{\mathrm{norm}} \gets \dfrac{m_{\mathrm{norm}}}{1 - \gamma_1^t}$, 
           $\widehat{v}_{\mathrm{norm}} \gets \dfrac{v_{\mathrm{norm}}}{1 - \gamma_2^t}$\tcp*[f]{Bias-corrected norm estimates}
           
    \quad $\mathrm{adaptive\_norm} \gets \dfrac{\widehat{m}_{\mathrm{norm}}}{\sqrt{\widehat{v}_{\mathrm{norm}}} + \epsilon}$\;
    
    \quad $g_t \gets \dfrac{g_t}{g_{\mathrm{norm}}} \times \mathrm{adaptive\_norm}$\;
    
    \quad \If{(\texttt{Mod}(t, $\Delta T$) = 0)}{
        \quad\quad $\bm{m} \gets \texttt{zeros\_like}(\bm{m})$\;
        
        \quad\quad $\bm{v} \gets \texttt{zeros\_like}(\bm{v})$\;
    }
    
    \quad $\bm{m}_t \gets \beta_1\, \bm{m}_{t-1} + (1 - \beta_1)\, g_t$\;
    
    \quad $\bm{v}_t \gets \beta_2\, \bm{v}_{t-1} + (1 - \beta_2)\, g_t^2$\;
    
    \quad $\widehat{\bm{m}}_t \gets \dfrac{\bm{m}_t}{1 - \beta_1^t}$\tcp*[f]{bias correction}\;
    
    \quad $\widehat{\bm{v}}_t \gets \dfrac{\bm{v}_t}{1 - \beta_2^t}$\tcp*[f]{bias correction}\;
    
    \quad $w_t \gets w_{t-1} - \alpha\, \dfrac{\widehat{\bm{m}}_t}{\sqrt{\widehat{\bm{v}}_t} + \epsilon}$\;
    
    \quad $t \gets t + 1$\;
}
\Return $w_T$.
\end{algorithm*}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
%

      \midrule
        Adam + \texttt{MoRet}+\texttt{AdaGN}+\textcolor{blue}{SpikeClip}~\cite{huang2025spam} &32.01& 28.90 \\
        Adam + \texttt{MoRet}+ \textcolor{blue}{GradClip}~\cite{goodfellow2016deep}+\texttt{AdaClip} &31.95  & 29.87\\
             \gr  Adam + \texttt{MoRet}+\texttt{AdaGN}+\texttt{AdaClip} (\textbf{\ours}) & \textbf{31.40} & \textbf{28.84} \\
        \midrule
%        \midrule
        Adam &25.36  &24.28 \\
        Adam + \texttt{MoRet} &  23.30 &22.70\\
                Adam + \texttt{MoRet} + \texttt{AdaClip} &22.94 & 22.22\\
        Adam + \texttt{MoRet} + \texttt{AdaGN} & 22.89 &22.58 \\
      \gr  Adam + \texttt{MoRet} + \texttt{AdaGN} + \texttt{AdaClip} (\textbf{\ours}) & \textbf{22.30} & \textbf{22.08} \\
      \midrule
%