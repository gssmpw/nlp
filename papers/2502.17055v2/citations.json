[
  {
    "index": 0,
    "papers": [
      {
        "key": "chowdhery2023palm",
        "author": "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others",
        "title": "Palm: Scaling language modeling with pathways"
      },
      {
        "key": "molybog2023theory",
        "author": "Molybog, Igor and Albert, Peter and Chen, Moya and DeVito, Zachary and Esiobu, David and Goyal, Naman and Koura, Punit Singh and Narang, Sharan and Poulton, Andrew and Silva, Ruan and others",
        "title": "A theory on adam instability in large-scale machine learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "goodfellow2016deep",
        "author": "Goodfellow, Ian",
        "title": "Deep learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "shazeer2018adafactor",
        "author": "Shazeer, Noam and Stern, Mitchell",
        "title": "Adafactor: Adaptive learning rates with sublinear memory cost"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "huang2025spam",
        "author": "Huang, Tianjin and Zhu, Ziquan and Jin, Gaojie and Liu, Lu and Wang, Zhangyang and Liu, Shiwei",
        "title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "xiong2020layer",
        "author": "Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan",
        "title": "On layer normalization in the transformer architecture"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dettmers20218",
        "author": "Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke",
        "title": "8-bit optimizers via block-wise quantization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "scao2022language",
        "author": "Scao, Teven Le and Wang, Thomas and Hesslow, Daniel and Saulnier, Lucile and Bekman, Stas and Bari, M Saiful and Biderman, Stella and Elsahar, Hady and Muennighoff, Niklas and Phang, Jason and others",
        "title": "What language model to train if you have one million gpu hours?"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ding2021cogview",
        "author": "Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others",
        "title": "Cogview: Mastering text-to-image generation via transformers"
      },
      {
        "key": "zeng2022glm",
        "author": "Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others",
        "title": "Glm-130b: An open bilingual pre-trained model"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2024deepnet",
        "author": "Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu",
        "title": "Deepnet: Scaling transformers to 1,000 layers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhai2023stabilizing",
        "author": "Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M",
        "title": "Stabilizing transformer training by preventing attention entropy collapse"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "takase2023spike",
        "author": "Takase, Sho and Kiyono, Shun and Kobayashi, Sosuke and Suzuki, Jun",
        "title": "Spike No More: Stabilizing the Pre-training of Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "nguyen2019transformers",
        "author": "Nguyen, Toan Q and Salazar, Julian",
        "title": "Transformers without tears: Improving the normalization of self-attention"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2019fixup",
        "author": "Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu",
        "title": "Fixup initialization: Residual learning without normalization"
      },
      {
        "key": "huang2020improving",
        "author": "Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims",
        "title": "Improving transformer optimization through better initialization"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wang2018training",
        "author": "Wang, Naigang and Choi, Jungwook and Brand, Daniel and Chen, Chia-Yu and Gopalakrishnan, Kailash",
        "title": "Training deep neural networks with 8-bit floating point numbers"
      },
      {
        "key": "lin2022device",
        "author": "Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song",
        "title": "On-device training under 256kb memory"
      },
      {
        "key": "xi2024coat",
        "author": "Xi, Haocheng and Cai, Han and Zhu, Ligeng and Lu, Yao and Keutzer, Kurt and Chen, Jianfei and Han, Song",
        "title": "COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training"
      },
      {
        "key": "xi2024jetfire",
        "author": "Xi, Haocheng and Chen, Yuxiang and Zhao, Kang and Teh, Kai Jun and Chen, Jianfei and Zhu, Jun",
        "title": "Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization"
      },
      {
        "key": "wortsman2023stable",
        "author": "Wortsman, Mitchell and Dettmers, Tim and Zettlemoyer, Luke and Morcos, Ari and Farhadi, Ali and Schmidt, Ludwig",
        "title": "Stable and low-precision training for large-scale vision-language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "micikevicius2017mixed",
        "author": "Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others",
        "title": "Mixed precision training"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "kalamkar2019study",
        "author": "Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and others",
        "title": "A study of BFLOAT16 for deep learning training"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "peng2023fp8",
        "author": "Peng, Houwen and Wu, Kan and Wei, Yixuan and Zhao, Guoshuai and Yang, Yuxiang and Liu, Ze and Xiong, Yifan and Yang, Ziyue and Ni, Bolin and Hu, Jingcheng and others",
        "title": "Fp8-lm: Training fp8 large language models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "fishman2024scaling",
        "author": "Fishman, Maxim and Chmiel, Brian and Banner, Ron and Soudry, Daniel",
        "title": "Scaling FP8 training to trillion-token LLMs"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "fishman2024scaling",
        "author": "Fishman, Maxim and Chmiel, Brian and Banner, Ron and Soudry, Daniel",
        "title": "Scaling FP8 training to trillion-token LLMs"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "ashkboos2025halo",
        "author": "Ashkboos, Saleh and Nikdan, Mahdi and Tabesh, Soroush and Castro, Roberto L and Hoefler, Torsten and Alistarh, Dan",
        "title": "HALO: Hadamard-Assisted Lossless Optimization for Efficient Low-Precision LLM Training and Fine-Tuning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "rouhani2023microscaling",
        "author": "Rouhani, Bita Darvish and Zhao, Ritchie and More, Ankit and Hall, Mathew and Khodamoradi, Alireza and Deng, Summer and Choudhary, Dhruv and Cornea, Marius and Dellinger, Eric and Denolf, Kristof and others",
        "title": "Microscaling data formats for deep learning"
      }
    ]
  }
]