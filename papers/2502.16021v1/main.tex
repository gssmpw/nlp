
\documentclass[11pt]{article} % For LaTeX2e
\usepackage{times}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{algorithmic}
\usepackage[ruled,vlined]{algorithm2e}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage[backref,colorlinks,citecolor=blue,bookmarks=true]{hyperref}
\usepackage{mathtools, amssymb, amsthm, bbm}
\numberwithin{equation}{section}
%\usepackage{natbib}

\usepackage{parskip}
\setlength{\parskip}{5pt}

\SetAlCapSkip{1em}
\input{more_defs.tex}
\usepackage{url}
\usepackage[title]{appendix}

\title{Learning Neural Networks with Distribution Shift: \\Efficiently Certifiable Guarantees}

\author{
     Gautam Chandrasekaran\thanks{\texttt{gautamc@cs.utexas.edu}. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML).} \\ UT Austin
     \and Adam R. Klivans\thanks{\texttt{klivans@cs.utexas.edu}. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of Machine Learning (IFML).} \\
	 UT Austin
     \and Lin Lin Lee \thanks{\texttt{llee3@utexas.edu}. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML).} \\ UT Austin
	 \and Konstantinos Stavropoulos\thanks{\texttt{kstavrop@cs.utexas.edu}. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML) and by scholarships from Bodossaki Foundation and Leventis Foundation.} \\
	 UT Austin
}
\date{\today}

\begin{document}

\maketitle
\begin{abstract}
We give the first provably efficient algorithms for learning neural networks with distribution shift. We work in the Testable Learning with Distribution Shift  framework (TDS learning) of \cite{klivans2023testable}, where the learner receives labeled examples from a training distribution and unlabeled examples from a test distribution and must either output a hypothesis with low test error or reject if distribution shift is detected.  No assumptions are made on the test distribution. 

All prior work in TDS learning focuses on classification, while here we must handle the setting of nonconvex regression. Our results apply to real-valued networks with arbitrary Lipschitz activations and work whenever the training distribution has strictly sub-exponential tails. For training distributions that are bounded and hypercontractive, we give a fully polynomial-time algorithm for TDS learning one hidden-layer networks with sigmoid activations. We achieve this by importing classical kernel methods into the TDS framework using data-dependent feature maps and a type of kernel matrix that couples samples from both train and test distributions. 

%outlined in \cite{reliable_goel2017}.
%   We provide the first efficient algorithms for learning neural networks with distribution shift.
%   We work in the framework of Testable Learning with Distribution Shift (TDS) of Klivans et al. (2024), where the learner receives labeled examples from a training distribution and unlabeled examples from some other test distribution and is asked to either output a hypothesis with low  test error or reject if distribution shift is detected.
%   All prior work in TDS learning concerns classification tasks, while we focus on $\gL_2$-regression.
%   Our results capture neural networks with arbitrary Lipschitz activations and work whenever the training distribution has strictly sub-exponential tails. %We follow a moment-based approach, similar to prior techniques in TDS learning, but we require less assumptions on the training distribution. %We achieve this by using polynomial approximation of Lipschitz functions.
%   %Our results capture neural networks with arbitrary Lipschitz activations and work whenever the training distribution has strictly sub-exponential tails. This covers a much broader range of distributions, significantly expanding beyond those for which TDS learning is feasible in classification.
%   % - For one hidden-layer networks with sigmoid activations, we obtain fully polynomial-time algorithms assuming that the training marginal is bounded and hypercontractive.
%   %- For bounded and hypercontractive training marginals, we recover all of the results of [Goel-reliable] in the TDS setting, by proposing a new efficient tester certifying low test error for their Kernel-based approach.
%   For training distributions that are bounded and hypercontractive, we provide improved bounds, including a fully polynomial-time algorithm for TDS learning one hidden-layer networks with sigmoid activations. 
%   %- We also give efficient algorithms for TDS learning deeper networks with arbitrary Lipschitz activations with an exponential dependence on the error parameter.
%   We accomplish this by proposing a new spectral tester that efficiently certifies low test error for the kernel-based method outlined in Goel et al. (2017).
%   %This extends all of their results to the TDS setting.
%   %- In particular, we obtain fully polynomial-time algorithms for TDS learning one hidden-layer networks with sigmoid activations.


% %- Our results capture neural networks with arbitrary Lipschitz activations and work whenever the training distribution has strictly sub-exponential tails.
%   %- Their work is based on the Kernel method for which we design a new efficient tester to certify low test error.
%   %- Goel et al. only consider bounded marginals, but we also obtain results for the larger class of arbitrary strictly subexponential training marginals.
\end{abstract}

\newpage
\section{Introduction}

Understanding when a model will generalize from a known training distribution to an unknown test distribution is a critical challenge in trustworthy machine learning and domain adaptation.  Traditional approaches to this problem prove generalization bounds in terms of various notions of distance between train and test distributions \cite{ben2006analysis,ben2010theory,mansour2009domadapt} but do not provide efficient algorithms.  Recent work due to \cite{klivans2023testable} departs from this paradigm and defines the model of Testable Learning with Distribution Shift (TDS learning), where a learner may reject altogether if significant distribution shift is detected.  When the learner accepts, however, it outputs a classifier and a proof that the classifier has nearly optimal test error.   

A sequence of works has given the first set of efficient algorithms in the TDS learning model for well-studied function classes where no assumptions are taken on the test distribution \cite{klivans2023testable,klivans2024learning,chandrasekaran2024efficient,goel2024tolerant}. These results, however, hold for classification and therefore do not apply to (nonconvex) regression problems and in particular to a long line of work giving provably efficient algorithms for learning simple classes of neural networks under natural distributional assumptions on the training marginal \cite{goel2019learning,diakonikolas2020approximation,diakonikolas2020algorithms,diakonikolas2022learning,chen2022learning,chen2023learning,wang2023robustly,gollakota2024agnostically,diakonikolas2024efficiently}.  




The main contribution of this work is the first set of efficient TDS learning algorithms for broad classes of (nonconvex) regression problems.  Our results apply to neural networks with arbitrary Lipschitz activations of any constant depth.  As one example, we obtain a fully polynomial-time algorithm for learning one hidden-layer neural networks with sigmoid activations with respect to any bounded and hypercontractive training distribution.  For bounded training distributions, the running times of our algorithms match the best known running times for ordinary PAC or agnostic learning (without distribution shift).  We emphasize that unlike all prior work in domain adaptation, we make no assumptions on the test distribution.  

%More recent work departs from this paradigm and considers scenarios where a classifier may abstain or the learner may reject altogether if significant distribution shift is detected (for example, PQ learning \cite{goldwasser2020beyond,kalai2021efficient,goel2024tolerant} or Testable Learning with Distribution Shift (TDS learning) ).


\noindent\textbf{Regression Setting.} We assume the learner has access to labeled examples from the training distribution and unlabeled examples from the marginal of the test distribution. We consider the squared loss $\gL_{\gD}(h) = \sqrt{\E_{(\x,y)\sim \gD}[(y-h(\x))^2]}$. The error benchmark is analogous to the benchmark for TDS learning in classification \cite{klivans2023testable} and depends on two quantities: the optimum training error achievable by a classifier in the learnt class, $\opt = \min_{f\in\gF}[\gL_{\Dtrain}(f)]$, and the best joint error achievable by a single classifier on both the training and test distributions, $\lambda = \min_{f'\in\gF}[\gL_{\Dtrain}(f')+\gL_{\Dtest}(f')]$. Achieving an error of $\opt + \lambda$ is the standard goal in domain adaptation \cite{ben2006analysis,blitzer2007learning,mansour2009domadapt}. We now formally define the TDS learning framework for regression.

\begin{definition}[Testable Regression with Distribution Shift]
    For $\eps,\delta\in(0,1)$ and a function class $\gF\subseteq\{\R^d\to \R\}$, the learner receives iid labeled examples from some unknown training distribution $\Dtrain$ over $\R^d\times \R$ and iid unlabeled examples from the marginal $\Dtestx$ of another unknown test distribution $\Dtest$ over $\R^d\times \R$. The learner  either rejects, or it accepts and outputs hypothesis $h:\R^d\to \R$ such that the following are true.
    \begin{enumerate}
        \item (Soundness) With probability at least $1-\delta$, if the algorithm accepts, then the output $h$ satisfies $\gL_{\Dtest}(h) \le \min_{f\in\gF}[\gL_{\Dtrain}(f)] + \min_{f'\in\gF}[\gL_{\Dtrain}(f')+\gL_{\Dtest}(f')] + \eps$.
        \item (Completeness) If $\Dtrainx = \Dtestx$, then the algorithm accepts with probability at least $1-\delta$.
    \end{enumerate}
\end{definition}


\subsection{Our Results} 

    Our results hold for classes of Lipschitz neural networks. In particular, we consider functions $f$ of the following form. Let $\sigma:\R\to\R$ be an activation function. Let $\W=\left(W^{(1)},\ldots W^{(t)}\right)$ with $W^{(i)}\in \R^{s_i\times s_{i-1}}$ be the tuple of weight matrices. Here, $s_0=d$ is the input dimension and $s_{t}=1$. Define recursively the function $f_i:\R^{d}\to \R^{s_i}$ as $f_i(\x)=W^{(i)}\cdot \sigma\bigl(f_{i-1}(\x)\bigr)$ with $f_1(\x)=W^{(1)}\cdot\x$. The function $f:\R^d \to \R$ computed by the neural network $(\W,\sigma)$ is defined as 
    $f(\x)\coloneq f_{t}(\x)$. %We denote $\norm{\W}_1=\sum_{k=2}^{t}\sum_{(i,j)\in [s_{i}]\times [s_{i-1}]}|W^{(k)}_{i,j}|$. 
    The depth of this network is $t$. 

    We now present our main results on TDS learning for neural networks. 

% {
% \color{red}


% \paragraph{Results:} (maybe try tables)
% \begin{enumerate}
%     \item Bounded+Hypercontractive:
%     \begin{enumerate}
%         \item Sigmoid, one hidden layer: fully poly
%         \item ReLU: poly up to error $1/\log d$
%         \item Nets with Lipschitz neurons: ...
%         \item More statements in Section blah
%     \end{enumerate}
%     \item Strictly Subexponential:
%     \begin{enumerate}
%         \item Sigmoid nets
%         \item Lipschitz nets
%     \end{enumerate}
%     \item Brief discussion about the labels (refer to the corresponding appendix).
%     \item A word about single-index models (different Lipschitz activations).
% \end{enumerate}

% \color{black}
% }
\renewcommand{\arraystretch}{2}
\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|}
\hline
Function Class &  Runtime (Bounded) & Runtime (Subgaussian)\\
\hline
\hline
One hidden-layer Sigmoid Net & $\poly(d,M,1/\epsilon)$& $d^{ \poly(k\log(M/\epsilon))}$\\
\hline
Single ReLU & $\poly(d,M)\cdot 2^{O(1/\epsilon)}$& $d^{ \poly(k\log M/\epsilon)}$\\
\hline
Sigmoid Nets &$\poly(d,M) \cdot 2^{O\left((\log(1/\epsilon))^{t-1}\right)}$& $d^{ \poly(k\log M(\log(1/\epsilon)^{t-1}))}$\\
\hline
$1$-Lipschitz Nets & $\poly(d,M) \cdot 2^{\tilde{O}(k\sqrt{k}2^{t-1}/\epsilon)}$& $d^{\poly(k2^{t-1}\log M/\epsilon)}$\\
\hline
\end{tabular}
\caption{In the above table, $k$ denotes the number of neurons in the first hidden layer. $M$ denotes a bound on the labels of the train and test distributions. One hidden-layer Sigmoid nets refers to depth $2$ neural networks with sigmoid activation. The bounded distributions considered in the above table have support on the unit ball. We assume that all relevant parameters of the neural network are bounded by constants. For more detailed statements and proofs, see (1) \Cref{clry:polytime_tds_sigmoid_appendix,clry:polytime_tds_relu_appendix,thm:tds_learning_sigmoid_appendix,thm:tds_learning_lipschitz_appendix}  for the bounded case, and (2) \Cref{thm:tds_learning_sigmoid_subexp_appendix,thm:tds_learning_lipschitz_subexp_appendix} for the Subgaussian case.} \label{table:main-results}
\end{table}

From the above table, we highlight that in the cases of bounded distributions with (1) one hidden-layer Sigmoid Nets, and (2) Single ReLU with $\epsilon<1/\log d$, we obtain TDS algorithms that run in polynomial time in all parameters. Moreover, for the last row, regarding Lipschitz Nets, each neuron is allowed to have a different and unknown Lipschitz activation. Therefore, in particular, our results capture the class of single-index models (see, e.g., \cite{glmtron,gollakota2024agnostically}).

In the results of \Cref{table:main-results}, we assume bounded labels for both the training and test distributions. This assumption can be relaxed to a bound on any moment whose degree is strictly higher than $2$ (see \Cref{corollary:label-moment-bound-assumption-suffices}). In fact, such an assumption is necessary, as we show in \Cref{proposition:bounded-labels-necessary}.


\subsection{Our Techniques}

\noindent\textbf{TDS Learning via Kernel Methods.} 
The major technical contribution of this work is devoted to importing classical kernel methods into the TDS learning framework.  A first attempt at testing distribution shift with respect to a fixed feature map would be to form two corresponding covariance matrices of the expanded features, one from samples drawn from the training distribution and the other from samples drawn from the test distribution, and test if these two matrices have similar eigendecompositions. This approach only yields efficient algorithms for linear kernels, however, as here we are interested in spectral properties of covariance matrices in the feature space corresponding to low-degree polynomials, whose dimension is too large. 

%kernel matrices, one from samples drawn from the training distribution and the other from samples drawn from the test distribution, and test if these two matrices have similar eigendecompositions.  This approach only works for linear kernels, however, and here we are interested in spectral properties of covariance matrices in the feature space corresponding to low-degree polynomials. 

%This approach does not work, however, as the feature map used for training-- in particular the standard monomial basis in our setting-- may not be the correct basis for an arbitrary test distribution.

Instead we form a new data-dependent and concise reference feature map $\phi$, that depends on examples from both $\Dtrainx$ and $\Dtestx$. We show that this feature map approximately represents the ground truth, i.e., some function with both low training and test error (this is due to the representer theorem, see \Cref{proposition:representer-theorem}). To certify that error bounds transfer from $\Dtrainx$ to $\Dtestx$, we require {\em relative error} closeness between  covariance matrix $\Phi' = \E_{\x\sim \Dtestx}[\phi(\x)\phi(\x)^\top]$ of the feature expansion $\phi$ over the test marginal with the corresponding matrix $\Phi = \E_{\x\sim \Dtrainx}[\phi(\x)\phi(\x)^\top]$ over the training marginal. We draw fresh sets of verification examples and show how the kernel trick can be used to efficiently achieve these approximations even though $\phi$ is a nonstandard feature map. We provide a more detailed technical overview and a formal proof in \Cref{section:tds-kernel}.

%In order to efficiently compare these matrices, we draw a fresh pair of sets of verification examples from $\Dtrainx$ and $\Dtestx$ and use the properties of the kernel method in a non-standard way.


%runs an appropriate spectral algorithm, which is efficient, due to the low dimension of $\phi$.


%Kernel methods are known to provide significant improvements in the runtime of learning algorithms. This follows from a set of properties of kernel functions related to generalization as well as optimization over the corresponding Hilbert space (see \Cref{section:bounded} for more details). In this work, we demonstrate another important advantage of the kernel method: it can be used to guide the tests required for TDS learning, improving the testing time as well. 

%More specifically, for function classes that can be approximately represented with respect to some efficiently computable kernel, we obtain a nonstandard data-dependent feature map that is useful for testing distribution shift. In particular, it suffices to measure the spectral properties of the covariance matrix of this data-dependent feature map.  Notably, these types of kernel matrices must be formed using covariates from both the training and test distribution. The final testing procedure can be implemented using known efficient algorithms for computing eigendecompositions. 

%certify properties of the feature map on the test distribution using fresh training and test examples. The quality of the feature map essentially corresponds to its ability of transferring bounds under the training distribution to bounds under the test distribution. This can be 

By instantiating the above results using a type of polynomial kernel, we can reduce the problem of TDS learning neural networks to the problem of obtaining an appropriate polynomial approximator.  Our final {\em training} algorithm (as opposed to the testing phase) will essentially be kernelized polynomial regression. 

%Overall, we obtain TDS learning algorithms that match the runtime of the corresponding classical (distribution shift-free) kernel-based algorithms. 

\noindent\textbf{TDS Learning and Uniform Approximation.} Prior work in TDS learning has established connections between polynomial approximation theory and efficient algorithms in the TDS setting. In particular, the existence of low-degree sandwiching approximators for a concept class is known to imply dimension-efficient TDS learning algorithms for binary classification. The notion of sandwiching approximators for a function $f$ refers to a pair of low-degree polynomials $p_{\mathrm{up}}, p_{\mathrm{down}}$ with two main properties: (1) $p_{\mathrm{down}} \le f\le p_{\mathrm{up}}$ everywhere and (2) the expected absolute distance between $p_{\mathrm{up}}$ and $p_{\mathrm{down}}$ over some reference distribution is small. The first property is of particular importance in the TDS setting, since it holds everywhere and, therefore, it holds for any test distribution unconditionally.

Here we make the simple observation that the incomparable notion of uniform approximation suffices for TDS learning.  A uniform approximator is a polynomial $p$ that approximates a function $f$ pointwise, meaning that $|p-f|$ is small in every point within a ball around the origin (there is no known direct relationship between sandwiching and uniform approximators). In our setting, uniform approximation is more convenient, due to the existence of powerful tools from polynomial approximation theory regarding Lipschitz and analytic functions.

Contrary to the sandwiching property, the uniform approximation property cannot hold everywhere if the approximated function class contains high-(or infinite-) degree functions. 
%However, we are able to obtain TDS learning results even for unbounded distributions, using uniform (and not necessarily sandwiching) approximators. This is possible due to the quantitative relationship between the degree of approximation and the radius of the ball where the uniform approximation property holds. In particular, for each of the function classes we consider, the degree scales linearly to the radius of approximation. 
When the training distribution has strictly sub-exponential tails, however, the expected error of approximation outside the radius of approximation is negligible. Importantly, this property can be certified for the test distribution by using a moment-matching tester. See \Cref{section:tds-via-uniform} for a more detailed technical overview and for the full proof.

\subsection{Related Work}

\noindent\textbf{Learning with Distribution Shift.} The field of domain adaptation has been studying the distribution shift problem for almost two decades \cite{ben2006analysis,blitzer2007learning,ben2010theory,mansour2009domadapt,david2010impossibility,mousavi2020minimax,redko2020survey,kalavasis2024transfer,hanneke2019value,hanneke2024more,awasthi2024best}, providing useful insights regarding the information-theoretic (im)possibilities for learning with distribution shift. The first efficient end-to-end algorithms for non-trivial concept classes with distribution shift were given for TDS learning in \cite{klivans2023testable,klivans2024learning,chandrasekaran2024efficient} and for PQ learning, originally defined by \cite{goldwasser2020beyond}, in \cite{goel2024tolerant}. These works focus on binary classification for classes like halfspaces, halfspace intersections, and geometric concepts. In the regression setting, we need to handle unbounded loss functions, but we are also able to use Lipschitz properties of real-valued networks to obtain results even for deeper architectures. For the special case of linear regression, efficient algorithms for learning with distribution shift are known to exist (see, e.g., \cite{lei2021near}), but our results capture much broader classes. 

Another distinction between the existing works in TDS learning and our work, is that our results require significantly milder assumptions on the training distribution. In particular, while all prior works on TDS learning require both concentration and anti-concentration for the training marginal \cite{klivans2023testable,klivans2024learning,chandrasekaran2024efficient}, we only assume strictly subexponential concentration in every direction. This is possible because the function classes we consider are Lipschitz, which is not the case for binary classification.

\noindent\textbf{Testable Learning.} More broadly, TDS learning is related to the notion of testable learning \cite{rubinfeld2022testing,gollakota2022moment,gollakota2023efficient,diakonikolas2023efficient,gollakota2024tester,diakonikolas2024testable,slot2024testably}, originally defined by \cite{rubinfeld2022testing} for standard agnostic learning, aiming to certify optimal performance for learning algorithms without relying directly on any distributional assumptions. The main difference between testable agnostic learning and TDS learning is that in TDS learning, we allow for distribution shift, while in testable agnostic learning the training and test distributions are the same. Because of this, TDS learning remains challenging even in the absence of label noise, in which case testable learning becomes trivial \cite{klivans2023testable}.


\noindent\textbf{Efficient Learning of Neural Networks.} Many works have focused on providing upper and lower bounds on the computational complexity of learning neural networks in the standard (distribution-shift-free) setting \cite{reliable_goel2017,goel2019learning,goel2020superpolynomial,goel2020statistical,diakonikolas2020approximation,diakonikolas2020near,diakonikolas2020algorithms,diakonikolas2022learning,chen2022hardness,chen2022learning,chen2023learning,wang2023robustly,gollakota2024agnostically,diakonikolas2024efficiently,li2020learning,gao2019learning,zhang2019learning,vempala2019gradient,allen2019learning,bakshi2019learning,manurangsi2018computational,ge2019learning,ge2018learning,du2018convolutional,goel2018learning,tian2017analytical,li2017convergence,brutzkus2017globally,zhong2017recovery,zhang2016l1,janzamin2015beating}. The majority of the upper bounds either require noiseless labels and shallow architectures or work only under Gaussian training marginals. Our results not only hold in the presence of distribution shift, but also capture deeper architectures, under any strictly subexponential training marginal and allow adversarial label noise.

The upper bounds that are closest to our work are those given by \cite{reliable_goel2017}. They consider ReLU as well as sigmoid networks, allow for adversarial label noise and assume that the training marginal is bounded but otherwise arbitrary. Our results in \Cref{section:bounded} extend all of the results in \cite{reliable_goel2017} to the TDS setting, by assuming additionally that the training distribution is hypercontractive (see \Cref{definition:hypercontractivity-standard}). This additional assumption is important to ensure that our tests will pass when there is no distribution shift. For a more thorough technical comparison with \cite{reliable_goel2017}, see \Cref{section:bounded}.

In \Cref{section:unbounded}, we provide upper bounds for TDS learning of Lipschitz networks even when the training marginal is an arbitrary strictly subexponential distribution. In particular, our results imply new bounds for standard agnostic learning of single ReLU neurons, where we achieve runtime $d^{\poly({1/\eps})}$. The only known upper bounds work under the Gaussian marginal \cite{diakonikolas2020approximation}, achieving similar runtime. In fact, in the statistical query  framework \cite{kearns1998efficient}, it is known that $d^{\poly(1/\eps)}$ runtime is necessary for agnostically learning the ReLU, even under the Gaussian distribution \cite{diakonikolas2020near,goel2020statistical}. 


% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

\section{Preliminaries}

We use standard vector and matrix notation. We denote with $\R, \sN$ the sets of real and natural numbers accordingly. We denote with $\gD$ labeled distributions over $\R^d\times \R$ and with $\gD_\x$ the marginal of $\gD$ on the features in $\R^d$. For a set $S$ of points in $\R^d$, we define the empirical probabilities (resp. expectations) as $\Pr_{\x\sim S}[E(\x)] = \frac{1}{|S|}\sum_{\x\in S} \ind\{E(\x)\}$ (resp. $\E_{\x\sim S}[f(\x)] = \frac{1}{|S|}\sum_{\x\in S}f(\x)$). We denote with $\bar{S}$ the labeled version of $S$ and we define the clipping function $\clip_M:\R\to [-M,M]$, that maps a number $t\in\R$ either to itself if $t\in[-M,M]$, or to $M\cdot\sign(t)$ otherwise.

\noindent\textbf{Loss function.} Throughout this work, we denote with $\gL_{\gD}(h)$ the squared loss of a hypothesis $h:\R^d\to \R$ with respect to a labeled distribution $\gD$, i.e., $\gL_{\gD}(h) = \sqrt{\E_{(\x,y)\sim \gD}[(y-h(\x))^2]}$. Moreover, for any function $f:\R^d \to \R$, we denote with $\|f\|_{\gD}$ the quantity $\|f\|_{\gD} = \sqrt{\E_{\x\sim \gD_{\x}}[(f(\x))^2]}$. For a set of labeled examples $\bar{S}$, we denote with $\gL_{\bar S}(h)$ the empirical loss on $\bar S$, i.e., $\gL_{\bar S}(h) = \sqrt{\frac{1}{|\bar S|}\sum_{(\x,y)\in \bar S}(y-h(\x))^2}$ and similarly for $\|f\|_{S}$.

% \noindent\textbf{Regression Setting.} We assume access to labeled examples from the training distribution and unlabeled examples from the marginal of the test distribution. The error benchmark is analogous to the benchmark for TDS learning in classification \cite{klivans2023testable}, i.e., consists of two factors, the optimum training error achievable by a classifier in the learnt class, $\opt = \min_{f\in\gF}[\gL_{\Dtrain}(f)]$, and the best joint error achievable by a single classifier on both the training and test distributions, $\lambda = \min_{f'\in\gF}[\gL_{\Dtrain}(f')+\gL_{\Dtest}(f')]$. Achieving an error of $\opt + \lambda$ is a standard goal in domain adaptation \cite{ben2006analysis,blitzer2007learning,mansour2009domadapt}. 

% \begin{definition}[Testable Regression with Distribution Shift]
%     For $\eps,\delta\in(0,1)$ and a function class $\gF\subseteq\{\R^d\to \R\}$, the learner receives a number of i.i.d. labeled examples from some unknown training distribution $\Dtrain$ over $\R^d\times \R$ and a number of i.i.d. unlabeled examples from the marginal $\Dtestx$ of another unknown test distribution $\Dtest$ over $\R^d\times \R$ and is asked to either reject, or accept and output some hypothesis $h:\R^d\to \R$ such that the following are true.
%     \begin{enumerate}
%         \item (Soundness) With probability at least $1-\delta$, if the algorithm accepts, then the output $h$ satisfies $\gL_{\Dtest}(h) \le \min_{f\in\gF}[\gL_{\Dtrain}(f)] + \min_{f'\in\gF}[\gL_{\Dtrain}(f')+\gL_{\Dtest}(f')] + \eps$.
%         \item (Completeness) If $\Dtrainx = \Dtestx$, then the algorithm accepts with probability at least $1-\delta$.
%     \end{enumerate}
% \end{definition}

\noindent\textbf{Distributional Assumptions.} In order to obtain efficient algorithms, we will either assume that the training marginal $\Dtrainx$ is bounded and hypercontractive (\Cref{section:bounded}) or that it has strictly subexponential tails in every direction (\Cref{section:unbounded}). We make no assumptions on the test marginal $\Dtestx$.

Regarding the labels, we assume some mild bound on the moments of the training and the test labels, e.g., (a) that $\E_{y\sim\Dtrain_y}[y^4], \E_{y\sim\Dtest_y}[y^4] \le M$ or (b) that $y\in[-M,M]$ a.s. for both $\Dtrain$ and $\Dtest$. Although, ideally, we want to avoid any assumptions on the test distribution, as we show in \Cref{proposition:bounded-labels-necessary}, a bound on some constant-degree moment of the test labels is necessary.

\section{Bounded Training Marginals}\label{section:bounded}

We begin with the scenario where the training distribution is known to be bounded. In this case, it is known that one-hidden-layer sigmoid networks can be agnostically learned (in the classical sense, without distribution shift) in fully polynomial time and single ReLU neurons can be learned up to error $O(\frac{1}{\log(d)})$ in polynomial time \cite{reliable_goel2017}. These results are based on a kernel-based approach, combined with results from polynomial approximation theory. While polynomial approximations can reduce the nonconvex agnostic learning problem to a convex one through polynomial feature expansions, the kernel trick enables further pruning of the search space, which is important for obtaining polynomial-time algorithms. Our work demonstrates another useful implication of the kernel trick: it leads to efficient algorithms for testing distribution shift.
%give tests doe. In particular, certifying low test error can be reduced to checking some simple property of the test distribution that is relative to a data-dependent reference feature map of low dimension, which is provided by the kernel trick.

We will require the following standard notions:

%A positive definite, symmetric (PSD) kernel is a function defined as follows.
\begin{definition}[Kernels \cite{Mercer1909FunctionsOP}]\label{definition:kernel}
     A function $\Kernel: \R^d \times \R^d \to \R$ is a kernel. If for any set of $m$ points $\vx_1,\dots,\vx_m$ in $\R^d$, the matrix $(\Kernel(\vx_i,\vx_j))_{(i,j)\in[m]}$ is positive semidefinite, we say that the kernel $\Kernel$ is positive definite. The kernel $\Kernel$ is symmetric if for all $\vx,\vx'\in\R^d$, $\Kernel(\vx,\vx') = \Kernel(\vx',\vx)$.
\end{definition}
Any PSD kernel is associated with some Hilbert space $\sH$ and some feature map from $\R^d$ to $\sH$.
\begin{fact}[Reproducing Kernel Hilbert Space]\label{fact:rkhs}
    For any positive definite and symmetric (PDS) kernel $\Kernel$, there is a Hilbert space $\sH$, equipped with the inner product $\inprod{\cdot}{\cdot}:\sH \times \sH \to \R$ and a function $\psi:\R^d \to \sH$ such that $\Kernel(\x,\x') = \inprod{\psi(\x)}{\psi(\x')}$ for all $\x,\x'\in\R^d$. We call $\sH$ the reproducing kernel Hilbert space (RKHS) for $\Kernel$ and $\psi$ the feature map for $\Kernel$.
\end{fact}
There are three main properties of the kernel method. First, although the associated feature map $\psi$ may correspond to a vector in an infinite-dimensional space, the kernel $\Kernel(\x,\x')$ may still be efficiently evaluated, due to its analytic expression in terms of $\x$, $\x'$. Second, the function class $\gF_\Kernel = \{\x\mapsto \inprod{\vv}{\psi(\x)}: \vv\in\sH, \inprod{\vv}{\vv} \le B\}$ has Rademacher complexity independent from the dimension of $\sH$, as long as the maximum value of $\Kernel(\x,\x)$ for $\x$ in the domain is bounded (Thm. 6.12 in \cite{mohri2018foundations}). Third, the time complexity of finding the function in $\gF_\Kernel$ that best fits a dataset is actually polynomial to the size of the dataset, due to the representer theorem (Thm. 6.11 in \cite{mohri2018foundations}). Taken together, these properties constitute the basis of the kernel method, implying learners with runtime independent from the effective dimension of the learning problem. 

In order to apply the kernel method to learn some function class $\gF$, it suffices to show that the class $\gF$ can be represented sufficiently well by the class $\gF_\Kernel$. We give the following definition.
\begin{definition}[Approximate Representation]\label{definition:approx-representation}
    Let $\gF$ be a function class over $\R^d$, $\Kernel:\R^d \times \R^d \to \R$ a PDS kernel, where $\sH$ is the corresponding RKHS and $\psi$ the feature map for $\Kernel$. We say that $\gF$ can be $(\epsilon, B)$-approximately represented within radius $R$ with respect to $\Kernel$ if for any $f\in \gF$, there is $\vv\in \sH$ with $\inprod{\vv}{\vv} \le B$ such that $|f(\x) - \inprod{\vv}{\psi(\x)}| \le \epsilon$, for all $\x\in \R^d:\|\x\|_2 \le R$.
\end{definition}
For the purposes of TDS learning, we will also require the training marginal to have be hypercontractive with respect to the kernel at hand. This is important to ensure that our test will accept whenever there is no distribution shift. More formally, we require the following.
\begin{definition}[Hypercontractivity]\label{definition:hypercontractivity}
    Let $\Dtrainx$ be some distribution over $\R^d$, let $\sH$ be a Hilbert space and let $\psi:\R^d\to \sH$.
    We say that $\Dtrainx$ is $(\psi,C,\ell)$-hypercontractive if for any $t\in\sN$ and $\vv\in\sH$:
    \[
        \E_{\x\sim\Dtrainx}[\inprod{\vv}{\psi(\x)}^{2t}] \le (Ct)^{2\ell t} (\E_{\x\sim\Dtrainx}[\inprod{\vv}{\psi(\x)}^2])^t
    \]
    If $\Kernel$ is the PDS kernel corresponding to $\psi$, we also say that $\Dtrainx$ is $(\Kernel,C,\ell)$-hypercontractive. 
\end{definition}

\subsection{TDS Regression via the Kernel Method}\label{section:tds-kernel}

We now give a general theorem on TDS regression for bounded distributions, under the following assumptions. Note that, although we assume that the training and test labels are bounded, this assumption can be relaxed in a black-box manner and bounding some constant-degree moment of the distribution of the labels suffices, as we show in \Cref{corollary:label-moment-bound-assumption-suffices}.

\begin{assumption}\label{assumption:bounded}
    For a function class $\gF\subseteq\{\R^d\to \R\}$, and training and test distributions $\Dtrain$, $\Dtest$ over $\R^d\times \R$, we assume the following.
    \begin{enumerate}
        \item $\gF$ is $(\eps,B)$-approximately represented within radius $R$ w.r.t. a PDS kernel $\Kernel:\R^d\times \R^d \to \R$, for some $\eps\in(0,1)$ and $B,R\ge 1$ and let $A = \sup_{\x:\|\x\|_2 \le R}\Kernel(\x,\x)$.
        \item The training marginal $\Dtrainx$ (1) is bounded within $\{\x: \|\x\|_2 \le R\}$ and (2) is $(\Kernel,C,\ell)$-hypercontractive for some $C,\ell \ge 1$.
        \item The training and test labels are both bounded in $[-M,M]$ for some $M\ge 1$. 
    \end{enumerate}
\end{assumption}

Consider the function class $\gF$, the kernel $\Kernel$ and the parameters $\eps, A,B,C,M,\ell$ as defined in the assumption above and let $\delta\in(0,1)$. Then, we obtain the following theorem.

\begin{theorem}[TDS Learning via the Kernel Method]\label{theorem:tds-via-kernels}
    Under \Cref{assumption:bounded}, \Cref{algorithm:tds-via-kernels} learns the class $\gF$ in the TDS regression setting up to excess error $5\epsilon$ and probability of failure $\delta$. The time complexity is $O(T) \cdot \poly(d,\frac{1}{\eps}, (\log(1/\delta))^\ell, A, B, C^\ell, 2^\ell, M)$, where $T$ is the evaluation time of $\Kernel$.
\end{theorem}

\begin{algorithm}[t]
\caption{TDS Regression via the Kernel Method}\label{algorithm:tds-via-kernels}
\KwIn{Parameters $M,R,B,A,C,\ell \ge 1$, $\eps,\delta\in(0,1)$ and sample access to $\Dtrain$, $\Dtestx$}
\vspace{.25em}
%Let $c\ge 1$ be some sufficiently large universal constant \\
Set $m = c\frac{(ABM)^4}{\eps^4}\log(\frac{1}{\delta})$, $N = c m^2\frac{ABC}{\eps^4}(4C\log(\frac{4}{\delta}))^{4\ell+1}$, $c$ large enough constant\\ %, $\tilde{\eps} = \frac{\eps^2}{50 AB}$ \\
\vspace{.25em}
Draw $m$ i.i.d. labeled examples $\bar{S}_{\mathrm{ref}}$ from $\Dtrain$ and $m$ i.i.d. unlabeled examples $S_{\mathrm{ref}}'$ from $\Dtestx$; \\
\If{for some $\x\in S_{\mathrm{ref}}'$ we have $\|\x\|_2 > R$}{\textbf{Reject} and terminate;}
Let $\hat \va = (\hat a_{\vz})_{\vz\in S_{\mathrm{ref}}}$ be the optimal solution to the following convex program
\begin{align*}
    \min_{\va \in \R^m} &\sum_{(\x,y)\in \bar{S}_\mathrm{ref}}\Bigr(y-\sum_{\vz\in S_{\mathrm{ref}}} a_{\vz}\Kernel(\vz,\x)\Bigr)^2 \\
    \text{s.t. } & \sum_{\vz,\vw\in S_{\mathrm{ref}}} a_{\vz}a_{\vw} \Kernel(\vz,\vw) \le B, \text{ where }\va = (a_{\vz})_{\vz\in S_{\mathrm{ref}}}
\end{align*}
\\
\vspace{.25em}
Draw $N$ i.i.d. unlabeled examples ${S}_{\mathrm{ver}}$ from $\Dtrainx$ and $N$ unlabeled examples $S_{\mathrm{ver}}'$ from $\Dtestx$; \\
\If{for some $\x\in S_{\mathrm{ver}}'$ we have $\|\x\|_2 > R$}{\textbf{Reject} and terminate;}
Compute the matrix $\hat\Phi = (\hat\Phi_{\vz,\vw})_{\vz,\vw\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'}$ with $\hat\Phi_{\vz,\vw} = \frac{1}{N}\sum_{\x\in S_{\mathrm{ver}}}\Kernel(\x,\vz)\Kernel(\x,\vw)$;\\
Compute the matrix $\hat\Phi' = (\hat\Phi'_{\vz,\vw})_{\vz,\vw\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'}$ with $\hat\Phi'_{\vz,\vw} = \frac{1}{N}\sum_{\x\in S_{\mathrm{ver}}'}\Kernel(\x,\vz)\Kernel(\x,\vw)$;\\
Let $\rho$ be the value of the following eigenvalue problem
\begin{align*}
    \max_{\va \in \R^{2m}} & \va^\top\hat\Phi'\va \,\,\,\text{ s.t. } \va^\top\hat\Phi\va \le 1
\end{align*}
\If{$\rho > 1+\frac{\eps^2}{50 AB}$}{\textbf{Reject} and terminate;}
Otherwise, \textbf{accept} and output $h: \x \mapsto h(\x) = \clip_M(\hat p(\x))$, where $\hat p(\x) = \sum_{\vz\in S_{\mathrm{ref}}} \hat{a}_{\vz}\Kernel(\vz,\x)$;
\end{algorithm}%The algorithm runs in two phases. In the first phase, it draws a set of labeled reference examples from the training distribution and uses them to find a candidate output hypothesis, exploiting the kernel trick to obtain runtime savings. It also draws a number of unlabeled reference examples from the test marginal. In the second (testing) phase, the algorithm draws fresh sets of verification examples from both the training and the test marginals. For each of these sets, it forms a matrix, which also depends on the reference examples, but can be written as an average over the corresponding set of verification examples. The algorithm then ensures that the maximum eigenvalue of the verification matrix corresponding to test data is bounded by a multiple of the maximum eigenvalue of the verification matrix corresponding to training data. 
%\subsection{Overview of the Algorithm}\label{section:tds-kernel-overview}
%In this section we describe the main ideas of the algorithm and its analysis.
The main ideas of our proof are the following.

\noindent\textbf{Obtaining a concise reference feature map.} The algorithm first draws reference sets $S_{\mathrm{ref}}, S_{\mathrm{ref}}'$ from both the training and the test distributions. The representer theorem, combined with the approximate representation assumption (\Cref{definition:approx-representation}) ensure that the reference examples define a new feature map $\phi: \R^d \to \R^{2m}$ with $\phi(\x) = (\Kernel(\x,\vz))_{\vz\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'}$ such that the ground truth $f^* = \arg \min_{f\in\gF} [\gL_{\Dtrain}(f)+ \gL_{\Dtest}(f)]$ can be approximately represented as a linear combination of the features in $\phi$ with respect to both $S_{\mathrm{ref}}$ and $S_{\mathrm{ref}}'$, i.e., $\|f^*-({\va^*})^\top{\phi}\|_{S_{\mathrm{ref}}}$ and $\|f^*-({\va^*})^{\top}{\phi}\|_{S_{\mathrm{ref}}'}$ are both small for some $\va^*\in \R^{2m}$.  In particular, we have the following.
\begin{proposition}[Representer Theorem, modification of Theorem 6.11 in \cite{mohri2018foundations}]\label{proposition:representer-theorem}
    Suppose that a function $f:\R^d\to \R$ can be $(\eps,B)$-approximately represented within radius $R$ w.r.t. some PDS kernel $\Kernel$ (as per \Cref{definition:approx-representation}). Then, for any set of examples $S$ in $\{\x\in\R^d : \|\x\|_2\le R\}$, there is $\va = (a_{\x})_{\x\in S} \in \R^{|S|}$ such that for $\tilde{p}(\x) = \sum_{\vz\in S}a_{\vz} \Kernel(\vz,\x)$ we have:
    \[ \|f-\tilde{p}\|_S \le \epsilon \text{ and } \sum_{\x,\vz\in S}a_{\x}a_{\vz} \Kernel(\vz,\x) \le B\]
\end{proposition}
\begin{proof}
    We first observe that there is some $\vv\in\sH$ such that $\inprod{\vv}{\vv}\le B$ and for $p(\x) = \inprod{\vv}{\psi(\x)}$ we have $\|f-p\|_S \le \epsilon$, because by \Cref{definition:approx-representation}, there is a pointwise approximator for $f$ with respect to $\Kernel$. By Theorem 6.11 in \cite{mohri2018foundations}, this implies the existence of $\tilde{p}$ as desired.
\end{proof}
Note that since the evaluation of $\phi(\x)$ only involves Kernel evaluations, we never need to compute the initial feature expansion $\psi(\x)$ which could be overly expensive.

\noindent\textbf{Forming a candidate output hypothesis.} We know that the reference feature map approximately represents the ground truth. However, having no access to test labels, we cannot directly hope to find the corresponding coefficient $\va^*\in\R^{2m}$. Instead, we use only the training reference examples to find a candidate hypothesis $\hat p$ with close-to-optimal performance on the training distribution which can be also expressed in terms of the reference feature map $\phi$, as $\hat p = \hat\va^\top \phi$. It then suffices to test the quality of $\phi$ on the test distribution.
\newpage
\noindent\textbf{Testing the quality of reference feature map on the test distribution.} We know that the function $\tilde{p}^* = (\va^*)^\top\phi$ performs well on the test distribution (since it is close to $f^*$ on a reference test set). We also know that the candidate output $\hat\va^\top \phi$ performs well on the training distribution. Therefore, in order to ensure that $\hat p$ performs well on the test distribution, it suffices to show that the distance between $\hat p$ and $\tilde{p}^*$ under the test distribution, i.e., $\|\hat\va^\top \phi -(\va^*)^\top\phi\|_{\Dtestx}$, is small. In fact, it suffices to bound this distance by the corresponding one under the training distribution, because $\hat p$ fits the training data well and $\|\hat\va^\top \phi -(\va^*)^\top\phi\|_{\Dtrainx}$ is indeed small. Since we do not know $\va^*$, we need to run a test on $\phi$ that certifies the desired bound for any $\va^*$. 


\noindent\textbf{Using the spectral tester.}
We observe that $\|\hat\va^\top \phi -(\va^*)^\top\phi\|_{\Dtrainx}^2 = (\hat\va - \va^*)^\top \Phi (\hat\va - \va^*)$, where $\Phi = \E_{\x\sim\Dtrainx}[\phi(\x)\phi(\x)^\top]$ and similarly $\|\hat\va^\top \phi -(\va^*)^\top\phi\|_{\Dtestx}^2 = (\hat\va - \va^*)^\top \Phi' (\hat\va - \va^*)$. Since we want to obtain a bound for all $\va^*$, we essentially want to ensure that for any $\va\in\R^{2m}$ we have $\va^\top \Phi' \va \le (1+\rho)\va^\top \Phi \va$ for some small $\rho$. Having a multiplicative bound is important because we do not have any bound on the norm of $\|\hat\va-\va^*\|_2$. 

To implement the test, and since we cannot test $\Phi$ and $\Phi'$ directly, we draw fresh verification examples $S_{\mathrm{ver}}, S'_{\mathrm{ver}}$ from $\Dtrainx$ and $\Dtestx$ and run a spectral test on the corresponding empirical versions $\hat\Phi,\hat\Phi'$ of the matrices $\Phi,\Phi'$. To ensure that the test will accept when there is no distribution shift, we use the following lemma (originally from \cite{goel2024tolerant}) on multiplicative spectral concentration for $\hat\Phi$, where the hypercontractivity assumption (\Cref{definition:hypercontractivity}) is important.

\begin{lemma}[Multiplicative Spectral Concentration, Lemma B.1 in \cite{goel2024tolerant}, modified]\label{lemma:relative-error-kernel-matrix}
    Let $\Dtrainx$ be a distribution over $\R^d$ and $\phi:\R^d \to \R^m$ such that $\Dtrainx$ is $(\phi,C,\ell)$-hypercontractive for some $C,\ell \ge 1$. Suppose that $S$ consists of $N$ i.i.d. examples from $\Dtrainx$ and let $\Phi = \E_{\x\sim\Dtrainx}[\phi(\x)\phi(\x)^\top]$, and $\hat\Phi = \frac{1}{N}\sum_{\x\in S}\phi(\x)\phi(\x)^\top$. For any $\eps,\delta\in(0,1)$, if $N\ge \frac{64 Cm^2}{\eps^2}(4C \log_2(\frac{4}{\delta}))^{4\ell+1}$, then with probability at least $1-\delta$, we have that
    \[
        \text{For any }\va\in\R^m: \va^\top \hat\Phi \va \in [{(1-\eps)} \va^\top \Phi\va, (1+\eps)\va^\top \Phi\va]
    \]
\end{lemma}
Note that the multiplicative spectral concentration lemma requires access to independent samples. However, the reference feature map $\phi$ depends on the reference examples $S_{\mathrm{ref}}, S_{\mathrm{ref}}'$. This is the reason why we do not reuse $S_{\mathrm{ref}}, S_{\mathrm{ref}}'$, but rather draw fresh verification examples. For the proof of \Cref{lemma:relative-error-kernel-matrix}, see \Cref{appendix:tds-kernels}.

We now provide the full formal proof of \Cref{theorem:tds-via-kernels}. The full proof involves appropriate uniform convergence bounds for kernel hypotheses, which are important in order to shift from the reference to the verification examples and back. %In high level, the reference examples are the ones we either know bounds or want to show bounds for, and the verification examples are the ones that we can run our tests on.

%\subsection{Full Proof of Theorem \ref{theorem:tds-via-kernels}}\label{section:tds-kernel-proof}


\begin{proof}[Proof of \Cref{theorem:tds-via-kernels}]
    Consider the reference feature map $\phi: \R^d \to \R^{2m}$ with $\phi(\x) = (\Kernel(\x,\vz))_{\vz\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'}$. Let $f^*= \arg \min_{f\in\gF} [\gL_{\Dtrain}(f)+ \gL_{\Dtest}(f)]$ and $f_\opt = \arg \min_{f\in\gF} [\gL_{\Dtrain}(f)]$. By \Cref{assumption:bounded}, we know that there are functions $p^*, p_\opt :\R^d \to \R$ with $p^*(\x) = \inprod{\vv^*}{\psi(\x)}$ and $p_\opt = \inprod{\vv_\opt}{\psi(\x)}$, that uniformly approximate $f^*$ and $f_\opt$ within the ball of radius $R$, i.e., $\sup_{\x:\|\x\|_2\le R}|f^*(\x) - p^*(\x)| \le \eps$ and $\sup_{\x:\|\x\|_2\le R}|f_\opt(\x) - p_\opt(\x)| \le \eps$. Moreover, $\inprod{\vv^*}{\vv^*}, \inprod{\vv_\opt}{\vv_\opt} \le B$.

    By \Cref{proposition:representer-theorem}, there is $\va^*\in \R^{2m}$ such that for $\tilde{p}^*:\R^d\to \R$ with $\tilde{p}^*(\x) = ({\va^*})^\top{\phi(\x)}$ we have $\|f^*-\tilde{p}^*\|_{S_{\mathrm{ref}}} \le 3\eps/2$ and $\|f^*-\tilde{p}^*\|_{S_{\mathrm{ref}}'} \le 3\eps/2$. Let $\mK$ be a matrix in $\R^{2m\times 2m}$ such that $\mK_{\vz,\vw} = \Kernel(\vz,\vw)$ for $\vz,\vw\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'$. We additionally have that $(\va^*)^\top \mK \va^* \le B$. Therefore, for any $\x\in\R^d$ we have 
    \begin{align*}
        (\tilde{p}^*(\x))^2 &= \Bigr(\Bigr\langle{\sum_{\vz\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'} a^*_z \psi(\vz) }, {\psi(\x)}\Bigr\rangle\Bigr)^2 \\
        &\le \Bigr\langle{\sum_{\vz\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'} a^*_z \psi(\vz) }, {\sum_{\vz\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'} a^*_z \psi(\vz)}\Bigr\rangle \cdot \inprod{\psi(\x)}{\psi(\x)} \\
        &= (\va^*)^\top \mK \va^* \cdot \Kernel(\x,\x) \le B\cdot \Kernel(\x,\x)\,,
    \end{align*}
    where we used the Cauchy-Schwarz inequality. For $\x$ with $\|\x\|_2\le R$, we, hence, have $(\tilde{p}^*(\x))^2 \le AB$ (recall that $A = \max_{\|\x\|_2\le R}\Kernel(\x,\x)$).

    Similarly, by applying the representer theorem (Theorem 6.11 in \cite{mohri2018foundations}) for $p_\opt$, we have that there exists $\va^\opt = (a^\opt_{\vz})_{\vz\in S_{\mathrm{ref}}}\in\R^{m}$ such that for $\tilde{p}_\opt:\R^d \to \R$ with $\tilde{p}_\opt(\x) = \sum_{\vz\in S_{\mathrm{ref}}} a^\opt_\vz \Kernel(\vz,\x)$ we have $\gL_{\bar{S}_{\mathrm{ref}}}(\tilde{p}_\opt) \le \gL_{\bar{S}_{\mathrm{ref}}}(p_\opt)$ and $\sum_{\vz,\vw\in S_{\mathrm{ref}}} a^\opt_\vz a^\opt_\vw \Kernel(\vz,\vw) \le B$. Since $\hat p$ in \Cref{algorithm:tds-via-kernels} is formed by solving a convex program whose search space includes $\tilde{p}_\opt$, we have
    \begin{equation}\label{equation:phat-optimality}
        \gL_{\bar{S}_{\mathrm{ref}}}(\hat{p}) \le \gL_{\bar{S}_{\mathrm{ref}}}(\tilde{p}_\opt) \le \gL_{\bar{S}_{\mathrm{ref}}}(p_\opt)
    \end{equation}
    In the following, we abuse the notation and consider $\hat\va$ to be a vector in $\R^{2m}$, by appending $m$ zeroes, one for each of the elements of $S'_{\mathrm{ref}}$. Note that we then have $\hat\va^\top \mK \hat\va \le B$, and, also, $(\hat p(\x))^2 \le A\cdot B$ for all $\x$ with $\|\x\|_2\le R$.

    \noindent\textbf{Soundness.} Suppose first that the algorithm has accepted. In what follows, we will use the triangle inequality of the norms to bound for functions $h_1,h_2,h_3$ the quantity $\|h_1 - h_2\|_{\Dtrain}$ by $\|h_1-h_3\|_{\Dtrain}+ \|h_2-h_3\|_\Dtrain$. We also use the inequality $\gL_\Dtrain(h_1) \le \gL_\Dtrain(h_2) + \|h_1-h_2\|_{\Dtrain}$, as well as the fact that $\|\clip_M \circ h_1 - \clip_M\circ h_2\|_\Dtrain \le \|\clip_M \circ h_1 -  h_2\|_\Dtrain \le \| h_1 - h_2\|_\Dtrain$. We bound the test error of the output hypothesis $h:\R^d \to [-M,M]$ of \Cref{algorithm:tds-via-kernels} as follows.
    \begin{align*}
        \gL_{\Dtest}(h) \le \| h - \clip_M\circ f^* \|_{\Dtestx} + \gL_\Dtest(f^*)
    \end{align*}
    Since $(h(\x) - \clip_M(f^*(\x)))^2 \le 4M^2$ for all $\x$ and the hypothesis $h$ does not depend on the set $S_{\mathrm{ref}}'$, by a Hoeffding bound and the fact that $m$ is large enough, we obtain that $\| h - \clip_M\circ f^* \|_{\Dtestx} \le \| h - \clip_M\circ f^* \|_{S_{\mathrm{ref}}'} + \eps/10$, with probability at least $1-\delta/10$.
    Moreover, we have $\| h - \clip_M\circ f^* \|_{S_{\mathrm{ref}}'} \le \| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'} + \| \tilde{p}^* -  f^* \|_{S_{\mathrm{ref}}'}$. We have already argued that $\| \tilde{p}^* -  f^* \|_{S_{\mathrm{ref}}'} \le 3\eps/2$.

    In order to bound the quantity $\| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'}$, we observe that while the function $h$ does not depend on $S_{\mathrm{ref}}'$, the function $\tilde{p}^*$ does depend on $S_{\mathrm{ref}}'$ and, therefore, standard concentration arguments fail to bound the $\| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'}$ in terms of $\| h - \clip_M\circ \tilde{p}^* \|_{\Dtestx}$. However, since we have clipped $\tilde{p}^*$, and $\tilde{p}^*$ is of the form $\inprod{\vv^*}{\psi}$, we may obtain a bound using standard results from generalization theory (i.e., bounds on the Rademacher complexity of kernel-based hypotheses like Theorem 6.12 in \cite{mohri2018foundations} and uniform convergence bounds for classes with bounded Rademacher complexity under Lipschitz and bounded losses like Theorem 11.3 in \cite{mohri2018foundations}). In particular, we have that with probability at least $1-\delta/10$
    \[
        \| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'} \le \| h - \clip_M\circ \tilde{p}^* \|_{\Dtestx} + \eps/10
    \]
    The corresponding requirement for $m = |S_{\mathrm{ref}}'|$ is determined by the bounds on the Lipschitz constant of the loss function $(y,t)\mapsto (y-\clip_M(t))^2$, with $y\in [-M,M]$ and $t\in\R$, which is at most $5M$, the overall bound on this loss function, which is at most $4M^2$, as well as the bounds $A = \max_{\x:\|\x\|_2\le R}\Kernel(\x,\x)$ and $(\va^*)^\top \mK \va \le B$ (which give bounds on the Rademacher complexity).

    By applying the Hoeffding bound, we are able to further bound the quantity $\| h - \clip_M\circ \tilde{p}^* \|_{\Dtestx}$ by $ \| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ver}}'} + \eps/10$, with probability at least $1-\delta$. We have effectively managed to bound the quantity $\| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'}$ by $\| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ver}}'}+\eps/5$. This is important, because the set $S_{\mathrm{ver}}'$ is a fresh set of examples and, therefore, independent from $\tilde{p}$. Our goal is now to use the fact that our spectral tester has accepted. We have the following for the matrix $\hat\Phi' = (\hat\Phi'_{\vz,\vw})_{\vz,\vw\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'}$ with $\hat\Phi'_{\vz,\vw} = \frac{1}{N}\sum_{\x\in S_{\mathrm{ver}}'}\Kernel(\x,\vz)\Kernel(\x,\vw)$.
    \begin{align*}
        \| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ver}}'}^2 
        &\le \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ver}}'}^2 \\
        &= (\hat\va - \va^*)^\top \hat\Phi' (\hat\va-\va^*)
    \end{align*}
    Since our test has accepted, we know that $(\hat\va - \va^*)^\top \hat\Phi' (\hat\va-\va^*) \le (1+\rho)(\hat\va - \va^*)^\top \hat\Phi (\hat\va-\va^*)$, for the matrix $\hat\Phi = (\hat\Phi_{\vz,\vw})_{\vz,\vw\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}}$ with $\hat\Phi_{\vz,\vw} = \frac{1}{N}\sum_{\x\in S_{\mathrm{ver}}}\Kernel(\x,\vz)\Kernel(\x,\vw)$. We note here that having a multiplicative bound of this form is important, because we do not have any upper bound on the norms of $\hat\va$ and $\va^*$. Instead, we only have bounds on distorted versions of these vectors, e.g., on $\hat\va^\top \mK \hat\va$, which does not imply any bound on the norm of $\hat\va$, because $\mK$ could have very small singular values.

    Overall, we have that 
    \begin{align*} \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ver}}'} - \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ver}}}
    &\le \sqrt{\rho (2\|\hat p\|_{S_{\mathrm{ver}}}^2+2\|\tilde{p}^*\|_{S_{\mathrm{ver}}}^2)} \\
    &\le \sqrt{4AB\rho}
    \le \frac{3\eps}{10}.
    \end{align*}

    By using results from generalization theory once more, we obtain that with probability at least $1-\delta/5$ we have $\| \hat p - \tilde{p}^* \|_{S_{\mathrm{ver}}} \le \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ref}}}+\eps/5$. This step is important, because the only fact we know about the quality of $\hat{p}$ is that it outperforms every polynomial on the sample $S_{\mathrm{ref}}$ (not necessarily over the entire training distribution). We once more may use bounds on the values of $\hat p$ and $\tilde{p}^*$, this time without requiring clipping, since we know that the training marginal is bounded and, hence, the values of $\hat p$ and $\tilde{p}^*$ are bounded as well. This was not true for the test distribution, since we did not make any assumptions about it.

    In order to bound $\| \hat p - \tilde{p}^* \|_{S_{\mathrm{ref}}}$, we have the following.
    \begin{align*}
        \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ref}}} &\le \gL_{\bar{S}_{\mathrm{ref}}}(\hat p) + \gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*) + \|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}} \\
        &\le \gL_{\bar{S}_{\mathrm{ref}}}(\tilde{p}_\opt) + \gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*) + \|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}} \tag{By \eqref{equation:phat-optimality}} \\
        &\le \gL_{\bar{S}_{\mathrm{ref}}}({p}_\opt) + \gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*) + \|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}} 
    \end{align*}
    The first term above is bounded as $\gL_{\bar{S}_{\mathrm{ref}}}({p}_\opt) \le \gL_{\bar{S}_{\mathrm{ref}}}(\clip_M\circ {f}_\opt)+\|p_\opt - f_\opt\|_{{S}_{\mathrm{ref}}}$, where the second term is at most $\eps$ (by the definition of $p_\opt$) and the first term can be bounded by $\gL_{\Dtrain}({f}_\opt)+\eps/10 = \opt+\eps/10$, with probability at least $1-\delta/10$, due to an application of the Hoeffding bound.

    For the term $\gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*)$ we can similarly use the Hoeffding bound to obtain, with probability at least $1-\delta/10$ that $\gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*) \le \gL_{\Dtrain}(f^*)+\eps/10$.

    Finally, for the term $\|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}}$, we have that $\|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}} \le 3\eps/2$, as argued above.

    Overall, we obtain a bound of the form $\gL_\Dtest(h) \le \gL_{\Dtrain}(f^*)=\gL_{\Dtest}(f^*) + \gL_{\Dtrain}(f_\opt) + 5\eps$, with probability at least $1-\delta$, as desired.
    
    \noindent\textbf{Completeness.} For the completeness criterion, we assume that the test marginal is equal to the training marginal. Then, by \Cref{lemma:relative-error-kernel-matrix} (where we observe that any $(\psi,C,\ell)$-hypercontractive distribution is also $(\phi,C,\ell)$-hypercontractive), with probability at least $1-\delta$, we have that for all $\va\in\R^{2m}$, $\va^\top \hat\Phi' \va \le \frac{1+(\rho/4)}{1-(\rho/4)}\va^\top \hat\Phi \va \le (1+\rho)\va^\top \hat\Phi \va$, because $\E[\hat\Phi] = \E[\hat\Phi']$ and the matrices are sums of independent samples of $\phi(\x)\phi(\x)^\top$, where $\x\sim\Dtrainx$. It is crucial here that $\phi$ (which recall is formed by using $S_{\mathrm{ref}}, S_{\mathrm{ref}}'$) does not depend on the verification samples $S_{\mathrm{ver}}$ and $S'_{\mathrm{ver}}$, which is why we chose them to be fresh. Therefore, the test will accept with probability at least $1-\delta$.

    \noindent\textbf{Efficient Implementation.} To compute $\hat \va$, we may run a least squares program, in time polynomial in $m$. For the spectral tester, we first compute the SVD of $\hat\Phi$ and check that any vector in the kernel of $\hat\Phi$ is also in the kernel of $\hat\Phi'$ (this can be checked without computing the SVD of $\hat\Phi'$). Otherwise, reject. Then, let $\hat\Phi^{\frac{\dagger}{2}}$ be the root of the Moore-Penrose pseudoinverse of $\hat\Phi$ and find the maximum singular value of the matrix $\hat\Phi^{\frac{\dagger}{2}}\hat\Phi'\hat\Phi^{\frac{\dagger}{2}}$. If the value is higher than $1+\rho$, reject. Note that this is equivalent to solving the eigenvalue problem described in \Cref{algorithm:tds-via-kernels}.
\end{proof}


\subsection{Applications}

Having obtained a general theorem for TDS learning under \Cref{assumption:bounded}, we can now instantiate it to obtain TDS learning algorithms for learning neural networks with Lipschitz activations. In particular, we recover all of the bounds of \cite{reliable_goel2017}, using the additional assumption that the training distribution is hypercontractive in the following standard sense. 

\begin{definition}[Hypercontractivity]\label{definition:hypercontractivity-standard}
    We say that $\Dtrain$ is $C$-hypercontractive if for all polynomials of degree $\ell$ and $t\in \mathbb{N}$, we have that 
     \[
    \E_{\x\sim \Dtrain}\left[p(\x)^{2t}\right]\leq (Ct)^{2\ell t}\left(\E_{\x\sim \Dtrain}\left[p(\x)^2\right]\right)^{t}.
    \]
    
\end{definition}

Note that many common distributions like log-concave or the uniform over the hypercube are known to be hypercontractive for some constant $C$ (see \cite{carbery2001distributional} and \cite{o2014analysis}).


% \begin{lemma}
% \label{lem:kernel_for_TDS_main}    
% Table \ref{table:kernel_properties} provides bounds on the parameters in \Cref{assumption:bounded} for specific classes of neural networks. 
\renewcommand{\arraystretch}{2}
\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Function Class &  Degree ($\ell$) &\begin{tabular}{c}\vspace{-0.3cm}Representation \\Bound ($B$)\end{tabular} & \begin{tabular}{c}\vspace{-0.3cm}Kernel \\ Bound ($A$)\end{tabular} \\
\hline
\hline
Sigmoid Nets &$O\left(RW^{t-2}(t\log(\frac{W}{\epsilon}))^{t-1}\log R\right)$& $ 2^{\ell}\cdot W^{ \tilde{O}(Wt \log(\frac{1}{\eps}))^{t-2}}$ & $(2R)^{2^t \ell}$ \\
\hline
$L$-Lipschitz Nets &$O\left((WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$ & $(k+\ell)^{O(\ell)}$& $R^{O(\ell)}$\\
\hline
\end{tabular}
\caption{We instantiate the parameters relevant to \Cref{assumption:bounded} for Sigmoid and Lipschitz Nets. We have: (1) $t$ denotes a bound on the depth of the network, (2) $W$ is a bound on the sum of network weights in all layers other than the first, (3) $(\eps,B)$ and radius $R$ are the approximate representation parameters, (4) $k$ is the number of hidden units in the first layer. The kernel function can be evaluated in time $\poly(d,\ell)$. For each of the classes, we assume that the maximum two norm of any row of the matrix corresponding to the weights of the first layer is bounded by $1$. The kernel we use is the composed multinomial kernel $\cmkl{t}$ with appropriately chosen degree vector $\vell$. Here, $\ell$ equals the product of the entries of $\vell$. Any $C$-hypercontractive distribution is also $(\cmkl{t},C,\ell)$ hypercontractive for $\ell$ as specified in the table. {For the case of $k=1$, the bound $B$ in the second row can be improved to $2^{O(\ell)}$.}}\label{table:kernel_properties}
\end{table}

% \end{lemma}

In \Cref{table:kernel_properties}, we provide bounds on the parameters in \Cref{assumption:bounded} for sigmoid networks and $L$-Lipschitz networks, whose proof we postpone to \cref{section:approximation_theory}
(see \Cref{thm:approx_lipschitz_nets,thm:approx_sigmoid_nets,lem:composed_multinomial_properties}). Combining bounds from \Cref{table:kernel_properties} with \Cref{theorem:tds-via-kernels}, we obtain the results of the middle column of \Cref{table:main-results}.






% From the above lemma, we obtain the following theorem regarding TDS learning under bounded and hypercontractive marginals. 



%O\left(W^{t-2}(t\log(W/\epsilon)^{t-2}\right)}




% All of our results use kernels of the following form.

% \begin{definition}[Composed multinomial kernel]
% \label{def:composed_multinomial_kernel-main}
%     Consider the mapping $\psi_{\ell}:\R^{n}\to \R^{N_{\ell}}$, where $N_d=\sum_{i=1}^{\ell}d^{\ell}$ indexed by tuples $(i_1,i_2,\ldots, i_{j})\in [d]^j$ for $j\in [\ell]$ such that value of $\psi_{\ell}(\x)$ at index $(i_1,i_2,\ldots,i_{j})$ is equal to $\prod_{t=1}^{j}x_{i_t}$. The multinomial kernel $\mkl$ is defined as 
%     $
%     \mkl(\x,\vy)=\langle\psi_{\ell}(\x),\psi_{\ell}(\vy)\rangle=\sum_{i=1}^{d}(\x\cdot\vy)^{i}
%     $. We denote the corresponding RKHS as $\hkml$.
    
%     Let $\vell=(\ell_1,\ldots,\ell_t)$ be a tuple in $\mathbb{N}^{t}$. We denote a sequence of mappings $\psi^{(0)}_{\vell},\psi^{(1)}_{\vell},\ldots,\psi^{(t)}_{\vell}$ inductively as follows: (1) $\psi^{(0)}_{\vell}(\x)=\x$, (2) $\psi^{(i)}_{\vell}(\x)=\psi_{\ell_i}(\psi^{(i-1)}_{\vell}(\x))$.
%     Let $N_{\vell}^{(i)}$ denote the number of coordinates in $\cpsi{i}$.
%     This induces a sequence of kernels $\cmkl{0},\cmkl{1},\ldots,\cmkl{t}$ defined as 
%     $
%     \cmkl{i}(\x,\vy)=\langle\cpsi{i}(\x),\cpsi{i}(\vy)\rangle=\sum_{j=0}^{\ell_{i}}(\langle\cpsi{i-1}(\x),\cpsi{i-1}(\vy)\rangle^j)
%     $ and a corresponding sequence of RKHS denoted by $\sH_{\cmkl{0}},\sH_{\cmkl{1}},\ldots \sH_{\cmkl{t}}$.
% \end{definition}

\section{Unbounded Distributions}\label{section:unbounded}

We showed that the kernel method provides runtime improvements for TDS learning, because it can be used to obtain a concise reference feature map, whose spectral properties on the test distribution are all we need to check to certify low test error. A similar approach would not provide any runtime improvements for the case of unbounded distributions, because the dimension of the reference feature space would not be significantly smaller than the dimension of the multinomial feature expansion. Therefore, we can follow the standard moment-matching testing approach commonly used in TDS learning \cite{klivans2023testable} and testable agnostic learning \cite{rubinfeld2022testing,gollakota2022moment}.

\subsection{Additional Preliminaries}\label{section:unbounded-overview}
%In this section, we outline the main ideas of the algorithm for TDS learning in the unbounded distribution setting. Keep in mind that we intend to apply the algorithm to learning neural networks. Intuitively, 
%We will view a neural network as a function of $W\x$ after the first layer of weights has been applied, which allows us to project from the higher $d$-dimensional input space to a $k$-dimensional subspace. This motivates the following definition.

We define the notion of subspace juntas, namely, functions that only depend on a low-dimensional projection of their input vector.

\begin{definition}[Subspace Junta]
    A function $f: \R^d \rightarrow \R$ is a $k$-subspace junta (where $k \le d$) if there exists $W \in \R^{k \times d}$ with $\|W\|_2 = 1$ and $WW^{\top} = I_k$ and a function $g: \R^k \rightarrow \R$ such that
    \[ f(\x) = f_W(\x) = g(W\x) \quad \forall \x \in \R^d. \]
    Note that by taking $k = d$, letting $W = I_d$ covers all functions $f: \R^d \rightarrow \R$.
\end{definition}

Note that neural networks are $k$-subspace juntas, where $k$ is the number of neurons in the first hidden layer. We also define the following notion of a uniform polynomial approximation within a ball of a certain radius.

\begin{definition}[$(\epsilon, R)$-Uniform Approximation]
    For $\epsilon > 0, R \ge 1,$ and $g: \R^k \rightarrow \R$, we say that $q: \R^k \rightarrow \R$ is an $(\epsilon, R)$-uniform approximation polynomial for $g$ if
        \[|q(\x) - g(\x)| \le \epsilon \quad \forall \Norm{x}_2 \le R.\]
\end{definition}

We obtain the following corollary which gives the analogous bound on the $(\epsilon, R)$-uniform approximation to a $k$-subspace junta, given the $(\epsilon, R)$-uniform approximation to the corresponding function $g$.
\begin{corollary}
    Let $\epsilon > 0, R \ge 1$, and $f: \R^d \rightarrow \R$ be a $k$-subspace junta, and consider the corresponding function $g(Wx)$. Let $q: \R^k \rightarrow \R$ be an $(\epsilon, R)$-uniform approximation polynomial for $g$, and define $p: \R^d \rightarrow \R$ as $p(\x) := q(Wx)$. Then $|p(\x) - f(\x)| \le \epsilon$ for all $\|Wx\|_2 \le R$.
\end{corollary}

Finally, we consider any distribution with strictly subexponential tails in every direction, which we define as follows.

\begin{definition}[Strictly Sub-exponential Distribution]
A distribution $\gD$ on $\R^d$ is $\gamma$-strictly subexponential if there exist constants $C, \gamma \in (0, 1]$ such that for all $\vw \in \R^d, \Norm{\vw} = 1, t \ge 0$,
\[
\Pr_{\x \sim \gD}[|\inprod{\vw}{\x}| > t] \le e^{-Ct^{1+\gamma}}.
\] 
\end{definition}

\subsection{TDS Regression via Uniform Approximation}\label{section:tds-via-uniform}

We will now present our main results on TDS regression for unbounded training marginals. We require the following assumptions.

\begin{assumption}\label{assumption:unbounded-main}
    For a function class $\gF\subseteq\{\R^d\to \R\}$ consisting of $k$-subspaces juntas, and training and test distributions $\Dtrain, \Dtest$ over $\R^d \times \R$, we assuming the following.
    \begin{enumerate}
        \item For $f \in \gF$, there exists an $(\epsilon, R)$-uniform approximation polynomial for $f$ with degree at most $\degbound = R\log R \cdot g_\gF(\epsilon)$, where $g_\gF(\epsilon)$ is a function depending only on the class $\gF$ and $\epsilon$.
        \item For $f \in \gF$, the value $r_f := \sup_{\Norm{W\x}_2 \le R}|f(x)|$ is bounded by a constant $r > 0$.
        \item The training marginal $\Dtrainx$ is a $\gamma$-strictly subexponential distribution for $\gamma \in (0,1]$.
        \item The training and test labels are both bounded in $[-M, M]$ for some $M \ge 1$.
    \end{enumerate}
\end{assumption}

% \begin{assumption}\label{assumption:unbounded-main}
%     For a function class $\gF\subseteq\{\R^d\to \R\}$, and training and test distributions $\Dtrain$, $\Dtest$ over $\R^d\times \R$, we assume the following.
%     \begin{enumerate}
%         \item For any $f\in\gF$, there is $W \in \R^{k \times d}$ with $\|W\|_2 = 1$ and $WW^{\top} = I_k$ and a function $g: \R^k \rightarrow \R$ such that $f(\x) = g(W\x)$ for all $\x \in \R^d.$ Moreover, $f(0) = O(1)$.
%         \item For any $f\in\gF$, with $f(\x) = g(W\x)$, there is polynomial $q$ over $\R^k$ of degree at most $\ell$ s.t. for any $\x\in\R^d$ with $\|\x\|_2\le R$ we have $|q(W\x)-g(W\x)| \le \eps$, where $R\ge 1$, $\eps\in(0,1)$. We also require that $\ell \le \tilde{O}_{\gF,\eps} (R)$, where $\tilde{O}_{\gF,\eps}$ is hiding factors that are at most logarithmic in $R$, but can also depend on $\eps,\gF$.
%         \item The training marginal $\Dtrainx$ is $\gamma$-strictly subexponential for $\gamma\in(0,1]$.
%         \item The training and test labels are both bounded in $[-M,M]$ for some $M\ge 1$. 
%     \end{enumerate}
% \end{assumption}

Consider the function class $\gF$, and the parameters $\eps,\gamma,M,k,\ell$ as defined in the assumption above and let $\delta\in(0,1)$. Then, we obtain the following theorem.

\begin{theorem}[TDS Learning via Uniform Approximation]\label{theorem:tds-via-uniform}
    Under \Cref{assumption:unbounded-main}, \Cref{algorithm:uniform-approx} learns the class $\gF$ in the TDS regression setting up to excess error $5\epsilon$ and probability of failure $\delta$. The time complexity is $\poly(d^{s},{1}/{\epsilon},\log(1/\delta)^{\ell})$ where $s=(\ell\log(M/\epsilon))^{O({1}/{\gamma})}$.
\end{theorem}

\begin{algorithm}
\caption{TDS Regression via Uniform Approximation}\label{algorithm:uniform-approx}
\KwIn{Parameters $\eps>0, \delta\in(0,1)$, $R \ge 1$, $M \ge 1$, and sample access to $\Dtrain, \Dtestx$}
Set $\eps' = \epsilon/11$, $\delta' = \delta/4$, $\ell = R \log R \cdot g_\gF(\epsilon)$, $t = 2\log \Paren{\frac{2M}{\epsilon'}}$, $B = r(2(k + \ell))^{3\ell}$, $\mslack=\frac{\epsilon'^2}{4B^2 d^{2\ell t}}$ \\
Set $\mtrain = \mtest = \poly(M, \ln(1/\delta)^\ell, 1/\epsilon, d^\ell, r)$ and draw $\mtrain$ i.i.d. labeled examples $S$ from $\Dtrain$ and $\mtest$ i.i.d. unlabeled examples $S'$ from $\Dtestx$. \\
For each $\mindex \in \N^d$ with $\|\mindex\|_1 \le 2\max(\ell, t)$, compute the quantity 
$\momentempirical_\mindex = \E_{\x\sim S'} [\x^\mindex] = \E_{\x\sim S'} \Brack{\prod_{i\in[d]}x_i^{\mindex_i}}$ \\
\textbf{Reject} and terminate if $|\momentempirical_\mindex-\E_{\x\sim \Dtrainx}[\x^\mindex]|>\mslack$ for some $\mindex$ with $\|\mindex\|_1 \le 2\max(\ell, t)$. \\
\textbf{Otherwise}, solve the following least squares problem on $S$ up to error $\eps'$
\begin{align*}
    \min_p \quad &\E_{(\x,y)\sim S}\Brack{(y-p(\x))^2} \\
    \text{s.t. } &p \text{ is a polynomial with degree at most }\degbound \\
    &\text{each coefficient of }p \text{ is absolutely bounded by }\pbound
\end{align*}
\\
Let $\hat{p}$ be an $\eps'^2$-approximate solution to the above optimization problem. \\
\textbf{Accept} and output $\clip_M(\hat p(\x))$.
\end{algorithm}

Note that \Cref{assumption:unbounded-main} involves a low-degree uniform approximation assumption, which only holds within some bounded-radius ball. Since we work under unbounded distributions, we also need to handle the errors outside the ball. To this end, we use the following lemma, which follows from results in \cite{ben2018classical}.

\begin{lemma}\label{lemma:poly-bnd}
    Suppose $f=f_W$ and $q$ satisfy parts 1 and 2 of \Cref{assumption:unbounded-main}. Then
    \[|p(\x)| \le (k\ell)^{O(\ell)} \Norm{{W\x}}_2^\ell \text{, for all } \Norm{W\x}_2 \ge R.\]
\end{lemma}

The lemma above gives a bound on the values of a low-degree uniform approximator outside the interval of approximation. Therefore, we can hope to control the error of approximation outside the interval by taking advantage of the tails of our target distribution as well as picking $R$ sufficiently large. In order for the strictly subexponential tails to suffice, the quantitative dependence of $\ell$ on $R$ is important. This is why we assume (see \Cref{assumption:unbounded-main}) that $\ell = \tilde{O}(R)$. In particular, in order to bound the quantity $\E_{\x\sim\Dtrainx}[p^2(\x)\ind\{\|W\x\|_2\ge R\}]$, we use \Cref{lemma:poly-bnd}, the Cauchy-Schwarz inequality, and the bounds $\E_{\x\sim\Dtrainx}[\Norm{{W\x}}_2^{4\ell}] \le (k\ell)^{O(\ell)}$ and $\pr_{\x\sim\Dtrainx}[\|W\x\|_2\ge R] \le \exp(-\Omega(R/k)^{1+\gamma})$. Substituting for $\ell = \tilde{O}(R)$, we observe that the overall bound on the quantity $\E_{\x\sim\Dtrainx}[p^2(\x)\ind\{\|W\x\|_2\ge R\}]$ decays with $R$, whenever $\gamma$ is strictly positive. Therefore, the overall bound can be made arbitrarily small with an appropriate choice of $R$ (and therefore $\ell$). 

%\subsection{Full Proof of Central Theorem (Theorem \ref{theorem:tds-via-uniform})}\label{section:unbounded-proof}

Apart from the careful manipulations described above, the proof of \Cref{theorem:tds-via-uniform} follows the lines of the corresponding results for TDS learning through sandwiching polynomials \cite{klivans2023testable}. 

% \subsection{Central Theorem}
% We now present the assumptions that are required by our TDS learner under strictly sub-exponential distributions.
% \begin{assumption}\label{assumption:uniform_approx_assump}
%     For a function class $\gF\subseteq\{\R^d\to \R\}$ consisting of $k$-subspaces juntas, and training and test distributions $\Dtrain, \Dtest$ over $\R^d \times \R$, we assuming the following.
%     \begin{enumerate}
%         \item For $f \in \gF$, there exists an $(\epsilon, R)$-uniform approximation polynomial for $f$ with degree at most $\degbound = R\log R \cdot g_\gF(\epsilon)$, where $g_\gF(\epsilon)$ is a function depending only on the class $\gF$ and $\epsilon$.
%         \item For $f \in \gF$, the value $r_f := \sup_{\Norm{W\x}_2 \le R}|f(x)|$ is bounded by a constant $r > 0$.
%         \item The training marginal $\Dtrainx$ is a $\gamma$-strictly subexponential distribution.
%         \item The training and test labels are both bounded in $[-M, M]$ for some $M \ge 1$.
%     \end{enumerate}
% \end{assumption}

% Given this assumption, we now give the statement of the TDS learning algorithm.

% \begin{theorem}[TDS Learning via Uniform Approximation]\label{theorem:tds-via-approx}
%     Assume \cref{assumption:unbounded-main} holds. Let $\eps,\delta\in(0,1)$.
%     Then, algorithm (\Cref{algorithm:uniform-approx}) learns $\gF$ in the TDS regression setting up to excess error $4\epsilon$ and has probability of failure $\delta$. The time complexity is $\poly(d^s, \ln(1/\delta)^\ell, 1/\epsilon)$ where $s = \poly \Paren{\Paren{kg_\gF(\epsilon) \log(r) \log(M/\epsilon)}^{1+1/\gamma}}$ and TDS learns $\gF$ with respect to $\Dtrainx$ up to excess error $4\epsilon$ and with failure probability $\delta$.
% \end{theorem}



The following lemma allows us to relate the squared loss of the difference of polynomials under a set $S$ and under $\gD$, as long as we have a bound on the coefficients of the polynomials. This will be convenient in the analysis of the algorithm.
\begin{lemma}[Transfer Lemma for Square Loss, see \cite{klivans2023testable}]\label{transfer_lemma}
    Let $\gD$ be a distribution over $\R^d$ and $S$ be a set of points in $\R^d$. If $|\E_{\x \sim S}[\x^\alpha] - \E_{x \sim \gD}[\x^\alpha]| \le \mslack$ for all $\alpha \in \N^d$ with $\Norm{\alpha}_1 \le 2\ell$, then for any degree $\ell$ polynomials $p_1, p_2$ with coefficients absolutely bounded by $B$, it holds that
    \[
    \Abs{\E_{\x \sim S}[(p_1(\x) - p_2(\x))^2] - \E_{x \sim \gD}[(p_1(\x) - p_2(\x))^2]}\le 4B^2 d^{2\ell} \mslack
    \]
\end{lemma}
% \begin{proof}
%     The polynomial $(p_1 - p_2)$ has degree $\ell$ and coefficients bounded in absolute value by $2B$. Let $p' = (p_1 - p_2)^2 = \sum_{\Norm{\alpha}_1 \le 2\ell} p_{\alpha}' \x^{\alpha}$. By \cref{lemma:sum_coeff_bound}, $\sum_{\Norm{\alpha}_1 \le 2\ell} |p_{\alpha}'| \le 4B^2 d^{2\ell}$. Using the moment matching assumption,
%     \begin{align*}
%     \Abs{\E_{\x \sim S}[p'(\x)] - \E_{\x \sim \gD}[p'(\x)]} &= \Abs{\sum_{\Norm{\alpha}_1 \le 2\ell} p_\alpha' \Paren{\E_{\x \sim S}[x^\alpha] - \E_{x \sim \gD}[x^\alpha]}} \\
%     &\le \sum_{\Norm{\alpha}_1 \le 2\max(\ell, t)} |p_{\alpha}'| \mslack \\
%     &\le 4B^2 d^{2\ell} \mslack.
%     \end{align*}
% \end{proof}
We are now ready to prove \Cref{theorem:tds-via-uniform}.

\begin{proof}[Proof of \cref{theorem:tds-via-uniform}]
    We will prove soundness and completeness separately.
    
    \textbf{Soundness.} Suppose the algorithm accepts and outputs $\phM$. Let $f^*= \arg \min_{f\in\gF} [\gL_{\Dtrain}(f)+ \gL_{\Dtest}(f)]$ and $f_\opt = \arg \min_{f\in\gF} [\gL_{\Dtrain}(f)]$. By the uniform approximation assumption in \cref{assumption:unbounded-main}, there are polynomials $p^*, p_{\opt}$ which are $(\epsilon, R)$-uniform approximations for $f^*$ and $f_\opt$, respectively. Let $f^*$ and $f_\opt$ have the corresponding matrices $W^*, W_\opt \in \R^{k \times d}$, respectively. Denote $\lambda_{\mathrm{train}} = \gL_{\gD}(f^*)$ and $\lambda_{\mathrm{test}} = \gL_{\gD'}(f^*)$. Note that for any $f,g: \R^d \rightarrow \R$, ``unclipping" both functions will not increase their squared loss under any distribution, i.e. $\Norm{\clip_M(f) - \clip_M(g)}_{\gD} \le \Norm{f - g}_{\gD}$, which can be seen through casework on $\x$ and when $f(\x), g(\x)$ are in $[-M, M]$ or not. 
    Recalling that the training and test labels are bounded, we can use this fact as we bound the error of the hypothesis on $\Dtest$.
    \begin{align*}
        \gL_{\Dtest}(\phM) &\le \gL_{\Dtest}(\fsM) + \Norm{\fsM - \phM}_{\Dtest} \\
        &\le \gL_{\Dtest}(f^*) + \Norm{\fsM - \phM}_{S'} + \epsilon'.
    \end{align*}
    The second inequality follows from unclipping the first term and by applying Hoeffding's inequality, so that for $\mtest \ge \frac{8M^4 \ln (2/\delta')}{\epsilon'^4}$, the second term is bounded with probability $\ge 1 - \delta'$. Proceeding with more unclipping and using the triangle inequality:
    \begin{align}
        \gL_{\Dtest}(\phM) &\le \lambda_{\mathrm{test}} + \Norm{\fsM - \psM}_{S'} + \Norm{\psM - \phM}_{S'} + \epsilon' \notag \\
        &\le \lambda_{\mathrm{test}} + \Norm{\fsM - \psM}_{S'} + \Norm{p^{*} - \hat{p}}_{S'} + \epsilon'. \label{eqn: phM_loss}
    \end{align}

    We first bound $\Norm{\fsM - \psM}_{S'} = \sqrt{\E_{\x \sim S'}[(\fsMx - \psMx)^2]}$. Since $p^*(\x)$ is an $(\epsilon, R)$-uniform approximation to $f^*(\x)$, we separately consider when we fall in the region of good approximation ($\Norm{W^*\x} \le R$) or not. 
    \begin{align*}
        \E_{\x \sim S'}&[(\fsMx - \psMx)^2] \\
        &= \E_{\x \sim S'}[(\fsMx - \psMx)^2 \cdot \ind[\Norm{W^*\x} \le R] \\
        &+ \E_{\x \sim S'}[(\fsMx - \psMx)^2 \cdot \ind[\Norm{W^*\x} >R]] \\
        &\le \epsilon^2 + \E_{\x \sim S'}[2(\fsMx^2 + \psMx^2) \cdot \ind[\Norm{W^*\x} >R]]
    \end{align*}
    Then by applying Cauchy-Schwarz, (and similarly for $\psM$):
    \[\E_{\x \sim S'}[\fsMx^2 \cdot \ind[\Norm{W^*\x} >R]] \le \sqrt{\E_{\x \sim S'}[\fsMx^4]} \cdot \sqrt{\Pr_{\x \sim S'}[\Norm{W^*\x} >R]]}.
    \] 
    By definition, $\psM^2, \fsM^2 \le M^2$. So it suffices to bound $\Pr_{\x \sim S'}[\Norm{W^*\x} >R]]$, since we now have
    \begin{equation}
    \label{eqn: clip_1}
    \E_{\x \sim S'}[(\fsMx - \psMx)^2] \le \epsilon^2 + 4M^2 \sqrt{\Pr_{\x \sim S'}[\Norm{W^*\x} >R]]}.
    \end{equation}
    In order to bound this probability of the test samples falling outside the region of good approximation, we use the property that the first $2t$ moments of $S'$ are close to the moments of $\gD$ (as tested by the algorithm). Applying Markov's inequality, we have
    \[
    \Pr_{\x \sim S'}[\Norm{W^*\x} >R]] \le \frac{\E_{\x \sim S'}[\Norm{W^*\x}^{2t}]}{R^{2t}}.
    \]

    Write $\Norm{W^*\x}^{2t} = \Paren{\sum_{i=1}^k \inprod{W_i^*}{\x}^2}^t$, where $\sum_{i=1}^k \inprod{W_i^*}{\x}^2 = \sum_{i=1}^k \Paren{\sum_{j=1}^d W_{ij}^* x_j}^2$ is a degree $2$ polynomial with each coefficient bounded in absolute value by $2k$ (noting that since $WW^\top = 1$, then $|W_{ij}| \le 1$). Let $a_\alpha$ denote the coefficients of $\Norm{W^*\x}^{2t}$. Applying \Cref{lemma:sum_coeff_bound}, $\sum_{\Norm{\alpha}_1 \le 2t} |a_\alpha| \le (2k)^t d^{2t}\leq d^{O(t)}$. By linearity of expectation, we also have $|\E_{\x \sim S'}[\Norm{W^*\x}^{2t} - \E_{x \sim \gD}[\Norm{W^*\x}^{2t}]| \le \sum_{\Norm{\alpha}_1 \le 2t} |a_\alpha| \cdot \mslack\leq d^{O(t)}\cdot \mslack\leq \epsilon'$, where $\mslack\leq \epsilon'\cdot d^{-\Omega(t)}$.  Since $\gD$ is $ \gamma$-strictly subexponential, then by \cref{fact:moment_bound}, $\E_{x \sim \gD}[\inprod{W_i^*}{\x}^{2t}] \le (2C't)^{\frac{2t}{1+\gamma}}$. Then, we can bound the numerator $\E_{\x \sim S'}[\Norm{W^*\x}^{2t}] \le \E_{x \sim \gD}[\Norm{W^*\x}^{2t}] + \epsilon' \le (Ckt)^{\frac{2t}{1+\gamma}}$ for some large constant $C$. So we have that 
    \[
    \Pr_{\x \sim S'}[\Norm{W^*\x} >R]] \le \frac{(Ckt)^{\frac{2t}{1+\gamma}}}{R^{2t}}.
    \] Setting $t\geq C'(\log(M/\epsilon))$ and $R \ge C'(kt)\geq C'k\log(M/\epsilon)$ for large enough $C'$ makes the above probability at most $16\epsilon'^4/M^4$ so that $4M^2 \sqrt{\Pr_{\x \sim S'}[\Norm{W^*\x} >R]]} \le \epsilon'^2$. Thus, from \Cref{eqn: clip_1}, we have that
    \begin{equation} \label{eqn: fsM_psM}
    \Norm{\fsM - \psM}_{S'} \le \epsilon + \epsilon'.
    \end{equation}
    We now bound the second term $\Norm{\psM - \phM}_{S'}$. By \cref{lemma:moment-concentration}, the first $2\ell$ moments of $S$ will concentrate around those of $\Dtrainx$ whenever $\mtrain \ge \frac{1}{\Delta^2}{(Cc)^{4\ell} \ell^{8\ell +1} (\log(20d/\delta))^{4\ell+1}}$, and similarly the first $2\ell$ moments of $S'$ match with $\Dtrainx$ because the algorithm accepted. Using the transfer lemma (\Cref{transfer_lemma}) when considering $p' = (p^* - \hat{p})^2$, along with the triangle inequality, we get:
    \begin{align*}
        \Norm{p^*(\x) - \hat{p}(\x)}_{S'} &\le \Norm{p^*(\x) - \hat{p}(\x)}_{\gD} + \sqrt{4B^2d^{2\ell}\mslack} \\
        &\le \Norm{p^*(\x) - \hat{p}(\x)}_S + 2\epsilon' \\
        &\le \gL_S({p^*}) + \gL_S(\hat{p}) + 2\epsilon',
    \end{align*}
    where we note that we can bound $B$, the sum of the magnitudes of the coefficients, by $r(2(k+\ell))^{3\ell}$ using \cref{lemma:ball_coeff_bounds}.
    Recall that by definition $\hat{p}$ is an $\epsilon'^2$-approximate solution to the optimization problem in \cref{algorithm:uniform-approx}, so $\gL_S(\hat{p}) \le \gL_S (p_{\mathrm{opt}})+ \epsilon'$. Plugging this in, we obtain
    \begin{align}
        \Norm{p^*(\x) - \hat{p}(\x)}_{S'} &\le \gL_S (p^*)+ \gL_S(p_\opt) + 3\epsilon' \notag \\
        &\le \Norm{p^* - \fsM}_S + \gL(\fsM)_S \notag \\ 
        &\quad + \Norm{p_\opt(\x) - \clip_M(f_\opt(\x))}_S + \gL_S(\clip_M(f_\opt)) + 3\epsilon'. \label{eqn: ps_phat}
    \end{align}
    By applying Hoeffding's inequality, we get that $\gL(\fsM)_S \le \Norm{\fsM - y}_\Dtrain + \epsilon'$ which holds with probability $\ge 1 - \delta'$ when $\mtrain \ge \frac{8M^4 \ln(2/\delta')}{\epsilon'^4}$. By unclipping $\fsM$, this is at most $\lambda_\mathrm{train} + \epsilon'$. Similarly, with probability $\ge 1 - \delta'$, $\gL_S(\clip_M(f_\opt)) \le \opt + \epsilon'$. It remains to bound $\Norm{p^*(\x) - \fsM}_S$ and $\Norm{p_\opt - \clip_M(f_\opt(\x))}_S$. The analysis for both is similar to how we bounded $\Norm{\psM - \fsM}_S$, except since we do not clip $p^*$ or $p_\opt$ we will instead take advantage of the bound on $p^*(\x)$ on $\Norm{W^*\x} > R$ (respectively $p_\opt(\x)$ on $\Norm{W_{\opt}\x} > R$). We show how to bound $\Norm{p^*(\x) - \fsM}_S$:
    \begin{align}
        \E_{\x \sim S}[(\fsMx - p^*(\x))^2] &= \E_{\x \sim S}[(\fsMx - p^*(\x))^2 \cdot \ind[\Norm{W^*\x} \le R] \notag \\
        &+ \E_{\x \sim S}[(\fsMx - p^*(\x))^2 \cdot \ind[\Norm{W^*\x} >R]] \notag \\
        &\le \epsilon^2 + 2\E_{\x \sim S}[\fsMx^2 \cdot \ind[\Norm{W^*\x} >R]] \notag \\
        &\hspace{2.2em} + 2\E_{\x \sim S}[p^*(\x)^2 \cdot \ind[\Norm{W^*\x} >R]]. \label{eqn: fsM_ps}
    \end{align}
    We can bound the first expectation term, $\E_{\x \sim S}[\fsMx^2 \cdot \ind[\Norm{W^*\x} >R]]$, with $\epsilon'^2/4$ since the same analysis holds for bounding $\E_{\x \sim S'}[\fsMx^2 \cdot \ind[\Norm{W^*\x} >R]]$, except instead of matching the first $2t$ moments of $S'$ with $\Dtrainx$, we match the first $2\ell$ moments of $S$ with $\Dtrainx$. We use the strictly subexponential tails of $\Dtrainx$ to bound the second term. Cauchy-Schwarz gives
    \[
    \E_{\x \sim S}[p^*(\x)^2 \cdot \ind[\Norm{W^*\x} >R]] \le \sqrt{\E_{\x \sim S}[p^*(\x)^4]\cdot \Pr_{\x \sim S}[\Norm{W^*\x} >R]]}
    \]
    Note that by definition of $r$ and using that $p^*$ is an $(\epsilon, R)$-uniform approximation of $f^*$, then $p^*(\x) \le (r + \epsilon)$ when $\Norm{W^*\x} \le R$. By \cref{lemma:ball_coeff_bounds}, $|p^*(\x)| \le (r + \epsilon) \cdot (2k\ell)^{c\ell}\Norm{(W^*x)/R}^\ell$ for sufficiently large constant $c_1 > 0$. Then since $R \ge 1$, $p^*(\x) \le (r + \epsilon)^4 \cdot (2k\ell)^{c\ell} \Norm{W^*\x}^{4\ell}$. Then we have
    \begin{align*}
        \E_{\x \sim S}[p^*(\x)^4] &\le (r + \epsilon)^4 \cdot (2k\ell)^{c_1\ell} \cdot \E_{\x \sim S}[\Norm{W^*\x}^{4\ell}] \\
        &\le (r + \epsilon)^4 \cdot (2k\ell)^{c_1\ell} \cdot (\E_{x \sim \Dtrainx}[\Norm{W^*\x}^{4\ell}] + 1) \\
        &\le (r + \epsilon)^4 \cdot (2k \ell)^{c\ell}
    \end{align*}
    where using \cref{fact:moment_bound} we bound on $\E_{x \sim \Dtrainx}[\Norm{W^*\x}^{4\ell}] \le k^{2\ell}(4\ell)^{\frac{4C\ell}{1+\gamma}}$ similar to above, which can be upper bounded with $(2k\ell)^{c_2\ell}$ for $c_2 > 0$ a sufficiently large constant. Take $c = c_1 + c_2$.
    We bound $\Pr_{\x \sim S}[\Norm{W^*\x} >R]]$ as follows:
    \begin{align*}
        \Pr_{\x \sim S}[\Norm{W^*\x} >R]] &= \Pr_{\x \sim S}\Brack{\sum_{i=1}^k \inprod{W_i^*}{\x}^2 > R^2} \\
        &\le \sum_{i=1}^k \Pr_{\x \sim S}[\inprod{W_i^*}{\x}^2 > R^2/k] \\
        &\le k \sup_{\twonorm{\vw} = 1} \Pr_{\x \sim S}[\inprod{W}{\x}^2 > R^2/k],
    \end{align*}
    where the first inequality follows from a union bound.
    Since $\inprod{\vw}{\x}^2$ is a degree $2$ polynomial, we can view $\sign(\inprod{\vw}{\x}^2 - R^2/k)$ as a degree-2 PTF. The class of these functions has VC dimension at most $d^2$ (e.g. by viewing it as the class of halfspaces in $d^2$ dimensions). Using standard VC arguments, whenever $\mtrain \ge C \cdot \frac{d^2 + \log(1/\delta')}{(\epsilon'' / k)^2}$ for some sufficiently large universal constant $C > 0$, with probability $\ge 1 - \delta'$ we have
    \[
    \Pr_{\x \sim S}[\inprod{\vw}{\x}^2 > R^2/k] \le \Pr_{x \sim \Dtrainx}[\inprod{\vw}{\x}^2 > R^2/k] + \epsilon''/k.
    \]
    Using the strictly subexponential tails of $\Dtrainx$, we have
    \begin{align*}
        \Pr_{\x \sim S}[\Norm{W^*\x} >R]] &\le k \Paren{\sup_{\Norm{w} = 1} \Pr_{x \sim \Dtrainx}[\inprod{\vw}{\x}^2 > R^2/k] + \epsilon''/k} \\
        &\le 2k \cdot \exp\Paren{-\Paren{R/k}^{1+\gamma}} + \epsilon''.
    \end{align*}
    Choose $\epsilon'' = \frac{\epsilon'^4}{(r + \epsilon)^4 (2k\ell)^{c\ell}}$. Putting it together:
    \begin{align*}
        \E_{\x \sim S}[p^*(\x)^4] \cdot \Pr_{\x \sim S}[\Norm{W^*\x} >R]] &\le (r + \epsilon)^4 \cdot (2k\ell)^{c\ell} e^{-(R/k)^{1+\gamma}} + \epsilon'^4 \\
        &\le (r + \epsilon)^4 \cdot \exp \Paren{c\ell \log(2k\ell) - (R/k)^{1+\gamma}} + \epsilon'^4.
    \end{align*}
    We want to bound the first part with $\epsilon'^4$. Equivalently, we need to show that the exponent is $\le 4 \ln \frac{\epsilon'}{r + \epsilon}$. Substituting $\ell = R\log R \cdot g_\gF(\epsilon)$, we get that $c\ell \log (2k \ell) \le c g_\gF(\epsilon) R (\log R)^2 \log(2k g_\gF(\epsilon))$. Thus, it suffices to show that
    \begin{align*}
        \Paren{\frac{R}{k}}^{1+\gamma} &\ge cg_\gF(\epsilon)R(\log R)^2 (2k g_\gF(\epsilon)) - 4\ln {\frac{\epsilon'}{r + \epsilon}}.
    \end{align*}
    This is satisfied when $R \ge \poly\left(\left(k g_\gF(\epsilon)\log(r)\log(M/\epsilon)\right)^{1+\frac{1}{\gamma}}\right)$. Then, we have that 
    \[
    \E_{\x \sim S}[p^*(\x)^2 \cdot \ind[\Norm{W^*\x} >R]] \le \epsilon'^2 \sqrt{2}.
    \]
    So, plugging this into \cref{eqn: fsM_ps}, we have
    \[
        \Norm{\fsM - p^*}_S \le \sqrt{\epsilon^2 + 2 \cdot \epsilon'^2/4 + 2\epsilon'^2 \sqrt{2}} \le \epsilon + 2\epsilon'.
    \]
    The same argument will also give
    \[
        \Norm{\clip_M(f_\opt(\x)) - p_\opt(\x)}_S \le \epsilon + 2\epsilon'.
    \]
    Combining \cref{eqn: fsM_psM} and the above two bounds into \cref{eqn: ps_phat}, we have from \cref{eqn: phM_loss} that
    \[
    \gL_{\Dtest} (\phM)\le \lambda + \opt + 3\epsilon + 11\epsilon' \le \lambda + \opt + 4\epsilon.
    \]
    The result holds with probability at least $1 - 5\delta' = 1 - \delta$ (taking a union bound over $5$ bad events).

    \textbf{Completeness.} For completeness, it is sufficient to ensure that $\mtest \ge N$ for $N$ in \cref{lemma:moment-concentration}. This is because when $\Dtrainx = \Dtestx$, our test samples $S'$ are in fact being drawn from the subexponential distribution $\Dtrainx$. Then the moment concentration of subexponential distributions (\cref{lemma:moment-concentration}) gives that the empirical moments of $S'$ are close to the moments of $\Dtrainx$ with probability $\ge 1 - \delta'$. This is the only condition for acceptance, so when $\Dtrainx = \Dtestx$, the probability of acceptance is at least $1 - \delta$, as required.

    \textbf{Runtime.} The runtime of the algorithm is $\poly(d^\ell, \mtrain, \mtest)$, where $\ell = R\log R \cdot g_\gF(\epsilon)$. The two lower bounds on $R$ required in the proof are satisfied by setting $R \ge \left(\left(kg_\gF(\epsilon)\log(r)\log(M/\epsilon)\right)^{O(\frac{1}{\gamma})}\right)$. Note that setting $\mtrain = \poly(M, \ln(1/\delta)^\ell, 1/\epsilon, d^\ell, r)$ satisfies the lower bounds on $\mtrain$ required in the proof. For $\mtest$ we required that $\mtest \ge \frac{8M^4 \ln(2/\delta')}{\epsilon'^4}$ and also $\mtest \ge N$ for $N$ in \cref{lemma:moment-concentration}. This is satisfied by choosing $\mtest = \mtrain$. Putting this altogether, we see that the runtime is $\poly(d^s, \ln(1/\delta)^\ell, 1/\epsilon)$ where $s =  \Paren{\Paren{kg_\gF(\epsilon) \log(r) \log(M/\epsilon)}^{O(1/\gamma)}}$.
\end{proof}

\subsection{Applications}

In order to obtain end-to-end results for classes of neural networks (see the rightmost column of \Cref{table:main-results}), we need to prove the existence of uniform polynomial approximators whose degree scales almost linearly with respect to the radius of approximation for the reasons described above. For arbitrary Lipschitz nets (see \Cref{thm:approx_lipschitz_nets}), we use a general tool from polynomial approximation theory, the multivariate Jackson's theorem (\Cref{thm:jackson}). This gives us a polynomial with degree scaling linearly in $R$ and polynomially on $\frac{1}{\epsilon}$ and the number of hidden units ($k$) in the first layer.

For sigmoid nets, a more careful derivation yields improved bounds (see \Cref{thm:approx_sigmoid_nets}) which have a poly-logarithmic dependence on $\frac{1}{\epsilon}$. Our construction involves composing approximators for the activations at each layer. Naively, the degree of this composition would be superlinear in $R$. To get around this, we use the key property that the size of the output of a sigmoid network at any layer is memoryless (i.e., has no $R$ dependence). This follows from the fact that the sigmoid is bounded in $[0,1]$. Using this, we obtain an approximator with almost-linear dependence on $R$. For more details see \Cref{sec:approx_sigmoid_appendix}.

% \section{Polynomial Approximations of Neural Networks}\label{section:approximation_theory}
% In this section we derive the polynomial approximations of neural networks with Lipschitz activations needed to instantiate \cref{theorem:tds-via-kernels} for bounded distributions and \cref{theorem:tds-via-uniform} for unbounded distributions. 

% Recall the definition of a neural network. 
% \begin{definition}[Neural Network]
%     \label{def:Neural Network}
%     Let $\sigma:\R\to\R$ be an activation function with $\sigma(0)\leq 1$. Let $\W=\left(W^{(1)},\ldots W^{(t)}\right)$ with $W^{(i)}\in \R^{s_i\times s_{i-1}}$ be the tuple of weight matrices. Here, $s_0=d$ is the input dimension and $s_{t}=1$. Define recursively the function $f_i:\R^{d}\to \R^{s_i}$ as $f_i(\x)=W^{(i)}\cdot \sigma\bigl(f_{i-1}(\x)\bigr)$ with $f_1(\x)=W^{(1)}\cdot\x$. The function $f:\R^d \to \R$ computed by the neural network $(\W,\sigma)$ is defined as $f(\x)\coloneq f_{t}(\x)$. We denote $\norm{\W}_1=\sum_{i=2}^{t}\norm{W^{(i)}}_1$. The depth of this network is $t$. 
% \end{definition}

% We also introduce some notation and basic facts that will be useful for this section.

% \subsection{Useful Notation and Facts}
% Given a univariate function $g$ on $\R$ and a vector $\x=(x_1,\ldots,x_d)\in \R^{d}$, the vector $g(\x)\in \R^{d}$ is defined as the vector with $i^{th}$ co-ordinate equal to $g(x_i)$.  For a matrix $A\in \R^{m\times n}$, we use the following notation:
%  \begin{itemize}
%      \item $\twonorm{A}\coloneq\sup_{\twonorm{x}=1}\twonorm{Ax}$,
%      \item $\normtwoinf{A}\coloneq\sqrt{\max_{i\in [m]}\sum_{j=1}^{n}(A_{ij})^2}$,
%      \item $\norm{A}_1\coloneq \sum_{(i,j)\in [n]\times [m]}|A_{ij}|$.
%  \end{itemize}
% \begin{fact}
% \label{fact:matrix_norms}
%     Given a matrix $W\in\R^{m\times n}$, we have that 
%     \begin{enumerate}
%         \item $\twonorm{A}\leq \norm{A}_1$,
%         \item $\twonorm{A}\leq \sqrt{m}\cdot \normtwoinf{A}$.
%     \end{enumerate}
% \end{fact}
% \begin{proof}
%     We first prove (1). We have that for an $\x\in \R^{n}$ with $\twonorm{\x}=1$,
%     \begin{align*}
%         \twonorm{A\x}\leq \sqrt{\sum_{i=1}^{m}(A_i\cdot \x)^2}\leq \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}(A_{ij})^2}\leq \norm{A}_1
%     \end{align*}
%     where the second inequality follows from Cauchy Schwartz and the last inequality follows from the fact that for any vector $\vv$, $\twonorm{\vv}\leq \norm{\vv}_1$.
%     We now prove (2). We have that 
%      \begin{align*}
%         \twonorm{A\x}\leq \sqrt{\sum_{i=1}^{m}(A_i\cdot \x)^2}\leq \sqrt{m\max_{i\in [m]}\sum_{j=1}^{n}(A_{ij})^2}\leq \sqrt{m}\normtwoinf{A}
%     \end{align*} where the second inequality follows from Cauchy Schwartz and the last inequality is the definition. 
% \end{proof}

% \subsection{Results from Approximation Theory}

% The following are useful facts about the coefficients of approximating polynomials.
% \begin{fact}[Lemma~23 from \cite{reliable_goel2017}]
% \label{lem:uni_poly_unit_coeff_bound}
%     Let $p$ be a polynomial of degree $\ell$ such that $|p(x)|\leq b$ for $|x|\leq 1$. Then, the sum of squares of all its coefficients is at most $b^2\cdot 2^{O(\ell)}$.
% \end{fact}
% \begin{lemma}
% \label{lem:uni_poly_coeff_bound}
%  Let $p$ be a polynomial of degree $\ell$ such that $|p(\x)|\leq b$ for $|x|\leq R$. Then, the sum of squares of all its coefficients is at most $b^2\cdot 2^{O(\ell)}$ when $R\geq 1$.
% \end{lemma}
% \begin{proof}
%     Consider $q(x)=p(Rx)$. Clearly, $|q(x)|\leq b$ for all $|x|\leq 1$. Thus, the sum of squares of its coefficients is at most $b^2\cdot 2^{O(\ell)}$ from \Cref{lem:uni_poly_unit_coeff_bound}. Now, $p(x)=q(x/R)$ has coefficients bounded by $b^2\cdot 2^{O(\ell)}$ when $R\geq 1$.
% \end{proof}
% % Then the magnitude of any coefficient of $q$ is at most $(2\ell)^{3\ell}$
% \begin{fact}[\cite{ben2018classical}]\label{lemma:cube_coeff_bounds}
%     Let $q$ be a polynomial with real coefficients on $k$ variables with degree $\ell$ such that for all $\x \in [0, 1]^k$, $|q(\x)| \le 1$. Then the magnitude of any coefficient of $q$ is at most $(2k\ell(k+\ell))^{\ell}$ and the sum of the magnitudes of all coefficients of $q$ is at most $(2(k+\ell))^{3\ell}$.
% \end{fact}
% \begin{lemma}\label{lemma:ball_coeff_bounds}
%     Let $q$ be a polynomial with real coefficients on $k$ variables with degree $\ell$ such that for all $\x \in \R^k$ with $\Norm{\x}_2 \le R$, $|q(\x)| \le b$. Then the sum of the magnitudes of all coefficients of $q$ is at most $b(2(k+\ell))^{3\ell}k^{\ell/2}$ for $R\geq 1$.
% \end{lemma}
% \begin{proof}
%     Consider the polynomial $h(\x) = 1/b \cdot q(R\x/\sqrt{k})$. Then $|h(\x)| = 1/b \cdot |q(R\x/\sqrt{k})| \le 1$ for $\|\x R/\sqrt{k}\|_2 \le R$, or equivalently for all $\Norm{x}_2 \le \sqrt{k}$. In particular, since the unit cube $[0,1]^k$ is contained in the $\sqrt{k}$ radius ball, then $|h(\x)| \le 1$ for $\x \in [0,1]^k$. By \cref{lemma:cube_coeff_bounds}, the sum of the magnitudes of the coefficients of $h$ is at most $(2(k+\ell))^{3\ell}$. Since $q(\x) = b \cdot h(\x\sqrt{k}/R)$, then the sum of the magnitudes of the coefficients of $q$ is at most $b(2(k + \ell))^{3\ell} k^{\ell/2}$.
% \end{proof}

% \begin{lemma}\label{lemma:sum_coeff_bound}
%     Let $p(\x)$ be a degree $\ell$ polynomial in $\x \in \R^d$ such that each coefficient is bounded in absolute value by $b$. Then the sum of the magnitudes of the coefficients of $p(\x)^t$ is at most $b^t d^{t \ell}$.
% \end{lemma}
% \begin{proof}
%     Note that $p(\x)$ has at most $d^\ell$ terms. Expanding $p(\x)^t$ gives at most $d^{t \ell}$ terms, where any monomial is formed from a product of $t$ terms in $p(\x)$. Then the coefficients of $p(\x)^t$ are bounded in absolute value by $B^t$. Summing over all monomials gives the bound.
% \end{proof}

% In the following lemma, we bound the magnitude of approximating polynomials for subspace juntas outside the radius of approximation.
% \begin{lemma}\label{lemma:bound-on-uniform-approximator-outside-interval}
%     Let $\epsilon > 0, R \ge 1$, and $f: \R^d \rightarrow \R$ be a $k$-subspace junta, and consider the corresponding function $g(W\x)$. Let $q: \R^k \rightarrow \R$ be an $(\epsilon, R)$-uniform approximation polynomial for $g$, and define $p: \R^d \rightarrow \R$ as $p(\x) := q(W\x)$. Let $r := \sup_{\Norm{W\x}_2 \le R} |g(W\x)|$. Then
%     \[|p(\x)| \le (r + \epsilon)(2(k+\ell))^{3\ell}k^{\ell/2} \Norm{\frac{W\x}{R}}_2^\ell \quad \forall \Norm{W\x}_2 \ge R.\]
% \end{lemma}
% \begin{proof}
%     Since $q(\x)$ is an $(\epsilon, R)$-uniform approximation for $g$, then $|q(\x) - g(\x)| \le \epsilon$ for $\Norm{\x}_2 \le R$. Let $h(\x) = q(R\x)$. Then $|h(\x/R) - g(\x)| \le \epsilon$ for $\Norm{\x}_2 \le R$, and so $|h(\x/R)| \le r + \epsilon$ for $\Norm{\x}_2 \le R$, or equivalently $|h(\x)| \le r + \epsilon$ for $\Norm{\x}_2 \le 1$. Write $h(\x) = \sum_{\Norm{\alpha}_1 \le \ell} h_\alpha x_1^{\alpha_1}\ldots x_k^{\alpha_k}$. By \cref{lemma:ball_coeff_bounds}, $\sum_{\Norm{\alpha}_1 \le \ell} |h_\alpha| \le (r + \epsilon)(2(k+\ell))^{3\ell} \cdot k^{\ell/2}$. Then for $\Norm{x}_2 \ge 1$,
%     \begin{align*}
%         |h(\x)| &\le \sum_{\Norm{\alpha}_1 \le \ell} |h_\alpha| |x_1^{\alpha_1}\ldots x_k^{\alpha_k}| \\
%         &\le \sum_{\Norm{\alpha}_1 \le \ell} |h_\alpha| \Norm{\x}_2^{\Norm{\alpha}_1} \\
%         &\le \Norm{\x}_2^\ell \cdot \sum_{\Norm{\alpha}_1 \le \ell} |h_\alpha|, 
%     \end{align*}
%     where the second inequality holds because $|x_i| \le \Norm{\x}_2$ for all $i$, and the last inequality holds because $\Norm{\x}_2^\ell \ge \Norm{\x}_2^{\Norm{\alpha}_1}$ for $\Norm{\alpha}_1 \le \ell$ when $\Norm{\x}_2 \ge 1$.
%     Then since $p(\x) = q(W\x) = h(W\x/R)$, we have $|p(\x)| \le \Norm{\frac{W\x}{R}}_2^\ell (r + \epsilon)(2(k+\ell))^{3\ell} k^{\ell/2}$ for $\Norm{W\x}_2 \ge R$.
% \end{proof}

% The following is an important theorem that we use later to obtain uniform approximators for Lipschitz Neural networks.
%  \begin{theorem}[\cite{Newman1964}]
% \label{thm:jackson}
%     Let $f:\R^{k}\to \R$ be a function continuous on the unit sphere $S_{k-1}$. Let $\omega_{f}$ be the function defined as $\omega_{f}(t)\coloneq\sup_{\substack{\twonorm{\x},\twonorm{\vy}\leq 1\\{\twonorm{\x-\vy}\leq t}}}|f(\x)-f(\vy)|$ for any $t\geq 0$.
% Then, we have that there exists a polynomial of degree $\ell$ such that 
% $\sup_{\twonorm{x}\leq 1}|f(\x)-p(\x)|\leq C\cdot \omega_{f}(k/\ell)
% $ where $C$ is a universal constant.
% \end{theorem} 

% This implies the following corollary.
% \begin{corollary}
%     \label{clry:lipschitz_jackson}
% Let $f:\R^{k}\to \R$ be an $L$-Lipschitz function for $L\geq 0$ and let $R\geq 0$. Then, for any $\epsilon\geq 0$, there exists a polynomial $p$ of degree $O(LRk/\epsilon)$ such that $p$ is an $(\epsilon,R)$-uniform approximation polynomial for $f$. 
% \end{corollary}
% \begin{proof}
%     Consider the function $g(\x)\coloneq f(R\x)$. Then, we have that $g$ is $RL$-Lipschitz. From statement of \Cref{thm:jackson}, we have that $\omega_g(t)\leq RLt$. Thus, from \Cref{thm:jackson}, there exists a polynomial $q$ of degree $O(LRk/\epsilon)$ such that $\sup_{\twonorm{\vy}\leq 1}|g(\vy)-q(\vy)|\leq \epsilon$. Thus, we have that $\sup_{\twonorm{\x}\leq R}|f(\x)-q(\x/R)|=\sup_{\twonorm{\x}\leq R}|g(\x/R)-q(\x/R)|=\sup_{\twonorm{\vy}\leq 1}|g(\vy)-q(\vy)|\leq \epsilon$. $p(\x)\coloneq q(\x/R)$ is the required polynomial of degree $O(LRk/\epsilon)$. 
% \end{proof}

% \subsection{Kernel Representations}
% We now state and prove facts about Kernel Representations that we require. First, we recall the multinomial kernel from \cite{reliable_goel2017}.
% \begin{definition}
% \label{def:multinomial_kernel}
%     Consider the mapping $\psi_{\ell}:\R^{n}\to \R^{N_{\ell}}$, where $N_d=\sum_{i=1}^{\ell}d^{\ell}$ is indexed by tuples $(i_1,i_2,\ldots, i_{j})\in [d]^j$ for $j\in [\ell]$ such that value of $\psi_{\ell}(\x)$ at index $(i_1,i_2,\ldots,i_{j})$ is equal to $\prod_{t=1}^{j}\x_{i_t}$. The kernel $\mkl$ is defined as 
%     \[
%     \mkl(\x,\vy)=\langle\psi_{\ell}(\x),\psi_{\ell}(\vy)\rangle=\sum_{i=1}^{d}(\x\cdot\vy)^{i}.
%     \] We denote the corrresponding RKHS as $\hkml$.
% \end{definition}

% We now prove that polynomial approximators of subspace juntas can be represented as elements of $\hkml$.
% \begin{lemma}
% \label{lem:subspace_junta_kernel}
%     Let $k\in \mathbb{N}$ and $\epsilon, R\geq 0$.  Let $f:\R^{d}\to \R$ be a $k$-subspace junta such that $f(\x)=g(W\x)$ where $g$ is a function on $\R^{k}$ and $W$ is a projection matrix from $\R^{k\times d}$. Suppose, there exists a polynomial $q$ of degree $\ell$ such that $\sup_{\twonorm{\vy}\leq R}|g(\vy)-q(\vy)|\leq \epsilon$ and the sum of squares of coefficients of $q$ is bounded above by $B^2$. Then, $f$ is $(\epsilon, B^2\cdot (k+1)^{\ell})$-approximately represented within radius $R$ with respect to $\hkml$.
% \end{lemma}
% \begin{proof}
%     We argue that there exists a vector $\vv\in \hkml$ such that $\langle\vv,\vv\rangle\leq B^2$ and $|f(\x)-\langle \vv,\sigma_{\ell}(\x)\rangle|\leq \epsilon$ for all $\twonorm{\x}\leq R$. Consider the polynomial $p$ of degree $\ell$ such that $p(\x)=q(W\x)$. We argue that $p(\x)=\langle \vv,\sigma_{\ell}(\x)\rangle$ for some $\vv$ and that $\langle\vv,\vv\rangle\leq B^2$. Let $q(\vy)=\sum_{S\in \mathbb{N}^{k},|S|\leq \ell}q_{S}\prod_{j=1}^{k}\vy^{|S_j|}$. From our assumption on $q$, we have that $\sum_{S\in \mathbb{N}^{k},|S|\leq \ell}|q_{S}|\leq B$. For $i\in \ell$, we use define $B_i$ as $B_i=\sum_{S\in \mathbb{N}^{k},|S|=\ell}|q_{S}|$. Given multi-index $S$, for any $i\in [d]$, we define $S(i)$ as the number $t$  such that $\sum_{i=1}^{j-1}|S_i|\leq j< \sum_{i=1}^{j}|S_i|$. We now compute the entry of $\vv$ indexed by $(i_1,i_2,\ldots,i_t)$. By expanding the expression for $p(\x)$, we obtain that 
%     \[v_{i_1,\ldots,i_t}=\sum_{|S|=t}q_{S}\prod_{j=1}^{t}W_{S(j),i_j}.\]

%     We are now ready to bound $\langle\vv,\vv\rangle$. We have that 
%     \begin{align*}
% \langle\vv,\vv\rangle&=\sum_{t=0}^{\ell}\sum_{(i_1,\ldots,i_t)\in [d]^{k}}(v_{i_1,\ldots,i_t})^2
%     =\sum_{t=0}^{\ell}\sum_{(i_1,\ldots,i_t)\in [d]^k}\left(\sum_{|S|=t}q_{S}\prod_{j=1}^{t}W_{S(j),i_j}\right)^2\\
%     &\leq \sum_{t=0}^{\ell}\sum_{(i_1,\ldots,i_t)\in [d]^k}\left(\sum_{|S|=t}q^2_{S}\right)\left(\sum_{|S|=t}\prod_{j=1}^{t}W^2_{S(j),i_j}\right)\\
%     &\leq \sum_{t=0}^{\ell}\left(\sum_{|S|=t}q^2_{S}\right)\left(\sum_{|S|=t}\prod_{j=1}^{t}\left(\sum_{i=1}^{d}W^2_{S(j),i}\right)\right)\leq \sum_{t=0}^{\ell}\left(\sum_{|S|=t}q^2_{S}\right)\cdot (k+1)^{t}
%     \\  &\leq\left(\sum_{|S|\leq \ell}q^2_{S}\right)\cdot (k+1)^{\ell}\leq B^2\cdot (k+1)^{\ell}.
%     \end{align*}
% Here, the first inequality follows from Cauchy-Schwartz, the second follows by rearranging terms. The third inequality follows from the fact that the number of multi-indices of size $t$ from a set of $k$ elements is at most $(k+1)^{t}$. The final inequality follows from the fact that the sum of the squares of the coefficients of $q$ is at most $B^2$.
% \end{proof}


% We introduce an extension of the multinomial kernel that will be useful for our application to sigmoid-nets.
% \begin{definition}[Composed multinomial kernel]
% \label{def:composed_multinomial_kernel}
%     Let $\vell=(\ell_1,\ldots,\ell_t)$ be a tuple in $\mathbb{N}^{t}$. We denote a sequence of mappings $\psi^{(0)}_{\vell},\psi^{(1)}_{\vell},\ldots,\psi^{(t)}_{\vell}$ on $\R^{d}$ inductively as follows:
%     \begin{enumerate}
%         \item $\psi^{(0)}_{\vell}(\x)=\x$ 
%         \item $\psi^{(i)}_{\vell}(\x)=\psi_{\ell_i}\left(\psi^{(i-1)}_{\vell}(\x)\right)$.
%     \end{enumerate}
%     Let $N_{\vell}^{(i)}$ denote the number of coordinates in $\cpsi{i}$.
%     This induces a sequence of kernels $\cmkl{0},\cmkl{1},\ldots,\cmkl{t}$ defined as 
%     \[
%     \cmkl{i}(\x,\vy)=\langle\cpsi{i}(\x),\cpsi{i}(\vy)\rangle=\sum_{j=0}^{\ell_{i}}\left(\langle\cpsi{i-1}(\x),\cpsi{i-1}(\vy)\rangle^j\right)
%     \] and a corresponding sequence of RKHS denoted by $\mathcal{H}_{\cmkl{0}},\mathcal{H}_{\cmkl{1}},\ldots \mathcal{H}_{\cmkl{t}}$.

%     Observe that the the multinomial Kernel $\mkl=\mathsf{MK}^{(1)}_{(\ell)}$ is an instantiation of the composed multinomial kernel.
% \end{definition}

% We now state some properties of the composed multinomial kernel.
% \begin{lemma}
% \label{lem:composed_multinomial_properties}
%     Let $\vell=(\ell_1,\ldots,\ell_{t})$ be a tuple in $\mathbb{N}^{t}$ and $R\geq 0$. Then, the following hold: 
%     \begin{enumerate}
%         \item $\sup_{\twonorm{\x}\leq R}\cmkl{t}(\x,\x)\leq \max\{1,(2R)^{2^{t}\prod_{i=1}^{t}\ell_{i}}\}$,
%         \item For any $\x,\vy\in \R^{d}$, $\cmkl{t}(\x,\vy)$ can be computed in time $\poly\left(d, (\sum_{i=1}^{t}\ell_i)\right)$,
%         \item For any $\vv\in \mathcal{H}_{\cmkl{t}}$ and $\x\in \R^{d}$, we have $\langle\vv,\cpsi{t}(\x)\rangle$ is a polynomial in $\x$ of degree $\prod_{i=1}^{t}\ell_i$.
%     \end{enumerate}
% \end{lemma}
% \begin{proof}
%     We assume without loss of generality that $R\geq 1$ as the kernel function is increasing in norm. To prove (1), observe that for any $\x$, we have that
%     \[\cmkl{i}(\x,\x)=\sum_{j=0}^{\ell_i}\left(\cmkl{i-1}(\x,\x)\right)^{j}\leq \left(2\cmkl{i-1}(\x,\x)\right)^{\ell_i+1}.\]
% We also have that $\sup_{\twonorm{x}\leq R}\cmkl{0}(\x,\x)=\x\cdot \x=R$. Thus, unrolling the recurrence gives us that $\cmkl{t}(\x,\x)\leq \max\{1,(2R)^{\prod_{i=1}^{t}(\ell_i+1)}\}\leq \max\{1,(2R)^{2^{t}\prod_{i=1}^{t}\ell_{i}}\}$.

% The run time follows from the fact that $\cmkl{i}(\x,\x)=\sum_{j=0}^{\ell_i}\left(\cmkl{i-1}(\x,\x)^{j}\right)$ and thus can be computed from $\cmkl{i-1}$ with $\ell_i$ additions and exponentiation operations. Recursing gives the final runtime.

% The fact that $\langle\vv,\cpsi{i}(\x)\rangle$ follows immediately from the fact the fact the entries of $\cpsi{i}(\x)$ arise from the multinomial kernel and hence are polynomials in $\x$. The degree is at most $\prod_{i=1}^{t}\ell_i$.
% \end{proof}
% We now argue that a distribution that is hypercontractive with respect to polynomials is hypercontractive with respect to the multinomial kernel.
% \begin{lemma}
% \label{lem:hc_implies_kernelhc}
%     Let $\Dtrain$ be a distribution on $\R^{d}$ that is $C$-hypercontractive for some constant $C$. Then, $\Dtrain$ is also $(\cmkl{t},C,\prod_{i=1}^{t}\ell_i)$-hypercontractive.
% \end{lemma}
% \begin{proof}
%     The proof immediately follows from \Cref{definition:hypercontractivity} and \Cref{lem:composed_multinomial_properties}(3).
% \end{proof}
% \subsection{Nets with Lipschitz activations}

% We are now ready to prove our theorem about uniform approximators for neural networks with Lipschitz activations. First, we prove that such networks describe a Lipschitz function. 

    
% \begin{lemma}
% \label{lem:neural_net_lipschitz}
%     Let $f:\R^{d}\to \R$ be the function computed by an $t$-layer neural network with $L$-Lipschitz activation function $\sigma$ and weight matrices $\W$. Say, $\norm{\W}_1\leq W$ for $W\geq 0$ and the first hidden layer has $k$ neurons. Then we have that $f$ is $\sqrt{k}\normtwoinf{W^{(1)}}(WL)^{t-1}$-Lipschitz.
% \end{lemma}
% \begin{proof}
%     First, observe from \Cref{fact:matrix_norms} that for all $1<i\leq T$, $\twonorm{W^{(i)}}\leq W$ (since $\norm{\W}_1\leq W$) and $\twonorm{W^{(1)}}\leq \sqrt{k}\normtwoinf{W^{(1)}}$. Recall from \Cref{def:Neural Network}, we have the functions $f_1,\ldots,f_{t}$ where $f_{i}(\x)=W^{(i)}\cdot \sigma\bigl(f_{i-1}(\x)\bigr)$ and $f_1(\x)=W^{(1)}\cdot\x$. We prove by induction on $i$ that $\twonorm{f_{i}(\x)-f_{i}(\x+\vu)}\leq \sqrt{k}\normtwoinf{W^{(1)}}(WL)^{i-1}\twonorm{\vu}$. For the base case, observe that 
%     \begin{align*}
%         \twonorm{f_1(\x+\vu)-f_1(\x)}&\leq \sqrt{\sum_{i=1}^{d_1}\biggl(\bigl(\langle W^{(1)}_i,\x\rangle-\langle W^{(1)}_i,\x+\vu\rangle\bigr)^2\biggr)}
%         \leq \sqrt{\sum_{i=1}^{d_1}\biggl(\langle W^{(1)}_i, \vu\rangle\biggr)^2}\\ &\leq \twonorm{W^{(1)}_i\vu}\leq\sqrt{k}\normtwoinf{W^{(1)}}\twonorm{\vu}
%     \end{align*} where the second inequality follows from the Lipschitzness of $\sigma$ and the final inequality follows from the definition of operator norm.
%     We now proceed to the inductive step. Assume by induction that $\twonorm{f_i(\x)-f_{i}(\x+\vu)}$ is at most $\sqrt{k}\normtwoinf{W^{(1)}}(WL)^{i-1}\twonorm{\vu}$. Thus, we have 
%     \begin{align*}
%         \twonorm{f_{i+1}(\x+\vu)-f_{i+1}(\x)}&=
%          \sqrt{\sum_{j=1}^{d_1}\biggl(\langle W^{(i+1)}_j,\sigma\left(f_i(\x)\right)\rangle-\langle W^{(i+1)}_j,\sigma\left(f_i(\x+\vu)\right)\rangle\biggr)^2} \\
%         &\leq \twonorm{W^{(i+1)}}\twonorm{\sigma(f_i(\x))-\sigma(f_i(\x+\vu))}\\
%         &\leq (WL)\sqrt{k}\normtwoinf{W^{(1)}}(WL)^{i-1}\twonorm{\vu}\leq \sqrt{k}\normtwoinf{W^{(1)}}(LW)^{i}\twonorm{\vu}
%     \end{align*}
%     where the third inequality follows from the Lipschitzness of $\sigma$ and the inductive hypothesis. Thus, we get that $|f(\x+\vu)-f(\x)|\leq \twonorm{f_{t}(\x+\vu)-f_{t}(\x)}\leq \sqrt{k}\normtwoinf{W^{(1)}}(WL)^{t-1}\cdot \twonorm{\vu}$.
% \end{proof}

% We now state are theorem regarding the uniform approximation of Lipschitz nets. We also prove that the approximators can be represented by low norm vectors in $\mathcal{R}_{\mkl}$ for appropriately chosen degree $\ell$.
% \begin{theorem}
% \label{thm:approx_lipschitz_nets}
% Let $\epsilon,R\geq 0$. Let $f: \R^d \rightarrow \R$ be a neural network with an $L$-Lipschitz activation function $\sigma$, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ where $W^{(i)}\in \R^{s_i\times s_{i-1}}$. Let $k$ be the number of neurons in the first hidden layer. Then, there exists of a polynomial $p$ of degree $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$ that is an $(\epsilon,R)$-uniform approximation polynomial for $f$. Furthermore,  $f$ is $(\epsilon, (k+\ell)^{O(\ell)})$-approximately represented within radius $R$ with respect to $\hkml=\mathbb{H}_{\mathsf{MK}^{(1)}_{(\ell)}}$. In fact, when $k=1$, it holds that $f$ is $(\epsilon,2^{O(\ell)})$-approximately represented within $R$ with respect to $\mathbb{H}_{\cmkl{1}}$.
% \end{theorem}
% \begin{proof}
%     We can express $f$ as $f(\x)=g(P\x)$ where $P$ is a projection matrix and $g$ is a neural network with input size $k$. We observe that the Lipschitz constant of $g$ is the same as the Lipschitz constant of $f$ since $P$ is a projection matrix. 
%     From \Cref{lem:neural_net_lipschitz}, we have that $g$ is $\normtwoinf{\sqrt{k}W^{(1)}}(WL)^{t-1}$-Lipshitz. From \Cref{clry:lipschitz_jackson}, we have that there exists a polynomial $q$ of degree $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$ that is an $(\epsilon,R)$-uniform approximation for $g$. From \Cref{lemma:ball_coeff_bounds}, we have that the sum of squares of  magnitudes of coefficients of $q$ is bounded by $\left(\normtwoinf{\sqrt{k}W^{(1)}}(WL)^{t-1}R\right)(k+\ell)^{O(\ell)}\leq (k+\ell)^{O(\ell)}$. Now, applying \Cref{lem:subspace_junta_kernel} yields the result.  When $k=1$, we apply \Cref{lem:uni_poly_coeff_bound} to obtain that the sum of squares of magnitudes of coefficients of $q$ is bounded by $\normtwoinf{W^{(1)}}(WL)^{t-1}\cdot 2^{O(\ell)}\leq 2^{O(\ell)}$.
% \end{proof}
% \subsection{Sigmoids and Sigmoid-nets}
% \label{sec:approx_sigmoid_appendix}
% We now give a custom proof for the case of neural networks with sigmoid activation. We do this as we can hope to get $O(\log(1/\epsilon)$ degree for our polynomial approximation. We largely follow the proof technique of \cite{reliable_goel2017} and \cite{zhang16}. The modifications we make are to handle the case where the radius of approximation is a variable $R$ instead of a constant. We require(for our applications to strictly-subexponential distributions) that the degree of approximation must scale linear in $R$, a property that does not follow directly from the analysis given in \cite{reliable_goel2017}. We modify their analysis to achieve this linear dependence. 

% We first state a result regarding polynomial approximations for a single sigmoid activation. 

% \begin{theorem}[\cite{livni14}]
% \label{thm:sigmoid_poly}
% Let $\sigma:\R\to \R$ denote the function $\sigma(x)=\frac{1}{1+e^{-x}}$. Let $R,\epsilon\geq 0$. Then, there exists a polynomial $p$ of degree $\ell=O(R\log(R/\epsilon))$ such that 
% $\sup_{|x|\leq R}|\sigma(x)-p(x)|\leq \epsilon$. Also, the sum of the squares of the coefficients of $p$ is bounded above by $2^{O(\ell)}$.
% \end{theorem}

% We now present a construction of a uniform approximation for neural networks with sigmoid activations. The construction is similar to the one in \cite{reliable_goel2017} but the analysis deviates as linear dependence on radius of approximation is important to us.

% \begin{theorem}
% \label{thm:approx_sigmoid_nets}
%     Let $\epsilon,R\geq 0$. Let $f$ on $\R^{d}$ be a neural network with sigmoid activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ where $W^{(i)}\in \R^{s_i\times s_{i-1}}$. Also, let $\norm{\W}_1\leq W$. Then, there exists of a polynomial $p$ of degree $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$ that is an $(\epsilon,R)$-uniform approximation polynomial for $f$. Furthermore,  $f$ is $(\epsilon, B)$-approximately represented within radius $R$ with respect to $H_{\cmkl{t}}$ where $\vell=(\ell_1,\ldots,\ell_{t-1})$ is a tuple of degrees whose product is bounded by $\ell$. Here, $B\leq (2\normtwoinf{W^{(1)}})^{\ell}\cdot W^{O\left(W^{t-2}(t\log(W/\epsilon)^{t-2}\right)}$.
% \end{theorem}
% \begin{proof}
%     First, let $q_{1}$ be the polynomial guaranteed by \Cref{thm:sigmoid_poly} that $(\epsilon/(2W)^t)$-approximates the sigmoid in an interval of radius $R\normtwoinf{W^{(1)}}$. Denote the degree of $q_1$ as $\ell_1=O\left(Rt\normtwoinf{W^{(1)}}\log(RW/\epsilon)\right)$. For all $1<i<t$, let $q_i$ be the polynomial that $(\epsilon/(2W)^{t})$-approximates the sigmoid upto radius $2W$. These have degree equal to $O\left(Wt\log(W/\epsilon)\right)$. Let $\vell=(\ell_1,\ldots \ell_{t-1})$. For all $i\in [t-1]$, let $q_{i}(x)=\sum_{j=0}^{\ell_i}\beta^{(i)}_jx^{j}$. We know that $\sum_{i=0}^{\ell_i}(\beta^{(i)}_j)^2\leq 2^{O(\ell_i)}$.
    
%     We now construct the polynomial $p$ that approximates $f$. For $i\in [t]$, define $p_{i}(\x)=W^{(i)}\cdot q_{i-1}\left(p_{i-1}(\x)\right)$ with $p_1(\x)=W^{(1)}\cdot \x$. Define $p(\x)=p_{t}(\x)$. Recall that $p_{i}(\x)$ is a vector of $s_{i}$ polynomials. We prove the following by induction: for every $i\in[t]$,
%     \begin{enumerate}
%         \item $\norm{p_{i}(\x)-f_{i}(\x)}_{\infty}\leq \epsilon/(2W)^{t-i}$,
%         \item For each $j\in [s_i]$, we have that $(p_{i})_{j}(\x)=\langle\vv,\cpsi{i}(\x)\rangle$ with $\langle\vv,\vv\rangle\leq (2\normtwoinf{W^{(1)}})^{O(\prod_{n=1}^{i-1}\ell_{n})}\cdot W^{O(\prod_{n=2}^{i-1}\ell_n)}$.
%     \end{enumerate}
%     where the function $f_i$ is as defined in \Cref{def:Neural Network}. 

%     The above holds trivially for $i=1$ and $f_1(\x)=p_1(\x)=W^{(1)}\cdot (\x)$ is an exact approximator. Also, $(p_1)_i(\x)=\langle W^{(1)}_i,\x\rangle=\langle W^{(1)}_i,\cpsi{1}(x)\rangle$ from the definition of $\cpsi{1}$. Clearly, $\langle W^{(1)}_i,W^{(1)}_i\rangle\leq \left(\normtwoinf{W^{(1)}}\right)^2.$ We now prove that the above holds for $i+1\in [t]$ assuming it holds for $i$. 

%     We first prove (1). For $j\in [s_{i+1}]$, we have that
%     \begin{align*}
%        |(p_{i+1})_{j}(\x)-(f_{i+1})_{j}(\x)|&=|W^{(i+1)}_{j}\bigl(q_{i}(p_{i}(\x))-\sigma(f_{i}(\x))\bigr)|\\
%        &\leq |W^{(i+1)}_{j}\bigl( q_{i}(p_{i}(\x))-\sigma(p_{i}(\x)\bigr)|+|W^{(i+1)}_{j}\bigl( \sigma(p_{i}(\x))-\sigma(f_{i}(\x)\bigr)|\\
%        &\leq W\cdot (\epsilon/(2W)^{t})+W\cdot\epsilon/(2W)^{t-i}\leq \epsilon/(2W)^{t-(i+1)}. 
%     \end{align*}
%     For the second inequality, we analyse the cases $i=1$ and $i>1$ separately. When $i=1$, we have that $(p_1)_{j}(\x)= (f_{1})_{j}(\x)\leq R\normtwoinf{W_1}$ and $\sigma(x)-q_1(x)\leq (\epsilon/(2W)^t)$ when $|x|\leq R\normtwoinf{W_1}$. For $i>1$, from the inductive hypothesis, we have that $|W^{(i+1)}p_i(\x)|\leq |W^{(i+1)}f_{i}(\x)|+\norm{W^{(i+1)}}_{1}\cdot (\epsilon/(2W)^{t-i})\leq 2W$. The second term in the second inequality is bounded since $\sigma$ is $1$-Lipschitz. 

%     We are now ready to prove that $(p_{i+1})_j$ is representable by small norm vectors in $\mathcal{H}_{\cmkl{i+1}}$ for all $j\in [s_{j+1}]$. We have that 
% \[
%         (p_{i+1})_{j}(\x)=\sum_{k=1}^{s_{i}}W^{(i+1)}_{jk}\cdot q_{i}\left((p_{i})_{k}(\x)\right).
% \]
% From the inductive hypothesis, we have that $(p_i)_{k}=\langle\vv^{(k)},\cpsi{i}\rangle$. Thus, we have that
% \[
%  (p_{i+1})_{j}(\x)=\sum_{k=1}^{s_{i}}W^{(i+1)}_{jk}\cdot q_{i}\left(\langle\vv^{(k)},\cpsi{i}\rangle\right).
% \]

% We expand each term in the above sum. We obtain,
% \begin{align*}
% q_{i}\left(\langle\vv^{(k)},\cpsi{i}\rangle\right)&=\sum_{n=0}^{\ell_i}\beta^{(i)}_{n}\left(\langle\vv^{(k)},\cpsi{i}\rangle\right)^{n}\\
% &=\sum_{n=0}^{\ell_i}\beta^{(i)}_{n}\sum_{(m_1,\ldots,m_n)\in [N_{\vell}^{(i)}]^{n}}v^{(k)}_{m_1}\ldots v^{(k)}_{m_n}\left(\cpsi{i}(\x)\right)_{m_1}\ldots \left(\cpsi{i}(\x)\right)_{m_n}\\
% &=\langle \vu^{(k)},\psi_{\ell_i}((\cpsi{i}(\x))\rangle=\langle\vu^{(k)},\cpsi{i+1}(\x)\rangle.\end{align*}
% The second inequality follows from expanding the equation. $\vu^{(k)}$ indexed by $(m_1,\ldots, m_n)\in [N^{(i)}_{\ell}]^n$ for $n\leq \ell_i$ has entries given by 
% $u^{(k)}_{(m_1,\ldots,m_n)}=\beta^{(i)}_n v^{(k)}_{m_1}\ldots v^{(k)}_{m_n}$. Putting things together, we obtain that
% \begin{align*}
%     (p_{i+1})_{j}(\x)&=\sum_{k=1}^{s_{i}}W^{(i+1)}_{jk}\cdot\langle\vu^{(k)},\cpsi{i+1}(\x)\rangle\\
%     &=\langle\sum_{k=1}^{s_i}W^{(i+1)}_{jk}\vu^{(k)},\cpsi{i+1}(\x)\rangle.
% \end{align*}
% Thus, we have proved that $(p_{i+1})_{j}$ is representable in $\mathcal{H}_{\cmkl{i+1}}$. We now prove that the norm of the representation is small. We have that 
% \begin{align*}
%     \twonorm{\sum_{k=1}^{s_i}W^{(i+1)}_{jk}\vu^{(k)}}\leq \norm{W^{(i+1)}}_1\max_{k\in [s_i]}\twonorm{\vu^{(k)}}\leq W\cdot\max_{k\in [s_i]}\twonorm{\vu^{(k)}}.
% \end{align*}
% We bound $\max_{k\in [s_i]}\twonorm{\vu^{(k)}}$. For any $k$, from the definition of $\vu^{(k)}$ and the inductive hypothesis, we have that 
% \begin{align*}
% \twonorm{\vu^{(k)}}^2&=\sum_{n=0}^{\ell_i}\left(\beta^{(i)}_{n}\right)^2\cdot\sum_{(m_1,\ldots,m_n)\in [N^{(i)}_{\vell}]^n}\prod_{j=1}^{n}\left(\vu^{(k)}_{m_j}\right)^2\\
% &=\sum_{n=0}^{\ell_i}\left(\beta^{(i)}_n\right)^2\twonorm{\vv^{(k)}}^{2n}\leq 2^{O(\ell_i)}\cdot \twonorm{\vv^{(k)}}^{2\ell_{i}}
% \end{align*}
% We analyse the case $i=1$ and $i>1$ separately. When $i=1$, we have $2^{O(\ell_1)}\twonorm{\vv^{(k)}}^{2\ell_1}\leq (2\normtwoinf{W^{(1)}})^{O(\ell_1)}$ from the bound on the base case. When $i>1$, we have 
% \begin{align*}
%      \twonorm{\sum_{k=1}^{s_i}W^{(i+1)}_{jk}\vu^{(k)}}^2&\leq W^2 2^{O(\ell_i)}\twonorm{\vv^{(k)}}^{2\ell_i}\\
%      &\leq W^2 2^{O(\ell_i)}\left((2\normtwoinf{W^{(1)}})^{O(\prod_{n=1}^{i-1}\ell_{n})}\cdot W^{O(\prod_{n=2}^{i-1}\ell_n)}\right)^{2\ell_i}\\
%      &\leq (2\normtwoinf{W^{(1)}})^{O(\prod_{n=1}^{i}\ell_{n})}\cdot W^{O(\prod_{n=2}^{i}\ell_n)}
% \end{align*} which completes the induction. We are ready to calculate the bound on the degree. 

% We have $\ell_1=O(Rt\normtwoinf{W^{(1)}}\log(RW/\epsilon))$. Also, for $i>1$, we have $\ell_{i}=O(Wt\log(W/\epsilon))$. Thus, the total degree is 
% $\ell\leq\prod_{i=1}^{t-1}\ell_i=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$. The square of the norm of the kernel representation is bounded by $B$ where
% \[
% B\leq (2\normtwoinf{W^{(1)}})^{\ell}\cdot W^{O\left(W^{t-2}(t\log(W/\epsilon)^{t-2}\right)}.
% \]

% This concludes the proof.
% \end{proof}

% \subsection{Applications for Bounded Distributions}
% We first state and prove our end to end results on TDS learning Sigmoid and Lipschitz nets over bounded marginals that are $C$-hypercontractive for some constant $C$. 

% \begin{theorem}[TDS Learning for Nets with Sigmoid Activation]
% \label{thm:tds_learning_sigmoid_appendix}
% Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with sigmoid activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
% \begin{enumerate}
%     \item $\Dtrainx$ is bounded within $\{\x:\twonorm{\x}\leq R\}$ and is $C$-hypercontractive for $R,C\geq 1$,
%     \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
% \end{enumerate}
% Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is \[\poly\left(d,\frac{1}{\epsilon},C^{\ell}, M,\log(1/\delta)^{\ell},(2R)^{2^t\cdot \ell},(2\normtwoinf{W^{(1)}})^{\ell}\cdot W^{O\left((Wt\log(W/\epsilon))^{t-2}\right)}\right)\] where $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$.
% \end{theorem}
% \begin{proof}
% From \Cref{thm:approx_sigmoid_nets}, we have that $\mathcal{F}$ is $\Paren{\epsilon, (2\normtwoinf{W^{(1)}})^{\ell} W^{O\left(W^{t-2}(t\log(W/\epsilon)^{t-2}\right)}}$-approximately represented within radius $R$ with respect to $\cmkl{t}$, where $\vell$ is a degree vector whose product is equal to $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$. Also, from \Cref{lem:composed_multinomial_properties}, we have that $A\coloneq \sup_{\twonorm{\x}\leq R}\cmkl{t}(\x,\x)\leq (2R)^{2^t\ell}$. From \Cref{lem:composed_multinomial_properties}, the entries of the kernel can be computed in $\poly(d,\ell)$ time and from \Cref{lem:hc_implies_kernelhc}, we have that $\Dtrainx$ is $\left(\cmkl{t},C,\ell\right)$ hypercontractive. Now, we obtain the result by applying \Cref{theorem:tds-via-kernels}.
% \end{proof}

% The following corollary on TDS learning two layer sigmoid networks in polynomial time readily follows. 
% \begin{corollary}
% \label{clry:polytime_tds_sigmoid_appendix}
%     Let $\mathcal{F}$ on $\R^{d}$ be the class of two-layer neural networks with weight matrices $\W=(W^{(1)},W^{(2)})$ and sigmoid activations. Let $\normtwoinf{W^{(1)}}\leq O(1)$ and $\norm{\W}_1\leq W$. Suppose the training and test distributions satisfy the assumptions from \Cref{thm:tds_learning_sigmoid_appendix} with $R=O(1)$. Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression setting up to excess error $\epsilon$ and probability of failure $0.1$ in time and sample complexity $\poly(d,1/\epsilon,W,M)$.
% \end{corollary}
% \begin{proof}
%     The proof immediately follows from \Cref{thm:tds_learning_sigmoid_appendix} by setting $t=2$ and the other parameters to the appropriate constants.
% \end{proof}


% \begin{theorem}[TDS Learning for Nets with Lipschitz Activation]
% \label{thm:tds_learning_lipschitz_appendix}
% Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with $L$-Lipschitz  activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
% \begin{enumerate}
%     \item $\Dtrainx$ is bounded within $\{\x:\twonorm{\x}\leq R\}$ and is $C$-hypercontractive for $R,C\geq 1$,
%     \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
% \end{enumerate}
% Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is $\poly\left(d,\frac{1}{\epsilon},C^{\ell}, M,\log(1/\delta)^{\ell},(2R(k+\ell))^{O(\ell)}\right)$, where $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$. In particular, when $k=1$, we have that the time and sample complexity is $\poly(d,\frac{1}{\epsilon},C^{\ell},M,\log(1/\delta)^{\ell},(2R)^{O(\ell)})$ where $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}R/\epsilon\right).$
% \end{theorem}
% \begin{proof}
%     From \Cref{thm:approx_lipschitz_nets}, for $k>1$ we have that $\mathcal{F}$ is $(\epsilon, (k+\ell)^{O(\ell)})$-approximately represented within radius $R$ w.r.t $\cmkl{1}$ where $\ell$ is a degree vector whose product is $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$. For $k=1$, we have that we have that $\mathcal{F}$ is $(\epsilon, 2^{O(\ell)})$-approximately represented within radius $R$ w.r.t $\cmkl{1}$ where $\vell$ is a degree vector whose product is equal to $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}R/\epsilon\right)$. Also, from \Cref{lem:composed_multinomial_properties}, we have that $A\coloneq \sup_{\twonorm{\x}\leq R}\cmkl{t}(\x,\x)\leq (2R)^{O(\ell)}$. From \Cref{lem:composed_multinomial_properties}, the entries of the kernel can be computed in $\poly(d,\ell)$ time and from \Cref{lem:hc_implies_kernelhc}, we have that $\Dtrainx$ is $\left(\cmkl{1},C,\ell\right)$ hypercontractive. Now, we obtain the result by applying \Cref{theorem:tds-via-kernels}.
% \end{proof}

% The above theorem implies the following corollary about TDS learning the class of ReLUs. 

% \begin{corollary}
% \label{clry:polytime_tds_relu_appendix}
%     Let $\mathcal{F}=\{\x\rightarrow \max(0,\vw\cdot \x):\twonorm{\vw}=1\}$ on $\R^{d}$ be the class of ReLU functions with unit weight vectors. Suppose the training and test distributions satisfy the assumptions from \Cref{thm:tds_learning_lipschitz_appendix} with $R=O(1)$. Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression setting up to excess error $\epsilon$ and probability of failure $0.1$ in time and sample complexity $\poly(d,2^{O(1/\epsilon)},M)$.
% \end{corollary}
% \begin{proof}
%     The proof immediately follows from \Cref{thm:tds_learning_lipschitz_appendix} by setting $t=2,\W=(\vw)$ and the activation to be the ReLU function. 
% \end{proof}

% In particular, this implies that the class of ReLUs is TDS learnable in polynomial time when $\epsilon<O(1/\log d)$.




% \subsection{Applications for Unbounded Distributions}\label{sec:tds_uniform_appendix}

% We are now ready to state our theorem for TDS learning neural networks with sigmoid activations.
% \begin{theorem}[TDS Learning for Nets with Sigmoid Activation and Strictly Subexponential Marginals]
% \label{thm:tds_learning_sigmoid_subexp_appendix}
% Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with sigmoid activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
% \begin{enumerate}
%     \item $\Dtrainx$ is $\gamma$-strictly subexponential,
%     \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
% \end{enumerate}
% Then, \Cref{algorithm:uniform-approx} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is at most \[\poly(d^{s},\log(1/\delta)^s),\] where $s=\left(k\log M\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)^{O(\frac{1}{\gamma})}.$
% \end{theorem}
% \begin{proof}
% From \Cref{thm:approx_sigmoid_nets}, we have that $\mathcal{F}$ there is an $(\epsilon, R)$-uniform approximation polynomial for $f$ with degree $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$. Here, let $g_{\mathcal{F}}(\epsilon)\coloneq  (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}$. We also have that $r=\sup_{\twonorm{\x}\leq R,f\in \mathcal{F}}|f(\x)|\leq \poly(Rk\normtwoinf{W^{(1)}}W^{t-2})$ from the Lipschitzness of the sigmoid nets (\Cref{lem:neural_net_lipschitz}) and the fact that the sigmoid evaluated at $0$ has value $1$. The theorem now directly follows from \Cref{theorem:tds-via-uniform}.
% \end{proof}

% We now state our theorem on TDS learning neural networks with arbitrary Lipschitz activations.


% \begin{theorem}[TDS Learning for Nets with Lipschitz Activation with strictly subexponential marginals]
% \label{thm:tds_learning_lipschitz_subexp_appendix}
% Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with $L$-Lipschitz  activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
% \begin{enumerate}
%     \item $\Dtrainx$ is $\gamma$-strictly subexponential,
%     \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
% \end{enumerate}
% Then, \Cref{algorithm:uniform-approx} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is at most \[\poly(d^{s},\log(1/\delta^s),\] where $s=\left(k\log M\cdot\normtwoinf{W^{(1)}}(WL)^{t-1}/\epsilon\right)^{O(\frac{1}{\gamma})}$.
% \end{theorem}
% \begin{proof}
%     From \Cref{thm:approx_lipschitz_nets}, we have that $\mathcal{F}$ there is an $(\epsilon, R)$-uniform approximation polynomial for $f$ with degree $\ell=O\left(Rk\sqrt{k}\cdot \normtwoinf{W^{(1)}}(WL)^{t-1}/\epsilon\right)$. Here, let $g_{\mathcal{F}}(\epsilon)\coloneq  k\sqrt{k}\normtwoinf{W^{(1)}}(WL)^{t-1}/\epsilon$. We also have that $r=\sup_{\twonorm{\x}\leq R,f\in \mathcal{F}}|f(\x)|\leq \poly(Rk\normtwoinf{W^{(1)}}W^{t-2})$ from the Lipschitz constant(\Cref{lem:neural_net_lipschitz}) and the fact that the each individual activation has value at most $1$ when evaluated at $0$ (see \Cref{def:Neural Network}. The theorem now directly follows from \Cref{theorem:tds-via-uniform}.
% \end{proof}

\bibliographystyle{alpha}
\bibliography{refs}

\input{appendix.tex}


\end{document}
