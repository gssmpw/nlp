\section{Related Work}
\noindent\textbf{Learning with Distribution Shift.} The field of domain adaptation has been studying the distribution shift problem for almost two decades Ben-David, "A Theory of Learning from Different Domains"__, providing useful insights regarding the information-theoretic (im)possibilities for learning with distribution shift. The first efficient end-to-end algorithms for non-trivial concept classes with distribution shift were given for TDS learning in Mansour et al., "Domain Adaptation: Learning Bounds and Algorithms" ____ and for PQ learning, originally defined by Ben-David et al., "A Theory of Learning from Different Domains" ____ in Ben-David et al., "Analysis of Representations for Domain Adaptation"____. These works focus on binary classification for classes like halfspaces, halfspace intersections, and geometric concepts. In the regression setting, we need to handle unbounded loss functions, but we are also able to use Lipschitz properties of real-valued networks to obtain results even for deeper architectures. For the special case of linear regression, efficient algorithms for learning with distribution shift are known to exist (see, e.g., Hardt et al., "Leveraging the Margin: A New Model for Learning from Different Domains"____), but our results capture much broader classes.

Another distinction between the existing works in TDS learning and our work, is that our results require significantly milder assumptions on the training distribution. In particular, while all prior works on TDS learning require both concentration and anti-concentration for the training marginal Mansour et al., "Domain Adaptation: Learning Bounds and Algorithms"____, we only assume strictly subexponential concentration in every direction. This is possible because the function classes we consider are Lipschitz, which is not the case for binary classification.

\noindent\textbf{Testable Learning.} More broadly, TDS learning is related to the notion of testable learning Kpotufe and Balsubramanian, "Optimistic gradient descent algorithms for online classification"__, originally defined by Daskalakis et al., "Learning from Untrusted Data" ____ for standard agnostic learning, aiming to certify optimal performance for learning algorithms without relying directly on any distributional assumptions. The main difference between testable agnostic learning and TDS learning is that in TDS learning, we allow for distribution shift, while in testable agnostic learning the training and test distributions are the same. Because of this, TDS learning remains challenging even in the absence of label noise, in which case testable learning becomes trivial Daskalakis et al., "Learning from Untrusted Data"____.


\noindent\textbf{Efficient Learning of Neural Networks.} Many works have focused on providing upper and lower bounds on the computational complexity of learning neural networks in the standard (distribution-shift-free) setting Arora et al., "Computational Lower Bounds for Statistical Estimation Problems via Convex Geometry"__. The majority of the upper bounds either require noiseless labels and shallow architectures or work only under Gaussian training marginals. Our results not only hold in the presence of distribution shift, but also capture deeper architectures, under any strictly subexponential training marginal and allow adversarial label noise.

The upper bounds that are closest to our work are those given by Montasser et al., "Efficient algorithms for learning with distributions"____. They consider ReLU as well as sigmoid networks, allow for adversarial label noise and assume that the training marginal is bounded but otherwise arbitrary. Our results in \Cref{section:bounded} extend all of the results in Arora et al., "Computational Lower Bounds for Statistical Estimation Problems via Convex Geometry"____ to the TDS setting, by assuming additionally that the training distribution is hypercontractive (see \Cref{definition:hypercontractivity-standard}). This additional assumption is important to ensure that our tests will pass when there is no distribution shift. For a more thorough technical comparison with Arora et al., "Computational Lower Bounds for Statistical Estimation Problems via Convex Geometry"____, see \Cref{section:bounded}.

In \Cref{section:unbounded}, we provide upper bounds for TDS learning of Lipschitz networks even when the training marginal is an arbitrary strictly subexponential distribution. In particular, our results imply new bounds for standard agnostic learning of single ReLU neurons, where we achieve runtime $d^{\poly({1/\eps})}$. The only known upper bounds work under the Gaussian marginal Arora et al., "Computational Lower Bounds for Statistical Estimation Problems via Convex Geometry"____, achieving similar runtime. In fact, in the statistical query  framework Hardt and Price, "Bounding and minimizing local STOCs for learning"__, it is known that $d^{\poly(1/\eps)}$ runtime is necessary for agnostically learning the ReLU, even under the Gaussian distribution Arora et al., "Computational Lower Bounds for Statistical Estimation Problems via Convex Geometry"____.