\section{Related Work}
\noindent\textbf{Learning with Distribution Shift.} The field of domain adaptation has been studying the distribution shift problem for almost two decades ____, providing useful insights regarding the information-theoretic (im)possibilities for learning with distribution shift. The first efficient end-to-end algorithms for non-trivial concept classes with distribution shift were given for TDS learning in ____ and for PQ learning, originally defined by ____, in ____. These works focus on binary classification for classes like halfspaces, halfspace intersections, and geometric concepts. In the regression setting, we need to handle unbounded loss functions, but we are also able to use Lipschitz properties of real-valued networks to obtain results even for deeper architectures. For the special case of linear regression, efficient algorithms for learning with distribution shift are known to exist (see, e.g., ____), but our results capture much broader classes. 

Another distinction between the existing works in TDS learning and our work, is that our results require significantly milder assumptions on the training distribution. In particular, while all prior works on TDS learning require both concentration and anti-concentration for the training marginal ____, we only assume strictly subexponential concentration in every direction. This is possible because the function classes we consider are Lipschitz, which is not the case for binary classification.

\noindent\textbf{Testable Learning.} More broadly, TDS learning is related to the notion of testable learning ____, originally defined by ____ for standard agnostic learning, aiming to certify optimal performance for learning algorithms without relying directly on any distributional assumptions. The main difference between testable agnostic learning and TDS learning is that in TDS learning, we allow for distribution shift, while in testable agnostic learning the training and test distributions are the same. Because of this, TDS learning remains challenging even in the absence of label noise, in which case testable learning becomes trivial ____.


\noindent\textbf{Efficient Learning of Neural Networks.} Many works have focused on providing upper and lower bounds on the computational complexity of learning neural networks in the standard (distribution-shift-free) setting ____. The majority of the upper bounds either require noiseless labels and shallow architectures or work only under Gaussian training marginals. Our results not only hold in the presence of distribution shift, but also capture deeper architectures, under any strictly subexponential training marginal and allow adversarial label noise.

The upper bounds that are closest to our work are those given by ____. They consider ReLU as well as sigmoid networks, allow for adversarial label noise and assume that the training marginal is bounded but otherwise arbitrary. Our results in \Cref{section:bounded} extend all of the results in ____ to the TDS setting, by assuming additionally that the training distribution is hypercontractive (see \Cref{definition:hypercontractivity-standard}). This additional assumption is important to ensure that our tests will pass when there is no distribution shift. For a more thorough technical comparison with ____, see \Cref{section:bounded}.

In \Cref{section:unbounded}, we provide upper bounds for TDS learning of Lipschitz networks even when the training marginal is an arbitrary strictly subexponential distribution. In particular, our results imply new bounds for standard agnostic learning of single ReLU neurons, where we achieve runtime $d^{\poly({1/\eps})}$. The only known upper bounds work under the Gaussian marginal ____, achieving similar runtime. In fact, in the statistical query  framework ____, it is known that $d^{\poly(1/\eps)}$ runtime is necessary for agnostically learning the ReLU, even under the Gaussian distribution ____. 


% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.