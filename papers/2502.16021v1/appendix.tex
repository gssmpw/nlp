
\appendix

\section{Proof of Multiplicative Spectral Concentration Lemma}\label{appendix:tds-kernels}

Here, we restate and prove the multiplicative spectral concentration lemma (\Cref{lemma:relative-error-kernel-matrix}).

\begin{lemma}[Multiplicative Spectral Concentration, Lemma B.1 in \cite{goel2024tolerant}, modified]\label{lemma:appendix-relative-error-kernel-matrix}
    Let $\Dtrainx$ be a distribution over $\R^d$ and $\phi:\R^d \to \R^m$ such that $\Dtrainx$ is $(\phi,C,\ell)$-hypercontractive for some $C,\ell \ge 1$. Suppose that $S$ consists of $N$ i.i.d. examples from $\Dtrainx$ and let $\Phi = \E_{\x\sim\Dtrainx}[\phi(\x)\phi(\x)^\top]$, and $\hat\Phi = \frac{1}{N}\sum_{\x\in S}\phi(\x)\phi(\x)^\top$. For any $\eps,\delta\in(0,1)$, if $N\ge \frac{64 Cm^2}{\eps^2}(4C \log_2(\frac{4}{\delta}))^{4\ell+1}$, then with probability at least $1-\delta$, we have that
    \[
        \text{For any }\va\in\R^m: \va^\top \hat\Phi \va \in [{(1-\eps)} \va^\top \Phi\va, (1+\eps)\va^\top \Phi\va]
    \]
\end{lemma}

\begin{proof}[Proof of \Cref{lemma:relative-error-kernel-matrix}]
    Let $\Phi = UDU^\top$ be the compact SVD of $\Phi$ (i.e., $D$ is square with dimension equal to the rank of $\Phi$ and $U$ is not necessarily square). Note that such a decomposition exists (where the row and column spaces are both spanned by the same basis $U$), because $\Phi = \Phi^\top$, by definition. Moreover, note that $UU^T$ is an orthogonal projection matrix that projects points in $\R^m$ on the span of the rows of $\Phi$. We also have that, $U^\top U = I$.

    \newcommand{\Phihalfinv}{\Phi^{\frac{\dagger}{2}}}

    Consider $\Phi^\dagger = UD^{-1}U^\top$ and $\Phihalfinv = UD^{-\frac{1}{2}}U^\top$. Our proof consists of two parts. We first show that it is sufficient to prove that $\|\Phihalfinv\Phi\Phihalfinv - \Phihalfinv\hat\Phi\Phihalfinv\|_2 \le \eps$ with probability at least $1-\delta$ and then we give a bound on the probability of this event.

    \begin{claim}
        Suppose that for $\mA = \Phihalfinv\Phi\Phihalfinv - \Phihalfinv\hat\Phi\Phihalfinv$ we have $\|\mA\|_2 \le \eps$. Then, for any $\va\in\R^m$: 
        \[ 
            \va^\top \hat\Phi \va \in [{(1-\eps)} \va^\top \Phi\va, (1+\eps)\va^\top \Phi\va]
        \]
    \end{claim}
    \begin{proof}
        Let $\va \in \R^m$, $\va_+ = UU^\top \va$, and $\va_0 = (I-UU^\top) \va$ (i.e., $\va = \va_0 + \va_+$, where $\va_0$ is the component of $\va$ lying in the nullspace of $\Phi$). We have that $\va^\top \Phi \va = \va^\top_+ \Phi \va_+$.

        Moreover, for $\va_0$, we have that $0=\va_0^\top \Phi \va_0 = \E_{\x\sim \Dtrainx}[(\phi(\x)^\top \va_0)^2]$ and, hence, $\phi(\x)^\top \va_0 = 0$ almost surely over $\Dtrainx$. Therefore, we also have $\va_0^\top \hat\Phi \va_0 = \frac{1}{N}\sum_{\x\in S}(\phi(\x)^\top \va_0)^2 = 0$, with probability $1$. Therefore, $\va^\top \hat\Phi \va = \va_+^\top \hat\Phi \va_+$. 

        Observe, now, that $\Phi^{\frac{1}{2}}\Phihalfinv = UD^{\frac{1}{2}}U^\top UD^{-\frac{1}{2}}U^\top = UU^\top$ and, hence, $\Phi^{\frac{1}{2}}\Phihalfinv\va_+ = (UU^\top)^2\va = UU^\top \va = \va_+$, because $UU^\top$ is a projection matrix. Overall, we obtain the following
        \begin{align*}
            \va^\top \hat\Phi \va &= \va^\top \Phi \va + \va_+^\top (\hat\Phi-\Phi) \va_+ \\
            &= \va^\top \Phi \va + \va_+^\top\Phi^{\frac{1}{2}} (\Phihalfinv\hat\Phi\Phihalfinv-\Phihalfinv\Phi\Phihalfinv) \Phi^{\frac{1}{2}}\va_+ \\
            &= \va^\top \Phi \va + \va_+^\top\Phi^{\frac{1}{2}} A \Phi^{\frac{1}{2}}\va_+
        \end{align*}
        Since $\|\mA\|_2 \le \eps$ and $\Phi^{\frac{1}{2}}\Phi^{\frac{1}{2}} = \Phi$, we have that $|\va_+^\top\Phi^{\frac{1}{2}} A \Phi^{\frac{1}{2}}\va_+| \le \eps |\va_+^\top \Phi \va_+| = \eps |\va^\top \Phi \va|$, which concludes the proof of the claim.
    \end{proof}

    It remains to show that for the matrix $\mA$ defined in the previous claim, we have $\|\mA\|_2 \le \eps$ with probability at least $1-\delta$. The randomness of $\mA$ depends on the random choice of $S$ from $\Dtrainx^{\otimes m}$. In the rest of the proof, therefore, consider all probabilities and expectations to be over $S \sim \Dtrainx^{\otimes m}$. We have the following for $t = \log_2(4/\delta)$.
    \begin{align*}
        \Pr[\|\mA\|_2 > \eps] &\le \pr[\|\mA\|_F > \eps] \le \frac{\E[\|\mA\|_F^{2t}]}{\eps^{2t}} 
    \end{align*}
    We will now bound the expectation of $\E[\|\mA\|_F^{2t}]$. To this end, we define $\va_i = \Phihalfinv \ve_i \in \R^m$ for $i\in[m]$. We have the following, by using Jensen's inequality appropriately.
    \begin{align*}
        \E[\|\mA\|_F^{2t}] &= \E\Bigr[\Bigr(\sum_{i,j\in[m]} (\va_i^\top \Phi \va_j - \va_i^\top \hat\Phi \va_j)^2 \Bigr)^t\Bigr] \\
        &\le m^{2(t-1)} \sum_{i,j\in[m]}\E[(\va_i^\top \Phi \va_j - \va_i^\top \hat\Phi \va_j)^{2t}] \\
        &\le m^{2t} \max_{i,j\in[m]}\E[(\va_i^\top \Phi \va_j - \va_i^\top \hat\Phi \va_j)^{2t}]
    \end{align*}
    In order to bound the term above, we may use Marcinkiewicz-Zygmund inequality (see \cite{FERGER201496marcinkiewicz}) to exploit the independence of the samples in $S$ and obtain the following.
    \begin{align*}
        \E[(\va_i^\top \Phi \va_j - \va_i^\top \hat\Phi \va_j)^{2t}] &\le \frac{2(4t)^t}{N^t} \E_{\x\sim \Dtrainx}[(\va_i^\top \Phi \va_j - \va_i^\top \phi(\x)\phi(\x)^\top \va_j)^{2t}] \\
        &\le \frac{2(4t)^t}{N^t} \bigr( 2^{2t} (\va_i^\top \Phi \va_j)^{2t} +2^{2t} \E_{\x\sim \Dtrainx}[(\va_i^\top \phi(\x)\phi(\x)^\top \va_j)^{2t}] \bigr)
    \end{align*}
    We now observe that $\E_{\x\sim \Dtrainx}[\va_i^\top \phi(\x)\phi(\x)^\top \va_j] = \va_i^\top \Phi \va_j = \ve_i^\top \Phihalfinv\Phi \Phihalfinv \ve_j = \ve_i^\top UU^T \ve_j$, which is at most equal to $1$. Therefore, we have $\E_{\x\sim \Dtrainx}[(\va_i^\top \phi(\x))^2] \le 1$ and, by the hypercontractivity property (which we assume to be with respect to the standard inner product in $\R^m$), we have $\E_{\x\sim \Dtrainx}[(\va_i^\top \phi(\x))^{4t}] \le (4Ct)^{4\ell t}$. We can bound $\E_{\x\sim \Dtrainx}[(\va_i^\top \phi(\x)\phi(\x)^\top \va_j)^{2t}]$ by applying the Cauchy-Schwarz inequality and using the bound for $\E_{\x\sim \Dtrainx}[(\va_i^\top \phi(\x))^{4t}]$. In total, we have the following bound.
    \[
        \pr[\|\mA\|_2 > \eps] \le 4\Bigr(\frac{16 m^2 t (4Ct)^{4\ell}}{N\eps^2}\Bigr)^t
    \]
    We choose $N$ such that $\frac{16 m^2 t (4Ct)^{4\ell}}{N\eps^2} \le \frac{1}{2}$ and $t = \log_2(4/\delta)$ so that the bound is at most $\delta$.
\end{proof}

\section{Moment Concentration of Subexponential Distributions}

We prove the following bounds on the moments of subexponential distributions, which allows us to control error outside the region of good approximation.
\begin{fact}[see \cite{vershynin2018high}] \label{fact:moment_bound}
    Let $\gD$ on $\R^d$ be a $\gamma$-strictly subexponential distribution. Then for all $\vw \in \R^d, \Norm{\vw} = 1, t \ge 0, p \ge 1$, there exists a constant $C'$ such that \[\E_{x \sim \gD}[|\inprod{\vw}{\x}|^p] \le (C'p)^{\frac{p}{1+\gamma}}.\] In fact, the two conditions are equivalent.
\end{fact}

We use the following bounds on the concentration of subexponential moments in the analysis of our algorithm. This will be useful in showing the sample complexity $N$ required in order for the empirical moments of the sample $S$ concentrate around the moments of the training marginal $\Dtrainx$.
\begin{lemma}[Moment Concentration of Subexponential Distributions]\label{lemma:moment-concentration}
    Let $\Dtrainx$ be a distribution over $\R^d$ such that for any $\vw\in\R^d$ with $\|\vw\|_2 =1$ and any $t\in\sN$ we have $\E_{\x\sim \Dtrainx}[|\vw\cdot \x|^t] \le (Ct)^{t}$ for some $C\ge 1$. For $\alpha=(\alpha_i)_{i\in [d]}\in \sN^d$, we denote with $\x^\alpha$ the quantity $\x^\alpha = \prod_{i=1}^d x_i^{\alpha_i}$, where $\x = (x_i)_{i\in [d]}$. Then, for any $\Delta, \delta\in(0,1)$, if $S$ is a set of at least $N = \frac{1}{\Delta^2}{(Cc)^{4\ell} \ell^{8\ell +1} (\log(20d/\delta))^{4\ell+1}}$ i.i.d. examples from $\Dtrainx$ for some sufficiently large universal constant $c\ge 2$, we have that with probability at least $1-\delta$, the following is true.
    \[
        \text{For any $\alpha\in\sN^d$ with $\|\alpha\|_1 \le 2\ell$ we have } |\E_{\x\sim S}[\x^\alpha] - \E_{\x\sim \Dtrainx}[\x^\alpha]| \le \Delta.
    \]
\end{lemma}

\begin{proof}
    Let $\alpha=(\alpha_i)_{i\in[d]}\in\sN^d$ with $\|\alpha\|_1 \le 2\ell$. Consider the random variable $X = \frac{1}{\mtrain}\sum_{\x\in S}\x^\alpha = \frac{1}{\mtrain}\sum_{\x\in S}\prod_{i\in[d]}x_i^{\alpha_i}$. We have that $\E[X] = \E_{\x\sim \Dtrainx}[\x^\alpha]$ and also the following.
    \begin{align*}
        \pr[|X-\E[X]| > \Delta] &\le \frac{\E[(X-\E[X])^{2t}]}{\Delta^{2t}} \\
        &\le \frac{2(4t)^t}{(N\Delta^2)^t} \E[(\x^\alpha-\E[\x^\alpha])^{2t}]
    \end{align*}
    where the last inequality follows from the Marcinkiewiczâ€“Zygmund inequality (see \cite{FERGER201496marcinkiewicz}). We have that $\E[(\x^\alpha-\E[\x^\alpha])^{2t}] \le 4^t \E[(\x^{\alpha})^{2t}]$. Since $\|\alpha\|_1\le 2\ell$, we have that $\E[(\x^{\alpha})^{2t}] \le \sup_{\|\vw\|_2=1}[\E[(\vw\cdot \x)^{4t \ell}]] \le (4Ct\ell)^{4t\ell}$, which yields the desired result, due to the choice of $N$ and after a union bound over all the possible choices of $\alpha$ (at most $d^{2\ell}$).
\end{proof}


% We are now ready to prove the main theorem, which we restate here for convenience.

% \begin{theorem}[TDS Learning via the Kernel Method]\label{theorem:appendix-tds-via-kernels}
%     Suppose that $\gF\subseteq\{\R^d\to \R\}$, the training and test distributions $\Dtrain$, $\Dtest$ over $\R^d\times \R$, are such that the following are true for $A,B,C,M,\ell \ge 1$ and $\eps\in(0,1)$.
%     \begin{enumerate}
%         \item $\gF$ is $(\eps,B)$-approximately represented within radius $R$ w.r.t. a PDS kernel $\Kernel:\R^d\times \R^d \to \R$, for some $\eps\in(0,1)$ and $B,R\ge 1$ and let $A = \sup_{\x:\|\x\|_2 \le R}\Kernel(\x,\x)$.
%         \item The training marginal $\Dtrainx$ (1) is bounded within $\{\x: \|\x\|_2 \le R\}$ and (2) is $(\Kernel,C,\ell)$-hypercontractive for some $C,\ell \ge 1$.
%         \item The training and test labels are both bounded in $[-M,M]$ for some $M\ge 1$. 
%     \end{enumerate}
%     Then, \Cref{algorithm:tds-via-kernels} learns the class $\gF$ in the TDS regression setting up to excess error $5\epsilon$ and probability of failure $\delta$. The time complexity is $O(T) \cdot \poly(d,\frac{1}{\eps}, (\log(1/\delta))^\ell, A, B, C^\ell, 2^\ell, M)$, where $T$ is the evaluation time of $\Kernel$.
% \end{theorem}



% \begin{proof}[Proof of \Cref{theorem:tds-via-kernels}]
%     Consider the reference feature map $\phi: \R^d \to \R^{2m}$ with $\phi(\x) = (\Kernel(\x,\vz))_{\vz\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'}$. Let $f^*= \arg \min_{f\in\gF} [\gL_{\Dtrain}(f)+ \gL_{\Dtest}(f)]$ and $f_\opt = \arg \min_{f\in\gF} [\gL_{\Dtrain}(f)]$. By \Cref{assumption:bounded}, we know that there are functions $p^*, p_\opt :\R^d \to \R$ with $p^*(\x) = \inprod{\vv^*}{\psi(\x)}$ and $p_\opt = \inprod{\vv_\opt}{\psi(\x)}$, that uniformly approximate $f^*$ and $f_\opt$ within the ball of radius $R$, i.e., $\sup_{\x:\|\x\|_2\le R}|f^*(\x) - p^*(\x)| \le \eps$ and $\sup_{\x:\|\x\|_2\le R}|f_\opt(\x) - p_\opt(\x)| \le \eps$. Moreover, $\inprod{\vv^*}{\vv^*}, \inprod{\vv_\opt}{\vv_\opt} \le B$.

%     By \Cref{proposition:representer-theorem}, there is $\va^*\in \R^{2m}$ such that for $\tilde{p}^*:\R^d\to \R$ with $\tilde{p}^*(\x) = ({\va^*})^\top{\phi(\x)}$ we have $\|f^*-\tilde{p}^*\|_{S_{\mathrm{ref}}} \le 3\eps/2$ and $\|f^*-\tilde{p}^*\|_{S_{\mathrm{ref}}'} \le 3\eps/2$. Let $\mK$ be a matrix in $\R^{2m\times 2m}$ such that $\mK_{\vz,\vw} = \Kernel(\vz,\vw)$ for $\vz,\vw\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'$. We additionally have that $(\va^*)^\top \mK \va^* \le B$. Therefore, for any $\x\in\R^d$ we have 
%     \begin{align*}
%         (\tilde{p}^*(\x))^2 &= \Bigr(\Bigr\langle{\sum_{\vz\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'} a^*_z \psi(\vz) }, {\psi(\x)}\Bigr\rangle\Bigr)^2 \\
%         &\le \Bigr\langle{\sum_{\vz\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'} a^*_z \psi(\vz) }, {\sum_{\vz\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'} a^*_z \psi(\vz)}\Bigr\rangle \cdot \inprod{\psi(\x)}{\psi(\x)} \\
%         &= (\va^*)^\top \mK \va^* \cdot \Kernel(\x,\x) \le B\cdot \Kernel(\x,\x)\,,
%     \end{align*}
%     where we used the Cauchy-Schwarz inequality. For $\x$ with $\|\x\|_2\le R$, we, hence, have $(\tilde{p}^*(\x))^2 \le AB$ (recall that $A = \max_{\|\x\|_2\le R}\Kernel(\x,\x)$).

%     Similarly, by applying the representer theorem (Theorem 6.11 in \cite{mohri2018foundations}) for $p_\opt$, we have that there exists $\va^\opt = (a^\opt_{\vz})_{\vz\in S_{\mathrm{ref}}}\in\R^{m}$ such that for $\tilde{p}_\opt:\R^d \to \R$ with $\tilde{p}_\opt(\x) = \sum_{\vz\in S_{\mathrm{ref}}} a^\opt_\vz \Kernel(\vz,\x)$ we have $\gL_{\bar{S}_{\mathrm{ref}}}(\tilde{p}_\opt) \le \gL_{\bar{S}_{\mathrm{ref}}}(p_\opt)$ and $\sum_{\vz,\vw\in S_{\mathrm{ref}}} a^\opt_\vz a^\opt_\vw \Kernel(\vz,\vw) \le B$. Since $\hat p$ in \Cref{algorithm:tds-via-kernels} is formed by solving a convex program whose search space includes $\tilde{p}_\opt$, we have
%     \begin{equation}\label{equation:phat-optimality}
%         \gL_{\bar{S}_{\mathrm{ref}}}(\hat{p}) \le \gL_{\bar{S}_{\mathrm{ref}}}(\tilde{p}_\opt) \le \gL_{\bar{S}_{\mathrm{ref}}}(p_\opt)
%     \end{equation}
%     In the following, we abuse the notation and consider $\hat\va$ to be a vector in $\R^{2m}$, by appending $m$ zeroes, one for each of the elements of $S'_{\mathrm{ref}}$. Note that we then have $\hat\va^\top \mK \hat\va \le B$, and, also, $(\hat p(\x))^2 \le A\cdot B$ for all $\x$ with $\|\x\|_2\le R$.

%     \paragraph{Soundness.} Suppose first that the algorithm has accepted. In what follows, we will use the triangle inequality of the norms to bound for functions $h_1,h_2,h_3$ the quantity $\|h_1 - h_2\|_{\Dtrain}$ by $\|h_1-h_3\|_{\Dtrain}+ \|h_2-h_3\|_\Dtrain$. We also use the inequality $\gL_\Dtrain(h_1) \le \gL_\Dtrain(h_2) + \|h_1-h_2\|_{\Dtrain}$, as well as the fact that $\|\clip_M \circ h_1 - \clip_M\circ h_2\|_\Dtrain \le \|\clip_M \circ h_1 -  h_2\|_\Dtrain \le \| h_1 - h_2\|_\Dtrain$. We bound the test error of the output hypothesis $h:\R^d \to [-M,M]$ of \Cref{algorithm:tds-via-kernels} as follows.
%     \begin{align*}
%         \gL_{\Dtest}(h) \le \| h - \clip_M\circ f^* \|_{\Dtestx} + \gL_\Dtest(f^*)
%     \end{align*}
%     Since $(h(\x) - \clip_M(f^*(\x)))^2 \le 4M^2$ for all $\x$ and the hypothesis $h$ does not depend on the set $S_{\mathrm{ref}}'$, by a Hoeffding bound and the fact that $m$ is large enough, we obtain that $\| h - \clip_M\circ f^* \|_{\Dtestx} \le \| h - \clip_M\circ f^* \|_{S_{\mathrm{ref}}'} + \eps/10$, with probability at least $1-\delta/10$.
%     Moreover, we have $\| h - \clip_M\circ f^* \|_{S_{\mathrm{ref}}'} \le \| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'} + \| \tilde{p}^* -  f^* \|_{S_{\mathrm{ref}}'}$. We have already argued that $\| \tilde{p}^* -  f^* \|_{S_{\mathrm{ref}}'} \le 3\eps/2$.

%     In order to bound the quantity $\| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'}$, we observe that while the function $h$ does not depend on $S_{\mathrm{ref}}'$, the function $\tilde{p}^*$ does depend on $S_{\mathrm{ref}}'$ and, therefore, standard concentration arguments fail to bound the $\| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'}$ in terms of $\| h - \clip_M\circ \tilde{p}^* \|_{\Dtestx}$. However, since we have clipped $\tilde{p}^*$, and $\tilde{p}^*$ is of the form $\inprod{\vv^*}{\psi}$, we may obtain a bound using standard results from generalization theory (i.e., bounds on the Rademacher complexity of kernel-based hypotheses like Theorem 6.12 in \cite{mohri2018foundations} and uniform convergence bounds for classes with bounded Rademacher complexity under Lipschitz and bounded losses like Theorem 11.3 in \cite{mohri2018foundations}). In particular, we have that with probability at least $1-\delta/10$
%     \[
%         \| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'} \le \| h - \clip_M\circ \tilde{p}^* \|_{\Dtestx} + \eps/10
%     \]
%     The corresponding requirement for $m = |S_{\mathrm{ref}}'|$ is determined by the bounds on the Lipschitz constant of the loss function $(y,t)\mapsto (y-\clip_M(t))^2$, with $y\in [-M,M]$ and $t\in\R$, which is at most $5M$, the overall bound on this loss function, which is at most $4M^2$, as well as the bounds $A = \max_{\x:\|\x\|_2\le R}\Kernel(\x,\x)$ and $(\va^*)^\top \mK \va \le B$ (which give bounds on the Rademacher complexity).

%     By applying the Hoeffding bound, we are able to further bound the quantity $\| h - \clip_M\circ \tilde{p}^* \|_{\Dtestx}$ by $ \| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ver}}'} + \eps/10$, with probability at least $1-\delta$. We have effectively managed to bound the quantity $\| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ref}}'}$ by $\| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ver}}'}+\eps/5$. This is important, because the set $S_{\mathrm{ver}}'$ is a fresh set of examples and, therefore, independent from $\tilde{p}$. Our goal is now to use the fact that our spectral tester has accepted. We have the following for the matrix $\hat\Phi' = (\hat\Phi'_{\vz,\vw})_{\vz,\vw\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}'}$ with $\hat\Phi'_{\vz,\vw} = \frac{1}{N}\sum_{\x\in S_{\mathrm{ver}}'}\Kernel(\x,\vz)\Kernel(\x,\vw)$.
%     \begin{align*}
%         \| h - \clip_M\circ \tilde{p}^* \|_{S_{\mathrm{ver}}'}^2 &\le \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ver}}'}^2 \\
%         &= (\hat\va - \va^*)^\top \hat\Phi' (\hat\va-\va^*)
%     \end{align*}
%     Since our test has accepted, we know that $(\hat\va - \va^*)^\top \hat\Phi' (\hat\va-\va^*) \le (1+\rho)(\hat\va - \va^*)^\top \hat\Phi (\hat\va-\va^*)$, for the matrix $\hat\Phi = (\hat\Phi_{\vz,\vw})_{\vz,\vw\in S_{\mathrm{ref}}\cup S_{\mathrm{ref}}}$ with $\hat\Phi_{\vz,\vw} = \frac{1}{N}\sum_{\x\in S_{\mathrm{ver}}}\Kernel(\x,\vz)\Kernel(\x,\vw)$. We note here that having a multiplicative bound of this form is important, because we do not have any upper bound on the norms of $\hat\va$ and $\va^*$. Instead, we only have bounds on distorted versions of these vectors, e.g., on $\hat\va^\top \mK \hat\va$, which does not imply any bound on the norm of $\hat\va$, because $\mK$ could have very small singular values.

%     Overall, we have that $ \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ver}}'} - \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ver}}}\le \sqrt{\rho (2\|\hat p\|_{S_{\mathrm{ver}}}^2+2\|\tilde{p}^*\|_{S_{\mathrm{ver}}}^2)} \le \sqrt{4AB\rho} \le \frac{3\eps}{10}$. 

%     By using results from generalization theory once more, we obtain that with probability at least $1-\delta/5$ we have $\| \hat p - \tilde{p}^* \|_{S_{\mathrm{ver}}} \le \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ref}}}+\eps/5$. This step is important, because the only fact we know about the quality of $\hat{p}$ is that it outperforms every polynomial on the sample $S_{\mathrm{ref}}$ (not necessarily over the entire training distribution). We once more may use bounds on the values of $\hat p$ and $\tilde{p}^*$, this time without requiring clipping, since we know that the training marginal is bounded and, hence, the values of $\hat p$ and $\tilde{p}^*$ are bounded as well. This was not true for the test distribution, since we did not make any assumptions about it.

%     In order to bound $\| \hat p - \tilde{p}^* \|_{S_{\mathrm{ref}}}$, we have the following.
%     \begin{align*}
%         \| \hat p - \tilde{p}^* \|_{S_{\mathrm{ref}}} &\le \gL_{\bar{S}_{\mathrm{ref}}}(\hat p) + \gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*) + \|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}} \\
%         &\le \gL_{\bar{S}_{\mathrm{ref}}}(\tilde{p}_\opt) + \gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*) + \|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}} \tag{By \eqref{equation:phat-optimality}} \\
%         &\le \gL_{\bar{S}_{\mathrm{ref}}}({p}_\opt) + \gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*) + \|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}} 
%     \end{align*}
%     The first term above is bounded as $\gL_{\bar{S}_{\mathrm{ref}}}({p}_\opt) \le \gL_{\bar{S}_{\mathrm{ref}}}(\clip_M\circ {f}_\opt)+\|p_\opt - f_\opt\|_{{S}_{\mathrm{ref}}}$, where the second term is at most $\eps$ (by the definition of $p_\opt$) and the first term can be bounded by $\gL_{\Dtrain}({f}_\opt)+\eps/10 = \opt+\eps/10$, with probability at least $1-\delta/10$, due to an application of the Hoeffding bound.

%     For the term $\gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*)$ we can similarly use the Hoeffding bound to obtain, with probability at least $1-\delta/10$ that $\gL_{\bar{S}_{\mathrm{ref}}}(\clip\circ f^*) \le \gL_{\Dtrain}(f^*)+\eps/10$.

%     Finally, for the term $\|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}}$, we have that $\|f^*-\tilde{p}^*\|_{{S}_{\mathrm{ref}}} \le 3\eps/2$, as argued above.

%     Overall, we obtain a bound of the form $\gL_\Dtest(h) \le \gL_{\Dtrain}(f^*)=\gL_{\Dtest}(f^*) + \gL_{\Dtrain}(f_\opt) + 5\eps$, with probability at least $1-\delta$, as desired.
    
%     \paragraph{Completeness.} For the completeness criterion, we assume that the test marginal is equal to the training marginal. Then, by \Cref{lemma:relative-error-kernel-matrix} (where we observe that any $(\psi,C,\ell)$-hypercontractive distribution is also $(\phi,C,\ell)$-hypercontractive), with probability at least $1-\delta$, we have that for all $\va\in\R^{2m}$, $\va^\top \hat\Phi' \va \le \frac{1+(\rho/4)}{1-(\rho/4)}\va^\top \hat\Phi \va \le (1+\rho)\va^\top \hat\Phi \va$, because $\E[\hat\Phi] = \E[\hat\Phi']$ and the matrices are sums of independent samples of $\phi(\x)\phi(\x)^\top$, where $\x\sim\Dtrainx$. It is crucial here that $\phi$ (which recall is formed by using $S_{\mathrm{ref}}, S_{\mathrm{ref}}'$) does not depend on the verification samples $S_{\mathrm{ver}}$ and $S'_{\mathrm{ver}}$, which is why we chose them to be fresh. Therefore, the test will accept with probability at least $1-\delta$.

%     \paragraph{Efficient Implementation.} To compute $\hat \va$, we may run a least squares program, in time polynomial in $m$. For the spectral tester, we first compute the SVD of $\hat\Phi$ and check that any vector in the kernel of $\hat\Phi$ is also in the kernel of $\hat\Phi'$ (this can be checked without computing the SVD of $\hat\Phi'$). Otherwise, reject. Then, let $\hat\Phi^{\frac{\dagger}{2}}$ be the root of the Moore-Penrose pseudoinverse of $\hat\Phi$ and find the maximum singular value of the matrix $\hat\Phi^{\frac{\dagger}{2}}\hat\Phi'\hat\Phi^{\frac{\dagger}{2}}$. If the value is higher than $1+\rho$, reject. Note that this is equivalent to solving the eigenvalue problem described in \Cref{algorithm:tds-via-kernels}.
% \end{proof}

% \subsection{Applications}
% We first state and prove our end to end results on TDS learning Sigmoid and Lipschitz nets over bounded marginals that are $C$-hypercontractive for some constant $C$. 

% \begin{theorem}[TDS Learning for Nets with Sigmoid Activation]
% \label{thm:tds_learning_sigmoid_appendix}
% Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with sigmoid activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
% \begin{enumerate}
%     \item $\Dtrainx$ is bounded within $\{\x:\twonorm{\x}\leq R\}$ and is $C$-hypercontractive for $R,C\geq 1$,
%     \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
% \end{enumerate}
% Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is $\poly\left(d,\frac{1}{\epsilon},C^{\ell}, M,\log(1/\delta)^{\ell},(2R)^{2^t\cdot \ell},(2\normtwoinf{W^{(1)}})^{\ell}\cdot W^{O\left((Wt\log(W/\epsilon))^{t-2}\right)}\right)$ where $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$.
% \end{theorem}
% \begin{proof}
% From \Cref{thm:approx_sigmoid_nets}, we have that $\mathcal{F}$ is $(\epsilon, (2\normtwoinf{W^{(1)}})^{\ell} W^{O\left(W^{t-2}(t\log(W/\epsilon)^{t-2}\right)})$-approximately represented within radius $R$ w.r.t $\cmkl{t}$ where $\ell$ is a degree vector whose product is equal to $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$. Also, from \Cref{lem:composed_multinomial_properties}, we have that $A\coloneq \sup_{\twonorm{\x}\leq R}\cmkl{t}(\x,\x)\leq (2R)^{2^t\ell}$. From \Cref{lem:composed_multinomial_properties}, the entries of the kernel can be computed in $\poly(d,\ell)$ time and from \Cref{lem:hc_implies_kernelhc}, we have that $\Dtrainx$ is $\left(\cmkl{t},C,\ell\right)$ hypercontractive. Now, we obtain the result by applying \Cref{theorem:tds-via-kernels}.
% \end{proof}

% The following corollary on TDS learning two layer sigmoid networks in polynomial time readily follows. 
% \begin{corollary}
% \label{clry:polytime_tds_sigmoid_appendix}
%     Let $\mathcal{F}$ on $\R^{d}$ be the class of two-layer neural networks with weight matrices $\W=(W^{(1)},W^{(2)})$ and sigmoid activations. Let $\normtwoinf{W^{(1)}}\leq O(1)$ and $\norm{\W}_1\leq W$. Suppose the training and test distributions satisfy the assumptions from \Cref{thm:tds_learning_sigmoid_appendix} with $R=O(1)$. Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression setting up to excess error $\epsilon$ and probability of failure $0.1$ in time and sample complexity $\poly(d,1/\epsilon,W,M)$.
% \end{corollary}
% \begin{proof}
%     The proof immediately follows from \Cref{thm:tds_learning_sigmoid_appendix} by setting $t=2$ and the other parameters to the appropriate constants.
% \end{proof}


% \begin{theorem}[TDS Learning for Nets with Lipschitz Activation]
% \label{thm:tds_learning_lipschitz_appendix}
% Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with $L$-Lipschitz  activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
% \begin{enumerate}
%     \item $\Dtrainx$ is bounded within $\{\x:\twonorm{\x}\leq R\}$ and is $C$-hypercontractive for $R,C\geq 1$,
%     \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
% \end{enumerate}
% Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is $\poly\left(d,\frac{1}{\epsilon},C^{\ell}, M,\log(1/\delta)^{\ell},(2R(k+\ell))^{O(\ell)}\right)$ where $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$. When $k=1$, we have that the time and sample complexity is $\poly(d,\frac{1}{\epsilon},C^{\ell},M,\log(1/\delta)^{\ell},(2R)^{O(\ell)}$ where $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}R/\epsilon\right)$
% \end{theorem}
% \begin{proof}
%     From \Cref{thm:approx_lipschitz_nets}, for $k>1$ we have that $\mathcal{F}$ is $(\epsilon, (k+\ell)^{O(\ell)})$-approximately represented within radius $R$ w.r.t $\cmkl{1}$ where $\ell$ is a degree vector whose product is equal to $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$. For $k=1$, we have that we have that $\mathcal{F}$ is $(\epsilon, 2^{O(\ell)})$-approximately represented within radius $R$ w.r.t $\cmkl{1}$ where $\ell$ is a degree vector whose product is equal to $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}R/\epsilon\right)$. Also, from \Cref{lem:composed_multinomial_properties}, we have that $A\coloneq \sup_{\twonorm{\x}\leq R}\cmkl{t}(\x,\x)\leq (2R)^{O(\ell)}$. From \Cref{lem:composed_multinomial_properties}, the entries of the kernel can be computed in $\poly(d,\ell)$ time and from \Cref{lem:hc_implies_kernelhc}, we have that $\Dtrainx$ is $\left(\cmkl{1},C,\ell\right)$ hypercontractive. Now, we obtain the result by applying \Cref{theorem:tds-via-kernels}.
% \end{proof}

% The above theorem implies the following corollary about TDS learning the class of ReLUs. 

% \begin{corollary}
% \label{clry:polytime_tds_relu_appendix}
%     Let $\mathcal{F}=\{\x\rightarrow \max(0,\vw\cdot \x):\twonorm{\vw}=1\}$ on $\R^{d}$ be the class of ReLU functions with unit weight vectors. Suppose the training and test distributions satisfy the assumptions from \Cref{thm:tds_learning_lipschitz_appendix} with $R=O(1)$. Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression setting up to excess error $\epsilon$ and probability of failure $0.1$ in time and sample complexity $\poly(d,2^{O(1/\epsilon)},M)$.
% \end{corollary}
% \begin{proof}
%     The proof immediately follows from \Cref{thm:tds_learning_lipschitz_appendix} by setting $t=2,\W=(\vw)$ and the activation to be the ReLU function. 
% \end{proof}

% In particular, this implies that the class of ReLUs is TDS learnable in polynomial time when $\epsilon<O(1/\log d)$.












% \subsection{Applications}\label{sec:tds_uniform_appendix}

% We are now ready to state our theorem for TDS learning neural networks with sigmoid activations.
% \begin{theorem}[TDS Learning for Nets with Sigmoid Activation and Strictly Subexponential Marginals]
% \label{thm:tds_learning_sigmoid_subexp_appendix}
% Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with sigmoid activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
% \begin{enumerate}
%     \item $\Dtrainx$ is $\gamma$-strictly subexponential,
%     \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
% \end{enumerate}
% Then, \Cref{algorithm:uniform-approx} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is at most $\poly(d^{s},\log(1/\delta)^s)$ where $s=\left(k\log M\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)^{O(\frac{1}{\gamma})}$.
% \end{theorem}
% \begin{proof}
% From \Cref{thm:approx_sigmoid_nets}, we have that $\mathcal{F}$ there is an $(\epsilon, R)$-uniform approximation polynomial for $f$ with degree $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$. Here, let $g_{\mathcal{F}}(\epsilon)\coloneq  (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}$. We also have that $r=\sup_{\twonorm{\x}\leq R,f\in \mathcal{F}}|f(\x)|\leq \poly(Rk\normtwoinf{W^{(1)}}W^{t-2})$ from the Lipschitzness of the sigmoid nets (\Cref{lem:neural_net_lipschitz}) and the fact that the sigmoid evaluated at $0$ has value $1$. The theorem now directly follows from \Cref{theorem:tds-via-uniform}.
% \end{proof}

% We now state our theorem on TDS learning neural networks with arbitrary Lipschitz activations.


% \begin{theorem}[TDS Learning for Nets with Lipschitz Activation with strictly subexponential marginals]
% \label{thm:tds_learning_lipschitz_subexp_appendix}
% Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with $L$-Lipschitz  activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
% \begin{enumerate}
%     \item $\Dtrainx$ is $\gamma$-strictly subexponential,
%     \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
% \end{enumerate}
% Then, \Cref{algorithm:uniform-approx} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is at most $\poly(d^{s},\log(1/\delta^s)$ where $s=\left(k\log M\cdot\normtwoinf{W^{(1)}}(WL)^{t-1}/\epsilon\right)^{O(\frac{1}{\gamma})}$.
% \end{theorem}
% \begin{proof}
%     From \Cref{thm:approx_lipschitz_nets}, we have that $\mathcal{F}$ there is an $(\epsilon, R)$-uniform approximation polynomial for $f$ with degree $\ell=O\left(Rk\sqrt{k}\cdot \normtwoinf{W^{(1)}}(WL)^{t-1}/\epsilon\right)$. Here, let $g_{\mathcal{F}}(\epsilon)\coloneq  k\sqrt{k}\normtwoinf{W^{(1)}}(WL)^{t-1}/\epsilon$. We also have that $r=\sup_{\twonorm{\x}\leq R,f\in \mathcal{F}}|f(\x)|\leq \poly(Rk\normtwoinf{W^{(1)}}W^{t-2})$ from the Lipschitz constant(\Cref{lem:neural_net_lipschitz}) and the fact that the each individual activation has value at most $1$ when evaluated at $0$ (see \Cref{def:Neural Network}. The theorem now directly follows from \Cref{theorem:tds-via-uniform}.
% \end{proof}



\section{Polynomial Approximations of Neural Networks}\label{section:approximation_theory}
In this section we derive the polynomial approximations of neural networks with Lipschitz activations needed to instantiate \cref{theorem:tds-via-kernels} for bounded distributions and \cref{theorem:tds-via-uniform} for unbounded distributions. 

Recall the definition of a neural network. 
\begin{definition}[Neural Network]
    \label{def:Neural Network}
    Let $\sigma:\R\to\R$ be an activation function with $\sigma(0)\leq 1$. Let $\W=\left(W^{(1)},\ldots W^{(t)}\right)$ with $W^{(i)}\in \R^{s_i\times s_{i-1}}$ be the tuple of weight matrices. Here, $s_0=d$ is the input dimension and $s_{t}=1$. Define recursively the function $f_i:\R^{d}\to \R^{s_i}$ as $f_i(\x)=W^{(i)}\cdot \sigma\bigl(f_{i-1}(\x)\bigr)$ with $f_1(\x)=W^{(1)}\cdot\x$. The function $f:\R^d \to \R$ computed by the neural network $(\W,\sigma)$ is defined as $f(\x)\coloneq f_{t}(\x)$. We denote $\norm{\W}_1=\sum_{i=2}^{t}\norm{W^{(i)}}_1$. The depth of this network is $t$. 
\end{definition}

We also introduce some notation and basic facts that will be useful for this section.

\subsection{Useful Notation and Facts}
Given a univariate function $g$ on $\R$ and a vector $\x=(x_1,\ldots,x_d)\in \R^{d}$, the vector $g(\x)\in \R^{d}$ is defined as the vector with $i^{th}$ co-ordinate equal to $g(x_i)$.  For a matrix $A\in \R^{m\times n}$, we use the following notation:
 \begin{itemize}
     \item $\twonorm{A}\coloneq\sup_{\twonorm{x}=1}\twonorm{Ax}$,
     \item $\normtwoinf{A}\coloneq\sqrt{\max_{i\in [m]}\sum_{j=1}^{n}(A_{ij})^2}$,
     \item $\norm{A}_1\coloneq \sum_{(i,j)\in [n]\times [m]}|A_{ij}|$.
 \end{itemize}
\begin{fact}
\label{fact:matrix_norms}
    Given a matrix $W\in\R^{m\times n}$, we have that 
    \begin{enumerate}
        \item $\twonorm{A}\leq \norm{A}_1$,
        \item $\twonorm{A}\leq \sqrt{m}\cdot \normtwoinf{A}$.
    \end{enumerate}
\end{fact}
\begin{proof}
    We first prove (1). We have that for an $\x\in \R^{n}$ with $\twonorm{\x}=1$,
    \begin{align*}
        \twonorm{A\x}\leq \sqrt{\sum_{i=1}^{m}(A_i\cdot \x)^2}\leq \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}(A_{ij})^2}\leq \norm{A}_1
    \end{align*}
    where the second inequality follows from Cauchy Schwartz and the last inequality follows from the fact that for any vector $\vv$, $\twonorm{\vv}\leq \norm{\vv}_1$.
    We now prove (2). We have that 
     \begin{align*}
        \twonorm{A\x}\leq \sqrt{\sum_{i=1}^{m}(A_i\cdot \x)^2}\leq \sqrt{m\max_{i\in [m]}\sum_{j=1}^{n}(A_{ij})^2}\leq \sqrt{m}\normtwoinf{A}
    \end{align*} where the second inequality follows from Cauchy Schwartz and the last inequality is the definition. 
\end{proof}

\subsection{Results from Approximation Theory}

The following are useful facts about the coefficients of approximating polynomials.
\begin{fact}[Lemma~23 from \cite{reliable_goel2017}]
\label{lem:uni_poly_unit_coeff_bound}
    Let $p$ be a polynomial of degree $\ell$ such that $|p(x)|\leq b$ for $|x|\leq 1$. Then, the sum of squares of all its coefficients is at most $b^2\cdot 2^{O(\ell)}$.
\end{fact}
\begin{lemma}
\label{lem:uni_poly_coeff_bound}
 Let $p$ be a polynomial of degree $\ell$ such that $|p(\x)|\leq b$ for $|x|\leq R$. Then, the sum of squares of all its coefficients is at most $b^2\cdot 2^{O(\ell)}$ when $R\geq 1$.
\end{lemma}
\begin{proof}
    Consider $q(x)=p(Rx)$. Clearly, $|q(x)|\leq b$ for all $|x|\leq 1$. Thus, the sum of squares of its coefficients is at most $b^2\cdot 2^{O(\ell)}$ from \Cref{lem:uni_poly_unit_coeff_bound}. Now, $p(x)=q(x/R)$ has coefficients bounded by $b^2\cdot 2^{O(\ell)}$ when $R\geq 1$.
\end{proof}
% Then the magnitude of any coefficient of $q$ is at most $(2\ell)^{3\ell}$
\begin{fact}[\cite{ben2018classical}]\label{lemma:cube_coeff_bounds}
    Let $q$ be a polynomial with real coefficients on $k$ variables with degree $\ell$ such that for all $\x \in [0, 1]^k$, $|q(\x)| \le 1$. Then the magnitude of any coefficient of $q$ is at most $(2k\ell(k+\ell))^{\ell}$ and the sum of the magnitudes of all coefficients of $q$ is at most $(2(k+\ell))^{3\ell}$.
\end{fact}
\begin{lemma}\label{lemma:ball_coeff_bounds}
    Let $q$ be a polynomial with real coefficients on $k$ variables with degree $\ell$ such that for all $\x \in \R^k$ with $\Norm{\x}_2 \le R$, $|q(\x)| \le b$. Then the sum of the magnitudes of all coefficients of $q$ is at most $b(2(k+\ell))^{3\ell}k^{\ell/2}$ for $R\geq 1$.
\end{lemma}
\begin{proof}
    Consider the polynomial $h(\x) = 1/b \cdot q(R\x/\sqrt{k})$. Then $|h(\x)| = 1/b \cdot |q(R\x/\sqrt{k})| \le 1$ for $\|\x R/\sqrt{k}\|_2 \le R$, or equivalently for all $\Norm{x}_2 \le \sqrt{k}$. In particular, since the unit cube $[0,1]^k$ is contained in the $\sqrt{k}$ radius ball, then $|h(\x)| \le 1$ for $\x \in [0,1]^k$. By \cref{lemma:cube_coeff_bounds}, the sum of the magnitudes of the coefficients of $h$ is at most $(2(k+\ell))^{3\ell}$. Since $q(\x) = b \cdot h(\x\sqrt{k}/R)$, then the sum of the magnitudes of the coefficients of $q$ is at most $b(2(k + \ell))^{3\ell} k^{\ell/2}$.
\end{proof}

\begin{lemma}\label{lemma:sum_coeff_bound}
    Let $p(\x)$ be a degree $\ell$ polynomial in $\x \in \R^d$ such that each coefficient is bounded in absolute value by $b$. Then the sum of the magnitudes of the coefficients of $p(\x)^t$ is at most $b^t d^{t \ell}$.
\end{lemma}
\begin{proof}
    Note that $p(\x)$ has at most $d^\ell$ terms. Expanding $p(\x)^t$ gives at most $d^{t \ell}$ terms, where any monomial is formed from a product of $t$ terms in $p(\x)$. Then the coefficients of $p(\x)^t$ are bounded in absolute value by $B^t$. Summing over all monomials gives the bound.
\end{proof}

In the following lemma, we bound the magnitude of approximating polynomials for subspace juntas outside the radius of approximation.
\begin{lemma}\label{lemma:bound-on-uniform-approximator-outside-interval}
    Let $\epsilon > 0, R \ge 1$, and $f: \R^d \rightarrow \R$ be a $k$-subspace junta, and consider the corresponding function $g(W\x)$. Let $q: \R^k \rightarrow \R$ be an $(\epsilon, R)$-uniform approximation polynomial for $g$, and define $p: \R^d \rightarrow \R$ as $p(\x) := q(W\x)$. Let $r := \sup_{\Norm{W\x}_2 \le R} |g(W\x)|$. Then
    \[|p(\x)| \le (r + \epsilon)(2(k+\ell))^{3\ell}k^{\ell/2} \Norm{\frac{W\x}{R}}_2^\ell \quad \forall \Norm{W\x}_2 \ge R.\]
\end{lemma}
\begin{proof}
    Since $q(\x)$ is an $(\epsilon, R)$-uniform approximation for $g$, then $|q(\x) - g(\x)| \le \epsilon$ for $\Norm{\x}_2 \le R$. Let $h(\x) = q(R\x)$. Then $|h(\x/R) - g(\x)| \le \epsilon$ for $\Norm{\x}_2 \le R$, and so $|h(\x/R)| \le r + \epsilon$ for $\Norm{\x}_2 \le R$, or equivalently $|h(\x)| \le r + \epsilon$ for $\Norm{\x}_2 \le 1$. Write $h(\x) = \sum_{\Norm{\alpha}_1 \le \ell} h_\alpha x_1^{\alpha_1}\ldots x_k^{\alpha_k}$. By \cref{lemma:ball_coeff_bounds}, $\sum_{\Norm{\alpha}_1 \le \ell} |h_\alpha| \le (r + \epsilon)(2(k+\ell))^{3\ell} \cdot k^{\ell/2}$. Then for $\Norm{x}_2 \ge 1$,
    \begin{align*}
        |h(\x)| &\le \sum_{\Norm{\alpha}_1 \le \ell} |h_\alpha| |x_1^{\alpha_1}\ldots x_k^{\alpha_k}| \\
        &\le \sum_{\Norm{\alpha}_1 \le \ell} |h_\alpha| \Norm{\x}_2^{\Norm{\alpha}_1} \\
        &\le \Norm{\x}_2^\ell \cdot \sum_{\Norm{\alpha}_1 \le \ell} |h_\alpha|, 
    \end{align*}
    where the second inequality holds because $|x_i| \le \Norm{\x}_2$ for all $i$, and the last inequality holds because $\Norm{\x}_2^\ell \ge \Norm{\x}_2^{\Norm{\alpha}_1}$ for $\Norm{\alpha}_1 \le \ell$ when $\Norm{\x}_2 \ge 1$.
    Then since $p(\x) = q(W\x) = h(W\x/R)$, we have $|p(\x)| \le \Norm{\frac{W\x}{R}}_2^\ell (r + \epsilon)(2(k+\ell))^{3\ell} k^{\ell/2}$ for $\Norm{W\x}_2 \ge R$.
\end{proof}

The following is an important theorem that we use later to obtain uniform approximators for Lipschitz Neural networks.
 \begin{theorem}[\cite{Newman1964}]
\label{thm:jackson}
    Let $f:\R^{k}\to \R$ be a function continuous on the unit sphere $S_{k-1}$. Let $\omega_{f}$ be the function defined as $\omega_{f}(t)\coloneq\sup_{\substack{\twonorm{\x},\twonorm{\vy}\leq 1\\{\twonorm{\x-\vy}\leq t}}}|f(\x)-f(\vy)|$ for any $t\geq 0$.
Then, we have that there exists a polynomial of degree $\ell$ such that 
$\sup_{\twonorm{x}\leq 1}|f(\x)-p(\x)|\leq C\cdot \omega_{f}(k/\ell)
$ where $C$ is a universal constant.
\end{theorem} 

This implies the following corollary.
\begin{corollary}
    \label{clry:lipschitz_jackson}
Let $f:\R^{k}\to \R$ be an $L$-Lipschitz function for $L\geq 0$ and let $R\geq 0$. Then, for any $\epsilon\geq 0$, there exists a polynomial $p$ of degree $O(LRk/\epsilon)$ such that $p$ is an $(\epsilon,R)$-uniform approximation polynomial for $f$. 
\end{corollary}
\begin{proof}
    Consider the function $g(\x)\coloneq f(R\x)$. Then, we have that $g$ is $RL$-Lipschitz. From statement of \Cref{thm:jackson}, we have that $\omega_g(t)\leq RLt$. Thus, from \Cref{thm:jackson}, there exists a polynomial $q$ of degree $O(LRk/\epsilon)$ such that $\sup_{\twonorm{\vy}\leq 1}|g(\vy)-q(\vy)|\leq \epsilon$. Thus, we have that $\sup_{\twonorm{\x}\leq R}|f(\x)-q(\x/R)|=\sup_{\twonorm{\x}\leq R}|g(\x/R)-q(\x/R)|=\sup_{\twonorm{\vy}\leq 1}|g(\vy)-q(\vy)|\leq \epsilon$. $p(\x)\coloneq q(\x/R)$ is the required polynomial of degree $O(LRk/\epsilon)$. 
\end{proof}

\subsection{Kernel Representations}
We now state and prove facts about Kernel Representations that we require. First, we recall the multinomial kernel from \cite{reliable_goel2017}.
\begin{definition}
\label{def:multinomial_kernel}
    Consider the mapping $\psi_{\ell}:\R^{n}\to \R^{N_{\ell}}$, where $N_d=\sum_{i=1}^{\ell}d^{\ell}$ is indexed by tuples $(i_1,i_2,\ldots, i_{j})\in [d]^j$ for $j\in [\ell]$ such that value of $\psi_{\ell}(\x)$ at index $(i_1,i_2,\ldots,i_{j})$ is equal to $\prod_{t=1}^{j}\x_{i_t}$. The kernel $\mkl$ is defined as 
    \[
    \mkl(\x,\vy)=\langle\psi_{\ell}(\x),\psi_{\ell}(\vy)\rangle=\sum_{i=1}^{d}(\x\cdot\vy)^{i}.
    \] We denote the corrresponding RKHS as $\hkml$.
\end{definition}

We now prove that polynomial approximators of subspace juntas can be represented as elements of $\hkml$.
\begin{lemma}
\label{lem:subspace_junta_kernel}
    Let $k\in \mathbb{N}$ and $\epsilon, R\geq 0$.  Let $f:\R^{d}\to \R$ be a $k$-subspace junta such that $f(\x)=g(W\x)$ where $g$ is a function on $\R^{k}$ and $W$ is a projection matrix from $\R^{k\times d}$. Suppose, there exists a polynomial $q$ of degree $\ell$ such that $\sup_{\twonorm{\vy}\leq R}|g(\vy)-q(\vy)|\leq \epsilon$ and the sum of squares of coefficients of $q$ is bounded above by $B^2$. Then, $f$ is $(\epsilon, B^2\cdot (k+1)^{\ell})$-approximately represented within radius $R$ with respect to $\hkml$.
\end{lemma}
\begin{proof}
    We argue that there exists a vector $\vv\in \hkml$ such that $\langle\vv,\vv\rangle\leq B^2$ and $|f(\x)-\langle \vv,\sigma_{\ell}(\x)\rangle|\leq \epsilon$ for all $\twonorm{\x}\leq R$. Consider the polynomial $p$ of degree $\ell$ such that $p(\x)=q(W\x)$. We argue that $p(\x)=\langle \vv,\sigma_{\ell}(\x)\rangle$ for some $\vv$ and that $\langle\vv,\vv\rangle\leq B^2$. Let $q(\vy)=\sum_{S\in \mathbb{N}^{k},|S|\leq \ell}q_{S}\prod_{j=1}^{k}\vy^{|S_j|}$. From our assumption on $q$, we have that $\sum_{S\in \mathbb{N}^{k},|S|\leq \ell}|q_{S}|\leq B$. For $i\in \ell$, we use define $B_i$ as $B_i=\sum_{S\in \mathbb{N}^{k},|S|=\ell}|q_{S}|$. Given multi-index $S$, for any $i\in [d]$, we define $S(i)$ as the number $t$  such that $\sum_{i=1}^{j-1}|S_i|\leq j< \sum_{i=1}^{j}|S_i|$. We now compute the entry of $\vv$ indexed by $(i_1,i_2,\ldots,i_t)$. By expanding the expression for $p(\x)$, we obtain that 
    \[v_{i_1,\ldots,i_t}=\sum_{|S|=t}q_{S}\prod_{j=1}^{t}W_{S(j),i_j}.\]

    We are now ready to bound $\langle\vv,\vv\rangle$. We have that 
    \begin{align*}
\langle\vv,\vv\rangle&=\sum_{t=0}^{\ell}\sum_{(i_1,\ldots,i_t)\in [d]^{k}}(v_{i_1,\ldots,i_t})^2
    =\sum_{t=0}^{\ell}\sum_{(i_1,\ldots,i_t)\in [d]^k}\left(\sum_{|S|=t}q_{S}\prod_{j=1}^{t}W_{S(j),i_j}\right)^2\\
    &\leq \sum_{t=0}^{\ell}\sum_{(i_1,\ldots,i_t)\in [d]^k}\left(\sum_{|S|=t}q^2_{S}\right)\left(\sum_{|S|=t}\prod_{j=1}^{t}W^2_{S(j),i_j}\right)\\
    &\leq \sum_{t=0}^{\ell}\left(\sum_{|S|=t}q^2_{S}\right)\left(\sum_{|S|=t}\prod_{j=1}^{t}\left(\sum_{i=1}^{d}W^2_{S(j),i}\right)\right)\leq \sum_{t=0}^{\ell}\left(\sum_{|S|=t}q^2_{S}\right)\cdot (k+1)^{t}
    \\  &\leq\left(\sum_{|S|\leq \ell}q^2_{S}\right)\cdot (k+1)^{\ell}\leq B^2\cdot (k+1)^{\ell}.
    \end{align*}
Here, the first inequality follows from Cauchy-Schwartz, the second follows by rearranging terms. The third inequality follows from the fact that the number of multi-indices of size $t$ from a set of $k$ elements is at most $(k+1)^{t}$. The final inequality follows from the fact that the sum of the squares of the coefficients of $q$ is at most $B^2$.
\end{proof}


We introduce an extension of the multinomial kernel that will be useful for our application to sigmoid-nets.
\begin{definition}[Composed multinomial kernel]
\label{def:composed_multinomial_kernel}
    Let $\vell=(\ell_1,\ldots,\ell_t)$ be a tuple in $\mathbb{N}^{t}$. We denote a sequence of mappings $\psi^{(0)}_{\vell},\psi^{(1)}_{\vell},\ldots,\psi^{(t)}_{\vell}$ on $\R^{d}$ inductively as follows:
    \begin{enumerate}
        \item $\psi^{(0)}_{\vell}(\x)=\x$ 
        \item $\psi^{(i)}_{\vell}(\x)=\psi_{\ell_i}\left(\psi^{(i-1)}_{\vell}(\x)\right)$.
    \end{enumerate}
    Let $N_{\vell}^{(i)}$ denote the number of coordinates in $\cpsi{i}$.
    This induces a sequence of kernels $\cmkl{0},\cmkl{1},\ldots,\cmkl{t}$ defined as 
    \[
    \cmkl{i}(\x,\vy)=\langle\cpsi{i}(\x),\cpsi{i}(\vy)\rangle=\sum_{j=0}^{\ell_{i}}\left(\langle\cpsi{i-1}(\x),\cpsi{i-1}(\vy)\rangle^j\right)
    \] and a corresponding sequence of RKHS denoted by $\mathcal{H}_{\cmkl{0}},\mathcal{H}_{\cmkl{1}},\ldots \mathcal{H}_{\cmkl{t}}$.

    Observe that the the multinomial Kernel $\mkl=\mathsf{MK}^{(1)}_{(\ell)}$ is an instantiation of the composed multinomial kernel.
\end{definition}

We now state some properties of the composed multinomial kernel.
\begin{lemma}
\label{lem:composed_multinomial_properties}
    Let $\vell=(\ell_1,\ldots,\ell_{t})$ be a tuple in $\mathbb{N}^{t}$ and $R\geq 0$. Then, the following hold: 
    \begin{enumerate}
        \item $\sup_{\twonorm{\x}\leq R}\cmkl{t}(\x,\x)\leq \max\{1,(2R)^{2^{t}\prod_{i=1}^{t}\ell_{i}}\}$,
        \item For any $\x,\vy\in \R^{d}$, $\cmkl{t}(\x,\vy)$ can be computed in time $\poly\left(d, (\sum_{i=1}^{t}\ell_i)\right)$,
        \item For any $\vv\in \mathcal{H}_{\cmkl{t}}$ and $\x\in \R^{d}$, we have $\langle\vv,\cpsi{t}(\x)\rangle$ is a polynomial in $\x$ of degree $\prod_{i=1}^{t}\ell_i$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    We assume without loss of generality that $R\geq 1$ as the kernel function is increasing in norm. To prove (1), observe that for any $\x$, we have that
    \[\cmkl{i}(\x,\x)=\sum_{j=0}^{\ell_i}\left(\cmkl{i-1}(\x,\x)\right)^{j}\leq \left(2\cmkl{i-1}(\x,\x)\right)^{\ell_i+1}.\]
We also have that $\sup_{\twonorm{x}\leq R}\cmkl{0}(\x,\x)=\x\cdot \x=R$. Thus, unrolling the recurrence gives us that $\cmkl{t}(\x,\x)\leq \max\{1,(2R)^{\prod_{i=1}^{t}(\ell_i+1)}\}\leq \max\{1,(2R)^{2^{t}\prod_{i=1}^{t}\ell_{i}}\}$.

The run time follows from the fact that $\cmkl{i}(\x,\x)=\sum_{j=0}^{\ell_i}\left(\cmkl{i-1}(\x,\x)^{j}\right)$ and thus can be computed from $\cmkl{i-1}$ with $\ell_i$ additions and exponentiation operations. Recursing gives the final runtime.

The fact that $\langle\vv,\cpsi{i}(\x)\rangle$ follows immediately from the fact the fact the entries of $\cpsi{i}(\x)$ arise from the multinomial kernel and hence are polynomials in $\x$. The degree is at most $\prod_{i=1}^{t}\ell_i$.
\end{proof}
We now argue that a distribution that is hypercontractive with respect to polynomials is hypercontractive with respect to the multinomial kernel.
\begin{lemma}
\label{lem:hc_implies_kernelhc}
    Let $\Dtrain$ be a distribution on $\R^{d}$ that is $C$-hypercontractive for some constant $C$. Then, $\Dtrain$ is also $(\cmkl{t},C,\prod_{i=1}^{t}\ell_i)$-hypercontractive.
\end{lemma}
\begin{proof}
    The proof immediately follows from \Cref{definition:hypercontractivity} and \Cref{lem:composed_multinomial_properties}(3).
\end{proof}
\subsection{Nets with Lipschitz activations}

We are now ready to prove our theorem about uniform approximators for neural networks with Lipschitz activations. First, we prove that such networks describe a Lipschitz function. 

    
\begin{lemma}
\label{lem:neural_net_lipschitz}
    Let $f:\R^{d}\to \R$ be the function computed by an $t$-layer neural network with $L$-Lipschitz activation function $\sigma$ and weight matrices $\W$. Say, $\norm{\W}_1\leq W$ for $W\geq 0$ and the first hidden layer has $k$ neurons. Then we have that $f$ is $\sqrt{k}\normtwoinf{W^{(1)}}(WL)^{t-1}$-Lipschitz.
\end{lemma}
\begin{proof}
    First, observe from \Cref{fact:matrix_norms} that for all $1<i\leq T$, $\twonorm{W^{(i)}}\leq W$ (since $\norm{\W}_1\leq W$) and $\twonorm{W^{(1)}}\leq \sqrt{k}\normtwoinf{W^{(1)}}$. Recall from \Cref{def:Neural Network}, we have the functions $f_1,\ldots,f_{t}$ where $f_{i}(\x)=W^{(i)}\cdot \sigma\bigl(f_{i-1}(\x)\bigr)$ and $f_1(\x)=W^{(1)}\cdot\x$. We prove by induction on $i$ that $\twonorm{f_{i}(\x)-f_{i}(\x+\vu)}\leq \sqrt{k}\normtwoinf{W^{(1)}}(WL)^{i-1}\twonorm{\vu}$. For the base case, observe that 
    \begin{align*}
        \twonorm{f_1(\x+\vu)-f_1(\x)}&\leq \sqrt{\sum_{i=1}^{d_1}\biggl(\bigl(\langle W^{(1)}_i,\x\rangle-\langle W^{(1)}_i,\x+\vu\rangle\bigr)^2\biggr)}
        \leq \sqrt{\sum_{i=1}^{d_1}\biggl(\langle W^{(1)}_i, \vu\rangle\biggr)^2}\\ &\leq \twonorm{W^{(1)}_i\vu}\leq\sqrt{k}\normtwoinf{W^{(1)}}\twonorm{\vu}
    \end{align*} where the second inequality follows from the Lipschitzness of $\sigma$ and the final inequality follows from the definition of operator norm.
    We now proceed to the inductive step. Assume by induction that $\twonorm{f_i(\x)-f_{i}(\x+\vu)}$ is at most $\sqrt{k}\normtwoinf{W^{(1)}}(WL)^{i-1}\twonorm{\vu}$. Thus, we have 
    \begin{align*}
        \twonorm{f_{i+1}(\x+\vu)-f_{i+1}(\x)}&=
         \sqrt{\sum_{j=1}^{d_1}\biggl(\langle W^{(i+1)}_j,\sigma\left(f_i(\x)\right)\rangle-\langle W^{(i+1)}_j,\sigma\left(f_i(\x+\vu)\right)\rangle\biggr)^2} \\
        &\leq \twonorm{W^{(i+1)}}\twonorm{\sigma(f_i(\x))-\sigma(f_i(\x+\vu))}\\
        &\leq (WL)\sqrt{k}\normtwoinf{W^{(1)}}(WL)^{i-1}\twonorm{\vu}\leq \sqrt{k}\normtwoinf{W^{(1)}}(LW)^{i}\twonorm{\vu}
    \end{align*}
    where the third inequality follows from the Lipschitzness of $\sigma$ and the inductive hypothesis. Thus, we get that $|f(\x+\vu)-f(\x)|\leq \twonorm{f_{t}(\x+\vu)-f_{t}(\x)}\leq \sqrt{k}\normtwoinf{W^{(1)}}(WL)^{t-1}\cdot \twonorm{\vu}$.
\end{proof}

We now state are theorem regarding the uniform approximation of Lipschitz nets. We also prove that the approximators can be represented by low norm vectors in $\mathcal{R}_{\mkl}$ for appropriately chosen degree $\ell$.
\begin{theorem}
\label{thm:approx_lipschitz_nets}
Let $\epsilon,R\geq 0$. Let $f: \R^d \rightarrow \R$ be a neural network with an $L$-Lipschitz activation function $\sigma$, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ where $W^{(i)}\in \R^{s_i\times s_{i-1}}$. Let $k$ be the number of neurons in the first hidden layer. Then, there exists of a polynomial $p$ of degree $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$ that is an $(\epsilon,R)$-uniform approximation polynomial for $f$. Furthermore,  $f$ is $(\epsilon, (k+\ell)^{O(\ell)})$-approximately represented within radius $R$ with respect to $\hkml=\mathbb{H}_{\mathsf{MK}^{(1)}_{(\ell)}}$. In fact, when $k=1$, it holds that $f$ is $(\epsilon,2^{O(\ell)})$-approximately represented within $R$ with respect to $\mathbb{H}_{\cmkl{1}}$.
\end{theorem}
\begin{proof}
    We can express $f$ as $f(\x)=g(P\x)$ where $P$ is a projection matrix and $g$ is a neural network with input size $k$. We observe that the Lipschitz constant of $g$ is the same as the Lipschitz constant of $f$ since $P$ is a projection matrix. 
    From \Cref{lem:neural_net_lipschitz}, we have that $g$ is $\normtwoinf{\sqrt{k}W^{(1)}}(WL)^{t-1}$-Lipshitz. From \Cref{clry:lipschitz_jackson}, we have that there exists a polynomial $q$ of degree $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$ that is an $(\epsilon,R)$-uniform approximation for $g$. From \Cref{lemma:ball_coeff_bounds}, we have that the sum of squares of  magnitudes of coefficients of $q$ is bounded by $\left(\normtwoinf{\sqrt{k}W^{(1)}}(WL)^{t-1}R\right)(k+\ell)^{O(\ell)}\leq (k+\ell)^{O(\ell)}$. Now, applying \Cref{lem:subspace_junta_kernel} yields the result.  When $k=1$, we apply \Cref{lem:uni_poly_coeff_bound} to obtain that the sum of squares of magnitudes of coefficients of $q$ is bounded by $\normtwoinf{W^{(1)}}(WL)^{t-1}\cdot 2^{O(\ell)}\leq 2^{O(\ell)}$.
\end{proof}
\subsection{Sigmoids and Sigmoid-nets}
\label{sec:approx_sigmoid_appendix}
We now give a custom proof for the case of neural networks with sigmoid activation. We do this as we can hope to get $O(\log(1/\epsilon)$ degree for our polynomial approximation. We largely follow the proof technique of \cite{reliable_goel2017} and \cite{zhang16}. The modifications we make are to handle the case where the radius of approximation is a variable $R$ instead of a constant. We require(for our applications to strictly-subexponential distributions) that the degree of approximation must scale linear in $R$, a property that does not follow directly from the analysis given in \cite{reliable_goel2017}. We modify their analysis to achieve this linear dependence. 

We first state a result regarding polynomial approximations for a single sigmoid activation. 

\begin{theorem}[\cite{livni14}]
\label{thm:sigmoid_poly}
Let $\sigma:\R\to \R$ denote the function $\sigma(x)=\frac{1}{1+e^{-x}}$. Let $R,\epsilon\geq 0$. Then, there exists a polynomial $p$ of degree $\ell=O(R\log(R/\epsilon))$ such that 
$\sup_{|x|\leq R}|\sigma(x)-p(x)|\leq \epsilon$. Also, the sum of the squares of the coefficients of $p$ is bounded above by $2^{O(\ell)}$.
\end{theorem}

We now present a construction of a uniform approximation for neural networks with sigmoid activations. The construction is similar to the one in \cite{reliable_goel2017} but the analysis deviates as linear dependence on radius of approximation is important to us.

\begin{theorem}
\label{thm:approx_sigmoid_nets}
    Let $\epsilon,R\geq 0$. Let $f$ on $\R^{d}$ be a neural network with sigmoid activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ where $W^{(i)}\in \R^{s_i\times s_{i-1}}$. Also, let $\norm{\W}_1\leq W$. Then, there exists of a polynomial $p$ of degree $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$ that is an $(\epsilon,R)$-uniform approximation polynomial for $f$. Furthermore,  $f$ is $(\epsilon, B)$-approximately represented within radius $R$ with respect to $H_{\cmkl{t}}$ where $\vell=(\ell_1,\ldots,\ell_{t-1})$ is a tuple of degrees whose product is bounded by $\ell$. Here, $B\leq (2\normtwoinf{W^{(1)}})^{\ell}\cdot W^{O\left(W^{t-2}(t\log(W/\epsilon)^{t-2}\right)}$.
\end{theorem}
\begin{proof}
    First, let $q_{1}$ be the polynomial guaranteed by \Cref{thm:sigmoid_poly} that $(\epsilon/(2W)^t)$-approximates the sigmoid in an interval of radius $R\normtwoinf{W^{(1)}}$. Denote the degree of $q_1$ as $\ell_1=O\left(Rt\normtwoinf{W^{(1)}}\log(RW/\epsilon)\right)$. For all $1<i<t$, let $q_i$ be the polynomial that $(\epsilon/(2W)^{t})$-approximates the sigmoid upto radius $2W$. These have degree equal to $O\left(Wt\log(W/\epsilon)\right)$. Let $\vell=(\ell_1,\ldots \ell_{t-1})$. For all $i\in [t-1]$, let $q_{i}(x)=\sum_{j=0}^{\ell_i}\beta^{(i)}_jx^{j}$. We know that $\sum_{i=0}^{\ell_i}(\beta^{(i)}_j)^2\leq 2^{O(\ell_i)}$.
    
    We now construct the polynomial $p$ that approximates $f$. For $i\in [t]$, define $p_{i}(\x)=W^{(i)}\cdot q_{i-1}\left(p_{i-1}(\x)\right)$ with $p_1(\x)=W^{(1)}\cdot \x$. Define $p(\x)=p_{t}(\x)$. Recall that $p_{i}(\x)$ is a vector of $s_{i}$ polynomials. We prove the following by induction: for every $i\in[t]$,
    \begin{enumerate}
        \item $\norm{p_{i}(\x)-f_{i}(\x)}_{\infty}\leq \epsilon/(2W)^{t-i}$,
        \item For each $j\in [s_i]$, we have that $(p_{i})_{j}(\x)=\langle\vv,\cpsi{i}(\x)\rangle$ with $\langle\vv,\vv\rangle\leq (2\normtwoinf{W^{(1)}})^{O(\prod_{n=1}^{i-1}\ell_{n})}\cdot W^{O(\prod_{n=2}^{i-1}\ell_n)}$.
    \end{enumerate}
    where the function $f_i$ is as defined in \Cref{def:Neural Network}. 

    The above holds trivially for $i=1$ and $f_1(\x)=p_1(\x)=W^{(1)}\cdot (\x)$ is an exact approximator. Also, $(p_1)_i(\x)=\langle W^{(1)}_i,\x\rangle=\langle W^{(1)}_i,\cpsi{1}(x)\rangle$ from the definition of $\cpsi{1}$. Clearly, $\langle W^{(1)}_i,W^{(1)}_i\rangle\leq \left(\normtwoinf{W^{(1)}}\right)^2.$ We now prove that the above holds for $i+1\in [t]$ assuming it holds for $i$. 

    We first prove (1). For $j\in [s_{i+1}]$, we have that
    \begin{align*}
       |(p_{i+1})_{j}(\x)-(f_{i+1})_{j}(\x)|&=|W^{(i+1)}_{j}\bigl(q_{i}(p_{i}(\x))-\sigma(f_{i}(\x))\bigr)|\\
       &\leq |W^{(i+1)}_{j}\bigl( q_{i}(p_{i}(\x))-\sigma(p_{i}(\x)\bigr)|+|W^{(i+1)}_{j}\bigl( \sigma(p_{i}(\x))-\sigma(f_{i}(\x)\bigr)|\\
       &\leq W\cdot (\epsilon/(2W)^{t})+W\cdot\epsilon/(2W)^{t-i}\leq \epsilon/(2W)^{t-(i+1)}. 
    \end{align*}
    For the second inequality, we analyse the cases $i=1$ and $i>1$ separately. When $i=1$, we have that $(p_1)_{j}(\x)= (f_{1})_{j}(\x)\leq R\normtwoinf{W_1}$ and $\sigma(x)-q_1(x)\leq (\epsilon/(2W)^t)$ when $|x|\leq R\normtwoinf{W_1}$. For $i>1$, from the inductive hypothesis, we have that $|W^{(i+1)}p_i(\x)|\leq |W^{(i+1)}f_{i}(\x)|+\norm{W^{(i+1)}}_{1}\cdot (\epsilon/(2W)^{t-i})\leq 2W$. The second term in the second inequality is bounded since $\sigma$ is $1$-Lipschitz. 

    We are now ready to prove that $(p_{i+1})_j$ is representable by small norm vectors in $\mathcal{H}_{\cmkl{i+1}}$ for all $j\in [s_{j+1}]$. We have that 
\[
        (p_{i+1})_{j}(\x)=\sum_{k=1}^{s_{i}}W^{(i+1)}_{jk}\cdot q_{i}\left((p_{i})_{k}(\x)\right).
\]
From the inductive hypothesis, we have that $(p_i)_{k}=\langle\vv^{(k)},\cpsi{i}\rangle$. Thus, we have that
\[
 (p_{i+1})_{j}(\x)=\sum_{k=1}^{s_{i}}W^{(i+1)}_{jk}\cdot q_{i}\left(\langle\vv^{(k)},\cpsi{i}\rangle\right).
\]

We expand each term in the above sum. We obtain,
\begin{align*}
q_{i}\left(\langle\vv^{(k)},\cpsi{i}\rangle\right)&=\sum_{n=0}^{\ell_i}\beta^{(i)}_{n}\left(\langle\vv^{(k)},\cpsi{i}\rangle\right)^{n}\\
&=\sum_{n=0}^{\ell_i}\beta^{(i)}_{n}\sum_{(m_1,\ldots,m_n)\in [N_{\vell}^{(i)}]^{n}}v^{(k)}_{m_1}\ldots v^{(k)}_{m_n}\left(\cpsi{i}(\x)\right)_{m_1}\ldots \left(\cpsi{i}(\x)\right)_{m_n}\\
&=\langle \vu^{(k)},\psi_{\ell_i}((\cpsi{i}(\x))\rangle=\langle\vu^{(k)},\cpsi{i+1}(\x)\rangle.\end{align*}
The second inequality follows from expanding the equation. $\vu^{(k)}$ indexed by $(m_1,\ldots, m_n)\in [N^{(i)}_{\ell}]^n$ for $n\leq \ell_i$ has entries given by 
$u^{(k)}_{(m_1,\ldots,m_n)}=\beta^{(i)}_n v^{(k)}_{m_1}\ldots v^{(k)}_{m_n}$. Putting things together, we obtain that
\begin{align*}
    (p_{i+1})_{j}(\x)&=\sum_{k=1}^{s_{i}}W^{(i+1)}_{jk}\cdot\langle\vu^{(k)},\cpsi{i+1}(\x)\rangle\\
    &=\langle\sum_{k=1}^{s_i}W^{(i+1)}_{jk}\vu^{(k)},\cpsi{i+1}(\x)\rangle.
\end{align*}
Thus, we have proved that $(p_{i+1})_{j}$ is representable in $\mathcal{H}_{\cmkl{i+1}}$. We now prove that the norm of the representation is small. We have that 
\begin{align*}
    \twonorm{\sum_{k=1}^{s_i}W^{(i+1)}_{jk}\vu^{(k)}}\leq \norm{W^{(i+1)}}_1\max_{k\in [s_i]}\twonorm{\vu^{(k)}}\leq W\cdot\max_{k\in [s_i]}\twonorm{\vu^{(k)}}.
\end{align*}
We bound $\max_{k\in [s_i]}\twonorm{\vu^{(k)}}$. For any $k$, from the definition of $\vu^{(k)}$ and the inductive hypothesis, we have that 
\begin{align*}
\twonorm{\vu^{(k)}}^2&=\sum_{n=0}^{\ell_i}\left(\beta^{(i)}_{n}\right)^2\cdot\sum_{(m_1,\ldots,m_n)\in [N^{(i)}_{\vell}]^n}\prod_{j=1}^{n}\left(\vu^{(k)}_{m_j}\right)^2\\
&=\sum_{n=0}^{\ell_i}\left(\beta^{(i)}_n\right)^2\twonorm{\vv^{(k)}}^{2n}\leq 2^{O(\ell_i)}\cdot \twonorm{\vv^{(k)}}^{2\ell_{i}}
\end{align*}
We analyse the case $i=1$ and $i>1$ separately. When $i=1$, we have $2^{O(\ell_1)}\twonorm{\vv^{(k)}}^{2\ell_1}\leq (2\normtwoinf{W^{(1)}})^{O(\ell_1)}$ from the bound on the base case. When $i>1$, we have 
\begin{align*}
     \twonorm{\sum_{k=1}^{s_i}W^{(i+1)}_{jk}\vu^{(k)}}^2&\leq W^2 2^{O(\ell_i)}\twonorm{\vv^{(k)}}^{2\ell_i}\\
     &\leq W^2 2^{O(\ell_i)}\left((2\normtwoinf{W^{(1)}})^{O(\prod_{n=1}^{i-1}\ell_{n})}\cdot W^{O(\prod_{n=2}^{i-1}\ell_n)}\right)^{2\ell_i}\\
     &\leq (2\normtwoinf{W^{(1)}})^{O(\prod_{n=1}^{i}\ell_{n})}\cdot W^{O(\prod_{n=2}^{i}\ell_n)}
\end{align*} which completes the induction. We are ready to calculate the bound on the degree. 

We have $\ell_1=O(Rt\normtwoinf{W^{(1)}}\log(RW/\epsilon))$. Also, for $i>1$, we have $\ell_{i}=O(Wt\log(W/\epsilon))$. Thus, the total degree is 
$\ell\leq\prod_{i=1}^{t-1}\ell_i=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$. The square of the norm of the kernel representation is bounded by $B$ where
\[
B\leq (2\normtwoinf{W^{(1)}})^{\ell}\cdot W^{O\left(W^{t-2}(t\log(W/\epsilon)^{t-2}\right)}.
\]

This concludes the proof.
\end{proof}

\subsection{Applications for Bounded Distributions}
We first state and prove our end to end results on TDS learning Sigmoid and Lipschitz nets over bounded marginals that are $C$-hypercontractive for some constant $C$. 

\begin{theorem}[TDS Learning for Nets with Sigmoid Activation]
\label{thm:tds_learning_sigmoid_appendix}
Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with sigmoid activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
\begin{enumerate}
    \item $\Dtrainx$ is bounded within $\{\x:\twonorm{\x}\leq R\}$ and is $C$-hypercontractive for $R,C\geq 1$,
    \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
\end{enumerate}
Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is \[\poly\left(d,\frac{1}{\epsilon},C^{\ell}, M,\log(1/\delta)^{\ell},(2R)^{2^t\cdot \ell},(2\normtwoinf{W^{(1)}})^{\ell}\cdot W^{O\left((Wt\log(W/\epsilon))^{t-2}\right)}\right)\] where $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$.
\end{theorem}
\begin{proof}
From \Cref{thm:approx_sigmoid_nets}, we have that $\mathcal{F}$ is $\Paren{\epsilon, (2\normtwoinf{W^{(1)}})^{\ell} W^{O\left(W^{t-2}(t\log(W/\epsilon)^{t-2}\right)}}$-approximately represented within radius $R$ with respect to $\cmkl{t}$, where $\vell$ is a degree vector whose product is equal to $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$. Also, from \Cref{lem:composed_multinomial_properties}, we have that $A\coloneq \sup_{\twonorm{\x}\leq R}\cmkl{t}(\x,\x)\leq (2R)^{2^t\ell}$. From \Cref{lem:composed_multinomial_properties}, the entries of the kernel can be computed in $\poly(d,\ell)$ time and from \Cref{lem:hc_implies_kernelhc}, we have that $\Dtrainx$ is $\left(\cmkl{t},C,\ell\right)$ hypercontractive. Now, we obtain the result by applying \Cref{theorem:tds-via-kernels}.
\end{proof}

The following corollary on TDS learning two layer sigmoid networks in polynomial time readily follows. 
\begin{corollary}
\label{clry:polytime_tds_sigmoid_appendix}
    Let $\mathcal{F}$ on $\R^{d}$ be the class of two-layer neural networks with weight matrices $\W=(W^{(1)},W^{(2)})$ and sigmoid activations. Let $\normtwoinf{W^{(1)}}\leq O(1)$ and $\norm{\W}_1\leq W$. Suppose the training and test distributions satisfy the assumptions from \Cref{thm:tds_learning_sigmoid_appendix} with $R=O(1)$. Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression setting up to excess error $\epsilon$ and probability of failure $0.1$ in time and sample complexity $\poly(d,1/\epsilon,W,M)$.
\end{corollary}
\begin{proof}
    The proof immediately follows from \Cref{thm:tds_learning_sigmoid_appendix} by setting $t=2$ and the other parameters to the appropriate constants.
\end{proof}


\begin{theorem}[TDS Learning for Nets with Lipschitz Activation]
\label{thm:tds_learning_lipschitz_appendix}
Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with $L$-Lipschitz  activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
\begin{enumerate}
    \item $\Dtrainx$ is bounded within $\{\x:\twonorm{\x}\leq R\}$ and is $C$-hypercontractive for $R,C\geq 1$,
    \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
\end{enumerate}
Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is $\poly\left(d,\frac{1}{\epsilon},C^{\ell}, M,\log(1/\delta)^{\ell},(2R(k+\ell))^{O(\ell)}\right)$, where $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$. In particular, when $k=1$, we have that the time and sample complexity is $\poly(d,\frac{1}{\epsilon},C^{\ell},M,\log(1/\delta)^{\ell},(2R)^{O(\ell)})$ where $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}R/\epsilon\right).$
\end{theorem}
\begin{proof}
    From \Cref{thm:approx_lipschitz_nets}, for $k>1$ we have that $\mathcal{F}$ is $(\epsilon, (k+\ell)^{O(\ell)})$-approximately represented within radius $R$ w.r.t $\cmkl{1}$ where $\ell$ is a degree vector whose product is $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}Rk\sqrt{k}/\epsilon\right)$. For $k=1$, we have that we have that $\mathcal{F}$ is $(\epsilon, 2^{O(\ell)})$-approximately represented within radius $R$ w.r.t $\cmkl{1}$ where $\vell$ is a degree vector whose product is equal to $\ell=O\left(\normtwoinf{W^{(1)}}(WL)^{t-1}R/\epsilon\right)$. Also, from \Cref{lem:composed_multinomial_properties}, we have that $A\coloneq \sup_{\twonorm{\x}\leq R}\cmkl{t}(\x,\x)\leq (2R)^{O(\ell)}$. From \Cref{lem:composed_multinomial_properties}, the entries of the kernel can be computed in $\poly(d,\ell)$ time and from \Cref{lem:hc_implies_kernelhc}, we have that $\Dtrainx$ is $\left(\cmkl{1},C,\ell\right)$ hypercontractive. Now, we obtain the result by applying \Cref{theorem:tds-via-kernels}.
\end{proof}

The above theorem implies the following corollary about TDS learning the class of ReLUs. 

\begin{corollary}
\label{clry:polytime_tds_relu_appendix}
    Let $\mathcal{F}=\{\x\rightarrow \max(0,\vw\cdot \x):\twonorm{\vw}=1\}$ on $\R^{d}$ be the class of ReLU functions with unit weight vectors. Suppose the training and test distributions satisfy the assumptions from \Cref{thm:tds_learning_lipschitz_appendix} with $R=O(1)$. Then, \Cref{algorithm:tds-via-kernels} learns the class $\mathcal{F}$ in the TDS regression setting up to excess error $\epsilon$ and probability of failure $0.1$ in time and sample complexity $\poly(d,2^{O(1/\epsilon)},M)$.
\end{corollary}
\begin{proof}
    The proof immediately follows from \Cref{thm:tds_learning_lipschitz_appendix} by setting $t=2,\W=(\vw)$ and the activation to be the ReLU function. 
\end{proof}

In particular, this implies that the class of ReLUs is TDS learnable in polynomial time when $\epsilon<O(1/\log d)$.




\subsection{Applications for Unbounded Distributions}\label{sec:tds_uniform_appendix}

We are now ready to state our theorem for TDS learning neural networks with sigmoid activations.
\begin{theorem}[TDS Learning for Nets with Sigmoid Activation and Strictly Subexponential Marginals]
\label{thm:tds_learning_sigmoid_subexp_appendix}
Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with sigmoid activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
\begin{enumerate}
    \item $\Dtrainx$ is $\gamma$-strictly subexponential,
    \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
\end{enumerate}
Then, \Cref{algorithm:uniform-approx} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is at most \[\poly(d^{s},\log(1/\delta)^s),\] where $s=\left(k\log M\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)^{O(\frac{1}{\gamma})}.$
\end{theorem}
\begin{proof}
From \Cref{thm:approx_sigmoid_nets}, we have that $\mathcal{F}$ there is an $(\epsilon, R)$-uniform approximation polynomial for $f$ with degree $\ell=O\left((R\log R)\cdot (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}\right)$. Here, let $g_{\mathcal{F}}(\epsilon)\coloneq  (\normtwoinf{W^{(1)}}W^{t-2})\cdot (t\log(W/\epsilon))^{t-1}$. We also have that $r=\sup_{\twonorm{\x}\leq R,f\in \mathcal{F}}|f(\x)|\leq \poly(Rk\normtwoinf{W^{(1)}}W^{t-2})$ from the Lipschitzness of the sigmoid nets (\Cref{lem:neural_net_lipschitz}) and the fact that the sigmoid evaluated at $0$ has value $1$. The theorem now directly follows from \Cref{theorem:tds-via-uniform}.
\end{proof}

We now state our theorem on TDS learning neural networks with arbitrary Lipschitz activations.


\begin{theorem}[TDS Learning for Nets with Lipschitz Activation with strictly subexponential marginals]
\label{thm:tds_learning_lipschitz_subexp_appendix}
Let $\mathcal{F}$ on $\R^{d}$ be the class of  neural network with $L$-Lipschitz  activations, depth $t$ and weight matrices $\W=(W^{(1)},\ldots,W^{(t)})$ such that $\norm{W}_1\leq W$.  Let $\epsilon\in (0,1)$. Suppose the training and test distributions $\Dtrain,\Dtest$ over $\R^{d}\times \R$ are such that the following are true:
\begin{enumerate}
    \item $\Dtrainx$ is $\gamma$-strictly subexponential,
    \item The training and test labels are bounded in $[-M,M]$ for some $M\geq 1$.
\end{enumerate}
Then, \Cref{algorithm:uniform-approx} learns the class $\mathcal{F}$ in the TDS regression up to excess error $\epsilon$ and probability of failure $\delta$. The time and sample complexity is at most \[\poly(d^{s},\log(1/\delta^s),\] where $s=\left(k\log M\cdot\normtwoinf{W^{(1)}}(WL)^{t-1}/\epsilon\right)^{O(\frac{1}{\gamma})}$.
\end{theorem}
\begin{proof}
    From \Cref{thm:approx_lipschitz_nets}, we have that $\mathcal{F}$ there is an $(\epsilon, R)$-uniform approximation polynomial for $f$ with degree $\ell=O\left(Rk\sqrt{k}\cdot \normtwoinf{W^{(1)}}(WL)^{t-1}/\epsilon\right)$. Here, let $g_{\mathcal{F}}(\epsilon)\coloneq  k\sqrt{k}\normtwoinf{W^{(1)}}(WL)^{t-1}/\epsilon$. We also have that $r=\sup_{\twonorm{\x}\leq R,f\in \mathcal{F}}|f(\x)|\leq \poly(Rk\normtwoinf{W^{(1)}}W^{t-2})$ from the Lipschitz constant(\Cref{lem:neural_net_lipschitz}) and the fact that the each individual activation has value at most $1$ when evaluated at $0$ (see \Cref{def:Neural Network}. The theorem now directly follows from \Cref{theorem:tds-via-uniform}.
\end{proof}




\section{Assumptions on the Labels}\label{appendix:label-assumptions}

Our main theorems involve assumptions on the labels of both the training and test distributions. Ideally, one would want to avoid any assumptions on the test distribution. However, we demonstrate that this is not possible, even when the training marginal and the training labels are bounded, and the test labels have bounded second moment. On the other hand, we show that obtaining algorithms that work for bounded labels is sufficient even in the unbounded labels case, as long as some moment of the labels (strictly higher than the second moment) is bounded.


We begin with the lower bound, which we state for the class of linear functions, but would also hold for the class of single ReLU neurons, as well as other unbounded classes.

\begin{proposition}[Label Assumption Necessity]\label{proposition:bounded-labels-necessary}
    Let $\gF$ be the class of linear functions over $\R^d$, i.e., $\gF = \{\x\mapsto \vw\cdot \x: \vw\in\R^d, \|\vw\|_2 \le 1\}$. Even if we assume that the training marginal is bounded within $\{\x\in\R^d: \|\x\|_2\le 1\}$, that the training labels are bounded in $[0,1]$, and that for the test labels we have $\E_{y\sim \Dtest_y}[y^2] \le Y$ where $Y>0$, no TDS regression algorithm with finite sample complexity can achieve excess error less than $Y/4$ and probability of failure less than $1/4$ for $\gF$.
\end{proposition}

The proof is based on the observation that because we cannot make any assumption on the test marginal, the test distribution could take some very large value with very small probability, while still being consistent with some linear function. The training distribution, on the other hand, gives no information about the ground truth and is information theoretically indistinguishable from the constructed test distribution. Therefore, the tester must accept and its output will have large excess error. The bound on the second moment of the labels does imply a bound on excess error, but this bound cannot be made arbitrarily small by drawing more samples.


\begin{proof}[Proof of \Cref{proposition:bounded-labels-necessary}]
    Suppose, for contradiction that we have a TDS regression algorithm for $\gF$ with excess error $\eps < Y/4$ and probability of failure $\delta<1/4$. Let $m\in \sN$ be the sample complexity of the algorithm and $p\in(0,1)$ such that $m \ll 1/p$. We consider three distributions over $\R^d\times \R$. First $\gD^{(1)}$ outputs $(0,0)$ with probability $1$. Second, $\gD^{(2)}$ outputs $(0,0)$ with probability $1-p$ and $(\frac{\sqrt{Y}}{\sqrt{p}}\vw, \frac{\sqrt{Y}}{\sqrt{p}})$ with probability $p$, for some $\vw\in\R^d$ with $\|\vw\|_2 = 1$. Third, $\gD^{(3)}$ outputs $(0,0)$ with probability $1-p$ and $(\frac{\sqrt{Y}}{\sqrt{p}}\vw, 0)$ with probability $p$.

    We consider two instances of the TDS regression problem. The first instance corresponds to the case $\Dtrain = \gD^{(1)}$ and $\Dtest = \gD^{(2)}$. The second corresponds to the case $\Dtrain = \gD^{(1)}$ and $\Dtest = \gD^{(3)}$. Note that the assumptions we asserted regarding the test distribution and the test labels are true for both instances. For $\gD^{(2)}$, in particular, we have $\E_{y\sim \gD^{(2)}_y}[y^2] = p \cdot (\sqrt{Y}/\sqrt{p})^2 = Y$. Moreover, in each of the cases, there is a hypothesis in $\gF$ that is consistent with all of the examples (either the hypothesis $\x\mapsto 0$ or $\x\mapsto \vw\cdot \x$), so $\opt:= \min_{f\in\gF}[\gL_{\Dtrain}(f)] = 0 = \min_{f'\in\gF}[\gL_{\Dtrain}(f')+\gL_{\Dtest}(f')] =: \lambda$.

    Note that the total variation distance between $\gD^{(1)}$ and $\gD^{(2)}$ is $p$ and similarly between $\gD^{(1)}$ and $\gD^{(3)}$. Therefore, by the completeness criterion, as well as the fact that sampling only increases total variation distance at a linear rate, i.e., $\tv((\gD)^{\otimes m},(\gD')^{\otimes m}) \le m \cdot \tv(\gD,\gD') \le m\cdot p$, we have that in each of the two instances, the algorithm will accept with probability at least $1-m\cdot p-\delta$ (due to the definition of total variation distance\footnote{We know that the algorithm would accept with probability at least $1-\delta$ if the set of test examples was drawn from $(\Dtrainx)^{\otimes m}$. Since $(\Dtestx)^{\otimes m}$ is $(mp)$-close to $(\Dtrainx)^{\otimes m}$, no algorithm can have different behavior if we substitute $(\Dtrainx)^{\otimes m}$ with $(\Dtestx)^{\otimes m}$ except with probability $m\cdot p$. Hence, any algorithm must accept with probability at least $1-m\cdot p-\delta$.}).

    Suppose that the algorithm accepts in both instances (which happens w.p. at least $1-2\delta-2mp$). By the soundness criterion, with overall probability at least $1-4\delta-2mp$, we have the following.
    \begin{align*}
        p\cdot (h(\x) - 0)^2 &< Y/4 \\
        p\cdot (h(\x) - \sqrt{Y}/\sqrt{p})^2 &< Y/4
    \end{align*}
    The inequalities above cannot be satisfied simultaneously, so we have arrived to a contradiction. It only remains to argue that $1-4\delta-2mp >0$, which is true if we choose $p<\frac{1-4\delta}{2m}$. Therefore, such a TDS regression algorithm cannot exist.
\end{proof}

The lower bound of \Cref{proposition:bounded-labels-necessary} demonstrates that, in the worst case, the best possible excess error scales with the second moment of the distribution of the test labels. In contrast, we show that a bound on any strictly higher moment is sufficient.


\begin{corollary}\label{corollary:label-moment-bound-assumption-suffices}
    Suppose that for any $M>0$, we have an algorithm that learns a class $\gF$ in the TDS setting up to excess error $\eps\in(0,1)$, assuming that both the training and test labels are bounded in $[-M,M]$. Let $T(M)$ and $m(M)$ be the corresponding time and sample complexity upper bounds. 
    
    Then, in the same setting, there is an algorithm that learns $\gF$ up to excess error $4\eps$ under the relaxed assumption that for both training and test labels we have $\E[y^2g(|y|)]\le Y$ for some $Y>0$ and $g$ some strictly increasing, positive-valued and unbounded function. The corresponding time and sample complexity upper bounds are $T(g^{-1}(Y/\eps^2))$ and $m(g^{-1}(Y/\eps^2))$.
\end{corollary}

The proof is based on the observation that the effect of clipping on the labels, as measured by the squared loss, can be controlled by drawing enough samples, whenever a moment that is strictly higher than the second moment is bounded.


\begin{lemma}\label{lemma:label-moment-bound-suffices}
    Let $Y>0$ and $g:(0,\infty) \to (0,\infty)$ be strictly increasing and surjective. Let $y$ be a random variable over $\R$ such that $\E[y^{2}g(|y|)] \le Y$. Then, for any $\eps\in(0,1)$, if $M \ge g^{-1}(Y/\epsilon^2)$, we have $\sqrt{\E[(y-\clip_M(y))^2]} \le \epsilon$.
\end{lemma}

\begin{proof}[Proof of \Cref{lemma:label-moment-bound-suffices}]
    We have that $\E[(y-\clip_M(y))^2] \le \E[y^2 \ind\{|y|> M\}]$, because $y\ge\clip_M(y)$ and $y$, $\clip_M(y)$ always have the same sign, so $(y-\clip_M(y))^2 \ge y^2$ and also $(y-\clip_M(y))^2 = 0$ if $|y|\le M$. Since $g(|y|)$ is non-zero whenever $y>0$, we have $\E[y^2 \ind\{|y|> M\}] = \E[y^2 \cdot \frac{g(|y|)}{g(|y|)} \cdot \ind\{|y|> M\}]$. We now use the fact that $g$ is increasing to conclude that $\E[y^2 \ind\{|y|> M\}] \le \frac{\E[y^2g(|y|)]}{g(M)} \le \frac{Y}{g(M)}$. By choosing $M \ge g^{-1}(Y/\eps^2)$, we obtain the desired bound.
\end{proof}

We are now ready to prove \Cref{corollary:label-moment-bound-assumption-suffices}, by reducing TDS learning with moment-bounded labels to TDS learning with bounded labels.

\begin{proof}[Proof of \Cref{corollary:label-moment-bound-assumption-suffices}]
    The idea is to reduce the problem under the relaxed label assumptions to a corresponding bounded-label problem for $M = g^{-1}(Y/\eps^2)$. In particular, consider a new training distribution $\clip_M\circ\Dtrain$ and a new test distribution $\clip_M\circ\Dtest$, where the samples are formed by drawing a sample $(\x,y)$ from the corresponding original distribution and clipping the label $y$ to $\clip_M(y)$. Note that whenever we have access to i.i.d. examples from $\Dtrain$, we also have access to i.i.d. examples from $\clip_M\circ\Dtrain$ and similarly for $(\Dtestx, \clip_M \circ\Dtestx)$. Therefore, we may solve the corresponding TDS problem for $\clip_M\circ\Dtrain$ and $\clip_M\circ\Dtest$, to either reject or obtain some hypothesis $h$ such that \[
    \gL_{\clip_M\circ\Dtest}(h) \le \min_{f\in\gF}[\gL_{\clip_M\circ\Dtrain}(f)] + \min_{f'\in\gF}[\gL_{\clip_M\circ\Dtrain}(f')+\gL_{\clip_M\circ\Dtest}(f')] + \eps\]

    Our algorithm either rejects when the algorithm for the bounded labels case rejects or accepts and outputs $h$. It suffices to show $\gL_{\Dtest}(h) \le \min_{f\in\gF}[\gL_{\Dtrain}(f)] + \min_{f'\in\gF}[\gL_{\Dtrain}(f')+\gL_{\Dtest}(f')] + 4\eps$, because the marginal distributions do not change and completeness is, therefore, satisfied directly.

    It suffices to show that for any distribution $\Dtrain$, we have $|\gL_{\Dtrain}(h)- \gL_{\clip_M\circ\Dtrain}(h)| \le \eps$. To this end, note that $\gL_{\clip_M\circ\Dtrain}(h) = \sqrt{\E_{(\x,y)\sim\Dtrain}[(\clip_M(y) - h(\x))^2]}$. We have the following.
    \begin{align*}
        \gL_{\clip_M\circ\Dtrain}(h) &= \sqrt{\E_{(\x,y)\sim\Dtrain}[(\clip_M(y) - h(\x))^2]} \\
            &=\sqrt{\E_{(\x,y)\sim\Dtrain}[(\clip_M(y) -y+y- h(\x))^2]} \\
            &\le \sqrt{\E_{(\x,y)\sim\Dtrain}[(\clip_M(y) -y)^2]} + \sqrt{\E_{(\x,y)\sim\Dtrain}[(y-h(\x))^2]} \\
            &\le \eps + \gL_{\Dtrain}(h)
    \end{align*}
    The first inequality follows from an application of the triangle inequality for the $\gL_2$-norm and the second inequality follows from \Cref{lemma:label-moment-bound-suffices}. The other side follows analogously.
\end{proof}




