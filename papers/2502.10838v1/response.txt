\section{Related Work}
Speech deepfake detection research which was systematized by ASVspoof challenge campaings**Wang, "Deep Learning for Speech Deepfakes Detection"**. Such systematization and data collection has marked an interest in the development of speech deepfake detectors to such an extent that {\em equal error rates} (EERs) obtain are easily $1<\%$ when the training and evaluation sets are coming from the same corpus. But such a results do not carry over to cross-corpus studies, where training is done on one corpus and testing on another**Pang, "Cross-Corpus Speech Deepfakes Detection"**. But this is precisely the speech deepfake generalization task.

Regularization
Used continual learning in finetuning to make sure that finetuned model does not have catastrophic forgetting**Kemker, "Learning and Forgetting with Generative and Discriminative Models"**. Authors used full corpora to finetune the model. This in contrast to**Von KÃ¼gelgen, "Meta-Learning for Few-Shot Finetuning of Deep Neural Networks"**, where authors used meta-learning to learn a few-shot finetuning. Authors noticed that only a few samples from the unknown attacks were enough to significantly reduce the EER. In contrast to the present work, we do not {\em any} samples from the unknown attacks.

In**Ravanelli, "Pooled Data for Improved Generalization of LCNN Classifiers"**, authors pooled data from multiple corpora and applied 5-fold cross-validation to improve the generalization performance of the LCNN classifier**Ravanelli, "Pooled Data for Improved Generalization of LCNN Classifiers"**. And finally, speech foundation models have also been used with the idea that such a model after finetuning with the speech deepfake corpora will work well on the unseen attacks**Liu, "Speech Foundation Models for Speech Deepfakes Detection"**. In the present work, we take this as a baseline where our results are compared against. This system is called in our Tables Wav2Vec-AASIST. Parameters to be finetuned is extremely large, namely 317M, comparing to proposed model that obtains better performance with only 840K parameters.