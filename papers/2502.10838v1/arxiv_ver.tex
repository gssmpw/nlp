%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{multirow}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Generalizable speech deepfake detection via meta-learned LoRA
}

\begin{document}

\twocolumn[
\icmltitle{Generalizable speech deepfake detection via meta-learned LoRA
}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Janne Laakkonen}{equal,yyy}
\icmlauthor{Ivan Kukanov}{equal,comp}
\icmlauthor{Ville Hautamäki}{yyy}

\end{icmlauthorlist}

\icmlaffiliation{yyy}{School of Computing, University of Eastern Finland, Joensuu, Finland}
\icmlaffiliation{comp}{KLASS Engineering and Solutions, Singapore}


\icmlcorrespondingauthor{Janne Laakkonen}{janne.laakkonen@uef.fi}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Generalizable deepfake detection can be formulated as a detection problem where labels (bonafide and fake) are fixed but distributional drift affects the deepfake set. We can always train our detector with one-selected attacks and bonafide data, but an attacker can generate new attacks by just retraining his generator with a different seed. One reasonable approach is to simply pool all different attack types available in training time. Our proposed approach is to utilize meta-learning in combination with LoRA adapters to learn the structure in the training data that is common to all attack types.
\end{abstract}

\section{Introduction}
{\em Domain generalization}~\cite{Zhou_2022} in machine learning is a long-standing problem. The key goal is to learn a classifier that can generalize to an unseen evaluation condition, where the semantic classes used in the training time are still present. A classic example in this space is to train a classifier to recognize elephants from photos, and then apply the classifier to hand-drawn images of elephants. It appears that the marginal distribution of pixels in the elephant class is radically different from the marginal distribution of the pixels of hand drawn elephants. However, the cues for the elephant class, such as large earlobes or the trunk, are invariant over different visual representations of elephants. Thus, it seems to make sense to try to {\em learn} the commonality between different representations. 

It is easy to understand that many practical classification tasks turns out to be useless in a deployed environment if domain generalization is not taken into account. Users of machine learning are typically not interested to know what data was used to train the model, they just want the trained model to work in their use case. {\em Deepfake detection} is a case in point, it is easy to collect large amounts of {\em bonafide} face images or utterances, but what about examples from the {\em fake} or generated class? These generation methods, referred to as {\em attacks} in this work, are attack-dependent. It is always possible to train a detector that performs well on a fixed set of attacks. However, evaluating on an unknown attack -- or even the same generative model initialized with a different random seed -- results in significantly degraded performance. 

Domain generalization problem can be solved in many different ways, with the two prominent techniques being {\em data augmentation} and the use of {\em foundation model}. The idea behind both approaches is that either incorporating a large dataset (foundation model) or perturbing the training data may improve the model's ability to generalize to unseen evaluation data. 

These ideas have seen empirical success in the field of deepfake detection~\cite{oneata2023generalisable, Wang2023}, but they are bound to fail if the unseen attacks do not fit the preset perturbation pattern. Alignment techniques, on the other hand, aim to force the model to be domain agnostic to specific pre-set domains, i.e. the attacks need to be known in advance. 

\begin{table*}[t]
    \centering
    \caption{A summary of the most relevant previous research used as inspiration for our work. TF denotes transformers.}
    \begin{tabular}{ccccc}
        \toprule
        \thead{Paper}  & \thead{Description} & \thead{Algorithms} & \thead{Model} &  \thead{Uses \\Evaluation Data}  \\
        \midrule
        \makecell{Attack Agnostic \\ \cite{Kawa2022} } & \makecell{Pools data from different corpora\\ 5-fold CV}      & \makecell{ERM}         & LCNN & \makecell{No} \\
         & & \\
        \makecell{Wav2Vec+AASIST \\ \cite{Tak2022Automatic}} & \makecell{Coupling Wav2Vec 2.0 front-end with \\ AASIST back-end for deepfake detection }& \makecell{ERM} & TF + GNN & \makecell{No}\\
         & & \\
         \makecell{Wav2Vec+LoRA \\ \cite{zhang2023adaptivefakeaudiodetection}} & \makecell{Continual learning from \\ three corpora with finetuned LoRA}      & ERM & \makecell{TF + SENet \\ LoRA} & \makecell{ Yes \& No} \\
         & & \\
        \makecell{Adaptation with \\Meta-Learning \\ \cite{kukanov2024metalearningapproachesimprovingdetection}} & \makecell{Adapt Wav2Vec params. \\ with ProtoMAML and ProtoNet}    & \makecell{Meta}         & TF+GNN       & \makecell{Yes} \\
         & & \\
        \makecell{\textbf{(Ours)}} & \makecell{Finetunes LoRA parameters \\ using MLDG.\\
        Only one corpus is used.}       & \makecell{Meta}          & \makecell{ TF+GNN \\ LoRA}        & \makecell{No} \\
        \bottomrule
    \end{tabular}
    \label{tab:previous_research}
\end{table*}

When multiple attack types are available at the training time, we can pool all the training data and train just one model. This baseline approach in domain generalization is known as the {\em empirical risk minimization} (ERM). Interestingly, via theoretical arguments \cite{gouk2024limitationsgeneralpurposedomain} have shown that ERM is hard to beat in the domain generalization task. Fortunately, we are able to use specific nature of the deepfake detection task to overcome the theoretical limits.  With meta-learning, we can take advantage of the known differences between the attack types by explicitly simulating domain shifts during training. This optimization approach is called {\em meta-learning domain generalization} (MLDG) \cite{Li2017LearningTG}. It splits the available attacks into meta-train and meta-test subsets in each iteration, forcing the model to adapt to certain attacks while retaining performance on others. In doing so, MLDG encourages parameters that generalize more effectively across domains than standard ERM might allow. But, as alluded earlier, we would need to learn the cues that are common between all attack types. This is not learned with the MLDG. 

Low-rank adaptation (LoRA) has emerged as an efficient approach for fine-tuning large-scale transformer-based models with minimal computational overhead \cite{hu2021loralowrankadaptationlarge}. LoRA achieves this by introducing trainable low-rank decomposition matrices into the weight update process, therefore reducing the number of trainable parameters while maintaining model performance. Specifically for deepfake detection, LoRA application demonstrates enhanced detection accuracy while preserving efficiency. 
Recent studies have explored the integration of LoRA with transformer-based models for robust spoof speech detection.

In \cite{zhang2023adaptivefakeaudiodetection}, the authors introduced a low-rank adaptation specifically tailored for the Wav2Vec 2.0 model. This approach freezes the weights of the pre-trained model and injects trainable low-rank matrices, drastically reducing trainable parameters. The results indicated that this method not only preserved the model's accuracy on known spoof audio types but also improved performance for new and unseen spoofing attacks. The study emphasizes that LoRA's efficiency allows for lower memory requirements and faster adaptation to emerging threats in audio spoofing, making it a promising strategy for real-time applications.

In \cite{wang2023lowrankadaptationmethodwav2vec2based}, the authors analyze the advantages of using LoRA over conventional fine-tuning methods. Their experiments revealed that applying LoRA to Wav2vec 2.0 reduced the number of trainable parameters by a factor of $198$ while achieving performance comparable to full fine-tuning approaches. This finding underscores LoRA's potential in maintaining high detection accuracy without incurring significant computational costs. It is crucial in dynamic environments where new spoofing techniques frequently emerge and require prompt adaptation.

The integration of LoRA into model backbone enables efficient adaptation without extensive retraining of large models. Moreover, integrating LoRAs in different layers of backbone model helps to leverage abstract features' representation throughout the model. We summarize our contributions as: 
\begin{itemize}
    \item Present a novel way to combine the MLDG optimization method with LoRA-based neural representation architecture. 
    \item As far as we are aware, this is the first time that MLDG is used in speech deepfake detection.   
    \item Compared to other meta-learning applications for fake speech detection \cite{kukanov2024metalearningapproachesimprovingdetection}, the proposed approach is zero-shot adaptation method.
    \item Obtain promising empirical performance across corpus in multiple domains and on unknown attacks.
\end{itemize}

\section{Related Work}
Speech deepfake detection research which was systematized by ASVspoof challenge campaings~\cite{Todisco2019ASVspoof2F, ASVspoof2021, Wang2024ASVspoof5C}. Such systematization and data collection has marked an interest in the development of speech deepfake detectors to such an extent that {\em equal error rates} (EERs) obtain are easily $1<\%$ when the training and evaluation sets are coming from the same corpus. But such a results do not carry over to cross-corpus studies, where training is done on one corpus and testing on another~\cite{müller2024doesaudiodeepfakedetection}. But this is precisely the speech deepfake generalization task. 

Regularization~\cite{Chen2020}
Used continual learning in finetuning to make sure that finetuned model does not have catastrophic forgetting~\cite{Ma2021}. Authors used full corpora to finetune the model. This in contrast to~\cite{kukanov2024metalearningapproachesimprovingdetection}, where authors used meta-learning to learn a few-shot finetuning. Authors noticed that only a few samples from the unknown attacks were enough to significantly reduce the EER. In contrast to the present work, we do not {\em any} samples from the unknown attacks. 

In~\cite{Kawa2022}, authors pooled data from multiple corpora and applied 5-fold cross-validation to improve the generalization performance of the LCNN classifier~\cite{lavrentyeva2019stcantispoofingsystemsasvspoof2019}. And finally, speech foundation models have also been used with the idea that such a model after finetuning with the speech deepfake corpora will work well on the unseen attacks~\cite{oneata2023generalisable, Wang2023}. In the present work, we take this as a baseline where our results are compared against. This system is called in our Tables Wav2Vec-AASIST. Parameters to be finetuned is extremely large, namely 317M, comparing to proposed model that obtains better performance with only 840K parameters.


\section{Speech Deepfake Detection}
\emph{Speech deepfake detection} targets the task of distinguishing \textit{genuine} (bonafide) speech from \textit{spoofed} audio. Spoofed utterances are commonly produced by \emph{text-to-speech (TTS)} or \emph{voice conversion (VC)} algorithms, both of which can generate realistic utterances intended to deceive. 

One of the state-of-the-art models in the speech deepfake detection comprises self-supervisely pre-trained front-end and graph attention back-end, Wav2Vec-AASIST \cite{Tak2022Automatic}. The Wav2Vec-AASIST model is designed for detecting speech deepfakes by leveraging the capabilities of the Wav2Vec 2.0 framework, it capture complex audio features learned from thousands of hours of speech that are crucial for distinguishing between real and spoofed audio signals. It followed by the AASIST, the spectro-temporal graph attention module. It aggregates information from time frames and frequency bins,
improving sensitivity to anomalies in the audio signal, the output is a binary decision.


\section{Proposed Method}
A common strategy in speech deepfake detection involves fully fine-tuned large pre-trained models (e.g., Wav2Vec 2.0) on a pooled dataset of attacks . Although this empirical risk minimization (ERM) approach can yield strong in-domain performance, it typically requires updating hundreds of millions of parameters resulting in high memory usage, slow training, and increased risk of over-fitting . Furthermore, assuming single, pooled dataset may fail to account for domain shifts when new or more sophisticated attack types emerge.

To address both parameter efficiency and cross-domain generalization, we integrate low-rank adapter (LoRA) into a meta-learning domain generalization (MLDG) framework. LoRA confines most updates to small, rank-deficient matrices within the model's attention weights, drastically reducing the number of trainable parameters. Meanwhile, MLDG partitions attacks into distinct meta-train and meta-test domains, optimizing for generalizability across these domains rather than focusing on a single dataset. By combining LoRA and MLDG, we largely preserve the main SSL backbone while ensuring that the model learns adapt to varying attack types in a domain-aware manner.

We hypothesize that restricting fine-tuning to low-rank adapters prevents the model from over-fitting to a single, known attack distribution, while the meta-learning component enforces cross-domain consistency. This design seeks to preserve the broad knowledge captured by the pre-trained model while developing specialized parameters for handling unseen attacks. Consequently, we expect fewer trainable parameters, faster adaptation, and improved out-of-domain performance compared to prior full fine-tuning or purely on ERM-based training.

\begin{figure}[!htb]

\centerline{\hspace{-0.1cm}\includegraphics[scale=0.22]{lora_modules.png}}
\caption{LoRA modules integrated in self-attention heads of the Wav2Vec-AASIST model.}
\label{fig:lora_modules}
\end{figure}

\subsection{Neural model}

\textbf{Low-rank Adapters.} Initially low-rank adaptation (LoRA) is designed to efficiently fine-tune large language models \cite{hu2021loralowrankadaptationlarge}, Fig. \ref{fig:lora_modules}. The primary idea behind LoRA is to reduce the number of parameters needed for fine-tuning by approximating weight updates as low-rank matrices, rather than updating the entire model's parameters. 

The general weight update in a neural network is defined as
\begin{equation}
\text{W}' = \text{W} + \Delta \text{W}
\end{equation}
where $\text{W}$ represents the pre-trained weights of backbone model, and $\Delta \text{W}$ represents the change introduced by fine-tuning. In LoRA, $\Delta \text{W}$ is parameterized as the product of two low-rank matrices:
\begin{equation}
\Delta \text{W} = \text{A} \text{B}
\end{equation}
where $\text{A} \in \mathbb{R}^{d \times r}$ and $\text{B} \in \mathbb{R}^{r \times m}$  are low-rank matrices with the rank $r$, much smaller than the dimensions of $\text{W}$: $r \ll d,m$. This low-rank approximation drastically reduces the number of parameters that need to be learned, improving both the efficiency and flexibility of the fine-tuning process. LoRAs are typically added as aside modules to the attention weights or feed-forward layers in the transformer. This allows the pre-trained model to retain its general knowledge while adapting to specific task requirements with minimal computational overhead.

\textbf{Motivation for LoRA.} The explicit benefit of fine-tuning with LoRA modules is that the proposed approach enables rapid and efficient fine-tuning for both empirical risk minimization (ERM) and meta-learning with domain generalization (MLDG). The latter is a computationally intensive optimization problem that requires computing the second-order derivatives. LoRA is a feasible solution by reducing the number of trainable parameters and without compromising performance.

Unlike \cite{kukanov2024metalearningapproachesimprovingdetection}, where ProtoMAML only adapts the final model layer. Here, LoRA is incorporated across all transformer layers, facilitating the full use of deep feature representations from the entire model. LoRA learns the contribution of different layers of a backbone model, since specific layers are more beneficial for a downstream task \cite{Pasad2024}. 


Leveraging LoRA enables the model to disentangle information specific to downstream task. This framework is adaptable, allowing the system to flexibly learn domain-specific or attack-based information. Furthermore, it provides the flexibility to replace modules as required to accommodate evolving task requirements.


\subsection{Optimization}
We adopt a first-order variant of Meta-learning Domain Generalization (MLDG) \cite{Li2017LearningTG} for a speech deepfake detection task. Suppose we have $K$ distinct domains (attack types), and each domain $\mathcal{D}_k$ has a set of training samples {($x_j, y_j$)}. We also define a standard loss function $\ell(\hat{y},y)$, such as negative log-likelihood, comparing predicted $\hat{y}$ to $y$. In our binary classification setting (bonafide vs. spoof) the negative log-likehood takes the form:
\begin{equation}
    \ell(\hat{y}, y) \;=\; -\log p\bigl(y \mid x; \Theta\bigr),
\end{equation} where \(\hat{y}\) can be interpreted as the model's predicted  probability of the label \(y\). In practice, this is  implemented via a log-softmax output layer and a standard two-class NLL objective.

\begin{figure*}[!htb]
    \centering
    \fbox{\includegraphics[width=\textwidth]{mldg_diagram.pdf}} 
    \caption{Methodological framework for generalizable speech deepfake detection using MLDG. Attack domains are randomly split into meta-train and meta-test subsets, enabling the model to simulate domain shifts, compute meta-train and meta-test losses, and optimize for robust generalization across unseen attack types.}
    \label{fig:mldg_diagram}
\end{figure*}


Let $\mathcal{S}$ denote the entire set of domains in a training batch. We randomly select a subset of domains to serve as \textbf{\textit{meta-train}}($\overline{\mathcal{S}}$) and another subset to serve as \textbf{\textit{meta-test}} ($\breve{\mathcal{S}}$). Speci\-fi\-cally, we partition the domain indices and treat $\overline{\mathcal{S}}$ for adaptation and $\breve{\mathcal{S}}$ for validation.

In the meta-training phase we perform a single-step adaptation. We gather all data from the meta-train domains $\overline{\mathcal{S}}$ and aggregate a loss function $F(\Theta)$:
\begin{equation}
    F(\Theta) = \frac{1}{|\overline{\mathcal{S}}|} \sum_{k \in \overline{\mathcal{S}}} \frac{1}{N_k} \sum_{(x_j, y_j) \in \mathcal{D}_k} \ell_{\Theta}(x_j,y_j),
\end{equation}where $\Theta$ is our current model parameters. We compute $\nabla_\Theta F(\Theta)$ and update with a single gradient step:
\begin{equation}
    \Theta' \leftarrow \Theta - \alpha \nabla_{\Theta}F(\Theta),
\end{equation}simulating the adaptation to $\overline{\mathcal{S}}$ with $\alpha$ as the meta-train learning rate.

In the meta-testing phase we evaluate the adapted parameters $\Theta'$ on the meta-test domains $\breve{\mathcal{S}}$. The meta-test loss is:
\begin{equation}
    G(\Theta') = \frac{1}{|\breve{\mathcal{S}}|} \sum_{k \in \breve{\mathcal{S}}} \frac{1}{N_k} \sum_{(x_j, y_j) \in \mathcal{D}_k} \ell_{\Theta'}(x_j,y_j).
\end{equation}

In the original MLDG formulation \cite{Li2017LearningTG}, we compute the gradient of $G(\Theta')$ through the update step $\Theta' = \Theta - \alpha \nabla_{\Theta} F(\Theta)$, yielding a second-order term $\nabla_{\Theta} G(\Theta') \bigr\rvert_{\Theta'(\Theta)}$. While this can be more accurate, it also includes higher computational cost in the form of second-order derivatives.

In our first-order approach, we do not compute the higher-order gradient of $G(\Theta')$ w.r.t. $\Theta$, resulting in a simpler, more efficient update step. The meta-train and meta-test losses are combined into a single objective, treating $\Theta'$ as a single-step adaptation of $\Theta$:
\begin{equation}
    \min_{\Theta} F(\Theta) + \beta G(\Theta - \alpha \nabla_{\Theta}F(\Theta)),
\end{equation} where $\beta$ balances the meta-train and meta-test terms.

\begin{algorithm}[tb]
   \caption{First-order MLDG algorithm}
   \label{alg:fomldg}
\begin{algorithmic}
   \STATE {\bfseries Input:} domains $\mathcal{S}$
   \STATE \textbf{Initialize:} Model parameters $\Theta$; hyperparameters $\alpha, \beta, \gamma$.
   \FOR{\texttt{iteration} = 1 \textbf{to} \texttt{maxIters}}
    \STATE \textbf{Split:} $(\overline{S}, \check{S}) \leftarrow S$
    
    \STATE \textbf{Meta-train:} Compute $\nabla_\Theta = F'(\overline{S}; \Theta)$
    \STATE updated parameters $\Theta' \leftarrow \Theta - \alpha\,\nabla_\Theta$
    
    \STATE \textbf{Meta-test:} Evaluate $G(\check{S}; \Theta')$ 
   
    \STATE \textbf{Meta-optimization (First-Order)}:
    \[
       \Theta \;\leftarrow\; \Theta 
         \;-\; \gamma\;\Bigl(
           \nabla_\Theta F(\overline{S}; \Theta) 
           \;+\; \beta\, \nabla_{\Theta'}G(\check{S}; \Theta')
         \Bigr)
    \]
\ENDFOR

\end{algorithmic}
\end{algorithm}
\section{Experimental Setup}
\subsection{Datasets}
Data used to train and evaluate the trained models are summarized in Table~\ref{tab:datasets}. To train and validate each model, we employed the Logical Access (LA) partition of the ASVspoof2019 dataset \cite{Todisco2019ASVspoof2F}, which is one of the most commonly used benchmarks in speech deepfake detection research. The dataset consists of English speech recordings derived from the VCTK corpus, covering both genuine (bonafide) and spoofed utterances. The spoofed data is generated by a variety of text-to-speech (TTS) and voice conversion (VC) algorithms, collectively referred as to "attacks". The training and development subsets share six known attack types, labeled A01-A06. The evaluation set features additional attacks, labeled A07-A19, many of which differ from those seen in training.

\begin{table*}[tb]
\footnotesize
\centering
\caption{Summary of datasets (corpora) used in the experiments. Horizontal line divides datasets used in training and evaluation. The “\#Attacks” column denotes the number of distinct spoofing algorithms; 
“N/R” indicates that the count is not reported. 
For the ASVspoof 2021 DF subset, more than 100 TTS and VC methods are used, 
combining audio from the ASVspoof2019-LA evaluation set and 
the Voice Conversion Challenge (VCC) 2018/2020 under various compression conditions, 
resulting in the large number different attacks.}\label{tab:datasets}
\setlength{\tabcolsep}{1.7mm}{
\begin{tabular}{lcccc}
\hline
\toprule
Dataset & Usage & \#Bonafide & \#Spoofed & \#Attacks \\ \hline
\midrule
ASVSpoof 2019 LA Train \cite{Todisco2019ASVspoof2F} & Training & 2,580 & 22,800 & 6 \\
ASVSpoof 2019 LA Dev \cite{Todisco2019ASVspoof2F} & Training &  2,548 & 22,296 & 6 \\ \hline
ASVSpoof 2019 LA Eval \cite{Todisco2019ASVspoof2F}& Evaluation & 7,355 & 63,882 & 13 \\
ASVSpoof 2021 LA Eval \cite{ASVspoof2021}& Evaluation & 14,816 & 133,360 & 13\\
ASVSpoof 2021 DF Eval \cite{ASVspoof2021}& Evaluation & 14,869 & 519,059 & 100+ \\
InTheWild \cite{müller2024doesaudiodeepfakedetection}& Evaluation & 19,963 & 11,816 & N/R \\
FakeAVCeleb \cite{khalid2022fakeavcelebnovelaudiovideomultimodal}& Evaluation & 10,209 & 11,335 & 1\\
ASVSpoof 5 LA Eval \cite{Wang2024ASVspoof5C}& Evaluation & 138,688 & 542,086 & 32\\
\bottomrule
\hline
\end{tabular}
}
\label{tab:results}
\end{table*}

To assess the generalization capability of our methods, we evaluate the models against both in-domain and out-of-domain datasets. The in-domain dataset include the evaluation sets of ASVSpoof 2019 LA, ASVSpoof 2021 LA and ASVSpoof 2021 DF \cite{ASVspoof2021}. The out-of-domain dataset are represented by In-The-Wild \cite{müller2024doesaudiodeepfakedetection} and FakeAVCeleb \cite{khalid2022fakeavcelebnovelaudiovideomultimodal}. Additionally, we evaluate our models against the recent ASVSpoof 5 \cite{Wang2024ASVspoof5C} evaluation set.
\subsection{Training Strategy}

\textbf{Baseline models.} We adopt Wav2Vec-AASIST as our primary baseline system, building on methods previously described in \cite{kukanov2024metalearningapproachesimprovingdetection}. Specifically, we leverage Wav2Vec 2.0 XLSR-53 (featuring a 1024-dimensional output embedding) as the front-end, coupled with an AASIST back-end -- a spectro-temporal graph attention network for speech deepfake detection. In this configuration, both the SSL front-end and the AASIST back-end parameters are jointly trained. In a variant we denote Wav2Vec-AASIST*, the Wav2Vec 2.0 component is frozen throughout training, and only the graph attention back-end is allowed to update. Both versions are trained using standard empirical risk minimization (ERM), in which we simply use the combined training data (bonafides + all spoofed attacks (A01-A06)) as a single dataset. For each of the baseline and all the consequent models we use the AdamW \cite{loshchilov2019decoupledweightdecayregularization} optimizer with a cyclic learning rate scheduler, oscillating between a minimum of $1 \times 10^{-7}$ and a maximum of $1 \times 10^{-5}$ over each cycle. We optimize a negative log-likelihood loss over two-class (bonafide vs spoof) log-softmax outputs. As a stopping criterion, we monitor validation performance using the ASVSpoof 2019 LA validation subset, and terminate training if no improvement occurs for more than 10 epochs, saving the best-performing checkpoint for final testing.

\textbf{ERM LoRA models.} In addition to the baseline Wav2Vec-AASIST variants, we implement LoRA within the Wav2Vec 2.0 encoder's self-attention modules. For models presented in Table \ref{tab:results}, LoRA adapters are inserted at the query, key, value, and out projection operations in each self-attention block of the Wav2Vec 2.0 encoder. We explore four LoRA configurations, differing only in the rank $r\in \{2,4,8,16\}$. In each variant, we keep the scaling factor fixed as 2. During training, only these LoRA parameters and the AASIST back-end parameters are unfrozen --the remainder of the Wav2Vec 2.0 front-end remains frozen. The ERM training scheme is used with the same optimizer and hyperparameters as with the baseline models.

\textbf{MLDG LoRA models.} We also explore LoRA-based models under a meta-learning regime rather than a straightforward ERM. We adopt a first-order MLDG approach to encourage domain generalization across six attack types (A01-A06) in the ASVSpoof 2019 LA training set. We partition the bonafide samples among these six subsets, each paired with the corresponding spoofed attack, resulting in six domain-labeled datasets  \(\{\mathcal{D}_1,\dots,\mathcal{D}_6\}\). At each training iteration we sample a mini-batch from each domain, then designate some subset as meta-train and the rest as meta-test. We create a clone of the main model's parameters $\Theta$, perform a single gradient step using the Adam \cite{kingma2017adammethodstochasticoptimization} optimizer with a fixed learning rate $\alpha=0.001$ on the meta-train data, and obtain adapted parameters $\Theta'$. Next, we evaluate $\Theta'$ on the meta-test domain(s), compute a first-order gradient, and accumulate it, scaled by a factor $\beta$, into the main model's gradient buffer. Finally, we update the main model with AdamW, applying early stopping based on validation performance.   

\section{Results}

An ERM LoRA model (rank=8, $\alpha$=2) was trained under different random seeds to assess initialization variability, with EER results across five evaluation sets summarized in Table~\ref{tab:seed_variation}. The mean and standard deviation in the last two rows indicate that certain seeds (e.g., 999 or 123) yield higher EER, whereas others (e.g., 555) exhibit notably lower EER, highlighting the effect of random initialization on final performance.

Table~\ref{tab:lora_placement} lists the EER (\%) on the ASVspoof2019 LA evaluation set for various LoRA adapter placements and ranks (r=4,8,16). Each row corresponds to one or more attention weight matrices $W_q$(query), $W_k$(key), $W_v$(value), and $W_{out}$(output projection) where LoRA is inserted. The table shows how EER varies based on which attention weights are adapted and which rank \textit{r} is used. Overall, the table highlights the variability in EER across different LoRA placements and ranks for in-domain evaluation.

In Fig.~\ref{fig:mldglora_valloss} we see learning curves, validation losses, of optimizing LoRA parameters using MLDG in our setup. We plot learning curves for LoRA ranks 2, 4, 8 and 16. What we notice is that the saturated, after around 10k steps, validation loss is very similar across all tested ranks. Learning curves exhibit very clean validation performance showing that the proposed combination of LoRA and MLDG converges. 


\begin{figure}[!htb]
\vspace{-0.1 in}
\centerline{\hspace{-0.2cm}\includegraphics[scale=0.45]{lora_val_loss_plot.pdf}}
\caption{Validation losses of optimizing LoRA using MLDG. Plotting learning curves for LoRA ranks 2, 4,  8 and 16.}
\label{fig:mldglora_valloss}
\end{figure}

\begin{table*}[!htb]
\centering
\caption{Comparison of fully fine-tuned baseline model vs models with LoRA adapters in $W_q,W_k,W_v,W_{out}$, using both ERM and meta-learning (MLDG) training strategies. Performance is reported in terms of EER (\%), where bolded numbers are the best in each column and underlined are the second best.}

\setlength{\tabcolsep}{0.8mm} % Reduce column spacing
\renewcommand{\arraystretch}{1.1} % Slightly increase row height for readability

\resizebox{\textwidth}{!}{ % Scale to fit page width
\begin{tabular}{lccccccccccccccccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{\shortstack{Trainable\\ Params.}} & \multirow{2}{*}{LoRA Rank} 
& \multicolumn{2}{c}{ASV19:LA} & \multicolumn{2}{c}{ASV21:LA} & \multicolumn{2}{c}{ASV21:DF} 
& \multicolumn{2}{c}{InTheWild } & \multicolumn{2}{c}{FakeAVCeleb} & \multicolumn{2}{c}{ASV5} & \multicolumn{2}{c}{Avg.} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17}
& & & ERM & MLDG & ERM & MLDG & ERM & MLDG & ERM & MLDG & ERM & MLDG & ERM & MLDG & ERM & MLDG \\
\midrule
Wav2Vec-AASIST & 317.8M & -- & \textbf{0.28} & -- & 5.84 & -- & 5.29 & -- & 14.03 & -- & 7.98 & -- & 23.88 & -- & 8.55 & -- \\
Wav2Vec-AASIST* & 447K & -- & \underline{0.36} & -- & 4.29 & -- & 7.97 & -- & 19.41 & -- & 4.84 & -- & 17.14 & -- & 9.00 & -- \\
\midrule
\multirow{4}{*}{LoRA} 
& 840K  & 2  & 0.54 & 0.68 & 8.27 & 4.91 & \textbf{3.85} & 4.90 & 13.56 & 13.96 & 14.81 & \underline{3.26} & 27.97 & 16.02 & 11.50 & 7.29 \\
& 1.23M & 4  & 0.41 & \underline{0.36} & 10.50 & 4.33 & 4.37 & \underline{4.33} & 10.69 & \textbf{8.53} & 9.17 & \textbf{2.30} & 25.97 & \textbf{13.39} & 10.18 & \textbf{5.54} \\
& 2.02M & 8  & 0.61 & 0.38 & 5.50 & \textbf{3.31} & 4.33 & 5.03 & 13.15 & \underline{9.61} & \underline{3.97} & 3.71 & 21.05 & \underline{15.82} & 8.22 & \underline{6.25} \\
& 3.59M & 16 & 0.39 & 0.42 & 5.32 & \underline{4.07} & 5.02 & 4.91 & 12.38 & 11.55 & 6.22 & 9.10 & 20.38 & 16.39 & 8.29 & 7.74 \\
\bottomrule
\end{tabular}
}
\label{tab:results}
\end{table*}


Table \ref{tab:results} compares the EER (\%) various models across several evaluation sets, including in-domain and out-of-domain scenarios. The Wav2Vec-AASIST model, which fully finetunes both the front-end and the back-end components, achieves a low EER of 0.28\% on the ASVSpoof19 LA evaluation set, but shows higher error rates on the out-of-domain datasets. By contrast, Wav2Vec-AASIST*, which freezes the SSL front-end, has fewer trainable parameters (447k vs 318M) but yields weaker average performance. Under the ERM training scheme, some LoRA configurations excel on certain datasets --for instance, ERM Lora with rank=2 obtains the best EER on ASVSpoof 2021 DF (3.85\%) with only 840k trainable parameters. The average EER across all datasets for ERM LoRA models typically remains around 8-11\%, giving them comparable performance with significantly reduced parameter counts compared to the fully fine-tuned model. Swapping ERM for MLDG while keeping the same LoRA placements and ranks substantially improves cross-domain generalization. MLDG LoRA r=4 provides the best overall average EER of 5.54(\%), and also excels on In-The-Wild (8.53\%), FakeAVCeleb (2.30\%), and ASVSpoof 5 (13.39\%).

\begin{table}[htb]
\footnotesize
\centering
\caption{Impact of LoRA placement and rank on ASVspoof2019 LA eval set performance. The table shows EER (\%) for different attention weight matrices (Q, K, V, Out-Projection).}
\setlength{\tabcolsep}{1.7mm}{
\begin{tabular}{lcccc}
\hline
\toprule
Weights  & r=4 & r=8 & r=16 \\ \hline
\midrule
$W_q$ & 0.94 & 1.31 & 1.03 \\ 
$W_k$ & 0.86 & 0.75 & 0.78 \\ 
$W_v$  & 0.65 & 1.10 & 0.24 \\ 
$W_{out}$  & 1.22 & 0.87 & 0.71 \\ 
$W_q,W_k$ & 0.90 & 0.58 & 0.56 \\ 
$W_q,W_v$ & 0.29 & 0.73 & 0.77 \\ 
$W_q,W_{out}$ & 0.79 & 0.49 & 0.44 \\ 
$W_k,W_v$	& 0.28  & 0.58 & 0.33 \\ 
$W_k,W_{out}$ & 0.62 & 0.40 & 0.30 \\ 
$W_v,W_{out}$ & 0.34 & 0.69 & 0.21 \\ 
$W_q,W_k,W_v$ & 0.33 & 0.26 & 0.17 \\
$W_q,W_k,W_{out}$ & 0.67 & 0.67 & 0.54 \\
$W_q,W_v,W_{out}$ & 0.37 & 0.24 & 0.23 \\
$W_k,W_v,W_{out}$ & 0.50 & 0.45 & 0.28 \\
$W_q,W_k,W_v,W_{out}$ & 0.36 & 0.38 & 0.27 \\
\bottomrule
\hline
\end{tabular}
}
\label{tab:lora_placement}
\end{table}

\begin{table*}[!htb]
\footnotesize
    \centering
    \caption{EER (\%) for an ERM LoRA (rank=8, $\alpha$=2) model under different random seeds, with average EER reported over the five evaluation datasets; the final rows report the mean and standard deviation across seeds.}
    \setlength{\tabcolsep}{1.7mm}{
    
    
    \begin{tabular}{rcccccc}
    \hline
    \toprule
    \#Seed & ASV19:LA & ASV21:LA & ASV21:DF & InTheWild & FakeAVCeleb & Avg. \\ \hline
    \midrule
    2023 &  0.35 & 5.24 & 4.41 & 9.82 & 2.53  & 4.47\\
    42 & 0.37 & 4.95 & 3.44 & 9.67 & 2.42 & 4.17 \\
    999 & 0.65 & 7.45 & 4.87 & 10.70 & 9.57 & 6.65\\
    123 & 0.35 & 10.79 & 5.51 & 13.66 & 11.16 & 8.29 \\
    555 & 0.33 & 5.46 & 4.27 & 7.24 & 1.24 & 3.65\\
    \midrule
    \textbf{Mean} & 0.41 & 6.78 & 4.50 & 10.22 & 5.38 & 5.45\\
    \textbf{Std.\ Dev.} & 0.14 & 2.45 & 0.76 & 2.31 & 4.61 & 1.95\\
    \end{tabular}
    }
    \label{tab:seed_variation}
\end{table*}




\section{Conclusions}
Generalization in speech deepfake detection has been a challenging problem to solve. How can one train for attack conditions that are not available in advance? Findings in the literature have been discouraging, as models trained on one deepfake detection corpus often fail to generalize to completely new corpora. However, the results in~\cite{kukanov2024metalearningapproachesimprovingdetection} were promising, updating the model with just a few samples from the evaluation corpus was enough to significantly improve the generalization performance. 

In this work, we demonstrate that zero-shot learning is feasible. The key insight is to update the low-rank approximated model parameters, LoRA, instead of the model parameters directly. The idea goes a long way to learning how to extract cues from generated speech that remain invariant across different attack types. We also demonstrate that LoRA alone, as the neural model, is not sufficient; but the use of meta-learning is essential to significantly improve over using just the baseline ERM training strategy. Therefore, it is the combination of LoRA and the MLDG training algorithm that is the key to generalizable speech deepfake detection. 

In future work, we plan to attack the visual deepfake detection problem with a similar set of tools. Our assumption is that the same basic idea should also be applicable to detecting generated faces in the visual modality. Finally, we plan to explore the learned invariant features in the trained LoRA weight space. 

\newpage

\section*{Acknowledgements}
VH was partially supported by the Jane and Aatos Erkko Foundation. The authors acknowledge CSC, IT Center for Science, Finland, for computational resources. Part of the computational analyzes were performed on servers provided by UEF Bioinformatics Center and Biocenter Kuopio, University of Eastern Finland, Finland.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. Task taken here, speech deepfake detection, has an immense societal need. Users of social media need to know when a given video has authentic speech from a user and when it is machine generated. However, detection methods need to be trained in a way that they can respond to previously unseen attacks which is precisely the topic of the present paper. Speech deepfake detection methods that work well on one corpus but fail on another are practically useless in the real world use cases.


\bibliography{example_paper}
\bibliographystyle{icml2025}


\end{document}

