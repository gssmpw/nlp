\documentclass[]{bytedance_seed}
\usepackage[toc,page,header]{appendix}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{minitoc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{fancybox}
\usepackage{xcolor}
\usepackage{pifont} % For checkmark and cross symbols
\usepackage{tikz}
\newcommand{\greencheck}{{\color{green}\checkmark}} 
\newcommand{\redcross}{{\color{red}$\times$}} 
% Option 1: Approx symbol
\newcommand{\semicorrect}{\textcolor{orange}{\ensuremath{\triangle}}}
% \usepackage[capitalize,noabbrev]{cleveref}





%%%%%%%%%%%%%%%%%%%%

\title{MIR-Bench: Benchmarking LLM's Long-Context 
 Intelligence via Many-Shot In-Context Inductive Reasoning}

\author[1,2,*,\dagger]{Kai Yan}
\author[1]{Zhan Ling}
\author[1]{Kang Liu}
\author[1]{Yifan Yang}
\author[1]{Ting-Han Fan}
\author[1]{Lingfeng Shen}
\author[1]{Zhengyin Du}
\author[1,\dagger]{Jiecao Chen}


\affiliation[1]{ByteDance Seed}
\affiliation[2]{University of Illinois Urbana-Champaign}


\contribution[*]{Work done at ByteDance}
\contribution[\dagger]{Corresponding authors}

\abstract{
Inductive Reasoning (IR), the ability to summarize rules from examples and apply on new ones, has long been viewed as a primal ability for general intelligence and widely studied by cognitive science and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually $<$10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations are mostly focused on classification (a very limited aspect of IR), and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for inductive reasoning and many-shot ICL, including robustness against erroneous shots and the effect of Chain-of-Thought (CoT), and acquired insightful findings. 
% \citep{li2024llava}
}

\date{\today}
\correspondence{Kai Yan at \email{kaiyan3@illinois.edu}, Jiecao Chen at \email{jiecao.chen@bytedance.com}}

% You can add additional info fields as follows 
\checkdata[Project Page]{https://github.com/KaiYan289/MIR-Bench}
\checkdata[Dataset]{https://huggingface.co/datasets/kaiyan289/MIR-Bench}

\begin{document}

\maketitle

%\newpage
%\tableofcontents
\input{sections/introduction}
\input{sections/relatedwork}
\input{sections/approach}
\input{sections/experiments}
\input{sections/conclusion}


\newpage

\bibliographystyle{plainnat}
\bibliography{main}

\clearpage

\beginappendix

\input{sections/appendix}

\end{document}