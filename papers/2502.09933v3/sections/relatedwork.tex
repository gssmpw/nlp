\section{Related Work}
\textbf{Long context LLMs.} Recent remarkable success of LLMs have given rise to expectations for LLMs to complete more difficult tasks, such as summarization of a whole book~\citep{chang2023booookscore}, modification over a complex code repository~\citep{jimenez2023swe}, test-time improvement~\citep{zhou2023language} and journey learning~\citep{qin2024o1}. To make sufficient room for related context and meet such demands, researchers have scaled up LLM models and data~\citep{zhang2024scaling, pearce2024scaling}, and proposed novel encoding methods such as Rotational Position Embedding (RoPE)~\citep{su2024roformer}, YaRN~\citep{peng2023yarn} and LongRoPE~\citep{ding2024longrope}. With such designs, LLMs have entered the long-context era where the LLM context lengths can reach 128K~\citep{hurst2024gpt, yang2024qwen2technicalreport, dubey2024llama}, 2M~\citep{team2024gemini}, or even an infinite number of tokens~\citep{munkhdalai2024leave}, enabling the novel many-shot ICL~\citep{agarwal2024many} paradigm.
To evaluate such models, many benchmarks have been proposed to evaluate LLM's long-context ability~\citep{wang2024loong, li2023loogle, zhang2024infty}, such as Question-Answering~\citep{shaham2023zeroscrolls, li2023loogle}, coding~\citep{zhang2024infty, dong2023bamboo}, math~\citep{an2023eval, zhang2024infty}, retrival~\citep{niah, hsieh2024ruler, wang2024loong} and summarization~\citep{shaham2023zeroscrolls, an2023eval}. However, very few long-context benchmarks consider inductive reasoning tasks. Among them, LongBench~\citep{bai2023longbench} only contains two many-shot classification tasks and few-shot summarization / QA tasks with existing dataset, while BABILong~\citep{kuratov2024babilong} only considers simple inductive reasoning from a few examples scattering in the long context. In contrast, our benchmark is a more diverse and large-scale evaluation for long-context inductive reasoning from many pieces of information.

\textbf{Many-Shot In-Context Learning (ICL).} Many-Shot ICL~\citep{agarwal2024many} is an emerging in-context learning paradigm where LLMs learn to complete new task with hundreds to thousands of examples (instead of the usual $<10$ examples for ICL works~\citep{li2024mirage, chollet2019measure, xu2023llms}) given in its context. Compared to Supervised Fine-Tuning (SFT), many-shot ICL makes full use of the current models' long-context capability, and is much more flexible with higher computational and data efficiency~\citep{agarwal2024many}. There are a large number of many-shot ICL empirical studies~\citep{bertsch2024context, zhao2024probing, song2024can, zhao2024context} with several benchmarks~\citep{yen2024helmet,li2024long, zou2024retrieval, ruoss2024lmact} containing many-shot ICL components; however, most of them only focused on classification~\citep{zhao2024probing, li2024long, li2023context, jiang2024many, bertsch2024context}, a very limited type of problems. While there are several works that studies decision-making~\citep{ruoss2024lmact}, math~\citep{agarwal2024many}, instruction following~\citep{zhao2024context} and LLM judges~\citep{song2024can}, none of the existing works has studied general inductive reasoning, the important measurement of intelligence level~\citep{chollet2019measure}. Also, most of the existing many-shot ICL evaluations are not diverse enough, which means they only have one pair of input-output types~\citep{ruoss2024lmact, jiang2024many}. Our work, on the contrary, measures LLM's intelligence level using inductive reasoning with diverse input-output types.

\textbf{Inductive reasoning.} Inductive reasoning~\citep{hayes2010inductive} has been widely studied as a primal mental ability of human intelligence~\citep{Kinshuk01052006} in IQ tests~\citep{ferrara1986children} and cognitive science~\citep{bisanz1994inductive, heit2000properties} long before LLMs exist, as it represents an abstract generalizability of rules from given examples without prior knowledge. As LLMs approaches human-level intelligence recently, many inductive reasoning-based approaches~\citep{qiu2023phenomenal, wang2023hypothesis} and benchmarks have been proposed~\citep{chollet2019measure,ma2024kor,  li2024mirage, xiao2024logicvista, banatt2024wilt}, among which the most representative is the Abstract Reasoning Corpus (ARC)~\citep{chollet2019measure} and its variants~\citep{kim2022playgrounds, xu2023llms}, which is recently used to demonstrate the intelligence level of OpenAI o3 models~\citep{o3}. However, none of the existing inductive reasoning benchmarks is designed for many-shot scenario (There is only one small mini-SCAN~\citep{qiu2023phenomenal, lake2019human} dataset appearing in ~\citet{qiu2023phenomenal} satisfies the definition of many-shot). By filling in this gap, our many-shot inductive reasoning benchmark not only enables the LLM inductive reasoning community to catch up with the long-context era, but also tests the ability of LLMs to gather information from thousands of pieces of data, a much larger number than existing inductive reasoning problems~\citep{chollet2019measure, li2024mirage}.

\textbf{Programming-by-Examples (PbE).} PbE~\citep{myers1986visual, cypher1993watch} is a classic programming paradigm where programs are automatically written with user-provided input-output pairs as examples; it can be seen as an application of inductive reasoning in coding, and has wide application in sheet processing~\citep{gulwani2011automating}, data parsing~\citep{leung2015interactive}, and systematic drawing~\citep{cheema2014practical}. It is traditionally addressed by symbolic-based approaches, such as heuristic search~\citep{gulwani2011automating, cheema2014practical}, version space algebra~\citep{lau2003programming} and learning weights for rule probabilities~\citep{menon2013machine}; this symbolic formulation has largely limited the generalizability of PbE. Recently, as LLMs have proved themselves to be strong coders~\citep{guo2024deepseek}, several works tried to address general-purpose PbE with LLMs~\citep{shi2023lambdabeam, shi2023exedec,shao2024case2code, li2024programming}. None of them, however, considers many-shot scenario with more than $10$ shots. Compared to existing works, Our benchmark is organized in a way that resembles many-shot PbE paradigm, but for most of the evaluations, the LLMs we tested are not required to write code; instead, they only need to directly predict output for new input. That being said, with minimal adaptation, our proposed benchmark can fill in the blank of many-shot PbE study (and we explored this in Sec.~\ref{sec:coding}).