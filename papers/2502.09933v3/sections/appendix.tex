%\newpage
%\onecolumn
%\section*{Appendix: MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot In-Context Inductive Reasoning}
\appendix
The appendix is organized as follows. First, we conduct an extended comparison to all related many-shot ICL or inductive reasoning works to further illustrate the position of our work in Sec.~\ref{sec:ext_related_work}. Then, in Sec.~\ref{sec:prompt}, we introduce more details in our experiments, including the prompts we adopted in our curation of dataset and ablation experiments and the regex rule we used for extracting the answer. After these, we provide statistical features of MIR-Bench in Sec.~\ref{sec:feature}, and more empirical analysis and ablation results in Sec.~\ref{sec:extraexp}. 

We hereby summarize the important novel insights obtained from experiments in the appendix:

\begin{enumerate}

    \item LLMs tend to underestimate inductive reasoning difficulty during evaluation given a concise ground truth. A better choice is to do a multi-round evaluation where LLMs can better evaluate difficulty by self-reflection on its attempt for solving the problem. (Sec.~\ref{sec:difficulty})

    \item While generally adding more shots increases LLM's inductive performance, the performance change varies with problem types. LLMs improve the most on string manipulation tasks where each character in the input serves as a ``shot'' inside each example, and will not improve if the functions are too straightforward or too difficult.  (Sec.~\ref{sec:probtype})
    
    \item The evaluation on our benchmark is robust across different random seeds; i.e., the standard deviation of the performance is low. (Sec.~\ref{sec:ablation_std})

    \item The performance of LLMs against erroneous shot largely depends on the ratio of errorneous shots; under the same ratio, the total number of shots does not change much. (Sec.~\ref{sec:more_error_abl})
    
\end{enumerate}


\section{Extended Comparison with Prior Works}
\label{sec:ext_related_work}

Tab.~\ref{table:related_work1} shows a detailed comparison of our work with existing works (including empirical study and benchmarks) in the field of many-shot and inductive reasoning. As shown in the table, our work is indeed unique among all the many-shot ICL and inductive reasoning works.

% \section{Test}
\begin{table}
    \setlength{\tabcolsep}{4pt}
    \caption{Comparison on our benchmark with the most related prior many-shot / long-context benchmarks or empirical studies (first part) and inductive reasoning benchmarks or empirical studies (second part). To save space, we abbreviate ``Many Shot'' as MS, ``Inductive Reasoning'' as IR ($\semicorrect $ represents ``classification only''), ``Prob.'' as problems, and ``I/O Div.'' as ``Input/Output Diversity'' (\textbf{having at least $2$ different input-output types, e.g., given an array and output an integer, or given a pair of strings and output a choice}). ``Gen.'' means ``Generative'', which means whether new test cases can be easily generated without much human effort. ``LB'' means whether a leaderboard is available, and ``EE'' means ``Easy Evaluation'', i.e., whether a pipeline for evaluating any given new model exists. ``New Data'' means whether the input-output data never appears in existing benchmarks ($\semicorrect$ means partly); if so, the benchmark is secured against data contamination. Note, the counting of \#IR Problems and ``Gen.'' take different target input-output for the same mapping into account, but \textbf{do not take different sets of shots into account}.}
    \centering
    \begin{tabular}{lccccccccc}
    \toprule
    \textbf{Evaluations} & 
    \textbf{MS} &
    \textbf{IR} &
    \textbf{\# IR Prob.} &
    \textbf{I/O Div.} &
    \textbf{Max \# Shots} & 
    \textbf{Gen.} & 
    \textbf{LB} & \textbf{EE} & 
    \textbf{New Data} 
    \\
    \midrule
   
    Classifications~\citep{li2023context} & \greencheck & \semicorrect & $\sim$25K & \redcross & 2000 & \redcross & \redcross & \redcross &  \redcross \\
    Many-Shot ICL~\citep{agarwal2024many} & \greencheck & \greencheck & 450 & \greencheck & 2048 & \greencheck  & \redcross  & \redcross & \semicorrect \\
    Classifications~\citep{bertsch2024context} & \greencheck & \semicorrect & 1250 & \redcross & 2000 & \redcross & \redcross & \semicorrect & \redcross \\
    Visual Classifications~\citep{jiang2024many} & \greencheck & \semicorrect & 4010 & \redcross & $\sim$2000  & \redcross & \redcross & \greencheck & \redcross \\
    Instruction Following~\citep{zhao2024context} & \greencheck & \redcross & 0 & \greencheck & 300 & \greencheck & \redcross & \greencheck & \redcross \\
    2D Classifications~\citep{zhao2024probing} & \greencheck & \semicorrect & 100 & \redcross & 256 & \greencheck & \redcross &  \semicorrect &  \greencheck\\
    LLM Judge~\citep{song2024can} & \greencheck & \redcross & 0 & \redcross & 512 & \greencheck & \redcross & \redcross &  \greencheck\\
    HELMET~\citep{yen2024helmet} & \greencheck & \semicorrect & 500 &\greencheck & $\sim$10K & \redcross & \redcross  & \greencheck & \redcross\\
    LongICLBench~\citep{li2024long} & \greencheck & \semicorrect & 3000 &\redcross & $\sim$2000 & \redcross & \greencheck & \greencheck & \redcross \\
    ManyICLBench~\citep{zou2024retrieval} & \greencheck & \semicorrect & 1000 & \greencheck &  7252 & \redcross & \redcross  & \redcross & \redcross \\
    LMAct~\citep{ruoss2024lmact}& \greencheck & \redcross & 0 & \redcross & 256 & \greencheck & \redcross & \greencheck & \greencheck \\
     LongBench~\citep{bai2023longbench} & \greencheck & \semicorrect & 400 & \greencheck & 600 & \redcross & \greencheck & \greencheck & \greencheck  \\
    \midrule
    BABILong~\citep{kuratov2024babilong} & \redcross & \greencheck & unknown & \greencheck & unknown & \greencheck & \greencheck & \greencheck & \greencheck \\
    KORBench~\citep{ma2024kor} & \redcross & \greencheck & 50 & \greencheck & 3  &  \redcross & \greencheck & \greencheck &  \greencheck  \\
    SolverLearner~\citep{cheng2024inductive}& \redcross  & \greencheck & 1300 & \greencheck & 16 & \greencheck & \redcross & \redcross & \greencheck\\
    Case2Code~\citep{shao2024case2code} & \redcross  & \greencheck &  1.3M & \greencheck & 10 & \redcross & \redcross & \redcross & \greencheck \\
    DEER~\citep{yang2022language} & \redcross  & \greencheck & 1250 & \redcross & 3 & \redcross & \redcross & \redcross & \greencheck \\
    List functions~\citep{rule2020child} & \redcross  & \greencheck & 4000 & \redcross & 5 & \greencheck & \redcross & \greencheck & \greencheck \\
    SyGus~\citep{wang2023hypothesis} & \redcross & \greencheck & 89 & \greencheck & 3 & \redcross & \redcross & \greencheck & \greencheck\\
    ARC~\citep{chollet2019measure} & \redcross  & \greencheck & 800 & \redcross & 3 & \redcross & \greencheck & \greencheck & \greencheck \\ 
    % H-ARC & \redcross  & \greencheck & 1000 && 3 & \redcross & \redcross & \greencheck & \greencheck\\
    1D-ARC~\citep{xu2023llms} & \redcross  & \greencheck & 900 & \redcross & 3 & \redcross & \redcross & \greencheck & \greencheck \\
    Mini-ARC~\citep{kim2022playgrounds} & \redcross & \greencheck & 150 & \redcross & 3 & \redcross & \redcross & \greencheck & \greencheck\\
    WILT~\citep{banatt2024wilt}  & \redcross & \greencheck & 50 & \redcross & 30 & \redcross & \greencheck & \greencheck & \greencheck \\
    LogicVista~\citep{xiao2024logicvista}  & \redcross & \greencheck& 107 & \greencheck & 10 & \redcross & \redcross & \greencheck & \greencheck \\
    CLUTRR~\citep{sinha2019clutrr} & \redcross & \greencheck& 70K & \redcross & N/A & \greencheck & \redcross & \greencheck & \greencheck \\
    MIRAGE~\citep{li2024mirage}  & \redcross & \greencheck & 2000 & \greencheck & 8 & \greencheck & \redcross & \redcross & \greencheck \\
    ACRE~\citep{zhang2021acre}  & \redcross & \greencheck & 30K & \redcross & 10 & \greencheck & \redcross &  \redcross & \greencheck \\
    Mini-SCAN~\citep{qiu2023phenomenal}  & \greencheck & \greencheck& 400 & \redcross & 100 & \greencheck & \redcross & \greencheck & \greencheck\\


    \midrule
    \textbf{Ours} & \greencheck & \greencheck & 6930 & \greencheck & 2048 & \greencheck & \greencheck& \greencheck& \greencheck\\
    \bottomrule
    \end{tabular}
  
   %  \caption{Summary of related work on many-shot in-context learning and inductive reasoning. We compare our work with both evaluation sets used in empirical study and benchmarks. To save space, we abbreviate ``Many Shot'' as MS, ``Inductive Reasoning'' as IR, ``Prob.'' as problems, and ``I/O Div.'' as ``Input/Output Diversity'' (\textbf{we define diversity as having at least $2$ different input-output types, e.g., given an array and output an integer, or given a pair of strings and output a choice}). ``Gen.'' is the abbreviation for ``Generative'', which means whether new test cases can be easily generated without much human effort. ``LB'' means whether a leaderboard is available, and ``EE'' means ``Easy Evaluation'', i.e., whether a pipeline for evaluating any given new model exists. ``New Data'' means whether the input-output data never appears in existing benchmarks; if so, the benchmark is secured against data contamination. The orange triangle represents ``classification only'' (for IR problems) or ``partial'' (for EE / New Data). Note, the counting of \#IR Problems and ``Gen.'' take different target input-output for the same mapping into account, but \textbf{do not take different numbers of shots into account}.}
    \label{table:related_work1}
\end{table}


% \begin{table*}
%     \centering
%     \begin{tabular}{lccc}
%     \toprule
%     \textbf{Works} & 
%     \textbf{Work Type} &
%     \textbf{Input-Output Types} &
%     \textbf{Problems Studied} 
%     \\
%     \midrule
    
%     \bottomrule
%     \end{tabular}

%     \caption{Summary of related work on many-shot in-context learning and inductive reasoning. }
%     \vspace{-0.1in}
%     \label{table:related_work2}
% \end{table*}


% \begin{table*}
%     \centering
%     \begin{tabular}{lccc}
%     \toprule
%     \textbf{Benchmarks} & 
%     \textbf{Problem Type} &
%     \\
%     \midrule
    
%     \bottomrule
%     \end{tabular}

%     \caption{Why is inductive reasoning an important problem for long-context LLMs?}
%     \vspace{-0.1in}
%     \label{table:related_work2}
% \end{table*}

\section{More Experiment Details} 
\label{sec:prompt}
\subsection{Prompts for Main Results}

We provide the prompt for the main results in Sec.~\ref{sec:mir-ext} and Sec.~\ref{sec:discr} in the box below (the first commented line is not a part of the prompt):

{
\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        
        \begin{minipage}{0.95\linewidth}
        
            \# prompt for main results
            \\
            
            You are given some function that takes something as input and output something. You need to predict the output for the target input of that function. Remember always end your answer with 'Output: {your answer}', with your answer in strict python format. Here are some examples:
            
            Input: $<$example input 1$>$
            
            Output: $<$example output 1$>$

            Input: $<$example input 2$>$

            Output: $<$example output 2$>$

            $\dots$ (omitting more shots)

            Input: $<$target input$>$
            
        \end{minipage}
    }
}
\label{sec:main_prompt}

\subsection{Prompts for Ablations}
\label{sec:abl_prompt}
\textbf{Effectiveness of CoT.} The following boxes demonstrate the prompt for the result used in Sec.~\ref{sec:cot} with forced CoT and no CoT respectively (the first commented line is not a part of the prompt):

{
\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
            \# prompt for forced CoT
            \\
            
            You are given some function that takes something as input and output something. You need to predict the output for the target input of that function. You need to first analyze it after 'Analysis:', then give your answer after 'Output:'. Remember always end your answer with 'Output: {your answer}', with your answer in strict python format. Here are some examples:
            
            Input: $<$example input 1$>$
            
            Output: $<$example output 1$>$

            Input: $<$example input 2$>$

            Output: $<$example output 2$>$

            $\dots$ (omitting more shots)

            Input: $<$target input$>$
            
        \end{minipage}
    }
}

{
\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
            \# prompt for no CoT
            \\
            
            You are given some function that takes something as input and output something. You need to predict the output for the target input of that function. Your answer should always be 'Output: {your answer}', with your answer in strict python format. DO NOT OUTPUT ANYTHING ELSE INCLUDING YOUR THOUGHTS. Here are some examples:"
            
            Input: $<$example input 1$>$
            
            Output: $<$example output 1$>$

            Input: $<$example input 2$>$

            Output: $<$example output 2$>$

            $\dots$ (omitting more shots)

            Input: $<$target input$>$
            
        \end{minipage}
    }
}

\textbf{Robustness of LLM inductive intelligence.} The following box demonstrates the prompt for the result used in Sec.~\ref{sec:robust}. For the ``unaware'' setting, we use the same prompt as that in the main results; for the ``aware error'' and ``aware ratio'' setting, we use the following prompts respectively:

{
\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
             \# prompt for ``aware error''
            \\
        
            You are given some function that takes something as input and output something. You need to predict the output for the target input of that function. Remember always end your answer with 'Output: {your answer}', with your answer in strict python format. Here are some examples. Note that not all shots are correct; there are a small portion of shots that are incorrect:
            
            Input: $<$example input 1$>$
            
            Output: $<$example output 1$>$

            Input: $<$example input 2$>$

            Output: $<$example output 2$>$

            $\dots$ (omitting more shots)

            Again, note that not all shots are correct; there are a small portion of shots that are incorrect. Use your caution and think wisely.

            Input: $<$target input$>$
            
        \end{minipage}
    }
}

{
\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
             \# prompt for ``aware ratio''
            \\
        
            You are given some function that takes something as input and output something. You need to predict the output for the target input of that function. Remember always end your answer with 'Output: {your answer}', with your answer in strict python format. Here are some examples. Note that not all shots are correct; there are $<$number of error shots$>$ out of $<$total number$>$ shots that are incorrect:
            
            Input: $<$example input 1$>$
            
            Output: $<$example output 1$>$

            Input: $<$example input 2$>$

            Output: $<$example output 2$>$

            $\dots$ (omitting more shots)

            Again, note that not all shots are correct; $<$number of error shots$>$ out of $<$total number$>$ shots that are incorrect. Use your caution and think wisely.

            Input: $<$target input$>$
            
        \end{minipage}
    }
}


% \textbf{Test of SolverLearner framework.} The following box demonstrates the prompt for the result used in Sec.~\ref{sec:coding}:

% {
% \setlength{\fboxsep}{0.3cm}
%     \ovalbox{\small
%          % Adjust the padding inside the ovalbox
%         \begin{minipage}{0.95\linewidth}
%                 \# prompt for coding
%         \\
        
%             You are given some function that takes something as input and output something. You need to write a python code of the function. You need to write your rationale after \# (as if it is a python comment), and give your answer after 'Code:'. DO NOT OUTPUT ANYTHING ELSE. Your function name should be 'solution'. You are not allowed to write other custom functions unless it is inside 'solution'. Use imports before using package functions. You must strictly follow python format, especially input / output format (e.g., if it is a dictionary, your param should also be a dictionary). DO NOT ADD ANY STATEMENT FOR EVALUATION AFTER 'solution'. Here are the input-output pairs for the function, with input followed by output:
            
%             Input: $<$example input 1$>$
            
%             Output: $<$example output 1$>$

%             Input: $<$example input 2$>$

%             Output: $<$example output 2$>$

%             $\dots$ (omitting more shots)

%             Input: $<$target input$>$
            
%         \end{minipage}
%     }
% }

\subsection{Prompts for Reformatting APPS problems (Sec.~\ref{sec:construction})}
\label{sec:rewrite}
The following box demonstrates the prompt for reformatting APPS problems in the ``function collection'' part of Sec.~\ref{sec:construction}.

{
\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
                \# prompt for reformatting
        \\
        You are a coding expert. You will be given a problem and corresponding solution. Rewrite the solution such that:
        
    1. It becomes a single function named 'solution', which takes parameters as input instead of reading from input() function if there is any;
    
    2. There is no code out of the solution function and no solution class. All auxiliary functions should be defined inside the solution function, and all imports should also be in the function.
    
    3. The solution function should not have any print() function. Instead, it should return the result of the function. If you need to output any rationale, leave them in comments. Your output must be directly runnable without any change.
    
    4. Just output the rewritten function; do not test it with extra statements. 
    
    Here is an example:

    [[Problem]]
    
problem: Given a string, you need to reverse the order of characters in each word within a sentence while still preserving whitespace and initial word order.

Example 1:

Input: "Let's take LeetCode contest"

Output: "s'teL ekat edoCteeL tsetnoc"



Note:

In the string, each word is separated by single space and there will not be any extra space in the string.

[[Solution]]

class Solution:

\qquad     def reverseWords(self, s):
     
\qquad \qquad          """
         
\qquad \qquad          :type s: str
         
\qquad \qquad          :rtype: str
         
\qquad \qquad          """
         
\qquad \qquad          rev\_str = s[::-1]
         
\qquad \qquad          rev\_arr = rev\_str.split()
         
\qquad \qquad          final = rev\_arr[::-1]

\qquad \qquad          return ' '.join(map(str, final))

[[Rewrite]]

def solution(s):

\qquad     """

\qquad     :type s: str

\qquad     :rtype: str

\qquad     """

\qquad     rev\_str = s[::-1]

\qquad     rev\_arr = rev\_str.split()

\qquad     final = rev\_arr[::-1]

\qquad     return ' '.join(map(str, final))
    
        \end{minipage}
    }
}

\subsection{Prompt for The Generation of Data Generator}
\label{sec:prompt_datagen}
The following box demonstrates the prompt for generating data generator:

\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
        \# prompt for generating data generator
        
        You are a coding expert. You will be provided a coding question and corresponding solution. Please write two python function that randomly generates test case for the question. Specifically:
    
    The first function's name is gen1, which generates random data (should be able to generate VERY DIVERSE, i.e., at least 1000 different data points).
    
    The second function's name is gen2, which generates data that is slightly harder than those generated in gen1. (should be able to generate at least 100 different data points).

    You shall not define any function outside gen1 or gen2. Should you use any helper function, make them inner functions inside gen1 or gen2. You gen1 and gen2 function should have and only have one int parameter, which is the number of cases.
    
    Finally, the special cases should be designed as informative as possible that reveals the underlying function when looking at the input and corresponding output from the solution.
    
    Here is an example. Note the output of gen1 and gen2 should be a list of dicts describing the parameters, and your special case input should be a dict describing the parameters. Please follow the format, and do not generate cases that are too long. Do not output any other text; put all your thoughts after "\# rationale:" as shown in the example.

    [[Problem]]

from typing import List


\qquad def has\_close\_elements(numbers: List[float], threshold: float) -$>$ bool:

\qquad    """ Check if in given list of numbers, are any two numbers closer to each other than given threshold.

\qquad $>>>$ has\_close\_elements([1.0, 2.0, 3.0], 0.5)

\qquad False

\qquad $>>>$ has\_close\_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)

\qquad True

\qquad """

from typing import List

[[Solution]]

sorted\_numbers = sorted(numbers)

for i in range(len(sorted\_numbers) - 1):

if sorted\_numbers[i + 1] - sorted\_numbers[i] $<$ threshold:

\qquad return True

return False

[[Gen1]]

\# rationale: none

import random

def gen1(num\_cases: int):

\qquad    low, high = 5, 10 \# generate lists between length 5 to 10

\qquad    data = []

\qquad    for i in range(num\_cases):

\qquad\qquad        N = random.randint(low, high)

\qquad\qquad        lst = [round(random.random() * 10, 1) for \_ in range(N)]

\qquad\qquad        threshold = round(random.random(), 1) + 0.1

\qquad\qquad        data.append({'numbers': lst, 'threshold': threshold})

\qquad     return data

[[Gen2]]

import random

def gen2(num\_cases: int): \# rationale: the data is slightly harder as the list is slightly longer

\qquad     low, high = 10, 20 \# generate lists between length 10 to 20

\qquad     data = []

\qquad     for i in range(num\_cases):

\qquad \qquad         N = random.randint(low, high)

\qquad         lst = [round(random.random() * 10, 1) for \_ in range(N)]

\qquad         threshold = round(random.random(), 1) + 0.1

\qquad         data.append({'numbers': lst, 'threshold': threshold})

\qquad     return data

        \end{minipage}
    }


\subsection{LLM Difficulty Labeling}
\label{sec:difficulty}

In Sec.~\ref{sec:discr}, we discussed several factors for problems which leads to LLM's few-shot and many-shot performance difference, and one of them is LLM-evaluated function difficulty level. While seemingly straightforward, a reliable evaluation of difficulty for LLM is highly non-trivial, as \textbf{LLM tend to underestimate inductive reasoning difficulty when given a simple ground truth function.} We provide an example below in the box with GPT-4o-latest as the prompted model:

\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}

        \# Prompt
        
       You are an expert in coding and inductive reasoning. You will now be given a function that solves some problems in python and some example input-output pairs. 



Please rate the difficulty of guessing the function through input-output pairs. You should judge the difficulty NOT on whether the function itself is easy to write, but on whether it is easy to guess. You should first give your rationale after [[Analysis]], then end your answer with 'Overall Score: [[x]]', where x is your score from 1 to 10.
     
     [[code]]

def solution(l):

\qquad  return [l[i] for i in range(1, len(l)) if l[i] \% i == 0]

[[data]]

Input: {'l': [40, -13]}

Output: [-13]

Input: {'l': [-87, 29, -27, 90]}

Output: [29, 90]

Input: {'l': [48, -4, 16, -63, -32, -5, -32, -45]}

Output: [-4, 16, -63, -32, -5]

Input: {'l': [-75, 77, 44, 10, 17, -71, -80, -11, -68, -41]}

Output: [77, 44]

Input: {'l': [87, 71]}

Output: [71]

\# LLM's response


[[Analysis]]  
To solve this puzzle, the key is to identify the rule used to filter the input list `l`. By observing the given input-output pairs and the function definition, the pattern can be deduced as follows:

\textbf{Function Logic.}  
The function constructs a new list from the original list `l`, starting from index `1`. For each element at index `i`, it checks if the element is divisible by its **index** (using `l[i] \% i == 0`). If true, it includes the element in the resulting list.

\textbf{Difficulty Analysis Based on Input-Output Pairs.}  
**Clues that might help guess the function:**
1. **The output never includes the first element of the input list.**  
   This suggests that index-based filtering is involved.


%      \end{minipage}
%     }

% \setlength{\fboxsep}{0.3cm}
%     \ovalbox{\small
%          % Adjust the padding inside the ovalbox
%         \begin{minipage}{0.95\linewidth}

   
2. **Only specific elements from the input list appear in the output.**  
   A guesser would likely try to find what distinguishes the selected elements from the others. Given the presence of both positive and negative numbers, divisibility by index is a plausible criterion.

3. **The divisibility condition isn't obvious at a glance.**  
   It isn't immediately clear that elements are being checked for divisibility by their **index**, as it's somewhat unconventional. Most people would initially look for a rule involving the values themselves, like even/odd checks, thresholds, or signs.
   
\textbf{What Makes It Difficult to Guess?}

1. The divisibility rule by index is **non-intuitive**.  
   Divisibility by **element values** is a common guess, but divisibility by **index** is not.  
   
2. The function starts from index `1` instead of `0`.  
   This subtle detail could mislead a guesser into thinking it's a filtering rule based on the **position** in the list rather than the index value.

3. The rule applies to **all elements**, regardless of their sign or magnitude.  
   This broad applicability makes it harder to pin down the exact condition.

\textbf{What Makes It Easier to Guess?}

1. **Simple filtering condition:**  
   Despite the challenge, once a guesser considers index-based filtering, the rule becomes easy to validate.

2. **The input-output pairs are consistent.**  
   There are no exceptions or edge cases that would throw off a guesser.

\textbf{Overall Score: [[7]] (moderately difficult)}

% The function has a moderately difficult rule to guess, primarily because the idea of divisibility by **index** is not immediately obvious. It requires some inductive reasoning to arrive at the exact condition, but the input-output pairs are consistent enough to make it possible.

        \end{minipage}
    }

GPT-4o-latest gives an average difficulty score of 7.2 / 10 out of $5$ evaluations (which indicates moderate-to-high difficulty), while GPT-4o-0806 gives much lower difficulty score (below 5). This problem, however, is in fact so difficult that we have to exclude it from our benchmark, i.e., have $0$ accuracy for all five models $\{$GPT-4o-0806, GPT-4o-mini-0718, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Mistral-Large-2$\}$ across $\{4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\}$ shots in $10$ test cases.

To address such issue, we adopt a multi-round conversation approach with self-reflection. We first let the LLM to try to solve the problem without code by itself, and then reveal the ground-truth answer and let LLM to score the difficulty based on self-reflection. The box below demonstrates an example:

\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
\# Round 1 prompt
        
      You are an expert in inductive reasoning. You will now be given a function that solves some problems and some example input-output pairs. You need to guess the answer for the one last input. After that, you will be given the answer and the code, you need to do a self-reflection on your attempt, and rate a difficulty level for future guessers. Here are some examples: 
      
      $<$few-shot examples of the whole conversation$>$

      Here is the target problem:  

[[Problem]]

Input: \{'lst': ['V4', 'V10', 'V4', 'V0', 'V9', 'V4', 'V2', 'V14', 'V16', 'V9']\}

Output: ['V0', 'V2', 'V4', 'V4', 'V4', 'V9', 'V9', 'V10', 'V14', 'V16']

$\dots$

Input: \{'lst': ['V0', 'V13', 'V1', 'V4', 'V2', 'V10', 'V6', 'V5', 'VB', 'V1']\}

Output: ['VB', 'V0', 'V1', 'V1', 'V2', 'V4', 'V5', 'V6', 'V10', 'V13']

Input: \{'lst': ['V0+', 'V3', 'V14', 'V13']\}

Output: ['V0+', 'V3', 'V13', 'V14']

Input: \{'lst': ['V12', 'V6', 'V7', 'V2', 'VB', 'V2', 'V3', 'V0', 'V4', 'V6']\}

[[Solution]]

\# Round 1 response

The solution appears to sort the elements in each list lexicographically, maintaining any duplicates in their respective positions within the sorted order. 

Following this rule, for the input `{'lst': ['V12', 'V6', 'V7', 'V2', 'VB', 'V2', 'V3', 'V0', 'V4', 'V6']}`, the output should be:

**Output:** `['V0', 'V2', 'V2', 'V3', 'V4', 'V6', 'V6', 'V7', 'V12', 'VB']`

     \end{minipage}
    }


\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
\# Round 2 prompt
        
        [[Response]]
Now, I will tell you the answer is ['VB', 'V0', 'V2', 'V2', 'V3', 'V4', 'V6', 'V6', 'V7', 'V12']. Here is the code for the function: 

def solution(lst):

    def grade(v):
    
    \qquad  \qquad   if v == 'VB': return -2
        
    \qquad \qquad    if v == 'V0': return -1
        
    \qquad  \qquad   if v == 'V0+': return 0
        
    \qquad \qquad    return int(v[1:])

    \qquad return sorted(lst, key=grade)

Based on the answer, your guess and the code, do a self-reflection on your attempt, and rate a difficulty level between 1 to 10 inclusive for future guessers with different shots sampled for the same function. You MUST always end your answer with "Difficulty: [[x]]" where x is 1 to 10.

\# Round 2 response

In my initial attempt, I focused on lexicographical sorting but overlooked the specific priority handling in the function for values like "VB" and "V0." The code clarifies that "VB" should always appear first, followed by "V0," and then "V0+," with all other values ordered by their numeric suffix. This customized grading system is subtle, especially without the code, as it combines both conditional priorities and numeric sorting.

Overall, understanding the need for this specific grading logic adds some complexity, but itâ€™s manageable once recognized.

Difficulty: [[4]]

 \end{minipage}
    }

With such paradigm, we found that the evaluation from LLMs are much more accurate.

% \textbf{LLM has different inductive reasoning ability than human.} The following example in the box shows an example where state-of-the-art LLMs (GPT-4o-0806 in this case), is unable to discover paradigms that are apparent to human even if it has the related knowledge:

% {
% \setlength{\fboxsep}{0.3cm}
%     \ovalbox{\small
%          % Adjust the padding inside the ovalbox
%         \begin{minipage}{0.95\linewidth}
%                 \# prompt for coding
%         \\
            
%         \end{minipage}
%     }
% }

%It is worth noting that LLM difficulty labeling is non-trivial {\color{red}[TBD]}

\subsection{Answer Extraction}
\label{sec:extract}
We use the following way to extract final answer:
\begin{enumerate}
    \item find the last occurence of ``utput:'' in LLM's response, which can be either ``output'' or ``Output'';
    \item remove trailing ``**'' after ``output:'' for bold, ``` and ```python in the answer for python format, and then remove all leading and trailing whitespaces and linebreaks;
    \item conduct an exact match. If succeeds, the LLM is considered to be correct; otherwise, we try to convert the output into a python object by using ast.literal\_eval(). If the convertion fails, the LLM is considered to be incorrect.
    \item If the output can be converted to a single-key dictionary or single-element set, we will do an exact match between the value of the dictionary / element of the set to the ground truth answer with both converted to string (This is to account for responses similar to $\{$``ans'': 3$\}$ with ground truth being 3); otherwise, we do an exact match between the whole output and the ground truth answer converted to string.  
\end{enumerate}

\subsection{Robustness Test: Erroneous Shots}
\label{sec:error_shots}
We generate test cases with erroneous shot in the following way:

\begin{enumerate}
    \item For $n$-shot with a given error rate $ER$, randomly sample $ER\times n$ indices to be the ``erroneous shots'' with incorrect answer. $ER\times n$ is guaranteed to be an integer.
    
    \item for each ``erroneous shot'', we randomly sample one unused shot as we generate $20000$ shots for each function, and substitute the original output with the selected shot's output. We will re-sample the unused shot if its answer is identical with the original shot.
    
\end{enumerate}

\subsection{SolverLearner}
\label{sec:solverlearner_app}
We use the following prompt for SolverLearner~\cite{cheng2024inductive}:

\ovalbox{\small
        \begin{minipage}{0.95\linewidth}
           \# Prompt for SolverLearner

           You are given some function that takes something as input and output something. You need to write a python code of the function. You need to write your rationale after \# (as if it is a python comment), and give your answer after 'Code:'. DO NOT OUTPUT ANYTHING ELSE. Your function name should be 'solution'. You are not allowed to write other custom functions unless it is inside 'solution'. Use imports before using package functions. You must strictly follow python format, especially input / output format (e.g., if it is a dictionary, your param should also be a dictionary). DO NOT ADD ANY STATEMENT FOR EVALUATION AFTER 'solution'. Here are the input-output pairs for the function, with input followed by output:"
            
           Input: $<$input 1$>$
           
           Output: $<$output 1$>$
           
           $\dots$
           
           Input: $<$input n$>$
           
           Output: $<$output n$>$
           
           Here is your code. Again, do not output anything else; Your function name should be 'solution'. You are not allowed to write other custom functions unless it is inside 'solution'. Use imports before using package functions. You must strictly follow python format, especially input / output format (e.g., if it is a dictionary, your param should also be a dictionary). DO NOT ADD ANY STATEMENT FOR EVALUATION AFTER 'solution'.
           
           Code:
           
        \end{minipage}
    }

\section{Statistical Features of MIR-Bench}
\label{sec:feature}
\subsection{Data Source}

Tab.~\ref{tab:data_source} shows that out of $693$ functions in MIR-Extended and $300$ functions in MIR-Core, how many problems are extracted from each coding benchmark (HumanEval+, MBPP+ and APPS).

\begin{table}[]
    \centering
    \caption{Number of functions extracted from each coding benchmark.}
    \begin{tabular}{ccccc}
        \toprule
          & HumanEval+ & MBPP+ & APPS & Total  \\
         \midrule
         MIR-Core & 26 & 35 & 239 & 300\\
         MIR-Extended & 53 & 89 & 551 & 693 \\
         \bottomrule
    \end{tabular}
    
    \label{tab:data_source}
\end{table}

\subsection{Problem Types in Sec.~\ref{sec:discr}}
\label{sec:probtype}

To study the effect on the topic of the problems for whether the problem benefits from many-shot, we first try to cluster the $693$ problems in MIR-Extended using GPT-4o-0806. More specifically, we first prompt the LLM to generate python-style tags for each problem with the following prompt:

{
\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
            \# prompt for tags
            \\
            You are an expert in coding. You will now be given a function that solves some problems and some example input-output pairs.
            You need to briefly summarize what the function is about in a tag in high-level, with no more than 5 words connected with '\_'. DO NOT OUTPUT ANYTHING ELSE. Here are some examples:
            
            $<$some examples$>$
            
            [[Code]]
            
            $\dots$

            [[Input-Output Pairs]]
            
            $\dots$

            [[Answer]]
            
        \end{minipage}
    }
}

after acquiring tags for each problem, we prompt the LLM to merge all different tags down to $30$ different tags with $6$ major types: $\{$List Analysis, List Manipulation, Mathematical Computations, String Analaysis, String Manipulations, Other$\}$. Tab.~\ref{tab:problem_type} shows the number of problems, detailed tags and metric $D$ (defined in Eq.~\eqref{eq:D}) for each problem type. 

Based on the results, we find that generally adding more problem will have a positive effect on performance; however, for some types of problem such as geometric calculation and summation, the performance will decrease with more shots included. Upon checking those problems, we found them mostly fall into two categories: 1) the function is relatively straightforward, but the LLM gets confused with more shots due to over-complicated guesses; 2) the function is too hard to guess, and the LLM cannot make reasonable guesses when aggregating many pieces of information. The boxes below give examples for case 1) and 2) respectively:


{
\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
            \# Case 1: Straightforward Problems

            [[Code]]
            
            def solution(num: int) $\rightarrow$ int:
    
    \qquad steps = 0
    
    \qquad while num $>$ 0:
    
     \qquad\qquad   if num \% 2 == 0:
            num /= 2
     
     \qquad\qquad   else:
            num -= 1
     
    \qquad\qquad steps += 1
    
    \qquad qquad return steps


[[Input-Output Pairs]]

Input: \{'num': 68037\}

Output: 23

$\dots$
        \end{minipage}
    }
}

{
\setlength{\fboxsep}{0.3cm}
    \ovalbox{\small
         % Adjust the padding inside the ovalbox
        \begin{minipage}{0.95\linewidth}
            \# Case 2: Difficult Problems

[[Code]]

def solution(boardSize, initPosition, initDirection, k):

    \qquad yq, yr = divmod(initPosition[0] + k * initDirection[0], 2 * boardSize[0])
    
    \qquad xq, xr = divmod(initPosition[1] + k * initDirection[1], 2 * boardSize[1])
    
    \qquad \# Calculate the final position considering reflections
    
    \qquad return [min(yr, 2 * boardSize[0] - yr - 1), min(xr, 2 * boardSize[1] - xr - 1)]


[[Input-Output Pairs]]

Input: \{'boardSize': [10, 11], 'initPosition': [5, 9], 'initDirection': [1, -1], 'k': 264\}

Output: [9, 9]
        
        \end{minipage}
    }
}

On the other hand, problems such as removing duplicates and string manipulation generally benefit more from many-shot, probably because manipulation on each element / character can be considered a shot by itself, and thus the effective number of ``shots'' in such types of problems are higher. Note, ``Others'' problems have relatively high value of $D$ because of one outlier; other than the outlier, it is almost equal to average level of $D$ across MIR-Extended.


\begin{table*}[ht]
    \centering
    %\setlength{\tabcolsep}{3pt}
    \small
    \caption{The tags for problem topics and related statistics; $D$ is the average metric (see Eq.~\eqref{eq:D} for definition) of the corresponding type of problems in MIR-Extended. We marked entries with $D>0.1$ {\color{blue}blue} and $D<-0.1$ {\color{red}red.} While increasing the number of shots generally brings better performance, We find that string manipulation benefits the most from many-shot.}
    \begin{tabular}{ccccc}
       \toprule
        Major Tag & Minor Tag & \# (MIR-Extended) & \# (MIR-Core) & $D$ \\ 
        \midrule
        List Analysis & Counting Elements & 4&2& 0.075 \\
        List Analysis & Counting Occurences &25&9& 0.017 \\
        List Analysis & Maximum/Minimum Elements &30&5& -0.024 \\
        List Analysis & Statistics &2& 0& 0.041 \\
        List Analysis &&61&16& 0.001 \\
        \midrule
        List Manipulation & Filtering Elements &30&14& 0.066 \\
        List Manipulation & Generating Sequences &15&6& 0.077\\
        List Manipulation & Mapping Elements &9&3& -0.022 \\
        List Manipulation & Removing Duplicates &6&4& {\color{blue}0.136} \\
        List Manipulation & Sorting Elements &16&7& -0.041 \\
        List Manipulation &&76&34&0.041  \\
        \midrule
        Mathematical Computations & Basic Arithmetic & 35 &12& 0.085 \\
        Mathematical Computations & Boolean Determination & 7&0& 0.033 \\
        Mathematical Computations & Calculations Based on Formulas & 98 &32& 0.051 \\
        Mathematical Computations & Condition Checking & 43&20& {\color{blue}0.114} \\
        Mathematical Computations & Geometric Calculation &4&2& {\color{red}-0.110}\\
        Mathematical Computations & Number Base Conversions &12&5& 0.038\\
        Mathematical Computations & Rounding &7&5& {\color{blue}0.185}\\
        Mathematical Computations & Summation &5&0& {\color{red}-0.128}\\
        Mathematical Computations && 211&76& 0.066 \\
        \midrule 
        String Analysis & Character Code Calculations &14&3& -0.004 \\
        String Analysis & Comparison &13&6& {\color{blue}0.255} \\
        String Analysis & 
        Counting Characters &29&10& 0.050 \\
        String Analysis & Pattern Matching &27&7& 0.084 \\
        String Analysis && 83& 26& 0.084\\
        \midrule
        String Manipulation & Case Transformation &19&7& 0.089\\
        String Manipulation & Encryption/Decryption &8&4& 0.061\\
        String Manipulation & Generating Substrings &8&4& -0.027\\
        String Manipulation & Rearranging Characters &48&22& {\color{blue}0.105}\\
        String Manipulation &  Substitution&36&30& {\color{blue}0.327} \\
        String Manipulation & Substring Replacement &33&22&{\color{blue}0.205} \\
        String Manipulation & Swapping Parts &6& 5& -0.053 \\
        String Manipulation & Transformation &34& 17 & 0.083\\
        String Manipulation && 192 & 111 & {\color{blue}0.160}\\
        \midrule
        Others && 76 & 37  & {\color{blue}0.123}\\
        Total && 693 & 300 & 0.092\\
        \bottomrule
    \end{tabular}
    
    \label{tab:problem_type}
\end{table*}

\subsection{Other Factors Studied in Sec.~\ref{sec:discr}}
\label{sec:example_factor}
% {\color{red}[TBD; will fill in with redrawn figures like Fig.~\ref{fig:example-factor}]}

Fig.~\ref{fig:fig_single_factor} illustrates the relation between our metric $D$ (see Eq.~\eqref{eq:D} for definition) for distinctiveness between few-shot and many-shot performance.

\begin{figure}[ht]
    \centering
    % First row of images
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pic/formal/64-shot-acc-single.pdf}
        \caption*{a) 64-Shot accuracy}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pic/formal/codelen-single.pdf}
        \caption*{b) Code length}
    \end{minipage}
    
    %\vspace{1em} % Space between rows
    
    % Second row of images
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pic/formal/difficulty-single.pdf}
        \caption*{c) Difficulty level}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pic/formal/most_common_answer_ratio-single.pdf}
        \caption*{d) Most common answer ratio}
    \end{minipage}
    
    %\vspace{1em} % Space between rows
    
    % Third row of images
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pic/formal/num_different_answer-single.pdf}
        %\vspace{0.5em}
        \caption*{e) \# Different answer}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pic/formal/shotlen-single.pdf}
        %\vspace{0.5em}
        \caption*{f) Shot length}
    \end{minipage}
    
    \caption{Single-factor analysis between each factor in Sec.~\ref{sec:discr} and our distinctiveness metric $D$. Each point represents one of the $693$ functions in our benchmark. The black line is the linear regression result of all functions in MIR-Extended; it is clearly shown that $D$ is positively related to difficulty level and code length. The 64-shot accuracy is an average of $\{$GPT-4o-0806, GPT-4o-mini-0718, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Mistral-Large-2$\}$ over $10$ test cases.}
    \label{fig:fig_single_factor}
\end{figure}


\section{More Experiment Results}
\label{sec:extraexp}
\subsection{Analysis on the Stability of Evaluation}
\label{sec:ablation_std}
% standard deviation

As we use $0$ temperature in the evaluations in our main papers, it is possible that the performance vary across LLM inferences with different random seeds, hence making our evaluation unreliable. To address such concern, we report the mean and standard deviation of the performance across $5$ models $\{$GPT-4o-0806, GPT-4o-mini-0718, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Mistral-Large-2$\}$ over $5$ different inferences with temperature $0.7$ in Fig.~\ref{fig:ablation_std} on MIR-Extended with ${4, 8, 16, 32, 64, 128, 256, 512}$ shots. The result clearly shows that the standard deviation for all models are very small, and thus our evaluation is reliable.

\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{pic/formal/duplicate-cropped.pdf}
    \caption{The performance of $5$ cutting-edge LLM models on MIR-Extended with temperature $0.7$ across $5$ runs. The result clearly shows that the standard deviation of accuracy is always below $0.01$, and thus the evaluation is highly stable.}
    \label{fig:ablation_std}
\end{figure}

\subsection{Complete Results on MIR-Extended and MIR-Core (Sec.~\ref{sec:main},~\ref{sec:discr})}
\label{sec:all_main}
For better readability, we only put the performance of part of the models for MIR-Extended and MIR-Core in the main paper; Fig.~\ref{fig:all_main} demonstrates the performance of all models. 

\begin{figure}[ht]
    \centering
    \begin{minipage}[c]{0.4\linewidth}
        \centering
        \includegraphics[height=4cm]{pic/formal/selected_all.pdf}
        \caption*{a) MIR-Extended}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.55\linewidth}
        \centering
        \includegraphics[height=4cm]{pic/formal/selected_quad.pdf}
        \vspace{0.5em}
        \caption*{b) MIR-CoRE}
    \end{minipage}
    
    \caption{The performance of all LLMs on MIR-Extended (panel (a)) and MIR-Core (panel (b)). As shown in Fig.~\ref{fig:main} in the main paper, the benchmark poses challenge to all models tested including o1-preview and o1-mini. Most models will ``saturate'' at a particular number of shots, i.e., their performances stop to improve when more shots are given due to limited information integration capability.}
    \label{fig:all_main}
\end{figure}

\subsection{Out-of-Context Rate for 1024 and 2048 shot in MIR-Extended}

Tab.~\ref{tab:error} shows the rate of out-of-context error we received when invoking APIs for MIR-Extended. Some models other than Gemini (which has $>1M$ context length) have an error rate of $0$, which could due to its internal truncation.

\begin{table}[ht]
    \caption{Out-of-context rate for model API calls on MIR-Extended.}
    \centering
    \begin{tabular}{ccc}
         \toprule
         Model & 1024-shot (\%)& 2048-shot (\%) \\
         \midrule
         Claude-3-Haiku & 0 & 1.67\\
         Claude-3-Sonnet & 0 & 1.67\\
         Claude-3.5-Haiku & 0.96 & 2.4\\
         Claude-3.5-Sonnet & 0 & 1.77\\
         Gemini 1.5-Flash-002 & 0 & 0\\
         Gemini 1.5-Pro-002 & 0 & 0\\
         Gemini 2.0-Flash & 0 & 0\\
         GLM-4-Plus & 0 & 0\\
         GPT-4o-0806 & 0.33 & 5.67\\
         GPT-4o-mini-0718 & 0.33 & 5.67\\
         Mistral-Large-2 & 0.67 & 10.67\\
         Moonshot-128K & 0 & 0\\
         o1-mini-0912 & 1 & 11\\
         o1-preview-0912 & 1 & 11\\
         Qwen2-72B-Instruct & 0 & 0\\
         \bottomrule
    \end{tabular}
    
    \label{tab:error}
\end{table}

\subsection{More Results on Many-Shot Inductive Reasoning with CoT}
\label{sec:cot_abl}

Tab.~\ref{tab:cot_abl} lists the ratio of CoT / no CoT and their respective performance for more models on MIR-Core using original prompt. Fig.~\ref{fig:fig_moremodels_cot} illustrates the performance difference for more models between forced CoT and no CoT. The result shows that results with no CoT are indeed better than those with forced CoTs, and such gap increases with the number of shots.

\begin{table}[ht]
    \centering
    \scriptsize
    %\setlength{\tabcolsep}{3pt}
    \caption{The results on MIR-Core of each model with and without CoT. Results are averaged over $\{4,8,16,32,64,128,256,512,1024,2048\}$-shot. Error rate include cases where answer cannot be extracted and API error for exceeding context length. Claude-3.5-Haiku often refuses to answer the question due to ``incomplete data''.}
    \begin{tabular}{cccccc}
    \toprule
        Model & Answer w./ CoT (\%) & Accuracy w./ CoT & Answer w./o. CoT (\%) & Accuracy w./o. CoT & Error (\%)  
        \\
        \midrule
        Claude-3-Haiku  & 51.03 & 0.278 & 47.66 & 0.441 & 1.31 \\
        Claude-3-Sonnet & 20.46 & 0.233 & 76.98 & 0.475 & 2.56 \\
        Claude-3.5-Haiku & 65.11 & 0.317 & 1.32 & 0.823 & 33.57 \\
        Claude-3.5-Sonnet & 98.73 & 0.585 & 1.08 & 0.775 & 0.19 \\
        Gemini 1.5-Flash-002 & 20.11 & 0.306 & 79.75 & 0.539 & 0.14 \\
        Gemini 1.5-Pro-002 & 20.96 & 0.339 & 78.90 & 0.561 & 0.14 \\
        Gemini 2.0-Flash & 24.43 & 0.363 & 74.59 & 0.498 & 0.98 \\
        GLM-4-Plus & 19.70 & 0.248 & 79.33 & 0.388 & 0.97 \\
        GPT-4o-0806 & 10.85 & 0.488 & 88.34 & 0.540 & 0.81 \\
        GPT-4o-mini-0718 & 37.21 & 0.279 & 61.77 & 0.414 & 1.02 \\
        Mistral-Large-2 & 75.66 & 0.306 & 21.64 & 0.403 & 2.70 \\
        Moonshot-128K & 43.40 & 0.242 & 53.11 & 0.398 & 3.50 \\
        o1-mini-0912 & 56.71 & 0.588 & 40.88 & 0.775 & 2.41 \\
        o1-preview-0912 & 2.54 & 0.334 & 93.63 & 0.696 & 3.82 \\
        Qwen2-72B-Instruct & 1.85 & 0.130 & 97.05 & 0.349 & 1.10 \\
         \bottomrule
    \end{tabular}
    
    \label{tab:cot_abl}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{pic/formal/allthreecots-diff99.pdf}
    \caption{Performance difference for all LLMs on MIR-Core between forced CoT and no CoT (dotted lines). Results forced CoTs almost always works worse (with the exception of GPT4o-mini-0718 and o1 models), and such gap increases with the number of shots. Mistral-Large-2's gap decreases dramatically at 2048-shot as such setting often exceeds its context length and the performance is low under both settings.}
    \label{fig:fig_moremodels_cot}
\end{figure}

% \begin{figure}
%     \centering
%     \subfigure[Part 1]{\includegraphics[width=0.45\textwidth]{pic/formal/selected_allthreecots_test2.pdf}}
%     \subfigure[Part 2]{\includegraphics[width=0.45\textwidth]{pic/formal/selected_allthreecots_test3.pdf}}
%     \caption{Performance for other LLMs on MIR-Core with forced CoT (dashed lines), no CoT (dotted lines), and no specification on CoT (solid lines). Results with no CoT works unanimously better than those with forced CoTs, and results with no specifications mostly lie between the two extreme settings. Due to hidden CoTs in background, performance with or without CoT for o1 models are almost identical.}
%     \label{fig:fig_moremodels_cot}
% \end{figure}


% \subsection{Results with Duplicated Few-shots}
% \label{sec:ablation_dup}

% To study whether the saturation of many-shot comes from the inability of retrieving the most useful shots for induction or the inability of aggregating many pieces of different, useful information, we conduct an ablation where we test $\{$GPT-4o-0806, GPT-4o-mini-0718, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Mistral-Large-2$\}$ on MIR-Core with $16$-shot, but with the following two settings:

% \begin{enumerate}
% \item one shot duplicated for $\{16-16, 32-16, 64-16, 128-16, 256-16, 512-16, 1024-16, 2048-16\}$ times, while other $15$ shots only appear once;
% \item each shot appearing $\{1, 2, 4, 8, 16, 32, 64, 128\}$ times.
% \end{enumerate}


% We ensure the examples in test cases with more shots are supersets of those with less shots, i.e., the information given in the input is strictly increasing with more shots. The result is shown in Fig.~\ref{fig:ablation_dup}, where solid lines are for original resutls on MIR-Core from Sec.~\ref{sec:discr}, dashed lines are for scenario 1 (one shot duplicate), and dotted lines are for scenario 2 (all shots duplicate).When the number of shots increase, as shown in panel (b), the performance difference between normal many-shot and both scenario 1 and 2 increases, which indicates that LLMs can indeed aggregate many pieces of information from more shots and acquire performance gain (which is almost not the case for Mistral-large-2, and thus its ``saturation point'' of performance with more shots is the lowest). However, the difference diminishes when there are more than $512$ shots (note this also applies for Gemini with 2M context length, thus this is not a problem of hard context limit). Such result indicates that too many pieces of information may actually harm LLMs' performance by distraction. Also, the performance of the dotted line (all shots duplicate) is in general not higher than that of the dashed line (one shot duplicate), which indicates that the problem is not in information retrieval as the two scenarios contain the same amount of information but the latter has higher difficulty for information retrieval.

% \begin{figure}
%     \centering
%     \subfigure[Accuracy]{\includegraphics[width=0.49\textwidth]{pic/formal/dup-original.pdf}}
%     \subfigure[Accuracy difference vs. no duplication]{\includegraphics[width=0.49\textwidth]{pic/formal/diff-dup.pdf}}
%     \caption{Results of duplicating shots, where solid lines are for original results on MIR-Core from Sec.~\ref{sec:discr}, dashed lines are for scenario 1 (one shot duplicate), and dotted lines are for scenario 2 (all shots duplicate); i.e., panel (b) is the result of dashed and dotted line subtracting solid line in panel (a).}
%     \label{fig:ablation_dup}
% \end{figure}





\subsection{Complete Results on Robustness of LLM Inductive Intelligence}
\label{sec:more_error_abl}
Fig.~\ref{fig:more_error} shows the results of models on $64$-shot, $256$-shot and $1024$-shot with different error rate for the shots, where the solid lines are $256$-shot or $1024$-shot accuracy respectively. We find that there are no significant performance difference across the same error rate with different number of shots (with the exception of o1-mini-0912 with $1024$ shots), and the robustness persists across different number of shots.

\begin{figure}[ht]
    \centering
    % First row: 64-shot images
    \begin{minipage}[t]{0.28\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error/selected_unaware_64shot.pdf}
        %\vspace{0.5em}
        \caption*{a) 64-Shot Unaware}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.28\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error/selected_aware1_64shot.pdf}
        %\vspace{0.5em}
        \caption*{b) 64-Shot Aware-Error}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.4\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error/selected_aware2_64shot.pdf}
        %\vspace{0.5em}
        \caption*{c) 64-Shot Aware-Ratio}
    \end{minipage}
    
    \vspace{1em} % Vertical space between rows
    
    % Second row: 256-shot images
    \begin{minipage}[t]{0.28\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error/selected_unaware_256shot.pdf}
        %\vspace{0.5em}
        \caption*{d) 256-Shot Unaware}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.28\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error/selected_aware1_256shot.pdf}
        %\vspace{0.5em}
        \caption*{e) 256-Shot Aware-Error}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.4\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error/selected_aware2_256shot.pdf}
        %\vspace{0.5em}
        \caption*{f) 256-Shot Aware-Ratio}
    \end{minipage}
    
    \vspace{1em} % Vertical space between rows
    
    % Third row: 1024-shot images
    \begin{minipage}[t]{0.28\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error/selected_unaware_1024shot.pdf}
        %\vspace{0.5em}
        \caption*{g) 1024-Shot Unaware}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.28\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error/selected_aware1_1024shot.pdf}
        %\vspace{0.5em}
        \caption*{h) 1024-Shot Aware-Error}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.4\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error/selected_aware2_1024shot.pdf}
        %\vspace{0.5em}
        \caption*{i) 1024-Shot Aware-Ratio}
    \end{minipage}
    
    \caption{The accuracy of models with erroneous shots under different prompt settings. The performance of the same error rate with different numbers of total shots are similar.}
    \label{fig:more_error}
\end{figure}





% \subsection{LLM-Generated Ground Truth Functions}
% \label{sec:llmgen}

% Here is an example of prompting GPT-4o-0806 to generate ground truth functions; the function are 
% https://chatgpt.com/share/6785ba07-844c-800d-8e96-998dccdf6b5e
% https://chatgpt.com/share/6785d4d1-a514-800d-b3fe-8d95d0e65ee0

%{\color{red}[TBD]}

% Here are four different Python functions that each take an array of numbers as input, along with an integer that defines how the array is processed:

% ### 1. **Function to Multiply Each Element by the Integer**
% This function multiplies each element of the array by the integer.

% ```python
% def multiply_elements(arr, multiplier):
%     return [x * multiplier for x in arr]
% ```

% **Example:**
% ```python
% arr = [1, 2, 3]
% multiplier = 2
% print(multiply_elements(arr, multiplier))  # Output: [2, 4, 6]
% ```

% ---

% ### 2. **Function to Rotate Array by the Integer**
% This function rotates the array by the integer value (positive for right rotation, negative for left rotation).

% ```python
% def rotate_array(arr, rotations):
%     n = len(arr)
%     rotations = rotations % n  # To handle rotations greater than the array length
%     return arr[-rotations:] + arr[:-rotations]
% ```

% **Example:**
% ```python
% arr = [1, 2, 3, 4, 5]
% rotations = 2
% print(rotate_array(arr, rotations))  # Output: [4, 5, 1, 2, 3]
% ```

% ---

% ### 3. **Function to Return the First N Elements of the Array**
% This function returns the first `n` elements of the array, where `n` is the integer passed.

% ```python
% def get_first_n_elements(arr, n):
%     return arr[:n]
% ```

% **Example:**
% ```python
% arr = [1, 2, 3, 4, 5]
% n = 3
% print(get_first_n_elements(arr, n))  # Output: [1, 2, 3]
% ```

% ---

% ### 4. **Function to Remove Every N-th Element**
% This function removes every N-th element from the array.

% ```python
% def remove_every_nth(arr, n):
%     return [x for i, x in enumerate(arr, 1) if i % n != 0]
% ```

% **Example:**
% ```python
% arr = [1, 2, 3, 4, 5, 6, 7]
% n = 3
% print(remove_every_nth(arr, n))  # Output: [1, 2, 4, 5, 7]
% ```

% These functions apply different operations based on the input integer and the array. Let me know if you'd like any further modifications!