\section{Introduction}
\label{sec:intro}
The tremendous success of Large Language Models (LLMs) in recent years~\citep{ouyang2022training, hurst2024gpt, jaech2024openai} has finally brought us to the extent where human-level Artificial General Intelligence (AGI) becomes seemingly within reach~\citep{jaech2024openai}. With such success, researchers have shifted their focus from syntax- and word-level traditional Natural Language Processing (NLP) tasks such as named entity recognition~\citep{mohit2014named, li2020survey}, sentiment classification~\citep{socher-etal-2013-recursive, tang2015effective} and translation~\citep{lopez2008statistical, stahlberg2020neural} onto those which measure abilities once considered unique to humans. Inductive Reasoning (IR)~\citep{hayes2010inductive}, which is the ability to summarize general high-level rules from existing examples, thus comes into attention~\citep{chollet2019measure}; instead of math or coding which require knowledge and experience in particular fields, inductive reasoning measures the abstract generalization power of an intelligence~\citep{chollet2019measure} and is considered as a primal mental ability~\citep{Kinshuk01052006}. Thus, it is long studied by the cognitive science community~\citep{bisanz1994inductive, heit2000properties}, adopted in IQ tests for human~\citep{ferrara1986children}, and is recently used as a measurement for the state-of-the-art LLMs such as o1~\citep{jaech2024openai} to show their intelligence level.
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{pic/mir-teaser-v3.pdf}
    \caption{A high-level illustration of our data generation pipeline. We first collect functions from existing coding benchmarks, then let GPT-4o-0806 write data generator for each function; we then run the data generator to produce input shots, and combine them with ground truth function to produce output shots. With input and output shots, we concatenate them and build MIR-extended; then, with initial tests on several models, we study the factor for what makes an inductive reasoning problem benefit from many-shot, and build MIR-core based on selection with the factors.}
    \label{fig:teaser}
\end{figure*}
While many IR benchmarks~\citep{banatt2024wilt, ma2024kor, li2024mirage} for LLMs have been proposed, such as ARC~\citep{chollet2019measure} and its variants~\citep{xu2023llms, kim2022playgrounds}, they all focused on few-shot In-Context Learning (ICL) with typically $<$10 examples. While induction from fewer examples may imply stronger reasoning ability, some underlying rules for IR problems are inherently too complicated or ambiguous for a few examples. For instance, consider a quadratic curve with clipping; with three examples, it is unknown whether the curve is sampled from a circle or a quadratic curve, let alone a clipped one; however, with $300$ examples, not only the quadratic function is clear, but the special clipping rule are also very likely to be retrieved. LLM should handle such long-context, many-example cases as well as few-shot inductive reasoning.

In fact, the scaling of the amount of ICL data is in line with the trend of the LLM community striving to expand the context length~\citep{peng2023yarn, su2024roformer} for super-human problem-solving efficiency. It is with this trend that a new paradigm emerged recently: Many-Shot ICL, which typically uses hundreds to thousands of examples for test-time task learning without using expensive and relatively data-inefficient fine-tuning~\citep{agarwal2024many}. However, many-shot evaluations are mostly focused on classifications~\citep{li2023context, bertsch2024context, zhao2024probing, li2024long, zou2024retrieval}, which is a very limited area for inductive reasoning. Other standard long-context LLM tasks, such as needle-in-a-haystack (NIAH)~\citep{niah}, are more of a retrieval problem than gathering understanding from many pieces of clues. With all these blanks in LLM evaluation (see Tab.~\ref{tab:brief_comparison} for a comparison with the most related benchmarks, and Tab.~\ref{table:related_work1} in Appendix~\ref{sec:ext_related_work} for a more complete version), we must ask: \textit{How to evaluate the LLM's ability to aggregate many pieces of information from many examples to perform inductive reasoning on various complicated problems?}

\begin{table}
    % \small

    \caption{Comparison on our benchmark with the most related prior many-shot / long-context benchmarks (first part) and inductive reasoning benchmarks (second part). See Tab.~\ref{table:related_work1} in Appendix~\ref{sec:ext_related_work} for a complete comparison. To save space, we abbreviate ``Many Shot'' as MS, ``Inductive Reasoning'' as IR ($\semicorrect $ represents ``classification only''), ``Prob.'' as problems, and ``I/O Div.'' as ``Input/Output Diversity'' (\textbf{having at least $2$ different input-output types, e.g., given an array and output an integer, or given a pair of strings and output a choice}). ``Gen.'' means ``Generative'', which means whether new test cases can be easily generated without much human effort. ``LB'' means whether a leaderboard is available, and ``EE'' means ``Easy Evaluation'', i.e., whether a pipeline for evaluating any given new model exists. ``New Data'' means whether the input-output data never appears in existing benchmarks; if so, the benchmark is secured against data contamination. Note, the counting of \#IR Problems and ``Gen.'' take different target input-output for the same function into account, but \textbf{do not take different sets of shots into account}.}
    
    \centering
    \begin{tabular}{lccccccccc}
    \toprule
    \textbf{Benchmarks} & 
    \textbf{MS} &
    \textbf{IR} &
    \textbf{\# IR Prob.} &
    \textbf{I/O Div.} &
    \textbf{Max \# Shots} & 
    \textbf{Gen.} & 
    \textbf{LB} & \textbf{EE} & 
    \textbf{New Data} 
    \\
    \midrule
    HELMET~\citep{yen2024helmet} & \greencheck & \semicorrect & 500 &\greencheck & $\sim$10K & \redcross & \redcross  & \greencheck & \redcross\\
    LongICLBench~\citep{li2024long} & \greencheck & \semicorrect & 3000 &\redcross & $\sim$2000 & \redcross & \greencheck & \greencheck & \redcross \\
    ManyICLBench~\citep{zou2024retrieval} & \greencheck & \semicorrect & 1000 & \greencheck &  7252 & \redcross & \redcross  & \redcross & \redcross \\
    LMAct~\citep{ruoss2024lmact}& \greencheck & \redcross & 0 & \redcross & 256 & \greencheck & \redcross & \greencheck & \greencheck \\
    LongBench~\citep{bai2023longbench} & \greencheck & \semicorrect & 400 & \greencheck & 600 & \redcross & \greencheck & \greencheck & \greencheck  \\
    \midrule
    KORBench~\citep{ma2024kor} & \redcross & \greencheck & 50 & \greencheck & 3  &  \redcross & \greencheck & \greencheck &  \greencheck  \\
    ARC~\citep{chollet2019measure} & \redcross  & \greencheck & 800 & \redcross & 3 & \redcross & \greencheck & \greencheck & \greencheck \\ 
    % H-ARC & \redcross  & \greencheck & 1000 && 3 & \redcross & \redcross & \greencheck & \greencheck\\
    WILT~\citep{banatt2024wilt}  & \redcross & \greencheck & 50 & \redcross & 30 & \redcross & \greencheck & \greencheck & \greencheck \\
    LogicVista~\citep{xiao2024logicvista}  & \redcross & \greencheck& 107 & \greencheck & 10 & \redcross & \redcross & \greencheck & \greencheck \\
    MIRAGE~\citep{li2024mirage}  & \redcross & \greencheck & 2000 & \greencheck & 8 & \greencheck & \redcross & \redcross & \greencheck \\
    % \midrule
    % InfiniteBench \\
    % Loong \\
    % NIAH \\
    % RULER \\
    % BaBILONG \\
    % BAMBOO \\
    % L-Eval \\
    % LooGLE \\
    % ZeroScrolls \\

    \midrule
    \textbf{MIR-Bench (Ours)} & \greencheck & \greencheck & 6930 & \greencheck & 2048 & \greencheck & \greencheck& \greencheck& \greencheck\\
    \bottomrule
    \end{tabular}

    
    \label{tab:brief_comparison}
\end{table}

To address the problem above and fix the limitation of existing LLM evaluation from both inductive reasoning and the many-shot/long-context community, we propose MIR-Bench, a large and diverse \textbf{M}any-shot \textbf{I}nductive \textbf{R}easoning benchmark, in which LLMs are given examples of input-output data generated by an underlying unknown function with diverse input-output forms 
%(e.g. mapping from string to integer, or string and integer to another string)
, and need to predict the output for new input. 

The benchmark is generated by the following pipeline as illustrated in Fig.~\ref{fig:teaser}: 1) we collect functions from introductory-level coding benchmarks including HumanEval+~\citep{evalplus}, MBPP+~\citep{evalplus} and APPS~\citep{hendrycks2021measuring}; 2) we use GPT-4o-0806 to write code as data generators that produces input-output pairs, and execute them to generate ICL shots and test input; 3) run ground truth function with generated inputs for ground-truth outputs; 4) use scripts to build prompts for target problem, and filter out problems with too long shot length or insufficient input-output diversity. With such procedure, we propose two sets of problems: \textbf{MIR-Core} and \textbf{MIR-Extended}, which contains $3000$ problems ($300$ functions $\times$ $10$ test cases), and $6930$ problems ($693$ functions $\times$ $10$ test cases) respectively, and can be easily supplemented by generating more test cases. The former is selected from the latter and contains the problems that LLM benefits the most from many-shot (see Sec.~\ref{sec:discr} for details).

% with the problems which ``needs many-shot the most'', i.e., with the highest performance increase from few-shot to many-shot inductive reasoning (see Sec.~\ref{sec:discr} for details).

To evaluate the long-context intelligence level of different LLMs, we test a variety of the cutting-edge LLMs on our benchmark, and find our benchmark to be both challenging and discriminative, as model performance vary greatly, but none saturates on our benchmark. We have also conducted the following important empirical studies with our benchmark, all of which are novel, important yet largely overlooked by previous works: 

\begin{enumerate}%[itemsep=0pt]
    \item What are the factors that determine ICL performance change with respect to number of shots (Sec.~\ref{sec:discr},~\ref{sec:ablation_dup})?
    \item Can Chain-of-Thought (CoT)~\citep{wei2022chain} help inductive reasoning? If so / if not, why (Sec.~\ref{sec:cot})?
    \item How robust is LLM's inductive intelligence against errorneous input-output pairs (Sec.~\ref{sec:robust})?
    \item Does the paradigm of first coding, then executing code for results~\citep{cheng2024inductive} work for many-shot in-context inductive reasoning (Sec.~\ref{sec:coding})?
\end{enumerate}

In conclusion, our key contributions can be summarized as: 1) We propose MIR-Bench, a large-scale and diverse many-shot inductive reasoning benchmark, which fills in the blank for both many-shot and inductive reasoning community; 2) We build a novel automatic pipeline for generating new many-shot inductive reasoning problems from existing coding benchmarks without using existing corpus as input/output (i.e., no data leakage issues); 3) We perform empirical study on many important problems overlooked by previous works and gained important insights on LLM's many-shot / long-context intelligence.

% ... {\color{red}[TODO]}


 %  is not only an apparent milestone for super human-level intelligence, but



% have   by leaps and bounds