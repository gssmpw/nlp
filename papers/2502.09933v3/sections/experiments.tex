\section{Experiments}
\label{sec:exp}
In this section, we will introduce general performance of existing models on our benchmark and a series of exploratory experiments which gives novel insights. More specifically, we first introduce the main results on our MIR-Extended benchmark in Sec.~\ref{sec:main}; then, we explore factors that indicate whether a problem can benefit from many-shot, and build MIR-Core in Sec.~\ref{sec:discr}. We further conduct more in-depth analysis on important properties of LLM's many-shot intelligence in several aspects on MIR-Core in Sec.~\ref{sec:ablation_dup}, ~\ref{sec:cot}, ~\ref{sec:robust} and~\ref{sec:coding}. For better readability, we defer more empirical ablation and analysis to Appendix~\ref{sec:extraexp}.

\subsection{MIR-Extended}
\label{sec:mir-ext}
\textbf{Evaluation setup.} We evaluate a set of 15 cutting-edge LLMs with context window $\geq$ 128K tokens on our MIR-Extended benchmark with $693$ different function and $10$ test cases per function (a total of $6930$ problems). The evaluated LLMs are: $\{$o1-preview-0912, o1-mini-0912, GPT-4o-0806, GPT-4o-mini-0708, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Gemini-Flash 2.0, Claude-3.5-Sonnet, Claude-3.5-Haiku, Claude-3-Haiku, Claude-3-Sonnet, Qwen2-72B-Instruct, Mistral-Large-2, Moonshot-128K, GLM-4-Plus$\}$ by invoking official APIs; see Appendix~\ref{sec:main_prompt} for detailed prompts. We use greedy decoding (with temperature $0$) for evaluation (See Appendix~\ref{sec:ablation_std} for ablations on the robustness of evaluation), and use exact match accuracy as the metric with rule-based extraction of the answer from LLM's response (See Appendix~\ref{sec:extract}). Each model is evaluated with $\{4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\}$-shot with shots uniformly randomly sampled from $20000$ shots generated in Sec.~\ref{sec:construction}.

% how many models? how to evaluate? temperature? prompt? what GPU? what metric?

\textbf{Results.} Fig.~\ref{fig:main}(a) illustrates the performance of all 15 LLMs on our MIR-Extended benchmark. The performance of the LLMs varies greatly; among all models, o1-mini-0912 and o1-preview-0912 clearly outperform all other models, followed by Claude-3.5-Sonnet and GPT-4o-0806. However, all LLMs evaluated are far from addressing our inductive reasoning task; the best model, o1-mini-0912, only reaches an accuracy of less than $0.7$, while most models such as GPT-4o-0806 only achieve less than $0.4$ accuracy. Such performance indicates that different from the conclusion in~\citet{cheng2024inductive}, LLMs' inductive reasoning abilities still limited in complicated tasks. Claude-3.5-Haiku achieves surprisingly low accuracy; upon checking examples, we find that the model often do not understand our prompt and see the target input as part of an incomplete data, thus refusing to answer the problem.

Interestingly, scaling up the number of shots is not always beneficial, similar to many tasks in~\citet{agarwal2024many}. For models other than Gemini, the performance drop over $512$ shots can be partly attributed to exceeding the 128K context limit~\footnote{Which only happens in $\leq 1\%$ case for $1024$ shots but more common for $2048$ shots. See Tab.~\ref{tab:error} for details.}; however, for most language models evaluated (including GPT and o1-mini), the performance growth often stops at no more than $256$ shots, where the context limit is not reached. Such issue stems from attention dispersion as stated in~\citet{yuan2024focused}; as the number of examples increases, the attention weights which should be cast on the most informative shots is distracted by the less informative ones instead of lack of information retrieval ability. We validate this via ablation in Sec.~\ref{sec:ablation_dup}.



\begin{figure}[ht]
    \centering
    \begin{minipage}[c]{0.4\linewidth}
        \centering
        \includegraphics[height=4.7cm]{pic/formal/all-v2.pdf}
        \caption*{a) MIR-Extended}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.55\linewidth}
        \centering
        \includegraphics[height=4.7cm]{pic/formal/quad-v2.pdf}
        \caption*{b) MIR-Core}
    \end{minipage}
    
    \caption{The performance of all LLMs on MIR-Extended (panel (a)) and MIR-Core (panel (b)). For better readability, we only show the most representative models; see Fig.~\ref{fig:all_main} in Appendix~\ref{sec:all_main} for the rest. The benchmark poses challenge to all models tested including o1-preview and o1-mini. Most models will ``saturate'' at a particular number of shots, i.e., their performances stop to improve when more shots are given due to limited information integration capability.}
    \label{fig:main}
\end{figure}



\label{sec:main}

\subsection{MIR-Core: Problems Requiring Many-Shot} % better name TBD
\label{sec:discr}

\textbf{Ablation on possible factors.} While we have obtained many inductive reasoning problems, not all of them necessarily benefits from many-shot ICL; for example, a simple function such as adding two numbers or absolute value can be induced in a few shots. To study the inductive reasoning problems whose difficulties are \textit{distinctive} between few-shot and many-shot, and curate a high-quality many-shot benchmark, we perform a detailed ablation study on possible factors for such distinctiveness. To better study such property, we define the following metric $D$:
\begin{equation}
\label{eq:D}
\begin{aligned}
%D&=\frac{1}{2}\bigg\{\frac{1}{2}\left[(\text{acc}@64+\text{acc}@128)-(\text{acc}@16+\text{acc}@32)\right]+\\
%&\frac{1}{3}[(\text{acc}@32+\text{acc}@64+\text{acc}@128)-(\text{acc}@4+\text{acc}@8+\\
%&\text{acc}@16)]\bigg\},
D&=\frac{D_1+D_2}{2},\text{ where }
%D_1&=\frac{1}{2}\left[(\text{acc}@64+\text{acc}@128)-(\text{acc}@16+\text{acc}@32)\right],\\
D_1=\left[\frac{\text{acc}@64+\text{acc}@128}{2}\right]-\left[\frac{\text{acc}@16+\text{acc}@32}{2}\right],\\
%D_2&=\frac{1}{3}[(\text{acc}@32+\text{acc}@64+\text{acc}@128)-(\text{acc}@4+\text{acc}@8+\text{acc}@16)]
D_2&=\left[\frac{\text{acc}@32+\text{acc}@64+\text{acc}@128}{3}\right]-\left[\frac{\text{acc}@4+\text{acc}@8+\text{acc}@16}{3}\right].
\end{aligned}
\end{equation}
 

In Eq.~\eqref{eq:D}, $\text{acc}@x$, $x\in\{4,8,16,32,64,128\}$ is the average accuracy of $\{$GPT-4o-0806, GPT-4o-mini-0718, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Mistral-Large-2$\}$ at $x$-shot, each over $10$ test cases. 
%The range of $x$ is selected based on prior inductive reasoning work such as ARC~\citep{chollet2019measure} and the number of shots where performance saturated on MIR-Extended. 
Intuitively, $D$ is a combination of two components $D_1$ and $D_2$, each measures average performance growth from different few-shot to many-shot ranges; The range of $x$ is based on prior inductive reasoning work~\citep{chollet2019measure} and the number of shots where performances saturate on MIR-Extended. Ideally, we want to identify the factors which are positively related to $D$, and curate MIR-Core with problems having higher values of $D$.

\begin{wrapfigure}{r}{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{pic/formal/coeff.pdf}
    \caption{The coefficients of the quadratic function fitting $D$ with the aforementioned factors normalized between $[0, 1]$. The blank row and column are for constant factors. LLM-labeled difficulty is the leading factor for $D$, while answer diversity and shot length are less important.}
    \label{fig:coeff}
    \vspace{-10pt}
\end{wrapfigure}

With such metric $D$, we consider the following factors that are potentially relevant to the distinctiveness between few-shot and many-shot:  \textbf{1) Ground truth function complexity:} $64$-shot accuracy, function code length, LLM-evaluated function difficulty level\footnote{A reliable evaluation is non-trivial; see Appendix~\ref{sec:difficulty}.} and problem topics;
\textbf{2) Answer complexity:} number of different answers across $20000$ shots, and the ratio of the most common answer out of $20000$ shots; \textbf{3) Input complexity:} input length per shot.



As we aim to ensure the diversity of our evaluation, we did not select problems based on problem topics (See Appendix~\ref{sec:probtype} for ablation on problem topics). For the rest of the factors, we fit the ground-truth metric $D$ using a quadratic function with these factors (after normalization) as self-variables. We use quadratic function as we found some factors (e.g. \# different answers), are roughly raised at both ends and concave in the middle, while others are roughly monotonic (e.g. code length); see Fig.~\ref{fig:fig_single_factor} in Appendix~\ref{sec:example_factor} for details.

The coefficients are illustrated as Fig.~\ref{fig:coeff}. as the result shows, ground truth function complexity is the dominating factor for distinctiveness between few-shot and many-shot performance, among which LLM-labeled difficulty is a leading, positive factor (i.e. more difficult problem will require more shots). Answer diversity and input complexity are relatively less important. See Appendix~\ref{sec:example_factor} for single-factor analysis.

%The global maximum of the fitted quadratic function solved by cvxpy.optimize~\citep{2020SciPy-NMeth}~\footnote{As the function is not necessarily convex, we enumerate all vertices on $[0, 1]^6$ as initial guesses to ensure the maxima is global.} $D$ over all factors $\in[0, 1]$ is $[1, 5.28\times 10^{-10}, 1, 4.68\times 10^{-10}, 0.317, 5.14\times 10^{-9}]$, which indicates that generally, a problem with higher difficulty 

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.85\linewidth]{pic/formal/coeff.pdf}
%     \caption{The coefficients of the quadratic function fitting $D$ with the aforementioned factors normalized between $[0, 1]$. The blank row and column are for constant factors. LLM-labeled difficulty is the leading factor for $D$, while answer diversity and shot length are less important.}
%     \label{fig:coeff}
% \end{figure}



\textbf{Selection of data for MIR-Core.} We adopt the quadratic function's fitting result and select the $300$ problems with the highest predicted $D$-value as MIR-Core. Such design is based on a balance between achieving higher $D$-value for MIR-Core and unbiased evaluation for the LLMs involved in computing $D$-value (thus we do not use the problems with highest ground truth $D$).

\textbf{Results on MIR-Core.} We again evaluate all $15$ LLMs in Sec.~\ref{sec:mir-ext} on our MIR-Core with $300$ problems with $10$ test case each. The results are illustrated in Fig.~\ref{fig:main}(b). while the performance difference between few-shot and many-shot are more distinctive as expected, the relative performance and many-shot saturation phenomenon between $15$ models remain unchanged.


\subsection{Results with Duplicated Few-shots}
\label{sec:ablation_dup}

To study whether the saturation of many-shot in Sec.~\ref{sec:main} and~\ref{sec:discr} comes from the inability of retrieving the most useful shots for induction or the inability of aggregating many pieces of different, useful information, we conduct an ablation where we test $\{$GPT-4o-0806, GPT-4o-mini-0718, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Mistral-Large-2$\}$ on MIR-Core with $16$-shot, but with the following two settings: \textbf{1) one shot duplicated} until total shots number of reach $\{16, 32, 64, 128, 256, 512, 1024, 2048\}$, while other $15$ shots only appear once; and \textbf{2) each of the $16$ shot reused} for $\{1, 2, 4, 8, 16, 32, 64, 128\}$ times.

We ensure the examples in test cases with more shots are supersets of those with less shots, i.e., the information given in the input is strictly increasing with more shots. The result is shown in Fig.~\ref{fig:ablation_dup}, where solid lines are for original resutls on MIR-Core from Sec.~\ref{sec:discr}, dashed lines are for scenario 1 (one shot duplicate), and dotted lines are for scenario 2 (all shots duplicate).When the number of shots increase, as shown in panel (b), the performance difference between normal many-shot and both scenario 1 and 2 increases, which indicates that LLMs can indeed aggregate many pieces of information from more shots and acquire performance gain (which is almost not the case for Mistral-Large-2, and thus its ``saturation point'' of performance with more shots is the lowest). However, the difference diminishes when there are more than $512$ shots (note this also applies for Gemini with 2M context length, thus this is not a problem of hard context limit). Such result indicates that too many pieces of information may actually harm LLMs' performance by distraction. Also, the performance of the dotted line (all shots duplicate) is in general not higher than that of the dashed line (one shot duplicate), which indicates that the problem is not in information retrieval as the two scenarios contain the same amount of information but the latter has higher difficulty for information retrieval. Here we summarize the insight:

    \ovalbox{
        \begin{minipage}{0.95\linewidth}
            \textbf{Insight 1:} The saturation of many-shots does not come from information retrieval, but from distraction when aggregating too many information.
        \end{minipage}
    }

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[height=5cm]{pic/formal/merge-three-round.pdf}
        \caption*{a) Accuracy}
    \end{minipage}
    \begin{minipage}{0.48\linewidth}
       \centering
        % Second image in left column
        \includegraphics[height=5cm]{pic/formal/merge-three-diff-round.pdf}
        \caption*{b) Accuracy difference}
        \end{minipage}
        \captionof{figure}{Results of duplicating shots, where solid lines are for original results on MIR-Core from Sec.~\ref{sec:discr}, dashed lines are for scenario 1 (one shot duplicate), and dotted lines are for scenario 2 (all shots duplicate); i.e., panel (b) is the result of dashed and dotted line subtracting solid line in panel (a).}
        \label{fig:ablation_dup}
\end{figure}


\subsection{The Effectiveness of CoT}
\label{sec:cot}

Chain of Thought (CoT)~\citep{wei2022chain} is a foundamental LLM technique proved to be of great help for LLMs in breaking down complex reasoning problem into step-by-step rationale. In this section, we will explore whether CoT helps many-shot inductive reasoning in our task.

% \begin{figure}[ht]
%     \centering
% % \includegraphics[width=0.7\linewidth]{pic/formal/selected_allthreecots_test1.pdf}
% \subfigure[Accuracy]{\includegraphics[height=3.5cm]{pic/formal/selected_allthreecots_test1.pdf}}
%     \subfigure[Accuracy difference]{\includegraphics[height=3.5cm]{pic/formal/allthreecots_diff1.pdf}}
%     \caption{Panel (a) shows the result of LLMs with forced CoT (dashed lines), no CoT (dotted lines) and no CoT specification (solid line; same as that in Fig.~\ref{fig:main}) on MIR-Core, and panel (b) shows the difference between forced CoT and no CoT. Surprisingly, forced CoT works generally worse than no CoT, and the gap increases with more shots.}
%     \label{fig:main_cot}
% \end{figure}

\textbf{Statistics in main results.} We first count the number of answers with and without CoT\footnote{We count answers with $\geq 20$ characters before the final ``Output:'' as the ones with CoT.} in MIR-Core results (Sec.~\ref{sec:discr}) and their respective correct rate; surprisingly, we find that in all $15$ models, including thinking models such as o1, answers without CoT have significantly higher accuracy than those with CoT (see Tab.~\ref{sec:cot_abl} for results).

\begin{figure}[ht]
        \centering
        % First image in right column
        \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[height=5cm]{pic/formal/selected_allthreecots_test1.pdf}
        \caption*{a) Accuracy}
        \end{minipage}
        % Second image in right column
        \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[height=5cm]{pic/formal/allthreecots_diff1.pdf}
        \caption*{b) Accuracy difference}
        \end{minipage}
        \captionof{figure}{Panel (a) shows the result of LLMs with forced CoT (dashed lines), no CoT (dotted lines) and no CoT specification (solid line; same as that in Fig.~\ref{fig:main}) on MIR-Core, and panel (b) shows the difference between forced CoT and no CoT. Surprisingly, forced CoT works generally worse than no CoT, and the gap increases with more shots.}
        \label{fig:main_cot}
\end{figure}

\textbf{Evaluation setup.} To further validate whether the performance difference comes from  CoT or from problems with different difficulty levels (e.g. LLMs only apply CoT on difficult problems), we further test MIR-Core with different prompts under two settings: 1) the ``Direct'' setting, where the models are required to \textbf{not} write CoT; 2) the ``CoT'' setting, where the models are \textbf{forced} to write CoT. We evaluate all $15$ models in Sec.~\ref{sec:main}. See Appendix~\ref{sec:abl_prompt} for prompts.




\textbf{Results.} Fig.~\ref{fig:main_cot} shows the result of $\{$GPT-4o-0806, GPT-4o-mini-0718, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Mistral-Large-2$\}$ (see Sec.~\ref{sec:cot_abl} for other LLMs) on MIR-Core. Surprisingly, for most models, forced CoT works much worse than no CoT, and such performance gap increases with the number of shots (See Fig.~\ref{fig:fig_moremodels_cot} in Appendix~\ref{sec:cot_abl} for details). We hypothesize such phenomenon comes from the complicated nature of CoT. For example, consider a problem with two integers $a$ and $b$ as input and $\max(a,b)$ as output; transformers can easily duplicate the mapping relation between the three sets of tokens $a$, $b$ and $\max(a,b)$ as if going through a gradient descent with regression loss on examples as the training set, as suggested by many theoretical works in ICL~\citep{dai2022can, von2023transformers, mahankali2023one}. However, a mapping from input to CoT makes the equivalent of gradient descent much more opaque. Here we summarize the insights:

%\colorbox{lightgray}{
    \ovalbox{
        \begin{minipage}{0.95\linewidth}
            \textbf{Insight 2:} CoT does not necessarily help many-shot inductive reasoning.
        \end{minipage}
    }
%}





\subsection{Robustness of LLM Inductive Intelligence}

While many works~\citep{agarwal2024many} have studied LLM's many-shot ICL performance, the robustness of LLM's many-shot ICL ability~\citep{zhao2024context}, i.e. the accuracy given incorrect examples, is still largely underexplored. In this section, we explore the performance change with increasing number of shots with incorrect answers.

\textbf{Evaluation Setup.} We test all 15 models in Sec.~\ref{sec:main} on MIR-Core with $3$ different settings: 1) the ``unaware'' setting, where the models do not know there are incorrect answers in the provided examples; 2) the ``aware-error'' setting, where the models know that some (unknown number of) examples are incorrect; and 3) the ``aware-ratio'' setting, where the models know exactly how many shots are incorrect out of all given shots. The three settings are mostly the same, with slight difference in prompt; see Appendix~\ref{sec:abl_prompt} for details. We test $\{64, 256, 1024\}$ shots $\times$ error ratio of $\{1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 3/4\}$ respectively. See Appendix~\ref{sec:error_shots} for data generation details.

\begin{figure*}[ht]
    \centering
    \begin{minipage}[t]{0.28\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error_part/selected_unaware_64shot.pdf}
        \caption*{a) Unaware}
        % {\small Unaware}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.28\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error_part/selected_aware1_64shot.pdf}
        \caption*{b) Aware-Error}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.4\textwidth}
        \centering
        \includegraphics[height=3.5cm]{pic/formal/error_part/selected_aware2_64shot.pdf}
        \caption*{c) Aware-Ratio}
    \end{minipage}
    
    \caption{The accuracy of representative models with erroneous shots under different prompt settings with 64 shots (see Fig.~\ref{fig:more_error} in Appendix~\ref{sec:more_error_abl} for full results). The result shows LLMs are generally quite robust against erroneous shots.}
    \label{fig:error}
\end{figure*}

\textbf{Results.} The results for $64$-shot are illustrated in Fig.~\ref{fig:error} (see Appendix~\ref{sec:more_error_abl} for the rest). Surprisingly, we found that LLMs are generally quite robust against errorneous shots; their performance are only slightly harmed below $1/8$, and can maintain decent performance even with $3/4$ error rate. We find that generally, there is generally no significant performance difference in different awareness level of erroneous shots; some exceptions are Gemini-2.0 Flash and Claude-3.5-Haiku (see Fig.~\ref{fig:more_error} in Appendix~\ref{sec:more_error_abl}), where the answering paradigm of the former remains the same, and the latter accepts the target input as part of the ``incomplete'' data and rejects answering questions less frequently. We summarize the most important insight of this experiment in the box below:



\label{sec:robust}

    \ovalbox{
        \begin{minipage}{0.95\linewidth}
            \textbf{Insight 3:} LLMs are quite robust against errorneous shots in many-shot inductive reasoning tasks.
        \end{minipage}
    }


\subsection{SolverLearner: Is ``First-Coding, Then-Running'' the Cure?}
\label{sec:coding}

For better inductive reasoning ability, \citet{cheng2024inductive} proposed SolverLearner, an inductive reasoning framework where LLMs write code first for inductive reasoning problems and then generate answers with python interpreter. With such framework, the authors claim that LLMs demonstrate remarkable
inductive reasoning capabilities under their framework. However, their study is limited to a few relatively weak LLMs, (GPT-3.5, GPT-3), limited amount of inductive reasoning problems and few-shot; to check whether such solution also works for the many-shot case, we re-implement their method on MIR-Core (see Appendix~\ref{sec:solverlearner_app} for prompts).

We test SolverLearner with $\{$Claude-35-Sonnet, GPT-4o-0806, GPT-4o-mini-0718, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Mistral-Large-2$\}$ for $\{16, 64, 256, 1024\}$ shots respectively on MIR-Core. For each code snippet, generated by LLMs, we set a limit of $1$ second for execution, as we need to run $300$ functions $\times$ $10$ test cases $\times$ $4$ different number of shots $\times$ $6$ models = $72000$ code snippets. 

\begin{table}[ht]
    \centering
    \caption{The accuracy at 16, 64, 256 and 1024 shots respectively for SolverLearner on MIR-Core, and its performance difference from normal inductive reasoning paradigm. We plot improvements with $>0.02$ \textbf{\color{blue}blue} and $<-0.02$ \textbf{\color{red}red}. We find that the performance of SolverLearner varies from model to model, and does not necessarily perform better than normal inductive reasoning paradigm. Also, SolverLearner curves under many-shot are more ``flat''; i.e., it does not seem to make good use of extra information from a large number of shots. Such result indicates that LLM many-shot inductive reasoning is still an open problem, and straightforward solutions such as SolverLearner are not suffice yet.}
    \begin{tabular}{ccccc}
       \toprule
        Model & Acc.@16 & Acc.@64 & Acc.@256 & Acc.@1024 \\
        \midrule
         Claude-3.5-Sonnet & 0.577(-0.009) & 0.604(-0.015) & 0.605(-0.017) & 0.603({\color{blue}+0.04}) \\
         GPT4o-0806 & 0.53(+0.012) & 0.534({\color{red}-0.033}) & 0.538({\color{red}-0.029}) & 0.556(+0.004) \\
         GPT4o-mini-0718 & 0.350(-0.006) & 0.375(+0.003) & 0.386(+0.008) & 0.370(+0.014) \\
         Gemini-1.5 Flash-002 & 0.473(-0.009) & 0.484({\color{red}-0.03}) & 0.479({\color{red}-0.038}) & 0.486({\color{red}-0.03}) \\
         Gemini-1.5 Pro-002 & 0.469({\color{red}-0.029}) & 0.495({\color{red}-0.055}) & 0.483({\color{red}-0.067}) & 0.491({\color{red}-0.04}) \\
         Mistral-Large-2 & 0.42(\color{blue}+0.057) & 0.430({\color{blue}+0.028}) & 0.428({\color{blue}+0.078}) & 0.356({\color{blue}+0.102}) \\
         \bottomrule
    \end{tabular}
    \label{tab:code_main}
\end{table}

Tab.~\ref{tab:code_main} demonstrates the accuracy of each model (with difference from the standard inductive reasoning paradigm) on MIR-Core, and Tab.~\ref{tab:code_err} demonstrates the error rate when writing code. We found that the effect of SolverLearner varies from model to model; i.e., SolverLearner does not necessarily improve performance on our benchmark. Also, SolverLearner does not seem to utilize many-shot well; the performance increase from $16$-shot to $1024$-shot is much smaller than that of standard inductive reasoning paradigm. We hypothesize such issue, similar to that in Sec.~\ref{sec:cot}, stems from the complicated nature of the code. Also, for models with relatively weaker long-context ability (e.g. Mistral-Large-2), the error rate with many-shot will largely increase beyond its ``effective''~\citep{hsieh2024ruler} context length. Thus, many-shot inductive reasoning is still an open problem and not yet solved by straightforward solutions such as SolverLearner. The insight can be summarized as follows:





\begin{table}[ht]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \caption{The Do-Not-Finish (i.e., no solution function generated) and Runtime Error (RE, including timeout and exception during running) rate at 16, 64, 256 and 1024 shots respectively for SolverLearner on MIR-Core. Generally, with more shots, the error rate of LLMs will increase. For models with relatively weaker long-context ability such as Mistral-Large-2, the error rate will wildly increase under long context scenario.}
    \begin{tabular}{ccccccccc}
       \toprule
        Model & DNF@16 & RE@16 & DNF@64 & RE@64 & DNF@256 & RE@256 & DNF@1024 & RE@1024 \\
        \midrule
        Claude-3.5-Sonnet & 0 & 0.0027 & 0 & 0.0063 & 0 & 0.0007 & 0 & 0.0037 \\
        GPT4o-0806  & 0 & 0.009 & 0 & 0.0103 & 0 & 0.0157 & 0.0033 & 0.0137\\
        GPT4o-mini-0718 & 0 & 0.0103 & 0 & 0.0147 & 0 & 0.0167 & 0.0033 & 0.017\\
        Gemini-1.5 Flash-002 & 0 & 0.0093 & 0 & 0.0117 & 0 & 0.0087 & 0 & 0.011  \\
        Gemini-1.5 Pro-002 & 0 & 0.0093 & 0 & 0.008 & 0 & 0.009 & 0 & 0.0107 \\
        Mistral-Large-2 & 0 & 0.008 & 0 & 0.0077 & 0.0047 & 0.012 & 0.1163 & 0.0473  \\
         \bottomrule
    \end{tabular}
    
    \label{tab:code_err}
\end{table}

\ovalbox{
        \begin{minipage}{0.95\linewidth}\textbf{Insight 4:} The first-coding, then-running paradigm are generally not scalable to many-shot case. Many-shot in-context inductive reasoning remains an open problem.
\end{minipage}}