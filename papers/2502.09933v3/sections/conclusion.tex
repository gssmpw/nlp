\section{Discussion and Conclusion}

In this paper, we propose MIR-Bench, a novel, large-scale many-shot in-context inductive reasoning benchmark and poses a difficult challenge for even the state-of-the-art LLMs. We test $15$ cutting-edge LLMs from $4$-shot to $2048$-shot on our benchmark, and conduct extensive ablations on many aspects such as CoT, robustness and coding paradigm in addressing inductive reasoning problems. With many important insights concluded from our experiments, we believe our work provides a unique way of understanding LLM's intelligence level under long-context scenario.

\textbf{Limitations and future works.} To curate MIR-Core with problems that requires many-shot ICL, we studied many related factors such as types of problem and difficulty of the problems; however, they are not decisive enough. A more explainable rule for determining whether a problem needs many-shot would be an interesting avenue for future many-shot ICL works. Also, our test of inductive reasoning is limited to text; 
it would be interesting for future work to explore the intelligence of multimodal models~\citep{sun2023aligning, man2024situational, choudhuri2024ow}.