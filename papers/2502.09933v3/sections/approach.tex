\section{MIR-Bench}

In this section, we will introduce our MIR-Bench in details, with Sec.~\ref{sec:formulation} discussing the formulation of the problems evaluated in our benchmark and Sec.~\ref{sec:construction} introducing the pipeline with which we build our benchmark.
%  Finally, in Sec.~\ref{sec:discr}, we will discuss our search for problems that poses unique challenge that requires many-shot through detailed ablation study on ...-Ex obtained from pipeline described in Sec.~\ref{sec:construction}.  

\subsection{Problem Formulation}
\label{sec:formulation}

The goal of the problems in our benchmark is for LLMs to predict the output for a new input given a list of examples. More specifically, assume we have an underlying function $y=f(x)$, where $x$ and $y$ can be arbitrary data. \footnote{In our implementation, $x$ is a dictionary with key being string (variable names) and values being arbitrary Python list, tuple, dictionary or/and number, while $y$ can be any Python list, tuple, dictionary or/and number.} 
% \footnote{In our implementation, $x$ is a dictionary with key being string (variable names) and values being arbitrary Python list, tuple, dictionary or/and number, while $y$ can be any Python list, tuple, dictionary or/and number.} 
Assume for $f$ we have a set of $n$ known example input-output pairs $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$, and a new input $x_{\text{new}}$; then, the LLMs' input will be $[c_1, \text{str}(x_1), \text{str}(y_1), \text{str}(x_2), \text{str}(y_2), \dots, \text{str}(x_n), \text{str}(y_n), c_2, x_{\text{new}}]$, where $[\cdot, \cdot, \dots, \cdot]$ is a string concatenation, $c_1$ and $c_2$ are general context prompts (e.g. ``You are an expert in reasoning'', or ``Here is the target input''; see Appendix~\ref{sec:prompt} for details), and $\text{str}(\cdot)$ is the string representation plus an ``Input: '' prefix for $x$ and ``Output: '' prefix for $y$. LLMs can output arbitrary rationale; however, they must end their answer with $\text{str}(y_{\text{new}})$, where $y_{\text{new}}=f(x_{\text{new}})$. The answer is extracted with rule-based scripts, and exact match will be performed to determine the LLM's performance in accuracy. See Appendix~\ref{sec:extract} for details on answer extraction.

\subsection{Benchmark Construction}
\label{sec:construction}

The construction of our benchmark can be decomposed into four steps: function collection, input generation, output generation, and prompt building.

\textbf{Function collection.} We begin by collecting introductory-level coding problems from three coding benchmarks: Humaneval+~\citep{evalplus}, MBPP+~\citep{evalplus}, and APPS~\citep{hendrycks2021measuring}. We use the whole Humaneval+ and MBPP+ dataset ($164$ and $378$ problems respectively); for APPS dataset, we select problems from its training dataset with difficulty level ``introductory'' ($2640$ problems). We ensure that each solution code is a single function without wrapping solution class or test statement; for codes in APPS that do not conform to this standard, we ask GPT-4o-0806 to rewrite the code given problem input and the solution code (See Appendix~\ref{sec:rewrite} for prompts). 

% We also explore the possibility of automatically generating functions using LLMs; however, we find that such generated functions usually lack diversity (See~\ref{sec:llmgen} for examples).

\textbf{Input generation.} We use GPT-4o-0806 to automatically generate inputs for each function acquired in the last step, for which prior works~\citep{shao2024case2code, li2024programming} usually directly generate input data. However, such method is not only non-scalable, but also prone to errors such as input format mismatch. To address this issue, we prompt GPT-4o-0806 to first generate ``data generators'' for each problem (See Appendix~\ref{sec:prompt_datagen} for prompts), then run each generator in Python interpreter to generate data. We generate $20000$ shots and $10$ test cases for each problem, which is impossible to acquire with prior methods. We wrote the prompt such that the test case is supposed to be slightly harder (e.g. with larger numbers / longer lists) than the shots. In this step, we filter out problems with the generated input too identical ($\leq 4096$ different shots out of $20000$), duplicate test cases, or test cases appearing in the shots.

\textbf{Output generation.} With input generated, we write a script to stitch generated input and ground truth function $f$ in the same Python script, and run them in the intepreter to acquire ground-truth output. In this step, we filter out problems with floating number output, unless the precision is fixed across all shots by rounding, given by input, or unimportant for exact matching (e.g. the function is to output absolute value). We also filter out problems with too low output diversity (no less than $50\%$ of the shots having the same answer), and problems with invalid output due to code error.

\textbf{Prompt building.} In this step, we use Python scripts to automatically stitch input-output pairs with task description to generate final input for LLMs. Finally, we also filter out problems that are unsolvable (either too difficult or data coverage are insufficient) for current state-of-the-art LLMs, which are the problems that have $0$ accuracy for all five models $\{$GPT-4o-0806, GPT-4o-mini-0718, Gemini-Pro 1.5-002, Gemini-Flash 1.5-002, Mistral-Large-2$\}$ across $\{4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\}$ shots in $10$ test cases. After this step, we have $693$ valid functions, each with $10$ test cases; these problems are the content of our benchmark version MIR-Extended. Within this version, we select $300$ problems that are challenging and can largely benefit from many-shot; See Sec.~\ref{sec:discr} for details. 





