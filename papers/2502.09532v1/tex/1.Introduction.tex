\section{Introduction}


\revision{The advent of large language models (LLMs) has transformed human-AI co-writing \cite{Reza2023-hp}. Modern writing assistants such as Microsoft 365 Copilot, Grammarly, or Jasper leverage multilingual LLMs to support a global user base in drafting, editing, and rewriting content \cite{noy2023experimental,herbold2023large}. However, despite their widespread use, LLMs still differ substantially between languages \cite{Ahuja2023-zu,Zhang2024-mp,huang2023not,bang2023multitask,joshi2020state,jiao2023chatgpt}. While English is the default language and therefore exhibits robust performance, lower resource languages often display deficiencies in fluency, coherence, sensitivity to jailbreaks, and contextual appropriateness \cite{conde2024opensourceconversationalllms, yong2023low, hasan2024largelanguagemodelsspeak, lawalcontextual}.}


\revision{These performance disparities in multilingual LLMs can have significant \textbf{behavioral implications}. Users interacting with LLMs in multilingual settings may not only be influenced by technical limitations but also by their perceptions and experiences from being exposed to heterogeneous output quality. Consider a bilingual journalist writing news articles in both English and Spanish. When using an LLM assistant, they notice it struggles with Spanish idioms and cultural references, requiring substantial editing. A rational writer would evaluate each language independently, maximizing the LLM's benefits in English while being more selective in Spanish. Yet, recent evidence has shown that humans use prior experience with LLMs to predict future performance \cite{rambachan2024llms} and, in particular, often violate the independence axiom of rational choice theory by generalizing AI errors to objectively unrelated tasks \cite{erlei2024understanding}. In the example above, this may cause the journalist to avoid the LLM for English articles due to a faulty generalization of their Spanish experience. Similar arguments apply to all regions or domains where linguistic diversity is high, such as customer support, technical documentation, or professional writing. Understanding how users react to heterogeneity in these scenarios is important, particularly given the current trend where LLM assistants are incrementally rolled out in multiple languages to meet global demands \cite{bensum2024languages,meta2024ai,venkatachary2024ai}.}

\revision{Importantly, there are good reasons to doubt that these behavioral patterns would automatically resolve solely through experience and learning-by-doing. First, human-AI interaction research highlights the critical role of \textbf{first impressions} and initial exposures for user behaviour and trust \cite{schwartz2023enhancing,nourani2020investigating,glikson2020human,tolmeijer2021second}. Even with repeated zero-stakes interactions, individuals frequently fail to optimally calibrate their reliance on AI systems after observing initial errors, exhibiting persistent downward bias in utilization patterns. In real-world settings, learning about LLM capabilities is costly, which can further deter users from updating their beliefs. Second, it is unclear whether choice independence violations stem from biased information updating or a biased choice rule. While the latter may improve over time, biased information updating inhibits learning and hence perpetuates inefficient under-utilization.}



In our study,\footnote{The codebase and datasets are openly available: https://sites.google.com/view/cillm-mind-the-gap} we examine how exposure to two different languages affects people's interactions with an LLM-based writing assistant in the context of persuasive writing. We focus on English and Spanish, two high-resource languages that allow us to commission multiple advertisements from writers and subsequently test them in a charitable giving task. Participants in the first experiment wrote two advertisements for the charity World Wildlife Fund (WWF), one in English and one in Spanish, with the explicitly incentivized assignment to make it as persuasive as possible. By varying the order of the language, we measure how exposure to the lower (higher) resource language Spanish (English) affects subsequent interaction with our LLM-based co-writing tool in the higher (lower) resource language English (Spanish). For comparison, we also ask native English and Spanish speakers to write advertisements without any AI assistance. The second experiment then evaluates the generated ads via a charitable giving task in which participants split an endowment of \pounds1.5 between themselves and the WWF after reading a treatment-dependent advertisement. In addition to the ads from the first experiment, we also evaluate advertisements that are purely derived from LLMs, as well as the WWF mission statement. Finally, we elicit donors' beliefs about the origin of the advertisement (human versus AI). Through this empirical setup, 
we address the following four research questions:

\begin{framed}
\begin{itemize}
    \item [\textbf{RQ 1:}] How does LLM performance in one language affect user utilization in a second language for persuasive co-writing?

    \item [\textbf{RQ 2:}] How do varying levels of LLM utilization in co-writing tasks across languages influence the persuasiveness of generated advertisements?

    \item [\textbf{RQ 3:}]
    How does altruism persuasiveness differ between human writers, human-LLM teams, and LLMs?

    \item [\textbf{RQ 4:}] How do donor beliefs about the source of an advertisement affect altruistic behaviour?
    
\end{itemize}
\end{framed}

Our results confirm that writers violate choice independence by generalizing their experience with one language to a second language and adapting their behaviour accordingly. In particular, being first exposed to the lower-resource Spanish AI assistant reduces subsequent reliance on the English LLM. There is also moderate evidence that writers who first experience the English LLM may use the Spanish writing assistant more in a follow-up task. Experiment 2 shows that these differences do not affect the persuasiveness of the generated advertisements. We find that donations are very stable across conditions, suggesting that participants' altruistic donation preferences are largely independent of the kind of advertisement we utilize in this study. However, we found moderate evidence that sole human writers may be less effective in eliciting donations, which is absent from human-LLM teams. Finally, participants cannot reliably identify whether an advertisement was generated by a human or an LLM, but may still condition their donation behavior on these beliefs. Here, we document strong cultural and gender effects, as female Spanish-speaking participants who believe that they are reading an AI-generated ad (1) donate substantially less and (2) are much more likely to not donate at all. For all other groups, results qualitatively point in the same direction, but are much smaller and not statistically significant. In combination with the fact that female participants are significantly more likely to donate, and donate more, human reactions to persuasive text that is perceived to be generated by LLMs are likely highly context-dependent. 


In general, our results have strong implications for the understanding, design and deployment of multilingual AI assistants. Following prior abstract work on user violations of choice independence, this is the first study replicating these results in an applied context, highlighting the need for theory to consider how humans systematically deviate from rational choice theory when exposed to AI output with heterogeneous quality across tasks. In particular, users do not appear to evaluate LLMs tasks independently, but holistically, even though the latter approach leads to faulty perceptions. Second, for practitioners, it is important to consider potential unintended second-order effects when deploying multilingual systems that vary significantly in quality. Those who utilize features in more than one language may adjust utilization downwards, decreasing demand. Employers themselves should also be mindful of how these patterns might shape employee performance, and adjust accordingly. From a societal perspective, there are two immediate implications. One, the adoption of LLMs may lag in non-English-speaking countries (particularly for those with low-resource native languages), and two, choice independence violations may increase inequity between countries with high- and low-resource languages. As high-resource languages are usually positively related to the native country's economic position, this implies potentially lower uptake in less wealthy countries, or from immigrated employees within wealthy countries, plausibly increasing inequality \cite{noy2023experimental}. Here, it is important to note that our results should provide a lower baseline for real-world impact, as Spanish is generally considered one of the highest resource languages. \revision{While it is possible that starker differences also make it easier for users to categorically distinguish between languages and thereby increase rational behavior, prior work suggests that users violate choice independence in the context of AI even after observing two distinct error graphs for two distinct problems \cite{erlei2024understanding}. This suggests that differentiation alone does not alleviate biased generalizations.} Finally, our results about donation behaviour as a response to beliefs about LLM-generated advertisements echo recent work whereby humans tend to prefer engagement with other humans and react adversely to AI or algorithm-generated output \cite{zhang2023human,bellaiche2023humans,millet2023defending,erlei2022s}, \revision{while also being unable to adequately identify it \cite{kobis2021artificial}.} Practitioners aiming to elicit donations may benefit strongly from \revision{reducing uncertainty around the use of AI in the context of charitable giving, e.g., through promoting the salience of humans in the context of persuasion or advertising.}