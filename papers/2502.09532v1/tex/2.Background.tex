
\section{Background and Related Work}
Our study integrates concepts from multilingual large language models, charitable giving, (persuasive) human-AI co-writing, expected utility theory, and human perceptions of AI-generated content. We present related literature in these realms and we position our work and contributions at their confluence.




\subsection{Multilingual LLMs}
Large Language Models have demonstrated strong multilingual capabilities across numerous tasks \cite{zhao2023survey,bang2023multitask,le2023bloom}, which has led to their widespread proliferation across various countries \cite{Kaddour_2023}. Many multinational companies rely on their translations to accelerate cross-national team cooperation, and individual workers across the globe benefit from individualized LLM assistance that enhances their productivity -- both in their native and the English language. Yet, despite their immense promise, LLMs still differ substantially across languages \cite{Ahuja2023-zu,Zhang2024-mp,huang2023not,bang2023multitask,joshi2020state,jiao2023chatgpt,hada2024akal}. LLMs not only show poorer text generation and problem-solving performance for low-resource languages, but also heightened security vulnerabilities, safety challenges, and tokenizer biases \cite{Shen_2024,ahia2023languagescostsametokenization,Ahuja2023-zu}. 




These caveats are not strictly limited to languages conventionally considered low-resource. Multiple open-source LLMs are disproportionately trained on English data. For example, PaLM2's training corpus consists of 78.99\% English data, compared to only 2.11\% for Spanish data \cite{chowdhery2023palm}, and LLAMA-2 exhibits a mere 0.11\% Spanish representation \cite{touvron2023llama}. \revision{Despite these gaps, Spanish is generally considered a relatively high-resource language. The GPT-4 technical report \cite{achiam2023gpt} shows benchmark accuracies of 85.5\% and 84\% for English and Spanish respectively in the multilingual version of the MMLU \cite{hendrycks2021ethics}. Spanish also performs relatively well in the QWEN2 technical report for professional annotator tasks \cite{yang2024qwen2}.}


\revision{The practical performance of multilingual LLMs in the Spanish language, however, is often relatively poor, especially in contextual usage and practical applications \cite{jin2024better, conde2024opensourceconversationalllms, zhang2024dolares}. A particularly striking finding is highlighted by \citet{conde2024opensourceconversationalllms}, who show that most open-source LLMs exhibit significant comprehension deficiencies for the Spanish vocabulary. Two-thirds of the models, including the Llama-2 series (a predecessor of the Llama 3.1 model used in our experiments), fail to provide valid definitions for more than 50\% of tested Spanish words. Moreover, when evaluated for contextual word usage, most models fall below 10\%. For instance, the Llama-2-7b model correctly defines only 42 out of 100 words and uses just 3 out of 100 words correctly in context. Importantly, these failures are not confined to low-frequency words; even highly frequent words like "minuto" fail in meaning across 8 out of 12 evaluated models. While their work highlights linguistic limitations, our research extends this by exploring the behavioural and real-world impacts of such deficiencies.}
Particularly in co-creation environments that span linguistic and cultural boundaries, varying performance levels can disrupt the collaborative process, leading to asymmetric misunderstandings of system capabilities and thereby in inappropriate reliance that hurts efficiency \cite{Dhillon2024-ol}. This may be particularly problematic in sensitive domains such as persuasive writing, which depend on subtle combinations of tangible and non-tangible elements, like emotional appeals and accurate fact specificities \cite{Hibbert2007-ty,Choi2020-eh,Wymer2023-pg,Dafouz-Milne2008-iy}. 
\revision{However, the ground reality is that multilingual LLMs are largely being deployed and used immaterial of their performance in specific languages. And while companies often do evaluate their models across languages, traditional technical benchmarks may not capture the full picture due to downstream consequences for user behaviour -- even when performance across languages appears comparable. Inspired by this real-world context, we explore the utilization of multilingual LLMs in a co-writing task and study how exposure effects with LLMs in different languages influence user interaction and behavior.}
  

\subsection{Charitable Persuasive Writing}

Persuasive writing is a form of communication that seeks to convince the reader to adopt a particular viewpoint or take a specific action \cite{jonsen2018convincing}. It can be viewed through the lens of sender-receiver games, a concept generated from economic theory frequently applied in computer science across domains such as recommender systems \cite{Apel2020-tt}, reinforcement learning \cite{hiraoka2014reinforcement}, multi-agent interaction \cite{meta2022human}, and most recently LLMs \cite{Shin_Kim_2024}.

Advertisers often leverage this sender-receiver model (e.g., by employing relevant recommender systems) to influence a receiverâ€™s behaviour by appealing to emotions or raising awareness, and charitable advertisement writing is no exception \cite{salvi2024conversational, furumai2024zero,Murphy2001-yn}.

With the rise of LLMs, the traditional dynamic of a human agent influencing a human receiver has evolved. Now, LLM agents can serve as persuasive entities, sometimes more efficiently and effectively than humans \cite{Breum2024-xf, Matz2024-pl, Zeng2024-lp, Xu2023-bs, Goldstein2023-wr,durmus2024measuring}. LLM-generated text has been shown to influence political attitudes \cite{voelkel2023artificial}, vaccine uptake \cite{karinshak2023working}, strategic negotiations \cite{meta2022human}, personal beliefs \cite{salvi2024conversational}, or even romantic conversations \cite{zhou2020design}. However, LLMs also exhibit many limitations -- such as hallucination, a lack of contextual knowledge and wordiness \cite{Myers_2024}--- prompting a shift towards co-creation where human and AI agents collaborate as persuasive agents to ensure both effectiveness and reliability \cite{Dhillon2024-ol, zhang2023human}. Specifically in the context of altruistic social preferences such as charitable giving, little is known about the effectiveness of LLMs in eliciting donations. Even the psychological literature is fragmented, lacking a coherent model about which factors specifically increase the effectiveness of charity advertisement \cite{xu2020relative,saeri2023works,schamp2023effectiveness,fan2020factors}. This makes charitable giving not only a novel but potentially high-value field of application for persuasive LLMs. We, therefore, consider the task of charitable persuasive writing as a lens to study user behavior with multilingual LLMs in this paper.




\subsection{LLM Augmented Co-writing}
There has been considerable interest from the HCI community in analyzing and fostering collaborative human-AI writing. Many popular real-world applications like Microsoft Word and Gmail already utilize smart features that, e.g., predict the next words a user is likely to write or provide context-dependent phrasing advice (auto-completion suggestions). LLMs themselves have demonstrated exceptional performance in open-ended writing, with great potential benefit for a wide variety of tasks \cite{lee2022coauthor,macneil2022automatically,meyer2022we,mirowski2023co,yuan2022wordcraft,zhao2023more,herbold2023ai}. 



Beyond evaluating the output of foundational LLMs, the HCI community has begun to create tools designed to support human writers. \citet{Kim2023-wn} introduce a framework that augments ``object-oriented'' interaction with LLMs. Their approach enables end-users to ``track, compare, and combine'' configurations, fostering inspiration and creativity in the design process. Based on this framework, there has been an ongoing development of novel and innovative systems, focusing on integral parts of the writing process such as non-linearity, rewriting or idea-generation \cite{Reza2023-hp,lu2024corporate, teufelberger2024llm}.

In this paper, we utilize and slightly modify ABScribe, a tool created by \citet{Reza2023-hp}, which allows users to ``track, compare, and modify variations" while interacting directly with an LLM system to generate new ideas during the writing process. The system is purposefully designed to allow for a seamless writing flow by providing users with flexibility and autonomy throughout the co-creating process. Through different functions that go beyond mere text generation, it provides users with a more targeted approach to exploit the various use cases of LLMs. 

The integration of LLMs into the writing process has the potential to transform how content is created across various fields. However, the effectiveness of LLM-augmented co-writing depends on several factors, including the quality of the LLM, the nature of the task, the expertise of the human writer, the interaction of the human writer with the LLM, and the perceptions of consumers towards LLMs. In this paper, we focus on the latter two aspects, as both appropriate reliance and consumer attitudes have been previously shown to be important facets in human-AI interaction  \cite{He_2023,sara_23,schemmer2023appropriate,erlei2024understanding,wester2024exploring,grassini2024understanding}.


\subsection{Choice Independence and Human-AI Interaction}
Our study focuses on choice independence as a particularly important aspect of human-AI interaction.\footnote{The von Neumann-Morgenstern's utility theory (or expected utility theory), provides a key mechanism for understanding the behaviour of a rational agent under uncertainty. In this framework, a rational agent makes decisions that maximize the subjective value of their utility when faced with stochastic outcomes \cite{vnm-original}. The theory is built upon four main axioms: \textit{completeness}, \textit{transitivity}, \textit{independence}, and \textit{continuity}. Among these, the independence axiom is the most contentious and has significant implications for rational decision-making \cite{Holt1986-jp}. 
Mathematically, this axiom is represented as follows,
\[
X \succ Y \implies pX + (1-p)Z \succ pY + (1-p)Z \quad \text{for} \quad 0 < p \leq 1
\]

Here, $X$, $Y$, and $Z$ represent lotteries, which can be thought of as stochastic processes or uncertain events that yield probability distributions over a set of outcomes. If amongst the lotteries $X$ and $Y$ a rational agent is said to prefer
lottery $X$ over $Y$ their preference should remain unchanged even if an irrelevant lottery is introduced and mixed with both $X$ and $Y$ in equal proportion. This axiom asserts the stability of preferences and is considered a cornerstone of rationality in decision theory.} 
In the context of multilingual LLMs, it postulates that users who experience two or more languages should evaluate them independently, adjusting their usage according to their language-specific experiences. Choice independence is a foundational axiom of expected utility theory \cite{vnm-original}, and often implicitly assumed when deploying novel technologies, systems and products \cite{erlei2024understanding,Ethayarajh2022-gj}. We argue that this is one of the reasons why companies tend to simultaneously deploy their AI assistants globally, despite the documented differences in performance across languages. The HCI literature has only recently begun to empirically scrutinize its applicability in the context of algorithmic and AI systems. \cite{pareek2024trust,erlei2024understanding}.



\revision{So far, evidence from HCI work is constrained to a limited number of abstract %laboratory-like 
decision tasks. For example, \citet{erlei2024understanding} uses an online experiment to explore how humans delegate decisions to a superior AI system across two independent abstract prediction tasks. They manipulate the performance of the AI system such that participants either observe an AI system that provides the best-possible prediction in both tasks or one that exhibits a systematic error in one of the two. Results show that the induced error in one task significantly reduces trust in and delegation to the AI system even for the second prediction, indicating that participants erroneously generalize AI errors across tasks, violating choice independence and undermining appropriate reliance. Similarly, \citet{pareek2024trust} conducted an online experiment to study trust dynamics in the context of complementary Human-AI expertise. Participants engage in classification tasks involving familiar (High Human Expertise, HHE) and unfamiliar (Low Human Expertise, LHE) stimuli. The paper shows that people calibrate trust in the AI for LHE tasks based on its performance in HHE tasks, demonstrating a spillover of trust judgments across tasks. While these studies are informative, abstract decision tasks are not only specifically designed to artificially test a specific hypothesis while controlling for the entire context (e.g., expertise, task), but also benefit from very focused attention towards the specific problem a researcher is interested in. Real-world decisions are often more complicated, limiting the extent to which certain laboratory results can be generalized or scaled \cite{list2022voltage,brandon2022human,sara_23,salimzadeh2024dealing}. For example, in \citet{erlei2024understanding}, AI errors are precisely quantifiable and codified, effectively alleviating any participant uncertainty about model performance, and facilitating easy comparisons of the AI's performance across tasks. In real-world settings, many people learn by updating their beliefs solely through experience and noisy feedback, while always being uncertain about the model's ``true'' performance.
Therefore, in this paper, we extend the analysis of human behaviour in the context of heterogeneous AI output across distinct tasks to the applied context of human-AI co-writing}. Multilingual LLMs represent a prime example to test whether humans tend to rationally learn about and evaluate LLMs, because (1) multilingual writing is everywhere, (2) writing and text generation belong to the most common use-cases of LLMs, (3) writing is a complex and non-linear activity for which humans possess intimate familiarity and expertise, and (4) producing persuasive text in one language is distinct from the problem of producing persuasive text in a second language.


\subsection{Human Attitudes Towards AI Generated Content} There has been a lot of 
interdisciplinary literature that has analyzed how humans react to AI-generated output while articulating the notions of algorithmic affinity and aversion~\cite{dietvorst2015algorithm,dietvorst2018overcoming}. Within the scope of this paper, we are primarily interested in textual or persuasive content. In addition, eliciting donations through advertisements is closely related to negotiation scenarios. Recent studies provide mixed results on human perceptions of AI-generated content. In \citet{lim2024effect}, disclosing AI as the source of communication negatively impacts human perceptions of messages. People may prefer AI advertisements depending on which kind of appeal is made \cite{chen2024consumer}, but can react negatively towards AI use by charities \cite{arango2023consumer} and generally appear to denigrate creators who transparently use AI \cite{rae2024effects,bruns2024you}. Other research finds positive effects of revealing the use of AI technology in the context of influencing and persuasion \cite{wang2024positive} and no creator loss in credibility \cite{huschens2023you}. In general, the literature documents several divergent effects, currently lacking a parsimonious explanation \cite{ferraro2024paradoxes}. Interestingly, people often appear unable to identify AI-generated content \cite{clark2021all} and only reveal preferences against it upon disclosure \cite{kobis2021artificial}, possibly due to inherent pro-human attitudes \cite{grassini2024understanding,zhang2023human}. In bargaining and negotiations, humans also tend to exhibit preferences for other humans \cite{erlei2022s}, and behave more self-interested \cite{erlei2020impact,shen2024bargaining,chugunova2022we,von2023social}. We add to this existing bed of literature by examining how peoples' beliefs about the origin of a charitable advertisement affect donation behaviour, and whether these beliefs correlate with true LLM usage.
