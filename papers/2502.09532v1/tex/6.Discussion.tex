\section{Discussion}
In this paper, we analyze how exposure to two different high-resource languages affects writers' reliance on LLM-generated persuasive advertising content and the accompanying downstream effects on charitable giving. We are the first to show how violations of rational choice can lead to unexpected negative consequences for human-AI reliance in an applied real-world task of persuasive co-writing. Furthermore, we show that humans can exhibit different altruistic preferences conditional on unreliable beliefs about the involvement of AI in charitable advertisements. In light of the ubiquity of multilingual LLMs throughout many organizations and socio-technical ecosystems \cite{wang2023mathcoder, luo2024integration, weber2024large, arora2024analyzing}, it is imperative to understand how humans interact with and react to heterogeneous AI output. While the current study focuses on a very specific writing task, we argue that the implications of our results are much broader. Generalizing prior laboratory results to a very complicated and nuanced real-life application further strengthens the relevance of more general HCI work on the unreliability of human judgments in the context of heterogeneous task performance. If these abstract behavioural patterns can be replicated in something as subjective as persuasive writing, they are likely to be relevant across many real-world decision applications. 
Notably, from the consumer side, we also add to the literature that finds potential detrimental effects of \revision{deploying} AI systems in social domains that have been traditionally characterized by strong exposure to human-human interactions \cite{lim2024effect, grassini2024understanding, shank2023ai}. \revision{In our case, this relates to consumer uncertainty surrounding the use of AI, rather than preferences towards AI systems or their output per se, as we show that incorrect beliefs about AI involvement can influence altruistic behavior.}




\vspace{.5em}
\noindent
\textit{RQ 1: How does LLM performance in one language affect user utilization in a second language for persuasive co-writing?}
\vspace{.5em}


\noindent
Human writers appear to condition their reliance on an LLM-based writing assistant on prior experience in a different language. In particular, exposing participants to a non-English high-resource language reduces subsequent utilization of an English LLM. When writers first interact with the English benchmark model, subsequent utilization of and reliance on the Spanish assistant appear to increase, albeit to a lesser extent. Hence, results point to an ``irrational'' violation of choice independence. \revision{This effect is consistent with previous research on choice independence violations in human-AI interaction - \citet{erlei2024understanding}, who found that users generalized errors of AI systems across different tasks, leading to decreased trust and reliance even when the AI made the best-possible prediction or \citet{pareek2024trust}, who observed that trust in AI systems could spill over across tasks with different expertise requirements. Our findings extend these insights to the domain of multilingual LLM-assisted co-writing, demonstrating that users may inappropriately generalize their experiences with an AI assistant in one language to their expectations and reliance in another language. This suggests that violations of choice independence are not confined to abstract tasks but also manifest in complex, real-life applications such as persuasive writing.}


\revision{Moreover, our study contributes to the literature on human AI co-writing by highlighting how second-order effects can influence LLM utilization. While previous research has explored how AI assistants enhance creativity and productivity in writing tasks \cite{Kim2023-wn, Reza2023-hp}, our findings indicate that negative experiences in one context (e.g., a different language) can hinder the effective use of AI tools in other contexts. This underscores the importance of considering user experiences holistically when designing and deploying AI-assisted writing tools in multilingual contexts \cite{bensum2024languages,meta2024ai,venkatachary2024ai}.}

\vspace{.5em}
\noindent
\textit{RQ 2: How do varying levels of LLM utilization in co-writing tasks across languages influence the persuasiveness
of generated advertisements?}
\vspace{.5em}

\noindent We find no consistent effect of LLM utilization in Experiment 1 on social persuasiveness in the context of charitable giving. \revision{This result contributes to a growing but mixed literature on AI-generated persuasive content. While some studies demonstrate the persuasiveness of LLMs, particularly chatbots, in, e.g., political or health messaging \cite{goldstein2023can,karinshak2023working,voelkel2023artificial}, others document significant limitations across different (strategic) contexts with substantial variance in effectiveness \cite{durmus2024measuring,breum2024persuasive,furumai2024zero,voelkel2023artificial}. Our results suggest that, specifically in charitable giving advertisements, LLMs do not significantly affect persuasive content effectiveness -- whether used alone or in collaboration with human writers.}



\vspace{.5em}
\noindent
\textit{RQ 3: How does altruism persuasiveness differ between human writers, human-LLM teams, and LLMs?} 
\vspace{.5em}

We found minor evidence that English human writers without an assistant may be less effective in eliciting donations than human-LLM teams or LLMs alone, but in general, donation behaviour appears largely unaffected by charitable advertisements.


\vspace{.5em}
\noindent
\textit{RQ 4: How do donor beliefs about the source of an advertisement affect altruistic behaviour?} 
\vspace{.5em}

\noindent
First, participants are generally unable to correctly differentiate between human-generated and AI-generated text, and there is no relationship between subject beliefs and actual LLM involvement. Second, despite being by and large uninformative, donors tend to condition their behaviour on these beliefs. These patterns are qualitatively present across the demographics of our subject pool, but primarily driven by Spanish-speaking female participants. For this subset, beliefs in AI-generated content substantially increase the share of people who act fully selfish, and strongly decrease absolute donations. This points to important cultural and gender effects in the evaluation of AI-generated charitable advertisement content. As female participants exhibit much stronger altruistic preferences, they may also react more elastically towards non-human involvement in charity work. \revision{This aligns with prior research demonstrating the significant role of gender in shaping perceptions of AI-generated content, particularly the heightened sensitivity of female participants \cite{zou2024pilot,zhang2023human}, and their concurrent stronger human preferences \cite{bellaiche2023humans}. Regarding potential cultural differences in the context of generative AI, the existing literature is scarce. Moreover, because we do not causally manipulate any cultural variables, restrict our analysis to a US-American sample, and do not observe specific cultural differences between our participants, our study does not provide any conclusions about the \textit{reasons} behind the difference between English and Spanish speaking donors. Instead, our results point to the existence of unobserved cultural endogeneity that may shape how people react to AI output. This relates, for example, to recent evidence that Western countries tend to be more critical and view generative AI as less aligned than Eastern countries \cite{globig2024perceived,arora2024pessimism}, or that European Americans are less likely to prefer AI with capabilities to influence, while exhibiting stronger preferences for control, than Chinese respondents \cite{ge2024culture}. In contrast, our study targets heterogeneity within the US-American society on a linguistic basis and therefore suggests that some cultural variables may influence AI perceptions on a more granular level. Finally, our findings contribute to a growing body of literature indicating that negative perceptions of AI-generated content are not necessarily merit-based, but often influenced by the belief that it was created by AI \cite{Harasta2024-jb,lim2024effect,shank2023ai}}.

\subsection{Implications}
As argued throughout this paper, our results relate to several interesting implications for different stakeholders. First, companies or industries that roll out LLM-based systems to different countries may want to consider \revision{evaluating the quality of their services post language add-on deployment, especially from a user behavioural perspective.}

Otherwise, exposure to mistakes in a particular native language may have broader consequences for the dissemination and adoption of their products. \revision{Especially those deploying multilingual LLM systems and benchmarking their performance in isolated, language-specific tests may overlook the cumulative impact of performance disparities across languages on user behaviour. For example, rolling out an LLM-based writing assistant without addressing cross-linguistic performance gaps could lead to disengagement among multilingual users, particularly in regions and domains where linguistic diversity and low(er)-resource languages are prevalent. This not only undermines the toolâ€™s utility and subdues engagement metrics but also potentially exacerbates global inequities in AI adoption, as lower-resource language users tend to be disproportionately disadvantaged. From an efficiency standpoint, these patterns may inadvertently lead to substantial losses in productivity gains, as LLMs have been shown to be of particular use for relatively low-performing users \cite{vuculescu2024leveling,noy2023experimental,doshi2023generative}.} This highlights a potential downside of strategically deploying AI models ``early'' based only on benchmark data \cite{zhang2024dolares, jin2024better, conde2024opensourceconversationalllms}.

Beyond the suppliers, business consumers should also be aware that their employees who work in more than one language may be prone to under-utilization of these productive tools, and take respective countermeasures. We believe that these implications go beyond multilingual models, and extend towards a wide variety of AI and LLM models that are being used across various tasks. Hence, the integration of modern LLM systems into an organization's framework should systematically consider a model of human behaviour that considers strategic deviations from rationality and anticipates how exposure to heterogeneous stimuli affects decision-making.

Regarding responsible AI practices, it may be beneficial to explicitly consider user reactions towards performance shifts in the design of AI assistants. Beyond traditional questions about e.g., how to reliably communicate uncertainty or the sensitivity of certain contents, responsible design could think about alerting users to potential shifts in performance. While these may alert users who are otherwise unaware, they could also prevent negative second-order effects by endowing users with a reasonable interpretation of the nature of the performance shift. Furthermore, they can be used to clearly distinguish between different use cases, e.g., text generation between languages, and thereby potentially avoid detrimental generalization patterns. Practitioners themselves are likely to bear large parts of the costs induced by violations of choice independence, as their productivity suffers. Hence, being aware of such biases and communicating them could endow workers with the necessary tools to change their decision-making.

For developers, traditional software evaluation methods, such as unit testing, integration testing and system testing have been the fundamentals of system validation \cite{jamil2016software}. However, as AI systems become increasingly integrated with software, the focus of evaluation may need to shift. While the standard practice regarding AI evaluation has been to benchmark these ``black box" systems using extensive datasets across a range of scenarios, the practical utilization of AI-driven tools often hinges on user perceptions -- which can be heavily influenced by their rationality. The first stage of our study highlights the influence of such lapses in rational behaviour through a reduction in usage and interaction with the underlying system. By introducing frameworks from rational choice theory, particularly through the lens of choice independence, developers can gain valuable insights into system utilization. This approach not only helps improve feature usage but also fosters innovation by aligning system performance with user expectations.

For societies, there are implications within and between different countries. Within societies, there may be a divergence in productivity between different workers, depending on which languages they are being exposed to. For example, while native German workers in Germany have access to two high-resource languages, immigrants from some Eastern European countries could learn about the usefulness of LLMs through experiences in relatively lower-resource languages, with subsequent effects on everyday productivity. More generally, LLMs have been shown to provide large marginal benefits to relatively low-skilled workers. In so far as lower skills are correlated with certain marginalized languages, it may inhibit the anticipated reduction in inequality. Between societies, inequalities may also increase, as non-English countries in general, and smaller countries with less representation in particular, are endowed with lower quality LLMs that affect user beliefs beyond what would be rational.




\subsection{Caveats, Limitations, and Future Work}
\textbf{LLM and Tool Selection.} This study relies on a single LLM-augmented co-writing tool and an underlying LLM system. Both are not specifically optimized for persuasive writing, and results may differ for future tools that are more refined. In addition, while the pre-defined recipes are informed by the literature on charitable giving, there is still much to learn about what persuades people in the social preference domain, limiting what guidance we could offer participants. Consequently, recipes were not particularly popular, and it is plausible that LLMs with more refined features may provide more utility.

\noindent
\textbf{Writers.} Due to selection and availability constraints related to English-Spanish multilingual workers on Prolific, the number of writers in our first experiment was limited.
To make this study possible, we also had to define ``writer" more broadly as someone working in a writing-related profession. Future research can consider a larger and more specialized subject pool and control for writing expertise. 

\noindent
\textbf{Writing Task.} We commission short advertisements designed to elicit donations for an animal charity. This induces two limitations. One, due to the kind and size of the writing, we limit skill expression, which also constrains the influence of our AI assistant. Hence, under-utilization due to violations of choice independence may have substantially stronger effects in more sophisticated writing tasks. Two, we focus on persuasive writing in a social preference context. While this is a very important domain, it is also notably difficult to change peoples' social preferences. Future work may consider different applications, like sales, narrative text such as poems, or translations, for which we would expect a stronger influence of LLM-generated text on subsequent behaviour or judgments. It is also important to explore how multilingual LLMs shape user trust and reliance in increasingly popular agentic workflows that require complex planning and execution~\cite{he2025plan}.

\noindent
\textbf{Language Selection.} Due to availability constraints as described earlier, we compared two high-resource languages. Therefore, our results \revision{could} be interpreted as a lower baseline. It is plausible that writers who are being exposed to the LLM's performance in a true low-resource language are even more likely to generalize these experiences, resulting in larger behavioural changes. Hence, our study may well under-estimate the detrimental effect of between-language variation on more marginalized languages, and thereby the negative implications for inequality. \revision{On the other hand, more pronounced differences could increase users propensity to clearly differentiate between languages, and thus reduce cognitive interdependencies between AI performances across different tasks. These questions cannot be answered by our study.} Beyond that, behavioral patterns may also differ depending on the perceived similarity of the experienced languages. For example, people may be more likely to generalize errors from a Spanish LLM to an English LLM than to generalize errors from a Chinese LLM to an English LLM, as the latter two belong to different language families.

\noindent
\textbf{Prior Experience and Time Horizon.} We do not consider panel data. Therefore, we cannot draw any conclusions about how writers may adapt over time, whether a competitive market ``induces" more rational behaviour over time, or how prior experience with LLMs shapes the prevalence of choice independence violations. \revision{In particular, participants' reactions to heterogeneous stimuli across tasks may be mediated by their AI literacy levels, and the concurrent ability to contextualize, explain and maybe predict such differences. Importantly, knowledge about (multilingual) generative AI is not exogenous but can be affected through knowledge dissemination and policy. There may also be relevant differences across different societal or global groups.} These are all rich avenues for future research.

\noindent
\textbf{Cultural and Demographic Effects.} This study points to different reactions towards LLM-generated content between demographic groups. \revision{However, we are not able to distinguish the specific factors determining these differences. Future work could take a more targeted and controlled approach to tease-out specific causal factors behind different behavioral attitudes towards generative AI or their content across groups.} \revision{Moreover}, our work is limited to English and Spanish-speaking individuals who reside in the USA. Therefore, one natural extension is to expand the analysis towards people from different linguistic, cultural, and educational backgrounds. 

\noindent
\revision{\textbf{Cumulative vs. Sequential Effects and Temporal Impact.} Lastly, our study focuses on sequential second-order effects. However, the results may be influenced by participants' prior exposure to similar systems, particularly in Stage 1 experiments, where such pre-existing familiarity could shape observed behaviours.  Additionally, temporal effects, such as whether the observed usage behaviours persist beyond the study's duration, are beyond the scope of this research. Future work could explore these aspects to better understand the long-term implications of our findings.}