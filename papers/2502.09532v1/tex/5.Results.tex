\section{Results}

\subsection{Experiment 1: Persuasive Writing Task}
We commissioned \revision{96} human-submitted texts (48 English, \revision{48} Spanish) with an average word length of \revision{$M = 181.52$ (SD = 108.47)}. As shown in Figure~\ref{fig:stage1_word_count} in the Appendix, the ENG\_1 group exhibits the longest and most varied word lengths, $M = 243.44$ (SD = 166.55), compared to the ENG\_2 group, $M = 183.12$ (SD = 112.98). This pattern reverses for the Spanish advertisements: ESP\_1 has shorter texts, $M = 159.06$ (SD = 62.56), while ESP\_2 produces longer texts, $M = 194.06$ (SD = 70.81). These early signals suggest that exposure to LLM performance in English generally leads to more diverse outcomes. However, word length alone does not fully capture the utilization and utility of LLM systems.


\textbf{User Behavior.} We assess the revealed user utility through their usage of the text generation feature, the preference score (PS), and the weighted average content similarity. The LLM text generation feature \texttt{AI drafter} was by far the most popular of the writing assistance features that was used and is, in contrast to the recipes, not endogenously affected by the experimenter's framing choices. 


In line with our prediction, we find that writers who were previously exposed to the Spanish LLM (ENG\_2) are subsequently less likely to utilize the  \textit{AI Drafter} feature when writing an English advertisement ($t = 2.2, p = 0.04$), despite no changes to the underlying LLM. Compared to ENG\_1, the frequency of use drops by roughly 64\% from 28 to 10 (see Figure \ref{fig:ai_feature}). Similarly, the number of writers who try the text generation feature at least once also drops in ENG\_2, confirming that writers do not use the \texttt{AI Drafter} feature more in ENG\_1 because they are unsatisfied with the outcome.  
%
For the Spanish LLM, results are exactly reversed, such that prior exposure to the English LLM in ESP\_2 is associated with a subsequent increase in LLM-based text generation ($t = 2.58, p = 0.017$).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\linewidth]{fig/stage1_ai_usage_total_count.pdf}
    \includegraphics[width=0.45\linewidth]{fig/stage1_ai_people_count.pdf}
    

\caption{\revision{Effect of Initial Language Exposure on AI Drafter Usage by Task Group. \textbf{Left:} Total usage count of the AI drafter feature shows a significant "gap" between task groups based on initial language exposure. The group exposed to English first (ENG\_1, followed by ESP\_2) shows substantially higher usage compared to the group exposed to Spanish first (ESP\_1, followed by ENG\_2), as indicated by the significant differences marked with \textbf{*} (\( p < 0.05 \)) and \textbf{**} (\( p < 0.01 \)). The results highlight that initial exposure to English led to more engagement with the AI feature, whereas starting with Spanish resulted in notably lower engagement in both ESP\_1 and ENG\_2. \textbf{Right:} The number of unique users, out of a maximum of 16, similarly reflects this trend, with more users engaging with the feature in the ESP\_2 task after beginning with English.}}
    \label{fig:ai_feature}
\end{figure}


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\linewidth]{fig/stage1_content_similarity_weighted_bars.pdf}
     \Description{A graph showing differences in AI-generated content similarity with the final text across different conditions and embedding models.}
    \caption{\revision{Weighted Average Similarity Percentage Across Task Groups and Models. The similarity scores vary across task groups depending on the initial language exposure but pattern remains consistent across embedding models. \textbf{ENG\_1 and ESP\_2}, which involve starting with English, show higher similarity percentages across models compared to \textbf{ESP\_1 and ENG\_2}, where the initial exposure is in Spanish. This pattern suggests that starting with English may lead to more AI-written text in the final generated content, reflected by higher similarity scores. Each bar colour represents a different embedding model.}}
    \label{fig:revealed_utility_similarity}
\end{figure}

Extending that analysis across all features -- including the recipes -- shows heterogeneity in the preference score for feature utilization between treatments (shown in Figure~\ref{fig:revealed_utility_button_clicks_eng} in the Appendix). Note that these other features were much less popular in comparison to the \texttt{AI drafter}. Here, the effects from above hold for some, but not other recipes, without a clear pattern. More importantly, our constructed weighted average similarity measurement further points towards potential choice independence violations. Figure~\ref{fig:revealed_utility_similarity} shows that advertisements in the ENG\_1 condition exhibit a 14.7\% higher similarity to AI-generated text than those in ENG\_2. This trend remains consistent across different model specifications. In the Spanish tasks, the differences in AI-generated content similarity are virtually non-existent, with only a 1\% difference between ESP\_1 and ESP\_2. Hence, it appears that writers who were previously exposed to the Spanish LLM rely less on subsequent English AI-generated text, as similarity drops considerably.


\begin{itemize}
    \item [] \textbf{Result 1:} Consistent with violations of choice independence, writers who first experience a Spanish LLM are subsequently less likely to utilize and rely on LLM-generated text in English. For those who first experience the highest resource language English, results suggest either no or moderately positive effects on AI usage.
\end{itemize}    


We provide some auxiliary results about stated user preferences in Figure~\ref{fig:stated_utility} in the Appendix. These are difficult to interpret due to differences in actual user exposure but are generally in line with the aforementioned preference scores.


\subsection{Experiment 2: Quantifying Persuasion in Charitable Giving}


\begin{figure}[t!]
    \centering
   

    
    \includegraphics[width=0.45\linewidth]{fig/reb_stage2_donation_data_dist.pdf}
     \includegraphics[width=0.45\linewidth]{fig/reb_stage2_avg_donor_share.pdf}
      
     \Description{A bar graph showing the distribution of donations for texts generated across different experimental conditions.}
    \caption{\textbf{Left:} Average donations across treatments. \textbf{Right:} Average share of donors across treatments. Error bars represent 95\% confidence intervals.}
    \label{fig:stage2_donation}
\end{figure}

Experiment 1 finds evidence that prior exposure to a lower resource language negatively affects reliance on LLM-generated content in a persuasive writing task. This indicates a violation of choice independence, as users generalize lower performance in Spanish to the AI assistant's performance in English. Experiment 2 quantifies whether these patterns translate into the persuasiveness of generated text in the context of charitable giving. Furthermore, we introduce additional advertisements to make more general inferences about the efficacy of human writers and LLMs and examine how subject beliefs about an advertisement's source (Human vs. AI) affect altruistic donation behaviour. 

\textbf{Donation Behavior and Choice Independence.} Figure \ref{fig:stage2_donation} shows average donations and the share of donors across conditions. \revision{Due to potential social confounders, we largely focus on within-language comparisons.} Donations are mostly stable across the different conditions, and there is no treatment effect. In particular, there are no differences between \textbf{ENG\_1} and \textbf{ENG\_2}, or \textbf{ESP\_1} and \textbf{ESP\_2}. Hence, we find no evidence that changes in LLM utilization from Experiment 1 translate into persuasiveness. Because donations are relatively evenly distributed, advertisements, on average, do not appear especially useful in eliciting charitable donations. It follows that any choice independence violation cannot meaningfully affect donation outcomes. 

The only outlier is advertisements from human writers in the \textbf{No\_LLM} condition, which exhibit the highest share of purely selfish non-donating individuals (31.25\%), significantly more than the simple WWF mission statement in \textbf{Control} ($16.25\%, \, \tilde{\chi}^2 = 4.97, p = 0.026$). Therefore, while some ads may exhibit detrimental donation effects as compared to the charity's mission statement, these are not caused by LLMs, and differences in utilization due to prior exposure to another language are not large enough to meaningfully affect persuasion in our charitable giving task. 



\begin{itemize}
    \item [] \textbf{Result 2:} Average donations do not differ between treatments, providing any evidence for a detrimental effect of choice independence violations on persuasion in a downstream task.
\end{itemize}



\revision{Finally, we analyze whether subjects correctly identify the source of the advertisement, and how that belief affects donation behavior. In the conditions without any writing assistant or LLM usage, roughly 57\% of subjects think the advertisement was generated by an LLM. In contrast, only 54\% do so when an LLM is involved in the writing process. Even for ads that were solely generated by an LLM, 42\% believe that humans wrote the advertisement. Hence, subjects are generally not able to discern whether a text was generated by a human writer.}

\revision{Despite no relation between subject beliefs and true LLM involvement, donors significantly condition their behaviour on these perceptions (see Figure \ref{fig:adsource}). When participants believe that the advertisement is written by an LLM, pooled average donations are around 7 percentage points lower ($\, t = 1.61, p = 0.1$), and the pooled probability that an individual does not donate anything to the charity increases by almost 50\% ($\, \tilde{\chi}^2 = 5.8, p = 0.001$). Regression results (Tables \ref{tab:demograph_donation_amount} and \ref{tab:demograph_is_donor} in the Appendix) confirm the negative effect of AI beliefs on donating. As shown in Figure \ref{fig:adsource}, this pattern exists across all writing sources, with a particularly pronounced effect for the LLM advertisements. Importantly, these effects appear to be driven by certain demographic variables. In accordance with the literature, female participants exhibit both higher average donations (\pounds0.83 vs. \pounds0.56, $\, t = 6.6, p < 0.001$), and higher donation shares (85\% vs. 70\%, $\, \tilde{\chi}^2 = 23.79, p < 0.001$). More strikingly, the negative effect of AI-perceptions on donation behaviour is almost wholly driven by Spanish-speaking female participants (see Figure \ref{fig:donation_beliefs}). In this subsample, average donations fall by 22\% when donors believe the ad to be written by an LLM (\pounds0.94 vs. \pounds0.73, $\, t = -2.71, p = 0.007$), while fully selfish choices increase almost fourfold (23\% vs. 6\%, $\, \tilde{\chi}^2 = 9.94, p = 0.001$). Results for Spanish-speaking males point qualitatively in the same direction, but are much smaller, whereas there is no such effect observed for English-speaking donors irrespective of their gender. These outcomes point towards relevant cultural and gendered impact on the role of LLMs in social preference persuasion.}



\begin{figure}[t!]
    \centering
    % \includegraphics[width=0.5\linewidth]{fig/reb_ad_source.pdf}
    \includegraphics[width=0.45\linewidth]{fig/reb_ad_source_avg_donation_errorbar.pdf}
    \includegraphics[width=0.45\linewidth]{fig/reb_ad_source_donor_share_errorbar.pdf}
     \Description{}
     \caption{\revision{The figure shows the differences in average donation amounts (\textbf{left}) and donation shares (\textbf{right}) across actual ad sources (Control, Human, Human-AI, and AI). Bars are further categorized by the perceived source of the ad text (AI or Human). Error bars represent the confidence intervals for each category. Statistically significant differences (\textbf{*}) at \( p < 0.05 \) are observed within the AI-written ads with perceived Human-generated ads resulting in higher average donation amount. No statistically significant differences were observed for donor shares but perceptual differences remain higher for human-written ads.
}}
   
    \label{fig:adsource}
\end{figure}





\begin{figure}[t!]
    \centering
   
   \includegraphics[width=0.45\linewidth]{fig/reb_ad_source_avg_donation_gender_errorbar.pdf}
    \includegraphics[width=0.45\linewidth]{fig/reb_ad_source_donor_share_gender_errorbar.pdf}
     \Description{}
    \caption{\revision{The figure shows the differences in average donation amounts (\textbf{left}) and donation shares (\textbf{right}) based on participant demographics (sex) and language groups (Spanish-speaking and English-speaking). Bars are further categorized by the perceived source of the persuasive ad text (AI or Human). Error bars represent the confidence intervals for each category. Statistically significant differences (\textbf{**}) at \( p < 0.01 \) are observed for Spanish-speaking females, indicating higher average donation amounts and donation shares in these groups.}}
    \label{fig:donation_beliefs}
\end{figure}
 
\begin{itemize}
    \item [] \textbf{Result 3:} Participants are not able to systematically identify whether the advertisement was written by a human or an LLM.
\end{itemize}

\begin{itemize}
    \item [] \textbf{Result 4:} The influence of LLM utilization beliefs on donation behaviour is mediated by the demographics of participants. Female Spanish-speaking participants strongly condition their donation behaviour on beliefs about the advertisement's writing source. Those who believe that the advertisement was written by an LLM are (1) less likely to donate and (2) donate less.
\end{itemize}

