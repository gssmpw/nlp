%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{cite}
%\usepackage{subfigure}
%\usepackage{refcheck}
%\nocite{*} 

\usepackage{subfig}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{dsfont}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
%\usepackage{mathspec}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{tabularx,booktabs}
\newcolumntype{C}{>{\centering\arraybackslash}X} % centered version of "X" type
\usepackage{subfig}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
%\title{Online Dictionary Learning and Basis Pursuit with STDP: a Theoretical Framework with }
\title{Benchmarking Online Object Trackers for Underwater Robot Position Locking Applications}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Ali~Safa,~\IEEEmembership{Member,~IEEE}, Waqas Aman,~\IEEEmembership{Member,~IEEE}, Ali Al-Zawqari,~\IEEEmembership{Member,~IEEE}, 
Saif Al-Kuwari,~\IEEEmembership{Senior Member,~IEEE}  % <-this % stops a space
        
\thanks{Ali Safa, Waqas Aman and Saif Al-Kuwari are with the College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar (e-mail: asafa@hbku.edu.qa; waman@hbku.edu.qa; smalkuwari@hbku.edu.qa).}% <-this % stops a space
\thanks{Ali Al-Zawqari is with the ELEC Department, Vrije Universiteit Brussels, 1050 Brussels, Belgium (e-mail: aalzawqa@vub.be).}
\thanks{Ali Safa supervised the project as principal investigator, designed the experiments and contributed to the technical developments. Waqas Aman designed and contributed to the experiments. Ali Al-Zawqari contributed to the technical developments. Saif Al-Kuwari provided advices during the redaction of the manuscript. All authors contributed to the writing of the manuscript.}
% <-this % stops a space
%\thanks{Georges G.E Gielen is with the Department of Electrical Engineering, KU Leuven, 3001 Leuven, Belgium (e-mail: Georges.Gielen@kuleuven.be).}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
% \markboth{IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS}%
% {Safa \MakeLowercase{\textit{et al.}}: Pre-Processing Impact on Spiking Neural Networks: Demonstration on Radar Gesture Recognition}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




\maketitle

\begin{abstract}
Autonomously controlling the position of Remotely Operated underwater Vehicles (ROVs) is of crucial importance for a wide range of underwater engineering applications, such as in the inspection and maintenance of underwater industrial structures. Consequently, studying vision-based underwater robot navigation and control has recently gained increasing attention to counter the numerous challenges faced in underwater conditions, such as lighting variability, turbidity, camera image distortions (due to bubbles), and ROV positional disturbances (due to underwater currents). In this paper, we propose (to the best of our knowledge) a first rigorous unified benchmarking of more than seven Machine Learning (ML)-based one-shot object tracking algorithms for vision-based position locking of ROV platforms. We propose a position-locking system that processes images of an object of interest in front of which the ROV must be kept stable. Then, our proposed system uses the output result of different object tracking algorithms to automatically correct the position of the ROV against external disturbances. We conducted numerous real-world experiments using a BlueROV2 platform within an indoor pool and provided clear demonstrations of the strengths and weaknesses of each tracking approach. Finally, to help alleviate the scarcity of underwater ROV data, we release our acquired data base as open-source with the hope of benefiting future research. 
\end{abstract}
 
\begin{IEEEkeywords}
Object tracking, One-shot Machine Learning-based detection, Underwater robot control, Position locking.
\end{IEEEkeywords}
%\IEEEPARstart{I}{n}
\section*{Supplementary Material}
The underwater ROV dataset acquired in this work is openly available at \texttt{https://tinyurl.com/3es8u4dn}
\section{Introduction}
\label{lintro}
%This paper provides a new theoretical basis for unsupervised Spike-Timing-Dependent (STDP) learning in spiking neural networks (SNNs), and applies the introduced theory for image classification and action recognition using \textit{event-based cameras} (as opposed to standard frame-based images).
%\textcolor{blue}{reformule}
\IEEEPARstart{U}{nderwater} robot navigation has gained increasing attention for applications ranging from the maintenance of underwater structures and energy plants to environmental and biodiversity monitoring applications \cite{8594445, 6347898, 10161282}. Vision-based underwater robot control is known to pose several challenges compared to on-land robot navigation \cite{joshi20243dwaterqualitymapping, 10214407}, due to environmental factors such as degraded lighting conditions, lack of high-quality visual features that can be tracked, visual distortions such as bubbles and turbidity, as well as challenging water dynamics caused by currents that further disturb the position of Remotely Operated underwater Vehicles (ROVs) \cite{7989087, 6913908, 7081165, 9775489}.

Providing vision-based underwater ROV control is crucial for achieving many useful applications such as \textit{target tracking} \cite{8715217, 9032954}, where a user-specified moving object needs to be tracked by the ROV \cite{9499961}, and \textit{position control and locking} \cite{10349373, 9624231, 8207352}, where the ROV's position must be kept as stable as possible even under strong disturbances. The latter application is crucial for scenarios such as underwater inspection and constitutes our focus in this paper.
\begin{figure}[t]
\centering
    \includegraphics[scale = 0.25]{pictures/frontbanner.png}
    \caption{\textit{\textbf{Benchmarking online tracking algorithms for underwater position locking.} a) the Blue Robotics BlueROV2 robot used in our experiments; b) An underwater structure is used as an object of interest to be tracked by the ROV; c) the ROV is equipped with a front-facing camera which is used by the object tracker under test to detect and track objects as part of the position locking system; d) a pole is used to generate arbitrary disturbances on the ROV position in order to study the robustness of the object trackers during our experiments.}}
    \label{entryconcept}
\end{figure}

In this paper, we investigate the problem of underwater ROV position locking using \textit{visual-inertial fusion} \cite{9675540, 10018713} by using both camera data and the output of the ROV's Inertial Measurement Unit (IMU), which provides estimations of the ROV's yaw angle pose. We propose a system that enables an external operator to define an object of interest in front of which the ROV must remain fixed. The object of interest is then tracked by machine learning (ML)-based one-shot detection and tracking algorithms \cite{Lukezic2017}. Finally, the result of the tracking is used within a Proportional-Integrate (PI) controller \cite{10815886} to stabilize the position of the ROV under various forms of disturbances (see Fig. \ref{entryconcept}). 
%
In particular, since visual object tracking constitutes a crucial aspect of the proposed position locking system, this paper provides a detailed study on the performance of \textit{seven} different popular object detection and tracking algorithms \cite{grabner2006real, 5206737, 5596017, 5539960, kalal2011tracking, danelljan2014adaptive, Lukezic2017} on ROV position locking accuracy. The goal of these experiments is to provide rigorous benchmarking of a wide range of object tracking algorithms found in the literature for the specific case of underwater ROV control, with the goal of helping other researchers to quickly identify which tracking algorithm would be better suited for their own application. 


The contributions of this paper can be summarized as follows:
 \begin{enumerate}
%      %\item We acquire a dataset for indoor mapping with a flying drone in a highly-redundant warehouse environment, leading to highly non-stationary data streams.
      \item We provide a unified benchmarking of seven different ML-based one-shot object detection and tracking methods for the specific context of underwater ROV position control. To the best of our knowledge, this is the first time such comprehensive benchmarking has been provided in the literature. 
      \item We describe the design of our automated ROV position locking system which integrates the output results of the different tracker algorithms assessed in this work with a Proportional-Integral (PI) controller in order to study the impact of each tracker on the real-time position locking performance.
      \item We conduct numerous real-world experiments using a real-world BlueROV2 platform to assess the performance of each tracking algorithm in terms of positional deviation error. This provides the research community with clear and comprehensive demonstrations of which tracker algorithms are better suited when designing underwater ROV control applications. 
      \item Finally, to help alleviate the scarcity of open-source underwater ROV datasets, we openly release the dataset recorded in this work as supplementary material with the hope of benefiting future research.
  \end{enumerate}
  
This rest of this paper is organized as follows. First, related works are reviewed in Section \ref{related}. Next, Section \ref{trackers} provides a technical background of the various tracker algorithms benchmarked in this paper. Then, Section \ref{control} describes the autonomous position locking control strategy used during our experiments, which takes the result of each tracker algorithm benchmarked in this work to lock the position of the ROV under various disturbances. After this, Section \ref{expsetup} describes the experimental setup used in this work while our experimental results are reported in Section \ref{expresult}. Finally, conclusions are provided in Section \ref{concsec}.

\section{Related Work}
\label{related}
Visual object tracking in underwater environments has a wide range of applications \cite{6263248, 9904898}, including monitoring animal behavior, tracking pollutants and habitat changes, exploring shipwrecks, intrusion detection, surveillance, and various industrial uses such as pipeline inspection, seabed mapping, and offshore operations. However, underwater environments present unique challenges such as poor visibility, dynamic lighting conditions, and occlusions caused by water turbidity \cite{fabbri2018underwater}. Although numerous benchmarks are available for the evaluation of object trackers in open-air environments, research on underwater-specific benchmarks remains limited \cite{9032954,Panetta:IJOE:2022, Alawode_2022_ACCV}.

In open-air environments, several benchmarks have been developed for both single-object and multi-object tracking. Among the most notable are Visual Object Tracking (VOT) \cite{kristan2015vot}, Multi Object Tracking (MOT) \cite{leal2015motchallenge} and the National University of Singapore People and Rigid Objects (NUS-PRO) tracking \cite{Li2015NUSPRO}, which have significantly helped advance the field, driving improvements in speed, precision, and success rates. In fact, object tracking is a highly important and active research area and a variety of visual object trackers have been proposed in the past few years \cite{chen2022visual}. Among the prominent and openly available Machine Learning-based one-shot trackers are Channel and Spatial Reliability Tracker \textit{(CSRT)} \cite{Lukezic2017}, Kernelized Correlation Filters \textit{(KCF)} \cite{henriques2014high}, \textit{Boosting} method \cite{Grabner2006}, Multiple Instance Learning tracker \textit{(MIL)} \cite{5206737}, Tracking-Learning-Detection method \textit{(TLD)} \cite{kalal2011tracking}, \textit{MedianFlow} \cite{5596017}, and Minimum Output Sum of Squared Error tracker \textit{(MOSSE)} \cite{5539960}. These trackers have been widely exploited in diverse computer vision applications, mostly in on-land and open-air applications \cite{dardagan2021multiple}.%Some of the prominent and openly available visual object trackers are \textit{channel and spatial reliability tracker \cite{Lukezic2017}, kernelized correlation filters tracker \cite{henriques2014high}, boosting tracker \cite{Grabner2006}, multiple instance learning tracker \cite{5206737}, tracking learning detection tracker \cite{kalal2011tracking}, median flow tracker \cite{5596017}, and minimum output sum of squared error tracker\cite{5539960}}, which are exploited in diverse computer vision applications \cite{dardagan2021multiple}.

Several studies \cite{9032954, Panetta:IJOE:2022, Alawode_2022_ACCV} emphasize the need to develop datasets and evaluation frameworks specifically tailored to underwater scenarios. Various one-shot visual object trackers such as \textit{KCF}, \textit{MOSSE}, \textit{Boosting}, as well as a number of deep learning (DL)-based trackers were evaluated within these object tracking benchmarks. But crucially, none of these benchmarks proposes to close the perception-control loop and to provide an assessment of the different trackers within a ROV navigation and position control context (as opposed to our study in this paper).

In addition to not assessing the tracking quality within a real-world ROV control scenario \cite{10638434}, most of these prior work focus on the use of DL-based tracker, which requires a prior offline training phase, demanding the availability and acquisition of a training data set that faithfully captures the scenarios that the ROV will encounter during its deployment underwater. In contrast, in this paper, we focus on the use of ML-based one-shot tracker algorithms that do not require any offline training phase or any tedious training data acquisition, making these methods more \textit{generalizable} to any application context and underwater environment encountered by the ROV. 
%such as multidomain network (MDNet), siamese fully convolutional (SiamFC), efficient convolution operators (ECO), and to name a few were exploited for benchmarking.  %Building on this, the authors of \cite{Alawode_2022_ACCV} introduced UTB180, a new underwater tracking benchmark comprising 180 sequences and 58,000 annotated frames, designed to address challenges like low visibility and motion blur using 15 different deep trackers.

%A variety of visual object trackers, including those based on deep learning, have been developed for general applications. 


%\subsection{Limitations of the state of the art}
 %Existing related benchmarking visual trackers for underwater objects works \cite{9032954,Panetta:IJOE:2022,Alawode_2022_ACCV} predominantly leverage deep trackers for benchmarking, utilizing frames or videos sourced from databases or passive platforms like YouTube, and Pexels. However, 
 
However, real-world and real-time benchmark experiments involving ROV platforms remains significantly unexplored. Real-time setups inherently introduce a range of complexities, including motion-induced blur from the dynamic movement of ROVs, latency in system response affecting tracking precision, and environmental disturbances particulate and bubble interference as well as underwater currents pushing the ROV platform away from its desired position. %such as varying water turbidity, particulate and bubble interference, and light refraction. 
%
This paper addresses these gaps by systematically benchmarking more than seven popular ML-based one-shot trackers within numerous real-world ROV position control and locking experiments in order to directly benchmark the effect of each tracker under test with the position control precision achieved by the ROV platform.
 %In this regard we outlined our contribution as follow:

% \begin{enumerate}
% %      %\item We acquire a dataset for indoor mapping with a flying drone in a highly-redundant warehouse environment, leading to highly non-stationary data streams.
% \item We use under remote operating vehicle to do real-time tracking and locking of underwater objects.
%       \item \textcolor{blue}{put}
%       \item To help alleviate the scarcity of open-source underwater ROV datasets, we openly release the dataset used in this work with the hope of benefiting future research.
%   \end{enumerate}



%However, all existing tracking benchmark datasets are exclusively focused on target tracking in open-air environments \cite{}. 
%Visual object tracking in underwater environments has diverse applications, including marine biology (monitoring animal behavior and coral reefs) and environmental monitoring (tracking pollutants and habitat changes). It aids maritime archaeology (shipwreck exploration), aquaculture (fish health and predator management), and security (intrusion detection and surveillance). Industrial uses include pipeline inspection, seabed mapping, and offshore operations, while it supports scientific exploration (deep-sea research) and disaster response (search and rescue). Recreationally, it enhances scuba diving safety and underwater filming, with robotics leveraging it for navigation and object retrieval despite underwater visibility challenges.
% Deep learning is exploited for underwater object tracking \cite{Panetta:IJOE:2022,}
% \\
% survey \cite{rout2024underwater, Alawode_2022_ACCV},
% \textbf{Datasets:}
% This paper \cite{Panetta:IJOE:2022} introduces UOT100, a comprehensive underwater object tracking benchmark dataset with 104 videos and over 74,000 annotated frames. It evaluates 20 tracking algorithms, revealing their challenges in underwater environments. A GAN-based cascaded residual network is proposed to enhance image quality, improving tracking performance. Precision and success rate metrics demonstrate significant improvements with enhanced data. The study aims to advance underwater applications like robotics, security, and exploration, with plans for future enhancements. 

% Following the previous paper, authors in \cite{Alawode_2022_ACCV} introduces UTB180, a high-quality underwater tracking benchmark with 180 sequences and 58,000 annotated frames, tackling challenges like low visibility and motion blur. It benchmarks 15 state-of-the-art trackers, revealing their limitations in underwater environments compared to open-air datasets. Fine-tuning efforts on UTB180 showed only minor performance gains, highlighting the need for specialized underwater trackers. The study emphasizes the dataset's complexity and its role in advancing underwater tracking research.
% \cite{Ronny:IJMET:2018}, \cite{Katija:WACV:2021}, \cite{li2023underwater}\cite{kartal2024autonomous},

\section{Background}
\label{trackers}

%The performance of ROVs relies on advanced visual tracking systems to maintain precise position locking with both dynamic and static underwater objects. 
This section provides an overview of each one-shot object tracking algorithm that we benchmark in this paper and briefly details their algorithmic principles.

\subsection{Boosting Tracker}
%\textcolor{blue}{cite opt flow etc} % Ali: I added Optical flow /FFT refs
The \textit{Boosting} Tracker employs an online version of the AdaBoost algorithm, which allows for real-time updating of classifier features during object tracking~\cite{grabner2006real}. This method is highly adaptive to changes in the object's appearance and is especially effective in distinguishing the object from its background. The core operation of the Boosting Tracker is described by the following mathematical model:
\begin{equation}
H(x) = \text{sign}\left(\sum_{t=1}^T \alpha_t h_t(x)\right)
\end{equation}
where $x$ is the input image, $h_t(x)$ represents the so-called $t$-th weak classifier, $\alpha_t$ is the weight assigned to this classifier, and $T$ is the total number of weak classifiers. These weights are dynamically adjusted based on the classifier's performance during the tracking process.
As soon as a region of interest (RoI) to track is specified, the weak classifiers are trained using image features that best differentiate the RoI from its background. The training process adapts to changes in the RoI object appearance by selecting the most discriminative features: %, which is vital in environments with variable conditions such as underwater:
\begin{equation}
h_t^* = \arg\min_{h_t} \sum_{i=1}^N w_i \times I(y_i \neq h_t(x_i))
\end{equation}
where $I$ is an indicator function that outputs 1 if the classifier's prediction $h_t(x_i)$ does not match the actual label $y_i$, and $w_i$ are dynamically adjusted weights for each successive RoI training instance $x_i$.
%%
\subsection{Multiple Instance Learning (MIL)}
The core concept of the \textit{MIL} Tracker revolves around the notion that instead of labeling individual samples as positive or negative, samples are grouped into \emph{bags}~\cite{5206737}. 
This algorithm addresses the inherent ambiguity in sample labeling by considering sets of image patches rather than individual examples.
The MIL tracker represents the target object using a set of image features collected in bags. Each bag $B_i$ is labeled as either positive or negative:
\begin{equation}
B_i = {x_{i1}, x_{i2}, ..., x_{iN}}
\end{equation}
where $x_{ij}$ represents the $j-th$ instance in the $i-th$ bag. A bag is labeled positive if at least one instance within it contains the target object, while negative bags contain no positive instances.
Similar to the \textit{Boosting} case, the classifier $H(x)$ is constructed as a combination of weak classifiers:
\begin{equation}
H(x) = \sum_{k=1}^K \alpha_k h_k(x)
\end{equation}
where $h_k(x)$ represents the $k-th$ weak classifier, and $\alpha_k$ is its corresponding weight. Each weak classifier operates on a single feature within a so-called Haar-like feature set and is defined as:
\begin{equation}
h_k(x) = \begin{cases}
1 & \text{if } p_k v_k(x) < p_k \theta_k \\
-1 & \text{otherwise}
\end{cases}
\end{equation}
where $v_k(x)$ is the feature value, $\theta_k$ is a threshold, and $p_k$ determines the direction of the inequality.
During tracking, the algorithm maintains two types of bags: positive bags constructed around the current RoI target location, and negative bags sampled from regions further from the target.
% The learning process optimizes the bag likelihood:
% \begin{equation}
% L(H) = \sum_{i} \log(1 - \prod_{j}(1 - p_{ij})) + \sum_{i} \log(\prod_{j}(1 - p_{ij}))
% \end{equation}
% where $p_{ij}$ represents the probability that instance $x_{ij}$ is positive.
\subsection{MedianFlow}
The MedianFlow Tracker utilizes a so-called Forward-Backward (FB) error to enhance tracking accuracy and reliability, particularly adept at detecting tracking failures~\cite{5596017}. This tracker calculates object displacements between consecutive frames using optical flow~\cite{horn1981determining}. The fundamental operation of the MedianFlow Tracker can be expressed by tracking feature points forward in time and then backward, ensuring that the start and end points align closely:
\begin{equation}
FB(T_k^f |S) = \text{distance}(T_k^f, T_k^b) = \|\mathbf{x}_t - \mathbf{\hat{x}}_t\|_2
\end{equation}
where $T_k^f$ represents the forward trajectory of a point $\mathbf{x}_t$, and $T_k^b$ represents the backward trajectory starting from $\mathbf{x}_{t+k}$ back to $\mathbf{x}_t $, where $\mathbf{\hat{x}}_t$ is the estimated initial position after backward tracking. 
The FB error is used to evaluate the consistency of the tracking, with high discrepancies indicating potential tracking failures. The tracker operates by initially detecting feature points within a user-defined RoI bounding box and tracking these points forward in time. After the forward tracking phase, the points are tracked backward to their initial positions.
The tracker performs robust filtering by automatically removing 50\% of the worst predictions based on their error measures. %Notably, combining FB error with Normalized Cross-Correlation (NCC) outperforms using either measure alone:
%\begin{equation}
%\beta_{t+1} = \text{Median}(\{{\mathbf{x}_t} | FB, NCC \leq p{50}\})
%\end{equation}
where the bounding box $\beta_{t+1}$  for the next frame $t+1$ is estimated by applying a median filter to the remaining feature points. The tracker must also account for changes in object size. It does this by computing a bounding box scale factor:
\begin{equation}
scale = \text{Median}\left(\frac{d(\mathbf{x}_{i}^{t+1}, \mathbf{x}_{j}^{t+1})}{d(\mathbf{x}_i^t, \mathbf{x}_j^t)}\right)
\end{equation}
where $d(\mathbf{x}_i, \mathbf{x}_j)$ represents the distance between the points i and j. 
% The combination of FB error with NCC, coupled with median-based filtering and scale estimation, provides robust tracking capabilities.
%%
\subsection{Minimum Output Sum of Squared Error (MOSSE)}
The MOSSE Tracker uses adaptive correlation filters to achieve rapid and accurate tracking performance~\cite{5539960}. These filters optimize tracking accuracy when objects change in scale, pose, and lighting conditions.
The tracker initializes its correlation filter using the initial user-selected RoI frame. This filter adapts quickly to the target object's appearance through the following formulation:
\begin{equation}
H = \frac{\sum_{i=1}^{N} \overline{G_i} \cdot F_i}{\sum_{i=1}^{N} F_i \cdot \overline{F_i}}
\label{eqmosse}
\end{equation}
where $F_i$ is the Fourier transform of the $i$-th training image, $G_i$ represents the desired output label peak indicator \textit{in the frequency domain}, and $H$ is the filter to be learned. The bar notation in (\ref{eqmosse}) indicates complex conjugation.
The filter minimizes the output sum of squared errors between estimated and actual target positions:
\begin{equation}
\min_H \sum_{i} || H \star x_i - y_i ||_2^2
\end{equation}
where $\star$ denotes correlation, $x_i$ represents training images, and $y_i$ represents desired correlation outputs.
%MOSSE achieves higher frame rates than traditional tracking algorithms. 
The tracker also includes a mechanism to detect tracking failures. It uses the Peak-to-Sidelobe Ratio (PSR) to assess the quality of tracking:
\begin{equation}
PSR = \frac{peak - mean(sidelobes)}{std(sidelobes)}
\end{equation}
The PSR score measures the clarity of the correlation peak response. A low PSR score indicates potential tracking issues. The tracker can pause and resume tracking adaptively based on this score. 
%This adaptive approach, combined with its computational efficiency, makes MOSSE effective for real-time tracking applications.

\subsection{Tracking Learning Detection (TLD)}
The TLD framework has been proposed for tracking unknown objects in video streams over long periods~\cite{kalal2011tracking}. TLD divides the tracking task into three components: tracking, learning, and detection. These components work together to handle occlusions, scale changes, and varying lighting conditions.
The tracking component estimates object motion between consecutive frames. It assumes the object makes small movements and stays visible. The detection component processes each frame independently. It detects all known appearances of the object and corrects the tracker when it loses the target.
The learning component introduces a novel P-N learning approach with so-called P-expert and N-expert models complementary working together to provide detection: %This approach improves the detector's performance through continuous analysis and adaptation:
\begin{equation}
 \text{P-N Learning} = \begin{cases}
\text{P-expert:} & \text{estimates missed detections} \\
\text{N-expert:} & \text{estimates false alarms}
\end{cases}
\end{equation}
The P-expert identifies missed detections whereas the N-expert identifies false alarms. This learning process is modeled as a discrete dynamical system, ensuring that the detector progressively improves and adapts to avoid previous errors and adapts to new object appearances. 
TLD combines the outputs from its tracker and detector components in real time to continuously update the detection model. This design allows TLD to track objects that may temporarily leave the camera's view. The framework shows significant improvements over traditional tracking methods, particularly in handling object disappearance and reappearance.
%
\subsection{Kernelized Correlation Filters (KCF)}
The KCF Tracker proposes an alternative tracking-by-detection method through the use of efficient Fast Fourier Transforms (FFT)~\cite{duhamel1990fast} in kernelized space~\cite{danelljan2014adaptive}. By using multi-dimensional color features and adaptive dimensionality reduction, KCF maintains accuracy amidst changes in object appearance. The tracker processes so-called sub-windows (sub-frames) from each input camera frame using circulant matrix structures for rapid correlation in the frequency domain:
\begin{equation}
H = \frac{\sum_{i=1}^{N} F_i \overline{Y_i}}{\sum_{i=1}^{N} F_i \overline{F_i} + \lambda}
\end{equation}
where $F_i$ is the FFT of the $i$-th training subwindow, $Y_i$ represents the desired output, $\overline{F_i}$ is the complex conjugate of $F_i$, and $\lambda$ is a regularization parameter. KCF incorporates color attributes and kernel methods to enhance object discrimination:
\begin{equation}
\text{Response} = \mathcal{F}^{-1} \left( H \odot \mathcal{F}(z) \right)
\end{equation}
where $\mathcal{F}$ and $\mathcal{F}^{-1}$ represent the Fourier and inverse Fourier transforms, respectively, $z$ is the current image patch, and $\odot$ denotes element-wise multiplication of the detection filter $H$. KCF employs an adaptive update scheme to adjust to appearance changes, efficiently updating the model without the need to store all previous appearances. The update process is defined as:
\begin{align}
A_p^N &= (1 - \gamma) A_{p-1}^N + \gamma Y_p U_p^x, \\
A_p^D &= (1 - \gamma) A_{p-1}^D + \gamma U_p^x (U_p^x + \lambda),
\end{align}
where $A_p^N$ and $A_p^D$ are the numerator and denominator of the updated classifier filter model, respectively, $\gamma$ is the learning rate, and $U_p^x$ is the Fourier transformed kernel output for the current frame. This update scheme enhances the model's responsiveness to new information while maintaining computational efficiency, enabling real-time tracking performance.
%%%%%%%
\subsection{Channel and Spatial Reliability Tracker (CSRT)}
\label{csrt}
CSRT~\cite{Lukezic2017} represents a popular algorithm within the class of discriminative correlation filter (DCF) tracking methods~\cite{5539960}. This algorithm introduces two key innovations: channel reliability weighting and spatial reliability mapping. At its core, CSRT employs a correlation filter $h$ that learns from user-selected RoI training samples $x$ to predict object locations. The basic correlation operation can be expressed as:
\begin{equation}
f(x) = h \star x
\end{equation}
where $\star$ denotes the correlation operation.  Channel reliability is used to adjust the response of each channel during the localization stage. %, though it does not directly alter the optimization formulation as previously suggested.
%Instead, 
Each channel's filter $h_c$ within the filter bank $h$ is optimized individually:
\begin{equation}
h^* = \arg\min_h \sum_{c=1}^C \left( \| h_c \star x_c - y \|^2 + \lambda \|h_c\|^2 \right)
\label{firstthi}
\end{equation}
where $x_c$ represents the feature channel $c$ of the input $x$, $y$ is the desired output, and $\lambda$ is a regularization parameter to alleviate over-fitting on the RoI that is being tracked.

CSRT also introduces the so-called spatial reliability map $m(x)$ into (\ref{firstthi}) which modulates the learning process to emphasize more on the reliable parts of the target. By introducing $m(x)$, the optimization process is re-defined as:
\begin{equation}
h^* = \arg\min_h \|m(x) \odot (h \star x - y)\|^2 + \lambda \|h\|^2
\end{equation}
where $m(x)$ is the spatial reliability map that applies weights at each spatial location, enhancing focus on regions suitable for tracking, and $\odot$ denotes element-wise multiplication. CSRT follows an iterative optimization process where the tracker updates channel weights and the spatial reliability map based on observed changes in the target's appearance. This adaptive approach is crucial for handling object deformation and partial occlusions effectively. 
% The spatial reliability component is especially beneficial in underwater tracking scenarios. It ensures the tracker maintains a lock on the target even when the target is partially obscured by marine debris or experiences varying illumination conditions. The channel reliability mechanism efficiently manages the color distortion common in underwater environments by dynamically adjusting the influence of each channel based on its reliability.

Crucially, all the methods covered in the Section are one-shot learning algorithms that do not require any model pre-training as in the case of Deep Learning-based techniques (which are not the scope of this work). Instead, the methods covered above only require an initial RoI selected by the user and indicating the object to be tracked within the scene. Then, the algorithms listed above keep on tracking the specified object while updating their detection model using the successively tracked RoIs that were detected within the camera frames. This enables these methods to still generalize well to any unseen environment in an agnostic manner, and perform without the need for tedious training set acquisition and labeling (which is needed to capture each specific environment in which the algorithm must be deployed).

\section{Autonomous ROV position locking pipeline}
\label{control}
This Section describes the ROV autonomous position control strategy adopted in this work. The position control pipeline takes as input the tracking result provided by the different tracker algorithms under test, and uses the tracked bounding box coordinates alongside a Proportional-Integral (PI) controller to regulate the position of the ROV. First, the pre-processing steps applied on the tracker results are detailed in Section \ref{preprocessbbox}. Then, the PI controller taking as input the result of the pre-processing scheme is described in Section \ref{univariatePID}. 

\subsection{Pre-processing Steps}
\label{preprocessbbox}
We use the camera data $C_k$ and the yaw angle measurement $\Psi_k$ as input for the controller pipeline proposed in this work (where $k$ denotes the discrete-time index).

In order to lock the position of the ROV with regard to a specific object that needs to be maintained within the ROV's field of view, an external operator can select an object of interest on the live camera feed $C_k$ by drawing a \textit{bounding box} around the target object. We denote the coordinates of the user-specified bounding box as $(x_u,y_u,w_u,h_u)$ where $x_u,y_u$ denote the corner origin coordinate of the bounding box on the image plane, and $w_u,h_u$ respectively denote its width and hight.

Then, once the user-specified bounding box is obtained, the locking control procedure begins by first detecting the user-specified object on the next camera frame $C_{k+1}$ and then using the new bounding box $(x_k,y_k,w_k,h_k)$ detected on the previous frame $C_k$ to measure in real-time the x-axis deviation $\Delta X$, y-axis deviation $\Delta Y$ and scale deviation $\Delta S$ between the current bounding box position of the object and the original user-specified bounding box $(x_u,y_u,w_u,h_u)$:
\begin{equation}
    \Delta X = \frac{x_u+w_u}{2} - \frac{x_k+w_k}{2}
    \label{eqx}
\end{equation}
\begin{equation}
    \Delta Y = \frac{y_u+h_u}{2} - \frac{y_k+h_k}{2}
    \label{eqy}
\end{equation}
\begin{equation}
    \Delta S = \frac{w_u - w_k}{2} + \frac{h_u - h_k}{2}
    \label{eqs}
\end{equation}

The object detection and tracking takes as initial point the region of interest (RoI) specified by the user-defined bounding box and is implemented using the trackers covered in Section \ref{trackers}. During our experiments, each one of the tracker algorithms covered in Section \ref{trackers} will be tried out to assess the impact of each tracking method on the position locking performance of the ROV.

In Equations (\ref{eqx}, \ref{eqy}, \ref{eqs}), $\Delta X$ and $\Delta Y$ respectively provide an indication of how much the ROV has deviated along its x- and y-axis (\textit{sway} and \textit{heave} axis), while the scale deviation $\Delta S$ provides information on how much the ROV has deviated along its \textit{surge} or z-axis (i.e., getting closer or farther from the target object).

In addition, we also compute the yaw angle deviation between the current yaw angle $\Psi_k$ and the yaw angle $\Psi_u$ that the ROV exhibited at the time of the user-defined object selection (measured using the IMU semsor on board of the ROV):
\begin{equation}
    \Delta \Psi = \Psi_k - \Psi_u
    \label{eqsp}
\end{equation}

\subsection{Proportional-Integral (PI) controller}
\label{univariatePID}
We set up a Proportional-Integral (PI) controller for each of the control axis of the ROV as follows:
%We consider a uni-variate Proportional-Integral (PI) controller built under the assumption that the deviations $\Delta X, \Delta Y, \Delta S, \Delta \Psi$ are independent and respectively contribute to the ROV's lateral, depth, forward and yaw-axis movements. As such, we set up a PI controller as follows:
\begin{multline}
    u_{\{X,Y,Z,\Psi\}} = K_p^{\{X,Y,Z,\Psi\}} \times \Delta \{X,Y,Z,\Psi\}_k 
    \\
    + K_i^{\{X,Y,Z,\Psi\}} \times \sum_{k=0}^{k^*} \Delta \{X,Y,Z,\Psi\}_k 
    \label{univarPID}
\end{multline}
where $k^*$ denotes the current time index, and where $K_p, K_i$ are respectively the proportional and integral coefficients. The indices ${\{X,Y,Z,\Psi\}}$ indicate that each of the $4$ available motor thrust axis is controlled by its own independent PI controller using the associated deviation corresponding to the thrust axis. The PI parameters $K_p^{\{X,Y,Z,\Psi\}}, K_i^{\{X,Y,Z,\Psi\}}$ are tuned empirically by visually assessing the behavior of the ROV under random position disturbances imposed during the tuning process. Finally, $u$ denotes the controller output command sent to the ROV.

In the next Section, the automated object tracking and position control strategy described above will be integrated with the BlueROV2 platform as part of the setup used during our experiments. 

\section{Experimental Setup}
\label{expsetup}

%In this Section, we provide an overview of the experimental setup we use to assess the performance of each tracker algorithm considered in this work.

We perform our experiments in an indoor pool using a BlueROV2 underwater robot platform connected to a control laptop via a tether connection. The control laptop enables both a manual control (using an Xbox controller) and an \textit{automated} control of the ROV using a \textit{Python} environment that communicates with the ROV through a \textit{MAVlink} interface. To conduct the experiments, we implement our position locking algorithm described in Section \ref{control} within a \textit{Python} script, which communicates with the ROV to acquire camera and IMU data, and sends back motor control commands for correcting the ROV's position when deviations occur. Fig. \ref{blockscem} provides a block diagram of the setup.
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.35]{pictures/schematic_diagram.png}
    \caption{\textit{\textbf{Block diagram of the ROV control setup.} The ROV platform is connected via a tether to a USB interface. The laptop running the position control script receives the ROV camera feed and IMU readings via the tether USB interface and sends control commands to the ROV using this same interface. Optionally, an Xbox controller can be used for the manual control of the ROV for initiating its position in the pool (not used during our automated control experiments). }}
    \label{blockscem}
\end{figure}

As objects to be tracked for the ROV position locking, we consider \textit{a)} a custom \texttt{structure} object built using \textit{MakerBeam} aluminum beams and \textit{b)} a \texttt{ladder} object mounted along the pool's wall (see Fig. \ref{typesofobject}). Using both of these structures during our experiments adds additional variability as the algorithm's accuracies will be assessed on two different objects with different properties in terms of location in the pool and visibility.% (the beams of the \texttt{structure} object being much finer than the thickness of the \texttt{ladder} object).
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.32]{pictures/objet_tracked_2.png}
    \caption{\textit{\textbf{Types of objects to be tracked for position locking.} a) \texttt{structure} object b) \texttt{ladder} object.}}
    \label{typesofobject}
\end{figure}

Using each structure, we perform three series of experiments:
\begin{enumerate}
    \item \textbf{With no disturbances:} In this first series of experiment, we are interested in studying the natural drift of each tracker algorithm caused by both visual tracking errors and the natural movement of the water in the pool.
    \item \textbf{With bubbles in front of the ROV's camera:} In this second series of experiments, we assess the robustness of each tracker to visual distortions caused by \textit{bubbles} generated using the disturbance pole (see Fig. \ref{entryconcept} d). In addition, since the disturbance pole is agitated in front of the ROV's camera, this second series of experiments also provides an assessment of the tracker algorithms accuracy under partial object \textit{occlusion} (caused by the pole that is agitated in between the object to be tracked and the ROV's camera).
    \item  \textbf{With positional disturbances:} In this last series of experiments, the disturbance pole is used to cause deviations along the yaw angle, x-axis, y-axis and z-axis position of the ROV. These disturbance deviations are meant to be large-scale so as to make the visual tracking and position locking problem more challenging ($\sim 30^\circ$ for the yaw angle and $\sim 30$ cm for the x,y,z-axis). In order to achieve systematic disturbances throughout the experiments, the disturbance sequence is kept the same across the trackers and the real-time measured deviation of the ROV due to the disturbance can be monitored using the control computer at the same time that the operator generates the ROV displacement with the pole. 
\end{enumerate}

Table \ref{dataacquisitions} summarizes the dataset acquired during the experiments conducted in this work (available as open source as supplementary material).

\begin{table}[htbp]
\centering
\begin{tabularx}{0.47\textwidth}{@{}l*{0}{c}c@{}}
\toprule
Acquisition Name  & Disturbance Type  \\ 
\midrule
%HOTS \cite{7508476}      & 80.8       & 27.1          \\ 
no-disturb-\{s,l\}-\{\texttt{tracker\_type}\}      &  no disturbances          \\ 
bubble-disturb-\{s,l\}-\{\texttt{tracker\_type}\}      &  bubble generated using pole    \\ 
with-disturb-\{s,l\}-\{\texttt{tracker\_type}\}     &     position deviations using pole      \\ 

%\midrule
%NeuNorm \cite{DBLP:journals/corr/abs-1809-05793} &  99.53 & 60.5  \\
%Deep STS-ResNet \cite{DBLP:journals/corr/abs-2003-12346} &  99.6 & 69.2  \\
\bottomrule
\end{tabularx}
\caption{\textit{\textbf{ROV dataset.} In the acquisition name, the suffix -s denotes visual locking on the \texttt{structure} object while the suffix -l denotes visual locking on the \texttt{ladder} object. In addition, the suffix \texttt{tracker\_type} denotes the tracking algorithm used during the data acquisition and ROV control (see tracker types in Section \ref{trackers}).}}
\label{dataacquisitions}
\end{table}




\section{Experimental Results}
\label{expresult}

This section reports our experimental results regarding the robustness and accuracy of each tracker algorithm that is benchmarked in this paper. First, each tracker is assessed without any external disturbances in Section \ref{withoutdist} in order to provide a baseline indication of the stability of each tracker. Then, in Section \ref{withbubble}, each tracker is assessed under \textit{occlusion and bubble-type visual disturbances} generated in front of the ROV's camera. The goal of this second series of experiments is to assess to robustness of each tracker to typical bubble-type visual distortions and partial occlusions. Finally, the last series of experiments that we perform in Section \ref{withexternaldist} consists of assessing the position locking accuracy of each tracker algorithm under \textit{strong positional disturbances}, by using a pole to push the robot in arbitrary directions (see Fig. \ref{entryconcept} d).

\subsection{Without external disturbances}
\label{withoutdist}

In this first series of experiments, we are interested to study the positional locking performance of each tracker without any external disturbances other than the natural movement of the water in the swimming pool environment where the ROV operates. These experiments will enable us to gain a first assessment of the \textit{baseline} performance of each tracker algorithm. We conduct our experiments with both the \texttt{structure} and the \texttt{ladder} objects and report the obtained position locking and yaw angle deviation results in Figures \ref{res13}, \ref{res3}, \ref{res15} and \ref{res5}.
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_13.png}
    \caption{\textit{\textbf{Position locking error} \textbf{-} \texttt{Structure} \textbf{-} \texttt{No disturbances}. }}
    \label{res13}
\end{figure}
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_3.png}
    \caption{\textit{\textbf{Yaw angle error} \textbf{-} \texttt{Structure} \textbf{-} \texttt{No disturbances}. }}
    \label{res3}
\end{figure}

In the case of the \texttt{structure} object (Figures \ref{res13} and \ref{res3}), we remark that both the \textit{MIL} and the \textit{MedianFlow} trackers are the top performers and achieve a similar median positional and yaw angle deviation error of around $10$ pixels and $0.05$ radians respectively. On the other hand, we remark on Fig. \ref{res13} and \ref{res3} that the \textit{TLD} and \textit{Boosting} methods perform the worse, with \textit{TLD} exhibiting the highest median positional error of around $35$ pixels and with \textit{Boosting} exhibiting the highest median yaw angle error of around $0.43$ radians.  

\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_15.png}
    \caption{\textit{\textbf{Position locking error} \textbf{-} \texttt{Ladder} \textbf{-} \texttt{No disturbances}. }}
    \label{res15}
\end{figure}
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_5.png}
    \caption{\textit{\textbf{Yaw angle error} \textbf{-} \texttt{Ladder} \textbf{-} \texttt{No disturbances}. }}
    \label{res5}
\end{figure}

In the case of the \texttt{ladder} object (Figures \ref{res15} and \ref{res5}), we observe that the \textit{TLD} is the worst performer, both in terms of position locking and in terms of yaw angle error. This confirms the observations previously obtained using the \texttt{structure} object in Fig. \ref{res13} where the \textit{TLD} method was also the worst performer. On the other hand, we observe in Figures \ref{res15} and \ref{res5} that the \textit{CSRT} and \textit{KCF} methods appear to be the top performers since these two methods jointly exhibit both a low positional error and a low yaw angle error (with \textit{KCF} outperforming \textit{CSRT} in terms of positional error and \textit{CSRT} outperforming \textit{KCF} in terms of yaw angle error).

Overall, we observe that the \textit{TLD} method systematically ranks among the worst-performing trackers throughout the experiments above while the \textit{KCF}, \textit{CSRT} and \textit{MIL} methods rank among the top performers (each outperforming the other methods depending on the different scenarios, either in terms of positional deviation or in terms of yaw angle deviation).

\subsection{With bubble generation in front of the camera}
\label{withbubble}

In this second experiment, we assess the robustness of each tracking algorithm against bubble-type disturbances and partial occlusion that can occur in front of the ROV's cameras. The distortion caused by the bubbles as well as the partial occlusion caused by the agitating pole makes the target locking problem more challenging since the ROV's camera feed becomes distorted as well, leading to potential tracking errors and even the loss of track. Fig. \ref{bubblescenario} provides a view of the bubble disturbance setup considered in these experiments. We perform experiments with both the \texttt{structure} and the \texttt{ladder} objects and report the position locking and the yaw angle deviation results in Figures \ref{res14}, \ref{res4}, \ref{res16} and \ref{res6}.
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.35]{pictures/bubbles_all_2.png}
    \caption{\textit{\textbf{Bubbles are generated} by using the pole to agitate the water in front of the ROV's camera. The blue bounding box indicates the object that is being tracked and the red bounding box shows the location and scale of the original bounding box selected by the user to which the blue bounding box must align with during the ROV control process.}}
    \label{bubblescenario}
\end{figure}
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_14.png}
    \caption{\textit{\textbf{Position locking error} \textbf{-} \texttt{Structure} \textbf{-} \texttt{Bubbles}. }}
    \label{res14}
\end{figure}
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_4.png}
    \caption{\textit{\textbf{Yaw angle error} \textbf{-} \texttt{Structure} \textbf{-} \texttt{Bubbles}. }}
    \label{res4}
\end{figure}

In the case of the \texttt{structure} object (Figures \ref{res14} and \ref{res4}), we remark that the \textit{KCF} and \textit{MedianFlow} methods are not robust to the presence of bubbles in the camera feed and lead to a catastrophic loss of track. During our experiments, we tried both \textit{KCF} and \textit{MedianFlow} multiple times and always remarked the catastrophic track loss phenomenon due to the distortion caused by the bubbles. 

Using the \texttt{structure} object, we observe that the \textit{Boosting} method gives the best results in terms of position locking (Fig. \ref{res14}) while the \textit{CSRT} method gives the best results in terms of yaw angle error (Fig. \ref{res4}). On the other hand, we observe that the \textit{Boosting} method performs the worse in terms of yaw angle error. 

\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_16.png}
    \caption{\textit{\textbf{Position locking error} \textbf{-} \texttt{Ladder} \textbf{-} \texttt{Bubbles}. }}
    \label{res16}
\end{figure}
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_6.png}
    \caption{\textit{\textbf{Yaw angle error} \textbf{-} \texttt{Ladder} \textbf{-} \texttt{Bubbles}. }}
    \label{res6}
\end{figure}

In the case of the \texttt{ladder} object (Figures \ref{res16} and \ref{res6}), we remark that all methods were able to keep their tracking, indicating that the \texttt{ladder} object is easier to distinguish and track compared to the \texttt{structure} object. In terms of position locking error, Fig. \ref{res16} shows that the \textit{MIL} and \textit{CSRT} methods provide the best results and achieve almost the same positional error.  

In terms of yaw angle error, Fig. \ref{res6} shows that the the \textit{KCF} and \textit{CSRT} methods achieve an on-par median error (see black line in the box plots) while \textit{CSRT} achieves a smaller maximum-minimum range (indicated by the T-bars in the box plots). 

Overall, the \textit{CSRT} tracker appears as the best performing method throughout the bubble disturbance experiments since it systematically ranks within the lowest reported positional and yaw angle errors, and does not suffer from track loss when using the \texttt{structure} object (as opposed to the \textit{KCF} tracker which failed when tracking the \texttt{structure} object).

\begin{table*}[!t]
\captionsetup{justification=centering}
\begin{tabularx}{\textwidth}{@{}l*{6}{C}c@{}}
\toprule
Experiments - \textbf{Position error}  & Boosting & MIL & MedianFlow & MOSSE & TLD & KCF & CSRT \\ 
\midrule
\textbf{1)} \texttt{Structure} - \texttt{No Disturbance}    & 12.1      & 10.5          & 10.6   & 19.3  & 38.4    & 17.8 & 18.1
  \\ 
\textbf{2)}  \texttt{Ladder} - \texttt{No Disturbance} & 6.7        & 7.9        & 13.4    & 7.8   & 25.1    & 5 & 8.4    \\ 
\textbf{3)}  \texttt{Structure} - \texttt{Bubbles}     & 18.6       & 21.7          & \textit{FAIL}  & 21.6  & 21.6   & \textit{FAIL} & 20.3   \\ 
\textbf{4)}  \texttt{Ladder} - \texttt{Bubbles}      & 18.1       & 16.7          & 17.9   & 38.8 & 37.9   & 20.5 & 17.3  \\ 
\textbf{5)}   \texttt{Structure} - \texttt{With disturbance}     & 20       & 30.4          & 19.1   & 19.5  & 37.2   & \textit{FAIL} & 18.2   \\
\textbf{6)}    \texttt{Ladder} - \texttt{With disturbance}    & 33.1       & \textit{FAIL}          & 17.2  & 25  & 24.9  & \textit{FAIL}& 20.6  \\
%\addlinespace
\midrule
\textbf{Average:} & 18.1 & \textit{FAIL} & \textit{FAIL} & 22 & 30.9 & \textit{FAIL} & \textbf{17.2}\\
\midrule
Experiments - \textbf{Yaw angle error} & & & & & & & \\
\midrule
\textbf{1)} \texttt{Structure} - \texttt{No Disturbance}       & 0.43        & 0.06          & 0.06   & 0.17 & 0.14 & 0.16   & 0.17  \\ 
\textbf{2)}  \texttt{Ladder} - \texttt{No Disturbance}      & 0.061        & 0.039          & 0.045  & 0.05 & 0.125  & 0.027    & 0.025  \\ 
\textbf{3)}  \texttt{Structure} - \texttt{Bubbles}      & 0.35       & 0.27         & \textit{FAIL} & 0.09  & 0.24  & \textit{FAIL}   & 0.18   \\ 
\textbf{4)}  \texttt{Ladder} - \texttt{Bubbles}         & 0.11       & 0.105         & 0.075   & 0.17 & 0.12  & 0.065    & 0.065  \\ 
\textbf{5)}   \texttt{Structure} - \texttt{With disturbance}      & 0.24       & 0.25         & 0.28  & 0.28  & 0.15  & \textit{FAIL}    & 0.15  \\ 
\textbf{6)}    \texttt{Ladder} - \texttt{With disturbance}     & 0.27       & \textit{FAIL}         & 0.25   & 0.18 & 0.37  & \textit{FAIL}    & 0.1   \\ 
\midrule
\textbf{Average:} & 0.24 & \textit{FAIL} & \textit{FAIL} & 0.16 & 0.19 & \textit{FAIL} & \textbf{0.115}\\
\bottomrule
\end{tabularx}
 \caption{\textit{\textbf{Result summary.} Both the median position error (in pixels) and the median yaw angle errors (in radians) are reported for each tracker under test. We denote by FAIL the presence of a tracking failure during the experiment.}}
\label{bigtable}
\end{table*}

\subsection{With external disturbances}
\label{withexternaldist}

In this final series of experiments, we assess the robustness of each tracking algorithm against strong positional disturbances. During our experiments, positional disturbances are applied by using the pole to cause arbitrary deviations in the yaw angle, surge (x-axis position), sway (y-axis position) and heave (z-axis position) of the ROV. The disturbance pattern is kept the same across the experiments and consists of causing \textit{i)} a yaw angle deviation in the positive direction, followed by a yaw disturbance in the negative one; \textit{ii)} a positive surge disturbance followed by a negative one; \textit{iii)} a positive sway disturbance, followed by a negative one; and \textit{iv)} a positive heave disturbance, followed by a negative one. The trained human operator manipulating the disturbance pole applies disturbances with similar amplitudes by monitoring the real-time position and yaw angle readings coming from the ROV's controller.

The results obtained with each tracker on both the \texttt{structure} and \texttt{ladder} objects are provided in Figures \ref{res11}, \ref{res1}, \ref{res12} and \ref{res2}.
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_11.png}
    \caption{\textit{\textbf{Position locking error} \textbf{-} \texttt{Structure} \textbf{-} \texttt{With disturbances}. }}
    \label{res11}
\end{figure}
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_1.png}
    \caption{\textit{\textbf{Yaw angle error} \textbf{-} \texttt{Structure} \textbf{-} \texttt{With disturbances}. }}
    \label{res1}
\end{figure}

In the case of the \texttt{structure} object (Figures \ref{res11} and \ref{res1}), we observe that the \textit{KCF} method fails to consistently track the object, leading to a track loss (as also observed in Figures \ref{res14}, \ref{res4} of the bubble disturbance case in Section \ref{withbubble}). On the other hand, we remark in Figures \ref{res11} and \ref{res1} that the \textit{CSRT} method achieves the lowest error in terms of both positional and yaw deviation error. 
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_12.png}
    \caption{\textit{\textbf{Position locking error} \textbf{-} \texttt{Ladder} \textbf{-} \texttt{With disturbances}. }}
    \label{res12}
\end{figure}
\begin{figure}[htbp]
\centering
    \includegraphics[scale = 0.57]{pictures/Figure_2.png}
    \caption{\textit{\textbf{Yaw angle error} \textbf{-} \texttt{Ladder} \textbf{-} \texttt{With disturbances}. }}
    \label{res2}
\end{figure}

Now, with regard to the case of the \texttt{ladder} object (Figures \ref{res12} and \ref{res2}), we observe once again that the \textit{KCF} method suffers from track loss. In addition, the \textit{MIL} method also fails by suffering from track losses during the experiments. On the other hand, we remark that both the \textit{CSRT} and \textit{MedianFlow} methods appear as the best performing trackers, with \textit{MedianFlow} outperforming \textit{CSRT} in terms of positional error and CSRT outperforming \textit{MedianFlow} in terms of yaw angle error.

Overall, the \textit{CSRT} tracker appears to be the best performer throughout our positional disturbance experiments as it systematically ranks among the best methods using both the \texttt{structure} and \texttt{ladder} objects, and in terms of both positional and yaw angle deviation errors. On the other hand, the use of the \textit{KCF} and \textit{MIL} methods lead to track loss, indicating their lack of robustness against the strong positional disturbances applied to the ROV during our experiments. 

\subsection{Discussion and summary of the results}
\label{discussion}

Table \ref{bigtable} summarizes the median errors (indicated by the black lines in the box plots) obtained using each tracking algorithm for every experiment we conducted. It can be clearly seen that on average, the \textit{CSRT} tracker appears to be the top performer throughout our experiments, as it achieves the lowest average median errors of 17.2 pixels for the positional deviation and 0.115 radians for the yaw angle deviation across all experiments (see averages along all experiments in Table \ref{bigtable}). 

On the other hand, the \textit{MIL}, \textit{MedianFlow} and \textit{KCF} methods are among the lowest performers since these algorithms suffered from \textit{tracking failures} during the experiments (i.e., the object to be tracked gets lost and as a consequence, the ROV control pipeline severely drifts away).

We did not observe tracking failures using the \textit{Boosting}, \textit{MOSSE} and \textit{TLD} trackers, but all these methods reach higher average median errors compared to the \textit{CSRT} tracker, as seen in Table \ref{bigtable}. 

During our experiments, we also visually remarked the superiority of the \textit{CSRT} method by observing that the bounding boxes produced by \textit{CSRT} appear to be more stable, as opposed to the other methods which often produced bounding boxes that would suddenly change in scale in between successive frames. In addition, we observed a greater robustness with \textit{CSRT} when it came to handling partial object occlusions (e.g., during the bubble experiments). These useful properties can be linked to the key feature of \textit{channel reliability tracking} found in the \textit{CSRT} algorithm \cite{Lukezic2017}, which is crucial for handling object deformation and partial occlusion (see Section \ref{csrt}).  

Therefore, in light of the comprehensive series of real-world underwater experiments conducted in this work, it clearly appears that the \textit{CSRT} tracker is a sound candidate when designing underwater vision-based ROV control systems and should be preferably selected over the other trackers benchmarked in this paper. 







\section{Conclusion}
\label{concsec}
This paper has provided what is, to the best of our knowledge, a first unified study on the use of seven different ML-based object detection and tracking algorithms in the context of underwater ROV control. First, after briefly reviewing the related works, this paper has provided a detailed description of seven popular ML-based object detection and tracking algorithms that have often been used in conventional computer vision applications, but rarely used in the underwater context. Then, this paper has described the design of our automated ROV position locking system built around the one-shot object trackers feeding their output bounding box deviations into a PI controller. Extensive real-world experiments were conducted using the proposed position locking pipeline which controls in real-time a BlueROV2 platform within a swimming pool environment. Our experiments clearly show that the \textit{Channel and Spatial Reliability Tracker (CSRT)} outperforms other methods for the design of underwater ROV control systems. %Finally, to help alleviate the scarcity of open-source underwater ROV datasets, we have released our dataset as supplementary material with the hope of benefiting future research.



\section*{Acknowledgement}

\footnotesize{
\bibliographystyle{IEEEtran}
\bibliography{References}
}


% \begin{thebibliography}{00}

% \bibitem{uavforindoorfire} P. Zhang, Z. Wu, J. Wang, S. Kong, M. Tan and J. Yu, "An Open-Source, Fiducial-Based, Underwater Stereo Visual-Inertial Localization Method with Refraction Correction," 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Prague, Czech Republic, 2021, pp. 4331-4336, doi: 10.1109/IROS51168.2021.9636198.

% \bibitem{hydrobatics} D. A. Duecker, N. Bauschmann, T. Hansen, E. Kreuzer and R. Seifried, "Towards Micro Robot Hydrobatics: Vision-based Guidance, Navigation, and Control for Agile Underwater Vehicles in Confined Environments," 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Las Vegas, NV, USA, 2020, pp. 1819-1826, doi: 10.1109/IROS45743.2020.9341051.

% \bibitem{navigation} M. Xanthidis et al., "AquaVis: A Perception-Aware Autonomous Navigation Framework for Underwater Vehicles," 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Prague, Czech Republic, 2021, pp. 5410-5417, doi: 10.1109/IROS51168.2021.9636124.

% \bibitem{trackingagain} D. A. Duecker, C. Horst and E. Kreuzer, "From Aerobatics to Hydrobatics: Agile Trajectory Planning and Tracking for Micro Underwater Robots," 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Prague, Czech Republic, 2021, pp. 8617-8624, doi: 10.1109/IROS51168.2021.9636154.

% \bibitem{poseestimationdeep} B. Joshi et al., "DeepURL: Deep Pose Estimation Framework for Underwater Relative Localization," 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Las Vegas, NV, USA, 2020, pp. 1777-1784, doi: 10.1109/IROS45743.2020.9341201.

% \bibitem{distributedslam} J. McConnell, Y. Huang, P. Szenher, I. Collado-Gonzalez and B. Englot, "DRACo-SLAM: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar Equipped Underwater Robot Teams," 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Kyoto, Japan, 2022, pp. 8457-8464, doi: 10.1109/IROS47612.2022.9981822.

% \bibitem{syntheticdataset} O. Álvarez-Tuñón et al., "MIMIR-UW: A Multipurpose Synthetic Dataset for Underwater Navigation and Inspection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6141-6148, doi: 10.1109/IROS55552.2023.10341436.

% \bibitem{CSRTtracker} A. Lukežic, T. Vojír, L. C. Zajc, J. Matas and M. Kristan, "Discriminative Correlation Filter with Channel and Spatial Reliability," 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 2017, pp. 4847-4856, doi: 10.1109/CVPR.2017.515.

% \bibitem{Opencv} Bradski, G. (2000). The OpenCV Library. Dr. Dobb's Journal of Software Tools.

% \bibitem{slidingmode} H. Tugal et al., "Sliding Mode Controller for Positioning of an Underwater Vehicle Subject to Disturbances and Time Delays," 2022 International Conference on Robotics and Automation (ICRA), Philadelphia, PA, USA, 2022, pp. 3034-3039, doi: 10.1109/ICRA46639.2022.9812005.

% \bibitem{monodepths} H. Liu, M. Roznere and A. Q. Li, "Deep Underwater Monocular Depth Estimation with Single-Beam Echosounder," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 1090-1097, doi: 10.1109/ICRA48891.2023.10161439.

% \bibitem{enYOLO} J. Wen et al., "EnYOLO: A Real-Time Framework for Domain-Adaptive Underwater Object Detection with Image Enhancement," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12613-12619, doi: 10.1109/ICRA57147.2024.10610639.

% \bibitem{docking} J. Chavez-Galaviz and N. Mahmoudian, "Underwater Dock Detection through Convolutional Neural Networks Trained with Artificial Image Generation," 2022 International Conference on Robotics and Automation (ICRA), Philadelphia, PA, USA, 2022, pp. 4621-4627, doi: 10.1109/ICRA46639.2022.9812143.

% \bibitem{multiagent} K. Yao et al., "Image-Based Visual Servoing Switchable Leader-follower Control of Heterogeneous Multi-agent Underwater Robot System," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5200-5206, doi: 10.1109/ICRA48891.2023.10160853.

% \bibitem{odometry} B. Joshi, H. Damron, S. Rahman and I. Rekleitis, "SM/VIO: Robust Underwater State Estimation Switching Between Model-based and Visual Inertial Odometry," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5192-5199, doi: 10.1109/ICRA48891.2023.10161407.

% \bibitem{modelpredictivecont} T. Gao et al., "Model Predictive Control for an Autonomous Underwater Robot with Fully Vectored Propulsion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10482-10488, doi: 10.1109/ICRA57147.2024.10611025.

% \bibitem{underwaterobotsimulator} E. Potokar, S. Ashford, M. Kaess and J. G. Mangelson, "HoloOcean: An Underwater Robotics Simulator," 2022 International Conference on Robotics and Automation (ICRA), Philadelphia, PA, USA, 2022, pp. 3040-3046, doi: 10.1109/ICRA46639.2022.9812353.


% \end{thebibliography}


% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.5in,clip,keepaspectratio]{ali_picture_new4.png}}]{Ali Safa}
(Member, IEEE) is currently an Assistant Professor with the College of Science and Engineering, Hamad Bin Khalifa University (HBKU), Doha, Qatar. He received the M.Sc. degree in electrical engineering from the Université Libre de Bruxelles, Brussels, Belgium, and the Ph.D. degree in AI-driven processing for extreme edge applications from the KU Leuven, Belgium, in collaboration with the IMEC R\&D Center, Leuven Belgium. He has been a Visiting Researcher with the University of California at San Diego (UC San Diego), La Jolla, USA, in Spring 2023. He has authored and co-authored more than 40 research articles in international journals and conferences, and is the author of a Springer book on the application of Neuromorphic Computing to Sensor Fusion and Drone Navigation tasks. His research interests include Edge AI, continual learning, and sensor fusion for robot perception. 
\end{IEEEbiography}
%\vspace{-5cm}
\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.5in,clip,keepaspectratio]{waqas.jpg}}]{Waqas Aman}
(Member, IEEE) is currently a Postdoctoral Fellow at the College of Science and Engineering, Hamad Bin Khalifa University, Qatar. He earned his Ph.D. in Electrical Engineering from the Information Technology University, Lahore, Pakistan, with a focus on security and reliability in underwater acoustic communication networks. During his Ph.D. studies, he was a visiting researcher at the University of Glasgow, UK, and a research intern at KAUST, Saudi Arabia for six months each.  His work has been recognized with prestigious awards, including the Best Paper Award at IEEE ISNCC 2023, and the top downloaded technical paper in 2018-2019 by Trans. on ETT, Wiley. So far, he has published 23 research articles in international journals and conferences. 
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.5in,clip,keepaspectratio]{ali.jpg}}]{Ali Al-Zawqari}
(Member, IEEE) received the M.Sc. degree in electrical engineering from the Bruface program, Brussels, Belgium, and the Ph.D. degree in enhancing generalization and fairness in machine learning from the Vrije Universiteit Brussel, Belgium. He is a senior fellow with the QatarDebate Center, Doha, Qatar. He is also a postdoctoral research associate at the ELEC department at Vrije Universiteit Brussel, Belgium. He has authored and co-authored more than 15 research articles in international journals and conferences. His research interests include digital signal processing, fundamental machine learning, graph theory, human Learning, and Arabic NLP. 
\end{IEEEbiography}

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.5in,clip,keepaspectratio]{saif.jpg}}]{Saif Al-Kuwari}
(Senior Member, IEEE)  is an Associate Professor in the Faculty of
Science and Engineering of HBKU. He received a BEng in
Computers and Networks from the University of Essex (UK)
in 2006 and two PhDs from the University of Bath and Royal
Holloway, University of London (UK) in Computer Science
in 2012. His research interests include applied cryptography,
Quantum Computing, Computational Forensics, and their
connections with Machine Learning. He is an IET and BCS
fellow and an IEEE and ACM senior member
\end{IEEEbiography}


% % if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% % insert where needed to balance the two columns on the last page with
% % biographies
% %\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks



\end{document}
