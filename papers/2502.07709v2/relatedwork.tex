\section{Related Work}
\label{sec:related_work}

\subsection{Goal selection in autotelic agents}
Autotelic agents exploring vast goal spaces face a critical challenge: they must prioritize which goals to pursue to efficiently develop general competence \citep{colas_autotelic_2022}. The automatic curriculum learning community has developed various approaches to address this challenge \cite{portelas_automatic_2020}, leveraging different forms of intrinsic motivation: pursuing goals of intermediate difficulty \citep{florensa_automatic_2018,racaniere_automated_2020, castanetSL23}, seeking novelty or uncertainty \cite{warde-farley_unsupervised_2018,pong_skew-fit_2020,pitis_maximum_2020}, or maximizing learning progress (LP) \cite{stout_competence_2010,matiisen_teacher-student_2017,fournier_accuracy-based_2018,portelas_teacher_2019,colas_curious_2019, kanitscheider_multi-task_2021,kovac_grimgep_2023,zhang_omni_2024}. Among these, LP-based methods have proven particularly robust\,---\,they adapt to the agent's capabilities without requiring environment knowledge and avoid common pitfalls like getting stuck on goals where progress plateaus or chasing uncontrollable novelty. The key challenge with LP approaches lies in efficiently estimating progress over large goal spaces, which is the focus of our work.


\subsection{Computing LP over goals} \label{sec:related_work_lp}
Learning Progress (LP) measures the expected future improvement in achieving a goal through practice \citep{oudeyer_intrinsic_2007}. Since future progress cannot be directly measured, most approaches use past progress as a proxy, with the recent exception of \cite{kumar_practice_2024}'s Bayesian prediction model. The most direct approach to estimate LP is to regularly reevaluate the agent's competence for each goal \citep{kanitscheider_multi-task_2021,zhang_omni_2024}, which accurately captures competence transfer\,---\,the phenomenon where practicing one goal affects performance on other goals. However, this becomes computationally prohibitive for large discrete goal spaces and is just impossible for continuous ones. 
One way to address this is to only rely on online estimations, where a goal's estimated competence is only updated when this goal is practiced. Online estimations nonetheless fail to capture competence transfer and existing methods addressed this by grouping goals with similar competence together. For continuous spaces, approaches either learn to partition the space directly when dimensionality is low \citep{oudeyer_intrinsic_2007,baranes_active_2013,portelas_teacher_2019}, or first embed high-dimensional goals into a lower-dimensional space before partitioning \citep{laversanne-finot_curiosity_2018,kovac_grimgep_2023}. For discrete spaces, methods typically rely on expert-defined groupings \citep{stout_competence_2010,matiisen_teacher-student_2017}. However, these grouping approaches are inherently brittle: they assume no transfer between groups while potentially masking competence variations within groups. This limitation is particularly acute for high-dimensional structured spaces like natural language, where competence transfer naturally occurs between semantically similar goals regardless of predefined groupings. Instead, MAGELLAN leverages an LLM's semantic understanding to dynamically model competence transfer between goals, enabling efficient and adaptive LP estimation without requiring predefined groupings or exhaustive evaluation.

\subsection{Autonomous LLM agents}
Recent work has explored using Large Language Models (LLMs) to solve complex tasks in interactive environments. Early approaches focused on direct action prediction using LLMs, either incorporating environmental feedback \cite{huang_language_2022, yao2022react, hao2023rap, shinn2023reflexion, wang_voyager_2023} or operating without it \cite{ahn_as_2022}. However, these methods did not update the LLM's knowledge through environmental interactions. A new direction emerged with GLAM \citep{carta_grounding_2023}, followed by \citep{wen_reinforcing_2024,wen_entropy-regularized_2024,zhou_archer_2024} that ground LLMs in interactive environments using online RL. The resulting LLM agents demonstrated remarkable generalization across language tasks; however, they lack the autotelic mechanisms necessary for navigating expansive goal spaces. In this paper, we enhance a SAC-GLAM \cite{gaven2024sacglam} LLM agent with metacognitive abilities, enabling it to estimate its LP and prioritize goals within large language spaces