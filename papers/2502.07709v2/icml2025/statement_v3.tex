Let $\mathcal{M}=(S, A, \mathcal{T}, R)$ be an MDP, with $S$ a set of states, $\mathcal{T}$ the transition function, $A$ the action space and $R$ the reward function. Let $G$ be a goal space and $\Pi$ the policy space. We define a competence function $C_{\mathcal{M},\pi}: G %\times \Pi 
\rightarrow \mathbb{R}$ that indicates the competence of a policy $\pi \in \Pi$ for a goal in $\mathcal{M}$.\footnote{In all generality a competence function does not assume the goal to be inside the MDP. For instance, $g$ could ask for a maximum number of steps.} The final aim is to find the optimal policy $\pi^*$ that maximizes
$$J_{\mathcal{M}}(\pi)=\mathbb{E}_{g \sim {\cal U}(G)} [C_{\mathcal{M}, \pi}(g)],$$ where $U(X)$ is the uniform distribution over a set $X$.

In this paper, we focus on episodic online goal-conditioned RL with sparse and binary rewards, defined on a goal-augmented MDP $(S, A, \mathcal{T}, G, R)$, with $R:S\times G \rightarrow \{0;1\}$ a binary success function indicating whether a state $s$ satisfies a goal $g$. Here, we define $G=\{S_0 \times I|S_0 \subseteq S\}$, with $I$ an instruction space and $S_0$ the set of initial states. We consider a textual environment where a prompting function $\phi:S\times I \rightarrow {\cal V}^K$ is given to transform any pair (state, instruction) into a textual prompt of $K$ tokens in a given vocabulary ${\cal V}$. Thus, from the agent side, the policy $\pi$ selects any action $a_h \in A$ by sampling from a categorical distribution $\pi(.|\phi(s_h, i))$ at any step $h$ of the episode.

For our competence function we use the success probability $\mathbb{P}_\pi(s_0,i)$ defined as the probability for $\pi$, starting from $s_0$, to fulfill $i$ within $H$ steps: $\mathbb{P}_\pi(s_0,i) = \mathbb{E}_{\tau \sim \pi(\tau|\phi(s_0,i))}[r_{\tau,i}]$, with $r_{\tau,i}=\mathds{1}(\exists s_h \in \tau, R(s_h,i)=1)$ the goal outcome of episode $\tau$  for instruction $i$,  %\mathbb{E}_{\tau \sim \pi(\tau|\phi(s_0,i))}[\mathds{1}(\exists s_h \in \tau, R(s_h,i)=1)]$, with 
$\mathds{1}$ the indicator function and
$\pi(\tau|\phi(s_0,i))$ the distribution of episodes of $H$ steps induced by $\pi$ to fulfilling $i$ from $s_0$. In this setting our objective becomes: 
$$J(\pi)=\mathbb{E}_{s_0 \sim {\cal U}(S_0), i \sim {\cal U}(I)} \left[\mathbb{P}_\pi(s_0,i) \right].$$   

However, given the possibly huge number of goals $(s_0,i)$, the direct maximization of the problem becomes particularly inefficient. %intractable.
Our aim is to leverage transfer of competence between goals and focus during training on the ones maximizing LP. We denote as $\pi^t$   %=\Xi(\pi^{n-1}, \Gamma^{n-1} \cup \{\tau^n\})$ % {\tau^i\}_{i \in [[1;N]])$ %$ \sim \pi(\tau|s_O,g))$
the policy obtained after $t$ episodes using an RL algorithm, with $\Gamma^{t}$ the set of all trajectories collected during training using $\{ \pi^{k-1}| k \in [|1, t|] \}$. The goal of each episode is sampled using a task selector $\eta_{G}$ that selects a goal based on collected trajectories $\eta_{G}(\Gamma^t)=g$. %  tasks  tuning on $n$ an initial policy on $\pi^{n-1}$ from experience $\Gamma^{n-1}$ at step $n-1$ augmented by a new episode sampled from $\pi^{n-1}$. % new experience acquired from sampling an episode from $$ 
Given a budget of $T$ training episodes, we thus consider the problem of approaching the optimal selector $\eta^*_{G} = \arg\max_{\eta_{G}} J(\pi^T)$. In particular, we build on prior work to construct $\eta_{G}$ on a proxy of the LP at each training episode $t$. We define the LP for any goal $g=(s_0,i)$ as the improvement of the policy at episode $t$ after $k$ episodes on goal $g$:  $LP^k_{\pi^t}(g)=\mathbb{P}_{\pi^{t+k}}(g) - \mathbb{P}_{\pi^{t}}(g)$. As highlighted in Section~\ref{sec:related_work_lp}, since computing the future competence on all goals is intractable, prior approaches approximate future progress with past progress ($LP^k_{\pi^t}(g) \approx \mathbb{P}_{\pi^{t}}(g) - \mathbb{P}_{\pi^{t-k}}(g)$). Nonetheless, accurately estimating past and current competence remains a challenge in large discrete goal spaces.
