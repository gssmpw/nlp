

% To thrive in an ever-changing world, humans must continuously explore and develop new skills throughout their lives. This remarkable capacity for open-ended learning is powered by \textit{curiosity}\,---\,the intrinsic motivation to explore for the sake of learning and discovery \cite{berlyne1954theory, kidd2015psychology}. As \textit{autotelic learners}, humans are motivated to imagine and pursue their own goals \cite{colas2022language}, but this raises a fundamental challenge: how can we effectively navigate a potentially infinite space of learning opportunities?

% To meet this challenge, humans evolved intrinsic motivation systems that help prioritize among possible goals \cite{oudeyer2016evolution,gottlieb2018towards}. Among these signals, learning progress (LP)\,---\,the measurable improvement in one's ability to achieve goals\,---\,plays a crucial role \cite{kaplan2007search}. Research has demonstrated LP's significance in both machine and human learning: it enables efficient automatic curriculum learning \cite{lopes2012strategic,poli2024curiosity} while producing developmental trajectories that mirror key patterns observed in infant development \cite{oudeyer2016evolution}. Recent experimental studies involving free exploration of learning activities have further confirmed that humans rely on metacognitive LP monitoring to effectively explore and navigate their learning objectives \cite{ten_humans_2021,leonard2023young, sayali2023learning, poli2024exploration}.


Humans are open-ended learners, continuously exploring and developing new skills through their lifetime. A key mechanism to enable this remarkable capacity is \textit{curiosity-driven learning}\,---\,the intrinsic motivation to explore for the sake of learning and discovery \cite{berlyne1954theory, kidd2015psychology}. Crucially, humans are \textit{autotelic learners} intrinsically motivated to represent, invent, select and solve their own goals \cite{colas2022language}. To navigate in a possibly infinite space of goals, without time to explore it exhaustively, they are equipped with intrinsic motivation signals \cite{baldassarre2012intrinsically, gottlieb2018towards}. Research on human curiosity has shown the key role of one such intrinsic signal: Learning Progress (LP), i.e. improvement of one’s own ability to solve goals \cite{kaplan2007search}. 
%Learning Progress (LP), defined
%for each candidate objective as the ability of the learner to
%progress in mastering it (given its current skills), has been
%shown to play a key role in human motivation  \cite{kaplan2007search}. 
Computational modeling work showed both how it enables efficient automatic curriculum learning \cite{lopes2012strategic,poli2024curiosity} and how it generates developmental trajectories that simulate key properties in the development of human infants \cite{oudeyer2016evolution}. Recently, several experimental paradigms where humans were free to explore various learning activities confirmed that humans use metacognitive LP monitoring to explore and prioritize goals \cite{ten_humans_2021,leonard2023young, sayali2023learning, poli2024exploration}. 

% todo: add references from other labs using LP (e.g. Clune, Bellemarre, etc)
Inspired by open-ended learning in humans and other natural systems, research in AI and artificial life has studied how to build machines with similar capabilities \citep{schmidhuber_powerplay_2013, jiang2023general, Sigaud2023ADO}. A promising direction focuses on developing autotelic artificial agents that, like humans, can self-generate their learning curriculum by progressively exploring goals with maximum LP \cite{baranes_active_2013, colas_autotelic_2022}. This approach efficiently allocates the agent's learning time by avoiding goals that are either too easy or too difficult \cite{portelas_automatic_2020,romac_teachmyagent_2021}, enabling even physical robots to acquire complex skills like tool use in just a few dozen hours \cite{forestier_intrinsically_2022}. However, while these methods show promise in constrained settings, scaling them to open-ended learning remains challenging. The key difficulty lies in efficiently estimating an agent's current competence and expected LP across potentially infinite, evolving, and high-dimensional goal spaces\,---\,a fundamental challenge we address in this paper.

% Parallel to this, a recent breakthrough has emerged in training large language model (LLM) agents to learn goal-directed behaviors through online interaction with their environment \cite{carta_grounding_2023,wen_entropy-regularized_2024,wen_reinforcing_2024}. These LLM agents possess a critical capability for open-ended learning: they can leverage the structure of language to generalize effectively, transferring skills learned from practiced goals to semantically similar goals. This language-based approach provides the rich expressiveness needed for open-ended learning settings, where agents must pursue an ever-expanding space of possible goals. However, this expressiveness creates a new challenge\,---\,the goal space becomes vast and high-dimensional, making effective prioritization crucial. Current approaches to learning progress (LP) estimation struggle with such spaces. They are limited to either small, low-dimensional goal spaces \cite{baranes_active_2013,portelas_teacher_2019,kanitscheider_multi-task_2021,zhang_omni_2024} or require expert-defined goal groupings \cite{colas_curious_2019,akakzia_grounding_2021,kumar_practice_2024}. Notably absent is the ability to automatically capture semantic relationships between goals\,---\,a capability essential for estimating how an LLM agent's skills will generalize.

Parallel to this, a recent breakthrough has emerged in training large language model (LLM) agents to learn goal-directed behaviors through online interaction with their environment \cite{carta_grounding_2023,wen_entropy-regularized_2024,wen_reinforcing_2024}. These LLM agents possess a critical capability for open-ended learning: they can leverage the structure of language to generalize effectively, transferring skills learned from practiced goals to semantically similar goals. %Using language-defined goals offer infinite possibilities on which goals to learn, as required for an open-ended learning artificial agent. 
Using language-instructed agents also offers extensive expressiveness for specifying goals, as required for the open-ended learning setting. This usually induces huge goal spaces, for which prioritization is crucial. %artificial agent. 
However, current approaches leveraging %computing
LP for this prioritization fall short at handling such discrete, high-dimensional and structured goal space. %Indeed, t
They either work only on small low-dimensional goal spaces \cite{baranes_active_2013,portelas_teacher_2019,kanitscheider_multi-task_2021,zhang_omni_2024} or rely on expert-defined goal groupings to reduce the number of goals \cite{colas_curious_2019,akakzia_grounding_2021,kumar_practice_2024}. In particular, none of them are able to capture the semantic relationships between goals to efficiently estimate an LLM agent's generalization abilities.

In this paper, we study how to estimate LP over natural language goals such that an LLM agent learning with online RL in an interactive environment could increase its overall competence as efficiently as possible. For this, we introduce \textbf{MAGELLAN}, for \textbf{M}et\textbf{A}cognitive \textbf{GE}neralization of \textbf{L}earning progress in \textbf{LAN}guage model agents. MAGELLAN leverages the LLM inside the agent to learn an LP estimator that automatically learns semantic relationships and tracks competence transfer between goals in a sample efficient manner (see Figure~\ref{fig:magellan}). We evaluate MAGELLAN in the Little-Zoo environment specifically designed as a carefully controlled experimental setup for commonsense-based generalization of agents in a textual environment. In particular, we study the following scientific questions:

\par\smallskip $\bullet$
\textbf{Q1.} Given an initial set of language goals, how does MAGELLAN's estimation of a learner's competence compare to more classic methods? How does this estimation scale with the size of the goal space?
\par\smallskip $\bullet$
\textbf{Q2.} Can MAGELLAN be used by an online RL LLM agent to self-organize an efficient learning curriculum over these goals?
\par\smallskip $\bullet$
\textbf{Q3.} How well can MAGELLAN's estimation generalize to predict the agent’s competence on unseen goals?
\par\smallskip $\bullet$
\textbf{Q4.} When these new unseen goals are introduced throughout training, can MAGELLAN leverage its generalization abilities to integrate new goals into the curriculum seamlessly?


We show that MAGELLAN 1) accurately and efficiently approximates LP, 2) allows an LLM agent to master all goals from Little-Zoo while prior methods fail when not provided extensive expert knowledge, and 3) generalizes its LP estimation to never-seen goals, enabling faster adaptation to evolving goal spaces. Moreover, we show MAGELLAN learns to cluster goals and achieves results comparable to an LP estimator with expert-defined groups.




%  For this, autotelic agents that learn to represent, generate, select and solve their own goals \cite{colas_autotelic_2022} by focusing on tasks with maximum learning progress (LP) was shown to allow agents to allocate their training budget efficiently \cite{lopes_strategic_2012,portelas_automatic_2020,romac_teachmyagent_2021}. Nonetheless, this requires agents to estimate their own competence and track its evolution over all possible goals, reacting to potential competence transfer between goals. 

% The current literature on LP mainly tracks competence by: 1) regularly evaluating the agent on all possible goals \cite{kanitscheider_multi-task_2021,zhang_omni_2024}, which is intractable in vast spaces; 2) using expert-defined groupings of goals with an assumed transfer, reducing the number of goals to track at the cost of potential wrong transfer assumptions \cite{lopes_strategic_2012,stout_competence_2010,matiisen_teacher-student_2017,fournier_accuracy-based_2018,colas_curious_2019,blaes_control_2019,akakzia_grounding_2021}. While some methods exist for automatically clustering continuous goal spaces \cite{baranes_r-iac_2009,baranes_active_2013,moulin-frier_exploration_2013,portelas_teacher_2019}, no work exists for high-dimensional and structured discrete spaces such as language-specified goals. We argue that, by leveraging semantic relationships between goals in such structured spaces, an autotelic LLM agent should learn how to automatically cluster goals in the goal space to predict its competence, i.e. a form metacognitive monitoring.