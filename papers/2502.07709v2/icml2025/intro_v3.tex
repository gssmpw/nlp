
% opening example, language as universal goal space and the open-ended learning challenge
Imagine teaching a robot to ``grasp a glass", ``cook a risotto", or ``compose a poem." The space of possible goals we can express in language is effectively infinite, with tasks ranging from simple physical manipulations to complex creative endeavors. This richness of language makes it a powerful medium for specifying goals to artificial agents. However, it also poses a fundamental challenge: how can an agent efficiently explore and master an ever-expanding space of possible goals? This question lies at the heart of open-ended learning\,---\,the ability to continuously acquire new skills throughout one's lifetime.

% human open-ended learning, and the quest to model it computationally
Humans display a remarkable capacity for such open-ended learning, driven by a remarkable intrinsic motivation to imagine and pursue goals of our own making \citep{colas_autotelic_2022}. This autotelic learning ability allows us to progressively build increasingly complex skills, from an infant's first attempts to grasp objects to an adult mastering a musical instrument. Understanding and replicating this ability in artificial systems represents a fundamental challenge in AI, with implications for both advancing machine intelligence and illuminating human development \citep{schmidhuber_powerplay_2013, jiang2023general, Sigaud2023ADO}. 

% intrinsic motivations, focus on LP and how it explains human development and learning trajectories
Evolution appears to have equipped humans with sophisticated intrinsic motivations to tackle this challenge of navigating vast possibility spaces \citep{oudeyer2016evolution, gottlieb2018towards}. At the core of this ability is learning progress (LP)\,---\,the measurable improvement in one's ability to achieve goals \citep{kaplan2007progress}. Rather than randomly sampling from all possible goals or pursuing objectives that are too difficult, LP-driven learners focus on the ``Goldilocks zone" of activities where they are making the most progress \citep{kidd2015psychology}. This scaffolding effect of LP has been extensively documented in developmental psychology: infants systematically explore their environment by selecting activities of increasing difficulty \citep{oudeyer2016evolution}, while recent studies show that both children and adults explicitly track their learning progress to guide their choice of learning activities \cite{ten_humans_2021,leonard2023young}. 

% these ideas have been translated to AI, where LP has been used to train agents and let them organize their curricula
Inspired by these findings, researchers have worked to computationally model learning progress as a key mechanism for open-ended learning in artificial agents \citep{schmidhuber_possibility_1991, kaplan2007progress, baranes_active_2013}. By designing agents that aim to maximize their learning progress, researchers have created systems that effectively generate their own training curricula \citep{portelas_automatic_2020,romac_teachmyagent_2021}, leading to notable successes in both simulated and physical robotic tasks \citep{colas_curious_2019, portelas_teacher_2019, kanitscheider_multi-task_2021, forestier_intrinsically_2022}. 

% but existing methods are hard to scale when goal spaces becomes large
However, scaling these successes to true open-ended learning in vast language goal spaces introduces fundamental new challenges. Traditional approaches face a critical dilemma: they must either extensively sample each possible goal to directly measure competence (prohibitively expensive), or rely on predefined groupings of goals to share competence estimates (brittle and limiting). What we need instead is a way to leverage the semantic structure inherent in language itself\,---\,understanding that success at ``stacking red blocks" likely translates to ``stacking blue blocks" but only partially helps with ``building a pyramid".

% recent breakthroughs in LLM agents suggest a promising direction
Language-conditioned reinforcement learning (RL), and its most recent extension leveraging large language models (LLMs) allow to train agent policies that efficiently generalize across vast language goal spaces \cite{carta_grounding_2023, wen_entropy-regularized_2024, wen_reinforcing_2024}. Scaling LP estimations to these spaces requires comparable efforts at the \textit{metacognitive level}\,---\,\ie giving agents the ability to model their own competence and learn patterns in how skills generalize across related goals. This work proposes to develop such a capability. 

% magellan
We introduce \textbf{MAGELLAN} (\textbf{M}et\textbf{A}cognitive \textbf{GE}neralization of \textbf{L}earning progress in \textbf{LAN}guage model agents), a framework that enables LLM agents to learn to predict their own competence and learning progress online. The key insight of MAGELLAN is to leverage the LLM's semantic understanding: rather than treating each goal independently or relying on predefined groupings, MAGELLAN learns to map goals into a latent space where semantically similar goals naturally cluster together, allowing competence estimates to efficiently generalize across related goals. 

% experiments and contributions
To validate MAGELLAN, we conduct experiments in an interactive learning environment specifically designed to study commonsense-based generalization across a large space of language goals. Our results demonstrate that MAGELLAN:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=15pt]
    \item enables more accurate and sample-efficient learning progress estimation compared to traditional approaches,
    \item improves curriculum learning, allowing agents to master a large goal space where prior methods fail, 
    \item generalizes to predict competence on new goals, enabling rapid adaptation as the goal space evolves.   
\end{itemize}
Importantly, MAGELLAN achieves this without requiring any expert-defined goal groupings, learning to organize the goal space purely from experience. 

These results demonstrate how augmenting LLM agents with metacognitive abilities can effectively scale curriculum learning to open-ended goal spaces. Beyond our specific experiments, this work suggests that metacognition may be essential for truly open-ended learning systems, just as it appears to be in human learning. As we push towards more general artificial intelligence, the ability to model and predict one's own learning capabilities may become increasingly crucial.

