% Following removed to gain space
% We start by defining learning progress for a policy $\pi$ w.r.t. a goal space $\mathcal{G}$. Considering that, at time $t$, $\pi$ has experienced and been updated on $t-1$ goals $\{g_1, g_2,...,g_{t-1}\} = \mathcal{E}_t$, we can define some competence measure $C_{\pi}(g|\mathcal{E}_t)$ for $\pi$ on some goal $g \in \mathcal{G}$ after having experienced $\mathcal{E}_t$. The expected learning progress of $\pi$ on $g$ at time $t$ is the competence change if $\pi$ experiences $g$: $LP_{\pi}(g) = C_{\pi}(g | \mathcal{E}_t \cup \{g\}) - C_{\pi}(g | \mathcal{E}_t)$. As future progress $C_{\pi}(g | \mathcal{E}_t)$ for all possible goals is usually unknown, most prior works approximate it using past progress between: $\hat{LP}_{\pi}(g) = C_{\pi}(g | \mathcal{E}_t) - C_{\pi}(g | \mathcal{E}_{t-1})$.




We start by defining learning progress for a policy $\pi$ w.r.t. a goal space $\mathcal{G}$ as the competence improvement on some goal $g$ when the policy practices $g$.
Here, we consider goals defined with natural language (i.e. $g \in \mathcal{G} \subset \mathcal{V}^K$ with $\mathcal{V}$ a vocabulary of possible tokens, $K$ the maximum number of tokens and $\mathcal{G}$ the set of goals) along with an LLM learner. We expect such an LLM-based policy to show generalization between semantically similar goals (i.e. competence transfer).

In experiments below, we focus on episodic RL with sparse and binary rewards (i.e. r=1 if the goal is reached, 0 otherwise). As the episode stops when a goal is reached or after a fixed number of steps, only two returns can be obtained for an episode where \( \pi \) tries to achieve the goal \( g \): 1 (success) or 0 (failure). We note an episode's outcome $\mathds{1}_{(\pi, g)}$. We thus define competence as the success probability: $\mathbb{P}_{\pi}(g) = \mathbb{E}_\pi\left[\mathds{1}_{(\pi, g)}\right]$. 

% When collecting multiple episodes of $\pi$ trying to achieve $g$, we define the success rate (SR) $SR_{\pi}(g)$ as the average outcome of these trials (i.e. a monte-carlo estimation of the success probability).

% We note these two possible returns $\mathds{1}_{(\pi, g)}$. In such settings, we define competence as the Success Rate (SR), denoted by:  

% \begin{equation}
%     SR_{\pi(g)} = \mathbb{E}_\pi\left[\mathds{1}_{(\pi, g)}\right].
% \end{equation}  
