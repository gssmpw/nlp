Building open-ended learning agents has been a long-standing goal of research communities studying artificial agents \citep{schmidhuber_powerplay_2013, Sigaud2023ADO}. Yet, two major challenges remain: 1) creating and learning increasingly abstract goals and 2) navigating the possibly infinite space of possible goals \citep{jiang_learning_2023}. Several recent works tackled the first challenge by leveraging natural language as a compositional goal space that can be used to create new levels of abstractions \citep{wang_voyager_2023, colas_augmenting_2023, pourcel_aces_2024, colas_language_2020}. Notably, the use of LLM-based agents drastically impacted this choice, as such agents show efficient generalization capabilities when facing new goals. However, this also impacts the second challenge, as using natural language for the goal space creates an extremely wide space to navigate.

For this, the field of Automatic Curriculum Learning (ACL) \citep{portelas_automatic_2019} has gathered various methods to prioritize goals adapted to an online RL agent's competence. Among them, methods based on Learning Progress (LP) showed excellent capabilities on a broad range of tasks spanning from continuous control to complex tasks in Minecraft \citep{baranes_r-iac_2009, portelas_teacher_2019, kanitscheider_multi-task_2021}. Empowering these agents with an LP estimator also able to generalize over unseen goals would allow them to navigate spaces of natural language goals. However, current methods estimating the LP for natural language goals remain largely limited and cannot leverage the structure of language to generalize the LP to unseen goals.


% As such agents face increasingly complex tasks, properly ordering the learning curriculum has been shown as key \cite{jiang_learning_2023}. In this line, the field of Automatic Curriculum Learning (ACL) \citep{portelas_automatic_2019} has gathered various methods automatically designing a learning curriculum for RL agents. Among them, methods based on Learning Progress (LP) showed excellent capabilities on a broad range of tasks spanning from continuous control to complex tasks in Minecraft \citep{baranes_r-iac_2009, portelas_teacher_2019, kanitscheider_multi-task_2021}.

% However, measuring an agent’s current LP over large task spaces remains challenging as it requires a past and current evaluation of its competence on all tasks. Prior work approximates this LP by partitioning the task space and producing local estimations \citep{baranes_r-iac_2009, abdulhai_lmrl_2023}. % Nonetheless, obtaining meaningful partitions is not straightforward, and several prior studies leveraged natural language to obtain such a partition \citep{wang_voyager_2023, colas_augmenting_2023, pourcel_aces_2024}. 

% In addition to this, recent work also showed how natural language could be used to expand an agent’s current task space by imagining new goals \citep{colas_language_2020, colas_vygotskian_2022}. Leveraging language notably allows commonsense-based generalization of goals to create new ones. However, current methods estimating the LP cannot generalize to tasks unseen by the agent, limiting the possible use of LP to scaffold how the agent must learn new tasks.

% In particular, we consider the setup where an LLM-based RL agent, coupled with an LP-based ACL method ordering its curriculum, learns to solve tasks described with natural language. We equip the LLM agent with current and past competence estimators, which we train to match the agent’s competence over the task space. We then use these estimators to approximate the agent’s LP and leverage this estimation in our ACL teacher, allowing the LLM agent not only to increase its competence over a growing set of tasks but also to order its own curriculum. We call our method \textbf{M}et\textbf{A}cognitive \textbf{GE}neralization of \textbf{L}earning progress in \textbf{LAN}guage model agents (\textbf{MAGELLAN}).


In this paper, we study how an LLM-based agent learning with online RL could navigate a space of natural language goals using an LP estimator that generalizes. We introduce \textbf{M}et\textbf{A}cognitive \textbf{GE}neralization of \textbf{L}earning progress in \textbf{LAN}guage model agents: \textbf{MAGELLAN}. MAGELLAN leverages the LLM inside the agent to learn an LP estimator that can generalize. By augmenting the LLM-based RL agent with the metacognitive capacity of estimating its own competence and LP, MAGELLAN allows the agent to self-organize its curriculum to navigate the goal space. We evaluate MAGELLAN in the little-Zoo environment specifically designed as a carefully controlled experimental setup for commonsense-based generalization of agents in a textual environment. In particular, we study the following scientific questions:
\begin{itemize}
    \item \textbf{Q1.} Given an initial set of language goals, how does MAGELLAN enable the LLM-based online RL agent to estimate its own LP over these goals? How does it compare to more classic methods, which are precise but do not generalize?
    %How does MAGELLAN’s LP estimation compare to more classic methods, which are precise but do not generalize?
    \item \textbf{Q2.} Can MAGELLAN be used by the LLM agent to self-organize an efficient learning curriculum over these goals?
    \item \textbf{Q3.} How well can MAGELLAN predict the agent’s competence and LP on new unseen goals?
    \item \textbf{Q4.} When a set of new language goals are introduced, can MAGELLAN leverage its predictions of LP for these new goals to efficiently prioritize their exploration?
%When facing new tasks to learn, can MAGELLAN be used to prioritize tasks for its agent by generalizing its competence estimation?
\end{itemize}

We show that MAGELLAN not only approximates the LP efficiently but also adapts faster than standard LP estimation methods that need to evaluate the agent on a task to update their estimation. Equipped with MAGELLAN, our LLM-based RL agent successfully orders its curriculum of tasks. More importantly, it is able to estimate the agent's competence accurately on a test set of unseen tasks. Finally, we show this commonsense-based generalization of competence and LP can be used to design a curriculum for the agent when it faces new tasks to learn.

