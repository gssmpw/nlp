% LLM agents and Online RL, conclude on open-endedness+goal composition+large task spaces
Building open-ended learning agents has been a long-standing goal of research communities studying artificial agents \cite{schmidhuber_powerplay_2013}, intrinsic motivation \cite{} or continual learning \cite{}. The recent rise of Large Language Models (LLMs) has shifted the way these communities build agents. Equipped with commonsense knowledge and cultural priors of interestingness \cite{zhang_omni_2024,klissarov_motif_2023,du_guiding_2023}, LLMs are now being extensively used as agents that learn to solve goals in various interactive environments leveraging advanced prompting or Reinforcement Learning (RL) \citep{yao_react_2022,wang_jarvis-1_2023,wang_voyager_2023,abdulhai_lmrl_2023,carta_grounding_2023,wen_reinforcing_2024,zhai_fine-tuning_2024,zhou_archer_2024}. Prior work also showed such LLMs' abilities can be leverage to propose tasks to pursue \cite{wang_voyager_2023,colas_augmenting_2023,pourcel_aces_2024} when considering environments where a large amount of data exist on internet (and by extension in the LLMs' training dataset) on what humans usually achieve. Proposing tasks to achieved and properly scaffolding the agent's learning by organizing a curriculum of tasks to tackle has been shown as key \cite{jiang_learning_2023} when considering building open-ended learning agents in environments offering a large amount of possible tasks to achieve. 

% Scaffolding with Automatic Curriculum Learning => focus on LP method
Notably, the field of Automatic Curriculum Learning (ACL) \cite{portelas_automatic_2019} has gathered various methods automatically designing a learning curriculum for RL agents. Among these methods, Learning-Progress (LP) based methods were shown to be very efficient to drive the agent towards learning as many tasks as it can \cite{baranes_r-iac_2009,portelas_teacher_2019,kanitscheider_multi-task_2021}. They stem from the Learning Progress Hypothesis \cite{kaplan_search_2007}, stating that in some cases, animal and human learners are intrinsically motivated by activities on which they experience progress, that is, an improvement in a competence measure (e.g. prediction error \cite{schmidhuber_curious_1991} or return \cite{portelas_teacher_2019}). Measuring one's own LP has been shown to require advanced metacognition abilities as it involves accurately self-assessing and monitoring one's own competence \cite{abdelghani_guiding_2024,murayama_process_2019,ten_role_2022}.

% Challenges in self-evaluation of competence when considering the space of natural language goals
As highlighted by \citet{kaplan_search_2007}, measuring LP can be challenging, as naively estimating the learner's change of competence on a task involves the learner experiencing twice the exact same task. As a result, estimating the LP over very large task spaces can easily become intractable. While methods such as ALP-GMM \cite{portelas_teacher_2019} proposed solutions to approximate the LP over continuous task spaces, much remains to be done on discrete spaces. 

It is in particular the case with natural language, which has been recently shown as key for open-ended learning agents not only to partition the space of possible tasks but also to imagine and generate new goals \cite{colas_language_2020,colas_vygotskian_2022}. In addition to this, language also offers the clear advantage of being the key vector used by humans to transmit cultural aspects such as values, interestingness or commonsense. It nonetheless remains an open question if an LP estimator over natural language tasks could leverage these cultural aspects to generalize the LP to unseen tasks (e.g. if one knows the agent's competence on walking, it could infer that the agent's competence on running could be slightly worse). Designing such an estimator would greatly facilitate the computation of LP as one would not need the agent to see all the tasks but could rather rely on the generalization abilities of the estimator to infer which tasks the agent should be seeing next based on the tasks it has previously seen.

% Contributions
In this paper, we propose to tackle this generalizable LP estimation by training a model to approximate the LP by using an LLM backbone. When equipped with this new LP approximation, we first empirically study how an existing LP-based ACL method compares to its version using a more classic and precise LP estimation \cite{baranes_r-iac_2009} when scaffolding the curriculum of a simulated learner in an environment with textual tasks. We then equip an adaptation of an existing LLM-based RL agent \cite{carta_grounding_2023} with our new LP estimator and study how such an agent both learning a policy and self-assessing its performance to scaffold its own curriculum can be trained in a textual environment where estimating the LP over the possible textual tasks would benefit from commonsense abilities. We notably propose to analyze how the agents generalizes both its policy and competence estimation on task it has never experienced before. Finally, we go even further by studying how our agent can leverage its generalization abilities on both its policy and LP estimation when asked to learn various new set of goals. In particular, we analyze how the agent adapts when facing new goals similar to the ones it has experienced but also deceptive goals on which its commonsense knowledge may be wrong.

We summarize the scientific questions studied in our paper as follows:
\begin{itemize}
    \item \textbf{Q1.} Can we leverage an LLM to estimate an agent's competence and Learning Progress when the task space is natural language?
    \item \textbf{Q2.} How does a classic ACL method scaffolds a learner's curriculum on textual tasks when equipped with such an LP estimator?
    \item \textbf{Q3.} Can an LLM-based RL agent can at the same time 1) learn a policy with online RL, 2) self-assess and monitor its own competence and 3) scaffold its own curriculum, when equipped with our LP estimator?
    \item \textbf{Q4.} Do the commonsense knowledge of this LLM agent allows it to generalize its competence estimation to unseen goals and how does this estimation compares to its real performance (i.e. its policy's generalization abilities)?
    \item \textbf{Q5.} Can the agent leverage its generalizable LP estimation to scaffold its learning when facing new unseen goals, and how does this estimation adapt when facing deceptive tasks? 
\end{itemize}

We study these in a new set of environments (called the Zoo environments) specifically designed as a carefully controlled experimental setup for commonsense-based generalization of agents in a textual environment. We detail this environment in Section \ref{sec:zoo_env}, highlighting its key advantages and differences over prior work.
We show our LLM-based LP estimator efficiently approximates a learner's competence and results in a classic ACL method achieving similar performance when equipped with this approximation rather than a more precise but not generalizable LP estimator. We then show an LLM-based RL agent can successfully use this LP estimation to monitor and scaffold its own learning but also very efficiently approximate its own performance on new unseen tasks leveraging the commonsense abilities of the LLM backbone. Finally TODO: update with results on training on new goals.

To summarize, this work introduces and studies how an LLM agent can showcase metacognition abilities by not only learning how to solve tasks by interacting with online RL but also self-assess its own performance on tasks it is experiencing and leverage this ability to choose which tasks to train on. Additionally, we show such an agent is also capable of estimating its performance on tasks it has never seen and leverage this approximation to organize a curriculum when facing new tasks without even needing to evaluate its performance on them. Our in-depth analysis of such metacognition and generalization abilities in a carefully-designed and controlled experimental setup opens up for future work leveraging our agent's abilities in open-ended environments where the agent needs to generate its own goals and select which ones to pursue next.

TODO: Also open for future work leveraging the latent representation?