\section{Related Work}
\paragraph{In-context learning.}
In-context learning (ICL) is a task adaptation strategy where a large language model (LLM) is provided with examples or demonstrations within the input prompt to guide its responses for various tasks~\cite{few_shot}. ICL enables LLMs to adapt to unseen tasks by observing a few demonstrations in their input, without requiring additional training data or fine-tuning~\cite{icl_survey}.
Since in-context demonstrations are crucial for ICL performance, various strategies have been proposed to improve demonstration selection, including retrieving semantically similar examples~\cite{LiuSZDCC22}, employing chain-of-thought reasoning~\cite{zero_shot_cot}, and decomposing tasks into subproblems using least-to-most prompting~\cite{ZhouSHWS0SCBLC23}. Additionally, research has shown that the effectiveness of ICL can be significantly improved through fine-tuning~\cite{WeiBZGYLDDL22} or adaptation during pretraining~\cite{metaicl}, even for smaller language models~\cite{SchickS21}. Recent work continues to refine demonstration selection strategies, further enhancing the adaptability and efficiency of in-context learning~\cite{LiuSZDCC22}.


\paragraph{Retrieval-augmented generation.}
Retrieval-augmented generation (RAG) is a context-based strategy that enhances LLMs by dynamically retrieving relevant information from a knowledge base or document corpus~\cite{rag}. Unlike ICL, which relies solely on in-prompt demonstrations, RAG provides external context to guide LLM responses, making it particularly effective for tasks that require up-to-date or domain-specific information where static prompts may be insufficient~\cite{rag_survey}.
RAG can also be integrated with ICL techniques, leveraging both retrieved knowledge and example-based reasoning to improve performance across diverse tasks~\cite{icl_survey}. More recently, several studies have explored the use of graph-aware retrieval mechanisms to improve RAG by providing structured contextual information~\cite{graphrag,gretriever}.



\paragraph{Graph neural networks.}
GNNs are powerful tools designed to operate on graph-structured data~\cite{message_passing}.
They have revolutionized the landscape by introducing the message-passing mechanism, where nodes iteratively aggregate information from their neighbors~\cite{chen2020survey}.
This paradigm has led to the development of several influential GNN architectures.
Popular GNN variants include graph convolutional networks (GCN)\cite{gcn}, graph attention networks (GAT)\cite{gat}, and scalable architectures such as GraphSAGE~\cite{graphsage} and RevGAT~\cite{revgat}, each advancing the message-passing paradigm in distinct ways.
While both GNNs and RAG leverage contextual information beyond the raw input, little work has explored the fundamental connections between them.