\section{Related Work}
\textbf{Generalized Category Discovery (GCD)} draws similarity with novel category discovery (NCD) in both containing labeled and unlabeled images and aiming to discover novel categories in the unlabeled set. Initial GCD method~\cite{vaze2022generalized} learns representations by self-supervised contrastive learning on all data, along with supervised contrastive learning on labeled subset. SimGCD~\cite{wen2023parametric} constructs an effective baseline using parametric classifier. A later variant $\mu$GCD~\cite{vaze2023norep} improves SimGCD by using a teacher network to provide supervision for self-augmented image pairs. More recently, SPTNet~\cite{wang2023sptnet} learns spatial prompts as an alternative to adapt data for better alignment with the model. DCCL~\cite{pu2023dynamic} proposes to mine sample relations by generating dynamic conceptions using improved Infomap clustering~\cite{rosvall2008infomap}, followed by conception and instance-level contrastive learning. Similarly, GPC~\cite{zhao2023learning} also estimates prototypes by Gaussian mixture model and a split-and-merge to take labeled instances into account. PromptCAL~\cite{zhang2023promptcal} improves the ViT backbone by learning auxiliary prompts, as well as affinity propagation on KNN graph to estimate instance relation. Although labeled data is exploited to assist clustering in these methods, it is often taken as a pre- or post-clustering refinement. As such, the potential benefit of labeled instances are not fully exploited.  As a comparison, we fully incorporate the labeled data prior during every step of the association process, empowering reliable association of unlabeled data by taking advantage of the labeled instances as bridges. 



\noindent\textbf{Prototypical Contrastive Learning (PCL).}
In recent years, contrastive learning~\cite{Gutmann2010NCE} has proven as an effective technique for self-supervised learning~\cite{wu2018memory, he2020momentum, Chen2020SimCLR, Li2021pcl} and other settings~\cite{Khosla2020SCL, Wang2021ICS, zhao2023learning}. In particular, prototypical contrastive learning compares instances with a set of prototypes encoding class-specific semantic structure, leading to discriminative embedding space. As such, many vision tasks have exploited PCL for method design. ProtoNCE~\cite{Li2021pcl} combines instance-wise contrastive learning and multi-grained PCL for transfer learning. \cite{ge2020self,Wang2021CAP} adopt iterative clustering based PCL for object re-ID. A few methods~\cite{pu2023dynamic, zhao2023learning} in GCD have also considered PCL to learn discriminative representation. The critical issue for prototypical contrast is how to obtain representative prototypes, which then comes down to designing effective association strategy. Our method also adopts PCL, however, our better utilization of prior and design of semi-supervised association lead to more reliable prototypes, which in turn facilitates learning better representation.


\noindent\textbf{Data Clustering and Association.}
Clustering has long been used as a way to discover potential semantic groups within the data. Unsupervised clustering methods like K-Means~\cite{hartigan1979algorithm}, DBSCAN~\cite{ester1996density} and hierarchical clustering~\cite{johnson1967hierarchical, murtagh2012algorithms} are widely used in many applications~\cite{ge2020self, wang2022hierarchical, pu2023dynamic}. Semi-supervised clustering is also studied in some works~\cite{bair2013semi, bilenko2004integrating}. Basu \textit{et al.}~\cite{basu2002semi} propose constrained K-Means by enforcing that labeled instances are assigned to their own cluster during K-Means iteration. COP-Kmeans~\cite{wagstaff2001} modifies K-Means to make sure no constraints are violated when assigning instances. Constrained DBSCAN~\cite{ruiz2010density} and hierarchical clustering~\cite{davidson2005clustering} are also considered. Metric-based methods~\cite{yin2010semi, klein2002instance, xing2002distance, lange2005learning, pu2023dynamic, zhang2023promptcal} modify the pairwise distance such that two instances with a "must-link" constraint have a lower distance, and those with a "cannot-link" constraint have a larger distance. Our proposed association is also constraint-based, but the constraints are enforced during a threshold-based group merging process, during which new categories are allowed to be discovered.