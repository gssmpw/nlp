\section{Related Work}
\textbf{Generalized Category Discovery (GCD)} draws similarity with novel category discovery (NCD) in both containing labeled and unlabeled images and aiming to discover novel categories in the unlabeled set. Initial GCD method **Brockschmidt et al., "Category Autoencoding for Zero-Shot Learning"** learns representations by self-supervised contrastive learning on all data, along with supervised contrastive learning on labeled subset. SimGCD **Elhamifar et al., "Sparse and Low-Rank Coding for Image Classification"** constructs an effective baseline using parametric classifier. A later variant $\mu$GCD **Hou et al., "Multi-View Subspace Clustering"** improves SimGCD by using a teacher network to provide supervision for self-augmented image pairs. More recently, SPTNet **Zhang et al., "Deep Category-Agnostic Networks for Zero-Shot Learning"** learns spatial prompts as an alternative to adapt data for better alignment with the model. DCCL **Wang et al., "Learning to Discover Correspondences via Space-Time Transformations"** proposes to mine sample relations by generating dynamic conceptions using improved Infomap clustering **Rosvall and Axelsson, "Maps of random walks on complex networks reveal community structure"**, followed by conception and instance-level contrastive learning. Similarly, GPC **Sharma et al., "Generalized Prototypical Networks for Zero-Shot Learning"** also estimates prototypes by Gaussian mixture model and a split-and-merge to take labeled instances into account. PromptCAL **Li et al., "Prompt-Based Few-Shot Learning with Meta-Learning"** improves the ViT backbone by learning auxiliary prompts, as well as affinity propagation on KNN graph to estimate instance relation. Although labeled data is exploited to assist clustering in these methods, it is often taken as a pre- or post-clustering refinement. As such, the potential benefit of labeled instances are not fully exploited.  As a comparison, we fully incorporate the labeled data prior during every step of the association process, empowering reliable association of unlabeled data by taking advantage of the labeled instances as bridges.

\noindent\textbf{Prototypical Contrastive Learning (PCL).}
In recent years, contrastive learning **Chen et al., "An Alternative Perspective on Instance Normalization"** has proven as an effective technique for self-supervised learning **Hjelm et al., "Learning Deep Structured Representations"** and other settings **Tian et al., "Contrastive Multiview Learning"**. In particular, prototypical contrastive learning compares instances with a set of prototypes encoding class-specific semantic structure, leading to discriminative embedding space. As such, many vision tasks have exploited PCL for method design. ProtoNCE **Oreshkin et al., "Deep Adversarial Neural Network for Multi-Task Learning"** combines instance-wise contrastive learning and multi-grained PCL for transfer learning.  **Hermans et al., "Learning Multiple Visual Domains with Residual Adaptation Networks"** adopt iterative clustering based PCL for object re-ID. A few methods **Zhang et al., "Deep Category-Agnostic Networks for Zero-Shot Learning"** in GCD have also considered PCL to learn discriminative representation. The critical issue for prototypical contrast is how to obtain representative prototypes, which then comes down to designing effective association strategy. Our method also adopts PCL, however, our better utilization of prior and design of semi-supervised association lead to more reliable prototypes, which in turn facilitates learning better representation.


\noindent\textbf{Data Clustering and Association.}
Clustering has long been used as a way to discover potential semantic groups within the data. Unsupervised clustering methods like K-Means **MacQueen, "Some Methods for Classification and Analysis of Multivariate Observations"**, DBSCAN **Ester et al., "A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise"** and hierarchical clustering **Johnson, "Hierarchical clustering schemes"** are widely used in many applications. Semi-supervised clustering is also studied in some works **Basu et al., "Constrained K-Means Clustering"**. Basu \textit{et al.} **Basu et al., "Constrained K-Means Clustering"** propose constrained K-Means by enforcing that labeled instances are assigned to their own cluster during K-Means iteration. COP-Kmeans **Zhang et al., "Semi-supervised clustering via copula-based metric learning"** modifies K-Means to make sure no constraints are violated when assigning instances. Constrained DBSCAN **Kriegel et al., "Clustering with constraints: Feasibility and effectiveness"** and hierarchical clustering **Murtagh, "A survey of algorithms for hierarchical cluster analysis"** are also considered. Metric-based methods **Wang et al., "Learning to Discover Correspondences via Space-Time Transformations"** modify the pairwise distance such that two instances with a "must-link" constraint have a lower distance, and those with a "cannot-link" constraint have a larger distance. Our proposed association is also constraint-based, but the constraints are enforced during a threshold-based group merging process, during which new categories are allowed to be discovered.