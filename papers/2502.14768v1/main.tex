\documentclass{article}
\usepackage[nonatbib, final]{nips_style}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{float}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{enumitem}
\usepackage[table,xcdraw]{xcolor}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{svg}
\usepackage{sectsty}
\usepackage{titlesec}
\usepackage{url}
\usepackage{float}
\usepackage{microtype}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage{graphicx}   
\usepackage{CJKutf8}
\usepackage{multirow}
\usepackage{interval}
\intervalconfig{soft open fences}
\usepackage{algorithm, algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{float}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{color}
\usepackage{tcolorbox}
\usepackage{verbatim}
\usepackage{colortbl} 
\usepackage{listings}
\usepackage{makecell}
\usepackage{siunitx}
\usepackage{xcolor}
\definecolor{bg}{RGB}{176,226,255}
\definecolor{bonus_green}{RGB}{0,100,0}
\newcommand{\bbonus}[1]{{\textcolor{bonus_green}{$^{\uparrow#1}$}}}


\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\title{Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning}
%Logic-RL: Analyzing LLM Reasoning Dynamics in Rule-Based Reinforcement Learning
%Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning
%Logic-RL: A Simple and Reproducible Reinforcement Learning Method

\author{
Tian Xie$^{1}$\thanks{~~Work done during internship at MSRA. Open-Source Research Project.} \quad
Zitian Gao$^{2}$ \quad
\textbf{Qingnan Ren}$^{3}$ \quad
Haoming Luo$^{3}$ \quad
\textbf{Yuqian Hong}$^{1}$\samethanks\\
\textbf{Bryan Dai}$^{2}$ \quad
\textbf{Joey Zhou}$^{2}$ \quad
\textbf{Kai Qiu}$^{1}$ \quad
\textbf{Zhirong Wu}$^{1}$ \quad
\textbf{Chong Luo}$^{1}$\thanks{~~Corresponding author.} \vspace{2mm}\\
$^1$Microsoft Research Asia \quad
$^2$Ubiquant \quad
$^3$Independent \vspace{2mm} \\
\texttt{\{unakar666, hmluo65536, hoknight0\}@gmail.com} \\
\texttt{\{v-yuqianhong, kai.qiu, wu.zhirong, chong.luo\}@microsoft.com} \\
\texttt{\{ztgao02, cbdai, jzhou\}@ubiquant.com} \\
}

\begin{document}
\maketitle

% %%%% authorstart
% \author{
% Tian Xie$^{1,4}$\thanks{~~Work done during internship at MSRA.} 
% \quad Zitian Gao$^{2}$ 
% \quad \textbf{Qingnan Ren}$^{3}$
% \quad Haoming Luo$^{3}$ 
% \quad \textbf{Yuqian Hong}$^{1,4}$\\ 

% \textbf{Bryan Dai}$^{2}$
% \quad \textbf{Jay Zhou}$^{2}$ 
% \quad \textbf{Kai Qiu}$^{1}$  
% \quad \textbf{Chong Luo}$^{1}$\thanks{~~Corresponding author.} \\







\vspace{-3mm}

% \begin{abstract}
% DeepSeek R1 has demonstrated impressive reasoning capabilities. However, only the model weights were open-sourced, without any training code or data, making it hard to reproduce its amazing Reinforcement Learning (RL) process. To address this gap, we propose \textbf{Logic-RL}, where we apply rule-based reinforcement learning to train our model. As a result, our model exhibits long-chain reasoning Chain of Thought (CoT) with reflection, verification, checking and summarization steps, even though such patterns were entirely absent from the training corpus. We outperformed xxx by an average of xx.x\% on the knights and knaves logical reasoning dataset using Qwen2.5-7B-Instruct-1M with Logic-RL. We trained our model on a logical reasoning dataset, yet it even achieved a 2x improvement on AIME, demonstrating amazing out-of-distribution generalization ability. Our code is available at \url{https://github.com/Unakar/Logic-RL}.
% \end{abstract}

% DeepSeek-R1's strong reasoning capabilities inspired us to explore the potential of rule-based reinforcement learning (RL) in reasoning tasks. However, the lack of open-sourced training code and data for R1 limits its reproducibility. We introduce \textbf{Logic-RL}, a multi-stage rule-based RL approach. Starting with Qwen2.5-7B-Instruct-1M as baseline, our model develops complex reasoning skills—such as reflection, verification, and summarization—that are entirely absent from our training data. Notably, our model outperforms OpenAI-o1 by an average of 51.9\% on the Knights\&Knaves logical puzzle. Even without mathematical training data, a model trained solely on 5K logic puzzles demonstrates exceptional generalization to challenging benchmarks like AIME. Our work also challenges common assumptions: (i) Increased response length will not definitely lead to improved reasoning performance; (ii) Language mixing often degrades reasoning performance; and (iii) A sudden "aha moment" may not exist. Interestingly, our Logic-RL model generates output patterns highly similar to those of R1, further validating our approach. Given the limited details about R1's implementation, our open-source release of Logic-RL at [\href{https://github.com/Unakar/Logic-RL}{https://github.com/Unakar/Logic-RL}] provides significant value to the research community, promoting transparency and enabling broader exploration of large reasoning models.
\begin{abstract}
% The remarkable performance of DeepSeek-R1 highlights the potential of rule-based reinforcement learning (RL) in large reasoning models. However, existing approaches mainly focus on math tasks, which are not ideal for RL mechanism studies. We generate controllable logic puzzles, which offer adjustable difficulty and easy verification, making them suitable for studying reasoning dynamics. 
% In this work, we propose Logic-RL, a multi-stage rule-based RL approach. Our model develops complex reasoning skills—such as reflection, verification, and summarization—that are entirely absent from logic corpus. Despite being trained on only 5K logic problems without any mathematical data, it generalizes well to challenging benchmarks like AIME.
% Additionally, our findings challenge common assumptions: (i) Longer responses do not always guarantee better reasoning; (ii) Language mixing often degrades reasoning capabilities; (iii) The notion of a sudden "aha moment" in training progression may be overstated.

% The remarkable performance of DeepSeek-R1 highlights the potential of rule-based reinforcement learning (RL) in large reasoning models. Inspired by this, we introduce \textbf{Logic-RL}, a multi-stage rule-based RL approach.
% It refines the chain-of-thought process, helping the model identify and correct errors, break down complex tasks into manageable parts, and explore alternative solution paths when an approach fails, form preliminary conclusions and them verify them—even though these capabilities are not present in our training puzzle corpus.
% Our model generalizes remarkably well to challenging benchmarks like AIME, even though it was trained on only 5K logic problems without any mathematical data.
% Additionally, our findings challenge common assumptions:
% (i) Longer responses do not always guarantee better reasoning.
% (ii) Language mixing phenemona often degrades reasoning capabilities
% (iii) The notion of a sudden "aha moment" in training progression may be overstated.

Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. 
To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification.
We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence.
% and a tempering-annealing strategy that balances exploration and exploitation.
Our 7B model develops advanced reasoning skills—such as reflection, verification, and summarization—that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC.



\begin{figure}[H]
\centering
% \begin{minipage}{0.8\linewidth}
    \centering
    \hspace{-2mm}{
    \includegraphics[width=1\linewidth]{figure/teaser.pdf}}
\caption{Validation accuracy and mean response length during RL training, illustrating how the model autonomously learns to allocate more thinking compute for improved performance. Remarkably, the model also demonstrates impressive generalization on completely unseen datasets (AIME, AMC).}
    \label{fig:len}
% \end{minipage}
\end{figure}
\end{abstract}



\input{section/intro}
\input{section/method}
\input{section/exp_adjust}
\input{section/discussion}
%\input{section/conclusion}

\bibliography{references}{}
\bibliographystyle{plain}

\renewcommand{\thesubsection}{\Alph{subsection}}
\section*{Appendix}
\setcounter{subsection}{0}
\input{section/appendix}

\end{document}
