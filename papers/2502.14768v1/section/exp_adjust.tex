\section{Experiment}
\label{main}
\vspace{-3mm}
We began by experimenting with various models from the Qwen2.5 series as potential baseline candidates. For instance, \texttt{Qwen2.5-Math-7B} exhibited a strong tendency to generate Python code blocks, which often conflicted with our strict formatting requirements. Despite efforts to mitigate this behavior by removing system prompts and penalizing specific markdown styles, it remained challenging to fully address.

Additionally, we tested both \texttt{Qwen2.5-7B-Base} and \texttt{Qwen2.5-7B-Instruct} as starting points. Surprisingly, we found that the base and instruct models displayed nearly identical training metrics during RL training, including validation accuracy, response length growth curves, and reward curves. A detailed comparison between base \& instruct model can be found in the appendix \ref{base_comp}. However, the instruct model demonstrated slightly higher test accuracy, making it the preferred choice. Consequently, we selected \textbf{Qwen2.5-7B-Instruct-1M}~\cite{qwen251m} as our baseline. See more base \& instruct model training dynamics comparision in Appendix \ref{fig:base_length}

Notably, despite the training dataset being limited to 3 to 7-person K\&K logic puzzles—with fewer than 5,000 synthetic samples—the model demonstrates a remarkable ability to generalize to out-of-distribution (OOD) scenarios, such as 8-person puzzles.

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \Large
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}l *{8}{c} @{}}
    \toprule
    \multirow{3}{*}{\centering\textbf{Model}} & \multicolumn{6}{c}{\textbf{Difficulty by Number of People}} \\
    \cmidrule(r){2-8}
     & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{\multirow{2}{*}[+2.5ex]{Avg.}} \\
    \midrule

    \textbf{o3-mini-high} & 0.99 & 0.98 & 0.97 & 0.95 & 0.94 & 0.89 & 0.83 & 
 0.94 \\
    \textbf{o1-2024-12-17} & 0.83 & 0.51 & 0.38 & 0.38 & 0.35 & 0.30 & 0.20 & 0.42\\
    \textbf{Deepseek-R1} & 0.91 & 0.73 & 0.77 & 0.78 & 0.75 & 0.88 & 0.83 & 0.81\\
    \midrule
    \textbf{GPT-4o} & 0.68 & 0.57 & 0.49 & 0.32 & 0.23 & 0.21 & 0.11 & 0.37\\
    \textbf{GPT-4o-mini} & 0.63 & 0.42 & 0.34 & 0.17 & 0.09 & 0.10 & 0.01 & 0.25\\
    \textbf{NuminaMath-7B-CoT} & 0.28 & 0.13 & 0.12 & 0.05 & 0.01 & 0.00 & 0.00 & 0.08 \\
    \textbf{Deepseek-Math-7B} & 0.35 & 0.21 & 0.08 & 0.06 & 0.02 & 0.00 & 0.00 & 0.10\\
    \textbf{Qwen2.5-Base-7B} & 0.41 & 0.34 & 0.16 & 0.09 & 0.00 & 0.00 & 0.00 & 0.14 \\
    \midrule
    \textbf{Qwen2.5-7B-Instruct-1M} & 0.49 & 0.40 & 0.25 & 0.11 & 0.06 & 0.02 & 0.01 & 0.19 \\
    \rowcolor{bg!70}
    + \textbf{Logic-RL} & 0.99\bbonus{0.50} & 0.99\bbonus{0.59} & 0.94\bbonus{0.69} & 0.92\bbonus{0.81} & 0.91\bbonus{0.85} & 0.80\bbonus{0.78} & 0.67\bbonus{0.48} & 0.89\bbonus{0.70}\\
    \bottomrule
    \end{tabular}
    }
    \vspace{4mm}
    \caption{\centering Comparison of different models including reasoning models and general models on K\&K logic puzzle across various difficulty.}
    \label{tab:distill_vs_rl}
\end{table}

Compared to the initial average length of 500 tokens, after 1k steps of RL, the output has almost linearly and steadily increased to 2000 tokens, a significant increase of 4 times. As the response length increases, the model begins to exhibit more complex behaviors, such as reflection and exploration of alternative solutions. These behaviors emerge naturally, with no related data in our training set, and enhance the model's ability to handle more complex tasks. These phenomena align closely with the results of R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}. We will further discuss this in Section~\ref{rq3}.



\vspace{-2mm}



\section{Research Questions}

\subsection*{RQ 1: How Does GRPO Compare to Other RL Algorithms?}
\label{rl_algos}

\emph{Does GRPO\cite{grpo} outperform other reinforcement learning algorithms, such as REINFORCE++ and PPO, in terms of training stability, speed, and performance accuracy?}

% To ensure a fair comparison, we applied 3 algorithms solely to the advantage estimator while keeping all other components consistent. Additionally, we made minor modifications to the original implementations of PPO and REINFORCE++, following recommendations from Deepseek-Math~\cite{grpo}. These small adjustments were primarily motivated by considerations of efficiency and computational accuracy

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figure/rq1.png}
    \caption{Comparison of GRPO (Blue), REINFORCE++ (Red), and PPO (Green) performance (averaged by sliding window = 50) in terms of training speed, accuracy, and reward gain.}
    \label{fig:rl}
\end{figure}


The results indicate that PPO~\cite{ppo} achieved significant advantages in both accuracy and reward. However, it was 138\% slower than REINFORCE++ in terms of training speed.
On the other hand, REINFORCE++~\cite{rpp} demonstrated superior stability, performance gains, and training efficiency compared to GRPO. Overall, REINFORCE++ outperformed GRPO across nearly all metrics, with \textbf{GRPO exhibiting the weakest performance among the three reinforcement learning algorithms evaluated in our experiments.}

\subsection*{RQ 2. Do certain thinking tokens and language-mixing phenemona improve reasoning?}
\emph{Does the inclusion of complex reasoning behaviours (such as exploration, verification, summarization, and backtracking) and language switching improve the model’s reasoning ability?}

\begin{figure}[H]
    \hspace{-0.5mm}\includegraphics[width=1\linewidth]{figure/rq7.png}
    \caption{Impact of complex reasoning behaviours and language mixing on reasoning performance. We analyzed the model's answer rewards for responses containing the tokens shown in the figure. Responses with "verify" and "re-evaluate" scored significantly higher than those without these words. Conversely, responses containing certain tokens from other languages generally received lower scores.}
\end{figure}

We observe that in our experiments:
\begin{enumerate}[leftmargin=*]
    \item Language mixing significantly decreases reasoning ability. 
    % When Chinese or French words appear in responses to English queries, the model struggles to achieve a positive format score, let alone a positive answer reward.
    \item  While terms like "wait," "verify," "yet," and "re-evaluate" show significant improvement, not all complex thinking tokens enhance reasoning ability, as exemplified by "recheck."
    \item The complex reasoning behaviour “recheck” markedly diminishes reasoning ability, likely because its use signals the model’s uncertainty about its answer.
    \item There’s a clear difference between "re-evaluate" and "reevaluate": the former leads to much higher answer scores, while the latter lowers them. When we checked its origin responses, "reevaluate" almost never appeared, while "re-evaluate" showed up frequently. This may suggest the model is more comfortable with words it has seen more often in pretrain corpus.
\end{enumerate}

\subsection*{RQ 3: Does an 'Aha Moment' Emerge During Training?}
\emph{Is there an observable 'Aha moment' where the model exhibits a significant leap in reasoning capability, such as the emergence of multi-step verification or reflection during the RL process?}

The emergence of sophisticated behaviors becomes increasingly evident as performance grows. These behaviors include reflective actions—where the model revisits and reevaluates prior steps—and the spontaneous exploration of alternative problem-solving strategies. Such behaviors are not explicitly planted into training corpus but emerge organically through the model's interaction with the reinforcement learning environment, consistent with findings by Ye et al.~\cite{ye2024physicslanguagemodels21}.

The "aha moment" referenced in the R1 report~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} primarily refers to the model's sudden acquisition of complex reasoning behaviors. A secondary interpretation involves the model spontaneously verbalizing "aha moment," such as in phrases like "Wait, wait. Wait. That’s an aha moment I can flag here." While our model did not exhibit this specific verbalization, Figure~\ref{fig:keywords} shows that it displayed some complex reasoning behaviors (e.g., self-reflection, exploration, verification, summarization) even by step 10.

Thus, we conclude that \textbf{the RL process likely lacks a sudden "aha moment"}—that is, complex reasoning behaviors do not abruptly emerge at a specific training step, aligned with Liu et al.~\cite{liu2025oatzero}.
\begin{figure}[H]
    \centering
    \captionsetup[subfigure]{labelformat=simple, labelsep=colon}
    
    % 第一行：三个子图（顶部对齐）
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/verify.png}
        \subcaption{Verify}
        \label{fig:verify}
    \end{subfigure}
    \hspace{-0.01\textwidth} % 减少水平间距
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/re-evaluate.png}
        \subcaption{Re-evaluate}
        \label{fig:reevaluate}
    \end{subfigure}
    \hspace{-0.01\textwidth}
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/check.png}
        \subcaption{Check}
        \label{fig:check}
    \end{subfigure}
    
    \vspace{0.52em} % 减少垂直间距
    
    % 第二行：三个子图（顶部对齐）
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/yet.png}
        \subcaption{Yet}
        \label{fig:yet}
    \end{subfigure}
    \hspace{-0.01\textwidth}
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/lets.png}
        \subcaption{Let's}
        \label{fig:Let's}
    \end{subfigure}
    \hspace{-0.01\textwidth}
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/chinese_yes.png}
        \subcaption{Chinese word}
        \label{fig:shi}
    \end{subfigure}
    
    \caption{Tracking the frequency of words in the first 1,800 training steps. \textbf{1.} Reflective words like "check" and "verify" slowly increased (a)-(c). \textbf{2.} Conversational phrases (e.g., "Let’s") and cautious terms (e.g., "yet") became more frequent  (d)-(e), \textbf{3.} Chinese words began appearing in English responses (f). The frequency of all these words developed steadily without sudden jumps,  suggesting that there may not be a distinct "aha moment."}
    \label{fig:keywords}
\end{figure}





\subsection*{RQ 4: Can the Model Generalize to Out-of-Distribution (OOD) Tasks?}  
\emph{To what extent can the trained model handle tasks that differ from its training data, particularly those that are out-of-distribution?}

\begin{figure}[H] 
\centering
\begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/aime.pdf} 
    \label{fig:aime}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/amc.pdf} 
    \label{fig:amc}
\end{minipage}
\vspace{-4mm}
\caption{Training Step vs. Accuracy on AIME (2021-2024) and AMC (2022-2023) Datasets.}
\label{fig:ood}
\end{figure}
\vspace{-2mm}

The ability of a model to generalize beyond its training distribution is a cornerstone of AI research. We investigate whether the complex reasoning abilities—such as exploration, verification, summarization, and backtracking—that naturally emerged during RL process can transfer to a highly challenging mathematical reasoning scenario, which we term \emph{Super OOD} .

This evaluation leverages the widely-adopted AIME 2021–2024 (American Invitational Mathematics Examination) and AMC 2022–2023 (American Mathematics Competitions) benchmarks, both known for their rigorous and diverse problem sets.

From Figure~\ref{fig:ood}, we observe that the model's Super OOD generalization capability is exceptionally strong, achieving an overall improvement of 125\% on the AIME dataset and 38\% on the AMC dataset. This synchronous improvement indicates that the reinforcement learning process not only enhances the model's performance on in-distribution tasks but also facilitates the emergence of robust and transferable reasoning strategies.

These findings show that the reasoning skills learned during RL training go beyond the specific patterns of the K\&K dataset. This highlights RL's potential to generalize beyond their training environment.


\vspace{-3mm}





\subsection*{RQ 5: Which Generalizes Better, SFT or RL?}

\emph{Can post-training methods achieve more than just superficial alignment, which just learns  format patterns? Can SFT or RL actually learn to learn, effectively generalizing to other domains?}


We investigate whether models merely memorize training data or truly learn reasoning skills. Following the setup in \cite{memllm}, we test this by comparing performance on familiar problems versus slightly altered ones.Two signs of memorization:  High accuracy on seen problems, and low accuracy on slightly perturbed versions. So, we measure these using two metrics, denote model as $f$, dataset as $\mathcal{D}$.
\begin{enumerate}[leftmargin=*]
    \item \textbf{Accuracy on Observed Problems}: $\mathrm{Acc}(f; \mathcal{D})$, the model's accuracy on the training set ($\mathrm{Tr}$). 
    \item \textbf{Consistency Ratio}: $\mathrm{CR}(f; \mathcal{D})$, the ratio of correct solutions after small changes (perturbations) to those solved without changes. Perturbations preserve the puzzle's core principle and difficulty.
\end{enumerate}
The \textbf{Local Inconsistency-based Memorization Score} is defined as:
\[
\mathrm{LiMem}(f; \mathcal{D}) = \mathrm{Acc}(f; \mathcal{D}) \cdot (1 - \mathrm{CR}(f; \mathcal{D}))
\]
This score captures both memorization and sensitivity to changes. If a model's performance drops significantly when the problem format is altered, it likely hasn't learned the true reasoning skills required to solve similar but modified puzzles.We study two types of perturbations: (i) changing one person's statement to another bool logic expression, and (ii) reordering the statements between different people. Examples of perturbations are as follows:
 
\begin{tcolorbox}[
    colframe=purple!70!black, % 边框颜色：深紫色带一点黑色
    colback=purple!10!white, % 背景颜色：浅紫色
    coltitle=white, % 标题文字颜色：白色
    fonttitle=\bfseries, % 标题字体加粗
    title=Perturbation Examples\label{long_open_q}, % 标题内容
    % attach boxed title to top left={ % 将标题放置在左上角
    %     yshift=-2mm, % 垂直偏移
    %     xshift=2mm % 水平偏移
    % },
    % boxed title style={ % 标题框样式
    %     colframe=purple!70!black, % 标题框边框颜色
    %     colback=purple!70!black % 标题框背景颜色
    % },
    sharp corners, % 圆角设置为锐角
    boxrule=0.5mm, % 边框宽度
    % drop fuzzy shadow=gray!30 % 添加柔和阴影
]
\textbf{Original Problem}: \\
Zoey remarked, "Oliver is not a knight". Oliver stated, "Oliver is a knight if and only if Zoey is a knave". So who is a knight and who is a knave?
\\
\textbf{Statement Perturbation}: \\
Zoey remarked, "Oliver is a knight or Zoey is a knight". Oliver stated, "Oliver is a knight if and only if Zoey is a knave"

\textbf{Reorder Perturbation}: \\
Oliver stated, "Oliver is a knight if and only if Zoey is a knave". Zoey remarked, "Oliver is not a knight"

\end{tcolorbox}

% \begin{equation*}
% \mathcal{J}_{RFT}(\theta)= \mathbb{E}[q \sim P_{sft}(Q), o \sim \pi_{sft}(O|q)]\left( \frac{1}{|o|}\sum_{t=1}^{|o|} \mathbb{I}(o) \log \pi_\theta(o_{t} | q, o_{<t})\right).
% \end{equation*}

% \begin{figure}[H]
% \centering
% \includegraphics[width=1\linewidth]{figure/limem_rl.jpg}
% \caption{Accuracy and memorization scores of the model after reinforcement learning on datasets with 3, 5, and 7 people.}
% \label{fig:rq4_rl}
% \end{figure}


While it is challenging to collect a suitable SFT dataset, we employ reject sampling to gather ground truth data, referred to as the RFT method.


To explore which training paradigm offers better generalization performance, we apply two types of disturbances on training dataset, then compare RFT and RL with \textbf{Test acc - Mem Score curve}. For RFT settings, we use Reject Sampling on origin model, then use a rule-based Best-of-N method to collect the correct yet the shortest response for further fine-tune. As a result, we obtained the curve shown in Fig. 6, which illustrates the changes in unseen test accuracy and training dataset memorization over training process.



\begin{figure}[H]
\centering
\captionsetup[subfigure]{labelformat=simple, labelsep=colon}
% 左子图
\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth, keepaspectratio]{figure/limem_rl32.png} % 左子图文件名
    \subcaption{RL}
    \label{fig:rq4_rl}
\end{subfigure}
\hfill
% 右子图
\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth, keepaspectratio]{figure/limem_rft32.png} % 右子图文件名
    \subcaption{RFT}
    \label{fig:rq4_rft}
\end{subfigure}
% 主标题
\caption{\textbf{RFT memorizes while RL generalizes.} \textbf{RFT} (Reject sampling Fine-Tuning) slightly improves test accuracy at the expense of rapidly increasing $LiMem(f;Tr)$, indicating it mainly learns superficial answer format than geniue reasoning. In contrast, \textbf{RL}  achieves higher test accuracy with minimal or even negative increase in $LiMem(f;Tr)$. Within the same $LiMem$ interval, RL outperform RFT in test acc greatly, suggesting better generalization ability. }
\label{fig:rq4_rl}
\end{figure}


A higher memorization score indicates that more questions, which were originally correct, have turned incorrect due to the disturbances caused by the training, reflecting a greater degree of memorization. SFT tends to superficial alignment, often becoming overly reliant on the original data's expression format. On the other hand, RL encourages the model to explore independently, fostering generalization abilities that stem from enhanced reasoning capabilities, consistent with findings in \cite{chu2025sftmemorizesrlgeneralizes}.

\subsection*{RQ 6: Is Curriculum Learning Still Necessary in RL?}
\emph{Does curriculum learning still matter in the RL paradigm? Specifically, is the sequential order of data important, or is it merely the curation ratio that matters?}
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{figure/rq5_Curriculum_Test_Score_Result}
\caption{Comparison of test scores for curriculum learning and mixed-difficulty training. The plot uses a rolling average (window size = 5)}
\label{fig:rq5_curriculum}
\end{figure}
% To evaluate the necessity of curriculum learning in our framework, we compare its effectiveness against a mixed-difficulty training approach. This comparison aims to verify whether the theoretically appealing concept of curriculum learning—gradually exposing models to increasingly complex tasks—provides tangible benefits over directly exposing them to a diverse range of difficulty levels. In our implementation, the curriculum learning strategy sequentially trains the model on datasets ranging from 3-7 people scenarios for one epoch each, with each training set representing an ascending level of difficulty. In contrast, the mixed-difficulty approach trains the model simultaneously on all difficulty levels (3–7 people) within a single epoch. All other hyperparameters are kept identical between the two training regimes.

To evaluate the necessity of curriculum learning, we compare its effectiveness to a mixed-difficulty approach. In curriculum learning, the model is trained sequentially on progressively more difficult datasets (3-7 people scenarios) for one epoch each. In contrast, the mixed-difficulty approach trains the model on all difficulty levels simultaneously within a single epoch. All other hyperparameters are kept constant between the two methods.



We analyze the test score trajectories using a rolling average (window size = 5) to mitigate stochastic fluctuations and highlight underlying trends. The results in Figure \ref{fig:rq5_curriculum} indicate that curriculum learning yields slightly higher test scores during intermediate training phases. However, this advantage diminishes in practical significance, as the performance difference during early training stages remains statistically negligible, suggesting a limited impact on initial convergence. While curriculum learning may offer a marginal theoretical benefit in terms of sample efficiency, its practical necessity is not conclusively supported, given the minimal real-world performance difference and the added complexity of staged training.



\subsection*{RQ 7: Does Longer Response Length Guarantee Better Reasoning?}
\label{rq3}

\emph{Does an increase in response length during training directly improve a model's reasoning performance, or are these trends merely correlated?}

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{figure/rq3.png}
\caption{Comparison of response length, validation accuracy, and mean reward across training steps for positive and negative example models.}
\label{fig:rq3}
\end{figure}

This experiment investigates whether an increase in response length during training causally enhances reasoning performance. We compared two models trained using the same algorithm and base model but with different hyperparameters and dataset difficulties.

\begin{itemize}[leftmargin=*]
\item \textbf{Positive Example Model (Blue):} Despite a slight decrease in response length over time, this model demonstrated significant improvements in both validation accuracy and reward, indicating stronger reasoning and generalization abilities.

\item \textbf{Negative Example Model (Red):} This model consistently increased its response length but showed no improvement in validation accuracy or reward. This suggests that increasing response length alone does not necessarily enhance reasoning capabilities.
\end{itemize}

Figure \ref{fig:rq3} illustrates these findings: the positive model's reward and accuracy improved while its response length decreased, whereas the negative model's length increased without any corresponding performance gains. These divergent trends suggest that changes in response length are likely a byproduct of training dynamics rather than a causal driver of reasoning improvements.

The observed increase in response length is likely a side effect of reinforcement learning (RL) dynamics. While some studies report a natural tendency for output length to grow as models generate more complex responses, this growth should be viewed as a correlate rather than a direct cause of improved reasoning. Importantly, there is no statistically significant evidence that the magnitude of length increase reliably predicts proportional gains in reasoning performance.

In conclusion, \textbf{longer responses do not always guarantee better reasoning.} Enhanced reasoning capabilities may naturally lead to more detailed explanations, but artificially extending response length does not necessarily result in proportional performance improvements.

