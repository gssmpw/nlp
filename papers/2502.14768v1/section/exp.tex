% \section{Experiment}
% \label{main}
% \vspace{-3mm}
% We initially experimented with various models from the Qwen2.5 series as baseline candidates. For instance, \texttt{Qwen-Math-7B} demonstrated a strong tendency to generate Python code blocks, which frequently conflicted with our strict formatting requirements. Despite attempts to mitigate this behavior by removing system prompts and penalizing specific markdown styles, it proved difficult to fully resolve. Consequently, we chose \textbf{Qwen2.5-7B-Instruct-1M}~\cite{qwen251m} for its superior instruction-following capabilities.

% Notably, despite the training dataset being limited to 3 to 7-person K\&K logic puzzles—with fewer than 5,000 synthetic samples—the model demonstrates a remarkable ability to generalize to out-of-distribution (OOD) scenarios, such as 8-person puzzles.

% % \begin{table}[H]
% % \setlength{\tabcolsep}{10pt} 
% % \centering
% % \resizebox{1.00\textwidth}{!}{
% % \begin{tabular}{llccccccc}
% % \toprule
% % \multicolumn{1}{c}{} & \multicolumn{1}{c}{\multirow{2}{*}[-0.5ex]{Models}} & \multicolumn{7}{c}{\textbf{Steps}} \\
% % \cmidrule(r){3-8}
% %  & & \multicolumn{1}{c}{Step 2} & \multicolumn{1}{c}{Step 4} & \multicolumn{1}{c}{Step 6} & \multicolumn{1}{c}{Step 8} & \multicolumn{1}{c}{Step 10} & \multicolumn{1}{c}{Step 12} & \multicolumn{1}{c}{\multirow{2}{*}[+2.5ex]{Avg.}} \\
% % \midrule

% % & \multirow{2}{*}{\makecell[l]{Llama-3.1-405B}}
% %  & 0.8108 & 0.6579 & 0.5931 & 0.5105 & 0.4272 & 0.3611 & 0.5482 \\ 
% %  & & 0.7838 & 0.8553 & 0.6483 & 0.4266 & 0.5049 & 0.4167 & 0.5852 \\ 
% % \cmidrule{2-9}

% % & \multirow{2}{*}{\makecell[l]{Llama-3.1-405B}}
% %  & 0.8108 & 0.6579 & 0.5931 & 0.5105 & 0.4272 & 0.3611 & 0.5482 \\ 
% %  & & 0.7838 & 0.8553 & 0.6483 & 0.4266 & 0.5049 & 0.4167 & 0.5852 \\ 
% % \cmidrule{2-9}

% % & \multirow{2}{*}{\makecell[l]{Llama-3.1-405B}}
% %  & 0.8108 & 0.6579 & 0.5931 & 0.5105 & 0.4272 & 0.3611 & 0.5482 \\ 
% %  & & 0.7838 & 0.8553 & 0.6483 & 0.4266 & 0.5049 & 0.4167 & 0.5852 \\ 
% % \cmidrule{2-9}

% % & \multirow{2}{*}{\makecell[l]{o1-mini}}
% %  & 0.9730 & 0.7368 & 0.5103 & 0.3846 & 0.3883 & 0.1944 & 0.4463 \\ 
% %  & & 0.9459 & 0.8026 & 0.6276 & 0.3497 & 0.3301 & 0.2222 & 0.5167 \\
% % \cmidrule{2-9}

% % & \multirow{2}{*}{\makecell[l]{GPT-4o}}
% %  & 0.5405 & 0.4868 & 0.3241 & 0.1818 & 0.1165 & 0.0556 & 0.2666 \\ 
% %  & & 0.5135 & 0.6579 & 0.6000 & 0.2797 & 0.3010 & 0.3611 & 0.4444 \\
% % \cmidrule{2-9}


% % % & \multirow{3}{*}{\makecell[l]{Llama-3.1-70B}} 
% % % & 0.5405 & 0.4868 & 0.4069 & 0.2238 & 0.2913 & 0.2174 & 0.3441 \\ 
% % % & & 1.0000 & 0.9605 & 0.8000 & 0.4336 & 0.2039 & 0.1111 & 0.5796 \\
% % % & & \cellcolor{gray!20} \hspace{-1.8mm} 1.0000 & \cellcolor{gray!20} \hspace{-1.8mm} 0.9737 & \cellcolor{gray!20} \hspace{-1.8mm} 0.7724 & \cellcolor{gray!20} \hspace{-1.8mm} 0.4503 & \cellcolor{gray!20} \hspace{-1.8mm} 0.3010 & \cellcolor{gray!20} \hspace{-1.8mm} 0.1944 & \cellcolor{gray!20} \hspace{-1.8mm} 0.6026 \\
% % % \cmidrule{2-9}

% % & \multirow{2}{*}{\makecell[l]{GPT-4o}}
% %  & 0.5405 & 0.4868 & 0.3241 & 0.1818 & 0.1165 & 0.0556 & 0.2666 \\ 
% %  & & 0.5135 & 0.6579 & 0.6000 & 0.2797 & 0.3010 & 0.3611 & 0.4444 \\
% % \cmidrule{2-9}

% % \midrule[1pt]
% % \addlinespace[3pt]
% % \end{tabular}}
% % \vspace{4mm}
% % \caption{Accuracy of various reasoning methods and models across steps and difficulty modes on the Blocksworld multi-step reasoning dataset.}
% % \label{table:1}
% % \end{table}
% \vspace{-3mm}
% \begin{table}[h]
%     \centering
%     \renewcommand{\arraystretch}{1.3}
%     \Large
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{@{}l *{8}{c} @{}}
%     \toprule
%     \multirow{3}{*}{\centering\textbf{Model}} & \multicolumn{6}{c}{\textbf{Difficulty by Number of People}} \\
%     \cmidrule(r){2-8}
%      & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{\multirow{2}{*}[+2.5ex]{Avg.}} \\
%     \midrule

%     \textbf{o3-mini-high} & 0.99 & 0.98 & 0.97 & 0.95 & 0.94 & 0.89 & 0.83 & 
%  0.94 \\
%     \textbf{o1-2024-12-17} & 0.83 & 0.51 & 0.38 & 0.38 & 0.35 & 0.30 & 0.20 & 0.42\\
%     \textbf{R1} & 0.91 & 0.73 & 0.77 & 0.78 & 0.75 & 0.88 & 0.83 & 0.81\\
%     \midrule
%     \textbf{GPT-4o} & 0.68 & 0.57 & 0.49 & 0.32 & 0.23 & 0.21 & 0.11 & 0.37\\
%     \textbf{GPT-4o-mini} & 0.63 & 0.42 & 0.34 & 0.17 & 0.09 & 0.10 & 0.01 & 0.25\\
%     % \textbf{Qwen2.5-Math-7B} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
%     \textbf{Deepseek-Math-7B} & 0.35 & 0.21 & 0.08 & 0.06 & 0.02 & 0.00 & 0.00 & 0.10\\
%     \textbf{NuminaMath-7B-CoT} & 0.28 & 0.13 & 0.12 & 0.05 & 0.01 & 0.00 & 0.00 & 0.08 \\
%     \midrule
%     \textbf{Qwen2.5-7B-Instruct-1M} & 0.49 & 0.40 & 0.25 & 0.11 & 0.06 & 0.02 & 0.01 & 0.19 \\
%     \rowcolor{bg!70}
%     + \textbf{Logic-RL} & 0.83\bbonus{0.34} & 0.88\bbonus{0.48} & 0.87\bbonus{0.62} & 0.84\bbonus{0.73} & 0.71\bbonus{0.65} & 0.67\bbonus{0.65} & 0.65\bbonus{0.64} & 0.78\bbonus{0.59}\\
%     \bottomrule
%     \end{tabular}
%     }
%     \vspace{4mm}
%     \caption{\centering Comparison of different models including reasoning models and general models on K\&K logic reasoning dataset across various number of people}
%     \label{tab:distill_vs_rl}
% \end{table}

% \vspace{-2mm}
% \begin{figure}[H]
%     \centering
%     \hspace{-10mm}
%     \includegraphics[width=0.9\linewidth]{figure/training_metrics.png}
%     \caption{Validation Accuracy and Mean Response Length of during RL by training steps, showing a trend where the model automatically learns to allocate more thinking time for better performance. }
%     \label{fig:len}
% \end{figure}
% \vspace{-2mm}

% Compared to the initial average length of 500 tokens, after 1k steps of RL, the output has almost linearly and steadily increased to 2000 tokens, a significant increase of 4 times. As the response length increases, the model begins to exhibit more complex behaviors, such as reflection and exploration of alternative solutions. These behaviors emerge naturally, with no related data in our training set, and enhance the model's ability to handle more complex tasks. These phenomena align closely with the results of R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}. We will further discuss this in Section~\ref{rq3}.



% \section{Research Questions}
% \subsection*{RQ 1: Which RL Algorithm Yields the Most Stable, Fastest, and Most Generalizable Model?}
% \label{rl_algos}
% \emph{Does the GRPO outperform other reinforcement learning algorithms, such as REINFORCE++ and PPO, in terms of training stability, speed, and the reasoning of the trained model?}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1\linewidth]{figure/rq1.png}
%     \caption{Performance comparison of GRPO, REINFORCE++ and PPO across different metrics over training steps}
%     \label{fig:rl}
% \end{figure}

% To investigate this question, we applied 3 algorithms using the same training data, base model and shared training hyperparameters. Our experimental results are summarized as follows:

% \begin{enumerate}[leftmargin=*]
% \item PPO~\cite{ppo} demonstrated significant advantages in both accuracy and reward. However, it was 138\% slower than REINFORCE++ in terms of training speed.

% \item REINFORCE++~\cite{rpp} exhibited greater stability, performance gains, and training efficiency compared to GRPO. Additionally, it achieved higher reward increases per unit of KL divergence consumption, aligned with previous findings.

% \item Overall, REINFORCE++ outperformed GRPO across nearly all metrics, with \textbf{GRPO showing the weakest performance among the three reinforcement learning algorithms in our experiments.}
% \end{enumerate}

% \subsection*{RQ 2: Does an 'Aha Moment' Emerge During Training?}
% \emph{Is there an observable 'Aha moment' where the model exhibits a significant leap in reasoning capability, such as the emergence of multi-step verification or reflection during the RL process?}

% \begin{figure}[H]
%     \centering
%     \captionsetup[subfigure]{labelformat=simple, labelsep=colon}
    
%     % 第一行：三个子图（顶部对齐）
%     \begin{subfigure}[t]{0.3\textwidth}
%         \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/verify.png}
%         \subcaption{Verify}
%         \label{fig:verify}
%     \end{subfigure}
%     \hspace{-0.01\textwidth} % 减少水平间距
%     \begin{subfigure}[t]{0.3\textwidth}
%         \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/re-evaluate.png}
%         \subcaption{Re-evaluate}
%         \label{fig:reevaluate}
%     \end{subfigure}
%     \hspace{-0.01\textwidth}
%     \begin{subfigure}[t]{0.3\textwidth}
%         \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/check.png}
%         \subcaption{Check}
%         \label{fig:check}
%     \end{subfigure}
    
%     \vspace{0.52em} % 减少垂直间距
    
%     % 第二行：三个子图（顶部对齐）
%     \begin{subfigure}[t]{0.3\textwidth}
%         \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/yet.png}
%         \subcaption{Yet}
%         \label{fig:yet}
%     \end{subfigure}
%     \hspace{-0.01\textwidth}
%     \begin{subfigure}[t]{0.3\textwidth}
%         \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/let's.png}
%         \subcaption{Let's}
%         \label{fig:Let's}
%     \end{subfigure}
%     \hspace{-0.01\textwidth}
%     \begin{subfigure}[t]{0.3\textwidth}
%         \includegraphics[width=\textwidth, keepaspectratio]{figure/word_frequency_charts/是.png}
%         \subcaption{A common Chinese word}
%         \label{fig:shi}
%     \end{subfigure}
    
%     \caption{Tracking the frequency of words in the first 1,800 training steps. \textbf{1.} Reflective words like "check" and "verify" slowly increased (a)-(c). \textbf{2.} Conversational phrases (e.g., "Let’s") and cautious terms (e.g., "yet") became more frequent  (d)-(e), \textbf{3.} Chinese words began appearing in English responses (f). The frequency of all these words developed steadily without sudden jumps,  suggesting that there may not be a distinct "aha moment."}
%     \label{fig:keywords}
% \end{figure}


% The emergence of sophisticated behaviors becomes increasingly evident as performance grows. These behaviors include reflective actions—where the model revisits and reevaluates prior steps—and the spontaneous exploration of alternative problem-solving strategies. Such behaviors are not explicitly planted into training corpus but emerge organically through the model's interaction with the reinforcement learning environment, consistent with findings by Ye et al.~\cite{ye2024physicslanguagemodels21}.

% The "aha moment" referenced in the R1 report~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} primarily refers to the model's sudden acquisition of complex reasoning behaviors. A secondary interpretation involves the model spontaneously verbalizing "aha moment," such as in phrases like "Wait, wait. Wait. That’s an aha moment I can flag here." While our model did not exhibit this specific verbalization, Figure~\ref{fig:keywords} shows that it displayed some complex reasoning behaviors (e.g., self-reflection, exploration, verification, summarization) even by step 10.

% Thus, we conclude that \textbf{the RL process likely lacks a sudden "aha moment"}—that is, complex reasoning behaviors do not abruptly emerge at a specific training step, aligned with Liu et al.~\cite{liu2025oatzero}.

% \subsection*{RQ 3: Does an increase in response length lead to improved reasoning?}
% \label{rq3}
% \emph{Does the increase in response length during training directly lead to an improvement in the model's performance, or are these phenomena merely correlated?}
% \begin{figure}[H]
% \centering
% \includegraphics[width=1\linewidth]{figure/rq3.png}
% \caption{Comparison of response length, validation accuracy, and mean reward across steps for positive and negative example models.}
% \label{fig:rq3}
% \end{figure}
% This experiment investigates whether increased response length during training causally improves reasoning performance. We compared two models with different hyperparameters and dataset difficulties but trained using the same algorithm and base model.



% \begin{itemize}[leftmargin=*]
%     \item Positive Example Model (Blue): Despite a slight decrease in response length over time, this model showed significant improvements in validation accuracy and reward, indicating enhanced reasoning and generalization abilities. This suggests that reduced length can coincide with improved performance, challenging the assumption that longer responses inherently improve reasoning.
%     \item Negative Example Model (Red): This model consistently increased its response length but showed no improvement in validation accuracy or reward, highlighting that length growth alone does not drive reasoning improvements.
% \end{itemize}


% Figure \ref{fig:rq3} illustrates these findings: the positive model's reward and accuracy increased while its response length decreased, whereas the negative model's length increased without any performance gains. These divergent trends indicate that changes in response length are epiphenomenal rather than causal drivers of reasoning capability.

% The observed increase in response length is likely a byproduct of reinforcement learning dynamics. While some studies report a natural tendency for output length to grow as models generate more complex responses, this increase should be viewed as a correlate rather than a direct cause of improved reasoning. Importantly, there is no statistically significant evidence that the magnitude of length increase reliably predicts proportional gains in reasoning performance.

% In conclusion, \textbf{the increase in response length does not have a direct causal relationship with reasoning improvement but emerges as an associated byproduct.} Capability enhancement may naturally lead to longer reasoning chains, but artificially extending response length does not induce proportional performance gains.

% What's more, we observe an overthinking phenomenon where models produce longer responses during RL training. While extended reasoning can improve performance, it raises computational costs and is less preferred by humans, who favor concise reasoning. Shorter responses are more token-efficient, reducing overhead and enhancing training stability. Future research could explore "long-to-short" optimization , extracting key reasoning patterns to balance exploration and efficiency.


% \subsection*{RQ 4: Can the Model Generalize to Out-of-Distribution (OOD) Tasks?}  
% \emph{To what extent can the trained model handle tasks that differ from its training data, particularly those that are out-of-distribution?}

% The ability of a model to generalize beyond its training distribution is a cornerstone of AI research. We investigate whether the complex reasoning abilities—such as exploration, verification, summarization, and backtracking—that naturally emerged during RL process can transfer to a highly challenging mathematical reasoning scenario, which we term \emph{Super OOD} .

% This evaluation leverages the widely-adopted AIME 2021–2024 (American Invitational Mathematics Examination) and AMC 2022–2023 (American Mathematics Competitions) benchmarks, both known for their rigorous and diverse problem sets.

% \vspace{-3mm}
% \begin{figure}[H] 
% \centering
% \begin{minipage}{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figure/aime.pdf} 
%     \label{fig:aime}
% \end{minipage}
% \hspace{0.02\linewidth}
% \begin{minipage}{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figure/amc.pdf} 
%     \label{fig:amc}
% \end{minipage}
% \vspace{-4mm}
% \caption{Training Step vs. Accuracy on AIME (2021-2024) and AMC (2022-2023) Datasets.}
% \label{fig:ood}
% \end{figure}
% \vspace{-2mm}

% From Figure~\ref{fig:ood}, we observe that the model's Super OOD generalization capability is exceptionally strong, achieving an overall improvement of 125\% on the AIME dataset and 38\% on the AMC dataset. This synchronous improvement indicates that the reinforcement learning process not only enhances the model's performance on in-distribution tasks but also facilitates the emergence of robust and transferable reasoning strategies.

% These findings show that the reasoning skills learned during RL training go beyond the specific patterns of the K\&K dataset. This highlights RL's potential to generalize beyond their training environment.

% \subsection*{RQ 5: Is Curriculum Learning Necessary?}
% \emph{Does curriculum learning still matter in the RL paradigm? Specifically, is the sequential order of data important, or is it merely the curation ratio that matters?}

% To evaluate the necessity of curriculum learning in our framework, we compare its effectiveness against a mixed-difficulty training approach. This comparison aims to verify whether the theoretically appealing concept of curriculum learning—gradually exposing models to increasingly complex tasks—provides tangible benefits over directly exposing them to a diverse range of difficulty levels. In our implementation, the curriculum learning strategy sequentially trains the model on datasets ranging from 3-person to 7-person scenarios for one epoch each, with each training set representing an ascending level of difficulty. In contrast, the mixed-difficulty approach trains the model simultaneously on all difficulty levels (3–7 people) within a single epoch. All other hyperparameters are kept identical between the two training regimes.

% \begin{figure}[H]
% \centering
% \includegraphics[width=1\linewidth]{figure/rq5_Curriculum_Test_Score_Result}
% \caption{Comparison of test scores for curriculum learning and mixed-difficulty training. The plot uses a rolling average (window size = 5)}
% \label{fig:rq5_curriculum}
% \end{figure}

% We analyze the test score trajectories using a rolling average (window size = 5) to mitigate stochastic fluctuations and highlight underlying trends. The results in Figure \ref{fig:rq5_curriculum} indicate that curriculum learning yields slightly higher test scores during intermediate training phases. However, this advantage diminishes in practical significance, as the performance difference during early training stages remains statistically negligible, suggesting a limited impact on initial convergence. While curriculum learning may offer a marginal theoretical benefit in terms of sample efficiency, its practical necessity is not conclusively supported, given the minimal real-world performance difference and the added complexity of staged training.

% \subsection*{RQ 6: Which Generalizes Better, SFT (RFT) or RL?}

% \emph{Can post-training methods achieve more than just superficial alignment, which just learns  format patterns? Can SFT or RL actually learn to learn, effectively generalizing to other domains?}


% We investigate whether models merely memorize training data or truly learn reasoning skills. Following the setup in \cite{memllm}, we test this by comparing performance on familiar problems versus slightly altered ones.Two signs of memorization:  High accuracy on seen problems, and low accuracy on slightly perturbed versions. So, we measure these using two metrics, denote model as $f$, dataset as $\mathcal{D}$.
% \begin{enumerate}[leftmargin=*]
%     \item \textbf{Accuracy on Observed Problems}: $\mathrm{Acc}(f; \mathcal{D})$, the model's accuracy on the training set ($\mathrm{Tr}$). 
%     \item \textbf{Consistency Ratio}: $\mathrm{CR}(f; \mathcal{D})$, the ratio of correct solutions after small changes (perturbations) to those solved without changes. Perturbations preserve the puzzle's core principle and difficulty.
% \end{enumerate}
% The \textbf{Local Inconsistency-based Memorization Score} is defined as:
% \[
% \mathrm{LiMem}(f; \mathcal{D}) = \mathrm{Acc}(f; \mathcal{D}) \cdot (1 - \mathrm{CR}(f; \mathcal{D}))
% \]
% This score captures both memorization and sensitivity to changes. If a model's performance drops significantly when the problem format is altered, it likely hasn't learned the true reasoning skills required to solve similar but modified puzzles.We study two types of perturbations: (i) changing one person's statement to another bool logic expression, and (ii) reordering the statements between different people. Examples of perturbations are as follows:
 
% \begin{tcolorbox}[
%     enhanced, % 启用增强模式以支持更多功能
%     colframe=purple!70!black, % 边框颜色：深紫色带一点黑色
%     colback=purple!10!white, % 背景颜色：浅紫色
%     coltitle=white, % 标题文字颜色：白色
%     fonttitle=\bfseries, % 标题字体加粗
%     title=Perturbation Examples\label{long_open_q}, % 标题内容
%     attach boxed title to top left={ % 将标题放置在左上角
%         yshift=-2mm, % 垂直偏移
%         xshift=2mm % 水平偏移
%     },
%     boxed title style={ % 标题框样式
%         colframe=purple!70!black, % 标题框边框颜色
%         colback=purple!70!black % 标题框背景颜色
%     },
%     sharp corners, % 圆角设置为锐角
%     boxrule=0.5mm, % 边框宽度
%     drop fuzzy shadow=gray!30 % 添加柔和阴影
% ]
% \textbf{Original Problem}: \\
% Zoey remarked, "Oliver is not a knight". Oliver stated, "Oliver is a knight if and only if Zoey is a knave". So who is a knight and who is a knave?
% \\
% \textbf{Statement Perturbation}: \\
% Zoey remarked, "Oliver is a knight or Zoey is a knight". Oliver stated, "Oliver is a knight if and only if Zoey is a knave"

% \textbf{Reorder Perturbation}: \\
% Oliver stated, "Oliver is a knight if and only if Zoey is a knave". Zoey remarked, "Oliver is not a knight"

% \end{tcolorbox}

% % \begin{equation*}
% % \mathcal{J}_{RFT}(\theta)= \mathbb{E}[q \sim P_{sft}(Q), o \sim \pi_{sft}(O|q)]\left( \frac{1}{|o|}\sum_{t=1}^{|o|} \mathbb{I}(o) \log \pi_\theta(o_{t} | q, o_{<t})\right).
% % \end{equation*}

% % \begin{figure}[H]
% % \centering
% % \includegraphics[width=1\linewidth]{figure/limem_rl.jpg}
% % \caption{Accuracy and memorization scores of the model after reinforcement learning on datasets with 3, 5, and 7 people.}
% % \label{fig:rq4_rl}
% % \end{figure}


% While it is challenging to collect a suitable SFT dataset, we employ reject sampling to gather ground truth data, referred to as the RFT method.


% To explore which training paradigm offers better generalization performance, we apply two types of disturbances on training dataset, then compare RFT and RL with \textbf{Test acc - Mem Score curve}. For RFT settings, we use Reject Sampling on origin model, then use a rule-based Best-of-N method to collect the correct yet the shortest response for further fine-tune. As a result, we obtained the curve shown in Fig. 6, which illustrates the changes in unseen test accuracy and training dataset memorization over training process.



% \begin{figure}[H]
% \centering
% \captionsetup[subfigure]{labelformat=simple, labelsep=colon}
% % 左子图
% \begin{subfigure}[t]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth, keepaspectratio]{figure/limem_rl32.png} % 左子图文件名
%     \subcaption{RL}
%     \label{fig:rq4_rl}
% \end{subfigure}
% \hfill
% % 右子图
% \begin{subfigure}[t]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth, keepaspectratio]{figure/limem_rft32.png} % 右子图文件名
%     \subcaption{RFT}
%     \label{fig:rq4_rft}
% \end{subfigure}
% % 主标题
% \caption{Compare Reject Sampling Fine-tuning with Pure RL. While \textbf{RFT} slightly improves test accuracy at the expense of rapidly increasing $LiMem(f;Tr)$, indicating it mainly learns superficial answer format than geniue reasoning. In contrast, \textbf{RL}  achieves higher test accuracy with minimal or even negative increase in $LiMem(f;Tr)$. Within the same $LiMem$ interval, RL outperform RFT in test acc greatly, suggesting better generalization ability. }
% \label{fig:rq4_rl}
% \end{figure}

% A higher memorization score indicates that more questions, which were originally correct, have turned incorrect due to the disturbances caused by the training, reflecting a greater degree of memorization. SFT tends to superficial alignment, often becoming overly reliant on the original data's expression format. On the other hand, RL encourages the model to explore independently, fostering generalization abilities that stem from enhanced reasoning capabilities.

% \subsection*{RQ 7. Do certain thinking tokens and language-mixing phenemona improve reasoning?}
% \emph{Does the inclusion of complex reasoning behaviours (such as exploration, verification, summarization, and backtracking) and language switching improve the model’s reasoning ability?}

% \begin{figure}[H]
%     \hspace{-0.5mm}\includegraphics[width=1\linewidth]{figure/rq7.png}
%     \caption{Impact of complex reasoning behaviours and language mixing on reasoning performance. We analyzed the model's answer rewards for responses containing the tokens shown in the figure. Responses with "verify" and "re-evaluate" scored significantly higher than those without these words. Conversely, responses containing certain tokens from other languages generally received lower scores.}
% \end{figure}

% We observe that in our experiments:
% \begin{enumerate}[leftmargin=*]
%     \item Language mixing significantly impairs reasoning ability. 
%     % When Chinese or French words appear in responses to English queries, the model struggles to achieve a positive format score, let alone a positive answer reward.
%     \item  While terms like "wait," "verify," "yet," and "re-evaluate" show significant improvement, not all complex thinking tokens enhance reasoning ability, as exemplified by "recheck."
%     \item The complex reasoning behaviour “recheck” markedly diminishes reasoning ability, likely because its use signals the model’s uncertainty about its answer.
%     \item There’s a clear difference between "re-evaluate" and "reevaluate": the former leads to much higher answer scores, while the latter lowers them. We think this is because the model isn’t used to using "reevaluate." When we checked its origin responses, "reevaluate" almost never appeared, while "re-evaluate" showed up frequently. This suggests the model is more comfortable with words it has seen more often in training corpus.
% \end{enumerate}



