%\section{Conclusion}
%We have shown that a three-phase, rule-based RL approach can reliably induce advanced reasoning features---reflection, verification, summarization, and a $\sim50\%$ increase in chain-of-thought length---in a 7B-scale language model. Despite the limited size of our synthetic dataset, we surpass OpenAI-o1-1217 on a logic reasoning dataset (0.41 accuracy vs.\ 0.30) and double the baseline modelâ€™s accuracy (0.20). Our results highlight that emergent reasoning capabilities need not exclusively originate from large-scale supervised instruction tuning; \emph{pure rule-based RL}, guided by carefully designed rewards, can elicit new behaviors and patterns of thought. We hope that our fully open-source code will fill in the missing pieces of the RL component in R1, providing the open-source community with a valuable resource to further explore and build upon for improving reasoning capabilities in large language models.
