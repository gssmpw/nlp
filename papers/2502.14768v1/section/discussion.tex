\section{Discussion and Future Work}
While our study demonstrates the potential of Logic-RL in developing complex reasoning skills, it is important to note that our findings are based on a small-scale logic dataset. The generalizability of our results to large-scale real-world mathematical or coding scenarios remains to be explored. Future work should focus on extending our approach to more diverse and complex datasets to further validate its effectiveness and robustness. Our work will remain an \textbf{open research project} to benefit the community.

\paragraph{Chain-of-Thought Long to Short Methods.}

Despite the fact that our initial prompt is concise and straightforward, the length of the responses can expand by up to four times after reinforcement learning training. In order to enhance token efficiency and accommodate a long-context friendly training paradigm, we find it particularly valuable to explore methods that transform long responses into shorter, more digestible formats. This investigation aims at improving overall efficiency and effectiveness in handling lengthy outputs, thereby optimizing the training process for better scalability and performance.

\paragraph{Stablize RL Training.}

We have found it beneficial in some cases to eliminate KL constraints, especially when starting from a strong foundation model. Additionally, introducing a higher temperature at the beginning of training appears to provide the model with a more diverse starting point. We plan to further investigate how the SFT stage impacts the effectiveness and efficiency of RL training. 


\paragraph{Mixed-Language Reasoning.} 

A curious phenomenon is the modelâ€™s frequent use of Chinese tokens in the \texttt{<think>} section, despite training data being fully in English. One hypothesis is that certain tokens in the Chinese vocabulary vector might produce hidden states that are ``favorable'' under our RL scheme. Investigating whether code-switching or even \emph{random token switching} could systematically aid internal reasoning is an exciting avenue.

\paragraph{Relaxing the Formatting Constraints.}

Although \texttt{<think>} \ldots \texttt{</think>} effectively organizes the chain of thought, it remains an open question whether an entirely unconstrained or latent approach might yield better results. The model might eventually ``invent'' its own internal representation for reasoning if given the right incentives.

