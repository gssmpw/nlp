\section{Related Work}
\paragraph{Large Language Model Reasoning}
A key focus for LLMs is improving their reasoning abilities, particularly for complex tasks like code generation and math problem-solving. Chain-of-Thought (CoT) reasoning~\cite{cot} has been crucial in breaking down problems into manageable steps, enhancing logical reasoning. Originally successful in AlphaGo's victory~\cite{alphazero}, MCTS has been adapted to guide model-based planning by balancing exploration and exploitation through tree-based search and random sampling, and later to large language model reasoning~\cite{sc-mcts, deepprover1.5, xu2023traingainunleashmathematical}.  However, recent research suggests that the vast token generation space of LLMs may make improving reasoning capabilities inefficient~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}. Additionally, long-path reasoning is likely influenced by the model's working memory~\cite{workingmemory}.


\paragraph{Large Language Model Post-training for Reasoning}
Recent work has focused on post-training strategies to enhance the reasoning capabilities of large language models, often through additional fine-tuning or reinforcement learning on specialized datasets with reasoning examples and chain-of-thought explanations~\cite{xu2025redstardoesscalinglongcot}. Reinforcement learning methods like Direct Preference Optimization (DPO)\cite{dpo}, Proximal Policy Optimization (PPO)\cite{ppo}, Group Relative Policy Optimization (GRPO)\cite{grpo}, and REINFORCE++\cite{rpp} are gaining attention. These strategies, alongside test-time scaling methods, are a promising frontier for advancing model reasoning.

\section{Comparion Between Base and Instruct Model}
We started with different model types (base, instruct) under the same training strategy and found that the RL curves of the two were surprisingly similar. The growth rates of the test set accuracy, curve, and reward curve were nearly the same. The slight difference is that the response length of the base model grows faster than that of the instruct model.
\label{base_comp}
\begin{figure}[H]
    \centering
    % 第一行，两张图片
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth, keepaspectratio]{figure/base_instruct/base_reps.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth, keepaspectratio]{figure/base_instruct/val_acc.png}
    \end{minipage}

    % 第二行，两张图片
    \vspace{1em} % 添加一点垂直间距
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth, keepaspectratio]{figure/base_instruct/Mean_reward.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth, keepaspectratio]{figure/base_instruct/KL_loss.png}
    \end{minipage}

    % 添加一个大标题
    \caption{Comparison of training metrics: Response Length, Validation Accuracy, Mean Reward, and KL Loss.}
    \label{fig:training_metrics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figure/base_instruct/base_length.png}
    \caption{Length dynamics between base \& instruct}
    \label{fig:base_length}
\end{figure}
Figure \ref{fig:base_length} shows that the base model quickly increases thinking compute, while the instruct model consistently maintains slightly higher validation accuracy.


\section{Qualitative Analysis of Emergent Reasoning}
We highlight four key emergent behaviors observed in the RL-trained model:

\paragraph{(1) Hesitation and Self-Verification}
The model occasionally uses phrases like "I am not entirely sure; let's re-check this step" in the \texttt{<think>} section. This hesitation, absent in pre-training, emerges as the model is rewarded for correct answers and penalized for errors. Before providing the final \texttt{<answer>}, it systematically verifies all prior steps.

\paragraph{(2) Multi-Path Exploration \& Backtracking}
Encouraging the model to reason thoroughly, we see it propose multiple solutions (''Let's test both possibilitie'') and backtrack to check for consistency, resembling human problem-solving in logic puzzles.

\begin{table}[H]
\centering
\fontsize{9}{10}\selectfont
\begin{tabular}{p{14cm}}
\toprule
Question: \\
A very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. Five residents (William, Chloe, Lily, Jack, and Logan) made the following statements: (1) William said: "William is a knight and Chloe is a knight." (2) Chloe said: "Lily is a knight." (3) Lily said: "William is a knave and Jack is a knight." (4) Jack said: "William is a knave." (5) Logan said: "Lily is a knave if and only if William is a knave." So
who is a knight and who is a knave? \\
\midrule
Response: <think> \\
To solve this, let's start analyzing the statements one by one, beginning with William's statement since it's a good starting point due to its self-referential nature. \\
\\
William's statement: "William is a knight and Chloe is a knight." \\
   - If William is telling the truth (i.e., he is a knight), then both parts of his statement must be true. However, if William is a knight, the statement "William is a knight" is true, \textcolor{red}{but the statement "Chloe is a knight" cannot determine the truth about Chloe at this point since we haven't analyzed Chloe's statement yet.}\textcolor{blue}{\textit{\textbf{ (Hesitate)}}
} Therefore, William cannot be a knight because it would lead to a contradiction. \\
% - If William is lying (i.e., he is a knave), then at least one part of his statement is false. \\
\ldots \\

% Since "William is a knight" is false, this part is indeed false, so his statement is technically true in the context of him being a knave, which is a contradiction. Therefore, William must be a knave. \\
% \\
% \textbf{Now that we know William is a knave, let's analyze Chloe's statement}: "Lily is a knight." \\
%    - Since William is a knave, Chloe's statement can be either true or false. If Chloe is a knight, her statement must be true, meaning Lily is a knight. If Chloe is a knave, her statement must be false, meaning Lily is not a knight. \\
   
\textcolor{red}{Let's test both possibilities.}\textcolor{blue}{\textit{\textbf{ (Explore)}}}\\
Testing Chloe as a knight: If Chloe is a knight, then her statement "Lily is a knight" is true, so Lily is a knight. \\
Analyzing Lily's statement: "William is a knave and Jack is a knight." \\
\ldots \\

%    - Since William is a knave and Lily is a knight, her statement is true. This means Lily is indeed telling the truth, so she is a knight. \\
% \\
% \textbf{Analyzing Jack's statement}: "William is a knave." \\
%    - Since William is a knave, Jack's statement is true, so Jack is a knight. \\
% \\
% \textbf{Analyzing Logan's statement}: "Lily is a knave if and only if William is a knave." \\
%    - Since Lily is a knight and William is a knave, this statement is false. Therefore, Logan is a knave. \\
% \\
% \textbf{Analyzing Chloe's statement again}: We already determined that if Chloe is a knight, her statement "Lily is a knight" is true, so she is a knight. \\
%    - Since all statements are consistent with Chloe being a knight, Chloe is indeed a knight. \\
% \\
\textcolor{red}{Now, we have determined:}\textcolor{blue}{\textit{\textbf{ (Summerize)}}} \\
- William is a knave. \\
- Chloe is a knight. \\
\ldots \\
\textcolor{red}{Let's verify all statements:} \textcolor{blue}{\textit{\textbf{ (Verify)}}} \\
- "William is a knight and Chloe is a knight" (False, since William is a knave). \\
- "Lily is a knight" (True). \\
\ldots \\
% - "Lily is a knave if and only if William is a knave" (False, since Lily is a knight and William is a knave). \\
% - "William is a knave" (True). \\
% - "William is a knave and Jack is a knight" (True). \\
% \\
% All statements are consistent with our assignments. \textless\textbackslash think>
\\
\hline
\end{tabular}
\vspace{5mm}
\caption{An example of a large language model trained through reinforcement learning naturally exhibiting reasoning behaviors such as hesitation, exploration, summarization, and verification, even though these behaviors were not present in the training data.}
\end{table}

\paragraph{(3) Formula Application}
After RL training, our model instinctively applied the "If P, then Q" implication formula when solving logical puzzles, like the Knights and Knaves problem. This formula asserts that the proposition is false only when P is true and Q is false. We were surprised to see that the model not only solved the puzzles through trial and error but also incorporated formal logical reasoning, resembling human problem-solving, despite no such data included in the training set.

\begin{table}[H]
\centering
\fontsize{9.5}{10}\selectfont
\begin{tabular}{p{14cm}}
\toprule
Question: \\
A very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. Three residents (Jack, Samuel, Lily) made the following statements: (1) Jack said, "If Samuel is a knight, then Lily is a knave." (2) Samuel said, "If Jack is a knight, then Lily is a knave." (3) Lily said, "Jack is a knight." So who is a knight and who is a knave? \\
\midrule
Response: <think> \\
Let's analyze the statements step by step. \\
1. Jack's statement is: "If Samuel is a knight then Lily is a knave." \\
\ldots \\
Now that we know Samuel is a knave, let's analyze Samuel's statement again: \\
- Samuel's statement: "If Jack is a knight then Lily is a knave." \\
\textcolor{red}{- An implication "If P then Q" is false only when P is true and Q is false.}\textcolor{blue}{\textit{\textbf{ (Formula Application)}}} Here, "If Jack is a knight then Lily is a knave" is false when Jack is a knight and Lily is not a knave. This is consistent with our previous conclusion that Jack is a knight and Lily is a knight. \\
\ldots \\
\hline
\end{tabular}
\vspace{5mm}
\caption{An example of using logical formulas to solve a Knights and Knaves puzzle.}
\end{table}

\paragraph{(4) Occasional Language Switching}
Interestingly, some \texttt{<think>} segments contain Chinese tokens (the base model is English-centric). The final \texttt{<answer>} remains in English, presumably to gain a format reward. In some demos, the model briefly interjects a line of Chinese while analyzing statements, then seamlessly switches back to English for the solution. This phenomenon may indicate that the model uses language mixing as a hidden fallback or distinct internal representation. 

\begin{table}[H]
\centering
\fontsize{9.5}{10}\selectfont
\begin{tabular}{p{14cm}}
\toprule 
Question:A very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves
always lie. Six residents (Benjamin, Michael, Evelyn, Sophia, James, and Jacob) made the following statements: (1) Benjamin said, "If Sophia is a knight, then Evelyn is also a knight." (2) Michael said, "Benjamin is a knave, or James is a knight." (3) Evelyn said, "Sophia is a knight, or James is a knave." (4) Sophia said, "If Michael is a knave, then James is a knight." (5) James said, "Sophia is a knight, or Evelyn is a knight." (6) Jacob said, "Benjamin is a knave, and Sophia is a knight." So who is a knight and who is a knave? \\
\midrule
Response: <think> \\
Let's analyze each statement step-by-step based on whether the speaker is a knight or a knave. \\

1. Benjamin: "If Sophia is a knight then Evelyn is a knight" \\
   - If Benjamin is a knight, then the statement must be true. Thus, if Sophia is a knight, Evelyn has to be a knight. \\

2. Michael: "Benjamin is a knave or James is a knight" \\
\ldots \\

From Jacob's statement, if Jacob is a knight then Sophia must be a knight and Benjamin must be a knave. \\

\begin{CJK*}{UTF8}{gbsn}
\fontsize{8.5}{10}\selectfont
\textcolor{red}{以卡文的方式推理: }

\textcolor{red}{- 假设 Jacob 是骑士，那么 Jacob 的声明 "Benjamin 是背叛者且 Sophia 是骑士" 就是真的。这意味着 Benjamin 是背叛者而 Sophia 是骑士。}

\textcolor{red}{- 既然 Sophia 是骑士，根据 Benjamin 的声明 "如果 Sophia 是骑士那么 Evelyn 也是骑士"，Evelyn 也是骑士。}

\ldots

\textcolor{red}{综上所述，Sophia, Evelyn, 和 Jacob 是骑士，而 Benjamin, Michael, 和 James 是背叛者。 }\textcolor{blue}{\textit{\textbf{ (Language Switching)}}}

% \textless\textbackslash think>
\end{CJK*}
\ldots \\
\hline
\end{tabular}
\vspace{5mm}
\caption{An example of our model used language switching to solve a Knights and Knaves puzzle.}
\end{table}




% \subsection{Reward Curves and Format Consistency}
% We record multiple metrics throughout training: 
% \begin{itemize}[leftmargin=*]
%     \item \textbf{Average Reward}, 
%     \item \textbf{Fraction of Perfect Completions}, 
%     \item \textbf{Format Error Rate}, 
%     \item \textbf{Answer Error Rate}.
% \end{itemize}

% Early in training, the model often violates format rules (\texttt{<think>} / \texttt{<answer>}) or tries to re-enter thinking mode after outputting an \texttt{<answer>}, incurring large negative rewards. Over time, \emph{the format error rate drops to near-zero}, and the answer accuracy climbs. However, we also see ephemeral ``collapses'' if the sampling temperature is too high (e.g., $T=1.5$). Reducing temperature in Phase~3 stabilizes the final policy.

% \subsection{Generalization Beyond the In-Distribution Set}
% Despite using only a small synthetic dataset, we see partial transfer of reasoning improvements to out-of-domain tasks such as GSM8K. Though we do not claim state-of-the-art results, the RL-trained model demonstrates more structured multi-step reasoning on math word problems, occasionally verifying each step similarly to how it verifies logic statements. Detailed cross-domain results will be explored in future work.

% \subsection{Source of Generalization Capability}
% We follow the experimental setup outlined in \cite{memllm} to investigate the source of generalization capability of our reinforcement learning model on reasoning datasets. Specifically, a \textit{Local Inconsistency-based Memorization Score} is formulated as \[\mathrm{LiMem}(f;\mathcal{D})=\mathrm{Acc}(f;\mathcal{D})\cdot(1-\mathrm{CR}(f;\mathcal{D})),\] which consists of two components: (i) $\mathrm{Acc}(f;\mathcal{D})$, representing the accuracy on the target dataset and reflecting the model's absolute ability; and (ii) $\mathrm{CR}(f;\mathcal{D})$, the ratio of the number of puzzles solved consecutively after local perturbations to the number solved without perturbations, indicating the model’s memorization inertia.

% We evaluate the generalization capability of our model on datasets with varying levels of difficulty. For each difficulty level, we apply the following perturbations: (i) randomly select a person and generate a new statement; (ii) randomly select a person and flip the truth value of part of their statement, effectively altering a “leaf” node in logical tree. (iii) replace character names with uncommon names; (iv) reorder the statements from the characters.  Further details on the experimental setup can be found in the Appendix.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1.0\textwidth]{figure/limem.png}
%     \caption{Relationship between $\mathrm{LiMem}(f;\mathrm{T_r})$ and $\mathrm{Acc}(f;\mathrm{T_{st}})$ for models trained with 100, 200, 300, and 400 steps on datasets of varying difficulty. With further training, the model exhibits relatively minor changes in memorization on the training set; however, the accuracy on the test set demonstrates a gradual improvement.}
%     \label{fig:limem}
% \end{figure}

% \textbf{Generalization Capability of RL Model}. The high accuracy on test and relatively low $\mathrm{LiMem}(f;\mathcal{D})$ score observed in Figure \ref{fig:limem} demonstrate that, the generalization capability of models trained via reinforcement learning across varying levels of data difficulty does not primarily stem from their memorization of the training data.

% \textbf{Generalization Capability of RFT Model}.

% \subsection{RL vs RFT}
% \textbf{RFT Setup}. We 

