
\section{Introduction}

The post-training phase of large language models (LLMs) has advanced rapidly~\cite{xu2025largereasoningmodelssurvey}, with models like DeepSeek-R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, Kimi-K1.5~\cite{kimiteam2025kimik15scalingreinforcement}, and OpenAI-o1~\cite{o1} demonstrating remarkable reasoning abilities. DeepSeek-R1, in particular, introduced a simple yet effective rule-based reinforcement learning (RL) approach, enabling emergent reasoning patterns without relying on traditional scaffolding techniques such as Monte Carlo Tree Search (MCTS)~\cite{sc-mcts, deepprover1.5, xu2023traingainunleashmathematical, feng2024alphazeroliketreesearchguidelarge} or Process Reward Models (PRM)\cite{lightman2023letsverifystepstep}.

Despite these advancements, substantial gaps remain in translating these developments into reproducible research. While DeepSeek-R1 provides open-source model weights, it does not release the corresponding training code or dataset. This absence raises critical questions: (1) Can similar reasoning abilities emerge in smaller-scale models? (2) What is the optimal training data structure for fostering such capabilities? (3) What methodologies can reliably replicate these results?

Addressing these questions requires controlled experimental frameworks that isolate key variables.
%In this paper, we explore to unleash the reasoning capcablilites of LLMs using rule-based reinforcement learning. 
While mathematics is often treated as the common testbed for reasoning, widely-used math datasets like GSM8K\cite{cobbe2021trainingverifierssolvemath} and Omini-MATH \cite{gao2024omnimathuniversalolympiadlevel} suffer as the training data due to its uncontrolled variance in problem complexity, which may span various logical induction depths. 
To overcome this limitation, we leverage a procedurally generated Knights and Knaves (K\&K) logic puzzle dataset~\cite{memllm}, which allows  controllable difficulty levels and ease of rule-based reward verification,  making it ideal for studying reasoning dynamics.


In this paper, we introduce Logic-RL, a rule-based reinforcement learning framework that acquires
R1-like reasoning patterns through training on logic puzzles.
The training framework adopts the REINFORCE++ algorithm~\cite{rpp} and the reward designs from DeepSeek-R1 for post-training.
While naive training would lead to collapsed solutions, we propose a practical system prompt and a stringent format reward to avoid the reasoning model for taking shortcuts.
We also incorporate a few modifications to the REINFORCE++ algorithm for improved performance. 


As the RL training undergoes, we observe that the model naturally allocates more training steps to reason. This computational expansion scales from generating hundreds to thousands of tokens, enabling deeper exploration and refinement of its thought processes.
We evaluate the model performance on the challenging math benchmarks for reasoning.
%Controlled experiments reveal surprising generalization: 
With merely 5,000 procedurally generated logic puzzles, our 7B model improves by 125\% on AIME and 38\% on AMC against the base model.
%, both known as hard math benchmarks. 
This cross-domain generalization capability suggests that RL-trained reasoning heuristics develop abstract problem-solving schemata rather than relying on domain-specific pattern matching.

Besides the technical contributions mentioned above, our study also makes several interesting findings:
\vspace{-5mm}
\begin{itemize}[leftmargin=*]
    \item \textbf{Longer responses donâ€™t guarantee better reasoning.} 
    Length alone is not a valid performance metric for training time evaluation. The most efficient reasoning comes from the shortest path.
    \item \textbf{Language mixing hinders reasoning.} This observation underscores the need for a language consistency penalty in reward modeling.
    \item \textbf{Increasing `thinking' tokens do help.} RL training naturally boosts the frequency of reflection-related words, suggesting a correlation between certain tokens' frequency and performance.
    \item \textbf{SFT memorizes; RL generalizes.} SFT relies heavily on memorization, often leading to superficial shortcut learning, whereas RL self-evolves with minimal dependence on dataset structure. 
    \item \textbf{Cold start is a bonus, not a necessity.} Training dynamics remain surprisingly similar whether starting from a base or instruct model, though the latter exhibits slightly better performance.
    \item  \textbf{Curriculum Learning still matters.} Under a fixed data curation ratio, a well-designed curriculum learning approach always outperforms random shuffle.
\end{itemize}
 % \item \textbf{A simple and reproducible training framework.} A strict rule-based reward model ensures an unhackable environment, while a temper-anneal sampling strategy effectively balances exploration and exploitation.
%\item \textbf{A simple and reproducible training framework.} A strict rule-based reward model ensures an unhackable environment, while a straightforward training strategy achieves stable convergence through continuous training without complex scheduling.