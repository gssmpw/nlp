\section{Method}
\subsection{Data Synthesis}

The Knights and Knaves (K\&K) puzzles \cite{memllm} constitute an algorithmically generated reasoning dataset. In these puzzles, characters are either knights, who always tell the truth, or knaves, who always lie. The objective is to determine the nature of each character based on their statements. This dataset is distinguished by its high degree of controllability:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Procedural Generation}: Puzzles are systematically generated using logic templates, ensuring both consistency and infinite variability. Importantly, these puzzles represent unseen data for the original model, making them ideal for testing generalization capabilities.
    
    \item \textbf{Controlled Difficulty Levels}: The difficulty of the puzzles can be precisely adjusted, enabling the design of a curriculum learning strategy. Difficulty is modulated by varying the number of characters (2–8) and the complexity of logical operations (1–4 combinations of Boolean operators). Furthermore, more complex puzzles can serve as out-of-distribution tests for models trained on simpler cases, providing insights into their ability to generalize.

    \item \textbf{Ease of Verification}: Each puzzle has a single, unambiguous ground truth answer, with correctness guaranteed by the generation algorithm. Solutions require strict deductive reasoning, allowing for accurate evaluation of model responses and minimizing the risk of reward hacking.
\end{enumerate}


% example就换成橙色吧，跟system prompt区分一下
% \begin{tcolorbox}[
%     colframe=orange!50!white,
%     colback=orange!5,
%     coltitle=black,
%     fonttitle=\bfseries,
%     title=An example of a K\&K puzzle\label{long_open_q},
% ]

\begin{tcolorbox}[
    % enhanced, % 启用增强模式以支持更多功能
    colframe=blue!70!black, % 边框颜色：深蓝色带一点黑色
    colback=blue!10!white, % 背景颜色：浅蓝色
    coltitle=white, % 标题文字颜色：白色
    fonttitle=\bfseries, % 标题字体加粗
    title=An example of a K\&K puzzle\label{long_open_q}, % 标题内容
    % attach boxed title to top left={ % 将标题放置在左上角
    %     yshift=-2mm, % 垂直偏移
    %     xshift=2mm % 水平偏移
    % },
    % boxed title style={ % 标题框样式
    %     colframe=blue!70!black, % 标题框边框颜色
    %     colback=blue!70!black % 标题框背景颜色
    % },
    sharp corners, % 圆角设置为锐角
    boxrule=0.5mm, % 边框宽度
    % drop fuzzy shadow=gray!30 % 添加柔和阴影
]
\textbf{Problem}:  
A very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. You meet 2 inhabitants: Zoey, and Oliver. Zoey remarked, "Oliver is not a knight". Oliver stated, "Oliver is a knight if and only if Zoey is a knave". So who is a knight and who is a knave? \\

\textbf{Solution}:  
(1) Zoey is a knave (2) Oliver is a knight
\label{puzzle}
\end{tcolorbox}

\vspace{2mm}
The K\&K puzzle \ref{puzzle} is exceptionally well-suited for further analysis due to its synthetic design and logical precision. Each puzzle is constructed using formal rules, ensuring that every problem has a unique solution that can be deterministically verified. This eliminates the ambiguities commonly encountered in natural language tasks, enabling us to clearly distinguish between genuine reasoning capabilities and superficial memorization.


\subsection{Rule Based Reward Modeling}
The reward serves as the primary training signal in reinforcement learning (RL), guiding the optimization process. We continuously monitored hacking behaviors in the model's outputs, refining our reward design iteratively. This led to a nearly unhackable, rule-based reward system that comprises only two types of rewards: Format Reward and Answer Reward.
\begin{tcolorbox}[
    colframe=teal!70!black, % 边框颜色：深青色带一点黑色
    colback=teal!10!white, % 背景颜色：浅青色
    coltitle=white, % 标题文字颜色：白色
    fonttitle=\bfseries, % 标题字体加粗
    title=System Prompt\label{long_open_q}, % 标题内容
    % attach boxed title to top left={ % 将标题放置在左上角
    %     yshift=-2mm, % 垂直偏移
    %     xshift=2mm % 水平偏移
    % },
    % boxed title style={ % 标题框样式
    %     colframe=teal!70!black, % 标题框边框颜色
    %     colback=teal!70!black % 标题框背景颜色
    % },
    sharp corners, % 圆角设置为锐角
    boxrule=0.5mm, % 边框宽度
    % drop fuzzy shadow=gray!30 % 添加柔和阴影
]
You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.  Now the user asks you to solve a logical reasoning problem. After thinking, when you finally reach a conclusion, clearly state the identity of each character within <answer> </answer> tags. i.e., <answer> (1) Zoey is a knight, (2) ... </answer>.
\label{prompt}
\end{tcolorbox}


\paragraph{Format Reward:} We use regular expression extraction to enforce a structured response format. The model is required to put its reasoning process within \texttt{<think></think>} tags and provide the final conclusion inside \texttt{<answer></answer>} tags. Additionally, we recommend including a \texttt{<think>} tag directly at the end of the prompt, which significantly reduces the difficulty for the base model to follow our instructions.

% To ensure strict adherence to this structure, we enforce that each tag appears exactly once and in the correct sequential order.  This design effectively prevents potential reward hacking behaviors, such as:

Under our early imperfect  rule design, we consistently observed  reward hacking phenomena, some of which are listed below:

\begin{itemize}[leftmargin=*]
    \item Skipping the \texttt{<think></think>} process and directly answering.
    \item Placing reasoning inside the \texttt{<answer></answer>} tag.
    \item Repeatedly guessing answers without proper reasoning.
    \item Including irrelevant nonsense in addition to providing the answer.
    \item Organizing correct answer in a wrong manner for extraction.
    \item Revisiting the thinking phase after already outputting an \texttt{<answer>} due to insufficient reasoning.
    \item Repeating the original question or using phrases like "thinking process here" to avoid true reasoning.
\end{itemize}

Accordingly, we iteratively refine our rule design. For example, each tag should appear exactly once and in the correct sequential order, the thinking process must include genuine reasoning, and the conclusion should be presented in an extractable and readable manner. By enforcing these constraints, we ensure that different actions receive appropriate rewards based on their adherence to the format. The format score (\( S_{format} \)) is computed as follows:

\[
S_{format} =
\begin{cases}
\text{1}, & \text{if format is correct} \\
\text{-1}, & \text{if format is incorrect}
\end{cases}
\]


\paragraph{Answer Reward:} The second component evaluates the correctness of the content in the model’s response. Once the format is validated, we check if the model's answer matches the ground truth. The answer score (\( S_{answer} \)) is computed as:

\[
S_{answer} =
\begin{cases}
2, & \text{if the answer fully matches the ground truth} \\
-1.5, & \text{if the answer partially mismatches the ground truth} \\
-2, & \text{if the answer cannot be parsed or is missing}
\end{cases}
\]

\subsection{RL Algorithm}
We adopt a modified version of \texttt{REINFORCE++} as our baseline algorithm, which has demonstrated superior performance compared to GRPO in our experimental setup. A detailed comparison of these algorithms is provided in Section~\ref{rl_algos}. 

\textbf{Reinforce Return Calculation:} The discounted cumulative rewards for each trajectory are computed as below, where $\gamma$ is the discount factor, set to 1 in our experiments:

\begin{equation*}
G_t = \sum_{k=t+1}^{T} \gamma^{k-t} r_k
\end{equation*}


Following recommendations from DeepSeek-Math~\cite{grpo}, we incorporate several minor refinements into the implementation of \texttt{REINFORCE++}.

\paragraph{First modification: Use KL Loss}
 The KL-divergence between the response distributions of the RL model and the SFT model is calculated for each token. This divergence is incorporated as a penalty term in the reward function of PPO during training. The per-token reward is defined as follows:
\begin{equation*}
r(s_t, a_t) = \mathbf{I}(s_t = [\text{EOS}]) r(x, y) - \beta \text{KL}(t),
\end{equation*}
where $\mathbf{I}(s_t = [\text{EOS}])$ is an identity function that evaluates to 1 when the \texttt{<eos>} token is reached, and $\beta$ controls the weight of the KL penalty.

In contrast, the GRPO implementation does not include the KL-divergence as part of the reward function. Instead, it directly incorporates the KL-divergence into the loss function, arguing that this approach simplifies the computation and avoids unnecessary complexity. Following this rationale, we also use KL loss like GRPO:
\begin{align*}
    \mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)]} \notag \\
    &\frac{1}{G}\sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert}\Bigg\{\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_{\text{old}}}^{i,t}}\hat{A}_{i,t}, \textrm{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_{\text{old}}}^{i,t}}, 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right] - \beta\mathbb{D}_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}]\Bigg\}.
\end{align*}

\paragraph{Second Modification: KL Estimation}

Another key distinction lies in how the KL-divergence is estimated. The default KL estimator for PPO is defined as follows:

\begin{equation*}
\text{KL}(t) = \log\left(\frac{\pi_{\theta_{\text{old}}}(a_t | s_t)}{\pi_{\theta}(a_t | s_t)}\right).
\end{equation*}

In contrast, GRPO employs an unbiased estimator for the KL-divergence, formulated as:

\begin{equation*}
\mathbb{D}_{\text{KL}}\left[\pi_{\theta} \| \pi_{\text{ref}}\right] = \frac{\pi_{\text{ref}}\left(o_{i, t} | q, o_{i,<t}\right)}{\pi_{\theta}\left(o_{i, t} | q, o_{i,<t}\right)} - \log \frac{\pi_{\text{ref}}\left(o_{i, t} | q, o_{i,<t}\right)}{\pi_{\theta}\left(o_{i, t} | q, o_{i,<t}\right)} - 1.
\end{equation*}

This approach ensures that the KL estimate is always non-negative, whereas the original formulation may yield negative values. GRPO's estimator provides a more stable and reliable measure of divergence during training.

After implementing these modifications, we proceed with our experiments.


\subsection{Training Schedule}
We directly train the model for 3600 steps with a constant learning rate of $4 \times 10^{-7}$ and temperature parameter of 0.7. During training, the model is directly exposed to mixed complexity logic puzzles ranging from 3 to 7 people. This straightforward training regimen achieves competitive performance as shown in the final results table. Through continuous training with these fixed hyperparameters, the model develops stable reasoning patterns characterized by logical exploration, intermediate verification, and systematic summarization before producing final answers. These emergent behaviors demonstrate the model's capacity to handle complex logical reasoning tasks effectively.


% We employ a multi-stage curriculum learning approach, gradually exposing the model to logic puzzles of increasing complexity, ranging from 3-person to 7-person problems. Both stages balance exploration and exploitation, fostering a model that generalizes effectively across diverse problems.

% \textbf{Tempering Stage:}  
% In the initial phase, we set a learning rate of $5 \times 10^{-7}$ and a temperature of $1.0$ to encourage exploration. This stage prioritizes token diversity over validation accuracy, enabling the model to explore a wide range of reasoning strategies. By breaking down the rigid markdown format used by the original model, we prevent it from converging on narrow solutions. To ensure stable updates during this exploratory phase, we use a relatively large rollout.

% \textbf{Annealing Stage:}  
% As training progresses, we gradually reduce the learning rate (first to $2 \times 10^{-7}$, then to $1 \times 10^{-7}$) and lower the temperature (first to $0.7$, then to $0.4$). This shift refines the model's policy, emphasizing accuracy and consistency in its solutions. During this phase, the model begins to exhibit stable reasoning patterns, characterized by behaviors such as exploration, hesitation, verification, and summarization before producing a final answer. These behaviors reflect a deeper understanding of problem structures and lead to more reliable outputs.

Other key parameters used during training are summarized in Table \ref{tab:training_parameters}

\begin{table}[h!]
    \centering
    \caption{Important Training Parameters}
    \label{tab:training_parameters}
    \begin{tabular}{ccccc}
        \toprule
        \textbf{Algorithm} & \textbf{Train Batch Size} & \textbf{Rollout N} & \textbf{KL Coef} & \textbf{Max Response Len} \\
        \midrule
        REINFORCE++ & 8 & 8 & 0.001 & 4096 \\
        \bottomrule
    \end{tabular}
\end{table}
% 这个table还是去掉吧，感觉两阶段文字描述就可以了