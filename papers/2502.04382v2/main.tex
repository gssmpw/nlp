
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{colortbl}
\usepackage[dvipsnames]{xcolor}
\usepackage[most]{tcolorbox}
\usepackage[hang,flushmargin]{footmisc}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{xspace}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\captionvskip}{0.1in}

\newcommand{\smalltt}[1]{{\small\texttt{#1}}\xspace}
\newcommand{\wiki}{\textsc{Wiki}\xspace}
\newcommand{\bills}{\textsc{Bills}\xspace}
\newcommand{\headlines}{\textsc{Headlines}\xspace}
\newcommand{\yelp}{\textsc{Yelp}\xspace}
\newcommand{\congress}{\textsc{Congress}\xspace}
\newcommand{\embeddingmodel}{text-embedding-3-small\xspace}
\newcommand\hyp[1]{\textit{#1}}

\newcommand{\ourmethod}{\textsc{HypotheSAEs}\xspace}
\newcommand{\nlparam}{\textsc{NLParam}\xspace}
\newcommand{\hypogenic}{\textsc{HypoGeniC}\xspace}
\newcommand{\bertopic}{\textsc{BERTopic}\xspace}
\newcommand{\topicgpt}{\textsc{TopicGPT}\xspace}

\newcommand{\XX}{\mathcal{X}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}

\DeclareMathOperator{\topk}{TopK}
\DeclareMathOperator{\auxk}{AuxK}
\DeclareMathOperator{\enc}{enc}
\DeclareMathOperator{\dec}{dec}
\DeclareMathOperator{\relu}{ReLU}

\usepackage{bm}

\newcommand\emma[1]{\textcolor{blue}{[Emma: #1]}}
\newcommand\raj[1]{\textcolor{orange}{[Raj: #1]}}
\newcommand\kenny[1]{\textcolor{ForestGreen}{[Kenny: #1]}}


\newcommand{\papertitle}{Sparse Autoencoders for Hypothesis Generation}

\usepackage[textsize=tiny]{todonotes}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=*}


\icmltitlerunning{\papertitle}

\begin{document}

\twocolumn[
\icmltitle{\papertitle}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rajiv Movva}{equal,berkeley}
\icmlauthor{Kenny Peng}{equal,cornell,tech}
\icmlauthor{Nikhil Garg}{tech}
\icmlauthor{Jon Kleinberg}{cornell}
\icmlauthor{Emma Pierson}{berkeley}
\end{icmlauthorlist}

\icmlaffiliation{berkeley}{UC Berkeley}
\icmlaffiliation{cornell}{Cornell University}
\icmlaffiliation{tech}{Cornell Tech}

\icmlcorrespondingauthor{Rajiv Movva}{rmovva@berkeley.edu}
\icmlcorrespondingauthor{Kenny Peng}{kennypeng@cs.cornell.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{}  %

\begin{abstract}
    We describe \ourmethod, a general method to hypothesize interpretable relationships between text data (e.g., headlines) and a target variable (e.g., clicks). \ourmethod has three steps: (1) train a sparse autoencoder on text embeddings to produce interpretable features describing the data distribution, (2) select features that predict the target variable, and (3) generate a natural language interpretation of each feature (e.g., \textit{mentions being surprised or shocked}) using an LLM. Each interpretation serves as a hypothesis about what predicts the target variable.
    Compared to baselines, our method better identifies reference hypotheses on synthetic datasets (at least +0.06 in F1) and produces more predictive hypotheses on real datasets ($\sim $twice as many significant findings), despite requiring 1-2 orders of magnitude less compute than recent LLM-based methods. 
    \ourmethod also produces novel discoveries on two well-studied tasks: explaining partisan differences in Congressional speeches and identifying drivers of engagement with online headlines.
\end{abstract}

\input{sections/intro}
\input{sections/related_work}
\input{sections/methods}
\input{sections/experiments}
\input{sections/results}
\input{sections/discussion}

\section*{Software and Data}
Code is \href{https://github.com/rmovva/HypotheSAEs}{available on GitHub}, and can be installed via pip with \texttt{pip install hypothesaes}. 
A notebook to reproduce experimental results in the paper is available on the repository.
All data used in the paper are \href{https://huggingface.co/datasets/rmovva/HypotheSAEs}{available on HuggingFace}.
A demo to visualize all SAE neurons and how well they predict the target for \headlines, \yelp, and \congress is available at \href{https://hypothesaes.org/}{https://hypothesaes.org/}.

\section*{Acknowledgements}
We thank Rishi Jha and Hal Triedman for helpful discussion about sparse autoencoders, Anna Lyubarskaja for helpful discussion about \Cref{prop:errors-in-variables}, Ruiqi Zhong for helpful discussion about \nlparam, Neil Movva and OpenAI for LLM inference credits, Serina Chang for useful comments, Sammi Cheung for writing feedback, and members of the Pierson and Garg groups for comments and discussions.

\section*{Impact Statement}

Our work advances the field of AI-assisted hypothesis generation, which we hope will drive progress in the social and natural sciences.
To the best of our knowledge, there are no particular negative social consequences imposed by our work compared to machine learning research in general.


\bibliography{raj,kenny}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn
\input{sections/appendix}


\end{document}
