

\section{Theoretical Framework}
\label{sec:theory}

In this section, we derive a theoretical result that shows how a language model with highly interpretable neurons can be used for hypothesis generation, as well as where performance loss arises in such a procedure.

Let $Z$ be an indicator for whether a neuron fires for a given text input (i.e., whether the activation exceeds a threshold). Let $\hat{Z}$ be an indicator for whether the text contains a specified natural language concept. Let $Y$ be the target variable. Define the predictiveness of $\hat{Z}$ as the separation score $$S(\hat{Z}):=|\EE[Y|\hat{Z}=1] - \EE[Y|\hat{Z}=0]|.$$ Define $S(Z)$ analogously.\footnote{Separation score is one way we evaluate hypotheses empirically. We suspect that analogs of \Cref{prop:errors-in-variables} hold for alternative measures of predictiveness, like explained variance.} 
We measure the fidelity of the interpretation $\hat{Z}$ by 
\begin{equation}
    \delta(\hat{Z}, Z) := \frac{1 - \min\left\{\text{recall}, \text{precision}\right\}}{\min\{\Pr[\hat{Z}=0],\Pr[Z=0]\}},
\end{equation}
where precision is how often the neuron fires when the concept is present and recall is how often the concept is present when the neuron fires. Lower $\delta$ implies higher fidelity. High recall and precision means the interpretation approximates the neuron's behavior, and is not overly narrow or generic. Empirically, the denominator $\min\{\Pr[\hat{Z}=0],\Pr[Z=0]\}$ is close to 1 (neurons and concepts activate infrequently), so maximizing fidelity is approximately maximizing the minimum of recall and precision.

\begin{proposition}[A Triangle Inequality for Hypothesis Generation]\label{prop:errors-in-variables}
Suppose $Y$ is supported on $[0,1]$. Then 
\begin{align}
    |S(\hat{Z}) - S(Z)| \le \delta(\hat{Z}, Z).
\end{align}
\end{proposition}
Intuitively, the result shows that if $\hat{Z}$ is a high-fidelity predictor of $Z$, then it will also have a similar separation score. A full proof is given in \Cref{sec:proof}.\footnote{There are two main observations that enable us to show the result. First, we focus on $|\EE[Y|\hat{Z}=1]-\EE[Y|Z=1]|,$ and show that if the false negative rate (FNR) is greater than the false discovery rate (FDR), this value is bounded by the FNR; otherwise, it is bounded by the FDR. Second, we show roughly that as long as $\Pr[\hat{Z}=1], \Pr[Z=1] < \Pr[\hat{Z}=0], \Pr[Z=0],$ then $|\EE[Y|\hat{Z}=0]-\EE[Y|Z=0]| < |\EE[Y|\hat{Z}=1]-\EE[Y|Z=1]|.$}


Notice that what we ultimately care about is $S(\hat{Z})$ (how predictive our actual natural language hypothesis is), but our procedure focuses primarily on generating, selecting, and then interpreting the neuron $Z$. \Cref{prop:errors-in-variables} shows that as long as we can generate a sufficiently high-fidelity interpretation $\hat{Z}$ of each neuron $Z$, then we can focus on finding predictive neurons as a way to ultimately find natural language hypotheses. In particular, the result shows that
\begin{align}
    S(\hat{Z}) &= S(Z) + (S(\hat{Z}) - S(Z))\\
    &\ge \underbrace{S(Z)}_{\scriptsize\shortstack{neuron\\predictiveness}} - \underbrace{\delta(\hat{Z}, Z)}_{\scriptsize\shortstack{interpretation\\fidelity}}.\label{eq:decomposition}
\end{align}
\Cref{eq:decomposition} shows that to find a natural language concept such that $S(\hat{Z})$ is large, it suffices to first identify $Z$ such that $S(Z)$ is high (i.e., find a predictive neuron) and then $\hat{Z}$ such that $\delta(\hat{Z}, Z)$ is small (i.e., find a high-fidelity interpretation).

This directly motivates the actual procedure employed in \ourmethod. First, we learn interpretable neurons $Z$ using an SAE. Second, we identify which neurons $Z$ are predictive. Third, we generate high-fidelity interpretations $\hat{Z}$ of these neurons with an LLM.

\Cref{eq:decomposition} also gives us a lens by which to analyze \ourmethod empirically. The result decomposes the performance of hypotheses $\hat{Z}$ into two parts: the predictiveness of the neurons $Z$ and the fidelity of the interpretations $\hat{Z}.$ For example, if the SAE neurons are similarly predictive to the text embeddings they encode, then most performance loss (compared to predicting directly from embeddings) comes from low-fidelity neuron interpretation. Imperfect neuron interpretations can be due to either neurons being fundamentally uninterpretable (there doesn't \textit{exist} a high-fidelity interpretation), or due to difficulties in actually \textit{finding} the interpretation. These challenges arise in steps 1 (training the SAE) and step 3 (generating the interpretation) respectively. We perform such an empirical analysis in \Cref{sec:performance_losses}.

\section{Methods}
\label{sec:methods_main}

In the previous section, we established theoretically that we can generate hypotheses by learning interpretable neurons, identifying which neurons are predictive, and then generating high-fidelity interpretations of the neurons. We now describe how we can accomplish these steps in practice.

We now describe the details of \ourmethod. Given a dataset of training examples $\{(x_i, y_i)\}_{i\in [N]},$ where $x_i$ is the input text and $y_i$ is the target variable annotation, \ourmethod outputs $H$ natural language concepts that serve as hypotheses. The goal is for these natural language concepts to predict the target variable.

\subsection{Feature Generation: Training Sparse Autoencoders.}

Let $e_i$ denote a $D$-dimensional text embedding of $x_i$. We train a $k$-sparse autoencoder \citep{makhzani_ksparse_2014}. $k$-sparse autoencoders have successfully generated interpretable features both when applied to intermediate layers of language models, as well as text embeddings \citep{gao_scaling_2024, oneill_disentangling_2024}. The SAE encodes a text embedding $e_i$ as follows (we use OpenAI's \embeddingmodel, where $D=1536$, in our experiments):
\begin{align}
    z_i &= \relu(\topk(W_{\enc}(e_i - b_{\mathrm{pre}}) + b_{\enc}))\\
    \hat{e_i} &= W_{\dec}z_i + b_{\dec},
\end{align}
where $b_{\mathrm{pre}} \in \RR^D, W_{\enc}\in \RR^{M\times D}, b_{\enc}\in \RR^{M}, W_{\dec}\in \RR^{D\times M}, b_{\dec}\in \RR^{D},$ and $\topk$ sets all activations except the top $k$ to zero. In a basic $k$-sparse autoencoder, the loss on one input is
\begin{equation}
\mathcal{L} = ||e_i - \hat{e_i}||_2^2.
\end{equation}
We follow \citet{gao_scaling_2024, oneill_disentangling_2024} in further adding an auxillary loss to avoid ``dead latents.'' We describe details and hyperparameters in Appendix \ref{sec:hyperparams}. 

Empirically, the dimensions of $z_i$ are often highly interpretable, corresponding to human concepts.
$M$ sets the total number of concepts learned across the entire dataset, and $k$ sets the number of concepts which can be used to reconstruct each instance. The output of this first step is an $N\times M$ activation matrix, $Z_{\text{SAE}}$.


\subsection{Feature Selection: Target Prediction with SAE Neurons.}
To select a predictive subset of SAE neurons, we fit an $L_1$-regularized linear or logistic regression predicting the target variable $Y$ from the activation matrix $Z_{\text{SAE}}$ \cite{tibshirani_regression_1996}. 
Formally, for regression\footnote{For classification we analogously use an $L_1$-regularized loss: $\mathcal{L}(\bm{\beta}; \lambda) = \frac{1}{N} \operatorname{BCE}(\mathbf{y}, \sigma(Z_{\text{SAE}}\bm{\beta})) + \lambda ||\bm{\beta}||_1$, where $\operatorname{BCE}$ is the binary cross-entropy loss and $\sigma(\cdot)$ is the sigmoid function.}, we optimize $$\min_{\bm{\beta}} \mathcal{L}(\bm{\beta}; \lambda) = \frac{1}{N} \left||\mathbf{y} - Z_{\text{SAE}}\bm{\beta}|\right|^2_2 + \lambda ||\bm{\beta}||_1,$$ where $\mathbf{y}$ is a length-$N$ vector, $Z_{\text{SAE}}$ is an $N \times M$ matrix, and $\bm{\beta}$ is a length-$M$ vector of feature coefficients for each SAE neuron.
The $L_1$ penalty produces sparse coefficient vectors $\bm{\beta}$ where some coefficients are exactly zero (indicating a dropped feature). 
To generate $H$ hypotheses, we perform binary search to identify a value of $\lambda$ which produces exactly $H$ nonzero coefficients.

\subsection{Feature Interpretation: Labeling Neurons with LLMs.}\label{sec:methods-interpretations}

In this step, we generate high-fidelity interpretations (i.e., natural language concepts) of the subset of predictive neurons. 
To generate interpretations of a neuron, we prompt an LLM (GPT-4o in our experiments) with example texts from a range of neuron activations and instruct it to identify a natural language concept that is present in high-activation texts and absent in low-activation texts. We then generate multiple interpretations by rerunning this procedure several times at temperature 0.7, and choose the best interpretation according to a measure of fidelity we describe below.

We test a variety of approaches for generating interpretations. In our final procedure, we sample high- and low-activating texts from the neuron's positive activation distribution, with the sampling bins given in \Cref{sec:hyperparams_neuronlabeling}. 
(We restrict our sample to texts that activate the neuron to ensure some examples contain the concept.) 
Treating these as true positives and true negatives, respectively, we prompt an LLM to generate a natural language concept that distinguishes 10 samples of each class. 
We then define an interpretation's fidelity as its F1 score in this prediction task, using an LLM (GPT-4o-mini in our experiments) to annotate 100 positive and 100 negative samples for the presence of the concept. 

This approach produced predictive interpretations when compared to other prompting and evaluation schemes, though some other methods performed similarly (see \Cref{sec:autointerp_expts} for details). Importantly, we found experimentally that higher-fidelity interpretations according to our measure have better predictive performance (\Cref{sec:fidelity_predictiveness}), justifying our approach to selecting interpretations.





































