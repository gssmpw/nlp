\section{Results}
\label{sec:results}


\subsection{Synthetic Datasets}


\paragraph{\ourmethod recovers ground-truth hypotheses.} 
Table \ref{tab:metrics-comb} shows quantitative metrics: \ourmethod beats all baselines on 8 of 9 dataset-metric pairs. 
As captured by surface similarity, most of \ourmethod's inferred hypotheses either perfectly match or are related to the references.
On \wiki-5 (Table \ref{tab:syndata-refvsinferred}), we retrieve two hypotheses exactly; the other three are slightly too specific (\ourmethod infers a topic \hyp{is about a New York State Route or highway}, but the reference includes all Northeastern roads, not just New York) or broad (it infers \hyp{historical battles} but does not specify \hyp{battles since 1800}), but all inferred hypotheses are always related to a reference, i.e., has surface similarity $\ge$ 0.5. 
No baselines meet this standard on \wiki-5: \nlparam and \hypogenic output 3/5 and 2/5 hypotheses that are unrelated to any of the reference topics, respectively. 
\bertopic performs well, but outputs redundant hypotheses about state highways and misses `battles' entirely.

Our hypotheses also closely match ground-truth when used to annotate heldout data.
The mean F1 score compares how closely annotations according to an inferred hypothesis (e.g.~\hyp{is about a Simpsons episode}) match ground-truth labels (whether the \wiki article belongs to the human-annotated Simpsons category). 
\ourmethod beats all baselines on this metric, especially outperforming \nlparam (+0.36, on average) and \hypogenic (+0.40).
Finally, regressing the label using each method's annotations, \ourmethod achieves higher AUC than all baselines.
Overall, \ourmethod usually outperforms \textsc{BERTopic}—which is designed exactly for this topic inference setting—and substantially outperforms two state-of-the-art LLM hypothesis generation methods.


\subsection{Real-World Datasets}

Table \ref{tab:metrics-comb} shows quantitative metrics for the real-world datasets. 
Tables \ref{tab:headlines-signif}, \ref{tab:congress-signif}, and \ref{tab:yelp-signif} show all significant hypotheses on \headlines, \congress, and \yelp respectively.


\paragraph{\ourmethod identifies more distinct hypotheses than other methods.} Compared to baselines, \ourmethod produces the most hypotheses which are significant in a multivariate regression.
Importantly, this evaluation reflects successful \textit{generalization}: the hypotheses are learned on the training and validation sets, and they remain significantly associated with the target variable when GPT-4o-mini annotates them on the heldout set.
Across the three datasets, \textbf{45 out of 60 candidate hypotheses are significant}, substantially ahead of the 24, 23, and 20 for \nlparam, \bertopic, and \hypogenic respectively. 

When evaluating overall predictive performance, \ourmethod beats baselines on 8 of 9 comparisons, demonstrating strongly predictive hypotheses. 
The exception is that \hypogenic achieves 2\% higher $R^2$ on Yelp.
However, \hypogenic's most predictive hypotheses, such as \hyp{expresses disappointment with the overall dining experience}, essentially restate the rating prediction task.
While LLMs can perform this prediction well, these hypotheses do not reveal specific insights for the task. 
In contrast, our hypotheses—in Table \ref{tab:yelp-signif} for \textsc{Yelp}—tend to be more specific: we find that negative reviews mention food poisoning, rude and aggressive staff, or dishonest business practices. 




\subsection{Evaluating Hypothesis Novelty}
\label{sec:novelty}

Beyond quantitative comparisons against baselines, we evaluate if \ourmethod discovers new hypotheses even on datasets that have been thoroughly analyzed in prior work. 


\paragraph{\congress.} A classic study from \citet{gentzkow_measuring_2016} identifies bigrams and trigrams in Congressional speeches that mark Republican (R) or Democrat (D) speakers. 
We compare to their work by computing counts, in each speech, of their full reported n-gram lists (150 R / 150 D); we also run \ourmethod to produce 100 hypotheses, of which 33 are significant.
A regression using only n-gram counts achieves AUC 0.61, while including our 33 hypotheses increases AUC to 0.74. 
Notably, 28 out of 33 hypotheses remain significantly predictive when controlling for n-grams, showing that our hypotheses add signal to classical methods.


Beyond predictive performance, \ourmethod offers two qualitative advantages. 
First, our hypotheses are inherently interpretable. For example, while \citet{gentzkow_measuring_2016} report that ``oil companies'' correlates with Democrats, it requires context to interpret: are speakers referring to oil companies positively or negatively? In contrast, \ourmethod's corresponding hypothesis is \textit{criticizes oil companies or oil industry practices}. 
Second, our hypotheses capture nuanced patterns that n-grams cannot expresss, like \textit{criticizes government resource allocation, highlighting disparities between domestic needs and actions abroad}.




\paragraph{\headlines.} 
To test our discoveries against more modern methods, we compare to \citet{batista_words_2024}, an LLM hypothesis generation study designed for \headlines and aimed at producing new insights for marketing researchers. 
Each of their 5 hypotheses is validated via human annotation.
Using our heldout evaluation protocol, we compute annotations using their hypotheses, which achieve AUC 0.59 at predicting binary engagement; adding in \ourmethod's 15 significant hypotheses increases AUC to 0.70.
Crucially, \textbf{all 15 of our hypotheses remain significant} when controlling for the prior discoveries in a shared regression.

Several other prior works have also studied \headlines using classical dictionary-based methods \citep{robertson_negativity_2023, gligoric_linguistic_2023, banerjee_language_2024, aubinlequere_when_2025}. 
\ourmethod qualitatively improves on these findings by producing more specific hypotheses.
For example, \citet{robertson_negativity_2023} find that negative words increase engagement, and positive words decrease it.
\ourmethod supports this finding, and also make it more precise: we find that \hyp{situations that evoke discomfort or embarrassment} and mentions of offensive behavior increase clicks, while \hyp{initiatives that positively impact a community} decreases them (Table \ref{tab:headlines-signif}).
\citet{banerjee_language_2024} find that words in an ``emotional intensity'' dictionary increase engagement, which may be difficult to operationalize; our corresponding finding is that headlines referencing \hyp{being surprised, shocked, or unprepared} increase engagement.
Our findings are more precise, and can therefore provide richer insights to researchers.



These results demonstrate that \ourmethod, despite no dataset-specific design, uncovers novel hypotheses that advance prior work from domain experts—suggesting broad applicability across diverse settings.







\subsection{Costs}

In Table \ref{tab:costs}, we report the runtimes, LLM inference token counts, and costs (at current OpenAI API pricing) for all methods on \congress; the trends are similar for all datasets.
Runtimes include training the SAEs on one NVIDIA A6000 GPU.
This step is fast because our SAEs can afford to be small ($\sim$$10^3$ features, compared to $10^7$ in \citet{gao_scaling_2024}), since our focus is to learn domain-specific features rather than features of all language.
\ourmethod is substantially cheaper than \nlparam and \hypogenic: despite the fact that the latter methods are trained on 20K examples on \congress (vs.~114K for \ourmethod), \textbf{\ourmethod is $\sim$30-50$\times$ faster and 10-50$\times$ cheaper}.
This is because \ourmethod's annotation requirement scales only with the number of hypotheses $H$, while these two methods scale with both $H$ and $N$.

\bertopic is cheapest, and remains a good baseline for a resource-limited analysis.
However, \ourmethod can also be made cheaper by skipping the non-essential label validation step: instead of generating 3 candidate labels per neuron and choosing the highest-fidelity one, we can simply generate 1 label per neuron and use it directly.
This reduces cost to that of \bertopic, while still exceeding its performance; we describe this ablation in Appendix \ref{sec:hyperparams_neuronlabeling}.



\input{tables/costs}
