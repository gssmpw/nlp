\section{Introduction}
\label{sec:intro}

Large language models (LLMs) show promise as a tool for \textit{hypothesis generation}. Discovering relationships between text data and a target variable is an important and fundamental task with diverse applications in economics, political science, sociology, medicine, and business \citep{grimmer2010bayesian, rathje2021out, sun_negative_2022, gentzkow_what_2010, monroe_fightin_2009, nelson2020computational, ranard2016yelp, ting2017using}. What features of a restaurant review predict a low rating? What features of a social media post predict whether it will go viral? What features of a patient's clinical notes predict if they will develop cancer? 

Automated approaches to answering these questions have the potential to significantly expand and accelerate scientific discovery \citep{ludwig_machine_2023a}. Indeed, multiple lines of work---spanning decades of research---have sought to extract text features that can predict a target variable \citep{blei_latent_2003, monroe_fightin_2009}. Recent LLM-based hypothesis generation methods are especially exciting since they operate directly at the level of human-interpretable natural language concepts \citep{zhou_hypothesis_2024, batista_words_2024, ludan_interpretabledesign_2024, zhong_explaining_2024}. Concretely, hypothesis generation methods take a dataset of texts linked to a target variable (e.g., headlines and their engagement level) and hypothesize natural language concepts that predict the target variable (e.g., \textit{mentions being surprised or shocked} or \textit{asks a question to the reader}).

However, basic hurdles impede the full usage of LLMs for hypothesis generation. Consider two natural approaches. The first prompts an LLM with positive and negative examples and asks it to hypothesize concepts that distinguish them \citep{zhou_hypothesis_2024, batista_words_2024, ludan_interpretabledesign_2024}. However, due to context window and reasoning limitations, we can only give LLMs a few training examples at a time, making this hard to scale. A second approach aims to interpret a model fine-tuned to predict the target variable \citep{zhong_explaining_2024}, leveraging the full training dataset. However, neurons are often hard to interpret \citep{elhage_toy_2022}. In summary, efficient and scalable learning of statistical relationships requires numerical representations of text (e.g., \textit{neurons}), but interpreting neurons is hard.

\begin{figure*}
\begin{center}
    \includegraphics[width=0.83\linewidth]{figures/HypotheSAEs.pdf}
\end{center}
    \caption{\ourmethod is a general method that outputs natural language hypotheses from text datasets.}
\end{figure*}

Our first contribution is a theoretical framework that connects model interpretation and hypothesis generation, clarifying the challenge above. \Cref{prop:errors-in-variables} gives a ``triangle inequality'' for hypothesis generation: if a neuron is predictive of the target variable, then a sufficiently high-fidelity interpretation of the neuron is also predictive. This result directly motivates a general hypothesis generation procedure: 
\begin{enumerate}
    \item (Feature generation) Learn interpretable neurons (i.e., that tend to fire in the presence of human concepts).
    \item (Feature selection) Select neurons that predict the target variable.
    \item (Feature interpretation) Generate high-fidelity natural language interpretations of these neurons.   
\end{enumerate}

Our second contribution is a method, \ourmethod, that successfully implements this procedure. A primary challenge is learning the interpretable neurons in step 1---after all, a long line of work has established the challenge of interpreting neural networks \citep{olah_feature_2017, kim_interpretability_2018, elhage_toy_2022}. To overcome this, we train a sparse autoencoder (SAE; \citet{makhzani_ksparse_2014}) on pre-trained text embeddings from our dataset. Neurons in the SAE hidden layer are highly interpretable, while retaining the explanatory power of embeddings \citep{cunningham2023sparse, bricken2023towards, templeton2024scaling, gao_scaling_2024, oneill_disentangling_2024}. Intuitively, this architecture leverages the fact that natural language is sparse; only a small fraction of all human concepts are expressed at a time. In step 2, we select neurons that are predictive of the target variable (e.g., using Lasso). In step 3, we automatically generate high-fidelity interpretations of these predictive neurons by prompting an LLM with texts that activate the neuron, building on recent work in autointerpretability \citep{bills2023language}. These interpretations serve as hypotheses.

We evaluate \ourmethod against recent state-of-the-art methods. We consider three synthetic tasks, as well as three real-world tasks of practical interest: hypothesizing the relationship between headlines and engagement, speech text and political party, and review text and rating. Example \ourmethod hypotheses are given below:

\begin{center}
\small
\begin{tabular}{p{1.7cm}p{5.7cm}}
\toprule
Task & Example Hypotheses (Abbreviated) \\ \midrule
\multirow{2}{2cm}{Headline Engagement}          & (+) \hyp{mentions being surprised or shocked} \\
 & (-) \hyp{asks a question directly to the reader} \\ 
\midrule
\multirow{2}{2cm}{Speaker Party}          & (Rep.) \hyp{discusses illegal immigration} \\
 & (Dem.) \hyp{criticizes tax breaks for the wealthy} \\ 
\midrule
\multirow{2}{2cm}{Restaurant Rating}          & (+) \hyp{mentions plans to return to the restaurant} \\
 & (-) \hyp{mentions issues related to food safety} \\ 
\bottomrule
\end{tabular}
\end{center}

\ourmethod produces many more significant hypotheses than three baseline methods. On three real-world tasks, 45/60 hypotheses generated by our method are significant, compared to at most 24 for the baselines. On two datasets, we identify new hypotheses which previous detailed analyses have not uncovered. Intuitively, by decoupling feature generation from the prediction task, \ourmethod is able to explore a wider range of hypotheses.

\ourmethod is also more than 10$\times$ faster and cheaper than recent LLM baselines, with similar runtime and cost to \bertopic, a standard embedding-based topic model. Filtering hypotheses based on the predictiveness of neurons is significantly cheaper than directly evaluating natural language concepts, since all neuron activations of an input can be computed from a single forward pass of an SAE (in comparison to one LLM call per natural language feature).
