\input{tables/wiki_hypothesis_recovery_SAE}
\input{tables/congress-signif}
\input{tables/yelp-signif}

\section{Data Preprocessing}
\label{sec:preprocessing}

\paragraph{\wiki.} 
We download the dataset as processed by \citet{pham_topicgpt_2024}; the data are derived from the WikiText corpus assembled by \citet{merity_pointer_2016}. 
The articles are categorized by Wikipedia editors; each article has a supercategory, category, subcategory (e.g., \smalltt{Media and Drama > Television > The Simpsons Episodes}). 
We focus on recovering article subcategories, since these are the most specific and therefore most difficult to fully recover.
After filtering out duplicates and infrequent ($<$100 articles) subtopics, the dataset contains 11,979 items spanning 69 subcategories.
For the \wiki-5 dataset, we set the ground-truth topics to the 5 most common subcategories. 
That is, we label all 1,771 articles (14.8\%) in these subcategories as positives, while all other article subcategories are negatives.
The \wiki-15 dataset is constructed similarly using the 15 most common subcategories (4,190 positives, 35.0\%).

\paragraph{\bills.}
We also download \bills from \citet{pham_topicgpt_2024}; the dataset was originally assembled by \citet{hoyle_are_2022} using GovTrack\footnote{\href{https://www.govtrack.us/congress/bills/}{https://www.govtrack.us/congress/bills/}}.
The bills are from the 110th-114th U.S. Congresses (Jan 2007 - Jan 2017).
Each bill has a topic and subtopic.
After filtering out very short bill texts (which may not have been properly transcribed), duplicate bill texts, bills with no subtopic, and bills with infrequent ($<$100) subtopics, there are 20,834 items spanning 70 subtopics.
We label the 5,038 bills (24.2\%) with the 5 most common subtopics as positives, and all others as negatives.

\paragraph{\headlines.} We use the \headlines dataset as collected and released by the Upworthy Research Archive \citep{matias_upworthy_2021}.
All of the data is derived from web traffic to Upworthy.com, a digital media platform known for its attention-grabbing articles.
The dataset consists of thousands of A/B tests, where users were randomly shown one of several possible headline variations for the same underlying article.
Each headline is linked to a corresponding number of impressions and clicks, allowing researchers to estimate the causal effect of headline text on \textit{click-rate} (clicks$/$impressions, also referred to as clickthrough-rate or engagement).

To preprocess the data, we start by downloading all publicly available data from the Upworthy archive\footnote{\href{https://upworthy.natematias.com/index}{https://upworthy.natematias.com/index}}, which includes clicks and impressions for 122,565 distinct headlines (we remove 28K rows from a problematic data range as recently reported by the dataset authors\footnote{\href{https://upworthy.natematias.com/2024-06-upworthy-archive-update.html}{https://upworthy.natematias.com/2024-06-upworthy-archive-update.html}}). 
We then group headlines which have the same \smalltt{test\_id} (signifying that those headlines were randomized against one another).
There are 14,128 such groups with at least two headlines.
Following \citet{zhou_hypothesis_2024}, we take the headlines with max and min click-rates in each group, and design a binary classification task to predict which headline in the pair received higher engagement.
That is, we construct tuples (headline A, headline B, label), where headline A and headline B are randomly shuffled, and the label is 1 if headline A received higher click-rate than headline B.

Following \citet{batista_words_2024}, we take specific care to prevent train-holdout leakage on this dataset. 
Specifically, the same headline texts may be used in multiple randomized experiments (i.e., the same headline may occur in different \smalltt{test\_id}'s).
We ensure that no identical headlines will appear in both the train split and the heldout split; we encourage future researchers to follow this practice, since splitting on \smalltt{test\_id} alone does not fully prevent headline leakage.

To handle the pairwise design, we train the SAE on the embeddings of all unique headlines in the training set, and then compute the activation matrices $Z_{\text{SAE}}^A$ and $Z_{\text{SAE}}^B$ for all of the headline A's and headline B's respectively.
We select predictive neurons by identifying columns in the matrix difference $\Delta Z_{\text{SAE}} := Z_{\text{SAE}}^A - Z_{\text{SAE}}^B$ which are predictive of $\mathbf{y}$.

\paragraph{\yelp.} We download the Yelp Open Dataset\footnote{\href{https://business.yelp.com/data/resources/open-dataset/}{https://business.yelp.com/data/resources/open-dataset/}}, and we filter to the 4.72M reviews where the business contains the `Restaurant' tag.
From this, we randomly sample training, validation, and heldout sets (200K, 10K, and 10K respectively).

\paragraph{\congress.} 
We download all congressional speech transcripts from the 109th U.S. Congress \citep{gentzkow_what_2010}\footnote{\href{https://data.stanford.edu/congress_text}{https://data.stanford.edu/congress\_text}}, which met from January 2005 to January 2007. 
We filter speeches which are very short (less than 250 characters) and include only speeches from Republican or Democrat speakers; the dataset is roughly balanced (51.7\% Republican).
If a speech is longer than ten sentences, we split it into ten sentence chunks, and exclude any chunks beyond the first five (to avoid over-representation of specific speeches).
For the heldout set, we restrict to one chunk per speech to ensure that our hypotheses generalize to a diverse corpus. 
Ultimately, we use 114K speech chunks for training, 16K for validation, and 12K for heldout eval. 
If a speech has multiple excerpts, we ensure that all of them are included in the same split to avoid leakage.


\section{Hyperparameters}
\label{sec:hyperparams}

\subsection{Training sparse autoencoders} 
We train $k$-sparse autoencoders, where each forward pass masks all neurons besides the top-$k$ activating ones \citep{makhzani_ksparse_2014, gao_scaling_2024}.
The most influential hyperparameters, which we tune for each dataset, are $M$ and $k$.
We choose $(M, k)$ by maximizing validation performance. 
Specifically, for each combination of $(M, k)$, we evaluate validation AUC (or $R^2$ on \yelp) after fitting an $L_1$-regularized predictor with exactly $H \ll M$ nonzero coefficients. 
We constrain $M$ and $k$ to powers of two to tractably limit the search space.

These parameters, broadly, control the level of granularity of the concepts learned by each SAE neuron: $M$ is the total number of concepts which can be learned across the entire dataset, and $k$ is the number of concepts which can be used to represent each instance.
For intuition, assume that with $(M, k) = (16, 4)$ on \yelp, the SAE learns a single neuron that fires when reviews mention \textit{price}; with $(M, k) = (32, 4)$, the SAE may learn separate neurons for reviews which mention \textit{high prices} vs.~\textit{low prices} (described as ``feature splitting'' in \citet{bricken2023towards}); with $(M, k) = (32, 8)$, the SAE may learn to represent more niche features altogether, such as the \textit{number of exclamation marks}.
In general, increasing either parameter yields more granular features, at the risk of producing some neurons that are uninterpretable or redundant.

In some cases we would like features at multiple levels of granularity: for example, we may want both the general feature ``mentions prices'' and the more specific feature ``mentions that the cocktails were cheap during happy hour'', but training a single large SAE (e.g. $(1024, 32)$) may only produce features as specific as the latter. 
A simple solution is to train multiple SAEs of different sizes, concatenate their activation matrices, and select features from any of them. 
We find that this approach is helpful (improves validation AUC) on the \headlines dataset. 
Ultimately, we use the following values:
 
\begin{itemize}
    \item \wiki-5: $(32, 4)$.
    \item \wiki-15 \& \bills: $(64, 4)$.
    \item \headlines: $(256, 8); (32, 4)$.
    \item \yelp: $(1024, 32)$.
    \item \congress: $(4096, 32)$.
\end{itemize}

Following prior work, we add several optimizations to improve SAE training. Following best practices based on results from \citet{bricken2023towards}, we (i) tie the weights of the pre-encoder bias and the decoder bias, $b_{\mathrm{pre}} = b_{\dec}$; (ii) initialize $b_{\mathrm{pre}}$ to the geometric median of the training set; (iii) initialize $W_{\dec} = W_{\enc}^T$; and (iv) force all decoder rows to have unit norm. 

To minimize the number of ``dead'' latent neurons in the SAE—neurons which are never selected by the $\topk$ filter—we use an auxiliary loss \citep{gao_scaling_2024, oneill_disentangling_2024}. 
The auxiliary loss term provides nonzero gradients for up to $k_{\text{aux}}$ neurons which were not active in the last several forward passes.
We fit these neurons to the residual reconstruction error left by the $\topk$ neurons. 
That is, let $z_i^{\auxk}$ consist of the activations of the top $k_{\text{aux}}$ dead neurons in $z_i$; then
$$\mathcal{L}_{\text{aux}} = ||W_{\dec} z_i^{\auxk} - (e_i - \hat{e}_i)||^2_2,$$
and the full loss, then, is $\mathcal{L} = \mathcal{L}_{\topk} + w_{\text{aux}}\mathcal{L}_{\auxk}$. 

Hyperparameters for the auxiliary loss and other training hyperparameters are given below (identical for all experiments):

\begin{itemize}
    \item $\bm{k_{\text{aux}}}$: $2\cdot k$; that is, up to $2\cdot k$ dead neurons are used to fit the $\topk$ reconstruction error.
    \item \textbf{Dead neuron threshold steps}: 256. This is the number of consecutive steps for which a neuron was not selected by $\topk$ to be considered ``dead.''
    \item \textbf{Auxilliary loss coefficient $w_{\text{aux}}$}: $1/32$. This parameter gives the weight of the auxiliary reconstruction's loss.
    \item \textbf{Batch size}: 512; \textbf{learning rate}: 5e-4; \textbf{gradient clipping threshold}: 1.0.
    \item \textbf{Epochs}: Up to $200$, with early stopping after 5 epochs of validation loss not decreasing.
\end{itemize}

\subsection{Interpreting neurons with LLMs}
\label{sec:hyperparams_neuronlabeling}

There are several parameters for neuron interpretation which we fix across experiments:
\begin{itemize}
    \item \textbf{Interpretation model}: GPT-4o (version 2024-11-20).
    \item \textbf{Temperature}: 0.7.
    \item \textbf{Number of highly-activating examples}: 10.
    \item \textbf{Number of weakly-activating examples}: 10.
    \item \textbf{Maximum word count per example}: 256 (examples longer than this are truncated).
    \item \textbf{Number of candidate interpretations}: 3 (we choose the highest-fidelity interpretation out of 3 candidates).
    \item \textbf{Number of samples to evaluate fidelity}: 200.
\end{itemize}

In Appendix \ref{sec:autointerp_expts}, we describe experiments which justify these choices: they either improve fidelity or have no effect compared to alternate choices. 
We also provide the full interpretation prompt in Figure \ref{fig:interp_prompt_binary}.

There is one hyperparameter which we adjust per dataset: the \textbf{bin percentiles} from which we sample highly-activating and weakly-activating examples for the neuron interpretation prompt.
By default, highly-activating texts are sampled from the top decile of positive activations $[90, 100]$, and weakly-activating texts are sampled from the bottom decile of positive activations $[0, 10]$.
This works well in general, especially for larger datasets (\yelp, \congress).
For smaller datasets like \wiki, the $[90, 100]$ bin produces interpretations which are too specific (e.g., ``articles about Napoleon'', even if the neuron fires on articles about European leaders more generally).
Note that this requires a judgment call: what is ``too specific'' depends on practitioner needs.
For example, on \wiki, we choose bins such that the resulting interpretations are roughly as granular as the topics in the dataset\footnote{Choosing this parameter requires similar decision-making to choosing the minimum cluster size in \bertopic.}. 
We ultimately choose the following percentile bins for highly-activating texts:

\begin{itemize}
    \item \wiki-5, \wiki-15, \headlines: $[80, 100]$.
    \item \bills: $[50, 100]$.
    \item \yelp, \congress: $[90, 100]$.
\end{itemize}

Choosing the \textbf{number of candidate interpretations} depends on budget. 
In Appendix \ref{sec:autointerp_expts} we find that selecting the interpretation with highest fidelity from a pool of 3 candidates increases the resulting fidelity modestly, yet statistically significantly; we therefore use \textbf{3 candidate interpretations} for all experiments.
However, we also ablate this step—that is, generating a single candidate interpretation and using it directly, without computing fidelity. 
On \congress (we did not test on the other datasets), the resulting hypotheses are of similar quality: 14/20 are significant, with AUC 0.70, compared to 15/20 significant and AUC 0.70 when using 3 candidate interpretations\footnote{To illustrate this point, we tested solely on \congress; we expect similarly modest differences on other datasets.}. 
This variant of our method, \textsc{Sae-NoVal}, costs as little as \bertopic (Table \ref{tab:costs}). 

\section{Supporting Experiments}
\label{sec:autointerp_expts}

Our final neuron interpretation procedure for our main experiments is described in \S\ref{sec:methods_main}, with hyperparameters provided in Appendix \ref{sec:hyperparams_neuronlabeling}. 
Here, we describe several experiments to justify our procedure. 
The section is organized as follows.
In \ref{sec:fidelity_predictiveness}, we show that our metric of interpretation fidelity—the F1 score comparing concept annotations against neuron activations—is a useful selection metric to improve downstream hypothesis quality.
In \ref{sec:interp_fidelity}, we test several different interpretation strategies and hyperparameters and measure their impacts on fidelity.
In \ref{sec:performance_losses}, we analyze how much prediction signal is lost at each stage of our method, shedding light on how future work can improve hypothesis quality.

\subsection{Higher-fidelity interpretations improve hypotheses.}
\label{sec:fidelity_predictiveness}

To test whether fidelity is a useful metric to select from different neuron interpretations, we conduct the following experiment on the Yelp dataset:
\begin{enumerate}
    \item Train an SAE on Yelp (Step 1), and identify the top-20 predictive neurons via Lasso (Step 2).
    \item Generate 10 candidate interpretations (at temperature 0.7, and with different random samples in the interpretation prompt) for each neuron using the hyperparameters in \ref{sec:hyperparams_neuronlabeling} (Step 3).
    \item Compute the \textit{fidelity} for each candidate interpretation: that is, binarizing 100 examples in the top decile of the neuron's positive activations as positives, and 100 examples in the bottom decile of the neuron's positive activations as negatives, compute the F1 score between the GPT-4o-mini annotations according to the interpretation against the binarized neuron activations. 
    \item Store two sets of neuron interpretations: one set includes the interpretations with the best F1 score for each neuron (high-fidelity), and the other set includes the interpretations with the worst F1 score for each neuron (low-fidelity).
    \item Compute annotations on the full heldout set (10K examples) for the high-fidelity hypotheses and the low-fidelity hypotheses, and compare how predictive they are as measured by $R^2$.
\end{enumerate}

This procedure yields three interesting results. 
First, there is enough variation in the 10 candidate interpretations such that the best and worst F1 scores are substantially different: the median best F1 is 0.79, while the median worst F1 is 0.53.
Second, we find that the high-fidelity set is indeed more predictive: in a multivariate OLS, the high-fidelity set achieves an $R^2$ of 0.76, compared to 0.61 for the low-fidelity set.

Third, most importantly, using a neuron's high-fidelity interpretation results in a more predictive hypothesis, on average.
Figure \ref{fig:fidelity_experiment} shows that, for 17 out of 20 neurons, moving from the low-fidelity interpretation to the high-fidelity interpretation increases the $R^2$ of the resulting hypothesis ($p$-value, paired $t$-test: 0.002).
For example, for one neuron, the high-fidelity interpretation is ``\textit{mentions disputes or issues related to billing or refunds}'' (F1: 0.79), while the low-fidelity interpretation is more generically ``\textit{mentions an attempt by the restaurant or management to address or respond to a problem or complaint}.'' 
The former hypothesis achieves heldout $R^2 = 0.10$ with restaurant rating, while the latter is not significantly correlated with rating ($R^2 = 0.003$).
These experiments show that \textbf{fidelity—as measured via F1 score—is a useful heuristic to choose between multiple candidate neuron interpretations}, when our goal is to produce predictive hypotheses.

\begin{figure*}[!h]
\begin{center}
    \includegraphics[width=0.5\linewidth]{figures/fidelity_R2_v1.png}
\end{center}
    \caption{For 17 of the top 20 neurons on \yelp, a higher-fidelity neuron interpretation results in a more predictive hypothesis. 
    Fidelity is measured by the F1 score between the concept annotations and the neuron activations on a sample of 100 highly-activating and 100 weakly-activating examples.
    We generate 10 candidate interpretations for each neuron, and compare the interpretations with best- and worst-F1.
    Predictiveness is measured by computing the $R^2$ between the concept annotations and $Y$ (restaurant rating) for 10K heldout examples.
    }
    \label{fig:fidelity_experiment}
\end{figure*}

Finally, following prior work \citep{bills2023language}, we also tested another metric of fidelity: the $R^2$ between the concept annotations and the neuron activations across 100 top-activating samples and 100 random samples. 
Choosing the top candidate interpretation based on this alternative fidelity metric did not produce hypotheses that were significantly more or less predictive than using our F1 fidelity metric. 

\subsection{Effects of the neuron interpretation procedure on fidelity.} 
\label{sec:interp_fidelity}

Having established that improving interpretation fidelity improves downstream predictiveness, we search across different hyperparameters for neuron interpretation to maximize fidelity. 
Our final procedure is to generate 3 candidate interpretations for each neuron by prompting GPT-4o with 10 examples in the top decile of positive activations vs. 10 examples in the bottom decile of positive activations. 
We use the highest fidelity interpretation out of these 3.
There are two sources of non-determinism: each candidate interpretation is generated with a different random seed to sample examples, and we use an LLM temperature of 0.7.

To compare this strategy against other strategies—or to compare strategy A and strategy B more generally—we compute interpretations, and their F1 scores, for the top 100 neurons on \yelp under both strategies separately.
We run a $t$-test comparing the pairs of resulting F1 scores to test whether one strategy produces consistently higher F1 scores than the other.

We observe the following:
\begin{itemize}
    \item \textbf{Number of candidate interpretations:} Relative to baseline (generating 3 candidate interpretations and taking the one with highest F1 score), generating only a single candidate interpretation is significantly worse (-0.04). 
    Generating 5 candidate interpretations improves significantly on 3 (+0.01 F1, paired $t$-test $p = 0.003$), but the improvement is modest and results in a slower, more expensive method. 
    Therefore, we use only 3 candidate interpretations.
    
    \item \textbf{Top-random vs. high-weak sampling strategy:} Prior work (e.g.~\citet{templeton2024scaling, bills2023language}) interprets SAE neurons by prompting with the top-activating texts and random texts. 
    Relative to our baseline (prompting with highly-activating texts, but not necessarily top-activating; and weakly-activating texts instead of random), this alternate approach has no significant effect on F1 score, though the interpretations tend to be more specific: that is, they have higher precision and lower recall. 
    This is because a neuron's absolute top-activating examples generally share a more specific characteristic than examples sampled randomly from the top decile.
    Though neither strategy wins clearly in terms of F1, we choose high-weak sampling for our experiments because, empirically, top-random produces hypotheses which can be \textit{too} specific (thereby lowering predictiveness); this is discussed further in \ref{sec:hyperparams_neuronlabeling}.
    \textbf{Our code defaults to top-random} for simplicity, but practitioners should treat sampling strategy as a tunable hyperparameter that depends on desired hypothesis granularity.
    
    \item \textbf{Continuous vs. binary prompt structure:} Prior work (e.g.~\citet{zhong_explaining_2024}) prompts the interpreter LLM with a sorted list of texts and their continuous activations. Relative to our baseline (showing separate lists of positive samples and negative samples), this significantly lowers F1 score (-0.06), so we use the binary prompt structure.
    
    \item \textbf{Chain-of-thought:} Prior work (e.g.~\citet{oneill_disentangling_2024}) uses a chain-of-thought reasoning prompt to interpret neurons, where the LLM is encouraged to iteratively discover a concept which all of the positive samples contain and none of the negative samples contain. 
    Relative to baseline, we found that CoT has an insignificant but weakly negative effect (-0.03 mean F1), and also sometimes fails to output any interpretation. 
    However, this is still a promising line for future work: it's possible that while GPT-4o does not reason well, better models or reasoning-specific models (OpenAI o1, DeepSeek R1, \textit{etc.}) will produce higher fidelity labels with chain-of-thought.
    
    \item \textbf{Sample count:} Relative to baseline (prompting with 20 total samples, 10 highly-activating and 10 weakly-activating), prompting with 10 total samples lowers average F1 score by 0.03. Prompting with 50 samples has no effect, so we use 20 to favor shorter prompts.
    
    \item \textbf{Temperature:} Relative to baseline (temperature 0.7), using a lower (0.0) or higher (1.0) temperature has an insignificant but weakly negative effect (-0.01 in mean F1), so we use 0.7.
\end{itemize}



\subsection{Localizing losses in predictive performance to different steps of the method.}
\label{sec:performance_losses}

\ourmethod starts from embeddings and ultimately produces hypotheses, after three steps. 
It is worth asking—how does predictive performance change at each step of our method?
Answering this question points to possible improvements: if the SAE's full hidden representation is much less predictive of restaurant ratings than the input text embeddings, perhaps we need a larger (more expressive) SAE, or more data to train the SAE's representations; meanwhile, if the largest performance loss comes from translating neurons into natural language hypotheses, perhaps we need a stronger LLM for interpretation.

Figure \ref{fig:performance_losses} displays this experiment. 
At each stage, we fit a logistic or linear regression on the training set to predict the labels on \headlines, \yelp, and \congress.
The four stages are: (i) text embeddings (length-1536 vectors of floats); (ii) the full SAE activation matrix, $Z_{\text{SAE}}$ (length-288, 1024, and 4096 vectors of floats for the three tasks, respectively); (iii) the activations of the top-20 selected SAE neurons (length-20 vectors of floats); and (iv) the annotations from the interpreted hypotheses (length-20 vectors of binary 0/1 values).

\begin{figure*}[!h]
\begin{center}
    \includegraphics[width=0.6\linewidth]{figures/performance_losses.png}
\end{center}
    \caption{Performance losses at each step of the \ourmethod pipeline. 
    The leftmost point shows the performance of training a supervised single-layer model to predict the outcome using text embeddings; the second point shows the supervised performance using the full SAE representation; the third point shows the supervised performance using the top-20 SAE neurons; the rightmost point shows the supervised performance using the top-20 hypothesis annotations after neuron interpretation. 
    }
    \label{fig:performance_losses}
\end{figure*}

The trends depend on the dataset. 
On \headlines, we find that the performance drops most at the first stage (-0.08 AUC): the full SAE is significantly less expressive than the original embeddings. This could suggest using a larger SAE, but empirically we find that increasing the SAE size does not improve AUC, perhaps because there is not enough data ($\sim$14K headlines) to make use of more parameters.
On \yelp, we find that performance is mostly retained until neuron interpration, at which point $R^2$ drops by 0.06. 
This suggests some combination of: our neuron interpretations are imperfect, GPT-4o-mini is not annotating the hypotheses faithfully, or binary annotations are not expressive enough to capture the neuron activations (which are continuous).
Finally, on \congress, the only drop comes from using the top-20 neurons to predict the label instead of the full SAE representation (4096 neurons). 
This implies that 20 features aren't expressive enough; or, more optimistically, that there are more than 20 distinct hypotheses to discover on this dataset (see \S\ref{sec:novelty}). 
Remarkably, there are no losses in the interpretation step on the \congress dataset, suggesting that on this dataset our interpretations are very high-fidelity.
Ultimately, this analysis can be a useful diagnostic tool to direct one's efforts when trying to improve hypothesis quality: losses from embedding $\Rightarrow$ full SAE suggest that the dataset and/or the SAE are too small; losses from full SAE $\Rightarrow$ top-$H$ SAE suggest that $H$ is too small; losses from top-$H$ SAE $\Rightarrow$ top-$H$ hypotheses suggest that the interpretation step can be improved.
Meanwhile, if the top-$H$ hypotheses achieve similar AUC to the underlying embeddings, this implies that there may be no more ``juice to squeeze'' in the dataset.

\section{Hyperparameters for Baseline Methods}

\paragraph{\bertopic \citep{grootendorst_bertopic_2022}.}
To run \bertopic\footnote{\href{https://github.com/MaartenGr/BERTopic}{https://github.com/MaartenGr/BERTopic}}, we feed in the same OpenAI  embeddings that we use for \ourmethod, and otherwise use default choices for the algorithm: UMAP for dimensionality reduction, HDBSCAN for clustering, and c-TF-IDF to compute the top words associated with each topic.
We tune the minimum cluster size hyperparameter in \bertopic, which controls topic granularity. 
The default value is 10, which often produces topics which are too granular (e.g., relative to the underlying granularity of the subtopics on \wiki and \bills). 
We test all values in $\{\text{10, 20, 50, 100, 200, 500}\}$. 
We choose this parameter by maximizing the validation performance of the $L_1$-regularized predictor which uses $H$ features; this is identical to how we choose hyperparameters for \ourmethod.
We use default values of all other hyperparameters, which is standard in prior work (e.g., \citet{pham_topicgpt_2024}). 
After running \bertopic, we label topics using the prompt Figure \ref{fig:bertopic_prompt}.

\paragraph{\nlparam \citep{zhong_explaining_2024}.} 
We run \nlparam using the publicly available implementation\footnote{\href{https://github.com/ruiqi-zhong/nlparam}{https://github.com/ruiqi-zhong/nlparam}}. 
We set the number of candidate labels per bottleneck layer neuron to 3 (the same as our method), and we use the same text embeddings as our method (OpenAI's text-3-embedding-small).
\citet{zhong_explaining_2024} release two different prompts, one for ``simple'' hypotheses and one for ``complex'' hypotheses; following their work, we use the ``simple'' prompt for our 3 synthetic tasks and the ``complex'' prompt for our 3 real-world tasks.
For all other hyperparameters, we use default values.

\paragraph{\hypogenic \citep{zhou_hypothesis_2024}.} We run \hypogenic using the publicly available implementation\footnote{\href{https://github.com/ChicagoHAI/hypothesis-generation}{https://github.com/ChicagoHAI/hypothesis-generation}}. 
We use the default parameters as defined in examples/generation.py. Following the format in their example tasks, we write custom prompts in a consistent format describing each our of tasks. 
The method uses the same LLM for hypothesis generation and hypothesis scoring, and we run all experiments using GPT-4o-mini (the default model in their code repository, and also the model that we use for scoring hypotheses in \ourmethod).




\section{Proofs}\label{sec:proof}

\begin{proof}[Proof of \Cref{prop:errors-in-variables}]
Set $p_{ab}:=\Pr[\hat{Z}=a, Z=b]$ and $y_{ab}:=\EE[Y|\hat{Z}=a, Z=b]$
for $a,b\in \{0,1\}.$ Then
\begin{align}
    \EE[Y|\hat{Z}=1] &= \frac{y_{11}p_{11} + y_{10}p_{10}}{p_{11}+p_{10}}\\
    \EE[Y|Z=1] &= \frac{y_{11}p_{11} + y_{01}p_{01}}{p_{11}+p_{01}}.
\end{align}
Therefore,
\begin{align}
    \EE[Y|\hat{Z}=1] - \EE[Y|Z=1] &= \frac{y_{11}p_{11} + y_{10}p_{10}}{p_{11}+p_{10}} - \frac{y_{11}p_{11} + y_{01}p_{01}}{p_{11}+p_{01}}\\
    &= y_{11}\left(\frac{p_{11}}{p_{11}+p_{10}} - \frac{p_{11}}{p_{11}+p_{01}}\right) + \frac{y_{10}p_{10}}{p_{11}+p_{10}} - \frac{y_{01}p_{01}}{p_{11}+p_{01}}\\
    &= y_{11}\left(\frac{p_{01}}{p_{11}+p_{01}} - \frac{p_{10}}{p_{11}+p_{10}}\right) + \frac{y_{10}p_{10}}{p_{11}+p_{10}} - \frac{y_{01}p_{01}}{p_{11}+p_{01}}\label{eq:cat}
\end{align}
Since $y_{10}\le 1$ and $y_{01}\ge 0,$ \eqref{eq:cat} is bounded from above by
\begin{align}
    y_{11}\left(\frac{p_{01}}{p_{11}+p_{01}} - \frac{p_{10}}{p_{11}+p_{10}}\right) + \frac{p_{10}}{p_{11} + p_{10}}.\label{eq:dog}
\end{align}
If $\frac{p_{01}}{p_{11}+p_{01}} - \frac{p_{10}}{p_{11}+p_{10}} \ge 0$, \eqref{eq:dog} is bounded from above by
\begin{align}
    &\left(\frac{p_{01}}{p_{11}+p_{01}} - \frac{p_{10}}{p_{11}+p_{10}}\right) + \frac{p_{10}}{p_{11} + p_{10}}
    = \frac{p_{01}}{p_{11}+p_{01}},
\end{align}
since $y_{11} \le 1.$ Otherwise, \eqref{eq:dog}  is bounded from above by
\begin{align}
    \frac{p_{10}}{p_{11}+p_{10}}.
\end{align}
Also note that \eqref{eq:cat} is bounded from below by
\begin{align}
    y_{11}\left(\frac{p_{11}}{p_{11}+p_{10}} - \frac{p_{11}}{p_{11}+p_{01}}\right) - \frac{p_{01}}{p_{11} + p_{10}} = y_{11}\left(\frac{p_{01}}{p_{11}+p_{01}} - \frac{p_{10}}{p_{11}+p_{10}}\right) - \frac{p_{01}}{p_{11} + p_{10}}.\label{eq:dog-2}
\end{align}
If $\frac{p_{01}}{p_{11}+p_{01}} - \frac{p_{10}}{p_{11}+p_{10}} \le 0$, \eqref{eq:dog-2} is bounded from below by
\begin{align}
    &\left(\frac{p_{01}}{p_{11}+p_{01}} - \frac{p_{10}}{p_{11}+p_{10}}\right) - \frac{p_{01}}{p_{11} + p_{10}}
    = -\frac{p_{10}}{p_{11}+p_{10}},
\end{align}
since $y_{11} \le 1.$ Otherwise, \eqref{eq:dog-2}  is bounded from below by
\begin{align}
    -\frac{p_{01}}{p_{11}+p_{01}}.
\end{align}
Therefore, 
\begin{equation}
    |\EE[Y|\hat{Z}=1] - \EE[Y|Z=1]|\le \max\left\{\frac{p_{10}}{p_{11} + p_{10}}, \frac{p_{01}}{p_{11} + p_{01}}\right\}.
\end{equation}




Analogously, we can bound $|\EE[Y|\hat{Z}=0] - \EE[Y|Z=0]|$ by 
\begin{equation}
    \max\left\{\frac{p_{10}}{p_{00} + p_{10}}, \frac{p_{01}}{p_{00} + p_{01}}\right\}.
\end{equation}
Notice that
\begin{align}
    \frac{p_{10}}{p_{00} + p_{10}} &= \frac{p_{10}}{p_{11}+p_{10}}\cdot \frac{p_{11}+p_{10}}{p_{00} + p_{10}}\\
    &=\frac{p_{10}}{p_{11}+p_{10}}\cdot \frac{\Pr[\hat{Z}=1]}{\Pr[Z=0]}
\end{align}
Similarly,
\begin{align}
    \frac{p_{01}}{p_{00} + p_{01}} &= \frac{p_{01}}{p_{11}+p_{01}}\cdot \frac{p_{11}+p_{01}}{p_{00} + p_{01}}\\
    &=\frac{p_{01}}{p_{11}+p_{01}}\cdot \frac{\Pr[Z=1]}{\Pr[\hat{Z}=0]}
\end{align}
Set $p:=\min\{\Pr[\hat{Z}=0], \Pr[Z=0]\}$. Then $\Pr[\hat{Z}=0], \Pr[Z=0]\ge p$ and $\Pr[\hat{Z}=1], \Pr[Z=1]\le 1-p$
\begin{equation}
    \frac{\Pr[\hat{Z}=1]}{\Pr[Z=0]}, \frac{\Pr[Z=1]}{\Pr[\hat{Z}=0]} \le \frac{1-p}{p}. 
\end{equation}
It follows that
\begin{equation}
    \left|S(\hat{Z}) - S(Z)\right| = \left|(\EE[Y|\hat{Z}=1] - \EE[Y|\hat{Z}=0]) - (\EE[Y|Z=1] - \EE[Y|Z=0])\right|
\end{equation}
is at most
\begin{align}
    &\left(1 + \frac{1-p}{p}\right)\max\left\{\frac{p_{10}}{p_{11}+p_{10}}, \frac{p_{01}}{p_{11}+p_{01}}\right\}
    = \frac{1}{p}\max\left\{\frac{p_{10}}{p_{11}+p_{10}}, \frac{p_{01}}{p_{11}+p_{01}}\right\}.
\end{align}
The result follows by noting that $p:=\min\{\Pr[\hat{Z}=0], \Pr[Z=0]\}$ and $\frac{p_{10}}{p_{11}+p_{10}}, \frac{p_{01}}{p_{11}+p_{01}} = 1 - \text{ recall}, 1 - \text{ precision}.$
\end{proof}

\section{Prompts}

\input{prompts/interp_prompt_binary}
\input{prompts/surface_similarity_prompt}
\input{prompts/concept_annotation_prompt}
\input{prompts/bertopic_prompt}
