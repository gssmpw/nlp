\section{Related Work}
\label{sec:relatedwork}

\subsection{Hypothesis Generation}

Classical text analysis methods, such as comparing $n$-gram frequencies between groups (Fightin' Words; \citet{monroe_fightin_2009}) or topic modeling (LDA, \citet{blei_latent_2003}; BERTopic, \citet{grootendorst_bertopic_2022}) remain standard tools to discover relationships between text and target variables \citep{grimmer_text_2022}.
A challenge with these methods is that their outputs---lists of words or documents---are not immediately interpretable by humans \citep{chang_reading_2009}.
As a result, applying these methods for scientific discovery requires sufficient domain expertise to prespecify hypotheses and assess whether the data support them \citep{demszky_analyzing_2019, gentzkow_measuring_2016, sun_negative_2022}.

Recently, several LLM-based methods aim to automate interpretable concept discovery from text datasets. \citet{grootendorst_llm_2024} proposes using LLMs to assign topics an interpretable label, while \citet{pham_topicgpt_2024} use LLMs for end-to-end topic modeling.
\citet{ludan_interpretabledesign_2024} directly prompt LLMs with a task description to propose interpretable concepts. 
\citet{sun_concept_2024} and \citet{zhong_explaining_2024} also use LLMs to propose concepts, guided by neurons from a supervised model.
\citet{zhou_hypothesis_2024} and \citet{batista_words_2024} focus specifically on hypothesis generation: LLMs propose natural language descriptions of predictive relationships, which can then be tested for generalization.


Unlike several of these methods, \ourmethod decouples feature generation (unsupervised, using the SAE), selection (supervised), and interpretation (the only LLM-dependent step). 
Comparing to baselines from each class of methods (\S\ref{sec:baselines}), we illustrate quantitative and qualitative improvements over classical and fully-LLM-driven baselines.


\subsection{Sparse Autoencoders}

A recent line of work demonstrates that sparse autoencoders \citep{makhzani_ksparse_2014} are an effective architecture to learn interpretable features from intermediate layers of large language models \citep{cunningham2023sparse, bricken2023towards, templeton2024scaling, bills2023language}. This literature is primarily motivated by mechanistic interpretability: since the neurons in the hidden layer of sparse autoencoders correspond to interpretable features, they can be strategically altered to steer the behavior of language models. In the present work, we instead apply sparse autoencoders to learn interpretable text features for the purpose of hypothesizing interpretable relationships between text data and a target variable. We follow \citet{oneill_disentangling_2024}, who show that SAE features can guide semantic search applications, in training a sparse autoencoder directly on text embeddings.

