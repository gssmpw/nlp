\section{Experiments}
\label{sec:experiments}

\input{tables/headlines-signif}

\subsection{Datasets}

We evaluate on synthetic and real-world datasets, described below. Appendix \ref{sec:preprocessing} provides more details on all datasets.

\paragraph{Synthetic datasets: recovering known, reference hypotheses.} 
Our synthetic evaluation is motivated by real-world settings in which there are multiple disjoint hypotheses we would like to discover.
We therefore use two datasets from prior work on interpretable clustering \citep{pham_topicgpt_2024, zhong_explaining_2024}: \textbf{\wiki} and \textbf{\bills}.
We construct a target variable that is positive if a text belongs to one of a pre-specified list of reference categories; these are the ground-truth ``hypotheses'' to recover.
Both datasets are labeled by humans with granular hierarchical topics, such as \textit{Media and Drama: Television: The Simpsons} in \wiki and \textit{Macroeconomics: Tax Code} in \bills. 

We generate ground-truth labels based on the most frequent granular topics: articles in any of the top-5 topics are labeled `1', and all others are labeled `0'.
This produces 5 reference hypotheses, e.g.,~\textit{the bill is about Macroeconomics: Tax Code} is one of them.
For \wiki, we also include a more challenging variation of recovering the top-15 most frequent topics. \wiki contains 11,979 total articles (15\% positive for \wiki-5; 35\% for \wiki-15), and \bills contains 21,149 (24\% positive).
For both datasets, we reserve 2,000 items for validation (i.e.,~SAE hyperparameter selection) and 2,000 heldout items to evaluate hypotheses; we use the remaining items for SAE training and feature selection.


\paragraph{Real-world datasets: Generating hypotheses that predict a target variable.} 
We apply \ourmethod to three real-world datasets which have been studied in prior work:
\begin{itemize}
    \item \textsc{Headlines} \cite{matias_upworthy_2021}: Which features of digital news headlines predict user engagement?
    Each instance is a pair of differently-phrased headlines for the same article; users on Upworthy.com were randomly shown one, and the task is to identify which headline received higher click-rate.
    We reserve a large heldout set to improve statistical power: our split sizes are 8.8K training, 1K  validation, and 4.4K heldout.
    \item \textsc{Yelp} \citep{yelp_yelp_2024}: Which features of Yelp restaurant reviews predict users' 1-5 star ratings? We use 200K reviews for training, 10K for validation, and 10K for heldout eval. 
    \item \textsc{Congress} \citep{gentzkow_what_2010}: Which features of U.S. congressional speeches predict party affiliation?
    Speeches are from the 109th Congress (2005--07) with binary labels (Rep. or Dem.). 
    Our split sizes are 114K training, 16K validation, and 12K heldout.
\end{itemize}

On each of these tasks, our goal is to identify interpretable natural language features that predict the target variable.
The former two are studied in prior work on hypothesis generation \citep{zhou_hypothesis_2024, batista_words_2024, ludan_interpretabledesign_2024}; the latter is a longstanding application in computational social science \citep{grimmer_machine_2021}.



\subsection{Metrics}
\label{sec:metrics}

\paragraph{Synthetic datasets.} On the synthetic datasets, we evaluate how well we recover the reference hypotheses.
We run \ourmethod by setting $H$ to the number of references (e.g., 5 for \wiki-5).
We use GPT-4o-mini to annotate each inferred hypothesis on the heldout set, producing an $N_\text{heldout} \times H$ binary matrix of annotations.
Following \citet{zhong_explaining_2024}, we compute the optimal matching between reference and inferred hypotheses via the Hungarian algorithm on the $H \times H$ correlation matrix of reference vs.~inferred hypothesis annotations, and we report three metrics:
\begin{itemize}
    \item \textbf{F1 Similarity}: For each matched (reference, inferred) pair, we compute the F1-score between the presence of the ground-truth reference topic and the inferred hypothesis annotations. 
    We report the mean across the $H$ pairs.
    
    \item \textbf{Surface Similarity}: For each matched pair, we prompt GPT-4o to assess whether the hypotheses are the same, related, or distinct, with corresponding scores of 1.0, 0.5, 0.0. 
    To improve stability, we sample 5 outputs at temperature 0.7 and average them.
    We report the mean value of this score across the $H$ pairs. (Table \ref{tab:syndata-refvsinferred} shows examples; Figure \ref{fig:surface_similarity_prompt} provides the prompt.)
    
    \item \textbf{Overall AUC}: Using annotations from the $H$ inferred hypotheses, we fit a multivariate logit and compute AUC to evaluate predictive performance. 
    (Recall that the ground-truth label is a logical OR of the reference topics.)
\end{itemize}

The first and second metrics are taken directly from \citet{zhong_explaining_2024}; they assess whether the inferred hypotheses qualitatively and quantitatively match the ground-truth. 
The third metric assesses overall prediction quality. 
Some hypotheses may not match a reference but still be predictive, so this metric rewards any interpretable hypotheses which contribute predictive value, even if they are not optimal.

\paragraph{Real datasets.} On the real datasets, we generate 20 hypotheses per method, and assess the hypothesis sets for two qualities: (1) \textit{breadth}: how many distinct, predictive hypotheses were identified? (2) \textit{predictiveness}: how well do the hypotheses collectively explain the label? 
We again compute an $N_{\text{heldout}} \times H$ annotation matrix for each method's hypotheses, and measure these constructs as follows: 
\begin{itemize}
    \item \textbf{Breadth}: We fit a multivariate logit (for binary labels) or OLS (continuous) regressing the target variable against all of the hypothesis annotations, and report the count of hypotheses which have a significant, nonzero coefficient\footnote{We use a Bonferroni-corrected $p$-value threshold of 2.5e-3.}. 
    This metric benefits distinct hypotheses and penalizes redundancy. 
    Breadth is important since the utility of hypotheses is often determined by criteria beyond predictiveness. 
    For example, a hypothesis may be more useful if it is connected to past theories or suggests a new direction for investigation. More breadth increases the likelihood that a hypothesis satisfying these criteria is produced.
    \item \textbf{Overall Predictive Performance}: Similar to the synthetic datasets, we report the overall prediction quality using all of the hypotheses in a shared regression, reporting AUC for classification and $R^2$ for regression.
\end{itemize}



\subsection{Baselines} 
\label{sec:baselines}

\input{tables/table-combined}

We compare \ourmethod against recent, state-of-the-art methods (\S\ref{sec:relatedwork}). 
For fair comparison, we run all methods with the same embeddings and LLMs as our method, unless stated otherwise:~OpenAI's \embeddingmodel for embeddings; GPT-4o for hypothesis proposal; and GPT-4o-mini for hypothesis annotation.
We run each method to produce $H$ hypotheses and evaluate all methods identically.

\textbf{\bertopic}~\cite{grootendorst_bertopic_2022} is a neural topic model which produces topics by clustering text embeddings.
To produce $H$ hypotheses from \bertopic, texts are clustered into topics, and we fit an $L_1$-regularized model to predict $Y$ from topic ID; like our method, we set the penalty such that there are exactly $H$ nonzero coefficients. 
We assign each topic a natural language label by prompting an LLM with a sample of documents and top terms (prompt in Figure \ref{fig:bertopic_prompt}).

\textbf{\nlparam}~\cite{zhong_explaining_2024} trains a neural network with a length-$K$ bottleneck layer to predict $Y$ from text embeddings, and then uses an LLM to propose labels corresponding to each one of the bottleneck neurons' activations. 
The hypotheses are iteratively refined; a hypothesis is retained only if its annotations (computed on the entire dataset) improve the loss.
We run \nlparam for the default 10 iterations, with 3 candidate hypotheses per neuron. %

\textbf{\hypogenic}~\cite{zhou_hypothesis_2024} directly prompts a language model to propose hypotheses given a task-specific prompt and labeled examples.
They use a bandit reward function to score hypotheses and propose new ones based on incorrectly-labeled examples. 
We run \hypogenic with default parameters; note that the method uses the same model for proposal and annotation, which defaults to GPT-4o-mini.

\textbf{Training set size.}~A key advantage of \ourmethod is that feature generation and selection require only an SAE forward pass, which is cheap even for large datasets; the only LLM-dependent step is feature interpretation.
\bertopic also scales well, as it only uses LLM inference for topic labeling.
In contrast, \nlparam and \hypogenic score and select hypotheses by annotating every training example, so their inference costs scale with training set size. 
Running either method on 200K Yelp reviews, for instance, would cost $>$\$500 and 78 hours (compared to $\sim$\$1 and 20 minutes for \ourmethod). 
To compare to the baselines as favorably as possible, we allow each method to use up to $50\times$ the cost and runtime of ours. 
To fit this budget, we use up to 20,000 training samples for \nlparam and \hypogenic, which accommodates the full training sets on \wiki, \bills, and \headlines; we use random subsets for \yelp and \congress. 
Notably, this sample budget is significantly larger than either method uses in their publication experiments (4,748 for \nlparam; 200 for \hypogenic), suggesting our comparisons are generous to the baselines. 





