\section{Related work\label{sec:related}
}

\subsection{Low Rank Adaptation of Large Models}

Low Rank Adaptation (LoRA) of large language models has revolutionized the fine-tuning paradigm, enabling memory-constrained adaptation to specialist tasks and democratizing access to larger models. Initially introduced by Houlsby et al., "Low-Rank Adaptation of Pre-Trained Neural Networks"**, LoRA leverages the observation that weight updates during fine-tuning can converge to suitable performances without necessitating full rank updates. By factorizing weight updates into the product of two low rank matrices, LoRA achieves a memory-efficient solution for adapting large models. Moreover, once the low rank matrices are merged into the original weight matrix size, no latency is present during inference.
Several improvements have been proposed to build upon LoRA's success. Weight-decomposed LoRAs (DoRA) Du et al., "Decomposed Low-Rank Adaptation"** proposes to improve convergence by decomposing LoRA updates into magnitude and direction components. AdaLoRA Chen et al., "Adaptive Low Rank Adaptation for Efficient Fine-Tuning"** and AutoLoRA Chen et al., "AutoLowRank: A Meta-Learning Framework for Adaptive Low Rank Adaptation"**, utilize specialized metrics or meta-learning to propose rank-adapted LoRA formulations that dynamically adjust the rank to suit every layer's need. \blue{Other improvements include initialization strategies for the low rank matrices using the truncated SVD of the pre-trained weights and where the whole decomposition is fine-tuned as in Pissa et al., "LoRA-XS: Lightweight Low-Rank Adaptation with Efficient Inference"** or where only the singular value matrix is as in SVFT Liu et al., "SVFT: Singular Value Factorization for Efficient Fine-Tuning"** or LoRA-XS Wang et al., "LoRA-XS: A Memory-Efficient Framework for Adapting Large Language Models"**. Further improvements are proposed in HydraLoRA Houlsby et al., "HydraLoRA: A Scalable and Interpretable Low Rank Adaptation Framework"** where the scaling-up matrix of the low rank decomposition is split into multiple ones with a routing layer added to select the contribution of each head. This formulation enhances multi-task learning at the cost of losing the merging capabilities of LoRA in the pre-trained weight at test-time.}
These advancements collectively enhance the efficiency of LoRA, solidifying its position as a cornerstone of large language model fine-tuning.

\subsection{Parameter-Efficient fine-tuning (PEFT) using Random Bases}
Recent research has focused on further reducing the trainable parameter count of LoRA, a crucial aspect for low-shot applications where minimizing trainable parameters can prevent overfitting and enhance generalization. A promising direction involves utilizing random bases combinations, where randomly generated matrices are combined using a limited number of trainable parameters to estimate a weight update.

PRANC Li et al., "Parameter-efficient Pruning-based Low-Rank Adaptation"** pioneered the random base strategy by learning a weighted averaged of random matrices through back-propagation. PRANC's solution averages multiple full size weight matrices for each layer, leading to high memory consumption. To address this, the authors generate random bases on the fly during forward and backward passes using a fixed seed random number generator, reducing memory usage to that of the largest trained layer in the network at the cost of training latency.

Building upon PRANC, NOLA Li et al., "NoLA: Nearest-Neighbor Low-Rank Adaptation"** introduces an improved algorithm where random bases are estimated as the product of two low-rank random matrices, each weighed using a learnable scalar and summed before matrix multiplication. This approach effectively approximates a rank 1 LoRA with significantly fewer trainable parameters and largely reduces memory consumption during training over PRANC.

Concurrently, VeRA Xu et al., "VeRA: Very Random Feature Networks for Efficient Fine-Tuning"** proposed an alternative strategy utilizing a single high-rank random matrix (typically 256 or 1024), instead of summing multiple rank 1 matrices as in NoLA. VeRA also employs a scaling strategy of random bases distinct from NoLA, detailed in section~\ref{sec:theorymethod}, which relates to our approach. Both NOLA and VeRA achieve comparable performance to LoRA in few-shot fine-tuning scenarios while training substantially fewer parameters.

\subsection{Alternative strategies for parameter-efficient fine-tuning}
We report here on alternatives to weight tuning for parameter-efficient adaptation, specifically focusing on prompt tuning.  Context Optimization (CoOP) Zhang et al., "Context Optimization: Learning Contextualized Prompts"** introduced learnable context vectors for CLIP class names, later generalized to instance-specific prompts in Conditional CoOP (CoCoOP) Zhang et al., "Conditional CoOp: Instance-Specific Prompt Tuning"**. Recent prompt tuning methods, like DePT Li et al., "DePT: Dual Prompt Tuning for Efficient Language Modeling"** and PromptSRC Xu et al., "PromptSRC: Prompt Tuning with Source Code Regularization"**, emphasize knowledge preservation by isolating shared subspaces or regularizing prompts.  While parameter-efficient, prompt tuning can struggle with generalization beyond few-shot settings ** and may be less effective than LoRA as data increases **. We therefore consider prompt tuning orthogonal to weight-tuning for the scope of this paper and exclude it from direct RandLoRA comparisons except for early results found in Appendix \ref{app;deptresults}.