\section{Related work\label{sec:related}
}

\subsection{Low Rank Adaptation of Large Models}

Low Rank Adaptation (LoRA) of large language models has revolutionized the fine-tuning paradigm, enabling memory-constrained adaptation to specialist tasks and democratizing access to larger models. Initially introduced by~\citep{2022_ICLR_lora}, LoRA leverages the observation that weight updates during fine-tuning can converge to suitable performances without necessitating full rank updates. By factorizing weight updates into the product of two low rank matrices, LoRA achieves a memory-efficient solution for adapting large models. Moreover, once the low rank matrices are merged into the original weight matrix size, no latency is present during inference.
Several improvements have been proposed to build upon LoRA's success. Weight-decomposed LoRAs (DoRA)~\citep{2024_ICML_DoRA} proposes to improve convergence by decomposing LoRA updates into magnitude and direction components. AdaLoRA~\citep{2023_ICLR_AdaLoRA} and AutoLoRA~\citep{2024_arxiv_autolora}, utilize specialized metrics or meta-learning to propose rank-adapted LoRA formulations that dynamically adjust the rank to suit every layer's need. \blue{Other improvements include initialization strategies for the low rank matrices using the truncated SVD of the pre-trained weights and where the whole decomposition is fine-tuned as in Pissa~\citep{2024_NeurIPS_Pissa} or where only the singular value matrix is as in SVFT~\citep{2024_ICMLW_SVFT} or LoRA-XS~\citep{2024_loraxs}. Further improvements are proposed in HydraLoRA~\citep{2024_NeurIPS_HydraLoRA} where the scaling-up matrix of the low rank decomposition is split into multiple ones with a routing layer added to select the contribution of each head. This formulation enhances multi-task learning at the cost of losing the merging capabilities of LoRA in the pre-trained weight at test-time.}
These advancements collectively enhance the efficiency of LoRA, solidifying its position as a cornerstone of large language model fine-tuning.

\subsection{Parameter-Efficient fine-tuning (PEFT) using Random Bases}
Recent research has focused on further reducing the trainable parameter count of LoRA, a crucial aspect for low-shot applications where minimizing trainable parameters can prevent overfitting and enhance generalization. A promising direction involves utilizing random bases combinations, where randomly generated matrices are combined using a limited number of trainable parameters to estimate a weight update.

PRANC~\citep{2023_ICCV_PRANC} pioneered the random base strategy by learning a weighted averaged of random matrices through back-propagation. PRANC's solution averages multiple full size weight matrices for each layer, leading to high memory consumption. To address this, the authors generate random bases on the fly during forward and backward passes using a fixed seed random number generator, reducing memory usage to that of the largest trained layer in the network at the cost of training latency.

Building upon PRANC, NOLA~\citep{2024_ICLR_NoLA} introduces an improved algorithm where random bases are estimated as the product of two low-rank random matrices, each weighed using a learnable scalar and summed before matrix multiplication. This approach effectively approximates a rank 1 LoRA with significantly fewer trainable parameters and largely reduces memory consumption during training over PRANC.

Concurrently, VeRA~\citep{2024_ICLR_VeRA} proposed an alternative strategy utilizing a single high-rank random matrix (typically 256 or 1024), instead of summing multiple rank 1 matrices as in NoLA. VeRA also employs a scaling strategy of random bases distinct from NoLA, detailed in section~\ref{sec:theorymethod}, which relates to our approach. Both NOLA and VeRA achieve comparable performance to LoRA in few-shot fine-tuning scenarios while training substantially fewer parameters.

\subsection{Alternative strategies for parameter-efficient fine-tuning}
We report here on alternatives to weight tuning for parameter-efficient adaptation, specifically focusing on prompt tuning.  Context Optimization (CoOP) \citep{2022_IJCV_CoOP} introduced learnable context vectors for CLIP class names, later generalized to instance-specific prompts in Conditional CoOP (CoCoOP) \citep{2022_CVPR_CoCoOP}. Recent prompt tuning methods, like DePT \citep{2024_CVPR_DEPT} and PromptSRC \citep{2023_CVPR_PromptSRC}, emphasize knowledge preservation by isolating shared subspaces or regularizing prompts.  While parameter-efficient, prompt tuning can struggle with generalization beyond few-shot settings \citep{2024_arxiv_peftsurvey} and may be less effective than LoRA as data increases \citep{2024_CVPR_promptvslora}. We therefore consider prompt tuning orthogonal to weight-tuning for the scope of this paper and exclude it from direct RandLoRA comparisons except for early results found in Appendix \ref{app;deptresults}.