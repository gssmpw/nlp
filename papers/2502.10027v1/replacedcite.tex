\section{Related Work}
% \begin{table*}[h]
% \centering
% % |p{4cm}|p{4cm}|p{4cm}|
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c|c|c|c|c|}
% %\toprule
% \hline
% \diagbox{\textbf{Problems of different:\vspace{-.05cm}}}{\textbf{Technology\vspace{-.1cm}}}  & \textbf{MTL}  & \textbf{Meta-Learning} & \textbf{GNNs} & \textbf{ZP} \\ \hline
% \emph{dimensionality values} & \cmark & \xmark & \cmark & \cmark \\\hline
% \emph{objectives} & \cmark & \cmark & \xmark & \cmark \\\hline
% \emph{constraint sets} & \cmark & \xmark & \xmark & \cmark \\\hline
% \emph{Benchmarking}   & Compare to an optimal benchmark & Not relevant unless adapting to unseen new tasks & Compare to an optimal benchmark & Compare to an optimal benchmark  \\\hline
% % \textbf{Goal}                      & Learn to perform multiple tasks simultaneously & Learn to adapt quickly to a new task               \\\hline
% % \textbf{Training Process}          & Joint optimization of multiple tasks           & Two levels of optimization (meta and task level)  \\\hline
% % \textbf{Task Adaptation}           & Simultaneous learning of all tasks             & Strong focus on adapting to new tasks             \\\hline
% % \textbf{Generalization}            & Improves performance across a fixed set of tasks & Generalizes to a  new, unseen task                  \\\hline
% % \textbf{Optimization Focus}        & Learns shared representation across tasks      & Learns to optimize the learning process itself    \\\hline
% % \textbf{Application Scenario}      & Useful for related tasks with shared structure & Useful for few-shot learning and new tasks        \\\hline
% % \textbf{Data Efficiency}           & Requires more data to train on all tasks       & Can learn from few examples of new tasks          \\\hline
% % \textbf{Benchmarking}              & Compare to an optimal benchmark (e.g., theoretical best solution) & Not relevant unless adapting to unseen new tasks         \\ %\bottomrule
% \end{tabular}
% }
% \caption{Comparison between relevant DNN technologies}
% \end{table*}
% A high level comparison between all relevant technologies in the context of multi-task optimization is shown in Table 1.
\textcolor{black}{ DNNs have  been widely used for tackling various complex optimization problems, especially in the context of wireless communications ____.  In ____ distributed and unsupervised learning (UL) based frameworks for constrained optimization were proposed. In ____, online-learning was used to tackle optimization problems in intelligent reflecting surface (IRS) networks, achieving greater performance against model-based methods. Moreover, meta-learning has emerged as a promising ML framework which enables DNNs to quickly adapt to new unseen tasks  using limited data ____.  Despite their novelty, the works of ____  cannot been applied to problems of varying dimensionality, while they also require that  the considered optimization tasks have the same set of constraints, which is in contrast to the MTL scheme proposed in this paper.}
% DNNs have  been widely used for tackling various complex optimization problems, especially in the context of wireless communications ____. Specifically, in ____, DNN-based frameworks were proposed for resource allocation in wireless communications. In ____, the class of ``learnable algorithms" and the design of
% DNNs to approximate some algorithms of interest in wireless were given, while in ____ a distributed and unsupervised learning (UL) based framework for constrained optimization was proposed. Both frameworks relied on primal-dual optimization for handling the constraints via the DNNs. Furthermore, in ____, the notion of intelligent resource allocation for wireless networks was discussed, while in ____ intelligent resource management based on online-learning for non-convex problems was investigated. In ____, online-learning was used to tackle the optimization problem for an intelligent reflecting surface (IRS) network, achieving greater performance against model-based methods.
% , while in ____ online-learning was utilized for orthogonal frequency-division multiplexing (OFDM) resource allocation. Moreover, DNN-aided resource allocation for vehicular and device-to-device (D2D) networks were proposed in ____ in respect. Furthermore, DL has been shown to improve the performance of edge and multi-cell networks, facilitating coordinated multi-point transmission and delay efficient communications ____. 
% Finally, in ____ bandwidth sharing based on DNNs was studied, while in ____ DL was employed for improving the cooperative transmission between multiple users. 
% /{\color{red} Should not we discuss papers related to multi-task learning?}
% ____
% Despite their novelty,
% However, none of these works considered the case of input data to the DNN which vary in dimension. To this end,  has been employed.
% % in several wireless communication problems. 
Another  technique which aims to enhance the ability of DNNs to generalize their performance to multiple tasks is the zero-padding (ZP).
% For instance, in ____, the ZP was used to perform resource allocation problems. 
For instance, in ____, ZP was used to handle the changing dimensionality of the input data due to the dynamic nature of wireless networks. 
% while in ____, a resource allocation scheme based on convolutional DNNs (CDNNs) was studied, where the ZP ensured that the size of the output remains the same as that of the input.
% a deep reinforcement learning (DLR) approach for wireless networks was proposed, where the ZP was used to ensure the same vector length for all devices. In ____, a resource allocation scheme based on convolutional DNNs (CDNNs) was studied, where the ZP ensured that the size of the output remains the same as that of the
% input. Also, in ____ ZP was used to handle the changing dimensionality of the input data associated with the resource management of the network, under the scenario of multiple access points (APs) and D2D communications. 
% ZP can be easily extended for use in MTL scenarios, but as it will be shown, it performs poorly.

Furthermore, a promising research direction which has shown great capability to address the scalability issues of wireless resource allocation is graph neural networks (GNNs) ____. In one approach called the random edge GNN (REGNN) ____, convolutions were applied over random graphs representing fading interference patterns in wireless networks. Additionally, GNNs have been applied to solve problems like optimal power control, beamforming, and user selection ____. These studies modelled these tasks using graphs and solve them effectively with GNNs. Moreover,  
% in a decentralized optimization scenario,
GNNs have been employed along with primal-dual optimization techniques ____ to perform optimal constrained resource allocation. Furthermore, GNNs have been used to learn power allocation in multi-cell-multi-user systems with heterogeneous GNNs ____.  Nonetheless, there are only a few studies that have explored MTL with GNNs  ____, \textcolor{black}{ but not in the context of multi-task constrained optimization.} These approaches rely on techniques such as soft parameter sharing or regularization between task-specific networks, which increase the total number of trainable parameters compared to a single-task DNN ____. 
% In contrast, the proposed MTL approach provides an equal number of trainable parameters.

% \begin{table}[h]
% \textcolor{black}{
% \centering
% % |p{4cm}|p{4cm}|p{4cm}|
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% % [width=19em,height=2.2em]
% \diagbox[width=17em,height=1.9em]{\textbf{Technology\hspace{-1cm}\vspace{-.1cm}}}{\vspace{1.5cm}\textbf{Varying\hspace{-0.15cm}}}  & \textbf{dimensionality} & \textbf{objective} & \textbf{constraints} \\\hline
% \textbf{MTL}  & \cmark & \cmark & \cmark  \\\hline
% \textbf{Meta-learning} & \xmark & \cmark & \xmark  \\\hline
% \textbf{GNNs} & \cmark & \xmark & \xmark  \\\hline
% \textbf{ZP}   & \cmark & \cmark & \cmark \\\hline
% \textbf{EMTO}   & \xmark & \cmark & \cmark \\\hline
% \end{tabular}
% }
% \caption{\textcolor{black}{Comparison between relevant  technologies.}\vspace{-0.5cm}}}
% \end{table}
Moreover,  evolutionary multi-tasking optimization (EMTO) aims at solving multiple optimization problems simultaneously ____.
% An approach which aims at solving multiple optimization problems simultaneously is evolutionary multi-tasking optimization (EMTO) ____. 
In EMTO, a population of candidate solutions evolves over generations, with each individual tasked with solving a specific optimization problem. 
% Unlike traditional evolutionary algorithms that focus on a single objective function, EMTO allocates resources to solve multiple optimization tasks concurrently.
Through the process of natural selection and mutation the population evolves to produce solutions that are adept at solving all the given tasks ____. Therefore, EMTO algorithms utilize the underlying similarity of optimization tasks to efficiently solve heterogeneous tasks just like MTL ____. Moreover, in ____, EMTO algorithms for constrained multi-task optimization problems were proposed which helped generalize EMTO to practical use cases. Nonetheless, EMTO is time-consuming and cannot be used to solve real-time problems in dynamic environments, such as wireless networks, where the parameters of the optimization problems change constantly.