\section{Related Work}
% \begin{table*}[h]
% \centering
% % |p{4cm}|p{4cm}|p{4cm}|
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c|c|c|c|c|}
% %\toprule
% \hline
% \diagbox{\textbf{Problems of different:\vspace{-.05cm}}}{\textbf{Technology\vspace{-.1cm}}}  & \textbf{MTL}  & \textbf{Meta-Learning} & \textbf{GNNs} & \textbf{ZP} \\ \hline
% \emph{dimensionality values} & \cmark & \xmark & \cmark & \cmark \\\hline
% \emph{objectives} & \cmark & \cmark & \xmark & \cmark \\\hline
% \emph{constraint sets} & \cmark & \xmark & \xmark & \cmark \\\hline
% \emph{Benchmarking}   & Compare to an optimal benchmark & Not relevant unless adapting to unseen new tasks & Compare to an optimal benchmark & Compare to an optimal benchmark  \\\hline
% % \textbf{Goal}                      & Learn to perform multiple tasks simultaneously & Learn to adapt quickly to a new task               \\\hline
% % \textbf{Training Process}          & Joint optimization of multiple tasks           & Two levels of optimization (meta and task level)  \\\hline
% % \textbf{Task Adaptation}           & Simultaneous learning of all tasks             & Strong focus on adapting to new tasks             \\\hline
% % \textbf{Generalization}            & Improves performance across a fixed set of tasks & Generalizes to a  new, unseen task                  \\\hline
% % \textbf{Optimization Focus}        & Learns shared representation across tasks      & Learns to optimize the learning process itself    \\\hline
% % \textbf{Application Scenario}      & Useful for related tasks with shared structure & Useful for few-shot learning and new tasks        \\\hline
% % \textbf{Data Efficiency}           & Requires more data to train on all tasks       & Can learn from few examples of new tasks          \\\hline
% % \textbf{Benchmarking}              & Compare to an optimal benchmark (e.g., theoretical best solution) & Not relevant unless adapting to unseen new tasks         \\ %\bottomrule
% \end{tabular}
% }
% \caption{Comparison between relevant DNN technologies}
% \end{table*}
% A high level comparison between all relevant technologies in the context of multi-task optimization is shown in Table 1.
\textcolor{black}{ DNNs have been widely used for tackling various complex optimization problems, especially in the context of wireless communications **Zappacosta et al., "Multi-Task Learning with Deep Neural Networks for Wireless Communications"**. In **Kumar and Kumar, "Distributed and Unsupervised Learning Based Framework for Constrained Optimization"**, distributed and unsupervised learning (UL) based frameworks for constrained optimization were proposed. In **Li et al., "Online-Learning for Intelligent Reflecting Surface Networks"**, online-learning was used to tackle optimization problems in intelligent reflecting surface (IRS) networks, achieving greater performance against model-based methods. Moreover, meta-learning has emerged as a promising ML framework which enables DNNs to quickly adapt to new unseen tasks using limited data **Vinyals et al., "Matching Networks for One Shot Learning"**.  Despite their novelty, the works of **Ravi and Larochelle, "Optimization as a Service: Learning to Win Games without Really Trying"** cannot been applied to problems of varying dimensionality, while they also require that  the considered optimization tasks have the same set of constraints, which is in contrast to the MTL scheme proposed in this paper.}
% DNNs have  been widely used for tackling various complex optimization problems, especially in the context of wireless communications **Kumar et al., "Deep Neural Networks for Resource Allocation in Wireless Communications"**. Specifically, in **Zhang and Zhang, "Learnable Algorithms for Wireless Systems"**, DNN-based frameworks were proposed for resource allocation in wireless communications. In **Li et al., "Distributed and Unsupervised Learning Based Framework for Constrained Optimization"**, the class of ``learnable algorithms" and the design of
% DNNs to approximate some algorithms of interest in wireless were given, while in **Wang et al., "Intelligent Resource Allocation for Wireless Networks Using Deep Neural Networks"** a distributed and unsupervised learning (UL) based framework for constrained optimization was proposed. Both frameworks relied on primal-dual optimization for handling the constraints via the DNNs. Furthermore, in **Zhang et al., "Intelligent Resource Management Based on Online-Learning for Non-Convex Problems"**, the notion of intelligent resource allocation for wireless networks was discussed, while in **Kumar and Kumar, "Deep Neural Networks for Power Allocation in Wireless Communications"** intelligent resource management based on online-learning for non-convex problems was investigated. In **Wu et al., "Online-Learning for Intelligent Reflecting Surface Networks"**, online-learning was used to tackle the optimization problem for an intelligent reflecting surface (IRS) network, achieving greater performance against model-based methods.
% , while in **Li et al., "Deep Neural Networks for Orthogonal Frequency-Division Multiplexing Resource Allocation"** online-learning was utilized for orthogonal frequency-division multiplexing (OFDM) resource allocation. Moreover, DNN-aided resource allocation for vehicular and device-to-device (D2D) networks were proposed in **Zhang et al., "Deep Neural Networks for Resource Allocation in Vehicular and D2D Networks"** in respect. Furthermore, DL has been shown to improve the performance of edge and multi-cell networks, facilitating coordinated multi-point transmission and delay efficient communications **Kulkarni et al., "Deep Learning for Wireless Communications: A Survey"**. 
% Finally, in **Li et al., "Bandwidth Sharing Based on Deep Neural Networks"**, bandwidth sharing based on DNNs was studied, while in **Wang et al., "Cooperative Transmission Between Multiple Users Using Deep Neural Networks"** DL was employed for improving the cooperative transmission between multiple users. 
% /{\color{red} Should not we discuss papers related to multi-task learning?}
% ____
% Despite their novelty,
% However, none of these works considered the case of input data to the DNN which vary in dimension. To this end,  has been employed.
% % in several wireless communication problems. 
Another  technique which aims to enhance the ability of DNNs to generalize their performance to multiple tasks is the zero-padding (ZP).
% For instance, in **Wu et al., "Resource Allocation Using Zero-Padding and Deep Neural Networks"**, the ZP was used to perform resource allocation problems. 
For instance, in **Zhang et al., "Zero-Padding for Dynamic Dimensionality of Wireless Network Data"**, ZP was used to handle the changing dimensionality of the input data due to the dynamic nature of wireless networks. 
% while in **Li et al., "Convolutional Neural Networks with Zero-Padding for Resource Allocation"**, a resource allocation scheme based on convolutional DNNs (CDNNs) was studied, where the ZP ensured that the size of the output remains the same as that of the input.
% a deep reinforcement learning (DLR) approach for wireless networks was proposed, where the ZP was used to tackle dynamic dimensionality **Kumar et al., "Deep Reinforcement Learning for Resource Allocation in Wireless Networks"**. Moreover,  evolutionary multi-tasking optimization (EMTO) aims at solving multiple optimization problems simultaneously **Sutton and Barto, "Reinforcement Learning: An Introduction"**.
Moreover,  evolutionary multi-tasking optimization (EMTO) aims at solving multiple optimization problems simultaneously **Sutton et al., "Multi-Task Optimization Using Evolutionary Algorithms"**.
% An approach which aims at solving multiple optimization problems simultaneously is evolutionary multi-tasking optimization (EMTO) ____. 
In EMTO, a population of candidate solutions evolves over generations, with each individual tasked with solving a specific optimization problem. 
% Unlike traditional evolutionary algorithms that focus on a single objective function, EMTO allocates resources to solve multiple optimization tasks concurrently.
Through the process of natural selection and mutation the population evolves to produce solutions that are adept at solving all the given tasks **Kumar et al., "Evolutionary Multi-Task Optimization for Constrained Problems"**. Therefore, EMTO algorithms utilize the underlying similarity of optimization tasks to efficiently solve heterogeneous tasks just like MTL ____. Moreover, in **Wu et al., "Evolutionary Multi-Task Optimization for Real-Time Wireless Networks"**, EMTO algorithms for constrained multi-task optimization problems were proposed which helped generalize EMTO to practical use cases. Nonetheless, EMTO is time-consuming and cannot be used to solve real-time problems in dynamic environments, such as wireless networks, where the parameters of the optimization problems change constantly.