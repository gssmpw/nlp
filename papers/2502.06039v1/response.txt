\section{Related Work}
\label{sec:related_work}

\subsection{Security of LLM-Generated Code}
A range of studies have investigated the security of LLM-generated code. Fu et al., "Investigating the Security Risks of GitHub Copilot" found that 32.8\% of Python and 24.5\% of JavaScript code snippets found in GitHub projects that were generated by GitHub Copilot, and marked as such, contained security issues. Pearce et al., "An Empirical Study on the Security Implications of Large Language Models in Code Generation" constructed a set of 89 ``CWE Scenarios'' and found that around 40\% of the suggested completions by GitHub Copilot contained vulnerabilities. Khoury et al., "A Case for Proactive Security: Investigating LLMs' Inherent Weaknesses" found that GPT-3.5 generated initially secure programs in only 5 out of their 21 use-cases.


\boxit{
These findings highlight the importance of addressing security concerns, as they demonstrate a significant prevalence of vulnerabilities in code generated by LLMs. Our work aims to offer a proactive solution to reducing these risks.
}

\subsection{Security Relevant Prompt Datasets}
Pearce et al., "An Empirical Study on the Security Implications of Large Language Models in Code Generation" published their dataset of 89 CWE-based code-completion scenarios, covering 18 out of the top 25 CWEs from 2021. The authors used examples from the CodeQL repository and MITRE, "Common Weakness Enumeration (CWE)" as sources and handcrafted some of the code completion tasks.

Tony et al., "LLMSecEval: A Dataset for Evaluating the Security of Large Language Model Generated Code" translated the completion scenarios from Pearce et al. into 150 natural language prompts. They introduced the idea of using their LLMSecEval dataset in combination with the CodeQL scanner to evaluate the security of LLM code generation.

Siddiq and Santos published SecurityEval, "SecurityEval: A Benchmark for Evaluating the Security of Large Language Model Generated Code". Their dataset consists of 130 prompts for 75 vulnerability types which are mapped to the corresponding CWE.
The prompts are code completion tasks with imports, a function header, and a natural language comment for the desired functionality.
The sources are CodeQL examples, CWE examples, Sonar examples, and the scenarios from Pearce et al..

In Meta's PurpleLLama CyberSecEval Study, "Cybersecurity Evaluation of Large Language Models" they used their Insecure Code Detector (ICD), which is based on weggli, "Weggli: A Framework for Evaluating the Security of Large Language Model Generated Code" and Semgrep, "Semgrep: A Fast and Flexible Code Analysis Tool" rules, to find insecure coding practices in open source repositories. They then took the 10 lines preceding the issues to create code completion tasks. Additionally, they translated the completion tasks into natural language instructions using an LLM.


\boxit{These resources enable an efficient evaluation of the security aspect of code generation by focusing on relevant scenarios. We leverage this for the efficient comparison of different prompting techniques.}

\subsection{Prompt Variations and Code Security}
\label{subsec:related_work_prompt_variations} 
Only few studies investigated the impact of prompt modifications on code security. Pearce et al., "An Empirical Study on the Security Implications of Large Language Models in Code Generation" tested prompt diversity for only one of their 89 scenarios and found that
``small changes in Copilotâ€™s prompt ( \ldots \space) can impact the safety of the generated code''. They hypothesize that ``the presence of either vulnerable or non-vulnerable SQL in a codebase \ldots \space has the strongest impact upon whether or not Copilot will itself generate SQL code vulnerable to injection''. If this is true for languages beyond SQL, minimizing vulnerability risks in initial prompts not only reduces the risk for the current output but also creates a compounding effect by lowering the likelihood of insecure code in subsequent generations, thereby significantly increasing the security of the entire codebase.


Firouzi and Ghafari, "Assessing the Security Risks of Large Language Models" prompted GPT-3.5 to answer 100 encryption-related Stack Overflow questions, finding that only three responses were free of security violations. When the prompt was modified to explicitly request a ``secure'' solution, the number of secure responses increased to 42.
In a more recent study Firouzi et al., "Evaluating the Security of Large Language Models in Code Generation" they compared ChatGPT's performance in detecting cryptographic misuses with that of state-of-the-art static analysis tools. Their findings indicate that, with appropriate prompts, ChatGPT outperforms leading static cryptography misuse detectors.


Tony et al., "LLMSecEval: A Dataset for Evaluating the Security of Large Language Model Generated Code" published a set of prompt templates for secure code generation based on a systematic literature review. Their test of the templates using the LLMSecEval dataset was limited by setting the temperature parameter to 0 and collecting only a single sample per prompt. 

\boxit{
These studies offer positive initial insights into the potential of prompt engineering for secure code generation. 
%
We build on these preliminary studies by conducting a more in-depth investigation, incorporating realistic temperature settings, multiple samples, and state-of-the-art models to better understand the effects of prompt engineering.
}

\subsection{Alternative Approaches to Increasing Code Security}
Some LLM-based tools rely on static scanners to detect vulnerabilities in the generated code. Meta's PurpleLLama CodeShield, "PurpleLLama CodeShield: A Tool for Detecting Vulnerabilities in Large Language Model Generated Code" is using ICD introduced in Firouzi et al., "Assessing the Security Risks of Large Language Models" to flag insecure code snippets and suggest actions (block or warn).

Kavian et al. introduced LLMSecGuard, "LLMSecGuard: A Framework for Enhancing the Security of Large Language Model Generated Code". However, they did not investigate the effectiveness of the proposed framework.

\boxit{
In contrast to methods that rely on external tools for post-generation vulnerability detection, our approach integrates both proactive prevention during code generation and the use of the LLM's capabilities to identify and mitigate remaining issues after generation. 
}