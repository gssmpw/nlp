A detailed overview of the proposed architecture that converts images and control commands
into trajectories is depicted in~\autoref{fig:monoforce}.
The model consists of several learnable modules that deeply interact with each other.
The \emph{terrain encoder} carefully transforms visual features from the input image
into the heightmap space using known camera geometry.
The resultant heightmap features are further refined into interpretable physical quantities
that capture properties of the terrain such as its shape, friction, stiffness, and damping.
Next, the \emph{physics engine} combines the terrain properties with the robot model,
robot state, and control commands and delivers reaction forces at points of robot-terrain contacts.
It then solves the equations of motion dynamics by integrating these forces
and delivers the trajectory of the robot.
Since the complete computational graph of the feedforward pass is retained,
the backpropagation from an arbitrary loss, constructed on top of delivered trajectories,
or any other intermediate outputs is at hand.

\subsection{Terrain Encoder}\label{subsec:terrain_encoder}

The part of the MonoForce architecture (\autoref{fig:monoforce})
that predicts terrain properties $\mathbf{m}$ from sensor measurements $\mathbf{z}$ is called \emph{terrain encoder}.
The proposed architecture starts by converting pixels from a 2D image plane into a heightmap with visual features.
Since the camera is calibrated, there is a substantial geometrical prior that connects heightmap cells with the pixels.
We incorporate the geometry through the Lift-Splat-Shoot architecture~\cite{philion2020lift}.
This architecture uses known camera intrinsic parameters to estimate rays corresponding to particular pixels~--
pixel rays, \autoref{fig:bevfusion}.
For each pixel ray, the convolutional network then predicts depth probabilities and visual features.
Visual features are vertically projected on a virtual heightmap for all possible depths along the corresponding ray.
The depth-weighted sum of visual features over each heightmap cell is computed,
and the resulting multichannel array is further refined by deep convolutional network
to estimate the terrain properties $\mathbf{m}$.

The terrain properties include the geometrical heightmap $\mathcal{H}_g$,
the heights of the terrain supporting layer hidden under the vegetation $\mathcal{H}_t = \mathcal{H}_g - \Delta\mathcal{H}$,
terrain friction $\mathcal{M}$, stiffness $\mathcal{K}$, and dampening $\mathcal{D}$.
The intuition behind the introduction of the $\Delta\mathcal{H}$ term is
that $\mathcal{H}_t$ models a partially flexible layer of terrain (e.g. mud) that is hidden under flexible vegetation,~\autoref{fig:monoforce_heightmaps}.


\subsection{Differentiable Physics Engine}\label{subsec:dphysics}
\input{dphysics}

\subsection{Data-driven Trajectory Prediction}\label{subsec:data_driven_baseline}
Inspired by the work~\cite{pang2019aircraft}, we design a data-driven LSTM architecture (\autoref{fig:traj_lstm}) for our outdoor mobile robot's trajectory prediction.
We call the model TrajLSTM and use it as a baseline for our $\nabla$Physics engine.
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{imgs/architectures/lstm}
    \caption{\textbf{TrajLSTM} architecture. The model takes as input: initial state $\mathbf{x}_0$, terrain $\mathcal{H}$, control sequence $\mathbf{u}_t, t \in \{0 \dots T\}$. It predicts the trajectory as a sequence of states $\mathbf{x}_t, t \in \{0 \dots T\}.$}
    \label{fig:traj_lstm}
\end{figure}
Given an initial robot's state $\mathbf{x}_0$ and a sequence of control inputs for a time horizon $T$, $\mathbf{u}_t, t \in \{0 \dots T\}$, the TrajLSTM model provides a sequence of states at control command time moments, $\mathbf{x}_t, t \in \{0 \dots T\}$.
As in outdoor scenarios the robot commonly traverses uneven terrain, we additionally include the terrain shape input to the model in the form of heightmap $\mathcal{H}=\mathcal{H}_0$ estimated at initial time moment $t=0$.
Each timestep's control input $\mathbf{u}_i$ is concatenated with the shared spatial features $\mathbf{x}_i$, as shown in \autoref{fig:traj_lstm}.
The combined features are passed through dense layers to prepare for temporal processing.
The LSTM unit~\cite{hochreiter1997long} processes the sequence of features (one for each timestep).
As in our experiments, the time horizon for trajectory prediction is reasonably small, $T=5 [\si{\sec}]$, and the robot's trajectories lie within the heightmap area, we use the shared heightmap input for all the LSTM units of the network.
So the heightmap is processed through the convolutional layers \textbf{once} and flattened, producing a fixed-size spatial feature vector.
This design choice (of not processing the heightmaps at different time moments) is also motivated by computational efficiency reason.
At each moment $t$, this heightmap vector is concatenated with the fused spatial-control features and processed by an LSTM unit.
The LSTM unit output for each timestep $t$ is passed through a fully connected (dense) layer to produce the next state $\mathbf{x}_{t+1}$.
The sequence of states form the predicted trajectory, $\{\mathbf{x}_0, \dots \mathbf{x}_T\}$.


\subsection{End-to-end Learning}\label{subsec:end2end_learning}
Self-supervised learning of the proposed architecture minimizes three different losses:

\textbf{Trajectory loss} that minimizes
the difference between SLAM-reconstructed trajectory $\tau^\star$ and predicted trajectory $\tau$:
\begin{equation}~\label{eq:traj_loss}
   \mathcal{L}_\tau = \|\tau-\tau^\star\|^2
\end{equation}

\textbf{Geometrical loss} that minimizes the difference between
ground truth lidar-reconstructed heightmap $\mathcal{H}_g^\star$
and predicted geometrical heightmap $\mathcal{H}_g$:
 \begin{equation}~\label{eq:geom_loss}
     \mathcal{L}_g = \|\mathbf{W}_g\circ(\mathcal{H}_g-\mathcal{H}_g^\star)\|^2
 \end{equation}
$\mathbf{W}_g$ denotes an array selecting the heightmap channel corresponding to the terrain shape.

\textbf{Terrain loss} that minimizes the difference between ground truth $\mathcal{H}_t^\star$
and predicted $\mathcal{H}_t$ supporting heightmaps containing rigid objects detected
with Microsoft's image segmentation model SEEM~\cite{zou2023segment},
that is derived from Segment Anything foundation model~\cite{li2023semantic}:
 \begin{equation}~\label{eq:terrain_loss}
     \mathcal{L}_t = \|\mathbf{W}_t\circ(\mathcal{H}_t-\mathcal{H}_t^\star)\|^2
 \end{equation}
$\mathbf{W}_t$ denotes the array selecting heightmap cells that are covered by rigid materials
(e.g. stones, walls, trunks), and $\circ$ is element-wise multiplication.

Since the architecture \autoref{fig:model_overview} is end-to-end differentiable,
we can directly learn to predict all intermediate outputs just using trajectory loss~\eqref{eq:traj_loss}.
An example of terrain learning with the trajectory loss is visualized in \autoref{fig:terrain_optim}.
To make the training more efficient and the learned model explainable, we employ the
geometrical loss~\eqref{eq:geom_loss} and terrain loss~\eqref{eq:terrain_loss} as regularization terms.
stat

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{imgs/predictions/monoforce/qualitative_results_experiments}
    \caption{\textbf{MonoForce prediction examples}.
    \emph{Left}: The robot is moving through a narrow passage between a wall and tree logs.
    \emph{Right}: The robot is moving on a gravel road with rocks on the sides.
    It starts its motion from the position marked with a coordinate frame and the trajectory is predicted for $10~[\si{\sec}]$ using real control commands.
    The camera images are taken from the robot's initial position (\emph{top row}).
    The visualization includes predicted supporting terrain $\mathcal{H}_t$ (\emph{second row}).
    It is additionally shown in 3D and colored with predicted friction values (\emph{third row}).
    }
    \label{fig:monoforce_predictions}
\end{figure*}

The \autoref{fig:monoforce_predictions} show the prediction examples of the MonoForce model in diverse outdoor environments.
From the example on the left,
we can see that the model correctly predicts the robot's trajectory and the terrain shape suppressing traversable vegetation,
while the rigid obstacles (wall and tree logs) are correctly detected.
The example on the right demonstrates the model's ability to predict the robot's trajectory ($10~[\si{\sec}]$-long horizon)
with reasonable accuracy and to detect the rigid obstacles (stones) on the terrain.
It could also be noticed that the surfaces that provide the robot good traction (paved and gravel roads) are marked with a higher friction value,
while for the objects that might not give good contact with the robot's tracks (walls and tree logs) the friction value is lower.

We argue that the friction estimates are approximate and an interesting research direction could be
comparing them with real-world measurements or with the values provided by a high-fidelity physics engine (e.g. AGX Dynamics~\cite{Berglund2019agxTerrain}).
However, one of the benefits of our differentiable approach is that the model does not require ground-truth friction values for training.
The predicted heightmap's size is $12.8\times12.8\si{\meter}^2$ and the grid resolution is $0.1\si{\meter}$.
It has an upper bound of $1~[\si{\meter}]$ and a lower bound of $-1~[\si{\meter}]$.
This constraint was introduced based on the robot's size and taking into account hanging objects (tree branches)
that should not be considered as obstacles (\autoref{fig:nav_monoforce}).
Additionally, the terrain is predicted in the gravity-aligned frame.
That is made possible thanks to the inclusion of camera intrinsics and extrinsics as input to the model,
\autoref{fig:monoforce}.
It also allows correctly modeling the robot-terrain interaction forces (and thus modeling the robot's trajectory accurately)
for the scenarios with non-flat terrain, for example, going uphill or downhill.
This will not be possible if only camera images are used as input.