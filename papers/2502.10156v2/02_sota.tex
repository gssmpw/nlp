\begin{figure}[t!]
\centering
%\includegraphics[width=0.5\textwidth]{imgs/results_generalization}
\includegraphics[width=\columnwidth]{imgs/predictions/dphysics_vs_lstm/dphysics_lstm}
\caption{\textbf{Generalization to robot-endangering scenarios} (non-present in dataset).
    Qualitative example of trajectory predictions with $\nabla$Physics and TrajLSTM (data-driven) models.
    The robot starts at the edge of a cliff and is given a command to move forward.
    \textbf{Left}: robot-terrain interaction prediction with $\nabla$Physics model (a correct estimate of falling down).
    \textbf{Right}: robot-terrain interaction prediction with TrajLSTM~\cite{pang2019aircraft} model
    (incorrect prediction of hovering over a cliff as the robot-endangering situations are typically not present in datasets).}
    \label{fig:dphysics_v_lstm}
\end{figure}
\begin{figure}
    \centering
    \subfigure{\includegraphics[width=\columnwidth]{imgs/predictions/dphysics_vs_lstm/prediction_errors}}
    \caption{\textbf{Generalization and results:} The proposed neuro-symbolic layer $\nabla$Physics generalizes to out-of-distribution examples well.
    The overall results are consistently better independent of the architecture
        (LSS~\cite{philion2020lift}, VoxelNet~\cite{zhou2018voxelnet}, BEV-Fusion~\cite{liu2023bevfusion}) used for terrain properties estimation.}
    \label{fig:traj_errors}
\end{figure}

\subsection{Black-box (Data-driven) Models}\label{subsec:black-box-models}

The environment representation modeling from a single~\cite{mani2020monolayout} or
multiple RGB images~\cite{philion2020lift} in a top-down view has been widely studied in the literature.
The geometry of the scene prediction (taking into account occlusions) from stereo or depth-camera input
has been addressed in~\cite{watson2020footprints}.
The importance and benefits of sensor fusion (camera images, lidar, and radar) for environment representation
have been demonstrated in~\cite{hendy2020fishing}.
The authors of RoadRunner~\cite{frey2024roadrunner} developed an end-to-end learning-based framework
that predicts the traversability and elevation maps from multiple images and a lidar voxel map.
However, the robot-terrain interaction and trajectory prediction tasks are not considered in these works.

The Recurrent Neural Networks (RNN) models~\cite{rumelhart1986learning} and
especially their variant Long Short-Term Memory (LSTM)~\cite{hochreiter1997long} and
Gated Recurrent Unit (GRU)~\cite{cho2014learning} have been widely and
successfully used~\cite{xie2020motion, yoon2022trajectory, pang2019aircraft}
for trajectory prediction tasks of various agents (mobile robots, cars, airplanes, people, and more).
However, their main drawback lies in the difficulty of capturing the underlying physical laws,
which leads to limited generalization capabilities
(more in Section~\ref{subsec:data_driven_baseline} and \autoref{fig:dphysics_v_lstm}).

\begin{figure*}[th]
    \centering
    \includegraphics[width=\textwidth]{imgs/architectures/monoforce/monoforce_v1}
    \caption{\textbf{Detailed architecture overview:} Our model consists of a data-driven \emph{Terrain Encoder} and physics-driven differentiable \emph{Physics Engine}. The \emph{Terrain Encoder} includes the adapted LSS~\cite{philion2020lift} model (that transforms weighted depth predictions
    and rich visual features for each pixel ray to vertically
    projected 2.5D representation) and task-specific convolutional heads (that generate different terrain properties).
    The terrain properties contain the geometrical heightmap $\mathcal{H}_{g}$, the heights of the rigid layer of terrain hidden
    under the vegetation, $\mathcal{H}_{t} = \mathcal{H}_{g} - \Delta\mathcal{H}$, friction, stiffness, and dampening.
    Inside the \emph{$\nabla$Physics engine}, given state, control, and terrain properties, forces at robot-terrain contacts are estimated.
    Finally, these forces are integrated to estimate the resulting robot trajectory.
    Learning employs three losses: \emph{trajectory loss}, $\mathcal{L}_{\tau}$, which measures the distance between
    the predicted and real trajectory; \emph{geometrical loss}, $\mathcal{L}_g$, which measures the distance between
    the predicted geometrical heightmap and lidar-estimated heightmap;
    \emph{terrain loss}, $\mathcal{L}_t$, which enforces rigid terrain on rigid semantic classes revealed through image foundation model (SEEM~\cite{zou2023segment}).
    }
    \label{fig:monoforce}
\end{figure*}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\columnwidth]{imgs/predictions/monoforce/monoforce_heightmaps}
    \caption{\textbf{Detailed view of MonoForce's heightmaps:} Geometrical heightmap replicates lidar heightmap. Soft heightmap outlines soft parts of the terrain (those that do not generate any robot-terrain interaction forces) such as vegetation. Terrain heightmap consists of two layers: (i) Supporting terrain: geometrical heightmap with subtracted soft parts of the terrain; (ii) Friction heightmap: see significantly higher values on dirt.
    }
    \label{fig:monoforce_heightmaps}
\end{figure}

\subsection{White-box (Physics-based) Models}\label{subsec:white-box-models}

The vehicle-terrain interaction modeling and traction mechanics have been widely studied in the literature~\cite{yong2012vehicle},~\cite{blau2008friction}.
To provide accurate analysis and prediction of off-road vehicles behavior on the terrain,
a set of high-fidelity physics-based software tools have been developed.
For example, Vortex Studio~\cite{vortexstudio2025} allows for realistic wheeled and tracked locomotion simulations
and extensive land and even planetary environment modeling.
The Chrono~\cite{serban2019integrated} is a physics-based engine designed for ground vehicle-tire-terrain interactions simulation.
It supports the integration of various vehicle and tire models and can simulate deformable terrain.
The AGX physics engine
(more specifically, its terrain dynamics model \textit{agxTerrain})~\cite{Berglund2019agxTerrain}
enables simulation of soil dynamics and interactions with heavy vehicles.
This software delivers realistic, high-fidelity simulations for applications
involving complex soil-tool interactions.
However, the mentioned physics engines are not differentiable,
which makes them unsuitable for end-to-end learning and integration of exteroceptive real sensor data.


\subsection{Grey-box (Hybrid) Models}\label{subsec:grey-box-models}

To achieve the accuracy and efficiency of data-driven methods and at the same time to
maintain the generalization capabilities of the physics-based models,
the \textit{hybrid} approaches have been developed for off-road navigation.
The PIAug~\cite{maheshwari2023piaug} is a physics-informed data augmentation methodology designed to enhance the learning
of vehicle dynamics, particularly in scenarios involving high-speed and aggressive maneuvers.
The robot's motion prediction in off-road scenarios is the main focus of the work, however,
the accuracy of terrain properties estimation is not addressed there.
The PhysORD~\cite{zhao2024physord} is a promising neuro-symbolic model;
the authors utilize neural networks to estimate the potential energy of a vehicle and
external forces acting on it during the terrain traversal.
The Hamiltonian and Lagrangian mechanic's laws are then used to predict the vehicle dynamics.
However, the integration of exteroceptive sensors (camera images and terrain properties)
is not implemented there, limiting the deployment of the method in real-world off-road scenarios.

Building on the preceding works~\cite{agishev2024monoforce, agishev2024endtoend},
we support the integration of other sensor modalities, such as lidar (Section~\ref{subsec:sensor_fusion}), into the MonoForce model,
improve the accuracy of the physics engine (in terms of trajectory prediction, \autoref{fig:traj_errors}) by using a precise robot model,
expand the data sequences (Section~\ref{subsec:rough_data}) that are used to train the model
focusing on tracked robots and more challenging navigation scenarios (low traction, diverse terrain inclinations, etc),
and perform benchmarking against data-driven baselines (Section~\ref{subsec:dphys_vs_lstm}).
%This paper substantially extends the related works~\cite{agishev2024monoforce, agishev2024endtoend} by supporting
%the integration of other sensor modalities, such as lidar (Section~\ref{subsec:sensor_fusion}), into the terrain prediction model.
%Additionally, the accuracy of their physics engine (in terms of trajectory prediction, \autoref{fig:traj_errors}) is improved using a precise robot model.
%In contrast to their work, we introduce the data sequences (Section~\ref{subsec:rough_data}) that are used to train the model
%focusing on tracked robots and more challenging navigation scenarios (low traction, diverse terrain inclinations, etc),
%and perform benchmarking against data-driven baselines (Section~\ref{subsec:dphys_vs_lstm}).