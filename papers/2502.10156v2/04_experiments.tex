\subsection{Dataset: ROUGH}\label{subsec:rough_data}
\begin{figure}
    \centering
    \includegraphics[width=0.99\columnwidth]{imgs/robots/robots}
%    \includegraphics[width=0.99\columnwidth]{imgs/robots/robots_blured}
    % \subfigure[\textit{Bluebotics Absolem} robot]{\includegraphics[width=0.38\columnwidth]{imgs/absolem}}
    % \subfigure[\textit{MARV} robot]{\includegraphics[width=0.44\columnwidth]{imgs/marv}}
    \caption{\textbf{Two robot platforms} used to collect the ROUGH dataset. Notice that one platform has only four massive flippers, and the other has also two main tracks.}
    \label{fig:robot_platforms}
\end{figure}
We pre-train the terrain encoder on the large-scale outdoor dataset RELLIS-3D~\cite{jiang2020rellis3d}.
It is a multimodal dataset collected in an off-road environment containing accurately localized (with RTK GPS)
$13,556$ lidar scans and corresponding time-synchronized RGB images.

Despite the amount and annotation quality of the data provided in the RELLIS-3D sequences,
it lacks examples of a robot moving over hills, obstacles, and traversing high grass.
To fill this gap in autonomous off-road navigation, a new
Rugged Obstacles and Uneven Ground Hardcore (\emph{ROUGH}) dataset will be released,
containing sensory data from forest and field scenarios recorded with the two robot platforms:
\textit{MARV}, \textit{Bluebotics Absolem},~\autoref{fig:robot_platforms}.
It contains several hours of driving with the mid-sized robots in challenging terrain.
The ultimate goal is to provide recordings of traversals on (or through) flexible natural obstacles like tall grass,
hanging tree branches, mud, dense undergrowth, etc.
The ROUGH sequences are collected with shape-changing tracked robots~(\autoref{fig:robot_platforms}),
which allows capturing a much larger range of dynamic responses - simply moving the auxiliary tracks
changes the center and moments of inertia and the contact surface.
To correctly utilize the effects of dynamics,
ROUGH provides not only sensory data but also robot models with dynamic properties.
The data set also contains carefully time-synchronized point cloud scans from Ouster OS0-128 lidar and corresponding RGB images
from $4$ Basler cameras installed on the robots (front, rear, left, and right).
The robot localization data obtained using ICP SLAM~\cite{Pomerleau-2013-AR} is also recorded.
It was a better choice than GPS which is unreliable near or under tree canopy and close to buildings.
The controls ($\mathbf{u}$ in~\autoref{fig:monoforce}) in the form of commanded tracks velocities and angles
are recorded and used to train and evaluate the MonoForce model.

After training the MonoForce terrain encoder on the RELLIS-3D dataset, we fine-tune it on the ROUGH data sequences.
Please note that the dataset is under construction at the moment of writing and will be extended
with the help of new robot platforms and data collection scenarios.
The ROUGH dataset will be made publicly available\footnote{\url{https://github.com/ctu-vras/rough-dataset}}.
%The ROUGH dataset will be made publicly available.


\subsection{Sensor Fusion}\label{subsec:sensor_fusion}
% BEVFusion architecture
\begin{figure*}[th]
    \centering
    \includegraphics[width=\textwidth]{imgs/architectures/bevfusion}
    \caption{\textbf{BEVFusion}~\cite{liu2023bevfusion} architecture. The model processes multi-modal inputs (RGB images and lidar point cloud) and fuses them into a shared bird's-eye view (BEV) space. The LSS~\cite{philion2020lift} architecture part is sketched in green (upper branch), while the model branch processing point cloud input (VoxelNet~\cite{zhou2018voxelnet}) is depicted in blue. The fused BEV features are further used for terrain properties prediction tasks.}
    \label{fig:bevfusion}
\end{figure*}

In this section, we study to what extent the inclusion of other sensor modalities (except for RGB images)
improves terrain estimation accuracy.
As we discuss in Section~\ref{subsec:terrain_encoder}, the terrain encoder model predicts terrain properties
solely from RGB input.
However, a natural question arises whether an additional point cloud input or a sensor fusion technique
could improve the prediction.
We adapt the architecture introduced in~\cite{liu2023bevfusion} for terrain properties prediction.
The modified BEVFusion architecture for the terrain prediction task from RGB and lidar inputs is provided
in the \autoref{fig:bevfusion}.

For different sensor inputs (multi-view RGB cameras and lidar),
modality-specific encoders are applied to extract their features.
Then the sensor-specific features are transformed into a unified BEV representation
that preserves both semantic and geometric information.
Following the~\cite{liu2023bevfusion} approach the convolution-based BEV encoder
is applied next to the stacked BEV features to account for possible local misalignment between different sensors.
The task-specific heads are added to predict different terrain properties
(for example, its shape as a form of \textit{heightmap} and \textit{friction})
as shown in the \autoref{fig:bevfusion}.
The LSS~\cite{philion2020lift} model is used as the camera input BEV-encoder,
as described in Section~\ref{subsec:terrain_encoder} and the BEVFusion~\cite{liu2023bevfusion} paper.
The VoxelNet~\cite{zhou2018voxelnet} model is utilized as the point cloud input BEV-encoder.
First, the raw point cloud input is voxelized into a $3D$ voxel grid.
The point cloud coordinates are mapped to a discrete grid of binary indices
(a value of $1$ denotes that the voxel is occupied).
Next, the $3D$-convolution-based lidar Encoder processes the voxelized input to extract
high-level feature representation.
This operation is followed by $\max$-pooling along the $z$-axis to reduce the $3D$ feature map to $2D$ BEV space.

We train and evaluate the LSS~\cite{philion2020lift}, VoxelNet~\cite{zhou2018voxelnet},
and BEVFusion~\cite{liu2023bevfusion} terrain encoders on the validation part of the ROUGH dataset (Section~\ref{subsec:rough_data})
following the procedure described in Section~\ref{subsec:end2end_learning}.
As the evaluation metrics, we use terrain prediction accuracy, \autoref{tab:results}.
The $L2$-norms are computed to estimate geometrical, $\Delta\mathcal{H}_g$~(\ref{eq:geom_loss}),
and terrain, $\Delta\mathcal{H}_t$~(\ref{eq:terrain_loss}), heightmap prediction accuracy
covering both geometric- and semantic- oriented tasks.
The results in the two right-most columns of the \autoref{tab:results} contain the terrain prediction accuracy metrics.
The lidar input-based model (VoxelNet) performs better than the camera input-based LSS.
However, the benefit of the sensor fusion on the terrain accuracy prediction is not obvious
as the metrics $\Delta\mathcal{H}_g$ and $\Delta\mathcal{H}_t$ for both models (VoxelNet and BEVFusion) are nearly equal.
\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{imgs/predictions/sensor_fusion/comparison_architectures}
    \caption{The supporting terrain $\mathcal{H}_t$ prediction with the LSS~\cite{philion2020lift}~(\textbf{first row}), VoxelNet~\cite{zhou2018voxelnet}~(\textbf{second row}),
        and BEVFusion~\cite{liu2023bevfusion}~(\textbf{third row}) models.
    The predicted supporting terrain $\mathcal{H}_t$ is projected onto camera images (as a point cloud)
        and visualized in the top-down view (on the right).}
    \label{fig:terrain_encoders_results}
\end{figure}
The qualitative results of the terrain properties prediction are visualized in \autoref{fig:terrain_encoders_results}.
It can be seen that although the LSS model provides a good estimate of the terrain shape
(and thus the predicted trajectory matches the ground truth quite closely),
it struggles to predict obstacles with sharp edges (tree trunks).
The models that have a lidar input (VoxelNet and BEVFusion) provide more accurate terrain predictions.
The BEVFusion prediction is also visualized in 3D in \autoref{fig:catch-eye} with the lidar point cloud input.
\begin{table*}
    \caption{Trajectory and terrain estimation accuracy.}\label{tab:results}
    \centering
    \begin{tabular}{c | c | c | c | c | c | c | c}
    \hline
    input & method & terrain encoder & $\tau$ pred. & $\Delta \mathbf{x}$ [\si{\meter}] & $\Delta\mathbf{R}$ [\si{\deg}] & $\Delta \mathcal{H}_{g}$ [\si{\meter}] & $\Delta \mathcal{H}_{t}$ [\si{\meter}] \\
    \hline

    RGB & hybrid & LSS~\cite{philion2020lift} & $\nabla$Physics~(\textbf{ours}) & 0.062 & 2.042 & \multirow{2}{*}{0.1177} & \multirow{2}{*}{0.0896} \\
    \cline{1-6}

    RGB & data-driven & LSS~\cite{philion2020lift} & TrajLSTM~\cite{pang2019aircraft}  & 0.128 & 3.949 &  &  \\
    \hline

    point cloud & hybrid & VoxelNet~\cite{zhou2018voxelnet} & $\nabla$Physics~(\textbf{ours}) & \textbf{0.056} & \textbf{1.939} & \multirow{2}{*}{\textbf{0.0987}} & \multirow{2}{*}{0.0774} \\
    \cline{1-6}

    point cloud & data-driven & VoxelNet~\cite{zhou2018voxelnet} & TrajLSTM~\cite{pang2019aircraft}  & 0.129 & 3.369 &  & \\
    \hline

    \makecell{RGB +\\ point cloud} & hybrid & BEVFusion~\cite{liu2023bevfusion} & $\nabla$Physics~(\textbf{ours}) & 0.068 & 1.966 & \multirow{3}{*}{0.0989} & \multirow{3}{*}{\textbf{0.0771}} \\
    \cline{1-6}

    \makecell{RGB +\\ point cloud} & data-driven & BEVFusion~\cite{liu2023bevfusion} & TrajLSTM~\cite{pang2019aircraft} & 0.125 & 3.067 & & \\
    \hline

    \end{tabular}
\end{table*}


\subsection{Physics-based vs Data-driven Baseline}\label{subsec:dphys_vs_lstm}

The following case study compares our hybrid physics-data-driven approach (Section~\ref{subsec:dphysics})
to a purely data-driven method (Section~\ref{subsec:data_driven_baseline}) in terms of trajectory estimation accuracy.
We compare the physics model $\nabla$Physics introduced in Section~\ref{subsec:dphysics} with the data-driven TrajLSTM network in terms of trajectory prediction accuracy.
The following metrics are computed for qualitative evaluation on the validation part of the ROUGH dataset (Section~\ref{subsec:rough_data}):
\begin{equation}~\label{eq:xyz_diff}
    \Delta\mathbf{x}=\sqrt{\frac{1}{T}\sum_{t=0}^T \|\mathbf{x}_t - \mathbf{x}_t^{*}\|}
\end{equation}
\begin{equation}~\label{eq:rot_diff}
    \Delta\mathbf{R}=\frac{1}{T}\sum_{t=0}^{T}\arccos\frac{\mathrm{tr}({\mathbf{R}^{\top}_t\mathbf{R}^{*}_t})-1}{2}
\end{equation}
The~\eqref{eq:xyz_diff} and~\eqref{eq:rot_diff} describe the predicted trajectory $\{\mathbf{x}_t, \mathbf{R}_t\}$ translational and rotational errors respectively w.r.t. the ground truth $\{\mathbf{x}_t^{*}, \mathbf{R}_t^{*}\}$, $t \in \{0 \dots T\}$.
The ground truth poses were recorded using the SLAM method introduced in~\cite{Pomerleau-2013-AR}.
The results are summarized in \autoref{tab:results} (columns $\Delta\mathbf{x}$ and $\Delta\mathbf{R}$)
and \autoref{fig:traj_errors}.
Note that for the fare comparison, the TrajLSTM architecture is designed in a way that
it has the same interface as the $\nabla$Physics module (takes the same input and yields the same output).
This allows to use the $\nabla$Physics and TrajLSTM models interchangeably
with different terrain encoder models
(LSS~\cite{philion2020lift}, VoxelNet~\cite{zhou2018voxelnet}, BEVFusion~\cite{liu2023bevfusion})
that provide terrain estimates for the trajectory predictors.
It can be seen from \autoref{fig:traj_errors} that the physics-driven
trajectory predictor ($\nabla$Physics) provides better trajectory prediction
(both in translational and rotational components) regardless of the terrain estimation method.
Additionally, for the data-driven trajectory predictor (TrajLSTM) we observe an impact of sensor fusion.
The usage of BEVFusion terrain encoder helps to better estimate the translation and rotation of the robot
w.r.t. its single-modality baselines (image-based LSS and lidar-based VoxelNet).

Overall, the point cloud input helps to predict terrain more accurately
(Section~\ref{subsec:sensor_fusion} and \autoref{tab:results})
and the physics-based trajectory predictor $\nabla$Physics provides better results
than its data-driven baseline TrajLSTM (\autoref{fig:traj_errors}).


\subsection{Physics Engine Computational Efficiency}\label{subsec:computational_efficiency}
\begin{figure}
    \centering
    \subfigure[\emph{CPU}:~AMD Ryzen 7 4800H, \emph{GPU}:~GeForce GTX 1660 Ti Mobile, $N_{traj}=512$]{\includegraphics[width=0.48\columnwidth]{imgs/results/dphys_runtime_ntrajs_512_laptop}}
    \subfigure[\emph{GPU}:~Tesla V100-SXM2-32GB, $N_{traj}=2048$]{\includegraphics[width=0.48\columnwidth]{imgs/results/dphys_runtime_ntrajs_2048_V100_GPU}}

    \caption{The results are provided with the following configurations:
    grid reslution: $0.1~[m]$, number of robot body points: $223$ (uniformly sampled with voxel size $0.1~[m]$).}
    \label{fig:dphys_runtime}
\end{figure}
The overall learning behavior (speed and convergence) is mainly determined by
(i) the time horizon over which the difference between trajectories is optimized and
(ii) the way the differentiable ODE solver is implemented.
We compare further the computational efficiency of the two implementations
of the differentiable physics engine: based on \textit{Neural ODE} and \textit{auto-differentiation}.

The \autoref{fig:dphys_runtime} shows the runtime of the differentiable physics engine depending on
the predicted trajectories time horizon for the two solvers' implementation methods and different hardware configurations.
The \autoref{fig:dphys_runtime}~(a) shows the runtime on the standard laptop hardware (CPU and GPU),
while the \autoref{fig:dphys_runtime}~(b) shows the runtime on the high-performance GPU.
It can be noticed that the runtime grows linearly with the time horizon in all cases and
the laptop GPU implementation is around 5 times faster than the CPU one.
The \textit{Neural ODE} solver~\cite{neural-ode-2021} is slightly more efficient than the \textit{auto-differentiation} one.
For example, on a high-performance GPU (Tesla V100-SXM2-32GB), the prediction of
2048 trajectories ($6~[\si{\sec}]$-long) takes around $1~[\si{\sec}]$.
The short runtime of the differentiable physics engine allows for its
efficient usage in the end-to-end learning pipeline (Section~\ref{subsec:end2end_learning}) and for real-time
trajectory shooting in navigation scenarios (Section~\ref{subsec:navigation}).

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{imgs/navigation/sweden/navigation_global}
    % \subfigure[The constructed point cloud map of the forest environment and robot's path during the navigation experiment;
    % the coordinate frames denote waypoints locations]{\includegraphics[width=0.40\textwidth]{imgs/navigation/pcd}}
    % \subfigure[The aerial photo of the forest area was taken for reference]{\includegraphics[width=0.45\textwidth]{imgs/navigation/drone_map}}
    \caption{The top-down view of the navigation experiment in the forest environment.
    During the experiment, the robot autonomously traverses the 260-meter-long path.
    The point cloud map construction~(a) and robot's localization were performed using the ICP SLAM method~\cite{Pomerleau-2013-AR}}.
    \label{fig:nav_maps}
\end{figure*}

\subsection{Navigation in Unstructured Environments}\label{subsec:navigation}
\begin{figure*}
    \centering
    %\includegraphics[width=\columnwidth]{imgs/navigation/navigation_sweden}
    \includegraphics[width=\textwidth]{imgs/navigation/sweden/navigation_mppi}
    \caption{Autonomous navigation in the forest environment with MonoForce.
    The robot follows a set of waypoints and at the same time avoids obstacles (trees, bushes, rocks, etc.).
    \textbf{First row (Control setup):}
    MonoForce prediction, based only on the onboard camera, includes terrain shape and a set of trajectories for different control commands sampled through a simplified MPPI technique.
    The colors of a possible trajectory correspond to the cost (red is the most expensive).
    The trajectory with green arrows is the one with the smallest total cost,~\eqref{eq:total_cost}.
    \textbf{Second row (Qualitative results)}: Predicted supporting terrain and robot trajectory given camera images.
    The predictions and frontal camera images correspond to different time moments of the navigation experiment in the forest environment~(\autoref{fig:nav_maps}).
    % \textbf{Left}: view from the robot's frontal camera.
    % \textbf{Right}: MonoForce prediction visualized in 3D for the frontal camera field of view.
    }
    \label{fig:nav_monoforce}
\end{figure*}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{imgs/navigation/petrin/monoforce_petrin}
    \caption{MonoForce prediction projected to robot's camera images.
    The prediction includes supporting terrain $H_t$;
    the colors correspond to the friction coefficient $\mu$ (violet is low, red is high).
    A set of 64 trajectories for different control commands is visualized in white and black (low and high trajectory cost~\eqref{eq:traj_cost} respectively).
    The black frames denote the camera's field of view.}
    \label{fig:nav_petrin}
\end{figure}
Thanks to integration of the extereoceptive sensor to \emph{Terrain Encoder} and GPU parallelization of the predicted trajectories (Section~\ref{subsec:dphysics}),
it is possible to use the MonoForce model for autonomous navigation in unstructured outdoor environments.
In the navigation experiments, the robot is given a set of distant waypoints to follow.
It is localized using the ICP SLAM method~\cite{Pomerleau-2013-AR}.
The MonoForce model runs onboard the robot's hardware and predicts terrain properties in front of the robot and a set of possible trajectories for different control commands.
In the case of a tracked robot platform (\autoref{fig:robot_platforms}), the controls are the commanded velocities of individual tracks.
A sample of predicted trajectories for given controls is visualized in \autoref{fig:nav_petrin}.
To reach a goal safely, the robot chooses the trajectory with the smallest cost and distance to the next waypoint,~\autoref{fig:nav_monoforce}
Thanks to the MonoForce ability to predict robot-terrain interaction forces for a trajectory pose,
it is possible to estimate the cost of each trajectory.
We decide to calculate the cost as a values variance of reaction forces acting on the robot along a trajectory $\tau$:
\begin{equation}
    \label{eq:traj_cost}
    \mathcal{C}_{\tau} = \frac{1}{T}\sum_{t=0}^{T}\left\|\mathbf{N}_t - \mathbf{\bar{N}}\right\|,
\end{equation}
where $\mathbf{N}_t$ is the predicted reaction force acting on the robot at a time moment $t$
and $\mathbf{\bar{N}}$ is the mean value of the total reaction forces along the trajectory.
Note that in the~\eqref{eq:traj_cost}, we denote
by $\mathbf{N}_t$ the total reaction force acting on all robot's body points at a time moment $t$.
The waypoint cost for a trajectory $\tau$ is simply calculated as the Euclidean distance between the trajectory and the next waypoint:
\begin{equation}
    \label{eq:waypoint_cost}
    \mathcal{C}_{\text{wp,$\tau$}} = \min_{x \in \tau}\left\|\mathbf{x} - \mathbf{x}_{\text{wp}}\right\|,
\end{equation}
The total cost used for the trajectory selection in navigation is the weighted sum of the trajectory and waypoint costs:
\begin{equation}
    \label{eq:total_cost}
    \mathcal{C}_{\text{total}} = \alpha \mathcal{C}_{\tau} + \beta \mathcal{C}_{\text{wp,$\tau$}},
\end{equation}
where $\alpha$ and $\beta$ are the hyperparameters for the trajectory and waypoint costs, respectively.
With the described navigation approach, the robot is able to autonomously navigate in the forest environment with uneven terrain,
avoiding obstacles and following the set of waypoints, \autoref{fig:nav_monoforce}.
The \autoref{fig:nav_maps} shows the constructed point cloud map of the forest environment during the navigation experiment.
It also contains an aerial photo of the forest area with the robot's trajectory and the waypoints.
The robot was able to autonomously navigate in the forest environment and traverse the 260-meter-long path.
