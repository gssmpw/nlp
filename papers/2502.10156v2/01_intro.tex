\IEEEPARstart{A}utonomous robotics in off-road environments holds immense promise for various commercial applications,
including outdoor logistics, inspections,
% and military operations.
and forestry operations.
Yet, unlike systems designed for controlled environments, such as factories, off-road autonomous systems still remain
prohibitively immature for dependable deployment.
The main challenge that prevents reliable off-road deployment lies in the inability to predict the behavior accurately
of the robot on the terrain.
For example, the prediction of robot trajectory in tall vegetation or muddy and rocky terrains,
especially from camera images and lidar scans, remains an unresolved problem in outdoor robotics.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.5\textwidth]{imgs/predictions/monoforce/qualitative_results_v2}
  %\includegraphics[width=0.5\textwidth]{imgs/overview}
  \caption{\textbf{Qualitative results:} The first three rows show (i) onboard camera images (the only input), (ii) predicted supporting terrain and (iii) a 3D view with predicted trajectory and visualized ground truth lidar point clouds (not used for estimation). While the majority of rigid objects are correctly reconstructed from the camera image, robot flippers, tall grass, fallen branches, or soft hanging branches are correctly suppressed as they do not influence the resulting robot trajectory. Predicted terrain friction, dampening, and stiffness are not visualized for brevity.
  %\textbf{Model overview:} The proposed model can be seen as an image-conditioned differentiable simulation that delivers a million simulated trajectories per second on the terrain depicted in the onboard camera image. The explainable structure also delivers many intermediate interpretable outputs that can serve for efficient self-supervision.
  }
  \label{fig:catch-eye}
\end{figure}


\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.75\textwidth]{imgs/architectures/overview_v2}
  \caption{
  \textbf{Model overview:} The proposed model can be seen as an image-conditioned differentiable simulation that delivers a million simulated trajectories per second on the terrain depicted in the onboard camera image. The explainable structure also delivers many intermediate interpretable outputs that can serve for efficient self-supervision.}
  \label{fig:model_overview}
\end{figure*}

Over the last decade,
roboticists proposed a wide variety of \emph{white-box}~\cite{Fabian2020, Dogru-AuRo-2021, manoharan-IROS-2024}
and \emph{black-box} models~\cite{loquercio-ICRA-2023, Niu-FRAI-2023, Wellhausen-RAL-2019, hdif2023, kahn2020badgr} for the off-road trajectory prediction.
Black-box models primarily suffer from a severe out-of-distribution problem~-- the phenomenon
where the distribution of training data does not correspond to the testing data.
This problem naturally arises from the fact that the training data includes only the trajectories from safe
terrain traversals, such as small terrain steps, while the decision about terrain traversability also naturally
comprises the non-traversable terrains, such as tall cliffs, \autoref{fig:dphysics_v_lstm}.
Since the step-to-cliff-generalization of large state-of-the-art black-box models is typically poor,
the safety of the resulting system is arbitrarily inferior.
% Unfortunately,
% the safety of the resulting system is mainly determined by the performance of the model on non-traversable terrains,
% which could be arbitrarily bad due to their absence in the training data.
On the other hand, \emph{white-box} architectures offer good generalization due to substantial inductive bias
but often suffer from the sim-to-real gap~-- the phenomenon where a model \("\)trained in\("\) or \("\)replaced by\("\)
simulation faces challenges or discrepancies when applied in the real world.
Although several techniques, such as rapid motor adaptation~\cite{rma-2021}, are known to partially suppress this issue
through domain randomization, the sim-to-real gap, as well as the out-of-distribution problem,
makes trajectory prediction from camera images prohibitively unreliable.
We introduce a \emph{grey-box} model that combines the best of both worlds to achieve better generalization and a smaller sim-to-real gap.
This model enforces white-box laws of classical mechanics through a physics-aware neural symbolic layer,
while preserving the ability to learn from huge data as it is end-to-end differentiable.
We demonstrate that even just a single onboard camera is enough for reliable autonomous deployment in off-road environments; see Figures~\ref{fig:catch-eye},~\ref{fig:catch-eye2} for some qualitative results and Figure~\ref{fig:traj_errors} for quantitative evaluation.

%explainable, physics-aware, and end-to-end differentiable model that improves generalization and suppresses the sim-to-real gap to the level at which the camera input becomes more reliable than existing competitors.

%We introduce a \emph{grey-box}, explainable, physics-aware, and end-to-end differentiable model that improves generalization and suppresses the sim-to-real gap to the level at which the camera input becomes more reliable than existing competitors.
%(i) improves generalization by introducing inductive bias through neural symbolic layers and (ii) suppresses the sim-to-real gap through end-to-end learnability.

%substantial physics and geometrical priors encoded in the neural symbolic layers and the efficient self-supervision enabled by model explainability.

% included in the neural symbolic physics layer 
%enables self-supervised learning also from non-traversable terrains.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.5\textwidth]{imgs/predictions/monoforce/qualitative_result2}
  \caption{
  \textbf{Qualitative results:} Given input onboard image from the robot's camera (first row), the method distinguishes stones and trees as obstacles, while vegetation is suppressed (see 3D view in third row) the stone is preserved despite having almost the same green color as vegetation due to the moss coverage. Heightmap color encodes predicted terrain friction, and dark blue arrows show predicted contact forces. The predicted robot trajectory is visualized in light green. The ground truth robot motion is captured in the second row.}
  \label{fig:catch-eye2}
\end{figure}

%%% STATISTICALLY INCONSISTENT LEARNING %%%%
We highlight that all state-of-the-art models, including the proposed one, are inherently statistically inconsistent
as it is impossible to provide training data covering dangerous cases.
%the difference between training/testing errors does not converge to zero
%as the number of training examples approaches infinity.
Consequently, providing a close-to-infinite number of training examples, as typical,
for large foundation models~\cite{Kirillov-ICCV-2023} does not imply a superior performance.
%The primary sources of the inconsistency are (i)
%the out-of-distribution problem on real data and (ii) the sim2real gap on simulated data.
Addressing the statistical inconsistency for the trajectory prediction task
without resorting to real-world robot-damaging trajectories remains an open challenge.
%It remains unclear how to avoid the inconsistency without the necessity of providing real robot-damaging trajectories.
%%We contribute by introducing the grey-box model, which combines (i) the strength of black-box models to learn from the real data to reduce the sim-to-real gap with (ii) the strength of white-box approaches to generalize well on unseen data to reduce the out-of-distribution problem.
In this work, we make a step toward this direction by introducing a grey-box model that leverages:
(i) the strengths of black-box models to learn effectively from real data, thereby reducing the sim-to-real gap, and
(ii) the strengths of white-box approaches to generalize effectively to unseen data, thereby addressing the out-of-distribution problem.

%However,
%it remains unclear if we could ever reduce the influence to the level
%that enables reliable prediction of the robot-terrain interaction.

In our implementation,
the black-box model predicts robot-terrain interaction forces and the shape of the robot-supporting terrain,
while the neural symbolic layer, which contains a differentiable physics engine, solves the robot's trajectory
by querying these forces at the robot-terrain contacts.
The main advantage of this approach is that it enables further self-supervision induced by lidar measurements,
which essentially provides an
% differentiable
upper bound on the terrain shape.
Since lidar measurements are not restricted to safe terrains,
the prediction of the shape remains statistically consistent.
If we could further assume that terrain textures on unsafe terrains also appear on safe terrains,
the whole procedure would be statistically consistent.
However, this is not true in general, as there could be some inherently unsafe textures,
%such as explosive mines,
such as holes covered by vegetation,
which do not have their safe counterpart.

%%% STRONG PRIORS, DIFFERENTIABILITY, APPLICATIONS, WARP/BRAX unstable gradients %%%
As the proposed architecture comprises substantial geometrical and physics priors,
the resulting model can be also seen as \emph{learnable physics engine conditioned by a real image}
that delivers \emph{$10^4$ trajectories per second}; see~\autoref{fig:model_overview} for details (the engine efficiency is also investigated in the Section~\ref{subsec:computational_efficiency}).
In addition to that, the model is end-to-end-differentiable; therefore, gradients can be backpropagated towards its
(i) convolutional filters, (ii) camera and robot parameters, (iii) control, (iv) positions, and (v) terrain properties.
The differentiability, in conjunction with the rapid simulation speed, makes the model suitable for a myriad of tasks,
including model predictive control~\cite{Amos-NEURIPS-2018}, trajectory shooting~\cite{Zeng-CVPR-2019},
supervised and reinforcement learning~\cite{schulman2017proximal}, SLAM~\cite{factorgraph-2017},
online robot-model reidentification or camera recalibration~\cite{Moravec-CVWW-2018}.
The explainable structure of the proposed architecture also delivers a variety of intermediate outputs,
such as terrain shape and its physical properties, robot-terrain reaction forces or contact points,
which can all serve as efficient sources of self-supervision if measured during the training set creation or restricted
in PINN-like manner~\cite{Farea-AI-2024}.
We observed that the instability of gradient computation in existing differentiable simulators,
such as NVIDIA's WARP~\cite{warp2022} and Google's BRAX~\cite{brax2021}, makes them prohibitively unreliable
and slow for both the learning and the inference.
%Despite the existence of differentiable simulators such as NVIDIA's WARP~\cite{warp2022} and Google's BRAX~\cite{brax2021},
%we observed
%that the instability of gradient computation in all available solvers makes them prohibitively unreliable and slow for both the learning and the inference.
As a suitable replacement, we introduce a differentiable neural symbolic layer, which involves our
from-scratch implemented physics engine, into the proposed architecture.
This solution outperforms existing works in gradient stability and computational speed
while preserving sufficient accuracy
for reliable trajectory prediction.
As all target applications, including learning, control, planning, and SLAM, can be naturally parallelized,
we also achieve significant speed-up through massive parallelization on GPU.
% In particular, we employ self-supervision from the robot's trajectories estimated by a common SLAM procedure, geometrical heightmaps estimated from lidar scans~\cite{Niu-FRAI-2023}, and material types estimated through Microsoft's image foundation model~\cite{li2023semantic}. While lidar scans serve as an upper bound on the shape of predicted heightmaps, the image foundation model delivers prior explicit knowledge about the rigidity of some objects that cannot be traversed. 

%%%%%%%%%%%%% DIFFERENTIABLE PHYSICS ENGINE %%%%%%
% We propose several differentiable physics engines that convert the predicted terrain and control into trajectory. The first implementation is based on a simple kinematic model which assumes that the robot always lies at the minimum of its potential energy. Since this model contains a non-convex constrained optimization problem in the feedforward pass, we backpropagate it through its KKT conditions.
% ~\cite{Salansky-RAL}.
% The remaining three implementations explicitly model physics interaction between the robot body and non-rigid terrain.
% ~\cite{Agishev-IROS-2024}.
% Backpropagation is based on auto-differentiation~\cite{Paszke-NIPS-2019}, Neural ODE~\cite{neural-ode-2021} and Nvidia's WARP~\cite{warp2022}.
% %%%%%%% CONTINUE FROM HERE %%%%%%%%%%
% \ifx\false
% It has been shown that supervised learning of robot-terrain reaction forces is possible for legged robots~\cite{li2023seeing} because the force appears only on its small-sized feet that are in contact with the terrain. 
% Such force is inherently well-localized in the camera frame, and its magnitude and direction can be measured accurately through torque sensors.
% Therefore, the training data can be easily provided. 
% However, direct measurement of contact forces for wheeled or tracked robots~\cite{Inoue2008} requires highly specialized sensors and is typically hard to achieve in outdoor scenarios.
% Thus, the contact forces have to be inferred from robot trajectories.  
% Since both the MonoForce predictor and the physics engine are fully differentiable in our implementation, we train the predictor by backpropagating the trajectory error through the physics engine into the predictor parameters. 
% In such a setup, the physics engine plays the role of a self-supervised loss that measures the consistency between predicted forces and ground truth trajectories.
% The resulting setup then resembles the well-known MonoDepth model~\cite{monodepth2}, which learns to predict depth from the image by optimizing \emph{color consistency} between the predicted depth and the image delivered by another calibrated camera onboard.
% %Supervised learning of the terrain encoder is not tractable because the ground truth robot-terrain interaction forces are not directly measurable. 
% %At this point, we take inspiration from the well-known MonoDepth model~\cite{monodepth2} which learns to predict depth in a self-supervised manner by optimizing the \emph{geometrical consistency} between the predicted depth and the image delivered by another calibrated camera onboard. Similarly, we optimize the \emph{physical consistency} between the predicted forces and the robot's trajectory. This setup is also self-supervised, since locally accurate ground truth robot trajectories can typically be recovered from onboard measurements via a common SLAM procedure. 
% The most straightforward architecture of the force predictor would be a black-box, deep-convolutional network that would transform images and control commands into a 3D force field over the state space.
% However, since there is an inherent training/testing distribution mismatch due to the natural absence of robot-endangering samples in the real training data, good generalization of the force predictor is crucial.
% Consequently, we search for architectures which combine black-box deep convolution layers with the white-box laws of classical mechanics and camera geometry.
% The resulting model builds on the lift-splat-shoot architecture~\cite{philion2020lift} that predicts the terrain properties, such as terrain shape or its rigidity, from a single image and delivers forces that are directly computed from these properties. 
% %Towards this end, we train the terrain encoder to predict the latent interaction forces such that when employed in the $\nabla$physics engine module, they generate similar trajectories to the ground truth ones. 
% %To proceed with such a physics-supervised setup, the $\nabla$physics module is designed as a custom-built differentiable ODE solver similar to NeuralODE~\cite{neural-ode-2021}. 
% We emphasize that the resulting model is also directly differentiable with respect to the robot model. Therefore, joint optimization of robot model parameters, such as its center of gravity or moment of inertia, is also possible. The model is also directly differentiable with respect to control; therefore, its plug-and-play application in state-of-the-art MPC controllers~\cite{Amos-NEURIPS-2018} is at hand.
% %The compound model is end-to-end-differentiable since the $\nabla$ part is a custom-built differentiable ODE solver similar to NeuralODE~[]. 
% \fi

\textbf{Our main contributions} are as follows.

\textbf{Step towards statistically consistent learning:} Explainability of the proposed grey-box model provides several
well-interpretable intermediate outputs that serve as a natural source of self-supervision.
The self-supervision, hand in hand with a substantial physical prior,
increases the statistical consistency of the learning procedure.

\textbf{Image-conditioned simulation:} The end-to-end differentiable image-conditioned simulation is suitable
for a myriad of robotics tasks such as control or SLAM, as it predicts $10^4$ trajectories in parallel per second.

\textbf{Experimental evaluation on non-rigid terrains:} The proposed model outperforms other state-of-the-art methods
on non-rigid terrains, such as grass or soft undergrowth that deforms when traversed by the robot. See \autoref{fig:traj_errors} for quantitative results.
%In contrast to existing black-box models~\cite{Wellhausen-RAL-2019, hdif2023}, we argue that any well-generalizing model should be knowledgeable about the white-box laws of classical mechanics, such as (i) if the robot is falling, then it is accelerated by the gravity or (ii) any contact with the terrain results in forces acting on the robot body.
