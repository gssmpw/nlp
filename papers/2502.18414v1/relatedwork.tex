\section{Related Work}
% \subsection{Generalized Category Discovery}

\noindent \textbf{Generalized Category Discovery. }
Generalized Category Discovery (GCD) \cite{vaze2022generalized, wen2023simgcd, an-etal-2024-generalized, liang-etal-2024-actively} is a recently emerged task that addresses a realistic scenario where only limited labeled data is available and new categories may emerge. The goal is to automatically cluster all unlabeled data from both seen and novel categories \cite{bai2023towards, zou-etal-2023-decrisismb}. Pioneering works \cite{vaze2022generalized, pu2023dynamic} employ supervised and unsupervised contrastive learning to obtain discriminative embeddings, followed by clustering methods such as K-Means. \citet{wen2023simgcd, bai2023towards} propose leveraging parametric classifiers and soft pseudo labels to mitigate model bias towards seen categories, thereby enhancing overall performance. Despite these advancements, traditional GCD methods that do not actively leverage additional human or LLM supervision signals \cite{zhang2021discovering, zhang-etal-2022-new, zhou-etal-2023-probabilistic, zhang2023clustering, liang-liao-2023-clusterprompt, Raedt2023IDASID, sung-etal-2023-pre} still face significant challenges, including difficulty in rectifying errors for confusing instances and an inability to leverage the semantic meanings of discovered clusters \cite{ma2024active, an-etal-2024-generalized}. \\


% \subsection{Active Learning and GCD}
\noindent \textbf{Active Learning and GCD. }
Active Learning (AL) \cite{ren2021survey, Ma2024ActiveGC} aims to improve model performance by selecting and annotating a limited number of informative samples. Diverse sample selection strategies have been proposed, including uncertainty-based methods \cite{wang2014new, zhang-etal-2023-clusterllm}, diversity-based methods \cite{sener2017active, Ash2020Deep} and hybrid methods \cite{agarwal2020contextual, huang2010active}. However, annotation in traditional AL is usually expensive and time-consuming \cite{cheng-etal-2023-improving, zhang-etal-2023-clusterllm}. Recent work in GCD, such as ALUP \cite{an-etal-2024-generalized} and Loop \cite{liang-etal-2024-actively}, has started to leverage LLMs feedback as a cost-effective alternative to human annotators to provide additional supervision signal for ambiguous data. However, these approaches utilize only a single type of LLM feedback in their model learning processes or do not account for feedback quality \cite{an-etal-2024-generalized, liang-etal-2024-actively, liang-etal-2024-synergizing}. These limitations restrict their ability to effectively refine model predictions, particularly in ambiguous or challenging cases. To address these issues, we propose a unified framework for GCD that actively learns from diverse and quality-enhanced LLM feedback. To the best of our knowledge, this is the first work to consider both the diversity and quality of LLM feedback in the context of GCD. Table \ref{tab:comp_closely_related_work} provides a detailed comparison of our approach and closely related work in terms of LLM feedback diversity and quality.


% Other existing GCD methods do not utilize LLM feedback at all. 
% but they have primarily focused on instance-level feedback for refining contrastive embeddings. Our work extends this concept by integrating diverse and effective LLM feedback for generalized category discovery.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%