% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\PassOptionsToPackage{table}{xcolor}  % Ensure xcolor is loaded with the table option
\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

%added package
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{colortbl}

\usepackage{lipsum} 
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{afterpage}
\usepackage{xspace}

\usepackage{pifont}

% \newcommand{\MethodName}{\textit{Glean}\xspace}
\newcommand{\MethodName}{GLEAN\xspace}
% \newcommand{\MethodName}{D{\small e}{\small LF}G{\small CD}\xspace}
% \newcommand{\MethodName}{D{\small e}{\small LF}GCD\xspace}

\newcommand{\sm}[1]{{\color{orange}~SM: #1}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Active Generalized Category Discovery with Diverse LLM Feedback}
\title{GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced LLM Feedback}

% \author{Henry Peng Zou$^{\diamondsuit}$\thanks{Work done as an intern at Amazon.}, Siffi Singh$^\spadesuit$, Yi Nian$^\spadesuit$, Jianfeng He$^\spadesuit$ \\ \bf Jason Cai$^{\spadesuit}$, Saab Mansour$^\spadesuit$, Hang Su$^\spadesuit$
% \\  $^\spadesuit$AWS AI Labs \quad $^{\diamondsuit}$University of Illinois Chicago \\
% \texttt{pzou3@uic.edu}
% }

\author{
 \textbf{Henry Peng Zou\textsuperscript{1}\thanks{Work done as an intern at Amazon.}},
 \textbf{Siffi Singh\textsuperscript{2}},
 \textbf{Yi Nian\textsuperscript{2}},
 \textbf{Jianfeng He\textsuperscript{2}},
\\
 \textbf{Jason Cai\textsuperscript{2}},
 \textbf{Saab Mansour\textsuperscript{2}},
 \textbf{Hang Su\textsuperscript{2}},
\\
 \textsuperscript{1}University of Illinois Chicago,
 \textsuperscript{2}AWS AI Labs
\\
 \texttt{
   % \textbf{Correspondence:} 
   % \href{mailto:pzou3@uic.edu}
   pzou3@uic.edu
 }
}



\begin{document}
\maketitle
\begin{abstract}
    \textit{Generalized Category Discovery} (GCD) is a practical and challenging open-world task that aims to recognize both known and novel categories in unlabeled data using limited labeled data from known categories. Due to the lack of supervision, previous GCD methods face significant challenges, such as difficulty in rectifying errors for confusing instances, and inability to effectively uncover and leverage the semantic meanings of discovered clusters. Therefore, additional annotations are usually required for real-world applicability. However, human annotation is extremely costly and inefficient. To address these issues, we propose \MethodName
    , a unified framework for generalized category discovery that actively learns from diverse and quality-enhanced LLM feedback. Our approach leverages three different types of LLM feedback to: (1) improve instance-level contrastive features, (2) generate category descriptions, and (3) align uncertain instances with LLM-selected category descriptions. Extensive experiments demonstrate the superior performance of \MethodName over state-of-the-art models across diverse datasets, metrics, and supervision settings. 
    Our code is available at \url{https://github.com/amazon-science/Glean}.
% \url{https://anonymous.4open.science/r/GCD-LLM-0F73}.
% https://github.com/amazon-science/GCDLLMs
\end{abstract}
% align embeddings of uncertain instances with embeddings of corresponding LLM-selected category descriptions.


\section{Introduction}

The success of many deep learning models often heavily depends on some ideal assumptions, such as the availability of large amounts of labeled data, and the closed-world setting where unlabeled data shares the same set of pre-defined categories as labeled data \cite{Zhong_2021_CVPR, zhang-etal-2022-new, an2023generalized, zou-caragea-2023-jointmatch}. However, these assumptions often do not hold in many real-world scenarios. For example, in customer service intent detection, new types of inquiries may emerge over time \cite{tang-etal-2023-rsvp, zhang-etal-2024-discrimination}. Similarly, e-commerce product categorization faces ongoing introduction of novel product types \cite{gong-etal-2023-transferable, zou-etal-2024-implicitave, zou-etal-2024-eiven}. In this work, we try to lift these assumptions by considering a more realistic and challenging setting: \textit{Generalized Category Discovery} (GCD) \cite{vaze2022generalized}.


As illustrated in Figure \ref{fig:gcd}, GCD addresses a scenario where only a portion of the dataset is labeled and only a subset of categories is known. It aims to automatically categorize all unlabeled data, including instances from both known and novel categories, by leveraging information from a limited number of labeled instances \cite{vaze2022generalized, wen2023simgcd}. This task is particularly relevant in realistic dynamic environments where new categories emerge over time, and manually labeling all data is impractical or prohibitively expensive \cite{ma2024active, zhang-etal-2023-clusterllm}.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/gcd.pdf} % example-image-duck
    \caption{\textit{Generalized Category Discovery} aims to automatically categorize unlabeled data by leveraging the information from a limited number of labeled data from known categories, while the unlabeled data may come from both known and novel categories.}
    \label{fig:gcd}
\end{figure}


\input{tables/closely_related_work}


Previous works on GCD \cite{vaze2022generalized, pu2023dynamic} have primarily focused on applying contrastive learning to both labeled and unlabeled data to learn discriminative representations, followed by clustering methods like K-Means to discover both seen and novel categories. However, these approaches face inherent challenges: (1) Due to the lack of supervision, their models struggle to correct errors for confusing instances and categories; (2) Moreover, these methods often fail to uncover and leverage the semantic meanings of discovered clusters effectively. 
Recent works \cite{liang-etal-2024-actively, an-etal-2024-generalized} have begun exploring the use of Large Language Models (LLMs) to provide additional supervision for ambiguous data. However, these approaches incorporate only a \textit{single} type of LLM feedback—i.e., Similar Instance Selection \cite{an-etal-2024-generalized}—in their \textit{model learning} processes and do not account for \textit{feedback quality}. These limitations restrict their ability to effectively refine model predictions, particularly in ambiguous or challenging cases. Other existing GCD methods do not utilize LLM feedback at all. Table \ref{tab:comp_closely_related_work} compares closely related work and ours in terms of LLM feedback diversity and quality. 

% However, these approaches typically limit LLM involvement to instance-level feedback for refining contrastive embeddings, leaving many other potentially effective LLM feedback strategies unexplored in the context of GCD.


To address the aforementioned issues and limitations, we propose \MethodName, a unified framework for generalized category discovery that actively learns from diverse and quality-enhanced LLM feedback. Our approach leverages LLMs in three effective ways: (1) Similar Instance Selection: We use LLMs to identify similar instances among ambiguous data points, refining embeddings through neighborhood contrastive learning \cite{zhong2021neighborhood}. (2) Category Characterization: LLMs generate interpretable names and descriptions for newly discovered categories, making novel categories more accessible and meaningful. (3) Pseudo Category Selection and Alignment: We associate instance embeddings with LLM-selected category descriptions, fostering improved representation learning that considers category-instance relationships.


By incorporating these diverse forms of LLM feedback, \MethodName achieves significant performance improvements over existing methods across multiple benchmark datasets. We also analyze the performance of \MethodName with varying numbers of known categories. Detailed ablation studies and analyses are provided to further understand each component and hyperparameter.




\begin{figure*}[!tbh]
    \centering
    \includegraphics[width=\textwidth]{figures/pipeline_6.pdf}
    \caption{Pipeline of \MethodName. Both labeled and unlabeled data are first forwarded to a text encoder/backbone to extract features for k-means clustering. Then we compute entropy and select instances with high entropy as ambiguous data to obtain LLM feedback for further refinement. Specifically, we query LLM to (1) select similar instances, (2) generate category descriptions and (3) assign pseudo categories to ambiguous data. Lastly, the three diverse feedback types are leveraged for model training via neighborhood contrastive learning and pseudo category alignment. During inference, we only utilize the text encoder and obtain final results via K-Means clustering on the extracted features. Illustration of the three types of LLM feedback with concrete examples is provided in Figure \ref{fig:llm_feedback}.}
    \label{fig:pipeline_illustration}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}


% \subsection{Generalized Category Discovery}

\noindent \textbf{Generalized Category Discovery. }
Generalized Category Discovery (GCD) \cite{vaze2022generalized, wen2023simgcd, an-etal-2024-generalized, liang-etal-2024-actively} is a recently emerged task that addresses a realistic scenario where only limited labeled data is available and new categories may emerge. The goal is to automatically cluster all unlabeled data from both seen and novel categories \cite{bai2023towards, zou-etal-2023-decrisismb}. Pioneering works \cite{vaze2022generalized, pu2023dynamic} employ supervised and unsupervised contrastive learning to obtain discriminative embeddings, followed by clustering methods such as K-Means. \citet{wen2023simgcd, bai2023towards} propose leveraging parametric classifiers and soft pseudo labels to mitigate model bias towards seen categories, thereby enhancing overall performance. Despite these advancements, traditional GCD methods that do not actively leverage additional human or LLM supervision signals \cite{zhang2021discovering, zhang-etal-2022-new, zhou-etal-2023-probabilistic, zhang2023clustering, liang-liao-2023-clusterprompt, Raedt2023IDASID, sung-etal-2023-pre} still face significant challenges, including difficulty in rectifying errors for confusing instances and an inability to leverage the semantic meanings of discovered clusters \cite{ma2024active, an-etal-2024-generalized}. \\


% \subsection{Active Learning and GCD}
\noindent \textbf{Active Learning and GCD. }
Active Learning (AL) \cite{ren2021survey, Ma2024ActiveGC} aims to improve model performance by selecting and annotating a limited number of informative samples. Diverse sample selection strategies have been proposed, including uncertainty-based methods \cite{wang2014new, zhang-etal-2023-clusterllm}, diversity-based methods \cite{sener2017active, Ash2020Deep} and hybrid methods \cite{agarwal2020contextual, huang2010active}. However, annotation in traditional AL is usually expensive and time-consuming \cite{cheng-etal-2023-improving, zhang-etal-2023-clusterllm}. Recent work in GCD, such as ALUP \cite{an-etal-2024-generalized} and Loop \cite{liang-etal-2024-actively}, has started to leverage LLMs feedback as a cost-effective alternative to human annotators to provide additional supervision signal for ambiguous data. However, these approaches utilize only a single type of LLM feedback in their model learning processes or do not account for feedback quality \cite{an-etal-2024-generalized, liang-etal-2024-actively, liang-etal-2024-synergizing}. These limitations restrict their ability to effectively refine model predictions, particularly in ambiguous or challenging cases. To address these issues, we propose a unified framework for GCD that actively learns from diverse and quality-enhanced LLM feedback. To the best of our knowledge, this is the first work to consider both the diversity and quality of LLM feedback in the context of GCD. Table \ref{tab:comp_closely_related_work} provides a detailed comparison of our approach and closely related work in terms of LLM feedback diversity and quality.


% Other existing GCD methods do not utilize LLM feedback at all. 
% but they have primarily focused on instance-level feedback for refining contrastive embeddings. Our work extends this concept by integrating diverse and effective LLM feedback for generalized category discovery.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\MethodName}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Formulation and Overview}
\noindent \textbf{Problem Formulation. }
Formally, assume we have an open-world dataset $\mathcal{D}$, comprising two subsets: a \underline{l}abeled set $\mathcal{D}_l=\{ (x_i, y_i)\}_{i=1}^{N_l}$ contains only known categories, and an \underline{u}nlabeled set $\mathcal{D}_u=\{x_i\}_{i=1}^{N_u} $ contains both known and novel categories. \textit{Generalized Category Discovery} (GCD) aims to accurately categorize all unlabeled data in $D_u$, having access to labels in $\mathcal{D}_l$. The total number of categories $K$ is regarded as a known prior \cite{wen2023simgcd, an-etal-2024-generalized}.\\
% commonly regarded as a known prior, or can be estimated through off-the-shelf methods \cite{han2019learning, vaze2022generalized}. \\

\noindent \textbf{Overview. }
The main pipeline for \MethodName is shown in Figure \ref{fig:pipeline_illustration}. Both labeled data $\mathcal{D}_l=\{ (x_i, y_i)\}_{i=1}^{N_l}$ and unlabeled data $\mathcal{D}_u=\{x_i\}_{i=1}^{N_u}$ are first forwarded to a backbone encoder $f(\cdot)$ to extract initial features $h_i = f(x_i)$. These features are then passed through a projection head $g(\cdot)$ to learn contrastive features $z_i = g(h_i)$. We mine ambiguous data and leverage diverse LLM feedback to: (1) refine their contrastive features, (2) generate category descriptions, and (3) align ambiguous instances with LLM-selected category descriptions. During inference, we only utilize the backbone and obtain final results via K-Means clustering on the post-backbone features. Next, we explain each main component and how we acquire and utilize each type of LLM feedback in detail.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/illustration_feedback_10.pdf} % example-image-duck
    \caption{Illustration of three different types of LLM feedback utilized in \MethodName. Illustration of the whole pipeline is provided in Figure \ref{fig:pipeline_illustration}.}
    \label{fig:llm_feedback}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LLM Feedback 1: Similar Instance Selection and Utilization}
\label{sec:feedback1}

Following \cite{zhang-etal-2023-clusterllm, liang-etal-2024-actively, an-etal-2024-generalized}, we mine ambiguous data and potential positive candidates to query LLM for similar instance selection and refine their embeddings to be more contrastive. \\

\label{sec:ambiguous_data_mining}
\noindent \textbf{Ambiguous Data Mining. }
We mine ambiguous instances based on entropy. Specifically, we first extract features from the backbone and perform clustering via K-Means. The soft assignment of each sample $x_i$ to each cluster $k$ is calculated with the commonly used Student's $t$-distribution \cite{xie2016unsupervised} as: 
\begin{equation}
    p_{ik} = \frac{(1 + \|h_i - \mu_k\|^2/\alpha)^{-\frac{\alpha+1}{2}}}{\sum_{k'} (1 + \|h_i - \mu_{k'}\|^2/\alpha)^{-\frac{\alpha+1}{2}}}
\end{equation}
where $h_i$ is the extracted feature, $\mu_k$ is the cluster center from K-Means, $\alpha$ is the degree of freedom in the Student's $t$-distribution. We then compute entropy to measure the uncertainty for each sample: 
\begin{equation}
    \mathcal{H}_i = - \sum_{k=1}^{K} p_{ik} \log p_{ik}
\end{equation}
The data with the highest entropy is regarded as ambiguous data and is used to form the query set:
\begin{equation}
    \mathcal{Q} = \{ x_i | \mathcal{H}_i \in top_v(\mathcal{H}) \}
\end{equation}
where $v$ is a hyperparameter that determines the total amount of samples to query LLMs.\\

\label{sec:similar_instance_selection}
\noindent \textbf{Similar Instance Selection. }
For each ambiguous data, we randomly sample $M$ instances from each of its closest $M$ clusters as candidates. We then query the LLM to choose the instance most similar to the ambiguous data as the \textit{positive} instance for the following neighborhood contrastive learning. The corresponding prompt and examples are provided in Figure \ref{fig:feedback1_prompt_example}. For data other than ambiguous data, we do not query the LLM but randomly sample an instance from its $k$-nearest neighbors and regard it as the \textit{positive} instance.  \\


\noindent  \textbf{Neighborhood Contrastive Learning. } 
Lastly, we finetune our model to refine embeddings to be more contrastive via the neighborhood contrastive learning loss \cite{zhong2021neighborhood}:

\begin{equation}
\mathcal{L}^{\mathrm{ncl}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{e^{ \left(z_i^T z_{p_i} / \tau\right)}}{\sum_{j \neq i} e^{ \left(z_i^T z_j / \tau\right)}}
\end{equation}
where $p_i$ is the LLM-selected \textit{positive} instance for the mined ambiguous data or the sampled positive neighbor for other data, $z_j$ is the embedding of other in-batch data, $\tau$ is the temperature parameter.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LLM Feedback 2: Category Characterization}
\label{sec:feedback2}

Upon identifying the ambiguous data in the previous step, we can query the LLM to obtain pseudo category labels for these samples and use them to enhance model performance. However, since the category labels of newly emerged categories remain unknown, it is infeasible to request LLMs to directly assign the selected data to unknown categories. Thus we propose \textit{\textbf{Category Characterization}} to characterize the novel categories and enable LLM to assign pseudo category labels. Specifically: (1) After obtaining the K-Means clustering results on post-backbone features, we first select top-$k$ samples closest to each cluster centroid as representative data. (2) Then for each cluster, we use its representative data to query LLM to generate a pair of cluster name and description. The corresponding prompt and example are provided in Figure \ref{fig:feedback2_prompt_example}. Comprehensive analyses are conducted to measure the performance of category characterization and the effect of different representative data selection strategies, which are presented in Appendix \ref{appendix:category_characterization}. Notably, using only 10 representatives, we achieve 77\% coverage and 71\% semantic matching score.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LLM Feedback 3: Pseudo Category Selection and Alignment}
\label{sec:feedback3}

Previous LLM-assisted GCD approaches \cite{liang-etal-2024-actively, an-etal-2024-generalized} only leverage instance-level relationship for model optimization. Here we take category-instance relationship into account and leverage category-instance LLM feedback to align ambiguous instances with LLM-selected positive category names \& descriptions.


Specifically, for each selected ambiguous instance, we query the LLM to identify the most similar category name \& description from those generated in the previous category characterization step. This selected category becomes the positive example, while all other unselected categories serve as negative examples. The corresponding prompt and example are provided in 
Figure \ref{fig:feedback3_prompt_example}.
Then we use a contrastive loss to align the embedding of queried ambiguous data with the embedding of the selected positive category name \& description:
\begin{equation}
\mathcal{L}^{align} = -\frac{1}{N} \sum_{i=1}^N \log \frac{e^{\left(z_i^T d_p / \tau \right)}}{\sum_{j \neq i} e^{{\left(z_i, d_j/ \tau \right)}}}
% \mathcal{L}^{align} = -\frac{1}{N} \sum_{i=1}^N \log \frac{e^{{{sim}\left(z_i, d_p\right) / \tau}}}{\sum_{j \neq i} e^{{{sim}\left(z_i, d_j\right) / \tau}}}
\end{equation}
where $d_p$ is the embedding of the selected positive category name \& description, $d_j$ is the embedding of other unselected negative category names \& descriptions, and $sim(\cdot, \cdot)$ represents the cosine similarity function.

As a result, the overall loss that leverages labeled data and LLM feedback is formulated as:
\begin{equation}
\mathcal{L} = \mathcal{L}^{ce} + \mathcal{L}^{ncl} + \lambda \mathcal{L}^{align}
\end{equation}
where $\lambda$ is the weight of the alignment loss. 
Note that during model training, we incorporate the supervised cross-entropy loss $\mathcal{L}^{ce}$ on labeled data to enhance model learning from known categories.


\input{tables/main_table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Setup}

\textbf{Dataset and Metrics.} We evaluate \MethodName on three standard generalized category discovery benchmarks: BANKING \cite{casanueva-etal-2020-efficient}, CLINC \cite{larson-etal-2019-evaluation} and StackOverflow \cite{xu-etal-2015-short}. We use the same training, validation, and testing splits as previous work \cite{liang-etal-2024-actively, an-etal-2024-generalized}. Descriptions, statistics and setup of all used datasets are provided in Appendix \ref{appendix:dataset}. 
Following \cite{Lin2019DiscoveringNI, zhang-etal-2022-new, liang-etal-2024-actively}, we adopt the following three metrics for evaluation: Clustering Accuracy (ACC), Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI). The specific definitions are provided in Appendix \ref{appendix:metrics}. \\


\noindent \textbf{Baselines.} We compare our model with two types of baselines: (1) Recent SOTA GCD method with LLM feedback: Loop \cite{an-etal-2024-generalized}, ALUP \cite{liang-etal-2024-actively}. (2) Various standard GCD approaches without leveraging LLM feedback: GCD \cite{vaze2022generalized}, SimGCD \cite{wen2023simgcd}, DeepAligned \cite{zhang2021discovering}, MTP-CLNN \cite{zhang-etal-2022-new}, ProbNID \cite{zhou-etal-2023-probabilistic}, USNID \cite{zhang2023clustering}, CsePL \cite{liang-liao-2023-clusterprompt}. \textit{gpt-4o-mini} is used as the default LLM to acquire LLM feedback.\\

 % Another recent LLM-assisted GCD method ALUP \cite{liang-etal-2024-actively} is not included in the comparison because their code has not been actually released yet.


\noindent \textbf{Implementation Details.} Following \cite{an-etal-2024-generalized}, we take the BERT-based-uncased model \cite{wolf-etal-2020-transformers} as our base model and pre-train it first on both labeled and unlabeled data from the current dataset using cross-entropy loss and masked language modeling loss \cite{Devlin2019BERTPO}. 
We use the \textit{[CLS]} token as initial text features for clustering. For contrastive learning, we employ a two-layer MLP to project the 768-d initial features into a 128-d space. A complete list of default hyper-parameters is provided in Appendix \ref{appendix:hyperparameters}. To reduce the computing and query cost, we follow the practice of \cite{an-etal-2024-generalized}: mine ambiguous data and update the query set every 5 epochs, and repeat the described approach 5 times.


\vspace{0.5cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main Results}
We summarize our main results under different known category ratios in Table \ref{tab:main_result} and describe our key findings below:
\vspace{0.5cm}

\noindent \textbf{Compare with standard GCD approaches.} It can be observed that our \MethodName consistently achieves stronger performance than standard GCD approaches. For example, on the most challenging BANKING dataset with 25\% known category ratio, \MethodName significantly outperforms CsePL \cite{liang-liao-2023-clusterprompt} by 5.35\%/7.42\%, and SimGCD \cite{wen2023simgcd} by 7.07\%/9.54\% in terms of ACC/ARI. These substantial improvements validate the effectiveness of our overall pipeline of active generalized category discovery from diverse LLM feedback.
\vspace{0.2cm}

\noindent \textbf{Compare with state-of-the-art LLM-based GCD.} \MethodName outperforms the recent state-of-the-art GCD method with LLM feedback, Loop \cite{an-etal-2024-generalized} and ALUP \cite{liang-etal-2024-actively}, across most settings and datasets. This verifies our motivation that multiple diverse feedback can be leveraged to effectively boost GCD model performance. Noteworthy is that, \MethodName can offer remarkable improvements over Loop even with extremely limited known categories: +9.38\% ACC on BANKING and +3.5\% ACC on StackOverflow with 5\% known category ratio. Note that we re-ran the released code for GCD, SimGCD, and Loop to obtain their results, while the results for other baselines were retrieved from \cite{liang-etal-2024-actively}.
\vspace{0.2cm}

\noindent \textbf{Superior Overall Performance.} The performance gain of \MethodName is generally maintained across different known category ratios, different metrics and datasets, showing the robustness of our approach. Furthermore, on the CLINC dataset, \MethodName achieves over 90\% ACC with 25\% KCR, demonstrating its potential for high-accuracy category discovery in these domains.



\input{tables/feedback_quality_investigation}

\input{tables/ablation_component}

\input{tables/variants_llm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation Study and Analysis}



% \subsection{LLM Feedback Quality Investigation}
% \lipsum[1]
\label{sec:llm_feedback_quality_investigation}
\subsection{LLM Feedback Quality Investigation}
In this section, we present our pilot investigation into the quality of diverse LLM feedback and introduce simple strategies to enhance its quality. We conduct experiments on three standard GCD datasets: BANKING \cite{casanueva-etal-2020-efficient}, CLINC \cite{larson-etal-2019-evaluation}, and StackOverflow \cite{xu-etal-2015-short}. Accuracy is used as the evaluation metric, calculated by comparing ground-truth answers with predictions. To better reflect real-world use cases, we evaluate performance on the 300 most challenging or ambiguous samples from each dataset. (Details on ambiguous data mining are provided in Section \ref{sec:ambiguous_data_mining}).

Table \ref{tab:feedback_quality_investigation} summarizes the results of our investigation into two types of LLM feedback quality:
(1) Instance-Instance LLM Feedback: Similar Instance Selection \cite{an-etal-2024-generalized}. For each ambiguous instance, we randomly sample $M$ instances from each of its closest $M$ clusters as candidates and then query the LLM to select the instance most similar to the ambiguous data as the positive instance.
(2) Cluster-Instance LLM Feedback: Category Selection. For each ambiguous instance, we query the LLM to identify the most similar category from a given list of candidate categories. Our findings show that naively prompting the LLM, as done in previous work \cite{liang-etal-2024-actively, an-etal-2024-generalized}, leads to unsatisfactory results, achieving an average accuracy of only 0.487 for similar instance selection. While this is still much higher than random selection or semantic nearest-neighbor selection and can aid model learning, improving feedback quality is more beneficial for model training. To address this, we adopt two simple strategies to enhance LLM feedback quality and mitigate noise. When querying the LLM, we incorporate in-context demonstrations from known categories and ask the LLM not only to provide an answer but also to output its confidence in the response, and we then filter out low-confidence answers. As shown in Table \ref{tab:feedback_quality_investigation}, these strategies substantially improve LLM feedback quality for both types of feedback.


% Findings: 
% (1) Naively prompting LLM, as done in previous work, yields unsatisfactory results, though still much higher than random and semantic nearest selection.
% (2) LLM feedback quality can be greatly enhanced with in-context
% demonstrations and filtering. 
% (3) Hardest/Ambiguous samples are much more challenging for LLM to get accurate answers than random samples. 



\begin{figure*}[thb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/num_query_samples.pdf} % example-image-duck
    \caption{Influence of the number of query samples. Increasing the number of query samples generally leads to better performance, especially on the CLINC dataset. The performance gain starts to saturate on the BANKING dataset as the number of query samples reaches 500, we hypothesize that this is because the BANKING dataset is more challenging and distinguishing ambiguous samples and categories becomes increasingly difficult.}
    \label{fig:num_query_samples}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Effectiveness of Each Component}

To show the effectiveness of each component in \MethodName, we measure the performance after removing different parts across the three benchmarking datasets with 25\% known category ratio in Table \ref{tab:ablation_component}. We observe that the performance decreases after stripping each component, suggesting that all components in \MethodName contribute to the final performance. The performance of \MethodName drops most significantly after removing Neighborhood Contrastive Learning across all three datasets. This justifies the importance of leveraging LLMs to identify similar instances among ambiguous data points and refining embeddings through neighborhood contrastive learning. Besides, it can be seen that in-context demonstration and filtering low-confidence LLM feedback indeed help boost model performance, which aligns with our investigation and findings in Section \ref{sec:llm_feedback_quality_investigation}.

Interestingly, compared to the Instance-Instance LLM Feedback - Similar Instance Selection, model performance drops more after removing Cluster-Instance LLM Feedback on most datasets and metrics. Specifically, removing Cluster-Instance LLM Feedback results in a 3.23\% decrease in ACC, while removing Instance-Instance LLM Feedback leads to a 2.33\% decrease. This finding underscores the benefits of aligning instance embeddings with corresponding LLM-selected category descriptions, which fosters improved representation learning by considering category-instance relationships. Not surprisingly, the removal of Cross-Entropy Loss also leads to a noticeable performance drop, particularly on the BANKING and StackOverflow datasets. This validates the importance of supervised learning on labeled data to enhance model performance on known categories and establish a strong foundation for subsequent LLM-enhanced learning processes. These results collectively demonstrate that each component of \MethodName plays a crucial role in its overall performance.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Different Variants of LLMs}
% to be updated
We investigate the performance impact of using different variants of LLMs in Table \ref{tab:variants_llm}. Note that the pricing for GPT-series models is sourced from OpenAI\footnote{https://platform.openai.com/docs/pricing}, while the pricing for all open-source models is sourced from Together.AI\footnote{https://www.together.ai/pricing}. It can be observed that DeepSeek-V3 achieves the highest overall average performance (81.75\%), closely followed by the closed-source gpt-4o-mini (81.45\%) and gpt-4o (81.05\%), demonstrating that open-source models can compete with proprietary alternatives. 
Notably, gpt-4o-mini offers the best balance between cost and effectiveness, delivering competitive performance at a significantly lower cost (\$0.15 / \$0.6). In dataset-specific trends, DeepSeek-V3 excels in CLINC with the highest accuracy (91.91\%), while gpt-4o-mini leads in BANKING (76.14\% ACC), and both DeepSeek-V3 and gpt-4o-mini perform best on Stackoverflow (83.40\% ACC). These results suggest that while OpenAI's closed-source models remain strong, open-source alternatives like DeepSeek-V3 are closing the gap and providing viable, high-performing options.


% It can observed that the GPT-4 series generally outperforms GPT-3.5-turbo across all datasets. Notably, GPT-4o-mini emerges as a cost-effective option, achieving competitive or superior performance compared to more expensive variants. For instance, on the BANKING dataset, GPT-4o-mini achieves the highest accuracy (68.34\%) and NMI (82.48\%), surpassing even GPT-4-turbo. Similarly, on the Stackoverflow dataset, it achieves the best performance across all metrics. While GPT-4-turbo shows slight advantages in some cases, particularly on the CLINC dataset, the performance gains are marginal considering its significantly higher cost. These findings suggest that GPT-4o-mini offers a strong balance between cost-efficiency and performance for \MethodName, making it a compelling choice for practical applications.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Influence of Query Sample Number}
We study the influence of the number of query samples in Figure \ref{fig:num_query_samples}. We can observe that increasing the number of query samples generally leads to better performance, as more LLM feedback signals are available. For instance, on the  CLINC dataset, the ARI increases from 77.5\% to 85.0\% as the number of query samples increases from 0 to 2000, and the trend shows that more performance gain can be obtained with more query samples. Yet, the performance gain starts to saturate on the BANKING dataset as the number of query samples reaches 500, we hypothesize this is because the BANKING dataset is more challenging and distinguishing ambiguous samples and categories becomes increasingly difficult as the number of samples increases.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
% In this paper, we propose \MethodName, a holistic framework that leverages diverse and effective LLM feedback for generalized category discovery. Motivated by the observation that existing methods suffer from the lack of supervision, self-correct mechanisms for ambiguous data and leveraging semantic meanings of discovered categories, we integrate instance-level and cluster-level LLM feedback into a contrastive learning framework and reveal the semantic meaning of discovered categories via our proposed LLM-based category characterization. Extensive experiments on three real-world datasets demonstrate the superiority of \MethodName over existing methods, achieving better performance in different supervision setups. We provided comprehensive ablation studies and analyses to understand each component in our framework.

This paper introduces \MethodName, a holistic framework that leverages diverse and quality-enhanced LLM feedback for generalized category discovery. Our approach addresses key limitations of existing methods, including insufficient supervision, lack of self-correction mechanisms for ambiguous data, and underutilization of semantic meanings in discovered categories. We integrate both instance-level and cluster-level LLM feedback into a contrastive learning framework, while aligning the embeddings of ambiguous instances with LLM-generated and selected category descriptions. Using three real-world datasets, we report new state-of-the-art results with  \MethodName over existing methods across various supervision setups. We provide comprehensive ablation studies and analyses to understand each component in our framework.
% Experiments on three real-world datasets demonstrate that \MethodName achieves new state-of-the-art results across various supervision setups. We also conduct comprehensive ablation studies and analyses to examine the contribution of each component in our framework.

% \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Limitations}
Our current framework is designed for textual data, which limits its applicability to other domains. In the future, we plan to extend it to vision and multimodal domains, exploring learning from multimodal large language models. Additionally, when using external LLMs, data privacy and security remain critical concerns that require ongoing vigilance. Leveraging open-source models such as DeepSeek and Llama could help mitigate these risks. Our results show that modern open-source LLMs can now rival proprietary LLMs in terms of both performance and cost and therefore serve as a good alternative.

% Our work has several limitations that we aim to address in future research. 
% First, while the three types of LLM feedback employed in this study have proven effective in enhancing overall model performance, they can be subject to bias and inaccuracies, particularly in challenging cases. We have not yet thoroughly investigated the accuracy of these different types of LLM feedback, nor have we implemented filtering mechanisms to identify and eliminate potentially incorrect feedback. (already addressed)




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix
\label{sec:appendix}

\clearpage


% \begin{figure*}[!thb]
%     \centering
%     \includegraphics[width=\textwidth]{figures/pipeline_11.pdf} % example-image-duck
%     \caption{Pipeline of \MethodName. Both labeled and unlabeled data are first forwarded to a text encoder to extract features for k-means clustering. Then we compute entropy and select instances with high entropy as ambiguous data to obtain LLM feedback for further refinement. Specifically, we query LLM to select similar instances, generate category descriptions and assign pseudo categories to ambiguous data. Lastly, the three diverse feedback types are leveraged for model training via neighborhood contrastive learning and pseudo category alignment. During inference, we only utilize the text encoder and obtain final results via K-Means clustering on the extracted features.}
%     \label{fig:pipeline_illustration}
% \end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Datasets}
\label{appendix:dataset}
In this section, we provide descriptions, statistics and setup for all evaluated datasets. \\

\noindent \textbf{Dataset Descriptions \& 
 Statistics } BANKING \cite{casanueva-etal-2020-efficient} is a dataset of online banking queries labeled with 77 fine-grained customer intents, such as card arrival, card payment not recognized, pending cash withdrawal and pending card payment. StackOverflow \cite{xu-etal-2015-short} consists of technical questions covering a wide range of programming languages, frameworks, and software tools, and annotated with labels such as MATLAB, WordPress, Apache and Bash. CLINC \cite{larson-etal-2019-evaluation} is a popular dataset for multi-domain intent detection. It contains 150 labels and covers diverse domains such as travel, utility and work. The statistics of these datasets are summarized in Table \ref{tab:dataset_statistics}.\\

\noindent \textbf{Dataset Setup } Following \cite{liang-etal-2024-actively}, we randomly select a specified ratio \{5\%, 10\%, 25\%, 50\%\} of all categories as known categories, denoted as known category ratio (KCR). For each known category, 10\% of the data is selected to form the labeled dataset $\mathcal{D}_l$, while the remaining samples constitute the unlabeled dataset $\mathcal{D}_u$. Our ablation studies in the main text use a default KCR of 25\%, while those in the appendix use a default KCR of 10\%.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!tbh]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Dataset} & \textbf{Domain} & \textbf{\# Categoreis} & \textbf{\# Samples} \\ \midrule
BANKING & Banking & 77 & 13,083 \\
CLINC & Multi-Domain & 150 & 22,500 \\
StackOverflow & Programming & 20 & 22,000 \\ \bottomrule
\end{tabular}%
}
\caption{Dataset statistics.}
\label{tab:dataset_statistics}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation Metrics}
\label{appendix:metrics}


We utilize three standard evaluation metrics to evaluate the GCD performance: ACC, ARI, and NMI. 
The accuracy metric (ACC) evaluates how well the predicted cluster assignments align with the ground-truth labels, and is formulated as:
$$
ACC = \frac{\sum_{i=1}^N \mathbbm{1}_{y_i=map(\hat{y}_i)}}{N}
$$
where $\hat{y}_i$ represents the predicted label and $y_i$ denotes the ground-truth label for each sample $x_i$. $map(\cdot)$ is a mapping function that uses the Hungarian algorithm \cite{kuhn1955hungarian} to establish a one-to-one correspondence between predicted labels $\hat{y}_i$ and their ground-truth counterparts $y_i$.

The Adjusted Rand Index (ARI) evaluates clustering quality by examining pairwise relationships between predicted and ground-truth cluster assignments. ARI can be calculated as:
$$
\textstyle ARI = \frac{\sum_{i,j} \binom{n_{ij}}{2} - [\sum_i \binom{u_i}{2} \sum_j \binom{v_j}{2}]/\binom{N}{2}}{\frac{1}{2}[\sum_i \binom{u_i}{2} + \sum_j \binom{v_j}{2}] - [\sum_i \binom{u_i}{2} \sum_j \binom{v_j}{2}]/\binom{N}{2}}
$$
where $u_i = \sum_j n_{i,j}$ and $v_j = \sum_i n_{i,j}$, $N$ represents the total sample count, and $n_{i,j}$ is the number of samples simultaneously belonging to the $i^{th}$ predicted cluster and $j^{th}$ ground-truth cluster.

The Normalized Mutual Information (NMI) metric measures the consistency between predicted and ground-truth clustering results by quantifying their mutual information. NMI is defined as:
$$
NMI(\hat{y}, y) = \frac{2 \cdot I(\hat{y}, y)}{H(\hat{y}) + H(y)}
$$
where $\hat{y}$ and $y$ represent the predicted and ground-truth label sets respectively. The mutual information between these sets is denoted by $I(\hat{y}, y)$, while $H(\cdot)$ represents the entropy function.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hyperparameters}
\label{appendix:hyperparameters}

A complete list of default hyperparameters on all evaluated datasets is provided in Table \ref{tab:hyperparameter}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!tbh]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c|ccc@{}}
\toprule
\multicolumn{1}{l|}{} & \multicolumn{1}{c|}{\textbf{BANKING}} & \multicolumn{1}{c|}{\textbf{CLINC}} & \textbf{StackOverflow} \\ \midrule
\# Query Samples $v$ & \multicolumn{3}{c}{500} \\ \midrule
Similar Instance Selection Options $M$ & \multicolumn{3}{c}{5} \\ \midrule
\multicolumn{1}{l|}{\# Representatives for Category Characterization} & \multicolumn{3}{c}{10} \\ \midrule
Pseudo Category Selection Candidate Ratio & \multicolumn{3}{c}{0.5} \\ \midrule
Batch Size & \multicolumn{3}{c}{80} \\ \midrule
Learning Rate & \multicolumn{3}{c}{1e-05} \\ \midrule
\# Training Epochs & \multicolumn{3}{c}{25} \\ \midrule
Query Set Update Epoch Interval & \multicolumn{3}{c}{5} \\ \midrule
Alignment Wieght $\lambda$ & \multicolumn{3}{c}{\{0.05, 0.1\}} \\ \midrule
Degree of Freedom $\alpha$ & \multicolumn{3}{c}{1} \\ \midrule
Temperature $\tau$ & \multicolumn{3}{c}{0.07} \\ \midrule
$k$-Nearest Neighbors $k$ & 50 & 50 & 500 \\ \bottomrule
\end{tabular}%
}
\caption{Complete list of default hyperparameters on BANKING, CLINC, StackOverflow datasets.}
\label{tab:hyperparameter}
\end{table}



% \begin{figure*}[!thb]
%     \centering
%     \includegraphics[width=\textwidth]{figures/pipeline_11.pdf} % example-image-duck
%     \caption{Pipeline of \MethodName. Labeled data and unlabeled data are first forwarded to a backbone to extract initial features and passed through a projector to learn contrastive features. Meanwhile, ambiguous data are mined to obtain LLM feedback. Diverse types of LLM feedback are leveraged to (1) refine contrastive features, (2) generate category descriptions,  and (3) align instances with corresponding category descriptions. During inference, we only utilize the backbone and obtain final results via K-Means clustering on the post-backbone features.}
%     \label{fig:pipeline}
% \end{figure*} 


% \begin{figure}[!thb]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/illustration_feedback_10.pdf} % example-image-duck
%     \caption{Illustration of three different types of LLM feedback utilized in \MethodName. Illustration of the whole pipeline is provided in Figure \ref{fig:pipeline_illustration}.}
%     \label{fig:llm_feedback}
% \end{figure}




\section{Additional Ablation Study Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Different Variants of Prompts}
To investigate the impact of different prompt components on the performance of \MethodName, we conducted experiments with various prompt variants of Category Characterization, as shown in Table \ref{tab:variants_of_prompts}. Notably, we also add demonstrations of some known category names in the full prompt. Our analysis reveals several key findings: (1) The full \MethodName prompt, which includes demonstrations, category names and descriptions, consistently achieves the best performance across all metrics on both the CLINC and Stackoverflow datasets. (2) Removing demonstrations from the prompt leads to a noticeable decrease in performance, particularly in ACC and ARI metrics. This suggests that providing examples helps the LLM better understand the task and generate more accurate category characterizations. (3) Omitting the category name generation results in the most significant performance drop on the CLINC dataset, indicating that concise category labels are particularly important for this dataset. (4) For the Stackoverflow dataset, removing the category description has a smaller impact compared to removing names or demonstrations, suggesting that category names might be more crucial for this technical domain. These results underscore the importance of carefully designed prompts in leveraging LLMs for generalized category discovery. The combination of demonstrations, category names, and descriptions in our prompts contributes to the overall effectiveness of \MethodName across different datasets.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!tbh]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcccccc@{}}
\toprule
 & \multicolumn{3}{c}{\textbf{CLINC}} & \multicolumn{3}{c}{\textbf{Stackoverflow}} \\ \midrule
\multicolumn{1}{l|}{Prompt Variants} & ACC & ARI & \multicolumn{1}{c|}{NMI} & ACC & ARI & NMI \\ \midrule
\multicolumn{1}{l|}{\MethodName} & \textbf{87.69} & \textbf{95.02} & \multicolumn{1}{c|}{\textbf{82.27}} & \textbf{82.40} & \textbf{79.67} & 62.81 \\
\multicolumn{1}{l|}{w.o. name} & 85.78 & 93.99 & \multicolumn{1}{c|}{79.65} & 79.50 & 76.66 & 61.97 \\
\multicolumn{1}{l|}{w.o. description} & 86.13 & 94.14 & \multicolumn{1}{c|}{79.68} & 81.00 & 76.88 & \textbf{63.22} \\ 
\multicolumn{1}{l|}{w.o. demonstration} & 86.22 & 94.55 & \multicolumn{1}{c|}{80.82} & 79.40 & 76.63 & 61.76 \\
\bottomrule
\end{tabular}%
}
\caption{Different variants of prompts in LLM Category Characterization. Full prompt with demonstrations, category names, and descriptions achieves the best results.}
\label{tab:variants_of_prompts}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Influence of Alignment Weight}
In Table \ref{tab:alignment_weight}, we show the influence of alignment weight $\lambda$ on model performance. We observe that the choice of alignment weight greatly influences the model's performance across different datasets. For the CLINC dataset, we observe a substantial improvement in performance as the alignment weight increases from 0 to 0.05, with ACC rising by 3.6 percentage points (from 89.64\% to 93.24\%) and ARI improving by 5.62 percentage points (from 83.82\% to 89.44\%). Similarly, for the Stackoverflow dataset, we see notable gains, particularly in ARI, which increases by 4.12 percentage points (from 74.20\% to 78.32\%) when the alignment weight is set to 0.05.

Nevertheless, setting the alignment weight as 0.05, 0.1 can generally offer us good performance improvements. Beyond this range, the performance boost tends to decline, with a particularly sharp drop observed when the weight is set to 1. This suggests that while the alignment between instance embeddings and category descriptions is crucial for improving model performance, it needs to be carefully balanced with other learning objectives. Too much emphasis on alignment (e.g., weight of 1) can lead to overfitting to the LLM-generated descriptions, potentially at the expense of other important features learned from the data.



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!tbh]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ccccccc@{}}
\toprule
 & \multicolumn{3}{c}{\textbf{CLINC}} & \multicolumn{3}{c}{\textbf{Stackoverflow}} \\ \midrule
\multicolumn{1}{c|}{Alignment Weight} & ACC & NMI & \multicolumn{1}{c|}{ARI} & ACC & NMI & ARI \\ \midrule
\multicolumn{1}{c|}{0} & 89.64 & 95.46 & \multicolumn{1}{c|}{83.82} & 86.90 & 81.59 & 74.20 \\
\multicolumn{1}{c|}{0.001} & 91.24 & 95.89 & \multicolumn{1}{c|}{85.84} & 87.30 & 82.18 & 74.79 \\
\multicolumn{1}{c|}{0.05} & \textbf{93.24} & \textbf{96.90} & \multicolumn{1}{c|}{\textbf{89.44}} & \textbf{89.00} & \textbf{84.43} & \textbf{78.32} \\
\multicolumn{1}{c|}{0.1} & 92.93 & 96.87 & \multicolumn{1}{c|}{89.27} & 88.00 & 83.02 & 76.54 \\
\multicolumn{1}{c|}{0.5} & 91.07 & 96.63 & \multicolumn{1}{c|}{87.43} & 86.60 & 81.45 & 74.00 \\
\multicolumn{1}{c|}{1} & 82.49 & 93.68 & \multicolumn{1}{c|}{76.74} & 87.20 & 82.30 & 74.67 \\ \bottomrule
\end{tabular}%
}
\caption{Influence of alignment weight with 50\% known category ratio. Setting the alignment weight as 0.05, 0.1 can generally offer us good performance improvements.}
\label{tab:alignment_weight}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Effect of Candidate Ratio}
We now analyze the effect of candidate ratio in Cluster-Instance LLM Feedback in Table \ref{tab:candidate_ratio}. Candidate ratio refers to the number of candidate categories provided for pseudo category selection over the total number of categories in the dataset. We observe that including more candidates can generally improve the model performance on the BANKING dataset as a small number of candidates may potentially omit the most relevant category. However, the performance gets slightly decreased on the CLINC dataset when increasing the ratio from 0.75 to 1, we hypothesize this is because CLINC contains a much larger number of clusters (150) and incorporating all of their descriptions results in a huge prompt length, leading to decreased LLM pseudo category selection performance as the LLM may be overwhelmed by the large number of options.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!tbh]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ccccccc@{}}
\toprule
 & \multicolumn{3}{c}{\textbf{BANKING}} & \multicolumn{3}{c}{\textbf{CLINC}} \\ \midrule
\multicolumn{1}{c|}{Candidate Ratio} & ACC & NMI & \multicolumn{1}{c|}{ARI} & ACC & NMI & ARI \\ \midrule
\multicolumn{1}{c|}{0.1} & 67.56 & 81.51 & \multicolumn{1}{c|}{55.77} & 84.49 & 94.01 & 79.33 \\
\multicolumn{1}{c|}{0.25} & 67.56 & 82.24 & \multicolumn{1}{c|}{57.33} & 87.87 & 94.98 & 82.51 \\
\multicolumn{1}{c|}{0.5} & 67.60 & 82.40 & \multicolumn{1}{c|}{58.38} & 87.42 & 94.30 & 80.55 \\
\multicolumn{1}{c|}{0.75} & 68.02 & 82.57 & \multicolumn{1}{c|}{58.16} & 87.78 & 94.32 & 81.25 \\
\multicolumn{1}{c|}{1} & 69.29 & 82.49 & \multicolumn{1}{c|}{58.37} & 86.80 & 94.51 & 81.13 \\ \bottomrule
\end{tabular}%
}
\caption{Effect of candidate ratio in Cluster-Instance LLM Feedback. Candidate Ratio: the number of candidate categories provided for pseudo category selection over the total number of categories in the dataset.}
\label{tab:candidate_ratio}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analyses on Category Characterization}
\label{appendix:category_characterization}

This section provides comprehensive analyses to measure the performance of category characterization, and the effect of different representative data selection strategies. \\

\noindent \textbf{Evaluation Metrics. } We evaluate the performance of category characterization with the following four metrics, each of which aims to answer different questions or aspects of cluster interpretation: (1) Coverage Score: Do the interpreted/characterized clusters cover all ground-truth categories? (2) Uniformity Score: How evenly do the interpreted clusters cover all ground-truth categories if not covering all ground-truth categories? (3) SeMatching Score: How well does the generated category name \& description match the ground-truth ground-truth categories in terms of semantics similarity?  (4) Informative Score: An overall metric that considers both semantic similarity and uniformity between the interpreted clusters and ground-truth categories. The specific implementation of these metrics is provided in Figure \ref{fig:cateogy_characterization_evaluation}. \\

\noindent \textbf{The effect of different sampling strategies. } We investigate three different sampling strategies for category characterization: (i) \textit{Random}: randomly select $n$ instances from each cluster as representatives; (ii) \textit{Nearest to Center}: For each cluster, select the $n$ instances that are nearest to the K-means cluster center as representatives; (iii) \textit{Sub-KMeans Centroids}: For each cluster, we first perform another K-means clustering and produce $n$ sub-level cluster centroids, which as used as representatives of the original cluster. Table \ref{tab:diff_sampling_strategies} summarizes the evaluation results with these sampling strategies and different numbers of representative samples. Not surprisingly, both \textit{Nearest to Center} and \textit{Sub-KMeans Centroids} sampling strategies perform much better than \textit{Random} sampling. Generally speaking, \textit{Nearest to Center} can achieve slightly better performance than \textit{Sub-KMeans Centroids}. We hypothesize the reason for this is that compared to \textit{Sub-KMeans Centroids}, instances closest to the cluster center are most representative of the cluster and have more coherent semantic meaning, which makes it easier for LLM to produce more accurate and non-overlapping cluster summarization. Furthermore, we observe that including more representatives improves both Coverage and Uniformity Scores, while slightly decreasing the SeMatching Score. This is because having more representatives helps generate unique summarization, but also introduces more noise to make it hard to produce semantically accurate and consistent category descriptions. \\


\noindent \textbf{Performance on different label settings. } Table \ref{tab:diff_cat_label_ratio} summarizes the evaluation results with varying known category ratios and different numbers of labeled data for the \textit{Nearest to Center} sampling strategy. It can be observed that increasing the known category ratio leads to consistently better performance, as more demonstrations of known category names can be provided in the prompt and thus LLM can generate more unique and accurate category names and descriptions. Besides, we can see that as more labeled data is added to known categories, most evaluation scores tend to first increase and then saturate. The SeMatching Score stays roughly the same, indicating that a few number of instances nearest to cluster centers is sufficient to produce semantically similar category names and descriptions with the ground-truth ones.\\


\noindent \textbf{Performance with different LLMs. } We now analyze the impact and costs of using different LLMs, or more specifically, different GPT models.  Table \ref{tab:diff_llms} demonstrates the results. Interestingly, more expensive models, such as \textit{gpt-4} and \textit{gpt-4-turbo}, do necessarily not perform better than cheaper models, such as \textit{gpt-3.5-turbo} and \textit{gpt-4o-mini}, in category characterization. Besides, we can observe that \textit{gpt-4o-mini} performs worse than \textit{gpt-4o} with 1, 10 representatives in Coverage, Uniformity and Informative Scores, but achieves better SeMatching Score and on-par overall performance with 100 representatives. Furthermore, while being 100\texttt{\string~}200 times cheaper than \textit{gpt-4}, \textit{gpt-4o-mini} achieves on-par or better performances in most evaluated scores and settings. The competitive performance and extremely low price of \textit{gpt-4o-mini} render it a good choice to be considered for category characterization. \\


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table*}[!tbh]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
\multicolumn{1}{c}{\textbf{Sampling Strategy}} & \textbf{\# Representatives} & \textbf{Coverage} & \textbf{Uniformity} & \textbf{SeMatching} & \textbf{Informative} \\ \midrule
 & 1 & 0.19 & 0.54 & 0.70 & 0.38 \\
 & 3 & \textbf{0.26} & \textbf{0.60} & 0.68 & \textbf{0.41} \\
 & 5 & 0.21 & 0.50 & \textbf{0.71} & 0.35 \\
\multicolumn{1}{c}{\textbf{Random}} & 10 & 0.23 & 0.56 & 0.67 & 0.37 \\
 & 20 & 0.25 & 0.56 & 0.65 & 0.37 \\
 & 50 & 0.23 & 0.54 & 0.65 & 0.35 \\
 & 100 & 0.23 & 0.52 & 0.64 & 0.33 \\ \midrule
 & 1 & 0.47 & 0.71 & \textbf{0.70} & 0.50 \\
 & 3 & 0.62 & 0.81 & 0.69 & 0.56 \\
 & 5 & 0.58 & 0.79 & \textbf{0.70} & 0.55 \\
\multicolumn{1}{c}{\textbf{Nearest to Center}} & 10 & 0.58 & 0.81 & \textbf{0.70} & 0.56 \\
 & 20 & 0.61 & 0.82 & 0.69 & 0.56 \\
 & 50 & 0.57 & 0.82 & 0.68 & 0.56 \\
 & 100 & \textbf{0.65} & \textbf{0.87} & 0.68 & \textbf{0.59} \\ \midrule
 & 1 & 0.45 & 0.71 & \textbf{0.70} & 0.49 \\
 & 3 & 0.55 & 0.79 & 0.66 & 0.52 \\
 & 5 & 0.62 & 0.82 & 0.66 & 0.54 \\
\multicolumn{1}{c}{\textbf{Sub-KMeans Centroids}} & 10 & 0.57 & 0.81 & 0.67 & 0.54 \\
 & 20 & 0.60 & 0.84 & 0.66 & 0.55 \\
 & 50 & 0.60 & 0.84 & 0.66 & 0.55 \\
 & 100 & \textbf{0.65} & \textbf{0.87} & 0.65 & \textbf{0.56} \\ \bottomrule
\end{tabular}%
}
\caption{Effect of different sampling strategies and number of representative samples.}
\label{tab:diff_sampling_strategies}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table*}[!tbh]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
\multicolumn{1}{c}{\textbf{Known Category Ratio}} & \textbf{Labeled Shot} & \textbf{Coverage} & \textbf{Uniformity} & \textbf{SeMatching} & \textbf{Informative} \\ \midrule
 & 5 & 0.45 & 0.75 & 0.63 & 0.47 \\
 & 10 & 0.58 & 0.80 & \textbf{0.70} & 0.56 \\
\multicolumn{1}{c}{0.1} & 20 & 0.62 & 0.83 & 0.69 & 0.57 \\
 & 50 & \textbf{0.68} & \textbf{0.87} & 0.69 & \textbf{0.60} \\
 & 100 & 0.64 & 0.86 & \textbf{0.67} & 0.58 \\ \midrule
 & 5 & 0.52 & 0.79 & 0.70 & 0.56 \\
 & 10 & 0.66 & 0.87 & 0.69 & 0.60 \\
\multicolumn{1}{c}{0.25} & 20 & \textbf{0.70} & \textbf{0.89} & 0.70 & \textbf{0.62} \\
 & 50 & 0.69 & 0.88 & 0.70 & \textbf{0.62} \\
 & 100 & 0.66 & 0.87 & \textbf{0.71} & 0.61 \\ \midrule
 & 5 & 0.73 & 0.90 & 0.71 & 0.64 \\
 & 10 & 0.77 & 0.92 & 0.71 & 0.66 \\
\multicolumn{1}{c}{0.5} & 20 & 0.79 & 0.92 & 0.71 & 0.66 \\
 & 50 & \textbf{0.81} & \textbf{0.93} & \textbf{0.72} & \textbf{0.67} \\
 & 100 & 0.78 & 0.92 & 0.71 & 0.66 \\ \bottomrule
\end{tabular}%
}
\caption{Performance on different known category ratios and different numbers of labeled data.}
\label{tab:diff_cat_label_ratio}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table*}[!tbh]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccc@{}}
\toprule
\multicolumn{1}{c}{\textbf{\# Representatives}} & \textbf{LLMs} & \textbf{Cost} & \textbf{Coverage} & \textbf{Uniformity} & \textbf{SeMatching} & \textbf{Informative} \\ \midrule
 & gpt-3.5-turbo & \$1/ \$2 & 0.47 & 0.75 & \textbf{0.70} & 0.52 \\
 & gpt-4o-mini & \$0.15 / \$0.6 & 0.48 & 0.72 & \textbf{0.70} & 0.50 \\
\multicolumn{1}{c}{1} & gpt-4o & \$5 / \$15 & \textbf{0.61} & \textbf{0.86} & 0.66 & \textbf{0.57} \\
 & gpt-4-turbo & \$10 / \$30 & 0.45 & 0.74 & 0.65 & 0.48 \\
 & gpt-4 & \$30 / \$60 & 0.45 & 0.71 & \textbf{0.70} & 0.49 \\ \midrule
 & gpt-3.5-turbo & \$1/ \$2 & 0.64 & 0.86 & 0.65 & 0.55 \\
 & gpt-4o-mini & \$0.15 / \$0.6 & 0.58 & 0.80 & \textbf{0.70} & 0.56 \\
\multicolumn{1}{c}{10} & gpt-4o & \$5 / \$15 & \textbf{0.66} & \textbf{0.88} & 0.67 & \textbf{0.59} \\
 & gpt-4-turbo & \$10 / \$30 & 0.62 & 0.86 & 0.64 & 0.55 \\
 & gpt-4 & \$30 / \$60 & 0.65 & 0.86 & 0.65 & 0.56 \\ \midrule
 & gpt-3.5-turbo & \$1/ \$2 & 0.58 & 0.84 & 0.64 & 0.53 \\
 & gpt-4o-mini & \$0.15 / \$0.6 & \textbf{0.66} & \textbf{0.87} & \textbf{0.67} & \textbf{0.58} \\
\multicolumn{1}{c}{100} & gpt-4o & \$5 / \$15 & 0.61 & 0.86 & 0.66 & 0.56 \\
 & gpt-4-turbo & \$10 / \$30 & 0.62 & 0.86 & 0.60 & 0.52 \\
 & gpt-4 & \$30 / \$60 & 0.64 & \textbf{0.87} & 0.61 & 0.53 \\ \bottomrule
\end{tabular}%
}
\caption{Performance with different LLMs.}
\label{tab:diff_llms}
\end{table*}


\begin{figure*}[!tbh]
    \centering
    \includegraphics[width=\textwidth]{figures/cateogy_characterization_evaluation.png} % example-image-duck
    \caption{Implementation of the four evaluation metrics for category characterization: Coverage Score, Uniformity Score, SeMatching Score and Informative Score.}
    \label{fig:cateogy_characterization_evaluation}
\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{LLM Feedback Prompts \& Examples}
\label{appendix:prompts}

This section provides all the prompts we used to acquire the three diverse LLM feedback and corresponding examples: (i) Similar Instance Selection - Figure \ref{fig:feedback1_prompt_example}. (ii) Category Characterization - Figure \ref{fig:feedback2_prompt_example}. (iii) Pseudo Category Selection - Figure \ref{fig:feedback3_prompt_example}.


\begin{figure*}[!tbh]
    \centering
    \includegraphics[width=\textwidth]{figures/feedback1_example.pdf} % example-image-duck
    \caption{LLM Feedback 1: Similar Instance Selection Prompt and Example. Click here to return to Section \ref{sec:feedback1}.}
    \label{fig:feedback1_prompt_example}
\end{figure*}


\clearpage

\begin{figure*}[!tbh]
    \centering
    \includegraphics[width=\textwidth]{figures/feedback2_example.pdf} % example-image-duck
    \caption{LLM Feedback 2: Category Characterization Prompt and Example. Click here to return to Section \ref{sec:feedback2}.}
    \label{fig:feedback2_prompt_example}
\end{figure*}

\clearpage

\begin{figure*}[!tbh]
    \centering
    \includegraphics[width=\textwidth]{figures/feedback3_example.pdf} % example-image-duck
    \caption{LLM Feedback 3: Pseudo Category Selection Prompt and Example. Click here to return to Section \ref{sec:feedback3}.}
    \label{fig:feedback3_prompt_example}
\end{figure*}

\clearpage





\end{document}


