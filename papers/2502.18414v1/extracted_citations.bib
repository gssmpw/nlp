@article{Ma2024ActiveGC,
  title={Active Generalized Category Discovery},
  author={Shijie Ma and Fei Zhu and Zhun Zhong and Xu-Yao Zhang and Cheng-Lin Liu},
  journal={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  pages={16890-16900},
  url={https://api.semanticscholar.org/CorpusID:268264086}
}

@article{Raedt2023IDASID,
  title={IDAS: Intent Discovery with Abstractive Summarization},
  author={Maarten De Raedt and Fr{\'e}deric Godin and Thomas Demeester and Chris Develder},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.19783},
  url={https://api.semanticscholar.org/CorpusID:258987814}
}

@inproceedings{agarwal2020contextual,
  title={Contextual diversity for active learning},
  author={Agarwal, Sharat and Arora, Himanshu and Anand, Saket and Arora, Chetan},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
  pages={137--153},
  year={2020},
  organization={Springer}
}

@inproceedings{an-etal-2024-generalized,
    title = "Generalized Category Discovery with Large Language Models in the Loop",
    author = "An, Wenbin  and
      Shi, Wenkai  and
      Tian, Feng  and
      Lin, Haonan  and
      Wang, QianYing  and
      Wu, Yaqiang  and
      Cai, Mingxiang  and
      Wang, Luyan  and
      Chen, Yan  and
      Zhu, Haiping  and
      Chen, Ping",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.512",
    pages = "8653--8665",
    abstract = "Generalized Category Discovery (GCD) is a crucial task that aims to recognize both known and novel categories from a set of unlabeled data by utilizing a few labeled data with only known categories. Due to the lack of supervision and category information, current methods usually perform poorly on novel categories and struggle to reveal semantic meanings of the discovered clusters, which limits their applications in the real world. To mitigate the above issues, we propose Loop, an end-to-end active-learning framework that introduces Large Language Models (LLMs) into the training loop, which can boost model performance and generate category names without relying on any human efforts. Specifically, we first propose Local Inconsistent Sampling (LIS) to select samples that have a higher probability of falling to wrong clusters, based on neighborhood prediction consistency and entropy of cluster assignment probabilities. Then we propose a Scalable Query strategy to allow LLMs to choose true neighbors of the selected samples from multiple candidate samples. Based on the feedback from LLMs, we perform Refined Neighborhood Contrastive Learning (RNCL) to pull samples and their neighbors closer to learn clustering-friendly representations. Finally, we select representative samples from clusters corresponding to novel categories to allow LLMs to generate category names for them. Extensive experiments on three benchmark datasets show that Loop outperforms SOTA models by a large margin and generates accurate category names for the discovered clusters. Code and data are available at https://github.com/Lackel/LOOP.",
}

@inproceedings{cheng-etal-2023-improving,
    title = "Improving Contrastive Learning of Sentence Embeddings from {AI} Feedback",
    author = "Cheng, Qinyuan  and
      Yang, Xiaogui  and
      Sun, Tianxiang  and
      Li, Linyang  and
      Qiu, Xipeng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.707",
    doi = "10.18653/v1/2023.findings-acl.707",
    pages = "11122--11138",
    abstract = "Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings.However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve Contrastive Learning of sentence embeddings from AI Feedback (CLAIF).Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings.Experimental results show that our method achieves state-of-the-art performance on several semantic textual similarity (STS) and transfer learning tasks compared to other unsupervised and supervised contrastive learning methods.",
}

@article{huang2010active,
  title={Active learning by querying informative and representative examples},
  author={Huang, Sheng-Jun and Jin, Rong and Zhou, Zhi-Hua},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

@inproceedings{liang-etal-2024-actively,
    title = "Actively Learn from {LLM}s with Uncertainty Propagation for Generalized Category Discovery",
    author = "Liang, Jinggui  and
      Liao, Lizi  and
      Fei, Hao  and
      Li, Bobo  and
      Jiang, Jing",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.434",
    doi = "10.18653/v1/2024.naacl-long.434",
    pages = "7845--7858",
    abstract = "Generalized category discovery faces a key issue: the lack of supervision for new and unseen data categories. Traditional methods typically combine supervised pretraining with self-supervised learning to create models, and then employ clustering for category identification. However, these approaches tend to become overly tailored to known categories, failing to fully resolve the core issue. Hence, we propose to integrate the feedback from LLMs into an active learning paradigm. Specifically, our method innovatively employs uncertainty propagation to select data samples from high-uncertainty regions, which are then labeled using LLMs through a comparison-based prompting scheme. This not only eases the labeling task but also enhances accuracy in identifying new categories. Additionally, a soft feedback propagation mechanism is introduced to minimize the spread of inaccurate feedback. Experiments on various datasets demonstrate our framework{'}s efficacy and generalizability, significantly improving baseline models at a nominal average cost.",
}

@inproceedings{liang-etal-2024-synergizing,
    title = "Synergizing Large Language Models and Pre-Trained Smaller Models for Conversational Intent Discovery",
    author = "Liang, Jinggui  and
      Liao, Lizi  and
      Fei, Hao  and
      Jiang, Jing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.840/",
    doi = "10.18653/v1/2024.findings-acl.840",
    pages = "14133--14147",
    abstract = "In Conversational Intent Discovery (CID), Small Language Models (SLMs) struggle with overfitting to familiar intents and fail to label newly discovered ones. This issue stems from their limited grasp of semantic nuances and their intrinsically discriminative framework. Therefore, we propose Synergizing Large Language Models (LLMs) with pre-trained SLMs for CID (SynCID). It harnesses the profound semantic comprehension of LLMs alongside the operational agility of SLMs. By utilizing LLMs to refine both utterances and existing intent labels, SynCID significantly enhances the semantic depth, subsequently realigning these enriched descriptors within the SLMs' feature space to correct cluster distortion and promote robust learning of representations. A key advantage is its capacity for the early identification of new intents, a critical aspect for deploying conversational agents successfully. Additionally, SynCID leverages the in-context learning strengths of LLMs to generate labels for new intents. Thorough evaluations across a wide array of datasets have demonstrated its superior performance over traditional CID methods."
}

@inproceedings{liang-liao-2023-clusterprompt,
    title = "{C}luster{P}rompt: Cluster Semantic Enhanced Prompt Learning for New Intent Discovery",
    author = "Liang, Jinggui  and
      Liao, Lizi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.702/",
    doi = "10.18653/v1/2023.findings-emnlp.702",
    pages = "10468--10481",
    abstract = "The discovery of new intent categories from user utterances is a crucial task in expanding agent skills. The key lies in how to efficiently solicit semantic evidence from utterances and properly transfer knowledge from existing intents to new intents. However, previous methods laid too much emphasis on relations among utterances or clusters for transfer learning, while paying less attention to the usage of semantics. As a result, these methods suffer from in-domain over-fitting and often generate meaningless new intent clusters due to data distortion. In this paper, we present a novel approach called Cluster Semantic Enhanced Prompt Learning (CsePL) for discovering new intents. Our method leverages two-level contrastive learning with label semantic alignment to learn meaningful representations of intent clusters. These learned intent representations are then utilized as soft prompt initializations for discriminating new intents, reducing the dominance of existing intents. Extensive experiments conducted on three public datasets demonstrate the superiority of our proposed method. It not only outperforms existing methods but also suggests meaningful intent labels and enables early detection of new intents."
}

@inproceedings{ma2024active,
  title={Active generalized category discovery},
  author={Ma, Shijie and Zhu, Fei and Zhong, Zhun and Zhang, Xu-Yao and Liu, Cheng-Lin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16890--16900},
  year={2024}
}

@inproceedings{pu2023dynamic,
  title={Dynamic conceptional contrastive learning for generalized category discovery},
  author={Pu, Nan and Zhong, Zhun and Sebe, Nicu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7579--7588},
  year={2023}
}

@article{ren2021survey,
  title={A survey of deep active learning},
  author={Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B and Chen, Xiaojiang and Wang, Xin},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={9},
  pages={1--40},
  year={2021},
  publisher={ACM New York, NY}
}

@article{sener2017active,
  title={Active learning for convolutional neural networks: A core-set approach},
  author={Sener, Ozan and Savarese, Silvio},
  journal={arXiv preprint arXiv:1708.00489},
  year={2017}
}

@inproceedings{sung-etal-2023-pre,
    title = "Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification",
    author = "Sung, Mujeen  and
      Gung, James  and
      Mansimov, Elman  and
      Pappas, Nikolaos  and
      Shu, Raphael  and
      Romeo, Salvatore  and
      Zhang, Yi  and
      Castelli, Vittorio",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.646/",
    doi = "10.18653/v1/2023.emnlp-main.646",
    pages = "10433--10442",
    abstract = "Intent classification (IC) plays an important role in task-oriented dialogue systems. However, IC models often generalize poorly when training without sufficient annotated examples for each user intent. We propose a novel pre-training method for text encoders that uses contrastive learning with intent psuedo-labels to produce embeddings that are well-suited for IC tasks, reducing the need for manual annotations. By applying this pre-training strategy, we also introduce Pre-trained Intent-aware Encoder (PIE), which is designed to align encodings of utterances with their intent names. Specifically, we first train a tagger to identify key phrases within utterances that are crucial for interpreting intents. We then use these extracted phrases to create examples for pre-training a text encoder in a contrastive manner. As a result, our PIE model achieves up to 5.4{\%} and 4.0{\%} higher accuracy than the previous state-of-the-art pre-trained text encoder for the N-way zero- and one-shot settings on four IC datasets."
}

@inproceedings{vaze2022generalized,
  title={Generalized category discovery},
  author={Vaze, Sagar and Han, Kai and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7492--7501},
  year={2022}
}

@inproceedings{wang2014new,
  title={A new active labeling method for deep learning},
  author={Wang, Dan and Shang, Yi},
  booktitle={2014 International joint conference on neural networks (IJCNN)},
  pages={112--119},
  year={2014},
  organization={IEEE}
}

@inproceedings{wen2023simgcd,
    author    = {Wen, Xin and Zhao, Bingchen and Qi, Xiaojuan},
    title     = {Parametric Classification for Generalized Category Discovery: A Baseline Study},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year      = {2023},
    pages     = {16590-16600}
}

@inproceedings{zhang-etal-2022-new,
    title = "New Intent Discovery with Pre-training and Contrastive Learning",
    author = "Zhang, Yuwei  and
      Zhang, Haode  and
      Zhan, Li-Ming  and
      Wu, Xiao-Ming  and
      Lam, Albert",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.21",
    doi = "10.18653/v1/2022.acl-long.21",
    pages = "256--269",
    abstract = "New intent discovery aims to uncover novel intent categories from user utterances to expand the set of supported intent classes. It is a critical task for the development and service expansion of a practical dialogue system. Despite its importance, this problem remains under-explored in the literature. Existing approaches typically rely on a large amount of labeled utterances and employ pseudo-labeling methods for representation learning and clustering, which are label-intensive, inefficient, and inaccurate. In this paper, we provide new solutions to two important research questions for new intent discovery: (1) how to learn semantic utterance representations and (2) how to better cluster utterances. Particularly, we first propose a multi-task pre-training strategy to leverage rich unlabeled data along with external labeled data for representation learning. Then, we design a new contrastive loss to exploit self-supervisory signals in unlabeled data for clustering. Extensive experiments on three intent recognition benchmarks demonstrate the high effectiveness of our proposed method, which outperforms state-of-the-art methods by a large margin in both unsupervised and semi-supervised scenarios. The source code will be available at \url{https://github.com/zhang-yu-wei/MTP-CLNN}.",
}

@inproceedings{zhang-etal-2023-clusterllm,
    title = "{C}luster{LLM}: Large Language Models as a Guide for Text Clustering",
    author = "Zhang, Yuwei  and
      Wang, Zihan  and
      Shang, Jingbo",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.858",
    doi = "10.18653/v1/2023.emnlp-main.858",
    pages = "13903--13920",
    abstract = "We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon {``}small{''} embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user{'}s preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by constructing hard triplet questions $<$does A better correspond to B than C$>$, where A, B and C are similar data points that belong to different clusters according to small embedder. We empirically show that this strategy is both effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on clustering granularity by carefully designed pairwise questions $<$do A and B belong to the same category$>$, and tune the granularity from cluster hierarchies that is the most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that ClusterLLM consistently improves clustering quality, at an average cost of {\textasciitilde}{\$}0.6 per dataset.",
}

@inproceedings{zhang2021discovering,
  title={Discovering new intents with deep aligned clustering},
  author={Zhang, Hanlei and Xu, Hua and Lin, Ting-En and Lyu, Rui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={14365--14373},
  year={2021}
}

@article{zhang2023clustering,
  title={A clustering framework for unsupervised and semi-supervised new intent discovery},
  author={Zhang, Hanlei and Xu, Hua and Wang, Xin and Long, Fei and Gao, Kai},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

@inproceedings{zhou-etal-2023-probabilistic,
    title = "A Probabilistic Framework for Discovering New Intents",
    author = "Zhou, Yunhua  and
      Quan, Guofeng  and
      Qiu, Xipeng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.209/",
    doi = "10.18653/v1/2023.acl-long.209",
    pages = "3771--3784",
    abstract = "Discovering new intents is of great significance for establishing the Task-Oriented Dialogue System. Most existing methods either cannot transfer prior knowledge contained in known intents or fall into the dilemma of forgetting prior knowledge in the follow-up. Furthermore, these methods do not deeply explore the intrinsic structure of unlabeled data, and as a result, cannot seek out the characteristics that define an intent in general. In this paper, starting from the intuition that discovering intents could be beneficial for identifying known intents, we propose a probabilistic framework for discovering intents where intent assignments are treated as latent variables. We adopt the Expectation Maximization framework for optimization. Specifically, In the E-step, we conduct intent discovery and explore the intrinsic structure of unlabeled data by the posterior of intent assignments. In the M-step, we alleviate the forgetting of prior knowledge transferred from known intents by optimizing the discrimination of labeled data. Extensive experiments conducted on three challenging real-world datasets demonstrate the generality and effectiveness of the proposed framework and implementation."
}

@inproceedings{zou-etal-2023-decrisismb,
    title = "{D}e{C}risis{MB}: Debiased Semi-Supervised Learning for Crisis Tweet Classification via Memory Bank",
    author = "Zou, Henry  and
      Zhou, Yue  and
      Zhang, Weizhi  and
      Caragea, Cornelia",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.406/",
    doi = "10.18653/v1/2023.findings-emnlp.406",
    pages = "6104--6115",
    abstract = "During crisis events, people often use social media platforms such as Twitter to disseminate information about the situation, warnings, advice, and support. Emergency relief organizations leverage such information to acquire timely crisis circumstances and expedite rescue operations. While existing works utilize such information to build models for crisis event analysis, fully-supervised approaches require annotating vast amounts of data and are impractical due to limited response time. On the other hand, semi-supervised models can be biased, performing moderately well for certain classes while performing extremely poorly for others, resulting in substantially negative effects on disaster monitoring and rescue. In this paper, we first study two recent debiasing methods on semi-supervised crisis tweet classification. Then we propose a simple but effective debiasing method, DeCrisisMB, that utilizes a Memory Bank to store and perform equal sampling for generated pseudo-labels from each class at each training iteration. Extensive experiments are conducted to compare different debiasing methods' performance and generalization ability in both in-distribution and out-of-distribution settings. The results demonstrate the superior performance of our proposed method. Our code is available at https://github.com/HenryPengZou/DeCrisisMB."
}

