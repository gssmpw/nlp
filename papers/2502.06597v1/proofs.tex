\section{Proofs of the Main Theorems}\label{sec:proofs}
In this section, we provide proofs of \Cref{thm:mainprivacy}
and \Cref{thm:main} from \Cref{sec:method} as well as \Cref{thm:Adam_sens} from \Cref{sec:applications}.

\subsection{Proof of Theorem \ref{thm:mainprivacy} and \cref{lem:jointsensitivity}}

The privacy of Algorithm~\ref{alg:JME} follows from the 
properties of matrix mechanism \citep{li2015matrix}, with 
a precise estimate of the \emph{sensitivity} of the joint 
estimation process that we will introduce and discuss later
in this section.

By means of the matrix factorization mechanism with $A_1=B_1C_1$
and $A_2=B_2C_2$, we write the joint moment estimate as, 
\begin{align}
\begingroup\setlength{\arraycolsep}{2pt} % adjust vertical space in pmatrix
    \begin{pmatrix}Y& \boldsymbol{0}\\ \boldsymbol{0}&S\end{pmatrix}
\endgroup
    &=
\begingroup\setlength{\arraycolsep}{1pt}
\begin{pmatrix}B_1& \boldsymbol{0} \\ \boldsymbol{0} & \lambda^{-\frac{1}{2}} B_2\end{pmatrix}
\endgroup
\begingroup\setlength{\arraycolsep}{1pt}
    \begin{pmatrix}C_1 X& \boldsymbol{0} \\ \boldsymbol{0} & \lambda^{\frac{1}{2}} C_2(X\face X)\end{pmatrix}
\endgroup
\label{eq:jointmoments}
\end{align}
where $\lambda>0$ is an arbitrary trade-off parameter that 
we will adjust optimally later. 
%
To make the estimate private, we privatize the rightmost matrix
in \eqref{eq:jointmoments}, which contains the data, 
by adding suitable scaled Gaussian noise. %
%
The subsequent matrix multiplication then acts on private 
data, so its result is also private. 

It remains to show that the noise level specified in Algorithm~\ref{alg:JME} suffices to guarantee $(\epsilon,\delta)$-privacy.

For this, we denote by $\mathcal{X}=\{(x_1,\dots,x_n)\;:\;\max_i \|x_i\|_2\leq \zeta \wedge x_i \in\mathbb{R}^{d}\}$ 
be the set of \emph{input sequences} where each $d$-dimensional vectors has bounded $\ell_2$ norm.
%
The \emph{sensitivity} of a computation is the maximal 
amount by which the result differs between two input 
sequences $X,X'\in\mathcal{X}$, which are identical 
except for a single data vector at an arbitrary 
index (denoted by $X\sim X'$). 

For \acronym, the relevant sensitivity is the 
one of the matrix we want to privatize, \ie (in the squared form)
%
\begin{align}
    &\operatorname{sens}_{\lambda}^2(C_1, C_2)
    \!=\!\! \sup_{X \sim X'} 
    \left\|
    \begingroup
    \setlength{\arraycolsep}{-1pt} % adjust vertical space in pmatrix
    \begin{pmatrix}
        C_1 & \textbf{0}\\
        \textbf{0} & \sqrt{\lambda} C_2\\
    \end{pmatrix}\endgroup \begin{pmatrix}
        X - X'\\
        X\face X\!-\!X'\face X'
    \end{pmatrix}\right\|_{\Fr}^2   
    \notag\\
    &\quad =\sup_{X \sim X'}\!\!\! \left[\|C_1(X\!-\!X')\|_{\Fr}^2 +  \lambda \|C_2
    (X\face X\!-\!X'\face X')\|_{\Fr}^2\right]
    \label{eq:sens_complete}
\end{align}
%
Due to the linearity of the operations and the condition imposed by $X\sim X'$, 
most terms in \eqref{eq:sens_complete} cancel out and the value 
of $\operatorname{sens}^2_{\lambda}(C_1,C_2)$ simplifies into 
the solution of the following optimization problem.

\begin{problem}[Sensitivity for Joint Moment Estimation]\label{prb:cov_sens}
\begin{equation}
   \max_{i=1, \dots, n} \max_{\substack{\|x\|_2 \le \zeta\\\|y\|_2 \le \zeta}} \alpha^2_i \|x - y\|_2^2 + \lambda \beta^2_i \|x \otimes x - y \otimes y\|_2^2,\label{eq:cov_sens}
\end{equation}
where $\alpha^2_i = \|(C_1)_i\|_2^2$ and $\beta^2_i = \|(C_2)_i\|_2^2$ are the column norms of the matrices $C_1$ and $C_2$, respectively. 
\end{problem}

To study Problem~\ref{prb:cov_sens}, we introduce as an intermediate object the formulation of \eqref{eq:cov_sens} in the special case of $\zeta=1$ and $\alpha_i=\beta_i=1$ for $i=1,\dots,n$, treated as a function of $\lambda$:
\begin{equation}\label{def:r_d_appendix}
    r_d(\lambda) := \max_{\substack{x,y\in\mathbb{R}^d\\\|x\|_2 \le 1\\\|y\|_2 \le 1}} \|x - y\|^2_2 + \lambda \|x \otimes x - y \otimes y\|_{\Fr}^2.
\end{equation}

The following theorems provide specific values for $r_d$ in 
the special case of $d=1$ (Theorem~\ref{thm:coef_sup}) and 
for general $d\geq 2$ (Theorem~\ref{thm:cov_matrix_sens}):

\begin{restatable}[$d=1$]{theorem}{theoremforkequalone}
\label{thm:coef_sup}
    For any $\lambda > 0$, it holds:
    \begin{equation}
        \label{eq:r_k=1}
        r_1(\lambda)    
        %&= \sup_{|x| \le 1, |y| \le 1}[(x - y)^2 + \lambda (x^2 - y^2)^2]\\
        \!=\!\begin{cases}
            4  & \text{if } \lambda \le \frac{11 + 5\sqrt{5}}{8}, \\
            \frac{1}{8} (3- \tau)^2 (\lambda \tau + 1 + \lambda)   & \text{otherwise.} 
            %for } \lambda > \frac{11 + 5\sqrt{5}}{8}, \\
        \end{cases}
    \end{equation}
where $\tau = \sqrt{1 - 2 /\lambda}$. Moreover, the function $r_1(\lambda)$ is a continuous function with respect to the parameter $\lambda > 0$. 
\end{restatable}

\begin{restatable}[Joint Sensitivity for Moments Estimation]{theorem}{theoremcovariancematrixsensitivity}
\label{thm:cov_matrix_sens}

For $d > 1$ and $\lambda > 0$:
\begin{align}
    r_d(\lambda) 
    &= \begin{cases}
        4 \quad &\text{if} \quad \lambda \leq \frac{1}{2},\\
        2 + 2\lambda + \frac{1}{2\lambda} \quad  & \text{otherwise.}\\
    \end{cases}
    \label{eq:rd_lambda}
\end{align}
\end{restatable}

The proofs of both theorems can be found further in the appendix.
%
Theorem~\ref{thm:coef_sup} requires only straightforward optimization. 
%
For theorem~\ref{thm:cov_matrix_sens}, we rewrite the Frobenius norm of 
the difference of Kronecker products and optimize over all possible values
for $\langle x, y \rangle$.

As a corollary of Theorems \ref{thm:coef_sup} and \ref{thm:cov_matrix_sens},
we obtain a characterization of the general solutions to Problem~\ref{prb:cov_sens}.

\begin{corollary}[Solution to Problem \ref{prb:cov_sens}]
\label{cor:dif_mat}
Assume that the matrices $C_1$ and $C_2$ have norm-decreasing columns. Then,
for any scaling parameter $\lambda > 0$, it holds that 
\begin{align}
     \text{sens}_{\lambda}^2(C_1, C_2) &= \zeta^2\alpha^2_1 r_d(\lambda \zeta^2 \beta^2_1/\alpha^2_1) 
     \label{eq:sens_from_rd}
\end{align}
where $r_d$ is specified in \eqref{eq:r_k=1} or \eqref{eq:rd_lambda}, 
and $\alpha^2_1$ and $\beta^2_1$ are the squared norms of the first columns of the matrices $C_1$ and $C_2$,
respectively. %= \|(C_1)_i\|_2^2$, $\beta_i = \|(C_2)_i\|_2^2$ are column norms of the matrices.
\end{corollary}
\begin{proof}
A straightforward calculation shows 
\begin{align*}
\text{sens}_{\lambda}^2(C_1, C_2) &=\max_{i=1, \dots, n} \sup_{\|x\| \le \zeta, \|y\| \le \zeta} \alpha^2_i \|x - y\|_2^2 + \lambda \beta^2_i \|x \otimes x - y \otimes y\|_2^2\\
    &\;=  \frac{\zeta^2}{\alpha^2_1} \left[\sup_{\|x\| \le 1, \|y\| \le 1} \|x - y\|_2^2 + \frac{\lambda\zeta^2 \beta^2_1}{\alpha^2_1}\|x \otimes x - y \otimes y\|_2^2\right]\\
    &\;= \zeta^2\alpha^2_1 r_d(\lambda \zeta^2 \beta^2_1/\alpha^2_1) 
\end{align*}
completing the proof of \Cref{cor:dif_mat}.
\end{proof}



Corollary~\ref{prb:cov_sens} implies that the joint estimate~(\ref{eq:jointmoments}) 
will be $(\epsilon,\delta)$-private, if we use noise of strength at least, 
$\sigma=\zeta^2\alpha_1 r_d(\lambda \zeta^2 \beta_1/\alpha_1)\sigma_{\epsilon,\delta}$, 
where $\sigma_{\epsilon,\delta}$ is the noise strength required for the Gaussian mechanism 
with sensitivity $1$. This finishes the proof of the \cref{lem:jointsensitivity}.

The claim of Theorem~\ref{thm:mainprivacy} follows, because Algorithm~\ref{alg:JME} 
corresponds exactly to the above construction, only making use of the 
identity $B(CX+Z) = A(X+C^{-1}Z)$, and for the special case of $\lambda:=\lambda^{*}$, defined as 
\begin{equation}
    \label{def:crit_lambda}
    \lambda^{*} := \frac{\alpha^2_1}{\beta^2_1 \zeta^2 c_d},
\end{equation}
with $c_d = 2$ if $d > 1$ and $\frac{8}{11 + 5\sqrt{5}}$ for $d = 1$,
%
which is the smallest values for $\lambda$, such that $r_d\left(\frac{\lambda \zeta^2 \beta^2_1}{\alpha^2_1}\right) = 4$. 
%
The sensitivity is then equal to $\text{sens}_{\lambda^*}(C_1, C_2) = 2\zeta \alpha_1 = 2\zeta \|C_1\|_{1 \to 2}$, where the last identity holds because of $C_1$'s decreasing column norm structure.

We furthermore note that this value is exactly the sensitivity of estimating 
the first moment alone, because 
%
\begin{align}
\max_{X\sim X'}\|C_1X\!-\!C_1X'\|^2 \!=\! \max_{i=1,\dots,n}\max_{\substack{\|x\|\leq \zeta\\\|y\|\leq \zeta}}
\alpha_i\|x-y\|^2 = 4\alpha_1^2\zeta^2
\end{align}
%
This means that \acronym estimates the second moment without increasing the noise for 
the first moment, proving our claim that we obtain the \textbf{second moment privacy for free}. 
  
\subsection{Proof of Theorem~\ref{thm:main}}
To prove Theorem~\ref{thm:main}, we have to determine how the left-hand side of \eqref{eq:jointmoments} changes
in expectation due to the added noise on the right-hand
side. Due to the additive nature of the moment estimation
process, we can do so explicitly. 

For $Z_1\sim[\mathcal{N}(0,\sigma^2\text{I})]^{d}$ it holds that 
%
\begin{align}
    \mathbb{E}_{Z}\|Y - \hat{Y}\|^2_{\Fr} &=\mathbb{E}_{Z} \|B_1 Z_1\|^2
    = d\sigma^2 \|B_1\|^2_{\Fr}.
\end{align}
Inserting $B_1=A_1C_1^{-1}$ and $\sigma=2\zeta\sigma_{\epsilon,\delta}\|C_1\|_{1\to 2}$, 
Equation~\eqref{eq:JMEquality1} follows.
%
Analogoulsy, for $Z_2\sim[\mathcal{N}(0,\sigma^2\text{I})]^{d\times d}$, 
\begin{align}
    \mathbb{E}_{Z}\|S - \hat{S}\|^2_{\Fr} &=\mathbb{E}_{Z} \|\frac{1}{\sqrt{\lambda^*}}B_2 Z_2\|^2
    = \frac{d^2\sigma^2}{\lambda^*}\|B_2\|^2_{\Fr}.
\end{align}
With $\sigma$ as above, Equation~\eqref{eq:JMEquality2} follows from $B_2=A_2C_2^{-1}$, 
and JME's specific choice of $\lambda^*=\frac{\|C_1\|^2_{1\to 2}}{\|C_2\|^2_{1\to 2} \zeta^2 c_d}$.





\subsection{Proof of Theorem~\ref{thm:Adam_sens}}
\label{sec:adam_sens_proof}

The proof of Theorem~\ref{thm:Adam_sens} goes similarly to \ref{thm:mainprivacy}, but now we estimate only the diagonal of the second moment matrix. We will show that the sensitivity remains unchanged by proving that the corresponding function $r_d(\lambda)$ in \eqref{def:r_d_appendix} also remains unchanged. Specifically:


\begin{align*}
r^{\text{diag}}_d(\lambda) = \sup\limits_{\|x\|_2 \leq 1, \|y\|_2 \leq 1}\!\!\!\big[\|x - y\|_2^2 + \lambda \|\diag(x \otimes x) - \diag(y \otimes y)\|_2^2\big] = r_d(\lambda)
\end{align*}

For simplicity, we denote $x \circ x = \diag(x \otimes x)$.

In dimension $d = 1$, the two functions are identical by construction. In dimension $d = 2$, we compute the new function explicitly using the following lemma:

\begin{restatable}{lemma}{lemmaforkequaltwo}
\label{lem:d=2}
Consider $x, y\in \mathbb{R}^2$, and let $\lambda > 0$ then,

 \begin{equation}
 r_2^{\text{diag}}(\lambda)= \sup_{\|x\|_2 \leq 1, \|y\|_2 \leq 1} \left[\|x - y\|_2^2 + \lambda \|x \circ x - y \circ y\|_2^2\right] = 
 \begin{cases}
     4, & \text{if } \lambda \leq \frac{1}{2},\\
     2 + 2\lambda + \frac{1}{2\lambda}, & \text{if } \lambda > \frac{1}{2}.
 \end{cases}
\end{equation}

\end{restatable}

Next, we use a dimension reduction argument to prove that $r^{\text{diag}}_d(\lambda) = r^{\text{diag}}_2(\lambda)$ for all $d \geq 2$, via the following lemma:

\begin{restatable}[Dimension Reduction]{lemma}{lemmadimensionreduction}
    \label{lem:dim_red}
    For any vectors $x, y \in \mathbb{R}^d$, where  $d \ge 3$, there exist vectors $x', y'\in \mathbb{R}^{d - 1}$ that for any $\lambda > 0$ satisfies the inequality:
    \begin{equation*}
    \|x - y\|_2^2 + \lambda \|x \circ x - y \circ y\|_2^2 \le \|x'- y'\|_2^2 + \lambda \|x' \circ x' - y' \circ y'\|_2^2.
    \end{equation*}
    
\end{restatable}
We apply this lemma recursively to prove that $r^{\text{diag}}_d(\lambda) = r^{\text{diag}}_2(\lambda)$ for all $d \geq 2$. The proof of the lemma can be found later in the appendix.

By combining these lemmas, we conclude the proof of the theorem.




