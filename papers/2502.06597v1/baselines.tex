\subsection{Comparison with Alternative Techniques}
Besides \acronym, a number of alternatives methods are possible that could be used to privately estimate the first and second moments.
%
In this section, we introduce some of them and discuss their relation to \acronym.

\subsubsection{Independent Moment Estimation (IME)} 
A straight-forward way to privately estimate first and second moments is to estimate and privatize both of them separately, where the 
necessary amount of noise is determined by the composition theorem of the Gaussian mechanism~\citep{abadi2016deep} (see \Cref{alg:alphaIME} in the appendix).

IME resembles \acronym in the sense that (i) separate estimates of the 
moments are created, and (ii) the privatized results are unbiased 
estimators.
%
However, it does not have \acronym's \emph{privacy for free} property. 
%
This is because IME relies on the composition theorem, so 
the privacy budget is split into two parts, one per estimate, 
where the exact split is a hyperparameter of the method.
%
As a consequence, IME's estimate of the first moment is always more noisy, and thereby of lower expected accuracy, than JME's. 
%
For the second moment, IME could in principle achieve a lower noise than plain \acronym by adjusting the budget split parameter in an uneven way. 
%
This, however, would come at the expense of further increase in the error for estimating  the first moment. 

The following theorem establishes that \textbf{$\lambda$-\acronym, the variant of \acronym with adjustable $\lambda$ parameter offers a strictly better trade-off than IME.} 


\begin{restatable}[\acronym vs IME]{theorem}{theoremJMEvsIME}\label{lem:JMEvsIME}
For any $\epsilon,\delta>0$, $\lambda$-\acronym Pareto-dominates IME with 
respect to the approximation error for the first vs second moment estimates.
\end{restatable}

The proof can be found in the appendix. Figure~\ref{fig:composition_vs_JME} visualizes this property graphically. 


\subsubsection{Concatenate-and-Split (CS)}
%
In the special case where the two noise shaping matrices are meant to be the same (\eg the trivial case where both are the identity matrix), it is possible to use a single privatization step for both moments.
%
For this, one forms a new observation vector, $\tilde x_i$ by concatenating $x_i$ with a vectorized (and potentially rescaled version of) $x_i \otimes x_i$. 
%
Then, one privatizes the resulting vector, taking into account that $\tilde x_i\in\mathbb{R}^{d(d+1)}$ has higher dimension and a larger norm than the original $x_i\in\mathbb{R}^{d}$. 
%
The result is split again into first and second order components, and the latter is unscaled. 
%
The result are private estimate of $x_t$ and $x_t\otimes x_t$, from which the two weighted  moment estimates can be constructed.
%
\Cref{alg:tauCS} in the appendix provides pseudocode 
for this \emph{concatenate-and-split (CS)} method. 

When applicable, CS is easy to implement and produces unbiased 
moment estimates.
%
However, like IME, it has an unfavorable privacy-accuracy trade-off curve compared to \acronym, as formalized in the following theorem.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{images/Composition_vs_JME.pdf}
    %
    \caption{Approximation errors for the first and second moments  ($d = 10$, $n = 100$, $C_1 = C_2 = I, A=E_1$, $\zeta = 1$, and $\sigma = 1$) across three methods: $\lambda$-\acronym with $\lambda$, IME with varying $\alpha$ parameters, and CS with varying $\tau$ parameter. 
    One can see that $\lambda$-\acronym Pareto-dominates the other two methods.}
    \label{fig:composition_vs_JME}
\end{figure}


\begin{restatable}[\acronym vs CS]{theorem}{lemmaclippingvsjointmomentestimation}\label{thm:JME_vs_CS}
For any $\epsilon,\delta>0$, $\lambda$-\acronym Pareto-dominates CS with 
respect to the approximation error for the first vs second moment estimates.
\end{restatable}

The proof can be found in the appendix. Figure~\ref{fig:composition_vs_JME} visualizes these cases (Theorems~\ref{lem:JMEvsIME} and~\ref{thm:JME_vs_CS}) graphically. 


\subsubsection{Post-processing (PP)} 
%
Post-processing (PP) is another easy-to-use method for joint moment estimation. It has appeared in the literature~\cite{dp_covariance_wishart_dist}, at least in its naive form without the matrix factorization mechanism. 
%
For any $x_i$, it first computes a private estimate $\widehat{x_t}$ by 
adding sufficient noise to it. 
%
It then sets $\widehat{x_t\otimes x_t}:=\widehat{x_t}\otimes \widehat{x_t}$, 
which is automatically private by the postprocessing property of DP, %~\cite{postprocessing},
and uses it to estimate the second moment matrix without additional privacy protection.
%
\Cref{alg:PP} in the appendix provides pseudocode. 

\begin{figure*}[t]
    \begin{center}
        \begin{subfigure}[c]{0.24\textwidth}
        \includegraphics[width=\linewidth]{images/Moments_d=1,n=1000_without_MF_prefix_sums.pdf}
        \subcaption{$d=1$, $C_1=C_2=\text{I}$}
        \end{subfigure}
        \begin{subfigure}[c]{0.24\textwidth}
        \includegraphics[width=\linewidth]{images/Moments_d=1,n=1000_with_MF_prefix_sums.pdf}
        \subcaption{$d=1$, $C_1=C_2=A^{1/2}$}
        \end{subfigure}
        \begin{subfigure}[c]{0.24\textwidth}
        \includegraphics[width=\linewidth]{images/Moments_d=1000,n=1000_without_MF_prefix_sums.pdf}
        \subcaption{$d=1000$, $C_1=C_2=\text{I}$}
        \end{subfigure}
        \begin{subfigure}[c]{0.24\textwidth}
        \includegraphics[width=\linewidth]{images/Moments_d=1000,n=1000_with_MF_prefix_sums.pdf}
        \subcaption{$d=1000$, $C_1=C_2=A^{1/2}$}
        \end{subfigure}
    \caption{Expected error of second moment estimation with JME versus PP with and without debiasing ($A=E_1$ (prefix sum), $n=1000$)
    %
    In line with our analysis, for $d=1$ JME consistently achieves a higher quality than PP. 
    %
    For $d=1000$, JME is preferable to PP in the high privacy regime. 
    %
    Furthermore, the square root matrix factorization substantially improves the quality of both methods.}
    \label{fig:moments_plot}
    \end{center}
\end{figure*}

Like \acronym, PP has the \emph{privacy-for-free} property, i.e., the private estimate of the second moment does not reduce the quality of the first moment. 
%
In contrast to \acronym, however, PP's estimate of the second moments is not unbiased because the noise that was added to the first moment is squared during the process. 
%
It is possible to compensate for this by explicitly subtracting the bias, which can be computed analytically. The bias depends only on hyperparameters such as $n$, $d$, and $\sigma$, and not on the private data $X$ or the sampled noise used to protect it. 
%
Its exact expression can be found in equation~\eqref{eq:PP_bias_term}.

For given privacy parameters, PP and \acronym use the same noise strength to
privatize the first moment and therefore their estimates are of equal quality.
%
For the second moment, PP and \acronym differ in their characteristics between 
the low privacy (small noise variance) and the high privacy (large noise variance) regime. 
%
To allow for a quantitative comparison, we derive characterizations of the 
approximation quality of PP. 

\begin{restatable}[Expected Second Moment Error for PP]{lemma}{lemmaexpectedsecondmomentPostPr}\label{lem:PostPr_Moments}
%
Assume the same setting as for Theorem~\ref{thm:main}, except that 
we compute the estimates $\widehat{Y}$ and $\widehat{S}_{\text{PP}}$ with the PP method.
%
Then, the expected approximation error of the second moment satisfies:
%
\vspace{-\baselineskip}
\begin{align}
&\sup_{X \in \mathcal{X}}\mathbb{E}\|S - \widehat{S}_{\text{PP}}\|^2_{\Fr}
=\overbrace{d\sigma^4 \zeta^2 \|C_1\|_{1 \to 2}^4 \tr(A_2^\top A_2 E_Q)}^{\text{bias}}
\notag\\
&\ \ + d(d + 1)\sigma^4\zeta^2  \|C_1\|_{1 \to 2}^4 \tr(A_2^\top A_2 (Q \circ Q)) \label{eq:PPquality}\\
&\ \ +2(d + 1)\sigma^2\zeta^2  \|C_1\|_{1 \to 2}^2 \sup_{X \in \mathcal{X}} \tr((A_2^\top A_2 \circ Q)XX^T),\notag
\end{align}
%
where $Q = C_1^{-1} C_1^{-\top}$ and $E_Q = \diag(Q)\diag(Q)^{\top}$.
%
For debiased PP, the term marked "bias" does not occur.
\end{restatable}

In order to get a better impression of the relation between PP and JME, we first study the special case of a trivial factorization. % \chl{for the first moment?}


\begin{corollary} %[Expected Approximate Error of \acronym and PP (trivial factorization)]
\label{cor:Comparison_Errors}
Assume that \acronym and PP use trivial factorizations, \ie $C_1=C_2=I$. 
%
Then, \acronym's expected approximation error for the second moment is 
\begin{align}
\mathbb{E}\|S - \widehat{S}\|^2_{\Fr} &= 4c_d d^2 \sigma^2 \zeta^2 \| A_2 \|_{\Fr}^2\label{eq:JMEqualitytrivial}
\intertext{and the corresponding value for PP is}
\sup_{X \in \mathcal{X}}\mathbb{E}\|S - \widehat{S}_{\text{PP}}\|^2_{\Fr} &= 
\big(d(d + 1) \sigma^4 + 2(d + 1)\sigma^2\big)\zeta^2\| A_2 \|_{\Fr}^2
\notag\\
&\quad+ \underbrace{d \sigma^4 \zeta^2 \| \sum_{i} (A_2)_{[i,\cdot]}\|_2^2}_{\text{bias}},\label{eq:PPqualitytrivial}
\end{align}
where $\sum_{i} (A_2)_{[i,\cdot]}$ is the row-wise summation of $A_2$. 

%
For debiased PP, the term marked "bias" does not occur.
\end{corollary}

\emph{Proof.} The proofs follow directly from \Cref{thm:main} and 
\Cref{lem:PostPr_Moments} by observing that $\|C_1\|_{1\to 2}= \|C_2\|_{1\to 2}=1$.

In dimension $d = 1$, JME has an error of $c_1 \sigma^2 \| A_2 \|_{\Fr}^2$ versus $(2\sigma^4 + 4\sigma^2) \| A_2 \|_{\Fr}^2$ for (debiased) PP. 
%
Since $c_1 < 1$, \textbf{for one-dimensional data, \acronym's error is always lower than PP's} (see Figure \ref{fig:moments_plot} for visual confirmation). 
%
In high dimensions ($d \gg 1$), the terms quadratic in $d$ dominate, so the comparison is between $2\sigma^2 \| A_2 \|_{\Fr}^2$ 
for \acronym and $d^2 \sigma^4 \| A_2 \|_{\Fr}^2$ for PP.
%
Consequently, at least \textbf{for $\sigma \ge \sqrt{2}$, 
JME achieves privacy with less added noise than PP.}


In the regime of \emph{low privacy} ($\sigma\to 0$) in high dimension $(d\gg 1)$, PP can be expected to result in higher accuracy estimates than JME, because the terms involving $\sigma^4$ make only minor 
contributions. 


For settings with general noise shaping matrix, we cannot provide an exact comparison between 
PP and JME, because the $\sup_X$-term in \Cref{eq:PPquality} has no closed-form solution. 
%
Instead, we derive upper and lower bounds (\Cref{lem:supX_upperlowerbound} in the Appendix),
and provide a numeric comparison in \Cref{fig:moments_plot}.

\begin{table*}[t]
    \centering
\begin{tabular}{c|c|c|c|l}
& \textbf{unbiased?} & \textbf{symmetric noise?} & \textbf{privacy-for-free?} &  \\
\textbf{Method}    & \textbf{1st / 2nd moment} & \textbf{1st / 2nd moment} & \textbf{2nd moment}
& \textbf{recommended regime of use} 
\\\hline
\acronym (proposed)  &   \cmark / \cmark  & \cmark / \cmark & \cmark  & high privacy or low dimensionality\\
IME             &  \cmark / \cmark  & \cmark / \cmark  & \xmark & never (Pareto-inferior to \acronym)\\
CS             &  \cmark / \cmark  & \cmark / \cmark  & \xmark & never (Pareto-inferior to \acronym)\\
PP             &  \cmark /\xmark  & \cmark / \xmark  & \cmark & low privacy in high dimensions \\ 
PP (Debiased)   &  \cmark /\cmark  & \cmark / \xmark & \cmark & low privacy in high dimensions\\
\end{tabular}
\caption{Overview of the properties for the methods discussed in Section~\ref{sec:method}.
Properties \emph{unbiased} and \emph{symmetric noise} apply to each entry, stated separately for first and second moment. \emph{privacy-for-free} applies only for the second moment.}
\label{tab:methods_properties}
\end{table*}
