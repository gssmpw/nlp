\section{\method (\acronym)}\label{sec:method}

\begin{algorithm}[t]
\caption{\method (\acronym)}\label{alg:JME}
\begin{algorithmic}
\INPUT stream of vectors $x_1, \dots, x_n\in \mathbb{R}^d$ with \mbox{$\|x_t\|_2 \leq \zeta$} % for $\zeta > 0$
\INPUT workload matrices $A_1, A_2\in \mathbb{R}^{n \times n}$
\INPUT privacy parameters $(\epsilon,\delta)$
\INPUT noise shaping matrices $C_1, C_2\in\mathbb{R}^{n \times n}$ (invertible with decreasing column norm)\quad default: $\text{I}_{n\times n},\text{I}_{n\times n}$

\smallskip
\STATE $\sigma_{\epsilon, \delta}\leftarrow$ noise strength for $(\epsilon,\delta)$-DP Gaussian mechanism
\STATE $\lambda\leftarrow \|C_1\|_{1\to 2}^2/(c_d\zeta^2\|C_2\|_{1 \to 2}^2)$ \hfill// scaling parameter, 

\qquad where $c_1=\frac{8}{11 + 5\sqrt{5}}$, and $c_d = 2$ for $d\geq 2$
\STATE $s\leftarrow 2\zeta \|C_1\|_{1 \to 2}$ \hfill// joint sensitivity
\STATE $Z_1\sim \big[\mathcal{N}(0, \sigma_{\epsilon, \delta}^2 s^2) \big]^{n\times d}$ \hfill// 1st moment noise
\STATE $Z_2\sim \big[\mathcal{N}(0, \sigma_{\epsilon, \delta}^2 s^2)
\big]^{n\times d^2}$ \hfill// 2nd moment noise 
\smallskip
\FOR{$t = 1, 2, \dots ,n$}
\STATE $\widehat{x_t} \leftarrow x_t +  [C_{1}^{-1}Z_1]_{[t,\cdot]}$
\STATE $\widehat{x_t \otimes x_t} \leftarrow x_t \otimes x_t + \lambda^{-1/2}[C_{2}^{-1}Z_2]_{[t,\cdot,\cdot]}$
\STATE \textbf{yield} \ \ $\widehat{Y_t}=\sum\limits_{i = 1}^{t} [A_1]_{t,i}\widehat{x_i}, \ \ \widehat{S_t}=\sum\limits_{i = 1}^{t} [A_2]_{t,i} \widehat{x_i \otimes x_i}$  
\ENDFOR
\end{algorithmic}
\end{algorithm}

We now introduce our main algorithmic contribution: the \method (\acronym) algorithm for solving the problem of differentially private (weighted) moment estimation in a continual release setting. 
%
\Cref{alg:JME} shows its pseudo-code. 
%
\acronym takes as input a stream of input data vectors, the weights for the desired estimate in the form of two lower triangular workload matrices, and privacy parameters $\epsilon,\delta>0$.
%
Optionally, it takes two noise-shaping matrices as input.
%
If these are not provided, $C_1=C_2=\text{I}_{n\times n}$, can serve as defaults. 

The algorithm uses the matrix mechanism to privately estimate 
the weighted sums of vectors for both the first and second moments. 
%
At each step, the algorithm receives a new data point, $x_t$, it creates private version of both $x_t$ and $x_t \otimes x_t$ by 
adding suitably scaled Gaussian noise.
%
The noise shaping matrices, if provided, determine the covariance structure of the noise. 
%

To balance between the estimates of the first moment and 
of the second moment, a scaling parameter, $\lambda$, is
used. 
%
In Algorithm~\ref{alg:JME}, this parameter is fixed such that 
the total sensitivity of estimating both moments is equal to the sensitivity of just 
estimating the first moment alone. 
%
This implies that the overall noise variance to make both moment estimates private is equal to that needed to achieve the same level of privacy for the first moment. 
%
Consequently, the fact that we also privately estimate the 
second moment does not increase the necessary noise level 
for the first moment, a property we call \textbf{(second moment) privacy for free}. 

Note that \emph{privacy for free} is a quite remarkable property of \acronym. 
%
Generally, when using the same data more than once for private
computations, more noise for each of them is required to ensure 
the overall privacy, following, \eg, the \emph{composition 
theorems of DP}~\citep{kairouz15}. 
%
In some situations, however, one might also prefer a more flexible way to trade-off between the two moment estimates.
%
For this, we present $\lambda$-\acronym in the appendix 
(\Cref{alg:lambdaJME}), a variant of \acronym that has $\lambda$ as a free hyper-parameter.


\subsection{Properties of \acronym}
%
For some inputs $X=(x_1,\dots,x_n)\in\mathbb{R}^{n\times d}$,
let $Y=(Y_1,\dots,Y_n)\in\mathbb{R}^{n\times d}$ and 
$S=(S_1,\dots,S_n)\in\mathbb{R}^{n\times d^2}$ be the 
matrices of their first and second moments after each step,
and let $\widehat Y=(Y_1,\dots,Y_n)\in\mathbb{R}^{n\times d}$ 
and $\widehat S=(S_1,\dots,S_n)\in\mathbb{R}^{n\times d^2}$ 
be the private estimates computed by \Cref{alg:JME}
with workload matrix $A$ and noise shaping matrix $C$. 
%
Then, the following properties hold.


\begin{theorem}[Noise properties of \acronym]\label{thm:jmenoise}
$\widehat Y$ and $\widehat S$ are unbiased estimates of $Y$ and $S$. 
%
Their estimation noise, $\widehat Y-Y$ and $\widehat S-S$, has 
a symmetric distribution.
\end{theorem}

\begin{theorem}[Privacy of \acronym]\label{thm:mainprivacy}
\Cref{alg:JME} is $(\epsilon, \delta)$-differentially private. 
\end{theorem}

\begin{theorem}[Utility of \acronym]\label{thm:main}
For any input $X$, the expected approximation error of \Cref{alg:JME} 
for the first and second moments are 
\begin{align}
     \sqrt{\mathbb{E}\|Y - \widehat{Y}\|^2_{\Fr}} &= 2\zeta \sqrt{d} \sigma_{\epsilon, \delta} \|C_1\|_{1 \to 2} \|A_1 C_1^{-1}\|_{\Fr}.
     \label{eq:JMEquality1} \\
     \sqrt{\mathbb{E}\|S - \widehat{S}\|^2_{\Fr}} &=2\zeta\sqrt{c_d}  d  \sigma_{\epsilon, \delta} \|C_2\|_{1 \to 2} \|A_2 C_2^{-1}\|_{\Fr},\label{eq:JMEquality2}
\end{align}
with $c_d$ as defined in \Cref{alg:JME}. \end{theorem}


\subsection{Proofs}
\begin{proof}[Proof of \Cref{thm:jmenoise}]
%
The statement follows from the fact that for any $t=1,\dots,n$,
\acronym's estimates $\widehat{x_t}$ and $\widehat{x_t\otimes x_t}$ 
of $x_t$ and $x_t\otimes x_t$ are unbiased with symmetric noise 
distribution because they are constructed by adding zero-mean 
Gaussian noise.
\end{proof}

\begin{proof}[Sketch of the Proof of \Cref{thm:mainprivacy}]
By the properties of the Gaussian mechanism, it suffices to show 
that the overall sensitivity of jointly estimating both moments 
has at most the value $s=2\zeta\|C_1\|_{1\to 2}$, as stated in \Cref{alg:JME}. 
%
\begin{definition}
For any noise-shaping matrices $C_1$ and $C_2$ and any $\lambda>0$, we define
the \emph{joint sensitivity} of estimating the first and $\lambda$-weighted 
second moment by
\begin{align}
    &\text{sens}_{\lambda}^2(C_1,C_2)
    = \sup_{X \sim X'}\!\Big\|
    \begingroup\setlength{\arraycolsep}{1pt}\begin{pmatrix}
        C_1 & \textbf{0}\\
        \textbf{0} & \sqrt{\lambda} C_2\\
    \end{pmatrix}\endgroup
    \begingroup\setlength{\arraycolsep}{1pt}\begin{pmatrix}
        X \!-\! X'\\
        X \face X \!-\! X' \face X'
    \end{pmatrix}\endgroup \Big\|_{\Fr}^2 
    \\
    &=\sup_{X \sim X'}\!\!\Big[\|C_1(X \!-\! X')\|_{\Fr}^2 +  \lambda \|C_2
    (X \face X \!-\! X' \face X')\|_{\Fr}^2\Big].\notag
\end{align}
\end{definition}

The following Lemma (proved in the appendix) characterizes the values of the joint sensitivity as a function of $\lambda$. 
\begin{lemma}[Joint Sensitivity]\label{lem:jointsensitivity}
Assume that the matrices $C_1$ and $C_2$ have norm-decreasing columns. Then, for any $\lambda > 0$ holds:  
\begin{align}
     \text{sens}_{\lambda}^2(C_1, C_2) &= \zeta^2 \|C_1\|_{1 \to 2}^2 r_d\left(\frac{\lambda \zeta^2 \|C_2\|_{1 \to 2}^2}{\|C_1\|_{1 \to 2}^2}\right),
\end{align}
where $\|\cdot\|_{1 \to 2}$ denotes the maximum column norm, corresponding to the norm of the first column of $C_1$ and $C_2$, respectively. The function $r_d(\nu)$ is given by 
\begin{equation}
        \label{eq:r_d}
        r_d(\nu) = 
        \begin{cases}
            \frac{1}{8} (3 - \tau)^2 (\nu \tau + 1 + \nu),  & \text{if } \nu > \frac{11 + 5\sqrt{5}}{8}, \, d = 1, \\[0.5em]
             2 + 2\nu + \frac{1}{2\nu}, & \text{if } \nu > \frac{1}{2}, \, d > 1, \\[0.5em]
             4,  & \text{otherwise,} %\\[-1\em]
        \end{cases}
\end{equation}
where $\tau = \sqrt{1 - \frac{2}{\nu}}$.
\end{lemma}

\Cref{alg:JME} uses the scaling parameter $\lambda=\|C_1\|_{1\to 2}^2/(c_d\zeta^2\|C_2\|_{1 \to 2}^2)$, so, by \Cref{lem:jointsensitivity}, the sensitivity of estimating both moment has the value $\sqrt{\zeta^2 \|C_1\|_{1 \to 2}^2 r_d(c_d)}$. The choice of $c_d$ implies that $r_d(c_d)=4$, which establishes that 
$\text{sens}_{\lambda}(C_1,C_2)=s$, which concludes the proof of \Cref{thm:mainprivacy}.
\end{proof}

\begin{proof}[Sketch of the Proof of \Cref{thm:main}] The identities follow from the general 
properties of the matrix mechanism.
%
For any input $X$, the output of \Cref{alg:JME} for the first moment is $\widehat Y=Y + A_1C^{-1}_1Z_1$, where $Z_1\sim[\mathcal{N}(0, \sigma_{\epsilon, \delta}^2 s^2)]^{n\times d}$. Consequently, $\widehat Y-Y = A_1C^{-1}_1Z_1$, and hence 
\begin{align*}
\E_{Z_1}\|\widehat Y-Y\|^2_{\Fr} 
&= \|A_1C^{-1}_1\|^2_{\Fr}\cdot \sigma^2_{\epsilon, \delta} s^2 d
\end{align*}
Equation~\eqref{eq:JMEquality1} follows by inserting $s=2\zeta \|C_1\|_{1 \to 2}$. 
%
For the second moment, Equation~\eqref{eq:JMEquality2} follows analogously using
that the output of \Cref{alg:JME} is $\widehat S=S+\lambda^{-\frac12}A_2C^{-1}_{2}Z_2$
with $Z_2\sim[\mathcal{N}(0, \sigma_{\epsilon, \delta}^2 s^2)]^{n\times d^2}$. 
\end{proof}