\begin{table}[t]
    \centering
    \begin{tabular}{c|c|c}
       \textbf{method} & \textbf{symbol} & \textbf{coefficients} \\\hline
         prefix sum  & $E_1$ &  $\mathbbm{1}{\{i \leq t\}}$ \\  %$a^t_i = 1$ \\
         exponential (weight $\beta$) & $E_{\beta}$ & $\beta^{t-i}\mathbbm{1}\{i \leq t\}$ \\
         standard average & $V$ & $\frac{1}{t}\mathbbm{1}\{i \leq t\}$ \\
         sliding window (size $k$) & $W_k$ &  $\frac{1}{k}\mathbbm{1}\{t-k\,<\,i\,\leq\,t\}$ \\
    \end{tabular}
    \caption{Common workload matrices for moment estimation in~\eqref{eq:moments}.}
    \label{tab:matrixentries}
\end{table}

\section{Background}\label{sec:background}

%\subsection{Problem Formulation}

We study the problem of continually estimating the pair of weighted sums of the first and second moments in a differentially private manner. Namely, consider a sequence of $d$-dimensional vectors $x_1, \dots, x_n\in\mathcal{X}$, where $\mathcal{X}=\{(x_1,\dots,x_n)\;:\;\max_i \|x_i\|_2\leq \zeta \wedge x_i \in\mathbb{R}^{d}\}$ for some fixed constant $\zeta>0$. At each step $t$, we aim to estimate the following pair of sums in a differentially private way:
%
\begin{align}
 Y_t &= \sum\limits_{i = 1}^{t} a^t_{i} x_i \in \mathbb{R}^{d} \ \ \text{and} \  \ S_t =\sum\limits_{i = 1}^{t} b^t_{i} x_i x_i^{\top} \in \mathbb{R}^{d \times d},
\label{eq:moments}
\end{align}
%
for arbitrary coefficients $a^t_{i} \in \mathbb R$ and $b^t_{i} \in \mathbb R$.
%
This formulation includes many practical schemes, see Table~\ref{tab:matrixentries}.


To express this problem compactly, we rewrite it in matrix form. 
%
We collect the coefficients into lower triangular workload matrices, 
$A_1 = (a_{i}^t)_{1 \le i,t\le n} \in \mathbb{R}^{n \times n}$ and 
$A_2 = (b_{i}^t)_{1 \le i,t\le n} \in \mathbb{R}^{n \times n}$. 
%
We stack the data as rows into a matrix $X\in\mathbb{R}^{n\times d}$.
%
Then, all terms of the sums in~\eqref{eq:moments} can then be expressed compactly\footnote{For simplicity, we slightly abuse the notation and make $S$ a matrix of shape $n\times d^2$ instead of a tensor of size $n\times d\times d$.} as $Y=A_1X$ and $S=A_2(X \face X)$, where the second 
 matrix consists of the data vectors stacked as rows, and $\bullet$ denotes the \emph{Face-Splitting Product} or \emph{transposed Khatri-Rao product}~\citep{esteve2009}, $(X \face X)_{ij}:= (x_i \otimes x_i)_j = \textsf{vec}(x_i x_i^\top)_j$, where $\otimes$ is the \textit{Kronecker product} of two vectors.

To protect the privacy of individual data elements, we rely on the notion of \emph{differential privacy}, considering neighboring datasets as those differing by a single data point; see \eg,~\citet{dwork2014algorithmic} for an introduction.
%
Recently, a series of works have developed effective methods for privately estimating individual matrices in the aforementioned product form by means of the \emph{the matrix mechanism}~\citep{li2015matrix, CNPBINDPL, Denisov,MultiEpoch,dvijotham2024efficient,kalinin2024,henzinger2024, henzinger2025improved, fichtenberger2023constant, henzinger2023almost, BandedMatrix,Kairouz,efficient_streaming}.
%
Its core insight is that, in the continual release setting, outputs at later steps should require less noise to be made private than earlier ones because more data points contributed to their computation. 
%
One can exploit this fact by adding noise that is \emph{correlated} between the steps instead of being independent.
%
This way, at each step, some of the noise added at an earlier stage can be removed again, thereby resulting in a less noisy result without lowering the privacy guarantees.

Technically, the matrix mechanism relies on an invertible \emph{noise shaping matrix} $C$.
%
For our theoretical results, we often also assume that $C$ has decreasing column norms, 
%
As the private estimate of a product $AX$, one computes $\widehat{AX}=A(X+C^{-1}Z)$, where $Z$ is a matrix of \iid Gaussian noise.
%
This estimate is $(\epsilon,\delta)$-differentially private if the noise magnitude is at least $\text{sens}(CX)\cdot\sigma_{\epsilon,\delta}$,
where $\sigma_{\epsilon,\delta}$ denotes the variance required in the Gaussian distribution to ensure $(\epsilon, \delta)$-DP for sensitivity $1$ queries\footnote{Here and in the following we do not provide a formula for $\sigma_{\epsilon,\delta}$, because closed-form expressions have been shown to be suboptimal in some regimes. Instead, we recommend determining its value numerically, such as in~\citep{balle2018improving}.
%
For reference, typical values of $\sigma_{\epsilon,\delta}$ lie between approximately $0.5$ (low privacy, \eg $\epsilon=8, \delta=10^{-3})$ and $50$ (high privacy, \eg $\epsilon=0.1, \delta=10^{-9})$.}.
%
$\text{sens}(\cdot)$ denotes the \emph{sensitivity}, which, for any function $F$, is defined as
\begin{align}
\text{sens}(F)
= \max_{X\sim X'}\|F(X)\!-\!F(X')\|_{\Fr} 
\end{align}
where $X\sim X'$ denote any two data matrices, that are \emph{neighboring}, \ie identical except for one of the data vectors.

